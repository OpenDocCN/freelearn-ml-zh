# 前言

教育不是学习事实，

但是训练思维。

- 阿尔伯特·爱因斯坦

数据是我们时代的新硅，而机器学习与生物启发式认知系统结合，不仅能够实现，还能够加速第四次工业革命的诞生。这本书献给我们的父母，他们通过极大的艰辛和牺牲，使我们的教育成为可能，并教导我们始终要行善。

*Apache Spark 2.x 机器学习食谱*由四位具有不同背景的朋友共同创作，他们在多个行业和学术学科中拥有丰富的经验。团队在手头的主题上拥有丰富的经验。这本书不仅关乎友谊，也关乎支撑 Spark 和机器学习的科学。我们希望将我们的想法汇集起来，为社区撰写一本书，不仅结合了 Spark 的 ML 代码和真实数据集，还提供了相关的解释、参考和阅读，以便更深入地理解并促进进一步的研究。这本书反映了我们团队在开始使用 Apache Spark 时希望拥有的东西。

我对机器学习和人工智能的兴趣始于八十年代中期，当时我有机会阅读两篇重要的文章，恰好在 1986 年 2 月的*人工智能国际期刊*第 28 卷第 1 期中依次列出。对于我这一代工程师和科学家来说，这是一个漫长的旅程，幸运的是，弹性分布式计算、云计算、GPU、认知计算、优化和先进的机器学习的进步使长达数十年的梦想成真。所有这些进步对当前一代机器学习爱好者和数据科学家都变得可及。

我们生活在历史上最罕见的时期之一——多种技术和社会趋势在同一时间点融合。具有内置 ML 和深度学习网络访问权限的云计算的弹性将提供一整套新的机会，以创造和占领新的市场。Apache Spark 作为近实时弹性分布式计算和数据虚拟化的*通用语言*的出现，为聪明的公司提供了在不需要大量投资于专门的数据中心或硬件的情况下应用 ML 技术的机会。

*Apache Spark 2.x 机器学习食谱*是对 Apache Spark 机器学习 API 最全面的处理之一，选择了 Spark 的子组件，为您提供在掌握机器学习和 Apache Spark 的高端职业之前所需的基础。本书的目标是提供清晰和易懂的内容，反映了我们自己的经验（包括阅读源代码）和学习曲线，我们从 Spark 1.0 开始。

*Apache Spark 2.x 机器学习食谱*处于 Apache Spark、机器学习和 Scala 的交汇处，面向开发人员和数据科学家，通过实践者的视角，他们不仅需要理解代码，还需要了解给定 Spark ML 算法或 API 的细节、理论和内部工作，以在新经济中建立成功的职业。

本书采用食谱格式，将可下载的即时运行的 Apache Spark ML 代码配方与背景、可操作的理论、参考、研究和真实数据集相结合，以帮助读者理解 Spark 为机器学习库提供的广泛功能背后的*什么*、*如何*和*为什么*。本书从奠定成功所需的基础开始，然后迅速发展到涵盖 Apache Spark 中所有有意义的 ML 算法。

# 本书内容

第一章，《使用 Scala 实现 Spark 的实用机器学习》，涵盖了在实际开发环境中安装和配置机器学习和 Apache Spark 的内容。通过屏幕截图，它引导您下载、安装和配置 Apache Spark 和 IntelliJ IDEA，以及必要的库，这些都反映了开发者在真实世界环境中的桌面。然后，它继续识别和列出了 40 多个真实数据集的数据存储库，这些数据集可以帮助读者通过实验和进一步的代码配方。最后，我们在 Spark 上运行我们的第一个 ML 程序，然后提供如何将图形添加到您的机器学习程序的指导，这些图形在后续章节中使用。

第二章，《Spark 中机器学习的线性代数基础》，涵盖了线性代数（向量和矩阵）的使用，这是机器学习中一些最重要的工作的基础。它通过该章的配方全面介绍了 Apache Spark 中可用的 DenseVector、SparseVector 和矩阵功能。它提供了本地和分布式矩阵的配方，包括 RowMatrix、IndexedRowMatrix、CoordinateMatrix 和 BlockMatrix，以提供对这个主题的详细解释。我们包括了这一章，因为只有通过逐行阅读大部分源代码，并理解矩阵分解和向量/矩阵算术在 Spark 中更粗粒度算法下的工作方式，才能掌握 Spark 和 ML/MLlib。

第三章，《Spark 的三大数据武士用于机器学习-完美结合》，提供了 Apache Spark 中具有弹性分布式数据处理和整理功能的三大支柱的端到端处理。该章节包括了从实践者的角度详细介绍 RDDs、DataFrame 和 Dataset 功能的详细配方。通过详尽的 17 个配方、示例、参考和解释，它奠定了在机器学习科学领域建立成功职业的基础。该章节提供了功能性（代码）和非功能性（SQL 接口）的编程方法，以巩固知识基础，反映了一名成功的 Spark ML 工程师在一线公司的真实需求。

第四章，《实现强大机器学习系统的常见配方》，通过 16 个简短但直截了当的代码配方，涵盖了大多数机器学习系统中常见的任务，并将这些任务因素化，读者可以在自己的真实世界系统中使用。它涵盖了一系列技术，从数据归一化到评估模型输出，使用了 Spark 的 ML/MLlib 功能的最佳实践指标，这些可能不会立即对读者可见。这是我们在日常工作中大多数情况下使用的配方的组合，但单独列出以节省空间和其他配方的复杂性。

第五章，《Spark 2.0 中回归和分类的实用机器学习-第一部分》，是探索 Apache Spark 中分类和回归的两章中的第一章。该章从广义线性回归（GLM）开始，扩展到具有不同类型优化的 Lasso、Ridge。然后，该章继续涵盖等温回归、生存回归、多层感知器（神经网络）和一对多分类器。

第六章，“Spark 2.0 中的回归和分类实用机器学习-第二部分”，是两个回归和分类章节中的第二部分。该章节涵盖了基于 RDD 的回归系统，从线性、逻辑和岭到套索，使用 Spark 中的随机梯度下降和 L_BFGS 优化。最后三个配方涵盖了支持向量机（SVM）和朴素贝叶斯，最后详细介绍了在 Spark ML 生态系统中占据重要位置的 ML 管道的配方。

第七章，“使用 Spark 扩展的推荐引擎”，介绍了如何探索数据集并利用 Spark 的 ML 库设施构建电影推荐引擎。它使用了大型数据集和一些配方，以及图表和解释，探索了推荐系统的各种方法，然后深入研究了 Spark 中的协同过滤技术。

第八章，“Apache Spark 2.0 中的无监督聚类”，涵盖了无监督学习中使用的技术，如 KMeans、混合和期望（EM）、幂迭代聚类（PIC）和潜在狄利克雷分布（LDA），同时也涵盖了为了帮助读者理解核心概念而介绍的原因和方法。利用 Spark Streaming，该章节以实时 KMeans 聚类配方开始，通过无监督手段将输入流分类为标记类。

第九章，“优化-使用梯度下降下山”，是一章独特的章节，它带领读者了解优化在机器学习中的应用。它从闭合形式公式和二次函数优化（例如成本函数）开始，到使用梯度下降（GD）来从头解决回归问题。该章节通过使用 Scala 代码来培养读者的技能，并深入解释如何编写和理解从头开始的随机下降（GD）。该章节以 Spark 的 ML API 结束，以实现我们从头开始编码的相同概念。

第十章，“使用决策树和集成模型构建机器学习系统”，深入介绍了 Spark 的机器学习库中用于分类和回归的树和集成模型。我们使用三个真实世界的数据集，使用决策树、随机森林树和梯度提升树来探索分类和回归问题。该章节提供了这些方法的深入解释，以及逐步探索 Apache Spark 的机器学习库的即插即用代码配方。

第十一章，“大数据中的高维度诅咒”，揭开了降维的艺术和科学之谜，并全面介绍了 Spark 的 ML/MLlib 库，该库在大规模机器学习中促进了这一重要概念。该章节充分深入地介绍了理论（什么和为什么），然后继续介绍了 Spark 中供读者使用的两种基本技术（如何）。该章节涵盖了与第二章相关的奇异值分解（SVD），然后深入研究了主成分分析（PCA），并附有代码和解释。

第十二章，*使用 Spark 2.0 ML 库实现文本分析*，介绍了 Spark 中用于实现大规模文本分析的各种技术。它从基础知识开始，如词频（TF）和相似性技术，如 Word2Vec，然后继续分析完整的维基百科转储，用于实际的 Spark ML 项目。本章最后深入讨论并提供了在 Spark 中实现潜在语义分析（LSA）和使用潜在狄利克雷分配（LDA）进行主题建模的代码。

第十三章，*Spark Streaming 和机器学习库*，首先介绍了 Spark 流处理的概念和未来发展方向，然后提供了基于 RDD（DStream）和结构化流的食谱，以建立基线。本章然后继续介绍了在撰写本书时 Spark 中所有可用的 ML 流算法。本章提供了代码，并展示了如何实现流 DataFrame 和流数据集，然后继续介绍了用于调试的 queueStream，然后进入了 Streaming KMeans（无监督学习）和使用真实世界数据集的流线性模型，如线性和逻辑回归。

# 本书所需的内容

请使用软件清单文档中的详细信息。

要执行本书中的食谱，您需要运行 Windows 7 及以上版本或 Mac 10 的系统，并安装以下软件：

+   Apache Spark 2.x

+   Oracle JDK SE 1.8.x

+   JetBrain IntelliJ Community Edition 2016.2.X 或更高版本

+   Scala 插件适用于 IntelliJ 2016.2.x

+   Jfreechart 1.0.19

+   breeze-core 0.12

+   Cloud9 1.5.0 JAR

+   Bliki-core 3.0.19

+   hadoop-streaming 2.2.0

+   Jcommon 1.0.23

+   Lucene-analyzers-common 6.0.0

+   Lucene-core-6.0.0

+   Spark-streaming-flume-assembly 2.0.0

+   Spark-streaming-kafka-assembly 2.0.0

此软件的硬件要求在本书的代码包中提供的软件清单中有所提及。

# 本书适合谁

本书适用于 Scala 开发人员，他们对机器学习技术有相当丰富的经验和理解，但缺乏 Spark 的实际实现。假定您具有扎实的机器学习算法知识，以及一些使用 Scala 实现 ML 算法的实际经验。但是，您不需要熟悉 Spark ML 库和生态系统。

# 部分

在本书中，您将经常看到几个标题（准备工作、如何做、工作原理、更多内容和参见）。为了清晰地说明如何完成食谱，我们使用以下部分：

# 准备工作

本节告诉您在食谱中可以期待什么，并描述了为食谱设置任何软件或任何先决设置所需的步骤。

# 如何做…

本节包含了遵循食谱所需的步骤。

# 工作原理…

本节通常包括对前一节中发生的事情的详细解释。

# 更多内容…

本节包括有关食谱的额外信息，以使读者更加了解食谱。

# 参见

本节提供了有关食谱的其他有用信息的链接。

# 约定

在本书中，您将找到许多文本样式，用于区分不同类型的信息。以下是一些样式的示例及其含义的解释。文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 句柄显示如下："Mac 用户请注意，我们在 Mac 机器上的`/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/`目录中安装了 Spark 2.0。"

代码块设置如下：

```scala
object HelloWorld extends App { 
   println("Hello World!") 
 } 
```

任何命令行输入或输出都按以下方式编写：

```scala
 mysql -u root -p
```

**新术语**和**重要单词**以粗体显示。您在屏幕上看到的单词，例如菜单或对话框中的单词，会以这种方式出现在文本中："配置全局库。选择 Scala SDK 作为您的全局库。"

警告或重要提示会显示为这样。

提示和技巧会显示为这样。
