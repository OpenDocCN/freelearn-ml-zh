# 前言

近年来，收集、存储和分析的数据量急剧增加，特别是与网络和移动设备上的活动以及通过传感器网络收集的物理世界的数据相关。尽管大规模数据存储、处理、分析和建模以前主要是谷歌、雅虎、Facebook、Twitter 和 Salesforce 等最大机构的领域，但越来越多的组织面临着如何处理大量数据的挑战。

面对如此大量的数据和实时利用的共同需求，人力系统很快变得不可行。这导致了所谓的大数据和机器学习系统的兴起，这些系统从数据中学习以做出自动决策。

为了应对处理越来越大规模的数据而不带来任何限制性成本的挑战，新的开源技术在谷歌、雅虎、亚马逊和 Facebook 等公司出现，旨在通过在计算机集群中分布数据存储和计算来更轻松地处理大规模数据量。

其中最广泛使用的是 Apache Hadoop，它显著地简化了存储大量数据（通过 Hadoop 分布式文件系统或 HDFS）和在这些数据上运行计算（通过 Hadoop MapReduce，在计算集群中并行执行计算任务的框架）的成本。

然而，MapReduce 存在一些重要的缺点，包括启动每个作业的高开销和依赖存储中间数据和计算结果到磁盘，这两点使得 Hadoop 相对不适合迭代或低延迟性质的用例。Apache Spark 是一个新的分布式计算框架，从根本上设计为优化低延迟任务，并将中间数据和结果存储在内存中，从而解决了 Hadoop 框架的一些主要缺点。Spark 提供了一个清晰、功能性和易于理解的 API 来编写应用程序，并且与 Hadoop 生态系统完全兼容。

此外，Spark 提供了 Scala、Java、Python 和 R 的本地 API。Scala 和 Python 的 API 允许直接在 Spark 应用程序中使用 Scala 或 Python 语言的所有优点，包括使用相关的解释器进行实时、交互式的探索。Spark 本身现在提供了一个工具包（1.6 中的 Spark MLlib 和 2.0 中的 Spark ML）用于分布式机器学习和数据挖掘模型，正在积极开发中，并已经包含了许多常见机器学习任务的高质量、可扩展和高效的算法，其中一些我们将在本书中深入探讨。

将机器学习技术应用于大规模数据集是具有挑战性的，主要是因为大多数众所周知的机器学习算法并不是为并行架构设计的。在许多情况下，设计这样的算法并不是一件容易的事。机器学习模型的性质通常是迭代的，因此 Spark 对于这种用例具有很强的吸引力。虽然有许多竞争性的并行计算框架，但 Spark 是少数几个将速度、可扩展性、内存处理和容错性与编程的简易性以及灵活、表达力和强大的 API 设计结合在一起的框架之一。

在本书中，我们将专注于机器学习技术的实际应用。虽然我们可能会简要涉及一些机器学习算法的理论方面和机器学习所需的数学知识，但本书通常会采用实际的、应用的方法，重点是使用示例和代码来说明如何有效地使用 Spark 和 MLlib 的特性，以及其他众所周知和免费提供的机器学习和数据分析包，来创建一个有用的机器学习系统。

# 本书涵盖的内容

第一章，“开始并运行 Spark”，展示了如何安装和设置 Spark 框架的本地开发环境，以及如何使用 Amazon EC2 在云中创建 Spark 集群。将介绍 Spark 编程模型和 API，并使用 Scala、Java 和 Python 创建一个简单的 Spark 应用程序。

第二章，“机器学习的数学”，提供了机器学习的数学介绍。理解数学和许多技术对于掌握算法的内部工作方式并获得最佳结果非常重要。

第三章，“设计机器学习系统”，介绍了机器学习系统的一个真实用例示例。我们将基于这个示例用例设计一个基于 Spark 的智能系统的高层架构。

第四章，“使用 Spark 获取、处理和准备数据”，详细介绍了如何获取用于机器学习系统的数据，特别是来自各种免费和公开可用的来源。我们将学习如何处理、清理和转换原始数据，以便在机器学习模型中使用，利用可用的工具、库和 Spark 的功能。

第五章，“使用 Spark 构建推荐引擎”，涉及基于协同过滤方法创建推荐模型。该模型将用于向特定用户推荐项目，以及创建与给定项目相似的项目列表。这里将介绍评估推荐模型性能的标准指标。

第六章，“使用 Spark 构建分类模型”，详细介绍了如何为二元分类创建模型，以及如何利用标准的分类任务性能评估指标。

第七章，“使用 Spark 构建回归模型”，展示了如何为回归创建模型，扩展了第六章“使用 Spark 构建分类模型”中创建的模型。这里将详细介绍回归模型性能的评估指标。

第八章，“使用 Spark 构建聚类模型”，探讨了如何创建聚类模型以及如何使用相关的评估方法。您将学习如何分析和可视化生成的聚类。

第九章，“使用 Spark 进行降维”，带领我们通过方法从数据中提取潜在结构，并减少数据的维度。您将学习一些常见的降维技术以及如何应用和分析它们。您还将了解如何将生成的数据表示用作另一个机器学习模型的输入。

第十章，“使用 Spark 进行高级文本处理”，介绍了处理大规模文本数据的方法，包括从文本中提取特征的技术，以及处理文本数据中典型的高维特征。

第十一章，“使用 Spark Streaming 进行实时机器学习”，概述了 Spark Streaming 以及它如何与在线和增量学习方法结合，应用于数据流的机器学习。

第十二章，“Spark ML 的管道 API”，提供了一套统一的 API，构建在数据框架之上，帮助用户创建和调整机器学习管道。

# 本书所需内容

在本书中，我们假设您具有 Scala 或 Python 编程的基本经验，并且对机器学习、统计和数据分析有一些基本知识。

# 本书的受众

本书旨在面向初级到中级数据科学家、数据分析师、软件工程师和从事机器学习或数据挖掘并对大规模机器学习方法感兴趣的从业者，但不一定熟悉 Spark。您可能具有一些统计或机器学习软件的经验（可能包括 MATLAB、scikit-learn、Mahout、R、Weka 等）或分布式系统（包括对 Hadoop 的一些了解）。

# 约定

在本书中，您将找到一些文本样式，用于区分不同类型的信息。以下是一些这些样式的示例及其含义的解释。

文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 句柄显示如下：“Spark 将用户脚本放置到`bin`目录中运行 Spark。”

代码块设置如下：

```scala
val conf = new SparkConf()
.setAppName("Test Spark App")
.setMaster("local[4]")
val sc = new SparkContext(conf)

```

任何命令行输入或输出都以以下方式编写：

```scala
>tar xfvz spark-2.1.0-bin-hadoop2.7.tgz
>cd spark-2.1.0-bin-hadoop2.7

```

**新术语**和**重要单词**以粗体显示。您在屏幕上看到的单词，例如菜单或对话框中的单词，会以这种方式出现在文本中：“这些可以从 AWS 主页上通过单击“帐户|安全凭据|访问凭据”来获取。”

警告或重要说明会出现在这样的框中。

提示和技巧会以这种方式出现。
