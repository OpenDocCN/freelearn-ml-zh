# 第一章. Python 机器学习入门

机器学习教会机器自主完成任务。就这么简单。复杂性体现在细节上，而这很可能是你正在阅读本书的原因。

或许你有太多的数据，却缺乏足够的洞察力。你希望通过使用机器学习算法来解决这个挑战，于是你开始深入研究这些算法。但经过一段时间后，你感到困惑：你到底应该选择哪一种成千上万的算法？

或者，也许你对机器学习总体感兴趣，一段时间以来一直在阅读相关的博客和文章。一切看起来都像是魔法和酷炫的东西，于是你开始了探索，并将一些玩具数据输入到决策树或支持向量机中。然而，在成功应用它到一些其他数据后，你开始疑惑：整个设置是正确的吗？你得到了最优的结果吗？你怎么知道是否没有更好的算法？或者你的数据是正确的吗？

欢迎加入这个俱乐部！我们（作者）也曾处于那样的阶段，寻找一些能讲述机器学习理论教材背后故事的信息。事实证明，很多这些信息是“黑魔法”，通常不会在标准教科书中教授。所以从某种意义上来说，我们写这本书是为了我们年轻时的自己。这本书不仅提供了机器学习的快速入门介绍，还教会了我们一路上学到的经验教训。我们希望它能为你顺利进入计算机科学中最激动人心的领域之一提供帮助。

# 机器学习和 Python —— 一个梦幻组合

机器学习的目标是通过提供一些示例（如何做或不做任务），来教会机器（软件）执行任务。假设每天早晨，当你打开电脑时，你都会做同样的任务，将电子邮件进行整理，以便只有属于同一主题的电子邮件会出现在同一个文件夹中。经过一段时间后，你可能会感到厌烦，并想要自动化这个繁琐的工作。一个方法是开始分析你的大脑，并写下你在整理电子邮件时处理的所有规则。然而，这会相当繁琐，并且永远不完美。在这个过程中，你会漏掉一些规则，或者过度指定其他规则。一个更好且更具未来适应性的方式是通过选择一组电子邮件元信息和正文/文件夹名称对来自动化这个过程，然后让一个算法得出最佳规则集。这些对就是你的训练数据，最终得到的规则集（也称为模型）可以应用于我们未曾见过的未来电子邮件。这就是最简单形式的机器学习。

当然，机器学习（通常也称为数据挖掘或预测分析）本身并不是一个全新的领域。恰恰相反，它近年来的成功可以归因于将其他成功领域（如统计学）中的扎实技术和见解应用到实际中的务实方法。在统计学中，目的是帮助我们人类从数据中获取洞见，例如，通过了解潜在的模式和关系。随着你阅读越来越多关于机器学习成功应用的内容（你已经查看了[www.kaggle.com](http://www.kaggle.com)，对吧？），你会发现应用统计学在机器学习专家中是一个常见的领域。

正如你稍后会看到的，提出一个合适的机器学习方法的过程从来不是一个瀑布式的过程。相反，你会看到自己在分析中来回反复，不断尝试不同版本的输入数据和多种机器学习算法。正是这种探索性的特点使得 Python 成为完美的选择。作为一种解释型高级编程语言，Python 看起来就是为这一过程而设计的，用于不断尝试不同的方式。更重要的是，它的执行速度也相当快。确实，它比 C 或其他类似的静态类型编程语言慢，但由于有大量易于使用的库（许多都是用 C 编写的），你无需为灵活性牺牲速度。

# 本书将教你什么（以及不教你什么）

本书将为你提供一个广泛的概述，介绍当前在机器学习各个领域中最常用的学习算法类型，以及在应用它们时需要注意的地方。然而，从我们的经验来看，我们知道，做一些“酷”的事情，也就是使用和调整像支持向量机、最近邻搜索或其集成算法等机器学习算法，只会消耗一个优秀机器学习专家时间的一小部分。通过观察下面的典型工作流程，我们看到大部分时间会花费在一些相对平凡的任务上：

+   读取数据并进行清洗

+   探索和理解输入数据

+   分析如何最好地将数据呈现给学习算法

+   选择正确的模型和学习算法

+   正确衡量性能

当谈到探索和理解输入数据时，我们将需要一些统计学和基础数学知识。然而，你会发现，在应用这些知识时，那些在数学课上看似枯燥的内容，实际上在用来观察有趣数据时会变得非常令人兴奋。

旅程从读取数据开始。当你需要回答如何处理无效或缺失值等问题时，你会发现这更像是一门艺术，而非精确的科学。这是一个非常有意义的过程，因为做对这一部分将使你的数据可以被更多的机器学习算法使用，从而提高成功的可能性。

当数据已经准备好并存在于程序的数据结构中时，你会希望对正在处理的数据有一个更直观的了解。你有足够的数据来回答你的问题吗？如果没有，你可能需要考虑其他方法来获得更多数据。你是否拥有过多的数据？那你可能需要考虑如何从中提取一个合适的样本。

通常，你不会直接将数据输入到机器学习算法中。相反，你会发现你可以在训练之前对数据的某些部分进行优化。很多时候，机器学习算法会通过提升性能来回报你。你甚至会发现，经过优化的数据所使用的简单算法通常比使用原始数据的复杂算法表现更好。机器学习工作流中的这一部分被称为**特征工程**，它通常是一个非常令人兴奋且有回报的挑战。你会立刻看到创意和智慧所带来的成果。

选择正确的学习算法并不仅仅是从你工具箱中选择三四个算法进行比较（你将会看到更多的选择）。这更像是一个深思熟虑的过程，需要权衡不同的性能和功能需求。你是否需要快速的结果并愿意牺牲质量？还是你宁愿花更多时间以获得尽可能最好的结果？你是否对未来的数据有明确的想法，还是应该在这方面保持更保守一些？

最后，衡量性能是大多数机器学习新手容易犯错的地方。有一些错误是简单的，比如用训练数据来测试你的方法。但也有更复杂的错误，尤其是当你有不平衡的训练数据时。同样，数据是决定你尝试是否成功的关键部分。

我们看到只有第四点涉及到复杂的算法。尽管如此，我们希望这本书能够说服你，其他四个任务不仅仅是日常琐事，它们同样可以令人兴奋。我们的希望是，到书的最后，你会真正爱上数据，而不仅仅是学习算法。

为此，我们不会给你带来过多的理论性内容，关于各种机器学习算法的优秀书籍已经涵盖了这些内容（你可以在附录中找到相关书目）。相反，我们将尽力在每个章节中提供对基本方法的直观理解——只需让你了解基本概念，并能够迈出第一步。因此，这本书绝非*机器学习的终极指南*。它更像是一个入门工具包。我们希望它能激发你的好奇心，让你迫不及待地去学习更多关于这个有趣领域的知识。

在本章的其余部分，我们将设置并了解基本的 Python 库 NumPy 和 SciPy，然后使用 scikit-learn 训练我们的第一个机器学习模型。在这个过程中，我们将介绍一些基本的机器学习概念，这些概念将在整本书中使用。接下来的章节将通过前面提到的五个步骤，详细介绍使用 Python 进行机器学习的不同方面，并结合不同的应用场景。

# 当你遇到困境时该怎么办

我们尽力传达书中每个步骤所需的思想。然而，仍然会有一些情况让你卡住。原因可能从简单的拼写错误、奇怪的包版本组合到理解问题不一而足。

在这种情况下，有很多不同的方式可以获得帮助。很可能，你的问题已经在以下优秀的问答网站中提出并得到解决：

[`metaoptimize.com/qa`](http://metaoptimize.com/qa)：这个问答网站专注于机器学习话题。几乎每个问题都包含来自机器学习专家的超出平均水平的回答。即使你没有任何问题，偶尔去浏览一下，阅读一些回答也是一个好习惯。

[`stats.stackexchange.com`](http://stats.stackexchange.com)：这个问答网站名为 Cross Validated，类似于 MetaOptimize，但更侧重于统计学问题。

[`stackoverflow.com`](http://stackoverflow.com)：这个问答网站和前面提到的类似，但它的焦点更广，涵盖了通用的编程话题。例如，它包含了我们在本书中将使用的一些包的问题，比如 SciPy 或 matplotlib。

在 [`freenode.net/`](https://freenode.net/) 上的 `#machinelearning`：这是一个专注于机器学习话题的 IRC 频道。它是一个规模较小但非常活跃且乐于助人的机器学习专家社区。

[`www.TwoToReal.com`](http://www.TwoToReal.com)：这是作者们创建的即时问答网站，旨在帮助你解决不适合前面所列类别的主题。如果你发布问题，作者中的一位如果在线，将会立即收到消息，并与你进行在线聊天。

如同一开始所述，本书试图帮助你快速入门机器学习。因此，我们强烈鼓励你建立自己的机器学习相关博客列表，并定期查看。这是了解什么有效、什么无效的最佳方式。

我们在这里想特别提到的唯一一个博客（更多内容见附录）是 [`blog.kaggle.com`](http://blog.kaggle.com)，这是 Kaggle 公司的博客，Kaggle 正在进行机器学习竞赛。通常，他们会鼓励竞赛的获胜者写下他们是如何接近竞赛的，哪些策略行不通，以及他们是如何得出获胜策略的。即使你不阅读其他任何内容，这也是必读的。

# 开始

假设你已经安装了 Python（至少是 2.7 及更新版本应该没问题），接下来我们需要安装 NumPy 和 SciPy 进行数值运算，以及安装 matplotlib 用于可视化。

## NumPy、SciPy 和 matplotlib 简介

在我们讨论具体的机器学习算法之前，必须先讨论如何最好地存储我们需要处理的数据。这很重要，因为即使是最先进的学习算法，如果永远无法完成，也对我们没有任何帮助。这可能是因为数据访问速度过慢，或者它的表示形式迫使操作系统整天进行交换。再加上 Python 是解释型语言（尽管它是高度优化的），在许多数值计算密集型算法中，相比于 C 或 FORTRAN，它运行较慢。那么，我们不禁要问，为什么那么多科学家和公司在高度计算密集的领域依然押注 Python 呢？

答案是，在 Python 中，将数字运算任务转交给低层的 C 或 FORTRAN 扩展是非常容易的。而这正是 NumPy 和 SciPy 的作用所在（[`scipy.org/Download`](http://scipy.org/Download)）。在这个配合下，NumPy 提供了高度优化的多维数组支持，这些数组是大多数先进算法的基本数据结构。SciPy 利用这些数组提供了一组快速的数值算法。最后，matplotlib（[`matplotlib.org/`](http://matplotlib.org/)）可能是使用 Python 绘制高质量图形最便捷且功能最丰富的库。

## 安装 Python

幸运的是，对于所有主要的操作系统——即 Windows、Mac 和 Linux——都有针对 NumPy、SciPy 和 matplotlib 的专用安装包。如果你不确定安装过程，可以考虑安装 Anaconda Python 发行版（可以通过[`store.continuum.io/cshop/anaconda/`](https://store.continuum.io/cshop/anaconda/)访问），该发行版由 SciPy 的创始贡献者 Travis Oliphant 主导。Anaconda 与其他发行版（如 Enthought Canopy，下载地址：[`www.enthought.com/downloads/`](https://www.enthought.com/downloads/)）或 Python(x,y)（访问地址：[`code.google.com/p/pythonxy/wiki/Downloads`](http://code.google.com/p/pythonxy/wiki/Downloads)）的不同之处在于，Anaconda 已经完全兼容 Python 3——这是我们将在本书中使用的 Python 版本。

## 使用 NumPy 高效处理数据，并使用 SciPy 智能处理

让我们快速浏览一些基本的 NumPy 示例，然后看看 SciPy 在其基础上提供了什么。在这个过程中，我们将借助精彩的 Matplotlib 包进行绘图，迈出第一步。

如果需要深入了解，你可能想看看 NumPy 提供的一些更有趣的示例，访问[`www.scipy.org/Tentative_NumPy_Tutorial`](http://www.scipy.org/Tentative_NumPy_Tutorial)。

你还会发现 *NumPy 初学者指南 - 第二版*，*Ivan Idris*，由 Packt Publishing 出版，非常有价值。更多的教程风格指南可以在 [`scipy-lectures.github.com`](http://scipy-lectures.github.com) 找到，官方的 SciPy 教程请访问 [`docs.scipy.org/doc/scipy/reference/tutorial`](http://docs.scipy.org/doc/scipy/reference/tutorial)。

### 注意

本书中，我们将使用版本 1.8.1 的 NumPy 和版本 0.14.0 的 SciPy。

## 学习 NumPy

所以让我们导入 NumPy 并稍微玩一下它。为此，我们需要启动 Python 交互式 shell：

```py
>>> import numpy
>>> numpy.version.full_version
1.8.1

```

由于我们不想污染我们的命名空间，当然不应该使用以下代码：

```py
>>> from numpy import *

```

因为例如，`numpy.array` 可能会与标准 Python 中包含的数组包发生冲突。因此，我们将使用以下方便的快捷方式：

```py
>>> import numpy as np
>>> a = np.array([0,1,2,3,4,5])
>>> a
array([0, 1, 2, 3, 4, 5])
>>> a.ndim
1
>>> a.shape
(6,)

```

所以，我们刚刚创建了一个数组，就像我们在 Python 中创建一个列表一样。然而，NumPy 数组有额外的形状信息。在这个例子中，它是一个包含六个元素的一维数组，到目前为止没什么意外。

我们现在可以将这个数组转换为一个二维矩阵：

```py
>>> b = a.reshape((3,2))
>>> b
array([[0, 1],
 [2, 3],
 [4, 5]])
>>> b.ndim
2
>>> b.shape
(3, 2)

```

有趣的事情发生在我们意识到 NumPy 包的优化程度时。例如，执行这一步可以尽可能避免复制：

```py
>>> b[1][0] = 77
>>> b
array([[ 0,  1],
 [77,  3],
 [ 4,  5]])
>>> a
array([ 0,  1, 77,  3,  4,  5])

```

在这个例子中，我们将 `b` 中的值 `2` 修改为 `77`，并立即看到 `a` 中也反映了相同的变化。请记住，任何时候你需要一个真正的副本时，可以随时执行：

```py
>>> c = a.reshape((3,2)).copy()
>>> c
array([[ 0,  1],
 [77,  3],
 [ 4,  5]])
>>> c[0][0] = -99
>>> a
array([ 0,  1, 77,  3,  4,  5])
>>> c
array([[-99,   1],
 [ 77,   3],
 [  4,   5]])

```

请注意，`c` 和 `a` 是完全独立的副本。

NumPy 数组的另一个大优点是操作会传播到各个元素。例如，乘以一个 NumPy 数组将生成一个与原数组大小相同的新数组，所有元素都被相乘：

```py
>>> d = np.array([1,2,3,4,5])
>>> d*2
array([ 2,  4,  6,  8, 10])

```

类似地，对于其他操作：

```py
>>> d**2
array([ 1,  4,  9, 16, 25])

```

将其与普通的 Python 列表进行对比：

```py
>>> [1,2,3,4,5]*2
[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]
>>> [1,2,3,4,5]**2
Traceback (most recent call last):
 File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'

```

当然，使用 NumPy 数组时，我们牺牲了 Python 列表所提供的灵活性。像添加或移除这样的简单操作对于 NumPy 数组来说稍显复杂。幸运的是，我们手头有两者，并且可以根据实际任务使用合适的工具。

### 索引

NumPy 的一大优势来自于其数组的多种访问方式。

除了常规的列表索引外，它还允许你使用数组本身作为索引，方法是执行：

```py
>>> a[np.array([2,3,4])]
array([77,  3,  4])

```

结合条件也会传播到各个元素这一事实，我们获得了一种非常方便的方式来访问数据：

```py
>>> a>4
array([False, False,  True, False, False,  True], dtype=bool)
>>> a[a>4]
array([77,  5])

```

通过执行以下命令，可以用来修剪异常值：

```py
>>> a[a>4] = 4
>>> a
array([0, 1, 4, 3, 4, 4])

```

由于这是一个常见的用例，因此有一个专门的裁剪函数来处理它，可以通过一次函数调用将值限制在区间的两端：

```py
>>> a.clip(0,4)
array([0, 1, 4, 3, 4, 4])

```

### 处理不存在的值

NumPy 强大的索引功能在处理我们刚从文本文件中读取的数据时非常有用。通常，这些数据会包含无效值，我们可以使用 `numpy.NAN` 来标记它们为非真实数字：

```py
>>> c = np.array([1, 2, np.NAN, 3, 4]) # let's pretend we have read this from a text file
>>> c
array([  1.,   2.,  nan,   3.,   4.])
>>> np.isnan(c)
array([False, False,  True, False, False], dtype=bool)
>>> c[~np.isnan(c)]
array([ 1.,  2.,  3.,  4.])
>>> np.mean(c[~np.isnan(c)])
2.5

```

### 比较运行时间

让我们比较一下 NumPy 和普通 Python 列表的运行时行为。在以下代码中，我们将计算从 1 到 1000 的所有平方数之和，并查看需要多少时间。我们执行 10,000 次，并报告总时间，以确保我们的测量足够准确。

```py
import timeit
normal_py_sec = timeit.timeit('sum(x*x for x in range(1000))',
 number=10000)
naive_np_sec = timeit.timeit(
 'sum(na*na)',
 setup="import numpy as np; na=np.arange(1000)",
 number=10000)
good_np_sec = timeit.timeit(
 'na.dot(na)',
 setup="import numpy as np; na=np.arange(1000)",
 number=10000)

print("Normal Python: %f sec" % normal_py_sec)
print("Naive NumPy: %f sec" % naive_np_sec)
print("Good NumPy: %f sec" % good_np_sec)

Normal Python: 1.050749 sec
Naive NumPy: 3.962259 sec
Good NumPy: 0.040481 sec

```

我们做出了两个有趣的观察。首先，仅仅将 NumPy 作为数据存储（朴素的 NumPy）就花费了 3.5 倍的时间，这令人惊讶，因为我们原以为它应该更快，因为它是作为 C 扩展写的。一个原因是从 Python 本身访问单个元素是非常昂贵的。只有当我们能够在优化过的扩展代码中应用算法时，才会获得速度的提升。另一个观察是相当令人震惊的：使用 NumPy 的`dot()`函数，尽管它做的是完全相同的事情，却让我们的速度提高了 25 倍以上。总之，在我们即将实现的每一个算法中，我们都应该始终检查如何将 Python 中的单个元素循环转移到一些高度优化的 NumPy 或 SciPy 扩展函数中。

然而，这种速度是有代价的。使用 NumPy 数组时，我们不再拥有 Python 列表那种几乎可以存储任何东西的极大灵活性。NumPy 数组始终只有一种数据类型。

```py
>>> a = np.array([1,2,3])
>>> a.dtype
dtype('int64')

```

如果我们尝试使用不同类型的元素，如以下代码所示，NumPy 会尽力将它们转换为最合理的公共数据类型：

```py
>>> np.array([1, "stringy"])
array(['1', 'stringy'], dtype='<U7')
>>> np.array([1, "stringy", set([1,2,3])])
array([1, stringy, {1, 2, 3}], dtype=object)

```

## 学习 SciPy

在 NumPy 高效数据结构的基础上，SciPy 提供了大量针对这些数组工作的算法。无论你从当前的数值计算书籍中挑选出哪种数值密集型算法，你很可能会以某种方式在 SciPy 中找到它的支持。无论是矩阵操作、线性代数、优化、聚类、空间操作，甚至是快速傅里叶变换，工具箱已经很充实。因此，在开始实现一个数值算法之前，养成检查`scipy`模块的好习惯。

为了方便，NumPy 的完整命名空间也可以通过 SciPy 访问。所以，从现在开始，我们将通过 SciPy 命名空间使用 NumPy 的工具。你可以通过比较任何基本函数的函数引用轻松检查这一点，例如：

```py
>>> import scipy, numpy
>>> scipy.version.full_version
0.14.0
>>> scipy.dot is numpy.dot
True

```

这些多样化的算法被分组到以下工具箱中：

| SciPy 包 | 功能 |
| --- | --- |
| `cluster` |

+   层次聚类（`cluster.hierarchy`）

+   向量量化 / k-means (`cluster.vq`)

|

| `constants` |
| --- |

+   物理和数学常数

+   转换方法

|

| `fftpack` | 离散傅里叶变换算法 |
| --- | --- |
| `integrate` | 积分例程 |
| `interpolate` | 插值（线性插值、三次插值等） |
| `io` | 数据输入和输出 |
| `linalg` | 使用优化过的 BLAS 和 LAPACK 库的线性代数例程 |
| `ndimage` | *n*维图像包 |
| `odr` | 正交距离回归 |
| `optimize` | 优化（寻找最小值和根） |
| `signal` | 信号处理 |
| `sparse` | 稀疏矩阵 |
| `spatial` | 空间数据结构和算法 |
| `special` | 特殊数学函数，如贝塞尔函数或雅可比函数 |
| `stats` | 统计工具包 |

对我们工作最感兴趣的工具包是`scipy.stats`、`scipy.interpolate`、`scipy.cluster`和`scipy.signal`。为了简洁起见，我们将简要探索一下 stats 包的一些功能，其余的将在各个章节中介绍。

# 我们的第一个（微小的）机器学习应用

让我们动手操作，看看我们的假设性网络初创公司 MLaaS，该公司通过 HTTP 提供机器学习算法服务。随着公司成功的不断增加，需求也在增长，需要更好的基础设施来成功地处理所有的网络请求。我们不希望分配过多资源，因为那样成本过高。另一方面，如果我们没有预留足够的资源来处理所有的请求，我们将会亏损。那么，问题来了，我们什么时候会达到当前基础设施的限制，我们预计这个限制是每小时 100,000 个请求。我们希望提前知道何时需要在云端申请更多的服务器，以便在不为未使用的资源付费的情况下，成功地处理所有传入请求。

## 读取数据

我们已经收集了过去一个月的网络统计数据，并将其汇总在`ch01/data/web_traffic.tsv`中（`.tsv`因为它包含制表符分隔的值）。它们按每小时的点击次数存储。每行包含连续的小时和该小时的网页点击次数。

前几行如下所示：

![读取数据](img/2772OS_01_09.jpg)

使用 SciPy 的`genfromtxt()`，我们可以轻松地读取数据，代码如下：

```py
>>> import scipy as sp
>>> data = sp.genfromtxt("web_traffic.tsv", delimiter="\t")

```

我们必须指定制表符作为分隔符，以便正确地确定各列。

快速检查显示我们已经正确读取了数据：

```py
>>> print(data[:10])
[[  1.00000000e+00   2.27200000e+03]
 [  2.00000000e+00              nan]
 [  3.00000000e+00   1.38600000e+03]
 [  4.00000000e+00   1.36500000e+03]
 [  5.00000000e+00   1.48800000e+03]
 [  6.00000000e+00   1.33700000e+03]
 [  7.00000000e+00   1.88300000e+03]
 [  8.00000000e+00   2.28300000e+03]
 [  9.00000000e+00   1.33500000e+03]
 [  1.00000000e+01   1.02500000e+03]]
>>> print(data.shape)
(743, 2)

```

如你所见，我们有 743 个数据点，包含两个维度。

## 数据预处理和清理

对 SciPy 来说，将维度分成两个大小为 743 的向量更加方便。第一个向量`x`包含小时，另一个向量`y`包含该小时的网页点击数。这个拆分是通过 SciPy 的特殊索引表示法完成的，利用该方法我们可以单独选择列：

```py
x = data[:,0]
y = data[:,1]

```

从 SciPy 数组中选择数据有很多方法。有关索引、切片和迭代的更多细节，请查看[`www.scipy.org/Tentative_NumPy_Tutorial`](http://www.scipy.org/Tentative_NumPy_Tutorial)。

一个警告是，我们的`y`中仍然包含一些无效值，`nan`。问题是我们该如何处理这些值。让我们通过运行以下代码来检查有多少小时包含无效数据：

```py
>>> sp.sum(sp.isnan(y))
8

```

如你所见，我们只有 743 个数据项中的 8 个缺失，因此我们可以去掉它们。记住，我们可以用另一个数组来索引一个 SciPy 数组。`Sp.isnan(y)`返回一个布尔数组，指示某个数据项是否为数字。通过使用`~`，我们可以逻辑取反这个数组，从而只选择那些`y`中包含有效数字的`x`和`y`元素：

```py
>>> x = x[~sp.isnan(y)]
>>> y = y[~sp.isnan(y)]

```

为了获得数据的初步印象，我们使用 matplotlib 绘制数据的散点图。matplotlib 包含了 pyplot 包，它试图模仿 MATLAB 的界面，正如你在以下代码中看到的，这是一种非常方便且易于使用的接口：

```py
>>> import matplotlib.pyplot as plt
>>> # plot the (x,y) points with dots of size 10
>>> plt.scatter(x, y, s=10)
>>> plt.title("Web traffic over the last month")
>>> plt.xlabel("Time")
>>> plt.ylabel("Hits/hour")
>>> plt.xticks([w*7*24 for w in range(10)],
 ['week %i' % w for w in range(10)])
>>> plt.autoscale(tight=True)
>>> # draw a slightly opaque, dashed grid
>>> plt.grid(True, linestyle='-', color='0.75')
>>> plt.show()

```

### 注意

你可以在[`matplotlib.org/users/pyplot_tutorial.html`](http://matplotlib.org/users/pyplot_tutorial.html)找到更多关于绘图的教程。

在结果图表中，我们可以看到，在前几周流量大致保持不变，而最后一周显示了急剧增加：

![预处理和清洗数据](img/2772OS_01_01.jpg)

## 选择合适的模型和学习算法

现在我们对数据有了初步印象，我们回到最初的问题：我们的服务器能够处理多少的 Web 流量？为了回答这个问题，我们需要做以下几点：

1.  找出噪声数据点背后的真实模型。

1.  接下来，使用该模型推测未来，找出我们需要扩展基础设施的时间点。

### 在构建我们的第一个模型之前……

当我们谈论模型时，你可以将它们视为复杂现实的简化理论近似。作为这样的一种模型，总是涉及到某种程度的不足，也叫做近似误差。这个误差将引导我们在众多选择中选出合适的模型。这个误差将通过计算模型预测与真实数据之间的平方距离来计算；例如，对于一个学习过的模型函数`f`，误差的计算如下：

```py
def error(f, x, y):
 return sp.sum((f(x)-y)**2)

```

向量`x`和`y`包含了我们之前提取的 Web 统计数据。这正是我们在这里利用 SciPy 的矢量化函数`f(x)`的美妙之处。假设训练过的模型接受一个向量并返回相同大小的结果向量，这样我们就可以用它来计算与`y`的差异。

### 从一条简单的直线开始

假设我们假设潜在的模型是一条直线。那么，挑战在于如何将这条直线最佳地放入图表中，以使得近似误差最小。SciPy 的`polyfit()`函数正是用来做这个的。给定数据`x`和`y`以及所需的多项式阶数（直线是阶数为 1），它会找到一个模型函数，最小化先前定义的误差函数：

```py
fp1, residuals, rank, sv, rcond = sp.polyfit(x, y, 1, full=True)

```

`polyfit()`函数返回拟合模型函数的参数`fp1`。通过设置`full=True`，我们还可以获得拟合过程的额外背景信息。其中，只有残差是我们关心的，它正是近似的误差：

```py
>>> print("Model parameters: %s" % fp1)
Model parameters: [   2.59619213  989.02487106]
>>> print(residuals)
[  3.17389767e+08]

```

这意味着最好的直线拟合是以下函数：

```py
f(x) = 2.59619213 * x + 989.02487106.

```

然后我们使用`poly1d()`从模型参数创建一个模型函数：

```py
>>> f1 = sp.poly1d(fp1)
>>> print(error(f1, x, y))
317389767.34

```

我们使用了`full=True`来获取更多的拟合过程细节。通常我们不需要这样做，在这种情况下，只会返回模型参数。

现在我们可以使用`f1()`来绘制我们训练的第一个模型。除了前面绘图的指令外，我们只需添加以下代码：

```py
fx = sp.linspace(0,x[-1], 1000) # generate X-values for plotting
plt.plot(fx, f1(fx), linewidth=4)
plt.legend(["d=%i" % f1.order], loc="upper left")

```

这将产生以下图表：

![从简单的直线开始](img/2772OS_01_02.jpg)

看起来前 4 周的预测误差并不大，尽管我们明显看到最初假设潜在模型是直线的假设存在问题。那么，317,389,767.34 的误差到底有多大呢？

误差的绝对值通常单独使用意义不大。然而，在比较两个竞争模型时，我们可以使用它们的误差来判断哪个模型更好。尽管我们的第一个模型显然不是我们会使用的，但它在工作流程中具有非常重要的作用。在我们找到一个更好的模型之前，它将作为我们的基准。未来我们提出的任何新模型，都将与当前的基准模型进行比较。

### 向更高级的内容迈进

现在让我们拟合一个更复杂的模型，一个 2 次方的多项式，看看它是否能更好地理解我们的数据：

```py
>>> f2p = sp.polyfit(x, y, 2)
>>> print(f2p)
array([  1.05322215e-02,  -5.26545650e+00,   1.97476082e+03])
>>> f2 = sp.poly1d(f2p)
>>> print(error(f2, x, y))
179983507.878

```

你将得到以下图表：

![向更高级的内容迈进](img/2772OS_01_03.jpg)

误差为 179,983,507.878，几乎是直线模型误差的一半。这是好的，但不幸的是，这也有一个代价：我们现在拥有了一个更复杂的函数，这意味着我们在`polyfit()`中需要调整更多的参数。拟合的多项式如下：

```py
f(x) = 0.0105322215 * x**2  - 5.26545650 * x + 1974.76082

```

所以，如果增加复杂度能带来更好的结果，为什么不进一步增加复杂度呢？让我们尝试 3 次方、10 次方和 100 次方的情况。

![向更高级的内容迈进](img/2772OS_01_04.jpg)

有趣的是，我们没有在拟合了 100 次方的多项式中看到`d=53`。相反，我们在控制台上看到了大量的警告：

```py
RankWarning: Polyfit may be poorly conditioned

```

这意味着由于数值误差，polyfit 无法以 100 次方确定一个好的拟合。相反，它认为 53 次方已经足够好了。

看起来曲线捕捉并改进拟合数据的能力随着其复杂度的增加而增强。错误也似乎讲述了同样的故事：

```py
Error d=1: 317,389,767.339778
Error d=2: 179,983,507.878179
Error d=3: 139,350,144.031725
Error d=10: 121,942,326.363461
Error d=53: 109,318,004.475556

```

然而，仔细观察拟合曲线后，我们开始怀疑它们是否也捕捉到了生成这些数据的真实过程。换句话说，我们的模型是否正确地表示了客户访问我们网站时的潜在行为？查看 10 次方和 53 次方的多项式，我们看到的行为是剧烈波动的。似乎模型过度拟合了数据，甚至不仅捕捉到了潜在的过程，还包括了噪声。这种现象称为**过拟合**。

在这一点上，我们有以下选择：

+   选择拟合的多项式模型之一。

+   切换到另一种更复杂的模型类别。样条曲线？

+   以不同的角度重新思考数据并重新开始。

在五个拟合模型中，一阶模型显然过于简单，而 10 阶和 53 阶的模型显然是过拟合的。只有二阶和三阶模型似乎在某种程度上与数据匹配。然而，如果我们在两个边界进行外推，就会看到它们变得异常。

切换到更复杂的类别似乎也不是正确的选择。有哪些理由支持选择哪种类别？此时，我们意识到我们可能还没有完全理解我们的数据。

### 回退再前进——重新审视我们的数据

因此，我们回退并再次审视数据。看起来在第 3 周和第 4 周之间存在一个拐点。那么让我们分离数据并使用第 3.5 周作为分割点来训练两条线：

```py
inflection = 3.5*7*24 # calculate the inflection point in hours
xa = x[:inflection] # data before the inflection point
ya = y[:inflection]
xb = x[inflection:] # data after
yb = y[inflection:]

fa = sp.poly1d(sp.polyfit(xa, ya, 1))
fb = sp.poly1d(sp.polyfit(xb, yb, 1))

fa_error = error(fa, xa, ya)
fb_error = error(fb, xb, yb)
print("Error inflection=%f" % (fa_error + fb_error))
Error inflection=132950348.197616

```

从第一条线开始，我们使用第 3 周的数据进行训练，而在第二条线中我们使用剩余的数据进行训练。

![回退再前进——重新审视我们的数据](img/2772OS_01_05.jpg)

显然，这两条线的组合比我们之前所拟合的任何模型更能符合数据。但即便如此，组合误差仍然高于高阶多项式。我们能信任最终的误差吗？

换个角度问，为什么我们更相信只在数据最后一周拟合的直线，而不是任何更复杂的模型？这是因为我们假设它能更好地捕捉未来的数据。如果我们将模型预测到未来，我们可以看到我们是否正确（**d=1**再次是我们最初的直线）。

![回退再前进——重新审视我们的数据](img/2772OS_01_06.jpg)

10 阶和 53 阶的模型似乎并不看好我们创业公司的未来。它们为了正确拟合给定的数据付出了极大的努力，结果显然无法用于外推。这就是所谓的过拟合。另一方面，低阶模型似乎无法充分捕捉数据的特征。这就是所谓的**欠拟合**。

所以让我们对二阶及以上的模型保持公正，尝试仅将它们拟合到最后一周的数据。毕竟，我们认为最后一周比之前的数据更能反映未来。结果可以在下面这张充满迷幻色彩的图表中看到，它进一步展示了过拟合问题有多严重。

![回退再前进——重新审视我们的数据](img/2772OS_01_07.jpg)

然而，从仅使用第 3.5 周及之后的数据进行训练时模型的误差来看，我们仍然应该选择最复杂的模型（注意，我们也只计算了拐点之后的误差）：

```py
Error d=1:   22,143,941.107618
Error d=2:   19,768,846.989176
Error d=3:   19,766,452.361027
Error d=10:  18,949,339.348539
Error d=53:  18,300,702.038119

```

### 训练与测试

如果我们有一些来自未来的数据可以用来评估我们的模型，那么我们应该仅根据由此产生的逼近误差来判断我们的模型选择。

虽然我们无法预测未来，但我们可以并且应该通过保留部分数据来模拟类似的效果。比如，去除一定比例的数据，并在剩余数据上进行训练。然后，我们使用保留的数据计算误差。由于模型在训练时未看到这些保留的数据，因此我们应该能够更真实地了解模型在未来的表现。

仅在拐点后时间段内训练的模型的测试误差现在显示出完全不同的图景：

```py
Error d=1: 6397694.386394
Error d=2: 6010775.401243
Error d=3: 6047678.658525
Error d=10: 7037551.009519
Error d=53: 7052400.001761

```

请看以下图表：

![训练与测试](img/2772OS_01_08.jpg)

看起来我们终于有了明确的结果：二次函数模型具有最低的测试误差，这是指使用模型在训练过程中未见过的数据进行测量时的误差。这给了我们希望，未来的数据到来时我们不会遇到不好的惊讶。

### 回答我们最初的问题

最终，我们得出了一个我们认为最能代表底层过程的模型；现在只需要简单地计算出我们的基础设施何时将达到每小时 100,000 次请求。我们需要计算何时我们的模型函数值会达到 100,000。

既然我们有一个二次多项式，我们可以简单地计算函数的反函数，并在 100,000 时计算它的值。当然，我们希望有一种适用于任何模型函数的方法。

这可以通过从多项式中减去 100,000 来实现，结果得到另一个多项式，并找到它的根。SciPy 的`optimize`模块有一个`fsolve`函数，可以通过提供初始起始位置参数`x0`来实现这一目标。由于我们输入数据文件中的每个条目对应一个小时，总共有 743 个小时，因此我们将起始位置设置为该时间段之后的某个值。让`fbt2`成为二次多项式模型。

```py
>>> fbt2 = sp.poly1d(sp.polyfit(xb[train], yb[train], 2))
>>> print("fbt2(x)= \n%s" % fbt2)
fbt2(x)=
 2
0.086 x - 94.02 x + 2.744e+04
>>> print("fbt2(x)-100,000= \n%s" % (fbt2-100000))
fbt2(x)-100,000=
 2
0.086 x - 94.02 x - 7.256e+04
>>> from scipy.optimize import fsolve
>>> reached_max = fsolve(fbt2-100000, x0=800)/(7*24)
>>> print("100,000 hits/hour expected at week %f" % reached_max[0])

```

预计在第 9.616071 周时，每小时将达到 100,000 次点击。因此，我们的模型告诉我们，鉴于当前的用户行为和我们初创公司的发展势头，再过一个月我们将达到容量阈值。

当然，我们的预测存在一定的不确定性。为了获得更真实的预测结果，可以引入更复杂的统计方法，以找出我们在未来的预测中需要预期的方差。

然后，还有用户和底层用户行为的动态，这是我们无法准确建模的。然而，在这一点上，我们对当前的预测是满意的。毕竟，我们现在可以准备所有耗时的操作。如果我们紧密监控网站流量，我们将及时看到何时需要分配新资源。

# 总结

恭喜你！你刚刚学到了两个重要的东西，其中最重要的一点是，作为一个典型的机器学习操作员，你将花费大部分时间来理解和优化数据——正是我们在第一个小型机器学习示例中所做的。我们希望这个示例能帮助你开始将注意力从算法转向数据。接着，你学到了正确的实验设置有多么重要，且避免混淆训练和测试数据是至关重要的。

诚然，使用多项式拟合在机器学习领域并不是最炫酷的事情。我们选择它是为了在传达我们之前总结的两个最重要的信息时，不让你被某些闪亮算法的酷炫分散注意力。

接下来，让我们进入下一章，在其中我们将深入探讨 scikit-learn 这一神奇的机器学习工具包，概述不同类型的学习，并展示特征工程的美妙。
