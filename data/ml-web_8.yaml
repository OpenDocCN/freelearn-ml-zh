- en: Chapter 8. Sentiment Analyser Application for Movie Reviews
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 电影评论情感分析应用
- en: In this chapter, we describe an application to determine the sentiment of movie
    reviews using algorithms and methods described throughout the book. In addition,
    the **Scrapy** library will be used to collect reviews from different websites
    through a search engine API (Bing search engine). The text and the title of the
    movie review is extracted using the newspaper library or following some pre-defined
    extraction rules of an HTML format page. The sentiment of each review is determined
    using a naive Bayes classifier on the most informative words (using the X *2*
    measure) in the same way as in [Chapter 4](text00032.html#ch04 "Chapter 4. Web
    Mining Techniques") , *Web Mining Techniques* . Also, the rank of each page related
    to each movie query is calculated for completeness using the PageRank algorithm
    discussed in [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques")
    , *Web Mining Techniques* . This chapter will discuss the code used to build the
    application, including the Django models and views and the Scrapy scraper is used
    to collect data from the web pages of the movie reviews. We start by giving an
    example of what the web application will be and explaining the search engine API
    used and how we include it in the application. We then describe how we collect
    the movie reviews, integrating the Scrapy library into Django, the models to store
    the data, and the main commands to manage the application. All the code discussed
    in this chapter is available in the GitHub repository of the author inside the
    `chapter_8` folder at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8)
    .
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了一个应用，该应用使用本书中描述的算法和方法来确定电影评论的情感。此外，我们将使用**Scrapy**库通过搜索引擎API（必应搜索引擎）从不同网站收集评论。电影评论的文本和标题使用newspaper库或遵循HTML格式页面的某些预定义提取规则进行提取。每个评论的情感是通过在最有信息量的单词上使用朴素贝叶斯分类器（使用X
    *2*度量）来确定的，这与[第4章](text00032.html#ch04 "第4章. 网络挖掘技术")中提到的*网络挖掘技术*相同。此外，使用在第4章中讨论的PageRank算法（*网络挖掘技术*）计算与每个电影查询相关的每个页面的排名，以确保完整性。本章将讨论构建应用所使用的代码，包括Django模型和视图，以及Scrapy爬虫用于从电影评论的网页中收集数据。我们首先给出一个网络应用的示例，并解释所使用的搜索引擎API以及如何将其包含在应用中。然后，我们描述如何收集电影评论，将Scrapy库集成到Django中，存储数据的模型以及管理应用的主要命令。本章中讨论的所有代码都可在作者的GitHub仓库中找到，位于`chapter_8`文件夹内，网址为[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8)。
- en: Application usage overview
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用使用概述
- en: 'The home web page is as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 主网页如下：
- en: '![Application usage overview](img/Image00541.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![应用使用概述](img/Image00541.jpg)'
- en: 'The user can type in the movie name, if they want to know the review''s sentiments
    and relevance. For example, we look for *Batman vs Superman Dawn of Justice* in
    the following screenshot:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以输入电影名称，如果他们想知道评论的情感和相关性。例如，我们在以下截图中搜索*蝙蝠侠大战超人 正义黎明*：
- en: '![Application usage overview](img/Image00542.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![应用使用概述](img/Image00542.jpg)'
- en: 'The application collects and scrapes 18 reviews from the Bing search engine
    and, using the Scrapy library, it analyzes their sentiment (15 positive and 3
    negative). All data is stored in Django models, ready to be used to calculate
    the relevance of each page using the PageRank algorithm (the links at the bottom
    of the page as seen in the preceding screenshot). In this case, using the PageRank
    algorithm, we have the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用从必应搜索引擎收集并爬取了18条评论，并使用Scrapy库分析了它们的情感（15条正面评论和3条负面评论）。所有数据都存储在Django模型中，准备好使用PageRank算法计算每个页面的相关性（如前一个截图中所见页面底部的链接）。在这种情况下，使用PageRank算法，我们有以下结果：
- en: '![Application usage overview](img/Image00543.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![应用使用概述](img/Image00543.jpg)'
- en: This is a list of the most relevant pages to our movie review search, setting
    a depth parameter 2 on the scraping crawler (refer the following section for further
    details). Note that to have a good result on page relevance, you have to crawl
    thousands of pages (the preceding screenshot shows results for around 50 crawled
    pages).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个与我们电影评论搜索最相关的页面列表，在爬虫爬取器上设置深度参数为2（有关更多详细信息，请参阅以下章节）。请注意，为了在页面相关性上获得良好的结果，您需要爬取数千个页面（前面的截图显示了大约50个爬取的页面）。
- en: 'To write the application, we start the server as usual (see [Chapter 6](text00046.html#ch06
    "Chapter 6. Getting Started with Django") , *Getting Started with Django* , and
    [Chapter 7](text00050.html#page "Chapter 7. Movie Recommendation System Web Application")
    , *Movie Recommendation System Web Application* ) and the main app in Django.
    First, we create a folder to store all our codes, `movie_reviews_analyzer_app`
    , and then we initialize Django using the following command:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写应用程序，我们像往常一样启动服务器（参见[第6章](text00046.html#ch06 "第6章 Django入门")，*Django入门*，和[第7章](text00050.html#page
    "第7章 电影推荐系统Web应用程序")，*电影推荐系统Web应用程序*）以及Django的主要应用程序。首先，我们创建一个文件夹来存储所有代码，`movie_reviews_analyzer_app`，然后我们使用以下命令初始化Django：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We set the settings in the `.py` file as we did in the *Settings* section of
    [Chapter 6](text00046.html#ch06 "Chapter 6. Getting Started with Django") , *Getting
    Started with Django* , and the *Application Setup* section of [Chapter 7](text00050.html#page
    "Chapter 7. Movie Recommendation System Web Application") , *Movie Recommendation
    System Web Application* (of course, in this case the name is `webmining_server`
    instead of `server_movierecsys` ).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`.py`文件中设置设置，就像在[第6章](text00046.html#ch06 "第6章 Django入门")，*Django入门*，和[第7章](text00050.html#page
    "第7章 电影推荐系统Web应用程序")，*电影推荐系统Web应用程序*的*设置*部分以及*应用程序设置*部分所做的那样（当然，在这种情况下，名称是`webmining_server`而不是`server_movierecsys`）。
- en: The sentiment analyzer application has the main views in the `.py` file in the
    main `webmining_server` folder instead of the `app` (pages) folder as we did previously
    (see [Chapter 6](text00046.html#ch06 "Chapter 6. Getting Started with Django")
    , *Getting Started with Django* , and [Chapter 7](text00050.html#page "Chapter 7. Movie
    Recommendation System Web Application") , *Movie Recommendation System Web Application*
    ), because the functions now refer more to the general functioning of the server
    instead of the specific app (pages).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析应用程序的主要视图位于主`webmining_server`文件夹中的`.py`文件，而不是我们之前所用的`app`（页面）文件夹中（参见[第6章](text00046.html#ch06
    "第6章 Django入门")，*Django入门*，和[第7章](text00050.html#page "第7章 电影推荐系统Web应用程序")，*电影推荐系统Web应用程序*），因为现在的功能更多地涉及到服务器的通用功能，而不是特定应用程序（页面）。
- en: 'The last operation to make the web service operational is to create a `superuser`
    account and go live with the server:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使网络服务可操作的最后一步是创建一个`superuser`账户并启动服务器：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that the structure of the application has been explained, we can discuss
    the different parts in more detail starting from the search engine API used to
    collect URLs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经解释了应用程序的结构，我们可以更详细地讨论不同部分，从用于收集URL的搜索引擎API开始。
- en: Search engine choice and the application code
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 搜索引擎选择和应用程序代码
- en: Since scraping directly from the most relevant search engines such as Google,
    Bing, Yahoo, and others is against their term of service, we need to take initial
    review pages from their REST API (using scraping services such as Crawlera, [http://crawlera.com/](http://www.crawlera.com/)
    , is also possible). We decided to use the Bing service, which allows 5,000 queries
    per month for free.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接从最相关的搜索引擎如Google、Bing、Yahoo等抓取数据违反了它们的条款服务，我们需要从它们的REST API（使用如Crawlera等抓取服务[http://crawlera.com/](http://www.crawlera.com/)，也是可能的）获取初始审查页面。我们决定使用Bing服务，该服务每月免费提供5,000次查询。
- en: 'In order to do that, we register to the Microsoft Service to obtain the key
    needed to allow the search. Briefly, we followed these steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们注册到Microsoft服务以获取允许搜索所需的密钥。简而言之，我们遵循了以下步骤：
- en: Register online on [https://datamarket.azure.com](https://datamarket.azure.com)
    .
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[https://datamarket.azure.com](https://datamarket.azure.com)上在线注册。
- en: In **My Account** , take the **Primary Account Key** .
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**我的账户**中，获取**主账户密钥**。
- en: 'Register a new application (under **DEVELOPERS** | **REGISTER** ; put **Redire**
    **ct URI** : `https://www.` `bing.com` )'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**开发者** | **注册**下注册一个新应用程序（将**重定向URI**设置为`https://www.bing.com`）
- en: 'After that, we can write a function that retrieves as many URLs relevant to
    our query as we want:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以编写一个函数，检索与我们查询相关的尽可能多的URL：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `API_KEY` parameter is taken from the Microsoft account, `query` is a string
    which specifies the movie name, and `num_reviews = 30` is the number of URLs returned
    in total from the Bing API. With the list of URLs that contain the reviews, we
    can now set up a scraper to extract from each web page the title and the review
    text using Scrapy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`API_KEY` 参数来自Microsoft账户，`query` 是一个字符串，用于指定电影名称，而 `num_reviews = 30` 是Bing
    API返回的总URL数量。有了包含评论的URL列表，我们现在可以设置一个爬虫，使用Scrapy从每个网页中提取标题和评论文本。'
- en: Scrapy setup and the application code
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy设置和应用程序代码
- en: 'Scrapy is a Python library is used to extract content from web pages or to
    crawl pages linked to a given web page (see the *Web crawlers (or spiders)* section
    of [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques") , *Web
    Mining Techniques* , for more details). To install the library, type the following
    in the terminal:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy是一个Python库，用于从网页中提取内容或爬取指向给定网页的页面（有关更多详细信息，请参阅[第4章](text00032.html#ch04
    "第4章。网络挖掘技术")中的*Web爬虫（或蜘蛛）*部分，*网络挖掘技术*）。要安装库，请在终端中输入以下内容：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install the executable in the `bin` folder:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在`bin`文件夹中安装可执行文件：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the `movie_reviews_analyzer_app` folder, we initialize our Scrapy project
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从`movie_reviews_analyzer_app`文件夹中，我们初始化Scrapy项目如下：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This command will create the following tree inside the `scrapy_spider` folder:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在`scrapy_spider`文件夹内创建以下目录结构：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `pipelines.py` and `items.py` files manage how the scraped data is stored
    and manipulated, and they will be discussed later in the *Spiders* and *Integrate
    Django with Scrapy* sections. The `settings.py` file sets the parameters each
    spider (or crawler) defined in the `spiders` folder uses to operate. In the following
    two sections, we describe the main parameters and spiders used in this application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipelines.py` 和 `items.py` 文件管理爬取数据的存储和处理方式，它们将在*蜘蛛*和*将Django与Scrapy集成*部分中稍后讨论。`settings.py`
    文件设置了`spiders`文件夹中定义的每个爬虫（或爬虫）使用的参数。在以下两个部分中，我们描述了在此应用程序中使用的主要参数和爬虫。'
- en: Scrapy settings
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scrapy设置
- en: 'The `settings.py` file collects all the parameters used by each spider in the
    Scrapy project to scrape web pages. The main parameters are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`settings.py` 文件收集了Scrapy项目中每个爬虫用于爬取网页的所有参数。主要参数如下：'
- en: '`DEPTH_LIMIT` : The number of subsequent pages crawled following an initial
    URL. The default is `0` and it means that no limit is set.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEPTH_LIMIT` : 在初始URL之后爬取的后续页面的数量。默认值为 `0`，表示未设置限制。'
- en: '`LOG_ENABLED` : To allow/deny Scrapy to log on the terminal while executing
    default is true.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LOG_ENABLED` : 允许/拒绝Scrapy在执行时在终端上记录日志，默认值为true。'
- en: '`ITEM_PIPELINES = {''scrapy_spider.pipelines.ReviewPipeline'': 1000,}` : The
    path of the pipeline function to manipulate data extracted from each web page.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ITEM_PIPELINES = {''scrapy_spider.pipelines.ReviewPipeline'': 1000,}` : 处理从每个网页中提取的数据的管道函数的路径。'
- en: '`CONCURRENT_ITEMS = 200` : The number of concurrent items processed in the
    pipeline.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONCURRENT_ITEMS = 200` : 在管道中处理的并发项目数量。'
- en: '`CONCURRENT_REQUESTS = 5000` : The maximum number of simultaneous requests
    handled by Scrapy.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONCURRENT_REQUESTS = 5000` : Scrapy处理的并发请求数量的最大值。'
- en: '`CONCURRENT_REQUESTS_PER_DOMAIN = 3000` : The maximum number of simultaneous
    requests handled by Scrapy for each specified domain.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONCURRENT_REQUESTS_PER_DOMAIN = 3000` : Scrapy为每个指定域名处理的并发请求数量的最大值。'
- en: 'The larger the depth, more the pages are scraped and, consequently, the time
    needed to scrape increases. To speed up the process, you can set high value on
    the last three parameters. In this application (the `spiders` folder), we set
    two spiders: a scraper to extract data from each movie review URL (`movie_link_results.py`
    ) and a crawler to generate a graph of webpages linked to the initial movie review
    URL (`recursive_link_results.py` ).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 深度越大，爬取的页面越多，因此爬取所需的时间也越长。为了加快过程，你可以将最后三个参数设置为高值。在此应用程序（`spiders` 文件夹中），我们设置了两个爬虫：一个用于从每个电影评论URL中提取数据（`movie_link_results.py`）的爬虫，以及一个用于生成指向初始电影评论URL的网页链接图（`recursive_link_results.py`）的爬虫。
- en: Scraper
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爬虫
- en: 'The scraper on `movie_link_results.py` looks as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_link_results.py` 上的爬虫如下所示：'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that the `Spider` class from `scrapy` is inherited by the `Search`
    class and the following standard methods have to be defined to override the standard
    methods:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`scrapy`中的`Spider`类被`Search`类继承，并且必须定义以下标准方法以覆盖标准方法：
- en: '`__init__` : The constructor of the spider needs to define the `start_urls`
    list that contains the URL to extract content from. In addition, we have custom
    variables such as `search_key` and `keywords` that store the information related
    to the query of the movie''s title used on the search engine API.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__`：蜘蛛的构造函数需要定义包含从其提取内容的URL的 `start_urls` 列表。此外，我们还有自定义变量，如 `search_key`
    和 `keywords`，它们存储与在搜索引擎API上使用的电影标题查询相关的信息。'
- en: '`start_requests` : This function is triggered when `spider` is called and it
    declares what to do for each URL in the `start_urls` list; for each URL, the custom
    `parse_site` function will be called (instead of the default `parse` function).'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_requests`：当调用 `spider` 时，此函数被触发，并声明对 `start_urls` 列表中的每个URL要执行的操作；对于每个URL，将调用自定义的
    `parse_site` 函数（而不是默认的 `parse` 函数）。'
- en: '`parse_site` : It is a custom function to parse data from each URL. To extract
    the title of the review and its text content, we used the newspaper library (`sudo
    pip install newspaper` ) or, if it fails, we parse the HTML file directly using
    some defined rules to avoid the noise due to undesired tags (each rule structure
    is defined with the `sel.xpath` command). To achieve this result, we select some
    popular domains (`rottentomatoes` , `cnn` , and so on) and ensure the parsing
    is able to extract the content from these websites (not all the extraction rules
    are displayed in the preceding code but they can be found as usual in the GitHub
    file). The data is then stored in a page `Django` model using the related Scrapy
    item and the `ReviewPipeline` function (see the following section).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_site`：这是一个自定义函数，用于解析每个URL的数据。为了提取评论的标题及其文本内容，我们使用了 newspaper 库（`sudo
    pip install newspaper`），或者在失败的情况下，直接使用一些定义的规则解析HTML文件，以避免因不想要的标签而产生的噪声（每个规则结构都是使用
    `sel.xpath` 命令定义的）。为了达到这个结果，我们选择了某些流行的域名（如 `rottentomatoes`、`cnn` 等），并确保解析能够从这些网站上提取内容（前述代码中没有显示所有提取规则，但它们可以在GitHub文件中找到）。然后，使用相关的Scrapy项和
    `ReviewPipeline` 函数将数据存储在页面 `Django` 模型中（见下一节）。'
- en: '`CheckQueryinReview` : This is a custom function to check whether the movie
    title (from the query) is contained in the content or title of each web page.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CheckQueryinReview`：这是一个自定义函数，用于检查电影标题（来自查询）是否包含在每个网页的内容或标题中。'
- en: 'To run the spider, we need to type in the following command from the `scrapy_spider`
    (internal) folder:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行蜘蛛，我们需要在 `scrapy_spider`（内部）文件夹中输入以下命令：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Pipelines
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管道
- en: 'The pipelines define what to do when a new page is scraped by the spider. In
    the preceding case, the `parse_site` function returns a `PageItem` object, which
    triggers the following pipeline (`pipelines.py` ):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 管道定义了当蜘蛛抓取新页面时应该执行的操作。在前面的例子中，`parse_site` 函数返回一个 `PageItem` 对象，这触发了以下管道（`pipelines.py`）：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This class simply saves each item (a new page in the spider notation).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此类简单地保存每个项目（在蜘蛛的术语中，即新页面）。
- en: Crawler
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爬虫
- en: 'As we showed in the overview (the preceding section), the relevance of the
    review is calculated using the PageRank algorithm after we have stored all the
    linked pages starting from the review''s URL. The crawler `recursive_link_results.py`
    performs this operation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在概述（前述章节）中所示，在存储从评论的URL开始的链接页面之后，使用PageRank算法计算评论的相关性。执行此操作的爬虫是 `recursive_link_results.py`：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `CrawlSpider` class from `scrapy` is inherited by the `Search` class, and
    the following standard methods have to be defined to override the standard methods
    (as for the spider case):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`scrapy` 中的 `CrawlSpider` 类继承自 `Search` 类，必须定义以下标准方法以覆盖标准方法（类似于蜘蛛的情况）：'
- en: '`__init__` : The is a constructor of the class. The `start_urls` parameter
    defines the starting URL from which the spider will start to crawl until the `DEPTH_LIMIT`
    value is reached. The `rules` parameter sets the type of URL allowed/denied to
    scrape (in this case, the same page but with different font sizes is disregarded)
    and it defines the function to call to manipulate each retrieved page (`parse_item`
    ). Also, a custom variable `search_id` is defined, which is needed to store the
    ID of the query within the other data.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__`：这是类的构造函数。`start_urls` 参数定义了蜘蛛开始爬取的起始URL，直到达到 `DEPTH_LIMIT` 值。`rules`
    参数设置允许/拒绝爬取的URL类型（在这种情况下，忽略具有不同字体大小的相同页面）并定义了用于操作每个检索到的页面的函数（`parse_item`）。此外，定义了一个自定义变量
    `search_id`，它需要存储查询的ID，以便在其它数据中存储。'
- en: '`parse_item` : This is a custom function called to store the important data
    from each retrieved page. A new Django item of the `Page` model (see the following
    section) from each page is created, which contains the title and content of the
    page (using the `xpath` HTML parser). To perform the PageRank algorithm, the connection
    from the page that links to each page and the page itself is saved as an object
    of the `Link` model using the related Scrapy item (see the following sections).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_item` : 这是一个被调用来存储从每个检索到的页面中提取的重要数据的自定义函数。为每个页面创建一个新的 Django `Page`
    模型项（见下文章节），其中包含页面的标题和内容（使用 `xpath` HTML 解析器）。为了执行 PageRank 算法，将链接到每个页面的页面和页面本身的关系保存为
    `Link` 模型的对象，使用相关的 Scrapy 项（见下文章节）。'
- en: 'To run the crawler, we need to type the following from the (internal) `scrapy_spider`
    folder:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行爬虫，我们需要在（内部）`scrapy_spider` 文件夹中输入以下命令：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Django models
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Django 模型
- en: 'The data collected using the spiders needs to be stored in a database. In Django,
    the database tables are called models and defined in the `models.py` file (within
    the `pages` folder). The content of this file is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蜘蛛收集的数据需要存储在数据库中。在 Django 中，数据库表被称为模型，并在 `models.py` 文件中定义（位于 `pages` 文件夹内）。该文件的内容如下：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Each movie title typed on the home page of the application is stored in the
    `SearchTerm` model, while the data of each web page is collected in an object
    of the `Page` model. Apart from the content field (HTML, title, URL, content),
    the sentiment of the review and the depth in graph network are recorded (a Boolean
    also indicates if the web page is a movie review page or simply a linked page).
    The `Link` model stores all the graph links between pages, which are then used
    by the PageRank algorithm to calculate the relevance of the reviews web pages.
    Note that the `Page` model and the `Link` model are both linked to the related
    `SearchTerm` through a foreign key. As usual, to write these models as database
    tables, we type the following commands:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序的首页键入的每个电影标题存储在 `SearchTerm` 模型中，而每个网页的数据则收集在 `Page` 模型的对象中。除了内容字段（HTML、标题、URL、内容）外，还记录了评论的情感和图网络中的深度（布尔值也指示网页是否是电影评论页面或仅仅是链接页面）。`Link`
    模型存储了页面之间的所有图链接，然后这些链接被 PageRank 算法用来计算评论网页的相关性。请注意，`Page` 模型和 `Link` 模型都通过外键与相关的
    `SearchTerm` 相关联。像往常一样，为了将这些模型作为数据库表编写，我们输入以下命令：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To populate these Django models, we need to make Scrapy interact with Django,
    and this is the subject of the following section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了填充这些 Django 模型，我们需要让 Scrapy 与 Django 交互，这是下文章节的主题。
- en: Integrating Django with Scrapy
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整合 Django 与 Scrapy
- en: 'To make paths easy to call, we remove the external `scrapy_spider` folder so
    that inside the `movie_reviews_analyzer_app` , the `webmining_server` folder is
    at the same level as the `scrapy_spider` folder:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使路径易于调用，我们移除了外部的 `scrapy_spider` 文件夹，这样在 `movie_reviews_analyzer_app` 中，`webmining_server`
    文件夹与 `scrapy_spider` 文件夹处于同一级别：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We set the Django path into the Scrapy `settings.py` file:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Django 路径设置到 Scrapy 的 `settings.py` 文件中：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can install the library that will allow managing Django models from
    Scrapy:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以安装允许从 Scrapy 管理 Django 模型的库：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the `items.py` file, we write the links between Django models and Scrapy
    items as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `items.py` 文件中，我们按照以下方式编写 Django 模型和 Scrapy 项之间的链接：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Each class inherits the `DjangoItem` class so that the original Django models
    declared with the `django_model` variable are automatically linked. The Scrapy
    project is now completed so we can continue our discussion explaining the Django
    codes that handle the data extracted by Scrapy and the Django commands needed
    to manage the applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类都继承自 `DjangoItem` 类，这样使用 `django_model` 变量声明的原始 Django 模型将自动链接。Scrapy 项目现在已完成，我们可以继续讨论解释处理
    Scrapy 提取的数据的 Django 代码以及管理应用程序所需的 Django 命令。
- en: Commands (sentiment analysis model and delete queries)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令（情感分析模型和删除查询）
- en: The application needs to manage some operations that are not allowed to the
    final user of the service, such as defining a sentiment analysis model and deleting
    a query of a movie in order to redo it instead of retrieving the existing data
    from memory. The following sections will explain the commands to perform these
    actions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序需要管理一些不允许最终用户执行的操作，例如定义情感分析模型和删除电影查询以便重新执行，而不是从内存中检索现有数据。以下章节将解释执行这些操作的命令。
- en: Sentiment analysis model loader
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感分析模型加载器
- en: 'The final goal of this application is to determine the sentiment (positive
    or negative) of the movie reviews. To achieve that, a sentiment classifier must
    be built using some external data, and then it should be stored in memory (cache)
    to be used by each query request. This is the purpose of the `load_sentimentclassifier.py`
    command displayed hereafter:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本应用程序的最终目标是确定电影评论的情感（正面或负面）。为了实现这一点，必须使用一些外部数据构建一个情感分类器，然后将其存储在内存（缓存）中，以便每个查询请求使用。这就是以下显示的`load_sentimentclassifier.py`命令的目的：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At the beginning of the file, the variable `method_selfeatures` sets the method
    of feature selection (in this case, the features are the words in the reviews;
    see [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques") , *Web
    Mining Techniques* , for further details) used to train the classifier `train_clf`
    . The maximum number of best words (features) is defined by the input parameter
    `num_bestwords` . The classifier and the best features (`bestwords` ) are then
    stored in the cache ready to be used by the application (using the `cache` module).
    The classifier and the methods to select the best words (features) are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件开头，变量`method_selfeatures`设置了用于训练分类器`train_clf`的特征选择方法（在这种情况下，特征是评论中的单词；详见[第4章](text00032.html#ch04
    "第4章。网络挖掘技术")，*网络挖掘技术*，以获取更多详细信息）。最佳单词（特征）的最大数量由输入参数`num_bestwords`定义。然后，分类器和最佳特征（`bestwords`）存储在缓存中，以便应用程序（使用`cache`模块）使用。分类器和选择最佳单词（特征）的方法如下：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Three methods are written to select words in the preceding code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，编写了三种方法来选择单词：
- en: '`stopword_filtered_words_features` : Eliminates the `stopwords` using the **Natural
    Language Toolkit** ( **NLTK** ) list of conjunctions and considers the rest as
    relevant words'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopword_filtered_words_features`：使用**自然语言工具包**（**NLTK**）的并列词列表消除`stopwords`，并将剩余的视为相关单词'
- en: '`best_words_features` : Using the *X²* measure (`NLTK` library), the most informative
    words related to positive or negative reviews are selected (see [Chapter 4](text00032.html#ch04
    "Chapter 4. Web Mining Techniques") , *Web Mining Techniques* , for further details)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_words_features`：使用*X²*度量（`NLTK`库），选择与正面或负面评论相关的最有信息量的单词（详见[第4章](text00032.html#ch04
    "第4章。网络挖掘技术")，*网络挖掘技术*，以获取更多详细信息）'
- en: '`best_bigrams_word_features` : Uses the *X²* measure (`NLTK` library) to find
    the 200 most informative bigrams from the set of words (see *Chapter 4* , *Web
    Mining Techniques* , for further details)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`best_bigrams_word_features`：使用*X²*度量（`NLTK`库）从单词集合中找到200个最有信息量的二元组（详见*第4章*，*网络挖掘技术*，以获取更多详细信息）'
- en: 'The chosen classifier is the Naive Bayes algorithm (see [Chapter 3](text00024.html#page
    "Chapter 3. Supervised Machine Learning") , *Supervised Machine Learning* ) and
    the labeled text (positive, negative sentiment) is taken from the `NLTK.corpus`
    of `movie_reviews` . To install it, open a terminal in Python and install `movie_reviews`
    from `corpus` :'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的分类器是朴素贝叶斯算法（见[第3章](text00024.html#page "第3章。监督机器学习")，*监督机器学习*），并且标记的文本（正面、负面情感）来自`NLTK.corpus`中的`movie_reviews`。要安装它，请在Python中打开一个终端并从`corpus`安装`movie_reviews`：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Deleting an already performed query
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除已执行过的查询
- en: 'Since we can specify different parameters (such as the feature selection method,
    the number of best words, and so on), we may want to perform and store again the
    sentiment of the reviews with different values. The `delete_query` command is
    needed for this purpose and it is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以指定不同的参数（例如特征选择方法、最佳单词数量等），我们可能希望再次执行并存储具有不同值的评论的情感。为此需要`delete_query`命令，如下所示：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we run the command without specifying the `searchid` (the ID of the query),
    the list of all the queries and related IDs will be shown. After that we can choose
    which query we want to delete by typing the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在不指定`searchid`（查询ID）的情况下运行该命令，将显示所有查询及其相关ID的列表。之后，我们可以通过键入以下内容来选择我们想要删除的查询：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can use the cached sentiment analysis model to show the user the online sentiment
    of the chosen movie, as we explain in the following section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用缓存的情感分析模型来向用户展示所选电影的在线情感，正如我们在下一节中解释的那样。
- en: Sentiment reviews analyser – Django views and HTML
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感评论分析器 – Django视图和HTML
- en: Most of the code explained in this chapter (commands, Bing search engine, Scrapy,
    and Django models) is used in the function analyzer in `views.py` to power the
    home webpage shown in the *Application usage overview* section (after declaring
    the URL in the `urls.py` file as `url(r'^$','webmining_server.views.analyzer')`
    ).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中解释的大多数代码（命令、必应搜索引擎、Scrapy和Django模型）都用于`views.py`中的`analyzer`函数，以支持*应用使用概述*部分显示的首页（在`urls.py`文件中声明URL为`url(r'^$','webmining_server.views.analyzer')`之后）。
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The inserted movie title is stored in the `query` variable and sent to the
    `bing_api` function to collect review''s URL. The URL are then scraped calling
    Scrapy to find the review texts, which are processed using the `clf` classifier
    model and the selected most informative words (`bestwords` ) retrieved from the
    cache (or the same model is generated again in case the cache is empty). The counts
    of the predicted sentiments of the reviews (`positive_counts` , `negative_counts`
    , and `reviews_classified` ) are then sent back to the `home.html` (the `templates`
    folder) page, which uses the following Google pie chart code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 插入的电影标题存储在`query`变量中，并传递给`bing_api`函数以收集评论的URL。然后调用Scrapy抓取评论文本，这些文本使用`clf`分类器模型进行处理，并从缓存中检索出选定的最有信息量的单词（`bestwords`）。在这种情况下，如果缓存为空，则再次生成相同的模型）。然后，将预测的评论情感计数（`positive_counts`、`negative_counts`和`reviews_classified`）发送回`home.html`（`templates`文件夹）页面，该页面使用以下Google饼图代码：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The function `drawChart` calls the Google `PieChart` visualization function,
    which takes as input the data (the positive and negative counts) to create the
    pie chart. To have more details about how the HTML code interacts with the Django
    views, refer to [Chapter 6](text00046.html#ch06 "Chapter 6. Getting Started with
    Django") , *Getting Started with Django* , in the *URL and views behind html web
    pages* section. From the result page with the sentiment counts (see the *Application
    usage overview* section), the PagerRank relevance of the scraped reviews can be
    calculated using one of the two links at the bottom of the page. The Django code
    behind this operation is discussed in the following section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`drawChart`调用Google的`PieChart`可视化函数，该函数以数据（正面和负面的计数）作为输入来创建饼图。要了解更多关于HTML代码如何与Django视图交互的细节，请参阅[第6章](text00046.html#ch06
    "第6章。开始使用Django")，*开始使用Django*，在*HTML网页背后的URL和视图*部分。从带有情感计数的结果页面（参见*应用使用概述*部分），可以使用页面底部的两个链接之一计算抓取评论的PageRank相关性。该操作的Django代码将在下一节中讨论。
- en: 'PageRank: Django view and the algorithm code'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PageRank：Django视图和算法代码
- en: 'To rank the importance of the online reviews, we have implemented the PageRank
    algorithm (see [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques")
    , *Web Mining Techniques* , in the *Ranking: PageRank algorithm* section) into
    the application. The `pgrank.py` file in the `pgrank` folder within the `webmining_server`
    folder implements the algorithm that follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对在线评论的重要性进行排序，我们在应用中实现了PageRank算法（参见[第4章](text00032.html#ch04 "第4章。网络挖掘技术")，*网络挖掘技术*，在*排名：PageRank算法*部分），其实现如下：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This code takes all the links stores associated with the given `SearchTerm`
    object and implements the PageRank score for each page *i* at time *t* , where
    *P(i)* is given by the recursive equation:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码获取与给定的`SearchTerm`对象关联的所有链接存储，并为每个页面*i*在时间*t*的PageRank分数实现，其中*P(i)*由以下递归方程给出：
- en: '![PageRank: Django view and the algorithm code](img/Image00544.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![PageRank: Django视图和算法代码](img/Image00544.jpg)'
- en: 'Here, *N* is the total number of pages, and ![PageRank: Django view and the
    algorithm code](img/Image00545.jpg) ( *N[j] * is the number of out links of page
    *j* ) if page *j* points to *i* ; otherwise, *N* is `0` . The parameter *D* is
    the so-called **damping factor** (set to 0.85 in the preceding code), and it represents
    the probability to follow the transition given by the transition matrix *A* .
    The equation is iterated until the convergence parameter `eps` is satisfied or
    the maximum number of iterations, `num_iterations` , is reached. The algorithm
    is called by clicking either **scrape and calculate page rank (may take a long
    time)** or **calculate page rank** links at the bottom of the `home.html` page
    after the sentiment of the movie reviews has been displayed. The link is linked
    to the function `pgrank_view` in the `views.py` (through the declared URL in `urls.py:
    url(r''^pg-rank/(?P<pk>\d+)/'',''webmining_server.views.pgrank_view'', name=''pgrank_view'')`
    ):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N* 是页面的总数，如果页面 *j* 指向 *i*，则 ![PageRank：Django视图和算法代码](img/Image00545.jpg)（
    *N[j] * 是页面 *j* 的出链数量）；否则，*N* 是 `0`。参数 *D* 是所谓的 **阻尼因子**（在前面的代码中设置为0.85），它表示跟随由转换矩阵
    *A* 给出的转换的概率。方程会迭代，直到满足收敛参数 `eps` 或达到最大迭代次数 `num_iterations`。算法通过点击 `home.html`
    页面底部的 **抓取并计算页面排名（可能需要很长时间）** 或 **计算页面排名** 链接来调用。该链接链接到 `views.py` 中的 `pgrank_view`
    函数（通过在 `urls.py` 中声明的URL：`url(r'^pg-rank/(?P<pk>\d+)/','webmining_server.views.pgrank_view',
    name='pgrank_view')`）：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This code calls the crawler to collect all the linked pages to the reviews and
    calculate the PageRank scores using the code discussed earlier. Then the scores
    are displayed in the `pg-rank.html` page (in descending order by page rank score)
    as we showed in the *Application usage overview* section of this chapter. Since
    this function can take a long time to process (to crawl thousands of pages), the
    command `run_scrapelinks.py` has been written to run the Scrapy crawler (the reader
    is invited to read or modify the script as they like as an exercise).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码调用爬虫收集所有链接到评论的页面，并使用前面讨论过的代码计算PageRank分数。然后，这些分数按页面排名分数降序显示在 `pg-rank.html`
    页面上（正如我们在本章的 *应用使用概述* 部分所展示的）。由于这个函数可能需要很长时间来处理（爬取数千个页面），因此已经编写了 `run_scrapelinks.py`
    命令来运行Scrapy爬虫（读者被邀请阅读或修改脚本作为练习）。
- en: Admin and API
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理和API
- en: 'As the last part of the chapter, we describe briefly some possible admin management
    of the model and the implementation of an API endpoint to retrieve the data processed
    by the application. In the `pages` folder, we can set two admin interfaces in
    the `admin.py` file to check the data collected by the `SearchTerm` and `Page`
    models:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的最后一部分，我们简要描述了一些可能的模型管理方法和实现API端点以检索应用程序处理的数据。在 `pages` 文件夹中，我们可以在 `admin.py`
    文件中设置两个管理界面来检查 `SearchTerm` 和 `Page` 模型收集的数据：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that both `SearchTermAdmin` and `PageAdmin` display objects with decreasing
    ID (and `new_rank` in the case of `PageAdmin` ). The following screenshot is an
    example:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`SearchTermAdmin` 和 `PageAdmin` 都会显示按递减的ID（在 `PageAdmin` 的情况下还有 `new_rank`）排列的对象。以下截图是一个示例：
- en: '![Admin and API](img/Image00546.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![管理和API](img/Image00546.jpg)'
- en: 'Note that although it is not necessary, the `Link` model has also been included
    in the admin interface (`admin.site.register(Link)` ). More interestingly, we
    can set up an API endpoint to retrieve the sentiment counts related to a movie''s
    title. In the `api.py` file inside the pages folder, we can have the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管不是必需的，`Link` 模型也已经包含在管理界面中（使用 `admin.site.register(Link)`）。更有趣的是，我们可以设置一个API端点来检索与电影标题相关的情感计数。在页面文件夹内的
    `api.py` 文件中，我们可以有如下代码：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `PageCounts` class takes as input the ID of the search (the movie''s title)
    and it returns the sentiments, that is, positive and negative counts, for the
    movie''s reviews. To get the ID of `earchTerm` from a movie''s title, you can
    either look at the admin interface or use the other API endpoint `SearchTermsList`
    ; this simply returns the list of the movies'' titles together with the associated
    ID. The serializer is set on the `serializers.py` file:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`PageCounts` 类接受搜索（电影的标题）的ID作为输入，并返回电影的评论的情感，即正面和负面的计数。要从电影的标题获取 `SearchTerm`
    的ID，你可以查看管理界面或使用其他API端点 `SearchTermsList`；这简单地返回与相关ID一起的电影标题列表。序列化器设置在 `serializers.py`
    文件中：'
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To call these endpoints, we can again use the swagger interface (see [Chapter
    6](text00046.html#ch06 "Chapter 6. Getting Started with Django") , *Getting Started
    with Django* ) or use the `curl` command in the terminal to make these calls.
    For instance:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用这些端点，我们又可以再次使用swagger接口（见[第6章](text00046.html#ch06 "第6章。Django入门")，*Django入门*），或者使用终端中的`curl`命令来执行这些调用。例如：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: and
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we described a movie review sentiment analyzer web application
    to make you familiar with some of the algorithms and libraries we discussed in
    [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning") , *Supervised
    Machine Learning* , [Chapter 4](text00032.html#ch04 "Chapter 4. Web Mining Techniques")
    , *Web Mining Techniques* , and [Chapter 6](text00046.html#ch06 "Chapter 6. Getting
    Started with Django") , *Getting Started with Django* .
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了一个电影评论情感分析网络应用程序，让你熟悉我们在[第3章](text00024.html#page "第3章。监督机器学习")，*监督机器学习*，[第4章](text00032.html#ch04
    "第4章。网络挖掘技术")，*网络挖掘技术*，和[第6章](text00046.html#ch06 "第6章。Django入门")，*Django入门*中讨论的一些算法和库。
- en: 'This is the end of a journey: by reading this book and experimenting with the
    codes provided, you should have acquired significant practical knowledge about
    the most important machine learning algorithms used in the commercial environment
    nowadays.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段旅程的结束：通过阅读这本书并尝试提供的代码，你应该已经获得了关于目前商业环境中使用的重要机器学习算法的显著实用知识。
- en: You should be now ready to develop your own web applications and ideas using
    Python and some machine learning algorithms, learned by reading this book. Many
    challenging data-related problems are present in the real world today, waiting
    to be solved by people who can grasp and apply the material treated in this book,
    and you, who have arrived at this point, are certainly one of those people.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该已经准备好使用Python和一些机器学习算法开发自己的网络应用程序和想法了，这些算法是通过阅读这本书学到的。在现实世界中，今天存在许多具有挑战性的数据相关问题，等待那些能够掌握并应用这本书中讨论的材料的人来解决，而你，已经到达这个位置，无疑是这些人之一。
- en: 读累了记得休息一会哦~
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读累了记得休息一会哦~
- en: '**公众号：古德猫宁李**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**公众号：古德猫宁李**'
- en: 电子书搜索下载
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子书搜索下载
- en: 书单分享
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书单分享
- en: 书友学习交流
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 书友学习交流
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**网站：**[沉金书屋](https://www.chenjin5.com)'
- en: 电子书搜索下载
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子书搜索下载
- en: 电子书打包资源分享
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子书打包资源分享
- en: 学习资源分享
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习资源分享
