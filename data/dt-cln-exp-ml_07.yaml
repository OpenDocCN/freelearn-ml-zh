- en: '*Chapter 5*: Feature Selection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*: 特征选择'
- en: Depending on how you began your data analytic work and your own intellectual
    interests, you might have a different perspective on the topic of **feature selection**.
    You might think, *yeah, yeah, it is an important topic, but I really want to get
    to the model building*. Or, at the other extreme, you might view feature selection
    as at the core of model building and believe that you are 90% of the way toward
    having your model once you have chosen your features. For now, let's just agree
    that we should spend a good chunk of time understanding the relationships between
    features – and their relationship to a target if we are building a supervised
    model – before we do any serious model specification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你开始数据分析工作和你的个人智力兴趣的不同，你可能会对**特征选择**这个话题有不同的看法。你可能认为，“嗯，嗯，这是一个重要的主题，但我真的想开始模型构建。”或者，在另一个极端，你可能会认为特征选择是模型构建的核心，并相信一旦你选择了特征，你就已经完成了模型构建的90%。现在，让我们先达成共识，在我们进行任何严肃的模型指定之前，我们应该花一些时间来理解特征之间的关系——如果我们正在构建监督模型，那么它们与目标之间的关系。
- en: It is helpful to approach our feature selection work with the attitude that
    less is more. If we can reach nearly the same degree of accuracy or explain as
    much of the variance with fewer features, we should select the simpler model.
    Sometimes, we can actually get better accuracy with fewer features. This can be
    hard to wrap our brains around, and even be a tad disappointing for those of us
    who cut our teeth on building models that told rich and complicated stories.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以“少即是多”的态度来处理我们的特征选择工作是有帮助的。如果我们能用更少的特征达到几乎相同的准确度或解释更多的方差，我们应该选择更简单的模型。有时，我们实际上可以用更少的特征获得更好的准确度。这可能会很难理解，甚至对我们这些从构建讲述丰富和复杂故事模型的实践中成长起来的人来说有些令人失望。
- en: But we are less concerned with parameter estimates than with the accuracy of
    our predictions when fitting machine learning models. Unnecessary features can
    contribute to overfitting and tax hardware resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们在拟合机器学习模型时，对参数估计的关注不如对预测准确性的关注。不必要的特征可能导致过拟合并消耗硬件资源。
- en: We can sometimes spend months specifying the features of our model, even when
    there is a limited number of columns in the data. Bivariate correlations, such
    as those created in [*Chapter 2*](B17978_02_ePub.xhtml#_idTextAnchor025), *Examining
    Bivariate and Multivariate Relationships between Features and Targets*, give us
    some sense of what to expect, but the importance of a feature can vary significantly
    once other potentially explanatory features are introduced. The feature may no
    longer be significant, or, conversely, may only be significant when other features
    are included. Two features might be so highly correlated that including both of
    them offers very little additional information than including just one.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能需要花费数月时间来指定模型的特征，即使数据中列的数量有限。例如，在[*第二章*](B17978_02_ePub.xhtml#_idTextAnchor025)“检查特征与目标之间的双变量和多变量关系”中创建的双变量相关性，给我们一些预期的感觉，但一旦引入其他可能的解释特征，特征的重要性可能会显著变化。该特征可能不再显著，或者相反，只有在包含其他特征时才显著。两个特征可能高度相关，以至于包含两个特征与只包含一个特征相比，提供的额外信息非常有限。
- en: 'This chapter takes a close look at feature selection techniques applicable
    to a variety of predictive modeling tasks. Specifically, we will explore the following
    topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨适用于各种预测建模任务的特征选择技术。具体来说，我们将探讨以下主题：
- en: Selecting features for classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分类模型选择特征
- en: Selecting features for regression models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为回归模型选择特征
- en: Using forward and backward feature selection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正向和反向特征选择
- en: Using exhaustive feature selection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用穷举特征选择
- en: Eliminating features recursively in a regression model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在回归模型中递归消除特征
- en: Eliminating features recursively in a classification model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类模型中递归消除特征
- en: Using Boruta for feature selection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Boruta进行特征选择
- en: Using regularization and other embedded methods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化和其他嵌入式方法
- en: Using principal component analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主成分分析
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will work with the `feature_engine`, `mlxtend`, and `boruta` packages in
    this chapter, in addition to the `scikit-learn` library. You can use `pip` to
    install these packages. I have chosen a dataset with a small number of observations
    for our work in this chapter, so the code should work fine even on suboptimal
    workstations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用`feature_engine`、`mlxtend`和`boruta`包，以及`scikit-learn`库。您可以使用`pip`安装这些包。我选择了一个观测值数量较少的数据集用于本章的工作，因此代码即使在次优工作站上也能正常运行。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will work exclusively in this chapter with data from The National Longitudinal
    Survey of Youth, conducted by the United States Bureau of Labor Statistics. This
    survey started with a cohort of individuals in 1997 who were born between 1980
    and 1985, with annual follow-ups each year through 2017\. We will work with educational
    attainment, household demographic, weeks worked, and wage income data. The wage
    income column represents wages earned in 2016\. The NLS dataset can be downloaded
    for public use at [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专门使用美国劳工统计局进行的《青年纵向调查》数据。这项调查始于1997年，调查对象为1980年至1985年间出生的一代人，每年进行一次年度跟踪调查，直至2017年。我们将使用教育成就、家庭人口统计、工作周数和工资收入数据。工资收入列代表2016年赚取的工资。NLS数据集可以下载供公众使用，网址为[https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search)。
- en: Selecting features for classification models
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为分类模型选择特征
- en: The most straightforward feature selection methods are based on each feature's
    relationship with a target variable. The next two sections examine techniques
    for determining the *k* best features based on their linear or non-linear relationship
    with the target. These are known as filter methods. They are also sometimes called
    univariate methods since they evaluate the relationship between the feature and
    the target independent of the impact of other features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的特征选择方法是基于每个特征与目标变量的关系。接下来的两个部分将探讨基于特征与目标变量之间的线性或非线性关系来确定最佳*k*个特征的技术。这些被称为过滤方法。它们有时也被称为单变量方法，因为它们评估特征与目标变量之间的关系，而不考虑其他特征的影响。
- en: We use somewhat different strategies when the target is categorical than when
    it is continuous. We'll go over the former in this section and the latter in the
    next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量为分类变量时，我们使用的策略与目标变量为连续变量时有所不同。在本节中，我们将介绍前者，在下一节中介绍后者。
- en: Mutual information classification for feature selection with a categorical target
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于分类目标的互信息特征选择
- en: We can use **mutual information** classification or **analysis of variance**
    (**ANOVA**) tests to select features when we have a categorical target. We will
    try mutual information classification first, and then ANOVA for comparison.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量为分类变量时，我们可以使用**互信息**分类或**方差分析**（**ANOVA**）测试来选择特征。我们将首先尝试互信息分类，然后进行ANOVA比较。
- en: Mutual information is a measure of how much information about a variable is
    provided by knowing the value of another variable. At the extreme, when features
    are completely independent, the mutual information score is 0.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息是衡量通过知道另一个变量的值可以获得多少关于变量的信息的度量。在极端情况下，当特征完全独立时，互信息分数为0。
- en: We can use `scikit-learn`'s `SelectKBest` class to select the *k* features that
    have the highest predictive strength based on mutual information classification
    or some other appropriate measure. We can use hyperparameter tuning to select
    the value of *k*. We can also examine the scores of all features, whether they
    were identified as one of the *k* best or not, as we will see in this section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scikit-learn`的`SelectKBest`类根据互信息分类或其他适当的度量选择具有最高预测强度的*k*个特征。我们可以使用超参数调整来选择*k*的值。我们还可以检查所有特征的分数，无论它们是否被识别为*k*个最佳特征之一，正如我们将在本节中看到的。
- en: 'Let''s first try mutual information classification to identify features that
    are related to completing a bachelor''s degree. Later, we will compare that with
    using ANOVA F-values as the basis for selection:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先尝试互信息分类来识别与完成学士学位相关的特征。稍后，我们将将其与使用ANOVA F值作为选择依据进行比较：
- en: 'We start by importing `OneHotEncoder` from `feature_engine` to encode some
    of the data, and `train_test_split` from `scikit-learn` to create training and
    testing data. We will also need `scikit-learn`''s `SelectKBest`, `mutual_info_classif`,
    and `f_classif` modules for our feature selection:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从`feature_engine`导入`OneHotEncoder`来编码一些数据，并从`scikit-learn`导入`train_test_split`来创建训练和测试数据。我们还需要`scikit-learn`的`SelectKBest`、`mutual_info_classif`和`f_classif`模块来进行特征选择：
- en: '[PRE0]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load NLS data that has a binary variable for having completed a bachelor''s
    degree and features possibly related to degree attainment: `gender` feature, and
    scale the other data:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了具有完成学士学位的二进制变量和可能与学位获得相关的特征的数据集：`gender`特征，并对其他数据进行缩放：
- en: '[PRE1]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We will do a complete case analysis of the NLS data throughout this chapter;
    that is, we will remove all observations that have missing values for any of the
    features. This is not usually a good approach and is particularly problematic
    when data is not missing at random or when there is a large number of missing
    values for one or more features. In such cases, it would be better to use some
    of the approaches that we used in [*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034),
    *Identifying and Fixing Missing Values*. We will do a complete case analysis in
    this chapter to keep the examples as straightforward as possible.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，我们将对NLS数据进行完整案例分析；也就是说，我们将删除任何特征缺失的观测值。这通常不是一个好的方法，尤其是在数据不是随机缺失或一个或多个特征有大量缺失值时尤其有问题。在这种情况下，最好使用我们在[*第3章*](B17978_03_ePub.xhtml#_idTextAnchor034)中使用的某些方法，*识别和修复缺失值*。我们将在本章中进行完整案例分析，以使示例尽可能简单。
- en: 'Now we are ready to select features for our model of bachelor''s degree completion.
    One approach is to use mutual information classification. To do that, we set the
    `score_func` value of `SelectKBest` to `mutual_info_classif` and indicate that
    we want the five best features. Then, we call `fit` and use the `get_support`
    method to get the five best features:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好为我们的学士学位完成模型选择特征。一种方法是用互信息分类。为此，我们将`SelectKBest`的`score_func`值设置为`mutual_info_classif`，并指出我们想要五个最佳特征。然后，我们调用`fit`并使用`get_support`方法来获取五个最佳特征：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we also want to see the score for each feature, we can use the `scores_`
    attribute, though we need to do a little work to associate the scores with a particular
    feature name and sort the scores in descending order:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们还想看到每个特征的得分，我们可以使用`scores_`属性，尽管我们需要做一些工作来将得分与特定的特征名称关联起来，并按降序排序：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This is a stochastic process, so we will get different results each time we
    run it.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个随机过程，所以每次运行它时我们都会得到不同的结果。
- en: 'To get the same results each time, you can pass a partial function to `score_func`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了每次都能得到相同的结果，你可以将一个部分函数传递给`score_func`：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can create a DataFrame with just the important features using the `selcols`
    array we created using `get_support`. (We could have used the `transform` method
    of `SelectKBest` instead. This would have returned the values of the selected
    features as a NumPy array.)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用使用`get_support`创建的`selcols`数组来创建仅包含重要特征的DataFrame。（我们也可以使用`SelectKBest`的`transform`方法。这将返回所选特征的值作为NumPy数组。）
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That is all we need to do to select the *k* best features for our model using
    mutual information.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用互信息来选择模型中*最佳k个特征*所需做的所有事情。
- en: ANOVA F-value for feature selection with a categorical target
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分类目标的特征选择的ANOVA F值
- en: Alternatively, we can use ANOVA instead of mutual information. ANOVA evaluates
    how different the mean for a feature is for each target class. This is a good
    metric for univariate feature selection when we can assume a linear relationship
    between features and the target and our features are normally distributed. If
    those assumptions do not hold, mutual information classification is a better choice.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用方差分析（ANOVA）而不是互信息。方差分析评估每个目标类中特征的平均值差异。当我们假设特征和目标之间存在线性关系，并且我们的特征是正态分布时，这是一个很好的单变量特征选择指标。如果这些假设不成立，互信息分类是一个更好的选择。
- en: 'Let''s try using ANOVA for our feature selection. We can set the `score_func`
    parameter of `SelectKBest` to `f_classif` to select based on ANOVA:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用ANOVA进行特征选择。我们可以将`SelectKBest`的`score_func`参数设置为`f_classif`，以便基于ANOVA进行选择：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This selected the same features as were selected when we used mutual information.
    Showing the scores gives us some indication of whether the selected value for
    *k* makes sense. For example, there is a greater drop in score from the fifth-
    to the sixth-best feature (77-61) than from the fourth to the fifth (85-77). There
    is an even bigger decline from the sixth to the seventh, however (61-37), suggesting
    that we should at least consider a value for *k* of 6\.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这选择了与我们使用互信息时选择的相同特征。显示得分给我们一些关于所选的*k*值是否合理的指示。例如，第五到第六个最佳特征的得分下降（77-61）比第四到第五个（85-77）的下降更大。然而，从第六到第七个的下降更大（61-37），这表明我们至少应该考虑*k*的值为6。
- en: ANOVA tests, and the mutual information classification we did earlier, do not
    take into account features that are only important in multivariate analysis. For
    example, `fatherhighgrade` might matter among individuals with similar GPA or
    SAT scores. We use multivariate feature selection methods later in this chapter.
    We do more univariate feature selection in the next section where we explore selection
    techniques appropriate for continuous targets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ANOVA测试和之前我们做的互信息分类没有考虑在多元分析中仅重要的特征。例如，`fatherhighgrade`可能在具有相似GPA或SAT分数的个人中很重要。我们将在本章后面使用多元特征选择方法。在下一节中，我们将进行更多单变量特征选择，以探索适合连续目标的特征选择技术。
- en: Selecting features for regression models
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择回归模型的特征
- en: '`scikit-learn`''s selection module provides several options for selecting features
    when building regression models. (By regression models here, I do not mean linear
    regression models. I am only referring to *models with continuous targets*.) Two
    good options are selection based on F-tests and selection based on mutual information
    for regression. Let''s start with F-tests.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`的选择模块在构建回归模型时提供了几个选择特征的选择。在这里，我不指线性回归模型。我只是在指*具有连续目标的模型*）。两个好的选择是基于F检验的选择和基于回归的互信息选择。让我们从F检验开始。'
- en: F-tests for feature selection with a continuous target
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于连续目标的特征选择的F检验
- en: The F-statistic is a measure of the strength of the linear correlation between
    a target and a single regressor. `Scikit-learn` has an `f_regression` scoring
    function, which returns F-statistics. We can use it with `SelectKBest` to select
    features based on that statistic.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: F统计量是目标与单个回归器之间线性相关强度的度量。`Scikit-learn`有一个`f_regression`评分函数，它返回F统计量。我们可以使用它与`SelectKBest`一起选择基于该统计量的特征。
- en: 'Let''s use F-statistics to select features for a model of wages. We use mutual
    information for regression in the next section to select features for the same
    target:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用F统计量来选择工资模型的特征。我们将在下一节中使用互信息来选择相同目标的特征：
- en: 'We start by importing the one-hot encoder from `feature_engine` and `train_test_split`
    and `SelectKBest` from `scikit-learn`. We also import `f_regression` to get F-statistics
    later:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从`feature_engine`导入one-hot编码器，从`scikit-learn`导入`train_test_split`和`SelectKBest`。我们还导入`f_regression`以获取后续的F统计量：
- en: '[PRE26]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we load the NLS data, including educational attainment, parental income,
    and wage income data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载NLS数据，包括教育成就、家庭收入和工资收入数据：
- en: '[PRE27]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we create training and testing DataFrames, encode the `gender` feature,
    and scale the training data. We need to scale the target in this case since it
    is continuous:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建训练和测试数据框，对`gender`特征进行编码，并对训练数据进行缩放。在这种情况下，我们需要对目标进行缩放，因为它是有连续性的：
- en: '[PRE28]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You may have noticed that we are not encoding or scaling the testing data. We
    will need to do that eventually to validate our models. We will introduce validation
    later in this chapter and go over it in much more detail in the next chapter.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们没有对测试数据进行编码或缩放。我们最终需要这样做以验证我们的模型。我们将在本章后面介绍验证，并在下一章中详细介绍。
- en: 'Now, we are ready to select features. We set `score_func` of `SelectKBest`
    to `f_regression` and indicate that we want the five best features. The `get_support`
    method of `SelectKBest` returns `True` for each feature that was selected:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已准备好选择特征。我们将`SelectKBest`的`score_func`设置为`f_regression`，并指出我们想要五个最佳特征。`SelectKBest`的`get_support`方法对每个被选中的特征返回`True`：
- en: '[PRE29]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can use the `scores_` attribute to see the score for each feature:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`scores_`属性来查看每个特征的得分：
- en: '[PRE30]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The disadvantage of the F-statistic is that it assumes a linear relationship
    between each feature and the target. When that assumption does not make sense,
    we can use mutual information for regression instead.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: F统计量的缺点是它假设每个特征与目标之间存在线性关系。当这个假设不合理时，我们可以使用互信息进行回归。
- en: Mutual information for feature selection with a continuous target
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于具有连续目标的特征选择中的互信息
- en: 'We can also use `SelectKBest` to select features using mutual information for
    regression:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`SelectKBest`通过回归中的互信息来选择特征：
- en: We need to set the `score_func` parameter of `SelectKBest` to `mutual_info_regression`,
    but there is a small complication. To get the same results each time we run the
    feature selection, we need to set a `random_state` value. As we discussed in the
    previous section, we can use a partial function for that. We pass `partial(mutual_info_regression,
    random_state=0)` to the score function.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将`SelectKBest`的`score_func`参数设置为`mutual_info_regression`，但存在一个小问题。为了每次运行特征选择时都能得到相同的结果，我们需要设置一个`random_state`值。正如我们在前一小节中讨论的，我们可以使用一个部分函数来做到这一点。我们将`partial(mutual_info_regression,
    random_state=0)`传递给评分函数。
- en: 'We can then run the `fit` method and use `get_support` to get the selected
    features. We can use the `scores_` attribute to give us the score for each feature:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以运行`fit`方法，并使用`get_support`来获取选定的特征。我们可以使用`scores_`属性来为每个特征给出分数：
- en: '[PRE31]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We get fairly similar results with mutual information for regression as we did
    with F-tests. `parentincome` was selected with F-tests and `fatherhighgrade` with
    mutual information. Otherwise, the same features are selected.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在回归中的互信息得到了与F检验相当相似的结果。`parentincome`通过F检验被选中，而`fatherhighgrade`通过互信息被选中。否则，选中的特征是相同的。
- en: A key advantage of mutual information for regression compared with F-tests is
    that it does not assume a linear relationship between the feature and the target.
    If that assumption turns out to be unwarranted, mutual information is a better
    approach. (Again, there is also some randomness in the scoring and the score for
    each feature can bounce around within a limited range.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与F检验相比，互信息在回归中的关键优势是它不假设特征与目标之间存在线性关系。如果这个假设被证明是不合理的，互信息是一个更好的方法。（再次强调，评分过程中也存在一些随机性，每个特征的分数可能会在一定范围内波动。）
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Our choice of `k=5` to get the five best features is quite arbitrary. We can
    make it much more scientific with some hyperparameter tuning. We will go over
    tuning in the next chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择`k=5`以获取五个最佳特征是非常随意的。我们可以通过一些超参数调整使其更加科学。我们将在下一章中介绍调整。
- en: The feature selection methods we have used so far are known as *filter methods*.
    They examine the univariate relationship between each feature and the target.
    They are a good starting point. Similar to our discussion in previous chapters
    of the usefulness of having correlations handy before we start examining multivariate
    relationships, it is helpful to at least explore filter methods. Often, though,
    our model fitting will require taking into account features that are important,
    or not, when other features are also included. To do that, we need to use wrapper
    or embedded methods for feature selection. We explore wrapper methods in the next
    few sections, starting with forward and backward feature selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止使用的特征选择方法被称为*过滤器方法*。它们检查每个特征与目标之间的单变量关系。它们是一个好的起点。类似于我们在前几章中讨论的，在开始检查多元关系之前，拥有相关性的有用性，至少探索过滤器方法是有帮助的。然而，通常我们的模型拟合需要考虑当其他特征也被包含时，哪些特征是重要的，哪些不是。为了做到这一点，我们需要使用包装器或嵌入式方法进行特征选择。我们将在下一节中探讨包装器方法，从前向和后向特征选择开始。
- en: Using forward and backward feature selection
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用前向和后向特征选择
- en: Forward and backward feature selection, as their names suggest, select features
    by adding them one by one – or subtracting them for backward selection – and assessing
    the impact on model performance after each iteration. Since both methods assess
    that performance based on a given algorithm, they are considered **wrapper** selection
    methods.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前向和后向特征选择，正如其名称所暗示的，通过逐个添加（或对于后向选择，逐个减去）特征来选择特征，并在每次迭代后评估对模型性能的影响。由于这两种方法都是基于给定的算法来评估性能，因此它们被认为是**包装器**选择方法。
- en: Wrapper feature selection methods have two advantages over the filter methods
    we have explored so far. First, they evaluate the importance of features as other
    features are included. Second, since features are evaluated based on their contribution
    to the performance of a specific algorithm, we get a better sense of which features
    will ultimately matter. For example, `satmath` seemed to be an important feature
    based on our results from the previous section. But it is possible that `satmath`
    is only important when we use a particular model, say linear regression, and not
    an alternative such as decision tree regression. Wrapper selection methods can
    help us discover that.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 包装特征选择方法相对于我们之前探索的过滤方法有两个优点。首先，它们在包含其他特征时评估特征的重要性。其次，由于特征是根据其对特定算法性能的贡献来评估的，因此我们能够更好地了解哪些特征最终会起作用。例如，根据我们上一节的结果，`satmath`似乎是一个重要的特征。但有可能`satmath`只有在使用特定模型时才重要，比如线性回归，而不是决策树回归等其他模型。包装选择方法可以帮助我们发现这一点。
- en: The main disadvantage of wrapper methods is that they can be quite expensive
    computationally since they retrain the model after each iteration. We will look
    at both forward and backward feature selection in this section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法的缺点主要在于它们在每次迭代后都会重新训练模型，因此在计算上可能相当昂贵。在本节中，我们将探讨前向和后向特征选择。
- en: Using forward feature selection
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用前向特征选择
- en: '**Forward feature selection** starts by identifying a subset of features that
    individually have a significant relationship with a target, not unlike the filter
    methods. But it then evaluates all possible combinations of the selected features
    for the combination that performs best with the chosen algorithm.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向特征选择**首先识别出与目标有显著关系的特征子集，这与过滤方法类似。但它随后评估所有可能的选择特征的组合，以确定与所选算法表现最佳的组合。'
- en: 'We can use forward feature selection to develop a model of bachelor''s degree
    completion. Since wrapper methods require us to choose an algorithm, and this
    is a binary target, let''s use `scikit-learn`''s `feature_selection` module of
    `mlxtend` to do the iteration required to select features:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前向特征选择来开发一个完成学士学位的模型。由于包装方法要求我们选择一个算法，而这是一个二元目标，因此让我们使用`scikit-learn`的`mlxtend`模块中的`feature_selection`来进行选择特征的迭代：
- en: 'We start by importing the necessary libraries:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库：
- en: '[PRE32]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we load the NLS data again. We also create a training DataFrame, encode
    the `gender` feature, and standardize the remaining features:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们再次加载NLS数据。我们还创建了一个训练DataFrame，对`gender`特征进行编码，并对剩余特征进行标准化：
- en: '[PRE33]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We create a random forest classifier object and then pass that object to the
    feature selector of `mlxtend`. We indicate that we want it to select five features
    and that it should forward select. (We can also use the sequential feature selector
    to select backward.) After running `fit`, we can use the `k_feature_idx_` attribute
    to get the list of selected features:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个随机森林分类器对象，然后将该对象传递给`mlxtend`的特征选择器。我们指出我们想要选择五个特征，并且应该进行前向选择。（我们也可以使用顺序特征选择器进行后向选择。）运行`fit`后，我们可以使用`k_feature_idx_`属性来获取所选特征的列表：
- en: '[PRE34]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You might recall from the first section of this chapter that our univariate
    feature selection for the completed bachelor''s degree target gave us somewhat
    different results:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得本章的第一节，我们针对完成学士学位目标的多变量特征选择给出了不同的结果：
- en: '[PRE35]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Three of the features – `satmath`, `satverbal`, and `gpaoverall` – are the same.
    But our forward feature selection has identified `parentincome` and `gender_Female`
    as more important than `gpascience` and `gpaenglish`, which were selected in the
    univariate analysis. Indeed, `gender_Female` had among the lowest scores in the
    earlier analysis. These differences likely reflect the advantages of wrapper feature
    selection methods. We can identify features that are not important unless other
    features are included, and we are evaluating the effect on the performance of
    a particular algorithm, in this case, random forest classification.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个特征——`satmath`、`satverbal`和`gpaoverall`——是相同的。但我们的前向特征选择已经将`parentincome`和`gender_Female`识别为比在单变量分析中选择的`gpascience`和`gpaenglish`更重要的特征。实际上，`gender_Female`在早期分析中的得分最低。这些差异可能反映了包装特征选择方法的优点。我们可以识别出除非包含其他特征，否则不重要的特征，并且我们正在评估对特定算法（在这种情况下是随机森林分类）性能的影响。
- en: One disadvantage of forward selection is that *once a feature is selected, it
    is not removed, even though it may decline in importance as additional features
    are added*. (Recall that forward feature selection adds features iteratively based
    on the contribution of that feature to the model.)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前向选择的缺点之一是，一旦选择了特征，它就不会被移除，即使随着更多特征的添加，它的重要性可能会下降。（回想一下，前向特征选择是基于该特征对模型的贡献迭代添加特征的。）
- en: Let's see whether our results vary with backward feature selection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的结果是否随着反向特征选择而变化。
- en: Using backward feature selection
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反向特征选择
- en: Backward feature selection starts with all features and eliminates the least
    important. It then repeats this process with the remaining features. We can use
    `mlxtend`'s `SequentialFeatureSelector` for backward selection in pretty much
    the same way we used it for forward selection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 反向特征选择从所有特征开始，并消除最不重要的特征。然后，它使用剩余的特征重复此过程。我们可以使用 `mlxtend` 的 `SequentialFeatureSelector`
    以与正向选择相同的方式用于反向选择。
- en: 'We instantiate a `RandomForestClassifier` object from the `scikit-learn` library
    and then pass it to `mlxtend`''s sequential feature selector:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `scikit-learn` 库实例化了一个 `RandomForestClassifier` 对象，然后将其传递给 `mlxtend` 的顺序特征选择器：
- en: '[PRE36]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Perhaps unsurprisingly, we get different results for our feature selection.
    `satmath` and `parentincome` are no longer selected, and `gpascience` and `gpaenglish`
    are.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 也许并不令人惊讶，我们在特征选择上得到了不同的结果。`satmath` 和 `parentincome` 不再被选中，而 `gpascience` 和
    `gpaenglish` 被选中。
- en: Backward feature selection has the opposite drawback to forward feature selection.
    *Once a feature has been removed, it is not re-evaluated, even though its importance
    may change with different feature mixtures*. Let's try exhaustive feature selection
    instead.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 反向特征选择与前向特征选择的缺点相反。*一旦移除了特征，它就不会被重新评估，即使其重要性可能会随着不同的特征组合而改变*。让我们尝试使用穷举特征选择。
- en: Using exhaustive feature selection
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用穷举特征选择
- en: If your results from forward and backward selection are unpersuasive, and you
    do not mind running a model while you go out for coffee or lunch, you can try
    exhaustive feature selection. **Exhaustive feature selection** trains a given
    model on all possible combinations of features and selects the best subset of
    features. But it does this at a price. As the name suggests, this procedure might
    exhaust both system resources and your patience.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的正向和反向选择的结果没有说服力，而且你不在意在喝咖啡或吃午餐的时候运行模型，你可以尝试穷举特征选择。**穷举特征选择**会在所有可能的特征组合上训练给定的模型，并选择最佳的特征子集。但这也需要付出代价。正如其名所示，这个过程可能会耗尽系统资源和你的耐心。
- en: 'Let''s use exhaustive feature selection for our model of bachelor''s degree
    completion:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为学士学位完成情况的模型使用穷举特征选择：
- en: 'We start by loading the required libraries, including the `RandomForestClassifier`
    and `LogisticRegression` modules from `scikit-learn` and `ExhaustiveFeatureSelector`
    from `mlxtend`. We also import the `accuracy_score` module so that we can evaluate
    a model with the selected features:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载所需的库，包括来自 `scikit-learn` 的 `RandomForestClassifier` 和 `LogisticRegression`
    模块，以及来自 `mlxtend` 的 `ExhaustiveFeatureSelector`。我们还导入了 `accuracy_score` 模块，这样我们就可以使用选定的特征来评估模型：
- en: '[PRE45]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we load the NLS educational attainment data and create training and testing
    DataFrames:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载 NLS 教育达成度数据，并创建训练和测试 DataFrame：
- en: '[PRE46]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, we encode and scale the training and testing data:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对训练和测试数据进行编码和缩放：
- en: '[PRE47]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We create a random forest classifier object and pass it to `mlxtend`''s `ExhaustiveFeatureSelector`.
    We tell the feature selector to evaluate all combinations of one to five features
    and return the combination with the highest accuracy in predicting degree attainment.
    After running `fit`, we can use the `best_feature_names_` attribute to get the
    selected features:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个随机森林分类器对象，并将其传递给 `mlxtend` 的 `ExhaustiveFeatureSelector`。我们告诉特征选择器评估所有一至五个特征的组合，并返回预测学位达成度最高的组合。运行
    `fit` 后，我们可以使用 `best_feature_names_` 属性来获取选定的特征：
- en: '[PRE48]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s evaluate the accuracy of this model. We first need to transform the
    training and testing data to include only the four selected features. Then, we
    can fit the random forest classifier again with just those features and generate
    the predicted values for bachelor''s degree completion. We can then calculate
    the percentage of the time we predicted the target correctly, which is 67%:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们评估这个模型的准确性。我们首先需要将训练和测试数据转换为只包含四个选定的特征。然后，我们可以仅使用这些特征再次拟合随机森林分类器，并生成学士学位完成情况的预测值。然后，我们可以计算我们正确预测目标的时间百分比，这是
    67%：
- en: '[PRE49]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We get the same answer if we just use scikit-learn's `accuracy score` instead.
    (We calculate it in the previous step because it is pretty straightforward and
    it gives us a better sense of what is meant by accuracy in this case.)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们只使用scikit-learn的`accuracy score`，我们也会得到相同的答案。（我们在上一步计算它，因为它相当直接，并且让我们更好地理解在这种情况下准确率的含义。）
- en: '[PRE50]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy score is often used to assess the performance of a classification
    model. We will lean on it in this chapter, but other measures might be equally
    or more important depending on the purposes of your model. For example, we are
    sometimes more concerned with sensitivity, the ratio of our correct positive predictions
    to the number of actual positives. We examine the evaluation of classification
    models in detail in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for Model Evaluation.*
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率分数通常用于评估分类模型的性能。在本章中，我们将依赖它，但根据您模型的目的，其他指标可能同样重要或更重要。例如，我们有时更关心灵敏度，即我们的正确阳性预测与实际阳性数量的比率。我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)中详细探讨了分类模型的评估，*准备模型评估*。
- en: 'Let''s now try exhaustive feature selection with a logistic model:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们尝试使用逻辑模型进行全面特征选择：
- en: '[PRE51]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s look at the accuracy of the logistic model. We get a fairly similar
    accuracy score:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看逻辑模型的准确率。我们得到了相当相似的准确率分数：
- en: '[PRE52]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: One key advantage of the logistic model is that it is much faster to train,
    which really makes a difference with exhaustive feature selection. If we time
    the training for each model (probably not a good idea to do that on your computer
    unless it's a pretty high-end machine or you don't mind walking away from your
    computer for a while), we see a substantial difference in average training time
    – from an amazing 5 minutes for the random forest to 4 seconds for the logistic
    regression. (Of course, the absolute numbers are machine-dependent.)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑模型的一个关键优势是它训练得更快，这对于全面特征选择来说确实有很大影响。如果我们为每个模型计时（除非你的电脑相当高端或者你不在乎离开电脑一会儿，否则这通常不是一个好主意），我们会看到平均训练时间有显著差异——从随机森林的惊人的5分钟到逻辑回归的4秒。（当然，这些绝对数字取决于机器。）
- en: '[PRE53]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Exhaustive feature selection can provide very clear guidance about the features
    to select, as I have mentioned, but that may come at too high a price for many
    projects. It may actually be better suited for *diagnostic work* than for use
    in a machine learning pipeline. If a linear model is appropriate, it can lower
    the computational costs considerably.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所述，全面特征选择可以提供关于要选择哪些特征的非常清晰的指导，但这可能对许多项目来说代价太高。实际上，它可能更适合于*诊断工作*而不是用于机器学习管道。如果一个线性模型是合适的，它可以显著降低计算成本。
- en: Wrapper methods, such as forward, backward, and exhaustive feature selection,
    tax system resources because they need to be trained with each iteration, and
    the more difficult the chosen algorithm is to implement, the more this is an issue.
    **Recursive feature elimination** (**RFE**) is something of a compromise between
    the simplicity of filter methods and the better information provided by wrapper
    methods. It is similar to backward feature selection, except it simplifies the
    removal of a feature at each iteration by basing it on the model's overall performance
    rather than re-evaluating each feature. We explore recursive feature selection
    in the next two sections.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 前向、后向和全面特征选择等包装方法会消耗系统资源，因为它们需要每次迭代时都进行训练，而选择的算法越难实现，这个问题就越严重。**递归特征消除（RFE**）在过滤方法的简单性和包装方法提供的信息之间是一种折衷。它与后向特征选择类似，但它在每次迭代中通过基于模型的整体性能而不是重新评估每个特征来简化特征的移除。我们将在下一节中探讨递归特征选择。
- en: Eliminating features recursively in a regression model
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在回归模型中递归消除特征
- en: A popular wrapper method is RFE. This method starts with all features, removes
    the lowest weighted one (based on a coefficient or feature importance measure),
    and repeats the process until the best-fitting model has been identified. When
    a feature is removed, it is given a ranking reflecting the point at which it was
    removed.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的包装方法是RFE。这种方法从所有特征开始，移除权重最低的一个（基于系数或特征重要性度量），然后重复此过程，直到确定最佳拟合模型。当移除一个特征时，它会得到一个反映其移除点的排名。
- en: 'RFE can be used for both regression models and classification models. We will
    start by using it in a regression model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: RFE可以用于回归模型和分类模型。我们将从在回归模型中使用它开始：
- en: 'We import the necessary libraries, three of which we have not used yet: the
    `RFE`, `RandomForestRegressor`, and `LinearRegression` modules from `scikit-learn`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的库，其中三个我们尚未使用：来自 `scikit-learn` 的 `RFE`、`RandomForestRegressor` 和 `LinearRegression`
    模块：
- en: '[PRE54]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we load the NLS data on wages and create training and testing DataFrames:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载工资的 NLS 数据并创建训练和测试 DataFrame：
- en: '[PRE55]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We need to encode the `gender` feature and standardize the other features and
    the target (`wageincome`). We do not do any encoding or scaling of `completedba`,
    which is a binary feature:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要编码 `gender` 特征并标准化其他特征以及目标（`wageincome`）。我们不编码或缩放二进制特征 `completedba`：
- en: '[PRE56]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now, we are ready to do some recursive feature selection. Since RFE is a wrapper
    method, we need to choose an algorithm around which the selection will be *wrapped*.
    Random forests for regression make sense in this case. We are modeling a continuous
    target and do not want to assume a linear relationship between the features and
    the target.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备进行一些递归特征选择。由于 RFE 是一种包装方法，我们需要选择一个算法，该算法将围绕选择进行包装。在这种情况下，回归的随机森林是有意义的。我们正在模拟一个连续的目标，并且不希望假设特征和目标之间存在线性关系。
- en: 'RFE is fairly easy to implement with `scikit-learn`. We instantiate an RFE
    object, telling it what estimator we want in the process. We indicate `RandomForestRegressor`.
    We then fit the model and use `get_support` to get the selected features. We limit
    `max_depth` to `2` to avoid overfitting:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `scikit-learn` 实现 RFE 比较简单。我们实例化一个 RFE 对象，在过程中指定我们想要的估计器。我们指示 `RandomForestRegressor`。然后我们拟合模型并使用
    `get_support` 获取选定的特征。我们将 `max_depth` 限制为 `2` 以避免过拟合：
- en: '[PRE57]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that this gives us a somewhat different list of features than using a filter
    method (with F-tests) for the wage income target. `gpaoverall` and `motherhighgrade`
    are selected here and not the `gender` flag or `gpascience`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与使用带有 F 检验的滤波方法（针对工资收入目标）得到的特征列表略有不同。在这里选择了 `gpaoverall` 和 `motherhighgrade`，而不是
    `gender` 标志或 `gpascience`。
- en: 'We can use the `ranking_` attribute to see when each of the eliminated features
    was removed:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `ranking_` 属性来查看每个被消除的特征何时被移除：
- en: '[PRE58]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`fatherhighgrade` was removed after the first interaction and `gpamath` after
    the second.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次交互后移除了 `fatherhighgrade`，在第二次交互后移除了 `gpamath`。
- en: Let's run some test statistics. We fit only the selected features on a random
    forest regressor model. The `transform` method of the RFE selector gives us just
    the selected features with `treesel.transform(X_train_enc)`. We can use the `score`
    method to get the r-squared value, also known as the coefficient of determination.
    R-squared is a measure of the percentage of total variation explained by our model.
    We get a very low score, indicating that our model explains only a little of the
    variation. (Note that this is a stochastic process, so we will likely get different
    results each time we fit the model.)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们运行一些测试统计量。我们仅在随机森林回归器模型上拟合选定的特征。RFE 选择器的 `transform` 方法给我们的是 `treesel.transform(X_train_enc)`
    中选定的特征。我们可以使用 `score` 方法来获取 r 平方值，也称为确定系数。r 平方是我们模型解释的总变异百分比的度量。我们得到了一个非常低的分数，表明我们的模型只解释了很少的变异。（请注意，这是一个随机过程，所以我们每次拟合模型时可能会得到不同的结果。）
- en: '[PRE59]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s see whether we get any better results using RFE with a linear regression
    model. This model returns the same features as the random forest regressor:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看使用带有线性回归模型的 RFE 是否能得到更好的结果。此模型返回与随机森林回归器相同的特征：
- en: '[PRE60]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s evaluate the linear model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们评估线性模型：
- en: '[PRE61]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The linear model is not really much better than the random forest model. This
    is likely a sign that, collectively, the features available to us only capture
    a small part of the variation in wages per week. This is an important reminder
    that we can identify several significant features and still have a model with
    limited explanatory power. (Perhaps it is also good news that our scores on standardized
    tests, and even our degree attainment, are important but not determinative of
    our wages many years later.)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型实际上并不比随机森林模型好多少。这可能是这样一个迹象，即我们可用的特征总体上只捕捉到每周工资变异的一小部分。这是一个重要的提醒，即我们可以识别出几个显著的特征，但仍然有一个解释力有限的模型。（也许这也是一个好消息，即我们的标准化测试分数，甚至我们的学位获得，虽然重要但不是多年后我们工资的决定性因素。）
- en: Let's try RFE with a classification model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用分类模型进行 RFE。
- en: Eliminating features recursively in a classification model
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分类模型中递归消除特征
- en: 'RFE can also be a good choice for classification problems. We can use RFE to
    select features for a model of bachelor''s degree completion. You may recall that
    we used exhaustive feature selection to select features for that model earlier
    in this chapter. Let''s see whether we get better accuracy or an easier-to-train
    model with RFE:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: RFE也可以是分类问题的一个很好的选择。我们可以使用RFE来选择完成学士学位模型的特征。你可能还记得，我们在本章前面使用穷举特征选择来选择该模型的特征。让我们看看使用RFE是否能获得更高的准确率或更容易训练的模型：
- en: 'We import the same libraries we have been working with so far in this chapter:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入本章迄今为止一直在使用的相同库：
- en: '[PRE62]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we create training and testing data from the NLS educational attainment
    data:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从NLS教育成就数据中创建训练和测试数据：
- en: '[PRE63]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we encode and scale the training and testing data:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们编码和缩放训练和测试数据：
- en: '[PRE64]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We instantiate a random forest classifier and pass it to the RFE selection method.
    We can then fit the model and get the selected features.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化一个随机森林分类器并将其传递给RFE选择方法。然后我们可以拟合模型并获取所选特征。
- en: '[PRE65]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We can also show how the features are ranked by using the RFE `ranking_` attribute:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用RFE的`ranking_`属性来展示特征的排名：
- en: '[PRE66]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let''s look at the accuracy of a model with the selected features using the
    same random forest classifier we used for our baseline model:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看使用与我们的基线模型相同的随机森林分类器，使用所选特征的模型的准确率：
- en: '[PRE67]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Recall that we had 67% accuracy with the exhaustive feature selection. We get
    about the same accuracy here. The benefit of RFE though is that it can be significantly
    easier to train than exhaustive feature selection.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们使用穷举特征选择获得了67%的准确率。这里我们得到的准确率大致相同。然而，RFE的好处是它比穷举特征选择更容易训练。
- en: Another option among wrapper and wrapper-like feature selection methods is the
    `scikit-learn` ensemble method. We use it with `scikit-learn`'s random forest
    classifier in the next section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 包装和类似包装特征选择方法中的另一种选择是`scikit-learn`集成方法。我们将在下一节中使用`scikit-learn`的随机森林分类器来使用它。
- en: Using Boruta for feature selection
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Boruta进行特征选择
- en: The Boruta package takes a unique approach to feature selection, though it has
    some similarities with wrapper methods. For each feature, Boruta creates a shadow
    feature, one with the same range of values as the original feature but with shuffled
    values. It then evaluates whether the original feature offers more information
    than the shadow feature, gradually removing features providing the least information.
    Boruta outputs confirmed, tentative, and rejected features with each iteration.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Boruta包在特征选择方面采用独特的方法，尽管它与包装方法有一些相似之处。对于每个特征，Boruta创建一个影子特征，它与原始特征具有相同的值范围，但具有打乱后的值。然后它评估原始特征是否比影子特征提供更多信息，逐渐移除提供最少信息的特征。Boruta在每个迭代中输出已确认、尝试和拒绝的特征。
- en: 'Let''s use the Boruta package to select features for a classification model
    of bachelor''s degree completion (you can install the Boruta package with `pip`
    if you have not yet installed it):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Boruta包来选择完成学士学位分类模型的特征（如果你还没有安装Boruta包，可以使用`pip`安装）：
- en: 'We start by loading the necessary libraries:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载必要的库：
- en: '[PRE68]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We load the NLS educational attainment data again and create the training and
    test DataFrames:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次加载NLS教育成就数据并创建训练和测试DataFrame：
- en: '[PRE69]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we encode and scale the training and test data:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对训练和测试数据进行编码和缩放：
- en: '[PRE70]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We run Boruta feature selection in much the same way that we ran RFE feature
    selection. We use random forest as our baseline method again. We instantiate a
    random forest classifier and pass it to Boruta''s feature selector. We then fit
    the model, which stops at `100` iterations, identifying `9` features that provide
    information:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们以与运行RFE特征选择相同的方式运行Boruta特征选择。我们再次使用随机森林作为基线方法。我们实例化一个随机森林分类器并将其传递给Boruta的特征选择器。然后我们拟合模型，该模型在`100`次迭代后停止，识别出提供信息的`9`个特征：
- en: '[PRE71]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can use the `ranking_` property to view the rankings of the features:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`ranking_`属性来查看特征的排名：
- en: '[PRE72]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'To evaluate the model''s accuracy, we fit the random forest classifier model
    with just the selected features. We can then make predictions for the testing
    data and compute accuracy:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估模型的准确率，我们仅使用所选特征来拟合随机森林分类器模型。然后我们可以对测试数据进行预测并计算准确率：
- en: '[PRE73]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Part of Boruta's appeal is the persuasiveness of its selection of each feature.
    If a feature has been selected, then it likely does provide information that is
    not captured by combinations of features that exclude it. However, it is quite
    computationally expensive, not unlike exhaustive feature selection. It can help
    us sort out which features matter, but it may not always be suitable for pipelines
    where training speed matters.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Boruta的吸引力之一在于其对每个特征选择的说服力。如果一个特征被选中，那么它很可能提供了信息，这些信息不是通过排除它的特征组合所捕获的。然而，它在计算上相当昂贵，与穷举特征选择不相上下。它可以帮助我们区分哪些特征是重要的，但可能并不总是适合那些训练速度很重要的流水线。
- en: The last few sections have shown some of the advantages and some disadvantages
    of wrapper feature selection methods. We explore embedded selection methods in
    the next section. These methods provide more information than filter methods but
    without the computational costs of wrapper methods. They do this by embedding
    feature selection into the training process. We will explore embedded methods
    with the same data we have worked with so far.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几节展示了包装特征选择方法的某些优点和缺点。在下一节中，我们将探讨嵌入式选择方法。这些方法比过滤器方法提供更多信息，但又不具备包装方法的计算成本。它们通过将特征选择嵌入到训练过程中来实现这一点。我们将使用我们迄今为止所使用的数据来探讨嵌入式方法。
- en: Using regularization and other embedded methods
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则化和其他嵌入式方法
- en: '**Regularization** methods are embedded methods. Like wrapper methods, embedded
    methods evaluate features relative to a given algorithm. But they are not as expensive
    computationally. That is because feature selection is built into the algorithm
    already and so happens as the model is being trained.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**方法是嵌入式方法。与包装方法一样，嵌入式方法根据给定的算法评估特征。但它们的计算成本并不高。这是因为特征选择已经嵌入到算法中，所以随着模型的训练而发生。'
- en: 'Embedded models use the following process:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式模型使用以下过程：
- en: Train a model.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型。
- en: Estimate each feature's importance to the model's predictions.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计每个特征对模型预测的重要性。
- en: Remove features with low importance.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除重要性低的特征。
- en: Regularization accomplishes this by adding a penalty to any model to constrain
    the parameters. **L1 regularization**, also referred to as **lasso regularization**,
    shrinks some of the coefficients in a regression model to 0, effectively eliminating
    those features.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化通过向任何模型添加惩罚来约束参数来实现这一点。**L1正则化**，也称为**lasso正则化**，将回归模型中的某些系数缩小到0，从而有效地消除了这些特征。
- en: Using L1 regularization
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用L1正则化
- en: 'We will use L1 regularization with logistic regression to select features for
    a bachelor''s degree attainment model:We need to first import the required libraries,
    including a module we will be using for the first time, `SelectFromModel` from
    `scikit-learn`:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用L1正则化和逻辑回归来选择学士学位达成模型的特征：我们需要首先导入所需的库，包括我们将首次使用的模块，`scikit-learn`中的`SelectFromModel`：
- en: '[PRE74]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Next, we load NLS data on educational attainment:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载关于教育成就的NLS数据：
- en: '[PRE75]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Then, we encode and scale the training and testing data:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对训练数据和测试数据进行编码和缩放：
- en: '[PRE76]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now we are ready to do feature selection based on logistic regression with
    an L1 penalty:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备根据逻辑回归和L1惩罚进行特征选择：
- en: '[PRE77]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s evaluate the accuracy of the model. We get an accuracy score of `0.68`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来评估模型的准确性。我们得到了一个准确率分数为`0.68`：
- en: '[PRE78]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: This gives us fairly similar results to that of the forward feature selection
    for bachelor's degree completion. We used a random forest classifier as a wrapper
    method in that example.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了与学士学位完成的前向特征选择相当相似的结果。在那个例子中，我们使用随机森林分类器作为包装方法。
- en: Lasso regularization is a good choice for feature selection in a case like this,
    particularly when performance is a key concern. It does, however, assume a linear
    relationship between the features and the target, which might not be appropriate.
    Fortunately, there are embedded feature selection methods that do not make that
    assumption. A good alternative to logistic regression for the embedded model is
    a random forest classifier. We try that next with the same data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Lasso正则化是特征选择的一个好选择，尤其是当性能是一个关键关注点时。然而，它确实假设特征与目标之间存在线性关系，这可能并不合适。幸运的是，有一些嵌入式特征选择方法不做出这种假设。对于嵌入式模型来说，逻辑回归的一个好替代品是随机森林分类器。我们将使用相同的数据尝试这种方法。
- en: Using a random forest classifier
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林分类器
- en: 'In this section, let''s use a random forest classifier:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用随机森林分类器：
- en: 'We can use `SelectFromModel` to use a random forest classifier rather than
    logistic regression:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`SelectFromModel`来使用随机森林分类器而不是逻辑回归：
- en: '[PRE79]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This actually selects very different features from the lasso regression. `satmath`,
    `fatherhighgrade`, and `gender_Female` are no longer selected, while `satverbal`
    and `gpaenglish` are. This is likely partly due to the relaxation of the assumption
    of linearity.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上选择与lasso回归非常不同的特征。`satmath`、`fatherhighgrade`和`gender_Female`不再被选中，而`satverbal`和`gpaenglish`被选中。这很可能部分是由于线性假设的放宽。
- en: 'Let''s evaluate the accuracy of the random forest classifier model. We get
    an accuracy score of **0.67**. This is pretty much the same score that we got
    with the lasso regression:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们评估随机森林分类器模型的准确性。我们得到了**0.67**的准确率。这几乎与我们在lasso回归中得到的分数相同：
- en: '[PRE80]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Embedded methods are generally less CPU-/GPU-intensive than wrapper methods
    but can nonetheless produce good results. With our models of bachelor's degree
    completion in this section, we get the same accuracy as we did with our models
    based on exhaustive feature selection.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入式方法通常比包装方法CPU-/GPU密集度低，但仍然可以产生良好的结果。在本节的学士学位完成模型中，我们得到了与基于穷举特征选择模型相同的准确率。
- en: Each of the methods we have discussed so far has important use cases, as we
    have discussed. However, we have not yet really discussed one very challenging
    feature selection problem. What do you do if you simply have too many features,
    many of which independently account for something important in your model? By
    too many, here I mean that there are so many features that the model cannot run
    efficiently, either for training or for predicting target values. How can we reduce
    the feature set without sacrificing some of the predictive power of our model?
    In that situation, **principal component analysis** (**PCA**) might be a good
    approach. We'll discuss PCA in the next section.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的每种方法都有重要的应用场景，正如我们所讨论的。然而，我们还没有真正讨论一个非常具有挑战性的特征选择问题。如果你简单地有太多的特征，其中许多特征在你的模型中独立地解释了某些重要内容，你会怎么做？在这里，“太多”意味着有如此多的特征，以至于模型无法高效地运行，无论是训练还是预测目标值。我们如何在不牺牲模型部分预测能力的情况下减少特征集？在这种情况下，**主成分分析（PCA**）可能是一个好的方法。我们将在下一节中讨论PCA。
- en: Using principal component analysis
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主成分分析
- en: A very different approach to feature selection than any of the methods we have
    discussed so far is PCA. PCA allows us to replace the existing feature set with
    a limited number of components, each of which explains an important amount of
    the variance. It does this by finding a component that captures the largest amount
    of variance, followed by a second component that captures the largest amount of
    remaining variance, and then a third component, and so on. One key advantage of
    this approach is that these components, known as **principal components**, are
    uncorrelated. We discuss PCA in detail in [*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170),
    *Principal Component Analysis*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种与之前讨论的任何方法都截然不同的特征选择方法。PCA允许我们用有限数量的组件替换现有的特征集，每个组件都解释了重要数量的方差。它是通过找到一个捕获最大方差量的组件，然后是一个捕获剩余最大方差量的第二个组件，然后是一个第三个组件，以此类推来做到这一点的。这种方法的一个关键优势是，这些被称为**主成分**的组件是不相关的。我们在[*第15章*](B17978_15_ePub.xhtml#_idTextAnchor170)，*主成分分析*中详细讨论PCA。
- en: Although I include PCA here as a feature selection approach, it is probably
    better to think of it as a tool for dimension reduction. We use it for feature
    selection when we need to limit the number of dimensions without sacrificing too
    much explanatory power.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我在这里将主成分分析（PCA）视为一种特征选择方法，但可能更合适将其视为降维工具。当我们需要限制维度数量而又不希望牺牲太多解释力时，我们使用它来进行特征选择。
- en: 'Let''s work with the NLS data again and use PCA to select features for a model
    of bachelor''s degree completion:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用NLS数据，并使用PCA为学士学位完成模型选择特征：
- en: 'We start by loading the necessary libraries. The only module we have not already
    used in this chapter is `scikit-learn`''s `PCA`:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载必要的库。在本章中，我们还没有使用过的模块是`scikit-learn`的`PCA`：
- en: '[PRE81]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Next, we create training and testing DataFrames once again:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们再次创建训练和测试DataFrame：
- en: '[PRE82]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We need to scale and encode the data. Scaling is particularly important with
    PCA:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要对数据进行缩放和编码。在PCA中，缩放尤其重要：
- en: '[PRE83]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now, we instantiate a `PCA` object and fit a model:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们实例化一个`PCA`对象并拟合模型：
- en: '[PRE84]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The `components_` attribute of the `PCA` object returns the scores of all 10
    features on each of the 5 components. The features that drive the first component
    most are those with scores that have the highest absolute value. In this case,
    that is `gpaoverall`, `gpaenglish`, and `gpascience`. For the second component,
    the most important features are `motherhighgrade`, `fatherhighgrade`, and `parentincome`.
    `satverbal` and `satmath` drive the third component.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PCA`对象的`components_`属性返回了所有10个特征在每个5个成分上的得分。对第一个成分贡献最大的特征是得分绝对值最高的那些，在这种情况下，是`gpaoverall`、`gpaenglish`和`gpascience`。对于第二个成分，最重要的特征是`motherhighgrade`、`fatherhighgrade`和`parentincome`。`satverbal`和`satmath`驱动第三个成分。'
- en: 'In the following output, columns **0** through **4** are the five principal
    components:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下输出中，列**0**到**4**是五个主成分：
- en: '[PRE85]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Another way to understand these scores is that they indicate how much each feature
    contributes to the component. (Indeed, if for each component, you square each
    of the 10 scores and then sum the squares, you get a total of 1.)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解这些得分的方式是，它们表明每个特征对成分的贡献程度。（实际上，如果对每个成分，你将10个得分平方然后求和，你会得到一个总和为1。）
- en: 'Let''s also examine how much of the variance in the features is explained by
    each component. The first component accounts for 46% of the variance alone, followed
    by an additional 19% for the second component. We can use NumPy''s `cumsum` method
    to see how much of feature variance is explained by the five components cumulatively.
    We can explain 87% of the variance in the 10 features with 5 components:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也检查每个成分解释了特征中多少方差。第一个成分单独解释了46%的方差，第二个成分额外解释了19%。我们可以使用NumPy的`cumsum`方法来查看五个成分累积解释了多少特征方差。我们可以用5个成分解释10个特征中的87%的方差：
- en: '[PRE86]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Let''s transform our features in the testing data based on these five principal
    components. This returns a NumPy array with only the five principal components.
    We look at the first few rows. We also need to transform the testing DataFrame:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们根据这五个主成分来转换测试数据中的特征。这返回了一个只包含五个主成分的NumPy数组。我们查看前几行。我们还需要转换测试DataFrame：
- en: '[PRE87]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: We can now fit a model of bachelor's degree completion using these principal
    components. Let's run a random forest classification.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这些主成分来拟合一个关于学士学位完成情况的模型。让我们运行一个随机森林分类。
- en: 'We first create a random forest classifier object. We then pass the training
    data with the principal components and the target values to its `fit` method.
    We pass the testing data with the components to the classifier''s `predict` method
    and then get an accuracy score:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个随机森林分类器对象。然后，我们将带有主成分和目标值的训练数据传递给其`fit`方法。我们将带有成分的测试数据传递给分类器的`predict`方法，然后得到一个准确度分数：
- en: '[PRE88]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: A dimension reduction technique such as PCA can be a good option when the feature
    selection challenge is that we have highly correlated features and we want to
    reduce the number of dimensions without substantially reducing the explained variance.
    In this example, the high school GPA features moved together, as did the parental
    education and income levels and the SAT features. They became the key features
    for our first three components. (An argument can be made that our model could
    have had just those three components since together they accounted for 74% of
    the variance of the features.)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征选择挑战是我们有高度相关的特征，并且我们希望在不过度减少解释方差的情况下减少维度数量时，PCA等降维技术可以是一个好的选择。在这个例子中，高中GPA特征一起移动，父母的教育水平和收入水平以及SAT特征也是如此。它们成为了我们前三个成分的关键特征。（可以认为我们的模型只需要那三个成分，因为它们共同解释了特征变异的74%。）
- en: There are several modifications to PCA that might be useful depending on your
    data and modeling objectives. This includes strategies to handle outliers and
    regularization. PCA can also be extended to situations where the components are
    not linearly separable by using kernels. We discuss PCA in detail in [*Chapter
    15*](B17978_15_ePub.xhtml#_idTextAnchor170)*,* *Principal Component Analysis*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的数据和建模目标，PCA（主成分分析）有几种修改方式可能是有用的。这包括处理异常值和正则化的策略。通过使用核函数，PCA还可以扩展到那些成分不能线性分离的情况。我们将在[*第15章*](B17978_15_ePub.xhtml#_idTextAnchor170)*，*《主成分分析》*中详细讨论PCA。
- en: Let's summarize what we've learned in this chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went over a range of feature selection methods, from filter
    to wrapper to embedded methods. We also saw how they work with categorical and
    continuous targets. For wrapper and embedded methods, we considered how they work
    with different algorithms.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了从过滤方法到包装方法再到嵌入式方法的一系列特征选择方法。我们还看到了它们如何与分类和连续目标一起工作。对于包装和嵌入式方法，我们考虑了它们如何与不同的算法一起工作。
- en: Filter methods are very easy to run and interpret and are easy on system resources.
    However, they do not take other features into account when evaluating each feature.
    Nor do they tell us how that assessment might vary by the algorithm used. Wrapper
    methods do not have any of these limitations but they are computationally expensive.
    Embedded methods are often a good compromise, selecting features based on multivariate
    relationships and a given algorithm without taxing system resources as much as
    wrapper methods. We also explored how a dimension reduction method, PCA, could
    improve our feature selection.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤方法运行和解释都非常简单，且对系统资源的影响较小。然而，它们在评估每个特征时并没有考虑其他特征。而且，它们也没有告诉我们这种评估可能会因所使用的算法而有所不同。包装方法没有这些限制，但计算成本较高。嵌入式方法通常是一个很好的折衷方案，它们根据多元关系和给定的算法选择特征，而不像包装方法那样对系统资源造成过多负担。我们还探讨了如何通过降维方法PCA来改进我们的特征选择。
- en: You also probably noticed that I slipped in a little bit of model validation
    during this chapter. We will go over model validation in much more detail in the
    next chapter.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到了，我在本章中稍微提到了一点模型验证。我们将在下一章更详细地介绍模型验证。
