- en: 2\. Hierarchical Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 层次聚类
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will implement the hierarchical clustering algorithm from
    scratch using common Python packages and perform agglomerative clustering. We
    will also compare k-means with hierarchical clustering. We will use hierarchical
    clustering to build stronger groupings that make more logical sense. By the end
    of this chapter, we will be able to use hierarchical clustering to build stronger
    groupings that make more logical sense.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用常见的 Python 包从头实现层次聚类算法，并进行凝聚层次聚类。我们还将比较 k-means 和层次聚类。我们将利用层次聚类构建更强的、更有逻辑性的分组。在本章结束时，我们将能够使用层次聚类构建更强的、更有逻辑性的分组。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In this chapter, we will expand on the basic ideas that we built in Chap*ter
    1*, *Introduction to Clustering*, by surrounding clustering with the concept of
    similarity. Once again, we will be implementing forms of the Euclidean distance
    to capture the notion of similarity. It is important to bear in mind that the
    Euclidean distance just happens to be one of the most popular distance metrics;
    it's not the only one. Through these distance metrics, we will expand on the simple
    neighbor calculations that we explored in the previous chapter by introducing
    the concept of hierarchy. By using hierarchy to convey clustering information,
    we can build stronger groupings that make more logical sense. Similar to k-means,
    hierarchical clustering can be helpful for cases such as customer segmentation
    or identifying similar product types. However, there is a slight benefit in being
    able to explain things in a clearer fashion with hierarchical clustering. In this
    chapter, we will outline some cases where hierarchical clustering can be the solution
    you're looking for.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在第*1章*，*聚类简介*的基础概念上展开，通过相似度的概念来围绕聚类展开。我们将再次实现欧几里得距离的不同形式，以捕捉相似性的概念。需要牢记的是，欧几里得距离只是最常用的距离度量之一，并非唯一的度量。通过这些距离度量，我们将在前一章探索的简单邻居计算基础上引入层次结构的概念。通过使用层次结构来传达聚类信息，我们可以构建更强的、更有逻辑性的分组。与
    k-means 类似，层次聚类对于客户细分或识别相似产品类型等场景非常有帮助。然而，层次聚类的一个小优势是，它能够以更清晰的方式解释结果。在本章中，我们将概述一些层次聚类可能是你所需要的解决方案的情况。
- en: Clustering Refresher
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类复习
- en: '*Chapter 1*, *Introduction to Clustering*, covered both the high-level concepts
    and in-depth details of one of the most basic clustering algorithms: k-means.
    While it is indeed a simple approach, do not discredit it; it will be a valuable
    addition to your toolkit as you continue your exploration of the unsupervised
    learning world. In many real-world use cases, companies experience valuable discoveries
    through the simplest methods, such as k-means or linear regression (for supervised
    learning). An example of this is evaluating a large selection of customer data
    – if you were to evaluate it directly in a table, it would be unlikely that you''d
    find anything helpful. However, even a simple clustering algorithm can identify
    where groups within the data are similar and dissimilar. As a refresher, let''s
    quickly walk through what clusters are and how k-means works to find them:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1章*，*聚类简介*，讲解了最基本的聚类算法之一：k-means的高层概念和深入细节。尽管它确实是一个简单的方法，但不要小看它；它将成为你继续探索无监督学习世界时非常有价值的工具。在许多实际应用场景中，公司通过最简单的方法，如
    k-means 或线性回归（用于有监督学习），获得了宝贵的发现。一个例子是评估大量的客户数据——如果直接在表格中评估这些数据，通常难以发现有价值的信息。然而，即使是一个简单的聚类算法，也能够识别数据中哪些组是相似的，哪些是不同的。为了复习一下，我们来快速回顾一下什么是聚类，以及
    k-means 如何找到这些聚类：'
- en: '![Figure 2.1: The attributes that separate supervised and unsupervised problems'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：区分有监督与无监督问题的属性'
- en: '](img/B15923_02_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_01.jpg)'
- en: 'Figure 2.1: The attributes that separate supervised and unsupervised problems'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：区分有监督与无监督问题的属性
- en: If you were given a random collection of data without any guidance, you would
    probably start your exploration using basic statistics – for example, the mean,
    median, and mode values for each of the features. Given a dataset, choosing supervised
    or unsupervised learning as an approach to derive insights is dependent on the
    data goals that you have set for yourself. If you were to determine that one of
    the features was actually a label and you wanted to see how the remaining features
    in the dataset influence it, this would become a supervised learning problem.
    However, if, after initial exploration, you realized that the data you have is
    simply a collection of features without a target in mind (such as a collection
    of health metrics, purchase invoices from a web store, and so on), then you could
    analyze it through unsupervised methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到了一组随机的数据，没有任何指导方针，你可能会用基本的统计方法开始探索——例如，计算每个特征的均值、中位数和众数。给定一个数据集，选择监督学习或无监督学习作为推导洞察的方法，取决于你为自己设定的数据目标。如果你确定其中一个特征实际上是标签，并且你想要查看数据集中的其他特征如何影响它，那么这将成为一个监督学习问题。然而，如果在初步探索后，你意识到你拥有的数据仅仅是一组没有目标的特征（例如一组健康指标、网上商店的购买发票等），那么你可以通过无监督方法进行分析。
- en: A classic example of unsupervised learning is finding clusters of similar customers
    in a collection of invoices from a web store. Your hypothesis is that by finding
    out which people are the most similar, you can create more granular marketing
    campaigns that appeal to each cluster's interests. One way to achieve these clusters
    of similar users is through k-means.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的经典例子是通过分析来自网上商店的一组发票，找到类似客户的聚类。你的假设是，通过找到最相似的人群，你可以创建更具针对性的营销活动，吸引每个聚类的兴趣。实现这些相似用户聚类的一种方法是使用k-means。
- en: The k-means Refresher
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 复习
- en: The k-means clustering works by finding "k" number of clusters in your data
    through certain distance calculations such as Euclidean, Manhattan, Hamming, Minkowski,
    and so on. "K" points (also called centroids) are randomly initialized in your
    data and the distance is calculated from each data point to each of the centroids.
    The minimum of these distances designates which cluster a data point belongs to.
    Once every point has been assigned to a cluster, the mean intra-cluster data point
    is calculated as the new centroid. This process is repeated until the newly calculated
    cluster centroid no longer changes position or until the maximum limit of iterations
    is reached.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类通过找到数据中“k”个聚类来工作，采用某些距离计算方法，如欧几里得距离、曼哈顿距离、汉明距离、明科夫斯基距离等。“K”个点（也称为质心）在数据中随机初始化，然后计算每个数据点到每个质心的距离。这些距离中的最小值决定了某个数据点属于哪个聚类。每个点被分配到一个聚类后，计算该聚类内的数据点的均值作为新的质心。这个过程会重复进行，直到新计算出的聚类质心的位置不再变化，或者达到最大迭代次数。
- en: The Organization of the Hierarchy
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层级结构的组织
- en: Both the natural and human-made world contain many examples of organizing systems
    into hierarchies and why, for the most part, it makes a lot of sense. A common
    representation that is developed from these hierarchies can be seen in tree-based
    data structures. Imagine that you have a parent node with any number of child
    nodes that can subsequently be parent nodes themselves. By organizing information
    into a tree structure, you can build an information-dense diagram that clearly
    shows how things are related to their peers and their larger abstract concepts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是自然界还是人造世界，都有许多将系统组织成层级结构的例子，并且大多数情况下，这样做是非常有意义的。从这些层级结构中发展出来的一个常见表示法可以在基于树的结构中看到。想象你有一个父节点，下面有若干子节点，子节点可以进一步成为父节点。通过将信息组织成树形结构，你可以构建一个信息密集型的图表，清晰地显示事物之间与同类及其更大的抽象概念的关系。
- en: 'An example from the natural world to help illustrate this concept can be seen
    in how we view the hierarchy of animals, which goes from parent classes to individual species:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从自然界中有一个例子可以帮助说明这个概念，即我们如何看待动物的层级结构，从父类到各个物种：
- en: '![Figure 2.2: The relationships of animal species in a hierarchical tree structure'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2：动物物种在层级树结构中的关系'
- en: '](img/B15923_02_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_02.jpg)'
- en: 'Figure 2.2: The relationships of animal species in a hierarchical tree structure'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：动物物种在层级树结构中的关系
- en: In the preceding diagram, you can see an example of how relational information
    between varieties of animals can be easily mapped out in a way that both saves
    space and still transmits a large amount of information. This example can be seen
    as both a tree of its own (showing how cats and dogs are different, but both are
    domesticated animals) and as a potential piece of a larger tree that shows a breakdown
    of domesticated versus non-domesticated animals.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示意图中，你可以看到一个例子，展示了动物品种之间的关系信息是如何以既节省空间又能传达大量信息的方式进行映射的。这个示例既可以看作是一棵独立的树（展示了猫和狗的区别，但它们都是家养动物），也可以看作是一个更大树的一部分，展示了家养与非家养动物的分类。
- en: 'As a business-facing example, let''s go back to the concept of a web store
    selling products. If you sold a large variety of products, then you would probably
    want to create a hierarchical system of navigation for your customers. By preventing
    all of the information in your product catalog from being presented at once, customers
    will only be exposed to the path down the tree that matches their interests. An
    example of the hierarchical system of navigation can be seen in the following
    diagram:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个面向商业的示例，让我们回到一个销售产品的在线商店的概念。如果你售卖种类繁多的产品，那么你可能会想为客户创建一个层级导航系统。通过避免一次性展示所有的产品目录信息，客户只会看到与他们兴趣相匹配的树形路径。下面的示意图展示了一个层级导航系统的示例：
- en: '![Figure 2.3: Product categories in a hierarchical tree structure'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：层级树结构中的产品分类'
- en: '](img/B15923_02_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_03.jpg)'
- en: 'Figure 2.3: Product categories in a hierarchical tree structure'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：层级树结构中的产品分类
- en: Clearly, the benefits of a hierarchical system of navigation cannot be overstated
    in terms of improving your customer experience. By organizing information into
    a hierarchical structure, you can build an intuitive structure into your data
    that demonstrates explicit nested relationships. If this sounds like another approach
    to finding clusters in your data, then you're definitely on the right track. Through
    the use of similar distance metrics, such as the Euclidean distance from k-means,
    we can develop a tree that shows the many cuts of data that allow a user to subjectively
    create clusters at their discretion.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，层级导航系统在提升客户体验方面的好处不容小觑。通过将信息组织成层级结构，你可以为数据构建一个直观的结构，展示明确的嵌套关系。如果这听起来像是另一种在数据中寻找簇的方法，那么你肯定走在了正确的道路上。通过使用类似的距离度量，例如
    k-means 的欧氏距离，我们可以开发出一棵树，展示许多数据切割，让用户可以根据自己的需求主观地创建簇。
- en: Introduction to Hierarchical Clustering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类简介
- en: 'So far, we have shown you that hierarchies can be excellent structures to organize
    information that clearly shows nested relationships among data points. While this
    helps us gain an understanding of the parent/child relationships between items,
    it can also be very handy when forming clusters. Expanding on the animal example
    in the previous section, imagine that you were simply presented with two features
    of animals: their height (measured from the tip of the nose to the end of the
    tail) and their weight. Using this information, you then have to recreate a hierarchical
    structure in order to identify which records in your dataset correspond to dogs
    and cats, as well as their relative subspecies.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经展示了层级结构如何成为组织信息的优秀方式，能够清晰地显示数据点之间的嵌套关系。虽然这有助于我们理解项目之间的父子关系，但在形成簇时也非常实用。以前一节中的动物示例为基础，假设你仅仅得到了两种动物的特征：它们的身高（从鼻尖到尾部的长度）和体重。使用这些信息，你需要重新创建一个层级结构，以便识别数据集中哪些记录对应的是狗和猫，以及它们相对的亚种。
- en: 'Since you are only given animal heights and weights, you won''t be able to
    deduce the specific names of each species. However, by analyzing the features
    that you have been provided with, you can develop a structure within the data
    that serves as an approximation of what animal species exist in your data. This
    perfectly sets the stage for an unsupervised learning problem that is well solved
    with hierarchical clustering. In the following plot, you can see the two features
    that we created on the left, with animal height in the left-hand column and animal
    weight in the right-hand column. This is then charted on a two-axis plot with
    the height on the X-axis and the weight on the Y-axis:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你只给出了动物的身高和体重，无法推断每个物种的具体名称。然而，通过分析你所提供的特征，你可以在数据中构建一个结构，近似于数据中存在哪些动物物种。这为无监督学习问题奠定了基础，而层次聚类是解决此类问题的理想方法。在以下图表中，你可以看到我们在左侧创建的两个特征，左列为动物身高，右列为动物体重。然后，这些数据被绘制在一个二维坐标图上，X轴为身高，Y轴为体重：
- en: '![Figure 2.4: An example of a two-feature dataset comprising animal height'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4：一个包含动物身高的双特征数据集示例'
- en: and animal weight
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 和动物体重
- en: '](img/B15923_02_04.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_04.jpg)'
- en: 'Figure 2.4: An example of a two-feature dataset comprising animal height and
    animal weight'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：一个包含动物身高和动物体重的双特征数据集示例
- en: One way to approach hierarchical clustering is by starting with each data point,
    serving as its own cluster, and recursively joining the similar points together
    to form clusters – this is known as **agglomerative** hierarchical clustering.
    We will go into more detail about the different ways of approaching hierarchical
    clustering in the *Agglomerative versus Divisive Clustering* section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一种处理层次聚类的方法是从每个数据点开始，将其视为自己的簇，并递归地将相似的点组合在一起形成簇——这就是**聚合**层次聚类。在*聚合与分裂聚类*章节中，我们将详细介绍不同的层次聚类方法。
- en: In the agglomerative hierarchical clustering approach, the concept of data point
    similarity can be thought of in the paradigm that we saw during k-means. In k-means,
    we used the Euclidean distance to calculate the distance from the individual points
    to the centroids of the expected "k" clusters. In this approach to hierarchical
    clustering, we will reuse the same distance metric to determine the similarity
    between the records in our dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合层次聚类方法中，数据点相似度的概念可以借鉴我们在k-means中看到的范式。在k-means中，我们使用欧几里得距离来计算每个数据点到期望的“k”个簇的质心的距离。在层次聚类方法中，我们将重新使用相同的距离度量来确定数据集中记录之间的相似性。
- en: Eventually, by grouping individual records from the data with their most similar
    records recursively, you end up building a hierarchy from the bottom up. The individual
    single-member clusters join into one single cluster at the top of our hierarchy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，通过递归地将数据中的单个记录与其最相似的记录组合在一起，你将从底部向上构建一个层次结构。最终，单独的单一成员簇将合并成一个位于层次结构顶部的单一簇。
- en: Steps to Perform Hierarchical Clustering
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行层次聚类的步骤
- en: 'To understand how agglomerative hierarchical clustering works, we can trace
    the path of a simple toy program as it merges to form a hierarchy:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解聚合层次聚类的工作原理，我们可以跟踪一个简单的示例程序，它是如何通过合并形成层次结构的：
- en: Given n sample data points, view each point as an individual "cluster" with
    just that one point as a member (the centroid).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定n个样本数据点，将每个点视为一个单独的“簇”，其中只有该点作为成员（质心）。
- en: Calculate the pairwise Euclidean distance between the centroids of all the clusters
    in your data. (Here, minimum distance between clusters, maximum distance between
    clusters, average distance between clusters, or distance between two centroids
    can also be considered. In this example, we are considering the distance between
    two cluster centroids).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据集中所有簇的质心之间的成对欧几里得距离。（在此，簇之间的最小距离、最大距离、平均距离或两个质心之间的距离也可以考虑。在这个示例中，我们考虑的是两个簇质心之间的距离）。
- en: Group the closest clusters/points together.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最接近的簇/点组合在一起。
- en: Repeat *Step 2* and *Step 3* until you get a single cluster containing all the
    data in your set.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*和*步骤3*，直到你得到一个包含所有数据的单一簇。
- en: Plot a dendrogram to show how your data has come together in a hierarchical
    structure. A dendrogram is simply a diagram that is used to represent a tree structure,
    showing an arrangement of clusters from top to bottom. We will go into the details
    of how this may be helpful in the following walkthrough.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制树状图以展示数据如何在层次结构中聚合。树状图只是用于表示树状结构的图示，显示簇从上到下的排列。我们将在接下来的演示中详细讨论这如何有助于理解数据的结构。
- en: Decide what level you want to create the clusters at.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定你希望在哪个层级创建簇。
- en: An Example Walkthrough of Hierarchical Clustering
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类的示例演示
- en: 'While slightly more complex than k-means, hierarchical clustering is, in fact,
    quite similar to it from a logistical perspective. Here is a simple example that
    walks through the preceding steps in slightly more detail:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管比 k-means 稍微复杂一些，层次聚类在逻辑上实际上与它非常相似。这里有一个简单的示例，稍微详细地介绍了前面的步骤：
- en: 'Given a list of four sample data points, view each point as a centroid that
    is also its own cluster with the point indices from 0 to 3:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定四个样本数据点的列表，将每个点视为一个质心，它也是其自身的簇，点的索引从 0 到 3：
- en: '[PRE0]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Calculate the pairwise Euclidean distance between the centroids of all the clusters.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有簇的质心之间的成对欧几里得距离。
- en: Note
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注
- en: Refer to the *K-means Clustering In-Depth Walkthrough* section in *Chapter 1*,
    *Introduction to Clustering* for a refresher on Euclidean distance.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考*第 1 章*中*K-means 聚类深入演练*部分，复习欧几里得距离的计算方法。
- en: 'In the matrix displayed in *Figure 2.5*, the point indices are between 0 and
    3 both horizontally and vertically, showing the distance between the respective
    points. Notice that the values are mirrored across the diagonal – this happens
    because you are comparing each point against all the other points, so you only
    need to worry about the set of numbers on one side of the diagonal:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图 2.5*中显示的矩阵里，点的索引在水平方向和垂直方向上都是 0 到 3，表示各个点之间的距离。注意，数值在对角线两侧是镜像对称的——这是因为你在比较每个点与其他所有点的距离，所以只需要关注对角线一侧的数值：
- en: '![Figure 2.5: An array of distances'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.5：一个距离数组'
- en: '](img/B15923_02_05.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_05.jpg)'
- en: 'Figure 2.5: An array of distances'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.5：一个距离数组
- en: Group the closest point pairs together.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将距离最短的点对组合在一起。
- en: 'In this case, points [1,7] and [-5,9] join into a cluster since they are the
    closest, with the remaining two points left as single-member clusters:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，点 [1,7] 和 [-5,9] 因为距离最近而合并成一个簇，其余两个点作为单成员簇保留：
- en: '![Figure 2.6: An array of distances'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.6：一个距离数组'
- en: '](img/B15923_02_06.jpg)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_06.jpg)'
- en: '[PRE1]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Calculate the mean point between the points of the two-member cluster to find
    the new centroid:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两个成员簇的点之间的均值，以找到新的质心：
- en: '[PRE2]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Add the centroid to the two single-member centroids and recalculate the distances:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将质心添加到两个单成员质心中，并重新计算距离：
- en: '[PRE3]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Centroids (3):'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 质心（3）：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once again, we''ll calculate the Euclidean distance between the points and
    the centroid:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次，我们将计算点与质心之间的欧几里得距离：
- en: '![Figure 2.7: An array of distances'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.7：一个距离数组'
- en: '](img/B15923_02_07.jpg)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_07.jpg)'
- en: 'Figure 2.7: An array of distances'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.7：一个距离数组
- en: 'As shown in the preceding image, point [-9,4 ] is the shortest distance from
    the centroid and thus it is added to cluster 1\. Now, the cluster list changes
    to the following:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前图所示，点 [-9,4] 是距离质心最近的点，因此它被添加到簇 1。现在，簇列表更新为以下内容：
- en: '[PRE5]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With only point [4,-2] left as the furthest distance away from its neighbors,
    you can just add it to cluster 1 to unify all the clusters:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只剩下点 [4,-2] 作为距离其邻近点最远的点，你可以将它直接加入簇 1 来统一所有簇：
- en: '[PRE6]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Plot a dendrogram to show the relationship between the points and the clusters:![Figure
    2.8: A dendrogram showing the relationship between the points and the clusters'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个树状图以展示数据点与簇之间的关系：![图 2.8：显示数据点与簇之间关系的树状图
- en: '](img/B15923_02_08.jpg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_08.jpg)'
- en: 'Figure 2.8: A dendrogram showing the relationship between the points and the
    clusters'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：显示数据点与簇之间关系的树状图
- en: Dendrograms show how data points are similar and will look familiar to the hierarchical
    tree structures that we discussed earlier. There is some loss of information,
    as with any visualization technique; however, dendrograms can be very helpful
    when determining how many clusters you want to form. In the preceding example,
    you can see four potential clusters across the X-axis, if each point was its own
    cluster. As you travel vertically, you can see which points are closest together
    and can potentially be clubbed into their own cluster. For example, in the preceding
    dendrogram, the points at indices 0 and 1 are the closest and can form their own
    cluster, while index 2 remains a single-point cluster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图展示了数据点之间的相似性，并且与我们之前讨论的层次树结构非常相似。与任何可视化技术一样，它会丢失一些信息；然而，当确定你希望形成多少个聚类时，树状图是非常有帮助的。在前面的示例中，你可以在
    X 轴上看到四个潜在的聚类，如果每个点都是一个独立的聚类。当你垂直查看时，可以看到哪些点距离最接近，可能会被归为同一聚类。例如，在前面的树状图中，索引 0
    和 1 的点最接近，可以形成一个聚类，而索引 2 仍然是一个单独的聚类。
- en: 'Revisiting the previous animal taxonomy example that involved dog and cat species,
    imagine that you were presented with the following dendrogram:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾前面的动物分类示例，其中涉及狗和猫物种，假设你看到了以下树状图：
- en: '![Figure 2.9: An animal taxonomy dendrogram'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：动物分类树状图'
- en: '](img/B15923_02_09.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_09.jpg)'
- en: 'Figure 2.9: An animal taxonomy dendrogram'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：动物分类树状图
- en: If you were just interested in grouping your species dataset into dogs and cats,
    you could stop clustering at the first level of the grouping. However, if you
    wanted to group all species into domesticated or non-domesticated animals, you
    could stop clustering at level two. The great thing about hierarchical clustering
    and dendrograms is that you can see the entire breakdown of potential clusters
    to choose from.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只对将物种数据集分为狗和猫感兴趣，你可以在第一个分组层级停止聚类。然而，如果你想将所有物种分为家养动物和非家养动物，你可以在第二个层级停止聚类。层次聚类和树状图的一个优点是，你可以看到潜在聚类的完整划分，以供选择。
- en: 'Exercise 2.01: Building a Hierarchy'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.01：构建层级结构
- en: 'Let''s implement the preceding hierarchical clustering approach in Python.
    With the framework for the intuition laid out, we can now explore the process
    of building a hierarchical cluster with some helper functions provided in `sciPy`.
    SciPy ([https://www.scipy.org/docs.html](https://www.scipy.org/docs.html)) is
    an open source library that packages functions that are helpful in scientific
    and technical computing. Examples of this include easy implementations of linear
    algebra and calculus-related methods. In this exercise, we will specifically be
    using helpful functions from the `cluster` subsection of SciPy. In addition to
    `scipy`, we will be using `matplotlib` to complete this exercise. Follow these
    steps to complete this exercise:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Python 中实现前述的层次聚类方法。通过概述直观的框架，我们现在可以使用 `SciPy` 提供的一些辅助函数来探索构建层次聚类的过程。SciPy（[https://www.scipy.org/docs.html](https://www.scipy.org/docs.html)）是一个开源库，提供了对科学计算和技术计算有用的函数。它的示例包括线性代数和微积分相关方法的简单实现。在这个练习中，我们将专门使用
    `SciPy` 中`cluster`子模块的有用函数。除了 `scipy`，我们还将使用 `matplotlib` 来完成这个练习。按照以下步骤完成这个练习：
- en: 'Generate some dummy data, as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一些虚拟数据，如下所示：
- en: '[PRE7]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Generate a random cluster dataset to experiment with. `X` = coordinate points,
    `y` = cluster labels (not needed):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机的聚类数据集进行实验。`X` = 坐标点，`y` = 聚类标签（不需要）：
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Visualize the data, as follows:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据，如下所示：
- en: '[PRE9]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.10: A plot of the dummy data'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.10：虚拟数据的绘图'
- en: '](img/B15923_02_10.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_10.jpg)'
- en: 'Figure 2.10: A plot of the dummy data'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.10：虚拟数据的绘图
- en: After plotting this simple toy example, it should be pretty clear that our dummy
    data comprises eight clusters.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在绘制这个简单的玩具示例后，应该能清楚地看到我们的虚拟数据包含了八个聚类。
- en: 'We can easily generate the distance matrix using the built-in `SciPy` package,
    `linkage`. We will go further into what''s happening with the linkage function
    shortly; however, for now it''s good to know that there are pre-built tools that
    calculate distances between points:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用内置的`SciPy`包中的`linkage`轻松生成距离矩阵。稍后我们将深入了解`linkage`函数的原理；但是，目前了解有现成的工具可以计算点与点之间的距离是非常重要的：
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.11: A matrix of the distances'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.11：距离矩阵'
- en: '](img/B15923_02_11.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_11.jpg)'
- en: 'Figure 2.11: A matrix of the distances'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.11：距离矩阵
- en: If you experiment with different methods by trying to autofill the `method`
    hyperparameter of the `linkage` function, you will see how they affect overall
    performance. Linkage works by simply calculating the distances between each of
    the data points. We will go into specifically what it is calculating in the *Linkage*
    topic. In the `linkage` function, we have the option to select both the metric
    and the method (we will cover this in more detail later).
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你尝试通过自动填写 `linkage` 函数的 `method` 超参数进行不同的方法实验，你会看到它们如何影响整体性能。链接方法通过简单地计算每个数据点之间的距离来工作。我们将在
    *Linkage* 主题中详细讲解它具体计算的内容。在 `linkage` 函数中，我们可以选择度量和方法（稍后会详细介绍）。
- en: After we determine the linkage matrix, we can easily pass it through the `dendrogram`
    function provided by `SciPy`. As the name suggests, the `dendrogram` function
    uses the distances calculated in *Step 4* to generate a visually clean way of
    parsing grouped information.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在确定链接矩阵后，我们可以轻松地通过 `SciPy` 提供的 `dendrogram` 函数进行传递。顾名思义，`dendrogram` 函数利用 *步骤
    4* 中计算的距离生成一种直观简洁的方式来解析分组信息。
- en: 'We will be using a custom function to clean up the styling of the original
    output (note that the function provided in the following snippet is using the
    base SciPy implementation of the dendrogram, and the only custom code is for cleaning
    up the visual output):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个自定义函数来清理原始输出的样式（请注意，以下代码片段中的函数使用的是 `SciPy` 的基础树状图实现，唯一的自定义代码是用来清理视觉输出的）：
- en: '[PRE11]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.12: A dendrogram of the distances'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.12：距离的树状图'
- en: '](img/B15923_02_12.jpg)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_12.jpg)'
- en: 'Figure 2.12: A dendrogram of the distances'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.12：距离的树状图
- en: This plot will give us some perspective on the potential breakouts of our data.
    Based on the distances calculated in prior steps, it shows a potential path that
    we can use to create three separate groups around the distance of seven that are
    distinctly different enough to stand on their own.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图形将帮助我们从数据潜在的分群角度获取一些见解。基于之前步骤计算的距离，它展示了一个潜在路径，我们可以用它来创建三个独立的群组，距离为七，且这些群组足够独特，能够独立存在。
- en: 'Using this information, we can wrap up our exercise on hierarchical clustering
    by using the `fcluster` function from `SciPy`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用这些信息，我们可以通过使用 `SciPy` 中的 `fcluster` 函数来结束我们的层次聚类练习：
- en: '[PRE12]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `fcluster` function uses the distances and information from the dendrogram
    to cluster our data into a number of groups based on a stated threshold. The number
    `3` in the preceding example represents the maximum inter-cluster distance threshold
    hyperparameter that you can set. This hyperparameter can be tuned based on the
    dataset that you are looking at; however, it is supplied to you as `3` for this
    exercise. The final output is as follows:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`fcluster` 函数利用来自树状图的距离和信息，根据指定的阈值将数据分组。前面示例中的数字 `3` 代表你可以设置的最大聚类间距离阈值超参数。这个超参数可以根据你正在处理的数据集进行调整；然而，在本练习中，它被设定为
    `3`。最终的输出如下：'
- en: '![Figure 2.13: A scatter plot of the distances'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.13：距离的散点图'
- en: '](img/B15923_02_13.jpg)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_13.jpg)'
- en: 'Figure 2.13: A scatter plot of the distances'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：距离的散点图
- en: In the preceding plot, you can see that by using our threshold hyperparameter,
    we've identified eight distinct clusters. By simply calling a few helper functions
    provided by `SciPy`, you can easily implement agglomerative clustering in just
    a few lines of code. While SciPy does help with many of the intermediate steps,
    this is still an example that is a bit more verbose than what you will probably
    see in your regular work. We will cover more streamlined implementations later.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，你可以看到通过使用我们的阈值超参数，我们已经识别出八个不同的聚类。只需调用 `SciPy` 提供的几个辅助函数，你就能轻松地在几行代码中实现聚合聚类。尽管
    `SciPy` 确实帮助处理了许多中间步骤，但这个示例仍然相对冗长，可能不完全符合你日常工作中的精简代码。我们稍后会介绍更简化的实现方式。
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer t o [https://packt.live/2VTRp5K](https://packt.live/2VTRp5K).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考 [https://packt.live/2VTRp5K](https://packt.live/2VTRp5K)。
- en: You can also run this example online at [https://packt.live/2Cdyiww](https://packt.live/2Cdyiww).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/2Cdyiww](https://packt.live/2Cdyiww)。
- en: Linkage
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接
- en: In *Exercise 2.01*, *Building a Hierarchy*, you implemented hierarchical clustering
    using what is known as **Centroid Linkage**. Linkage is the concept of determining
    how you can calculate the distances between clusters and is dependent on the type
    of problem you are facing. Centroid linkage was chosen for *Exercise 2.02*, *Applying
    Linkage Criteria*, as it essentially mirrors the new centroid search that we used
    in k-means. However, this is not the only option when it comes to clustering data
    points. Two other popular choices for determining distances between clusters are
    single linkage and complete linkage.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 2.01*，*建立层次结构* 中，你使用被称为**质心连接**的方法实现了层次聚类。连接是确定如何计算聚类之间距离的概念，并且依赖于你面临的问题类型。选择质心连接用于*练习
    2.02*，*应用连接准则*，因为它基本上镜像了我们在 k-means 中使用的新质心搜索。然而，这并不是聚类数据点时唯一的选项。确定聚类之间距离的另外两个常见选择是单一连接和完全连接。
- en: '**Single Linkage** works by finding the minimum distance between a pair of
    points between two clusters as its criteria for linkage. Simply put, it essentially
    works by combining clusters based on the closest points between the two clusters.
    This is expressed mathematically as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**单一连接** 通过找到两聚类之间一对点之间的最小距离作为连接的标准。简单来说，它基本上通过根据两个聚类之间最接近的点来组合聚类。数学上表示如下：'
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, `a[i]` is the *i*th point within first cluster where
    `b[j]` is *j*th point of second cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`a[i]` 是第一个聚类中的 *i* 点，而 `b[j]` 是第二个聚类中的 *j* 点。
- en: '**Complete Linkage** is the opposite of single linkage and it works by finding
    the maximum distance between a pair of points between two clusters as its criteria
    for linkage. Simply put, it works by combining clusters based on the furthest
    points between the two clusters. This is mathematically expressed as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**完全连接** 是单一连接的对立面，它通过找到两聚类之间一对点之间的最大距离作为连接的标准。简单来说，它通过根据两个聚类之间最远的点来组合聚类。数学上表示如下：'
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, `a[i]` and `b[j]` are *i*th and *j*th point of first
    and second cluster respectively. Determining what linkage criteria is best for
    your problem is as much art as it is science, and it is heavily dependent on your
    particular dataset. One reason to choose single linkage is if your data is similar
    in a nearest-neighbor sense; therefore, when there are differences, the data is
    extremely dissimilar. Since single linkage works by finding the closest points,
    it will not be affected by these distant outliers. However, as single linkage
    works by finding the smallest distance between a pair of points, it is quite prone
    to the noise distributed between the clusters. Conversely, complete linkage may
    be a better option if your data is distant in terms of inter-cluster state; complete
    linkage causes incorrect splitting when the spatial distribution of cluster is
    fairly imbalanced. Centroid linkage has similar benefits but falls apart if the
    data is very noisy and there are less clearly defined "centers" of clusters. Typically,
    the best approach is to try a few different linkage criteria options and see which
    fits your data in a way that's the most relevant to your goals.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`a[i]` 和 `b[j]` 分别是第一个和第二个聚类中的 *i* 和 *j* 点。确定哪种连接准则最适合你的问题既是艺术也是科学，而且它在很大程度上依赖于你的特定数据集。选择单一连接的一个原因是如果你的数据在最近邻的意义上相似；因此，当存在差异时，数据将非常不相似。由于单一连接通过寻找最接近的点来工作，它不会受到这些遥远异常值的影响。然而，由于单一连接通过寻找一对点之间的最小距离来工作，它很容易受到分布在聚类之间的噪声影响。相反，如果你的数据在聚类间的状态上相距较远，那么完全连接可能是更好的选择；当聚类的空间分布不平衡时，完全连接会导致错误的分割。质心连接具有类似的优点，但如果数据非常嘈杂且聚类的“中心”不够明显，它就会失效。通常，最佳的方法是尝试几种不同的连接准则选项，看看哪种最适合你的数据，并以对你的目标最相关的方式进行操作。
- en: 'Exercise 2.02: Applying Linkage Criteria'
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：应用连接准则
- en: Recall the dummy data of the eight clusters that we generated in the previous
    exercise. In the real world, you may be given real data that resembles discrete
    Gaussian blobs in the same way. Imagine that the dummy data represents different
    groups of shoppers in a particular store. The store manager has asked you to analyze
    the shopper data in order to classify the customers into different groups so that
    they can tailor marketing materials to each group.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆我们在前一个练习中生成的八个聚类的虚拟数据。在现实世界中，你可能会得到类似于离散高斯簇的真实数据。假设这些虚拟数据代表了某个商店中不同购物群体的情况。商店经理要求你分析这些购物者数据，以便将客户分为不同的群体，进而为每个群体量身定制营销材料。
- en: Using the data we generated in the previous exercise, or by generating new data,
    you are going to analyze which linkage types do the best job of grouping the customers
    into distinct clusters.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在前一个练习中生成的数据，或者生成新数据，你将分析哪些连接方法能更好地将客户分组到不同的聚类中。
- en: 'Once you have generated the data, view the documents supplied using SciPy to
    understand what linkage types are available in the `linkage` function. Then, evaluate
    the linkage types by applying them to your data. The linkage types you should
    test are shown in the following list:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成数据，查看通过 SciPy 提供的文档，了解在 `linkage` 函数中有哪些连接类型可用。然后，通过将它们应用于你的数据来评估这些连接类型。你应测试的连接类型在以下列表中展示：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We haven''t covered all of the previously mentioned linkage types yet – a key
    part of this activity is to learn how to parse the docstrings that are provided
    using packages to explore all of their capabilities. Follow these steps to complete
    this exercise:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未覆盖所有之前提到的连接类型——这项活动的关键部分是学习如何解析使用包提供的文档字符串，以探索它们的所有功能。请按照以下步骤完成本练习：
- en: 'Visualize the `x` dataset that we created in *Exercise 2.01*, *Building a Hierarchy*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化我们在 *练习 2.01* 中创建的 `x` 数据集，*构建层次结构*：
- en: '[PRE16]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Generate a random cluster dataset to experiment on. `X` = coordinate points,
    `y` = cluster labels (not needed):'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机的聚类数据集进行实验。`X` = 坐标点，`y` = 聚类标签（不需要）：
- en: '[PRE17]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Visualize the data, as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示可视化数据：
- en: '[PRE18]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.14: A scatter plot of the generated cluster dataset'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.14: 生成的聚类数据集的散点图'
- en: '](img/B15923_02_14.jpg)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_14.jpg)'
- en: 'Figure 2.14: A scatter plot of the generated cluster dataset'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.14: 生成的聚类数据集的散点图'
- en: 'Create a list with all the possible linkage method hyperparameters:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含所有可能连接方法超参数的列表：
- en: '[PRE19]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Loop through each of the methods in the list that you just created and display
    the effect that they have on the same dataset:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历你刚创建的列表中的每个方法，并显示它们对相同数据集的影响：
- en: '[PRE20]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The plot for centroid linkage is as follows:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重心连接法的图示如下：
- en: '![Figure 2.15: A scatter plot for centroid linkage method'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.15: 重心连接方法的散点图'
- en: '](img/B15923_02_15.jpg)'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_15.jpg)'
- en: 'Figure 2.15: A scatter plot for centroid linkage method'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.15: 重心连接方法的散点图'
- en: 'The plot for single linkage is as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 单连接法的图示如下：
- en: '![Figure 2.16: A scatter plot for single linkage method'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16: 单连接方法的散点图'
- en: '](img/B15923_02_16.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_16.jpg)'
- en: 'Figure 2.16: A scatter plot for single linkage method'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.16: 单连接方法的散点图'
- en: 'The plot for complete linkage is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接法的图示如下：
- en: '![Figure 2.17: A scatter plot for complete linkage method'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17: 完全连接方法的散点图'
- en: '](img/B15923_02_17.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_17.jpg)'
- en: 'Figure 2.17: A scatter plot for complete linkage method'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.17: 完全连接方法的散点图'
- en: 'The plot for average linkage is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 平均连接法的图示如下：
- en: '![Figure 2.18: A scatter plot for average linkage method'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18: 平均连接方法的散点图'
- en: '](img/B15923_02_18.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_18.jpg)'
- en: 'Figure 2.18: A scatter plot for average linkage method'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.18: 平均连接方法的散点图'
- en: 'The plot for weighted linkage is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 加权连接法的图示如下：
- en: '![Figure 2.19: A scatter plot for weighted linkage method'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19: 加权连接方法的散点图'
- en: '](img/B15923_02_19.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_19.jpg)'
- en: 'Figure 2.19: A scatter plot for weighted linkage method'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.19: 加权连接方法的散点图'
- en: As shown in the preceding plots, by simply changing the linkage criteria, you
    can dramatically change the efficacy of your clustering. In this dataset, centroid
    and average linkage work best at finding discrete clusters that make sense. This
    is clear from the fact that we generated a dataset of eight clusters, and centroid
    and average linkage are the only ones that show the clusters that are represented
    using eight different colors. The other linkage types fall short – most noticeably,
    single linkage. Single linkage falls short because it operates on the assumption
    that the data is in a thin "chain" format versus the clusters. The other linkage
    methods are superior due to their assumption that the data is coming in as clustered
    groups.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图所示，通过简单地改变连接准则，你可以显著改变聚类的效果。在这个数据集中，质心和平均连接在找到有意义的离散聚类方面效果最好。从我们生成的包含八个聚类的数据集来看，质心和平均连接是唯一能够显示出用八种不同颜色表示的聚类的连接方式。其他连接类型效果较差，最明显的是单一连接。单一连接的效果较差，因为它假设数据是以细长的“链”格式存在，而不是聚类的形式。其他连接方法更优秀，因为它们假设数据是以聚集组的形式呈现。
- en: Note
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VWwbEv](https://packt.live/2VWwbEv).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2VWwbEv](https://packt.live/2VWwbEv)。
- en: You can also run this example online at [https://packt.live/2Zb4zgN](https://packt.live/2Zb4zgN).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在在线运行这个示例：[https://packt.live/2Zb4zgN](https://packt.live/2Zb4zgN)。
- en: Agglomerative versus Divisive Clustering
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合型与分裂型聚类
- en: 'So far, our instances of hierarchical clustering have all been agglomerative
    – that is, they have been built from the bottom up. While this is typically the
    most common approach for this type of clustering, it is important to know that
    it is not the only way a hierarchy can be created. The opposite hierarchical approach,
    that is, built from the top up, can also be used to create your taxonomy. This
    approach is called **divisive** hierarchical clustering and works by having all
    the data points in your dataset in one massive cluster. Many of the internal mechanics
    of the divisive approach will prove to be quite similar to the agglomerative approach:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的层次聚类实例都是聚合型的——也就是说，它们是从底部向上构建的。虽然这是这种聚类类型最常见的方法，但重要的是要知道，这并不是创建层次结构的唯一方式。相反的层次方法——即从顶部向下构建，也可以用于创建你的分类法。这种方法称为**分裂型**层次聚类，它通过将数据集中的所有数据点放入一个大的聚类中来工作。分裂型方法的许多内部机制与聚合型方法非常相似：
- en: '![Figure 2.20: Agglomerative versus divisive hierarchical clustering'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.20：聚合型与分裂型层次聚类'
- en: '](img/B15923_02_20.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_20.jpg)'
- en: 'Figure 2.20: Agglomerative versus divisive hierarchical clustering'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20：聚合型与分裂型层次聚类
- en: As with most problems in unsupervised learning, deciding on the best approach
    is often highly dependent on the problem you are faced with solving.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数无监督学习中的问题一样，决定最佳方法通常高度依赖于你所面临的具体问题。
- en: Imagine that you are an entrepreneur who has just bought a new grocery store
    and needs to stock it with goods. You receive a large shipment of food and drink
    in a container, but you've lost track of all the shipment information. In order
    to effectively sell your products, you must group similar products together (your
    store will be a huge mess if you just put everything on the shelves in a random
    order). Setting out on this organizational goal, you can take either a bottom-up
    or top-down approach. On the bottom-up side, you will go through the shipping
    container and think of everything as disorganized – you will then pick up a random
    object and find its most similar product. For example, you may pick up apple juice
    and realize that it makes sense to group it together with orange juice. With the
    top-down approach, you will view everything as organized in one large group. Then,
    you will move through your inventory and split the groups based on the largest
    differences in similarity. For example, if you were organizing a grocery store,
    you may originally think that apples and apple juice go together, but on second
    thoughts, they are quite different. Therefore, you will break them into smaller,
    dissimilar groups.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一位刚刚购买了一家新杂货店的创业者，现需要为店铺备货。你收到了一大批食品和饮料的运输箱，但你已经失去了所有的运输信息。为了有效地销售你的产品，你必须将相似的产品归为一类（如果你把所有东西随便放到货架上，店铺会变得一团糟）。为了实现这个组织目标，你可以采取自下而上或自上而下的方法。在自下而上的方法中，你会仔细查看运输箱里的物品，并认为一切都是杂乱无章的——然后，你会拿起一个随机物品，找出最相似的产品。例如，你可能拿起苹果汁，意识到将其与橙汁放在一起是有道理的。在自上而下的方法中，你会认为所有物品一开始就已按某种方式组织成一个大类。然后，你会逐步检查库存，并根据相似度的最大差异将其拆分成多个组。例如，如果你正在组织一家杂货店，你可能一开始认为苹果和苹果汁是属于同一类，但再想想，它们其实是相当不同的。因此，你会将它们拆分成更小的、不相似的组。
- en: In general, it helps to think of agglomerative as the bottom-up approach and
    divisive as the top-down approach – but how do they trade off in terms of performance?
    This behavior of immediately grabbing the closest thing is known as "greedy learning;"
    it has the potential to be fooled by local neighbors and not see the larger implications
    of the clusters it forms at any given time. On the flip side, the divisive approach
    has the benefit of seeing the entire data distribution as one from the beginning
    and choosing the best way to break down clusters. This insight into what the entire
    dataset looks like is helpful for potentially creating more accurate clusters
    and should not be overlooked. Unfortunately, a top-down approach typically trades
    off greater accuracy for deeper complexity. In practice, an agglomerative approach
    works most of the time and should be the preferred starting point when it comes
    to hierarchical clustering. If, after reviewing the hierarchies, you are unhappy
    with the results, it may help to take a divisive approach.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，可以将凝聚聚类看作自下而上的方法，将分裂聚类看作自上而下的方法——但它们在性能上有何权衡？这种立刻抓住最接近的物品的行为被称为“贪婪学习”；它有可能被局部邻居所欺骗，无法看到它在任何给定时刻所形成的簇的更大影响。另一方面，分裂方法的优势在于它从一开始就能看到整个数据分布作为一个整体，并选择最好的方式来拆分簇。对整个数据集外观的这种洞察对于创建更精确的簇非常有帮助，不容忽视。不幸的是，自上而下的方法通常会以牺牲更高的准确性为代价，换来更深的复杂性。在实践中，凝聚方法大多数情况下效果较好，应当作为层次聚类的首选起点。如果在查看层次结构后，你对结果不满意，尝试使用分裂方法可能会有所帮助。
- en: 'Exercise 2.03: Implementing Agglomerative Clustering with scikit-learn'
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.03：使用 scikit-learn 实现凝聚聚类
- en: 'In most business use cases, you will likely find yourself implementing hierarchical
    clustering with a package that abstracts everything away, such as scikit-learn.
    Scikit-learn is a free package that is indispensable when it comes to machine
    learning in Python. It conveniently provides highly optimized forms of the most
    popular algorithms, such as regression, classification, and clustering. By using
    an optimized package such as scikit-learn, your work becomes much easier. However,
    you should only use it when you fully understand how hierarchical clustering works,
    as we discussed in the previous sections. This exercise will compare two potential
    routes that you can take when forming clusters – using SciPy and scikit-learn.
    By completing this exercise, you will learn what the pros and cons are of each,
    and which suits you best from a user perspective. Follow these steps to complete
    this exercise:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数商业应用场景中，你很可能会使用一个抽象了所有内容的包来实现层次聚类，比如 scikit-learn。Scikit-learn 是一个免费包，是
    Python 中进行机器学习时不可或缺的工具。它便捷地提供了最流行算法（如回归、分类和聚类）的高度优化版本。通过使用像 scikit-learn 这样的优化包，你的工作变得更加轻松。然而，只有在你完全理解层次聚类的工作原理之后，才应该使用它，正如我们在前面的章节中讨论的那样。这个练习将比较使用
    SciPy 和 scikit-learn 进行聚类的两种潜在路线。通过完成此练习，你将了解它们各自的优缺点，以及从用户角度来看哪个最适合你。请按照以下步骤完成此练习：
- en: 'Scikit-learn makes implementation as easy as just a few lines of code. First,
    import the necessary packages and assign the model to the `ac` variable. Then,
    create the blob data as shown in the previous exercises:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learn 使得实现变得轻松，仅需几行代码。首先，导入必要的包并将模型分配给 `ac` 变量。然后，创建如前述练习中所示的 blob 数据：
- en: '[PRE21]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: First, we assign the model to the `ac` variable by passing in parameters that
    we are familiar with, such as `affinity` (the distance function) and `linkage`.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们通过传入熟悉的参数（如 `affinity`（距离函数）和 `linkage`）将模型分配给 `ac` 变量。
- en: 'Then reuse the `linkage` function and `fcluster` objects we used in prior exercises:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后重用我们在之前练习中使用的 `linkage` 函数和 `fcluster` 对象：
- en: '[PRE22]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After instantiating our model into a variable, we can simply fit the dataset
    to the desired model using `.fit_predict()` and assign it to an additional variable.
    This will give us information on the ideal clusters as part of the model fitting process.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在将模型实例化为一个变量后，我们可以简单地使用 `.fit_predict()` 将数据集拟合到所需模型，并将其赋值给另一个变量。这将为我们提供有关理想聚类的信息，作为模型拟合过程的一部分。
- en: 'Then, we can compare how each of the approaches work by comparing the final
    cluster results through plotting. Let''s take a look at the clusters from the
    scikit-learn approach:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以通过绘图比较每种方法的最终聚类结果。让我们来看一下 scikit-learn 方法的聚类结果：
- en: '[PRE23]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the output for the clusters from the scikit-learn approach:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是 scikit-learn 方法的聚类结果输出：
- en: '![Figure 2.21: A plot of the scikit-learn approach'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.21：scikit-learn 方法的图示'
- en: '](img/B15923_02_21.jpg)'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_02_21.jpg)'
- en: 'Figure 2.21: A plot of the scikit-learn approach'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：scikit-learn 方法的图示
- en: 'Take a look at the clusters from the SciPy approach:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 来看看 SciPy 方法的聚类结果：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.22: A plot of the SciPy approach'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.22：SciPy 方法的图示'
- en: '](img/B15923_02_22.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_22.jpg)'
- en: 'Figure 2.22: A plot of the SciPy approach'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22：SciPy 方法的图示
- en: As you can see, the two converge to basically the same clusters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两个方法基本上汇聚到了相同的聚类结果。
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2DngJuz](https://packt.live/2DngJuz).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考 [https://packt.live/2DngJuz](https://packt.live/2DngJuz)。
- en: You can also run this example online at [https://packt.live/3f5PRgy](https://packt.live/3f5PRgy).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在网上运行此示例，网址为 [https://packt.live/3f5PRgy](https://packt.live/3f5PRgy)。
- en: While this is great from a toy problem perspective, in the next activity, you
    will learn that small changes to the input parameters can lead to wildly different
    results.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于玩具问题来说很棒，但在下一次活动中，你将了解到，输入参数的微小变化可能会导致截然不同的结果。
- en: 'Activity 2.01: Comparing k-means with Hierarchical Clustering'
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.01：比较 K-means 和层次聚类
- en: 'You are managing a store''s inventory and receive a large shipment of wine,
    but the brand labels fell off the bottles in transit. Fortunately, your supplier
    has provided you with the chemical readings for each bottle, along with their
    respective serial numbers. Unfortunately, you aren''t able to open each bottle
    of wine and taste test the difference – you must find a way to group the unlabeled
    bottles back together according to their chemical readings. You know from the
    order list that you ordered three different types of wine and are given only two
    wine attributes to group the wine types back together. In this activity, we will
    be using the wine dataset. This dataset comprises chemical readings from three
    different types of wine, and as per the source on the UCI Machine Learning Repository,
    it contains these features:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在管理一家商店的库存，并收到了大量葡萄酒货物，但品牌标签在运输过程中从瓶子上脱落。幸运的是，您的供应商为每瓶酒提供了化学成分数据，并附上了各自的序列号。不幸的是，您无法打开每瓶酒进行品尝测试——您必须找到一种方法，根据化学成分数据将未贴标签的酒瓶重新分组。您从订单单上知道，您订购了三种不同类型的葡萄酒，并且只提供了两种葡萄酒属性来重新分组这些酒。在此活动中，我们将使用葡萄酒数据集。该数据集包含三种不同类型葡萄酒的化学成分数据，根据UCI机器学习库中的来源，包含以下特征：
- en: Alcohol
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 酒精
- en: Malic acid
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苹果酸
- en: Ash
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰分
- en: Alkalinity of ash
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灰分的碱度
- en: Magnesium
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镁
- en: Total phenols
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总酚类
- en: Flavanoids
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄酮类
- en: Nonflavanoid phenols
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非黄酮类酚
- en: Proanthocyanins
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原花青素
- en: Color intensity
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 颜色强度
- en: Hue
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 色调
- en: OD280/OD315 of diluted wines
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀释葡萄酒的OD280/OD315
- en: Proline
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前花青素
- en: Note
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The wine dataset is sourced from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/).[UCI
    Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.]
    It can also be accessed at [https://packt.live/3aP8Tpv](https://packt.live/3aP8Tpv).'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该葡萄酒数据集来源于[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)。[UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。美国加利福尼亚州尔湾市：加利福尼亚大学信息与计算机科学学院]。它也可以通过[https://packt.live/3aP8Tpv](https://packt.live/3aP8Tpv)访问。
- en: The aim of this activity is to implement k-means and hierarchical clustering
    on the wine dataset and to determine which of these approaches is more accurate
    in forming three separate clusters for each wine type. You can try different combinations
    of scikit-learn implementations and use helper functions in SciPy and NumPy. You
    can also use the silhouette score to compare the different clustering methods
    and visualize the clusters on a graph.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目的是在葡萄酒数据集上实现k-means和层次聚类，并确定哪种方法在为每种葡萄酒类型形成三个独立的聚类时更加准确。您可以尝试不同的scikit-learn实现组合，并使用SciPy和NumPy中的辅助函数。您还可以使用轮廓系数来比较不同的聚类方法，并在图表上可视化聚类结果。
- en: After completing this activity, you will see first-hand how two different clustering
    algorithms perform on the same dataset, allowing easy comparison when it comes
    to hyperparameter tuning and overall performance evaluation. You will probably
    notice that one method performs better than the other, depending on how the data
    is shaped. Another key outcome from this activity is gaining an understanding
    of how important hyperparameters are in any given use case.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，您将亲眼看到两种不同的聚类算法在同一数据集上的表现，从而便于在超参数调优和整体性能评估时进行比较。您可能会注意到，根据数据的形状，一种方法的表现优于另一种方法。此活动的另一个关键结果是理解超参数在任何给定用例中的重要性。
- en: 'Here are the steps to complete this activity:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成此活动的步骤：
- en: Import the necessary packages from scikit-learn (`KMeans`, `AgglomerativeClustering`,
    and `silhouette_score`).
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn导入必要的包（`KMeans`、`AgglomerativeClustering`和`silhouette_score`）。
- en: Read the wine dataset into a pandas DataFrame and print a small sample.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将葡萄酒数据集读取到pandas DataFrame中并打印一个小样本。
- en: Visualize some features from the dataset by plotting the OD Reading feature
    against the proline feature.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制OD读取特征与前花青素特征的关系，来可视化数据集中的一些特征。
- en: Use the `sklearn` implementation of k-means on the wine dataset, knowing that
    there are three wine types.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sklearn`实现的k-means算法对葡萄酒数据集进行聚类，知道数据集中有三种葡萄酒类型。
- en: Use the `sklearn` implementation of hierarchical clustering on the wine dataset.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sklearn`实现的层次聚类对葡萄酒数据集进行聚类。
- en: Plot the predicted clusters from k-means.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制k-means算法预测的聚类。
- en: Plot the predicted clusters from hierarchical clustering.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制层次聚类算法预测的聚类。
- en: Compare the silhouette score of each clustering method.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较每种聚类方法的轮廓系数。
- en: 'Upon completing this activity, you should have plotted the predicted clusters
    you obtained from k-means as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，你应该已经绘制出了从 k-means 获得的预测簇，如下所示：
- en: '![Figure 2.23: The expected clusters from the k-means method'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.23：来自 k-means 方法的预期簇'
- en: '](img/B15923_02_23.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_23.jpg)'
- en: 'Figure 2.23: The expected clusters from the k-means method'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.23：来自 k-means 方法的预期簇
- en: 'A similar plot should also be obtained for the cluster that was predicted by
    hierarchical clustering, as shown here:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于层次聚类预测的簇，也应获得类似的图，如下所示：
- en: '![Figure 2.24: The expected clusters from the agglomerative method'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.24：来自聚合方法的预期簇'
- en: '](img/B15923_02_24.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_02_24.jpg)'
- en: 'Figure 2.24: The expected clusters from the agglomerative method'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.24：来自聚合方法的预期簇
- en: Note
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 423.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第 423 页找到。
- en: k-means versus Hierarchical Clustering
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 与层次聚类
- en: In the previous chapter, we explored the merits of k-means clustering. Now,
    it is important to explore where hierarchical clustering fits into the picture.
    As we mentioned in the *Linkage* section, there is some potential direct overlap
    when it comes to grouping data points together using centroids. Universal to all
    of the approaches we've mentioned so far is the use of a distance function to
    determine similarity. Due to our in-depth exploration in the previous chapter,
    we used the Euclidean distance here, but we understand that any distance function
    can be used to determine similarities.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了 k-means 聚类的优点。现在，了解层次聚类在其中的作用是非常重要的。正如我们在*连接*部分提到的那样，当使用质心对数据点进行分组时，可能存在一些直接的重叠。到目前为止，我们提到的所有方法都普遍使用了距离函数来确定相似性。由于我们在上一章的深入探讨，我们在这里使用了欧几里得距离，但我们理解任何距离函数都可以用来确定相似性。
- en: 'In practice, here are some quick highlights for choosing one clustering method
    over another:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，选择一种聚类方法而非另一种时，可以参考以下几点：
- en: Hierarchical clustering benefits from not needing to pass in an explicit "k"
    number of clusters a priori. This means that you can find all the potential clusters
    and decide which clusters make the most sense after the algorithm has completed.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类的优势在于不需要预先传入明确的“k”簇数。这意味着你可以找到所有潜在的簇，并在算法完成后决定哪些簇最有意义。
- en: The k-means clustering benefits from a simplicity perspective – oftentimes,
    in business use cases, there is a challenge when it comes to finding methods that
    can be explained to non-technical audiences but are still accurate enough to generate
    quality results. k-means can easily fill this niche.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从简便性角度来看，k-means 聚类具有优势——在商业用例中，通常面临一个挑战：如何找到既能向非技术观众解释又能生成高质量结果的方法。k-means
    可以轻松填补这一空缺。
- en: Hierarchical clustering has more parameters to tweak than k-means clustering
    when it comes to dealing with abnormally shaped data. While k-means is great at
    finding discrete clusters, it can falter when it comes to mixed clusters. By tweaking
    the parameters in hierarchical clustering, you may find better results.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类在处理形状异常的数据时，调整的参数比 k-means 聚类更多。虽然 k-means 在发现离散簇方面表现优秀，但在处理混合簇时可能会遇到困难。通过调整层次聚类中的参数，你可能会得到更好的结果。
- en: Vanilla k-means clustering works by instantiating random centroids and finding
    the closest points to those centroids. If they are randomly instantiated in areas
    of the feature space that are far away from your data, then it can end up taking
    quite some time to converge, or it may never even get to that point. Hierarchical
    clustering is less prone to falling prey to this weakness.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本的 k-means 聚类通过实例化随机质心并找到与这些质心最接近的点来工作。如果它们被随机实例化在特征空间中远离数据的区域，那么可能需要相当长的时间才能收敛，或者可能根本无法达到收敛点。层次聚类则较不容易受到这种缺陷的影响。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed how hierarchical clustering works and where it
    may be best employed. In particular, we discussed various aspects of how clusters
    can be subjectively chosen through the evaluation of a dendrogram plot. This is
    a huge advantage over k-means clustering if you have absolutely no idea of what
    you''re looking for in the data. Two key parameters that drive the success of
    hierarchical clustering were also discussed: the agglomerative versus divisive
    approach and linkage criteria. Agglomerative clustering takes a bottom-up approach
    by recursively grouping nearby data together until it results in one large cluster.
    Divisive clustering takes a top-down approach by starting with the one large cluster
    and recursively breaking it down until each data point falls into its own cluster.
    Divisive clustering has the potential to be more accurate since it has a complete
    view of the data from the start; however, it adds a layer of complexity that can
    decrease the stability and increase the runtime.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了层次聚类的工作原理以及它可能的最佳应用场景。特别是，我们讨论了如何通过评估树状图来主观地选择簇的各个方面。如果你完全不知道在数据中寻找什么，这比k均值聚类有一个巨大的优势。我们还讨论了两个推动层次聚类成功的关键参数：合并方法和分裂方法，以及聚类标准。合并聚类采用自下而上的方法，通过递归地将邻近的数据合并在一起，直到结果为一个大簇。分裂聚类采用自上而下的方法，从一个大簇开始，递归地将其拆分，直到每个数据点都进入其自己的簇。分裂聚类由于从一开始就可以全面查看数据，因此有可能更准确；然而，它增加了一层复杂性，这可能会降低稳定性并增加运行时间。
- en: Linkage criteria grapples with the concept of how distance is calculated between
    candidate clusters. We have explored how centroids can make an appearance again
    beyond k-means clustering, as well as single and complete linkage criteria. Single
    linkage finds cluster distances by comparing the closest points in each cluster,
    while complete linkage finds cluster distances by comparing more distant points
    in each cluster. With the knowledge that you have gained in this chapter, you
    are now able to evaluate how both k-means and hierarchical clustering can best
    fit the challenge that you are working on.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类标准涉及如何计算候选簇之间的距离。我们已经探讨了质心如何在k均值聚类之外再次出现，以及单一链接和完全链接标准。单一链接通过比较每个簇中最近的点来计算簇间距离，而完全链接通过比较每个簇中较远的点来计算簇间距离。通过本章所学的知识，你现在能够评估k均值聚类和层次聚类如何最好地适应你正在处理的挑战。
- en: 'While hierarchical clustering can result in better performance than k-means
    due to its increased complexity, please remember that more complexity is not always
    good. Your duty as a practitioner of unsupervised learning is to explore all the
    options and identify the solution that is both resource-efficient and performant.
    In the next chapter, we will cover a clustering approach that will serve us best
    when it comes to highly complex and noisy data: **Density-Based Spatial Clustering
    of Applications with Noise**.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然由于层次聚类的复杂性增加，它可能比k均值聚类表现更好，但请记住，更多的复杂性并不总是好的。作为无监督学习的从业者，你的职责是探索所有选项，并确定既高效又具有性能的解决方案。在下一章中，我们将介绍一种在处理高度复杂和噪声数据时最适用的聚类方法：**基于密度的空间聚类应用与噪声**。
