- en: Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 分类（I）- 树、懒惰和概率
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下步骤：
- en: Preparing the training and testing datasets
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备训练和测试数据集
- en: Building a classification model with recursive partitioning trees
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用递归分区树构建分类模型
- en: Visualizing a recursive partitioning tree
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化递归分区树
- en: Measuring the prediction performance of a recursive partitioning tree
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量递归分区树的预测性能
- en: Pruning a recursive partitioning tree
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝递归分区树
- en: Building a classification model with a conditional inference tree
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用条件推断树构建分类模型
- en: Visualizing a conditional inference tree
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化条件推断树
- en: Measuring the prediction performance of a conditional inference tree
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量条件推断树的预测性能
- en: Classifying data with a k-nearest neighbor classifier
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k近邻分类器对数据进行分类
- en: Classifying data with logistic regression
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归对数据进行分类
- en: Classifying data with the Naïve Bayes classifier
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类器对数据进行分类
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Classification is used to identify a category of new observations (testing datasets)
    based on a classification model built from the training dataset, of which the
    categories are already known. Similar to regression, classification is categorized
    as a supervised learning method as it employs known answers (label) of a training
    dataset to predict the answer (label) of the testing dataset. The main difference
    between regression and classification is that regression is used to predict continuous
    values.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分类用于根据从训练数据集构建的分类模型识别新观察值的类别（测试数据集），其中类别已经已知。与回归类似，分类被归类为监督学习方法，因为它使用训练数据集的已知答案（标签）来预测测试数据集的答案（标签）。回归和分类之间的主要区别在于，回归用于预测连续值。
- en: In contrast to this, classification is used to identify the category of a given
    observation. For example, one may use regression to predict the future price of
    a given stock based on historical prices. However, one should use the classification
    method to predict whether the stock price will rise or fall.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，分类用于识别给定观察值的类别。例如，一个人可能使用回归来根据历史价格预测给定股票的未来价格。然而，应该使用分类方法来预测股票价格是上涨还是下跌。
- en: In this chapter, we will illustrate how to use R to perform classification.
    We first build a training dataset and a testing dataset from the churn dataset,
    and then apply different classification methods to classify the churn dataset.
    In the following recipes, we will introduce the tree-based classification method
    using a traditional classification tree and a conditional inference tree, lazy-based
    algorithm, and a probabilistic-based method using the training dataset to build
    up a classification model, and then use the model to predict the category (class
    label) of the testing dataset. We will also use a confusion matrix to measure
    the performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将说明如何使用R进行分类。我们首先从客户流失数据集中构建训练数据集和测试数据集，然后应用不同的分类方法对客户流失数据集进行分类。在下面的步骤中，我们将介绍使用传统分类树和条件推断树、基于懒惰的算法以及使用基于概率的方法（使用训练数据集构建分类模型），然后使用该模型预测测试数据集的类别（类别标签）。我们还将使用混淆矩阵来衡量性能。
- en: Preparing the training and testing datasets
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备训练和测试数据集
- en: Building a classification model requires a training dataset to train the classification
    model, and testing data is needed to then validate the prediction performance.
    In the following recipe, we will demonstrate how to split the telecom churn dataset
    into training and testing datasets, respectively.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分类模型需要训练数据集来训练分类模型，并且需要测试数据来验证预测性能。在下面的步骤中，我们将演示如何将电信客户流失数据集分割成训练数据集和测试数据集。
- en: Getting ready
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the telecom churn dataset as the input data source,
    and split the data into training and testing datasets.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将使用电信客户流失数据集作为输入数据源，并将数据分割成训练数据集和测试数据集。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to split the churn dataset into training and testing
    datasets:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤将客户流失数据集分割成训练数据集和测试数据集：
- en: 'You can retrieve the churn dataset from the `C50` package:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以从`C50`包中检索客户流失数据集：
- en: '[PRE0]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Use `str` to read the structure of the dataset:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`str`读取数据集的结构：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can remove the `state`, `area_code`, and `account_length` attributes, which
    are not appropriate for classification features:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以移除`state`、`area_code`和`account_length`属性，这些属性不适合作为分类特征：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, split 70 percent of the data into the training dataset and 30 percent
    of the data into the testing dataset:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将70%的数据分割为训练集，30%的数据分割为测试集：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Lastly, use `dim` to explore the dimensions of both the training and testing
    datasets:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`dim`来探索训练集和测试集的维度：
- en: '[PRE4]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we use the telecom churn dataset as our example data source.
    The dataset contains 20 variables with 3,333 observations. We would like to build
    a classification model to predict whether a customer will churn, which is very
    important to the telecom company as the cost of acquiring a new customer is significantly
    more than retaining one.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们使用电信流失数据集作为示例数据源。该数据集包含20个变量和3,333个观测值。我们希望构建一个分类模型来预测客户是否会流失，这对电信公司来说非常重要，因为获取新客户的成本显著高于保留现有客户。
- en: Before building the classification model, we need to preprocess the data first.
    Thus, we load the churn data from the `C50` package into the R session with the
    variable name as `churn`. As we determined that attributes such as `state`, `area_code`,
    and `account_length` are not useful features for building the classification model,
    we remove these attributes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分类模型之前，我们首先需要预处理数据。因此，我们使用变量名`churn`将流失数据从`C50`包加载到R会话中。由于我们确定属性如`state`、`area_code`和`account_length`对于构建分类模型不是有用的特征，我们移除了这些属性。
- en: After preprocessing the data, we split it into training and testing datasets,
    respectively. We then use a sample function to randomly generate a sequence containing
    70 percent of the training dataset and 30 percent of the testing dataset with
    a size equal to the number of observations. Then, we use a generated sequence
    to split the churn dataset into the training dataset, `trainset`, and the testing
    dataset, `testset`. Lastly, by using the `dim` function, we found that 2,315 out
    of the 3,333 observations are categorized into the training dataset, `trainset`,
    while the other 1,018 are categorized into the testing dataset, `testset`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理数据后，我们将数据分别分割为训练集和测试集。然后，我们使用一个样本函数随机生成一个包含70%训练集和30%测试集的序列，其大小等于观测值的数量。然后，我们使用生成的序列将流失数据集分割为训练集`trainset`和测试集`testset`。最后，通过使用`dim`函数，我们发现3,333个观测值中有2,315个被归类到训练集`trainset`，而其他1,018个被归类到测试集`testset`。
- en: There's more...
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'You can combine the split process of the training and testing datasets into
    the `split.data` function. Therefore, you can easily split the data into the two
    datasets by calling this function and specifying the proportion and seed in the
    parameters:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将训练集和测试集的分割过程合并到`split.data`函数中。因此，您可以通过调用此函数并指定参数中的比例和种子来轻松地将数据分割成两个数据集：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Building a classification model with recursive partitioning trees
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归分割树构建分类模型
- en: A classification tree uses a split condition to predict class labels based on
    one or multiple input variables. The classification process starts from the root
    node of the tree; at each node, the process will check whether the input value
    should recursively continue to the right or left sub-branch according to the split
    condition, and stops when meeting any leaf (terminal) nodes of the decision tree.
    In this recipe, we will introduce how to apply a recursive partitioning tree on
    the customer churn dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树使用分割条件根据一个或多个输入变量预测类标签。分类过程从树的根节点开始；在每个节点，过程将检查输入值是否应该根据分割条件递归地继续到右子分支或左子分支，并在遇到决策树的任何叶（终端）节点时停止。在本食谱中，我们将介绍如何将递归分割树应用于客户流失数据集。
- en: Getting ready
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by splitting the churn dataset
    into the training dataset (`trainset`) and testing dataset (`testset`), and each
    dataset should contain exactly 17 variables.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成之前的食谱，将流失数据集分割为训练集（`trainset`）和测试集（`testset`），并且每个数据集应恰好包含17个变量。
- en: How to do it...
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to split the churn dataset into training and testing
    datasets:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤将流失数据集分割为训练集和测试集：
- en: 'Load the `rpart` package:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`rpart`包：
- en: '[PRE6]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Use the `rpart` function to build a classification tree model:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`rpart`函数构建分类树模型：
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Type `churn.rp` to retrieve the node detail of the classification tree:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入`churn.rp`以检索分类树的节点细节：
- en: '[PRE8]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, use the `printcp` function to examine the complexity parameter:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`printcp`函数来检查复杂度参数：
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, use the `plotcp` function to plot the cost complexity parameters:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`plotcp`函数来绘制成本复杂度参数：
- en: '[PRE10]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![How to do it...](img/00101.jpeg)'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00101.jpeg)'
- en: 'Figure 1: The cost complexity parameter plot'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1：成本复杂度参数图
- en: 'Lastly, use the `summary` function to examine the built model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`summary`函数来检查构建的模型：
- en: '[PRE11]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In this recipe, we use a recursive partitioning tree from the `rpart` package
    to build a tree-based classification model. The recursive portioning tree includes
    two processes: recursion and partitioning. During the process of decision induction,
    we have to consider a statistic evaluation question (or simply a yes/no question)
    to partition the data into different partitions in accordance with the assessment
    result. Then, as we have determined the child node, we can repeatedly perform
    the splitting until the stop criteria is satisfied.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用`rpart`包中的递归分割树来构建基于树的分类模型。递归分割树包括两个过程：递归和分割。在决策诱导的过程中，我们必须考虑一个统计评估问题（或者简单地说是一个是/否问题），根据评估结果将数据分割成不同的分区。然后，当我们确定了子节点后，我们可以重复执行分割，直到满足停止标准。
- en: 'For example, the data (shown in the following figure) in the root node can
    be partitioned into two groups with regard to the question of whether **f1** is
    smaller than **X**. If so, the data is divided into the left-hand side. Otherwise,
    it is split into the right-hand side. Then, we can continue to partition the left-hand
    side data with the question of whether **f2** is smaller than **Y**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，根节点中的数据（如下所示）可以根据**f1**是否小于**X**的问题分为两组。如果是，数据被分割到左侧。否则，它被分割到右侧。然后，我们可以继续使用**f2**是否小于**Y**的问题来分割左侧数据：
- en: '![How it works...](img/00102.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/00102.jpeg)'
- en: 'Figure 2: Recursive partioning tree'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：递归分割树
- en: In the first step, we load the `rpart` package with the `library` function.
    Next, we build a classification model using the `churn` variable as a classification
    category (class label) and the remaining variables as input features.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们使用`library`函数加载`rpart`包。接下来，我们使用`churn`变量作为分类类别（类别标签）和剩余变量作为输入特征来构建分类模型。
- en: After the model is built, you can type the variable name of the built model,
    `churn.rp`, to display the tree node details. In the printed node detail, `n`
    indicates the sample size, `loss` indicates the misclassification cost, `yval`
    stands for the classified membership (`no` or `yes`, in this case), and `yprob`
    stands for the probabilities of two classes (the left value refers to the probability
    reaching label `no`, and the right value refers to the probability reaching label,
    `yes`).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完成后，你可以输入构建的模型变量名`churn.rp`来显示树节点细节。在打印的节点细节中，`n`表示样本大小，`loss`表示误分类成本，`yval`表示分类成员（在这种情况下为`no`或`yes`），而`yprob`表示两个类的概率（左值表示达到标签`no`的概率，右值表示达到标签`yes`的概率）。
- en: Then, we use the `printcp` function to print the complexity parameters of the
    built tree model. From the output of `printcp`, one should find the value of CP,
    a complexity parameter, which serves as a penalty to control the size of the tree.
    In short, the greater the CP value, the fewer the number of splits there are (`nsplit`).
    The output value (the `rel` error) represents the average deviance of the current
    tree divided by the average deviance of the null tree. A `xerror` value represents
    the relative error estimated by a 10-fold classification. `xstd` stands for the
    standard error of the relative error.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`printcp`函数来打印构建的树模型的复杂度参数。从`printcp`的输出中，应该找到一个复杂度参数CP的值，它作为惩罚来控制树的大小。简而言之，CP值越大，分割的数量（`nsplit`）就越少。输出值（`rel`误差）表示当前树的平均偏差除以空树的平均偏差。`xerror`值表示由10折分类估计的相对误差。`xstd`表示相对误差的标准误差。
- en: To make the **CP** (**cost complexity parameter**) table more readable, we use
    `plotcp` to generate an information graphic of the CP table. As per the screenshot
    (step 5), the x-axis at the bottom illustrates the `cp` value, the y-axis illustrates
    the relative error, and the upper x-axis displays the size of the tree. The dotted
    line indicates the upper limit of a standard deviation. From the screenshot, we
    can determine that minimum cross-validation error occurs when the tree is at a
    size of 12.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使**CP**（**成本复杂度参数**）表更易于阅读，我们使用`plotcp`生成CP表的信息图形。根据截图（步骤5），底部x轴表示`cp`值，y轴表示相对误差，上x轴显示树的大小。虚线表示标准差的上线。从截图可以确定，当树的大小为12时，最小交叉验证误差发生。
- en: We can also use the `summary` function to display the function call, complexity
    parameter table for the fitted tree model, variable importance, which helps identify
    the most important variable for the tree classification (summing up to 100), and
    detailed information of each node.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`summary`函数显示函数调用、拟合树模型的复杂度参数表、变量重要性，这有助于识别对树分类最重要的变量（总和为100），以及每个节点的详细信息。
- en: The advantage of using the decision tree is that it is very flexible and easy
    to interpret. It works on both classification and regression problems, and more;
    it is nonparametric. Therefore, one does not have to worry about whether the data
    is linear separable. As for the disadvantage of using the decision tree, it is
    that it tends to be biased and over-fitted. However, you can conquer the bias
    problem through the use of a conditional inference tree, and solve the problem
    of over-fitting through a random forest method or tree pruning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树的优点在于它非常灵活且易于解释。它适用于分类和回归问题，以及更多；它是非参数的。因此，无需担心数据是否线性可分。至于使用决策树的缺点，它倾向于有偏差且过拟合。然而，你可以通过使用条件推断树来克服偏差问题，并通过随机森林方法或树剪枝来解决过拟合问题。
- en: See also
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information about the `rpart`, `printcp`, and `summary` functions,
    please use the `help` function:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`rpart`、`printcp`和`summary`函数的更多信息，请使用`help`函数：
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`C50` is another package that provides a decision tree and a rule-based model.
    If you are interested in the package, you may refer to the document at [http://cran.r-project.org/web/packages/C50/C50.pdf](http://cran.r-project.org/web/packages/C50/C50.pdf).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C50`是另一个提供决策树和基于规则的模型的包。如果你对这个包感兴趣，可以参考[http://cran.r-project.org/web/packages/C50/C50.pdf](http://cran.r-project.org/web/packages/C50/C50.pdf)文档。'
- en: Visualizing a recursive partitioning tree
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化递归分割树
- en: From the last recipe, we learned how to print the classification tree in a text
    format. To make the tree more readable, we can use the `plot` function to obtain
    the graphical display of a built classification tree.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从最后一个配方中，我们学习了如何以文本格式打印分类树。为了使树更易于阅读，我们可以使用`plot`函数来获取已建分类树的图形显示。
- en: Getting ready
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: One needs to have the previous recipe completed by generating a classification
    model, and assign the model into the `churn.rp` variable.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 需要完成上一个配方，通过生成分类模型，并将模型分配给`churn.rp`变量。
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to visualize the classification tree:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以可视化分类树：
- en: 'Use the `plot` function and the `text` function to plot the classification
    tree:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot`函数和`text`函数绘制分类树：
- en: '[PRE13]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How to do it...](img/00103.jpeg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00103.jpeg)'
- en: 'Figure 3: The graphical display of a classification tree'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3：分类树的图形显示
- en: 'You can also specify the `uniform`, `branch`, and `margin` parameter to adjust
    the layout:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以指定`uniform`、`branch`和`margin`参数来调整布局：
- en: '[PRE14]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![How to do it...](img/00104.jpeg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00104.jpeg)'
- en: 'Figure 4: Adjust the layout of the classification tree'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4：调整分类树的布局
- en: How it works...
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Here, we demonstrate how to use the `plot` function to graphically display a
    classification tree. The `plot` function can simply visualize the classification
    tree, and you can then use the `text` function to add text to the plot.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们演示如何使用`plot`函数图形化显示分类树。`plot`函数可以简单地可视化分类树，然后你可以使用`text`函数向图中添加文本。
- en: In *Figure 3*, we assign margin = 0.1 as a parameter to add extra white space
    around the border to prevent the displayed text being truncated by the margin.
    It shows that the length of the branches displays the relative magnitude of the
    drop in deviance. We then use the text function to add labels for the nodes and
    branches. By default, the text function will add a split condition on each split,
    and add a category label in each terminal node. In order to add extra information
    on the tree plot, we set the parameter as all equal to `TRUE` to add a label to
    all the nodes. In addition to this, we add a parameter by specifying `use.n =
    TRUE` to add extra information, which shows that the actual number of observations
    fall into two different categories (no and yes).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3*中，我们将`margin`参数设置为0.1，以在边界周围添加额外的空白，防止显示的文本被边界截断。它显示分支的长度表示偏差下降的相对幅度。然后我们使用文本函数为节点和分支添加标签。默认情况下，文本函数将在每个分割处添加一个分割条件，并在每个终端节点添加一个类别标签。为了在树图中添加更多信息，我们将参数设置为所有等于`TRUE`以添加所有节点的标签。此外，我们通过指定`use.n
    = TRUE`添加一个参数，以添加额外信息，这表明实际观测数分为两个不同的类别（是和否）。
- en: In *Figure 4*, we set the option branch to 0.6 to add a shoulder to each plotted
    branch. In addition to this, in order to display branches of an equal length rather
    than relative magnitude of the drop in deviance, we set the option uniform to
    `TRUE`. As a result, *Figure 4* shows a classification tree with short shoulders
    and branches of equal length.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4*中，我们将分支选项设置为0.6，为每个绘制的分支添加一个肩部。此外，为了显示等长的分支而不是偏差下降的相对幅度，我们将选项`uniform`设置为`TRUE`。因此，*图4*显示了一个具有短肩部和等长分支的分类树。
- en: See also
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: You may use `?plot.rpart` to read more about the plotting of the classification
    tree. This document also includes information on how to specify the parameters,
    `uniform`, `branch`, `compress`, `nspace`, `margin`, and `minbranch`, to adjust
    the layout of the classification tree.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用`?plot.rpart`来了解更多关于分类树绘制的相关信息。此文档还包括如何指定参数`uniform`、`branch`、`compress`、`nspace`、`margin`和`minbranch`以调整分类树布局的信息。
- en: Measuring the prediction performance of a recursive partitioning tree
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量递归分割树的预测性能
- en: Since we have built a classification tree in the previous recipes, we can use
    it to predict the category (class label) of new observations. Before making a
    prediction, we first validate the prediction power of the classification tree,
    which can be done by generating a classification table on the testing dataset.
    In this recipe, we will introduce how to generate a predicted label versus a real
    label table with the `predict` function and the `table` function, and explain
    how to generate a confusion matrix to measure the performance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在之前的配方中构建了一个分类树，我们可以用它来预测新观测的类别（类别标签）。在做出预测之前，我们首先验证分类树的预测能力，这可以通过在测试数据集上生成分类表来完成。在本配方中，我们将介绍如何使用`predict`函数和`table`函数生成预测标签与真实标签表，并解释如何生成混淆矩阵以衡量性能。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the previous recipe completed by generating the classification
    model, `churn.rp`. In addition to this, you have to prepare the training dataset,
    `trainset`, and the testing dataset, `testset`, generated in the first recipe
    of this chapter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成之前的配方，生成分类模型`churn.rp`。此外，您还需要准备训练数据集`trainset`和测试数据集`testset`，这些数据集是在本章第一节的第一个配方中生成的。
- en: How to do it...
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to validate the prediction performance of a classification
    tree:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以验证分类树的预测性能：
- en: 'You can use the `predict` function to generate a predicted label of testing
    the dataset:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`predict`函数为测试数据集生成预测标签：
- en: '[PRE15]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use the `table` function to generate a classification table for the testing
    dataset:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`table`函数为测试数据集生成分类表：
- en: '[PRE16]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'One can further generate a confusion matrix using the `confusionMatrix` function
    provided in the `caret` package:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以进一步使用`caret`包中提供的`confusionMatrix`函数生成混淆矩阵：
- en: '[PRE17]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works...
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we use a `predict` function and built up classification model,
    `churn.rp`, to predict the possible class labels of the testing dataset, `testset`.
    The predicted categories (class labels) are coded as either no or yes. Then, we
    use the `table` function to generate a classification table on the testing dataset.
    From the table, we discover that there are 859 correctly predicted as no, while
    18 are misclassified as yes. 100 of the yes predictions are correctly predicted,
    but 41 observations are misclassified into no. Further, we use the `confusionMatrix`
    function from the `caret` package to produce a measurement of the classification
    model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用 `predict` 函数和构建的分类模型 `churn.rp` 来预测测试数据集 `testset` 的可能类别标签。预测的类别（类别标签）编码为是或否。然后，我们使用
    `table` 函数在测试数据集上生成一个分类表。从表中，我们发现 859 个被正确预测为否，而 18 个被错误分类为是。100 个是的预测被正确预测，但有
    41 个观测值被错误分类为否。进一步，我们使用来自 `caret` 包的 `confusionMatrix` 函数来产生分类模型的测量值。
- en: See also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: You may use `?confusionMatrix` to read more about the performance measurement
    using the confusion matrix
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 `?confusionMatrix` 了解更多关于使用混淆矩阵进行性能测量的信息
- en: For those who are interested in the definition output by the confusion matrix,
    please refer to the Wikipedia entry, **Confusion_matrix** ([http://en.wikipedia.org/wiki/Confusion_matrix](http://en.wikipedia.org/wiki/Confusion_matrix))
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些对混淆矩阵定义输出感兴趣的人，请参阅维基百科条目，**混淆矩阵** ([http://en.wikipedia.org/wiki/Confusion_matrix](http://en.wikipedia.org/wiki/Confusion_matrix))
- en: Pruning a recursive partitioning tree
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修剪递归分割树
- en: In previous recipes, we have built a complex decision tree for the churn dataset.
    However, sometimes we have to remove sections that are not powerful in classifying
    instances to avoid over-fitting, and to improve the prediction accuracy. Therefore,
    in this recipe, we introduce the cost complexity pruning method to prune the classification
    tree.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的菜谱中，我们已经为 churn 数据集构建了一个复杂的决策树。然而，有时我们必须移除在分类实例中不起作用的部分，以避免过拟合，并提高预测精度。因此，在这个菜谱中，我们介绍了成本复杂度修剪方法来修剪分类树。
- en: Getting ready
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the previous recipe completed by generating a classification
    model, and assign the model into the `churn.rp` variable.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要完成之前的菜谱，生成一个分类模型，并将模型分配给 `churn.rp` 变量。
- en: How to do it...
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to prune the classification tree:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来修剪分类树：
- en: 'Find the minimum cross-validation error of the classification tree model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到分类树模型的最低交叉验证误差：
- en: '[PRE18]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Locate the record with the minimum cross-validation errors:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位具有最低交叉验证误差的记录：
- en: '[PRE19]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Get the cost complexity parameter of the record with the minimum cross-validation
    errors:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取具有最低交叉验证误差的记录的成本复杂度参数：
- en: '[PRE20]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Prune the tree by setting the `cp` parameter to the CP value of the record
    with minimum cross-validation errors:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 `cp` 参数设置为具有最低交叉验证误差的记录的 CP 值来修剪树：
- en: '[PRE21]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Visualize the classification tree by using the `plot` and `text` function:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plot` 和 `text` 函数可视化分类树：
- en: '[PRE22]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![How to do it...](img/00105.jpeg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00105.jpeg)'
- en: 'Figure 5: The pruned classification tree'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5：修剪后的分类树
- en: 'Next, you can generate a classification table based on the pruned classification
    tree model:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以根据修剪后的分类树模型生成一个分类表：
- en: '[PRE23]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lastly, you can generate a confusion matrix based on the classification table:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以根据分类表生成一个混淆矩阵：
- en: '[PRE24]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we discussed pruning a classification tree to avoid over-fitting
    and producing a more robust classification model. We first located the record
    with the minimum cross-validation errors within the `cptable`, and we then extracted
    the CP of the record and assigned the value to `churn.cp`. Next, we used the `prune`
    function to prune the classification tree with `churn.cp` as the parameter. Then,
    by using the `plot` function, we graphically displayed the pruned classification
    tree. From *Figure 5*, it is clear that the split of the tree is less than the
    original classification tree (*Figure 3*). Lastly, we produced a classification
    table and used the confusion matrix to validate the performance of the pruned
    tree. The result shows that the accuracy (0.9411) is slightly lower than the original
    model (0.942), and also suggests that the pruned tree may not perform better than
    the original classification tree as we have pruned some split conditions (Still,
    one should examine the change in sensitivity and specificity). However, the pruned
    tree model is more robust as it removes some split conditions that may lead to
    over-fitting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们讨论了剪枝分类树以避免过拟合并产生更健壮的分类模型。我们首先在`cptable`中找到具有最小交叉验证错误的记录，然后提取该记录的CP并将其值分配给`churn.cp`。接下来，我们使用`prune`函数以`churn.cp`作为参数剪枝分类树。然后，通过使用`plot`函数，我们图形化地显示了剪枝后的分类树。从*图5*中可以看出，树的分割小于原始分类树（*图3*）。最后，我们生成了一个分类表，并使用混淆矩阵来验证剪枝树的性能。结果显示，准确率（0.9411）略低于原始模型（0.942），同时也表明剪枝树可能不如原始分类树表现好，因为我们已经剪掉了一些分割条件（尽管如此，人们应该检查敏感性和特异性的变化）。然而，剪枝后的树模型更健壮，因为它去除了可能导致过拟合的一些分割条件。
- en: See also
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'For those who would like to know more about cost complexity pruning, please
    refer to the Wikipedia article for **Pruning (decision_trees)**: [http://en.wikipedia.org/wiki/Pruning_(decision_trees](http://en.wikipedia.org/wiki/Pruning_(decision_trees)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些想了解更多关于成本复杂度剪枝的人来说，请参阅维基百科关于**剪枝（决策树**）的文章：[http://en.wikipedia.org/wiki/Pruning_(decision_trees](http://en.wikipedia.org/wiki/Pruning_(decision_trees)
- en: Building a classification model with a conditional inference tree
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用条件推理树构建分类模型
- en: In addition to traditional decision trees (`rpart`), conditional inference trees
    (`ctree`) are another popular tree-based classification method. Similar to traditional
    decision trees, conditional inference trees also recursively partition the data
    by performing a univariate split on the dependent variable. However, what makes
    conditional inference trees different from traditional decision trees is that
    conditional inference trees adapt the significance test procedures to select variables
    rather than selecting variables by maximizing information measures (`rpart` employs
    a Gini coefficient). In this recipe, we will introduce how to adapt a conditional
    inference tree to build a classification model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的决策树（`rpart`）之外，条件推理树（`ctree`）是另一种流行的基于树的分类方法。与传统决策树类似，条件推理树也通过对因变量进行单变量分割来递归地分割数据。然而，使条件推理树与传统决策树不同的地方在于，条件推理树将显著性检验程序适应于选择变量，而不是通过最大化信息度量（`rpart`使用基尼系数）来选择变量。在本食谱中，我们将介绍如何适应条件推理树以构建分类模型。
- en: Getting ready
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the first recipe completed by generating the training dataset,
    `trainset`, and the testing dataset, `testset`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成第一个步骤，通过生成训练数据集`trainset`和测试数据集`testset`。
- en: How to do it...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to build the conditional inference tree:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以构建条件推理树：
- en: 'First, we use `ctree` from the `party` package to build the classification
    model:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用`party`包中的`ctree`构建分类模型：
- en: '[PRE25]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we examine the built tree model:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们检查构建的树模型：
- en: '[PRE26]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we used a conditional inference tree to build a classification
    tree. The use of `ctree` is similar to `rpart`. Therefore, you can easily test
    the classification power using either a traditional decision tree or a conditional
    inference tree while confronting classification problems. Next, we obtain the
    node details of the classification tree by examining the built model. Within the
    model, we discover that `ctree` provides information similar to a split condition,
    criterion (1 – p-value), statistics (test statistics), and weight (the case weight
    corresponding to the node). However, it does not offer as much information as
    `rpart` does through the use of the `summary` function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用条件推理树构建了一个分类树。`ctree`的使用与`rpart`类似。因此，在面临分类问题时，你可以轻松地使用传统的决策树或条件推理树来测试分类能力。接下来，我们通过检查构建的模型来获取分类树的节点细节。在模型中，我们发现`ctree`提供的信息类似于分割条件、标准（1
    – p值）、统计（测试统计量）和权重（与节点对应的案例权重）。然而，它提供的信息不如`rpart`通过使用`summary`函数提供的信息多。
- en: See also
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You may use the `help` function to refer to the definition of **Binary** **Tree**
    **Class** and read more about the properties of binary trees:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用`help`函数查阅**二叉树类**的定义，并了解更多关于二叉树属性的信息：
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Visualizing a conditional inference tree
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化条件推理树
- en: Similar to `rpart`, the `party` package also provides a visualization method
    for users to plot conditional inference trees. In the following recipe, we will
    introduce how to use the `plot` function to visualize conditional inference trees.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 与`rpart`类似，`party`包也为用户提供了一种可视化条件推理树的方法。在下面的菜谱中，我们将介绍如何使用`plot`函数来可视化条件推理树。
- en: Getting ready
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the first recipe completed by generating the conditional inference
    tree model, `ctree.model`. In addition to this, you need to have both, `trainset`
    and `testset`, loaded in an R session.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要完成第一个菜谱，生成条件推理树模型`ctree.model`。此外，你还需要在R会话中加载`trainset`和`testset`。
- en: How to do it...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to visualize the conditional inference tree:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以可视化条件推理树：
- en: 'Use the `plot` function to plot `ctree.model` built in the last recipe:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot`函数绘制在上一菜谱中构建的`ctree.model`：
- en: '[PRE28]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![How to do it...](img/00106.jpeg)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00106.jpeg)'
- en: 'Figure 6: A conditional inference tree of churn data'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6：客户流失数据的条件推理树
- en: 'To obtain a simple conditional inference tree, one can reduce the built model
    with less input features, and redraw the classification tree:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获得一个简单的条件推理树，可以通过减少输入特征来简化构建的模型，并重新绘制分类树：
- en: '[PRE29]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![How to do it...](img/00107.jpeg)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00107.jpeg)'
- en: 'Figure 7: A conditional inference tree using the total_day_charge variable
    as only split condition'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7：使用total_day_charge变量作为唯一分割条件的条件推理树
- en: How it works...
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: To visualize the node detail of the conditional inference tree, we can apply
    the `plot` function on a built classification model. The output figure reveals
    that every intermediate node shows the dependent variable name and the p-value.
    The split condition is displayed on the left and right branches. The terminal
    nodes show the number of categorized observations, *n*, and the probability of
    a class label of either 0 or 1.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化条件推理树的节点细节，我们可以对构建的分类模型应用`plot`函数。输出图显示，每个中间节点都显示了因变量名称和p值。分割条件显示在左右分支上。终端节点显示了分类观察数的数量，*n*，以及类别标签为0或1的概率。
- en: Taking *Figure 7* as an example, we first build a classification model using
    `total_day_charge` as the only feature and `churn` as the class label. The built
    classification tree shows that when `total_day_charge` is above 48.18, the lighter
    gray area is greater than the darker gray in node 9, which indicates that the
    customer with a day charge of over 48.18 has a greater likelihood to churn (label
    = yes).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以*图7*为例，我们首先使用`total_day_charge`作为唯一特征，`churn`作为类别标签构建一个分类模型。构建的分类树显示，当`total_day_charge`超过48.18时，节点9中较浅的灰色区域大于较深的灰色区域，这表明日收费超过48.18的客户更有可能流失（标签=是）。
- en: See also
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'The visualization of the conditional inference tree comes from the `plot.BinaryTree`
    function. If you are interested in adjusting the layout of the classification
    tree, you may use the `help` function to read the following document:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件推理树的可视化来自`plot.BinaryTree`函数。如果你对调整分类树的布局感兴趣，可以使用`help`函数阅读以下文档：
- en: '[PRE30]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Measuring the prediction performance of a conditional inference tree
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量条件推理树的预测性能
- en: After building a conditional inference tree as a classification model, we can
    use the `treeresponse` and `predict` functions to predict categories of the testing
    dataset, `testset`, and further validate the prediction power with a classification
    table and a confusion matrix.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建条件推理树作为分类模型之后，我们可以使用`treeresponse`和`predict`函数来预测测试数据集`testset`的类别，并进一步通过分类表和混淆矩阵验证预测能力。
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the previous recipe completed by generating the conditional
    inference tree model, `ctree.model`. In addition to this, you need to have both
    `trainset` and `testset` loaded in an R session.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成上一个菜谱，生成条件推理树模型`ctree.model`。此外，您还需要在R会话中加载`trainset`和`testset`。
- en: How to do it...
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to measure the prediction performance of a conditional
    inference tree:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以测量条件推理树的预测性能：
- en: 'You can use the `predict` function to predict the category of the testing dataset,
    `testset`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`predict`函数来预测测试数据集`testset`的类别：
- en: '[PRE31]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Furthermore, you can use `confusionMatrix` from the caret package to generate
    the performance measurements of the prediction result:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您可以使用来自caret包的`confusionMatrix`函数来生成预测结果的性能度量：
- en: '[PRE32]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can also use the `treeresponse` function, which will tell you the list
    of class probabilities:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以使用`treeresponse`函数，它将告诉您类概率列表：
- en: '[PRE33]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we first demonstrate that one can use the `prediction` function
    to predict the category (class label) of the testing dataset, `testset`, and then
    employ a `table` function to generate a classification table. Next, you can use
    the `confusionMatrix` function built into the caret package to determine the performance
    measurements.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们首先演示了如何使用`prediction`函数预测测试数据集`testset`的类别（类标签），然后使用`table`函数生成分类表。接下来，您可以使用内置在caret包中的`confusionMatrix`函数来确定性能度量。
- en: In addition to the `predict` function, `treeresponse` is also capable of estimating
    the class probability, which will often classify labels with a higher probability.
    In this example, we demonstrated how to obtain the estimated class probability
    using the top five records of the testing dataset, `testset`. The `treeresponse`
    function returns a list of five probabilities. You can use the list to determine
    the label of instance.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`predict`函数外，`treeresponse`还可以估计类概率，这通常会将具有更高概率的标签分类。在本例中，我们演示了如何使用测试数据集`testset`的前五条记录来获取估计的类概率。`treeresponse`函数返回一个包含五个概率的列表。您可以使用该列表来确定实例的标签。
- en: See also
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: For the `predict` function, you can specify the type as `response`, `prob`,
    or `node`. If you specify the type as `prob` when using the `predict` function
    (for example, `predict(… type="prob")`), you will get exactly the same result
    as what `treeresponse` returns.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`predict`函数，您可以指定类型为`response`、`prob`或`node`。如果您在调用`predict`函数时指定类型为`prob`（例如，`predict(…
    type="prob")`），您将得到与`treeresponse`返回的完全相同的结果。
- en: Classifying data with the k-nearest neighbor classifier
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k近邻分类器对数据进行分类
- en: '**K-nearest neighbor** (**knn**) is a nonparametric lazy learning method. From
    a nonparametric view, it does not make any assumptions about data distribution.
    In terms of lazy learning, it does not require an explicit learning phase for
    generalization. The following recipe will introduce how to apply the k-nearest
    neighbor algorithm on the churn dataset.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**K近邻**（**knn**）是一种非参数的懒惰学习方法。从非参数的角度来看，它不对数据分布做出任何假设。在懒惰学习方面，它不需要一个显式的学习阶段来进行泛化。以下菜谱将介绍如何在流失数据集上应用k近邻算法。'
- en: Getting ready
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the previous recipe completed by generating the training and
    testing datasets.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成上一个菜谱，生成训练和测试数据集。
- en: How to do it...
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to classify the churn data with the k-nearest neighbor
    algorithm:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，使用k近邻算法对流失数据进行分类：
- en: 'First, one has to install the `class` package and have it loaded in an R session:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，必须安装`class`包并在R会话中加载它：
- en: '[PRE34]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Replace `yes` and `no` of the `voice_mail_plan` and `international_plan` attributes
    in both the training dataset and testing dataset to 1 and 0:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据集和测试数据集中`voice_mail_plan`和`international_plan`属性的`yes`和`no`替换为1和0：
- en: '[PRE35]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Use the `knn` classification method on the training dataset and the testing
    dataset:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集和测试数据集上使用 knn 分类方法：
- en: '[PRE36]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, you can use the `summary` function to retrieve the number of predicted
    labels:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用 `summary` 函数检索预测标签的数量：
- en: '[PRE37]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, you can generate the classification matrix using the `table` function:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用 `table` 函数生成分类矩阵：
- en: '[PRE38]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Lastly, you can generate a confusion matrix by using the `confusionMatrix`
    function:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用 `confusionMatrix` 函数生成混淆矩阵：
- en: '[PRE39]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How it works...
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: '**knn** trains all samples and classifies new instances based on a similarity
    (distance) measure. For example, the similarity measure can be formulated as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**knn** 通过训练所有样本并根据相似性（距离）度量对新实例进行分类。例如，相似性度量可以表示如下：'
- en: '**Euclidian Distance**: ![How it works...](img/00108.jpeg)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欧几里得距离**: ![如何工作...](img/00108.jpeg)'
- en: '**Manhattan Distance**:![How it works...](img/00109.jpeg)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿距离**: ![如何工作...](img/00109.jpeg)'
- en: In knn, a new instance is classified to a label (class) that is common among
    the k-nearest neighbors. If *k = 1*, then the new instance is assigned to the
    class where its nearest neighbor belongs. The only required input for the algorithm
    is k. If we give a small k input, it may lead to over-fitting. On the other hand,
    if we give a large k input, it may result in under-fitting. To choose a proper
    k-value, one can count on cross-validation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在 knn 中，一个新的实例被分类到一个标签（类别），这个标签在 k 个最近邻中是共同的。如果 *k = 1*，那么新的实例将被分配到其最近邻所属的类别。算法的唯一输入是
    k。如果我们给出小的 k 输入，可能会导致过拟合。另一方面，如果我们给出大的 k 输入，可能会导致欠拟合。为了选择合适的 k 值，可以依靠交叉验证。
- en: 'The advantages of knn are:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: knn 的优点是：
- en: The cost of the learning process is zero
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程没有成本
- en: It is nonparametric, which means that you do not have to make the assumption
    of data distribution
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是非参数的，这意味着你不需要对数据分布做出假设。
- en: You can classify any data whenever you can find similarity measures of given
    instances
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你能找到给定实例的相似性度量时，你可以对任何数据进行分类
- en: 'The main disadvantages of knn are:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: knn 的主要缺点是：
- en: It is hard to interpret the classified result.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释分类结果很困难。
- en: It is an expensive computation for a large dataset.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集来说，这是一个昂贵的计算。
- en: The performance relies on the number of dimensions. Therefore, for a high dimension
    problem, you should reduce the dimension first to increase the process performance.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能依赖于维数的数量。因此，对于高维问题，你应该首先降低维度以提高处理性能。
- en: The use of knn does not vary significantly from applying a tree-based algorithm
    mentioned in the previous recipes. However, while a tree-based algorithm may show
    you the decision tree model, the output produced by knn only reveals classification
    category factors. However, before building a classification model, one should
    replace the attribute with a string type to an integer since the k-nearest neighbor
    algorithm needs to calculate the distance between observations. Then, we build
    up a classification model by specifying *k=3*, which means choosing the three
    nearest neighbors. After the classification model is built, we can generate a
    classification table using predicted factors and the testing dataset label as
    the input. Lastly, we can generate a confusion matrix from the classification
    table. The confusion matrix output reveals an accuracy result of (0.8723), which
    suggests that both the tree-based methods mentioned in previous recipes outperform
    the accuracy of the k-nearest neighbor classification method in this case. Still,
    we cannot determine which model is better depending merely on accuracy, one should
    also examine the specificity and sensitivity from the output.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: knn 的使用与之前菜谱中提到的基于树的算法应用没有显著差异。然而，虽然基于树的算法可能会展示决策树模型，但 knn 生成的输出仅揭示分类类别因素。然而，在构建分类模型之前，应该将属性从字符串类型替换为整数，因为
    k 近邻算法需要计算观测之间的距离。然后，我们通过指定 *k=3* 来构建分类模型，这意味着选择最近的三个邻居。在分类模型构建完成后，我们可以使用预测因素和测试数据集标签作为输入生成分类表。最后，我们可以从分类表中生成混淆矩阵。混淆矩阵的输出显示准确率为
    (0.8723)，这表明之前菜谱中提到的基于树的算法在此情况下优于 k 近邻分类方法的准确率。尽管如此，我们无法仅根据准确率来确定哪个模型更好，还应该检查输出的特异性和敏感性。
- en: See also
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'There is another package named `kknn`, which provides a weighted k-nearest
    neighbor classification, regression, and clustering. You can learn more about
    the package by reading this document: [http://cran.r-project.org/web/packages/kknn/kknn.pdf](http://cran.r-project.org/web/packages/kknn/kknn.pdf).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另有一个名为`kknn`的包，它提供了加权k最近邻分类、回归和聚类。您可以通过阅读此文档了解更多关于该包的信息：[http://cran.r-project.org/web/packages/kknn/kknn.pdf](http://cran.r-project.org/web/packages/kknn/kknn.pdf)。
- en: Classifying data with logistic regression
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行数据分类
- en: Logistic regression is a form of probabilistic statistical classification model,
    which can be used to predict class labels based on one or more features. The classification
    is done by using the `logit` function to estimate the outcome probability. One
    can use logistic regression by specifying the family as a binomial while using
    the `glm` function. In this recipe, we will introduce how to classify data using
    logistic regression.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种概率统计分类模型，可以用来根据一个或多个特征预测类别标签。分类是通过使用`logit`函数来估计结果概率来完成的。可以通过指定家族为二项式并使用`glm`函数来使用逻辑回归。在本菜谱中，我们将介绍如何使用逻辑回归进行数据分类。
- en: Getting ready
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the first recipe by generating training and testing
    datasets.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成第一个菜谱，通过生成训练集和测试集。
- en: How to do it...
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to classify the churn data with logistic regression:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用逻辑回归对流失数据进行分类：
- en: 'With the specification of family as a binomial, we apply the `glm` function
    on the dataset, `trainset`, by using churn as a class label and the rest of the
    variables as input features:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在指定家族为二项式的情况下，我们通过使用`glm`函数对数据集`trainset`应用，以`churn`作为类别标签，其余变量作为输入特征：
- en: '[PRE40]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Use the `summary` function to obtain summary information of the built logistic
    regression model:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary`函数来获取构建的逻辑回归模型的摘要信息：
- en: '[PRE41]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we find that the built model contains insignificant variables, which
    would lead to misclassification. Therefore, we use significant variables only
    to train the classification model:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们发现构建的模型包含不显著的变量，这可能导致误分类。因此，我们只使用显著变量来训练分类模型：
- en: '[PRE42]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, you can then use a fitted model, `fit`, to predict the outcome of `testset`.
    You can also determine the class by judging whether the probability is above 0.5:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用拟合模型`fit`来预测`testset`的结果。您也可以通过判断概率是否高于0.5来确定类别：
- en: '[PRE43]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, the use of the `summary` function will show you the binary outcome count,
    and reveal whether the probability is above 0.5:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`summary`函数将显示二元结果计数，并揭示概率是否高于0.5：
- en: '[PRE44]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You can generate the counting statistics based on the testing dataset label
    and predicted result:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以根据测试数据集的标签和预测结果生成计数统计信息：
- en: '[PRE45]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You can turn the statistics of the previous step into a classification table,
    and then generate the confusion matrix:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以将上一步的统计信息转换为分类表，然后生成混淆矩阵：
- en: '[PRE46]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Logistic regression is very similar to linear regression; the main difference
    is that the dependent variable in linear regression is continuous, but the dependent
    variable in logistic regression is dichotomous (or nominal). The primary goal
    of logistic regression is to use logit to yield the probability of a nominal variable
    is related to the measurement variable. We can formulate logit in following equation:
    ln(P/(1-P)), where P is the probability that certain event occurs.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归与线性回归非常相似；主要区别在于线性回归中的因变量是连续的，而逻辑回归中的因变量是二元的（或名义的）。逻辑回归的主要目标是使用logit来得出名义变量与测量变量相关的概率。我们可以用以下方程表示logit：ln(P/(1-P))，其中P是某个事件发生的概率。
- en: The advantage of logistic regression is that it is easy to interpret, it directs
    model logistic probability, and provides a confidence interval for the result.
    Unlike the decision tree, which is hard to update the model, you can quickly update
    the classification model to incorporate new data in logistic regression. The main
    drawback of the algorithm is that it suffers from multicollinearity and, therefore,
    the explanatory variables must be linear independent. `glm` provides a generalized
    linear regression model, which enables specifying the model in the option family.
    If the family is specified to a binomial logistic, you can set the family as a
    binomial to classify the dependent variable of the category.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的优势在于它易于解释，它指导模型逻辑概率，并为结果提供置信区间。与难以更新模型的决策树不同，您可以在逻辑回归中快速更新分类模型以包含新数据。该算法的主要缺点是它受到多重共线性问题的影响，因此解释变量必须是线性独立的。`glm`
    提供了一个广义线性回归模型，允许在选项中指定模型。如果将家族指定为二项逻辑，则可以将家族设置为二项以对分类因变量进行分类。
- en: The classification process begins by generating a logistic regression model
    with the use of the training dataset by specifying `Churn` as the class label,
    the other variables as training features, and family set as binomial. We then
    use the `summary` function to generate the model's summary information. From the
    summary information, we may find some insignificant variables (p-values > 0.05),
    which may lead to misclassification. Therefore, we should consider only significant
    variables for the model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 分类过程从使用训练数据集生成逻辑回归模型开始，指定 `Churn` 作为类标签，其他变量作为训练特征，并将家族设置为二项。然后我们使用 `summary`
    函数生成模型的摘要信息。从摘要信息中，我们可能会发现一些不显著的变量（p 值 > 0.05），这可能导致误分类。因此，我们应该只考虑显著变量来构建模型。
- en: Next, we use the `fit` function to predict the categorical dependent variable
    of the testing dataset, `testset`. The `fit` function outputs the probability
    of a class label, with a result equal to 0.5 and below, suggesting that the predicted
    label does not match the label of the testing dataset, and a probability above
    0.5 indicates that the predicted label matches the label of the testing dataset.
    Further, we can use the `summary` function to obtain the statistics of whether
    the predicted label matches the label of the testing dataset. Lastly, in order
    to generate a confusion matrix, we first generate a classification table, and
    then use `confusionMatrix` to generate the performance measurement.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `fit` 函数来预测测试数据集 `testset` 的分类因变量。`fit` 函数输出一个类标签的概率，结果等于或低于 0.5 表示预测的标签与测试数据集的标签不匹配，而概率高于
    0.5 则表示预测的标签与测试数据集的标签匹配。此外，我们可以使用 `summary` 函数来获取预测标签是否与测试数据集标签匹配的统计信息。最后，为了生成混淆矩阵，我们首先生成一个分类表，然后使用
    `confusionMatrix` 生成性能度量。
- en: See also
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more information of how to use the `glm` function, please refer to [Chapter
    4](part0046_split_000.html#page "Chapter 4. Understanding Regression Analysis"),
    *Understanding Regression Analysis*, which covers how to interpret the output
    of the `glm` function
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关如何使用 `glm` 函数的更多信息，请参阅[第 4 章](part0046_split_000.html#page "第 4 章。理解回归分析")，*理解回归分析*，其中涵盖了如何解释
    `glm` 函数的输出。
- en: Classifying data with the Naïve Bayes classifier
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Naïve Bayes 分类器进行数据分类
- en: The Naïve Bayes classifier is also a probability-based classifier, which is
    based on applying the Bayes theorem with a strong independent assumption. In this
    recipe, we will introduce how to classify data with the Naïve Bayes classifier.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes 分类器也是一种基于概率的分类器，它基于应用贝叶斯定理并假设强独立性。在本食谱中，我们将介绍如何使用 Naïve Bayes 分类器对数据进行分类。
- en: Getting ready
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have the first recipe completed by generating training and testing
    datasets.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成第一个食谱，生成训练和测试数据集。
- en: How to do it...
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to classify the churn data with the Naïve Bayes
    classifier:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Naïve Bayes 分类器对流失数据进行分类的以下步骤：
- en: 'Load the `e1071` library and employ the `naiveBayes` function to build the
    classifier:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `e1071` 库并使用 `naiveBayes` 函数构建分类器：
- en: '[PRE47]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Type `classifier` to examine the function call, a-priori probability, and conditional
    probability:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `classifier` 以检查函数调用、先验概率和条件概率：
- en: '[PRE48]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, you can generate a classification table for the testing dataset:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以生成测试数据集的分类表：
- en: '[PRE49]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Lastly, you can generate a confusion matrix from the classification table:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以从分类表中生成混淆矩阵：
- en: '[PRE50]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How it works...
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'Naive Bayes assumes that features are conditionally independent, which the
    effect of a predictor(x) to class (c) is independent of the effect of other predictors
    to class(c). It computes the posterior probability, *P(c|x)*, as the following
    formula:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设特征在条件上是独立的，即预测变量（x）对类别（c）的影响独立于其他预测变量对类别（c）的影响。它计算后验概率 *P(c|x)*，如下公式所示：
- en: '![How it works...](img/00110.jpeg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00110.jpeg)'
- en: 'Where *P(x|c)* is called likelihood, *p(x)* is called the marginal likelihood,
    and *p(c)* is called the prior probability. If there are many predictors, we can
    formulate the posterior probability as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P(x|c)* 被称为似然，*p(x)* 被称为边缘似然，*p(c)* 被称为先验概率。如果有许多预测变量，我们可以将后验概率如下公式化：
- en: '![How it works...](img/00111.jpeg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00111.jpeg)'
- en: The advantage of Naïve Bayes is that it is relatively simple and straightforward
    to use. It is suitable when the training set is relative small, and may contain
    some noisy and missing data. Moreover, you can easily obtain the probability for
    a prediction. The drawbacks of Naïve Bayes are that it assumes that all features
    are independent and equally important, which is very unlikely in real-world cases.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的优势在于它相对简单且易于使用。当训练集相对较小，可能包含一些噪声和缺失数据时，它很适用。此外，你可以轻松地获得预测的概率。朴素贝叶斯的缺点在于它假设所有特征都是独立的且同等重要，这在现实世界中是非常不可能的。
- en: In this recipe, we use the Naïve Bayes classifier from the `e1071` package to
    build a classification model. First, we specify all the variables (excluding the
    `churn` class label) as the first input parameters, and specify the `churn` class
    label as the second parameter in the `naiveBayes` function call. Next, we assign
    the classification model into the variable classifier. Then, we print the variable
    classifier to obtain information, such as function call, A-priori probabilities,
    and conditional probabilities. We can also use the `predict` function to obtain
    the predicted outcome and the `table` function to retrieve the classification
    table of the testing dataset. Finally, we use a confusion matrix to calculate
    the performance measurement of the classification model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用来自 `e1071` 包的朴素贝叶斯分类器来构建分类模型。首先，我们将所有变量（不包括 `churn` 类标签）指定为第一个输入参数，并将
    `churn` 类标签指定为 `naiveBayes` 函数调用中的第二个参数。然后，我们将分类模型分配给变量分类器。接下来，我们打印变量分类器以获取信息，例如函数调用、先验概率和条件概率。我们还可以使用
    `predict` 函数获取预测结果，以及使用 `table` 函数检索测试数据集的分类表。最后，我们使用混淆矩阵来计算分类模型的性能度量。
- en: 'At last, we list a comparison table of all the mentioned algorithms in this
    chapter:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们列出本章中提到的所有算法的比较表：
- en: '| Algorithm | Advantage | Disadvantage |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 优点 | 缺点 |'
- en: '| --- | --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Recursive partitioning tree |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 递归分割树 |'
- en: Very flexible and easy to interpret
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常灵活且易于解释
- en: Works on both classification and regression problems
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于分类和回归问题
- en: Nonparametric
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Prone to bias and over-fitting
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易产生偏差和过拟合
- en: '|'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Conditional inference tree |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 条件推断树 |'
- en: Very flexible and easy to interpret
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常灵活且易于解释
- en: Works on both classification and regression problems
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于分类和回归问题
- en: Nonparametric
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数
- en: Less prone to bias than a recursive partitioning tree
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比递归分割树更不容易产生偏差
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Prone to over-fitting
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易过拟合
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| K-nearest neighbor classifier |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| K最近邻分类器 |'
- en: The cost of the learning process is zero
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程成本为零
- en: Nonparametric
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非参数方法
- en: You can classify any data whenever you can find similarity measures of any given
    instances
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你能找到任何给定实例的相似度度量时，你可以对任何数据进行分类
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Hard to interpret the classified result
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类结果难以解释
- en: Computation is expensive for a large dataset
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大数据集，计算成本高昂
- en: The performance relies on the number of dimensions
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能依赖于维度数量
- en: '|'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Logistic regression |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 |'
- en: Easy to interpret
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于解释
- en: Provides model logistic probability
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供模型逻辑概率
- en: Provides confidence interval
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供置信区间
- en: You can quickly update the classification model to incorporate new data
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以快速更新分类模型以包含新数据
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Suffers multicollinearity
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 患有多重共线性
- en: Does not handle the missing value of continuous variables
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法处理连续变量的缺失值
- en: Sensitive to extreme values of continuous variables
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对连续变量的极端值敏感
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Naïve Bayes |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 朴素贝叶斯 |'
- en: Relatively simple and straightforward to use
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对简单直观易用
- en: Suitable when the training set is relative small
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练集相对较小时适用
- en: Can deal with some noisy and missing data
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理一些噪声和缺失数据
- en: Can easily obtain the probability for a prediction
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以轻松获得预测的概率
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Assumes all features are independent and equally important, which is very unlikely
    in real-world cases
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设所有特征都是独立且同等重要的，这在现实世界中是非常不可能的
- en: Prone to bias when the number of training sets increase
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练集数量增加时，容易出现偏差
- en: '|'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: See also
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'To learn more about the Bayes theorem, you can refer to the following Wikipedia
    article: [http://en.wikipedia.org/wiki/Bayes''_theorem](http://en.wikipedia.org/wiki/Bayes''_theorem)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想了解更多关于贝叶斯定理的信息，您可以参考以下维基百科文章：[http://en.wikipedia.org/wiki/Bayes'_theorem](http://en.wikipedia.org/wiki/Bayes'_theorem)
