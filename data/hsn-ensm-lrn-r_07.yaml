- en: Chapter 7. The General Ensemble Technique
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 通用集成技术
- en: 'The previous four chapters have dealt with the ensembling techniques for decision
    trees. In each of the topics discussed in those chapters, the base learner was
    a decision tree and, consequently, we delved into the homogenous ensembling technique.
    In this chapter, we will demonstrate that the base learner can be any statistical
    or machine learning technique and their ensemble will lead to improved precision
    in predictions. An important requirement will be that the base learner should
    be better than a random guess. Through R programs, we will discuss and clarify
    the different possible cases in which ensembling will work. Voting is an important
    trait of the classifiers – we will state two different methods for this and illustrate
    them in the context of bagging and random forest ensemblers. The averaging technique
    is an ensembler for regression variables, which will follow the discussion of
    classification methods. The chapter will conclude with a detailed discussion of
    stacking methods, informally introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    The topic flow unfolds as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前四章讨论了决策树的集成技术。在这些章节中讨论的每个主题中，基础学习器都是决策树，因此我们深入研究了同质集成技术。在本章中，我们将展示基础学习器可以是任何统计或机器学习技术，并且它们的集成将提高预测的精度。一个重要要求是基础学习器应该比随机猜测更好。通过R程序，我们将讨论和阐明集成将有效的情况。投票是分类器的一个重要特性——我们将陈述两种不同的方法，并在bagging和随机森林集成器的情况下进行说明。平均技术是回归变量的集成器，它将遵循分类方法的讨论。本章将以对[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章. 集成技术简介")中非正式介绍的堆叠方法的详细讨论结束，*集成技术简介*。主题流程如下：
- en: Why does ensembling work?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成为什么有效？
- en: Ensembling by voting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过投票进行集成
- en: Ensembling by averaging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过平均进行集成
- en: Stack ensembles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 栈集成
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The libraries that will be used in this chapter are as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用以下库：
- en: '`rpart`'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: '`randomForest`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`randomForest`'
- en: Why does ensembling work?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成为什么有效？
- en: When using the bagging method, we combine the result of many decision trees
    and produce a single output/prediction by taking a majority count. Under a different
    sampling mechanism, the results had been combined to produce a single prediction
    for the random forests. Under a sequential error reduction method for decision
    trees, the boosting method also provides improved answers. Although we are dealing
    with uncertain data, which involves probabilities, we don't intend to have methodologies
    that give results out of a black box and behave without consistent solutions.
    A theory should explain the working and we need an assurance that the results
    will be consistent and there is no black magic about it. Arbitrary and uncertain
    answers are completely unwanted. In this section, we will look at how and why
    the ensembling solutions work, as well as scenarios where they will not work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用bagging方法时，我们将许多决策树的结果结合起来，通过多数计数来生成单个输出/预测。在不同的采样机制下，随机森林的结果被结合起来生成单个预测。在决策树的顺序误差减少方法下，提升方法也提供了改进的答案。尽管我们处理的是不确定数据，这涉及到概率，但我们不打算有那种给出黑盒结果且行为不一致的方法论。一个理论应该解释其工作原理，我们需要保证结果的一致性，并且对此没有神秘之处。任意和不确定的答案是完全不受欢迎的。在本节中，我们将探讨集成解决方案如何工作以及它们不工作的场景。
- en: Ensembling methods have strong mathematical and statistical underpinnings that
    explain why they give the solutions that they do. We will consider the classification
    problem first. We will begin with a simplified setup and assume that we have *T*
    classifiers that are independent of each other, and that the accuracy associated
    with each of them is the same as ![Why does ensembling work?](img/00304.jpeg).
    This is one of the simplest cases, and we will generalize the scenario later.
    Now, if we have *T* classifiers and each of them votes on observations such as
    +1 or -1, it begs the question, what will the overall accuracy be? Since the number
    of correct classifications of the *T* classifiers must outnumber the misclassifications,
    we would need at least ![Why does ensembling work?](img/00305.jpeg) classifiers
    to vote the correct outcome. Here, ![Why does ensembling work?](img/00306.jpeg)
    denotes the greatest integer that is less than the given fractional number. The
    majority classification is correct whenever ![Why does ensembling work?](img/00307.jpeg)
    or a higher number of classifiers vote for the correct class.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法有强大的数学和统计基础，解释了为什么它们能给出这样的解决方案。我们首先考虑分类问题。我们将从一个简化的设置开始，并假设我们有*T*个相互独立的分类器，并且每个分类器相关的准确率与![为什么集成工作？](img/00304.jpeg)相同。这是一个最简单的情况，我们稍后会推广这个场景。现在，如果我们有*T*个分类器，并且每个分类器对+1或-1等观察进行投票，这就引出了一个问题，整体准确率会是什么？由于*T*个分类器的正确分类数量必须超过错误分类数量，我们需要至少![为什么集成工作？](img/00305.jpeg)个分类器来投票正确的结果。在这里，![为什么集成工作？](img/00306.jpeg)表示小于给定分数的最大整数。只要![为什么集成工作？](img/00307.jpeg)或更多数量的分类器为正确的类别投票，多数分类就是正确的。
- en: To clarify, it is important to note that when we say a classifier has an accuracy
    *p*, we don't mean that the probability of the classifier marking the observation
    as +1 is *p*. Rather, what we mean here is that if the classifier makes 100 predictions,
    the predictions can be any combination of +1 and -1; 100*p predictions are correctly
    identified by the classifier. The accuracy is independent of the distribution
    of +1 and -1 in the population.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，重要的是要注意，当我们说一个分类器的准确率是*p*时，我们并不是指分类器将观察标记为+1的概率是*p*。相反，我们在这里的意思是，如果分类器做出100次预测，预测可以是+1和-1的任何组合；100*p*次预测被分类器正确识别。准确率与+1和-1在人群中的分布无关。
- en: 'Under this setup, the probability of the number of classifiers marking a correct
    observation follows a binomial distribution with *n = T* and a probability of
    *p*. Thus, the probability of the majority vote getting the correct prediction
    is as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，分类器标记正确观察的概率遵循参数为*n = T*和概率*p*的二项分布。因此，多数投票得到正确预测的概率如下：
- en: '![Why does ensembling work?](img/00308.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![为什么集成工作？](img/00308.jpeg)'
- en: 'Since we have mentioned that the classifier must be better than a random guess,
    we will need the classifier accuracy to be in excess of 0.5\. We will then increment
    the accuracy over multiple points and see how the increase in the number of classifiers
    impacts the probability of a majority vote:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经提到分类器必须比随机猜测更好，我们就需要分类器的准确率超过0.5。然后我们将逐步增加准确率，并观察分类器数量的增加如何影响多数投票的概率：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `seq` function sets up an odd-numbered sequence of the number of classifiers
    in the `classifiers R` numeric vector. The accuracy percentage varies from `0.55`
    to `0.85` in the `accuracy` vector. To kick off the proceedings, we set up an
    empty `plot` of sorts, with appropriate *x* and *y* axis labels. Now, for each
    accuracy value, we will calculate the probability of the majority vote for range
    `floor(classifiers[j]/2+1):classifiers[j]`. The `floor(./2+1)` ensures that we
    select the correct starting point. For example, if the number of classifiers is
    nine, then the value of `floor(./2+1)` is `5`. Furthermore, when we have nine
    classifiers, we need a minimum of five votes in favor of the event of interest.
    On the other hand, for an even number of classifiers (for example, eight) the
    value of `floor(./2+1)` is `5`. The `dbinom` function calculates the probability
    of that specific value for the given size and probability. Over the range of `floor(classifiers[j]/2+1):
    classifiers[j]`, it gives the probability of the majority vote, or the accuracy
    of the majority vote. The output of the preceding code is presented in *Figure
    1*. We can see from the result that as the number of classifiers increases (each
    with the same accuracy and better than the random guess), the accuracy of the
    majority voting also increases:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`seq`函数设置了一个奇数序列，表示`classifiers R`数值向量中分类器的数量。准确率百分比在`accuracy`向量中从`0.55`到`0.85`不等。为了启动这个过程，我们设置了一个带有适当的*x*和*y*轴标签的空`plot`。现在，对于每个准确率值，我们将计算范围`floor(classifiers[j]/2+1):classifiers[j]`内多数投票的概率。`floor(./2+1)`确保我们选择了正确的起点。例如，如果分类器的数量是九个，那么`floor(./2+1)`的值是`5`。此外，当我们有九个分类器时，我们需要至少五个赞成事件发生的投票。另一方面，对于偶数个分类器（例如，八个）`floor(./2+1)`的值也是`5`。`dbinom`函数计算给定大小和概率的特定值的概率。在`floor(classifiers[j]/2+1):
    classifiers[j]`的范围内，它给出了多数投票的概率，或者多数投票的准确率。前面代码的输出在*图1*中展示。我们可以从结果中看到，随着分类器数量的增加（每个分类器具有相同的准确率且优于随机猜测），多数投票的准确率也在增加：'
- en: '![Why does ensembling work?](img/00309.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![为什么集成方法有效？](img/00309.jpeg)'
- en: 'Figure 1: Why should ensembling work?'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：为什么集成方法应该有效？
- en: 'It would help us to see `Prob_MV` for one choice of the accuracy – for example,
    0.65\. We will run the loop with index `j` separately for `prob=0.65` and look
    at how the accuracy of the majority vote increases as the number of classifiers
    increases:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助我们看到某个准确率选择下的`Prob_MV`值——例如，0.65。我们将分别对`prob=0.65`运行循环，观察随着分类器数量的增加，多数投票的准确率是如何提高的：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Consequently, as the number of classifiers with equal accuracy increases, we
    can see that the accuracy of the majority vote also increases. Also, what is noteworthy
    here is that even though each of our classifiers had an accuracy of a mere `0.65`,
    the ensemble has way higher accuracy and almost becomes a perfect classifier.
    This is the main advantage of ensemble.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着具有相同准确率的分类器数量的增加，我们可以看到多数投票的准确率也在增加。值得注意的是，尽管我们每个分类器的准确率仅为`0.65`，但集成方法的准确率要高得多，几乎成为了一个完美的分类器。这就是集成方法的主要优势。
- en: 'Will ensembling help any sort of classifier? If we have classifiers whose accuracy
    is worse than the random guess and hence is less than `0.5`, then we will search
    in the same way that we did with the previous case. For a host of a number of
    classifiers and accuracies less than `0.5`, we will compute the accuracy of the
    majority vote classifier:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是否对任何类型的分类器都有帮助？如果我们有准确率低于随机猜测（即小于`0.5`）的分类器，那么我们将以与之前相同的方式进行搜索。对于许多准确率低于`0.5`的分类器，我们将计算多数投票分类器的准确率：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result of the preceding R program is shown in *Figure 2.* Now, the first
    observation should be that it does not matter whether the accuracy is closer to
    `0.5` or to `0`, the probability/accuracy of the majority vote classifier is on
    a decline and this adversely affects the performance. In every case, we see that
    the accuracy will eventually approach zero. The changes in the block of R code
    are the classifier sequence `seq(6,50,2)`, and the accuracy levels decrease from
    `0.45` to `0.05` in `seq(0.45,0.05,-0.05)`. Now, consider the case of accuracy
    being slightly less than `0.5`. For example, let's keep it to `0.4999`. Will we
    be lucky enough to see a performance improvement now?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 前面R程序的结果显示在 *图2* 中。现在，第一个观察结果是，无论准确率是接近 `0.5` 还是接近 `0`，多数投票分类器的概率/准确率都在下降，这不利于性能。在所有情况下，我们都看到准确率最终会接近零。R代码块中的变化是分类器序列
    `seq(6,50,2)`，准确率水平从 `0.45` 下降到 `0.05`，在 `seq(0.45,0.05,-0.05)` 中。现在，考虑准确率略小于
    `0.5` 的情况。例如，让我们将其保持在 `0.4999`。我们现在会幸运地看到性能改进吗？
- en: '![Why does ensembling work?](img/00310.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![为什么集成有效？](img/00310.jpeg)'
- en: 'Figure 2: Ensemble is not alchemy!'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：集成不是炼金术！
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Again, it turns out that we can't match the accuracy of a single classifier.
    Consequently, we have the important and crucial condition that the classifier
    must be better than a random guess. What about the random guess itself? It is
    not at all difficult to pretend that we have a host of classifiers which are all
    random guesses. If the performance of the ensemble improves with the random guesses,
    we don't typically have to build any of the statistical or machine learning techniques.
    Given a set of random guesses, we can always improve the accuracy. Let's check
    this out.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们发现我们无法匹配单个分类器的准确率。因此，我们有一个重要且关键的条件，即分类器必须比随机猜测更好。那么随机猜测本身呢？假装我们有一系列都是随机猜测的分类器并不困难。如果集成随着随机猜测的性能提高，我们通常不需要构建任何统计或机器学习技术。给定一组随机猜测，我们总能提高准确率。让我们来看看。
- en: 'There are two cases – an odd number of classifiers and an even number of classifiers
    – and we provide the program for both scenarios:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种情况——分类器数量为奇数和偶数——我们为这两种情况都提供了程序：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is interesting! An ensemble of random guesses remains the same irrespective
    of the number of classifiers. Here, there is neither any improvement nor deterioration.
    Consequently, for ensembling purposes, we always need classifiers that are better
    than random guesses.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣！无论分类器的数量如何，随机猜测的集成保持不变。在这里，既没有改进也没有恶化。因此，为了集成目的，我们总是需要比随机猜测更好的分类器。
- en: It is good to follow your intuition when it comes to understanding how ensembling
    works. We began with an oversimplified assumption that all models have the same
    accuracy, but such an assumption does not work well if we deal with models with
    varying accuracies. As a result, we need to consider cases in which we may have
    different accuracies for different classifiers. We will first consider a case
    where each classifier has an accuracy of higher than 0.5, or where each of them
    is better than a random guess. The approach to finding the accuracy of the majority
    vote is to evaluate the probabilities for each possible combination of the classifiers'
    outcomes. We consider the simpler case when the number of classifiers is an odd
    number.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解集成如何工作时，遵循你的直觉是很好的。我们从一个所有模型都具有相同准确率的过度简化假设开始，但如果我们处理具有不同准确率的模型，这样的假设就不适用了。因此，我们需要考虑可能对不同分类器有不同的准确率的情况。我们首先考虑每个分类器的准确率都高于
    0.5，或者每个分类器都比随机猜测更好的情况。找到多数投票准确率的方法是评估分类器结果的每种可能组合的概率。我们考虑分类器数量为奇数时的简单情况。
- en: Suppose we have *T* number of classifiers, and the accuracy of each classifier
    is ![Why does ensembling work?](img/00311.jpeg). Note that ![Why does ensembling
    work?](img/00312.jpeg), as these correspond to different measures.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有 *T* 个分类器，每个分类器的准确率如 ![为什么集成有效？](img/00311.jpeg) 所示。请注意 ![为什么集成有效？](img/00312.jpeg)，因为这些对应于不同的度量。
- en: 'The steps involved in evaluating the probability of a majority vote with unequal
    accuracies is given in the following steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 评估具有不等准确率的多数投票概率所涉及步骤如下：
- en: 'List all possible elementary events. If each classifier votes TRUE or FALSE
    for a given case, this means that it has two possible outcomes, and *T* number
    of classifiers. List the ![Why does ensembling work?](img/00313.jpeg) possible
    outcomes:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出所有可能的基本事件。如果每个分类器对一个给定案例投票为真或假，这意味着它有两种可能的结果，以及 *T* 个分类器。列出 ![为什么集成工作？](img/00313.jpeg)
    可能的结果：
- en: 'Example: If we have three classifiers, there would be eight possible cases,
    as follows:'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：如果我们有三个分类器，那么会有八种可能的情况，如下所示：
- en: '| Classifier 1 | Classifier 2 | Classifier 3 |'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 分类器 1 | 分类器 2 | 分类器 3 |'
- en: '| --- | --- | --- |'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| TRUE | TRUE | TRUE |'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 真 | 真 |'
- en: '| FALSE | TRUE | TRUE |'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 真 | 真 |'
- en: '| TRUE | FALSE | TRUE |'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 假 | 真 |'
- en: '| FALSE | FALSE | TRUE |'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 假 | 真 |'
- en: '| TRUE | TRUE | FALSE |'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 真 | 假 |'
- en: '| FALSE | TRUE | FALSE |'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 真 | 假 |'
- en: '| TRUE | FALSE | FALSE |'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 假 | 假 |'
- en: '| FALSE | FALSE | FALSE |'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 假 | 假 |'
- en: 'Compute the probability of each possible event. Since each classifier has a
    different accuracy, the probabilities would then be different for each possible
    outcome:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个可能事件的概率。由于每个分类器的准确度不同，因此每个可能结果的概率也会不同：
- en: 'Example: If the accuracies of the three classifiers (for TRUE) are 0.6, 0.7,
    and 0.8, then the probabilities of FALSE are, respectively, 0.4, 0.3, and 0.2,
    and the probabilities of the preceding table would be as follows:'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：如果三个分类器（对于真）的准确度分别为 0.6、0.7 和 0.8，那么假的概率分别为 0.4、0.3 和 0.2，前表中的概率如下：
- en: '| Classifier 1 | Classifier 2 | Classifier 3 |'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 分类器 1 | 分类器 2 | 分类器 3 |'
- en: '| --- | --- | --- |'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0.6 | 0.7 | 0.8 |'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.7 | 0.8 |'
- en: '| 0.4 | 0.7 | 0.8 |'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.7 | 0.8 |'
- en: '| 0.6 | 0.3 | 0.8 |'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.3 | 0.8 |'
- en: '| 0.4 | 0.3 | 0.8 |'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.3 | 0.8 | 0.336 |'
- en: '| 0.6 | 0.7 | 0.2 |'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.7 | 0.2 |'
- en: '| 0.4 | 0.7 | 0.2 |'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.7 | 0.2 |'
- en: '| 0.6 | 0.3 | 0.2 |'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.3 | 0.2 |'
- en: '| 0.4 | 0.3 | 0.2 |'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.3 | 0.2 |'
- en: 'In the next step, obtain the probability of the elementary event, which will
    be the product of the numbers in each column:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一步中，获取基本事件的概率，这将是每列数字的乘积：
- en: '| Classifier 1 | Classifier 2 | Classifier 3 | Probability |'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 分类器 1 | 分类器 2 | 分类器 3 | 概率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 0.6 | 0.7 | 0.8 | 0.336 |'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.7 | 0.8 | 0.336 |'
- en: '| 0.4 | 0.7 | 0.8 | 0.224 |'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.7 | 0.8 | 0.224 |'
- en: '| 0.6 | 0.3 | 0.8 | 0.144 |'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.3 | 0.8 | 0.144 |'
- en: '| 0.4 | 0.3 | 0.8 | 0.096 |'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.3 | 0.8 | 0.096 |'
- en: '| 0.6 | 0.7 | 0.2 | 0.084 |'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.7 | 0.2 | 0.084 |'
- en: '| 0.4 | 0.7 | 0.2 | 0.056 |'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.7 | 0.2 | 0.056 |'
- en: '| 0.6 | 0.3 | 0.2 | 0.036 |'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.6 | 0.3 | 0.2 | 0.036 |'
- en: '| 0.4 | 0.3 | 0.2 | 0.024 |'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0.4 | 0.3 | 0.2 | 0.024 |'
- en: 'Find the events which have a majority count. In this case, this refers to a
    sum greater than or equal to 2:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出具有多数计数的事件。在这种情况下，这指的是大于或等于 2 的总和：
- en: '| Classifier 1 | Classifier 2 | Classifier 3 | Vote Count |'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 分类器 1 | 分类器 2 | 分类器 3 | 投票数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| TRUE | TRUE | TRUE | 3 |'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 真 | 真 | 3 |'
- en: '| FALSE | TRUE | TRUE | 2 |'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 真 | 真 | 2 |'
- en: '| TRUE | FALSE | TRUE | 2 |'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 假 | 真 | 2 |'
- en: '| FALSE | FALSE | TRUE | 1 |'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 假 | 真 | 1 |'
- en: '| TRUE | TRUE | FALSE | 2 |'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 真 | 假 | 2 |'
- en: '| FALSE | TRUE | FALSE | 1 |'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 真 | 假 | 1 |'
- en: '| TRUE | FALSE | FALSE | 1 |'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 真 | 假 | 假 | 1 |'
- en: '| FALSE | FALSE | FALSE | 0 |'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 假 | 假 | 假 | 0 |'
- en: The probability of the majority vote is then simply the sum of the probability
    in cases where the vote count is greater than or equal to 2\. This is the sum
    of the entries in rows 1, 2, 3, and 5 of the Probability column, as 0.336 + 0.224
    + 0.144 + 0.084 = 0.788.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多数投票的概率然后就是投票数大于或等于 2 的情况的概率之和。这是概率列中第 1、2、3 和 5 行条目的总和，即 0.336 + 0.224 + 0.144
    + 0.084 = 0.788。
- en: 'We need to define a function here called `Get_Prob`, as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里定义一个名为 `Get_Prob` 的函数，如下所示：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Given a logical vector and a vector of corresponding probabilities, the `Get_Prob`
    function will return a vector that consists of the probability that the logical
    condition is `TRUE`. If the logical value is `FALSE`, the complement (1 – probability)
    is returned.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个逻辑向量和相应的概率向量，`Get_Prob` 函数将返回一个向量，该向量包含逻辑条件为 `真` 的概率。如果逻辑值为 `假`，则返回补数（1
    – 概率）。
- en: 'The preceding steps are put in an R program, and are listed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤被放入 R 程序中，如下所示：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Given a numeric vector with accuracies, named `accuracy`, with an odd number
    of classifiers, we first find the number of classifiers in it with the `length`
    function and store it in `NT`. All possible combinations of `APC` are then generated
    using the `expand.grid` function, where the `rep` function will repeat the vector
    `(TRUE, FALSE) NT` number of times. Each element of the column of the `APC` object
    will then generate a column where the `TRUE` and `FALSE` condition will be replaced
    by the corresponding classifier''s accuracy as well as the appropriate complement
    by using the `Get_Prob` function. Since we consider an odd number of classifiers,
    the majority vote is attended in cases when the number of `TRUE` in that elementary
    event is greater than 50 percent of the number of classifiers (that is, greater
    than `NT/2`). The rest of the computations are easier to follow. If the accuracy
    of the nine classifiers is 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, and 0.9,
    then the computations show that the accuracy of the ensemble would be 0.9113,
    higher than the most accurate classifier here, which is 0.9\. However, we must
    remember that each of the eight classifiers is less accurate than 0.9\. Despite
    this, the ensemble accuracy is higher than the highest classifier we have on hand.
    To verify that the computations are working fine, we apply this approach to the
    example given on page 74 of Zhou (2012), and confirm the final majority vote probability
    at 0.933:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个名为 `accuracy` 的包含准确率的数值向量，其中分类器的数量为奇数，我们首先使用 `length` 函数找到其中的分类器数量，并将其存储在
    `NT` 中。然后使用 `expand.grid` 函数生成所有可能的 `APC` 组合，其中 `rep` 函数将向量 `(TRUE, FALSE) NT`
    重复 `NT` 次。`APC` 对象的每一列将生成一个列，其中 `TRUE` 和 `FALSE` 条件将使用 `Get_Prob` 函数替换为相应的分类器准确率以及适当的补数。由于我们考虑的是奇数个分类器，当该基本事件中的
    `TRUE` 数量大于分类器数量的50%（即大于 `NT/2`）时，才会进行多数投票。其余的计算比较容易理解。如果九个分类器的准确率分别为0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85和0.9，那么计算表明集成准确率为0.9113，高于这里最准确的分类器，即0.9。然而，我们必须记住，八个分类器中的每一个的准确率都低于0.9。尽管如此，集成准确率仍然高于我们手头上的最高分类器。为了验证计算是否正常工作，我们将这种方法应用于周（2012）第74页上的示例，并确认最终多数投票概率为0.933：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'What happens to the case when each classifier is worse than a random guess?
    We will simply turn out the accuracies of the nine classifier scenarios and repeat
    the program to get the following answer:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个分类器都不如随机猜测时会发生什么？我们将简单地输出九个分类器场景的准确率，并重复程序以获得以下答案：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When each of the classifiers is worse than a random guess, the majority vote
    classifier gives horrible results in the case of ensembling. This leaves us with
    the final case. What if we have a mixture of classifiers of which some are better
    than the random guess classifier and some are worse than the random guess classifier?
    We will put the computing code block in a function known as `Random_Accuracy`.
    The accuracies in the classifiers then become randomly generated numbers in the
    unit interval. The function `Random_Accuracy` is then run over ten times to generate
    the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个分类器都不如随机猜测时，在集成的情况下，多数投票分类器会给出可怕的结果。这让我们面临最后一个情况。如果我们有一组分类器，其中一些比随机猜测分类器好，而一些比随机猜测分类器差呢？我们将计算代码块放入一个名为
    `Random_Accuracy` 的函数中。然后，分类器中的准确率变成了单位区间内随机生成的数字。`Random_Accuracy` 函数随后运行十次，生成以下输出：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: A mixed bag of results. As a result, if we need to get reasonable accuracy and
    performance from the ensembling method, it is imperative to ensure that each classifier
    is better than the random guess. A central assumption in our analysis thus far
    has been that the classifiers are independent of each other. This assumption is
    seldom true in practical settings, as the classifiers are built using the same
    dataset. However, this topic will be dealt with in the following chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 结果参差不齐。因此，如果我们需要从集成方法中获得合理的准确性和性能，确保每个分类器都比随机猜测要好是至关重要的。到目前为止，我们分析的一个核心假设是分类器之间是相互独立的。在实际设置中，这个假设很少成立，因为分类器是使用相同的训练集构建的。然而，这个话题将在下一章中讨论。
- en: We will now move on to the problem of ensembling by voting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向投票集成的问题。
- en: Ensembling by voting
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票集成
- en: Ensembling by voting can be used efficiently for classification problems. We
    now have a set of classifiers, and we need to use them to predict the class of
    an unknown case. The combining of the predictions of the classifiers can proceed
    in multiple ways. The two options that we will consider are majority voting, and
    weighted voting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过投票集成可以有效地用于分类问题。我们现在有一组分类器，我们需要使用它们来预测未知案例的类别。分类器预测的组合可以以多种方式进行。我们将考虑的两个选项是多数投票和加权投票。
- en: Majority voting
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多数投票
- en: 'Ideas related to voting will be illustrated through an ensemble based on the
    homogeneous base learners of decision trees, as used in the development of bagging
    and random forests. First, we will create 500 base learners using the `randomForest`
    function and repeat the program in the first block, as seen in [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*. Ensembling has already been performed
    in that chapter, and we will elaborate on those steps here. First, the code block
    for setting up the random forest is given here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用决策树作为基础学习器构建的集成，我们可以通过投票相关的想法进行说明，正如在开发袋装和随机森林时使用的那样。首先，我们将使用 `randomForest`
    函数创建500个基础学习器，并重复第一个块中的程序，如[第4章](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "第4章。随机森林")中所示，*随机森林*。集成已经在那一章中完成，我们将在这里详细说明那些步骤。首先，给出设置随机森林的代码块：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we will use the standard `predict` function to predict the class for
    the `GC2_TestX` data, and then, using the option of `predict.all=TRUE`, obtain
    the prediction for each tree generated in the random forest:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用标准的 `predict` 函数来预测 `GC2_TestX` 数据的类别，然后，使用 `predict.all=TRUE` 选项，获取随机森林中生成的每个树的预测结果：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The predicted `GC2_RF_Test_Predict` object will consist of further `individual`
    objects, which will have the predictions for each decision tree. We will first
    define a function called `Row_Count_Max`, which will return the prediction whose
    count is a maximum in the forest. The rudimentary voting method is then compared
    with the `predict` function''s outcomes in the following code block:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的 `GC2_RF_Test_Predict` 对象将包含进一步的 `individual` 对象，这些对象将包含每个决策树的预测。我们首先定义一个名为
    `Row_Count_Max` 的函数，该函数将返回森林中计数最大的预测。然后，基本的投票方法将在以下代码块中与 `predict` 函数的结果进行比较：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Consequently, we can see that the `predict` function implements the majority
    count technique. Next, we will quickly illustrate the ideas and thinking behind
    weighted voting.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到 `predict` 函数实现了多数计数技术。接下来，我们将快速说明加权投票背后的思想和思考。
- en: Weighted voting
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加权投票
- en: An implicit assumption in the use of simple voting is that all classifiers are
    equally accurate, or that all classifiers have equal voting power. Consider the
    simpler case in which we have five classifiers, three of them with an accuracy
    of 0.51 and the remaining two with an accuracy of 0.99\. If the less accurate
    classifier votes an observation as a negative case (-1) and the two more accurate
    classifiers as a positive case (+1), then the simple voting method will call the
    observation (-1). With this voting pattern, the probability of the observation
    being -1 is ![Weighted voting](img/00314.jpeg), while that of it being +1 is ![Weighted
    voting](img/00315.jpeg). Thus, we can't pretend that all classifiers should have
    the same voting power. This is where we will make good use of the weighted voting
    method.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单投票的使用中存在的一个隐含假设是所有分类器都是同样准确的，或者所有分类器都有相同的投票权。考虑一个更简单的情况，我们有五个分类器，其中三个的准确率为0.51，剩下的两个准确率为0.99。如果准确度较低的分类器将观察结果投票为负案例(-1)，而两个更准确的分类器将其投票为正案例(+1)，那么简单的投票方法将把观察结果标记为(-1)。在这种投票模式中，观察结果为-1的概率是![加权投票](img/00314.jpeg)，而为+1的概率是![加权投票](img/00315.jpeg)。因此，我们不能假装所有分类器都应该有相同的投票权。这就是我们将充分利用加权投票方法的地方。
- en: In this analysis, we will take the accuracy of the classifiers over the training
    dataset as the weights. We will treat ![Weighted voting](img/00316.jpeg) as the
    weight associated with the classifier ![Weighted voting](img/00317.jpeg). An important
    characteristic of the weights is that they should be non-negative and should add
    up to 1, that is, ![Weighted voting](img/00318.jpeg). We will normalize the accuracy
    of the classifiers to satisfy this constraint.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次分析中，我们将使用训练数据集上分类器的准确率作为权重。我们将![加权投票](img/00316.jpeg)视为与![加权投票](img/00317.jpeg)相关的权重。权重的一个重要特征是它们应该是非负的，并且它们的总和应该是1，即![加权投票](img/00318.jpeg)。我们将归一化分类器的准确率以满足这一约束。
- en: 'We will continue the analysis with the German Credit dataset. First, we will
    obtain the predictions for the 500 trees over the training dataset, and then obtain
    the accuracies:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用德国信用数据集进行分析。首先，我们将获得训练数据集上500棵树的预测，然后获得准确率：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'What is the `headtail` function? It is available in the `Utilities.R` file.
    The analysis is repeated with the bagging ensemble as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`headtail`函数是什么？它在`Utilities.R`文件中可用。以下是对`bagging`集成器进行重复分析：'
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we normalize the weights and calculate the weighted votes for the observations
    in the test samples, as shown in the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将归一化权重并计算测试样本中观测值的加权投票，如下所示：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The weighted voting analysis is repeated for the `bagging` objects, as shown
    here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，对`bagging`对象重复进行加权投票分析：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, with the voting mechanisms behind us, we turn our attention to regression
    problems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着投票机制的问题解决，我们将注意力转向回归问题。
- en: Ensembling by averaging
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过平均进行集成
- en: Within the context of regression models, the predictions are the numeric values
    of the variables of interest. Combining the predictions of the output due to the
    various ensemblers is rather straightforward; because of the ensembling mechanism,
    we simply interpret the average of the predicted values across the ensemblers
    as the predicted value. Within the context of the classification problem, we can
    carry out simple averaging and weighted averaging. In the previous section, the
    ensemble had homogeneous base learners. However, in this section, we will deal
    with heterogeneous base learners.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归模型的背景下，预测是感兴趣变量的数值。结合由于各种集成器导致的输出预测相对简单；由于集成机制，我们只需将集成器之间预测值的平均值解释为预测值。在分类问题的背景下，我们可以进行简单的平均和加权平均。在前一节中，集成器具有同质的基础学习器。然而，在本节中，我们将处理异质的基础学习器。
- en: 'We will now consider a regression problem that is dealt with in detail in [Chapter
    8](part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee "Chapter 8. Ensemble
    Diagnostics"), *Ensemble Diagnostics*. The problem is the prediction of housing
    prices based on over 60 explanatory variables. We have the dataset in training
    and testing partitions, and load them to kick off the proceedings:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将考虑一个在[第8章](part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee
    "第8章。集成诊断")《集成诊断》中详细处理的回归问题。问题是基于超过60个解释变量的房价预测。我们拥有训练和测试数据集，并将它们加载以启动过程：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Consequently, we have many observations to build our models. The `SalePrice`
    is the variable of interest here. First, we create a `formula` and build a linear
    model; four regression trees with different depths; four neural networks with
    a different number of hidden neurons; and a support vector machine model in the
    following code block:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有大量观测数据来构建我们的模型。`SalePrice`是这里感兴趣的变量。首先，我们创建一个`公式`并构建一个线性模型；四个不同深度的回归树；四个具有不同隐藏神经元的神经网络；以及以下代码块中的支持向量机模型：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have the required setup here to consider the heterogeneous ensemble.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了考虑异质集成的所需设置。
- en: Simple averaging
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单平均
- en: 'We built ten models using the training dataset, and we will now make the predictions
    for these models on the training dataset using the `predict` function, as shown
    in the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练数据集构建了十个模型，现在我们将使用`predict`函数在这些模型上对训练数据集进行预测，如下所示：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When it comes to classification problems, the predictions are either based
    on the class labels or the probability of the class of interest. Consequently,
    we don''t come across *bad predictions* in terms of the magnitude of predictions,
    though we need to at least check if the predictions give a mixture of +1s or -1s.
    If the classifiers predict only +1 or -1, such classifiers can then be discarded
    from further analysis. For the regression problems, we need to see if the models
    can make reasonable predictions in terms of the magnitude, and we will simply
    obtain a plot of the magnitude of the predictions, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到分类问题时，预测要么基于类别标签，要么基于感兴趣类别的概率。因此，在预测的幅度方面，我们不会遇到*不良预测*，尽管我们至少需要检查预测是否给出+1和-1的混合。如果分类器只预测+1或-1，那么这样的分类器可以被从进一步的分析中丢弃。对于回归问题，我们需要看看模型是否能在幅度上做出合理的预测，我们将简单地获得预测幅度的图，如下所示：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result of the preceding code block is shown in the following figure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块的结果如下所示：
- en: '![Simple averaging](img/00319.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![简单平均](img/00319.jpeg)'
- en: 'Figure 3: A simple plot of the predictions for the ten heterogeneous base learners'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：十个异构基学习器的预测简单图
- en: 'We can see that the predictions related to the neural network models with two
    or three hidden neurons produce no variation in the predictions. Consequently,
    we delete these two models from further analysis. The ensemble prediction is simply
    the average of the predictions across the remaining eight models:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与具有两个或三个隐藏神经元的神经网络模型相关的预测没有产生预测上的变化。因此，我们将这两个模型从进一步的分析中删除。集成预测只是剩余八个模型预测的平均值：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Simple averaging](img/00320.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![简单平均](img/00320.jpeg)'
- en: 'Figure 4: Ensemble predictions for the housing dataset'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：住房数据集的集成预测
- en: As with the extension of simple voting to weighted voting, we will now look
    at weighted averaging.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如将简单投票扩展到加权投票一样，我们现在将探讨加权平均。
- en: Weight averaging
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重平均
- en: 'In the case of classifiers, the weights were chosen from the accuracies of
    the classifiers for the training dataset. In this instance, we need unifying measures
    like this. A regressor model is preferred if it has less residual variance, and
    we will select the variance as a measure of accuracy. Suppose that the estimated
    residual variance for a weak base model *i* is ![Weight averaging](img/00321.jpeg).
    In the context of ensemble neural networks, Perrone and Cooper (1993) claim that
    the optimal weight for the *ith* weak base model can be obtained using the following
    equation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类器的情况下，权重是从训练数据集的分类器的准确性中选择的。在这种情况下，我们需要像这样的统一度量。如果回归模型具有更小的残差方差，则更倾向于选择回归模型，我们将选择方差作为准确性的度量。假设弱基模型*i*的估计残差方差为![权重平均](img/00321.jpeg)。在集成神经网络的情况下，Perrone和Cooper（1993）声称可以使用以下方程获得*i*个弱基模型的最佳权重：
- en: '![Weight averaging](img/00322.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![权重平均](img/00322.jpeg)'
- en: 'Since the proportional constants do not matter, we will simply substitute ![Weight
    averaging](img/00323.jpeg) with the mean of residual squares. In this direction,
    we will first obtain the ![Weight averaging](img/00324.jpeg) up to a constant,
    by simply calculating `mean(residuals(model)^2)` for the eight models considered
    in the context of simple averaging, as shown:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于比例常数无关紧要，我们将简单地用残差平方的平均值![权重平均](img/00323.jpeg)来代替。在这个方向上，我们将首先通过简单地计算简单平均情况下考虑的八个模型的`mean(residuals(model)^2)`来获得![权重平均](img/00324.jpeg)（加上一个常数），如下所示：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we simply implement the formula of weights ![Weight averaging](img/00325.jpeg)as
    follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们简单地实现权重公式![权重平均](img/00325.jpeg)，如下所示：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `rowMeans` and `cbind` functions simply give away the weighted averaging
    predictions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`rowMeans`和`cbind`函数简单地给出了加权平均预测：'
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output for the preceding code is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码的输出如下所示：
- en: '![Weight averaging](img/00326.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![权重平均](img/00326.jpeg)'
- en: 'Figure 5: Weighted averaging predictions for the housing price'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：住房价格的加权平均预测
- en: Stack ensembling
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠集成
- en: An introductory and motivational example of the stacked regression was provided
    in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    Here, we will continue the discussion of stacked ensembles for a regression problem
    which has not been previously developed.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "第1章。集成技术简介")中提供了一个堆叠回归的入门和激励示例，*集成技术简介*。在这里，我们将继续讨论一个尚未开发的回归问题的堆叠集成。
- en: 'With stacked ensembling, the outputs of several weak models are given as an
    input variable, along with the covariates used to build the earlier models, to
    build a stack model. The form of the stack model might be one of these, or it
    can be a different model. Here, we will simply use the eight regression models
    (used in previous sections) as weak models. The stacking regression model is selected
    as the gradient boosting model, and it will be given the original input variables
    and predictions of the new models, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆叠集成中，几个弱模型的输出作为输入变量，以及用于构建早期模型的协变量，来构建一个堆叠模型。堆叠模型的形式可能是以下之一，或者可以是不同的模型。在这里，我们将简单地使用前几节中使用的八个回归模型作为弱模型。堆叠回归模型被选为梯度提升模型，并将给出原始输入变量和新模型的预测，如下所示：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This concludes our simple discussion of stacked ensemble regressions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对堆叠集成回归的简单讨论。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at why ensemble works in the context of classification
    problems. A series of detailed programs illustrated the point that each classifier
    must be better than a random guess. We considered scenarios where all the classifiers
    have the same accuracy, different accuracy, and finally a scenario with completely
    arbitrary accuracies. Majority and weighted voting was illustrated within the
    context of the random forest and bagging methods. For the regression problem,
    we used a different choice of base learners and allowed them to be heterogeneous.
    Simple and weighted averaging methods were illustrated in relation to the housing
    sales price data. A simple illustration of stacked regression ultimately concluded
    the technical section of this chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在分类问题背景下集成为什么有效。一系列详细程序说明了每个分类器必须比随机猜测更好的观点。我们考虑了所有分类器具有相同准确度、不同准确度和最后是完全任意准确度的场景。在随机森林和袋装方法背景下说明了多数和加权投票。对于回归问题，我们使用了不同的基学习器选择，并允许它们是异质的。在住房销售价格数据相关方面，我们展示了简单和加权平均方法。堆叠回归的简单说明最终结束了本章的技术部分。
- en: In the following chapter, we will look at ensembling diagnostics.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨集成诊断。
