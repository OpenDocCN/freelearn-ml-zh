- en: The Y is as Important as the X
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: Y与X同样重要
- en: A lot of attention is given to the input features, that is, our x's. We have
    used algorithms to scale them, select from them, and engineer new features to
    add to them. Nonetheless, we should also give as much attention to the targets,
    the y's. Sometimes, scaling your targets can help you use a simpler model. Some
    other times, you may need to predict multiple targets at once. It is, then, essential
    to know the distribution of your targets and their interdependencies. In this
    chapter, we are going to focus on the targets and how to deal with them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给予输入特征很多关注，即我们的`x`。我们使用算法对它们进行缩放、从中选择，并工程化地添加新的特征。尽管如此，我们也应当同样关注目标变量，即`y`。有时，缩放你的目标可以帮助你使用更简单的模型。而有时候，你可能需要一次预测多个目标。那时，了解你的目标的分布及其相互依赖关系是至关重要的。在本章中，我们将重点讨论目标以及如何处理它们。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Scaling your regression targets
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放你的回归目标
- en: Estimating multiple regression targets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计多个回归目标
- en: Dealing with compound classification targets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理复合分类目标
- en: Calibrating a classifier's probabilities
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 校准分类器的概率
- en: Calculating the precision at K
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算K的精确度
- en: Scaling your regression targets
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放你的回归目标
- en: In regression problems, sometimes scaling the targets can save time and allow
    us to use simpler models for the problems at hand. In this section, we are going
    to see how to make our estimator's life easier by changing the scale of our targets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，有时对目标进行缩放可以节省时间，并允许我们为当前问题使用更简单的模型。在本节中，我们将看到如何通过改变目标的尺度来简化估计器的工作。
- en: In the following example, the relation between the target and the input is non-linear.
    Therefore, a linear model would not give the best results. We can either use a
    non-linear algorithm, transform our features, or transform our targets. Out of
    the three options, transforming the targets can be the easiest sometimes. Notice
    that we only have one feature here, but when dealing with a number of features,
    it makes sense to think of transforming your targets first.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，目标与输入之间的关系是非线性的。因此，线性模型不能提供最佳结果。我们可以使用非线性算法、转换特征或转换目标。在这三种选择中，转换目标有时可能是最简单的。请注意，我们这里只有一个特征，但在处理多个特征时，首先考虑转换目标是有意义的。
- en: 'The following plot shows the relation between a single feature, `x`, and a
    dependent variable, `y`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了单一特征`x`与因变量`y`之间的关系：
- en: '![](img/22d34bc7-9b36-46fb-a3df-2eb30c6767d4.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22d34bc7-9b36-46fb-a3df-2eb30c6767d4.png)'
- en: 'Between you and me, the following code was used to generate data, but for the
    sake of learning, we can pretend that we do not know the relation between the
    y''s and the x''s for now:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在你我之间，以下代码用于生成数据，但为了学习的目的，我们可以假装目前不知道`y`和`x`之间的关系：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The one-dimensional input (`x`) is uniformly distributed between *5* and *20*.
    The relation between *y* and *x* is cubical, with some normally distributed noise
    added to the *x*'s.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一维输入（`x`）在*5*和*20*之间均匀分布。`y`与`x`之间的关系是立方的，并且向`x`添加了少量正态分布的噪声。
- en: 'Before splitting out data, we need to transform the *x*''s from a vector into
    a matrix, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在拆分数据之前，我们需要将*x*从向量转换为矩阵，如下所示：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, if we split our data into training and test sets, and run a ridge regression,
    we will get a **Mean Absolute Error** (**MAE**) of `559`. Your mileage may vary
    due to the randomly generated data. Can we do better than this?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们将数据拆分为训练集和测试集，并运行岭回归，我们将得到一个**平均绝对误差**（**MAE**）为`559`。由于数据是随机生成的，你的结果可能有所不同。我们能做得更好吗？
- en: Please keep in mind that in most of the examples mentioned in this chapter,
    the final results you will get may differ from mine. I preferred not to use random
    states when generating and splitting the data as my main goal here is to explain
    the concepts, regardless of the final results and the accuracy scores we get when
    running the code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在本章中提到的大多数示例中，你最终得到的结果可能与我的有所不同。我在生成和拆分数据时选择不使用随机状态，因为我的主要目标是解释概念，而不是关注最终的结果和运行代码时的准确度分数。
- en: 'Let''s create a simple transformer to convert the target based on a given `power`.
    When `power` is set to `1`, no transformation is done to the target; otherwise,
    the target is raised to the given power. Our transformer has a complementary `inverse_transform()`
    method to retransform the targets back to their original scale:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的变换器，根据给定的 `power` 来转换目标。当 `power` 设置为 `1` 时，不对目标进行任何变换；否则，目标会被提升到给定的幂次。我们的变换器有一个互补的
    `inverse_transform()` 方法，将目标重新转换回原始尺度：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we can try different settings for the power and loop over the different
    transformations until we find the one that gives the best results:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以尝试不同的幂次设置，并循环遍历不同的变换，直到找到给出最佳结果的变换：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It is essential that we convert the predicted values to their original. Otherwise,
    the calculated error metrics will not be comparable given the different data scales
    achieved by the different power settings.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测值转换回原始值是至关重要的。否则，计算的误差指标将无法进行比较，因为不同的幂次设置会导致不同的数据尺度。
- en: 'Ergo, the`inverse_transform()` method is used here after the prediction step.
    Running the code on my randomly generated data gave me the following results:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`inverse_transform()` 方法在预测步骤后使用。在我的随机生成的数据上运行代码得到了以下结果：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As expected, the lowest error and the highest `R²` are achieved when the right
    transformation was used, which is when the power is set to ![](img/01bbc660-8e6f-4a20-8edb-4b814412761f.png).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，当使用正确的变换时，最低的误差和最高的 `R²` 被实现，这正是当幂次设置为 ![](img/01bbc660-8e6f-4a20-8edb-4b814412761f.png)
    时。
- en: The logarithmic, exponential, and square root transformations are the ones most
    commonly used by statisticians. It makes sense to use them when performing a prediction
    task, especially when a linear model is used.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换、指数变换和平方根变换是统计学家最常用的变换。当执行预测任务时，特别是在使用线性模型时，使用这些变换是有意义的。
- en: The logarithmic transformation is only useful for positive values. `Log(0)`
    is undefined, and the logarithm of a negative number gives us imaginary values.
    Thus, the logarithmic transformation is usually applied when dealing with non-negative
    targets. One other trick to make sure that we do not encounter `log(0)` is to
    add 1 to all your target values before transforming them, then subtract 1 after
    transforming your predictions back. Similarly, for the square root transformation,
    we have to make sure not to have negative targets in the first place.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换仅对正值有效。`Log(0)` 是未定义的，对负数取对数会得到虚数值。因此，对数变换通常应用于处理非负目标。为了确保我们不会遇到 `log(0)`，一个技巧是，在转换目标之前，先给所有目标值加1，然后在将预测结果反向转换后再减去1。同样，对于平方根变换，我们也需要确保一开始没有负目标值。
- en: Rather than dealing with one target at a time, we may sometimes want to predict
    multiple targets at once. Combining multiple regression tasks into a single model
    can simplify your code when they all use the same features. It's also recommended
    when your targets are interdependent. In the next section, we are going to see
    how to estimate multiple regression targets at once.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与其一次处理一个目标，我们有时可能希望一次预测多个目标。当多个回归任务使用相同特征时，将它们合并为一个模型可以简化代码。如果你的目标是相互依赖的，推荐使用这种方法。在下一部分，我们将看到如何一次性估计多个回归目标。
- en: Estimating multiple regression targets
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估算多个回归目标
- en: In your online business, you may want to estimate the lifetime value of your
    users in the next month, the next quarter, and the next year. You could build
    three different regressors for each one of these three separate estimations. However,
    when the three estimations use the exact same features, it becomes more practical
    to build one regressor with three outputs. In the next section, we are going to
    see how to build a multi-output regressor, then we will learn how to inject interdependencies
    between those estimations using regression chains.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的在线业务中，你可能想估算用户在下个月、下个季度和明年的生命周期价值。你可以为这三个单独的估算构建三个不同的回归模型。然而，当这三个估算使用完全相同的特征时，构建一个具有三个输出的回归器会更为实用。在下一部分，我们将看到如何构建一个多输出回归器，然后我们将学习如何使用回归链在这些估算之间注入相互依赖关系。
- en: Building a multi-output regressor
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个多输出回归器
- en: Some regressors allow us to predict multiple targets at once. For example, the
    ridge regressor allows for a two-dimensional target to be given. In other words,
    rather than having `y` as a single-dimensional array, it can be given as a matrix,
    where each column represents a different target. For the other regressors where
    only single targets are allowed, we may need to use the multi-output regressor
    meta-estimator.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一些回归器允许我们一次预测多个目标。例如，岭回归器允许给定二维目标。换句话说，`y`不再是单维数组，而是可以作为矩阵给定，其中每列代表一个不同的目标。对于只允许单一目标的其他回归器，我们可能需要使用多输出回归器元估算器。
- en: 'To demonstrate this meta-estimator, I am going to use the `make_regression`
    helper to create a dataset that we can fiddle with:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个元估算器，我将使用`make_regression`辅助函数来创建一个我们可以调整的数据集：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we create `500` samples, with 8 features and 3 targets; that is, the
    shapes of the returned `x` and `y` are (`500`, `8`) and (`500`, `3`) respectively.
    We can also give the features and the targets different names, and then split
    the data into training and test sets as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建`500`个样本，具有8个特征和3个目标；即返回的`x`和`y`的形状分别为（`500`，`8`）和（`500`，`3`）。我们还可以为特征和目标指定不同的名称，然后按如下方式将数据拆分为训练集和测试集：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since `SGDRegressor` does not support multiple targets, the following code
    will throw a value error complaining about the shape of the inputs:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`SGDRegressor`不支持多目标，因此以下代码将抛出一个值错误，抱怨输入的形状不正确：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Therefore, we have to wrap `MultiOutputRegressor` around `SGDRegressor` for
    it to work:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须将`MultiOutputRegressor`包裹在`SGDRegressor`周围才能使其工作：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can now output predictions into a dataframe:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将预测结果输出到数据框中：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, check the first few predictions for each one of the three targets. Here
    is an example of the predictions I got here. Keep in mind that you may get different
    results:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，检查每个目标的前几次预测。以下是我在这里得到的预测示例。请记住，您可能会得到不同的结果：
- en: '![](img/ca98a341-b016-44b6-99b7-bb18471acae7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca98a341-b016-44b6-99b7-bb18471acae7.png)'
- en: 'We can also print the model''s performance for each target separately:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以分别打印每个目标的模型表现：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In some scenarios, knowing one target may serve as a stepping stone in knowing
    the others. In the aforementioned lifetime value estimation example, the predictions
    for the next month are helpful for the quarterly and yearly predictions. To use
    the predictions for one target as inputs in the consecutive regressors, we need
    to use the regressor chain meta-estimator.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，知道一个目标可能有助于了解其他目标。在前面提到的生命周期价值估算示例中，预测下一个月的结果对季度和年度预测非常有帮助。为了将一个目标的预测作为输入传递给连续的回归器，我们需要使用回归器链元估算器。
- en: Chaining multiple regressors
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接多个回归器
- en: In the dataset from the previous section, we do not know whether the generated
    targets are interdependent or not. For now, let's assume the second target is
    dependent on the first one, and the third target is dependent on the first two.
    We are going to validate these assumptions later. To inject these interdependencies,
    we are going to use `RegressorChain` and specify the order of the assumed interdependencies.
    The order of the IDs in the `order` list specify that each ID in the list depends
    on the previous IDs. It makes sense to use a regularized regressor. The regularization
    is needed to ignore any assumed dependencies that do not exist between the targets.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节的数据集中，我们无法确定生成的目标是否相互依赖。现在，假设第二个目标依赖于第一个目标，第三个目标依赖于前两个目标。我们稍后将验证这些假设。为了引入这些相互依赖关系，我们将使用`RegressorChain`并指定假设的依赖关系顺序。`order`列表中的ID顺序指定列表中的每个ID依赖于前面的ID。使用正则化回归器是有意义的。正则化是必要的，用于忽略目标之间不存在的任何假定依赖关系。
- en: 'Here is the code for creating the regressor chain:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是创建回归器链的代码：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The test set performance is almost identical to the one achieved with the `MultiOutputRegressor`.
    It looks like chaining did not help with the dataset at hand. We can display the
    coefficients each of the three `Ridge` regressors had after training. The first
    estimator only uses the input feature, while the later ones assign coefficients
    to the input features as well as the previous targets. Here is how to display
    the coefficients for the third estimator in the chain:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的表现几乎与使用`MultiOutputRegressor`时的表现相同。看起来链式方法并没有帮助当前的数据集。我们可以显示每个`Ridge`回归器在训练后的系数。第一个估算器只使用输入特征，而后面的估算器则为输入特征以及之前的目标分配系数。以下是如何显示链中第三个估算器的系数：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'From the calculated coefficients, we can see that the first two targets were
    almost ignored by the third estimator in the chain. Since the targets are independent,
    each estimator in the chain used the input features only. Although the coefficients
    you will get when running the code may vary, the coefficients given to the first
    two targets will still be negligible due to the targets'' independence:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算出的系数来看，我们可以看到链中的第三个估算器几乎忽略了前两个目标。由于这些目标是独立的，链中的每个估算器仅使用输入特征。尽管你在运行代码时得到的系数可能有所不同，但由于目标的独立性，分配给前两个目标的系数仍然是微不足道的：
- en: '![](img/5c2f9e5a-8c98-4842-9b78-71b672431718.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c2f9e5a-8c98-4842-9b78-71b672431718.png)'
- en: In cases where the targets are dependent, we expect to see bigger coefficients
    assigned to the targets. In practice, we may try different permutations for the
    `order` hyperparameter until the best performance is found.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标之间存在依赖关系的情况下，我们期望目标会分配更大的系数。在实际应用中，我们可能会尝试不同的`order`超参数组合，直到找到最佳性能为止。
- en: As in the regression problems, the classifiers can also deal with multiple targets.
    Nonetheless, one target can either be binary or have more than two values. This
    adds more nuance to the classification cases. In the next section, we are going
    to learn how to build classifiers to meet the needs of compound targets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与回归问题一样，分类器也可以处理多个目标。然而，单个目标可以是二元的，或者具有两个以上的值。这为分类问题增添了更多细节。在下一部分，我们将学习如何构建分类器来满足复合目标的需求。
- en: Dealing with compound classification targets
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理复合分类目标
- en: 'As with regressors, classifiers can also have multiple targets. Additionally,
    due to their discrete targets, a single target can have two or more values. To
    be able to differentiate between the different cases, machine learning practitioners
    came up with the following terminologies:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与回归器类似，分类器也可以有多个目标。此外，由于目标是离散的，单个目标可以具有两个或更多的值。为了区分不同的情况，机器学习实践者提出了以下术语：
- en: Multi-class
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类
- en: Multi-label (and multi-output)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签（和多输出）
- en: 'The following matrix summarizes the aforementioned terminologies. I will follow
    up with an example to clarify more, and will also shed some light on the subtle
    difference between the multi-label and multi-output terms later in this chapter:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵总结了上述术语。我将通过一个示例进一步说明，并在本章后续内容中也会详细阐述多标签和多输出术语之间的细微区别：
- en: '![](img/355c77a8-3fba-4834-8392-835c859e8b53.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/355c77a8-3fba-4834-8392-835c859e8b53.png)'
- en: Imagine a scenario where you are given a picture and you need to classify it
    based on whether it contains a cat or not. In this case, a binary classifier is
    needed, that is, where the targets are either zeroes or ones. When the problem
    involves figuring out whether the picture contains a cat, a dog, or a human being,
    then the cardinality of our target is beyond two, and the problem is then formulated
    as a multi-class classification problem.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，你需要根据图片中是否包含猫来进行分类。在这种情况下，需要一个二元分类器，也就是说，目标值要么是零，要么是一个。当问题涉及判断图片中是否有猫、狗或人类时，目标的种类数就超出了二个，此时问题就被表述为多类分类问题。
- en: 'The pictures can also contain more than one object. One picture can only have
    a cat in it, while the other has a human being and a cat together. In a multi-label
    setting, we would build a set of binary classifiers: one to tell whether the picture
    has a cat or not, another one for dogs, and one for human beings. To inject interdependency
    between the different targets, you may want to predict all the simultaneous labels
    at once. In such a scenario, the term multi-output is usually used.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片中也可能包含多个对象。一个图片可能只包含一只猫，而另一个图片则同时包含人类和猫。在多标签设置中，我们将构建一组二元分类器：一个用于判断图片中是否有猫，另一个用于判断是否有狗，再有一个用于判断是否有人类。为了在不同目标之间注入相互依赖关系，你可能希望一次性预测所有同时出现的标签。在这种情况下，通常会使用“多输出”这一术语。
- en: Furthermore, you can solve a multi-class problem using a set of binary-classifiers.
    Rather than telling whether the picture has a cat, a dog, or a human being, you
    can have a classifier telling whether it has a cat or not, one for whether a dog
    exists, and a third for whether there is a human being or not. This can be useful
    for model interpretability since the coefficients of each of the three classifiers
    can be mapped to a single class. In the next section, we are going to use the*One-vs-Rest*
    strategy to convert a multi-class problem into a set of binary ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用一组二分类器来解决多类问题。与其判断图片里是否有猫、狗或人类，不如设置一个分类器判断是否有猫，一个分类器判断是否有狗，另一个判断是否有人类。这对于模型的可解释性很有用，因为每个分类器的系数可以映射到单一类别。在接下来的章节中，我们将使用*一对多*策略将多类问题转换为一组二分类问题。
- en: Converting a multi-class problem into a set of binary classifiers
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将多类问题转换为一组二分类器
- en: We do not have to stick to the multi-class problems. We can simply convert the
    multi-class problem at hand into a set of binary classification problems.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必局限于多类问题。我们可以简单地将手头的多类问题转换为一组二分类问题。
- en: 'Here, we build a dataset with 5,000 samples, 15 features, and 1 label with
    4 possible values:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们构建了一个包含5000个样本、15个特征和1个标签（具有4个可能值）的数据集：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After splitting the data as we usually do, and keeping 25% of it for testing,
    we can apply the *One-vs-Rest*strategy on top of`LogisticRegression`. As the name
    suggests, it is a meta-estimator that builds multiple classifiers to tell whether
    each sample belongs to one class or not, and finally combines all the decisions
    made:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在通常的方式划分数据后，并保留25%用于测试，我们可以在`LogisticRegression`之上应用*一对多*策略。顾名思义，它是一个元估计器，构建多个分类器来判断每个样本是否属于某个类别，最终将所有的决策结合起来：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: I used the saga solver as it converges more quickly for larger datasets. The
    *One-vs-Rest*strategy gave me an accuracy score of `0.43`. We can access theunderlying
    binary classifiers used by the meta-estimator via its `estimators`**method, then
    we can reveal the coefficients learned for each feature by each one of the underlying
    binary classifiers.**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了saga求解器，因为它对较大数据集收敛更快。*一对多*策略给我带来了`0.43`的准确率。我们可以通过`estimators`方法访问元估计器使用的底层二分类器，然后可以揭示每个底层二分类器为每个特征学习的系数。**
- en: '**Another strategy is *One-vs-One*. It builds separate classifiers for each
    pair of classes, and can be used as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**另一种策略是*一对一*。它为每一对类别构建独立的分类器，使用方法如下：'
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The *One-vs-One***strategy gave me a comparable accuracy of `0.44`. We can see
    how, when dealing with a large number of classes, the previous two strategies
    may not scale well. `OutputCodeClassifier` is a more scalable solution. It can
    encode the labels into a denser representation by setting its `code_size` hyperparameter
    to a value less than one. A lower `code_size` will increase its computational
    performance at the expense of its accuracy and interpretability.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*一对一*策略给我带来了`0.44`的可比准确率。我们可以看到，当处理大量类别时，之前的两种策略可能无法很好地扩展。`OutputCodeClassifier`是一种更具可扩展性的解决方案。通过将其`code_size`超参数设置为小于1的值，它可以将标签编码为更密集的表示。较低的`code_size`将提高其计算性能，但会以牺牲准确性和可解释性为代价。**'
- en: '**In general, *One-vs-Rest*is the most commonly used strategy, and it is a
    good starting point if your aim is to separate the coefficients for each class.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**通常，*一对多*是最常用的策略，如果你的目标是为每个类别分离系数，它是一个很好的起点。'
- en: To make sure the retuned probabilities for all the classes add up to one, the
    *One-vs-Rest* strategy normalizes the probabilities by dividing them by their
    total. One other approach to probability normalization is the`Softmax()` function.
    It instead divides the exponent of each probability by the sum of the exponents
    of all the probabilities. The `Softmax()` function is also used in multinomial
    logistic regression instead of the`Logistic()` function for it to function as
    a multi-class classifier without the need for the*One-vs-Rest*or*One-vs-One*strategies.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保所有类别的返回概率加起来为1，*一对多*策略通过将概率除以其总和来规范化这些概率。另一种概率规范化的方法是`Softmax()`函数。它将每个概率的指数除以所有概率指数的总和。`Softmax()`函数也用于多项式逻辑回归，而不是`Logistic()`函数，使其作为多类分类器运作，而无需使用*一对多*或*一对一*策略。
- en: Estimating multiple classification targets
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计多个分类目标
- en: As with `MultiOutputRegressor`, `MultiOutputClassifier` is a meta-estimator
    that allows the underlying estimators to deal with multiple outputs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `MultiOutputRegressor` 一样，`MultiOutputClassifier` 是一个元估算器，允许底层估算器处理多个输出。
- en: 'Let''s create a new dataset to see how we can use `MultiOutputClassifier`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的数据集，看看如何使用 `MultiOutputClassifier`：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first thing to notice here is that the terms `n_classes` and `n_labels`
    are misleading in the `make_multilabel_classification` helper. The previous setting
    creates 500 samples with 3 binary targets. We can confirm this by printing the
    shapes of the returned `x` and `y`, as well as the cardinality of the y''s:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里首先需要注意的是，`n_classes` 和 `n_labels` 这两个术语在 `make_multilabel_classification`
    辅助函数中具有误导性。前面的设置创建了 500 个样本，包含 3 个二元目标。我们可以通过打印返回的 `x` 和 `y` 的形状，以及 `y` 的基数来确认这一点：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We then force the third label to be perfectly dependent on the first one. We
    will make use of this fact in a moment:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们强制第三个标签完全依赖于第一个标签。我们稍后会利用这个事实：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After we split our dataset as we usually do, and dedicate 25% for testing,
    wewill notice that`GradientBoostingClassifier`is not able to deal with the three
    targets we have. Some classifiers are able to deal with multiple targets without
    any external help. Nonetheless, the `MultiOutputClassifier` estimator is required
    for the classifier we decided to use this time:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在像往常一样划分数据集，并将 25% 用于测试之后，我们会注意到 `GradientBoostingClassifier` 无法处理我们所拥有的三个目标。一些分类器能够在没有外部帮助的情况下处理多个目标。然而，`MultiOutputClassifier`
    估算器是我们这次决定使用的分类器所必需的：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We already know that the first and third targets are dependent. Thus, a `ClassifierChain`
    may be a good alternative to try instead of an `MultiOutputClassifier` estimator.
    We can then dictate the target''s dependencies using its `order` hyperparameter
    as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，第一个和第三个目标是相关的。因此，`ClassifierChain` 可能是一个很好的替代选择，可以尝试代替 `MultiOutputClassifier`
    估算器。然后，我们可以使用它的 `order` 超参数来指定目标的依赖关系，如下所示：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, if we display the coefficients of the third estimator as we did earlier
    with the `RegressorChain`, we can see that it just copied the predictions it made
    for the first target and used them as they are. Hence, all the coefficients were
    set to zero except for the coefficient assigned to the first target, as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们像之前对 `RegressorChain` 所做的那样，显示第三个估算器的系数，我们可以看到它只是复制了对第一个目标所做的预测，并直接使用这些预测。因此，除了分配给第一个目标的系数外，所有系数都被设置为零，如下所示：
- en: '![](img/94202336-1625-477a-ad3d-8994ebdd6e19.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94202336-1625-477a-ad3d-8994ebdd6e19.png)'
- en: As you can see, we are covered whenever the estimators we want to use do not
    support multiple targets. We are also able to tell our estimators which targets
    to use when predicting the next one.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每当我们希望使用的估算器不支持多个目标时，我们都能得到覆盖。我们还可以告诉我们的估算器在预测下一个目标时应使用哪些目标。
- en: In many real-life scenarios, we care about the classifier's predicted probabilities
    more than its binary decisions. A well-calibrated classifier produces reliable
    probabilities, which are paramount in risk calculations and in achieving higher
    precision.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实生活中的场景中，我们更关心分类器预测的概率，而不是它的二元决策。一个良好校准的分类器会产生可靠的概率，这在风险计算中至关重要，并有助于实现更高的精度。
- en: In the next section, we will see how to calibrate our classifiers if their estimated
    probabilities are not reliable by default.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将看到如何校准我们的分类器，特别是当它们的估计概率默认情况下不可靠时。
- en: Calibrating a classifier's probabilities
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 校准分类器的概率
- en: '"Every business and every product has risks. You can''t get around it."'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: “每个企业和每个产品都有风险。你无法回避它。”
- en: – Lee Iacocca
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: – 李·艾科卡
- en: 'Say we want to predict whether someone will catch a viral disease. We can then
    build a classifier to predict whether they will catch the viral infection or not.
    Nevertheless, when the percentage of those who may catch the infection is too
    low, the classifier''s binary predictions may not be precise enough. Thus, with
    such uncertainty and limited resources, we may want to only put in quarantine
    those with more than a 90% chance of catching the infection. The classifier''s
    predicted probability sounds like a good source for such estimation. Nevertheless,
    we can only call this probability reliable if 9 out of 10 of the samples we predict
    to be in a certain class with probabilities above 90% are actually in this class.
    Similarly, 80% of the samples with probabilities above 80% should also end up
    being in that class. In other words, for a perfectly calibrated model, we should
    get the following 45^o line whenever we plot the % of samples in the target class
    versus the classifier''s predicted probabilities:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要预测某人是否会感染病毒性疾病。然后我们可以构建一个分类器来预测他们是否会感染该病毒。然而，当可能感染的人群比例过低时，分类器的二分类预测可能不够精确。因此，在这种不确定性和有限资源的情况下，我们可能只想将那些感染概率超过90%的人隔离起来。分类器的预测概率听起来是一个很好的估算来源。然而，只有当我们预测为某一类别且其概率超过90%的样本中，90%（9个中有9个）最终确实属于该类别时，这个概率才能被认为是可靠的。同样，对于80%以上的概率，最终80%的样本也应该属于该类别。换句话说，对于一个完美校准的模型，我们在绘制目标类别样本百分比与分类器预测概率之间的关系时，应该得到一条45°的直线：
- en: '![](img/a7569ea0-34e5-4a49-b662-30e8a783cacb.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7569ea0-34e5-4a49-b662-30e8a783cacb.png)'
- en: 'Some models are usually well calibrated, such as the logistic regression classifier.
    Some other models require us to calibrate their probabilities before using them.
    To demonstrate this, we are going to create the following binary-classification
    dataset, with 50,000 samples and `15` features. I used a lower value for `class_sep`
    to ensure that the two classes aren''t easily separable:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型通常已经经过良好的校准，例如逻辑回归分类器。另一些模型则需要我们在使用之前对其概率进行校准。为了演示这一点，我们将创建一个以下的二分类数据集，包含50,000个样本和`15`个特征。我使用了较低的`class_sep`值，以确保这两个类别不容易分开：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then I trained a Gaussian Naive Bayes classifier and stored the predicted probabilities
    of the positive class. Naive Bayes classifiers tend to return unreliable probabilities
    due to their naive assumption, as we discussed in[Chapter 6](0bad86d8-cebe-4da0-a28e-611d9d7b0a65.xhtml),
    *Classifying Text using Naive Bayes*. The `GaussianNB` classifier is used here
    since we are dealing with continuous features:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我训练了一个高斯朴素贝叶斯分类器，并存储了正类的预测概率。由于其天真的假设，朴素贝叶斯分类器通常会返回不可靠的概率，正如我们在[第6章](0bad86d8-cebe-4da0-a28e-611d9d7b0a65.xhtml)《使用朴素贝叶斯分类文本》中讨论的那样。由于我们处理的是连续特征，因此这里使用`GaussianNB`分类器：
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Scikit-learn has tools for plotting the calibration curves for our classifiers.
    It splits the estimated probabilities into bins and calculates the fraction of
    the sample that falls in the positive class for each bin. In the following code
    snippet, we set the number of bins to `10`, and use the calculated probabilities
    to create a calibration curve:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn提供了绘制分类器校准曲线的工具。它将估计的概率划分为多个区间，并计算每个区间中属于正类的样本比例。在以下代码片段中，我们将区间数设置为`10`，并使用计算出的概率来创建校准曲线：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'I skipped the parts of the code responsible for the graph''s formatting for
    brevity. Running the code gives me the following curve:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我为了简洁起见，省略了负责图形格式化的代码部分。运行代码后，我得到了以下的曲线：
- en: '![](img/0f59b05a-027b-462c-b7ee-a5f6c680de9c.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f59b05a-027b-462c-b7ee-a5f6c680de9c.png)'
- en: 'As you can tell, the model is far from being calibrated. Hence, we can use
    `CalibratedClassifierCV` to adjust its probabilities:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型远未经过校准。因此，我们可以使用`CalibratedClassifierCV`来调整其概率：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the next graph, we can see the effect of `CalibratedClassifierCV`**on the
    model, where the new probability estimates are more reliable:**
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到`CalibratedClassifierCV`对模型的影响，其中新的概率估算更加可靠：**
- en: '**![](img/f98678b8-ec51-45e4-bd52-051d2ea40076.png)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/f98678b8-ec51-45e4-bd52-051d2ea40076.png)'
- en: '`CalibratedClassifierCV`uses two calibration methods: the**`sigmoid()`and`isotonic()`methods.
    The `sigmoid()` method is recommended for small datasets since the `isotonic()`
    method tends to overfit. Furthermore, the calibration should be done on separate
    data from that used for fitting the model.`CalibratedClassifierCV`allows us to
    cross-validate to separate the data used for fitting the underlying estimator
    from the data used for calibration. A three-fold cross-validation was used in
    the previous code.**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`CalibratedClassifierCV`使用两种校准方法：**`sigmoid()`和`isotonic()`方法。推荐在小数据集上使用`sigmoid()`方法，因为`isotonic()`方法容易过拟合。此外，校准应在与模型拟合时使用的不同数据上进行。`CalibratedClassifierCV`允许我们进行交叉验证，将用于拟合基础估计器的数据与用于校准的数据分开。在之前的代码中使用了三折交叉验证。**'
- en: '**If linear regression aims to minimize the squared errors while assuming the
    relation between the targets, *y*, and the features, *x*, to be a linear equation
    expressed by *y = f(x)*, then isotonic regression aims to minimize the squared
    errors with a different assumption. It assumes *f(x)*to be a non-linear yet monotonic
    function. In other words, it either continues to increase or decrease with the
    increase of *x*. This monotonicity attribute of isotonic regression**makes it
    suitable for probability calibration.**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果线性回归旨在最小化平方误差，并假设目标*y*与特征*x*之间的关系为由*y = f(x)*表示的线性方程，那么等距回归则有不同的假设，旨在最小化平方误差。它假设*f(x)*是一个非线性但单调的函数。换句话说，它随着*x*的增大要么持续增加，要么持续减少。等距回归的这种单调性特征**使其适用于概率校准。**'
- en: '**Besides calibration graphs, the **Brier score** is a good way to check whether
    a model is calibrated or not. It basically calculates the **Mean Squared Error
    (MSE)**between the predicted probabilities and the actual targets. Thus, a lower
    Brier score reflects more reliable probabilities.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**除了校准图，**Brier得分**是检查模型是否校准的好方法。它基本上计算了预测概率与实际目标之间的**均方误差（MSE）**。因此，较低的Brier得分反映出更可靠的概率。**'
- en: In the next section, we are going to learn how to use a classifier to order
    a list of predictions and then how to evaluate this order.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用分类器对预测结果进行排序，然后如何评估这个排序。
- en: Calculating the precision at k
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算k时的精度
- en: In the example of the viral infection from the previous section, your quarantine
    capacity may be limited to, say, 500 patients. In such a case, you would want
    as many positive cases to be in the top 500 patients according to their predicted
    probabilities. In other words, we do not care much about the model's overall precision,
    since we only care about its precision for the top `k` samples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中关于病毒感染的示例中，您的隔离能力可能仅限于例如500名患者。在这种情况下，您会希望根据预测概率，尽可能多的阳性病例出现在前500名患者中。换句话说，我们不太关心模型的整体精度，因为我们只关心它在前`k`样本中的精度。
- en: 'We can calculate the precision for the top `k` samples using the following
    code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码计算前`k`样本的精度：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If you are not a big fan of the functional programming paradigm, then let me
    explain the code to you in detail. The `zip()` method combines the two lists and
    returns a list of tuples. The first tuple in the list will contain the first item
    of `y_true` along with the first item of `y_pred_proba`. The second tuple will
    hold the second item of each of them, and so on. Then, I sorted the list of tuples
    in descending order (`reverse=True`) based on the second items of the tuples,
    that is, `y_pred_proba`.Then, I took the top `k` items of the sorted list of tuples
    and compared the `y_true` part of them to the `pos_label` parameter. The `pos_label`
    parameter allows me to decide which label to base my precision calculations on.
    Finally, I calculated the ratio of items in `topk`where an actual member of the
    class specified by`pos_label` is captured.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不太喜欢函数式编程范式，那么让我详细解释一下代码。`zip()`方法将两个列表合并，并返回一个元组列表。列表中的第一个元组将包含`y_true`的第一个项以及`y_pred_proba`的第一个项。第二个元组将包含它们的第二个项，以此类推。然后，我根据元组的第二个元素，即`y_pred_proba`，对元组列表按降序进行排序（`reverse=True`）。接着，我取排序后的前`k`个元组，并将它们的`y_true`部分与`pos_label`参数进行比较。`pos_label`参数允许我决定基于哪个标签进行精度计算。最后，我计算了在`topk`中实际属于`pos_label`类的元素所占的比例。
- en: 'Now, we can calculate the precision for the top 500 predictions made by the
    uncalibrated `GaussianNB` classifier:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算未校准的`GaussianNB`分类器在前500个预测中的精度：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This gives us a precision of `82%` for the top `500` samples, compared to the
    overall precision of `62%` for all the positively classified samples. Once more,
    your results may differ from mine.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了前`500`个样本的`82%`精度，相比之下，所有正类样本的总体精度为`62%`。再次提醒，你的结果可能与我的不同。
- en: The precision at the `k` metric is a very useful tool when dealing with imbalanced
    data or classes that aren't easy to separate, and you only care about the model's
    accuracy for the top few predictions. It allows you to tune your model to capture
    the samples that matter the most. I bet Google cares about the search results
    you see on the first page way more than the results on the 80^(th) page. And if
    I only have money to buy 20 stocks in the stock exchange, I would like a model
    that gets the top 20 stocks right, and I wouldn't care much about its accuracy
    for the 100^(th) stock.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`k` 精度指标是处理不平衡数据或难以分离的类别时非常有用的工具，尤其是当你只关心模型在前几个预测中的准确度时。它允许你调整模型，以捕捉最重要的样本。我敢打赌，谷歌比起你在第80页看到的搜索结果，更关心你在第一页看到的结果。而如果我只有足够的钱购买20只股票，我希望模型能正确预测前20只股票的走势，对于第100只股票的准确性我倒不太关心。'
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: When dealing with a classification or a regression problem, we tend to start
    by thinking about the features we should include in our models. Nonetheless, it
    is often that the key to the solution lies in the target values. As we have seen
    in this chapter, rescaling our regression target can help us use a simpler model.
    Furthermore, calibrating the probabilities given by our classifiers may quickly
    give a boost to our accuracy scores and help us quantify our uncertainties. We
    also learned how to deal with multiple targets by writing a single estimator to
    predict multiple outputs at once. This helps to simplify our code and allows the
    estimator to use the knowledge it learns from one label to predict the others.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理分类或回归问题时，我们通常会首先考虑我们应该在模型中包含哪些特征。然而，解决方案的关键往往在于目标值。正如我们在本章所看到的，重新缩放我们的回归目标可以帮助我们使用更简单的模型。此外，校准我们分类器给出的概率可以迅速提高我们的准确度，并帮助我们量化不确定性。我们还学会了通过编写一个单一的估计器来同时预测多个输出，从而处理多个目标。这有助于简化我们的代码，并使得估计器可以利用从一个标签中学到的知识来预测其他标签。
- en: It is common in real-life classification problems that classes are imbalanced.
    When detecting fraudulent incidents, the majority of your data is usually comprised
    of non-fraudulent cases. Similarly, for problems such as who would click on your
    advertisement, and who would subscribe to your newsletter, it is always the minority
    class that is more interesting for you to detect.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中的分类问题中，类别不平衡是常见的。当检测欺诈事件时，你的数据中大多数通常是非欺诈案例。同样，对于诸如谁会点击你的广告，谁会订阅你的新闻通讯等问题，通常是少数类对你来说更为重要。
- en: In the next chapter, we are going to see how to make it easier for a classifier
    to deal with an imbalanced dataset by altering its training data.**********
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到如何通过修改训练数据来让分类器更容易处理不平衡的数据集。**********
