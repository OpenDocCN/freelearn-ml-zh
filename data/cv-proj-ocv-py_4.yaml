- en: Human Pose Estimation with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行人体姿态估计
- en: In this chapter, we're going to cover human pose estimation with TensorFlow
    using the DeeperCut algorithm. We will learn single-person and multi-person pose
    detection using the DeeperCut and ArtTrack models. Later, we will also learn how
    to use the model with videos and retrain it to use it for the customized images
    in our projects.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用DeeperCut算法通过TensorFlow进行人体姿态估计。我们将学习使用DeeperCut和ArtTrack模型进行单人和多人姿态检测。稍后，我们还将学习如何使用该模型与视频一起使用，并重新训练它以用于我们项目中的定制图像。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Pose estimation with DeeperCut and ArtTrack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DeeperCut和ArtTrack进行姿态估计
- en: Single-person pose detection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单人姿态检测
- en: Multi-person pose detection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多人姿态检测
- en: Videos and retraining
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频和重新训练
- en: Pose estimation using DeeperCut and ArtTrack
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DeeperCut和ArtTrack进行姿态估计
- en: Human pose estimation is the process of estimating the configuration of the
    body (pose) from an image or video. It includes landmarks (points), which are
    similar to joints such as the feet, ankles, chin, shoulder, elbows, hands, head,
    and so on. We will be doing this automatically using deep learning. If you consider
    a face, the landmarks are relatively rigid or, rather, relatively constant from
    face to face, such as the relative position of the eyes to the nose, the mouth
    to the chin, and so forth.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计是从图像或视频中估计身体（姿态）配置的过程。它包括地标（点），这些点类似于脚、脚踝、下巴、肩膀、肘部、手、头部等关节。我们将使用深度学习自动完成这项工作。如果您考虑面部，地标相对刚性，或者说相对恒定，例如眼睛相对于鼻子的相对位置，嘴巴相对于下巴，等等。
- en: 'The following photo provides an example:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下照片提供了一个示例：
- en: '![](img/a2c43966-d8da-481e-aff1-ff313b244730.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2c43966-d8da-481e-aff1-ff313b244730.png)'
- en: 'Although the body structure remains the same, our bodies aren''t rigid. So,
    we need to detect the different parts of our body relative to the other parts.
    For example, detecting the feet relative to the knee is very challenging compared
    to facial detection. Also, we can move our hands and feet, which can lead to a
    wide variety of positions. The following picture gives an example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然身体结构保持不变，但我们的身体不是刚性的。因此，我们需要检测身体的不同部位相对于其他部位的位置。例如，相对于膝盖检测脚部是非常具有挑战性的，与面部检测相比。此外，我们可以移动我们的手和脚，这可能导致各种各样的姿势。以下图片提供了一个示例：
- en: '![](img/85561526-7b48-492b-b381-eb42bfddb855.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85561526-7b48-492b-b381-eb42bfddb855.png)'
- en: This was very difficult until we had some breakthroughs in computer vision from
    different groups around the world. Different code has been developed to carry
    out pose estimation, but we will cover an algorithm called **DeeperCut**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们从世界各地不同研究小组在计算机视觉方面的突破之前是非常困难的。已经开发了不同的代码来执行姿态估计，但我们将介绍一个名为**DeeperCut**的算法。
- en: You can refer to MPII Human Pose Models ([pose.mpi-inf.mpg.de](http://pose.mpi-inf.mpg.de))
    for detailed information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考MPII人体姿态模型([pose.mpi-inf.mpg.de](http://pose.mpi-inf.mpg.de))以获取详细信息。
- en: 'DeeperCut was developed by a group in Germany at the Max Planck Society, in
    conjunction with Stanford University, who released their algorithm and published
    papers. It is recommended to checkout their paper *DeepCut: Joint Subset Partition
    and Labeling for Multi Person Pose Estimation*, which gives an overview of an
    earlier algorithm, before DeeperCut, where they talk about how they detected body
    parts and how they ran an optimization algorithm to achieve good results. You
    can also refer to their subsequent paper, *DeeperCuts*: *a deeper, stronger and
    faster multi person pose estimation model*, which was published by the same group
    of authors, as this will cover a lot of the technical details. We will definitely
    not get exact results, but you can determine things with a reasonable amount of
    probability.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DeeperCut是由德国马克斯·普朗克学会的一个研究小组开发的，与斯坦福大学合作，他们发布了他们的算法并发表了论文。建议查看他们的论文《DeepCut：用于多人姿态估计的联合子集划分和标记》，该论文概述了DeeperCut之前的早期算法，其中他们讨论了如何检测身体部位以及他们如何运行优化算法以获得良好的结果。您还可以参考他们后续的论文《DeeperCuts》：*一个更深、更强、更快的多人姿态估计模型*，该论文由同一组作者发表，这将涵盖许多技术细节。我们肯定不会得到精确的结果，但您可以用合理的概率确定一些事情。
- en: 'On the GitHub page, [https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow),
    there is the public implementation of their code, which covers DeeperCut and a
    new version called ArtTrack. It is articulated multi-person tracking in the wild,
    and you can see the output result in the following photo:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub页面[https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow)，有他们代码的公开实现，包括DeeperCut和一个新版本ArtTrack。这是在野外进行的人体姿态跟踪，你可以在下面的照片中看到输出结果：
- en: '![](img/aaf57d4e-3c54-4ea2-8635-9385de2c2ab3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aaf57d4e-3c54-4ea2-8635-9385de2c2ab3.png)'
- en: We are going to run a modified version of the code, which is made to run in
    the Jupyter Notebook environment and is made for all learning purposes, so it
    should be a little easier than just getting it straight from GitHub. We will learn
    exactly how we can run the code and use it in our own projects. All of the pre-trained
    models are included here: [https://github.com/eldar/pose-tensorflow.](https://github.com/eldar/pose-tensorflow)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行一个修改后的代码版本，它被设计在Jupyter Notebook环境中运行，并且适用于所有学习目的，因此它应该比直接从GitHub获取要简单一些。我们将学习如何运行代码并在我们的项目中使用它。所有预训练的模型都包含在这里：[https://github.com/eldar/pose-tensorflow.](https://github.com/eldar/pose-tensorflow)
- en: Single-person pose detection
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单人姿态检测
- en: Now that we have an overview of human pose estimation and the new DeeperCut
    algorithm, we can run the code for single-person pose detection and check that
    out in the Jupyter Notebook.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了人体姿态估计和新的DeeperCut算法的概述，我们可以运行单人姿态检测的代码，并在Jupyter Notebook中检查它。
- en: We will start with single-person detection. Before starting, we need to make
    sure that we are using a clean kernel. You can restart your kernel, or you can
    use the hotkeys to do the same. You can then press the *0* key twice when you're
    in command mode, which is opposed to edit mode when you're actually editing the
    cells.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从单人检测开始。在开始之前，我们需要确保我们使用的是一个干净的内核。你可以重启你的内核，或者你可以使用快捷键来完成同样的操作。当你处于命令模式时，你可以按下*0*键两次，这与实际编辑单元格时的编辑模式相反。
- en: 'Let''s start with our single-person detection code, as shown in the following
    example:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下示例中的单人检测代码开始：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The exclamation mark means execute a shell command. This will install a couple
    of libraries that you might not have, and if you have Python 3 installed in your
    system, you might need to change the command to `pip 3`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 感叹号表示执行一个shell命令。这将安装一些你可能没有的库，如果你在你的系统中安装了Python 3，你可能需要将命令更改为`pip 3`。
- en: 'In the next cell, we will call the `%pylab notebook` function, which will allow
    us to look at images with some useful widgets in the notebook, as well as load
    some numerical libraries, such as `numpy` and so forth. We will do some general
    imports, such as `os`, `sys` and `cv2`. To do some annotations, we will use `imageio` for
    the `imread` function and get everything from `randint`. You don''t need to import
    `numpy` because we have already used `%pylab notebook`, but in case you want to
    copy and paste this code outside of the notebook, you will need it. Then, we need
    to import `tensorflow`, which already has some glued utilities here that come
    from the `pose-tensorflow` repository. The code, for your reference, is shown
    in the following example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个单元格中，我们将调用`%pylab notebook`函数，这将允许我们在笔记本中使用一些有用的控件查看图像，以及加载一些数值库，例如`numpy`等。我们将进行一些通用导入，例如`os`、`sys`和`cv2`。为了进行注释，我们将使用`imageio`的`imread`函数并从`randint`获取一切。你不需要导入`numpy`，因为我们已经使用了`%pylab
    notebook`，但如果你想在笔记本外复制粘贴此代码，你需要它。然后，我们需要导入`tensorflow`，它已经包含了一些来自`pose-tensorflow`仓库的粘合工具。代码，仅供参考，如下所示：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We then execute the preceding cell.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行前面的单元格。
- en: 'We will now set up pose prediction, as shown in the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将设置姿态预测，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It will start our session and load our model. We will be using a pre-trained
    model, which you can quickly access from the GitHub repository. The `tf.Session()`
    will start the TensorFlow session and save it to the `sess` variable, which we're
    going to return. Note that when you run this function, it's going to leave the
    TensorFlow session open, so if you want to move on and do something else, such
    as load a new model, then you will have to close the session or restart. It's
    useful here because we're going to be looking at multiple images and it will be
    slower if you load the session every single time. We will then take the configuration,
    which loads the corresponding model and variables, and is going to return the
    necessary values in order to actually run the model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它将启动会话并加载我们的模型。我们将使用一个预训练模型，您可以从GitHub仓库快速访问它。`tf.Session()`将启动TensorFlow会话并将其保存到`sess`变量中，我们将返回它。请注意，当您运行此函数时，它将保持TensorFlow会话开启，所以如果您想继续做其他事情，比如加载新模型，那么您将不得不关闭会话或重新启动。在这里这很有用，因为我们将要查看多张图片，如果每次都加载会话，将会更慢。然后，我们将获取配置，它加载相应的模型和变量，并将返回运行模型所需的必要值。
- en: 'Then, we extract CNN outputs using the `extract_cnn_outputs` function. In the
    output, we''ll get joint locations to know where everything is, exactly relative
    to something else. We want a nice ordered 2D array where we know the X and Y locations
    of where the ankles, hands, or shoulders are present. This is demonstrated in
    the following example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`extract_cnn_outputs`函数提取CNN输出。在输出中，我们将获得联合位置，以了解一切相对于其他事物的确切位置。我们希望得到一个有序的二维数组，其中我们知道脚踝、手或肩膀的位置的X和Y坐标。以下是一个示例：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is going to take the output from the neural network (which is kind of
    unintelligible) and put it in a form we can actually use. Then, we will feed the
    output to something else, or just visualize it in this case. `argmax_pose_predict`
    is complementary to what we did before. It is another utility function that is
    going to help us understand the output, which is shown in the following example:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把神经网络输出（有点难以理解）转换成我们可以实际使用的格式。然后，我们将输出传递给其他东西，或者在这种情况下可视化它。`argmax_pose_predict`与我们之前所做的是互补的。它是一个辅助函数，将帮助我们理解输出，以下是一个示例：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's now execute that cell in which we have defined the functions. It will
    run instantly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行定义函数的那个单元格。它将立即运行。
- en: 'The following code will load the configuration file, which is `demo/pose_cfg.yaml`,
    and `setup_pose_prediction(cfg)` will return `sess`, `inputs`, and `outputs`.
    This is shown in the following example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将加载配置文件，即`demo/pose_cfg.yaml`，`setup_pose_prediction(cfg)`将返回`sess`、`inputs`和`outputs`。以下是一个示例：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When we run the preceding code, it will leave the TensorFlow session open and
    it is recommended to run it only once to avoid errors, or you might have to restart
    the kernel. So, if the command gets executed, we understand that the model has
    been loaded, as you can see in the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，它将保持TensorFlow会话开启，建议只运行一次以避免错误，或者您可能需要重新启动内核。因此，如果命令被执行，我们理解模型已经被加载，正如您在以下输出中看到的那样：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we''ll see how to actually apply the model:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何实际应用该模型：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For our model, we have to give our file a name. So, we have a directory called
    `testcases` with a bunch of stock photos of people in various poses, which we
    will be using for our test. We then need to load the `standing-leg-lift.jpg` image
    in a suitable format. We will convert the image to something that TensorFlow actually
    needs. The input is like an `image_batch`, which is going to expand the dimensions
    along the `0` axis. So, just create an array that TensorFlow can actually use.
    Then, `outputs_np` will run the session, extract the CNN output in the next line,
    and then do the actual pose prediction. The `pose` variable is the best to use
    here. We should then execute the cell and hit *Esc* button to get into the command
    mode. Then, we need to create a new cell; type `pose` and hit C*trl *+ E*nter*.
    We will then get the following 2D array output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们必须给我们的文件命名。因此，我们有一个名为`testcases`的目录，里面有一系列不同姿势的人的股票照片，我们将使用这些照片进行测试。然后，我们需要以合适的格式加载`standing-leg-lift.jpg`图像。我们将把图像转换成TensorFlow实际需要的格式。输入类似于`image_batch`，它将在`0`轴上扩展维度。所以，只需创建TensorFlow可以使用的数组。然后，`outputs_np`将运行会话，在下一行提取CNN输出，然后进行实际的姿态预测。`pose`变量在这里使用最好。然后我们应该执行单元格并按*Esc*按钮进入命令模式。然后，我们需要创建一个新的单元格；输入`pose`并按*C*trl
    *+ E*nter*。然后我们将得到以下2D数组输出：
- en: '![](img/e66b51af-effe-45c2-8b51-20e4e815f1fc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e66b51af-effe-45c2-8b51-20e4e815f1fc.png)'
- en: 'The output gives us the `x` and `y` coordinates corresponding to the joints
    such as wrists, ankles, knees, head, chin, shoulders, and so on. From this, we
    get the `x` coordinate, `y` coordinate, and matching score. We do not need the
    sub-pixel level, so we can round it to the nearest integer. In the following example,
    you can see that we have labeled our opposing joints with numbers and drawn lines
    between them:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出给出了与手腕、脚踝、膝盖、头部、下巴、肩膀等关节对应的`x`和`y`坐标。从这些坐标中，我们得到`x`坐标、`y`坐标和匹配分数。我们不需要亚像素级别的精度，所以我们可以将其四舍五入到最接近的整数。在下面的示例中，你可以看到我们已经用数字标记了对应的关节，并在它们之间画了线：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We need to create a `pose2D` label here, and then we are going to extract the
    x and y coordinates in the first two columns. We will make a copy using `image.copy()`,
    because we want our annotated image to be separate from our original image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里创建一个`pose2D`标签，然后我们将从前两列中提取x和y坐标。我们将使用`image.copy()`来制作一个副本，因为我们希望我们的注释图像与原始图像分开。
- en: 'We will run the following code to show our original image:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行以下代码来显示原始图像：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are now going to learn how to annotate the original image. We're going to
    create a copy of the image and then we're going to iterate it over the first six
    joints and draw lines between them. It starts on the ankle, `1`, goes up through
    the hips, and then goes down to the the other ankle. Numbers `6` through `11`
    will be the arms and shoulders, and the last two points are the chin and the top
    of the head. We're now going to connect all these points with lines from our `pose2D`.
    We actually don't have points for the `waist` and the `collar`, but we can easily
    estimate those from the midpoints of the hips and the shoulders, which is useful
    for completing the skeleton.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习如何注释原始图像。我们将创建图像的一个副本，然后我们将遍历前六个关节并在它们之间画线。它从脚踝开始，标记为`1`，然后穿过臀部，最后下降到另一只脚踝。数字`6`到`11`将是手臂和肩膀，最后两个点是下巴和头顶。我们现在将使用`pose2D`中的所有这些点用线连接起来。实际上，我们没有`腰部`和`衣领`的点，但我们可以很容易地从臀部和肩膀的中点估计它们，这对于完成骨骼很有用。
- en: 'Let''s look at the following code, which helps us estimate the midpoints:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下代码，它帮助我们估计中点：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can now draw a spine by drawing a point from the waist to the collar, and
    the collar to the chin. We can also label these joints to show exactly what we
    are joining, and this will help in your customized application. We are going to
    label the joints, create the figure, show the annotated image, and deal with random
    colors. The following screenshot shows what the output looks like:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过从腰部到衣领，再从衣领到下巴画点来绘制脊柱。我们还可以标记这些关节，以显示我们连接的确切位置，这有助于你的定制应用。我们将标记关节，创建图形，显示注释图像，并处理随机颜色。以下截图显示了输出看起来像什么：
- en: '![](img/cbe433a8-4db9-4dcc-b044-b7a2d51bad78.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cbe433a8-4db9-4dcc-b044-b7a2d51bad78.png)'
- en: Here, 1 is the right ankle, but it could be the left ankle depending on which
    way the person's facing. So, all the links are joined except for 13, which is
    a bit occluded here, and 14, which is slightly out of the image. The nice thing
    about this is that it potentially works even if other joints are occluded (for
    instance, if they're off-screen or covered up by something). You will notice that
    the image is simple with a flat background, flat floor, and a simple pose and
    clothes. The code will also work with more complicated images, and if you have
    any trouble reading the details, you can use the widgets here and zoom in.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，1是右脚踝，但它可能是左脚踝，取决于人的面向方向。所以，除了13（在这里有点遮挡）和14（稍微超出图像）之外，所有的链接都已经连接。这个的好处是，即使其他关节被遮挡（例如，如果它们在屏幕外或被某物覆盖），它也可能工作。你会注意到图像很简单，有一个平坦的背景，平坦的地板，简单的姿态和衣服。代码也可以处理更复杂的图像，如果你在阅读细节时遇到任何困难，可以使用这里的工具并放大查看。
- en: 'Let''s try using different images and analyze our results, which are shown
    in the following example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用不同的图像并分析我们的结果，如下所示：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following shows the photo we will be testing:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将要测试的图片：
- en: '![](img/a2eb4e31-b79b-46d9-b13a-301bab289c87.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a2eb4e31-b79b-46d9-b13a-301bab289c87.png)'
- en: 'When we run our model again, using a different image, we get the following
    output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们再次运行我们的模型，使用不同的图像时，我们得到以下输出：
- en: '![](img/19445ba0-e0f2-4b91-be29-1b3ab509e062.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19445ba0-e0f2-4b91-be29-1b3ab509e062.png)'
- en: 'If we take an image of a guy with crossed arms, We get the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们拍一个交叉双臂的人的图像，我们会得到以下截图：
- en: '![](img/68002f06-88dc-43a5-8da1-c0a1019254c7.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68002f06-88dc-43a5-8da1-c0a1019254c7.png)'
- en: The result is very good, even though the arms are crossed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 即使双臂交叉，结果仍然非常好。
- en: Now, let's take a look at a few difficult images. This might not give us the
    accurate results of a complete motion capture pose estimation solution, but is
    still very impressive.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些比较困难的图像。这可能不会给我们一个完整的运动捕捉姿态估计解决方案的准确结果，但仍然非常令人印象深刻。
- en: 'Select `acrobatic.jpeg`, which is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 选择`acrobatic.jpeg`，如下所示：
- en: '![](img/e6d30678-98a9-45f0-bbed-2b148f6e243e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e6d30678-98a9-45f0-bbed-2b148f6e243e.png)'
- en: 'The output we get when we run this photo is shown in the following example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这张照片时，得到的输出如下所示：
- en: '![](img/6a998f24-5ca7-4468-a184-b9e35e12a17c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6a998f24-5ca7-4468-a184-b9e35e12a17c.png)'
- en: It looks as if it found the joints, more or less, but did not connect them properly.
    It shows that the guy's head is on his hand, which is touching the ground. We
    can see that the results are not that good. But we cannot expect accurate results
    for all images, even though this is state of the art.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来它找到了关节，或多或少，但没有正确连接它们。它显示这个人的头在他的手上，手触地。我们可以看到结果并不那么好。但我们不能期望所有图像都能得到准确的结果，即使这是最先进的技术。
- en: Multi-person pose detection
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多人姿态检测
- en: Now, let's move from single-person pose detection to multi-person pose detection.
    With single-person pose detection, we saw that the code will take an image of
    a single person and generate pose estimation with all the joints labeled. We will
    now learn a more advanced model called ArtTrack, which will allow us to count
    the number of people, find people, and estimate their poses.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从单人姿态检测转到多人姿态检测。在单人姿态检测中，我们看到代码会取一个单个人的图像，并生成带有所有关节标记的姿态估计。我们现在将学习一个更高级的模型，称为ArtTrack，它将允许我们计数人数，找到人，并估计他们的姿态。
- en: 'Let''s look at the code for multi-person pose detection, which is shown in
    the following example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看多人姿态检测的代码，以下是一个示例：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is a little more complicated. We will first list our directories here using
    the `!ls` command in the current directory, where you will find a file called
    `compile.sh`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点复杂。我们将首先使用当前目录下的`!ls`命令列出我们的目录，在那里你会找到一个名为`compile.sh`的文件。
- en: We need to run this file because there are some binary dependencies in this
    module. But this is a shell script file, and you might face some issues on macOS
    or Linux. So, in order to generate those files/commands that are OS-specific,
    you will need to run that script. For Windows, those binary files have already
    been generated. So, if you are using the latest version of Python and TensorFlow,
    then the files will be compatible and the binary should work.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要运行这个文件，因为这个模块中包含一些二进制依赖项。但是这是一个shell脚本文件，你可能在macOS或Linux上遇到一些问题。因此，为了生成那些特定于操作系统的文件/命令，你需要运行这个脚本。对于Windows，那些二进制文件已经生成好了。所以，如果你使用的是Python和TensorFlow的最新版本，那么文件将是兼容的，二进制文件应该可以正常工作。
- en: If it does not work, you will need to download and install Visual Studio Community.
    There are some instructions that you can follow at [https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow) under
    the `demo` code section for multi-person pose.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它不起作用，您将需要下载并安装 Visual Studio Community。您可以在[https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow)的`demo`代码部分找到一些关于多人姿态的安装说明。
- en: Once you have everything up and running, we can start with our example. Also,
    as we have already discussed, we need to make sure that we restart the kernel.
    This is important because if you have your session already open for running a
    different project, TensorFlow might not be able to compute the code as the previous
    model is loaded. It is always a good practice to start from a fresh kernel.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一切运行正常，我们就可以开始我们的示例。此外，正如我们之前已经讨论过的，我们需要确保重启内核。这是因为如果您已经打开了会话来运行不同的项目，TensorFlow
    可能无法计算代码，因为已经加载了之前的模型。始终从一个全新的内核开始是一个好的做法。
- en: We will run our `%pylab notebook` to make our visualizations and numerix. The
    code works similarly to what we have already covered, where we have the boilerplate
    and load a pre-trained model. The pre-trained model is already included, so we
    don't need to download it. The code will execute within a second because of TensorFlow,
    and we will get the modules imported and load the repositories as well. Also,
    we need to load the model and actually do the predictions separately. If we hit
    C*trl *+ S*hift *+ *-*,we can separate the predictions into different cells to
    make it look neat.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行我们的 `%pylab notebook` 来进行可视化和数值计算。代码的工作方式与我们之前已经覆盖的类似，其中我们有一些模板代码并加载了一个预训练模型。预训练模型已经包含在内，所以我们不需要下载它。由于
    TensorFlow 的原因，代码将在一秒内执行，我们将导入模块并加载存储库。此外，我们还需要分别加载模型并进行预测。如果我们按下 C*trl *+ S*hift *+
    *-*，我们可以将预测分别放入不同的单元格中，使其看起来更整洁。
- en: 'When we run the first cell, we get the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行第一个单元格时，我们得到以下输出：
- en: '![](img/a2797da1-e46c-40d2-8f76-5cc565d2b363.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2797da1-e46c-40d2-8f76-5cc565d2b363.png)'
- en: This is not a big error message, and is because `imread` was defined here; the
    Notebook clobbers that and just gives you a warning message. We can just rerun
    that code to ignore the warning and get a tidy output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个大的错误消息，这是因为在这里定义了 `imread`；笔记本覆盖了它，只给你一个警告消息。我们可以重新运行那段代码来忽略警告并得到整洁的输出。
- en: In this cell, we are going load the configuration file for multiple people provided
    by the authors of ArtTrack/DeeperCut.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个单元格中，我们将加载 ArtTrack/DeeperCut 作者提供的多人配置文件。
- en: 'The following line loads the dataset:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行加载了数据集：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, the following line creates the model and loads it:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，以下行创建模型并加载它：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we execute that, we get the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行这个操作时，我们得到以下输出：
- en: '![](img/75cf5251-4345-430b-8250-fac3e0b66805.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75cf5251-4345-430b-8250-fac3e0b66805.png)'
- en: We will keep the session open here so that we can keep running different things
    and quickly run through different frames.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里保持会话开启，这样我们就可以继续运行不同的事情，并快速浏览不同的帧。
- en: 'We will now run our code for some test cases that actually have multiple people,
    as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将运行一些测试案例，这些案例实际上有多个人，如下所示：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We need to go to the `np.array` and convert it to a flat array network to compute
    the predictions with `sess.run`, and then extract the CNN output and `detections`
    using the model utilities. We will not label the bones here, but we will instead
    label the joints with numbers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将 `np.array` 转换为平面数组网络，以便使用 `sess.run` 进行预测，然后使用模型工具提取 CNN 输出和 `detections`。我们在这里不会标记骨骼，而是用数字标记关节。
- en: 'When we run the code, we get the following output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们得到以下输出：
- en: '![](img/19c5a46a-bb1f-4893-b003-02abe003965d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19c5a46a-bb1f-4893-b003-02abe003965d.png)'
- en: 'This is a simple image of multiple people, in plain dress and with a flat background.
    This actually worked. However, the numbers aren''t the same as before. Previously,
    number 1 corresponded to the right ankle and went up through 2, 3, 4, 5, and 6,
    and then 7 was the right wrist, and so on. So, the numbers are different, and
    there are more of them, which actually detects more joints because they have multiple
    numbers for the face, so there are multiple points here. Let''s zoom in to check
    the details, as shown in the following picture:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张多人的简单图像，穿着朴素的衣服，背景平坦。这实际上起作用了。然而，数字与之前不同。之前，数字1对应右脚踝，向上通过2、3、4、5和6，然后7是右腕，以此类推。所以，数字不同，而且更多，这实际上检测到了更多的关节，因为面部有多个数字，所以这里有多点。让我们放大查看细节，如下面的图片所示：
- en: '![](img/53647dd1-9068-4d16-b90c-91ae7364ca89.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/53647dd1-9068-4d16-b90c-91ae7364ca89.png)'
- en: Here, we have the facial landmarks as 1, 2, 3, 4, and 5, and hence this could
    be used in conjunction with the dlib detector, which is covered in [Chapter 6](c59fb392-c966-4da6-987a-625378474e71.xhtml),
    *Facial Feature Tracking and Classification with dlib*. If we want to know somebody's
    facial expression, in addition to the full-body landmark detectors and their pose,
    then this could be done here. We can also get a really thorough description of
    which way people are facing and exactly what they're doing within the image.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有了面部特征点1、2、3、4和5，因此这可以与第6章中提到的dlib检测器结合使用，即*dlib的面部特征追踪和分类*。如果我们想了解某人的面部表情，除了全身特征检测器和它们的姿态，这里也可以做到。我们还可以得到一个非常详细的描述，说明人们面向哪个方向，以及他们在图像中确切在做什么。
- en: 'Let''s try another `exercise_class.jpeg` image, which gives us the following
    output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个`exercise_class.jpeg`图像，它给出了以下输出：
- en: '![](img/5770e208-4320-4fd7-8d02-c2567cc740b4.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5770e208-4320-4fd7-8d02-c2567cc740b4.png)'
- en: Here, we can see how multiple points are present on the knees for the lady on
    the extreme right. It is still a good result.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到最右侧的女士膝盖上有多个点。这仍然是一个不错的结果。
- en: Let's try one more image, which we saw previously on the GitHub page, `gym.png`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一张图片，这是我们之前在GitHub页面上看到的，`gym.png`。
- en: 'You can see the output as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到以下输出：
- en: '![](img/8ab0b140-7c79-4dda-952f-4f795d32db80.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8ab0b140-7c79-4dda-952f-4f795d32db80.png)'
- en: This does detect the body parts here. So, let's try using this model to detect
    the pose for a single person. Do you think it will work? The answer is *yes*,
    it does work. You must be wondering why we would use the previous model if this
    is available. This model is slightly more computationally efficient, so if you
    know you only have one person, you don't actually need it, because this algorithm
    provides the number of people.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型确实检测到了这里的身体部位。所以，让我们尝试使用这个模型来检测单个人的姿态。你认为它会起作用吗？答案是*是的，它确实起作用了*。你可能想知道为什么我们有这个模型，还要使用之前的模型。这个模型在计算上稍微高效一些，所以如果你知道只有一个人，实际上你不需要它，因为这个算法提供了人数。
- en: 'You can select the photo of a single person from among the photos available.
    For example, we''ll select `mountain_pose.jpg`, which gives the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从可用的照片中选择单个人的照片。例如，我们将选择`mountain_pose.jpg`，它给出了以下输出：
- en: '![](img/a40eaebf-7ee6-440f-9b1a-491c13fd042f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a40eaebf-7ee6-440f-9b1a-491c13fd042f.png)'
- en: 'It will also show the number of people, as demonstrated by the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这也将显示人数，如下面的代码所示：
- en: '![](img/c96952fb-1a85-4c57-9c85-272547f8ff93.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c96952fb-1a85-4c57-9c85-272547f8ff93.png)'
- en: But, if you use the multi-person detector for a single person, it might be prone
    to over-fitting and detecting more people than are actually in the image. So,
    if you already know there is only one person, then it may still be a good idea
    to just use that original model rather than the ArtTrack model. But if it does
    work, try both, or use whatever is best for your application. However, this might
    not work perfectly for complex images and a complicated variety of poses.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你为单个人使用多人检测器，它可能会过度拟合，并检测到图像中实际不存在的人数。所以，如果你已经知道只有一个人，那么仍然使用原始模型而不是ArtTrack模型可能仍然是一个好主意。但如果它确实起作用，尝试两者，或者使用最适合你应用的方法。然而，这可能在复杂图像和复杂多样的姿态上可能不会完美工作。
- en: 'Let''s try one last `island_dance.jpeg` image. The following screenshot shows
    the result:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试最后一个`island_dance.jpeg`图像。以下截图显示了结果：
- en: '![](img/ca3d7ae4-b8d8-4b8f-a451-b77bd2361b8f.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca3d7ae4-b8d8-4b8f-a451-b77bd2361b8f.png)'
- en: Retraining the human pose estimation model
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新训练人体姿态估计模型
- en: 'We will now discuss how to handle videos and retrain our human pose estimation
    network. We have already covered face detection and how to apply a model to a
    video. Opening a video is pretty straightforward and OpenCV provides a mechanism
    for that. It''s basically doing the same thing one frame at a time. The following
    example shows the code for this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论如何处理视频以及重新训练我们的人类姿态估计网络。我们已经涵盖了人脸检测以及如何将模型应用于视频。打开视频相当直接，OpenCV 提供了相应的机制。它基本上是逐帧执行相同的事情。以下示例显示了相应的代码：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'First, we need to create a `cv2` capture device, then open the file, and while
    reading the file, we should load the image and run the network on the image. Please
    refer to the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个 `cv2` 捕获设备，然后打开文件，在读取文件的同时，我们应该加载图像并在图像上运行网络。请参考以下代码：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Using a good GPU, we should be able to do the computation in few frames per
    second, if not 30 to 60 FPS, depending on your hardware. You should be able to
    do it almost in real time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个好的 GPU，我们应该能够以每秒几帧的速度进行计算，如果不是 30 到 60 FPS，这取决于你的硬件。你应该几乎能够实时完成。
- en: For training your model, you need to first make sure that you have good hardware
    and a lot of time. First, you need to download the ImageNet and ResNet models.
    Then, you need to go through the steps and instructions on the [https://github.com/eldar/pose-tensorflow/blob/master/models/README.md](https://github.com/eldar/pose-tensorflow/blob/master/models/README.md) page.
    You will need a lot of data, so you can use the data they provide. Using your
    own data could be time consuming and difficult to obtain, but it is possible.
    You can refer to the previous link provided for complete instructions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练你的模型，你首先需要确保你有良好的硬件和大量的时间。首先，你需要下载 ImageNet 和 ResNet 模型。然后，你需要查看[https://github.com/eldar/pose-tensorflow/blob/master/models/README.md](https://github.com/eldar/pose-tensorflow/blob/master/models/README.md)页面上的步骤和说明。你需要大量的数据，因此你可以使用他们提供的数据。使用你自己的数据可能既耗时又难以获得，但这是可能的。你可以参考提供的上一个链接以获取完整的说明。
- en: The instructions here use MATLAB at one point to convert the data, although
    there are ways to do that in Python and train the model with the MS COCO dataset.
    This is similar to what we did in [Chapter 2](d5bf725b-9e79-4865-96e9-338481599464.xhtml),
    *Image Captioning with TensorFlow* and it also provides instructions on how to
    train the model with your own data set. This involves a lot of work and a lot
    of computational power. You can try this or use what has already been provided
    in the pre-trained model, which can do a lot of things.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的说明在某些地方使用了 MATLAB 来转换数据，尽管在 Python 中也有方法可以做到这一点，并用 MS COCO 数据集训练模型。这与我们在[第
    2 章](d5bf725b-9e79-4865-96e9-338481599464.xhtml)中做的类似，即使用 TensorFlow 进行图像标题生成，同时也提供了如何使用自己的数据集训练模型的说明。这需要大量的工作和计算能力。你可以尝试这样做，或者使用预训练模型中已经提供的内容，这些内容可以做很多事情。
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned the basics of human pose estimation and then used
    the DeeperCut and ArtTrack models in our project for human pose estimation. Using
    these models, we carried out single-person and multi-person pose detection. Towards
    the end of the chapter, we learned how to use the model with videos and retrained
    the model for customized images.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了人类姿态估计的基础知识，然后在我们的项目中使用了 DeeperCut 和 ArtTrack 模型进行人类姿态估计。使用这些模型，我们进行了单人和多人的姿态检测。在章节的末尾，我们学习了如何使用模型处理视频，并对定制图像重新训练了模型。
- en: In the next chapter, [Chapter 5](c8202017-404a-42aa-9f24-93488e3abd0a.xhtml), *Handwritten
    Digit Recognition with scikit-learn and TensorFlow*, we will learn handwritten
    digit recognition with scikit-learn and TensorFlow.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章[第 5 章](c8202017-404a-42aa-9f24-93488e3abd0a.xhtml)，《使用 scikit-learn 和 TensorFlow
    进行手写数字识别》中，我们将学习如何使用 scikit-learn 和 TensorFlow 进行手写数字识别。
