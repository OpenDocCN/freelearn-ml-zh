- en: 6\. t-Distributed Stochastic Neighbor Embedding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. t-分布随机邻域嵌入
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will discuss **Stochastic Neighbor Embedding** (**SNE**)
    and **t-Distributed Stochastic Neighbor Embedding** (**t-SNE**) as a means of
    visualizing high-dimensional datasets. We will implement t-SNE models in scikit-learn
    and explain the limitations of t-SNE. Being able to extract high-dimensional information
    into lower dimensions will prove helpful for visualization and exploratory analysis,
    as well as being helpful in conjunction with the clustering algorithms we explored
    in prior chapters. By the end of this chapter, we will be able to find clusters
    in high-dimensional data, such as user-level information or images in a low-dimensional
    space.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**随机邻域嵌入**（**SNE**）和**t-分布随机邻域嵌入**（**t-SNE**）作为可视化高维数据集的一种方法。我们将实现t-SNE模型并解释t-SNE的局限性。能够将高维信息提取到低维空间将有助于可视化和探索性分析，同时也能与我们在前几章中探讨的聚类算法相结合。到本章结束时，我们将能够在低维空间中找到高维数据的聚类，例如用户级别信息或图像。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: So far, we have described a number of different methods for reducing the dimensionality
    of a dataset as a means of cleaning the data, reducing its size for computational
    efficiency, or for extracting the most important information available within
    the dataset. While we have demonstrated many methods for reducing high-dimensional
    datasets, in many cases, we are unable to reduce the number of dimensions to a
    size that can be visualized, that is, two or three dimensions, without excessively
    degrading the quality of the data. Consider the MNIST dataset that we used earlier
    in this book, which was a collection of digitized handwritten digits of the numbers
    0 through 9\. Each image is 28 x 28 pixels in size, providing 784 individual dimensions
    or features. If we were to reduce these 784 dimensions down to 2 or 3 for visualization
    purposes, we would lose almost all the available information.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了多种不同的方法来减少数据集的维度，作为清洗数据、减少计算效率所需的大小或提取数据集中最重要信息的手段。虽然我们已经展示了许多减少高维数据集的方法，但在许多情况下，我们无法将维度的数量减少到可以可视化的大小，也就是二维或三维，而不会过度降低数据质量。考虑我们之前在本书中使用的MNIST数据集，这是一个包含数字0到9的手写数字图像的集合。每个图像的大小为28
    x 28像素，提供784个独立的维度或特征。如果我们将这784个维度减少到2或3个以便进行可视化，我们几乎会失去所有可用的信息。
- en: In this chapter, we will discuss SNE and t-SNE as means of visualizing high-dimensional
    datasets. These techniques are extremely helpful in unsupervised learning and
    the design of machine learning systems because being able to visualize data is
    a powerful thing. Being able to visualize data allows relationships to be explored,
    groups to be identified, and results to be validated. t-SNE techniques have been
    used to visualize cancerous cell nuclei that have over 30 characteristics of interest,
    whereas data from documents can have over thousands of dimensions, sometimes even
    after applying techniques such as PCA.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论SNE和t-SNE作为可视化高维数据集的一种手段。这些技术在无监督学习和机器学习系统设计中非常有用，因为能够可视化数据是一件强大的事情。能够可视化数据可以探索关系、识别群体并验证结果。t-SNE技术已被用于可视化癌细胞核，这些细胞核具有超过30个特征，而文档中的数据可能具有上千维，有时即使在应用了像PCA这样的技术后也是如此。
- en: The MNIST Dataset
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据集
- en: 'Now, we will explore SNE and t-SNE using the MNIST dataset provided with the
    accompanying source code as the basis of our practical examples. Before we continue,
    we will quickly review MNIST and the data that is within it. The complete MNIST
    dataset is a collection of 60,000 training and 10,000 test examples of handwritten
    digits of the numbers 0 to 9, represented as black and white (or grayscale) images
    that are 28 x 28 pixels in size (giving 784 dimensions or features) with equal
    numbers of each type of digit (or class) in the dataset. Due to its size and the
    quality of the data, MNIST has become one of the quintessential datasets in machine
    learning, often being used as the reference dataset for many research papers in
    machine learning. One of the advantages of using MNIST to explore SNE and t-SNE
    compared to other datasets is that while the samples contain a high number of
    dimensions, they can be visualized even after dimensionality reduction because
    they can be represented as an image. *Figure 6.1* shows a sample of the MNIST
    dataset:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用附带源代码提供的MNIST数据集作为实际示例，探索SNE和t-SNE。在继续之前，我们将快速回顾一下MNIST及其中的数据。完整的MNIST数据集包含60,000个训练样本和10,000个测试样本，这些样本是手写数字0到9，表示为黑白（或灰度）图像，大小为28
    x 28像素（即784个维度或特征），每个数字类别的样本数量相等。由于数据集的大小和数据质量，MNIST已经成为机器学习中最具代表性的数据集之一，通常被作为许多机器学习研究论文中的参考数据集。与其他数据集相比，使用MNIST探索SNE和t-SNE的一个优势是，虽然样本包含大量维度，但即使在降维后，仍然可以将其可视化，因为它们可以表示为图像。*图6.1*展示了MNIST数据集的一个样本：
- en: '![Figure 6.1: MNIST data sample'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1：MNIST数据样本'
- en: '](img/B15923_06_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_01.jpg)'
- en: 'Figure 6.1: MNIST data sample'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：MNIST数据样本
- en: 'The following figure shows the same sample reduced to 30 components using PCA:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了通过PCA将相同样本降至30个主成分：
- en: '![Figure 6.2: MNIST reduced using PCA to 30 components'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2：通过PCA将MNIST数据集降至30个主成分'
- en: '](img/B15923_06_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_02.jpg)'
- en: 'Figure 6.2: MNIST reduced using PCA to 30 components'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：通过PCA将MNIST数据集降至30个主成分
- en: Stochastic Neighbor Embedding (SNE)
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机邻居嵌入（SNE）
- en: 'SNE is one of a number of different methods that fall within the category of
    **manifold learning**, which aims to describe high-dimensional spaces within low-dimensional
    manifolds or bounded areas. At first thought, this seems like an impossible task;
    how can we reasonably represent data in two dimensions if we have a dataset with
    at least 30 features? As we work through the derivation of SNE, it is hoped that
    you will see how this is possible. Don''t worry – we will not be covering the
    mathematical details of this process in great depth as it is outside of the scope
    of this chapter. Constructing an SNE can be divided into the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SNE是多种**流形学习**方法中的一种，旨在描述低维流形或有界区域中的高维空间。乍一看，这似乎是一个不可能完成的任务；如果我们有一个至少包含30个特征的数据集，如何合理地在二维空间中表示数据呢？随着我们逐步推导SNE的过程，希望你能够看到这是如何可能的。别担心——我们不会在这一章中深入探讨这个过程的数学细节，因为那超出了本章的范围。构建SNE可以分为以下几个步骤：
- en: Convert the distances between datapoints in the high-dimensional space into
    conditional probabilities. Say we had two points, *x*i and *x*j, in a high-dimensional
    space and we wanted to determine the probability (*p*i|j) that *x*j would be picked
    as a neighbor of *x*i. To define this probability, we use a Gaussian curve. By
    doing this, we see that the probability is high for nearby points, while it is
    very low for distant points.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将高维空间中数据点之间的距离转换为条件概率。假设我们有两个点，*x*i和*x*j，位于高维空间中，并且我们想要确定*x*j作为*x*i邻居的概率（*p*i|j）。为了定义这个概率，我们使用高斯曲线。这样，我们可以看到，对于附近的点，概率较高，而对于远离的点，概率非常低。
- en: We need to determine the width of the Gaussian curve as this controls the rate
    of probability selection. A wide curve would suggest that many neighboring points
    are far away, while a narrow curve suggests that they are tightly compacted.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要确定高斯曲线的宽度，因为它控制着概率选择的速率。宽曲线意味着许多邻近点相距较远，而窄曲线则意味着它们紧密地聚集在一起。
- en: Once we project the data into the low-dimensional space, we can also determine
    the corresponding probability (*q*i|j) between the corresponding low-dimensional
    data, *y*i and *y*j.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们将数据投影到低维空间，我们还可以确定相应低维数据之间的概率（*q*i|j），即*y*i和*y*j之间的概率。
- en: 'What SNE aims to do is position the data in the lower dimensions to minimize
    the differences between *p*i|j and *q*i|j over all the datapoints using a cost
    function (C). This is known as the **Kullback-Leibler** (**KL**) divergence:![Figure
    6.3: KL divergence'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SNE的目标是通过使用成本函数(C)最小化所有数据点之间的*p*i|j和*q*i|j之间的差异，将数据放置到低维空间中。这被称为**Kullback-Leibler**
    (**KL**)散度：![图 6.3：KL 散度
- en: '](img/B15923_06_03.jpg)'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_03.jpg)'
- en: 'Figure 6.3: KL divergence'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：KL 散度
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: For Python code to construct a Gaussian distribution, please refer to the `GaussianDist.ipynb`
    Jupyter notebook at [https://packt.live/2UMVubU](https://packt.live/2UMVubU).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建高斯分布的Python代码，请参考`GaussianDist.ipynb` Jupyter笔记本，链接为[https://packt.live/2UMVubU](https://packt.live/2UMVubU)。
- en: When Gaussian distribution is used in SNE, it reduces the dimensions of data
    by preserving localized patterns. To do this, SNE uses the process of gradient
    descent to minimize C using the standard parameters of the learning rate and epochs,
    as we covered in the preceding chapter when we looked at neural networks and autoencoders.
    SNE implements an additional term in the training process—**perplexity**. Perplexity
    is a selection of the effective number of neighbors used in the comparison and
    is relatively stable for the values of perplexity between 5 and 50\. In practice,
    going through a process of trial and error using perplexity values within this
    range is recommended.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当在SNE中使用高斯分布时，它通过保持局部模式来减少数据的维度。为了实现这一点，SNE使用梯度下降过程来最小化C，使用学习率和训练周期等标准参数，正如我们在前一章中讨论神经网络和自编码器时所提到的那样。SNE在训练过程中实现了一个额外的项——**困惑度**。困惑度是在比较中选择有效邻居数量的一个参数，对于困惑度值在5到50之间时，它相对稳定。实际上，建议在这一范围内使用困惑度值进行反复试验。
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: Perplexity is covered in detail later in this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章后面将详细讨论困惑度。
- en: SNE provides an effective way of visualizing high-dimensional data in a low-dimensional
    space, though it still suffers from an issue known as **the crowding problem**.
    The crowding problem can occur if we have some points positioned approximately
    equidistantly within a region around a point, *i*. When these points are visualized
    in the lower-dimensional space, they crowd around each other, making visualization
    difficult. This problem is exacerbated if we try to put some more space between
    these crowded points, because any other points that are further away will be placed
    very far away within the low-dimensional space. Essentially, we are trying to
    balance being able to visualize close points while not losing information provided
    by points that are further away.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SNE提供了一种有效的方式，将高维数据可视化到低维空间，尽管它仍然存在一个被称为**拥挤问题**的问题。拥挤问题可能出现在我们有一些点大致等距地分布在一个点周围的区域内，*i*。当这些点在低维空间中被可视化时，它们会相互拥挤，导致可视化困难。如果我们试图在这些拥挤的点之间留出更多空间，问题会加剧，因为任何距离更远的点会在低维空间中被放置得非常远。实质上，我们是在努力平衡既能可视化近距离点，又不失去远离点所提供的信息。
- en: t-Distributed SNE
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-分布SNE
- en: t-SNE aims to address the crowding problem using a modified version of the KL
    divergence cost function and by substituting the Gaussian distribution with the
    Student's t-distribution in the low-dimensional space. The Student's t-distribution
    is a probability distribution much like Gaussian and is used when we have a small
    sample size and unknown population standard deviation. It is often used in the
    Student's t-test.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE旨在通过修改后的KL散度成本函数，使用学生t分布替代低维空间中的高斯分布，从而解决拥挤问题。学生t分布是一种概率分布，类似于高斯分布，通常用于样本量较小且总体标准差未知的情况。它常用于学生t检验中。
- en: The modified KL cost function considers the pairwise distances in the low-dimensional
    space equally, while the Student's distribution employs a heavy tail in the low-dimensional
    space to avoid the crowding problem. In the higher-dimensional probability calculation,
    the Gaussian distribution is still used to ensure that a moderate distance in
    the higher dimensions is still represented as such in the lower dimensions. This
    combination of different distributions in the respective spaces allows for the
    faithful representation of datapoints separated by small and moderate distances.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的 KL 成本函数在低维空间中对每对数据点的距离给予相等的权重，而学生分布在低维空间中采用较重的尾部以避免拥挤问题。在高维概率计算中，仍然使用高斯分布，以确保在高维空间中适度的距离在低维空间中也能得到忠实的表示。不同分布在各自空间中的组合，允许忠实地表示由小距离和适度距离分开的数据点。
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For some example code regarding how to reproduce the Student's t-distribution
    in Python, please refer to the Jupyter notebook at [https://packt.live/2UMVubU](https://packt.live/2UMVubU).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 若需要一些关于如何在 Python 中重现学生 t 分布的示例代码，请参考 [https://packt.live/2UMVubU](https://packt.live/2UMVubU)
    中的 Jupyter notebook。
- en: Thankfully, we don't need to worry about implementing t-SNE manually because
    scikit-learn provides a very effective implementation in its straightforward API.
    What we need to remember is that both SNE and t-SNE determine the probability
    of two points being neighbors in both high- and low-dimensionality spaces and
    aim to minimize the difference in the probability between the two spaces.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要手动实现 t-SNE，因为 scikit-learn 提供了一个非常有效的实现，且其 API 非常简洁。我们需要记住的是，SNE 和
    t-SNE 都是通过计算两个点在高维空间和低维空间中作为邻居的概率，并尽量最小化两个空间之间概率的差异。
- en: 'Exercise 6.01: t-SNE MNIST'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.01：t-SNE MNIST
- en: 'In this exercise, we will use the MNIST dataset (provided in the accompanying
    source code) to explore the scikit-learn implementation of t-SNE. As we described
    earlier, using MNIST allows us to visualize the high-dimensional space in a way
    that is not possible in other datasets, such as the Boston Housing Price or Iris
    dataset. Perform the following steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 MNIST 数据集（随附源代码提供）来探索 scikit-learn 中 t-SNE 的实现。正如我们之前描述的那样，使用 MNIST
    让我们能够以其他数据集（如波士顿房价数据集或鸢尾花数据集）无法实现的方式来可视化高维空间。请执行以下步骤：
- en: 'For this exercise, import `pickle`, `numpy`, `PCA`, and `TSNE` from scikit-learn,
    as well as `matplotlib`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本练习，导入 `pickle`、`numpy`、`PCA` 和 `TSNE`（来自 scikit-learn），以及 `matplotlib`：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load and visualize the MNIST dataset that is provided with the accompanying
    source code:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并可视化提供的 MNIST 数据集及随附源代码：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.4: Output after loading the dataset'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.4：加载数据集后的输出'
- en: '](img/B15923_06_04.jpg)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_04.jpg)'
- en: 'Figure 6.4: Output after loading the dataset'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.4：加载数据集后的输出
- en: This demonstrates that MNIST has been successfully loaded.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明 MNIST 数据集已成功加载。
- en: In this exercise, we will use PCA on the dataset to extract the first 30 components.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本练习中，我们将对数据集应用 PCA，提取前 30 个主成分。
- en: '[PRE2]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Visualize the effect of reducing the dataset to 30 components. To do this,
    we must transform the dataset into the lower-dimensional space and then use the
    `inverse_transform` method to return the data to its original size for plotting.
    We will, of course, need to reshape the data before and after the transform process:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化将数据集降至 30 个主成分后的效果。为此，我们必须将数据集转换到低维空间，然后使用 `inverse_transform` 方法将数据恢复到原始大小，以便进行绘图。当然，在转换前后，我们需要对数据进行重塑：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.5: Visualizing the effect of reducing the dataset'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.5：可视化数据集降维的效果'
- en: '](img/B15923_06_05.jpg)'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_05.jpg)'
- en: 'Figure 6.5: Visualizing the effect of reducing the dataset'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.5：可视化数据集降维的效果
- en: Note that while we have lost some clarity in the images, for the most part,
    the numbers are still clearly visible due to the dimension reduction process.
    It is interesting to note, however, that the number four (4) seems to have been
    the most visually affected by this process. Perhaps much of the discarded information
    from the PCA process contained information specific to the samples of four (4).
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，尽管图像清晰度有所下降，但由于降维过程，大部分数字仍然清晰可见。值得注意的是，数字 4 似乎受此过程的影响最大。也许 PCA 过程中丢弃的大部分信息都包含了与数字
    4 特有样本相关的信息。
- en: 'Now, we will apply t-SNE to the PCA-transformed data to visualize the 30 components
    in a two-dimensional space. We can construct a t-SNE model in scikit-learn using
    the standard model API interface. We will start off by using the default values
    that specify that we are embedding the 30 dimensions into two for visualization
    using a perplexity of 30, a learning rate of 200, and 1,000 iterations. We will
    specify a `random_state` value of 0 and set `verbose` to 1:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将应用t-SNE算法对PCA变换后的数据进行处理，以在二维空间中可视化30个主成分。我们可以通过scikit-learn中的标准模型API接口来构建一个t-SNE模型。我们将从使用默认值开始，这些值指定了我们将在二维空间中嵌入30个维度进行可视化，使用的困惑度为30，学习率为200，迭代次数为1,000。我们将设置`random_state`为0，并将`verbose`设置为1：
- en: '[PRE4]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.6: Applying t-SNE to PCA-transformed data'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.6：应用t-SNE到PCA变换后的数据'
- en: '](img/B15923_06_06.jpg)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_06.jpg)'
- en: 'Figure 6.6: Applying t-SNE to PCA-transformed data'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.6：应用t-SNE到PCA变换后的数据
- en: In the preceding screenshot, we can see a number of configuration options that
    are available for the t-distributed SNE model, with some more important than the
    others. We will focus on the values of `learning_rate`, `n_components`, `n_iter`,
    `perplexity`, `random_state`, and `verbose`. For `learning_rate`, as we discussed
    previously, t-SNE uses stochastic gradient descent to project the high-dimensional
    data into a low-dimensional space. The learning rate controls the speed at which
    the process is executed. If the learning rate is too high, the model may fail
    to converge on a solution, and if it's too slow, it may take a very long time
    to reach it (if at all). A good rule of thumb is to start with the default; if
    you find the model producing NaNs (not-a-number values), you may need to reduce
    the learning rate. Once you are happy with the model, it is also wise to reduce
    the learning rate and let it run for longer (increase `n_iter`) as you may get
    a slightly better result. `n_components` is the number of dimensions in the embedding
    (or visualization space). More often than not, you would like a two-dimensional
    plot of the data, so you just need the default value of `2`. Now, `n_iter` is
    the maximum number of iterations of gradient descent. `perplexity` is the number
    of neighbors to use when visualizing the data.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上述截图中，我们可以看到t-SNE模型提供了多个配置选项，其中一些比其他选项更为重要。我们将重点关注`learning_rate`、`n_components`、`n_iter`、`perplexity`、`random_state`和`verbose`的值。对于`learning_rate`，正如我们之前所讨论的，t-SNE使用随机梯度下降将高维数据投影到低维空间。学习率控制该过程执行的速度。如果学习率太高，模型可能无法收敛到一个解；如果太低，可能需要很长时间才能得到结果（如果能得到的话）。一个好的经验法则是从默认值开始；如果你发现模型产生了NaN（非数值）结果，可能需要降低学习率。一旦对模型的结果满意，最好降低学习率并让其运行更长时间（增加`n_iter`），这样可能会得到稍微更好的结果。`n_components`是嵌入空间（或可视化空间）的维度数。通常情况下，你会希望数据的可视化是二维图，所以只需要使用默认值`2`。`n_iter`是梯度下降的最大迭代次数。`perplexity`是可视化数据时使用的邻居数量。
- en: Typically, a value between 5 and 50 will be appropriate, knowing that larger
    datasets typically require more perplexity than smaller ones. `random_state` is
    an important variable for any model or algorithm that initializes its values randomly
    at the start of training. The random number generators provided within computer
    hardware and software tools are not, in fact, truly random; they are actually
    pseudo-random number generators. They give a good approximation of randomness
    but are not truly random. Random numbers within computers start with a value known
    as a seed and are then produced in a complicated manner after that. By providing
    the same seed at the start of the process, the same "random numbers" are produced
    each time the process is run. While this sounds counter-intuitive, it is great
    for reproducing machine learning experiments as you won't see any difference in
    performance solely due to the initialization of the parameters at the start of
    training. This can provide more confidence that a change in performance is due
    to the considered change to the model or training; for example, the architecture
    of the neural network.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，5到50之间的值是合适的，考虑到较大的数据集通常需要比较小的数据集更多的困惑度。`random_state`是任何模型或算法中的一个重要变量，它会在训练开始时初始化其值。计算机硬件和软件工具中提供的随机数生成器实际上并不是真正的随机数生成器；它们实际上是伪随机数生成器。它们提供了接近随机性的良好近似，但并不是真正的随机。计算机中的随机数从一个称为种子的值开始，然后以复杂的方式生成。通过在过程开始时提供相同的种子，每次运行该过程时都会生成相同的“随机数”。虽然这听起来违反直觉，但它对于再现机器学习实验非常有用，因为你不会看到仅由于训练开始时参数初始化的不同而导致的性能差异。这可以提供更多信心，表明性能的变化是由于对模型或训练的有意改变；例如，神经网络的架构。
- en: Note
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Producing true random sequences is actually one of the hardest tasks to achieve
    with a computer. Computer software and hardware is designed so that the instructions
    that are provided are executed in exactly the same way each time they are run
    so that you get the same result. Random differences in execution, while being
    ideal for producing sequences of random numbers, would be a nightmare in terms
    of automating tasks and debugging problems.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成真正的随机序列实际上是用计算机完成的最困难的任务之一。计算机软件和硬件的设计方式是，提供的指令每次执行时都完全相同，以便你获得相同的结果。执行中的随机差异，虽然在生成随机数字序列时理想，但在自动化任务和调试问题时将是噩梦。
- en: '`verbose` is the verbosity level of the model and describes the amount of information
    that''s printed to the screen during the model fitting process. A value of 0 indicates
    no output, while 1 or greater indicates increasing levels of detail in the output.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`verbose`是模型的详细程度，描述了在模型拟合过程中打印到屏幕上的信息量。值为0表示没有输出，而值为1或更大表示输出中详细程度的增加。'
- en: 'Use t-SNE to transform the decomposed dataset of MNIST:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用t-SNE转换MNIST的分解数据集：
- en: '[PRE5]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.7: Transforming the decomposed dataset'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.7：转换分解后的数据集'
- en: '](img/B15923_06_07.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_07.jpg)'
- en: 'Figure 6.7: Transforming the decomposed dataset'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.7：转换分解后的数据集
- en: The output provided during the fitting process provides an insight into the
    calculations being completed by scikit-learn. We can see that it is indexing and
    computing neighbors for all the samples and is then determining the conditional
    probabilities of being neighbors for the data in batches of 10\. At the end of
    the process, it provides a mean standard deviation value of `304.9988` with KL
    divergence after 250 and 1,000 iterations of gradient descent.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在拟合过程中提供的输出提供了对scikit-learn所完成计算的洞察。我们可以看到它正在为所有样本建立索引并计算邻居，然后以批次为10的数据来确定邻居的条件概率。在过程结束时，它提供了一个均值标准差值`304.9988`，并且在250次和1,000次梯度下降迭代后，给出了KL散度。
- en: 'Now, visualize the number of dimensions in the returned dataset:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，视觉化返回数据集中的维度数量：
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have successfully reduced the 784 dimensions down to 2 for visualization,
    so what does it look like?
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经成功地将784个维度降到2维以便可视化，那么它看起来怎么样呢？
- en: 'Create a scatter plot of the two-dimensional data produced by the model:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建由模型生成的二维数据的散点图：
- en: '[PRE8]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.8: 2D representation of MNIST (no labels)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.8：MNIST的二维表示（无标签）'
- en: '](img/B15923_06_08.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_08.jpg)'
- en: 'Figure 6.8: 2D representation of MNIST (no labels)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.8：MNIST的二维表示（无标签）
- en: In the preceding plot, we can see that we have represented the MNIST data in
    two dimensions, but we can also see that it seems to be grouped together. There
    are a number of different clusters or clumps of data congregated together and
    separated from other clusters by some white space. There also seem to be about
    nine different groups of data. All these observations suggest that there is a
    relationship within and between the individual clusters.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上面的图中，我们可以看到已经将 MNIST 数据表示为二维形式，但也可以看到它似乎被聚集在一起。数据集中有许多不同的簇或数据块聚集在一起，并且通过一些空白区域与其他簇分开。似乎有大约九个不同的数据组。所有这些观察结果表明，在各个簇内部和它们之间存在某种关系。
- en: Plot the two-dimensional data that's been grouped by the corresponding image
    labels and use markers to separate the individual labels.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制按相应图像标签分组的二维数据，并使用标记将各个标签分开。
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.9: 2D representation of MNIST with labels'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.9：带标签的 MNIST 二维表示'
- en: '](img/B15923_06_09.jpg)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_09.jpg)'
- en: 'Figure 6.9: 2D representation of MNIST with labels'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.9：带标签的 MNIST 二维表示
- en: 'The preceding plot is very interesting. Here, we can see that the clusters
    correspond to each of the different image classes (zero through nine) within the
    dataset. In an unsupervised fashion, that is, without providing the labels in
    advance, a combination of PCA and t-SNE has been able to separate and group the
    individual classes within the MNIST dataset. What is particularly interesting
    is that there seems to be some confusion within the data regarding the number
    four images and the number nine images, as well as for the five and three images;
    the two clusters somewhat overlap. This makes sense if we look at the number nine
    and number four PCA images we extracted from *Step 4* of *Exercise 6.01*, *t-SNE
    MNIST*:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的图非常有趣。在这里，我们可以看到各个簇与数据集中的不同图像类别（从零到九）对应。通过无监督的方式，也就是不提前提供标签，结合 PCA 和 t-SNE，已经能够将
    MNIST 数据集中的各个类别分离并归类。特别有趣的是，数据中似乎存在一些混淆，尤其是数字四与数字九的图像，五和三的图像之间也有一定重叠；这两个簇部分重合。如果我们查看在*步骤
    4*中的数字九和数字四的 PCA 图像，*t-SNE MNIST*，这一点就更有意义了：
- en: '![Figure 6.10: PCA images of nine'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.10：九的 PCA 图像'
- en: '](img/B15923_06_10.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_10.jpg)'
- en: 'Figure 6.10: PCA images of nine'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.10：九的 PCA 图像
- en: 'They do, in fact, look quite similar; perhaps this is due to the uncertainty
    in the shape of the number four. Looking at the image that follows, we can see
    from the four on the left-hand side that the two vertical lines almost join, while
    the four on the right-hand side has the two lines parallel:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它们确实看起来非常相似；也许这与数字四的形状的不确定性有关。观察接下来的图像，我们可以从左侧的四中看到，两条垂直线几乎相交，而右侧的四则是两条线平行：
- en: '![Figure 6.11: Shape of number four'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.11：数字四的形状'
- en: '](img/B15923_06_11.jpg)'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_11.jpg)'
- en: 'Figure 6.11: Shape of number four'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.11：数字四的形状
- en: The other interesting feature to note in *Figure 6.9* is the edge cases, which
    are shown in color in the Jupyter notebooks. Around the edges of each cluster,
    we can see that some samples would be misclassified in the traditional supervised
    learning sense but represent samples that may have more in common with other clusters
    than their own. Let's take a look at an example; there are a number of samples
    of the number three that are quite far from the correct cluster.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图 6.9*中另一个有趣的特征是边缘情况，这些在 Jupyter 笔记本中以不同颜色显示。在每个簇的边缘，我们可以看到一些样本在传统的监督学习中会被错误分类，但它们代表的是与其他簇更相似的样本，而不是它们自己的簇。我们来看一个例子；有一些数字三的样本距离正确的簇非常远。
- en: 'Get the index of all the number threes in the dataset:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据集中所有数字三的索引：
- en: '[PRE10]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Find the threes that were plotted with an `x` value of less than 0:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找 x 值小于 0 的数字三：
- en: '[PRE12]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.12: The threes with an x value less than zero'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.12：x 值小于零的三'
- en: '](img/B15923_06_12.jpg)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_12.jpg)'
- en: 'Figure 6.12: The threes with an x value less than zero'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.12：x 值小于零的三
- en: 'Display the coordinates to find one that is reasonably far from the three cluster:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示坐标，找到一个与三类簇相距较远的点：
- en: '[PRE13]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.13: Coordinates away from the three cluster'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.13：远离三类簇的坐标'
- en: '](img/B15923_06_13.jpg)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_13.jpg)'
- en: 'Figure 6.13: Coordinates away from the three cluster'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.13：远离三类簇的坐标
- en: Choose a sample with a reasonably high negative value as an `x` coordinate.
    In this example, we will select the second sample, which is sample `11`.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个具有合理高负值作为 `x` 坐标的样本。在本例中，我们将选择第二个样本，即样本 `11`。
- en: '[PRE14]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.14: Image of sample 11'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.14：样本 11 的图像'
- en: '](img/B15923_06_14.jpg)'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_14.jpg)'
- en: 'Figure 6.14: Image of sample 11'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：样本 11 的图像
- en: Looking at this sample image and the corresponding t-SNE coordinates, that is,
    approximately (-33, 26), it is not surprising that this sample lies near the cluster
    of eights and fives as there are quite a few features that are common to both
    of those numbers in this image. In this example, we applied a simplified SNE,
    demonstrating some of its efficiencies as well as possible sources of confusion
    and the output of unsupervised learning.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个示例图像及其对应的 t-SNE 坐标，即大约 (-33, 26)，并不奇怪，因为这个样本位于数字 8 和 5 的群集附近，这些数字在这幅图像中有许多共同特征。在这个例子中，我们应用了简化的
    SNE，展示了它的一些效率以及可能的混淆来源和无监督学习的输出。
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3iDsCNf](https://packt.live/3iDsCNf)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/3iDsCNf](https://packt.live/3iDsCNf)
- en: You can also run this example online at [https://packt.live/3gBdrSK](https://packt.live/3gBdrSK)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 [https://packt.live/3gBdrSK](https://packt.live/3gBdrSK) 上在线运行此示例。
- en: 'Activity 6.01: Wine t-SNE'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 6.01：葡萄酒 t-SNE
- en: In this activity, we will reinforce our knowledge of t-SNE using the Wine dataset.
    By completing this activity, you will be able to build-SNE models for your own
    custom applications. The Wine dataset ([https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine))
    is a collection of attributes regarding the chemical analysis of wine from Italy
    from three different producers, but the same type of wine for each producer. This
    information could be used as an example to verify the validity of a bottle of
    wine made from the grapes from a specific region in Italy. The 13 attributes are
    Alcohol, Malic acid, Ash, Alkalinity of ash, Magnesium, Total phenols, Flavanoids,
    Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted
    wines, and Proline.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将通过使用葡萄酒数据集加强我们对 t-SNE 的了解。通过完成此活动，您将能够为自己的定制应用程序构建 t-SNE 模型。葡萄酒数据集（[https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)）收集了关于意大利葡萄酒化学分析的属性，来自三个不同生产商，但每个生产商都是同一种类型的葡萄酒。这些信息可以用作验证特定意大利地区葡萄酒制成的瓶子的有效性的示例。13
    个属性包括酒精、苹果酸、灰分、灰的碱性、镁、总酚、类黄酮、非黄烷类酚、前花青素、颜色强度、色调、稀释酒的 OD280/OD315 和 脯氨酸。
- en: Each sample contains a class identifier (1 – 3).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本包含一个类别标识符（1 - 3）。
- en: Note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).
    It can also be downloaded from [https://packt.live/3e1JOcY](https://packt.live/3e1JOcY).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集来源于 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)（UCI
    机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。尔湾，加利福尼亚：加利福尼亚大学信息与计算机科学学院）。也可以从
    [https://packt.live/3e1JOcY](https://packt.live/3e1JOcY) 下载。
- en: 'These steps will help you complete this activity:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助您完成此活动：
- en: Import `pandas`, `numpy`, and `matplotlib`, as well as the `t-SNE` and `PCA`
    models from scikit-learn.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`numpy` 和 `matplotlib`，以及从 scikit-learn 导入的 `t-SNE` 和 `PCA` 模型。
- en: Load the Wine dataset using the `wine.data` file included in the accompanying
    source code and display the first five rows of data.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用附带源代码中包含的 `wine.data` 文件加载 Wine 数据集，并显示前五行数据。
- en: Note
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can delete columns within pandas DataFrames by using the `del` keyword.
    Simply pass `del` the DataFrame and the selected column within the square root.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以使用 `del` 关键字在 pandas DataFrames 中删除列。只需将 `del` 关键字传递给数据帧和在平方根内选择的列即可。
- en: The first column contains the labels; extract this column and remove it from
    the dataset.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一列包含标签；提取此列并从数据集中删除。
- en: Execute PCA to reduce the dataset to the first six components.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 PCA 将数据集减少到前六个组件。
- en: Determine the amount of variance within the data described by these six components.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定描述这六个组件的数据中的方差量。
- en: Create a t-SNE model using a specified random state and a `verbose` value of
    1.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的随机状态创建 t-SNE 模型，并将 `verbose` 值设置为 1。
- en: Fit the PCA data to the t-SNE model.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 PCA 数据拟合到 t-SNE 模型。
- en: Confirm that the shape of the t-SNE fitted data is two-dimensional.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认 t-SNE 拟合数据的形状是二维的。
- en: Create a scatter plot of the two-dimensional data.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建二维数据的散点图。
- en: Create a secondary scatter plot of the two-dimensional data with the class labels
    applied to visualize any clustering that may be present.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个带有类标签的二维数据散点图，以可视化可能存在的任何聚类。
- en: 'By the end of this activity, you will have constructed a t-SNE visualization
    of the Wine dataset using its six components and identified some relationships
    in the location of the data within the plot. The final plot will look similar
    to the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到本活动结束时，你将使用 Wine 数据集的六个成分构建一个 t-SNE 可视化图，并在图中的数据位置识别一些关系。最终的图形将类似于以下内容：
- en: '![Figure 6.15: The expected plot'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15：预期的绘图'
- en: '](img/B15923_06_15.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_15.jpg)'
- en: 'Figure 6.15: The expected plot'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：预期的绘图
- en: Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 460.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 460 页找到。
- en: Interpreting t-SNE Plots
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释 t-SNE 图
- en: Now that we are able to use t-distributed SNE to visualize high-dimensional
    data, it is important to understand the limitations of such plots and what aspects
    are important in interpreting and generating them. In this section, we will highlight
    some of the important features of t-SNE and demonstrate how care should be taken
    when using this visualization technique.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 t 分布的 SNE 可视化高维数据，重要的是要理解此类图表的局限性以及在解读和生成这些图表时需要关注哪些方面。在本节中，我们将重点介绍
    t-SNE 的一些重要特性，并演示在使用这种可视化技术时需要小心的地方。
- en: Perplexity
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 困惑度
- en: As we described in the introduction to t-SNE, the perplexity values specify
    the number of nearest neighbors to be used when computing the conditional probability.
    The selection of this value can make a significant difference to the end result;
    with a low value of perplexity, local variations in the data dominate because
    a small number of samples are used in the calculation. Conversely, a large value
    of perplexity considers more global variations as many more samples are used in
    the calculation. Typically, it is worth trying a range of different values to
    investigate the effect of perplexity. Again, values between 5 and 50 tend to work
    quite well.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 t-SNE 的介绍中所描述的，困惑度值指定了计算条件概率时要使用的最近邻居数量。选择该值会对最终结果产生重大影响；当困惑度值较低时，数据中的局部变化主导，因为计算中只使用了少量样本。相反，困惑度值较大时，会考虑更多的全局变化，因为计算中使用了更多的样本。通常，尝试一系列不同的值来研究困惑度的效果是值得的。再次强调，困惑度值在
    5 到 50 之间通常效果不错。
- en: 'Exercise 6.02: t-SNE MNIST and Perplexity'
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.02：t-SNE MNIST 和困惑度
- en: 'In this exercise, we will try a range of different values for perplexity and
    look at the effect in the visualization plot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将尝试不同的困惑度值，并观察其在可视化图中的效果：
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as `PCA` and `t-SNE` from
    scikit-learn:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pickle`、`numpy` 和 `matplotlib`，以及从 scikit-learn 中导入 `PCA` 和 `t-SNE`：
- en: '[PRE15]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Load the MNIST dataset.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集。
- en: '[PRE16]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using PCA, select only the first 30 components of variance from the image data:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 PCA，只选择图像数据的前 30 个方差成分：
- en: '[PRE17]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In this exercise, we are investigating the effect of perplexity on the t-SNE
    manifold. Iterate through a model/plot loop with a perplexity of 3, 30, and 300:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本练习中，我们正在研究困惑度对 t-SNE 流形的影响。以困惑度 3、30 和 300 进行模型/图形循环：
- en: '[PRE18]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.16: Iterating through a model'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.16：迭代模型'
- en: '](img/B15923_06_16.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_16.jpg)'
- en: 'Figure 6.16: Iterating through a model'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16：迭代模型
- en: Note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding output has been truncated for presentation purposes. Standard
    outputs like this would typically be much longer. However, it has been included
    as it is important to keep an eye on such outputs while the model is training.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出已被截断以便展示。像这样的标准输出通常会更长。不过，还是将其包含在内，因为在模型训练时，保持关注此类输出非常重要。
- en: Note the KL divergence in each of the three different perplexity values, along
    with the increase in the average standard deviation (variance). By looking at
    the following t-SNE plots with class labels, we can see that with a low perplexity
    value, the clusters are nicely contained with relatively few overlaps. However,
    there is almost no space between the clusters. As we increase the perplexity,
    the space between the clusters improves with reasonably clear distinctions at
    a perplexity of 30\. As the perplexity increases to 300, we can see that the clusters
    of eight and five, along with nine, four, and seven, are starting to converge.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在三种不同困惑度值下的 KL 散度，以及平均标准差（方差）的增加。通过查看以下带有类别标签的 t-SNE 图，我们可以看到，在较低困惑度值下，聚类被很好地包含，重叠较少。然而，聚类之间几乎没有空间。随着困惑度的增加，聚类之间的空间得到改善，并且在困惑度为
    30 时，区分相对清晰。随着困惑度增加到 300，我们可以看到，8 和 5 号聚类以及 9、4 和 7 号聚类开始趋于合并。
- en: 'Let''s start with a low perplexity value:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个较低的困惑度值开始：
- en: '![Figure 6.17: Plot of the low perplexity value'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17：低困惑度值的图表'
- en: '](img/B15923_06_17.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_17.jpg)'
- en: 'Figure 6.17: Plot of the low perplexity value'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17：低困惑度值的图表
- en: Note
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The plotting function in *Step 4* would result in this plot. The subsequent
    outputs are the plots for varying values of perplexity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 4 步*中的绘图函数将生成此图。接下来的输出是不同困惑度值下的图表。'
- en: 'Increasing the perplexity by a factor of 10 shows much clearer clusters:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 将困惑度增加 10 倍后，聚类变得更加清晰：
- en: '![Figure 6.18: Plot after increasing perplexity by a factor of 10'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.18：将困惑度增加 10 倍后的图表'
- en: '](img/B15923_06_18.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_18.jpg)'
- en: 'Figure 6.18: Plot after increasing perplexity by a factor of 10'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18：将困惑度增加 10 倍后的图表
- en: 'By increasing the perplexity to 300, we start to merge more of the labels together:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将困惑度增加到 300，我们开始将更多标签合并在一起：
- en: '![Figure 6.19: Increasing the perplexity value to 300'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.19：将困惑度值增加到 300'
- en: '](img/B15923_06_19.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_19.jpg)'
- en: 'Figure 6.19: Increasing the perplexity value to 300'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19：将困惑度值增加到 300
- en: In this exercise, we developed our understanding of the effect of perplexity
    and the sensitivity of this value to the overall result. A small perplexity value
    can lead to a more homogenous mix of locations with very little space between
    them. Increasing the perplexity separates the clusters more effectively, but an
    excessive value leads to overlapping clusters.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们加深了对困惑度影响的理解，并了解了该值对整体结果的敏感性。较小的困惑度值可能导致位置之间空间非常小的更均匀混合。增加困惑度可以更有效地分离聚类，但过高的值会导致聚类重叠。
- en: Note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gI0zdp](https://packt.live/3gI0zdp)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/3gI0zdp](https://packt.live/3gI0zdp)
- en: You can also run this example online at [https://packt.live/3gDcjxR](https://packt.live/3gDcjxR)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在网上运行这个示例，访问 [https://packt.live/3gDcjxR](https://packt.live/3gDcjxR)
- en: 'Activity 6.02: t-SNE Wine and Perplexity'
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 6.02：t-SNE 葡萄酒与困惑度
- en: In this activity, we will use the Wine dataset to further reinforce the influence
    of perplexity on the t-SNE visualization process. In this activity, we will try
    to determine whether we can identify the source of the wine based on its chemical
    composition. The t-SNE process provides an effective means of representing and
    possibly identifying the sources.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用 Wine 数据集进一步强化困惑度对 t-SNE 可视化过程的影响。在本活动中，我们将尝试根据葡萄酒的化学成分来判断其来源。t-SNE
    过程提供了一种有效的手段来表示并可能识别来源。
- en: Note
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).
    It can be downloaded from [https://packt.live/3aPOmRJ](https://packt.live/3aPOmRJ).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集来源于 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)
    （UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]）。它可以从
    [https://packt.live/3aPOmRJ](https://packt.live/3aPOmRJ) 下载。
- en: Import `pandas`, `numpy`, and `matplotlib`, as well as the `t-SNE` and `PCA`
    models from scikit-learn.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`numpy` 和 `matplotlib`，以及来自 scikit-learn 的 `t-SNE` 和 `PCA` 模型。
- en: Load the Wine dataset and inspect the first five rows.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 Wine 数据集并检查前五行数据。
- en: The first column provides the labels; extract these from the DataFrame and store
    them in a separate variable. Ensure that the column is removed from the DataFrame.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一列提供标签；从DataFrame中提取这些标签，并将它们存储在一个单独的变量中。确保从DataFrame中删除该列。
- en: Execute PCA on the dataset and extract the first six components.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集执行PCA，并提取前六个成分。
- en: Construct a loop that iterates through the perplexity values (1, 5, 20, 30,
    80, 160, 320). For each loop, generate a t-SNE model with the corresponding perplexity
    and print a scatter plot of the labeled wine classes. Note the effect of different
    perplexity values.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个循环，遍历困惑度值（1、5、20、30、80、160、320）。在每次循环中，生成一个具有相应困惑度的t-SNE模型，并绘制标记的葡萄酒类别的散点图。注意不同困惑度值的影响。
- en: 'By the end of this activity, you will have generated a two-dimensional representation
    of the Wine dataset and inspected the resulting plot for clusters or groupings
    of data. The plot for perplexity value 320 looks as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动结束时，你将生成Wine数据集的二维表示，并检查生成的图形，以查找数据的簇或分组。困惑度值320的图形如下所示：
- en: '![Figure 6.20: Expected output'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.20：预期输出'
- en: '](img/B15923_06_20.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_20.jpg)'
- en: 'Figure 6.20: Expected output'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20：预期输出
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 464.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第464页找到。
- en: Iterations
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迭代次数
- en: The final parameter we will experiment with is the number of iterations, which,
    as per our investigation of autoencoders, is simply the number of training epochs
    to apply to gradient descent. Thankfully, the number of iterations is a reasonably
    simple parameter to adjust and often requires only a certain amount of patience
    as the position of the points in the low-dimensional space stabilize in their
    final locations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实验的最后一个参数是迭代次数，正如我们在自编码器中的研究所示，它实际上就是应用于梯度下降的训练周期数。幸运的是，迭代次数是一个相对简单的参数，通常只需要一定的耐心，因为低维空间中点的位置会稳定在最终位置。
- en: 'Exercise 6.03: t-SNE MNIST and Iterations'
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.03：t-SNE MNIST与迭代次数
- en: 'In this exercise, we will look at the influence of a range of different iteration
    parameters that have been applied to the t-SNE model and highlight some indicators
    that perhaps more training is required. Again, the value of these parameters is
    highly dependent on the dataset and the volume of data that''s available for training.
    We will use MNIST in this example:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将观察不同迭代参数对t-SNE模型的影响，并突出显示一些指标，表明可能需要更多的训练。再次强调，这些参数的值高度依赖于数据集和可用于训练的数据量。在这个例子中，我们将使用MNIST：
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as `PCA` and `t-SNE` from
    scikit-learn:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及从scikit-learn导入`PCA`和`t-SNE`：
- en: '[PRE19]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Load the MNIST dataset:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集：
- en: '[PRE20]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Using PCA, select only the first 30 components of variance from the image data:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PCA，从图像数据中选择前30个方差成分：
- en: '[PRE21]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this exercise, we are investigating the effect of iterations on the t-SNE
    manifold. Iterate through a model/plot loop with iteration and iterate with the
    progress values `250`, `500`, and `1000`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本练习中，我们正在研究迭代对t-SNE流形的影响。通过迭代模型/绘图循环，使用迭代进度值`250`、`500`和`1000`：
- en: '[PRE22]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.21: Iterating through a model'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.21：遍历模型'
- en: '](img/B15923_06_21.jpg)'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_21.jpg)'
- en: 'Figure 6.21: Iterating through a model'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.21：遍历模型
- en: 'Plot the results:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE23]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'A reduced number of iterations limits the extent to which the algorithm can
    find relevant neighbors, leading to ill-defined clusters:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代次数减少会限制算法找到相关邻居的能力，导致簇的定义不清：
- en: '![Figure 6.22: Plot after 250 iterations'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.22：250次迭代后的图形'
- en: '](img/B15923_06_22.jpg)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_06_22.jpg)'
- en: 'Figure 6.22: Plot after 250 iterations'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22：250次迭代后的图形
- en: 'Increasing the number of iterations provides the algorithm with enough time
    to adequately project the data:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 增加迭代次数为算法提供了足够的时间来充分投影数据：
- en: '![Figure 6.23: Plot after increasing the iterations to 500'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.23：将迭代次数增加到500后的图形'
- en: '](img/B15923_06_23.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_23.jpg)'
- en: 'Figure 6.23: Plot after increasing the iterations to 500'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23：将迭代次数增加到500后的图形
- en: 'Once the clusters have settled, increased iterations have an extremely small
    effect and essentially lead to increased training time:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦簇稳定下来，增加迭代次数的影响极小，实际上只是增加了训练时间：
- en: '![Figure 6.24: Plot after 1,000 iterations'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.24：1,000次迭代后的图形'
- en: '](img/B15923_06_24.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_06_24.jpg)'
- en: 'Figure 6.24: Plot after 1,000 iterations'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24：1,000次迭代后的图形
- en: Looking at the previous plots, we can see that the cluster positions with iteration
    values of 500 and 1,000 are stable and relatively unchanged between the plots.
    The most interesting plot is that of an iteration value of 250, where it seems
    as though the clusters are still in a process of motion, making their way to the
    final positions. As such, there is sufficient evidence to suggest that an iteration
    value of 500 is sufficient.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图表来看，我们可以看到迭代值为500和1,000的聚类位置是稳定的，并且在不同的图表之间几乎没有变化。最有趣的图表是迭代值为250的那一张，似乎聚类仍处于运动过程中，正朝着最终位置移动。因此，有足够的证据表明迭代值500已经足够。
- en: Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Zaw1uZ](https://packt.live/2Zaw1uZ)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参见[https://packt.live/2Zaw1uZ](https://packt.live/2Zaw1uZ)
- en: You can also run this example online at [https://packt.live/3gCOiHf](https://packt.live/3gCOiHf)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在线运行这个示例，网址为[https://packt.live/3gCOiHf](https://packt.live/3gCOiHf)
- en: 'Activity 6.03: t-SNE Wine and Iterations'
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.03：t-SNE酒类数据与迭代
- en: In this activity, we will investigate the effect of the number of iterations
    on the visualization of the Wine dataset. This is a process that's commonly used
    during the exploration phase of data processing, cleaning, and understanding the
    relationships in the data. Depending on the dataset and the type of analysis,
    we may need to try a number of different iterations, such as the ones we will
    look at in this activity.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将研究迭代次数对酒类数据集可视化效果的影响。这个过程在数据处理、清理和理解数据关系的探索阶段中非常常见。根据数据集和分析类型，我们可能需要尝试多种不同的迭代次数，就像我们在本活动中将要看的那样。
- en: As we mentioned previously, this process is extremely helpful for projecting
    high-dimensional data down to a lower, more understandable number of dimensions.
    In this case, our dataset has 13 features; however, in the real world, you can
    have datasets with hundreds or even thousands of features. A common instance of
    this would be person-level data, which can have any number of demographic- or
    action-related features, which can make regular off-the-shelf analysis impossible.
    t-SNE is a helpful tool for working high-dimensional data into a more intuitive
    state.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，这个过程对于将高维数据降维到更低且更易理解的维度非常有帮助。在这个例子中，我们的数据集有13个特征；然而，在现实世界中，你可能会遇到具有数百甚至数千个特征的数据集。一个常见的例子是个人级别的数据，它可能包含任何数量的与人口统计或行为相关的特征，这使得常规的现成分析变得不可能。t-SNE是一个有助于将高维数据转化为更直观状态的工具。
- en: Note
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).
    It can be downloaded from [https://packt.live/2xXgHXo](https://packt.live/2xXgHXo).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)（UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院）。它可以从[https://packt.live/2xXgHXo](https://packt.live/2xXgHXo)下载。
- en: 'These steps will help you complete this activity:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助你完成此活动：
- en: Import `pandas`, `numpy`, and `matplotlib`, as well as the `t-SNE` and `PCA`
    models from scikit-learn.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`matplotlib`，以及来自scikit-learn的`t-SNE`和`PCA`模型。
- en: Load the Wine dataset and inspect the first five rows.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载酒类数据集并检查前五行。
- en: The first column provides the labels; extract these from the DataFrame and store
    them in a separate variable. Ensure that the column is removed from the DataFrame.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一列提供了标签；从DataFrame中提取这些标签，并将它们存储在一个单独的变量中。确保该列已从DataFrame中移除。
- en: Execute PCA on the dataset and extract the first six components.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集上执行PCA并提取前六个主成分。
- en: Construct a loop that iterates through the iteration values (`250`, `500`, `1000`).
    For each loop, generate a t-SNE model with the corresponding number of iterations
    and an identical number of iterations without progress values.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个循环，迭代不同的迭代值（`250`，`500`，`1000`）。对于每次循环，生成一个具有相应迭代次数的t-SNE模型，以及一个没有进度值的相同迭代次数。
- en: Construct a scatter plot of the labeled wine classes. Note the effect of different
    iteration values.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个带有标签的酒类散点图。注意不同迭代值的效果。
- en: By completing this activity, we will have seen the effect of modifying the iteration
    parameter of the model. This is an important parameter in ensuring that the data
    has settled into a somewhat final position in the low-dimensional space.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本活动后，我们将看到修改模型迭代参数的效果。这是一个重要的参数，确保数据在低维空间中已经稳定到一个相对最终的位置。
- en: Note
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 473.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解答可以在第473页找到。
- en: Final Thoughts on Visualizations
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于可视化的最终思考
- en: As we conclude this chapter, there are a couple of important aspects to note
    regarding visualizations. The first is that the size of the clusters or the relative
    space between clusters may not actually provide any real indication of proximity.
    As we discussed earlier in this chapter, a combination of Gaussian and Student's
    t-distributions is used with SNE to represent high-dimensional data in a low-dimensional
    space. As such, there is no guarantee of a linear relationship in distance since
    t-SNE balances the positions of localized and global data structures. The actual
    distance between the points in local structures may be visually very close within
    the representation, but still might be some distance away in the high-dimensional
    space.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，有几个关于可视化的重要方面需要注意。第一个是聚类的大小或聚类之间的相对空间，可能并不能真正反映它们的接近程度。正如本章前面所讨论的，SNE通过组合高斯分布和Student's
    t分布来将高维数据表示在低维空间中。因此，由于t-SNE平衡了局部数据结构和全局数据结构的位置，距离之间没有线性关系的保证。在局部结构中的点之间的实际距离，可能在可视化中看起来非常接近，但在高维空间中可能仍然有一定的距离。
- en: This property also has additional consequences in that, sometimes, random data
    can appear as if it has some structure, and that it is often required to generate
    multiple visualizations using different values of perplexity, learning rate, number
    of iterations, and random seed values.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这一特性还有其他后果，即有时随机数据可能看起来像是具有某种结构，通常需要使用不同的困惑度、学习率、迭代次数和随机种子值来生成多个可视化图。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to t-distributed SNEs as a means of visualizing
    high-dimensional information that may have been produced from prior processes,
    such as PCA or autoencoders. We discussed the means by which t-SNEs produce this
    representation and generated a number of them using the MNIST and Wine datasets
    and scikit-learn. In this chapter, we were able to look at some of the power of
    unsupervised learning because PCA and t-SNE were able to cluster the classes of
    each image without knowing the ground truth result. In the next chapter, we will
    build on this practical experience by looking into applications of unsupervised
    learning, including basket analysis and topic modeling.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了t-分布SNE（t-distributed SNE）作为一种可视化高维信息的方法，这些信息可能来自于先前的处理过程，例如PCA或自动编码器。我们讨论了t-SNE如何生成这种表示，并使用MNIST和Wine数据集以及scikit-learn生成了多个可视化结果。在本章中，我们能够看到无监督学习的一些强大之处，因为PCA和t-SNE能够在不知道真实标签的情况下，将每个图像的类别进行聚类。在下一章中，我们将通过研究无监督学习的应用（包括篮子分析和主题建模）来基于这一实践经验进行进一步探讨。
