- en: Chapter 5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章
- en: Comparing Models
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 比较模型
- en: A map is not the territory it represents, but, if correct, it has a similar
    structure to the territory. – Alfred Korzybski
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 地图不是它所代表的领土，但如果它是正确的，它具有与领土相似的结构。 – 阿尔弗雷德·科尔齐布斯基
- en: Models should be designed as approximations to help us understand a particular
    problem or a class of related problems. Models are not designed to be verbatim
    copies of the *real world*. Thus, all models are wrong in the same sense that
    maps are not the territory. But not all models are equally wrong; some models
    will be better than others at describing a given problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型应当作为近似值来设计，帮助我们理解一个特定问题或一类相关问题。模型并不是为了完美复制*真实世界*而设计的。因此，所有模型都在某种意义上是错误的，就像地图不是领土一样。但并非所有模型都是同样错误的；有些模型在描述某个特定问题时会比其他模型更好。
- en: 'In the previous chapters, we focused our attention on the inference problem,
    that is, how to learn the values of parameters from data. In this chapter, we
    are going to focus on a complementary problem: how to compare two or more models
    for the same data. As we will learn, this is both a central problem in data analysis
    and a tricky one. In this chapter, we are going to keep examples super simple,
    so we can focus on the technical aspects of model comparison. In the forthcoming
    chapters, we are going to apply what we learn here to more complex examples.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们将注意力集中在推断问题上，也就是如何从数据中学习参数值。在本章中，我们将重点关注一个互补的问题：如何比较针对同一数据的两个或多个模型。正如我们将要学习的，这既是数据分析中的一个核心问题，也是一个棘手的问题。在本章中，我们将保持示例简单，以便专注于模型比较的技术细节。在接下来的章节中，我们将把在这里学到的知识应用于更复杂的例子。
- en: 'In this chapter, we will explore the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: Overfitting and underfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合与欠拟合
- en: Information criteria
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息准则
- en: Cross-validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Bayes factors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯因子
- en: 5.1 Posterior predictive checks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 后验预测检查
- en: We have previously introduced and discussed posterior predictive checks as a
    way to assess how well a model explains the data used to fit a model. The purpose
    of this type of testing is not to determine whether a model is incorrect; we already
    know this! The goal of the exercise is to understand how well we are capturing
    the data. By performing posterior predictive checks, we aim to better understand
    the limitations of a model. Once we understand the limitations, we can simply
    acknowledge them or try to remove them by improving the model. It is expected
    that a model will not be able to reproduce all aspects of a problem and this is
    usually not a problem as models are built with a purpose in mind. As different
    models often capture different aspects of data, we can compare models using posterior
    predictive checks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前介绍并讨论了后验预测检查，作为评估模型如何解释用于拟合模型的数据的一种方法。这种测试的目的是不是确定模型是否错误；我们早就知道这一点了！这个过程的目标是理解我们如何捕捉数据。通过进行后验预测检查，我们旨在更好地理解模型的局限性。一旦我们理解了局限性，我们可以简单地承认它们，或者通过改进模型来尝试去除它们。预计模型无法重现问题的所有方面，这通常不是问题，因为模型是为了特定目的构建的。由于不同的模型通常捕捉数据的不同方面，我们可以通过后验预测检查来比较模型。
- en: 'Let’s look at a simple example. We have a dataset with two variables, `x` and
    `y`. We are going to fit these data with a linear model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子。我们有一个包含两个变量`x`和`y`的数据集。我们将使用线性模型来拟合这些数据：
- en: '![y = 𝛼 + 𝛽x ](img/file139.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![y = 𝛼 + 𝛽x ](img/file139.jpg)'
- en: 'We will also fit the data using a quadratic model, that is, a model with one
    more term than the linear model. For this extra term, we just take *x* to the
    power of 2 and add a *β* coefficient:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用二次模型来拟合数据，即一个比线性模型多一个项的模型。对于这个额外的项，我们只需将*x*的平方加上一个*β*系数：
- en: '![y = 𝛼 + 𝛽0x + 𝛽1x2 ](img/file140.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![y = 𝛼 + 𝛽0x + 𝛽1x2 ](img/file140.jpg)'
- en: 'We can write these models in PyMC as usual; refer to the following code block.
    The only difference from all previous models we have seen so far is that we pass
    the argument `idata_kwargs="log_likelihood": True` to `pm.sample`. This extra
    step will store the log-likelihood in the `InferenceData` object, and we will
    use this info later:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以像往常一样在PyMC中编写这些模型；参考以下代码块。与我们之前见过的所有模型唯一的不同之处在于，我们向`pm.sample`传递了`idata_kwargs="log_likelihood":
    True`这个参数。这个额外的步骤将把对数似然值存储在`InferenceData`对象中，我们稍后将使用这个信息：'
- en: '**Code 5.1**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.1**'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure [5.1](#x1-96028r1)* shows the mean fit from both models. Visually,
    both models seem to provide a reasonable fit to the data. At least for me, it
    is not that easy to see which model is best. What do you think?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [5.1](#x1-96028r1)* 显示了两个模型的均值拟合。从视觉上看，两个模型似乎都对数据提供了合理的拟合。至少对我来说，要看出哪个模型最好并不那么容易。你怎么看？'
- en: '![PIC](img/file141.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file141.png)'
- en: '**Figure 5.1**: Mean fit for `model_l` (linear) and `model_q` (quadratic)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.1**：`model_l`（线性模型）和`model_q`（二次模型）的均值拟合'
- en: To gain further insights, we can do a posterior predictive check. *Figure [5.2](#x1-96030r2)*
    shows KDEs for the observed and predicted data. Here, it is easy to see that `model_q`,
    the quadratic model, provides a better fit to the data. We can also see there
    is a lot of uncertainty, in particular at the tails of the distributions. This
    is because we have a small number of data points.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更多的见解，我们可以进行后验预测检查。*图 [5.2](#x1-96030r2)* 显示了观察值和预测值的数据的KDE（核密度估计）。在这里，很容易看出`model_q`，即二次模型，更好地拟合了数据。我们还可以看到，特别是在分布的尾部，有很多不确定性。这是因为我们数据点的数量很少。
- en: '![PIC](img/file142.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file142.png)'
- en: '**Figure 5.2**: Posterior predictive checks for `model_l` and `model_q` created
    with the `az.plot_ppc` function'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.2**：使用`az.plot_ppc`函数创建的`model_l`和`model_q`的后验预测检查'
- en: Posterior predictive checks are a very versatile idea. We can compare observed
    and predicted data in so many ways. For instance, instead of comparing the densities
    of the distributions, we can compare summary statistics. In the top panel of *Figure
    [5.3](#x1-96032r3)*, we have the distributions of means for both models. The dot
    over the x axis indicates the observed value. We can see that both models capture
    the mean very well, with the quadratic model having less variance. That both models
    capture the mean very well is not surprising as we are explicitly modeling the
    mean. In the bottom panel, we have the distributions of the interquartile range.
    This comparison favors the linear model instead.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 后验预测检查是一个非常灵活的概念。我们可以通过许多方式比较观察值和预测值。例如，我们可以比较分布的密度，而不是直接比较密度，我们还可以比较总结统计量。在*图
    [5.3](#x1-96032r3)*的顶部面板中，我们展示了两个模型的均值分布。x轴上的点表示观察值。我们可以看到，两个模型都很好地捕捉了均值，二次模型的方差更小。两个模型都能够很好地捕捉均值并不令人惊讶，因为我们显式地建模了均值。在底部面板中，我们展示了四分位间距的分布。这一比较则偏向于线性模型。
- en: '![PIC](img/file143.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file143.png)'
- en: '**Figure 5.3**: Posterior predictive checks for `model_l` and `model_q` created
    with the `az.plot_bpv` function'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.3**：使用`az.plot_bpv`函数创建的`model_l`和`model_q`的后验预测检查'
- en: In general, a statistic that is *orthogonal* to what the model is explicitly
    modeling will be more informative for evaluating the model. When in doubt, it
    may be convenient to evaluate more than one statistic. A useful question is to
    ask yourself what aspects of the data you are interested in capturing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，与模型明确建模的内容*正交*的统计量对于评估模型来说会更具信息量。如果有疑问，评估多个统计量可能会更方便。一个有用的问题是，问问自己你对数据的哪些方面感兴趣。
- en: 'To generate *Figure [5.3](#x1-96032r3)*, we used the `az.plot_bpv` ArviZ function.
    An excerpt of the full code to generate that figure is the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成*图 [5.3](#x1-96032r3)*，我们使用了`az.plot_bpv` ArviZ函数。生成该图的完整代码片段如下：
- en: '**Code 5.2**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.2**'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that we use the `kind="t_stat"` argument to indicate that we are going
    to use a summary statistic. We can pass a string as in `t_stat="mean"`, to indicate
    that we want to use the mean as the summary statistic. Or, we can use a user-defined
    function, as in `t_stat=iqr`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了`kind="t_stat"`参数来指示我们将使用总结统计量。我们可以传递一个字符串，比如`t_stat="mean"`，表示我们想要使用均值作为总结统计量。或者，我们也可以使用用户定义的函数，比如`t_stat=iqr`。
- en: 'You may have noticed that *Figure [5.3](#x1-96032r3)* also includes a legend
    with `bpv` values. **bpv** stands for Bayesian p-value. This is a numerical way
    of summarizing a comparison between simulated and observed data. To obtain them,
    a summary statistic *T* is chosen, such as the mean, median, standard deviation,
    or whatever you may think is worth comparing. Then *T* is calculated for the observed
    data *T*[obs] and for the simulated data *T*[sim]. Finally, we ask ourselves the
    question ”what is the probability that *T*[sim] is less than or equal to *T*[obs]?”.
    If the observed values agree with the predicted ones, the expected value will
    be 0.5\. In other words, half of the predictions will be below the observations
    and half will be above. This quantity is known as the **Bayesian** **p-value**:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，*图 [5.3](#x1-96032r3)* 也包含了带有 `bpv` 值的图例。**bpv** 代表贝叶斯p值。这是一种以数值方式总结模拟数据和观察数据之间比较的方法。为了获得它们，选择一个汇总统计量
    *T*，比如均值、中位数、标准差，或者你认为值得比较的任何东西。然后，计算观察数据 *T*[obs] 和模拟数据 *T*[sim] 的 *T*。最后，我们问自己这个问题：“*T*[sim]
    小于或等于 *T*[obs] 的概率是多少？”如果观察值与预测值一致，那么期望值将为 0.5。换句话说，一半的预测值将低于观察值，一半将高于观察值。这个量被称为
    **贝叶斯** **p值**：
- en: '![Bayesian p-value ≜ p(Tsim ≤ Tobs | ˜Y) ](img/file144.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯 p值 ≜ p(Tsim ≤ Tobs | ˜Y)](img/file144.jpg)'
- en: There is yet another way to compute a Bayesian p-value. Instead of using a summary
    statistic, we can use the entire distribution. In this case, we can ask ourselves
    the question ”what is the probability of predicting a lower or equal value for
    **each observed value**?”. If the model is well calibrated, these probabilities
    should be the same for all observed values. Because the model is capturing all
    observations equally well, we should expect a Uniform distribution. ArviZ can
    help us with the computations; this time we need to use the `az.plot_bpv` function
    with the `kind="p_value"` argument (which is the default). *Figure [5.4](#x1-96050r4)*
    shows the results of this calculation. The white line indicates the expected Uniform
    distribution and the gray band shows the expected deviation given the finite size
    of the sample. It can be seen that these models are very similar.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种计算贝叶斯p值的方法。我们可以使用整个分布，而不是使用汇总统计量。在这种情况下，我们可以问自己这个问题：“对于**每个观察值**，预测一个更小或相等的值的概率是多少？”。如果模型被很好地校准，那么这些概率应该对于所有观察值是相同的。因为模型能够同样好地捕捉所有观察值，所以我们应该期待一个均匀分布。ArviZ
    可以帮助我们进行计算；这次我们需要使用带有 `kind="p_value"` 参数（这是默认值）的 `az.plot_bpv` 函数。*图 [5.4](#x1-96050r4)*
    显示了此计算的结果。白色线条表示期望的均匀分布，灰色带状区域显示了由于样本的有限大小而预期的偏差。可以看出，这些模型非常相似。
- en: '![PIC](img/file145.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file145.png)'
- en: '**Figure 5.4**: Posterior predictive checks for `model_l` and `model_q` created
    with the `az.plot_bpv` function'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.4**：通过 `az.plot_bpv` 函数创建的 `model_l` 和 `model_q` 的后验预测检验'
- en: Not Those p-values
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 那些不是 p 值
- en: For those who are familiar with p-values and their use in frequentist statistics,
    there are a couple of clarifications. What is *Bayesian* about these p-values
    is that we are NOT using a sampling distribution but the posterior predictive
    distribution. Additionally, we are not doing a null hypothesis test, nor trying
    to declare that a difference is ”significant.” We are simply trying to quantify
    how well the model explains the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些熟悉 p 值及其在频率统计中的使用的人来说，有一些需要澄清的地方。关于这些 p 值的“贝叶斯”之处在于，我们并没有使用抽样分布，而是使用了后验预测分布。此外，我们并没有进行原假设检验，也没有试图声明一个差异是“显著的”。我们只是试图量化模型如何解释数据。
- en: Posterior predictive checks provide a very flexible framework for evaluating
    and comparing models, either using plots or numerical summaries such as Bayesian
    p-values, or a combination of both. The concept is general enough to allow an
    analyst to use their imagination to find different ways to explore the model’s
    predictions and use the ones that best suit their modeling goals.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 后验预测检验提供了一个非常灵活的框架，用于评估和比较模型，无论是使用图形，还是使用诸如贝叶斯 p 值之类的数值摘要，或两者的组合。这个概念足够通用，允许分析人员发挥想象力，寻找不同的方式来探索模型的预测，并使用最适合其建模目标的方式。
- en: In the following sections, we will explore other methods for comparing models.
    These new methods can be used in combination with posterior predictive checks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探索其他比较模型的方法。这些新方法可以与后验预测检验结合使用。
- en: 5.2 The balance between simplicity and accuracy
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 简单性与准确性之间的平衡
- en: When choosing between alternative explanations, there is a principle known as
    Occam’s razor. In very general terms, this principle establishes that given two
    or more equivalent explanations for the same phenomenon, the simplest is the preferred
    explanation. A common criterion of simplicity is the number of parameters in a
    model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择不同解释时，有一个原则被称为奥卡姆剃刀。一般来说，这个原则规定，当有两个或更多等效的解释时，最简单的解释是首选解释。简洁性的一种常见标准是模型中的参数数量。
- en: There are many justifications for this heuristic. We are not going to discuss
    any of them; we are just going to accept them as a reasonable guide.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这一启发式方法有很多理由支持。我们不会讨论它们的具体内容，我们只会将它们视为一种合理的指导原则。
- en: Another factor that we generally have to take into account when comparing models
    is their accuracy, that is, how good a model is at fitting the data. According
    to this criterion, if we have two (or more) models and one of them explains the
    data better than the other, then that is the preferred model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在比较模型时通常需要考虑的另一个因素是它们的准确性，也就是模型对数据的拟合程度。根据这一标准，如果我们有两个（或更多）模型，其中一个比另一个更好地解释了数据，那么这个模型就是首选模型。
- en: Intuitively, it seems that when comparing models, we tend to prefer those that
    best fit the data and those that are simple. But what should we do if these two
    principles lead us to different models? Or, more generally, is there a quantitative
    way to balance both contributions? The short answer is yes, and in fact, there
    is more than one way to do it. But first, let’s see an example to gain intuition.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，似乎在比较模型时，我们倾向于偏好那些最能拟合数据且简单的模型。但如果这两个原则导致我们选择不同的模型，我们该怎么做呢？或者更一般地说，是否有一种量化的方法来平衡这两者的贡献？简短的回答是有，而且实际上有不止一种方法可以做到。但首先，让我们看一个例子，以便获得直觉。
- en: 5.2.1 Many parameters (may) lead to overfitting
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 多个参数（可能）导致过拟合
- en: '*Figure [5.5](#x1-98002r5)* shows three models with an increasing number of
    parameters. The first one (order 0) is just a constant value: whatever the value
    of *X*, the model always predicts the same value for *Y* . The second model (order
    1) is a linear model, as we saw in *Chapter [4](CH04.xhtml#x1-760004)*. The last
    one (order 5) is a polynomial model of order 5\. We will discuss polynomial regression
    in more depth in *Chapter [6](CH06.xhtml#x1-1200006)*, but for the moment, we
    just need to know that the core of the model has the form *α* + *β*[0]*x* + *β*[0]*x*²
    + *β*[0]*x*³ + *β*[0]*x*⁴ + *β*[0]*x*⁵.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [5.5](#x1-98002r5)*展示了三个参数数量逐渐增加的模型。第一个（零次）只是一个常数值：无论 *X* 的值是多少，模型总是预测相同的
    *Y* 值。第二个模型（一阶）是一个线性模型，就像我们在*第 [4](CH04.xhtml#x1-760004) 章*中看到的那样。最后一个模型（五次）是一个五次多项式模型。我们将在*第
    [6](CH06.xhtml#x1-1200006) 章*中更深入地讨论多项式回归，但目前我们只需要知道，该模型的核心形式是 *α* + *β*[0]*x*
    + *β*[0]*x*² + *β*[0]*x*³ + *β*[0]*x*⁴ + *β*[0]*x*⁵。'
- en: '![PIC](img/file146.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file146.png)'
- en: '**Figure 5.5**: Three models for a simple dataset'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.5**：简单数据集的三种模型'
- en: In *Figure [5.5](#x1-98002r5)*, we can see that the increase in the complexity
    of the model (number of parameters) is accompanied by a greater accuracy reflected
    in the coefficient of determination *R*². This is a way to measure the fit of
    a model (for more information, please read [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)).
    In fact, we can see that the polynomial of order 5 fits the data perfectly, obtaining
    *R*² = 1.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 [5.5](#x1-98002r5)*中，我们可以看到，随着模型复杂度（参数数量）的增加，模型的准确度也得到了提升，这一变化反映在决定系数 *R*²
    中。这是一种衡量模型拟合度的方式（欲了解更多信息，请阅读[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)）。事实上，我们可以看到五次多项式完美地拟合了数据，得到了
    *R*² = 1。
- en: Why can the polynomial of order 5 capture the data without errors? The reason
    is that we have the same number of parameters as data, that is, six. Therefore,
    the model is simply acting as an alternative way of expressing the data. The model
    is not learning patterns about the data, it is memorizing the data! This can be
    problematic. The easier way to notice this is by thinking about what will happen
    to a model that memorizes data when presented with new, unobserved data. What
    do you think will happen?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么五次多项式可以在没有误差的情况下拟合数据呢？原因在于我们有与数据相同数量的参数，也就是六个。因此，模型实际上只是作为一种表达数据的替代方式。模型并没有学习数据的模式，而是在记忆数据！这可能是个问题。察觉这一点的更简单方式是，思考当面对新的、未见过的数据时，记住数据的模型会发生什么。你认为会发生什么？
- en: Well, the performance is expected to be bad, like someone who just memorizes
    the questions for an exam only to find the questions have been changed at the
    last minute! This situation is represented in *Figure [5.6](#x1-98004r6)*; here,
    we have added two new data points. Maybe we got the money to perform a new experiment
    or our boss just sent us new data. We can see that the model of order 5, which
    was able to exactly fit the data, now has a worse performance than the linear
    model, as measured by *R*². From this simple example, we can see that a model
    with the best fit is not always the ideal one.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，性能预期会很差，就像某人只是背了考试问题，却发现问题在最后一刻被更改了！这种情况在*图[5.6](#x1-98004r6)*中有所展示；这里，我们添加了两个新数据点。也许我们有资金进行新的实验，或者我们的老板刚刚给我们发送了新的数据。我们可以看到，原本能够完美拟合数据的5阶模型，现在在*R*²的度量下比线性模型表现得更差。从这个简单的例子中，我们可以看出，最适合的模型不一定是理想的模型。
- en: '![PIC](img/file147.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file147.png)'
- en: '**Figure 5.6**: Three models for a simple dataset, plus two new points'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5.6**：三个简单数据集的模型，外加两个新点'
- en: 'Loosely speaking, when a model fits the dataset used to learn the parameters
    of that model very well but fits new datasets very poorly, we have overfitting.
    This is a very common problem when analyzing data. A useful way to think about
    overfitting is to consider a dataset as having two components: the signal and
    the noise. The signal is what we want to capture (or learn) from the data. If
    we use a dataset, it is because we believe there is a signal there, otherwise
    it will be an exercise in futility. Noise, on the other hand, is not useful and
    is the product of measurement errors, limitations in the way the data was generated
    or captured, the presence of corrupted data, etc. A model overfits when it is
    so flexible (for a dataset) that it is capable of learning noise. This has the
    consequence that the signal is hidden.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 大致来说，当一个模型非常好地拟合了用于学习该模型参数的数据集，但在拟合新数据集时表现很差，我们就遇到了过拟合问题。这是分析数据时非常常见的一个问题。思考过拟合的一个有用方法是将数据集视为包含两个部分：信号和噪声。信号是我们希望从数据中捕捉到的（或学习到的）内容。如果我们使用某个数据集，那是因为我们认为其中有信号，否则这将是徒劳的。另一方面，噪声是没有用的，它是测量误差、数据生成或捕获方式的局限性、数据损坏等因素的产物。当模型过于灵活（对于一个数据集）到能够学习噪声时，就会发生过拟合。这导致信号被隐藏起来。
- en: This is a practical justification for Occam’s razor, and also a warning that,
    at least in principle, it is always possible to create a model so complex that
    it explains all the details in a dataset, even the most irrelevant ones — like
    the cartographers in Borges’ tale, who crafted a map of the Empire as vast as
    the Empire itself, perfectly replicating every detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是奥卡姆剃刀的一个实际论据，同时也是一个警告：至少在原则上，总是可以创建一个复杂到足以解释数据集中所有细节的模型，甚至是最不相关的细节——就像博尔赫斯故事中的制图师，他们制作了一张与帝国一样庞大的地图，完美复制了每一个细节。
- en: 5.2.2 Too few parameters lead to underfitting
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 参数过少导致欠拟合
- en: Continuing with the same example but at the other extreme of complexity, we
    have the model of order 0\. This model is simply a Gaussian disguised as a linear
    model. This model is only capable of capturing the value of the mean of *Y* and
    is therefore totally indifferent to the values of *X*. We say that this model
    has underfitted the data. Models that underfit can also be misleading, especially
    if we are unaware of it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用相同的例子，但从复杂度的另一极来看，我们得到了0阶模型。这个模型仅仅是一个伪装成线性模型的高斯分布。这个模型只能捕捉到*Y*的均值，因此完全不关心*X*的值。我们说这个模型对数据进行了欠拟合。欠拟合的模型也可能具有误导性，尤其是在我们没有意识到这一点时。
- en: 5.3 Measures of predictive accuracy
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 预测准确性的度量
- en: ”Everything should be made as simple as possible, but not simpler” is a quote
    often attributed to Einstein. As in a healthy diet, when modeling, we have to
    maintain a balance. Ideally, we would like to have a model that neither underfits
    nor overfits the data. We want to somehow balance simplicity and goodness of fit.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: “一切应当尽可能简化，但不能简化得过度”是一句常被归因于爱因斯坦的话。就像健康饮食一样，建模时我们也需要保持平衡。理想情况下，我们希望有一个既不过度拟合也不欠拟合数据的模型。我们希望在简洁性和拟合优度之间找到某种平衡。
- en: In the previous example, it is relatively easy to see that the model of order
    0 is too simple, while the model of order 5 is too complex. In order to get a
    general approach that will allow us to rank models, we need to formalize our intuition
    about this balance of simplicity and accuracy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，相对容易看出，0阶模型过于简单，而5阶模型过于复杂。为了得到一个通用的方法，使我们能够对模型进行排序，我们需要将这种简单性和准确性之间的平衡形式化。
- en: 'Let’s look at a couple of terms that will be useful to us:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几个对我们有用的术语：
- en: '**Within-sample accuracy**: The accuracy is measured with the same data used
    to fit the model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本内准确度**：使用与拟合模型相同的数据来衡量的准确度。'
- en: '**Out-of-sample accuracy**: The accuracy measured with data not used to fit
    the model.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本外准确度**：使用未用于拟合模型的数据来衡量的准确度。'
- en: 'The within-sample accuracy will, on average, be greater than the out-of-sample
    accuracy. That is why using the within-sample accuracy to evaluate a model, in
    general, will lead us to think that we have a better model than we really have.
    Using out-of-sample accuracy is therefore a good idea to avoid fooling ourselves.
    However, leaving data out means we will have less data to inform our models, which
    is a luxury we generally cannot afford. Since this is a central problem in data
    analysis, there are several proposals to address it. Two very popular approaches
    are:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 样本内准确度的平均值通常会大于样本外准确度。这就是为什么一般来说，使用样本内准确度来评估模型会让我们误以为模型比实际更好。因此，使用样本外准确度是一个不错的选择，可以避免我们自我欺骗。然而，留下数据意味着我们将拥有更少的数据来训练模型，而这通常是我们不能奢侈的。因此，这个问题在数据分析中是一个核心问题，已有多个提案来解决它。两种非常流行的方法是：
- en: '**Information criteria**: This is a general term that’s used to refer to various
    expressions that approximate out-of-sample accuracy as in-sample accuracy plus
    a term that penalizes model complexity.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息准则**：这是一个总称，用来指代各种表达式，这些表达式将样本外准确度近似为样本内准确度加上一个惩罚模型复杂性的项。'
- en: '**Cross-validation**: This is an empirical strategy based on dividing the available
    data into separate subsets that are alternatively used to fit and evaluate the
    models.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：这是一种基于将可用数据分为不同子集的方法，这些子集交替用于拟合和评估模型。'
- en: Let’s look at both of those approaches in more detail in the following sections.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的部分更详细地讨论这两种方法。
- en: 5.3.1 Information criteria
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 信息准则
- en: Information criteria are a collection of closely related tools used to compare
    models in terms of goodness-of-fit and model complexity. In other words, the information
    criteria formalize the intuition that we developed at the beginning of the chapter.
    The exact way in which these quantities are derived has to do with a field known
    as Information Theory ([[MacKay](Bibliography.xhtml#Xmackay_2003), [2003](Bibliography.xhtml#Xmackay_2003)]),
    which is fun, but we will pursue a more intuitive explanation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 信息准则是一组密切相关的工具，用于从拟合优度和模型复杂度的角度比较模型。换句话说，信息准则形式化了我们在本章开始时发展起来的直觉。这些量的具体推导方式与一个叫做信息理论的领域有关（[[MacKay](Bibliography.xhtml#Xmackay_2003),
    [2003](Bibliography.xhtml#Xmackay_2003)]），这是一个有趣的领域，但我们将追求一个更直观的解释。
- en: 'One way to measure how well a model fits the data is to calculate the root
    mean square error between the data and the predictions made by the model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量模型拟合数据好坏的一种方法是计算数据与模型预测值之间的均方根误差：
- en: '![ n 1-∑ 2 n (yi − E(yi | θ)) i=1 ](img/file148.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![ n 1-∑ 2 n (yi − E(yi | θ)) i=1 ](img/file148.jpg)'
- en: E(*y*[*i*]|*θ*) is the predicted value given the estimated parameters. It is
    important to note that this is essentially the average of the squared difference
    between the observed and predicted data. Taking the square of the errors ensures
    that the differences do not cancel out and emphasizes large errors compared to
    other alternatives such as calculating the absolute value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: E(*y*[*i*]|*θ*)是给定估计参数后的预测值。值得注意的是，这本质上是观察值和预测数据之间平方差的平均值。将误差平方可以确保差异不会相互抵消，并且相对于其他方法（例如计算绝对值），它能更强调较大的误差。
- en: 'The root mean square error may be familiar to you. It is a very popular measure
    – so popular that we may have never spent time thinking about it. But if we do,
    we will see that, in principle, there is nothing special about it and we could
    well devise other similar expressions. When we adopt a probabilistic approach,
    as we do in this book, a more general (and *natural*) expression is the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差可能你已经很熟悉了。它是一个非常流行的度量——流行到我们可能从未花时间思考它。但如果我们仔细想想，就会发现，原则上它并没有什么特别之处，我们完全可以设计出其他类似的表达式。当我们采用概率方法时，正如本书中所做的那样，一个更一般（和*自然的*）的表达式如下：
- en: '![∑n log p(yi | θ) i=1 ](img/file149.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![∑n log p(yi | θ) i=1 ](img/file149.jpg)'
- en: That is, we compute the likelihood for each of the *n* observations. We take
    the sum instead of the product because we are working with logarithms. Why do
    we say this is *natural*? Because we can think that, when choosing a likelihood
    for a model, we are implicitly choosing how we want to penalize deviations between
    the data and predictions. In fact, when *p*(*y*[*i*]|*θ*) is a Gaussian, then
    the above expression will be proportional to the root mean square error.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们为每个*n*个观测值计算似然。我们使用和而不是乘积，因为我们在使用对数。为什么我们说这是*自然的*呢？因为我们可以认为，在为模型选择似然时，我们实际上在选择如何惩罚数据与预测之间的偏差。事实上，当
    *p*(*y*[*i*]|*θ*) 是高斯分布时，上述表达式将与均方根误差成比例。
- en: Now, let’s shift our focus to a detailed exploration of a few specific information
    criteria.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向对几个特定信息准则的详细探索。
- en: Akaike Information Criterion
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 赤池信息量准则
- en: '**Akaike Information Criterion** (**AIC**) is a well-known and widely used
    information criterion outside the Bayesian universe and is defined as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**赤池信息量准则**（**AIC**）是一个著名且广泛使用的信息准则，尤其在贝叶斯领域外，定义为：'
- en: '![ ∑n ˆ AIC = − 2 log p(yi |θmle)+ 2k i=1 ](img/file150.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑n ˆ AIC = − 2 log p(yi |θmle)+ 2k i=1 ](img/file150.jpg)'
- en: '*k* is the number of model parameters and ![](img/hat_theta.png)[*mle*] is
    the maximum likelihood estimate for *θ*.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 是模型参数的数量，![](img/hat_theta.png)[*mle*] 是 *θ* 的最大似然估计。'
- en: Maximum likelihood estimation is common practice for non-Bayesians and is, in
    general, equivalent to Bayesian **maximum a posteriori** (**MAP**) estimation
    when *flat* priors are used. It is important to note that ![](img/hat_theta.png)[*mle*]
    is a point estimate and not a distribution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最大似然估计是非贝叶斯方法中的常见做法，并且通常，当使用*平坦的*先验时，它等同于贝叶斯**最大后验估计**（**MAP**）。需要注意的是，![](img/hat_theta.png)[*mle*]
    是一个点估计，而不是一个分布。
- en: The factor −2 is just a constant, and we could omit it but usually don’t. What
    is important, from a practical point of view, is that the first term takes into
    account how well the model fits the data, while the second term penalizes the
    complexity of the model. Therefore, if two models fit the data equally well, AIC
    says that we should choose the model with the fewest parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因子 −2 只是一个常数，我们可以省略它，但通常不这么做。从实际的角度来看，重要的是，第一个项考虑了模型与数据的拟合程度，而第二个项则惩罚了模型的复杂度。因此，如果两个模型对数据的拟合程度相同，AIC
    表示我们应该选择参数最少的模型。
- en: AIC works fine in non-Bayesian approaches but is problematic otherwise. One
    reason is that it does not use the posterior distribution of *θ* and therefore
    discards information. Also, AIC, from a Bayesian perspective, assumes that priors
    are *flat* and therefore AIC is incompatible with informative and slightly informative
    priors like those used in this book. Also, the number of parameters in a model
    is not a good measure of the model’s complexity when using informative priors
    or structures like hierarchical structures, as these are ways of reducing the
    effective number of parameters, also known as *regularization*. We will return
    to this idea of regularization later.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: AIC 在非贝叶斯方法中表现良好，但在其他情况下存在问题。一个原因是它没有使用*θ*的后验分布，因此丢失了信息。此外，从贝叶斯的角度来看，AIC 假设先验是*平坦的*，因此
    AIC 与像本书中使用的那些信息丰富或稍微信息丰富的先验不兼容。另外，当使用信息丰富的先验或层次结构等结构时，模型中的参数数量并不是衡量模型复杂度的好方法，因为这些方法会减少有效参数的数量，也称为*正则化*。我们将在后面回到正则化的这个概念。
- en: Widely applicable information criteria
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 广泛适用的信息准则
- en: '**Widely applicable information criteria** (**WAIC**) is something like the
    Bayesian version of AIC. It also has two terms, one that measures how good the
    fit is and the other that penalizes complex models. But WAIC uses the full posterior
    distribution to estimate both terms. The following expression assumes that the
    posterior distribution is represented as a sample of size *S* (as obtained from
    an MCMC method):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**广泛应用的信息准则** (**WAIC**) 类似于AIC的贝叶斯版本。它也有两个项，一个衡量拟合的好坏，另一个对复杂模型进行惩罚。但WAIC使用完整的后验分布来估计这两个项。以下表达式假设后验分布作为大小为*S*的样本（通过MCMC方法获得）来表示：'
- en: '![ ( ) ∑n 1 ∑S s ∑ n ( S s) W AIC = − 2 log S- p(yi | θ ) + 2 Vs=1logp(yi |
    θ) i s=1 i ](img/file151.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) ∑n 1 ∑S s ∑ n ( S s) W AIC = − 2 log S- p(yi | θ ) + 2 Vs=1logp(yi |
    θ) i s=1 i ](img/file151.jpg)'
- en: The first term is similar to the Akaike criterion, except it is evaluated for
    all the observations and all the samples of the posterior. The second term is
    a bit more difficult to justify without getting into technicalities. But it can
    be interpreted as the effective number of parameters. What is important from a
    practical point of view is that WAIC uses the entire posterior (and not a point
    estimate) for the calculation of both terms, so WAIC can be applied to virtually
    any Bayesian model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项与赤池准则类似，只不过它是在所有观测值和所有后验样本上进行评估的。第二个项有点难以解释，除非涉及到一些技术细节。但它可以被理解为有效参数的数量。从实践角度来看，重要的是WAIC使用整个后验分布（而非点估计）来计算这两个项，因此WAIC可以应用于几乎任何贝叶斯模型。
- en: Other information criteria
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他信息准则
- en: Another widely used information criterion is the **Deviance Information** **Criterion**
    (**DIC**). If we use the *bayes-o-meter*^(TM), DIC is more Bayesian than AIC but
    less than WAIC. Although still popular, WAIC and mainly LOO (see the next section)
    have been shown to be more useful both theoretically and empirically than DIC.
    Therefore, we do not recommend its use.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的信息准则是**偏差信息准则** (**DIC**)。如果我们使用*bayes-o-meter*^(TM)，DIC比AIC更具贝叶斯特性，但不如WAIC。虽然仍然很流行，但已通过理论和实证研究表明，WAIC和主要的LOO（见下一节）比DIC更有用。因此，我们不推荐使用它。
- en: Another widely used criterion is **Bayesian Information Criteria** (**BIC**).
    Like logistic regression and my mother’s *dry soup*, this name can be misleading.
    BIC was proposed as a way to correct some of the problems with AIC and the authors
    proposed a Bayesian justification for it. But BIC is not really Bayesian in the
    sense that, like AIC, it assumes flat priors and uses maximum likelihood estimation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的准则是**贝叶斯信息准则** (**BIC**)。就像逻辑回归和我母亲的*干汤*一样，这个名字可能会让人误解。BIC作为一种修正AIC问题的方法提出，作者也为其提出了贝叶斯的理论依据。但BIC并不完全是贝叶斯的，因为像AIC一样，它假设使用平坦先验，并使用最大似然估计。
- en: But more importantly, BIC differs from AIC and WAIC in its objective. AIC, WAIC,
    and LOO (see next section) try to reflect which model generalizes better to other
    data (predictive accuracy), while BIC tries to identify which is the *correct*
    model and therefore is more related to Bayes factors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但更重要的是，BIC与AIC和WAIC在目标上有所不同。AIC、WAIC和LOO（见下一节）试图反映哪个模型能更好地推广到其他数据（预测精度），而BIC试图识别哪个是*正确*的模型，因此更与贝叶斯因子相关。
- en: 5.3.2 Cross-validation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 交叉验证
- en: Cross-validation is a simple and, in most cases, effective solution for comparing
    models. We take our data and divide it into K slices. We try to keep the slices
    more or less the same (in size and sometimes also in other characteristics, such
    as the number of classes). We then use K-1 slices to train the model and slice
    to test it. This process is the systematically repeated omission, for each iteration,
    of a different slice from the training set and using that slice as the evaluation
    set. This is repeated until we have completed K fit-and-evaluation rounds, as
    can be seen in *Figure [5.7](#x1-105004r7)*. The accuracy of the model will be
    the average over the accuracy for each of the K rounds. This is known as K-fold
    cross-validation. Finally, once we have performed cross-validation, we use all
    the data for one last fit and this is the model that is used to make predictions
    or for any other purpose.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种简单且在大多数情况下有效的模型比较方法。我们将数据划分为K个切片，尽量使这些切片在大小上保持一致（有时也会在其他特征上保持一致，如类别数量）。然后，我们使用K-1个切片来训练模型，使用剩下的一个切片来进行测试。这个过程是通过系统地反复省略每次训练集中的一个不同切片，并使用该切片作为评估集来完成的。直到完成K轮拟合与评估，可以在*图
    [5.7](#x1-105004r7)*中看到。模型的准确度将是K轮中每一轮准确度的平均值。这被称为K折交叉验证。最后，在执行完交叉验证后，我们使用所有数据进行最后一次拟合，这个模型就可以用来进行预测或其他目的。
- en: '![PIC](img/file152.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file152.png)'
- en: '**Figure 5.7**: K-fold cross-validation'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.7**：K折交叉验证'
- en: When K equals the number of data points, we get what is known as **leave-one-out
    cross-validation** (**LOOCV**), meaning we fit the model to all but one data point
    each time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当K等于数据点的数量时，我们得到的就是**留一法交叉验证**（**LOOCV**），即每次都将模型拟合到除了一个数据点之外的所有数据点上。
- en: Cross-validation is a routine practice in machine learning, and we have barely
    described the most essential aspects of this practice. There are many other variants
    of the schema presented here. For more information, you can read [James et al.](Bibliography.xhtml#Xjames_2023) [[2023](Bibliography.xhtml#Xjames_2023)]
    or [Raschka et al.](Bibliography.xhtml#Xraschka_2022) [[2022](Bibliography.xhtml#Xraschka_2022)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是机器学习中的常规实践，我们所描述的仅仅是这种实践的最基本方面。这里展示的模式还有很多变体。有关更多信息，您可以阅读[James 等人](Bibliography.xhtml#Xjames_2023)
    [[2023](Bibliography.xhtml#Xjames_2023)]或[Raschka 等人](Bibliography.xhtml#Xraschka_2022)
    [[2022](Bibliography.xhtml#Xraschka_2022)]的研究。
- en: Cross-validation is a very simple and useful idea, but for some models or for
    large amounts of data, the computational cost of cross-validation may be beyond
    our means. Many people have tried to find simpler quantities to calculate, like
    Information Criteria. In the next section, we discuss a method to approximate
    cross-validation from a single fit to all the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一个非常简单且有用的概念，但对于某些模型或大规模数据集，交叉验证的计算成本可能超出了我们的能力。许多人尝试找到更简单的计算量，比如信息准则。在接下来的章节中，我们将讨论一种通过对所有数据进行单次拟合来近似交叉验证的方法。
- en: Approximating cross-validation
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 近似交叉验证
- en: 'Cross-validation is a nice idea, but it can be expensive, particularly variants
    like leave-one-out-cross-validation. Luckily, it is possible to approximate it
    using the information from a single fit to the data! The method for doing this
    is called ”Pareto smooth importance sampling leave-one-out cross-validation.”
    The name is so long that in practice we call it LOO. Conceptually, what we are
    trying to calculate is:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一个不错的想法，但它可能代价高昂，特别是像留一法交叉验证这样的变体。幸运的是，利用单次拟合的数据可以近似交叉验证！这一方法称为“帕累托平滑重要性采样留一法交叉验证”。这个名字非常长，因此在实践中我们通常简称它为LOO。从概念上来说，我们尝试计算的是：
- en: '![ n ∫ ∑ ELPDLOO -CV = log p(yi | θ) p(θ | y−i)dθ i=1 ](img/file153.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![ n ∫ ∑ ELPDLOO -CV = log p(yi | θ) p(θ | y−i)dθ i=1 ](img/file153.jpg)'
- en: This is the Expected Log-Pointwise-predictive Density (ELPD). We add the subscript
    *LOO-CV* to make it explicit we are computing the ELPD using leave-one-out cross-validation.
    The [−*i*] means that we leave the observation *i* out.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是期望的对数逐点预测密度（ELPD）。我们加上下标*LOO-CV*，以明确表示我们使用留一法交叉验证来计算ELPD。[−*i*]表示我们省略了观测值*i*。
- en: 'This expression is very similar to the one for the posterior predictive distribution.
    The difference is that, now, we want to compute the posterior predictive distribution
    for observation *y*[*i*] from a posterior distribution computed without the observation
    *y*[*i*]. The first approximation we take is to prevent the explicit computation
    of the integral by taking samples from the posterior distribution. Thus, we can
    write:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式与后验预测分布的表达式非常相似。不同之处在于，现在我们要计算的是从没有包含观察值 *y*[*i*] 的后验分布中计算的后验预测分布。我们采取的第一个逼近方法是通过从后验分布中抽样来避免显式计算积分。因此，我们可以写成：
- en: '![ n ( s ) ∑ ( 1-∑ j ) log S p(yi | θ−i) i j ](img/file154.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![ n ( s ) ∑ ( 1-∑ j ) log S p(yi | θ−i) i j ](img/file154.jpg)'
- en: Here, the sum is over *S* posterior samples. We have been using MCMC samples
    in this book a lot. So, this approximation should not sound unfamiliar to you.
    The tricky part comes next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，求和是对 *S* 后验样本进行的。在本书中，我们经常使用 MCMC 样本。因此，这种逼近方法对你来说应该不陌生。接下来是比较棘手的部分。
- en: It is possible to approximate ![](img/Formula_03.PNG) using importance sampling.
    We are not going to discuss the details of that statistical method, but we are
    going to see how importance sampling is a way of approximating a target distribution
    by re-weighting values obtained from another distribution. This method is useful
    when we do not know how to sample from the target distribution but we know how
    to sample from another distribution. Importance sampling works best when the known
    distribution is *wider* than the target one.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用重要性抽样来逼近 ![](img/Formula_03.PNG)。我们不会详细讨论这种统计方法，但我们将看到重要性抽样是一种通过重新加权从另一个分布中获得的值来逼近目标分布的方法。当我们不知道如何从目标分布中抽样，但知道如何从另一个分布中抽样时，这种方法很有用。重要性抽样在已知分布比目标分布更*宽泛*时效果最好。
- en: 'In our case, the known distribution, once a model has been fitted, is the log-likelihood
    for all the observations. And we want to approximate the log-likelihood if we
    had dropped one observation. For this, we need to estimate the ”importance” (or
    weight) that each observation has in determining the posterior distribution. The
    ”importance” of a given observation is proportional to the effect the variable
    will produce on the posterior if removed. Intuitively, a relatively unlikely observation
    is more important (or carries more weight) than an expected one. Luckily, these
    weights are easy to compute once we have computed the posterior distribution.
    In fact, the weight of the observation *i* for the *s* posterior sample is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，一旦模型拟合完成，已知分布就是所有观察值的对数似然。我们希望逼近的是如果我们去掉一个观察值后的对数似然。为此，我们需要估计每个观察值在确定后验分布中的“重要性”（或权重）。一个观察值的“重要性”与如果该观察值被移除时该变量对后验分布的影响成正比。直观地说，一个相对不太可能的观察值比一个预期中的观察值更为重要（或权重更大）。幸运的是，一旦我们计算了后验分布，这些权重就容易计算。实际上，观察值
    *i* 对于 *s* 后验样本的权重是：
- en: '![ 1 ws = -------- p(yi | θs) ](img/file155.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ws = -------- p(yi | θs) ](img/file155.jpg)'
- en: This *w*[*s*] may not be reliable. The main issue is that sometimes a few *w*[*s*]
    could be so large that they dominate our calculations, making them unstable. To
    tame these crazy weights, we can use Pareto smoothing. This solution consists
    of replacing some of these weights with weights obtained from fitting a Pareto
    distribution. Why a Pareto distribution? Because the theory indicates that the
    weights should follow this distribution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 *w*[*s*] 可能不可靠。主要的问题是，有时一些 *w*[*s*] 可能会非常大，以至于它们主导了我们的计算，使得计算不稳定。为了控制这些极端的权重，我们可以使用
    Pareto 平滑方法。这个解决方案包括将这些权重中的一部分替换为通过拟合 Pareto 分布获得的权重。为什么使用 Pareto 分布？因为理论表明，权重应该遵循这种分布。
- en: So, for each observation, *y*[*i*], the largest weights are used to estimate
    a Pareto distribution, and that distribution is used to replace those weights
    with ”smoothed” weights. This procedure gives robustness to the estimation of
    the ELPD and also provides a way to diagnose the approximation, i.e., to get a
    warning that the LOO method may be failing. For this, we need to pay attention
    to the values of *k*, which is a parameter of the Pareto distribution. Values
    of *k* greater than 0.7 indicate that we may have very influential observations.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个观察值 *y*[*i*]，使用最大的权重来估计 Pareto 分布，并且使用该分布来替换这些权重为“平滑”后的权重。这个过程为 ELPD
    的估计提供了稳健性，并且提供了一种诊断逼近的方法，即，能够发出警告，提示 LOO 方法可能出现问题。为此，我们需要关注 *k* 的值，*k* 是 Pareto
    分布的一个参数。*k* 值大于 0.7 表明我们可能有非常有影响力的观察值。
- en: 5.4 Calculating predictive accuracy with ArviZ
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 使用ArviZ计算预测准确性
- en: 'Fortunately, calculating WAIC and LOO with ArviZ is very simple. We just need
    to be sure that the Inference Data has the log-likelihood group. When computing
    a posterior with PyMC, this can be achieved by doing `pm.sample(idata_kwargs="log_likelihood":
    True)`. Now, let’s see how to compute LOO:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '幸运的是，使用ArviZ计算WAIC和LOO非常简单。我们只需要确保推理数据包含对数似然组。使用PyMC计算后验时，可以通过执行`pm.sample(idata_kwargs="log_likelihood":
    True)`来实现这一点。现在，让我们看看如何计算LOO：'
- en: '**Code 5.3**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.3**'
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of `az.loo` has two sections. In the first section, we get a table
    with two rows. The first row is the ELPD (`elpd_loo`) and the second one is the
    effective number of parameters (`p_loo`). In the second section, we have the Pareto
    k diagnostic. This is a measure of the reliability of the LOO approximation. Values
    of k greater than 0.7 indicate that we possibly have very influential observations.
    In this case, we have 33 observations and all of them are good, so we can trust
    the approximation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`az.loo`的输出分为两部分。在第一部分，我们得到一个包含两行的表格。第一行是ELPD（`elpd_loo`），第二行是有效参数数（`p_loo`）。在第二部分，我们有Pareto
    k诊断。这是LOO近似可靠性的一种度量。k值大于0.7表示我们可能有非常有影响力的观测值。在这种情况下，我们有33个观测值，且它们都是好的，因此我们可以信任这个近似。'
- en: 'To compute WAIC, you can use `az.waic`; the output will be similar, except
    that we will not get the Pareto k diagnostic, or any similar diagnostics. This
    is a downside of WAIC: we do not get any information about the reliability of
    the approximation.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算WAIC，你可以使用`az.waic`；输出将类似，只是我们不会得到Pareto k诊断或任何类似的诊断信息。这是WAIC的一个缺点：我们无法获得任何关于近似可靠性的信息。
- en: If we compute LOO for the quadratic model, we will get a similar output, but
    the ELPD will be higher (around -4), indicating that the quadratic model is better.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们为二次模型计算LOO，将会得到类似的输出，但ELPD值会更高（大约-4），这表明二次模型更好。
- en: 'Values of ELPD are not that useful by themselves and must be interpreted in
    relation to other ELPD values. That is why ArviZ provides two helper functions
    to facilitate this comparison. Let’s look at `az.compare` first:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ELPD的数值本身并不太有用，必须与其他ELPD值进行比较来解读。这就是为什么ArviZ提供了两个辅助函数来方便这种比较。我们先来看`az.compare`：
- en: '**Code 5.4**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.4**'
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  | **rank** | **elpd_loo** | **p_loo** | **elpd_diff** | **weight** | **se**
    | **dse** | **warning** | **scale** |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | **rank** | **elpd_loo** | **p_loo** | **elpd_diff** | **weight** | **se**
    | **dse** | **warning** | **scale** |'
- en: '| **model_q** | 0 | -4.6 | 2.68 | 0 | 1 | 2.36 | 0 | False | log |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **model_q** | 0 | -4.6 | 2.68 | 0 | 1 | 2.36 | 0 | False | log |'
- en: '| **model_l** | 1 | -14.3 | 2.42 | 9.74 | 3.0e-14 | 2.67 | 2.65 | False | log
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **model_l** | 1 | -14.3 | 2.42 | 9.74 | 3.0e-14 | 2.67 | 2.65 | False | log
    |'
- en: 'In the rows, we have the compared models, and in the columns, we have:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在行中，我们列出了比较的模型，在列中，我们有：
- en: '`rank`: The order of the models (from best to worst).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`: 模型的顺序（从最好到最差）。'
- en: '`elpd_loo`: The point estimate of the ELPD'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elpd_loo`: ELPD的点估计。'
- en: '`p_loo`: The effective numbers parameters.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p_loo`: 有效的参数数。'
- en: '`elpd_diff`: The difference between the ELPD of the best model and the other
    models.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elpd_diff`: 最佳模型与其他模型之间的ELPD差异。'
- en: '`weight`: The relative weight of each model. If we wanted to make predictions
    by combining the different models instead of choosing just one, this would be
    the weight that we should assign to each model. In this case, we see that the
    polynomial model takes all the weight.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`: 每个模型的相对权重。如果我们想通过结合不同的模型来进行预测，而不是仅选择一个模型，这将是我们应该赋予每个模型的权重。在这种情况下，我们看到多项式模型占用了所有的权重。'
- en: '`se`: The standard error of the ELPD.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`se`: ELPD的标准误差。'
- en: '`dse`: The standard error of the differences.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dse`: 差异的标准误差。'
- en: '`warning`: A warning about high k values.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warning`: 关于高k值的警告。'
- en: '`scale`: The scale on which the ELPD is calculated.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`: 计算ELPD时所用的尺度。'
- en: 'The other helper function provided by ArviZ is `az.compareplot`. This function
    provides similar information to `az.compare`, but graphically. *Figure [5.8](#x1-107011r8)*
    shows the output of this function. Notice that:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ArviZ提供的另一个辅助函数是`az.compareplot`。这个函数提供了与`az.compare`相似的信息，但以图形方式呈现。*图[5.8](#x1-107011r8)*展示了该函数的输出。请注意：
- en: The empty circles represent the ELPD values and the black lines are the standard
    error.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空心圆代表ELPD值，黑线是标准误差。
- en: The highest value of the ELPD is indicated with a vertical dashed gray line
    to facilitate comparison with other values.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最高的ELPD值通过一条垂直的虚线灰线标示，以便与其他值进行比较。
- en: For all models except *the best*, we also get a triangle indicating the value
    of the ELPD difference between each model and the *best* model. The gray error
    bar indicates the standard error of the differences between the point estimates.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有模型，除了*最优模型*，我们还会得到一个三角形，表示每个模型与*最优模型*之间ELPD差值的大小。灰色误差条表示点估计之间差异的标准误差。
- en: '![PIC](img/file156.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file156.png)'
- en: '**Figure 5.8**: Output of `az.compareplot(cmp_df)`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.8**：`az.compareplot(cmp_df)`的输出'
- en: The easiest way to use LOO (or WAIC) is to choose a single model. Just choose
    the model with the highest ELPD value. If we follow this rule, we will have to
    accept that the quadratic model is the best. Even if we take into account the
    standard errors, we can see that they do not overlap. This gives us some certainty
    that indeed the models are *different enough* from each other. If instead, the
    standard errors overlap, we should provide a more nuanced answer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LOO（或WAIC）最简单的方法是选择一个单一模型。只需选择ELPD值最高的模型。如果我们遵循这个规则，我们将不得不接受二次模型是最优的。即使考虑到标准误差，我们也可以看到它们并没有重叠。这给了我们一些确定性，表明这些模型确实*足够不同*。如果标准误差重叠，我们应该给出更细致的答案。
- en: 5.5 Model averaging
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 模型平均
- en: Model selection is attractive for its simplicity, but we might be missing information
    about uncertainty in our models. This is somewhat similar to calculating the full
    posterior and then just keeping the posterior mean; this can lead us to be overconfident
    about what we think we know.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择因其简便性而具有吸引力，但我们可能忽视了模型中关于不确定性的信息。这有点类似于计算完整的后验分布，然后仅保留后验均值；这可能导致我们对自己所知的内容过于自信。
- en: An alternative is to select a single model but to report and analyze the different
    models together with the values of the calculated information criteria, their
    standard errors, and perhaps also the posterior predictive checks. It is important
    to put all these numbers and tests in the context of our problem so that we and
    our audience can get a better idea of the possible limitations and shortcomings
    of the models. For those working in academia, these elements can be used to add
    elements to the discussion section of a paper, presentation, thesis, etc. In industry,
    this can be useful for informing stakeholders about the advantages and limitations
    of models, predictions, and conclusions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是选择一个单一模型，但报告并分析不同的模型，以及计算出的信息标准、它们的标准误差，或许还包括后验预测检查。将所有这些数字和测试放在我们的实际问题背景下非常重要，这样我们和我们的观众可以更好地了解模型可能的局限性和缺陷。对于学术界的人来说，这些元素可以用来在论文、报告、论文等的讨论部分中增加内容。在行业中，这对向利益相关者提供有关模型、预测和结论的优缺点非常有用。
- en: 'Another possibility is to average the models. In this way, we keep the uncertainty
    about the goodness of fit of each model. We then obtain a meta-model (and meta-predictions)
    using a weighted average of each model. ArviZ provides a function for this task,
    `az.weight_predictions`, which takes as arguments a list of InferenceData objects
    and a list of weights. The weights can be calculated using the `az.compare` function.
    For example, if we want to average the two models we have been using, we can do
    the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能性是对模型进行平均。通过这种方式，我们保留了每个模型拟合优度的不确定性。然后我们使用每个模型的加权平均值来获得一个元模型（以及元预测）。ArviZ
    提供了一个用于此任务的函数 `az.weight_predictions`，它接受推断数据对象列表和权重列表作为参数。权重可以使用 `az.compare`
    函数计算。例如，如果我们想要对我们一直在使用的两个模型进行平均，可以按照以下方式操作：
- en: '**Code 5.5**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.5**'
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure [5.9](#x1-108006r9)* shows the results of this calculation. The light
    gray dashed line is the weighted average of the two models, the black solid line
    is the linear model, and the gray solid line is the quadratic one.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [5.9](#x1-108006r9)* 显示了该计算的结果。浅灰色虚线是两个模型的加权平均，黑色实线是线性模型，灰色实线是二次模型。'
- en: '![PIC](img/file157.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file157.png)'
- en: '**Figure 5.9**: Weighted average of the linear and quadratic models'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.9**：线性和二次模型的加权平均'
- en: There are other ways to average models, such as explicitly building a meta-model
    that includes all models of interest as particular cases. For example, an order
    2 polynomial contains a linear model as a particular case, or a hierarchical model
    is the continuous version between two extremes, a grouped model and an ungrouped
    model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方式来对模型进行平均，例如显式地构建一个元模型，将所有感兴趣的模型作为特定情况包含在内。例如，二阶多项式包含了线性模型作为特定情况，或者分层模型是两种极端模型之间的连续版本，一个是分组模型，另一个是非分组模型。
- en: 5.6 Bayes factors
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 贝叶斯因子
- en: An alternative to LOO, cross-validation, and information criteria is Bayes factors.
    It is common for Bayes factors to show up in the literature as a Bayesian alternative
    to frequentist hypothesis testing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: LOO、交叉验证和信息准则的替代方法是贝叶斯因子。贝叶斯因子通常作为频率派假设检验的贝叶斯替代方案出现在文献中。
- en: The *Bayesian way* of comparing *k* models is to calculate the **marginal**
    **likelihood** of each model *p*(*y*|*M*[*k*]), i.e., the probability of the observed
    data *Y* given the model *M*[*k*]. The marginal likelihood is the normalization
    constant of Bayes’ theorem. We can see this if we write Bayes’ theorem and make
    explicit the fact that all inferences depend on the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 比较*k*个模型的*贝叶斯方法*是计算每个模型的**边际** **似然** *p*(*y*|*M*[*k*])，即给定模型*M*[*k*]时观察数据*Y*的概率。边际似然是贝叶斯定理的归一化常数。我们可以通过写出贝叶斯定理并明确表明所有推断都依赖于模型来看到这一点。
- en: '![p(θ | Y,Mk ) = p(Y-| θ,Mk-)p(θ-| Mk-) p(Y | Mk ) ](img/file158.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![p(θ | Y,Mk ) = p(Y-| θ,Mk-)p(θ-| Mk-) p(Y | Mk ) ](img/file158.jpg)'
- en: where, *y* is the data, *θ* is the parameters, and *M*[*k*] is a model out of
    *k* competing models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*y*是数据，*θ*是参数，*M*[*k*]是从*k*个竞争模型中选择的模型。
- en: 'If our main objective is to choose only one model, the *best* from a set of
    models, we can choose the one with the largest value of *p*(*y*|*M*[*k*]). This
    is fine if we assume that all models have the same prior probability. Otherwise,
    we must calculate:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的主要目标是从一组模型中选择一个模型，即选择*最佳*模型，那么我们可以选择具有最大*p*(*y*|*M*[*k*])值的那个模型。如果我们假设所有模型的先验概率相同，这样做是可以的。否则，我们必须计算：
- en: '![p(Mk | y) ∝ p(y | Mk )p(Mk ) ](img/file159.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![p(Mk | y) ∝ p(y | Mk )p(Mk ) ](img/file159.jpg)'
- en: 'If, instead, our main objective is to compare models to determine which are
    more likely and to what extent, this can be achieved using the Bayes factors:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的主要目标是比较模型，以确定哪些模型更可能且可能的程度，那么可以使用贝叶斯因子来实现：
- en: '![BF01 = p(y | M0-) p(y | M1 ) ](img/file160.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![BF01 = p(y | M0-) p(y | M1 ) ](img/file160.jpg)'
- en: That is the ratio between the marginal likelihood of two models. The higher
    the value of *BF*[01], the *better* the model in the numerator (*M*[0] in this
    example). To facilitate the interpretation of the Bayes factors, and to put numbers
    into words, Harold Jeffreys proposed a scale for their interpretation, with levels
    of *support* or *strength* (see *Table [5.1](#x1-109004r1)*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两个模型的边际似然比。*BF*[01]的值越高，分子中的模型（本例中的*M*[0]）就越*好*。为了方便解读贝叶斯因子，并将数字转化为文字，Harold
    Jeffreys 提出了一个解读贝叶斯因子的尺度，其中包括*支持*或*强度*的不同级别（见*表 [5.1](#x1-109004r1)*）。
- en: '| **Bayes Factor** | **Support** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **贝叶斯因子** | **支持度** |'
- en: '| 1–3 | Anecdotal |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 1–3 | 轶事 |'
- en: '| 3–10 | Moderate |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 3–10 | 中等 |'
- en: '| 10–30 | Strong |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 10–30 | 强烈 |'
- en: '| 30–100 | Very Strong |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 30–100 | 非常强烈 |'
- en: '| *>*100 | Extreme |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| *>*100 | 极端 |'
- en: '**Table 5.1**: Support for model *M*[0], the one in the numerator'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5.1**：分子中的模型*M*[0]的支持度'
- en: Keep in mind that if you get numbers below 1, then the support is for *M*[1],
    i.e., the model in the denominator. Tables are also available for those cases,
    but notice that you can simply take the inverse of the obtained value.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果你得到的数值低于1，那么支持的是*M*[1]，即分母中的模型。对于这些情况，也有表格可供参考，但请注意，你可以简单地取获得值的倒数。
- en: It is very important to remember that these rules are just conventions – simple
    guides at best. Results should always be put in the context of our problems and
    should be accompanied by enough detail so that others can assess for themselves
    whether they agree with our conclusions. The proof necessary to ensure something
    in particle physics, or in court, or to decide to carry out an evacuation in the
    face of a looming natural catastrophe is not the same.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，这些规则只是约定——充其量只是简单的指南。结果应始终放在我们的具体问题背景下，并附上足够的细节，以便他人可以自行评估他们是否同意我们的结论。在粒子物理学中确保某些东西的证明，或者在法庭上，或者在面对即将发生的自然灾难时决定是否进行撤离的证明是不一样的。
- en: 5.6.1 Some observations
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 一些观察结果
- en: 'We will now briefly discuss some key facts about the marginal likelihood:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简要讨论一些关于边际似然的关键事实：
- en: 'The good: Occam’s razor included. Models with lots of parameters have a higher
    penalty than models with few parameters. The intuitive reason is that the greater
    the number of parameters, the more the prior *extends* with respect to the likelihood.
    An example where it is easy to see this is with nested models: for example, a
    polynomial of order 2 ”contains” the models polynomial of order 1 and polynomial
    of order 0.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点：包括奥卡姆剃刀。参数较多的模型比参数较少的模型有更大的惩罚。直观的理由是，参数越多，先验相对于似然的*扩展*就越大。一个容易看出这种情况的例子是嵌套模型：例如，二次多项式“包含”了一次多项式和零次多项式模型。
- en: 'The bad: For many problems, the marginal likelihood cannot be calculated analytically.
    Also, approximating it numerically is usually a difficult task that in the best
    of cases requires specialized methods and, in the worst case, the estimates are
    either impractical or unreliable. In fact, the popularity of the MCMC methods
    is that they allow obtaining the posterior distribution without the need to calculate
    the marginal likelihood.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏处：对于许多问题，边际似然无法进行解析计算。此外，数值近似通常是一项艰巨的任务，在最好的情况下需要专门的方法，而在最坏的情况下，估算结果要么不切实际，要么不可靠。事实上，MCMC方法的流行在于它们可以在不计算边际似然的情况下获得后验分布。
- en: 'The ugly: The marginal likelihood depends *very sensitively* on the prior distribution
    of the parameters in each model *p*(*θ*[*k*]|*M*[*k*]).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺点：边际似然对每个模型中的参数先验分布*非常敏感*（*p*(*θ*[*k*]|*M*[*k*])）。
- en: It is important to note that the *good* and the *ugly* points are related. Using
    marginal likelihood to compare models is a good idea because it already includes
    a penalty for complex models (which helps us prevent overfitting), and at the
    same time, a change in the prior will affect the marginal likelihood calculations.
    At first, this sounds a bit silly; we already know that priors affect calculations
    (otherwise we could just avoid them). But we are talking about changes in the
    prior that would have a small effect in the posterior but a great impact on the
    value of the marginal likelihood.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，*优点*和*缺点*是相关的。使用边际似然来比较模型是一个好主意，因为它已经对复杂模型进行了惩罚（这有助于我们防止过拟合），同时，先验的变化将影响边际似然的计算。起初，这听起来有些傻；我们已经知道先验会影响计算（否则我们可以直接避免使用它们）。但我们这里谈论的是先验的变化，这些变化在后验中可能只有小的影响，但对边际似然值有很大影响。
- en: 'The use of Bayes factors is often a watershed among Bayesians. The difficulty
    of its calculation and the sensitivity to the priors are some of the arguments
    against it. Another reason is that, like p-values and hypothesis testing in general,
    Bayes factors favor dichotomous thinking over the estimation of the ”effect size.”
    In other words, instead of asking ourselves questions like: How many more years
    of life can a cancer treatment provide? We end up asking if the difference between
    treating and not treating a patient is ”statistically significant.” Note that
    this last question can be useful in some contexts. The point is that in many other
    contexts, this type of question is not the question that interests us; we’re only
    interested in the one that we were taught to answer.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯因子的使用常常是贝叶斯学派的分水岭。其计算的难度以及对先验的敏感性是反对其使用的一些论点。另一个原因是，像p值和假设检验一样，贝叶斯因子更倾向于二分法思维，而不是“效应大小”的估计。换句话说，我们不是在问自己像这样的问题：癌症治疗可以延长多少年的生命？而是最终问，治疗与不治疗之间的差异是否是“统计显著的”。请注意，这个最后的问题在某些背景下是有用的。关键是，在许多其他情况下，这种问题并不是我们感兴趣的问题；我们只对我们被教导去回答的问题感兴趣。
- en: 5.6.2 Calculation of Bayes factors
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 贝叶斯因子的计算
- en: As we have already mentioned, marginal likelihood (and the Bayes factors derived
    from it) is generally not available in closed form, except for some models. For
    this reason, many numerical methods have been devised for its calculation. Some
    of these methods are so simple and naive ( [https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever))
    that they work very poorly in practice.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，边际似然（以及由此得出的贝叶斯因子）通常无法以闭式形式提供，除非是某些特定模型。因此，已经设计了许多数值方法来计算它。其中一些方法非常简单且天真（[https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever)），以至于在实践中效果非常差。
- en: Analytically
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析上
- en: 'For some models, such as the BetaBinomial model, we can calculate the marginal
    likelihood analytically. If we write this model as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些模型，如 BetaBinomial 模型，我们可以解析计算边际似然。如果我们将该模型表示为：
- en: '|  | *θ* ∼ *Beta*(**α*,*β**) |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | *θ* ∼ *Beta*(**α*,*β**) |  |'
- en: '|  | *y* ∼ *Bin*(*n* = 1*,p* = *θ*) |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | *y* ∼ *Bin*(*n* = 1*,p* = *θ*) |  |'
- en: 'then the marginal likelihood will be:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，边际似然将是：
- en: '![ (n) B (𝛼+ h, 𝛽 + n − h) p(y) = ------------------- h B (𝛼,𝛽) ](img/file161.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![ (n) B (𝛼+ h, 𝛽 + n − h) p(y) = ------------------- h B (𝛼,𝛽) ](img/file161.jpg)'
- en: '*B* is the beta function (not to be confused with the *Beta* distribution),
    *n* is the number of attempts, and *h* is the success number.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*B* 是贝塔函数（不要与 *Beta* 分布混淆），*n* 是实验次数，*h* 是成功次数。'
- en: 'Since we only care about the relative value of the marginal likelihood under
    two different models (for the same data), we can omit the binomial coefficient
    ![(n) h](img/file162.jpg), so we can write:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只关心在两种不同模型下的边际似然相对值（对于相同的数据），我们可以省略二项系数 ![(n) h](img/file162.jpg)，因此我们可以写成：
- en: '![ B (𝛼 + h, 𝛽 + n − h) p(y) ∝ ------B-(𝛼,-𝛽)------ ](img/file163.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![ B (𝛼 + h, 𝛽 + n − h) p(y) ∝ ------B-(𝛼,-𝛽)------ ](img/file163.jpg)'
- en: This expression has been coded in the next code block but with a twist. We will
    use the `betaln` function, which returns the natural logarithm of the `beta` function,
    it is common in statistics to do calculations on a
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式已在下一个代码块中编写，但有些变化。我们将使用 `betaln` 函数，它返回 `beta` 函数的自然对数，通常在统计学中，计算使用对数尺度。这可以减少在处理概率时的数值问题。
- en: logarithmic scale. This reduces numerical problems when working with probabilities.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对数尺度。这减少了处理概率时的数值问题。
- en: '**Code 5.6**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.6**'
- en: '[PRE6]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our data for this example consists of 100 coin tosses and the same number of
    heads and tails. We will compare two models, one with a Uniform prior and one
    with a *more concentrated* prior around *θ* = 0*.*5:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的数据由 100 次抛硬币实验组成，正反面数量相同。我们将比较两种模型，一种具有均匀先验，另一种具有围绕 *θ* = 0*.*5 的 *更集中*
    先验：
- en: '**Code 5.7**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.7**'
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Figure [5.10](#x1-112026r10)* shows the two priors. The Uniform prior is the
    black line, and the peaked prior is the gray line.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [5.10](#x1-112026r10)* 显示了两个先验分布。均匀先验是黑线，尖峰先验是灰线。'
- en: '![PIC](img/file164.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file164.png)'
- en: '**Figure 5.10**: Uniform and peaked priors'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.10**：均匀先验和尖峰先验'
- en: 'Now, we can calculate the marginal likelihood for each model and the Bayes
    factor, which turns out to be 5:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以为每个模型计算边际似然和贝叶斯因子，结果为 5：
- en: '**Code 5.8**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.8**'
- en: '[PRE8]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We see that the model with the prior beta(30*,*30), more concentrated, has ≈
    5 times more support than the model with the beta(1*,*1). This is to be expected
    since the prior for the first case is concentrated around *θ* = 0*.*5 and the
    data *Y* have the same number of heads and tails, that is, they agree with a value
    of *θ* around 0.5.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，具有先验 beta(30*,*30) 的模型，其浓度更高，支持度大约是 beta(1*,*1) 模型的 5 倍。这是可以预期的，因为第一个模型的先验集中在
    *θ* = 0*.*5 附近，而数据 *Y* 中正反面数量相同，即它们与 *θ* 约为 0.5 的值一致。
- en: Sequential Monte Carlo
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列蒙特卡洛
- en: The **Sequential Monte Carlo** (**SMC**) method is a sampling method that works
    by progressing through a series of successive stages that bridge one distribution
    that is easy to sample from and the posterior of interest. In practice, the starting
    distribution is usually the prior. A byproduct of the SMC sampler is the estimate
    of the marginal likelihood.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列蒙特卡洛**（**SMC**）方法是一种通过一系列连续阶段的采样方法，逐步连接一个易于采样的分布和感兴趣的后验分布。在实践中，起始分布通常是先验分布。SMC采样器的副产品是边际似然的估计。'
- en: '**Code 5.9**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.9**'
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we can see from the preceding code block, SMC also gives us a Bayes factor
    of 5, the same answer as the analytical calculation! The advantage of using SMC
    to calculate marginal likelihood is that we can use it for a wider range of models
    since we no longer need to know an expression in closed form. The price we pay
    for this flexibility is a higher computational cost. Also, keep in mind that SMC
    (with an independent Metropolis-Hastings kernel, as implemented in PyMC) is not
    as efficient as NUTS. As the dimensionality of the problem increases, a more precise
    estimate of the posterior and the marginal likelihood will require a larger number
    of samples of the posterior.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码块中可以看出，SMC 也给出了贝叶斯因子 5，结果与解析计算一致！使用 SMC 计算边际似然的优点是我们可以将其应用于更广泛的模型，因为我们不再需要知道一个封闭形式的表达式。我们为这种灵活性付出的代价是更高的计算成本。此外，值得注意的是，SMC（使用独立的
    Metropolis-Hastings 核心，如 PyMC 中实现的）并不像 NUTS 那样高效。随着问题的维度增加，更精确的后验估计和边际似然需要更多的后验样本。
- en: Log Space
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对数空间
- en: In computational statistics, we usually perform computations in log space. This
    helps provide numerical stability and computational efficiency, among other things.
    See, for example, the preceding code block; you can see that we calculated a difference
    (instead of a division) and then we took the exponential before returning the
    result.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算统计学中，我们通常在对数空间中进行计算。这有助于提供数值稳定性和计算效率等。举个例子，参考前面的代码块；你可以看到我们计算了差异（而不是除法），然后在返回结果之前取了指数。
- en: Savage–Dickey ratio
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Savage–Dickey比率
- en: 'For the above examples, we have compared two BetaBinomial models. We could
    have compared two completely different models, but there are times when we want
    to compare a null hypothesis `H_0` (or null model) against an alternative *H_1*
    hypothesis. For example, to answer the question ”Is this coin biased?”, we could
    compare the value *θ* = 0*.*5 (representing no bias) with the output of a model
    in which we allow *θ* to vary. For this type of comparison, the null model is
    nested within the alternative, which means that the null is a particular value
    of the model we are building. In those cases, calculating the Bayes factor is
    very easy and does not require any special methods. We only need to compare the
    prior and posterior evaluated at the null value (for example, *θ* = 0*.*5) under
    the alternative model. We can see that this is true from the following expression:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述例子，我们比较了两个BetaBinomial模型。我们本可以比较两个完全不同的模型，但有时我们希望比较一个零假设`H_0`（或零模型）与一个备择假设*H_1*。例如，为了回答“这个硬币是否有偏？”的问题，我们可以将值*θ*
    = 0*.*5（表示没有偏差）与允许*θ*变化的模型的输出进行比较。对于这种比较，零模型嵌套在备择模型中，这意味着零假设是我们正在构建的模型中的一个特定值。在这些情况下，计算贝叶斯因子非常简单，不需要任何特殊的方法。我们只需要比较在备择模型下评估零值（例如*θ*
    = 0*.*5）时的先验和后验。我们可以从以下表达式中看到这一点：
- en: '![ p(y | H )p(θ = 0.5 | y, H ) BF01 = ------0---------------1- p(y | H1 ) p(θ
    = 0.5 | H1 ) ](img/file165.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![ p(y | H )p(θ = 0.5 | y, H ) BF01 = ------0---------------1- p(y | H1 ) p(θ
    = 0.5 | H1 ) ](img/file165.jpg)'
- en: 'This is true only when *H*[0] is a particular case of *H*[1], ([https://statproofbook.github.io/P/bf-sddr](https://statproofbook.github.io/P/bf-sddr)).
    Next, let’s do it with PyMC and ArviZ. We only need to sample the prior and posterior
    for a model. Let’s try the BetaBinomial model with a Uniform prior:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 仅当*H*[0]是*H*[1]的特定情况时，这才成立（[https://statproofbook.github.io/P/bf-sddr](https://statproofbook.github.io/P/bf-sddr)）。接下来，让我们用PyMC和ArviZ来实现。我们只需要为一个模型进行先验和后验的采样。让我们尝试使用Uniform先验的BetaBinomial模型：
- en: '**Code 5.10**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.10**'
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The result is shown in *Figure [5.11](#x1-114012r11)*. We can see one KDE for
    the prior (black) and one for the posterior (gray). The two black dots show that
    we evaluated both distributions at the value 0.5\. We can see that the Bayes factor
    in favor of the null hypothesis, `BF_01`, is ≈ 8, which we can interpret as *moderate
    evidence* in favor of the null hypothesis (see *Table [5.1](#x1-109004r1)*).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在*图 [5.11](#x1-114012r11)*中。我们可以看到一个先验的KDE（黑色）和一个后验的KDE（灰色）。两个黑点显示我们在值0.5处评估了这两个分布。我们可以看到支持零假设的贝叶斯因子`BF_01`约为8，我们可以将其解释为*适度证据*支持零假设（见*表
    [5.1](#x1-109004r1)*）。
- en: '![PIC](img/file166.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file166.png)'
- en: '**Figure 5.11**: Bayes factor for the BetaBinomial model with Uniform prior'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.11**：具有Uniform先验的BetaBinomial模型的贝叶斯因子'
- en: As we have already discussed, the Bayes factors measure which model, as a whole,
    is better at explaining the data. This includes the prior, even if the prior has
    a relatively low impact on the computation of the posterior. We can also see this
    prior effect by comparing a second model to the null model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的，贝叶斯因子衡量的是哪个模型更好地解释了数据。这包括先验，即使先验对后验计算的影响相对较小。我们也可以通过将第二个模型与零模型进行比较，看到这种先验效应。
- en: 'If, instead, our model were a BetaBinomial with a Beta prior (30, 30), the
    `BF_01` would be lower (*anecdotal* on the Jeffrey scale). This is because, according
    to this model, the value of *θ* = 0*.*5 is much more likely a priori than for
    a Uniform prior, and therefore the prior and posterior will be much more similar.
    That is, it is not very *surprising* to see that the posterior is concentrated
    around 0.5 after collecting data. Don’t just believe me, let’s calculate it:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型是一个带有Beta先验（30, 30）的BetaBinomial模型，那么`BF_01`会更低（在Jeffrey尺度上为*轶事证据*）。这是因为根据这个模型，*θ*
    = 0*.*5的值在先验中比Uniform先验更有可能，因此先验和后验将更加相似。也就是说，在收集数据后，看到后验集中在0.5附近并不令人*惊讶*。不要只相信我，我们来计算一下：
- en: '**Code 5.11**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 5.11**'
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Figure [5.12](#x1-114022r12)* shows the result. We can see that the `BF_01`
    is ≈ 1*.*6, which we can interpret as *anecdotal evidence* in favor of the null
    hypothesis (see the Jeffreys’ scale, discussed earlier).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [5.12](#x1-114022r12)* 显示了结果。我们可以看到`BF_01`大约是1*.*6，这可以解释为支持零假设的*轶事证据*（参见之前讨论的Jeffreys尺度）。'
- en: '![PIC](img/file167.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file167.png)'
- en: '**Figure 5.12**: Bayes factor for the BetaBinomial model with peaked prior'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.12**：具有尖峰先验的BetaBinomial模型的贝叶斯因子'
- en: 5.7 Bayes factors and inference
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 贝叶斯因子与推断
- en: So far, we have used Bayes factors to judge which model seems to be better at
    explaining the data, and we found that one of the models is ≈ 5 times *better*
    than the other.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用贝叶斯因子来判断哪个模型在解释数据方面似乎更好，我们发现其中一个模型的表现大约是另一个的5倍*更好*。
- en: 'But what about the posterior we get from these models? How different are they?
    *Table [5.2](#x1-115002r2)* summarizes these two posteriors:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，来自这些模型的后验怎么样呢？它们有多不同？*表 [5.2](#x1-115002r2)* 总结了这两种后验：
- en: '|  | **mean** | **sd** | **hdi_3%** | **hdi_97%** |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | **均值** | **标准差** | **hdi_3%** | **hdi_97%** |'
- en: '| **uniform** | 0.5 | 0.05 | 0.4 | 0.59 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| **均匀** | 0.5 | 0.05 | 0.4 | 0.59 |'
- en: '| **peaked** | 0.5 | 0.04 | 0.42 | 0.57 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| **尖峰** | 0.5 | 0.04 | 0.42 | 0.57 |'
- en: '**Table 5.2**: Statistics for the models with uniform and peaked priors computed
    using the ArviZ summary function'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 5.2**：使用ArviZ汇总函数计算的具有均匀和尖峰先验的模型统计数据'
- en: We can argue that the results are quite similar; we have the same mean value
    for *θ* and a slightly wider posterior for `model_0`, as expected since this model
    has a wider prior. We can also check the posterior predictive distribution to
    see how similar they are (see *Figure [5.13](#x1-115004r13)*).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说结果非常相似；我们得到了相同的*θ*均值，而`model_0`的后验略宽，这是预期的，因为该模型具有较宽的先验。我们还可以检查后验预测分布，看看它们有多相似（见*图
    [5.13](#x1-115004r13)*）。
- en: '![PIC](img/file168.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file168.png)'
- en: '**Figure 5.13**: Posterior predictive distributions for models with uniform
    and peaked priors'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.13**：具有均匀和尖峰先验的模型的后验预测分布'
- en: In this example, the observed data is more consistent with `model_1`, because
    the prior is concentrated around the correct value of *θ*, while `model_0`, assigns
    the same probability to all possible values of *θ*. This difference between the
    models is captured by the Bayes factor. We could say that the Bayes factors measure
    which model, as a whole, is better for explaining the data. This includes the
    details of the prior, no matter how similar the model predictions are. In many
    scenarios, this is not what interests us when comparing models, and instead, we
    prefer to evaluate models in terms of how similar their predictions are. For those
    cases, we can use LOO.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，观察到的数据更符合`model_1`，因为其先验集中在正确的*θ*值附近，而`model_0`则对所有可能的*θ*值赋予相同的概率。这种模型之间的差异由贝叶斯因子捕捉到。我们可以说，贝叶斯因子衡量的是哪个模型作为整体更适合解释数据。这包括了先验的细节，无论模型预测多么相似。在许多情况下，比较模型时，我们并不关心这些细节，反而更倾向于评估模型预测的相似度。对于这些情况，我们可以使用LOO。
- en: 5.8 Regularizing priors
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 正则化先验
- en: Using informative and weakly informative priors is a way of introducing bias
    in a model and, if done properly, this can be really good because bias prevents
    overfitting and thus contributes to models being able to make predictions that
    generalize well. This idea of adding a bias element to reduce generalization errors
    without affecting the ability of the model to adequately model a problem is known
    as **regularization**. This regularization often takes the form of a term penalizing
    certain values for the parameters in a model, like too-big coefficients in a regression
    model. Restricting parameter values is a way of reducing the data a model can
    represent, thus reducing the chances that a model will capture noise instead of
    the signal.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用信息丰富和弱信息先验是一种在模型中引入偏差的方法，如果做得恰当，这实际上是非常有益的，因为偏差可以防止过拟合，从而帮助模型进行具有良好泛化能力的预测。这种向模型中加入偏差元素以减少泛化误差而不影响模型充分建模问题能力的想法被称为**正则化**。这种正则化通常表现为对模型中某些参数值进行惩罚的项，例如在回归模型中对过大的系数进行惩罚。限制参数值是减少模型能够表示的数据量的一种方式，从而减少模型捕捉噪声而非信号的可能性。
- en: This regularization idea is so powerful and useful that it has been discovered
    several times, including outside the Bayesian framework. For regression models,
    and outside Bayesian statistics, two popular regularization methods are ridge
    regression and lasso regression. From the Bayesian point of view, ridge regression
    can be interpreted as using Normal distributions for the *β* coefficients of a
    linear model, with a small standard deviation that pushes the coefficients toward
    zero. In this sense, we have been doing something very close to ridge regression
    for every single linear model in this book (except the examples in this chapter
    that use SciPy!).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个正则化思想非常强大且有用，以至于它在贝叶斯框架之外也被多次发现。对于回归模型，且不局限于贝叶斯统计，两个流行的正则化方法是岭回归和lasso回归。从贝叶斯角度来看，岭回归可以解释为对线性模型的*β*系数使用正态分布作为先验，其中的标准差较小，使得系数趋向于零。从这个意义上讲，我们在本书中的每个线性模型（除了本章中使用SciPy的例子）实际上都在做类似岭回归的事情！
- en: On the other hand, lasso regression can be interpreted from a Bayesian point
    of view as the MAP of the posterior computed from a model with Laplace priors
    for the *β* coefficients. The Laplace distribution looks similar to the Gaussian
    distribution but with a sharp peak at zero. You can also interpret it as two *back-to-back*
    Exponential distributions (try `pz.Laplace(0, 1).plot_pdf()`). The Laplace distribution
    concentrates its probability mass much closer to zero compared to the Gaussian
    distribution. The idea of using such a prior is to provide both regularization
    and variable selection. The idea is that since we have this peak at zero, we expect
    the prior distribution to induce sparsity, that is, we create a model with a lot
    of parameters and the prior will automatically make most of them zero, keeping
    only the relevant variables contributing to the output of the model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，lasso回归可以从贝叶斯角度解释为通过从具有拉普拉斯先验的模型中计算的后验的最大后验估计（MAP），其中*β*系数使用拉普拉斯分布作为先验。拉普拉斯分布看起来类似于高斯分布，但在零点处有一个尖锐的峰值。你也可以将其解释为两个*背对背*的指数分布（试试`pz.Laplace(0,
    1).plot_pdf()`）。与高斯分布相比，拉普拉斯分布的概率质量更集中在零附近。使用这种先验的思想是提供正则化和变量选择。具体来说，由于我们在零点处有一个峰值，我们期望先验分布能够引入稀疏性，也就是说，我们创建一个具有大量参数的模型，而先验会自动使大多数参数为零，仅保留对模型输出有贡献的相关变量。
- en: 'Unfortunately, contrary to ridge regression, this idea does not directly translate
    from the frequentist realm to the Bayesian one. Nevertheless, there are Bayesian
    priors that can be used for inducing sparsity and performing variable selection,
    like the horseshoe prior. If you want to learn more about the horseshoe and other
    shrinkage priors, you may find the article by [Piironen and Vehtari](Bibliography.xhtml#XPiironen2017) [[2017](Bibliography.xhtml#XPiironen2017)]
    at [https://arxiv.org/abs/1707.01694](https://arxiv.org/abs/1707.01694) very interesting.
    In the next chapter, we will discuss more about variable selection. Just one final
    note: it is important to notice that the classical versions of ridge and lasso
    regressions correspond to single-point estimates, while the Bayesian versions
    yield full posterior distributions.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，与岭回归不同，这一思想并不能直接从频率学派转化到贝叶斯学派。然而，确实存在一些贝叶斯先验可以用来引入稀疏性并执行变量选择，比如马鞍先验。如果你想了解更多关于马鞍先验和其他收缩型先验的信息，你可以参考[Piironen和Vehtari](Bibliography.xhtml#XPiironen2017)的文章[[2017](Bibliography.xhtml#XPiironen2017)]，它可以在[https://arxiv.org/abs/1707.01694](https://arxiv.org/abs/1707.01694)找到。在下一章中，我们将进一步讨论变量选择。最后补充一句：值得注意的是，岭回归和lasso回归的经典版本对应于单点估计，而贝叶斯版本则产生完整的后验分布。
- en: 5.9 Summary
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 总结
- en: In this chapter, we have seen how to compare models using posterior predictive
    checks, information criteria, approximated cross-validation, and Bayes factors.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到如何使用后验预测检查、信息准则、近似交叉验证和贝叶斯因子来比较模型。
- en: Posterior predictive check is a general concept and practice that can help us
    understand how well models are capturing different aspects of the data. We can
    perform posterior predictive checks with just one model or with many models, and
    thus we can use it as a method for model comparison. Posterior predictive checks
    are generally done via visualizations, but numerical summaries like Bayesian values
    can also be helpful.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 后验预测检查是一个通用概念和实践，它帮助我们理解模型在捕捉数据的不同方面方面的表现。我们可以仅用一个模型或多个模型进行后验预测检查，因此我们可以将其用作模型比较的方法。后验预测检查通常通过可视化方式进行，但像贝叶斯值这样的数值摘要也能提供帮助。
- en: 'Good models have a good balance between complexity and predictive accuracy.
    We exemplified this feature by using the classical example of polynomial regression.
    We discussed two methods to estimate the out-of-sample accuracy without leaving
    data aside: cross-validation and information criteria. From a practical point
    of view, information criteria is a family of theoretical methods looking to balance
    two contributions: a measurement of how well a model fits the data and a penalization
    term for complex models. We briefly discussed AIC, for its historical importance,
    and then WAIC, which is a better method for Bayesian models as it takes into account
    the entire posterior distribution and uses a more sophisticated method to compute
    the effective number of parameters.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 好的模型在复杂度和预测准确性之间有一个良好的平衡。我们通过使用经典的多项式回归例子来展示这一特点。我们讨论了两种方法来估计外样本准确性，而不需要将数据排除在外：交叉验证和信息准则。从实际角度来看，信息准则是一类理论方法，旨在平衡两个方面的贡献：衡量模型如何拟合数据的度量和对复杂模型的惩罚项。我们简要讨论了AIC，因其历史重要性，然后讨论了WAIC，它是贝叶斯模型的更好方法，因为它考虑了整个后验分布，并使用更复杂的方法来计算有效参数数目。
- en: We also discussed cross-validation, and we saw we can approximate leave-one-out
    cross-validation using LOO. Both WAIC and LOO tend to produce very similar results,
    but LOO can be more reliable. So we recommend its use. Both WAIC and LOO can be
    used for model selection and model averaging. Instead of selecting a single best
    model, model averaging is about combining all available models by taking a weighted
    average of them.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了交叉验证，看到我们可以使用LOO来近似留一法交叉验证。WAIC和LOO通常会产生非常相似的结果，但LOO可能更为可靠。所以我们推荐使用它。WAIC和LOO都可以用于模型选择和模型平均。与其选择一个最佳模型，模型平均是通过加权平均所有可用的模型来实现的。
- en: 'A different approach to model selection, comparison, and model averaging is
    Bayes factors, which are the ratio of the marginal likelihoods of two models.
    Bayes factor computations can be really challenging. In this chapter, we showed
    two routes to compute them with PyMC and ArviZ: using the sampling method known
    as Sequential Monte Carlo and using the Savage–Dickey ratio. The first method
    can be used for any model as long as Sequential Monte Carlo provides a good posterior.
    With the current implementation of SMC in PyMC, this can be challenging for high-dimensional
    models or hierarchical models. The second method can only be used when the null
    model is a particular case of the alternative model. Besides being computationally
    challenging, Bayes factors are problematic to use given that they are very (overly)
    sensitive to prior specifications.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择、比较和模型平均的另一种方法是贝叶斯因子，它是两个模型的边际似然比。贝叶斯因子的计算可能非常具有挑战性。在本章中，我们展示了使用PyMC和ArviZ计算贝叶斯因子的两种方法：一种是使用被称为顺序蒙特卡洛的方法，另一种是使用萨维奇–迪基比率。第一种方法可以用于任何模型，只要顺序蒙特卡洛提供了良好的后验分布。由于PyMC中SMC的当前实现，对于高维模型或层次模型来说，这可能会具有挑战性。第二种方法仅在原假设模型是备择模型的特定情况时才可使用。除了计算上具有挑战性外，贝叶斯因子的使用也存在问题，因为它们对先验设定非常（过度）敏感。
- en: We have shown that Bayes factors and LOO/WAIC are the answers to two related
    but different questions. The former is focused on identifying the right model
    and the other is on identifying the model with lower generalization loss, i.e.,
    the model making the best predictions. None of these methods are free of problems,
    but WAIC, and in particular LOO, are much more robust than the others in practice.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了贝叶斯因子和LOO/WAIC是回答两个相关但不同问题的工具。前者侧重于识别正确的模型，而后者则侧重于识别具有较低泛化损失的模型，即做出最佳预测的模型。这些方法都不是没有问题的，但WAIC，特别是LOO，在实践中比其他方法更为稳健。
- en: 5.10 Exercises
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.10 练习
- en: This exercise is about regularization priors. In the code that generates the
    `x_c, y_c` data (see [https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)),
    change `order=2` to another value, such as `order=5`. Then, fit `model_q` and
    plot the resulting curve. Repeat this, but now using a prior for *β* with `sd=100`
    instead of `sd=1` and plot the resulting curve. How do the curves differ? Try
    this out with `sd=np.array([10, 0.1, 0.1, 0.1, 0.1])`, too.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本练习涉及正则化先验。在生成`x_c, y_c`数据的代码中（见[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)），将`order=2`更改为其他值，例如`order=5`。然后，拟合`model_q`并绘制结果曲线。重复此步骤，但这次使用`sd=100`的*β*先验，而不是`sd=1`，并绘制结果曲线。这些曲线有何不同？也试试使用`sd=np.array([10,
    0.1, 0.1, 0.1, 0.1])`。
- en: Repeat the previous exercise but increase the amount of data to 500 data points.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复之前的练习，但将数据量增加到500个数据点。
- en: Fit a cubic model (order 3), compute WAIC and LOO, plot the results, and compare
    them with the linear and quadratic models.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合一个三次模型（阶数3），计算 WAIC 和 LOO，绘制结果，并与线性和二次模型进行比较。
- en: Use `pm.sample_posterior_predictive()` to rerun the PPC example, but this time,
    plot the values of `y` instead of the values of the mean.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pm.sample_posterior_predictive()` 重新运行 PPC 示例，但这次绘制 `y` 的值，而不是均值的值。
- en: Read and run the posterior predictive example from PyMC’s documentation at [https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html).
    Pay special attention to the use of shared variables and `pm.MutableData`.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读并运行 PyMC 文档中的后验预测示例，链接：[https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html)。特别注意共享变量和
    `pm.MutableData` 的使用。
- en: Go back to the code that generated *Figure [5.5](#x1-98002r5)* and *Figure [5.6](#x1-98004r6)*
    and modify it to get new sets of six data points. Visually evaluate how the different
    polynomials fit these new datasets. Relate the results to the discussions in this
    book.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回生成*图 [5.5](#x1-98002r5)* 和 *图 [5.6](#x1-98004r6)* 的代码，并修改它以获取新的六个数据点集。直观评估不同的多项式如何拟合这些新数据集。将结果与本书中的讨论联系起来。
- en: Read and run the model averaging example from PyMC’s documentation at [https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html).
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读并运行 PyMC 文档中的模型平均示例，链接：[https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html)。
- en: Compute the Bayes factor for the coin problem using a uniform prior, Beta(1,
    1), and priors such as Beta(0.5, 0.5). Set 15 heads and 30 coins. Compare this
    result with the inference we got in the first chapter of this book.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均匀先验 Beta(1, 1) 和如 Beta(0.5, 0.5) 等先验计算硬币问题的贝叶斯因子。设定15次正面和30枚硬币。将此结果与本书第一章的推断结果进行比较。
- en: Repeat the last example where we compare Bayes factors and Information Criteria,
    but now reduce the sample size.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上一个示例，我们比较贝叶斯因子和信息准则，但这次减少样本量。
- en: Join our community Discord space
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的社区 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过5000名成员一起学习，链接：[https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
