- en: ChapterÂ 5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äº”ç« 
- en: Comparing Models
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒæ¨¡å‹
- en: A map is not the territory it represents, but, if correct, it has a similar
    structure to the territory. â€“ Alfred Korzybski
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ°å›¾ä¸æ˜¯å®ƒæ‰€ä»£è¡¨çš„é¢†åœŸï¼Œä½†å¦‚æœå®ƒæ˜¯æ­£ç¡®çš„ï¼Œå®ƒå…·æœ‰ä¸é¢†åœŸç›¸ä¼¼çš„ç»“æ„ã€‚ â€“ é˜¿å°”å¼—é›·å¾·Â·ç§‘å°”é½å¸ƒæ–¯åŸº
- en: Models should be designed as approximations to help us understand a particular
    problem or a class of related problems. Models are not designed to be verbatim
    copies of the *real world*. Thus, all models are wrong in the same sense that
    maps are not the territory. But not all models are equally wrong; some models
    will be better than others at describing a given problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹åº”å½“ä½œä¸ºè¿‘ä¼¼å€¼æ¥è®¾è®¡ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£ä¸€ä¸ªç‰¹å®šé—®é¢˜æˆ–ä¸€ç±»ç›¸å…³é—®é¢˜ã€‚æ¨¡å‹å¹¶ä¸æ˜¯ä¸ºäº†å®Œç¾å¤åˆ¶*çœŸå®ä¸–ç•Œ*è€Œè®¾è®¡çš„ã€‚å› æ­¤ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯é”™è¯¯çš„ï¼Œå°±åƒåœ°å›¾ä¸æ˜¯é¢†åœŸä¸€æ ·ã€‚ä½†å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½æ˜¯åŒæ ·é”™è¯¯çš„ï¼›æœ‰äº›æ¨¡å‹åœ¨æè¿°æŸä¸ªç‰¹å®šé—®é¢˜æ—¶ä¼šæ¯”å…¶ä»–æ¨¡å‹æ›´å¥½ã€‚
- en: 'In the previous chapters, we focused our attention on the inference problem,
    that is, how to learn the values of parameters from data. In this chapter, we
    are going to focus on a complementary problem: how to compare two or more models
    for the same data. As we will learn, this is both a central problem in data analysis
    and a tricky one. In this chapter, we are going to keep examples super simple,
    so we can focus on the technical aspects of model comparison. In the forthcoming
    chapters, we are going to apply what we learn here to more complex examples.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰å‡ ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›é›†ä¸­åœ¨æ¨æ–­é—®é¢˜ä¸Šï¼Œä¹Ÿå°±æ˜¯å¦‚ä½•ä»æ•°æ®ä¸­å­¦ä¹ å‚æ•°å€¼ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ä¸€ä¸ªäº’è¡¥çš„é—®é¢˜ï¼šå¦‚ä½•æ¯”è¾ƒé’ˆå¯¹åŒä¸€æ•°æ®çš„ä¸¤ä¸ªæˆ–å¤šä¸ªæ¨¡å‹ã€‚æ­£å¦‚æˆ‘ä»¬å°†è¦å­¦ä¹ çš„ï¼Œè¿™æ—¢æ˜¯æ•°æ®åˆ†æä¸­çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæ£˜æ‰‹çš„é—®é¢˜ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¿æŒç¤ºä¾‹ç®€å•ï¼Œä»¥ä¾¿ä¸“æ³¨äºæ¨¡å‹æ¯”è¾ƒçš„æŠ€æœ¯ç»†èŠ‚ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠåœ¨è¿™é‡Œå­¦åˆ°çš„çŸ¥è¯†åº”ç”¨äºæ›´å¤æ‚çš„ä¾‹å­ã€‚
- en: 'In this chapter, we will explore the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Overfitting and underfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ
- en: Information criteria
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿¡æ¯å‡†åˆ™
- en: Cross-validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯
- en: Bayes factors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å› å­
- en: 5.1 Posterior predictive checks
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 åéªŒé¢„æµ‹æ£€æŸ¥
- en: We have previously introduced and discussed posterior predictive checks as a
    way to assess how well a model explains the data used to fit a model. The purpose
    of this type of testing is not to determine whether a model is incorrect; we already
    know this! The goal of the exercise is to understand how well we are capturing
    the data. By performing posterior predictive checks, we aim to better understand
    the limitations of a model. Once we understand the limitations, we can simply
    acknowledge them or try to remove them by improving the model. It is expected
    that a model will not be able to reproduce all aspects of a problem and this is
    usually not a problem as models are built with a purpose in mind. As different
    models often capture different aspects of data, we can compare models using posterior
    predictive checks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰ä»‹ç»å¹¶è®¨è®ºäº†åéªŒé¢„æµ‹æ£€æŸ¥ï¼Œä½œä¸ºè¯„ä¼°æ¨¡å‹å¦‚ä½•è§£é‡Šç”¨äºæ‹Ÿåˆæ¨¡å‹çš„æ•°æ®çš„ä¸€ç§æ–¹æ³•ã€‚è¿™ç§æµ‹è¯•çš„ç›®çš„æ˜¯ä¸æ˜¯ç¡®å®šæ¨¡å‹æ˜¯å¦é”™è¯¯ï¼›æˆ‘ä»¬æ—©å°±çŸ¥é“è¿™ä¸€ç‚¹äº†ï¼è¿™ä¸ªè¿‡ç¨‹çš„ç›®æ ‡æ˜¯ç†è§£æˆ‘ä»¬å¦‚ä½•æ•æ‰æ•°æ®ã€‚é€šè¿‡è¿›è¡ŒåéªŒé¢„æµ‹æ£€æŸ¥ï¼Œæˆ‘ä»¬æ—¨åœ¨æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„å±€é™æ€§ã€‚ä¸€æ—¦æˆ‘ä»¬ç†è§£äº†å±€é™æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°æ‰¿è®¤å®ƒä»¬ï¼Œæˆ–è€…é€šè¿‡æ”¹è¿›æ¨¡å‹æ¥å°è¯•å»é™¤å®ƒä»¬ã€‚é¢„è®¡æ¨¡å‹æ— æ³•é‡ç°é—®é¢˜çš„æ‰€æœ‰æ–¹é¢ï¼Œè¿™é€šå¸¸ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºæ¨¡å‹æ˜¯ä¸ºäº†ç‰¹å®šç›®çš„æ„å»ºçš„ã€‚ç”±äºä¸åŒçš„æ¨¡å‹é€šå¸¸æ•æ‰æ•°æ®çš„ä¸åŒæ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åéªŒé¢„æµ‹æ£€æŸ¥æ¥æ¯”è¾ƒæ¨¡å‹ã€‚
- en: 'Letâ€™s look at a simple example. We have a dataset with two variables, `x` and
    `y`. We are going to fit these data with a linear model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå˜é‡`x`å’Œ`y`çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çº¿æ€§æ¨¡å‹æ¥æ‹Ÿåˆè¿™äº›æ•°æ®ï¼š
- en: '![y = ğ›¼ + ğ›½x ](img/file139.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![y = ğ›¼ + ğ›½x ](img/file139.jpg)'
- en: 'We will also fit the data using a quadratic model, that is, a model with one
    more term than the linear model. For this extra term, we just take *x* to the
    power of 2 and add a *Î²* coefficient:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†ä½¿ç”¨äºŒæ¬¡æ¨¡å‹æ¥æ‹Ÿåˆæ•°æ®ï¼Œå³ä¸€ä¸ªæ¯”çº¿æ€§æ¨¡å‹å¤šä¸€ä¸ªé¡¹çš„æ¨¡å‹ã€‚å¯¹äºè¿™ä¸ªé¢å¤–çš„é¡¹ï¼Œæˆ‘ä»¬åªéœ€å°†*x*çš„å¹³æ–¹åŠ ä¸Šä¸€ä¸ª*Î²*ç³»æ•°ï¼š
- en: '![y = ğ›¼ + ğ›½0x + ğ›½1x2 ](img/file140.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![y = ğ›¼ + ğ›½0x + ğ›½1x2 ](img/file140.jpg)'
- en: 'We can write these models in PyMC as usual; refer to the following code block.
    The only difference from all previous models we have seen so far is that we pass
    the argument `idata_kwargs="log_likelihood": True` to `pm.sample`. This extra
    step will store the log-likelihood in the `InferenceData` object, and we will
    use this info later:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'æˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·åœ¨PyMCä¸­ç¼–å†™è¿™äº›æ¨¡å‹ï¼›å‚è€ƒä»¥ä¸‹ä»£ç å—ã€‚ä¸æˆ‘ä»¬ä¹‹å‰è§è¿‡çš„æ‰€æœ‰æ¨¡å‹å”¯ä¸€çš„ä¸åŒä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬å‘`pm.sample`ä¼ é€’äº†`idata_kwargs="log_likelihood":
    True`è¿™ä¸ªå‚æ•°ã€‚è¿™ä¸ªé¢å¤–çš„æ­¥éª¤å°†æŠŠå¯¹æ•°ä¼¼ç„¶å€¼å­˜å‚¨åœ¨`InferenceData`å¯¹è±¡ä¸­ï¼Œæˆ‘ä»¬ç¨åå°†ä½¿ç”¨è¿™ä¸ªä¿¡æ¯ï¼š'
- en: '**CodeÂ 5.1**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.1**'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure [5.1](#x1-96028r1)* shows the mean fit from both models. Visually,
    both models seem to provide a reasonable fit to the data. At least for me, it
    is not that easy to see which model is best. What do you think?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [5.1](#x1-96028r1)* æ˜¾ç¤ºäº†ä¸¤ä¸ªæ¨¡å‹çš„å‡å€¼æ‹Ÿåˆã€‚ä»è§†è§‰ä¸Šçœ‹ï¼Œä¸¤ä¸ªæ¨¡å‹ä¼¼ä¹éƒ½å¯¹æ•°æ®æä¾›äº†åˆç†çš„æ‹Ÿåˆã€‚è‡³å°‘å¯¹æˆ‘æ¥è¯´ï¼Œè¦çœ‹å‡ºå“ªä¸ªæ¨¡å‹æœ€å¥½å¹¶ä¸é‚£ä¹ˆå®¹æ˜“ã€‚ä½ æ€ä¹ˆçœ‹ï¼Ÿ'
- en: '![PIC](img/file141.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file141.png)'
- en: '**FigureÂ 5.1**: Mean fit for `model_l` (linear) and `model_q` (quadratic)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.1**ï¼š`model_l`ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰å’Œ`model_q`ï¼ˆäºŒæ¬¡æ¨¡å‹ï¼‰çš„å‡å€¼æ‹Ÿåˆ'
- en: To gain further insights, we can do a posterior predictive check. *Figure [5.2](#x1-96030r2)*
    shows KDEs for the observed and predicted data. Here, it is easy to see that `model_q`,
    the quadratic model, provides a better fit to the data. We can also see there
    is a lot of uncertainty, in particular at the tails of the distributions. This
    is because we have a small number of data points.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—æ›´å¤šçš„è§è§£ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡ŒåéªŒé¢„æµ‹æ£€æŸ¥ã€‚*å›¾ [5.2](#x1-96030r2)* æ˜¾ç¤ºäº†è§‚å¯Ÿå€¼å’Œé¢„æµ‹å€¼çš„æ•°æ®çš„KDEï¼ˆæ ¸å¯†åº¦ä¼°è®¡ï¼‰ã€‚åœ¨è¿™é‡Œï¼Œå¾ˆå®¹æ˜“çœ‹å‡º`model_q`ï¼Œå³äºŒæ¬¡æ¨¡å‹ï¼Œæ›´å¥½åœ°æ‹Ÿåˆäº†æ•°æ®ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒçš„å°¾éƒ¨ï¼Œæœ‰å¾ˆå¤šä¸ç¡®å®šæ€§ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ•°æ®ç‚¹çš„æ•°é‡å¾ˆå°‘ã€‚
- en: '![PIC](img/file142.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file142.png)'
- en: '**FigureÂ 5.2**: Posterior predictive checks for `model_l` and `model_q` created
    with the `az.plot_ppc` function'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.2**ï¼šä½¿ç”¨`az.plot_ppc`å‡½æ•°åˆ›å»ºçš„`model_l`å’Œ`model_q`çš„åéªŒé¢„æµ‹æ£€æŸ¥'
- en: Posterior predictive checks are a very versatile idea. We can compare observed
    and predicted data in so many ways. For instance, instead of comparing the densities
    of the distributions, we can compare summary statistics. In the top panel of *Figure
    [5.3](#x1-96032r3)*, we have the distributions of means for both models. The dot
    over the x axis indicates the observed value. We can see that both models capture
    the mean very well, with the quadratic model having less variance. That both models
    capture the mean very well is not surprising as we are explicitly modeling the
    mean. In the bottom panel, we have the distributions of the interquartile range.
    This comparison favors the linear model instead.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒé¢„æµ‹æ£€æŸ¥æ˜¯ä¸€ä¸ªéå¸¸çµæ´»çš„æ¦‚å¿µã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¸å¤šæ–¹å¼æ¯”è¾ƒè§‚å¯Ÿå€¼å’Œé¢„æµ‹å€¼ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒåˆ†å¸ƒçš„å¯†åº¦ï¼Œè€Œä¸æ˜¯ç›´æ¥æ¯”è¾ƒå¯†åº¦ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ¯”è¾ƒæ€»ç»“ç»Ÿè®¡é‡ã€‚åœ¨*å›¾
    [5.3](#x1-96032r3)*çš„é¡¶éƒ¨é¢æ¿ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸¤ä¸ªæ¨¡å‹çš„å‡å€¼åˆ†å¸ƒã€‚xè½´ä¸Šçš„ç‚¹è¡¨ç¤ºè§‚å¯Ÿå€¼ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½å¾ˆå¥½åœ°æ•æ‰äº†å‡å€¼ï¼ŒäºŒæ¬¡æ¨¡å‹çš„æ–¹å·®æ›´å°ã€‚ä¸¤ä¸ªæ¨¡å‹éƒ½èƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰å‡å€¼å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¾å¼åœ°å»ºæ¨¡äº†å‡å€¼ã€‚åœ¨åº•éƒ¨é¢æ¿ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å››åˆ†ä½é—´è·çš„åˆ†å¸ƒã€‚è¿™ä¸€æ¯”è¾ƒåˆ™åå‘äºçº¿æ€§æ¨¡å‹ã€‚
- en: '![PIC](img/file143.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file143.png)'
- en: '**FigureÂ 5.3**: Posterior predictive checks for `model_l` and `model_q` created
    with the `az.plot_bpv` function'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.3**ï¼šä½¿ç”¨`az.plot_bpv`å‡½æ•°åˆ›å»ºçš„`model_l`å’Œ`model_q`çš„åéªŒé¢„æµ‹æ£€æŸ¥'
- en: In general, a statistic that is *orthogonal* to what the model is explicitly
    modeling will be more informative for evaluating the model. When in doubt, it
    may be convenient to evaluate more than one statistic. A useful question is to
    ask yourself what aspects of the data you are interested in capturing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œä¸æ¨¡å‹æ˜ç¡®å»ºæ¨¡çš„å†…å®¹*æ­£äº¤*çš„ç»Ÿè®¡é‡å¯¹äºè¯„ä¼°æ¨¡å‹æ¥è¯´ä¼šæ›´å…·ä¿¡æ¯é‡ã€‚å¦‚æœæœ‰ç–‘é—®ï¼Œè¯„ä¼°å¤šä¸ªç»Ÿè®¡é‡å¯èƒ½ä¼šæ›´æ–¹ä¾¿ã€‚ä¸€ä¸ªæœ‰ç”¨çš„é—®é¢˜æ˜¯ï¼Œé—®é—®è‡ªå·±ä½ å¯¹æ•°æ®çš„å“ªäº›æ–¹é¢æ„Ÿå…´è¶£ã€‚
- en: 'To generate *Figure [5.3](#x1-96032r3)*, we used the `az.plot_bpv` ArviZ function.
    An excerpt of the full code to generate that figure is the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç”Ÿæˆ*å›¾ [5.3](#x1-96032r3)*ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†`az.plot_bpv` ArviZå‡½æ•°ã€‚ç”Ÿæˆè¯¥å›¾çš„å®Œæ•´ä»£ç ç‰‡æ®µå¦‚ä¸‹ï¼š
- en: '**CodeÂ 5.2**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.2**'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that we use the `kind="t_stat"` argument to indicate that we are going
    to use a summary statistic. We can pass a string as in `t_stat="mean"`, to indicate
    that we want to use the mean as the summary statistic. Or, we can use a user-defined
    function, as in `t_stat=iqr`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†`kind="t_stat"`å‚æ•°æ¥æŒ‡ç¤ºæˆ‘ä»¬å°†ä½¿ç”¨æ€»ç»“ç»Ÿè®¡é‡ã€‚æˆ‘ä»¬å¯ä»¥ä¼ é€’ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ¯”å¦‚`t_stat="mean"`ï¼Œè¡¨ç¤ºæˆ‘ä»¬æƒ³è¦ä½¿ç”¨å‡å€¼ä½œä¸ºæ€»ç»“ç»Ÿè®¡é‡ã€‚æˆ–è€…ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„å‡½æ•°ï¼Œæ¯”å¦‚`t_stat=iqr`ã€‚
- en: 'You may have noticed that *Figure [5.3](#x1-96032r3)* also includes a legend
    with `bpv` values. **bpv** stands for Bayesian p-value. This is a numerical way
    of summarizing a comparison between simulated and observed data. To obtain them,
    a summary statistic *T* is chosen, such as the mean, median, standard deviation,
    or whatever you may think is worth comparing. Then *T* is calculated for the observed
    data *T*[obs] and for the simulated data *T*[sim]. Finally, we ask ourselves the
    question â€what is the probability that *T*[sim] is less than or equal to *T*[obs]?â€.
    If the observed values agree with the predicted ones, the expected value will
    be 0.5\. In other words, half of the predictions will be below the observations
    and half will be above. This quantity is known as the **Bayesian** **p-value**:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œ*å›¾ [5.3](#x1-96032r3)* ä¹ŸåŒ…å«äº†å¸¦æœ‰ `bpv` å€¼çš„å›¾ä¾‹ã€‚**bpv** ä»£è¡¨è´å¶æ–¯på€¼ã€‚è¿™æ˜¯ä¸€ç§ä»¥æ•°å€¼æ–¹å¼æ€»ç»“æ¨¡æ‹Ÿæ•°æ®å’Œè§‚å¯Ÿæ•°æ®ä¹‹é—´æ¯”è¾ƒçš„æ–¹æ³•ã€‚ä¸ºäº†è·å¾—å®ƒä»¬ï¼Œé€‰æ‹©ä¸€ä¸ªæ±‡æ€»ç»Ÿè®¡é‡
    *T*ï¼Œæ¯”å¦‚å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®ï¼Œæˆ–è€…ä½ è®¤ä¸ºå€¼å¾—æ¯”è¾ƒçš„ä»»ä½•ä¸œè¥¿ã€‚ç„¶åï¼Œè®¡ç®—è§‚å¯Ÿæ•°æ® *T*[obs] å’Œæ¨¡æ‹Ÿæ•°æ® *T*[sim] çš„ *T*ã€‚æœ€åï¼Œæˆ‘ä»¬é—®è‡ªå·±è¿™ä¸ªé—®é¢˜ï¼šâ€œ*T*[sim]
    å°äºæˆ–ç­‰äº *T*[obs] çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿâ€å¦‚æœè§‚å¯Ÿå€¼ä¸é¢„æµ‹å€¼ä¸€è‡´ï¼Œé‚£ä¹ˆæœŸæœ›å€¼å°†ä¸º 0.5ã€‚æ¢å¥è¯è¯´ï¼Œä¸€åŠçš„é¢„æµ‹å€¼å°†ä½äºè§‚å¯Ÿå€¼ï¼Œä¸€åŠå°†é«˜äºè§‚å¯Ÿå€¼ã€‚è¿™ä¸ªé‡è¢«ç§°ä¸º
    **è´å¶æ–¯** **på€¼**ï¼š
- en: '![Bayesian p-value â‰œ p(Tsim â‰¤ Tobs | ËœY) ](img/file144.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![è´å¶æ–¯ på€¼ â‰œ p(Tsim â‰¤ Tobs | ËœY)](img/file144.jpg)'
- en: There is yet another way to compute a Bayesian p-value. Instead of using a summary
    statistic, we can use the entire distribution. In this case, we can ask ourselves
    the question â€what is the probability of predicting a lower or equal value for
    **each observed value**?â€. If the model is well calibrated, these probabilities
    should be the same for all observed values. Because the model is capturing all
    observations equally well, we should expect a Uniform distribution. ArviZ can
    help us with the computations; this time we need to use the `az.plot_bpv` function
    with the `kind="p_value"` argument (which is the default). *Figure [5.4](#x1-96050r4)*
    shows the results of this calculation. The white line indicates the expected Uniform
    distribution and the gray band shows the expected deviation given the finite size
    of the sample. It can be seen that these models are very similar.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å¦ä¸€ç§è®¡ç®—è´å¶æ–¯på€¼çš„æ–¹æ³•ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•´ä¸ªåˆ†å¸ƒï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ±‡æ€»ç»Ÿè®¡é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é—®è‡ªå·±è¿™ä¸ªé—®é¢˜ï¼šâ€œå¯¹äº**æ¯ä¸ªè§‚å¯Ÿå€¼**ï¼Œé¢„æµ‹ä¸€ä¸ªæ›´å°æˆ–ç›¸ç­‰çš„å€¼çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿâ€ã€‚å¦‚æœæ¨¡å‹è¢«å¾ˆå¥½åœ°æ ¡å‡†ï¼Œé‚£ä¹ˆè¿™äº›æ¦‚ç‡åº”è¯¥å¯¹äºæ‰€æœ‰è§‚å¯Ÿå€¼æ˜¯ç›¸åŒçš„ã€‚å› ä¸ºæ¨¡å‹èƒ½å¤ŸåŒæ ·å¥½åœ°æ•æ‰æ‰€æœ‰è§‚å¯Ÿå€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥æœŸå¾…ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒã€‚ArviZ
    å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œè®¡ç®—ï¼›è¿™æ¬¡æˆ‘ä»¬éœ€è¦ä½¿ç”¨å¸¦æœ‰ `kind="p_value"` å‚æ•°ï¼ˆè¿™æ˜¯é»˜è®¤å€¼ï¼‰çš„ `az.plot_bpv` å‡½æ•°ã€‚*å›¾ [5.4](#x1-96050r4)*
    æ˜¾ç¤ºäº†æ­¤è®¡ç®—çš„ç»“æœã€‚ç™½è‰²çº¿æ¡è¡¨ç¤ºæœŸæœ›çš„å‡åŒ€åˆ†å¸ƒï¼Œç°è‰²å¸¦çŠ¶åŒºåŸŸæ˜¾ç¤ºäº†ç”±äºæ ·æœ¬çš„æœ‰é™å¤§å°è€Œé¢„æœŸçš„åå·®ã€‚å¯ä»¥çœ‹å‡ºï¼Œè¿™äº›æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚
- en: '![PIC](img/file145.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file145.png)'
- en: '**FigureÂ 5.4**: Posterior predictive checks for `model_l` and `model_q` created
    with the `az.plot_bpv` function'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.4**ï¼šé€šè¿‡ `az.plot_bpv` å‡½æ•°åˆ›å»ºçš„ `model_l` å’Œ `model_q` çš„åéªŒé¢„æµ‹æ£€éªŒ'
- en: Not Those p-values
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£äº›ä¸æ˜¯ p å€¼
- en: For those who are familiar with p-values and their use in frequentist statistics,
    there are a couple of clarifications. What is *Bayesian* about these p-values
    is that we are NOT using a sampling distribution but the posterior predictive
    distribution. Additionally, we are not doing a null hypothesis test, nor trying
    to declare that a difference is â€significant.â€ We are simply trying to quantify
    how well the model explains the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›ç†Ÿæ‚‰ p å€¼åŠå…¶åœ¨é¢‘ç‡ç»Ÿè®¡ä¸­çš„ä½¿ç”¨çš„äººæ¥è¯´ï¼Œæœ‰ä¸€äº›éœ€è¦æ¾„æ¸…çš„åœ°æ–¹ã€‚å…³äºè¿™äº› p å€¼çš„â€œè´å¶æ–¯â€ä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰ä½¿ç”¨æŠ½æ ·åˆ†å¸ƒï¼Œè€Œæ˜¯ä½¿ç”¨äº†åéªŒé¢„æµ‹åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¿›è¡ŒåŸå‡è®¾æ£€éªŒï¼Œä¹Ÿæ²¡æœ‰è¯•å›¾å£°æ˜ä¸€ä¸ªå·®å¼‚æ˜¯â€œæ˜¾è‘—çš„â€ã€‚æˆ‘ä»¬åªæ˜¯è¯•å›¾é‡åŒ–æ¨¡å‹å¦‚ä½•è§£é‡Šæ•°æ®ã€‚
- en: Posterior predictive checks provide a very flexible framework for evaluating
    and comparing models, either using plots or numerical summaries such as Bayesian
    p-values, or a combination of both. The concept is general enough to allow an
    analyst to use their imagination to find different ways to explore the modelâ€™s
    predictions and use the ones that best suit their modeling goals.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒé¢„æµ‹æ£€éªŒæä¾›äº†ä¸€ä¸ªéå¸¸çµæ´»çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œæ¯”è¾ƒæ¨¡å‹ï¼Œæ— è®ºæ˜¯ä½¿ç”¨å›¾å½¢ï¼Œè¿˜æ˜¯ä½¿ç”¨è¯¸å¦‚è´å¶æ–¯ p å€¼ä¹‹ç±»çš„æ•°å€¼æ‘˜è¦ï¼Œæˆ–ä¸¤è€…çš„ç»„åˆã€‚è¿™ä¸ªæ¦‚å¿µè¶³å¤Ÿé€šç”¨ï¼Œå…è®¸åˆ†æäººå‘˜å‘æŒ¥æƒ³è±¡åŠ›ï¼Œå¯»æ‰¾ä¸åŒçš„æ–¹å¼æ¥æ¢ç´¢æ¨¡å‹çš„é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨æœ€é€‚åˆå…¶å»ºæ¨¡ç›®æ ‡çš„æ–¹å¼ã€‚
- en: In the following sections, we will explore other methods for comparing models.
    These new methods can be used in combination with posterior predictive checks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å…¶ä»–æ¯”è¾ƒæ¨¡å‹çš„æ–¹æ³•ã€‚è¿™äº›æ–°æ–¹æ³•å¯ä»¥ä¸åéªŒé¢„æµ‹æ£€éªŒç»“åˆä½¿ç”¨ã€‚
- en: 5.2 The balance between simplicity and accuracy
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 ç®€å•æ€§ä¸å‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡
- en: When choosing between alternative explanations, there is a principle known as
    Occamâ€™s razor. In very general terms, this principle establishes that given two
    or more equivalent explanations for the same phenomenon, the simplest is the preferred
    explanation. A common criterion of simplicity is the number of parameters in a
    model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©ä¸åŒè§£é‡Šæ—¶ï¼Œæœ‰ä¸€ä¸ªåŸåˆ™è¢«ç§°ä¸ºå¥¥å¡å§†å‰ƒåˆ€ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªåŸåˆ™è§„å®šï¼Œå½“æœ‰ä¸¤ä¸ªæˆ–æ›´å¤šç­‰æ•ˆçš„è§£é‡Šæ—¶ï¼Œæœ€ç®€å•çš„è§£é‡Šæ˜¯é¦–é€‰è§£é‡Šã€‚ç®€æ´æ€§çš„ä¸€ç§å¸¸è§æ ‡å‡†æ˜¯æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ã€‚
- en: There are many justifications for this heuristic. We are not going to discuss
    any of them; we are just going to accept them as a reasonable guide.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€å¯å‘å¼æ–¹æ³•æœ‰å¾ˆå¤šç†ç”±æ”¯æŒã€‚æˆ‘ä»¬ä¸ä¼šè®¨è®ºå®ƒä»¬çš„å…·ä½“å†…å®¹ï¼Œæˆ‘ä»¬åªä¼šå°†å®ƒä»¬è§†ä¸ºä¸€ç§åˆç†çš„æŒ‡å¯¼åŸåˆ™ã€‚
- en: Another factor that we generally have to take into account when comparing models
    is their accuracy, that is, how good a model is at fitting the data. According
    to this criterion, if we have two (or more) models and one of them explains the
    data better than the other, then that is the preferred model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æ¯”è¾ƒæ¨¡å‹æ—¶é€šå¸¸éœ€è¦è€ƒè™‘çš„å¦ä¸€ä¸ªå› ç´ æ˜¯å®ƒä»¬çš„å‡†ç¡®æ€§ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ã€‚æ ¹æ®è¿™ä¸€æ ‡å‡†ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰æ¨¡å‹ï¼Œå…¶ä¸­ä¸€ä¸ªæ¯”å¦ä¸€ä¸ªæ›´å¥½åœ°è§£é‡Šäº†æ•°æ®ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¨¡å‹å°±æ˜¯é¦–é€‰æ¨¡å‹ã€‚
- en: Intuitively, it seems that when comparing models, we tend to prefer those that
    best fit the data and those that are simple. But what should we do if these two
    principles lead us to different models? Or, more generally, is there a quantitative
    way to balance both contributions? The short answer is yes, and in fact, there
    is more than one way to do it. But first, letâ€™s see an example to gain intuition.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚ä¸Šï¼Œä¼¼ä¹åœ¨æ¯”è¾ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å€¾å‘äºåå¥½é‚£äº›æœ€èƒ½æ‹Ÿåˆæ•°æ®ä¸”ç®€å•çš„æ¨¡å‹ã€‚ä½†å¦‚æœè¿™ä¸¤ä¸ªåŸåˆ™å¯¼è‡´æˆ‘ä»¬é€‰æ‹©ä¸åŒçš„æ¨¡å‹ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿæˆ–è€…æ›´ä¸€èˆ¬åœ°è¯´ï¼Œæ˜¯å¦æœ‰ä¸€ç§é‡åŒ–çš„æ–¹æ³•æ¥å¹³è¡¡è¿™ä¸¤è€…çš„è´¡çŒ®ï¼Ÿç®€çŸ­çš„å›ç­”æ˜¯æœ‰ï¼Œè€Œä¸”å®é™…ä¸Šæœ‰ä¸æ­¢ä¸€ç§æ–¹æ³•å¯ä»¥åšåˆ°ã€‚ä½†é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼Œä»¥ä¾¿è·å¾—ç›´è§‰ã€‚
- en: 5.2.1 Many parameters (may) lead to overfitting
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 å¤šä¸ªå‚æ•°ï¼ˆå¯èƒ½ï¼‰å¯¼è‡´è¿‡æ‹Ÿåˆ
- en: '*Figure [5.5](#x1-98002r5)* shows three models with an increasing number of
    parameters. The first one (order 0) is just a constant value: whatever the value
    of *X*, the model always predicts the same value for *Y* . The second model (order
    1) is a linear model, as we saw in *Chapter [4](CH04.xhtml#x1-760004)*. The last
    one (order 5) is a polynomial model of order 5\. We will discuss polynomial regression
    in more depth in *Chapter [6](CH06.xhtml#x1-1200006)*, but for the moment, we
    just need to know that the core of the model has the form *Î±* + *Î²*[0]*x* + *Î²*[0]*x*Â²
    + *Î²*[0]*x*Â³ + *Î²*[0]*x*â´ + *Î²*[0]*x*âµ.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [5.5](#x1-98002r5)*å±•ç¤ºäº†ä¸‰ä¸ªå‚æ•°æ•°é‡é€æ¸å¢åŠ çš„æ¨¡å‹ã€‚ç¬¬ä¸€ä¸ªï¼ˆé›¶æ¬¡ï¼‰åªæ˜¯ä¸€ä¸ªå¸¸æ•°å€¼ï¼šæ— è®º *X* çš„å€¼æ˜¯å¤šå°‘ï¼Œæ¨¡å‹æ€»æ˜¯é¢„æµ‹ç›¸åŒçš„
    *Y* å€¼ã€‚ç¬¬äºŒä¸ªæ¨¡å‹ï¼ˆä¸€é˜¶ï¼‰æ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨*ç¬¬ [4](CH04.xhtml#x1-760004) ç« *ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚æœ€åä¸€ä¸ªæ¨¡å‹ï¼ˆäº”æ¬¡ï¼‰æ˜¯ä¸€ä¸ªäº”æ¬¡å¤šé¡¹å¼æ¨¡å‹ã€‚æˆ‘ä»¬å°†åœ¨*ç¬¬
    [6](CH06.xhtml#x1-1200006) ç« *ä¸­æ›´æ·±å…¥åœ°è®¨è®ºå¤šé¡¹å¼å›å½’ï¼Œä½†ç›®å‰æˆ‘ä»¬åªéœ€è¦çŸ¥é“ï¼Œè¯¥æ¨¡å‹çš„æ ¸å¿ƒå½¢å¼æ˜¯ *Î±* + *Î²*[0]*x*
    + *Î²*[0]*x*Â² + *Î²*[0]*x*Â³ + *Î²*[0]*x*â´ + *Î²*[0]*x*âµã€‚'
- en: '![PIC](img/file146.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file146.png)'
- en: '**FigureÂ 5.5**: Three models for a simple dataset'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.5**ï¼šç®€å•æ•°æ®é›†çš„ä¸‰ç§æ¨¡å‹'
- en: In *Figure [5.5](#x1-98002r5)*, we can see that the increase in the complexity
    of the model (number of parameters) is accompanied by a greater accuracy reflected
    in the coefficient of determination *R*Â². This is a way to measure the fit of
    a model (for more information, please read [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)).
    In fact, we can see that the polynomial of order 5 fits the data perfectly, obtaining
    *R*Â² = 1.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾ [5.5](#x1-98002r5)*ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€æ¨¡å‹å¤æ‚åº¦ï¼ˆå‚æ•°æ•°é‡ï¼‰çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‡†ç¡®åº¦ä¹Ÿå¾—åˆ°äº†æå‡ï¼Œè¿™ä¸€å˜åŒ–åæ˜ åœ¨å†³å®šç³»æ•° *R*Â²
    ä¸­ã€‚è¿™æ˜¯ä¸€ç§è¡¡é‡æ¨¡å‹æ‹Ÿåˆåº¦çš„æ–¹å¼ï¼ˆæ¬²äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·é˜…è¯»[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)ï¼‰ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°äº”æ¬¡å¤šé¡¹å¼å®Œç¾åœ°æ‹Ÿåˆäº†æ•°æ®ï¼Œå¾—åˆ°äº†
    *R*Â² = 1ã€‚
- en: Why can the polynomial of order 5 capture the data without errors? The reason
    is that we have the same number of parameters as data, that is, six. Therefore,
    the model is simply acting as an alternative way of expressing the data. The model
    is not learning patterns about the data, it is memorizing the data! This can be
    problematic. The easier way to notice this is by thinking about what will happen
    to a model that memorizes data when presented with new, unobserved data. What
    do you think will happen?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆäº”æ¬¡å¤šé¡¹å¼å¯ä»¥åœ¨æ²¡æœ‰è¯¯å·®çš„æƒ…å†µä¸‹æ‹Ÿåˆæ•°æ®å‘¢ï¼ŸåŸå› åœ¨äºæˆ‘ä»¬æœ‰ä¸æ•°æ®ç›¸åŒæ•°é‡çš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯å…­ä¸ªã€‚å› æ­¤ï¼Œæ¨¡å‹å®é™…ä¸Šåªæ˜¯ä½œä¸ºä¸€ç§è¡¨è¾¾æ•°æ®çš„æ›¿ä»£æ–¹å¼ã€‚æ¨¡å‹å¹¶æ²¡æœ‰å­¦ä¹ æ•°æ®çš„æ¨¡å¼ï¼Œè€Œæ˜¯åœ¨è®°å¿†æ•°æ®ï¼è¿™å¯èƒ½æ˜¯ä¸ªé—®é¢˜ã€‚å¯Ÿè§‰è¿™ä¸€ç‚¹çš„æ›´ç®€å•æ–¹å¼æ˜¯ï¼Œæ€è€ƒå½“é¢å¯¹æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®æ—¶ï¼Œè®°ä½æ•°æ®çš„æ¨¡å‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä½ è®¤ä¸ºä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
- en: Well, the performance is expected to be bad, like someone who just memorizes
    the questions for an exam only to find the questions have been changed at the
    last minute! This situation is represented in *Figure [5.6](#x1-98004r6)*; here,
    we have added two new data points. Maybe we got the money to perform a new experiment
    or our boss just sent us new data. We can see that the model of order 5, which
    was able to exactly fit the data, now has a worse performance than the linear
    model, as measured by *R*Â². From this simple example, we can see that a model
    with the best fit is not always the ideal one.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæ€§èƒ½é¢„æœŸä¼šå¾ˆå·®ï¼Œå°±åƒæŸäººåªæ˜¯èƒŒäº†è€ƒè¯•é—®é¢˜ï¼Œå´å‘ç°é—®é¢˜åœ¨æœ€åä¸€åˆ»è¢«æ›´æ”¹äº†ï¼è¿™ç§æƒ…å†µåœ¨*å›¾[5.6](#x1-98004r6)*ä¸­æœ‰æ‰€å±•ç¤ºï¼›è¿™é‡Œï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸¤ä¸ªæ–°æ•°æ®ç‚¹ã€‚ä¹Ÿè®¸æˆ‘ä»¬æœ‰èµ„é‡‘è¿›è¡Œæ–°çš„å®éªŒï¼Œæˆ–è€…æˆ‘ä»¬çš„è€æ¿åˆšåˆšç»™æˆ‘ä»¬å‘é€äº†æ–°çš„æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸæœ¬èƒ½å¤Ÿå®Œç¾æ‹Ÿåˆæ•°æ®çš„5é˜¶æ¨¡å‹ï¼Œç°åœ¨åœ¨*R*Â²çš„åº¦é‡ä¸‹æ¯”çº¿æ€§æ¨¡å‹è¡¨ç°å¾—æ›´å·®ã€‚ä»è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œæœ€é€‚åˆçš„æ¨¡å‹ä¸ä¸€å®šæ˜¯ç†æƒ³çš„æ¨¡å‹ã€‚
- en: '![PIC](img/file147.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file147.png)'
- en: '**FigureÂ 5.6**: Three models for a simple dataset, plus two new points'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾5.6**ï¼šä¸‰ä¸ªç®€å•æ•°æ®é›†çš„æ¨¡å‹ï¼Œå¤–åŠ ä¸¤ä¸ªæ–°ç‚¹'
- en: 'Loosely speaking, when a model fits the dataset used to learn the parameters
    of that model very well but fits new datasets very poorly, we have overfitting.
    This is a very common problem when analyzing data. A useful way to think about
    overfitting is to consider a dataset as having two components: the signal and
    the noise. The signal is what we want to capture (or learn) from the data. If
    we use a dataset, it is because we believe there is a signal there, otherwise
    it will be an exercise in futility. Noise, on the other hand, is not useful and
    is the product of measurement errors, limitations in the way the data was generated
    or captured, the presence of corrupted data, etc. A model overfits when it is
    so flexible (for a dataset) that it is capable of learning noise. This has the
    consequence that the signal is hidden.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è‡´æ¥è¯´ï¼Œå½“ä¸€ä¸ªæ¨¡å‹éå¸¸å¥½åœ°æ‹Ÿåˆäº†ç”¨äºå­¦ä¹ è¯¥æ¨¡å‹å‚æ•°çš„æ•°æ®é›†ï¼Œä½†åœ¨æ‹Ÿåˆæ–°æ•°æ®é›†æ—¶è¡¨ç°å¾ˆå·®ï¼Œæˆ‘ä»¬å°±é‡åˆ°äº†è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¿™æ˜¯åˆ†ææ•°æ®æ—¶éå¸¸å¸¸è§çš„ä¸€ä¸ªé—®é¢˜ã€‚æ€è€ƒè¿‡æ‹Ÿåˆçš„ä¸€ä¸ªæœ‰ç”¨æ–¹æ³•æ˜¯å°†æ•°æ®é›†è§†ä¸ºåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼šä¿¡å·å’Œå™ªå£°ã€‚ä¿¡å·æ˜¯æˆ‘ä»¬å¸Œæœ›ä»æ•°æ®ä¸­æ•æ‰åˆ°çš„ï¼ˆæˆ–å­¦ä¹ åˆ°çš„ï¼‰å†…å®¹ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨æŸä¸ªæ•°æ®é›†ï¼Œé‚£æ˜¯å› ä¸ºæˆ‘ä»¬è®¤ä¸ºå…¶ä¸­æœ‰ä¿¡å·ï¼Œå¦åˆ™è¿™å°†æ˜¯å¾’åŠ³çš„ã€‚å¦ä¸€æ–¹é¢ï¼Œå™ªå£°æ˜¯æ²¡æœ‰ç”¨çš„ï¼Œå®ƒæ˜¯æµ‹é‡è¯¯å·®ã€æ•°æ®ç”Ÿæˆæˆ–æ•è·æ–¹å¼çš„å±€é™æ€§ã€æ•°æ®æŸåç­‰å› ç´ çš„äº§ç‰©ã€‚å½“æ¨¡å‹è¿‡äºçµæ´»ï¼ˆå¯¹äºä¸€ä¸ªæ•°æ®é›†ï¼‰åˆ°èƒ½å¤Ÿå­¦ä¹ å™ªå£°æ—¶ï¼Œå°±ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚è¿™å¯¼è‡´ä¿¡å·è¢«éšè—èµ·æ¥ã€‚
- en: This is a practical justification for Occamâ€™s razor, and also a warning that,
    at least in principle, it is always possible to create a model so complex that
    it explains all the details in a dataset, even the most irrelevant ones â€” like
    the cartographers in Borgesâ€™ tale, who crafted a map of the Empire as vast as
    the Empire itself, perfectly replicating every detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¥¥å¡å§†å‰ƒåˆ€çš„ä¸€ä¸ªå®é™…è®ºæ®ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªè­¦å‘Šï¼šè‡³å°‘åœ¨åŸåˆ™ä¸Šï¼Œæ€»æ˜¯å¯ä»¥åˆ›å»ºä¸€ä¸ªå¤æ‚åˆ°è¶³ä»¥è§£é‡Šæ•°æ®é›†ä¸­æ‰€æœ‰ç»†èŠ‚çš„æ¨¡å‹ï¼Œç”šè‡³æ˜¯æœ€ä¸ç›¸å…³çš„ç»†èŠ‚â€”â€”å°±åƒåšå°”èµ«æ–¯æ•…äº‹ä¸­çš„åˆ¶å›¾å¸ˆï¼Œä»–ä»¬åˆ¶ä½œäº†ä¸€å¼ ä¸å¸å›½ä¸€æ ·åºå¤§çš„åœ°å›¾ï¼Œå®Œç¾å¤åˆ¶äº†æ¯ä¸€ä¸ªç»†èŠ‚ã€‚
- en: 5.2.2 Too few parameters lead to underfitting
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 å‚æ•°è¿‡å°‘å¯¼è‡´æ¬ æ‹Ÿåˆ
- en: Continuing with the same example but at the other extreme of complexity, we
    have the model of order 0\. This model is simply a Gaussian disguised as a linear
    model. This model is only capable of capturing the value of the mean of *Y* and
    is therefore totally indifferent to the values of *X*. We say that this model
    has underfitted the data. Models that underfit can also be misleading, especially
    if we are unaware of it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ç»­ä½¿ç”¨ç›¸åŒçš„ä¾‹å­ï¼Œä½†ä»å¤æ‚åº¦çš„å¦ä¸€ææ¥çœ‹ï¼Œæˆ‘ä»¬å¾—åˆ°äº†0é˜¶æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å‹ä»…ä»…æ˜¯ä¸€ä¸ªä¼ªè£…æˆçº¿æ€§æ¨¡å‹çš„é«˜æ–¯åˆ†å¸ƒã€‚è¿™ä¸ªæ¨¡å‹åªèƒ½æ•æ‰åˆ°*Y*çš„å‡å€¼ï¼Œå› æ­¤å®Œå…¨ä¸å…³å¿ƒ*X*çš„å€¼ã€‚æˆ‘ä»¬è¯´è¿™ä¸ªæ¨¡å‹å¯¹æ•°æ®è¿›è¡Œäº†æ¬ æ‹Ÿåˆã€‚æ¬ æ‹Ÿåˆçš„æ¨¡å‹ä¹Ÿå¯èƒ½å…·æœ‰è¯¯å¯¼æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æˆ‘ä»¬æ²¡æœ‰æ„è¯†åˆ°è¿™ä¸€ç‚¹æ—¶ã€‚
- en: 5.3 Measures of predictive accuracy
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 é¢„æµ‹å‡†ç¡®æ€§çš„åº¦é‡
- en: â€Everything should be made as simple as possible, but not simplerâ€ is a quote
    often attributed to Einstein. As in a healthy diet, when modeling, we have to
    maintain a balance. Ideally, we would like to have a model that neither underfits
    nor overfits the data. We want to somehow balance simplicity and goodness of fit.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä¸€åˆ‡åº”å½“å°½å¯èƒ½ç®€åŒ–ï¼Œä½†ä¸èƒ½ç®€åŒ–å¾—è¿‡åº¦â€æ˜¯ä¸€å¥å¸¸è¢«å½’å› äºçˆ±å› æ–¯å¦çš„è¯ã€‚å°±åƒå¥åº·é¥®é£Ÿä¸€æ ·ï¼Œå»ºæ¨¡æ—¶æˆ‘ä»¬ä¹Ÿéœ€è¦ä¿æŒå¹³è¡¡ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ä¸ªæ—¢ä¸è¿‡åº¦æ‹Ÿåˆä¹Ÿä¸æ¬ æ‹Ÿåˆæ•°æ®çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨ç®€æ´æ€§å’Œæ‹Ÿåˆä¼˜åº¦ä¹‹é—´æ‰¾åˆ°æŸç§å¹³è¡¡ã€‚
- en: In the previous example, it is relatively easy to see that the model of order
    0 is too simple, while the model of order 5 is too complex. In order to get a
    general approach that will allow us to rank models, we need to formalize our intuition
    about this balance of simplicity and accuracy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œç›¸å¯¹å®¹æ˜“çœ‹å‡ºï¼Œ0é˜¶æ¨¡å‹è¿‡äºç®€å•ï¼Œè€Œ5é˜¶æ¨¡å‹è¿‡äºå¤æ‚ã€‚ä¸ºäº†å¾—åˆ°ä¸€ä¸ªé€šç”¨çš„æ–¹æ³•ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå¯¹æ¨¡å‹è¿›è¡Œæ’åºï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ç§ç®€å•æ€§å’Œå‡†ç¡®æ€§ä¹‹é—´çš„å¹³è¡¡å½¢å¼åŒ–ã€‚
- en: 'Letâ€™s look at a couple of terms that will be useful to us:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹å‡ ä¸ªå¯¹æˆ‘ä»¬æœ‰ç”¨çš„æœ¯è¯­ï¼š
- en: '**Within-sample accuracy**: The accuracy is measured with the same data used
    to fit the model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ·æœ¬å†…å‡†ç¡®åº¦**ï¼šä½¿ç”¨ä¸æ‹Ÿåˆæ¨¡å‹ç›¸åŒçš„æ•°æ®æ¥è¡¡é‡çš„å‡†ç¡®åº¦ã€‚'
- en: '**Out-of-sample accuracy**: The accuracy measured with data not used to fit
    the model.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ ·æœ¬å¤–å‡†ç¡®åº¦**ï¼šä½¿ç”¨æœªç”¨äºæ‹Ÿåˆæ¨¡å‹çš„æ•°æ®æ¥è¡¡é‡çš„å‡†ç¡®åº¦ã€‚'
- en: 'The within-sample accuracy will, on average, be greater than the out-of-sample
    accuracy. That is why using the within-sample accuracy to evaluate a model, in
    general, will lead us to think that we have a better model than we really have.
    Using out-of-sample accuracy is therefore a good idea to avoid fooling ourselves.
    However, leaving data out means we will have less data to inform our models, which
    is a luxury we generally cannot afford. Since this is a central problem in data
    analysis, there are several proposals to address it. Two very popular approaches
    are:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æœ¬å†…å‡†ç¡®åº¦çš„å¹³å‡å€¼é€šå¸¸ä¼šå¤§äºæ ·æœ¬å¤–å‡†ç¡®åº¦ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€èˆ¬æ¥è¯´ï¼Œä½¿ç”¨æ ·æœ¬å†…å‡†ç¡®åº¦æ¥è¯„ä¼°æ¨¡å‹ä¼šè®©æˆ‘ä»¬è¯¯ä»¥ä¸ºæ¨¡å‹æ¯”å®é™…æ›´å¥½ã€‚å› æ­¤ï¼Œä½¿ç”¨æ ·æœ¬å¤–å‡†ç¡®åº¦æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå¯ä»¥é¿å…æˆ‘ä»¬è‡ªæˆ‘æ¬ºéª—ã€‚ç„¶è€Œï¼Œç•™ä¸‹æ•°æ®æ„å‘³ç€æˆ‘ä»¬å°†æ‹¥æœ‰æ›´å°‘çš„æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œè¿™é€šå¸¸æ˜¯æˆ‘ä»¬ä¸èƒ½å¥¢ä¾ˆçš„ã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜åœ¨æ•°æ®åˆ†æä¸­æ˜¯ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼Œå·²æœ‰å¤šä¸ªææ¡ˆæ¥è§£å†³å®ƒã€‚ä¸¤ç§éå¸¸æµè¡Œçš„æ–¹æ³•æ˜¯ï¼š
- en: '**Information criteria**: This is a general term thatâ€™s used to refer to various
    expressions that approximate out-of-sample accuracy as in-sample accuracy plus
    a term that penalizes model complexity.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¿¡æ¯å‡†åˆ™**ï¼šè¿™æ˜¯ä¸€ä¸ªæ€»ç§°ï¼Œç”¨æ¥æŒ‡ä»£å„ç§è¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼å°†æ ·æœ¬å¤–å‡†ç¡®åº¦è¿‘ä¼¼ä¸ºæ ·æœ¬å†…å‡†ç¡®åº¦åŠ ä¸Šä¸€ä¸ªæƒ©ç½šæ¨¡å‹å¤æ‚æ€§çš„é¡¹ã€‚'
- en: '**Cross-validation**: This is an empirical strategy based on dividing the available
    data into separate subsets that are alternatively used to fit and evaluate the
    models.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äº¤å‰éªŒè¯**ï¼šè¿™æ˜¯ä¸€ç§åŸºäºå°†å¯ç”¨æ•°æ®åˆ†ä¸ºä¸åŒå­é›†çš„æ–¹æ³•ï¼Œè¿™äº›å­é›†äº¤æ›¿ç”¨äºæ‹Ÿåˆå’Œè¯„ä¼°æ¨¡å‹ã€‚'
- en: Letâ€™s look at both of those approaches in more detail in the following sections.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†æ›´è¯¦ç»†åœ°è®¨è®ºè¿™ä¸¤ç§æ–¹æ³•ã€‚
- en: 5.3.1 Information criteria
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 ä¿¡æ¯å‡†åˆ™
- en: Information criteria are a collection of closely related tools used to compare
    models in terms of goodness-of-fit and model complexity. In other words, the information
    criteria formalize the intuition that we developed at the beginning of the chapter.
    The exact way in which these quantities are derived has to do with a field known
    as Information Theory ([[MacKay](Bibliography.xhtml#Xmackay_2003),Â [2003](Bibliography.xhtml#Xmackay_2003)]),
    which is fun, but we will pursue a more intuitive explanation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿¡æ¯å‡†åˆ™æ˜¯ä¸€ç»„å¯†åˆ‡ç›¸å…³çš„å·¥å…·ï¼Œç”¨äºä»æ‹Ÿåˆä¼˜åº¦å’Œæ¨¡å‹å¤æ‚åº¦çš„è§’åº¦æ¯”è¾ƒæ¨¡å‹ã€‚æ¢å¥è¯è¯´ï¼Œä¿¡æ¯å‡†åˆ™å½¢å¼åŒ–äº†æˆ‘ä»¬åœ¨æœ¬ç« å¼€å§‹æ—¶å‘å±•èµ·æ¥çš„ç›´è§‰ã€‚è¿™äº›é‡çš„å…·ä½“æ¨å¯¼æ–¹å¼ä¸ä¸€ä¸ªå«åšä¿¡æ¯ç†è®ºçš„é¢†åŸŸæœ‰å…³ï¼ˆ[[MacKay](Bibliography.xhtml#Xmackay_2003),
    [2003](Bibliography.xhtml#Xmackay_2003)]ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é¢†åŸŸï¼Œä½†æˆ‘ä»¬å°†è¿½æ±‚ä¸€ä¸ªæ›´ç›´è§‚çš„è§£é‡Šã€‚
- en: 'One way to measure how well a model fits the data is to calculate the root
    mean square error between the data and the predictions made by the model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¡é‡æ¨¡å‹æ‹Ÿåˆæ•°æ®å¥½åçš„ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—æ•°æ®ä¸æ¨¡å‹é¢„æµ‹å€¼ä¹‹é—´çš„å‡æ–¹æ ¹è¯¯å·®ï¼š
- en: '![ n 1-âˆ‘ 2 n (yi âˆ’ E(yi | Î¸)) i=1 ](img/file148.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![ n 1-âˆ‘ 2 n (yi âˆ’ E(yi | Î¸)) i=1 ](img/file148.jpg)'
- en: E(*y*[*i*]|*Î¸*) is the predicted value given the estimated parameters. It is
    important to note that this is essentially the average of the squared difference
    between the observed and predicted data. Taking the square of the errors ensures
    that the differences do not cancel out and emphasizes large errors compared to
    other alternatives such as calculating the absolute value.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: E(*y*[*i*]|*Î¸*)æ˜¯ç»™å®šä¼°è®¡å‚æ•°åçš„é¢„æµ‹å€¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯è§‚å¯Ÿå€¼å’Œé¢„æµ‹æ•°æ®ä¹‹é—´å¹³æ–¹å·®çš„å¹³å‡å€¼ã€‚å°†è¯¯å·®å¹³æ–¹å¯ä»¥ç¡®ä¿å·®å¼‚ä¸ä¼šç›¸äº’æŠµæ¶ˆï¼Œå¹¶ä¸”ç›¸å¯¹äºå…¶ä»–æ–¹æ³•ï¼ˆä¾‹å¦‚è®¡ç®—ç»å¯¹å€¼ï¼‰ï¼Œå®ƒèƒ½æ›´å¼ºè°ƒè¾ƒå¤§çš„è¯¯å·®ã€‚
- en: 'The root mean square error may be familiar to you. It is a very popular measure
    â€“ so popular that we may have never spent time thinking about it. But if we do,
    we will see that, in principle, there is nothing special about it and we could
    well devise other similar expressions. When we adopt a probabilistic approach,
    as we do in this book, a more general (and *natural*) expression is the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å‡æ–¹æ ¹è¯¯å·®å¯èƒ½ä½ å·²ç»å¾ˆç†Ÿæ‚‰äº†ã€‚å®ƒæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„åº¦é‡â€”â€”æµè¡Œåˆ°æˆ‘ä»¬å¯èƒ½ä»æœªèŠ±æ—¶é—´æ€è€ƒå®ƒã€‚ä½†å¦‚æœæˆ‘ä»¬ä»”ç»†æƒ³æƒ³ï¼Œå°±ä¼šå‘ç°ï¼ŒåŸåˆ™ä¸Šå®ƒå¹¶æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼Œæˆ‘ä»¬å®Œå…¨å¯ä»¥è®¾è®¡å‡ºå…¶ä»–ç±»ä¼¼çš„è¡¨è¾¾å¼ã€‚å½“æˆ‘ä»¬é‡‡ç”¨æ¦‚ç‡æ–¹æ³•æ—¶ï¼Œæ­£å¦‚æœ¬ä¹¦ä¸­æ‰€åšçš„é‚£æ ·ï¼Œä¸€ä¸ªæ›´ä¸€èˆ¬ï¼ˆå’Œ*è‡ªç„¶çš„*ï¼‰çš„è¡¨è¾¾å¼å¦‚ä¸‹ï¼š
- en: '![âˆ‘n log p(yi | Î¸) i=1 ](img/file149.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![âˆ‘n log p(yi | Î¸) i=1 ](img/file149.jpg)'
- en: That is, we compute the likelihood for each of the *n* observations. We take
    the sum instead of the product because we are working with logarithms. Why do
    we say this is *natural*? Because we can think that, when choosing a likelihood
    for a model, we are implicitly choosing how we want to penalize deviations between
    the data and predictions. In fact, when *p*(*y*[*i*]|*Î¸*) is a Gaussian, then
    the above expression will be proportional to the root mean square error.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ª*n*ä¸ªè§‚æµ‹å€¼è®¡ç®—ä¼¼ç„¶ã€‚æˆ‘ä»¬ä½¿ç”¨å’Œè€Œä¸æ˜¯ä¹˜ç§¯ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ä½¿ç”¨å¯¹æ•°ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¯´è¿™æ˜¯*è‡ªç„¶çš„*å‘¢ï¼Ÿå› ä¸ºæˆ‘ä»¬å¯ä»¥è®¤ä¸ºï¼Œåœ¨ä¸ºæ¨¡å‹é€‰æ‹©ä¼¼ç„¶æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šåœ¨é€‰æ‹©å¦‚ä½•æƒ©ç½šæ•°æ®ä¸é¢„æµ‹ä¹‹é—´çš„åå·®ã€‚äº‹å®ä¸Šï¼Œå½“
    *p*(*y*[*i*]|*Î¸*) æ˜¯é«˜æ–¯åˆ†å¸ƒæ—¶ï¼Œä¸Šè¿°è¡¨è¾¾å¼å°†ä¸å‡æ–¹æ ¹è¯¯å·®æˆæ¯”ä¾‹ã€‚
- en: Now, letâ€™s shift our focus to a detailed exploration of a few specific information
    criteria.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘å¯¹å‡ ä¸ªç‰¹å®šä¿¡æ¯å‡†åˆ™çš„è¯¦ç»†æ¢ç´¢ã€‚
- en: Akaike Information Criterion
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: èµ¤æ± ä¿¡æ¯é‡å‡†åˆ™
- en: '**Akaike Information Criterion** (**AIC**) is a well-known and widely used
    information criterion outside the Bayesian universe and is defined as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**èµ¤æ± ä¿¡æ¯é‡å‡†åˆ™**ï¼ˆ**AIC**ï¼‰æ˜¯ä¸€ä¸ªè‘—åä¸”å¹¿æ³›ä½¿ç”¨çš„ä¿¡æ¯å‡†åˆ™ï¼Œå°¤å…¶åœ¨è´å¶æ–¯é¢†åŸŸå¤–ï¼Œå®šä¹‰ä¸ºï¼š'
- en: '![ âˆ‘n Ë† AIC = âˆ’ 2 log p(yi |Î¸mle)+ 2k i=1 ](img/file150.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‘n Ë† AIC = âˆ’ 2 log p(yi |Î¸mle)+ 2k i=1 ](img/file150.jpg)'
- en: '*k* is the number of model parameters and ![](img/hat_theta.png)[*mle*] is
    the maximum likelihood estimate for *Î¸*.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* æ˜¯æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œ![](img/hat_theta.png)[*mle*] æ˜¯ *Î¸* çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚'
- en: Maximum likelihood estimation is common practice for non-Bayesians and is, in
    general, equivalent to Bayesian **maximum a posteriori** (**MAP**) estimation
    when *flat* priors are used. It is important to note that ![](img/hat_theta.png)[*mle*]
    is a point estimate and not a distribution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯éè´å¶æ–¯æ–¹æ³•ä¸­çš„å¸¸è§åšæ³•ï¼Œå¹¶ä¸”é€šå¸¸ï¼Œå½“ä½¿ç”¨*å¹³å¦çš„*å…ˆéªŒæ—¶ï¼Œå®ƒç­‰åŒäºè´å¶æ–¯**æœ€å¤§åéªŒä¼°è®¡**ï¼ˆ**MAP**ï¼‰ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ![](img/hat_theta.png)[*mle*]
    æ˜¯ä¸€ä¸ªç‚¹ä¼°è®¡ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåˆ†å¸ƒã€‚
- en: The factor âˆ’2 is just a constant, and we could omit it but usually donâ€™t. What
    is important, from a practical point of view, is that the first term takes into
    account how well the model fits the data, while the second term penalizes the
    complexity of the model. Therefore, if two models fit the data equally well, AIC
    says that we should choose the model with the fewest parameters.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å› å­ âˆ’2 åªæ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œæˆ‘ä»¬å¯ä»¥çœç•¥å®ƒï¼Œä½†é€šå¸¸ä¸è¿™ä¹ˆåšã€‚ä»å®é™…çš„è§’åº¦æ¥çœ‹ï¼Œé‡è¦çš„æ˜¯ï¼Œç¬¬ä¸€ä¸ªé¡¹è€ƒè™‘äº†æ¨¡å‹ä¸æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ï¼Œè€Œç¬¬äºŒä¸ªé¡¹åˆ™æƒ©ç½šäº†æ¨¡å‹çš„å¤æ‚åº¦ã€‚å› æ­¤ï¼Œå¦‚æœä¸¤ä¸ªæ¨¡å‹å¯¹æ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ç›¸åŒï¼ŒAIC
    è¡¨ç¤ºæˆ‘ä»¬åº”è¯¥é€‰æ‹©å‚æ•°æœ€å°‘çš„æ¨¡å‹ã€‚
- en: AIC works fine in non-Bayesian approaches but is problematic otherwise. One
    reason is that it does not use the posterior distribution of *Î¸* and therefore
    discards information. Also, AIC, from a Bayesian perspective, assumes that priors
    are *flat* and therefore AIC is incompatible with informative and slightly informative
    priors like those used in this book. Also, the number of parameters in a model
    is not a good measure of the modelâ€™s complexity when using informative priors
    or structures like hierarchical structures, as these are ways of reducing the
    effective number of parameters, also known as *regularization*. We will return
    to this idea of regularization later.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: AIC åœ¨éè´å¶æ–¯æ–¹æ³•ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¶ä»–æƒ…å†µä¸‹å­˜åœ¨é—®é¢˜ã€‚ä¸€ä¸ªåŸå› æ˜¯å®ƒæ²¡æœ‰ä½¿ç”¨*Î¸*çš„åéªŒåˆ†å¸ƒï¼Œå› æ­¤ä¸¢å¤±äº†ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œä»è´å¶æ–¯çš„è§’åº¦æ¥çœ‹ï¼ŒAIC å‡è®¾å…ˆéªŒæ˜¯*å¹³å¦çš„*ï¼Œå› æ­¤
    AIC ä¸åƒæœ¬ä¹¦ä¸­ä½¿ç”¨çš„é‚£äº›ä¿¡æ¯ä¸°å¯Œæˆ–ç¨å¾®ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒä¸å…¼å®¹ã€‚å¦å¤–ï¼Œå½“ä½¿ç”¨ä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒæˆ–å±‚æ¬¡ç»“æ„ç­‰ç»“æ„æ—¶ï¼Œæ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡å¹¶ä¸æ˜¯è¡¡é‡æ¨¡å‹å¤æ‚åº¦çš„å¥½æ–¹æ³•ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•ä¼šå‡å°‘æœ‰æ•ˆå‚æ•°çš„æ•°é‡ï¼Œä¹Ÿç§°ä¸º*æ­£åˆ™åŒ–*ã€‚æˆ‘ä»¬å°†åœ¨åé¢å›åˆ°æ­£åˆ™åŒ–çš„è¿™ä¸ªæ¦‚å¿µã€‚
- en: Widely applicable information criteria
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å¹¿æ³›é€‚ç”¨çš„ä¿¡æ¯å‡†åˆ™
- en: '**Widely applicable information criteria** (**WAIC**) is something like the
    Bayesian version of AIC. It also has two terms, one that measures how good the
    fit is and the other that penalizes complex models. But WAIC uses the full posterior
    distribution to estimate both terms. The following expression assumes that the
    posterior distribution is represented as a sample of size *S* (as obtained from
    an MCMC method):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¹¿æ³›åº”ç”¨çš„ä¿¡æ¯å‡†åˆ™** (**WAIC**) ç±»ä¼¼äºAICçš„è´å¶æ–¯ç‰ˆæœ¬ã€‚å®ƒä¹Ÿæœ‰ä¸¤ä¸ªé¡¹ï¼Œä¸€ä¸ªè¡¡é‡æ‹Ÿåˆçš„å¥½åï¼Œå¦ä¸€ä¸ªå¯¹å¤æ‚æ¨¡å‹è¿›è¡Œæƒ©ç½šã€‚ä½†WAICä½¿ç”¨å®Œæ•´çš„åéªŒåˆ†å¸ƒæ¥ä¼°è®¡è¿™ä¸¤ä¸ªé¡¹ã€‚ä»¥ä¸‹è¡¨è¾¾å¼å‡è®¾åéªŒåˆ†å¸ƒä½œä¸ºå¤§å°ä¸º*S*çš„æ ·æœ¬ï¼ˆé€šè¿‡MCMCæ–¹æ³•è·å¾—ï¼‰æ¥è¡¨ç¤ºï¼š'
- en: '![ ( ) âˆ‘n 1 âˆ‘S s âˆ‘ n ( S s) W AIC = âˆ’ 2 log S- p(yi | Î¸ ) + 2 Vs=1logp(yi |
    Î¸) i s=1 i ](img/file151.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![ ( ) âˆ‘n 1 âˆ‘S s âˆ‘ n ( S s) W AIC = âˆ’ 2 log S- p(yi | Î¸ ) + 2 Vs=1logp(yi |
    Î¸) i s=1 i ](img/file151.jpg)'
- en: The first term is similar to the Akaike criterion, except it is evaluated for
    all the observations and all the samples of the posterior. The second term is
    a bit more difficult to justify without getting into technicalities. But it can
    be interpreted as the effective number of parameters. What is important from a
    practical point of view is that WAIC uses the entire posterior (and not a point
    estimate) for the calculation of both terms, so WAIC can be applied to virtually
    any Bayesian model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªé¡¹ä¸èµ¤æ± å‡†åˆ™ç±»ä¼¼ï¼Œåªä¸è¿‡å®ƒæ˜¯åœ¨æ‰€æœ‰è§‚æµ‹å€¼å’Œæ‰€æœ‰åéªŒæ ·æœ¬ä¸Šè¿›è¡Œè¯„ä¼°çš„ã€‚ç¬¬äºŒä¸ªé¡¹æœ‰ç‚¹éš¾ä»¥è§£é‡Šï¼Œé™¤éæ¶‰åŠåˆ°ä¸€äº›æŠ€æœ¯ç»†èŠ‚ã€‚ä½†å®ƒå¯ä»¥è¢«ç†è§£ä¸ºæœ‰æ•ˆå‚æ•°çš„æ•°é‡ã€‚ä»å®è·µè§’åº¦æ¥çœ‹ï¼Œé‡è¦çš„æ˜¯WAICä½¿ç”¨æ•´ä¸ªåéªŒåˆ†å¸ƒï¼ˆè€Œéç‚¹ä¼°è®¡ï¼‰æ¥è®¡ç®—è¿™ä¸¤ä¸ªé¡¹ï¼Œå› æ­¤WAICå¯ä»¥åº”ç”¨äºå‡ ä¹ä»»ä½•è´å¶æ–¯æ¨¡å‹ã€‚
- en: Other information criteria
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: å…¶ä»–ä¿¡æ¯å‡†åˆ™
- en: Another widely used information criterion is the **Deviance Information** **Criterion**
    (**DIC**). If we use the *bayes-o-meter*^(TM), DIC is more Bayesian than AIC but
    less than WAIC. Although still popular, WAIC and mainly LOO (see the next section)
    have been shown to be more useful both theoretically and empirically than DIC.
    Therefore, we do not recommend its use.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„ä¿¡æ¯å‡†åˆ™æ˜¯**åå·®ä¿¡æ¯å‡†åˆ™** (**DIC**)ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨*bayes-o-meter*^(TM)ï¼ŒDICæ¯”AICæ›´å…·è´å¶æ–¯ç‰¹æ€§ï¼Œä½†ä¸å¦‚WAICã€‚è™½ç„¶ä»ç„¶å¾ˆæµè¡Œï¼Œä½†å·²é€šè¿‡ç†è®ºå’Œå®è¯ç ”ç©¶è¡¨æ˜ï¼ŒWAICå’Œä¸»è¦çš„LOOï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰æ¯”DICæ›´æœ‰ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸æ¨èä½¿ç”¨å®ƒã€‚
- en: Another widely used criterion is **Bayesian Information Criteria** (**BIC**).
    Like logistic regression and my motherâ€™s *dry soup*, this name can be misleading.
    BIC was proposed as a way to correct some of the problems with AIC and the authors
    proposed a Bayesian justification for it. But BIC is not really Bayesian in the
    sense that, like AIC, it assumes flat priors and uses maximum likelihood estimation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å‡†åˆ™æ˜¯**è´å¶æ–¯ä¿¡æ¯å‡†åˆ™** (**BIC**)ã€‚å°±åƒé€»è¾‘å›å½’å’Œæˆ‘æ¯äº²çš„*å¹²æ±¤*ä¸€æ ·ï¼Œè¿™ä¸ªåå­—å¯èƒ½ä¼šè®©äººè¯¯è§£ã€‚BICä½œä¸ºä¸€ç§ä¿®æ­£AICé—®é¢˜çš„æ–¹æ³•æå‡ºï¼Œä½œè€…ä¹Ÿä¸ºå…¶æå‡ºäº†è´å¶æ–¯çš„ç†è®ºä¾æ®ã€‚ä½†BICå¹¶ä¸å®Œå…¨æ˜¯è´å¶æ–¯çš„ï¼Œå› ä¸ºåƒAICä¸€æ ·ï¼Œå®ƒå‡è®¾ä½¿ç”¨å¹³å¦å…ˆéªŒï¼Œå¹¶ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚
- en: But more importantly, BIC differs from AIC and WAIC in its objective. AIC, WAIC,
    and LOO (see next section) try to reflect which model generalizes better to other
    data (predictive accuracy), while BIC tries to identify which is the *correct*
    model and therefore is more related to Bayes factors.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ›´é‡è¦çš„æ˜¯ï¼ŒBICä¸AICå’ŒWAICåœ¨ç›®æ ‡ä¸Šæœ‰æ‰€ä¸åŒã€‚AICã€WAICå’ŒLOOï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰è¯•å›¾åæ˜ å“ªä¸ªæ¨¡å‹èƒ½æ›´å¥½åœ°æ¨å¹¿åˆ°å…¶ä»–æ•°æ®ï¼ˆé¢„æµ‹ç²¾åº¦ï¼‰ï¼Œè€ŒBICè¯•å›¾è¯†åˆ«å“ªä¸ªæ˜¯*æ­£ç¡®*çš„æ¨¡å‹ï¼Œå› æ­¤æ›´ä¸è´å¶æ–¯å› å­ç›¸å…³ã€‚
- en: 5.3.2 Cross-validation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 äº¤å‰éªŒè¯
- en: Cross-validation is a simple and, in most cases, effective solution for comparing
    models. We take our data and divide it into K slices. We try to keep the slices
    more or less the same (in size and sometimes also in other characteristics, such
    as the number of classes). We then use K-1 slices to train the model and slice
    to test it. This process is the systematically repeated omission, for each iteration,
    of a different slice from the training set and using that slice as the evaluation
    set. This is repeated until we have completed K fit-and-evaluation rounds, as
    can be seen in *Figure [5.7](#x1-105004r7)*. The accuracy of the model will be
    the average over the accuracy for each of the K rounds. This is known as K-fold
    cross-validation. Finally, once we have performed cross-validation, we use all
    the data for one last fit and this is the model that is used to make predictions
    or for any other purpose.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯æ˜¯ä¸€ç§ç®€å•ä¸”åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æœ‰æ•ˆçš„æ¨¡å‹æ¯”è¾ƒæ–¹æ³•ã€‚æˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†ä¸ºKä¸ªåˆ‡ç‰‡ï¼Œå°½é‡ä½¿è¿™äº›åˆ‡ç‰‡åœ¨å¤§å°ä¸Šä¿æŒä¸€è‡´ï¼ˆæœ‰æ—¶ä¹Ÿä¼šåœ¨å…¶ä»–ç‰¹å¾ä¸Šä¿æŒä¸€è‡´ï¼Œå¦‚ç±»åˆ«æ•°é‡ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨K-1ä¸ªåˆ‡ç‰‡æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨å‰©ä¸‹çš„ä¸€ä¸ªåˆ‡ç‰‡æ¥è¿›è¡Œæµ‹è¯•ã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯é€šè¿‡ç³»ç»Ÿåœ°åå¤çœç•¥æ¯æ¬¡è®­ç»ƒé›†ä¸­çš„ä¸€ä¸ªä¸åŒåˆ‡ç‰‡ï¼Œå¹¶ä½¿ç”¨è¯¥åˆ‡ç‰‡ä½œä¸ºè¯„ä¼°é›†æ¥å®Œæˆçš„ã€‚ç›´åˆ°å®ŒæˆKè½®æ‹Ÿåˆä¸è¯„ä¼°ï¼Œå¯ä»¥åœ¨*å›¾
    [5.7](#x1-105004r7)*ä¸­çœ‹åˆ°ã€‚æ¨¡å‹çš„å‡†ç¡®åº¦å°†æ˜¯Kè½®ä¸­æ¯ä¸€è½®å‡†ç¡®åº¦çš„å¹³å‡å€¼ã€‚è¿™è¢«ç§°ä¸ºKæŠ˜äº¤å‰éªŒè¯ã€‚æœ€åï¼Œåœ¨æ‰§è¡Œå®Œäº¤å‰éªŒè¯åï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰æ•°æ®è¿›è¡Œæœ€åä¸€æ¬¡æ‹Ÿåˆï¼Œè¿™ä¸ªæ¨¡å‹å°±å¯ä»¥ç”¨æ¥è¿›è¡Œé¢„æµ‹æˆ–å…¶ä»–ç›®çš„ã€‚
- en: '![PIC](img/file152.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file152.png)'
- en: '**FigureÂ 5.7**: K-fold cross-validation'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.7**ï¼šKæŠ˜äº¤å‰éªŒè¯'
- en: When K equals the number of data points, we get what is known as **leave-one-out
    cross-validation** (**LOOCV**), meaning we fit the model to all but one data point
    each time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å½“Kç­‰äºæ•°æ®ç‚¹çš„æ•°é‡æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å°±æ˜¯**ç•™ä¸€æ³•äº¤å‰éªŒè¯**ï¼ˆ**LOOCV**ï¼‰ï¼Œå³æ¯æ¬¡éƒ½å°†æ¨¡å‹æ‹Ÿåˆåˆ°é™¤äº†ä¸€ä¸ªæ•°æ®ç‚¹ä¹‹å¤–çš„æ‰€æœ‰æ•°æ®ç‚¹ä¸Šã€‚
- en: Cross-validation is a routine practice in machine learning, and we have barely
    described the most essential aspects of this practice. There are many other variants
    of the schema presented here. For more information, you can read [James etÂ al.](Bibliography.xhtml#Xjames_2023)Â [[2023](Bibliography.xhtml#Xjames_2023)]
    or [Raschka etÂ al.](Bibliography.xhtml#Xraschka_2022)Â [[2022](Bibliography.xhtml#Xraschka_2022)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„å¸¸è§„å®è·µï¼Œæˆ‘ä»¬æ‰€æè¿°çš„ä»…ä»…æ˜¯è¿™ç§å®è·µçš„æœ€åŸºæœ¬æ–¹é¢ã€‚è¿™é‡Œå±•ç¤ºçš„æ¨¡å¼è¿˜æœ‰å¾ˆå¤šå˜ä½“ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥é˜…è¯»[James ç­‰äºº](Bibliography.xhtml#Xjames_2023)
    [[2023](Bibliography.xhtml#Xjames_2023)]æˆ–[Raschka ç­‰äºº](Bibliography.xhtml#Xraschka_2022)
    [[2022](Bibliography.xhtml#Xraschka_2022)]çš„ç ”ç©¶ã€‚
- en: Cross-validation is a very simple and useful idea, but for some models or for
    large amounts of data, the computational cost of cross-validation may be beyond
    our means. Many people have tried to find simpler quantities to calculate, like
    Information Criteria. In the next section, we discuss a method to approximate
    cross-validation from a single fit to all the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯æ˜¯ä¸€ä¸ªéå¸¸ç®€å•ä¸”æœ‰ç”¨çš„æ¦‚å¿µï¼Œä½†å¯¹äºæŸäº›æ¨¡å‹æˆ–å¤§è§„æ¨¡æ•°æ®é›†ï¼Œäº¤å‰éªŒè¯çš„è®¡ç®—æˆæœ¬å¯èƒ½è¶…å‡ºäº†æˆ‘ä»¬çš„èƒ½åŠ›ã€‚è®¸å¤šäººå°è¯•æ‰¾åˆ°æ›´ç®€å•çš„è®¡ç®—é‡ï¼Œæ¯”å¦‚ä¿¡æ¯å‡†åˆ™ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ç§é€šè¿‡å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œå•æ¬¡æ‹Ÿåˆæ¥è¿‘ä¼¼äº¤å‰éªŒè¯çš„æ–¹æ³•ã€‚
- en: Approximating cross-validation
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: è¿‘ä¼¼äº¤å‰éªŒè¯
- en: 'Cross-validation is a nice idea, but it can be expensive, particularly variants
    like leave-one-out-cross-validation. Luckily, it is possible to approximate it
    using the information from a single fit to the data! The method for doing this
    is called â€Pareto smooth importance sampling leave-one-out cross-validation.â€
    The name is so long that in practice we call it LOO. Conceptually, what we are
    trying to calculate is:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰éªŒè¯æ˜¯ä¸€ä¸ªä¸é”™çš„æƒ³æ³•ï¼Œä½†å®ƒå¯èƒ½ä»£ä»·é«˜æ˜‚ï¼Œç‰¹åˆ«æ˜¯åƒç•™ä¸€æ³•äº¤å‰éªŒè¯è¿™æ ·çš„å˜ä½“ã€‚å¹¸è¿çš„æ˜¯ï¼Œåˆ©ç”¨å•æ¬¡æ‹Ÿåˆçš„æ•°æ®å¯ä»¥è¿‘ä¼¼äº¤å‰éªŒè¯ï¼è¿™ä¸€æ–¹æ³•ç§°ä¸ºâ€œå¸•ç´¯æ‰˜å¹³æ»‘é‡è¦æ€§é‡‡æ ·ç•™ä¸€æ³•äº¤å‰éªŒè¯â€ã€‚è¿™ä¸ªåå­—éå¸¸é•¿ï¼Œå› æ­¤åœ¨å®è·µä¸­æˆ‘ä»¬é€šå¸¸ç®€ç§°å®ƒä¸ºLOOã€‚ä»æ¦‚å¿µä¸Šæ¥è¯´ï¼Œæˆ‘ä»¬å°è¯•è®¡ç®—çš„æ˜¯ï¼š
- en: '![ n âˆ« âˆ‘ ELPDLOO -CV = log p(yi | Î¸) p(Î¸ | yâˆ’i)dÎ¸ i=1 ](img/file153.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![ n âˆ« âˆ‘ ELPDLOO -CV = log p(yi | Î¸) p(Î¸ | yâˆ’i)dÎ¸ i=1 ](img/file153.jpg)'
- en: This is the Expected Log-Pointwise-predictive Density (ELPD). We add the subscript
    *LOO-CV* to make it explicit we are computing the ELPD using leave-one-out cross-validation.
    The [âˆ’*i*] means that we leave the observation *i* out.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœŸæœ›çš„å¯¹æ•°é€ç‚¹é¢„æµ‹å¯†åº¦ï¼ˆELPDï¼‰ã€‚æˆ‘ä»¬åŠ ä¸Šä¸‹æ ‡*LOO-CV*ï¼Œä»¥æ˜ç¡®è¡¨ç¤ºæˆ‘ä»¬ä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯æ¥è®¡ç®—ELPDã€‚[âˆ’*i*]è¡¨ç¤ºæˆ‘ä»¬çœç•¥äº†è§‚æµ‹å€¼*i*ã€‚
- en: 'This expression is very similar to the one for the posterior predictive distribution.
    The difference is that, now, we want to compute the posterior predictive distribution
    for observation *y*[*i*] from a posterior distribution computed without the observation
    *y*[*i*]. The first approximation we take is to prevent the explicit computation
    of the integral by taking samples from the posterior distribution. Thus, we can
    write:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¡¨è¾¾å¼ä¸åéªŒé¢„æµ‹åˆ†å¸ƒçš„è¡¨è¾¾å¼éå¸¸ç›¸ä¼¼ã€‚ä¸åŒä¹‹å¤„åœ¨äºï¼Œç°åœ¨æˆ‘ä»¬è¦è®¡ç®—çš„æ˜¯ä»æ²¡æœ‰åŒ…å«è§‚å¯Ÿå€¼ *y*[*i*] çš„åéªŒåˆ†å¸ƒä¸­è®¡ç®—çš„åéªŒé¢„æµ‹åˆ†å¸ƒã€‚æˆ‘ä»¬é‡‡å–çš„ç¬¬ä¸€ä¸ªé€¼è¿‘æ–¹æ³•æ˜¯é€šè¿‡ä»åéªŒåˆ†å¸ƒä¸­æŠ½æ ·æ¥é¿å…æ˜¾å¼è®¡ç®—ç§¯åˆ†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: '![ n ( s ) âˆ‘ ( 1-âˆ‘ j ) log S p(yi | Î¸âˆ’i) i j ](img/file154.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![ n ( s ) âˆ‘ ( 1-âˆ‘ j ) log S p(yi | Î¸âˆ’i) i j ](img/file154.jpg)'
- en: Here, the sum is over *S* posterior samples. We have been using MCMC samples
    in this book a lot. So, this approximation should not sound unfamiliar to you.
    The tricky part comes next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œæ±‚å’Œæ˜¯å¯¹ *S* åéªŒæ ·æœ¬è¿›è¡Œçš„ã€‚åœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨ MCMC æ ·æœ¬ã€‚å› æ­¤ï¼Œè¿™ç§é€¼è¿‘æ–¹æ³•å¯¹ä½ æ¥è¯´åº”è¯¥ä¸é™Œç”Ÿã€‚æ¥ä¸‹æ¥æ˜¯æ¯”è¾ƒæ£˜æ‰‹çš„éƒ¨åˆ†ã€‚
- en: It is possible to approximate ![](img/Formula_03.PNG) using importance sampling.
    We are not going to discuss the details of that statistical method, but we are
    going to see how importance sampling is a way of approximating a target distribution
    by re-weighting values obtained from another distribution. This method is useful
    when we do not know how to sample from the target distribution but we know how
    to sample from another distribution. Importance sampling works best when the known
    distribution is *wider* than the target one.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨é‡è¦æ€§æŠ½æ ·æ¥é€¼è¿‘ ![](img/Formula_03.PNG)ã€‚æˆ‘ä»¬ä¸ä¼šè¯¦ç»†è®¨è®ºè¿™ç§ç»Ÿè®¡æ–¹æ³•ï¼Œä½†æˆ‘ä»¬å°†çœ‹åˆ°é‡è¦æ€§æŠ½æ ·æ˜¯ä¸€ç§é€šè¿‡é‡æ–°åŠ æƒä»å¦ä¸€ä¸ªåˆ†å¸ƒä¸­è·å¾—çš„å€¼æ¥é€¼è¿‘ç›®æ ‡åˆ†å¸ƒçš„æ–¹æ³•ã€‚å½“æˆ‘ä»¬ä¸çŸ¥é“å¦‚ä½•ä»ç›®æ ‡åˆ†å¸ƒä¸­æŠ½æ ·ï¼Œä½†çŸ¥é“å¦‚ä½•ä»å¦ä¸€ä¸ªåˆ†å¸ƒä¸­æŠ½æ ·æ—¶ï¼Œè¿™ç§æ–¹æ³•å¾ˆæœ‰ç”¨ã€‚é‡è¦æ€§æŠ½æ ·åœ¨å·²çŸ¥åˆ†å¸ƒæ¯”ç›®æ ‡åˆ†å¸ƒæ›´*å®½æ³›*æ—¶æ•ˆæœæœ€å¥½ã€‚
- en: 'In our case, the known distribution, once a model has been fitted, is the log-likelihood
    for all the observations. And we want to approximate the log-likelihood if we
    had dropped one observation. For this, we need to estimate the â€importanceâ€ (or
    weight) that each observation has in determining the posterior distribution. The
    â€importanceâ€ of a given observation is proportional to the effect the variable
    will produce on the posterior if removed. Intuitively, a relatively unlikely observation
    is more important (or carries more weight) than an expected one. Luckily, these
    weights are easy to compute once we have computed the posterior distribution.
    In fact, the weight of the observation *i* for the *s* posterior sample is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä¸€æ—¦æ¨¡å‹æ‹Ÿåˆå®Œæˆï¼Œå·²çŸ¥åˆ†å¸ƒå°±æ˜¯æ‰€æœ‰è§‚å¯Ÿå€¼çš„å¯¹æ•°ä¼¼ç„¶ã€‚æˆ‘ä»¬å¸Œæœ›é€¼è¿‘çš„æ˜¯å¦‚æœæˆ‘ä»¬å»æ‰ä¸€ä¸ªè§‚å¯Ÿå€¼åçš„å¯¹æ•°ä¼¼ç„¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¼°è®¡æ¯ä¸ªè§‚å¯Ÿå€¼åœ¨ç¡®å®šåéªŒåˆ†å¸ƒä¸­çš„â€œé‡è¦æ€§â€ï¼ˆæˆ–æƒé‡ï¼‰ã€‚ä¸€ä¸ªè§‚å¯Ÿå€¼çš„â€œé‡è¦æ€§â€ä¸å¦‚æœè¯¥è§‚å¯Ÿå€¼è¢«ç§»é™¤æ—¶è¯¥å˜é‡å¯¹åéªŒåˆ†å¸ƒçš„å½±å“æˆæ­£æ¯”ã€‚ç›´è§‚åœ°è¯´ï¼Œä¸€ä¸ªç›¸å¯¹ä¸å¤ªå¯èƒ½çš„è§‚å¯Ÿå€¼æ¯”ä¸€ä¸ªé¢„æœŸä¸­çš„è§‚å¯Ÿå€¼æ›´ä¸ºé‡è¦ï¼ˆæˆ–æƒé‡æ›´å¤§ï¼‰ã€‚å¹¸è¿çš„æ˜¯ï¼Œä¸€æ—¦æˆ‘ä»¬è®¡ç®—äº†åéªŒåˆ†å¸ƒï¼Œè¿™äº›æƒé‡å°±å®¹æ˜“è®¡ç®—ã€‚å®é™…ä¸Šï¼Œè§‚å¯Ÿå€¼
    *i* å¯¹äº *s* åéªŒæ ·æœ¬çš„æƒé‡æ˜¯ï¼š
- en: '![ 1 ws = -------- p(yi | Î¸s) ](img/file155.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ws = -------- p(yi | Î¸s) ](img/file155.jpg)'
- en: This *w*[*s*] may not be reliable. The main issue is that sometimes a few *w*[*s*]
    could be so large that they dominate our calculations, making them unstable. To
    tame these crazy weights, we can use Pareto smoothing. This solution consists
    of replacing some of these weights with weights obtained from fitting a Pareto
    distribution. Why a Pareto distribution? Because the theory indicates that the
    weights should follow this distribution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª *w*[*s*] å¯èƒ½ä¸å¯é ã€‚ä¸»è¦çš„é—®é¢˜æ˜¯ï¼Œæœ‰æ—¶ä¸€äº› *w*[*s*] å¯èƒ½ä¼šéå¸¸å¤§ï¼Œä»¥è‡³äºå®ƒä»¬ä¸»å¯¼äº†æˆ‘ä»¬çš„è®¡ç®—ï¼Œä½¿å¾—è®¡ç®—ä¸ç¨³å®šã€‚ä¸ºäº†æ§åˆ¶è¿™äº›æç«¯çš„æƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨
    Pareto å¹³æ»‘æ–¹æ³•ã€‚è¿™ä¸ªè§£å†³æ–¹æ¡ˆåŒ…æ‹¬å°†è¿™äº›æƒé‡ä¸­çš„ä¸€éƒ¨åˆ†æ›¿æ¢ä¸ºé€šè¿‡æ‹Ÿåˆ Pareto åˆ†å¸ƒè·å¾—çš„æƒé‡ã€‚ä¸ºä»€ä¹ˆä½¿ç”¨ Pareto åˆ†å¸ƒï¼Ÿå› ä¸ºç†è®ºè¡¨æ˜ï¼Œæƒé‡åº”è¯¥éµå¾ªè¿™ç§åˆ†å¸ƒã€‚
- en: So, for each observation, *y*[*i*], the largest weights are used to estimate
    a Pareto distribution, and that distribution is used to replace those weights
    with â€smoothedâ€ weights. This procedure gives robustness to the estimation of
    the ELPD and also provides a way to diagnose the approximation, i.e., to get a
    warning that the LOO method may be failing. For this, we need to pay attention
    to the values of *k*, which is a parameter of the Pareto distribution. Values
    of *k* greater than 0.7 indicate that we may have very influential observations.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªè§‚å¯Ÿå€¼ *y*[*i*]ï¼Œä½¿ç”¨æœ€å¤§çš„æƒé‡æ¥ä¼°è®¡ Pareto åˆ†å¸ƒï¼Œå¹¶ä¸”ä½¿ç”¨è¯¥åˆ†å¸ƒæ¥æ›¿æ¢è¿™äº›æƒé‡ä¸ºâ€œå¹³æ»‘â€åçš„æƒé‡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸º ELPD
    çš„ä¼°è®¡æä¾›äº†ç¨³å¥æ€§ï¼Œå¹¶ä¸”æä¾›äº†ä¸€ç§è¯Šæ–­é€¼è¿‘çš„æ–¹æ³•ï¼Œå³ï¼Œèƒ½å¤Ÿå‘å‡ºè­¦å‘Šï¼Œæç¤º LOO æ–¹æ³•å¯èƒ½å‡ºç°é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å…³æ³¨ *k* çš„å€¼ï¼Œ*k* æ˜¯ Pareto
    åˆ†å¸ƒçš„ä¸€ä¸ªå‚æ•°ã€‚*k* å€¼å¤§äº 0.7 è¡¨æ˜æˆ‘ä»¬å¯èƒ½æœ‰éå¸¸æœ‰å½±å“åŠ›çš„è§‚å¯Ÿå€¼ã€‚
- en: 5.4 Calculating predictive accuracy with ArviZ
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 ä½¿ç”¨ArviZè®¡ç®—é¢„æµ‹å‡†ç¡®æ€§
- en: 'Fortunately, calculating WAIC and LOO with ArviZ is very simple. We just need
    to be sure that the Inference Data has the log-likelihood group. When computing
    a posterior with PyMC, this can be achieved by doing `pm.sample(idata_kwargs="log_likelihood":
    True)`. Now, letâ€™s see how to compute LOO:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¹¸è¿çš„æ˜¯ï¼Œä½¿ç”¨ArviZè®¡ç®—WAICå’ŒLOOéå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€è¦ç¡®ä¿æ¨ç†æ•°æ®åŒ…å«å¯¹æ•°ä¼¼ç„¶ç»„ã€‚ä½¿ç”¨PyMCè®¡ç®—åéªŒæ—¶ï¼Œå¯ä»¥é€šè¿‡æ‰§è¡Œ`pm.sample(idata_kwargs="log_likelihood":
    True)`æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è®¡ç®—LOOï¼š'
- en: '**CodeÂ 5.3**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.3**'
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of `az.loo` has two sections. In the first section, we get a table
    with two rows. The first row is the ELPD (`elpd_loo`) and the second one is the
    effective number of parameters (`p_loo`). In the second section, we have the Pareto
    k diagnostic. This is a measure of the reliability of the LOO approximation. Values
    of k greater than 0.7 indicate that we possibly have very influential observations.
    In this case, we have 33 observations and all of them are good, so we can trust
    the approximation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`az.loo`çš„è¾“å‡ºåˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚åœ¨ç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåŒ…å«ä¸¤è¡Œçš„è¡¨æ ¼ã€‚ç¬¬ä¸€è¡Œæ˜¯ELPDï¼ˆ`elpd_loo`ï¼‰ï¼Œç¬¬äºŒè¡Œæ˜¯æœ‰æ•ˆå‚æ•°æ•°ï¼ˆ`p_loo`ï¼‰ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬æœ‰Pareto
    kè¯Šæ–­ã€‚è¿™æ˜¯LOOè¿‘ä¼¼å¯é æ€§çš„ä¸€ç§åº¦é‡ã€‚kå€¼å¤§äº0.7è¡¨ç¤ºæˆ‘ä»¬å¯èƒ½æœ‰éå¸¸æœ‰å½±å“åŠ›çš„è§‚æµ‹å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰33ä¸ªè§‚æµ‹å€¼ï¼Œä¸”å®ƒä»¬éƒ½æ˜¯å¥½çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä¿¡ä»»è¿™ä¸ªè¿‘ä¼¼ã€‚'
- en: 'To compute WAIC, you can use `az.waic`; the output will be similar, except
    that we will not get the Pareto k diagnostic, or any similar diagnostics. This
    is a downside of WAIC: we do not get any information about the reliability of
    the approximation.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®¡ç®—WAICï¼Œä½ å¯ä»¥ä½¿ç”¨`az.waic`ï¼›è¾“å‡ºå°†ç±»ä¼¼ï¼Œåªæ˜¯æˆ‘ä»¬ä¸ä¼šå¾—åˆ°Pareto kè¯Šæ–­æˆ–ä»»ä½•ç±»ä¼¼çš„è¯Šæ–­ä¿¡æ¯ã€‚è¿™æ˜¯WAICçš„ä¸€ä¸ªç¼ºç‚¹ï¼šæˆ‘ä»¬æ— æ³•è·å¾—ä»»ä½•å…³äºè¿‘ä¼¼å¯é æ€§çš„ä¿¡æ¯ã€‚
- en: If we compute LOO for the quadratic model, we will get a similar output, but
    the ELPD will be higher (around -4), indicating that the quadratic model is better.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä¸ºäºŒæ¬¡æ¨¡å‹è®¡ç®—LOOï¼Œå°†ä¼šå¾—åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼Œä½†ELPDå€¼ä¼šæ›´é«˜ï¼ˆå¤§çº¦-4ï¼‰ï¼Œè¿™è¡¨æ˜äºŒæ¬¡æ¨¡å‹æ›´å¥½ã€‚
- en: 'Values of ELPD are not that useful by themselves and must be interpreted in
    relation to other ELPD values. That is why ArviZ provides two helper functions
    to facilitate this comparison. Letâ€™s look at `az.compare` first:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ELPDçš„æ•°å€¼æœ¬èº«å¹¶ä¸å¤ªæœ‰ç”¨ï¼Œå¿…é¡»ä¸å…¶ä»–ELPDå€¼è¿›è¡Œæ¯”è¾ƒæ¥è§£è¯»ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆArviZæä¾›äº†ä¸¤ä¸ªè¾…åŠ©å‡½æ•°æ¥æ–¹ä¾¿è¿™ç§æ¯”è¾ƒã€‚æˆ‘ä»¬å…ˆæ¥çœ‹`az.compare`ï¼š
- en: '**CodeÂ 5.4**'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.4**'
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|  | **rank** | **elpd_loo** | **p_loo** | **elpd_diff** | **weight** | **se**
    | **dse** | **warning** | **scale** |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | **rank** | **elpd_loo** | **p_loo** | **elpd_diff** | **weight** | **se**
    | **dse** | **warning** | **scale** |'
- en: '| **model_q** | 0 | -4.6 | 2.68 | 0 | 1 | 2.36 | 0 | False | log |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **model_q** | 0 | -4.6 | 2.68 | 0 | 1 | 2.36 | 0 | False | log |'
- en: '| **model_l** | 1 | -14.3 | 2.42 | 9.74 | 3.0e-14 | 2.67 | 2.65 | False | log
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **model_l** | 1 | -14.3 | 2.42 | 9.74 | 3.0e-14 | 2.67 | 2.65 | False | log
    |'
- en: 'In the rows, we have the compared models, and in the columns, we have:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¡Œä¸­ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†æ¯”è¾ƒçš„æ¨¡å‹ï¼Œåœ¨åˆ—ä¸­ï¼Œæˆ‘ä»¬æœ‰ï¼š
- en: '`rank`: The order of the models (from best to worst).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rank`: æ¨¡å‹çš„é¡ºåºï¼ˆä»æœ€å¥½åˆ°æœ€å·®ï¼‰ã€‚'
- en: '`elpd_loo`: The point estimate of the ELPD'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elpd_loo`: ELPDçš„ç‚¹ä¼°è®¡ã€‚'
- en: '`p_loo`: The effective numbers parameters.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p_loo`: æœ‰æ•ˆçš„å‚æ•°æ•°ã€‚'
- en: '`elpd_diff`: The difference between the ELPD of the best model and the other
    models.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elpd_diff`: æœ€ä½³æ¨¡å‹ä¸å…¶ä»–æ¨¡å‹ä¹‹é—´çš„ELPDå·®å¼‚ã€‚'
- en: '`weight`: The relative weight of each model. If we wanted to make predictions
    by combining the different models instead of choosing just one, this would be
    the weight that we should assign to each model. In this case, we see that the
    polynomial model takes all the weight.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`: æ¯ä¸ªæ¨¡å‹çš„ç›¸å¯¹æƒé‡ã€‚å¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡ç»“åˆä¸åŒçš„æ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹ï¼Œè€Œä¸æ˜¯ä»…é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬åº”è¯¥èµ‹äºˆæ¯ä¸ªæ¨¡å‹çš„æƒé‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çœ‹åˆ°å¤šé¡¹å¼æ¨¡å‹å ç”¨äº†æ‰€æœ‰çš„æƒé‡ã€‚'
- en: '`se`: The standard error of the ELPD.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`se`: ELPDçš„æ ‡å‡†è¯¯å·®ã€‚'
- en: '`dse`: The standard error of the differences.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dse`: å·®å¼‚çš„æ ‡å‡†è¯¯å·®ã€‚'
- en: '`warning`: A warning about high k values.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`warning`: å…³äºé«˜kå€¼çš„è­¦å‘Šã€‚'
- en: '`scale`: The scale on which the ELPD is calculated.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`: è®¡ç®—ELPDæ—¶æ‰€ç”¨çš„å°ºåº¦ã€‚'
- en: 'The other helper function provided by ArviZ is `az.compareplot`. This function
    provides similar information to `az.compare`, but graphically. *Figure [5.8](#x1-107011r8)*
    shows the output of this function. Notice that:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ArviZæä¾›çš„å¦ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ˜¯`az.compareplot`ã€‚è¿™ä¸ªå‡½æ•°æä¾›äº†ä¸`az.compare`ç›¸ä¼¼çš„ä¿¡æ¯ï¼Œä½†ä»¥å›¾å½¢æ–¹å¼å‘ˆç°ã€‚*å›¾[5.8](#x1-107011r8)*å±•ç¤ºäº†è¯¥å‡½æ•°çš„è¾“å‡ºã€‚è¯·æ³¨æ„ï¼š
- en: The empty circles represent the ELPD values and the black lines are the standard
    error.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©ºå¿ƒåœ†ä»£è¡¨ELPDå€¼ï¼Œé»‘çº¿æ˜¯æ ‡å‡†è¯¯å·®ã€‚
- en: The highest value of the ELPD is indicated with a vertical dashed gray line
    to facilitate comparison with other values.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€é«˜çš„ELPDå€¼é€šè¿‡ä¸€æ¡å‚ç›´çš„è™šçº¿ç°çº¿æ ‡ç¤ºï¼Œä»¥ä¾¿ä¸å…¶ä»–å€¼è¿›è¡Œæ¯”è¾ƒã€‚
- en: For all models except *the best*, we also get a triangle indicating the value
    of the ELPD difference between each model and the *best* model. The gray error
    bar indicates the standard error of the differences between the point estimates.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰æ¨¡å‹ï¼Œé™¤äº†*æœ€ä¼˜æ¨¡å‹*ï¼Œæˆ‘ä»¬è¿˜ä¼šå¾—åˆ°ä¸€ä¸ªä¸‰è§’å½¢ï¼Œè¡¨ç¤ºæ¯ä¸ªæ¨¡å‹ä¸*æœ€ä¼˜æ¨¡å‹*ä¹‹é—´ELPDå·®å€¼çš„å¤§å°ã€‚ç°è‰²è¯¯å·®æ¡è¡¨ç¤ºç‚¹ä¼°è®¡ä¹‹é—´å·®å¼‚çš„æ ‡å‡†è¯¯å·®ã€‚
- en: '![PIC](img/file156.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file156.png)'
- en: '**FigureÂ 5.8**: Output of `az.compareplot(cmp_df)`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.8**ï¼š`az.compareplot(cmp_df)`çš„è¾“å‡º'
- en: The easiest way to use LOO (or WAIC) is to choose a single model. Just choose
    the model with the highest ELPD value. If we follow this rule, we will have to
    accept that the quadratic model is the best. Even if we take into account the
    standard errors, we can see that they do not overlap. This gives us some certainty
    that indeed the models are *different enough* from each other. If instead, the
    standard errors overlap, we should provide a more nuanced answer.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LOOï¼ˆæˆ–WAICï¼‰æœ€ç®€å•çš„æ–¹æ³•æ˜¯é€‰æ‹©ä¸€ä¸ªå•ä¸€æ¨¡å‹ã€‚åªéœ€é€‰æ‹©ELPDå€¼æœ€é«˜çš„æ¨¡å‹ã€‚å¦‚æœæˆ‘ä»¬éµå¾ªè¿™ä¸ªè§„åˆ™ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸æ¥å—äºŒæ¬¡æ¨¡å‹æ˜¯æœ€ä¼˜çš„ã€‚å³ä½¿è€ƒè™‘åˆ°æ ‡å‡†è¯¯å·®ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°å®ƒä»¬å¹¶æ²¡æœ‰é‡å ã€‚è¿™ç»™äº†æˆ‘ä»¬ä¸€äº›ç¡®å®šæ€§ï¼Œè¡¨æ˜è¿™äº›æ¨¡å‹ç¡®å®*è¶³å¤Ÿä¸åŒ*ã€‚å¦‚æœæ ‡å‡†è¯¯å·®é‡å ï¼Œæˆ‘ä»¬åº”è¯¥ç»™å‡ºæ›´ç»†è‡´çš„ç­”æ¡ˆã€‚
- en: 5.5 Model averaging
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 æ¨¡å‹å¹³å‡
- en: Model selection is attractive for its simplicity, but we might be missing information
    about uncertainty in our models. This is somewhat similar to calculating the full
    posterior and then just keeping the posterior mean; this can lead us to be overconfident
    about what we think we know.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹é€‰æ‹©å› å…¶ç®€ä¾¿æ€§è€Œå…·æœ‰å¸å¼•åŠ›ï¼Œä½†æˆ‘ä»¬å¯èƒ½å¿½è§†äº†æ¨¡å‹ä¸­å…³äºä¸ç¡®å®šæ€§çš„ä¿¡æ¯ã€‚è¿™æœ‰ç‚¹ç±»ä¼¼äºè®¡ç®—å®Œæ•´çš„åéªŒåˆ†å¸ƒï¼Œç„¶åä»…ä¿ç•™åéªŒå‡å€¼ï¼›è¿™å¯èƒ½å¯¼è‡´æˆ‘ä»¬å¯¹è‡ªå·±æ‰€çŸ¥çš„å†…å®¹è¿‡äºè‡ªä¿¡ã€‚
- en: An alternative is to select a single model but to report and analyze the different
    models together with the values of the calculated information criteria, their
    standard errors, and perhaps also the posterior predictive checks. It is important
    to put all these numbers and tests in the context of our problem so that we and
    our audience can get a better idea of the possible limitations and shortcomings
    of the models. For those working in academia, these elements can be used to add
    elements to the discussion section of a paper, presentation, thesis, etc. In industry,
    this can be useful for informing stakeholders about the advantages and limitations
    of models, predictions, and conclusions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ–¹æ³•æ˜¯é€‰æ‹©ä¸€ä¸ªå•ä¸€æ¨¡å‹ï¼Œä½†æŠ¥å‘Šå¹¶åˆ†æä¸åŒçš„æ¨¡å‹ï¼Œä»¥åŠè®¡ç®—å‡ºçš„ä¿¡æ¯æ ‡å‡†ã€å®ƒä»¬çš„æ ‡å‡†è¯¯å·®ï¼Œæˆ–è®¸è¿˜åŒ…æ‹¬åéªŒé¢„æµ‹æ£€æŸ¥ã€‚å°†æ‰€æœ‰è¿™äº›æ•°å­—å’Œæµ‹è¯•æ”¾åœ¨æˆ‘ä»¬çš„å®é™…é—®é¢˜èƒŒæ™¯ä¸‹éå¸¸é‡è¦ï¼Œè¿™æ ·æˆ‘ä»¬å’Œæˆ‘ä»¬çš„è§‚ä¼—å¯ä»¥æ›´å¥½åœ°äº†è§£æ¨¡å‹å¯èƒ½çš„å±€é™æ€§å’Œç¼ºé™·ã€‚å¯¹äºå­¦æœ¯ç•Œçš„äººæ¥è¯´ï¼Œè¿™äº›å…ƒç´ å¯ä»¥ç”¨æ¥åœ¨è®ºæ–‡ã€æŠ¥å‘Šã€è®ºæ–‡ç­‰çš„è®¨è®ºéƒ¨åˆ†ä¸­å¢åŠ å†…å®¹ã€‚åœ¨è¡Œä¸šä¸­ï¼Œè¿™å¯¹å‘åˆ©ç›Šç›¸å…³è€…æä¾›æœ‰å…³æ¨¡å‹ã€é¢„æµ‹å’Œç»“è®ºçš„ä¼˜ç¼ºç‚¹éå¸¸æœ‰ç”¨ã€‚
- en: 'Another possibility is to average the models. In this way, we keep the uncertainty
    about the goodness of fit of each model. We then obtain a meta-model (and meta-predictions)
    using a weighted average of each model. ArviZ provides a function for this task,
    `az.weight_predictions`, which takes as arguments a list of InferenceData objects
    and a list of weights. The weights can be calculated using the `az.compare` function.
    For example, if we want to average the two models we have been using, we can do
    the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§å¯èƒ½æ€§æ˜¯å¯¹æ¨¡å‹è¿›è¡Œå¹³å‡ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬ä¿ç•™äº†æ¯ä¸ªæ¨¡å‹æ‹Ÿåˆä¼˜åº¦çš„ä¸ç¡®å®šæ€§ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨æ¯ä¸ªæ¨¡å‹çš„åŠ æƒå¹³å‡å€¼æ¥è·å¾—ä¸€ä¸ªå…ƒæ¨¡å‹ï¼ˆä»¥åŠå…ƒé¢„æµ‹ï¼‰ã€‚ArviZ
    æä¾›äº†ä¸€ä¸ªç”¨äºæ­¤ä»»åŠ¡çš„å‡½æ•° `az.weight_predictions`ï¼Œå®ƒæ¥å—æ¨æ–­æ•°æ®å¯¹è±¡åˆ—è¡¨å’Œæƒé‡åˆ—è¡¨ä½œä¸ºå‚æ•°ã€‚æƒé‡å¯ä»¥ä½¿ç”¨ `az.compare`
    å‡½æ•°è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦å¯¹æˆ‘ä»¬ä¸€ç›´åœ¨ä½¿ç”¨çš„ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¹³å‡ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ“ä½œï¼š
- en: '**CodeÂ 5.5**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.5**'
- en: '[PRE5]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure [5.9](#x1-108006r9)* shows the results of this calculation. The light
    gray dashed line is the weighted average of the two models, the black solid line
    is the linear model, and the gray solid line is the quadratic one.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [5.9](#x1-108006r9)* æ˜¾ç¤ºäº†è¯¥è®¡ç®—çš„ç»“æœã€‚æµ…ç°è‰²è™šçº¿æ˜¯ä¸¤ä¸ªæ¨¡å‹çš„åŠ æƒå¹³å‡ï¼Œé»‘è‰²å®çº¿æ˜¯çº¿æ€§æ¨¡å‹ï¼Œç°è‰²å®çº¿æ˜¯äºŒæ¬¡æ¨¡å‹ã€‚'
- en: '![PIC](img/file157.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file157.png)'
- en: '**FigureÂ 5.9**: Weighted average of the linear and quadratic models'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.9**ï¼šçº¿æ€§å’ŒäºŒæ¬¡æ¨¡å‹çš„åŠ æƒå¹³å‡'
- en: There are other ways to average models, such as explicitly building a meta-model
    that includes all models of interest as particular cases. For example, an order
    2 polynomial contains a linear model as a particular case, or a hierarchical model
    is the continuous version between two extremes, a grouped model and an ungrouped
    model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–æ–¹å¼æ¥å¯¹æ¨¡å‹è¿›è¡Œå¹³å‡ï¼Œä¾‹å¦‚æ˜¾å¼åœ°æ„å»ºä¸€ä¸ªå…ƒæ¨¡å‹ï¼Œå°†æ‰€æœ‰æ„Ÿå…´è¶£çš„æ¨¡å‹ä½œä¸ºç‰¹å®šæƒ…å†µåŒ…å«åœ¨å†…ã€‚ä¾‹å¦‚ï¼ŒäºŒé˜¶å¤šé¡¹å¼åŒ…å«äº†çº¿æ€§æ¨¡å‹ä½œä¸ºç‰¹å®šæƒ…å†µï¼Œæˆ–è€…åˆ†å±‚æ¨¡å‹æ˜¯ä¸¤ç§æç«¯æ¨¡å‹ä¹‹é—´çš„è¿ç»­ç‰ˆæœ¬ï¼Œä¸€ä¸ªæ˜¯åˆ†ç»„æ¨¡å‹ï¼Œå¦ä¸€ä¸ªæ˜¯éåˆ†ç»„æ¨¡å‹ã€‚
- en: 5.6 Bayes factors
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 è´å¶æ–¯å› å­
- en: An alternative to LOO, cross-validation, and information criteria is Bayes factors.
    It is common for Bayes factors to show up in the literature as a Bayesian alternative
    to frequentist hypothesis testing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: LOOã€äº¤å‰éªŒè¯å’Œä¿¡æ¯å‡†åˆ™çš„æ›¿ä»£æ–¹æ³•æ˜¯è´å¶æ–¯å› å­ã€‚è´å¶æ–¯å› å­é€šå¸¸ä½œä¸ºé¢‘ç‡æ´¾å‡è®¾æ£€éªŒçš„è´å¶æ–¯æ›¿ä»£æ–¹æ¡ˆå‡ºç°åœ¨æ–‡çŒ®ä¸­ã€‚
- en: The *Bayesian way* of comparing *k* models is to calculate the **marginal**
    **likelihood** of each model *p*(*y*|*M*[*k*]), i.e., the probability of the observed
    data *Y* given the model *M*[*k*]. The marginal likelihood is the normalization
    constant of Bayesâ€™ theorem. We can see this if we write Bayesâ€™ theorem and make
    explicit the fact that all inferences depend on the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒ*k*ä¸ªæ¨¡å‹çš„*è´å¶æ–¯æ–¹æ³•*æ˜¯è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„**è¾¹é™…** **ä¼¼ç„¶** *p*(*y*|*M*[*k*])ï¼Œå³ç»™å®šæ¨¡å‹*M*[*k*]æ—¶è§‚å¯Ÿæ•°æ®*Y*çš„æ¦‚ç‡ã€‚è¾¹é™…ä¼¼ç„¶æ˜¯è´å¶æ–¯å®šç†çš„å½’ä¸€åŒ–å¸¸æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å†™å‡ºè´å¶æ–¯å®šç†å¹¶æ˜ç¡®è¡¨æ˜æ‰€æœ‰æ¨æ–­éƒ½ä¾èµ–äºæ¨¡å‹æ¥çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: '![p(Î¸ | Y,Mk ) = p(Y-| Î¸,Mk-)p(Î¸-| Mk-) p(Y | Mk ) ](img/file158.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![p(Î¸ | Y,Mk ) = p(Y-| Î¸,Mk-)p(Î¸-| Mk-) p(Y | Mk ) ](img/file158.jpg)'
- en: where, *y* is the data, *Î¸* is the parameters, and *M*[*k*] is a model out of
    *k* competing models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ*y*æ˜¯æ•°æ®ï¼Œ*Î¸*æ˜¯å‚æ•°ï¼Œ*M*[*k*]æ˜¯ä»*k*ä¸ªç«äº‰æ¨¡å‹ä¸­é€‰æ‹©çš„æ¨¡å‹ã€‚
- en: 'If our main objective is to choose only one model, the *best* from a set of
    models, we can choose the one with the largest value of *p*(*y*|*M*[*k*]). This
    is fine if we assume that all models have the same prior probability. Otherwise,
    we must calculate:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯ä»ä¸€ç»„æ¨¡å‹ä¸­é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼Œå³é€‰æ‹©*æœ€ä½³*æ¨¡å‹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€‰æ‹©å…·æœ‰æœ€å¤§*p*(*y*|*M*[*k*])å€¼çš„é‚£ä¸ªæ¨¡å‹ã€‚å¦‚æœæˆ‘ä»¬å‡è®¾æ‰€æœ‰æ¨¡å‹çš„å…ˆéªŒæ¦‚ç‡ç›¸åŒï¼Œè¿™æ ·åšæ˜¯å¯ä»¥çš„ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å¿…é¡»è®¡ç®—ï¼š
- en: '![p(Mk | y) âˆ p(y | Mk )p(Mk ) ](img/file159.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![p(Mk | y) âˆ p(y | Mk )p(Mk ) ](img/file159.jpg)'
- en: 'If, instead, our main objective is to compare models to determine which are
    more likely and to what extent, this can be achieved using the Bayes factors:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ¯”è¾ƒæ¨¡å‹ï¼Œä»¥ç¡®å®šå“ªäº›æ¨¡å‹æ›´å¯èƒ½ä¸”å¯èƒ½çš„ç¨‹åº¦ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨è´å¶æ–¯å› å­æ¥å®ç°ï¼š
- en: '![BF01 = p(y | M0-) p(y | M1 ) ](img/file160.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![BF01 = p(y | M0-) p(y | M1 ) ](img/file160.jpg)'
- en: That is the ratio between the marginal likelihood of two models. The higher
    the value of *BF*[01], the *better* the model in the numerator (*M*[0] in this
    example). To facilitate the interpretation of the Bayes factors, and to put numbers
    into words, Harold Jeffreys proposed a scale for their interpretation, with levels
    of *support* or *strength* (see *Table [5.1](#x1-109004r1)*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸¤ä¸ªæ¨¡å‹çš„è¾¹é™…ä¼¼ç„¶æ¯”ã€‚*BF*[01]çš„å€¼è¶Šé«˜ï¼Œåˆ†å­ä¸­çš„æ¨¡å‹ï¼ˆæœ¬ä¾‹ä¸­çš„*M*[0]ï¼‰å°±è¶Š*å¥½*ã€‚ä¸ºäº†æ–¹ä¾¿è§£è¯»è´å¶æ–¯å› å­ï¼Œå¹¶å°†æ•°å­—è½¬åŒ–ä¸ºæ–‡å­—ï¼ŒHarold
    Jeffreys æå‡ºäº†ä¸€ä¸ªè§£è¯»è´å¶æ–¯å› å­çš„å°ºåº¦ï¼Œå…¶ä¸­åŒ…æ‹¬*æ”¯æŒ*æˆ–*å¼ºåº¦*çš„ä¸åŒçº§åˆ«ï¼ˆè§*è¡¨ [5.1](#x1-109004r1)*ï¼‰ã€‚
- en: '| **Bayes Factor** | **Support** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **è´å¶æ–¯å› å­** | **æ”¯æŒåº¦** |'
- en: '| 1â€“3 | Anecdotal |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 1â€“3 | è½¶äº‹ |'
- en: '| 3â€“10 | Moderate |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 3â€“10 | ä¸­ç­‰ |'
- en: '| 10â€“30 | Strong |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 10â€“30 | å¼ºçƒˆ |'
- en: '| 30â€“100 | Very Strong |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 30â€“100 | éå¸¸å¼ºçƒˆ |'
- en: '| *>*100 | Extreme |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| *>*100 | æç«¯ |'
- en: '**TableÂ 5.1**: Support for model *M*[0], the one in the numerator'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨ 5.1**ï¼šåˆ†å­ä¸­çš„æ¨¡å‹*M*[0]çš„æ”¯æŒåº¦'
- en: Keep in mind that if you get numbers below 1, then the support is for *M*[1],
    i.e., the model in the denominator. Tables are also available for those cases,
    but notice that you can simply take the inverse of the obtained value.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œå¦‚æœä½ å¾—åˆ°çš„æ•°å€¼ä½äº1ï¼Œé‚£ä¹ˆæ”¯æŒçš„æ˜¯*M*[1]ï¼Œå³åˆ†æ¯ä¸­çš„æ¨¡å‹ã€‚å¯¹äºè¿™äº›æƒ…å†µï¼Œä¹Ÿæœ‰è¡¨æ ¼å¯ä¾›å‚è€ƒï¼Œä½†è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ç®€å•åœ°å–è·å¾—å€¼çš„å€’æ•°ã€‚
- en: It is very important to remember that these rules are just conventions â€“ simple
    guides at best. Results should always be put in the context of our problems and
    should be accompanied by enough detail so that others can assess for themselves
    whether they agree with our conclusions. The proof necessary to ensure something
    in particle physics, or in court, or to decide to carry out an evacuation in the
    face of a looming natural catastrophe is not the same.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œè¿™äº›è§„åˆ™åªæ˜¯çº¦å®šâ€”â€”å……å…¶é‡åªæ˜¯ç®€å•çš„æŒ‡å—ã€‚ç»“æœåº”å§‹ç»ˆæ”¾åœ¨æˆ‘ä»¬çš„å…·ä½“é—®é¢˜èƒŒæ™¯ä¸‹ï¼Œå¹¶é™„ä¸Šè¶³å¤Ÿçš„ç»†èŠ‚ï¼Œä»¥ä¾¿ä»–äººå¯ä»¥è‡ªè¡Œè¯„ä¼°ä»–ä»¬æ˜¯å¦åŒæ„æˆ‘ä»¬çš„ç»“è®ºã€‚åœ¨ç²’å­ç‰©ç†å­¦ä¸­ç¡®ä¿æŸäº›ä¸œè¥¿çš„è¯æ˜ï¼Œæˆ–è€…åœ¨æ³•åº­ä¸Šï¼Œæˆ–è€…åœ¨é¢å¯¹å³å°†å‘ç”Ÿçš„è‡ªç„¶ç¾éš¾æ—¶å†³å®šæ˜¯å¦è¿›è¡Œæ’¤ç¦»çš„è¯æ˜æ˜¯ä¸ä¸€æ ·çš„ã€‚
- en: 5.6.1 Some observations
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 ä¸€äº›è§‚å¯Ÿç»“æœ
- en: 'We will now briefly discuss some key facts about the marginal likelihood:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç®€è¦è®¨è®ºä¸€äº›å…³äºè¾¹é™…ä¼¼ç„¶çš„å…³é”®äº‹å®ï¼š
- en: 'The good: Occamâ€™s razor included. Models with lots of parameters have a higher
    penalty than models with few parameters. The intuitive reason is that the greater
    the number of parameters, the more the prior *extends* with respect to the likelihood.
    An example where it is easy to see this is with nested models: for example, a
    polynomial of order 2 â€containsâ€ the models polynomial of order 1 and polynomial
    of order 0.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜ç‚¹ï¼šåŒ…æ‹¬å¥¥å¡å§†å‰ƒåˆ€ã€‚å‚æ•°è¾ƒå¤šçš„æ¨¡å‹æ¯”å‚æ•°è¾ƒå°‘çš„æ¨¡å‹æœ‰æ›´å¤§çš„æƒ©ç½šã€‚ç›´è§‚çš„ç†ç”±æ˜¯ï¼Œå‚æ•°è¶Šå¤šï¼Œå…ˆéªŒç›¸å¯¹äºä¼¼ç„¶çš„*æ‰©å±•*å°±è¶Šå¤§ã€‚ä¸€ä¸ªå®¹æ˜“çœ‹å‡ºè¿™ç§æƒ…å†µçš„ä¾‹å­æ˜¯åµŒå¥—æ¨¡å‹ï¼šä¾‹å¦‚ï¼ŒäºŒæ¬¡å¤šé¡¹å¼â€œåŒ…å«â€äº†ä¸€æ¬¡å¤šé¡¹å¼å’Œé›¶æ¬¡å¤šé¡¹å¼æ¨¡å‹ã€‚
- en: 'The bad: For many problems, the marginal likelihood cannot be calculated analytically.
    Also, approximating it numerically is usually a difficult task that in the best
    of cases requires specialized methods and, in the worst case, the estimates are
    either impractical or unreliable. In fact, the popularity of the MCMC methods
    is that they allow obtaining the posterior distribution without the need to calculate
    the marginal likelihood.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åå¤„ï¼šå¯¹äºè®¸å¤šé—®é¢˜ï¼Œè¾¹é™…ä¼¼ç„¶æ— æ³•è¿›è¡Œè§£æè®¡ç®—ã€‚æ­¤å¤–ï¼Œæ•°å€¼è¿‘ä¼¼é€šå¸¸æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œåœ¨æœ€å¥½çš„æƒ…å†µä¸‹éœ€è¦ä¸“é—¨çš„æ–¹æ³•ï¼Œè€Œåœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œä¼°ç®—ç»“æœè¦ä¹ˆä¸åˆ‡å®é™…ï¼Œè¦ä¹ˆä¸å¯é ã€‚äº‹å®ä¸Šï¼ŒMCMCæ–¹æ³•çš„æµè¡Œåœ¨äºå®ƒä»¬å¯ä»¥åœ¨ä¸è®¡ç®—è¾¹é™…ä¼¼ç„¶çš„æƒ…å†µä¸‹è·å¾—åéªŒåˆ†å¸ƒã€‚
- en: 'The ugly: The marginal likelihood depends *very sensitively* on the prior distribution
    of the parameters in each model *p*(*Î¸*[*k*]|*M*[*k*]).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¼ºç‚¹ï¼šè¾¹é™…ä¼¼ç„¶å¯¹æ¯ä¸ªæ¨¡å‹ä¸­çš„å‚æ•°å…ˆéªŒåˆ†å¸ƒ*éå¸¸æ•æ„Ÿ*ï¼ˆ*p*(*Î¸*[*k*]|*M*[*k*])ï¼‰ã€‚
- en: It is important to note that the *good* and the *ugly* points are related. Using
    marginal likelihood to compare models is a good idea because it already includes
    a penalty for complex models (which helps us prevent overfitting), and at the
    same time, a change in the prior will affect the marginal likelihood calculations.
    At first, this sounds a bit silly; we already know that priors affect calculations
    (otherwise we could just avoid them). But we are talking about changes in the
    prior that would have a small effect in the posterior but a great impact on the
    value of the marginal likelihood.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ*ä¼˜ç‚¹*å’Œ*ç¼ºç‚¹*æ˜¯ç›¸å…³çš„ã€‚ä½¿ç”¨è¾¹é™…ä¼¼ç„¶æ¥æ¯”è¾ƒæ¨¡å‹æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºå®ƒå·²ç»å¯¹å¤æ‚æ¨¡å‹è¿›è¡Œäº†æƒ©ç½šï¼ˆè¿™æœ‰åŠ©äºæˆ‘ä»¬é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ï¼ŒåŒæ—¶ï¼Œå…ˆéªŒçš„å˜åŒ–å°†å½±å“è¾¹é™…ä¼¼ç„¶çš„è®¡ç®—ã€‚èµ·åˆï¼Œè¿™å¬èµ·æ¥æœ‰äº›å‚»ï¼›æˆ‘ä»¬å·²ç»çŸ¥é“å…ˆéªŒä¼šå½±å“è®¡ç®—ï¼ˆå¦åˆ™æˆ‘ä»¬å¯ä»¥ç›´æ¥é¿å…ä½¿ç”¨å®ƒä»¬ï¼‰ã€‚ä½†æˆ‘ä»¬è¿™é‡Œè°ˆè®ºçš„æ˜¯å…ˆéªŒçš„å˜åŒ–ï¼Œè¿™äº›å˜åŒ–åœ¨åéªŒä¸­å¯èƒ½åªæœ‰å°çš„å½±å“ï¼Œä½†å¯¹è¾¹é™…ä¼¼ç„¶å€¼æœ‰å¾ˆå¤§å½±å“ã€‚
- en: 'The use of Bayes factors is often a watershed among Bayesians. The difficulty
    of its calculation and the sensitivity to the priors are some of the arguments
    against it. Another reason is that, like p-values and hypothesis testing in general,
    Bayes factors favor dichotomous thinking over the estimation of the â€effect size.â€
    In other words, instead of asking ourselves questions like: How many more years
    of life can a cancer treatment provide? We end up asking if the difference between
    treating and not treating a patient is â€statistically significant.â€ Note that
    this last question can be useful in some contexts. The point is that in many other
    contexts, this type of question is not the question that interests us; weâ€™re only
    interested in the one that we were taught to answer.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å› å­çš„ä½¿ç”¨å¸¸å¸¸æ˜¯è´å¶æ–¯å­¦æ´¾çš„åˆ†æ°´å²­ã€‚å…¶è®¡ç®—çš„éš¾åº¦ä»¥åŠå¯¹å…ˆéªŒçš„æ•æ„Ÿæ€§æ˜¯åå¯¹å…¶ä½¿ç”¨çš„ä¸€äº›è®ºç‚¹ã€‚å¦ä¸€ä¸ªåŸå› æ˜¯ï¼Œåƒpå€¼å’Œå‡è®¾æ£€éªŒä¸€æ ·ï¼Œè´å¶æ–¯å› å­æ›´å€¾å‘äºäºŒåˆ†æ³•æ€ç»´ï¼Œè€Œä¸æ˜¯â€œæ•ˆåº”å¤§å°â€çš„ä¼°è®¡ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä¸æ˜¯åœ¨é—®è‡ªå·±åƒè¿™æ ·çš„é—®é¢˜ï¼šç™Œç—‡æ²»ç–—å¯ä»¥å»¶é•¿å¤šå°‘å¹´çš„ç”Ÿå‘½ï¼Ÿè€Œæ˜¯æœ€ç»ˆé—®ï¼Œæ²»ç–—ä¸ä¸æ²»ç–—ä¹‹é—´çš„å·®å¼‚æ˜¯å¦æ˜¯â€œç»Ÿè®¡æ˜¾è‘—çš„â€ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªæœ€åçš„é—®é¢˜åœ¨æŸäº›èƒŒæ™¯ä¸‹æ˜¯æœ‰ç”¨çš„ã€‚å…³é”®æ˜¯ï¼Œåœ¨è®¸å¤šå…¶ä»–æƒ…å†µä¸‹ï¼Œè¿™ç§é—®é¢˜å¹¶ä¸æ˜¯æˆ‘ä»¬æ„Ÿå…´è¶£çš„é—®é¢˜ï¼›æˆ‘ä»¬åªå¯¹æˆ‘ä»¬è¢«æ•™å¯¼å»å›ç­”çš„é—®é¢˜æ„Ÿå…´è¶£ã€‚
- en: 5.6.2 Calculation of Bayes factors
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 è´å¶æ–¯å› å­çš„è®¡ç®—
- en: As we have already mentioned, marginal likelihood (and the Bayes factors derived
    from it) is generally not available in closed form, except for some models. For
    this reason, many numerical methods have been devised for its calculation. Some
    of these methods are so simple and naive ( [https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever))
    that they work very poorly in practice.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬å·²ç»æåˆ°çš„ï¼Œè¾¹é™…ä¼¼ç„¶ï¼ˆä»¥åŠç”±æ­¤å¾—å‡ºçš„è´å¶æ–¯å› å­ï¼‰é€šå¸¸æ— æ³•ä»¥é—­å¼å½¢å¼æä¾›ï¼Œé™¤éæ˜¯æŸäº›ç‰¹å®šæ¨¡å‹ã€‚å› æ­¤ï¼Œå·²ç»è®¾è®¡äº†è®¸å¤šæ•°å€¼æ–¹æ³•æ¥è®¡ç®—å®ƒã€‚å…¶ä¸­ä¸€äº›æ–¹æ³•éå¸¸ç®€å•ä¸”å¤©çœŸï¼ˆ[https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever](https://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever)ï¼‰ï¼Œä»¥è‡³äºåœ¨å®è·µä¸­æ•ˆæœéå¸¸å·®ã€‚
- en: Analytically
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åˆ†æä¸Š
- en: 'For some models, such as the BetaBinomial model, we can calculate the marginal
    likelihood analytically. If we write this model as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæŸäº›æ¨¡å‹ï¼Œå¦‚ BetaBinomial æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥è§£æè®¡ç®—è¾¹é™…ä¼¼ç„¶ã€‚å¦‚æœæˆ‘ä»¬å°†è¯¥æ¨¡å‹è¡¨ç¤ºä¸ºï¼š
- en: '|  | *Î¸* âˆ¼ *Beta*(**Î±*,*Î²**) |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | *Î¸* âˆ¼ *Beta*(**Î±*,*Î²**) |  |'
- en: '|  | *y* âˆ¼ *Bin*(*n* = 1*,p* = *Î¸*) |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | *y* âˆ¼ *Bin*(*n* = 1*,p* = *Î¸*) |  |'
- en: 'then the marginal likelihood will be:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¾¹é™…ä¼¼ç„¶å°†æ˜¯ï¼š
- en: '![ (n) B (ğ›¼+ h, ğ›½ + n âˆ’ h) p(y) = ------------------- h B (ğ›¼,ğ›½) ](img/file161.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![ (n) B (ğ›¼+ h, ğ›½ + n âˆ’ h) p(y) = ------------------- h B (ğ›¼,ğ›½) ](img/file161.jpg)'
- en: '*B* is the beta function (not to be confused with the *Beta* distribution),
    *n* is the number of attempts, and *h* is the success number.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*B* æ˜¯è´å¡”å‡½æ•°ï¼ˆä¸è¦ä¸ *Beta* åˆ†å¸ƒæ··æ·†ï¼‰ï¼Œ*n* æ˜¯å®éªŒæ¬¡æ•°ï¼Œ*h* æ˜¯æˆåŠŸæ¬¡æ•°ã€‚'
- en: 'Since we only care about the relative value of the marginal likelihood under
    two different models (for the same data), we can omit the binomial coefficient
    ![(n) h](img/file162.jpg), so we can write:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬åªå…³å¿ƒåœ¨ä¸¤ç§ä¸åŒæ¨¡å‹ä¸‹çš„è¾¹é™…ä¼¼ç„¶ç›¸å¯¹å€¼ï¼ˆå¯¹äºç›¸åŒçš„æ•°æ®ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥çœç•¥äºŒé¡¹ç³»æ•° ![(n) h](img/file162.jpg)ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å†™æˆï¼š
- en: '![ B (ğ›¼ + h, ğ›½ + n âˆ’ h) p(y) âˆ ------B-(ğ›¼,-ğ›½)------ ](img/file163.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![ B (ğ›¼ + h, ğ›½ + n âˆ’ h) p(y) âˆ ------B-(ğ›¼,-ğ›½)------ ](img/file163.jpg)'
- en: This expression has been coded in the next code block but with a twist. We will
    use the `betaln` function, which returns the natural logarithm of the `beta` function,
    it is common in statistics to do calculations on a
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¡¨è¾¾å¼å·²åœ¨ä¸‹ä¸€ä¸ªä»£ç å—ä¸­ç¼–å†™ï¼Œä½†æœ‰äº›å˜åŒ–ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `betaln` å‡½æ•°ï¼Œå®ƒè¿”å› `beta` å‡½æ•°çš„è‡ªç„¶å¯¹æ•°ï¼Œé€šå¸¸åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œè®¡ç®—ä½¿ç”¨å¯¹æ•°å°ºåº¦ã€‚è¿™å¯ä»¥å‡å°‘åœ¨å¤„ç†æ¦‚ç‡æ—¶çš„æ•°å€¼é—®é¢˜ã€‚
- en: logarithmic scale. This reduces numerical problems when working with probabilities.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ•°å°ºåº¦ã€‚è¿™å‡å°‘äº†å¤„ç†æ¦‚ç‡æ—¶çš„æ•°å€¼é—®é¢˜ã€‚
- en: '**CodeÂ 5.6**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.6**'
- en: '[PRE6]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our data for this example consists of 100 coin tosses and the same number of
    heads and tails. We will compare two models, one with a Uniform prior and one
    with a *more concentrated* prior around *Î¸* = 0*.*5:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç¤ºä¾‹çš„æ•°æ®ç”± 100 æ¬¡æŠ›ç¡¬å¸å®éªŒç»„æˆï¼Œæ­£åé¢æ•°é‡ç›¸åŒã€‚æˆ‘ä»¬å°†æ¯”è¾ƒä¸¤ç§æ¨¡å‹ï¼Œä¸€ç§å…·æœ‰å‡åŒ€å…ˆéªŒï¼Œå¦ä¸€ç§å…·æœ‰å›´ç»• *Î¸* = 0*.*5 çš„ *æ›´é›†ä¸­*
    å…ˆéªŒï¼š
- en: '**CodeÂ 5.7**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.7**'
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*Figure [5.10](#x1-112026r10)* shows the two priors. The Uniform prior is the
    black line, and the peaked prior is the gray line.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [5.10](#x1-112026r10)* æ˜¾ç¤ºäº†ä¸¤ä¸ªå…ˆéªŒåˆ†å¸ƒã€‚å‡åŒ€å…ˆéªŒæ˜¯é»‘çº¿ï¼Œå°–å³°å…ˆéªŒæ˜¯ç°çº¿ã€‚'
- en: '![PIC](img/file164.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file164.png)'
- en: '**FigureÂ 5.10**: Uniform and peaked priors'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.10**ï¼šå‡åŒ€å…ˆéªŒå’Œå°–å³°å…ˆéªŒ'
- en: 'Now, we can calculate the marginal likelihood for each model and the Bayes
    factor, which turns out to be 5:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªæ¨¡å‹è®¡ç®—è¾¹é™…ä¼¼ç„¶å’Œè´å¶æ–¯å› å­ï¼Œç»“æœä¸º 5ï¼š
- en: '**CodeÂ 5.8**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.8**'
- en: '[PRE8]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We see that the model with the prior beta(30*,*30), more concentrated, has â‰ˆ
    5 times more support than the model with the beta(1*,*1). This is to be expected
    since the prior for the first case is concentrated around *Î¸* = 0*.*5 and the
    data *Y* have the same number of heads and tails, that is, they agree with a value
    of *Î¸* around 0.5.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°ï¼Œå…·æœ‰å…ˆéªŒ beta(30*,*30) çš„æ¨¡å‹ï¼Œå…¶æµ“åº¦æ›´é«˜ï¼Œæ”¯æŒåº¦å¤§çº¦æ˜¯ beta(1*,*1) æ¨¡å‹çš„ 5 å€ã€‚è¿™æ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå› ä¸ºç¬¬ä¸€ä¸ªæ¨¡å‹çš„å…ˆéªŒé›†ä¸­åœ¨
    *Î¸* = 0*.*5 é™„è¿‘ï¼Œè€Œæ•°æ® *Y* ä¸­æ­£åé¢æ•°é‡ç›¸åŒï¼Œå³å®ƒä»¬ä¸ *Î¸* çº¦ä¸º 0.5 çš„å€¼ä¸€è‡´ã€‚
- en: Sequential Monte Carlo
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: åºåˆ—è’™ç‰¹å¡æ´›
- en: The **Sequential Monte Carlo** (**SMC**) method is a sampling method that works
    by progressing through a series of successive stages that bridge one distribution
    that is easy to sample from and the posterior of interest. In practice, the starting
    distribution is usually the prior. A byproduct of the SMC sampler is the estimate
    of the marginal likelihood.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**åºåˆ—è’™ç‰¹å¡æ´›**ï¼ˆ**SMC**ï¼‰æ–¹æ³•æ˜¯ä¸€ç§é€šè¿‡ä¸€ç³»åˆ—è¿ç»­é˜¶æ®µçš„é‡‡æ ·æ–¹æ³•ï¼Œé€æ­¥è¿æ¥ä¸€ä¸ªæ˜“äºé‡‡æ ·çš„åˆ†å¸ƒå’Œæ„Ÿå…´è¶£çš„åéªŒåˆ†å¸ƒã€‚åœ¨å®è·µä¸­ï¼Œèµ·å§‹åˆ†å¸ƒé€šå¸¸æ˜¯å…ˆéªŒåˆ†å¸ƒã€‚SMCé‡‡æ ·å™¨çš„å‰¯äº§å“æ˜¯è¾¹é™…ä¼¼ç„¶çš„ä¼°è®¡ã€‚'
- en: '**CodeÂ 5.9**'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.9**'
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As we can see from the preceding code block, SMC also gives us a Bayes factor
    of 5, the same answer as the analytical calculation! The advantage of using SMC
    to calculate marginal likelihood is that we can use it for a wider range of models
    since we no longer need to know an expression in closed form. The price we pay
    for this flexibility is a higher computational cost. Also, keep in mind that SMC
    (with an independent Metropolis-Hastings kernel, as implemented in PyMC) is not
    as efficient as NUTS. As the dimensionality of the problem increases, a more precise
    estimate of the posterior and the marginal likelihood will require a larger number
    of samples of the posterior.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å‰é¢çš„ä»£ç å—ä¸­å¯ä»¥çœ‹å‡ºï¼ŒSMC ä¹Ÿç»™å‡ºäº†è´å¶æ–¯å› å­ 5ï¼Œç»“æœä¸è§£æè®¡ç®—ä¸€è‡´ï¼ä½¿ç”¨ SMC è®¡ç®—è¾¹é™…ä¼¼ç„¶çš„ä¼˜ç‚¹æ˜¯æˆ‘ä»¬å¯ä»¥å°†å…¶åº”ç”¨äºæ›´å¹¿æ³›çš„æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦çŸ¥é“ä¸€ä¸ªå°é—­å½¢å¼çš„è¡¨è¾¾å¼ã€‚æˆ‘ä»¬ä¸ºè¿™ç§çµæ´»æ€§ä»˜å‡ºçš„ä»£ä»·æ˜¯æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSMCï¼ˆä½¿ç”¨ç‹¬ç«‹çš„
    Metropolis-Hastings æ ¸å¿ƒï¼Œå¦‚ PyMC ä¸­å®ç°çš„ï¼‰å¹¶ä¸åƒ NUTS é‚£æ ·é«˜æ•ˆã€‚éšç€é—®é¢˜çš„ç»´åº¦å¢åŠ ï¼Œæ›´ç²¾ç¡®çš„åéªŒä¼°è®¡å’Œè¾¹é™…ä¼¼ç„¶éœ€è¦æ›´å¤šçš„åéªŒæ ·æœ¬ã€‚
- en: Log Space
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ•°ç©ºé—´
- en: In computational statistics, we usually perform computations in log space. This
    helps provide numerical stability and computational efficiency, among other things.
    See, for example, the preceding code block; you can see that we calculated a difference
    (instead of a division) and then we took the exponential before returning the
    result.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—ç»Ÿè®¡å­¦ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸åœ¨å¯¹æ•°ç©ºé—´ä¸­è¿›è¡Œè®¡ç®—ã€‚è¿™æœ‰åŠ©äºæä¾›æ•°å€¼ç¨³å®šæ€§å’Œè®¡ç®—æ•ˆç‡ç­‰ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå‚è€ƒå‰é¢çš„ä»£ç å—ï¼›ä½ å¯ä»¥çœ‹åˆ°æˆ‘ä»¬è®¡ç®—äº†å·®å¼‚ï¼ˆè€Œä¸æ˜¯é™¤æ³•ï¼‰ï¼Œç„¶ååœ¨è¿”å›ç»“æœä¹‹å‰å–äº†æŒ‡æ•°ã€‚
- en: Savageâ€“Dickey ratio
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Savageâ€“Dickeyæ¯”ç‡
- en: 'For the above examples, we have compared two BetaBinomial models. We could
    have compared two completely different models, but there are times when we want
    to compare a null hypothesis `H_0` (or null model) against an alternative *H_1*
    hypothesis. For example, to answer the question â€Is this coin biased?â€, we could
    compare the value *Î¸* = 0*.*5 (representing no bias) with the output of a model
    in which we allow *Î¸* to vary. For this type of comparison, the null model is
    nested within the alternative, which means that the null is a particular value
    of the model we are building. In those cases, calculating the Bayes factor is
    very easy and does not require any special methods. We only need to compare the
    prior and posterior evaluated at the null value (for example, *Î¸* = 0*.*5) under
    the alternative model. We can see that this is true from the following expression:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸Šè¿°ä¾‹å­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ä¸ªBetaBinomialæ¨¡å‹ã€‚æˆ‘ä»¬æœ¬å¯ä»¥æ¯”è¾ƒä¸¤ä¸ªå®Œå…¨ä¸åŒçš„æ¨¡å‹ï¼Œä½†æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›æ¯”è¾ƒä¸€ä¸ªé›¶å‡è®¾`H_0`ï¼ˆæˆ–é›¶æ¨¡å‹ï¼‰ä¸ä¸€ä¸ªå¤‡æ‹©å‡è®¾*H_1*ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†å›ç­”â€œè¿™ä¸ªç¡¬å¸æ˜¯å¦æœ‰åï¼Ÿâ€çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å°†å€¼*Î¸*
    = 0*.*5ï¼ˆè¡¨ç¤ºæ²¡æœ‰åå·®ï¼‰ä¸å…è®¸*Î¸*å˜åŒ–çš„æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºè¿™ç§æ¯”è¾ƒï¼Œé›¶æ¨¡å‹åµŒå¥—åœ¨å¤‡æ‹©æ¨¡å‹ä¸­ï¼Œè¿™æ„å‘³ç€é›¶å‡è®¾æ˜¯æˆ‘ä»¬æ­£åœ¨æ„å»ºçš„æ¨¡å‹ä¸­çš„ä¸€ä¸ªç‰¹å®šå€¼ã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œè®¡ç®—è´å¶æ–¯å› å­éå¸¸ç®€å•ï¼Œä¸éœ€è¦ä»»ä½•ç‰¹æ®Šçš„æ–¹æ³•ã€‚æˆ‘ä»¬åªéœ€è¦æ¯”è¾ƒåœ¨å¤‡æ‹©æ¨¡å‹ä¸‹è¯„ä¼°é›¶å€¼ï¼ˆä¾‹å¦‚*Î¸*
    = 0*.*5ï¼‰æ—¶çš„å…ˆéªŒå’ŒåéªŒã€‚æˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹è¡¨è¾¾å¼ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š
- en: '![ p(y | H )p(Î¸ = 0.5 | y, H ) BF01 = ------0---------------1- p(y | H1 ) p(Î¸
    = 0.5 | H1 ) ](img/file165.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![ p(y | H )p(Î¸ = 0.5 | y, H ) BF01 = ------0---------------1- p(y | H1 ) p(Î¸
    = 0.5 | H1 ) ](img/file165.jpg)'
- en: 'This is true only when *H*[0] is a particular case of *H*[1], ([https://statproofbook.github.io/P/bf-sddr](https://statproofbook.github.io/P/bf-sddr)).
    Next, letâ€™s do it with PyMC and ArviZ. We only need to sample the prior and posterior
    for a model. Letâ€™s try the BetaBinomial model with a Uniform prior:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…å½“*H*[0]æ˜¯*H*[1]çš„ç‰¹å®šæƒ…å†µæ—¶ï¼Œè¿™æ‰æˆç«‹ï¼ˆ[https://statproofbook.github.io/P/bf-sddr](https://statproofbook.github.io/P/bf-sddr)ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ç”¨PyMCå’ŒArviZæ¥å®ç°ã€‚æˆ‘ä»¬åªéœ€è¦ä¸ºä¸€ä¸ªæ¨¡å‹è¿›è¡Œå…ˆéªŒå’ŒåéªŒçš„é‡‡æ ·ã€‚è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨Uniformå…ˆéªŒçš„BetaBinomialæ¨¡å‹ï¼š
- en: '**CodeÂ 5.10**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.10**'
- en: '[PRE12]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The result is shown in *Figure [5.11](#x1-114012r11)*. We can see one KDE for
    the prior (black) and one for the posterior (gray). The two black dots show that
    we evaluated both distributions at the value 0.5\. We can see that the Bayes factor
    in favor of the null hypothesis, `BF_01`, is â‰ˆ 8, which we can interpret as *moderate
    evidence* in favor of the null hypothesis (see *Table [5.1](#x1-109004r1)*).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¾ç¤ºåœ¨*å›¾ [5.11](#x1-114012r11)*ä¸­ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªå…ˆéªŒçš„KDEï¼ˆé»‘è‰²ï¼‰å’Œä¸€ä¸ªåéªŒçš„KDEï¼ˆç°è‰²ï¼‰ã€‚ä¸¤ä¸ªé»‘ç‚¹æ˜¾ç¤ºæˆ‘ä»¬åœ¨å€¼0.5å¤„è¯„ä¼°äº†è¿™ä¸¤ä¸ªåˆ†å¸ƒã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ”¯æŒé›¶å‡è®¾çš„è´å¶æ–¯å› å­`BF_01`çº¦ä¸º8ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§£é‡Šä¸º*é€‚åº¦è¯æ®*æ”¯æŒé›¶å‡è®¾ï¼ˆè§*è¡¨
    [5.1](#x1-109004r1)*ï¼‰ã€‚
- en: '![PIC](img/file166.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file166.png)'
- en: '**FigureÂ 5.11**: Bayes factor for the BetaBinomial model with Uniform prior'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.11**ï¼šå…·æœ‰Uniformå…ˆéªŒçš„BetaBinomialæ¨¡å‹çš„è´å¶æ–¯å› å­'
- en: As we have already discussed, the Bayes factors measure which model, as a whole,
    is better at explaining the data. This includes the prior, even if the prior has
    a relatively low impact on the computation of the posterior. We can also see this
    prior effect by comparing a second model to the null model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„ï¼Œè´å¶æ–¯å› å­è¡¡é‡çš„æ˜¯å“ªä¸ªæ¨¡å‹æ›´å¥½åœ°è§£é‡Šäº†æ•°æ®ã€‚è¿™åŒ…æ‹¬å…ˆéªŒï¼Œå³ä½¿å…ˆéªŒå¯¹åéªŒè®¡ç®—çš„å½±å“ç›¸å¯¹è¾ƒå°ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡å°†ç¬¬äºŒä¸ªæ¨¡å‹ä¸é›¶æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹åˆ°è¿™ç§å…ˆéªŒæ•ˆåº”ã€‚
- en: 'If, instead, our model were a BetaBinomial with a Beta prior (30, 30), the
    `BF_01` would be lower (*anecdotal* on the Jeffrey scale). This is because, according
    to this model, the value of *Î¸* = 0*.*5 is much more likely a priori than for
    a Uniform prior, and therefore the prior and posterior will be much more similar.
    That is, it is not very *surprising* to see that the posterior is concentrated
    around 0.5 after collecting data. Donâ€™t just believe me, letâ€™s calculate it:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªå¸¦æœ‰Betaå…ˆéªŒï¼ˆ30, 30ï¼‰çš„BetaBinomialæ¨¡å‹ï¼Œé‚£ä¹ˆ`BF_01`ä¼šæ›´ä½ï¼ˆåœ¨Jeffreyå°ºåº¦ä¸Šä¸º*è½¶äº‹è¯æ®*ï¼‰ã€‚è¿™æ˜¯å› ä¸ºæ ¹æ®è¿™ä¸ªæ¨¡å‹ï¼Œ*Î¸*
    = 0*.*5çš„å€¼åœ¨å…ˆéªŒä¸­æ¯”Uniformå…ˆéªŒæ›´æœ‰å¯èƒ½ï¼Œå› æ­¤å…ˆéªŒå’ŒåéªŒå°†æ›´åŠ ç›¸ä¼¼ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨æ”¶é›†æ•°æ®åï¼Œçœ‹åˆ°åéªŒé›†ä¸­åœ¨0.5é™„è¿‘å¹¶ä¸ä»¤äºº*æƒŠè®¶*ã€‚ä¸è¦åªç›¸ä¿¡æˆ‘ï¼Œæˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹ï¼š
- en: '**CodeÂ 5.11**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç  5.11**'
- en: '[PRE13]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Figure [5.12](#x1-114022r12)* shows the result. We can see that the `BF_01`
    is â‰ˆ 1*.*6, which we can interpret as *anecdotal evidence* in favor of the null
    hypothesis (see the Jeffreysâ€™ scale, discussed earlier).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ [5.12](#x1-114022r12)* æ˜¾ç¤ºäº†ç»“æœã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°`BF_01`å¤§çº¦æ˜¯1*.*6ï¼Œè¿™å¯ä»¥è§£é‡Šä¸ºæ”¯æŒé›¶å‡è®¾çš„*è½¶äº‹è¯æ®*ï¼ˆå‚è§ä¹‹å‰è®¨è®ºçš„Jeffreyså°ºåº¦ï¼‰ã€‚'
- en: '![PIC](img/file167.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file167.png)'
- en: '**FigureÂ 5.12**: Bayes factor for the BetaBinomial model with peaked prior'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.12**ï¼šå…·æœ‰å°–å³°å…ˆéªŒçš„BetaBinomialæ¨¡å‹çš„è´å¶æ–¯å› å­'
- en: 5.7 Bayes factors and inference
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 è´å¶æ–¯å› å­ä¸æ¨æ–­
- en: So far, we have used Bayes factors to judge which model seems to be better at
    explaining the data, and we found that one of the models is â‰ˆ 5 times *better*
    than the other.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨è´å¶æ–¯å› å­æ¥åˆ¤æ–­å“ªä¸ªæ¨¡å‹åœ¨è§£é‡Šæ•°æ®æ–¹é¢ä¼¼ä¹æ›´å¥½ï¼Œæˆ‘ä»¬å‘ç°å…¶ä¸­ä¸€ä¸ªæ¨¡å‹çš„è¡¨ç°å¤§çº¦æ˜¯å¦ä¸€ä¸ªçš„5å€*æ›´å¥½*ã€‚
- en: 'But what about the posterior we get from these models? How different are they?
    *Table [5.2](#x1-115002r2)* summarizes these two posteriors:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæ¥è‡ªè¿™äº›æ¨¡å‹çš„åéªŒæ€ä¹ˆæ ·å‘¢ï¼Ÿå®ƒä»¬æœ‰å¤šä¸åŒï¼Ÿ*è¡¨ [5.2](#x1-115002r2)* æ€»ç»“äº†è¿™ä¸¤ç§åéªŒï¼š
- en: '|  | **mean** | **sd** | **hdi_3%** | **hdi_97%** |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | **å‡å€¼** | **æ ‡å‡†å·®** | **hdi_3%** | **hdi_97%** |'
- en: '| **uniform** | 0.5 | 0.05 | 0.4 | 0.59 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| **å‡åŒ€** | 0.5 | 0.05 | 0.4 | 0.59 |'
- en: '| **peaked** | 0.5 | 0.04 | 0.42 | 0.57 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| **å°–å³°** | 0.5 | 0.04 | 0.42 | 0.57 |'
- en: '**TableÂ 5.2**: Statistics for the models with uniform and peaked priors computed
    using the ArviZ summary function'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨ 5.2**ï¼šä½¿ç”¨ArviZæ±‡æ€»å‡½æ•°è®¡ç®—çš„å…·æœ‰å‡åŒ€å’Œå°–å³°å…ˆéªŒçš„æ¨¡å‹ç»Ÿè®¡æ•°æ®'
- en: We can argue that the results are quite similar; we have the same mean value
    for *Î¸* and a slightly wider posterior for `model_0`, as expected since this model
    has a wider prior. We can also check the posterior predictive distribution to
    see how similar they are (see *Figure [5.13](#x1-115004r13)*).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯´ç»“æœéå¸¸ç›¸ä¼¼ï¼›æˆ‘ä»¬å¾—åˆ°äº†ç›¸åŒçš„*Î¸*å‡å€¼ï¼Œè€Œ`model_0`çš„åéªŒç•¥å®½ï¼Œè¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºè¯¥æ¨¡å‹å…·æœ‰è¾ƒå®½çš„å…ˆéªŒã€‚æˆ‘ä»¬è¿˜å¯ä»¥æ£€æŸ¥åéªŒé¢„æµ‹åˆ†å¸ƒï¼Œçœ‹çœ‹å®ƒä»¬æœ‰å¤šç›¸ä¼¼ï¼ˆè§*å›¾
    [5.13](#x1-115004r13)*ï¼‰ã€‚
- en: '![PIC](img/file168.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file168.png)'
- en: '**FigureÂ 5.13**: Posterior predictive distributions for models with uniform
    and peaked priors'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.13**ï¼šå…·æœ‰å‡åŒ€å’Œå°–å³°å…ˆéªŒçš„æ¨¡å‹çš„åéªŒé¢„æµ‹åˆ†å¸ƒ'
- en: In this example, the observed data is more consistent with `model_1`, because
    the prior is concentrated around the correct value of *Î¸*, while `model_0`, assigns
    the same probability to all possible values of *Î¸*. This difference between the
    models is captured by the Bayes factor. We could say that the Bayes factors measure
    which model, as a whole, is better for explaining the data. This includes the
    details of the prior, no matter how similar the model predictions are. In many
    scenarios, this is not what interests us when comparing models, and instead, we
    prefer to evaluate models in terms of how similar their predictions are. For those
    cases, we can use LOO.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè§‚å¯Ÿåˆ°çš„æ•°æ®æ›´ç¬¦åˆ`model_1`ï¼Œå› ä¸ºå…¶å…ˆéªŒé›†ä¸­åœ¨æ­£ç¡®çš„*Î¸*å€¼é™„è¿‘ï¼Œè€Œ`model_0`åˆ™å¯¹æ‰€æœ‰å¯èƒ½çš„*Î¸*å€¼èµ‹äºˆç›¸åŒçš„æ¦‚ç‡ã€‚è¿™ç§æ¨¡å‹ä¹‹é—´çš„å·®å¼‚ç”±è´å¶æ–¯å› å­æ•æ‰åˆ°ã€‚æˆ‘ä»¬å¯ä»¥è¯´ï¼Œè´å¶æ–¯å› å­è¡¡é‡çš„æ˜¯å“ªä¸ªæ¨¡å‹ä½œä¸ºæ•´ä½“æ›´é€‚åˆè§£é‡Šæ•°æ®ã€‚è¿™åŒ…æ‹¬äº†å…ˆéªŒçš„ç»†èŠ‚ï¼Œæ— è®ºæ¨¡å‹é¢„æµ‹å¤šä¹ˆç›¸ä¼¼ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ¯”è¾ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¹¶ä¸å…³å¿ƒè¿™äº›ç»†èŠ‚ï¼Œåè€Œæ›´å€¾å‘äºè¯„ä¼°æ¨¡å‹é¢„æµ‹çš„ç›¸ä¼¼åº¦ã€‚å¯¹äºè¿™äº›æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨LOOã€‚
- en: 5.8 Regularizing priors
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 æ­£åˆ™åŒ–å…ˆéªŒ
- en: Using informative and weakly informative priors is a way of introducing bias
    in a model and, if done properly, this can be really good because bias prevents
    overfitting and thus contributes to models being able to make predictions that
    generalize well. This idea of adding a bias element to reduce generalization errors
    without affecting the ability of the model to adequately model a problem is known
    as **regularization**. This regularization often takes the form of a term penalizing
    certain values for the parameters in a model, like too-big coefficients in a regression
    model. Restricting parameter values is a way of reducing the data a model can
    represent, thus reducing the chances that a model will capture noise instead of
    the signal.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¿¡æ¯ä¸°å¯Œå’Œå¼±ä¿¡æ¯å…ˆéªŒæ˜¯ä¸€ç§åœ¨æ¨¡å‹ä¸­å¼•å…¥åå·®çš„æ–¹æ³•ï¼Œå¦‚æœåšå¾—æ°å½“ï¼Œè¿™å®é™…ä¸Šæ˜¯éå¸¸æœ‰ç›Šçš„ï¼Œå› ä¸ºåå·®å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä»è€Œå¸®åŠ©æ¨¡å‹è¿›è¡Œå…·æœ‰è‰¯å¥½æ³›åŒ–èƒ½åŠ›çš„é¢„æµ‹ã€‚è¿™ç§å‘æ¨¡å‹ä¸­åŠ å…¥åå·®å…ƒç´ ä»¥å‡å°‘æ³›åŒ–è¯¯å·®è€Œä¸å½±å“æ¨¡å‹å……åˆ†å»ºæ¨¡é—®é¢˜èƒ½åŠ›çš„æƒ³æ³•è¢«ç§°ä¸º**æ­£åˆ™åŒ–**ã€‚è¿™ç§æ­£åˆ™åŒ–é€šå¸¸è¡¨ç°ä¸ºå¯¹æ¨¡å‹ä¸­æŸäº›å‚æ•°å€¼è¿›è¡Œæƒ©ç½šçš„é¡¹ï¼Œä¾‹å¦‚åœ¨å›å½’æ¨¡å‹ä¸­å¯¹è¿‡å¤§çš„ç³»æ•°è¿›è¡Œæƒ©ç½šã€‚é™åˆ¶å‚æ•°å€¼æ˜¯å‡å°‘æ¨¡å‹èƒ½å¤Ÿè¡¨ç¤ºçš„æ•°æ®é‡çš„ä¸€ç§æ–¹å¼ï¼Œä»è€Œå‡å°‘æ¨¡å‹æ•æ‰å™ªå£°è€Œéä¿¡å·çš„å¯èƒ½æ€§ã€‚
- en: This regularization idea is so powerful and useful that it has been discovered
    several times, including outside the Bayesian framework. For regression models,
    and outside Bayesian statistics, two popular regularization methods are ridge
    regression and lasso regression. From the Bayesian point of view, ridge regression
    can be interpreted as using Normal distributions for the *Î²* coefficients of a
    linear model, with a small standard deviation that pushes the coefficients toward
    zero. In this sense, we have been doing something very close to ridge regression
    for every single linear model in this book (except the examples in this chapter
    that use SciPy!).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ­£åˆ™åŒ–æ€æƒ³éå¸¸å¼ºå¤§ä¸”æœ‰ç”¨ï¼Œä»¥è‡³äºå®ƒåœ¨è´å¶æ–¯æ¡†æ¶ä¹‹å¤–ä¹Ÿè¢«å¤šæ¬¡å‘ç°ã€‚å¯¹äºå›å½’æ¨¡å‹ï¼Œä¸”ä¸å±€é™äºè´å¶æ–¯ç»Ÿè®¡ï¼Œä¸¤ä¸ªæµè¡Œçš„æ­£åˆ™åŒ–æ–¹æ³•æ˜¯å²­å›å½’å’Œlassoå›å½’ã€‚ä»è´å¶æ–¯è§’åº¦æ¥çœ‹ï¼Œå²­å›å½’å¯ä»¥è§£é‡Šä¸ºå¯¹çº¿æ€§æ¨¡å‹çš„*Î²*ç³»æ•°ä½¿ç”¨æ­£æ€åˆ†å¸ƒä½œä¸ºå…ˆéªŒï¼Œå…¶ä¸­çš„æ ‡å‡†å·®è¾ƒå°ï¼Œä½¿å¾—ç³»æ•°è¶‹å‘äºé›¶ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè®²ï¼Œæˆ‘ä»¬åœ¨æœ¬ä¹¦ä¸­çš„æ¯ä¸ªçº¿æ€§æ¨¡å‹ï¼ˆé™¤äº†æœ¬ç« ä¸­ä½¿ç”¨SciPyçš„ä¾‹å­ï¼‰å®é™…ä¸Šéƒ½åœ¨åšç±»ä¼¼å²­å›å½’çš„äº‹æƒ…ï¼
- en: On the other hand, lasso regression can be interpreted from a Bayesian point
    of view as the MAP of the posterior computed from a model with Laplace priors
    for the *Î²* coefficients. The Laplace distribution looks similar to the Gaussian
    distribution but with a sharp peak at zero. You can also interpret it as two *back-to-back*
    Exponential distributions (try `pz.Laplace(0, 1).plot_pdf()`). The Laplace distribution
    concentrates its probability mass much closer to zero compared to the Gaussian
    distribution. The idea of using such a prior is to provide both regularization
    and variable selection. The idea is that since we have this peak at zero, we expect
    the prior distribution to induce sparsity, that is, we create a model with a lot
    of parameters and the prior will automatically make most of them zero, keeping
    only the relevant variables contributing to the output of the model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œlassoå›å½’å¯ä»¥ä»è´å¶æ–¯è§’åº¦è§£é‡Šä¸ºé€šè¿‡ä»å…·æœ‰æ‹‰æ™®æ‹‰æ–¯å…ˆéªŒçš„æ¨¡å‹ä¸­è®¡ç®—çš„åéªŒçš„æœ€å¤§åéªŒä¼°è®¡ï¼ˆMAPï¼‰ï¼Œå…¶ä¸­*Î²*ç³»æ•°ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒä½œä¸ºå…ˆéªŒã€‚æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçœ‹èµ·æ¥ç±»ä¼¼äºé«˜æ–¯åˆ†å¸ƒï¼Œä½†åœ¨é›¶ç‚¹å¤„æœ‰ä¸€ä¸ªå°–é”çš„å³°å€¼ã€‚ä½ ä¹Ÿå¯ä»¥å°†å…¶è§£é‡Šä¸ºä¸¤ä¸ª*èƒŒå¯¹èƒŒ*çš„æŒ‡æ•°åˆ†å¸ƒï¼ˆè¯•è¯•`pz.Laplace(0,
    1).plot_pdf()`ï¼‰ã€‚ä¸é«˜æ–¯åˆ†å¸ƒç›¸æ¯”ï¼Œæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡æ›´é›†ä¸­åœ¨é›¶é™„è¿‘ã€‚ä½¿ç”¨è¿™ç§å…ˆéªŒçš„æ€æƒ³æ˜¯æä¾›æ­£åˆ™åŒ–å’Œå˜é‡é€‰æ‹©ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äºæˆ‘ä»¬åœ¨é›¶ç‚¹å¤„æœ‰ä¸€ä¸ªå³°å€¼ï¼Œæˆ‘ä»¬æœŸæœ›å…ˆéªŒåˆ†å¸ƒèƒ½å¤Ÿå¼•å…¥ç¨€ç–æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå…·æœ‰å¤§é‡å‚æ•°çš„æ¨¡å‹ï¼Œè€Œå…ˆéªŒä¼šè‡ªåŠ¨ä½¿å¤§å¤šæ•°å‚æ•°ä¸ºé›¶ï¼Œä»…ä¿ç•™å¯¹æ¨¡å‹è¾“å‡ºæœ‰è´¡çŒ®çš„ç›¸å…³å˜é‡ã€‚
- en: 'Unfortunately, contrary to ridge regression, this idea does not directly translate
    from the frequentist realm to the Bayesian one. Nevertheless, there are Bayesian
    priors that can be used for inducing sparsity and performing variable selection,
    like the horseshoe prior. If you want to learn more about the horseshoe and other
    shrinkage priors, you may find the article by [Piironen and Vehtari](Bibliography.xhtml#XPiironen2017)Â [[2017](Bibliography.xhtml#XPiironen2017)]
    at [https://arxiv.org/abs/1707.01694](https://arxiv.org/abs/1707.01694) very interesting.
    In the next chapter, we will discuss more about variable selection. Just one final
    note: it is important to notice that the classical versions of ridge and lasso
    regressions correspond to single-point estimates, while the Bayesian versions
    yield full posterior distributions.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œä¸å²­å›å½’ä¸åŒï¼Œè¿™ä¸€æ€æƒ³å¹¶ä¸èƒ½ç›´æ¥ä»é¢‘ç‡å­¦æ´¾è½¬åŒ–åˆ°è´å¶æ–¯å­¦æ´¾ã€‚ç„¶è€Œï¼Œç¡®å®å­˜åœ¨ä¸€äº›è´å¶æ–¯å…ˆéªŒå¯ä»¥ç”¨æ¥å¼•å…¥ç¨€ç–æ€§å¹¶æ‰§è¡Œå˜é‡é€‰æ‹©ï¼Œæ¯”å¦‚é©¬éå…ˆéªŒã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºé©¬éå…ˆéªŒå’Œå…¶ä»–æ”¶ç¼©å‹å…ˆéªŒçš„ä¿¡æ¯ï¼Œä½ å¯ä»¥å‚è€ƒ[Piironenå’ŒVehtari](Bibliography.xhtml#XPiironen2017)çš„æ–‡ç« [[2017](Bibliography.xhtml#XPiironen2017)]ï¼Œå®ƒå¯ä»¥åœ¨[https://arxiv.org/abs/1707.01694](https://arxiv.org/abs/1707.01694)æ‰¾åˆ°ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥è®¨è®ºå˜é‡é€‰æ‹©ã€‚æœ€åè¡¥å……ä¸€å¥ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå²­å›å½’å’Œlassoå›å½’çš„ç»å…¸ç‰ˆæœ¬å¯¹åº”äºå•ç‚¹ä¼°è®¡ï¼Œè€Œè´å¶æ–¯ç‰ˆæœ¬åˆ™äº§ç”Ÿå®Œæ•´çš„åéªŒåˆ†å¸ƒã€‚
- en: 5.9 Summary
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.9 æ€»ç»“
- en: In this chapter, we have seen how to compare models using posterior predictive
    checks, information criteria, approximated cross-validation, and Bayes factors.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°å¦‚ä½•ä½¿ç”¨åéªŒé¢„æµ‹æ£€æŸ¥ã€ä¿¡æ¯å‡†åˆ™ã€è¿‘ä¼¼äº¤å‰éªŒè¯å’Œè´å¶æ–¯å› å­æ¥æ¯”è¾ƒæ¨¡å‹ã€‚
- en: Posterior predictive check is a general concept and practice that can help us
    understand how well models are capturing different aspects of the data. We can
    perform posterior predictive checks with just one model or with many models, and
    thus we can use it as a method for model comparison. Posterior predictive checks
    are generally done via visualizations, but numerical summaries like Bayesian values
    can also be helpful.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒé¢„æµ‹æ£€æŸ¥æ˜¯ä¸€ä¸ªé€šç”¨æ¦‚å¿µå’Œå®è·µï¼Œå®ƒå¸®åŠ©æˆ‘ä»¬ç†è§£æ¨¡å‹åœ¨æ•æ‰æ•°æ®çš„ä¸åŒæ–¹é¢æ–¹é¢çš„è¡¨ç°ã€‚æˆ‘ä»¬å¯ä»¥ä»…ç”¨ä¸€ä¸ªæ¨¡å‹æˆ–å¤šä¸ªæ¨¡å‹è¿›è¡ŒåéªŒé¢„æµ‹æ£€æŸ¥ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†å…¶ç”¨ä½œæ¨¡å‹æ¯”è¾ƒçš„æ–¹æ³•ã€‚åéªŒé¢„æµ‹æ£€æŸ¥é€šå¸¸é€šè¿‡å¯è§†åŒ–æ–¹å¼è¿›è¡Œï¼Œä½†åƒè´å¶æ–¯å€¼è¿™æ ·çš„æ•°å€¼æ‘˜è¦ä¹Ÿèƒ½æä¾›å¸®åŠ©ã€‚
- en: 'Good models have a good balance between complexity and predictive accuracy.
    We exemplified this feature by using the classical example of polynomial regression.
    We discussed two methods to estimate the out-of-sample accuracy without leaving
    data aside: cross-validation and information criteria. From a practical point
    of view, information criteria is a family of theoretical methods looking to balance
    two contributions: a measurement of how well a model fits the data and a penalization
    term for complex models. We briefly discussed AIC, for its historical importance,
    and then WAIC, which is a better method for Bayesian models as it takes into account
    the entire posterior distribution and uses a more sophisticated method to compute
    the effective number of parameters.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„æ¨¡å‹åœ¨å¤æ‚åº¦å’Œé¢„æµ‹å‡†ç¡®æ€§ä¹‹é—´æœ‰ä¸€ä¸ªè‰¯å¥½çš„å¹³è¡¡ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç»å…¸çš„å¤šé¡¹å¼å›å½’ä¾‹å­æ¥å±•ç¤ºè¿™ä¸€ç‰¹ç‚¹ã€‚æˆ‘ä»¬è®¨è®ºäº†ä¸¤ç§æ–¹æ³•æ¥ä¼°è®¡å¤–æ ·æœ¬å‡†ç¡®æ€§ï¼Œè€Œä¸éœ€è¦å°†æ•°æ®æ’é™¤åœ¨å¤–ï¼šäº¤å‰éªŒè¯å’Œä¿¡æ¯å‡†åˆ™ã€‚ä»å®é™…è§’åº¦æ¥çœ‹ï¼Œä¿¡æ¯å‡†åˆ™æ˜¯ä¸€ç±»ç†è®ºæ–¹æ³•ï¼Œæ—¨åœ¨å¹³è¡¡ä¸¤ä¸ªæ–¹é¢çš„è´¡çŒ®ï¼šè¡¡é‡æ¨¡å‹å¦‚ä½•æ‹Ÿåˆæ•°æ®çš„åº¦é‡å’Œå¯¹å¤æ‚æ¨¡å‹çš„æƒ©ç½šé¡¹ã€‚æˆ‘ä»¬ç®€è¦è®¨è®ºäº†AICï¼Œå› å…¶å†å²é‡è¦æ€§ï¼Œç„¶åè®¨è®ºäº†WAICï¼Œå®ƒæ˜¯è´å¶æ–¯æ¨¡å‹çš„æ›´å¥½æ–¹æ³•ï¼Œå› ä¸ºå®ƒè€ƒè™‘äº†æ•´ä¸ªåéªŒåˆ†å¸ƒï¼Œå¹¶ä½¿ç”¨æ›´å¤æ‚çš„æ–¹æ³•æ¥è®¡ç®—æœ‰æ•ˆå‚æ•°æ•°ç›®ã€‚
- en: We also discussed cross-validation, and we saw we can approximate leave-one-out
    cross-validation using LOO. Both WAIC and LOO tend to produce very similar results,
    but LOO can be more reliable. So we recommend its use. Both WAIC and LOO can be
    used for model selection and model averaging. Instead of selecting a single best
    model, model averaging is about combining all available models by taking a weighted
    average of them.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è®¨è®ºäº†äº¤å‰éªŒè¯ï¼Œçœ‹åˆ°æˆ‘ä»¬å¯ä»¥ä½¿ç”¨LOOæ¥è¿‘ä¼¼ç•™ä¸€æ³•äº¤å‰éªŒè¯ã€‚WAICå’ŒLOOé€šå¸¸ä¼šäº§ç”Ÿéå¸¸ç›¸ä¼¼çš„ç»“æœï¼Œä½†LOOå¯èƒ½æ›´ä¸ºå¯é ã€‚æ‰€ä»¥æˆ‘ä»¬æ¨èä½¿ç”¨å®ƒã€‚WAICå’ŒLOOéƒ½å¯ä»¥ç”¨äºæ¨¡å‹é€‰æ‹©å’Œæ¨¡å‹å¹³å‡ã€‚ä¸å…¶é€‰æ‹©ä¸€ä¸ªæœ€ä½³æ¨¡å‹ï¼Œæ¨¡å‹å¹³å‡æ˜¯é€šè¿‡åŠ æƒå¹³å‡æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æ¥å®ç°çš„ã€‚
- en: 'A different approach to model selection, comparison, and model averaging is
    Bayes factors, which are the ratio of the marginal likelihoods of two models.
    Bayes factor computations can be really challenging. In this chapter, we showed
    two routes to compute them with PyMC and ArviZ: using the sampling method known
    as Sequential Monte Carlo and using the Savageâ€“Dickey ratio. The first method
    can be used for any model as long as Sequential Monte Carlo provides a good posterior.
    With the current implementation of SMC in PyMC, this can be challenging for high-dimensional
    models or hierarchical models. The second method can only be used when the null
    model is a particular case of the alternative model. Besides being computationally
    challenging, Bayes factors are problematic to use given that they are very (overly)
    sensitive to prior specifications.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹é€‰æ‹©ã€æ¯”è¾ƒå’Œæ¨¡å‹å¹³å‡çš„å¦ä¸€ç§æ–¹æ³•æ˜¯è´å¶æ–¯å› å­ï¼Œå®ƒæ˜¯ä¸¤ä¸ªæ¨¡å‹çš„è¾¹é™…ä¼¼ç„¶æ¯”ã€‚è´å¶æ–¯å› å­çš„è®¡ç®—å¯èƒ½éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨PyMCå’ŒArviZè®¡ç®—è´å¶æ–¯å› å­çš„ä¸¤ç§æ–¹æ³•ï¼šä¸€ç§æ˜¯ä½¿ç”¨è¢«ç§°ä¸ºé¡ºåºè’™ç‰¹å¡æ´›çš„æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯ä½¿ç”¨è¨ç»´å¥‡â€“è¿ªåŸºæ¯”ç‡ã€‚ç¬¬ä¸€ç§æ–¹æ³•å¯ä»¥ç”¨äºä»»ä½•æ¨¡å‹ï¼Œåªè¦é¡ºåºè’™ç‰¹å¡æ´›æä¾›äº†è‰¯å¥½çš„åéªŒåˆ†å¸ƒã€‚ç”±äºPyMCä¸­SMCçš„å½“å‰å®ç°ï¼Œå¯¹äºé«˜ç»´æ¨¡å‹æˆ–å±‚æ¬¡æ¨¡å‹æ¥è¯´ï¼Œè¿™å¯èƒ½ä¼šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç¬¬äºŒç§æ–¹æ³•ä»…åœ¨åŸå‡è®¾æ¨¡å‹æ˜¯å¤‡æ‹©æ¨¡å‹çš„ç‰¹å®šæƒ…å†µæ—¶æ‰å¯ä½¿ç”¨ã€‚é™¤äº†è®¡ç®—ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§å¤–ï¼Œè´å¶æ–¯å› å­çš„ä½¿ç”¨ä¹Ÿå­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬å¯¹å…ˆéªŒè®¾å®šéå¸¸ï¼ˆè¿‡åº¦ï¼‰æ•æ„Ÿã€‚
- en: We have shown that Bayes factors and LOO/WAIC are the answers to two related
    but different questions. The former is focused on identifying the right model
    and the other is on identifying the model with lower generalization loss, i.e.,
    the model making the best predictions. None of these methods are free of problems,
    but WAIC, and in particular LOO, are much more robust than the others in practice.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å±•ç¤ºäº†è´å¶æ–¯å› å­å’ŒLOO/WAICæ˜¯å›ç­”ä¸¤ä¸ªç›¸å…³ä½†ä¸åŒé—®é¢˜çš„å·¥å…·ã€‚å‰è€…ä¾§é‡äºè¯†åˆ«æ­£ç¡®çš„æ¨¡å‹ï¼Œè€Œåè€…åˆ™ä¾§é‡äºè¯†åˆ«å…·æœ‰è¾ƒä½æ³›åŒ–æŸå¤±çš„æ¨¡å‹ï¼Œå³åšå‡ºæœ€ä½³é¢„æµ‹çš„æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•éƒ½ä¸æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œä½†WAICï¼Œç‰¹åˆ«æ˜¯LOOï¼Œåœ¨å®è·µä¸­æ¯”å…¶ä»–æ–¹æ³•æ›´ä¸ºç¨³å¥ã€‚
- en: 5.10 Exercises
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.10 ç»ƒä¹ 
- en: This exercise is about regularization priors. In the code that generates the
    `x_c, y_c` data (see [https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)),
    change `order=2` to another value, such as `order=5`. Then, fit `model_q` and
    plot the resulting curve. Repeat this, but now using a prior for *Î²* with `sd=100`
    instead of `sd=1` and plot the resulting curve. How do the curves differ? Try
    this out with `sd=np.array([10, 0.1, 0.1, 0.1, 0.1])`, too.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ¬ç»ƒä¹ æ¶‰åŠæ­£åˆ™åŒ–å…ˆéªŒã€‚åœ¨ç”Ÿæˆ`x_c, y_c`æ•°æ®çš„ä»£ç ä¸­ï¼ˆè§[https://github.com/aloctavodia/BAP3](https://github.com/aloctavodia/BAP3)ï¼‰ï¼Œå°†`order=2`æ›´æ”¹ä¸ºå…¶ä»–å€¼ï¼Œä¾‹å¦‚`order=5`ã€‚ç„¶åï¼Œæ‹Ÿåˆ`model_q`å¹¶ç»˜åˆ¶ç»“æœæ›²çº¿ã€‚é‡å¤æ­¤æ­¥éª¤ï¼Œä½†è¿™æ¬¡ä½¿ç”¨`sd=100`çš„*Î²*å…ˆéªŒï¼Œè€Œä¸æ˜¯`sd=1`ï¼Œå¹¶ç»˜åˆ¶ç»“æœæ›²çº¿ã€‚è¿™äº›æ›²çº¿æœ‰ä½•ä¸åŒï¼Ÿä¹Ÿè¯•è¯•ä½¿ç”¨`sd=np.array([10,
    0.1, 0.1, 0.1, 0.1])`ã€‚
- en: Repeat the previous exercise but increase the amount of data to 500 data points.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ä¹‹å‰çš„ç»ƒä¹ ï¼Œä½†å°†æ•°æ®é‡å¢åŠ åˆ°500ä¸ªæ•°æ®ç‚¹ã€‚
- en: Fit a cubic model (order 3), compute WAIC and LOO, plot the results, and compare
    them with the linear and quadratic models.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‹Ÿåˆä¸€ä¸ªä¸‰æ¬¡æ¨¡å‹ï¼ˆé˜¶æ•°3ï¼‰ï¼Œè®¡ç®— WAIC å’Œ LOOï¼Œç»˜åˆ¶ç»“æœï¼Œå¹¶ä¸çº¿æ€§å’ŒäºŒæ¬¡æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: Use `pm.sample_posterior_predictive()` to rerun the PPC example, but this time,
    plot the values of `y` instead of the values of the mean.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `pm.sample_posterior_predictive()` é‡æ–°è¿è¡Œ PPC ç¤ºä¾‹ï¼Œä½†è¿™æ¬¡ç»˜åˆ¶ `y` çš„å€¼ï¼Œè€Œä¸æ˜¯å‡å€¼çš„å€¼ã€‚
- en: Read and run the posterior predictive example from PyMCâ€™s documentation at [https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html).
    Pay special attention to the use of shared variables and `pm.MutableData`.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é˜…è¯»å¹¶è¿è¡Œ PyMC æ–‡æ¡£ä¸­çš„åéªŒé¢„æµ‹ç¤ºä¾‹ï¼Œé“¾æ¥ï¼š[https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html](https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/posterior_predictive.html)ã€‚ç‰¹åˆ«æ³¨æ„å…±äº«å˜é‡å’Œ
    `pm.MutableData` çš„ä½¿ç”¨ã€‚
- en: Go back to the code that generated *Figure [5.5](#x1-98002r5)* and *Figure [5.6](#x1-98004r6)*
    and modify it to get new sets of six data points. Visually evaluate how the different
    polynomials fit these new datasets. Relate the results to the discussions in this
    book.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿”å›ç”Ÿæˆ*å›¾ [5.5](#x1-98002r5)* å’Œ *å›¾ [5.6](#x1-98004r6)* çš„ä»£ç ï¼Œå¹¶ä¿®æ”¹å®ƒä»¥è·å–æ–°çš„å…­ä¸ªæ•°æ®ç‚¹é›†ã€‚ç›´è§‚è¯„ä¼°ä¸åŒçš„å¤šé¡¹å¼å¦‚ä½•æ‹Ÿåˆè¿™äº›æ–°æ•°æ®é›†ã€‚å°†ç»“æœä¸æœ¬ä¹¦ä¸­çš„è®¨è®ºè”ç³»èµ·æ¥ã€‚
- en: Read and run the model averaging example from PyMCâ€™s documentation at [https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html).
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é˜…è¯»å¹¶è¿è¡Œ PyMC æ–‡æ¡£ä¸­çš„æ¨¡å‹å¹³å‡ç¤ºä¾‹ï¼Œé“¾æ¥ï¼š[https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html](https://www.pymc.io/projects/examples/en/latest/diagnostics_and_criticism/model_averaging.html)ã€‚
- en: Compute the Bayes factor for the coin problem using a uniform prior, Beta(1,
    1), and priors such as Beta(0.5, 0.5). Set 15 heads and 30 coins. Compare this
    result with the inference we got in the first chapter of this book.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‡åŒ€å…ˆéªŒ Beta(1, 1) å’Œå¦‚ Beta(0.5, 0.5) ç­‰å…ˆéªŒè®¡ç®—ç¡¬å¸é—®é¢˜çš„è´å¶æ–¯å› å­ã€‚è®¾å®š15æ¬¡æ­£é¢å’Œ30æšç¡¬å¸ã€‚å°†æ­¤ç»“æœä¸æœ¬ä¹¦ç¬¬ä¸€ç« çš„æ¨æ–­ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚
- en: Repeat the last example where we compare Bayes factors and Information Criteria,
    but now reduce the sample size.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤ä¸Šä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬æ¯”è¾ƒè´å¶æ–¯å› å­å’Œä¿¡æ¯å‡†åˆ™ï¼Œä½†è¿™æ¬¡å‡å°‘æ ·æœ¬é‡ã€‚
- en: Join our community Discord space
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒº Discord ç©ºé—´
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„ Discord ç¤¾åŒºï¼Œç»“è¯†å¿—åŒé“åˆçš„äººï¼Œå¹¶ä¸è¶…è¿‡5000åæˆå‘˜ä¸€èµ·å­¦ä¹ ï¼Œé“¾æ¥ï¼š[https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
