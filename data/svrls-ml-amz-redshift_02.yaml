- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Data Loading and Analytics on Redshift Serverless
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Redshift Serverless上加载数据和分析
- en: In the previous chapter, we introduced you to **Amazon Redshift Serverless**
    and demonstrated how to create a serverless endpoint from the Amazon Redshift
    console. We also explained how to connect and query your data warehouse using
    **Amazon Redshift query editor v** In this chapter, we will dive deeper into the
    different ways you can load data into your Amazon Redshift Serverless data warehouse.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们向您介绍了**Amazon Redshift Serverless**，并演示了如何从Amazon Redshift控制台创建无服务器端点。我们还解释了如何使用**Amazon
    Redshift查询编辑器v**连接和查询您的数据仓库。在本章中，我们将更深入地探讨您可以将数据加载到Amazon Redshift Serverless数据仓库的不同方法。
- en: We will cover three main topics in this chapter to help you load your data efficiently
    into Redshift Serverless. First, we will demonstrate how to load data using Amazon
    Redshift query editor v where you will learn how to load data from your Amazon
    S3 bucket and local data file onto your computer using the GUI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖三个主要主题，以帮助您高效地将数据加载到Redshift Serverless中。首先，我们将演示如何使用Amazon Redshift查询编辑器v加载数据，您将学习如何使用GUI将数据从您的Amazon
    S3存储桶和本地数据文件加载到您的计算机上。
- en: Next, we will explore the `COPY` command in detail, and you will learn how to
    load a file by writing a `COPY` command to load the data. We will cover everything
    you need to know to use this command effectively and load your data smoothly into
    Redshift Serverless.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细探讨`COPY`命令，您将学习如何通过编写`COPY`命令来加载数据。我们将涵盖您需要了解的所有内容，以有效地使用此命令并将数据顺利加载到Redshift
    Serverless中。
- en: Finally, we will cover the built-in native API interface to access and load
    data into your Redshift Serverless endpoint using Jupyter Notebook. We will guide
    you through the process of setting up and using the **Redshift** **Data API**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将介绍内置的本地API接口，使用Jupyter Notebook访问和将数据加载到您的Redshift Serverless端点。我们将指导您完成设置和使用**Redshift**
    **数据API**的过程。
- en: 'The topics are as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 主题如下：
- en: Data loading using Amazon Redshift query editor v
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon Redshift查询编辑器v加载数据
- en: Data loading from Amazon S3 using the COPY command
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用COPY命令从Amazon S3加载数据
- en: Data loading using the Redshift Data API
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Redshift数据API加载数据
- en: The goal of this chapter is to equip you with the knowledge and skills to load
    data into Amazon Redshift Serverless using different mechanisms. By the end of
    this chapter, you will be able to load data quickly and efficiently into Redshift
    Serverless using the methods covered in this chapter, which will enable you to
    perform analytics on your data and extract valuable insights.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是让您掌握使用不同机制将数据加载到Amazon Redshift Serverless的知识和技能。到本章结束时，您将能够使用本章介绍的方法快速有效地将数据加载到Redshift
    Serverless中，这将使您能够对数据进行分析并提取有价值的见解。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires a web browser and access to the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要网络浏览器以及访问以下内容：
- en: An AWS account
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个AWS账户
- en: Amazon Redshift
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Redshift
- en: Amazon Redshift Query Editor v2
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Redshift查询编辑器v2
- en: Amazon SageMaker for Jupyter Notebook
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon SageMaker for Jupyter Notebook
- en: The code snippets in this chapter are available in this book’s GitHub repository
    at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码片段可在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2).
- en: 'The data files used in this chapter can be found in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的数据文件可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).
- en: Data loading using Amazon Redshift Query Editor v2
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon Redshift查询编辑器v2加载数据
- en: Query Editor v2 supports different database actions, including **data definition
    language** (**DDL**), to create schema and tables and load data from data files
    with just a click of a button. Let’s take a look at how you can carry out these
    tasks to enable easy analytics on your data warehouse. Log in to your AWS console,
    navigate to your Amazon Redshift Serverless endpoint, and select **Query data**.
    This will open **Redshift query editor v2** in a new tab. Using the steps we followed
    in [*Chapter 1*](B19071_01.xhtml#_idTextAnchor015), log in to your database and
    perform the tasks outlined in the following subsections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Query Editor v2 支持不同的数据库操作，包括 **数据定义语言** (**DDL**)，只需点击一下按钮即可创建架构和表，并从数据文件中加载数据。让我们看看如何执行这些任务，以便在您的数据仓库上轻松进行数据分析。登录到您的
    AWS 控制台，导航到您的 Amazon Redshift Serverless 端点，并选择 **查询数据**。这将在新标签页中打开 **Redshift
    查询编辑器 v2**。按照我们在 [*第 1 章*](B19071_01.xhtml#_idTextAnchor015) 中遵循的步骤，登录到您的数据库并执行以下小节中概述的任务。
- en: Creating tables
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建表
- en: 'Query editor v2 provides a wizard to execute the DDL commands shown in *Figure
    2**.1*. Let’s create a new schema named `chapter2` first:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Query editor v2 提供了一个向导来执行图 *图 2**.1* 中显示的 DDL 命令。首先，让我们创建一个名为 `chapter2` 的新架构：
- en: Click on **Create** and select **Schema**, as shown here.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建** 并选择 **架构**，如图所示。
- en: '![Figure 2.1 – The creation wizard](img/B19071_02_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 创建向导](img/B19071_02_001.jpg)'
- en: Figure 2.1 – The creation wizard
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 创建向导
- en: Ensure that your `chapter2`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的 `chapter2`。
- en: 'Then, click on **Create schema**, as shown in *Figure 2**.2*:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击 **创建架构**，如图 *图 2**.2* 所示。
- en: '![Figure 2.2 – Create schema](img/B19071_02_002.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – 创建架构](img/B19071_02_002.jpg)'
- en: Figure 2.2 – Create schema
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 创建架构
- en: Once you have your schema created, navigate to the `chapter2` in the `customer`.
    With Query Editor v2, you can either enter the column names and their data type
    manually, or you can use the data file to automatically infer the column names
    and their data type.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了架构，导航到 `customer` 中的 `chapter2`。使用 Query Editor v2，您可以手动输入列名及其数据类型，或者可以使用数据文件自动推断列名及其数据类型。
- en: Let’s create a table with a data file. We will use `customer.csv`, which is
    available in this book’s GitHub repository at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).
    You can download this file locally to create the table using the wizard.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用数据文件创建一个表。我们将使用 `customer.csv`，该文件可在本书的 GitHub 仓库中找到，网址为 [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2)。您可以将此文件本地下载以使用向导创建表。
- en: 'The file contains a subset of the data from the `TPC-H` dataset, available
    in this book''s GitHub repository: [https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件包含 `TPC-H` 数据集的子集，可在本书的 GitHub 仓库中找到：[https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH)。
- en: 'On the **Create table** wizard, click on **Load from CSV** under the **Columns**
    tab, and provide a path to the CSV file. Once the file is selected, the schema
    will be inferred and automatically populated from the file, as shown in *Figure
    2**.3*. Optionally, you can modify the schema in the **Column name**, **Data type**,
    and **Encoding** fields, and under **Column options**, you can select different
    options such as the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **创建表** 向导中，在 **列** 选项卡下点击 **从 CSV 加载**，并提供 CSV 文件的路径。一旦选择了文件，架构将从文件中推断并自动填充，如图
    *图 2**.3* 所示。可选地，您可以在 **列名**、**数据类型**和**编码**字段中修改架构，在**列选项**下，您可以选择以下选项：
- en: Choose a default value for the column.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为列选择一个默认值。
- en: Optionally, you can turn on **Automatically increment** if you want the column
    values to increment. If you enable this option, only then can you specify a value
    for **Auto increment seed** and **Auto** **increment step**.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选地，如果您想使列值递增，可以打开 **自动递增**。如果启用此选项，则可以指定 **自动递增种子** 和 **自动递增步长** 的值。
- en: Enter a size value for the column.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为列输入一个大小值。
- en: You also have the option to define constraints such as **Not NULL**, **Primary
    key**, and **Unique key**.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以定义约束，例如 **非空**、**主键**和**唯一键**。
- en: '![Figure 2.3 – Create table](img/B19071_02_003.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 创建表](img/B19071_02_003.jpg)'
- en: Figure 2.3 – Create table
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 创建表
- en: Additionally, as shown in *Figure 2**.4*, under the **Table details** tab, you
    can optionally set the table properties, such as **Distribution key**, **Distribution
    style**, **Sort key**, and **Sort type**. When these options are not set, Redshift
    will pick default settings for you, which are **Auto Distribution Key** and **Auto**
    **Sort Key**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如图**图2**.4所示，在**表详情**选项卡下，您可以可选地设置表属性，例如**分布键**、**分布样式**、**排序键**和**排序类型**。当这些选项未设置时，Redshift将为您选择默认设置，即**自动分布键**和**自动**
    **排序键**。
- en: '![Figure 2.4 – Table details](img/B19071_02_004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 表详情](img/B19071_02_004.jpg)'
- en: Figure 2.4 – Table details
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 表详情
- en: Amazon Redshift distributes data in a table according to the table’s distribution
    style (`DISTSTYLE`). The data rows are distributed within each compute node according
    to the number of slices. When you run a query against the table, all the slices
    of the compute node process the rows that are assigned in parallel. As a best
    practice ([https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html)),
    select a table’s `DISTSTYLE` parameter to ensure even distribution of the data
    or use automatic distribution.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift根据表的分布样式（`DISTSTYLE`）在表中分布数据。数据行根据每个计算节点上的切片数量在各个计算节点内分布。当您对表运行查询时，计算节点的所有切片并行处理分配的行。作为一个最佳实践（[https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html)），选择表的`DISTSTYLE`参数以确保数据的均匀分布，或使用自动分布。
- en: Amazon Redshift orders data within each slice using the table’s sort key. Amazon
    Redshift also enables you to define a table with compound sort keys, interleaved
    sort keys, or no sort keys. As a best practice ([https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html)),
    define the sort keys and style according to your data access pattern. Having a
    proper sort key defined on a table can hugely improve your query performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift使用表的排序键在每个切片内排序数据。Amazon Redshift还允许您定义具有复合排序键、交错排序键或没有排序键的表。作为一个最佳实践（[https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html](https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html)），根据您的数据访问模式定义排序键和样式。在表上定义适当的排序键可以极大地提高您的查询性能。
- en: 'Lastly, under **Other options** you can select the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在**其他选项**下，您可以选择以下内容：
- en: Whether to include your table in automated and manual snapshots
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否将您的表包含在自动和手动快照中
- en: Whether to create a session-based temporary table instead of a permanent database
    table
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否创建基于会话的临时表而不是永久数据库表
- en: Once you have entered all the details, you can view the DDL of your table by
    clicking **Open query in editor**. You can use this later or even share it with
    other users.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您已输入所有详细信息，您可以通过单击**在编辑器中打开查询**来查看您表的DDL。您可以使用它稍后或甚至与其他用户共享。
- en: Now, let’s create our table by clicking on the **Create table** button (*Figure
    2**.4*).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过单击**创建表**按钮（*图2**.4*）来创建我们的表。
- en: As you can see, it is easy for any data scientist, analyst, or user to use this
    wizard to create database objects (such as tables) without having to write DDL
    and enter each column's data type and its length.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，任何数据科学家、分析师或用户都可以使用此向导创建数据库对象（如表），而无需编写DDL或输入每一列的数据类型及其长度。
- en: Let’s now work on loading data in the customer table. Query Editor v2 enables
    you to load data from Amazon S3 or the local file on your computer. Please note
    that, at the time of writing, the option to load a local file currently supports
    only CSV files with a maximum size of 5 MB.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在客户表中加载数据。查询编辑器v2允许您从Amazon S3或您计算机上的本地文件加载数据。请注意，在撰写本文时，加载本地文件选项目前仅支持最大大小为5
    MB的CSV文件。
- en: Loading data from Amazon S3
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Amazon S3加载数据
- en: Query editor v2 enables you to load data from Amazon S3 buckets into an existing
    Redshift table.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 查询编辑器v2允许您将Amazon S3存储桶中的数据加载到现有的Redshift表中。
- en: 'The `COPY` command, which really makes it easier for a data analyst or data
    scientist, as they don’t have to remember the intricacies of the `COPY` command.
    You can load data from various file formats supported by the `COPY` command, such
    as CSV, JSON, Parquet, Avro, and Orc. Refer to this link for all the supported
    data formats: [https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY`命令，它真正使数据分析师或数据科学家的工作变得更简单，因为他们不需要记住`COPY`命令的复杂性。您可以使用`COPY`命令支持的多种文件格式加载数据，例如CSV、JSON、Parquet、Avro和Orc。有关所有支持的数据格式，请参阅此链接：[https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format)。'
- en: 'Let’s look at loading data using the `customer.csv`), which is stored in the
    following Amazon S3 location: s3://packt-serverless-ml-redshift/chapter02/customer.csv.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用`customer.csv`加载数据，该文件存储在以下Amazon S3位置：s3://packt-serverless-ml-redshift/chapter02/customer.csv。
- en: Note that if you want to use your own Amazon S3 bucket to load the data, then
    download the data file from the GitHub location mentioned in the *Technical* *requirements*
    section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您想使用自己的Amazon S3存储桶来加载数据，那么请从*技术要求*部分中提到的GitHub位置下载数据文件。
- en: 'To download a data file from GitHub, navigate to your repository, select the
    file, right-click the **View raw** button at the top of the file, select **Save
    Link As…** (as shown in the following screenshot), choose the location on your
    computer where you want to save the file, and select **Save**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要从GitHub下载数据文件，导航到您的仓库，选择文件，在文件顶部右键单击**查看原始文件**按钮，选择**另存为…**（如以下截图所示），选择您要在计算机上保存文件的路径，然后选择**保存**：
- en: '![Figure 2.5 – Saving the data file](img/B19071_02_005.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – 保存数据文件](img/B19071_02_005.jpg)'
- en: Figure 2.5 – Saving the data file
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 保存数据文件
- en: On Query Editor v2, click on **Load data**, which opens up the data load wizard.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询编辑器v2中，单击**加载数据**，这将打开数据加载向导。
- en: Under **Data source**, select the **Load from S3** radio button. You can browse
    the S3 bucket in your account to select the data file or a folder that you want
    to load, or you can select a manifest file. For this exercise, paste the aforementioned
    S3 file location.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在**数据源**下，选择**从S3加载**单选按钮。您可以在您的账户中浏览S3存储桶以选择要加载数据的文件或文件夹，或者您可以选择一个清单文件。对于这个练习，粘贴上述S3文件位置。
- en: If the data file is in a different region than your Amazon Redshift Serverless,
    you can select the source region from the S3 file location dropdown. The wizard
    provides different options if you want to load a Parquet file. Then, select an
    option from **File format**, or under **File** options, you can select **Delimiter**
    if your data is delimited by a different character. If your file is compressed,
    then you can select the appropriate compression from the dropdown, such as **gzip**,
    **lzop**, **zstd**, or **bzip2**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据文件位于与您的Amazon Redshift Serverless不同的区域，您可以从S3文件位置下拉菜单中选择源区域。如果您想加载Parquet文件，向导会提供不同的选项。然后，从**文件格式**选项中选择一个选项，或者在**文件**选项下，如果您数据由不同的字符分隔，可以选择**分隔符**。如果您的文件已压缩，您可以从下拉菜单中选择适当的压缩方式，例如**gzip**、**lzop**、**zstd**或**bzip2**。
- en: 'Under **Advanced settings**, note that there are two options, **Data conversion
    parameters** and **Load operations**:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在**高级设置**下，请注意有两个选项，**数据转换参数**和**加载操作**：
- en: 'Under the `TIMEFORMAT`) as `‘MM.DD.YYYY HH:MI:SS''`. Refer to this documentation
    link for a full list of parameters: [https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`TIMEFORMAT`)中将它设置为`‘MM.DD.YYYY HH:MI:SS'`。有关完整参数列表，请参阅此文档链接：[https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat)。
- en: 'Under `COMPROWS`) as `1,000,000`. Refer to this documentation for a full list
    of options: [https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`COMPROWS`)中将它设置为`1,000,000`。有关完整选项列表，请参阅此文档：[https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html](https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html)。
- en: As our file contains the header row, please ensure that under **Advanced settings**
    | **Data conversion parameters** | **Frequently used parameters**, the **Ignore
    header rows (as 1)** option is checked.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的文件包含标题行，请确保在 **高级设置** | **数据转换参数** | **常用参数** 下，**忽略标题行（作为 1）** 选项被选中。
- en: 'As shown in *Figure 2**.6*, select the **Target table** parameters and **IAM
    role** to load the data:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如 **图 2**.6 所示，选择 **目标表** 参数和 **IAM 角色** 以加载数据：
- en: '![Figure 2.6 – Load data](img/B19071_02_006.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 加载数据](img/B19071_02_006.jpg)'
- en: Figure 2.6 – Load data
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 加载数据
- en: Once you click on `COPY` command in the editor and start loading by running
    the `COPY` statement.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在编辑器中点击 `COPY` 命令并运行 `COPY` 语句开始加载。
- en: 'Now that we have loaded our data, let’s quickly verify the load and check the
    data by querying the table, as shown here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载数据，让我们快速验证加载并查询表来检查数据，如下所示：
- en: '![Figure 2.7 – Querying the data](img/B19071_02_007.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 查询数据](img/B19071_02_007.jpg)'
- en: Figure 2.7 – Querying the data
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 查询数据
- en: 'Query Editor v2 enables you to save your queries in the editor for later use.
    You can do so by clicking on the `COPY` command) in the future and, let’s say,
    the target table is the same but the data location on Amazon S3 is different,
    then you can easily modify this query and load the data quickly. Alternatively,
    you can even parameterize the query to pass, for example, an S3 location as `${s3_location}`,
    as shown in *Figure 2**.8*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Query Editor v2 允许您在编辑器中保存查询以供以后使用。您可以通过点击 `COPY` 命令）在将来执行，假设目标表相同，但 Amazon
    S3 上的数据位置不同，那么您可以轻松修改此查询并快速加载数据。或者，您甚至可以将查询参数化，例如，将 S3 位置作为 `${s3_location}` 传递，如
    **图 2**.8 所示：
- en: '![Figure 2.8 – Saving the query](img/B19071_02_008.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 保存查询](img/B19071_02_008.jpg)'
- en: Figure 2.8 – Saving the query
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 保存查询
- en: Sharing queries
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 分享查询
- en: 'With Query Editor v2, you can share your saved queries with your team. This
    way, many users can collaborate and share the same query. Internally, Query Editor
    manages the query versions, so you can track the changes as well. To learn more
    about this, refer to this AWS documentation: [https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Query Editor v2，您可以与您的团队共享您保存的查询。这样，许多用户可以协作并共享相同的查询。内部，Query Editor 管理查询版本，因此您可以跟踪更改。有关更多信息，请参阅此
    AWS 文档：[https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share)。
- en: Now that we have covered how Query Editor v2 enables users to easily create
    database objects and load data using the UI interface with a click of a few buttons,
    let us dive into Amazon Redshift’s `COPY` command to load the data into your data
    warehouse.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 Query Editor v2 如何通过点击几个按钮的 UI 界面轻松创建数据库对象并加载数据，让我们深入了解 Amazon Redshift
    的 `COPY` 命令，以便将数据加载到您的数据仓库中。
- en: Loading data from a local drive
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从本地驱动器加载数据
- en: Query Editor v2 enables users to load data from a local file on their computer
    and perform analysis on it quickly. Often, database users such as data analysts
    or data scientists have data files on their local computer that they want to load
    quickly into a Redshift table, without moving the file into a remote location
    such as Amazon S3.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Query Editor v2 允许用户从他们的计算机上的本地文件加载数据并进行快速分析。通常，数据库用户如数据分析师或数据科学家在他们的本地计算机上拥有他们想要快速加载到
    Redshift 表中的数据文件，而无需将文件移动到远程位置，如 Amazon S3。
- en: 'In order to load the data from a local file, Query Editor v2 requires a staging
    Amazon S3 bucket in your account. If it is not configured, then you will see an
    error similar to the one seen in the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从本地文件加载数据，Query Editor v2 要求在您的账户中配置一个暂存 Amazon S3 存储桶。如果没有配置，您将看到类似于以下截图中的错误：
- en: '![Figure 2.9 – An error message](img/B19071_02_009.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – 错误消息](img/B19071_02_009.jpg)'
- en: Figure 2.9 – An error message
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 错误消息
- en: 'To avoid the preceding error, users must do the following configuration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免前面的错误，用户必须进行以下配置：
- en: 'The account users must be configured with the proper permissions, as follows.
    Attach the following policy to your Redshift Serverless IAM role. Replace the
    resource names as highlighted:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 账户用户必须配置适当的权限，如下所示。将以下策略附加到您的 Redshift Serverless IAM 角色。替换高亮显示的资源名称：
- en: '[PRE0]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Your administrator must configure the common Amazon S3 bucket in the **Account
    settings** window, as shown here:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的管理员必须在**账户设置**窗口中配置通用的Amazon S3存储桶，如图所示：
- en: 'Click on the settings icon (![](img/B19071_02_icon_1.png)) and select **Account
    settings**, as shown in the following screenshot:'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击设置图标(![](img/B19071_02_icon_1.png))并选择**账户设置**，如图下截图所示：
- en: '![Figure 2.10 – Account settings](img/B19071_02_010.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10 – 账户设置](img/B19071_02_010.jpg)'
- en: Figure 2.10 – Account settings
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 账户设置
- en: 'In the **Account settings** window, under **General settings** | **S3 bucket**
    | **S3 URI**, enter the URI of the S3 bucket that will be used for staging during
    the local file load, and then click on **Save**. Ensure that your IAM role has
    permission to read and write on the S3 bucket:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**账户设置**窗口中，在**常规设置** | **S3存储桶** | **S3 URI**下，输入将在本地文件加载期间用于暂存的S3存储桶的URI，然后点击**保存**。确保您的IAM角色具有在S3存储桶上读取和写入的权限：
- en: '![Figure 2.11 – Enter the URI of the S3 bucket under General settings](img/B19071_02_011.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – 在常规设置下输入S3存储桶的URI](img/B19071_02_011.jpg)'
- en: Figure 2.11 – Enter the URI of the S3 bucket under General settings
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 在常规设置下输入S3存储桶的URI
- en: 'Refer to this documentation for complete information:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅此文档以获取完整信息：
- en: '[https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local)'
- en: Creating a table and loading data from a local CSV file
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建表并从本地CSV文件加载数据
- en: 'Let’s create a new table. Navigate to Query Editor v2 and create a supplier
    table using the following DDL command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的表。导航到查询编辑器v2，并使用以下DDL命令创建供应商表：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We will load the data into our supplier table from our data file (`supplier.csv`),
    which is stored in the following GitHub location: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从以下GitHub位置的数据文件(`supplier.csv`)将数据加载到我们的供应商表中：[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv)。
- en: To download the file on your local computer, right-click on **Raw** and click
    on **Save** **Link as**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的本地计算机上下载文件，请右键单击**原始**并点击**另存为**。
- en: 'In order to load data into the supplier table from Query Editor v2, click on
    `supplier.csv` file from your local drive. Under the **Target table** options,
    set **Schema** as **chapter2** and **Table** as **supplier**. Click on **Load
    data** to start the load:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从查询编辑器v2将数据加载到供应商表中，点击您本地驱动器上的`supplier.csv`文件。在**目标表**选项下，将**模式**设置为**chapter2**，将**表**设置为**supplier**。点击**加载数据**以开始加载：
- en: '![Figure 2.12 – The Load data wizard](img/B19071_02_012.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – 加载数据向导](img/B19071_02_012.jpg)'
- en: Figure 2.12 – The Load data wizard
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 加载数据向导
- en: 'Once the data is loaded successfully, you would see a message like the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据成功加载，您将看到如下消息：
- en: '![Figure 2.13 – The message after successfully loading the data](img/B19071_02_013.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – 数据成功加载后的消息](img/B19071_02_013.jpg)'
- en: Figure 2.13 – The message after successfully loading the data
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 数据成功加载后的消息
- en: 'Verify the data load by running the following SQL query:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下SQL查询验证数据加载：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should be able to see 100 rows loaded from the file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够看到从文件中加载的100行数据：
- en: '![Figure 2.14 – Data load verification](img/B19071_02_014.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – 数据加载验证](img/B19071_02_014.jpg)'
- en: Figure 2.14 – Data load verification
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 数据加载验证
- en: We have now successfully loaded our data from the Query Editor v2 `COPY` command
    in the next section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已成功从下一节的查询编辑器v2 `COPY` 命令中加载数据。
- en: Data loading from Amazon S3 using the COPY command
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用COPY命令从Amazon S3加载数据
- en: Data warehouses are typically designed to ingest and store huge volumes of data,
    and one of the key aspects of any analytical process is to ingest such huge volumes
    in the most efficient way. Loading such huge data can take a long time as well
    as consume a lot of compute resources. As pointed out earlier, there are several
    ways to load data in your Redshift Serverless data warehouse, and one of the fastest
    and most scalable methods is the `COPY` command.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库通常设计用于摄取和存储大量数据，任何分析过程的关键方面之一是以最有效的方式摄取这些大量数据。加载数据可能需要很长时间，同时也会消耗大量计算资源。如前所述，有几种方法可以在您的Redshift无服务器数据仓库中加载数据，其中最快和最可扩展的方法之一是`COPY`命令。
- en: The `COPY` command loads your data in parallel from files, taking advantage
    of Redshift’s **massively parallel processing** (**MPP**) architecture. It can
    load data from Amazon S3, Amazon EMR, Amazon DynamoDB, or text files on remote
    hosts (SSH). It is the most efficient way to load a table in your Redshift data
    warehouse. With proper IAM policies, you can securely control who can access and
    load data in your database.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY`命令通过并行从文件加载数据，利用Redshift的**大规模并行处理**（**MPP**）架构。它可以从Amazon S3、Amazon
    EMR、Amazon DynamoDB或远程主机上的文本文件加载数据。这是在Redshift数据仓库中加载数据最有效的方法。通过适当的IAM策略，您可以安全地控制谁可以访问和加载数据库中的数据。'
- en: In the earlier section, we saw how Query Editor v2 generates the `COPY` command
    to load data from the wizard. In this section, we will dive deep and talk about
    how you can write the `COPY` command and load data from Amazon S3, and what some
    of the best practices are.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们看到了如何使用Query Editor v2生成`COPY`命令从向导加载数据。在本节中，我们将深入探讨如何编写`COPY`命令并从Amazon
    S3加载数据，以及一些最佳实践。
- en: 'Let’s take a look at the `COPY` command to load data into your Redshift data
    warehouse:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用`COPY`命令将数据加载到您的Redshift数据仓库中：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `COPY` command requires three parameters:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY`命令需要三个参数：'
- en: '`table-name`: The target table name existing in the database (persistent or
    temporary)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`table-name`：数据库中存在的目标表名（持久或临时）'
- en: '`data_source`: The data source location (such as the S3 bucket)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_source`：数据源位置（例如S3存储桶）'
- en: '`authorization`: The authentication method (for example, the IAM role)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`authorization`：认证方法（例如IAM角色）'
- en: By default, the `COPY` command source data format is expected to be in character-delimited
    UTF-8 text files, with a pipe character (`|`) as the default delimiter. If your
    source data is in another format, you can pass it as a parameter to specify the
    data format. Amazon Redshift supports different data formats, such as fixed-width
    text files, character-delimited files, CSV, JSON, Parquet, and Avro.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`COPY`命令的源数据格式预期为以字符分隔的UTF-8文本文件，默认分隔符为管道字符（`|`）。如果您的源数据是其他格式，您可以将其作为参数传递以指定数据格式。Amazon
    Redshift支持不同的数据格式，如固定宽度文本文件、字符分隔文件、CSV、JSON、Parquet和Avro。
- en: 'Additionally, the `COPY` command provides optional parameters to handle data
    conversion such as the data format, `null`, and encoding. To get the latest details,
    refer to this AWS documentation: [https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`COPY`命令提供了可选参数来处理数据转换，例如数据格式、`null`和编码。要获取最新详细信息，请参阅此AWS文档：[https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax)。
- en: Loading data from a Parquet file
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Parquet文件加载数据
- en: 'In the earlier section, we worked on loading a CSV file into the customer table
    in our database. For this exercise, let’s try to load a columnar data format file
    such as Parquet. We will be using a subset of `TPC-H` data, which may be found
    here: [https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们尝试将CSV文件加载到我们数据库中的客户表中。为此练习，让我们尝试加载一种列式数据格式文件，如Parquet。我们将使用`TPC-H`数据的一个子集，可能在这里找到：[https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB)。
- en: 'The TPC is an organization focused on developing data benchmark standards.
    You may read more about TPC here: [https://www.tpc.org/default5.asp](https://www.tpc.org/default5.asp).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: TPC是一个专注于开发数据基准标准的组织。您可以在[https://www.tpc.org/default5.asp](https://www.tpc.org/default5.asp)了解更多关于TPC的信息。
- en: 'The modified data (`lineitem.parquet`) is available on GitHub: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的数据（`lineitem.parquet`）可在GitHub上找到：[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2)。
- en: 'The data needed for the `COPY` command is available here: `s3://packt-serverless-ml-redshift/chapter02/lineitem.parquet`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY`命令所需的数据在此处可用：`s3://packt-serverless-ml-redshift/chapter02/lineitem.parquet`。'
- en: 'This file contains approximately 6 million rows and is around 200 MB in size:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含大约600万行，大小约为200 MB：
- en: 'Let’s first start by creating a table named `lineitem` in the `chapter2` schema:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先从在`chapter2`模式中创建一个名为`lineitem`的表开始：
- en: '[PRE29]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, let’s load the data using the `COPY` command from the `lineitem.parquet`
    file:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用`COPY`命令从`lineitem.parquet`文件加载数据：
- en: '[PRE48]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now that we have loaded our data, let’s quickly verify the load and check the
    data by querying the table, as shown in the following screenshot:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载数据，让我们快速验证加载并查询表来检查数据，如下面的截图所示：
- en: '![Figure 2.15 – The query table](img/B19071_02_015.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图2.15 – 查询表](img/B19071_02_015.jpg)'
- en: Figure 2.15 – The query table
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – 查询表
- en: In this section, we discussed how the `COPY` command helps load your data in
    different formats, such as CSV, Parquet, and JSON, from Amazon S3 buckets. Let’s
    see how you can automate the `COPY` command to load the data as soon as it is
    available in an Amazon S3 bucket. The next section on automating a `COPY` job
    is currently in public preview at the time of writing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了`COPY`命令如何帮助您从Amazon S3存储桶加载数据，例如CSV、Parquet和JSON格式。让我们看看您如何自动化`COPY`命令，以便在Amazon
    S3存储桶中数据可用时立即加载数据。撰写本文时，自动化`COPY`作业的下一节正处于公开预览阶段。
- en: Automating file ingestion with a COPY job
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`COPY`作业自动化文件摄取
- en: In your data warehouse, data is continuously ingested from Amazon S3\. Previously,
    you wrote custom code externally or locally to achieve this continuous ingestion
    of data with scheduling tools. With Amazon Redshift’s auto-copy feature, users
    can easily automate data ingestion from Amazon S3 to Amazon Redshift. To achieve
    this, you will write a simple SQL command to create a `COPY` job ([https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html)),
    and the `COPY` command will trigger automatically as and when it detects new files
    in the source Amazon S3 path. This will ensure that users have the latest data
    for processing available shortly after it lands in the S3 path, without having
    to build an external custom framework.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的数据仓库中，数据持续从Amazon S3摄取。之前，您通过外部或本地编写自定义代码，并使用调度工具来实现这种持续的数据摄取。借助Amazon Redshift的自动复制功能，用户可以轻松自动化从Amazon
    S3到Amazon Redshift的数据摄取。为此，您将编写一个简单的SQL命令来创建`COPY`作业（[https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html)），并且当它检测到源Amazon
    S3路径中的新文件时，`COPY`命令将自动触发。这将确保用户在数据到达S3路径后不久即可获得最新数据以进行处理，而无需构建外部自定义框架。
- en: 'To get started, you can set up a `COPY` job, as shown here, or modify the existing
    `COPY` command by adding the `JOB` `CREATE` parameter:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您可以设置一个`COPY`作业，如下所示，或者通过添加`JOB CREATE`参数来修改现有的`COPY`命令：
- en: '[PRE52]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s break this down:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: '`job-name` is the name of the job'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`job-name`是作业的名称'
- en: '`AUTO ON | OFF` indicates whether the data from Amazon S3 has loaded automatically
    into an Amazon Redshift table'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AUTO ON | OFF`表示从Amazon S3加载数据是否自动加载到Amazon Redshift表中'
- en: As you can see, the `COPY` job is an extension of the `COPY` command, and auto-ingestion
    of `COPY` jobs is enabled by default.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`COPY`作业是`COPY`命令的扩展，并且默认启用了`COPY`作业的自动摄取。
- en: 'If you want to run a `COPY` job, you can do so by running the following command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想运行一个`COPY`作业，你可以通过运行以下命令来完成：
- en: '[PRE53]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'For the latest details, refer to this AWS documentation: [https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html](https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最新详情，请参阅此AWS文档：[https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html](https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html)。
- en: Best practices for the COPY command
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`COPY`命令的最佳实践'
- en: 'The following best practices will help you get the most out of the `COPY` command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下最佳实践将帮助您充分利用`COPY`命令：
- en: Make the most of parallel processing by splitting data into multiple compressed
    files or by defining distribution keys on your target tables, as we did in our
    example.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将数据分割成多个压缩文件或在目标表上定义分布键来充分利用并行处理，就像我们在示例中所做的那样。
- en: Use a single `COPY` command to load data from multiple files. If you use multiple
    concurrent `COPY` commands to load the same target table from multiple files,
    then the load is done serially, which is much slower than a single `COPY` command.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单个 `COPY` 命令从多个文件加载数据。如果你使用多个并发 `COPY` 命令从多个文件加载数据到同一个目标表，那么加载数据是串行的，这比单个
    `COPY` 命令慢得多。
- en: If your data file contains an uneven or mismatched number of fields, then provide
    the list of columns as comma-separated values.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据文件包含不均匀或匹配的列数，请提供列的逗号分隔值列表。
- en: 'When you want to load a single target table from multiple data files and your
    data files have a similar structure but different naming conventions, or are in
    different folders in an Amazon S3 bucket, then use a manifest file. You can supply
    the full path of the files to be loaded in a JSON-formatted text file. The following
    is the syntax to use a manifest file:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你想从多个数据文件加载数据到单个目标表，并且你的数据文件具有相似的结构但不同的命名约定，或者位于 Amazon S3 桶的不同文件夹中时，请使用清单文件。你可以在一个
    JSON 格式的文本文件中提供要加载的文件的完整路径。以下是如何使用清单文件的语法：
- en: '[PRE54]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: For a `COPY` job, use unique filenames for each file that you want to load.
    If a file is already processed and any changes are done after that, then the `COPY`
    job will not process the file, so remember to rename the updated file.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `COPY` 作业，为每个要加载的文件使用唯一的文件名。如果文件已经被处理，并且在此之后进行了任何更改，那么 `COPY` 作业将不会处理该文件，所以请记住重命名更新后的文件。
- en: So far, we have seen two approaches to data loading in your Amazon Redshift
    data warehouse – using the Query Editor v2 wizard and writing an individual `COPY`
    command to trigger ad hoc data loading. Let us now look into how you can use an
    AWS SDK to load data using the Redshift Data API.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了在 Amazon Redshift 数据仓库中加载数据的两种方法——使用 Query Editor v2 向导和编写单个 `COPY`
    命令来触发临时数据加载。现在让我们看看如何使用 AWS SDK 通过 Redshift 数据 API 加载数据。
- en: Data loading using the Redshift Data API
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Redshift 数据 API 加载数据
- en: The Amazon Redshift Data API is a built-in native API interface to access your
    Amazon Redshift database without configuring any **Java Database Connectivity**
    (**JDBC**) or **Open Database Connectivity** (**ODBC**) drivers. You can ingest
    or query data with a simple API endpoint without managing a persistent connection.
    The Data API provides a secure way to access your database by using either IAM
    temporary credentials or AWS Secrets Manager. It provides a secure HTTP endpoint
    to run SQL statements asynchronously, meaning you can retrieve your results later.
    By default, your query results are stored for 24 hours. The Redshift Data API
    integrates seamlessly with different AWS SDKs, such as Python, Go, Java, Node.js,
    PHP, Ruby, and C++. You can also integrate the API with AWS Glue for an ETL data
    pipeline or use it with AWS Lambda to invoke different SQL statements.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 数据 API 是一个内置的本地 API 接口，用于访问你的 Amazon Redshift 数据库，无需配置任何 **Java
    数据库连接** (**JDBC**) 或 **开放数据库连接** (**ODBC**) 驱动程序。你可以通过简单的 API 端点进行数据摄取或查询，无需管理持久连接。数据
    API 通过使用 IAM 临时凭证或 AWS Secrets Manager 提供了一种安全的方式来访问你的数据库。它提供了一个安全的 HTTP 端点来异步运行
    SQL 语句，这意味着你可以稍后检索结果。默认情况下，你的查询结果会存储 24 小时。Redshift 数据 API 与不同的 AWS SDK 无缝集成，例如
    Python、Go、Java、Node.js、PHP、Ruby 和 C++。你还可以将 API 与 AWS Glue 集成以创建 ETL 数据管道，或者使用
    AWS Lambda 调用不同的 SQL 语句。
- en: 'There are many use cases where you can utilize the Redshift Data API, such
    as ETL orchestration with AWS Step Functions, web service-based applications,
    event-driven applications, and accessing your Amazon Redshift database using Jupyter
    notebooks. If you want to just run an individual SQL statement, then you can use
    the **AWS Command-Line Interface** (**AWS CLI**) or any programming language.
    The following is an example of executing a single SQL statement in Amazon Redshift
    Serverless from the AWS CLI:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以利用 Redshift 数据 API 的许多用例，例如与 AWS Step Functions 的 ETL 调度、基于 Web 服务的应用程序、事件驱动应用程序，以及使用
    Jupyter notebooks 访问你的 Amazon Redshift 数据库。如果你只想运行单个 SQL 语句，则可以使用 **AWS 命令行界面**
    (**AWS CLI**) 或任何编程语言。以下是从 AWS CLI 在 Amazon Redshift Serverless 中执行单个 SQL 语句的示例：
- en: '[PRE57]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Note that, for Redshift Serverless, you only need to provide the workgroup
    name and database name. Temporary user credentials are pulled from IAM authorization.
    For Redshift Serverless, add the following permission in the IAM policy attached
    to your cluster IAM role to access the Redshift Data API:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于 Redshift Serverless，您只需要提供工作组名称和数据库名称。临时用户凭证从 IAM 授权中获取。对于 Redshift Serverless，请将以下权限添加到附加到您的集群
    IAM 角色的 IAM 策略中，以访问 Redshift Data API：
- en: '[PRE58]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In order to showcase how you can ingest data using the Redshift Data API, we
    will carry out the following steps using Jupyter Notebook. Let’s create a notebook
    instance in our AWS account.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示您如何使用 Redshift Data API 摄入数据，我们将使用 Jupyter Notebook 执行以下步骤。让我们在我们的 AWS 账户中创建一个笔记本实例。
- en: On the console home page, search for `Amazon SageMaker`. Click on the hamburger
    icon (![](img/B19071_02_icon_2.png)) in the top-left corner, then **Notebook**,
    and then **Notebook instances**. Click on **Create notebook instance** and provide
    the necessary input. Once the notebook instance is in service, click on **Open
    Jupyter**.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台主页上，搜索 `Amazon SageMaker`。点击左上角的汉堡图标 (![](img/B19071_02_icon_2.png))，然后
    **笔记本**，然后 **笔记本实例**。点击 **创建笔记本实例** 并提供必要的输入。一旦笔记本实例投入使用，点击 **打开 Jupyter**。
- en: 'The following screenshot shows a created notebook instance:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个创建的笔记本实例：
- en: '![Figure 2.16 – Creating a notebook instance](img/B19071_02_016.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.16 – 创建笔记本实例](img/B19071_02_016.jpg)'
- en: Figure 2.16 – Creating a notebook instance
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 – 创建笔记本实例
- en: 'The Jupyter notebook for this exercise is available at this GitHub location:
    [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb).
    Download this notebook to your local machine and save it in a folder.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的 Jupyter 笔记本可在以下 GitHub 位置找到：[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb)。请将此笔记本下载到您的本地计算机并在文件夹中保存。
- en: 'The data (`orders.parquet`) for this exercise is available on GitHub at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2),
    as well as this Amazon S3 location: `s3://packt-serverless-ml-redshift/chapter2/orders.parquet`.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的数据 (`orders.parquet`) 可在 GitHub 上找到：[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2)，以及此
    Amazon S3 位置：`s3://packt-serverless-ml-redshift/chapter2/orders.parquet`。
- en: 'We will use a subset of the `orders` data, which is referenced from the `TPC-H`
    dataset available here: [https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `orders` 数据的一个子集，该数据来自以下位置的 `TPC-H` 数据集：[https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH](https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH)。
- en: 'Let’s first open the downloaded notebook (`Chapter2.ipynb`) by following these
    steps:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先按照以下步骤打开下载的笔记本 (`Chapter2.ipynb`)：
- en: On the Jupyter Notebook landing page, click on **Upload** and open the previously
    downloaded notebook.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 登录页面，点击 **上传** 并打开之前下载的笔记本。
- en: Select the kernel (`conda_python3`) once the notebook is uploaded.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 笔记本上传后，选择内核 (`conda_python3`)。
- en: Note
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Redshift Serverless requires your `boto3` version to be greater than version
    1.24.32.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Redshift Serverless 需要您的 `boto3` 版本高于 1.24.32。
- en: Let’s check our `boto3` library version, as shown in *Figure 2**.17*.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查我们的 `boto3` 库版本，如图 *图 2.17* 所示。
- en: '![Figure 2.17 – Checking the boto3 version](img/B19071_02_017.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.17 – 检查 boto3 版本](img/B19071_02_017.jpg)'
- en: Figure 2.17 – Checking the boto3 version
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 – 检查 boto3 版本
- en: 'If you want to install a specific version greater than 1.24.32, then check
    the following example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想安装一个高于 1.24.32 的特定版本，请参考以下示例：
- en: '[PRE59]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Creating table
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建表
- en: 'As you can see in the `chapter2.ipynb` notebook, we have provided step-by-step
    instructions to connect to your Redshift Serverless endpoint and perform the necessary
    operations:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 `chapter2.ipynb` 笔记本中看到的，我们提供了逐步说明，以连接到您的 Redshift Serverless 端点并执行必要的操作：
- en: 'Let’s start by setting up the parameters and importing the necessary libraries
    for this exercise. We will set the following two parameters:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先设置参数并导入此练习所需的必要库。我们将设置以下两个参数：
- en: '`REDSHIFT_WORKGROUP`: The name of the Redshift Serverless workgroup'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`REDSHIFT_WORKGROUP`：Redshift Serverless工作组的名称'
- en: '`S3_DATA_FILE`: The source data file for the load:'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`S3_DATA_FILE`：加载数据的源数据文件：'
- en: '[PRE60]'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Remember to set the parameters as per your settings in the Jupyter notebook.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 记住根据Jupyter笔记本中的设置设置参数。
- en: 'In order to create the table, let’s first prepare our DDL and assign it to
    a `table_ddl` variable:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了创建表，我们首先准备我们的DDL并将其分配给`table_ddl`变量：
- en: '[PRE68]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Using the `boto3` library, we will connect to the Redshift Serverless workgroup:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`boto3`库，我们将连接到Redshift Serverless工作组：
- en: '[PRE82]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'There are different methods that are available to execute different operations
    on your Redshift Serverless endpoint. Check out the entire list in this documentation:
    [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法可用于在Redshift Serverless端点上执行不同的操作。请在此文档中查看整个列表：[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html)。
- en: 'We will use the `execute_statement` method to run an SQL statement, which can
    be in the `BatchExecuteStatement`. To get a complete list of different methods
    and how to use them, please refer to this AWS documentation: [https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html](https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html):'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`execute_statement`方法运行一个SQL语句，该语句可以是`BatchExecuteStatement`中的。要获取不同方法和如何使用它们的完整列表，请参阅此AWS文档：[https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html](https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html)：
- en: '[PRE83]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: As you can see from the preceding code block, we will first set the client as
    `redshift-data` and then call `execute_statement` to connect the Serverless endpoint,
    using the `Database` name and `WorkgroupName`. The method uses temporary credentials
    to connect to your Serverless workgroup.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码块所示，我们首先将客户端设置为`redshift-data`，然后调用`execute_statement`连接到Serverless端点，使用`Database`名称和`WorkgroupName`。此方法使用临时凭证连接到你的Serverless工作组。
- en: We will also pass `table_ddl` as a parameter to create the table. We will create
    the `Orders` table in our `chapter2` schema.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将`table_ddl`作为参数传递以创建表。我们将在`chapter2`模式中创建`Orders`表。
- en: 'The Redshift Data API sends back a response element once the action is successful,
    in a JSON format as a dictionary object. One of the response elements is a SQL
    statement identifier. This value is universally unique and generated by the Amazon
    Redshift Data API. As you can see in the following code, we have captured the
    response element, `Id`, from the output object, `res`:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当操作成功时，Redshift Data API会返回一个响应元素，以JSON格式作为字典对象。响应元素之一是SQL语句标识符。此值是全局唯一的，由Amazon
    Redshift Data API生成。如以下代码所示，我们已经从输出对象`res`中捕获了响应元素`Id`：
- en: '[PRE85]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: In order to make sure that your query is completed, you can use the `describe_statement`
    method and pass your `id` statement as a parameter. This method sends out the
    response, which contains information that includes when the query started, when
    it finished, the query status, the number of rows returned, and the SQL statement.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保你的查询已完成，你可以使用`describe_statement`方法并传递你的`id`语句作为参数。此方法发送响应，其中包含包括查询开始时间、完成时间、查询状态、返回的行数和SQL语句的信息。
- en: '![Figure 2.18 – Checking the query status](img/B19071_02_018.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图2.18 – 检查查询状态](img/B19071_02_018.jpg)'
- en: Figure 2.18 – Checking the query status
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 – 检查查询状态
- en: As you can see in *Figure 2**.18*, we have captured the status of the statement
    that we ran, and it sends out the status as `FINISHED`. This means that we have
    created our table in the database, and you can verify this by writing a simple
    `SELECT` statement against the table.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2**.18*所示，我们已经捕获了我们运行的语句的状态，并且它发送出状态为`FINISHED`。这意味着我们已经创建了数据库中的表，你可以通过针对该表编写一个简单的`SELECT`语句来验证这一点。
- en: Loading data using the Redshift Data API
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Redshift Data API加载数据
- en: 'Now, let’s move forward to load data into this newly created table. You can
    use the S3 location for the source data, as mentioned previously. If you use a
    different S3 location, then remember to replace the path in the parameter (`S3_DATA_FILE`):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续将数据加载到这个新创建的表中。你可以使用之前提到的S3位置作为源数据。如果你使用不同的S3位置，那么请记住替换参数中的路径（`S3_DATA_FILE`）：
- en: 'Let’s write a `COPY` command, as shown in the following code block. We will
    create the `COPY` command in the `load_data` variable, using the S3 path as a
    parameter:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编写一个`COPY`命令，如下面的代码块所示。我们将在`load_data`变量中创建`COPY`命令，使用S3路径作为参数：
- en: '[PRE87]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Next, we will use the `execute_statement` method to run this `COPY` command
    and capture the `id` statement:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`execute_statement`方法来运行这个`COPY`命令并捕获`id`语句：
- en: '[PRE91]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: Be sure to check whether the status of the query is `FINISHED`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保检查查询的状态是否为`FINISHED`。
- en: 'Once the statement status is defined as `FINISHED`, we will verify our data
    load by running a count query, as shown here:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦将状态定义为`FINISHED`，我们将通过运行一个计数查询来验证我们的数据加载，如下所示：
- en: '[PRE95]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We will now print the results:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将打印结果：
- en: '![Figure 2.19 – Count query results](img/B19071_02_019.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图2.19 – 计数查询结果](img/B19071_02_019.jpg)'
- en: Figure 2.19 – Count query results
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 – 计数查询结果
- en: As you can see in *Figure 2**.19*, we have successfully loaded 1.5 million rows.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*图2**.19*中看到的，我们已经成功加载了150万行。
- en: In the notebook, we have provided a combined code block to show how you can
    convert all these steps into a function, calling it as and when you require it
    to load data into a new table.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们提供了一个组合代码块，展示了如何将这些步骤转换成一个函数，并在需要时调用它来将数据加载到新表中。
- en: We also have a GitHub repository ([https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/](https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/)),
    which showcases how to get started with the Amazon Redshift Data API in different
    languages, such as Go, Java, JavaScript, Python, and TypeScript. You can go through
    the step-by-step process explained in the repository to build your custom application
    in all these languages, using the Redshift Data API.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个GitHub仓库([https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/](https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/))，展示了如何在不同语言中开始使用Amazon
    Redshift Data API，例如Go、Java、JavaScript、Python和TypeScript。你可以通过仓库中解释的逐步过程，使用Redshift
    Data API在这些所有语言中构建你的自定义应用程序。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we showcased how you can load data into your Amazon Redshift
    Serverless database using three different tools and methods, by using the query
    editor v GUI interface, the Redshift `COPY` command to load the data, and the
    Redshift Data API using Python in a Jupyter notebook. All three methods are efficient
    and easy to use for your different use cases.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何使用三种不同的工具和方法将数据加载到你的Amazon Redshift Serverless数据库中，通过使用查询编辑器v GUI界面，Redshift
    `COPY`命令来加载数据，以及使用Python在Jupyter笔记本中调用Redshift Data API。所有三种方法都高效且易于使用，适用于你的不同用例。
- en: We also talked about some of the best practices for the `COPY` command to make
    efficient use of it.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了一些关于`COPY`命令的最佳实践，以使其更有效地使用。
- en: In the next chapter, we will start with our first topic concerning Amazon Redshift
    machine learning, and you will see how you can leverage it in your Amazon Redshift
    Serverless data warehouse.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从关于Amazon Redshift机器学习的第一个主题开始，你将看到如何在你的Amazon Redshift Serverless数据仓库中利用它。
