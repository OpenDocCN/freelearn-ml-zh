- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Predicting Stock Prices with Artificial Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工神经网络预测股票价格
- en: Continuing the same project of stock price prediction from the last chapter,
    in this chapter, we will introduce and explain neural network models in depth.
    We will start by building the simplest neural network and go deeper by adding
    more computational units to it. We will cover neural network building blocks and
    other important concepts, including activation functions, feedforward, and backpropagation.
    We will also implement neural networks from scratch with scikit-learn, TensorFlow,
    and PyTorch. We will pay attention to how to learn with neural networks efficiently
    without overfitting, utilizing dropout and early stopping techniques. Finally,
    we will train a neural network to predict stock prices and see whether it can
    beat what we achieved with the three regression algorithms in the previous chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上章的股票价格预测项目，在本章中，我们将深入介绍神经网络模型。我们将从构建最简单的神经网络开始，并通过向其中添加更多计算单元来深入探讨。我们将介绍神经网络的构建模块和其他重要概念，包括激活函数、前向传播和反向传播。我们还将使用
    scikit-learn、TensorFlow 和 PyTorch 从头开始实现神经网络。我们将关注如何在不发生过拟合的情况下高效地使用神经网络进行学习，采用
    Dropout 和早停技术。最后，我们将训练一个神经网络来预测股票价格，看看它是否能超越我们在上一章中使用的三种回归算法的结果。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下内容：
- en: Demystifying neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解密神经网络
- en: Building neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: Picking the right activation functions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的激活函数
- en: Preventing overfitting in neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止神经网络过拟合
- en: Predicting stock prices with neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络预测股票价格
- en: Demystifying neural networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密神经网络
- en: Here comes probably the most frequently mentioned model in the media, **Artificial
    Neural Networks** (**ANNs**); more often, we just call them **neural networks**.
    Interestingly, the neural network has been (falsely) considered equivalent to
    machine learning or artificial intelligence by the general public.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是媒体中最常提到的模型之一，**人工神经网络**（**ANNs**）；更常见的叫法是**神经网络**。有趣的是，神经网络曾被大众（错误地）认为等同于机器学习或人工智能。
- en: An ANN is just one type of algorithm among many in machine learning, and machine
    learning is a branch of artificial intelligence. It is one of the ways we achieve
    **Artificial General Intelligence** (**AGI**), which is a hypothetical type of
    AI that can think, learn, and solve problems like a human.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）只是机器学习中众多算法之一，而机器学习是人工智能的一个分支。它是实现**通用人工智能**（**AGI**）的途径之一，AGI是一种假设中的人工智能类型，能够像人类一样思考、学习和解决问题。
- en: Regardless, it is one of the most important machine learning models and has
    been rapidly evolving along with the revolution of **Deep Learning** (**DL**).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，它仍然是最重要的机器学习模型之一，并且随着**深度学习**（**DL**）革命的推进，神经网络也在快速发展。
- en: Let’s first understand how neural networks work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解神经网络是如何工作的。
- en: Starting with a single-layer neural network
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从单层神经网络开始
- en: We start by explaining different layers in a network, then move on to the activation
    function, and finally, training a network with backpropagation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从解释网络中的不同层开始，然后介绍激活函数，最后介绍使用反向传播训练网络。
- en: Layers in neural networks
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络中的层
- en: 'A simple neural network is composed of three layers—the **input layer**, **hidden
    layer**, and **output layer— a**s shown in the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的神经网络由三层组成——**输入层**、**隐藏层**和**输出层**，如下图所示：
- en: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_06_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_06_01.png)'
- en: 'Figure 6.1: A simple shallow neural network'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：一个简单的浅层神经网络
- en: A **layer** is a conceptual collection of **nodes** (also called **units**),
    which simulate neurons in a biological brain. The input layer represents the input
    features, **x**, and each node is a predictive feature, *x*. The output layer
    represents the target variable(s).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**层**是**节点**（也称为**单元**）的概念集合，模拟生物大脑中的神经元。输入层表示输入特征**x**，每个节点是一个预测特征，*x*。输出层表示目标变量。'
- en: In binary classification, the output layer contains only one node, whose value
    is the probability of the positive class. In multiclass classification, the output
    layer consists of *n* nodes, where *n* is the number of possible classes and the
    value of each node is the probability of predicting that class. In regression,
    the output layer contains only one node, the value of which is the prediction
    result.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类问题中，输出层只包含一个节点，其值表示正类的概率。在多分类问题中，输出层由*n*个节点组成，其中*n*是可能的类别数，每个节点的值表示预测该类别的概率。在回归问题中，输出层只包含一个节点，其值表示预测结果。
- en: The hidden layer can be considered a composition of latent information extracted
    from the previous layer. There can be more than one hidden layer. Learning with
    a neural network with two or more hidden layers is called **deep learning**. In
    this chapter, we will focus on one hidden layer to begin with.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层可以视为从前一层提取的潜在信息的组合。可以有多个隐藏层。使用具有两个或更多隐藏层的神经网络进行学习称为**深度学习**。在本章中，我们将首先聚焦于一个隐藏层。
- en: Two adjacent layers are connected by conceptual edges (sort of like the synapses
    in a biological brain), which transmit signals from one neuron in a layer to another
    neuron in the next layer. The **edges** are parameterized by the weights, *W*,
    of the model. For example, *W*^((1)) in the preceding diagram connects the input
    and hidden layers and *W*^((2)) connects the hidden and output layers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两个相邻的层通过概念性边（类似于生物大脑中的突触）连接，这些边传递来自一个神经元的信号到下一个层中的另一个神经元。**边**通过模型的权重*W*进行参数化。例如，前图中的*W*^((1))连接输入层和隐藏层，*W*^((2))连接隐藏层和输出层。
- en: In a standard neural network, data is conveyed only from the input layer to
    the output layer, through a hidden layer(s). Hence, this kind of network is called
    a **feedforward** neural network. Basically, logistic regression is a feedforward
    neural network with no hidden layer where the output layer connects directly with
    the input layer. Adding hidden layers between the input and output layers introduces
    non-linearity. This allows the neural networks to learn more about the underlying
    relationship between the input data and the target.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准神经网络中，数据仅从输入层传递到输出层，通过一个或多个隐藏层。因此，这种网络被称为**前馈**神经网络。基本上，逻辑回归是一个没有隐藏层的前馈神经网络，输出层直接与输入层连接。添加隐藏层在输入层和输出层之间引入了非线性。这使得神经网络能够更好地学习输入数据与目标之间的潜在关系。
- en: Activation functions
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: An **activation function** is a mathematical operation applied to the output
    of each neuron in a neural network. It determines whether the neuron should be
    activated (i.e., its output value should be propagated forward to the next layer)
    based on the input it receives.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**是应用于神经网络中每个神经元输出的数学操作。它根据神经元接收到的输入来决定该神经元是否应被激活（即，其输出值是否应传播到下一层）。'
- en: 'Suppose the input, *x*, is of *n* dimensions, and the hidden layer is composed
    of *H* hidden units. The weight matrix, *W*^((1)), connecting the input and hidden
    layers is of size *n* by *H*, where each column, ![](img/B21047_06_001.png), represents
    the coefficients associating the input with the *h*-th hidden unit. The output
    (also called **activation**) of the hidden layer can be expressed mathematically
    as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入*x*是*n*维的，隐藏层由*H*个隐藏单元组成。连接输入层和隐藏层的权重矩阵*W*^((1))的大小是*n*乘*H*，其中每一列，![](img/B21047_06_001.png)，表示与第*h*个隐藏单元相关的输入系数。隐藏层的输出（也称为**激活**）可以用以下数学公式表示：
- en: '![](img/B21047_06_002.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_002.png)'
- en: 'Here, *f(z)* is an activation function. As its name implies, the activation
    function checks how activated each neuron is, simulating the way our brains work.
    Their primary purpose is to introduce non-linearity into the output of a neuron,
    allowing the network to learn and perform complex mappings between inputs and
    outputs. Typical activation functions include the logistic function (more often
    called the **sigmoid** function in neural networks) and the **tanh** function,
    which is considered a rescaled version of the logistic function, as well as **ReLU**
    (short for **Rectified Linear Unit**), which is often used in DL:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f(z)* 是一个激活函数。顾名思义，激活函数检查每个神经元的激活程度，模拟我们大脑的工作方式。它们的主要目的是为神经元的输出引入非线性，使得网络能够学习并执行输入与输出之间的复杂映射。典型的激活函数包括逻辑函数（在神经网络中更常被称为**sigmoid**函数）和**tanh**函数，后者被认为是逻辑函数的重新缩放版本，以及**ReLU**（**Rectified
    Linear Unit**的简称），它在深度学习中经常使用：
- en: '![](img/B21047_06_003.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_003.png)'
- en: '![](img/B21047_06_004.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_004.png)'
- en: '![](img/B21047_06_005.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_005.png)'
- en: 'We plot these three activation functions as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们绘制这三种激活函数如下：
- en: 'The **logistic** (**sigmoid**) function where the output value is in the range
    of (`0, 1`):'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑**（**sigmoid**）函数，其中输出值的范围为`(0, 1)`：'
- en: '![A graph with a line  Description automatically generated](img/B21047_06_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![A graph with a line  Description automatically generated](img/B21047_06_02.png)'
- en: 'Figure 6.2: The logistic function'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：逻辑函数
- en: 'The visualization is produced by the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化通过以下代码生成：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The **tanh** function plot where the output value is in the range of `(-1,
    1)`:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tanh**函数图像，其中输出值的范围为`(-1, 1)`：'
- en: '![A graph with a line  Description automatically generated](img/B21047_06_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![A graph with a line  Description automatically generated](img/B21047_06_03.png)'
- en: 'Figure 6.3: The tanh function'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：tanh函数
- en: 'The visualization is produced by the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化通过以下代码生成：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The **ReLU** function plot where the output value is in the range of `(0, +inf)`:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU**函数图像，其中输出值的范围为`(0, +inf)`：'
- en: '![A graph with a line  Description automatically generated](img/B21047_06_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![A graph with a line  Description automatically generated](img/B21047_06_04.png)'
- en: 'Figure 6.4: The ReLU function'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：ReLU函数
- en: 'The visualization is produced by the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化通过以下代码生成：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As for the output layer, let’s assume that there is one output unit (regression
    or binary classification) and that the weight matrix, *W*^((2)), connecting the
    hidden layer to the output layer is of size *H* by *1*. In regression, the output
    can be expressed mathematically as follows (for consistency, I here denote it
    as *a*^((3)) instead of *y*):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 至于输出层，假设有一个输出单元（回归或二分类），且连接隐层和输出层的权重矩阵*W*^((2))的大小为*H* × *1*。在回归问题中，输出可以通过以下数学公式表示（为保持一致性，我这里使用*a*^((3))而非*y*）：
- en: '![](img/B21047_06_006.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_006.png)'
- en: The **Universal Approximation Theorem** is a key concept in understanding how
    neural networks enable learning. It states that a feedforward neural network with
    a single hidden layer containing a finite number of neurons can approximate any
    continuous function to arbitrary precision, given a sufficiently large number
    of neurons in the hidden layer. During the training process, the neural network
    learns to approximate the target function by adjusting its parameters (weights).
    This is typically done using optimization algorithms, such as gradient descent,
    which iteratively update the parameters to minimize the difference between the
    predicted outputs and the true targets. Let’s see this process in detail in the
    next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**普适逼近定理**是理解神经网络如何实现学习的关键概念。该定理指出，具有单个隐层且隐层包含有限数量神经元的前馈神经网络，能够逼近任何连续函数，并达到任意精度，只要隐层的神经元数量足够大。在训练过程中，神经网络通过调整其参数（权重）来学习逼近目标函数。通常，这是通过优化算法实现的，例如梯度下降法，算法通过迭代更新参数，以最小化预测输出与真实目标之间的差异。让我们在下一节中详细了解这个过程。'
- en: Backpropagation
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: So, how can we obtain the optimal weights, *W = {W(1), W(2)}*, of the model?
    Similar to logistic regression, we can learn all weights using gradient descent
    with the goal of minimizing the **mean squared error** (**MSE**) cost or other
    loss function, *J(W)*. The difference is that the gradients, ![](img/B21047_06_007.png),
    are computed through **backpropagation**. After each forward pass through a network,
    a backward pass is performed to adjust the model’s parameters.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何获得模型的最优权重，*W = {W(1), W(2)}*呢？与逻辑回归类似，我们可以通过梯度下降法学习所有权重，目标是最小化**均方误差**（**MSE**）代价函数或其他损失函数，*J(W)*。不同之处在于，梯度！[](img/B21047_06_007.png)是通过**反向传播**计算的。每次通过网络的前向传播后，都会执行反向传播来调整模型的参数。
- en: 'As the word *back* in the name implies, the computation of the gradient proceeds
    backward: the gradient of the final layer is computed first and the gradient of
    the first layer is computed last. As for *propagation*, it means that partial
    computations of the gradient on one layer are reused in the computation of the
    gradient on the previous layer. Error information is propagated layer by layer,
    instead of being calculated separately.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如“*back*”这个词在名称中所暗示的那样，梯度的计算是从后往前进行的：首先计算最后一层的梯度，然后计算第一层的梯度。至于“*propagation*”，它意味着在计算一个层的梯度时，部分计算结果会在计算前一层梯度时被重复使用。误差信息是层层传播的，而不是单独计算的。
- en: 'In a single-layer network, the detailed steps of backpropagation are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在单层网络中，反向传播的详细步骤如下：
- en: We travel through the network from the input to the output and compute the output
    values, *a*^((2)), of the hidden layer as well as the output layer, *a*^((3)).
    This is the feedforward step.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从输入层到输出层遍历整个网络，并计算隐藏层的输出值，*a*^((2))，以及输出层的输出值，*a*^((3))。这是前向传播步骤。
- en: 'For the last layer, we calculate the derivative of the cost function with regard
    to the input to the output layer:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于最后一层，我们计算代价函数相对于输出层输入的导数：
- en: '![](img/B21047_06_008.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_008.png)'
- en: 'For the hidden layer, we compute the derivative of the cost function with regard
    to the input to the hidden layer:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于隐藏层，我们计算代价函数相对于隐藏层输入的导数：
- en: '![](img/B21047_06_009.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_009.png)'
- en: 'We compute the gradients by applying the **chain rule**:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过应用**链式法则**计算梯度：
- en: '![](img/B21047_06_010.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_010.png)'
- en: '![](img/B21047_06_011.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_011.png)'
- en: 'We update the weights with the computed gradients and learning rate ![](img/B21047_06_012.png):'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们用计算出的梯度和学习率更新权重！[](img/B21047_06_012.png)：
- en: '![](img/B21047_06_013.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_013.png)'
- en: '![](img/B21047_06_014.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_014.png)'
- en: Here, *m* is the number of samples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*m*是样本的数量。
- en: We repeatedly update all the weights by taking these steps with the latest weights
    until the cost function converges or the model goes through enough iterations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下步骤反复更新所有权重，使用最新的权重直到代价函数收敛或模型完成足够的迭代。
- en: The chain rule is a fundamental concept in calculus. It allows you to find the
    derivative of a composite function. You can read more in the mathematics course
    from Stanford University ([https://mathematics.stanford.edu/events/chain-rule-calculus](https://mathematics.stanford.edu/events/chain-rule-calculus)),
    or the differential calculus course, *Module 6, Applications of Differentiation*,
    from MIT ([https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/](https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则是微积分中的一个基本概念，它允许你找到复合函数的导数。你可以在斯坦福大学的数学课程中了解更多相关内容（[https://mathematics.stanford.edu/events/chain-rule-calculus](https://mathematics.stanford.edu/events/chain-rule-calculus)），或者在麻省理工学院的微分数学课程*第6模块，微分应用*中学习（[https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/](https://ocw.mit.edu/courses/18-03sc-differential-equations-fall-2011/)）。
- en: This might not be easy to digest at first glance, so right after the next section,
    we will implement it from scratch, which will help you understand neural networks
    better.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能一开始不容易理解，因此在接下来的章节中，我们将从零开始实现它，这将帮助你更好地理解神经网络。
- en: 'Adding more layers to a neural network: DL'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向神经网络添加更多层：深度学习（DL）
- en: 'In real applications, a neural network usually comes with multiple hidden layers.
    That is how DL got its name—learning using neural networks with “stacked” hidden
    layers. An example of a DL model is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，神经网络通常有多个隐藏层。这就是深度学习（DL）得名的原因——使用具有“堆叠”隐藏层的神经网络进行学习。一个深度学习模型的示例如下：
- en: '![](img/B21047_06_05.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_06_05.png)'
- en: 'Figure 6.5: A deep neural network'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：一个深度神经网络
- en: In a stack of multiple hidden layers, the input of one hidden layer is the output
    of its previous layer, as you can see from *Figure 6.5*. Features (signals) are
    extracted from each hidden layer. Features from different layers represent patterns
    from different levels. Going beyond shallow neural networks (usually with only
    one hidden layer), a DL model (usually with two or more hidden layers) with the
    right network architectures and parameters can better learn complex non-linear
    relationships from data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在多层隐藏层的堆栈中，一个隐藏层的输入是其前一个层的输出，正如你在*图 6.5*中看到的那样。特征（信号）是从每个隐藏层提取的。来自不同层的特征表示来自不同层次的模式。超越浅层神经网络（通常只有一个隐藏层），一个具有正确网络架构和参数的深度学习模型（通常有两个或更多隐藏层）能够更好地从数据中学习复杂的非线性关系。
- en: Let’s see some typical applications of DL so that you will be more motivated
    to get started with upcoming DL projects.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些深度学习的典型应用，让你更有动力开始即将到来的深度学习项目。
- en: '**Computer vision** is widely considered the area with massive breakthroughs
    in DL. You will learn more about this in *Chapter 11*, *Categorizing Images of
    Clothing with Convolutional Neural Networks*, and *Chapter 14*, *Building an Image
    Search Engine Using CLIP: A Multimodal Approach*. For now, here is a list of common
    applications in computer vision:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机视觉**被广泛认为是深度学习取得巨大突破的领域。你将在*第 11 章*《使用卷积神经网络对服装图像进行分类》和*第 14 章*《使用 CLIP
    构建图像搜索引擎：一种多模态方法》中学到更多内容。现在，以下是计算机视觉中的一些常见应用：'
- en: Image recognition, such as face recognition and handwritten digit recognition.
    Handwritten digit recognition, along with the common evaluation dataset MNIST,
    has become a “Hello, World!” project in DL.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别，如人脸识别和手写数字识别。手写数字识别以及常见的评估数据集 MNIST 已成为深度学习中的“Hello, World！”项目。
- en: Image-based search engines heavily utilize DL techniques in their image classification
    and image similarity encoding components.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图像的搜索引擎在其图像分类和图像相似度编码组件中大量使用深度学习技术。
- en: Machine vision, which is a critical part of autonomous vehicles, perceives camera
    views to make real-time decisions.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器视觉，这是自动驾驶汽车的关键部分，能够感知摄像头视图并做出实时决策。
- en: Color restoration from black and white photos and art transfer that ingeniously
    blends two images of different styles. The artificial masterpieces in Google Arts
    & Culture ([https://artsandculture.google.com/](https://artsandculture.google.com/))
    are impressive.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从黑白照片恢复颜色以及艺术风格转移，巧妙地融合两种不同风格的图像。谷歌艺术与文化（[https://artsandculture.google.com/](https://artsandculture.google.com/)）中的人工艺术作品令人印象深刻。
- en: Realistic image generation based on textual descriptions. This has applications
    in creating visual storytelling content and assisting in content creation for
    marketing and advertising.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于文本描述的逼真图像生成。这在创建视觉故事内容和帮助营销广告创作中有应用。
- en: '**Natural Language Processing** (**NLP**) is another field where you can see
    the dominant use of DL in its modern solutions. You will learn more about this
    in *Chapter 12*, *Making Predictions with Sequences Using Recurrent Neural Networks,*
    and*Chapter 13*, *Advancing Language Understanding and Generation with the Transformer
    Models*. But let’s quickly look at some examples now:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是另一个你可以看到深度学习在其现代解决方案中占主导地位的领域。你将在*第 12 章*《使用递归神经网络进行序列预测》和*第
    13 章*《利用 Transformer 模型推进语言理解与生成》中学到更多内容。但现在让我们快速看一些示例：'
- en: Machine translation, where DL has dramatically improved accuracy and fluency,
    for example, the sentence-based **Google Neural Machine Translation** (**GNMT**)
    system.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译，深度学习极大地提高了其准确性和流畅性，例如基于句子的**谷歌神经机器翻译**（**GNMT**）系统。
- en: Text generation reproduces text by learning the intricate relationships between
    words in sentences and paragraphs with deep neural networks. You can become a
    virtual J. K. Rowling or Shakespeare if you train a model well on their works.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成通过学习句子和段落中单词之间复杂的关系，利用深度神经网络再现文本。如果你能在 J. K. 罗琳或莎士比亚的作品上充分训练模型，你就能成为一位虚拟的
    J. K. 罗琳或莎士比亚。
- en: Image captioning, also known as image-to-text, leverages deep neural networks
    to detect and recognize objects in images and “describe” those objects in a comprehensible
    sentence. It couples recent breakthroughs in computer vision and NLP. Examples
    can be found at [https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/](https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/)
    (developed by Andrej Karpathy from Stanford University).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述（也称为图像到文本）利用深度神经网络来检测和识别图像中的物体，并用易于理解的句子“描述”这些物体。它结合了计算机视觉和自然语言处理（NLP）领域的最新突破。示例可以在[https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/](https://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/)找到（由斯坦福大学的Andrej
    Karpathy开发）。
- en: In other common NLP tasks such as sentiment analysis and information retrieval
    and extraction, DL models have achieved state-of-the-art performance.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在其他常见的自然语言处理任务中，如情感分析和信息检索与提取，深度学习模型已经取得了最先进的性能。
- en: '**Artificial Intelligence-Generated Content** (**AIGC**) is one of the recent
    breakthroughs. It uses DL technologies to create or assist in creating various
    types of content, such as articles, product descriptions, music, images, and videos.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能生成内容**（**AIGC**）是近年来的一个重大突破。它使用深度学习技术来创建或辅助创作各种类型的内容，如文章、产品描述、音乐、图像和视频。'
- en: Similar to shallow networks, we learn all the weights in a deep neural network
    using gradient descent with the goal of minimizing the MSE cost, *J(W)*. And gradients,
    ![](img/B21047_06_007.png), are computed through backpropagation. The difference
    is that we backpropagate more than one hidden layer. In the next section, we will
    implement neural networks by starting with shallow networks and then moving on
    to deep ones.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于浅层网络，我们通过梯度下降法在深度神经网络中学习所有的权重，目标是最小化 MSE 损失函数，*J(W)*。梯度，![](img/B21047_06_007.png)，是通过反向传播计算的。不同之处在于，我们反向传播的不止一层隐藏层。在接下来的章节中，我们将从浅层网络开始，逐步实现深度神经网络。
- en: Building neural networks
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: This practical section will start with implementing a shallow network from scratch,
    followed by a deep network with two layers using scikit-learn. We will then implement
    a deep network with TensorFlow and PyTorch.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本实操部分将从实现一个浅层网络开始，随后使用 scikit-learn 构建一个包含两层的深度网络。然后，我们将用 TensorFlow 和 PyTorch
    实现一个深度网络。
- en: Implementing neural networks from scratch
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从零开始实现神经网络
- en: To demonstrate how activation functions work, we will use sigmoid as the activation
    function in this example.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示激活函数的工作原理，我们将在这个示例中使用 sigmoid 作为激活函数。
- en: 'We first define the `sigmoid` function and its derivative function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义`sigmoid`函数及其导数函数：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can derive the derivative yourself if you are interested in verifying it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣验证它，你可以自行推导导数。
- en: 'We then define the training function, which takes in the training dataset,
    the number of units in the hidden layer (we will only use one hidden layer as
    an example), and the number of iterations:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义训练函数，该函数接受训练数据集、隐藏层单元数（我们将仅使用一个隐藏层作为示例）和迭代次数：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that besides weights, *W*, we also employ bias, *b*. Before training, we
    first randomly initialize weights and biases. In each iteration, we feed all layers
    of the network with the latest weights and biases, then calculate the gradients
    using the backpropagation algorithm, and finally, update the weights and biases
    with the resulting gradients. For training performance inspection, we print out
    the loss and the MSE for every 100 iterations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了权重 *W* 外，我们还使用偏置 *b*。在训练之前，我们首先随机初始化权重和偏置。在每次迭代中，我们将网络的所有层馈送上最新的权重和偏置，然后使用反向传播算法计算梯度，最后用得到的梯度更新权重和偏置。为了检查训练性能，我们每100次迭代输出一次损失和
    MSE。
- en: 'To test the model, we will use California house prices as the example dataset
    again. As a reminder, data normalization is usually recommended whenever gradient
    descent is used. Hence, we will standardize the input data by removing the mean
    and scaling to unit variance:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试模型，我们将再次使用加利福尼亚房价作为示例数据集。提醒一下，使用梯度下降时通常建议进行数据归一化。因此，我们将通过去除均值并缩放到单位方差来标准化输入数据：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With the scaled dataset, we can now train a one-layer neural network with `20`
    hidden units, a `0.1` learning rate, and `2000` iterations:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用缩放后的数据集，我们现在可以训练一个包含`20`个隐藏单元、学习率为`0.1`，并进行`2000`次迭代的单层神经网络：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we define a prediction function, which will take in a model and produce
    the regression results:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个预测函数，该函数将接受一个模型并生成回归结果：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we apply the trained model on the testing set:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练好的模型应用到测试集上：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Print out the predictions and their ground truths to compare them:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出预测结果及其真实值以进行对比：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After successfully building a neural network model from scratch, we will move
    on to the implementation with scikit-learn.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功构建了一个从零开始的神经网络模型之后，我们将开始使用scikit-learn实现它。
- en: Implementing neural networks with scikit-learn
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现神经网络
- en: 'We will utilize the `MLPRegressor` class (**MLP** stands for **multi-layer
    perceptron**, a nickname for neural networks) to implement neural networks:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`MLPRegressor`类（**MLP**代表**多层感知器**，是神经网络的别名）来实现神经网络：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `hidden_layer_sizes` hyperparameter represents the number of hidden neurons.
    In this example, the network contains two hidden layers with `16` and `8` nodes,
    respectively. ReLU activation is used.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`hidden_layer_sizes`超参数表示隐藏神经元的数量。在这个例子中，网络包含两个隐藏层，分别有`16`个节点和`8`个节点。使用ReLU激活函数。'
- en: The Adam optimizer is a replacement for the stochastic gradient descent algorithm.
    It updates the gradients adaptively based on training data. For more information
    about Adam, check out the paper at [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化器是随机梯度下降算法的替代品。它基于训练数据自适应地更新梯度。有关Adam的更多信息，请查看[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)中的论文。
- en: 'We fit the neural network model on the training set and predict on the testing
    data:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上拟合神经网络模型，并在测试数据上进行预测：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And we calculate the MSE on the prediction:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算预测的MSE：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We’ve implemented a neural network with scikit-learn. Let’s do so with TensorFlow
    in the next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用scikit-learn实现了神经网络。接下来，我们将在下一部分使用TensorFlow实现神经网络。
- en: Implementing neural networks with TensorFlow
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现神经网络
- en: 'In TensorFlow 2.x, it is simple to initiate a deep neural network model using
    the Keras ([https://keras.io/](https://keras.io/)) module. Let’s implement neural
    networks with TensorFlow by following these steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2.x中，通过Keras模块（[https://keras.io/](https://keras.io/)）启动一个深度神经网络模型非常简单。让我们按照以下步骤使用TensorFlow实现神经网络：
- en: 'First, we import the necessary modules and set a random seed, which is recommended
    for reproducible modeling:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的模块并设置随机种子，推荐使用随机种子以确保模型的可重复性：
- en: '[PRE13]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we create a Keras Sequential model by passing a list of layer instances
    to the constructor, including two fully connected hidden layers with `16` nodes
    and `8` nodes, respectively. And again, ReLU activation is used:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过将一系列层实例传递给构造函数来创建一个Keras顺序模型，其中包括两个完全连接的隐藏层，分别包含`16`个节点和`8`个节点。同时，使用ReLU激活函数：
- en: '[PRE14]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We compile the model by using Adam as the optimizer with a learning rate of
    `0.01` and MSE as the learning goal:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用Adam优化器，学习率为`0.01`，并将MSE作为学习目标来编译模型：
- en: '[PRE15]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After defining the model, we now train it against the training set:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义模型之后，我们现在开始在训练集上进行训练：
- en: '[PRE16]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We fit the model with `300` iterations. In each iteration, the training loss
    (MSE) is displayed.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`300`次迭代来拟合模型。在每次迭代中，都会显示训练损失（MSE）。
- en: 'Finally, we use the trained model to predict the testing cases and print out
    the predictions and their MSE:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的模型对测试案例进行预测，并打印出预测结果及其MSE：
- en: '[PRE17]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, we add layer by layer to the neural network model in the TensorFlow
    Keras API. We start from the first hidden layer (with 16 nodes), then the second
    hidden layer (with 8 nodes), and finally, the output layer (with 1 unit, the target
    variable). It is quite similar to building with LEGO.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在TensorFlow Keras API中逐层添加神经网络模型。我们从第一个隐藏层（16个节点）开始，然后是第二个隐藏层（8个节点），最后是输出层（1个单元，目标变量）。这与构建乐高积木非常相似。
- en: In the industry, neural networks are often implemented with PyTorch. Let’s see
    how to do it in the next section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业界，神经网络通常使用PyTorch实现。让我们在下一部分看看如何实现。
- en: Implementing neural networks with PyTorch
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch实现神经网络
- en: 'We will now implement neural networks with PyTorch by following these steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将按照以下步骤使用PyTorch实现神经网络：
- en: 'First, we import the necessary modules and set a random seed, which is recommended
    for reproducible modeling:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的模块并设置随机种子，推荐使用随机种子以确保模型的可重复性：
- en: '[PRE18]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we create a `torch.nn` Sequential model by passing a list of layer instances
    to the constructor, including two fully connected hidden layers with `16` nodes
    and `8` nodes, respectively. Again, ReLU activation is used in each fully connected
    layer:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过将一个包含两个全连接隐藏层（分别有 `16` 个节点和 `8` 个节点）实例的层列表传递给构造函数，创建一个 `torch.nn` Sequential
    模型。每个全连接层中都使用了 ReLU 激活函数：
- en: '[PRE19]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We initialize an Adam optimizer with a learning rate of `0.01` and MSE as the
    learning goal:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化一个学习率为`0.01`、目标为 MSE 的 Adam 优化器：
- en: '[PRE20]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After defining the model, we need to create tensor objects from the input NumPy
    arrays before using them to train the PyTorch model:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义完模型后，我们需要在使用它训练 PyTorch 模型之前，从输入的 NumPy 数组中创建张量对象：
- en: '[PRE21]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can train the model against the PyTorch-compatible training set. We
    first define a training function that will be called in each epoch as follows:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以在与 PyTorch 兼容的训练集上训练模型。我们首先定义一个训练函数，在每个周期内调用它，如下所示：
- en: '[PRE22]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We fit the model with `500` iterations. In every 100 iterations, the training
    loss (MSE) is displayed as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们用`500`次迭代来训练模型。在每 100 次迭代中，显示训练损失（MSE）如下：
- en: '[PRE23]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we use the trained model to predict the testing cases and print out
    the predictions and their MSE:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的模型预测测试样本并输出预测值及其 MSE：
- en: '[PRE24]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It turns out that developing a neural network model with PyTorch is as simple
    as building with LEGO too.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，使用 PyTorch 开发神经网络模型就像搭建乐高一样简单。
- en: Both PyTorch and TensorFlow are popular deep learning frameworks, and their
    popularity can vary depending on different factors such as application domains,
    research communities, industry adoption, and personal preferences. However, as
    of 2023, PyTorch has been more widely adopted and has a larger user base overall,
    according to Papers With Code ([https://paperswithcode.com/trends](https://paperswithcode.com/trends))
    and Google Trends ([https://trends.google.com/trends/explore?geo=US&q=tensorflow,pytorch&hl=en](https://trends.google.com/trends/explore?geo=US&q=tensorflow,pytorch&hl=en)).
    Hence, we will focus on PyTorch implementations for DL throughout the rest of
    the book.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 和 TensorFlow 都是流行的深度学习框架，它们的流行程度可能会因应用领域、研究社区、行业采纳和个人偏好等不同因素而有所不同。然而，截至
    2023 年，PyTorch 的普及度更高，整体用户基础更大，根据 Papers With Code（[https://paperswithcode.com/trends](https://paperswithcode.com/trends)）和
    Google Trends（[https://trends.google.com/trends/explore?geo=US&q=tensorflow,pytorch&hl=en](https://trends.google.com/trends/explore?geo=US&q=tensorflow,pytorch&hl=en)）的数据。因此，接下来我们将专注于本书中的
    PyTorch 实现。
- en: Next, we will look at how to choose the right activation functions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何选择合适的激活函数。
- en: Picking the right activation functions
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的激活函数
- en: 'So far, we have used the ReLU and sigmoid activation functions in our implementations.
    You may wonder how to pick the right activation function for your neural networks.
    Detailed advice on when to choose a particular activation function is given next:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在实现中使用了 ReLU 和 sigmoid 激活函数。你可能会想知道如何为你的神经网络选择合适的激活函数。接下来将给出详细的建议，告诉你什么时候选择特定的激活函数：
- en: '**Linear**: *f(z) = z*. You can interpret this as no activation function. We
    usually use it in the output layer in regression networks as we don’t need any
    transformation to the outputs.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linear**：*f(z) = z*。你可以将其解释为没有激活函数。我们通常在回归网络的输出层中使用它，因为我们不需要对输出进行任何转换。'
- en: '**Sigmoid** (logistic) transforms the output of a layer to a range between
    0 and 1\. You can interpret it as the probability of an output prediction. Therefore,
    we usually use it in the output layer in **binary classification** networks. Besides
    that, we sometimes use it in hidden layers. However, it should be noted that the
    sigmoid function is monotonic but its derivative is not. Hence, the neural network
    may get stuck at a suboptimal solution.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**（逻辑）将层的输出转换为 0 和 1 之间的范围。你可以将其理解为输出预测的概率。因此，我们通常在**二分类**网络的输出层中使用它。除此之外，有时我们也会在隐藏层中使用它。然而，需要注意的是，sigmoid
    函数是单调的，但其导数并非单调。因此，神经网络可能会陷入一个次优解。'
- en: '**Softmax**: As was mentioned in *Chapter 4*, *Predicting Online Ad Click-Through
    with Logistic Regression*, softmax is a generalized logistic function used for
    multiclass classification. Hence, we use it in the output layer in **multiclass
    classification** networks.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：正如在*第4章*《使用逻辑回归预测在线广告点击率》中提到的，softmax 是一种广义的逻辑函数，用于多类别分类。因此，我们在**多类别分类**网络的输出层中使用它。'
- en: '**Tanh** is a better version of the sigmoid function with stronger gradients.
    As you can see in the plots earlier in the chapter, the derivatives in the tanh
    function are steeper than those for the sigmoid function. It has a range of `-1`
    to `1`. It is common to use the `tanh` function in hidden layers.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tanh** 是一种比 sigmoid 更好的激活函数，具有更强的梯度。正如本章早些时候的图表所示，tanh 函数的导数比 sigmoid 函数的导数更陡峭。它的取值范围是
    `-1` 到 `1`。在隐藏层中，常常使用 `tanh` 函数。'
- en: '**ReLU** is probably the most frequently used activation function nowadays.
    It is the “default” one in hidden layers in feedforward networks. Its range is
    from `0` to infinity, and both the function itself and its derivative are monotonic.
    It has several benefits over `tanh`. One is sparsity, meaning that only a subset
    of neurons are activated at any given time. This can help reduce the computational
    cost of training and inference, as fewer neurons need to be computed. ReLU also
    mitigates the vanishing gradient problem, which occurs when gradients become very
    small during backpropagation, leading to slow or stalled learning. ReLU does not
    saturate for positive inputs, allowing gradients to flow more freely during training.
    One drawback of the ReLU function is the inability to appropriately map the negative
    part of the input where all negative inputs are transformed to 0\. To fix the
    “dying negative” problem in ReLU, **Leaky ReLU** was invented to introduce a small
    slope in the negative part. When *z < 0*, *f(z) = az*, where *a* is usually a
    small value, such as `0.01`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU** 可能是当今使用最频繁的激活函数。它是前馈网络中隐藏层的“默认”激活函数。它的取值范围是从 `0` 到无穷大，且该函数及其导数都是单调的。与
    `tanh` 相比，ReLU 有几个优势。首先是稀疏性，意味着在任何给定时刻，只有一部分神经元被激活。这有助于减少训练和推理的计算成本，因为只需计算较少的神经元。ReLU
    还可以缓解梯度消失问题，即在反向传播过程中，梯度变得非常小，导致学习变慢或停滞。ReLU 对于正输入不饱和，允许梯度在训练过程中更自由地流动。ReLU 的一个缺点是无法适当地映射输入的负部分，所有负输入都会被转换为
    0。为了解决 ReLU 中的“负值消失”问题，**Leaky ReLU** 被发明出来，它在负部分引入了一个小的斜率。当 *z < 0* 时，*f(z) =
    az*，其中 *a* 通常是一个小值，如 `0.01`。'
- en: To recap, ReLU is usually in hidden layer activation. You can try Leaky ReLU
    if ReLU doesn’t work well. Sigmoid and tanh can be used in hidden layers but are
    not recommended in deep networks with many layers. For the output layer, linear
    activation (or no activation) is used in the regression network; sigmoid is for
    the binary classification network and softmax is for the multiple classification
    case.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，ReLU 通常用于隐藏层激活函数。如果 ReLU 效果不好，可以尝试 Leaky ReLU。Sigmoid 和 tanh 可以用于隐藏层，但不建议在层数很多的深度网络中使用。在输出层，回归网络使用线性激活（或没有激活）；二分类网络使用
    sigmoid；多分类问题则使用 softmax。
- en: Picking the right activation is important, and so is avoiding overfitting in
    neural networks. Let’s see how to do this in the next section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的激活函数非常重要，同样避免神经网络中的过拟合也是如此。接下来我们看看如何做到这一点。
- en: Preventing overfitting in neural networks
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防止神经网络中过拟合
- en: A neural network is powerful as it can derive hierarchical features from data
    with the right architecture (the right number of hidden layers and hidden nodes).
    It offers a great deal of flexibility and can fit a complex dataset. However,
    this advantage will become a weakness if the network is not given enough control
    over the learning process. Specifically, it may lead to overfitting if a network
    is only good at fitting to the training set but is not able to generalize to unseen
    data. Hence, preventing overfitting is essential to the success of a neural network
    model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络之所以强大，是因为它能够从数据中提取层次特征，只要有合适的架构（合适的隐藏层数和隐藏节点数）。它提供了极大的灵活性，并且可以拟合复杂的数据集。然而，如果网络在学习过程中没有足够的控制，这一优势就会变成劣势。具体来说，如果网络仅仅擅长拟合训练集而不能推广到未见过的数据，就可能导致过拟合。因此，防止过拟合对于神经网络模型的成功至关重要。
- en: 'There are mainly three ways to impose restrictions on our neural networks:
    L1/L2 regularization, dropout, and early stopping. We practiced the first method
    in *Chapter 4*, *Predicting Online Ad Click-Through with Logistic Regression*,
    and will discuss the other two in this section.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目前主要有三种方法可以对神经网络进行限制：L1/L2 正则化、dropout 和提前停止。在*第4章*《使用逻辑回归预测在线广告点击率》中我们练习了第一种方法，接下来将在本节讨论另外两种方法。
- en: Dropout
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dropout
- en: '**Dropout** means ignoring a certain set of hidden nodes during the learning
    phase of a neural network. And those hidden nodes are chosen randomly given a
    specified probability. In the forward pass during a training iteration, the randomly
    selected nodes are temporarily not used in calculating the loss; in the backward
    pass, the randomly selected nodes are not updated temporarily.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**丢弃**意味着在神经网络的学习阶段忽略某些隐藏节点。这些隐藏节点是根据指定的概率随机选择的。在训练过程的前向传播中，随机选择的节点会暂时不用于计算损失；在反向传播中，随机选择的节点也不会暂时更新。'
- en: 'In the following diagram, we choose three nodes in the network to ignore during
    training:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们选择了网络中的三个节点，在训练过程中忽略它们：
- en: '![A diagram of a network  Description automatically generated with medium confidence](img/B21047_06_06.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![一个网络图，描述自动生成，信心中等](img/B21047_06_06.png)'
- en: 'Figure 6.6: Three nodes to ignore in a neural network'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：在神经网络中需要忽略的三个节点
- en: Recall that a regular layer has nodes fully connected to nodes from the previous
    layer and the following layer. It will lead to overfitting if a large network
    develops and memorizes the co-dependency between individual pairs of nodes. Dropout
    breaks this co-dependency by temporarily deactivating certain nodes in each iteration.
    Therefore, it effectively reduces overfitting and won’t disrupt learning at the
    same time.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，常规层的节点与前一层和下一层的节点是完全连接的。如果网络规模过大，模型会记住个别节点对之间的相互依赖，导致过拟合。丢弃通过在每次迭代中暂时停用某些节点来打破这种依赖关系。因此，它有效地减少了过拟合，并且不会同时扰乱学习过程。
- en: The fraction of nodes being randomly chosen in each iteration is also called
    the dropout rate. In practice, we usually set a dropout rate no greater than 50%.
    If the dropout rate is too high, it can excessively hinder the model’s learning
    capacity, slowing down training and reducing the model’s ability to extract useful
    patterns from the data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中随机选择的节点比例也称为丢弃率。在实践中，我们通常将丢弃率设置为不超过50%。如果丢弃率过高，会过度影响模型的学习能力，减慢训练速度，并降低模型从数据中提取有用模式的能力。
- en: '**Best practice**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Determining the dropout rate empirically involves experimenting with different
    dropout rates and evaluating their effects on the model’s performance. Here’s
    a typical approach:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 确定丢弃率需要通过经验进行实验，测试不同丢弃率对模型性能的影响。以下是一个典型的方法：
- en: Start with a low rate (e.g., `0.1` or `0.2`) and train the model on your dataset.
    Monitor the model’s performance metrics on a validation set.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个较低的丢弃率（例如`0.1`或`0.2`）开始，并在你的数据集上训练模型。监控模型在验证集上的性能指标。
- en: Gradually increase the dropout rate in small increments (e.g., by `0.1`) and
    retrain the model each time. Monitor the performance metrics after each training
    run.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐渐以小幅度（例如`0.1`）增加丢弃率，并每次重新训练模型。在每次训练后监控性能指标。
- en: Evaluate performance obtained with different dropout rates. Be mindful of overfitting,
    as too high of a dropout rate can hinder model performance; if the dropout rate
    is too low, the model may not effectively prevent overfitting.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估在不同丢弃率下获得的性能。注意过拟合问题，因为过高的丢弃率可能会影响模型性能；如果丢弃率过低，模型可能无法有效防止过拟合。
- en: 'In PyTorch, we use the `torch.nn.Dropout` object to add dropout to a layer.
    An example is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们使用`torch.nn.Dropout`对象将丢弃添加到层中。以下是一个示例：
- en: '[PRE25]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding example, 10% of nodes randomly picked from the first hidden
    layer are ignored in an iteration during training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，训练过程中在每次迭代时随机忽略了来自第一隐藏层的10%节点。
- en: 'Keep in mind that dropout should only occur in the training phase. In the prediction
    phase, all nodes should be fully connected again. Hence, we have to switch the
    model to evaluation mode with the `.eval()` method to disable dropout before we
    evaluate the model or make predictions with the trained model. Let’s see it in
    the following California housing example:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，丢弃只应发生在训练阶段。在预测阶段，所有节点应该重新完全连接。因此，在评估模型或使用训练好的模型进行预测之前，我们需要通过`.eval()`方法将模型切换到评估模式，以禁用丢弃。我们可以在下面的加利福尼亚住房示例中看到：
- en: 'First, we compile the model (with dropout) by using Adam as the optimizer with
    a learning rate of `0.01` and MSE as the learning goal:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用Adam优化器和学习率`0.01`，以MSE为学习目标来编译模型（带丢弃）：
- en: '[PRE26]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we can train the model (with dropout) for `1,000` iterations:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以训练模型（带丢弃）`1,000`次迭代：
- en: '[PRE27]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In every 100 iterations, the training loss (MSE) is displayed.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 每100次迭代，显示一次训练损失（MSE）。
- en: 'Finally, we use the trained model (with dropout) to predict the testing cases
    and print out the MSE:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的模型（带有dropout）来预测测试案例并输出均方误差（MSE）：
- en: '[PRE28]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As mentioned earlier, don’t forget to run `model_with_dropout.eval()` before
    evaluating the model with dropout. Otherwise, the dropout layers will continue
    to randomly deactivate neurons, leading to inconsistent results between different
    model evaluations on the same data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，别忘了在评估带有dropout的模型之前运行`model_with_dropout.eval()`。否则，dropout层将继续随机关闭神经元，导致对相同数据进行多次评估时结果不一致。
- en: Early stopping
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止
- en: As the name implies, training a network with **early stopping** will end if
    the model performance doesn’t improve for a certain number of iterations. The
    model performance is measured on a validation set that is different from the training
    set, in order to assess how well it generalizes. During training, if the performance
    degrades after several (let’s say 50) iterations, it means the model is overfitting
    and not able to generalize well anymore. Hence, stopping the learning early in
    this case helps prevent overfitting. Usually, we evaluate the model against a
    validation set. If the metric on the validation set is not improving for more
    than *n* epochs, we stop the training process.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，使用**提前停止**训练的网络将在模型性能在一定次数的迭代内没有改进时停止训练。模型性能是通过与训练集不同的验证集来评估的，目的是衡量其泛化能力。在训练过程中，如果经过若干次（比如50次）迭代后性能下降，说明模型发生了过拟合，无法再很好地进行泛化。因此，在这种情况下，提前停止学习有助于防止过拟合。通常，我们会通过验证集来评估模型。如果验证集上的指标在超过*n*个训练周期内没有改善，我们就会停止训练过程。
- en: 'We will demonstrate how to apply early stopping in PyTorch using the California
    housing example as well:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将演示如何在PyTorch中应用提前停止，并使用加利福尼亚房价示例：
- en: 'First, we recreate the model and optimizer as we did previously:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们像之前一样重新创建模型和优化器：
- en: '[PRE29]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we define the early stopping criterion as the test loss doesn’t improve
    for more than `100` epochs:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义提前停止标准，即测试损失在`100`个周期内没有改进：
- en: '[PRE30]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we adopt early stopping and train the model for, at most, `500` iterations:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们采用提前停止，并将模型训练至最多`500`个迭代周期：
- en: '[PRE31]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Following every training step, we compute the test loss and compare it to the
    previously recorded best one. If it shows improvement, we save the current model
    using the `copy` module and reset the `epochs_no_improve` counter. However, if
    there is no improvement in the test loss for up to 100 consecutive iterations,
    we stop the training process as we have reached the tolerance threshold (`patience`).
    In our example, training stopped after epoch `224`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步训练后，我们计算测试损失并将其与之前记录下来的最佳损失进行比较。如果测试损失有所改善，我们使用`copy`模块保存当前模型，并重置`epochs_no_improve`计数器。然而，如果测试损失在100次连续迭代中没有改善，我们就会停止训练过程，因为已经达到了容忍阈值（`patience`）。在我们的例子中，训练在第`224`个周期后停止。
- en: 'Finally, we use the previously recorded best model to predict the testing cases
    and print out the predictions and their MSE:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用先前记录下来的最佳模型来预测测试案例，并输出预测结果及其均方误差（MSE）：
- en: '[PRE32]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This is better than `0.0069`, which we obtained in the vanilla approach, and
    `0.0057`, which we achieved using dropout for overfitting prevention.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们在传统方法中得到的`0.0069`要好，也比使用dropout防止过拟合得到的`0.0057`要好。
- en: While the Universal Approximation Theorem guarantees that neural networks can
    represent any function, it doesn’t guarantee good generalization performance.
    Overfitting can occur if the model has too much capacity relative to the complexity
    of the underlying data distribution. Therefore, controlling the capacity of the
    model through techniques like regularization and early stopping is essential to
    ensure that the learned function generalizes well to unseen data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通用逼近定理保证神经网络能够表示任何函数，但它并不保证良好的泛化性能。如果模型的容量相对于数据分布的复杂度过大，就可能发生过拟合。因此，通过正则化和提前停止等技术来控制模型的容量，对于确保学习到的函数能够很好地泛化到未见过的数据至关重要。
- en: Now that you’ve learned about neural networks and their implementation, let’s
    utilize them to solve our stock price prediction problem.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了神经网络及其实现，接下来我们将利用它们来解决股票价格预测问题。
- en: Predicting stock prices with neural networks
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络预测股票价格
- en: We will build the stock predictor with PyTorch in this section. We will start
    with feature generation and data preparation, followed by network building and
    training. After that, we will fine-tune the network to boost the stock predictor.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 PyTorch 构建股票预测器。我们将从特征生成和数据准备开始，然后构建网络并进行训练。之后，我们将微调网络以提升股票预测器的性能。
- en: Training a simple neural network
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个简单的神经网络
- en: 'We prepare data and train a simple neural work with the following steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下步骤准备数据并训练一个简单的神经网络：
- en: 'We load the stock data, generate features, and label the `generate_features`
    function we developed in *Chapter 5*, *Predicting Stock Prices with Regression
    Algorithms*:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载股票数据，生成特征，并标记 `generate_features` 函数，这个函数我们在 *第 5 章*，*使用回归算法预测股票价格* 中开发过：
- en: '[PRE33]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We construct the training set using data from 1990 to 2022 and the testing
    set using data from the first half of 2023:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 1990 到 2022 年的数据构建训练集，并使用 2023 年上半年的数据构建测试集：
- en: '[PRE34]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We need to normalize features into the same or a comparable scale. We do so
    by removing the mean and rescaling to unit variance:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将特征标准化到相同或可比较的尺度。我们通过去除均值并缩放到单位方差来实现这一点：
- en: '[PRE35]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We rescale both sets with the scaler taught by the training set:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练集教授的缩放器对两个数据集进行重新缩放：
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we need to create tensor objects from the input NumPy arrays before using
    them to train the PyTorch model:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要从输入的 NumPy 数组中创建张量对象，然后使用它们来训练 PyTorch 模型：
- en: '[PRE37]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We now build a neural network using the `torch.nn` module:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用 `torch.nn` 模块构建一个神经网络：
- en: '[PRE38]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The network we begin with has one hidden layer with `32` nodes followed by a
    ReLU function.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始时的网络有一个包含 `32` 个节点的隐藏层，后面接着一个 ReLU 函数。
- en: 'We compile the model by using Adam as the optimizer with a learning rate of
    `0.3` and MSE as the learning goal:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用 Adam 作为优化器，学习率为 `0.3`，MSE 作为学习目标来编译模型：
- en: '[PRE39]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After defining the model, we perform training for `1,000` iterations:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型后，我们进行 `1,000` 次迭代的训练：
- en: '[PRE40]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Finally, we use the trained model to predict the testing data and display metrics:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的模型来预测测试数据并显示指标：
- en: '[PRE41]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We achieve an *R*² of `0.954` with a simple neural network model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个简单的神经网络模型达到了 *R*² 为 `0.954`。
- en: Fine-tuning the neural network
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调神经网络
- en: 'Can we do better? Of course, we haven’t fine-tuned the hyperparameters yet.
    We perform model fine-tuning in PyTorch with the following steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？当然可以，我们还没有微调超参数。我们将在 PyTorch 中通过以下步骤进行模型微调：
- en: 'TensorBoard provides functionality for logging various metrics and visualizations
    during model training and evaluation. You can use TensorBoard with PyTorch to
    track and visualize metrics such as loss, accuracy, gradients, and model architectures,
    among others. We rely on the `tensorboard` module in PyTorch `utils`, so we import
    it first:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorBoard 提供了在模型训练和评估过程中记录各种指标和可视化的功能。你可以与 PyTorch 一起使用 TensorBoard 来跟踪和可视化诸如损失、准确率、梯度和模型架构等指标。我们依赖于
    PyTorch `utils` 中的 `tensorboard` 模块，所以我们首先导入它：
- en: '[PRE42]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We want to tweak the number of hidden nodes in the hidden layer (again, we
    are using one hidden layer for this example), the number of training iterations,
    and the learning rate. We pick the following values of hyperparameters to experiment
    on:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望调整隐藏层中隐节点的数量（这里，我们使用的是一个隐藏层），训练迭代次数和学习率。我们选择了以下超参数值来进行实验：
- en: '[PRE43]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, we experiment with two options for the number of hidden nodes, `16` and
    `32`; we use two options for the number of iterations, `300` and `1000`; and we
    use two options, `0.1` and `0.3`, for the learning rate.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们尝试了两个隐层节点数选项，`16` 和 `32`；我们使用了两个迭代次数选项，`300` 和 `1000`；还使用了两个学习率选项，`0.1`
    和 `0.3`。
- en: 'After initializing the hyperparameters to optimize, we will iterate each hyperparameter
    combination, and train and validate the model using a given set of hyperparameters
    by calling the helper function `train_validate_model` as follows:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化优化的超参数后，我们将对每个超参数组合进行迭代，并通过调用帮助函数 `train_validate_model` 来训练和验证模型，具体如下：
- en: '[PRE44]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Here, in each hyperparameter combination, we build and fit a neural network
    model based on the given hyperparameters, including the number of hidden nodes,
    the learning rate, and the number of training iterations. There’s nothing much
    different here from what we did before. But when we train the model, we also update
    TensorBoard by logging the hyperparameters and metrics including the train loss
    and test loss with the `add_scalar` method.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个超参数组合中，我们根据给定的超参数（包括隐藏节点的数量、学习率和训练迭代次数）构建并拟合一个神经网络模型。这里与之前做的没有太大区别。但在训练模型时，我们还会通过
    `add_scalar` 方法更新 TensorBoard，记录超参数和指标，包括训练损失和测试损失。
- en: The TensorBoard writer object is straightforward. It provides visualization
    for the model graph and metrics during training and validation.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 编写器对象非常直观。它为训练和验证过程中的模型图和指标提供可视化。
- en: At the end, we compute and display the *R*² of the prediction on the test set.
    We also log the test MSE and *R*² using the `add_hparams` method along with the
    given hyperparameter combination.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算并显示测试集预测的 *R*²。我们还使用 `add_hparams` 方法记录测试集的 MSE 和 *R*²，并附上给定的超参数组合。
- en: 'Now we fine-tune the neural network by iterating eight hyperparameter combinations:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过迭代八种超参数组合来微调神经网络：
- en: '[PRE45]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You will see the following output:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下输出：
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Experiment 4 with the combination of `(hidden_size=16, epochs=3000, learning_rate=0.3)`
    is the best performing one, where we achieve an *R*² of `0.977`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用超参数组合 `(hidden_size=16, epochs=3000, learning_rate=0.3)` 的实验 4 表现最佳，我们获得了
    *R*² 为 `0.977`。
- en: '**Best practice**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Hyperparameter tuning for neural networks can significantly impact model performance.
    Here are some best practices for hyperparameter tuning in neural networks:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的超参数调优可以显著影响模型性能。以下是一些神经网络超参数调优的最佳实践：
- en: '**Define a search space**: Determine which hyperparameters to tune and their
    ranges. Common hyperparameters include learning rate, batch size, number of hidden
    layers, number of neurons per layer, activation functions, dropout rates, and
    so on.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义搜索空间**：确定需要调整的超参数及其范围。常见的超参数包括学习率、批量大小、隐藏层数量、每层神经元数量、激活函数、丢弃率等。'
- en: '**Use cross-validation**: This helps prevent overfitting and provides a more
    robust estimate of model performance.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用交叉验证**：这有助于防止过拟合，并提供更稳健的模型性能估计。'
- en: '**Monitor performance metrics**: Track relevant metrics such as loss, accuracy,
    precision, recall, MSE, *R*², and so on during training and validation.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控性能指标**：在训练和验证过程中，跟踪相关的指标，如损失、准确率、精确度、召回率、均方误差（MSE）、*R*² 等。'
- en: '**Early stopping**: Monitor the validation loss during training, and stop training
    when it starts to increase consistently while the training loss decreases.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早停法**：在训练过程中监控验证损失，当验证损失持续增加，而训练损失减少时，停止训练。'
- en: '**Regularization**: Use regularization techniques such as L1 and L2 regularization
    and dropout to prevent overfitting and improve generalization performance.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：使用 L1 和 L2 正则化以及丢弃法等正则化技术，防止过拟合并提高模型的泛化性能。'
- en: '**Experiment with different architectures**: Try different network architectures,
    including the number of layers, the number of neurons per layer, and the activation
    functions. Experiment with deep vs. shallow networks and wide vs. narrow networks.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尝试不同的架构**：尝试不同的网络架构，包括层数、每层神经元数量和激活函数。可以尝试深度与浅层网络、宽网络与窄网络。'
- en: '**Use parallelism**: If computational resources allow, parallelize the hyperparameter
    search process to speed up experimentation. Tools like TensorFlow’s `tf.distribute.Strategy`
    or PyTorch’s `torch.nn.DataParallel` can be used to distribute training across
    multiple GPUs or machines.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用并行化**：如果计算资源允许，可以将超参数搜索过程并行化，以加速实验过程。可以使用像 TensorFlow 的 `tf.distribute.Strategy`
    或 PyTorch 的 `torch.nn.DataParallel` 这样的工具，将训练分布到多个 GPU 或机器上。'
- en: 'You will notice that a new folder, `runs`, is created after these experiments
    start. It contains the training and validation performance for each experiment.
    After 8 experiments finish, it’s time to launch TensorBoard. We use the following
    command:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会注意到，在这些实验开始后，会创建一个新的文件夹 `runs`。它包含每次实验的训练和验证性能。在 8 次实验完成后，是时候启动 TensorBoard
    了。我们使用以下命令：
- en: '[PRE47]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Once it is launched, you will see the beautiful dashboard at `http://localhost:6006/`.
    You can see a screenshot of the expected result here:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 启动后，你会看到美观的仪表板，访问地址为 `http://localhost:6006/`。你可以在这里看到预期结果的截图：
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_06_07.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图 说明自动生成](img/B21047_06_07.png)'
- en: 'Figure 6.7: Screenshot of TensorBoard'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：TensorBoard 截图
- en: The time series of train and test loss provide valuable insights. They allow
    us to assess the progress of training and identify signs of overfitting or underfitting.
    Overfitting can be identified when the train loss decreases over time while the
    test loss remains stagnant or increases. On the other hand, underfitting is indicated
    by relatively high train and test loss values, indicating that the model fails
    to adequately fit the training data.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 火车和测试损失的时间序列提供了有价值的见解。它们让我们能够评估训练进度，并识别过拟合或欠拟合的迹象。过拟合可以通过观察训练损失随时间下降，而测试损失保持停滞或上升来识别。另一方面，欠拟合则表现为训练损失和测试损失值较高，表明模型未能充分拟合训练数据。
- en: 'Next, we click on the **HPARAMS** tab to see the hyperparameter logs. You can
    see all the hyperparameter combinations and the respective metrics (MSE and *R*²)
    displayed in a table, as shown here:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们点击 **HPARAMS** 标签查看超参数日志。你可以看到所有的超参数组合及其对应的指标（MSE 和 *R*²）以表格形式展示，如下所示：
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_06_08.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图 说明自动生成](img/B21047_06_08.png)'
- en: 'Figure 6.8: Screenshot of TensorBoard for hyperparameter tuning'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：TensorBoard 超参数调优的截图
- en: Again, you can see experiment 4 yields the best performance.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 再次可以看到，实验 4 的表现最好。
- en: 'Finally, we use the optimal model to make predictions:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用最优模型进行预测：
- en: '[PRE48]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Plot the prediction along with the ground truth as follows:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，绘制预测值与真实值的对比图：
- en: '[PRE49]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Refer to the following screenshot for the result:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下截图以查看结果：
- en: '![A graph with blue lines and numbers  Description automatically generated](img/B21047_06_09.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![带有蓝色线条和数字的图表 说明自动生成](img/B21047_06_09.png)'
- en: 'Figure 6.9: Prediction and ground truth of stock prices'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：股票价格的预测与真实值对比
- en: The fine-tuned neural network does a good job of predicting stock prices.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的神经网络在预测股票价格方面表现良好。
- en: In this section, we further improved the neural network stock predictor by fine-tuning
    the hyperparameters. Feel free to use more hidden layers, or apply dropout or
    early stopping to see whether you can get a better result.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过微调超参数进一步改进了神经网络股票预测器。你可以自由地增加更多的隐藏层，或应用丢弃法（dropout）或早停法（early stopping），看看是否能获得更好的结果。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we worked on the stock prediction project again, but with neural
    networks this time. We started with a detailed explanation of neural networks,
    including the essential components (layers, activations, feedforward, and backpropagation),
    and transitioned to DL. We moved on to implementations from scratch with scikit-learn,
    TensorFlow, and PyTorch. We also learned about ways to avoid overfitting, such
    as dropout and early stopping. Finally, we applied what we covered in this chapter
    to solve our stock price prediction problem.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们再次进行了股票预测项目的工作，不过这次是使用神经网络。我们从详细解释神经网络开始，包括基本组件（层、激活函数、前向传播和反向传播），并过渡到深度学习（DL）。接着，我们使用
    scikit-learn、TensorFlow 和 PyTorch 从头开始进行实现。我们还学习了避免过拟合的方法，如丢弃法和早停法。最后，我们将本章所学应用于解决股票价格预测问题。
- en: In the next chapter, we will explore NLP techniques and unsupervised learning.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索自然语言处理（NLP）技术和无监督学习。
- en: Exercises
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: As mentioned, can you use more hidden layers in the neural network stock predictor
    and rerun the model fine-tuning? Can you get a better result?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，能否在神经网络股票预测器中使用更多的隐藏层并重新调整模型？你能得到更好的结果吗？
- en: Following the first exercise, can you apply dropout and/or early stopping and
    see if you can beat the current best *R*² of `0.977`?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个练习之后，能否应用丢弃法和/或早停法，看看能否打破当前最佳的 *R*² 值 `0.977`？
- en: Join our book’s Discord space
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
