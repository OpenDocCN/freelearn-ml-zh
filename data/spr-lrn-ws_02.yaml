- en: 2\. Exploratory Data Analysis and Visualization
  id: totrans-0
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2. 探索性数据分析与可视化
- en: Overview
  id: totrans-1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter takes us through how to perform exploration and analysis on a new
    dataset. By the end of this chapter, you will be able to explain the importance
    of data exploration and communicate the summary statistics of a dataset. You will
    visualize patterns in missing values in data and be able to replace null values
    appropriately. You will be equipped to identify continuous features, categorical
    features and visualize distributions of values across individual variables. You
    will also be able to describe and analyze relationships between different types
    of variables using correlation and visualizations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章带领我们了解如何对一个新的数据集进行探索和分析。到本章结束时，你将能够解释数据探索的重要性，并能够传达数据集的汇总统计信息。你将能够可视化数据中缺失值的模式，并能够适当地替换空值。你将学会识别连续特征、分类特征，并可视化各个变量的值分布。你还将能够使用相关性和可视化来描述和分析不同类型变量之间的关系。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 介绍
- en: Say we have a problem statement that involves predicting whether a particular
    earthquake caused a tsunami. How do we decide what model to use? What do we know
    about the data we have? Nothing! But if we don't know and understand our data,
    chances are we'll end up building a model that's not very interpretable or reliable.
    When it comes to data science, it's important to have a thorough understanding
    of the data we're dealing with, in order to generate features that are highly
    informative and, consequently, to build accurate and powerful models. To acquire
    this understanding, we perform an exploratory analysis of the data to see what
    the data can tell us about the relationships between the features and the target
    variable (the value that you are trying to predict using the other variables).
    Getting to know our data will even help us interpret the model we build and identify
    ways we can improve its accuracy. The approach we take to achieve this is to allow
    the data to reveal its structure or model, which helps us gain some new, often
    unsuspected, insight into the data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个问题陈述，涉及预测某次地震是否引发了海啸。我们如何决定使用什么模型？我们对现有的数据了解多少？什么都不知道！但如果我们不了解数据，最终可能会建立一个不太可解释或不可靠的模型。在数据科学中，彻底理解我们所处理的数据非常重要，以便生成高度信息化的特征，并因此构建准确而强大的模型。为了获得这种理解，我们对数据进行探索性分析，看看数据能告诉我们关于特征和目标变量（你试图通过其他变量预测的值）之间关系的信息。了解数据甚至有助于我们解释所构建的模型，并找出改进其准确性的方法。我们采取的做法是让数据揭示其结构或模型，这有助于我们获得一些新的、往往是意想不到的见解。
- en: We will first begin with a brief introduction to exploratory data analysis and
    then progress to explaining summary statistics and central values. This chapter
    also teaches you how to find and visualize missing values and then describes the
    various imputation strategies for addressing the problem of missing values. The
    remainder of the chapter then focuses on visualizations. Specifically, the chapter
    teaches you how to create various plots such as scatter plot, histograms, pie
    charts, heatmaps, pairplots and more. Let us begin with exploratory data analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先简要介绍探索性数据分析，然后逐步解释汇总统计和中心值。本章还将教你如何查找和可视化缺失值，并描述处理缺失值问题的各种填充策略。接下来的部分将专注于可视化。具体来说，本章教你如何创建各种图表，如散点图、直方图、饼图、热图、配对图等。让我们从探索性数据分析开始。
- en: Exploratory Data Analysis (EDA)
  id: totrans-6
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: Exploratory data analysis (EDA) is defined as a method to analyze datasets and
    sum up their main characteristics to derive useful conclusions, often with visual
    methods.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）被定义为一种分析数据集并总结其主要特征的方法，通过这种方法得出有用的结论，通常采用可视化方法。
- en: 'The purpose of EDA is to:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: EDA的目的是：
- en: Discover patterns within a dataset
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 发现数据集中的模式
- en: Spot anomalies
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 发现异常值
- en: Form hypotheses regarding the behavior of data
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据行为形成假设
- en: Validate assumptions
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 验证假设
- en: Everything from basic summary statistics to complex visualizations helps us
    gain an intuitive understanding of the data itself, which is highly important
    when it comes to forming new hypotheses about the data and uncovering what parameters
    affect the target variable. Often, discovering how the target variable varies
    across a single feature gives us an indication of how important a feature might
    be, and a variation across a combination of several features helps us to come
    up with ideas for new informative features to engineer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从基本的摘要统计到复杂的可视化帮助我们直观地理解数据本身，这在形成关于数据的新假设并揭示哪些参数影响目标变量时极为重要。通常，发现目标变量如何在单一特征上变化，会给我们提供该特征可能有多重要的指示，而多个特征组合的变化有助于我们提出新的有信息量的特征工程思路。
- en: Most explorations and visualizations are intended to understand the relationship
    between the features and the target variable. This is because we want to find
    out what relationships exist (or don't exist) between the data we have and the
    values we want to predict.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数探索性分析和可视化的目的是理解特征与目标变量之间的关系。这是因为我们希望找出我们所拥有的数据与我们要预测的值之间存在（或不存在）什么关系。
- en: 'EDA can tell us about:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: EDA可以告诉我们：
- en: Features that are unclean, have missing values, or have outliers
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有缺失值、脏数据或异常值的特征
- en: Features that are informative and are a good indicator of the target
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有助于目标识别的具有信息性的特征
- en: The kind of relationships features have with the target
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 特征与目标之间的关系类型
- en: Further features that the data might need that we don't already have
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可能需要的进一步特征，我们当前并没有
- en: Edge cases you might need to account for separately
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要单独考虑的边缘情况
- en: Filters you might need to apply to the dataset
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要应用于数据集的过滤条件
- en: The presence of incorrect or fake data points
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 存在不正确或虚假数据点
- en: Now that we've looked at why EDA is important and what it can tell us, let's
    talk about what exactly EDA involves. EDA can involve anything from looking at
    basic summary statistics to visualizing complex trends over multiple variables.
    However, even simple statistics and plots can be powerful tools, as they may reveal
    important facts about the data that could change our modeling perspective. When
    we see plots representing data, we are able to easily detect trends and patterns,
    compared to just raw data and numbers. These visualizations further allow us to
    ask questions such as "How?" and "Why?", and form hypotheses about the dataset
    that can be validated by further visualizations. This is a continuous process
    that leads to a deeper understanding of the data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了EDA的重要性以及它能告诉我们什么，接下来让我们讨论EDA到底涉及什么。EDA可以包括从查看基本的摘要统计数据到可视化多变量之间的复杂趋势。尽管如此，即便是简单的统计数据和图表也可以是强大的工具，因为它们可能揭示出关于数据的重要事实，这些事实可能会改变我们建模的视角。当我们看到表示数据的图表时，相比于仅仅是原始数据和数字，我们能更容易地检测到趋势和模式。这些可视化还可以让我们提出“如何？”和“为什么？”这样的问题，并对数据集形成可以通过进一步可视化验证的假设。这是一个持续的过程，最终会加深我们对数据的理解。
- en: 'The dataset that we will use for our exploratory analysis and visualizations
    has been taken from the Significant Earthquake Database from NOAA, available as
    a public dataset on Google BigQuery (table ID: ''bigquery-public-data.noaa_significant_earthquakes.earthquakes'').
    We will be using a subset of the columns available, the metadata for which is
    available at https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table,
    and will load it into a pandas DataFrame to perform the exploration. We''ll primarily
    be using Matplotlib for most of our visualizations, along with the Seaborn and
    Missingno libraries for some. It is to be noted, however, that Seaborn merely
    provides a wrapper over Matplotlib''s functionalities, so anything that is plotted
    using Seaborn can also be plotted using Matplotlib. We''ll try to keep things
    interesting by using visualizations from both libraries.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于探索性分析和可视化的数据集来自NOAA的显著地震数据库，该数据库作为公共数据集可以在Google BigQuery上访问（表ID：'bigquery-public-data.noaa_significant_earthquakes.earthquakes'）。我们将使用其中一部分列，相关元数据可以在[https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table](https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table)获取，并将其加载到pandas
    DataFrame中进行探索。我们主要将使用Matplotlib进行大多数可视化，同时也会使用Seaborn和Missingno库进行部分可视化。然而需要注意的是，Seaborn只是Matplotlib功能的封装，因此，使用Seaborn绘制的图形也可以通过Matplotlib绘制。我们将通过这两种库的可视化来保持内容的趣味性。
- en: 'The exploration and analysis will be conducted keeping in mind a sample problem
    statement: Given the data we have, we want to predict whether an earthquake caused
    a tsunami. This will be a classification problem (more on this in Chapter 5, Classification
    Techniques) where the target variable is the flag_tsunami column.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和分析将基于一个示例问题：根据我们拥有的数据，我们想预测地震是否引发了海啸。这将是一个分类问题（更多内容请参见第 5 章，分类技术），目标变量是 flag_tsunami
    列。
- en: Before we begin, let's first import the required libraries, which we will be
    using for most of our data manipulations and visualizations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，首先导入所需的库，这些库将用于我们大多数的数据操作和可视化。
- en: 'In a Jupyter notebook, import the following libraries:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中，导入以下库：
- en: import json
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: import pandas as pd
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import missingno as msno
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: import missingno as msno
- en: from sklearn.impute import SimpleImputer
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.impute import SimpleImputer
- en: import matplotlib.pyplot as plt
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import seaborn as sns
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: import seaborn as sns
- en: 'We can also read in the metadata containing the data types for each column,
    which are stored in the form of a JSON file. Do this using the following command.
    This command opens the file in a readable format and uses the json library to
    read the file into a dictionary:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以读取包含每列数据类型的元数据，这些数据以 JSON 文件的形式存储。使用以下命令来读取。此命令以可读格式打开文件，并使用 json 库将文件读取为字典：
- en: 'with open(''..\dtypes.json'', ''r'') as jsonfile:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''..\dtypes.json'', ''r'') as jsonfile:'
- en: dtyp = json.load(jsonfile)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: dtyp = json.load(jsonfile)
- en: Note
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The output of the preceding command can be found here: https://packt.live/3a4Zjhm'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前述命令的输出可以在这里找到：https://packt.live/3a4Zjhm
- en: Summary Statistics and Central Values
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概要统计和中心值
- en: In order to find out what our data really looks like, we use a technique known
    as data profiling. This is defined as the process of examining the data available
    from an existing information source (for example, a database or a file) and collecting
    statistics or informative summaries about that data. The goal is to make sure
    that you understand your data well and are able to identify any challenges that
    the data may pose early on in the project, which is done by summarizing the dataset
    and assessing its structure, content, and quality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的数据真实情况，我们使用一种称为数据概况的技术。数据概况被定义为检查现有信息来源（例如，数据库或文件）中的数据，并收集该数据的统计信息或信息性摘要的过程。其目的是确保你充分理解数据，并能够及早识别数据可能在项目中带来的挑战，这通常通过总结数据集并评估其结构、内容和质量来实现。
- en: Data profiling includes collecting descriptive statistics and data types. Common
    data profile commands include those you have seen previously, including data.describe(),
    data.head(), and data.tail(). You can also use data.info(), which tells you how
    many non-null values there are in each column, along with the data type of the
    values (non-numeric types are represented as object types).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据概况包括收集描述性统计信息和数据类型。常见的数据概况命令包括你之前见过的命令，例如 data.describe()、data.head() 和 data.tail()。你还可以使用
    data.info()，它会告诉你每列中有多少非空值，以及这些值的数据类型（非数字类型表示为对象类型）。
- en: 'Exercise 2.01: Summarizing the Statistics of Our Dataset'
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.01：总结我们的数据集统计信息
- en: 'In this exercise, we will use the summary statistics functions we read about
    previously to get a basic idea of our dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用之前学到的概要统计函数，初步了解我们的数据集：
- en: Note
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be found on our GitHub repository here: https://packt.live/2TjU9aj'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在我们的 GitHub 仓库中找到，链接如下：https://packt.live/2TjU9aj
- en: 'Read the earthquakes data into a data pandas DataFrame and use the dtyp dictionary
    we read using the json library in the previous section, to specify the data types
    of each column in the CSV. Begin by loading the requisite libraries and the JSON
    file we have prepared with the data types. You can inspect the data types before
    reading the data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将地震数据读取到 pandas DataFrame 中，并使用我们在前一节中使用 json 库读取的 dtyp 字典，指定 CSV 中每列的数据类型。首先加载所需的库和我们已经准备好的包含数据类型的
    JSON 文件。你可以在读取数据之前检查数据类型：
- en: import json
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: import json
- en: import pandas as pd
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import missingno as msno
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: import missingno as msno
- en: from sklearn.impute import SimpleImputer
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.impute import SimpleImputer
- en: import matplotlib.pyplot as plt
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import seaborn as sns
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: import seaborn as sns
- en: 'with open(''../dtypes.json'', ''r'') as jsonfile:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''../dtypes.json'', ''r'') as jsonfile:'
- en: dtyp = json.load(jsonfile)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: dtyp = json.load(jsonfile)
- en: dtyp
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: dtyp
- en: 'The output will be as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.1: Inspecting data types'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：检查数据类型'
- en: '](img/image-J7XDCDBE.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-J7XDCDBE.jpg)'
- en: 'Figure 2.1: Inspecting data types'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：检查数据类型
- en: 'Use the data.info() function to get an overview of the dataset:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 data.info() 函数获取数据集的概览：
- en: data = pd.read_csv('../Datasets/earthquake_data.csv', dtype = dtyp)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: data = pd.read_csv('../Datasets/earthquake_data.csv', dtype = dtyp)
- en: data.info()
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: data.info()
- en: 'The output will be as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.2: Overview of the dataset'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2：数据集概览'
- en: '](img/image-R6JIB68X.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-R6JIB68X.jpg)'
- en: 'Figure 2.2: Overview of the dataset'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：数据集概览
- en: 'Print the first five and the last five rows of the dataset. The first five
    rows are printed as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 打印数据集的前五行和最后五行。前五行打印如下：
- en: data.head()
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: data.head()
- en: data.tail()
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: data.tail()
- en: 'The output will be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.3: The first and last five rows'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：前五行和最后五行'
- en: '](img/image-2B3IJSFX.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-2B3IJSFX.jpg)'
- en: 'Figure 2.3: The first and last five rows'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：前五行和最后五行
- en: We can see in these outputs that there are 28 columns, but not all of them are
    displayed. Only the first 10 and last 10 columns are displayed, with the ellipses
    representing the fact that there are columns in between that are not displayed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些输出中看到，数据集有28列，但并不是所有的列都显示出来。只显示了前10列和最后10列，省略号表示其中有未显示的列。
- en: 'Use data.describe() to find the summary statistics of the dataset. Run data.describe().T:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 data.describe() 查找数据集的汇总统计信息。运行 data.describe().T：
- en: data.describe().T
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: data.describe().T
- en: Here, .T indicates that we're taking a transpose of the DataFrame to which it
    is applied, that is, turning the columns into rows and vice versa. Applying it
    to the describe() function allows us to see the output more easily with each row
    in the transposed DataFrame now corresponding to the statistics for a single feature.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，.T 表示我们正在对应用它的 DataFrame 进行转置，即将列变成行，反之亦然。将其应用于 describe() 函数，使我们能更容易地查看输出，每一行在转置后的
    DataFrame 中对应一个特征的统计信息。
- en: 'We should get an output like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到类似这样的输出：
- en: '![Figure 2.4: Summary statistics'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4：汇总统计信息'
- en: '](img/image-1BKB9Y9V.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-1BKB9Y9V.jpg)'
- en: 'Figure 2.4: Summary statistics'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：汇总统计信息
- en: Note
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Yl5qer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/2Yl5qer](https://packt.live/2Yl5qer)。
- en: You can also run this example online at https://packt.live/2V3I76D. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/2V3I76D](https://packt.live/2V3I76D)。你必须执行整个
    Notebook 才能得到期望的结果。
- en: Notice here that the describe() function only shows the statistics for columns
    with numerical values. This is because we cannot calculate the statistics for
    the columns having non-numerical values (although we can visualize their values,
    as we will see later).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里 describe() 函数只显示数值型列的统计信息。这是因为我们无法对非数值型列计算统计数据（尽管我们可以像稍后所见那样可视化它们的值）。
- en: Missing Values
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失值
- en: When there is no value (that is, a null value) recorded for a particular feature
    in a data point, we say that the data is missing. Having missing values in a real
    dataset is inevitable; no dataset is ever perfect. However, it is important to
    understand why the data is missing, and whether there is a factor that has affected
    the loss of data. Appreciating and recognizing this allows us to handle the remaining
    data in an appropriate manner. For example, if the data is missing randomly, then
    it's highly likely that the remaining data is still representative of the population.
    However, if the missing data is not random in nature and we assume that it is,
    it could bias our analysis and subsequent modeling.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据点的某个特征没有记录值（即为空值）时，我们称数据为缺失数据。在实际数据集中出现缺失值是不可避免的；没有数据集是完美的。然而，理解数据缺失的原因以及是否有某些因素影响了数据丢失非常重要。理解并识别这一点可以帮助我们以合适的方式处理其余数据。例如，如果数据是随机缺失的，那么剩余的数据很可能仍然能够代表整个数据集。但如果缺失的数据并非随机缺失，而我们假设它是随机的，这可能会导致我们的分析和后续建模出现偏差。
- en: 'Let''s look at the common reasons (or mechanisms) for missing data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下缺失数据的常见原因（或机制）：
- en: 'Missing Completely at Random (MCAR): Values in a dataset are said to be MCAR
    if there is no correlation whatsoever between the value missing and any other
    recorded variable or external parameter. This means that the remaining data is
    still representative of the population, though this is rarely the case and taking
    missing data to be completely random is usually an unrealistic assumption.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 完全随机缺失（MCAR）：如果数据集中缺失的值与任何其他记录的变量或外部参数之间没有任何相关性，那么这些缺失值被称为 MCAR。这意味着其余数据仍然能够代表整个群体，尽管这种情况很少发生，且假设缺失数据是完全随机的通常是不现实的。
- en: For example, in a study that involves determining the reason for obesity among
    K12 children, MCAR is when the parents forgot to take their children to the clinic
    for the study.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一项研究中，如果要确定 K12 学生肥胖的原因，MCAR 的情况是父母忘记带孩子去诊所参加研究。
- en: 'Missing at Random (MAR): If the case where the data is missing is related to
    the data that was recorded rather than the data that was not, then the data is
    said to be MAR. Since it''s unfeasible to statistically verify whether data is
    MAR, we''d have to depend on whether it''s a reasonable possibility.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随机缺失（MAR）：如果数据缺失的原因与已记录的数据相关，而与未记录的数据无关，那么这些数据就被称为 MAR。由于无法通过统计方法验证数据是否为 MAR，我们只能依赖是否存在合理的可能性来判断。
- en: Using the K12 study, missing data in this case is due to parents moving to a
    different city, hence the children had to leave the study; missingness has nothing
    to do with the study itself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以 K12 研究为例，缺失数据的原因是父母搬到了其他城市，导致孩子不得不退出研究；缺失与研究本身无关。
- en: 'Missing Not at Random (MNAR): Data that is neither MAR nor MCAR is said to
    be MNAR. This is the case of a non-ignorable non-response, that is, the value
    of the variable that''s missing is related to the reason it is missing.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 非随机缺失（MNAR）：既不是 MAR 也不是 MCAR 的数据被称为 MNAR。这种情况通常是不可忽略的非响应情况，也就是说，缺失的变量值与其缺失的原因有关。
- en: Continuing with the example of the case study, data would be MNAR if the parents
    were offended by the nature of the study and did not want their children to be
    bullied, so they withdrew their children from the study.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用案例研究的例子，如果父母因为研究的性质感到不悦，不希望孩子被欺负，因此将孩子从研究中撤出，那么数据就是 MNAR（非随机缺失）。
- en: Finding Missing Values
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查找缺失值
- en: So, now that we know why it's important to familiarize ourselves with the reasons
    behind why our data is missing, let's talk about how we can find these missing
    values in a dataset. For a pandas DataFrame, this is most commonly executed using
    the .isnull() method on a DataFrame to create a mask of the null values (that
    is, a DataFrame of Boolean values) indicating where the null values exist—a True
    value at any position indicates a null value, while a False value indicates the
    existence of a valid value at that position.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们知道了为什么了解数据缺失背后的原因如此重要，接下来让我们讨论如何在数据集中找到这些缺失值。对于一个 pandas DataFrame，最常见的做法是使用
    `.isnull()` 方法，这个方法会在 DataFrame 上创建一个缺失值的掩码（即一个布尔值的 DataFrame），用来指示缺失值的位置——任何位置上为
    True 的值表示该位置是缺失值，而 False 则表示该位置有有效值。
- en: Note
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The .isnull() method can be used interchangeably with the .isna() method for
    pandas DataFrames. Both these methods do exactly the same thing—the reason there
    are two methods to do the same thing is pandas DataFrames were originally based
    on R DataFrames, and hence have reproduced much of the syntax and ideas of the
    latter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`.isnull()` 方法与 `.isna()` 方法可以互换使用，在 pandas DataFrame 中，两者做的事情完全相同——之所以有两个方法做同一件事，是因为
    pandas DataFrame 最初是基于 R DataFrame 开发的，因此复用了很多 R DataFrame 的语法和思想。'
- en: 'It may not be immediately obvious whether the missing data is random or not.
    Discovering the nature of missing values across features in a dataset is possible
    through two common visualization techniques:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据是否随机可能一开始并不明显。通过两种常见的可视化技术，我们可以发现数据集中特征的缺失值性质：
- en: 'Nullity matrix: This is a data-dense display that lets us quickly visualize
    the patterns in data completion. It gives us a quick glance at how the null values
    within a feature (and across features) are distributed, how many there are, and
    how often they appear with other features.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失矩阵：这是一种数据密集型的显示方式，可以让我们快速可视化数据完成情况的模式。它可以帮助我们快速查看一个特征（以及多个特征）中的缺失值是如何分布的，缺失值的数量是多少，以及它们与其他特征的关联频率。
- en: 'Nullity-correlation heatmap: This heatmap visually describes the nullity relationship
    (or a data completeness relationship) between each pair of features; that is,
    it measures how strongly the presence or absence of one variable affects the presence
    of another.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 空值相关性热图：该热图直观地描述了每对特征之间的空值关系（或数据完整性关系）；即它衡量了一个变量的存在或缺失如何影响另一个变量的存在。
- en: Akin to regular correlation, nullity correlation values range from -1 to 1,
    the former indicating that one variable appears when the other definitely does
    not, and the latter indicating the simultaneous presence of both variables. A
    value of 0 implies that one variable having a null value has no effect on the
    other being null.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于常规的相关性分析，空值相关性值的范围从 -1 到 1，前者表示一个变量出现时，另一个变量肯定不出现，后者则表示两个变量同时存在。值为 0 表示一个变量的空值对另一个变量的空值没有影响。
- en: 'Exercise 2.02: Visualizing Missing Values'
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.02：可视化缺失值
- en: Let's analyze the nature of the missing values by first looking at the count
    and percentage of missing values for each feature, and then plotting a nullity
    matrix and correlation heatmap using the missingno library in Python. We will
    be using the same dataset from the previous exercises.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过查看每个特征的缺失值数量和百分比，来分析缺失值的性质，然后使用 Python 中的 missingno 库绘制空值矩阵和相关性热图。我们将继续使用前面练习中的相同数据集。
- en: 'Please note that this exercise is a continuation of Exercise 2.01: Summarizing
    the Statistics of Our Dataset.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本练习是练习 2.01：总结我们的数据集统计信息的延续。
- en: 'The following steps will help you complete this exercise to visualize the missing
    values in the dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习，以可视化数据集中的缺失值：
- en: Calculate the count and percentage of missing values in each column and arrange
    these in decreasing order. We will use the .isnull() function on the DataFrame
    to get a mask. The count of null values in each column can then be found using
    the .sum() function over the DataFrame mask. Similarly, the fraction of null values
    can be found using .mean() over the DataFrame mask and multiplied by 100 to convert
    it to a percentage.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每列缺失值的数量和百分比，并按降序排列。我们将使用 `.isnull()` 函数对 DataFrame 进行掩码操作。然后，可以使用 `.sum()`
    函数计算每列的空值数量。类似地，空值的比例可以通过对 DataFrame 掩码使用 `.mean()` 并乘以 100 来转换为百分比。
- en: 'Then, we combine the total and percentage of null values into a single DataFrame
    using the pd.concat() function, and subsequently sort the rows by percentage of
    missing values and print the DataFrame:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `pd.concat()` 函数将空值的总数和百分比合并到一个 DataFrame 中，接着按照缺失值的百分比对行进行排序，并打印出该
    DataFrame：
- en: mask = data.isnull()
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: mask = data.isnull()
- en: total = mask.sum()
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: total = mask.sum()
- en: percent = 100*mask.mean()
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: percent = 100*mask.mean()
- en: missing_data = pd.concat([total, percent], axis=1,join='outer', \
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: missing_data = pd.concat([total, percent], axis=1,join='outer', \
- en: keys=['count_missing', 'perc_missing'])
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: keys=['count_missing', 'perc_missing'])
- en: missing_data.sort_values(by='perc_missing', ascending=False, \
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: missing_data.sort_values(by='perc_missing', ascending=False, \
- en: inplace=True)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: inplace=True)
- en: missing_data
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: missing_data
- en: 'The output will be as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.5: The count and percentage of missing values in each column'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：每列缺失值的数量和百分比'
- en: '](img/image-Z70NA5T0.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-Z70NA5T0.jpg)'
- en: 'Figure 2.5: The count and percentage of missing values in each column'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：每列缺失值的数量和百分比
- en: Here, we can see that the state, total_damage_millions_dollars, and damage_millions_dollars
    columns have over 90% missing values, which means that data for fewer than 10%
    of the data points in the dataset are available for these columns. On the other
    hand, year, flag_tsunami, country, and region_code have no missing values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 state、total_damage_millions_dollars 和 damage_millions_dollars 列的缺失值超过
    90%，意味着数据集中这些列的可用数据点不到 10%。另一方面，year、flag_tsunami、country 和 region_code 列没有缺失值。
- en: 'Plot the nullity matrix. First, we find the list of columns that have any null
    values in them using the .any() function on the DataFrame mask from the previous
    step. Then, we use the missingno library to plot the nullity matrix for a random
    sample of 500 data points from our dataset, for only those columns that have missing
    values:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制空值矩阵。首先，我们使用 `.any()` 函数从前一步骤的 DataFrame mask 中找出包含空值的列。然后，我们使用 missingno
    库绘制一个空值矩阵，针对数据集中仅包含缺失值的列，从 500 条随机数据中取样：
- en: nullable_columns = data.columns[mask.any()].tolist()
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: nullable_columns = data.columns[mask.any()].tolist()
- en: msno.matrix(data[nullable_columns].sample(500))
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: msno.matrix(data[nullable_columns].sample(500))
- en: plt.show()
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.6: The nullity matrix'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6：缺失值矩阵'
- en: '](img/image-G34KOPOB.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-G34KOPOB.jpg)'
- en: 'Figure 2.6: The nullity matrix'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：缺失值矩阵
- en: Here, black lines represent non-nullity while the white lines indicate the presence
    of a null value in that column. At a glance, location_name appears to be completely
    populated (we know from the previous step that there is, in fact, only one missing
    value in this column), while latitude and longitude seem mostly complete, but
    spottier.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，黑色线条表示非空值，而白色线条表示该列中存在缺失值。总体来看，location_name列似乎完全填充（我们从前面的步骤中得知，这一列实际上只有一个缺失值），而latitude和longitude列似乎大部分完整，但有些地方较为稀疏。
- en: The spark line on the right summarizes the general shape of the data completeness
    and points out the rows with the maximum and minimum nullity in the dataset. Note
    that this is only for the sample of 500 points.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的火花图总结了数据完整性的整体形态，并指出了数据集中最大和最小缺失值所在的行。请注意，这仅适用于500个数据点的样本。
- en: 'Plot the nullity correlation heatmap. We will plot the nullity correlation
    heatmap using the missingno library for our dataset, for only those columns that
    have missing values:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制缺失值相关性热图。我们将使用missingno库绘制缺失值相关性热图，仅针对那些存在缺失值的列：
- en: msno.heatmap(data[nullable_columns], figsize=(18,18))
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: msno.heatmap(data[nullable_columns], figsize=(18,18))
- en: plt.show()
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.7: The nullity correlation heatmap'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.7：缺失值相关性热图'
- en: '](img/image-UT14Y4SX.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-UT14Y4SX.jpg)'
- en: 'Figure 2.7: The nullity correlation heatmap'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：缺失值相关性热图
- en: 'Here, we can also see some boxes labeled <1: this just means that the correlation
    values in those cases are all close to 1.0, but still not quite perfectly so.
    We can see a value of <1 between injuries and total_injuries, which means that
    the missing values in each category are correlated. We would need to dig deeper
    to understand whether the missing values are correlated because they are based
    upon the same or similar information, or for some other reason.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还可以看到一些标有<1的框：这仅意味着这些情况下的相关性值都接近1.0，但仍然不是完全一致。我们可以看到在injuries和total_injuries之间有<1的值，这意味着每个类别中的缺失值是相关的。我们需要深入挖掘，了解这些缺失值是否因为基于相同或类似的信息而相关，或者是出于其他原因。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2YSXq3k.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考https://packt.live/2YSXq3k。
- en: You can also run this example online at https://packt.live/2Yn3Us7\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在https://packt.live/2Yn3Us7上在线运行此示例。您必须执行整个Notebook才能获得所需的结果。
- en: Imputation Strategies for Missing Values
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺失值的填充策略
- en: 'There are multiple ways of dealing with missing values in a column. The simplest
    way is to simply delete rows having missing values; however, this can result in
    the loss of valuable information from other columns. Another option is to impute
    the data, that is, replace the missing values with a valid value inferred from
    the known part of the data. The common ways in which this can be done are listed
    here:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 处理列中缺失值有多种方法。最简单的方法是直接删除包含缺失值的行；然而，这可能会导致丧失其他列中的有价值信息。另一种选择是对数据进行填充，即用从已知数据部分推断出的有效值替换缺失值。常见的填充方法如下：
- en: Create a new value that is distinct from the other values to replace the missing
    values in the column so as to differentiate those rows altogether. Then, use a
    non-linear machine learning algorithm (such as ensemble models or support vectors)
    that can separate the values out.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个与其他值不同的新值，以替换列中的缺失值，从而区分这些行。然后，使用非线性机器学习算法（如集成模型或支持向量机），将这些值分离出来。
- en: Use an appropriate central value from the column (mean, median, or mode) to
    replace the missing values.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列中的适当中心值（均值、中位数或众数）来替换缺失值。
- en: Use a model (such as a K-nearest neighbors or a Gaussian mixture model) to learn
    the best value with which to replace the missing values.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型（例如K近邻或高斯混合模型）来学习最佳值，以替换缺失值。
- en: 'Python has a few functions that are useful for replacing null values in a column
    with a static value. One way to do this is to use the inherent pandas .fillna(0)
    function: there is no ambiguity in imputation here—the static value with which
    to substitute the null data point in the column is the argument being passed to
    the function (the value in the brackets).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Python有一些函数可以用来将列中的空值替换为静态值。实现这一点的一种方式是使用pandas本身的.fillna(0)函数：这里没有歧义——替换空数据点的静态值就是传递给函数的参数（括号中的值）。
- en: 'However, if the number of null values in a column is significant and it''s
    not immediately obvious what the appropriate central value is that can be used
    to replace each null value, then we can either delete the rows having null values
    or delete the column altogether from the modeling perspective, as it may not add
    any significant value. This can be done by using the .dropna() function on the
    DataFrame. The parameters that can be passed to the function are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果列中空值的数量较大，并且无法立即确定可以用来替换每个空值的合适中心值，那么我们可以从建模的角度出发，选择删除含有空值的行，或者直接删除整个列，因为它可能不会增加任何重要的价值。这可以通过在DataFrame上使用.dropna()函数来完成。可以传递给该函数的参数如下：
- en: 'axis: This defines whether to drop rows or columns, which is determined by
    assigning the parameter a value of 0 or 1, respectively.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 'axis: 这定义了是删除行还是列，具体取决于为参数分配0或1的值。'
- en: 'how: A value of all or any can be assigned to this parameter to indicate whether
    the row/column should contain all null values to drop the column, or whether to
    drop the column if there is at least one null value.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'how: 可以为此参数分配“all”或“any”值，表示行/列是否应包含所有空值才能删除该列，或者是否应在至少有一个空值时删除该列。'
- en: 'thresh: This defines the minimum number of null values the row/column should
    have in order to be dropped.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 'thresh: 这定义了行/列应具有的最小空值数量，才会被删除。'
- en: Additionally, if an appropriate replacement for a null value for a categorical
    feature cannot be determined, a possible alternative to deleting the column is
    to create a new category in the feature that can represent the null values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果无法确定一个适当的替代值来填充分类特征的空值，那么删除该列的一个可能替代方案是为该特征创建一个新的类别，用来表示空值。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If it is immediately obvious how a null value for a column can be replaced from
    an intuitive understanding or domain knowledge, then we can replace the value
    on the spot. Keep in mind that any such data changes should be made in your code
    and never directly on the raw data. One reason for this is that it allows the
    strategy to be updated easily in the future. Another reason is that it makes it
    visible to others who may later be reviewing the work where changes were made.
    Directly changing raw data can lead to data versioning problems and make it impossible
    for others to reproduce your work. In many cases, inferences become more obvious
    at later stages in the exploration process. In these cases, we can substitute
    null values as and when we find an appropriate way to do so.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以通过直观理解或领域知识立即得出一个适合的值来替换列中的空值，那么我们可以当场进行替换。请记住，任何此类数据更改应在代码中进行，而绝不是直接在原始数据上操作。这样做的一个原因是它使得将来可以轻松地更新策略；另一个原因是它使得其他人如果以后审查这项工作时，可以清楚地看到在哪里进行了更改。直接更改原始数据可能会导致数据版本控制问题，并使得其他人无法重现你的工作。在很多情况下，推断会在后续的探索阶段变得更加明显。在这种情况下，我们可以在找到合适的方法时，随时替换空值。
- en: 'Exercise 2.03: Performing Imputation Using Pandas'
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习2.03：使用Pandas进行填充
- en: 'Let''s look at missing values and replace them with zeros in time-based (continuous)
    features having at least one null value (month, day, hour, minute, and second).
    We do this because, for cases where we do not have recorded values, it would be
    safe to assume that the events take place at the beginning of the time duration.
    This exercise is a continuation of Exercise 2.02: Visualizing Missing Values:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下缺失值，并在具有至少一个空值的基于时间（连续）的特征中将它们替换为零（如月份、日期、小时、分钟和秒）。我们这么做是因为对于那些没有记录值的情况，可以安全地假设事件发生在时间段的开始。这项操作是练习2.02：可视化缺失值的延续：
- en: 'Create a list containing the names of the columns whose values we want to impute:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含我们想要填充的列名的列表：
- en: time_features = ['month', 'day', 'hour', 'minute', 'second']
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: time_features = ['month', 'day', 'hour', 'minute', 'second']
- en: 'Impute the null values using .fillna(). We will replace the missing values
    in these columns with 0 using the inherent pandas .fillna() function and pass
    0 as an argument to the function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 .fillna() 填充空值。我们将使用 pandas 自带的 .fillna() 函数，并传递 0 作为参数来替换这些列中的缺失值：
- en: data[time_features] = data[time_features].fillna(0)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: data[time_features] = data[time_features].fillna(0)
- en: 'Use the .info() function to view null value counts for the imputed columns:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 .info() 函数查看填充列的空值计数：
- en: data[time_features].info()
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: data[time_features].info()
- en: 'The output will be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.8: Null value counts'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：空值计数'
- en: '](img/image-Z1ZGRYK5.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-Z1ZGRYK5.jpg)'
- en: 'Figure 2.8: Null value counts'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：空值计数
- en: As we can now see, all values for our features in the DataFrame are now non-null.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，我们可以看到数据框中所有特征的值都不再是空值。
- en: Note
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2V9nMx3.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参阅 https://packt.live/2V9nMx3。
- en: You can also run this example online at https://packt.live/2BqoZZM. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址为 https://packt.live/2BqoZZM。你必须执行整个 Notebook 才能得到预期的结果。
- en: 'Exercise 2.04: Performing Imputation Using Scikit-Learn'
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.04：使用 Scikit-Learn 执行填充
- en: 'In this exercise, you will replace the null values in the description-related
    categorical features using scikit-learn''s SimpleImputer class. In Exercise 2.02:
    Visualizing Missing Values, we saw that almost all of these features comprised
    more than 50% of null values in the data. Replacing these null values with a central
    value might bias any model we try to build using the features, deeming them irrelevant.
    Let''s instead replace the null values with a separate category, having the value
    NA. This exercise is a continuation of Exercise 2.02: Visualizing Missing Values:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你将使用 scikit-learn 的 SimpleImputer 类来替换描述相关的类别特征中的空值。在练习 2.02：可视化缺失值中，我们看到几乎所有这些特征都包含超过
    50% 的缺失值。用中央值替换这些缺失值可能会导致我们构建的模型产生偏差，从而认为它们无关紧要。我们不妨将这些空值替换为一个单独的类别，赋值为 NA。本练习是练习
    2.02：可视化缺失值的延续：
- en: 'Create a list containing the names of the columns whose values we want to impute:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含我们希望填充其值的列名的列表：
- en: description_features = ['injuries_description', \
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: description_features = ['injuries_description', \
- en: '''damage_description'', \'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '''damage_description'', \'
- en: '''total_injuries_description'', \'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '''total_injuries_description'', \'
- en: '''total_damage_description'']'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '''total_damage_description'']'
- en: 'Create an object of the SimpleImputer class. Here, we first create an imp object
    of the SimpleImputer class and initialize it with parameters that represent how
    we want to impute the data. The parameters we will pass to initialize the object
    are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 SimpleImputer 类的对象。在这里，我们首先创建一个 imp 对象，并使用表示我们希望如何填充数据的参数对其进行初始化。我们将传递给对象初始化的参数如下：
- en: 'missing_values: This is the placeholder for the missing values, that is, all
    occurrences of the values in the missing_values parameter will be imputed.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: missing_values：这是缺失值的占位符，也就是说，missing_values 参数中所有出现的值都会被填充。
- en: 'strategy: This is the imputation strategy, which can be one of mean, median,
    most_frequent (that is, the mode), or constant. While the first three can only
    be used with numeric data and will replace missing values using the specified
    central value along each column, the last one will replace missing values with
    a constant as per the fill_value parameter.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: strategy：这是填充策略，可以是 mean、median、most_frequent（即众数）或 constant。前三者仅能用于数值数据，并将使用指定的中央值沿每列替换缺失值，而最后一种将根据
    fill_value 参数使用常数来替换缺失值。
- en: 'fill_value: This specifies the value with which to replace all occurrences
    of missing_values. If left to the default, the imputed value will be 0 when imputing
    numerical data and the missing_value string for strings or object data types:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: fill_value：指定用来替换所有缺失值的值。如果保持默认值，当填充数值数据时，填充值将为 0，对于字符串或对象数据类型，则为 missing_value
    字符串：
- en: imp = SimpleImputer(missing_values=np.nan, \
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: imp = SimpleImputer(missing_values=np.nan, \
- en: strategy='constant', \
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: strategy='constant', \
- en: fill_value='NA')
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: fill_value='NA')
- en: 'Perform the imputation. We will use imp.fit_transform() to actually perform
    the imputation. It takes the DataFrame with null values as input and returns the
    imputed DataFrame:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 执行填充操作。我们将使用 imp.fit_transform() 来实际执行填充。它接受包含空值的 DataFrame 作为输入，并返回填充后的 DataFrame：
- en: data[description_features] = \
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: data[description_features] = \
- en: imp.fit_transform(data[description_features])
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: imp.fit_transform(data[description_features])
- en: 'Use the .info() function to view null value counts for the imputed columns:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 .info() 函数查看插补列的空值计数：
- en: data[description_features].info()
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: data[description_features].info()
- en: 'The output will be as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.9: The null value counts'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：空值计数'
- en: '](img/image-PSVVIOTZ.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-PSVVIOTZ.jpg)'
- en: 'Figure 2.9: The null value counts'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：空值计数
- en: Note
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: To access the source code for this specific section, please refer to https://packt.live/3ervLgk.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/3ervLgk。
- en: You can also run this example online at https://packt.live/3doEX3G. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 https://packt.live/3doEX3G。你必须执行整个 Notebook 才能得到预期的结果。
- en: In the last two exercises, we looked at two ways to use pandas and scikit-learn
    methods to impute missing values. These methods are very basic methods we can
    use if we have little or no information about the underlying data. Next, we'll
    look at more advanced techniques we can use to fill in missing data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个练习中，我们研究了使用 pandas 和 scikit-learn 方法进行缺失值插补的两种方法。这些方法是我们在对底层数据几乎没有或没有任何信息时可以使用的非常基础的方法。接下来，我们将看看我们可以使用的更高级的技术来填补缺失数据。
- en: 'Exercise 2.05: Performing Imputation Using Inferred Values'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.05：使用推断值进行插补
- en: 'Let''s replace the null values in the continuous damage_millions_dollars feature
    with information from the categorical damage_description feature. Although we
    may not know the exact dollar amount that was incurred, the categorical feature
    gives us information on the range of the amount that was incurred due to damage
    from the earthquake. This exercise is a continuation of Exercise 2.04: Performing
    Imputation Using scikit-learn:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用来自类别 damage_description 特征的信息替换连续的 damage_millions_dollars 特征中的空值。虽然我们可能不知道具体的损失金额，但类别特征能提供关于地震损失金额区间的信息。这个练习是练习
    2.04（使用 scikit-learn 进行插补）的延续：
- en: 'Find how many rows have null damage_millions_dollars values, and how many of
    those have non-null damage_description values:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 查找多少行具有空的 damage_millions_dollars 值，并且其中有多少行具有非空的 damage_description 值：
- en: print(data[pd.isnull(data.damage_millions_dollars)].shape[0])
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: print(data[pd.isnull(data.damage_millions_dollars)].shape[0])
- en: print(data[pd.isnull(data.damage_millions_dollars) \
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: print(data[pd.isnull(data.damage_millions_dollars) \
- en: '& (data.damage_description != ''NA'')].shape[0])'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '& (data.damage_description != ''NA'')].shape[0])'
- en: 'The output will be as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '5594'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '5594'
- en: '3849'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '3849'
- en: 'As we can see, 3,849 of 5,594 null values can be easily substituted with the
    help of another variable. For example, we know that all variables having column
    names ending with _description are a descriptor field containing estimates for
    data that may not be available in the original numerical column. For deaths, injuries,
    and total_injuries, the corresponding categorical values represent the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，3849 个空值（在 5594 个空值中）可以通过另一个变量轻松替代。例如，我们知道所有列名以 _description 结尾的变量是描述字段，包含一些原始数值列可能缺失的数据估算值。对于死亡、伤害和总伤害，相关的类别值表示如下：
- en: 0 = None
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 0 = 无
- en: 1 = Few (~1 to 50 deaths)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 1 = 少量（~1 到 50 死亡）
- en: 2 = Some (~51 to 100 deaths)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 2 = 一些（~51 到 100 死亡）
- en: 3 = Many (~101 to 1,000 deaths)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 3 = 多（~101 到 1000 死亡）
- en: 4 = Very Many (~1,001 or more deaths)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 4 = 非常多（~1001 或更多死亡）
- en: 'As regards damage_millions_dollars, the corresponding categorical values represent
    the following:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 damage_millions_dollars，相应的类别值表示如下：
- en: 0 = None
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 0 = 无
- en: 1 = Limited (roughly corresponding to less than 1 million dollars)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 1 = 有限（大致对应不到 100 万美元）
- en: 2 = Moderate (~1 to 5 million dollars)
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 2 = 中等（~1 到 500 万美元）
- en: 3 = Severe (~>5 to 24 million dollars)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 3 = 严重（~>5 到 2400 万美元）
- en: 4 = Extreme (~25 million dollars or more)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 4 = 极端（~2500 万美元或更多）
- en: 'Find the mean damage_millions_dollars value for each category. Since each of
    the categories in damage_description represents a range of values, we find the
    mean damage_millions_dollars value for each category from the non-null values
    already available. These provide a reasonable estimate for the most likely value
    for that category:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 查找每个类别的平均 damage_millions_dollars 值。由于 damage_description 中的每个类别代表一个值的范围，因此我们从已经有的非空值中找到每个类别的平均
    damage_millions_dollars 值。这些值为该类别提供了一个合理的最可能的估算值：
- en: category_means = data[['damage_description', \
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: category_means = data[['damage_description', \
- en: '''damage_millions_dollars'']]\'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '''damage_millions_dollars'']]\'
- en: .groupby('damage_description').mean()
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: .groupby('damage_description').mean()
- en: category_means
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: category_means
- en: 'The output will be as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.10: The mean damage_millions_dollars value for each category'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10：每个类别的平均 damage_millions_dollars 值'
- en: '](img/image-QMB0CEWJ.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-QMB0CEWJ.jpg)'
- en: 'Figure 2.10: The mean damage_millions_dollars value for each category'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：每个类别的平均 damage_millions_dollars 值
- en: 'Note that the first three values make intuitive sense given the preceding definitions:
    0.42 is between 0 and 1, 3.1 is between 1 and 5, and 13.8 is between 5 and 24\.
    The last category is defined as 25 million or more; it transpires that the mean
    of these extreme cases is very high (3,575!).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面定义的前三个值具有直观意义：0.42 位于 0 和 1 之间，3.1 位于 1 和 5 之间，13.8 位于 5 和 24 之间。最后一个类别定义为
    2500 万或更多；事实证明，这些极端情况的均值非常高（3575！）。
- en: Store the mean values as a dictionary. In this step, we will convert the DataFrame
    containing the mean values to a dictionary (a Python dict object), so that accessing
    them is convenient.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 将均值存储为字典。在这一步，我们将包含均值的DataFrame转换为字典（Python dict对象），以便方便地访问它们。
- en: 'Additionally, since the value for the newly created NA category (the imputed
    value in the previous exercise) was NaN, and the value for the 0 category was
    absent (no rows had damage_description equal to 0 in the dataset), we explicitly
    added these values to the dictionary as well:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于新创建的 NA 类别的值（前一步骤中的插补值）是 NaN，且 0 类别的值缺失（数据集中没有 damage_description 等于 0
    的行），我们还将这些值显式地添加到字典中：
- en: replacement_values = category_means\
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: replacement_values = category_means\
- en: .damage_millions_dollars.to_dict()
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: .damage_millions_dollars.to_dict()
- en: replacement_values['NA'] = -1
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: replacement_values['NA'] = -1
- en: replacement_values['0'] = 0
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: replacement_values['0'] = 0
- en: replacement_values
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: replacement_values
- en: 'The output will be as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.11: The dictionary of mean values'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11：均值字典'
- en: '](img/image-M7BYA4VU.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-M7BYA4VU.jpg)'
- en: 'Figure 2.11: The dictionary of mean values'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11：均值字典
- en: 'Create a series of replacement values. For each value in the damage_description
    column, we map the categorical value onto the mean value using the map function.
    The .map() function is used to map the keys in the column to the corresponding
    values for each element from the replacement_values dictionary:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一系列替代值。对于 damage_description 列中的每个值，我们使用 map 函数将类别值映射到均值。使用 .map() 函数将列中的键映射到
    replacement_values 字典中对应元素的值：
- en: imputed_values = data.damage_description.map(replacement_values)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: imputed_values = data.damage_description.map(replacement_values)
- en: 'Replace null values in the column. We do this by using np.where as a ternary
    operator: the first argument is the mask, the second is the series from which
    to take the value if the mask is positive, and the third is the series from which
    to take the value if the mask is negative.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 替换列中的空值。我们通过使用 np.where 作为三元运算符来完成这一步：第一个参数是掩码，第二个参数是当掩码为正时取值的系列，第三个参数是当掩码为负时取值的系列。
- en: 'This ensures that the array returned by np.where only replaces the null values
    in damage_millions_dollars with values from the imputed_values series:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了 np.where 返回的数组只会用 imputed_values 系列中的值替换 damage_millions_dollars 中的空值：
- en: data['damage_millions_dollars'] = \
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: data['damage_millions_dollars'] = \
- en: np.where(data.damage_millions_dollars.isnull(), \
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: np.where(data.damage_millions_dollars.isnull(), \
- en: data.damage_description.map(replacement_values), \
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: data.damage_description.map(replacement_values), \
- en: data.damage_millions_dollars)
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: data.damage_millions_dollars)
- en: 'Use the .info() function to view null value counts for the imputed columns:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 .info() 函数查看插补列的空值计数：
- en: data[['damage_millions_dollars']].info()
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: data[['damage_millions_dollars']].info()
- en: 'The output will be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.12: The null value counts'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12：空值计数'
- en: '](img/image-H5UW1GNA.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-H5UW1GNA.jpg)'
- en: 'Figure 2.12: The null value counts'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：空值计数
- en: We can see that, after replacement, there are no null values in the damage_millions_dollars
    column.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在替换后，damage_millions_dollars 列中没有空值。
- en: Note
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3fMRqQo.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 https://packt.live/3fMRqQo。
- en: You can also run this example online at https://packt.live/2YkBgYC. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在 https://packt.live/2YkBgYC 上在线运行此示例。您必须执行整个 Notebook，才能获得预期结果。
- en: In this section, we have looked at replacing missing values in more than one
    way. In one case, we replaced values with zeros; in another case, we looked at
    more information about the dataset to reason that we could replace missing values
    with a combination of information from a descriptive field and the means of values
    we did have. These sorts of decisions and steps are extremely common when working
    with real data. We also noted that, occasionally, when we have sufficient data
    and the instances with missing values are few, we can just drop them. In the following
    activity, we'll use a different dataset for you to practice and reinforce these
    methods.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已研究了多种替换缺失值的方法。某些情况下，我们将缺失值替换为零；另一些情况下，我们通过进一步了解数据集的信息来推测，可以用描述性字段的信息和我们已有值的均值来替换缺失值。在处理真实数据时，这类决策和步骤非常常见。我们还注意到，偶尔在数据足够充分且缺失值实例较少的情况下，我们可以直接删除这些实例。在接下来的活动中，我们将使用不同的数据集，以帮助你练习并巩固这些方法。
- en: 'Activity 2.01: Summary Statistics and Missing Values'
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 2.01：汇总统计与缺失值
- en: 'In this activity, we''ll revise some of the summary statistics and missing
    value exploration we have looked at thus far in this chapter. We will be using
    a new dataset, House Prices: Advanced Regression Techniques, available on Kaggle.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将回顾本章到目前为止我们所研究的一些汇总统计和缺失值探索内容。我们将使用一个新的数据集“房价：高级回归技术”，该数据集可在 Kaggle
    上获得。
- en: Note
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The original dataset is available at https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
    or on our GitHub repository at https://packt.live/2TjU9aj.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集可通过以下链接获取：https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data，或者访问我们的
    GitHub 仓库：https://packt.live/2TjU9aj。
- en: While the Earthquakes dataset used in the exercises is aimed at solving a classification
    problem (when the target variable has only discrete values), the dataset we will
    use in the activities will be aimed at solving a regression problem (when the
    target variable takes on a range of continuous values). We will use pandas functions
    to generate summary statistics and visualize missing values using a nullity matrix
    and nullity correlation heatmap.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然用于练习的地震数据集是为了处理分类问题（当目标变量只有离散值时），但我们在活动中将使用的数据集是为了解决回归问题（当目标变量为连续值范围时）。我们将使用
    pandas 函数生成汇总统计，并通过空值矩阵和空值相关性热图来可视化缺失值。
- en: 'The steps to be performed are as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Read the data (house_prices.csv).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据（house_prices.csv）。
- en: Use pandas' .info() and .describe() methods to view the summary statistics of
    the dataset.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 的 .info() 和 .describe() 方法查看数据集的汇总统计。
- en: 'The output of the info() method will be as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: info() 方法的输出如下所示：
- en: '![Figure 2.13: The output of the info() method (abbreviated)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13：info() 方法的输出（简略版）'
- en: '](img/image-Q9SXD9EK.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-Q9SXD9EK.jpg)'
- en: 'Figure 2.13: The output of the info() method (abbreviated)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：info() 方法的输出（简略版）
- en: 'The output of the describe() method will be as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: info() 方法的输出如下所示：
- en: '![Figure 2.14: The output of the describe() method (abbreviated)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14：describe() 方法的输出（简略版）'
- en: '](img/image-99VP2HBT.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-99VP2HBT.jpg)'
- en: 'Figure 2.14: The output of the describe() method (abbreviated)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：describe() 方法的输出（简略版）
- en: Note
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The outputs of the info() and describe() methods have been truncated for presentation
    purposes. You can find the outputs in their entirety here: https://packt.live/2TjZSgi'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示方便，info() 和 describe() 方法的输出已被截断。你可以在这里找到完整的输出：https://packt.live/2TjZSgi
- en: Find the total count and total percentage of missing values in each column of
    the DataFrame and display them for columns having at least one null value, in
    descending order of missing percentages.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 查找 DataFrame 中每一列的缺失值总数和缺失值百分比，并按缺失百分比降序显示至少有一个空值的列。
- en: Plot the nullity matrix and nullity correlation heatmap.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制空值矩阵和空值相关性热图。
- en: 'The nullity matrix will be as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 空值矩阵如下所示：
- en: '![Figure 2.15: Nullity matrix'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：空值矩阵'
- en: '](img/image-TTRK4BON.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-TTRK4BON.jpg)'
- en: 'Figure 2.15: Nullity matrix'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：空值矩阵
- en: 'The nullity correlation heatmap will be as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 空值相关性热图如下所示：
- en: '![Figure 2.16: Nullity correlation heatmap'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16：空值相关性热图'
- en: '](img/image-QT4NJDMU.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-QT4NJDMU.jpg)'
- en: 'Figure 2.16: Nullity correlation heatmap'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16：空值相关性热图
- en: Delete the columns having more than 80% of their values missing.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 删除缺失值超过 80% 的列。
- en: Replace null values in the FireplaceQu column with NA values.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 将 FireplaceQu 列中的空值替换为 NA 值。
- en: Note
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可通过此链接找到。
- en: You should now be comfortable using the approaches we've learned to investigate
    missing values in any type of tabular data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能够使用我们学到的方法来调查任何类型表格数据中的缺失值。
- en: Distribution of Values
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 值的分布
- en: In this section, we'll look at how individual variables behave—what kind of
    values they take, what the distribution across those values is, and how those
    distributions can be represented visually.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究各个变量的行为——它们取什么样的值，这些值的分布情况如何，以及如何通过视觉手段表示这些分布。
- en: Target Variable
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标变量
- en: The target variable can either have values that are continuous (in the case
    of a regression problem) or discrete (as in the case of a classification problem).
    The problem statement we're looking at in this chapter involves predicting whether
    an earthquake caused a tsunami, that is, the flag_tsunami variable, which takes
    on two discrete values only—making it a classification problem.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量可以是连续值（在回归问题中）或离散值（如分类问题中）。本章我们研究的问题涉及预测地震是否引发海啸，即 flag_tsunami 变量，该变量只有两个离散值，使其成为分类问题。
- en: One way of visualizing how many earthquakes resulted in tsunamis and how many
    didn't involves the use of a bar chart, where each bar represents a single discrete
    value of the variable, and the height of the bars is equal to the count of the
    data points having the corresponding discrete value. This gives us a good comparison
    of the absolute counts of each category.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可视化方法是使用柱状图来显示有多少地震引发了海啸，有多少没有引发。在柱状图中，每个柱子代表变量的一个单一离散值，柱子的高度等于具有相应离散值的数据点的计数。这使我们能够很好地比较每个类别的绝对计数。
- en: 'Exercise 2.06: Plotting a Bar Chart'
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Exercise 2.06: 绘制柱状图'
- en: 'Let''s look at how many of the earthquakes in our dataset resulted in a tsunami.
    We will do this by using the value_counts() method over the column and using the
    .plot(kind=''bar'') function directly on the returned pandas series. This exercise
    is a continuation of Exercise 2.05: Performing Imputation Using Inferred Values:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们看看我们的数据集中有多少地震引发了海啸。我们将使用 value_counts() 方法对列进行操作，并直接在返回的 pandas 系列上使用 .plot(kind=''bar'')
    函数。这个练习是 Exercise 2.05: 使用推断值进行插补的延续：'
- en: 'Use plt.figure() to initiate the plotting:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 plt.figure() 开始绘图：
- en: plt.figure(figsize=(8,6))
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(8,6))
- en: 'Next, type in our primary plotting command:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，键入我们的主要绘图命令：
- en: data.flag_tsunami.value_counts().plot(kind='bar', \
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: data.flag_tsunami.value_counts().plot(kind='bar', \
- en: color = ('grey', \
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: color = ('grey', \
- en: '''black''))'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '''black''))'
- en: 'Set the display parameters and display the plot:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 设置显示参数并显示图表：
- en: plt.ylabel('Number of data points')
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('数据点数目')
- en: plt.xlabel('flag_tsunami')
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('flag_tsunami')
- en: plt.show()
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.17: Bar chart showing how many earthquakes resulted in a tsunami'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.17: 显示引发海啸的地震数量的柱状图'
- en: '](img/image-2DJC2J7X.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-2DJC2J7X.jpg)'
- en: 'Figure 2.17: Bar chart showing how many earthquakes resulted in a tsunami'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.17: 显示引发海啸的地震数量的柱状图'
- en: From this bar plot, we can see that most of the earthquakes did not result in
    tsunamis and that fewer than one-third of the earthquakes actually did. This shows
    us that the dataset is slightly imbalanced.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个条形图中，我们可以看到大多数地震并没有引发海啸，实际上不到三分之一的地震确实引发了海啸。这显示数据集略微不平衡。
- en: Note
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: To access the source code for this specific section, please refer to https://packt.live/2Yn4UfR.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2Yn4UfR。
- en: You can also run this example online at https://packt.live/37QvoJI. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在线运行此示例，网址为 https://packt.live/37QvoJI。您必须执行整个笔记本才能获得所需的结果。
- en: 'Let''s look more closely at what these Matplotlib commands do:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些 Matplotlib 命令的作用：
- en: 'plt.figure(figsize=(8,6)): This command defines how big our plot should be,
    by providing width and height values. This is always the first command before
    any plotting command is written.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 'plt.figure(figsize=(8,6)): 此命令定义了我们的图表大小，提供了宽度和高度数值。这是在任何绘图命令之前始终首先执行的命令。'
- en: 'plt.xlabel() and plt.ylabel(): These commands take a string as input and allow
    us to specify what the labels for the X and Y axes on the plot should be.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel() 和 plt.ylabel()：这些命令接受字符串作为输入，并允许我们指定绘图的 X 和 Y 轴标签应该是什么。
- en: 'plt.show(): This is the final command that is written when plotting a visualization
    that displays the plot inline within the Jupyter notebook.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()：这是绘制可视化时所写的最终命令，用于在Jupyter笔记本中内联显示图表。
- en: Categorical Data
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类别数据
- en: Categorical variables are ones that take discrete values representing different
    categories or levels of observation that can either be string objects or integer
    values. For example, our target variable, flag_tsunami, is a categorical variable
    with two categories, Tsu and No.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量是那些取离散值，表示不同类别或观察级别的变量，这些值可以是字符串对象或整数值。例如，我们的目标变量flag_tsunami是一个类别变量，有两个类别，分别是Tsu和No。
- en: 'Categorical variables can be of two types:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量可以分为两种类型：
- en: 'Nominal variables: Variables in which the categories are labeled without any
    order of precedence are called nominal variables. An example of a nominal variable
    from our dataset would be location_name. The values that this variable takes cannot
    be said to be ordered, that is, one location is not greater than the other. Similarly,
    more examples of such a variable would be color, types of footwear, ethnicity
    type, and so on.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 名义变量：没有优先顺序的类别标签的变量称为名义变量。我们数据集中的一个名义变量例子是location_name。这个变量的值不能说是有顺序的，即一个位置不大于另一个位置。同样，更多这样的变量例子包括颜色、鞋类类型、种族类型等。
- en: 'Ordinal variables: Variables that have some order associated with them are
    called ordinal variables. An example from our dataset would be damage_description
    since each value represents an increasing value of damage incurred. Another example
    could be days of the week, which would have values from Monday to Sunday, which
    have some order associated with them and we know that Thursday comes after Wednesday
    but before Friday.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 有序变量：具有某种顺序关系的变量称为有序变量。我们数据集中的一个例子是damage_description，因为每个值表示一个逐渐增加的损害程度。另一个例子可以是星期几，它的值从星期一到星期天，具有一定的顺序关系，我们知道星期四在星期三之后但在星期五之前。
- en: Although ordinal variables can be represented by object data types, they are
    often represented as numerical data types as well, often making it difficult to
    differentiate between them and continuous variables.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有序变量可以用对象数据类型表示，但它们通常也用数值数据类型表示，这常常使得它们与连续变量之间的区别变得困难。
- en: One of the major challenges faced when dealing with categorical variables in
    a dataset is high cardinality, that is, a large number of categories or distinct
    values with each value appearing a relatively small number of times. For example,
    location_name has a large number of unique values, with each value occurring a
    small number of times in the dataset.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据集中的类别变量时面临的一个主要挑战是高基数，即类别或不同值的数量非常大，每个值出现的次数相对较少。例如，location_name有大量唯一的值，每个值在数据集中出现的次数很少。
- en: Additionally, non-numerical categorical variables will always require some form
    of preprocessing to be converted into a numerical format so that they can be ingested
    for training by a machine learning model. It can be a challenge to encode categorical
    variables numerically without losing out on contextual information that, despite
    being easy for humans to interpret (due to domain knowledge or otherwise just
    plain common sense), would be hard for a computer to automatically understand.
    For example, a geographical feature such as country or location name by itself
    would give no indication of the geographical proximity of different values, but
    that might just be an important feature—what if earthquakes that occur at locations
    in South East Asia trigger more tsunamis than those that occur in Europe? There
    would be no way of capturing that information by merely encoding the feature numerically.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非数值的类别变量总是需要进行某种形式的预处理，以将其转换为数值格式，以便可以供机器学习模型进行训练。如何将类别变量转换为数值而不丢失上下文信息是一大挑战，尽管这些信息对于人类来说容易理解（由于领域知识或常识），但对于计算机来说却很难自动理解。例如，像国家或位置名称这样的地理特征本身无法表明不同值之间的地理接近性，但这可能是一个重要特征——如果东南亚地区发生的地震比欧洲发生的地震引发更多的海啸呢？仅通过将该特征编码为数值，无法捕捉到这种信息。
- en: 'Exercise 2.07: Identifying Data Types for Categorical Variables'
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.07：识别类别变量的数据类型
- en: 'Let''s establish which variables in our Earthquake dataset are categorical
    and which are continuous. As we now know, categorical variables can also have
    numerical values, so having a numeric data type doesn''t guarantee that a variable
    is continuous. This exercise is a continuation of Exercise 2.05: Performing Imputation
    Using Inferred Values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确定 Earthquake 数据集中哪些变量是分类变量，哪些是连续变量。正如我们现在所知道的，分类变量也可以具有数值，因此仅有数值数据类型并不能保证变量是连续的。本练习是练习
    2.05：使用推测值进行插补的延续：
- en: 'Find all the columns that are numerical and object types. We use the .select_dtypes()
    method on the DataFrame to create a subset DataFrame having numeric (np.number)
    and categorical (np.object) columns, and then print the column names for each.
    For numeric columns, use this command:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 查找所有数值型和对象型的列。我们在 DataFrame 上使用 .select_dtypes() 方法，创建一个子集 DataFrame，包含数值型（np.number）和分类型（np.object）列，然后打印每个列的列名。对于数值列，使用以下命令：
- en: numeric_variables = data.select_dtypes(include=[np.number])
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: numeric_variables = data.select_dtypes(include=[np.number])
- en: numeric_variables.columns
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: numeric_variables.columns
- en: 'The output will be as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.18: All columns that are numerical'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18：所有数值型列'
- en: '](img/image-76AZUMB9.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-76AZUMB9.jpg)'
- en: 'Figure 2.18: All columns that are numerical'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18：所有数值型列
- en: 'For categorical columns, use this command:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类列，使用以下命令：
- en: object_variables = data.select_dtypes(include=[np.object])
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: object_variables = data.select_dtypes(include=[np.object])
- en: object_variables.columns
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: object_variables.columns
- en: 'The output will be as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.19: All columns that are object types'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19：所有对象类型的列'
- en: '](img/image-81ULCRD2.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-81ULCRD2.jpg)'
- en: 'Figure 2.19: All columns that are object types'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19：所有对象类型的列
- en: Here, it is evident that the columns that are object types are categorical variables.
    To differentiate between the categorical and continuous variables from the numeric
    columns, let's see how many unique values there are for each of these features.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，显然，属于对象类型的列是分类变量。为了区分数值列中的分类变量和连续变量，让我们看看每个特征的唯一值数量。
- en: 'Find the number of unique values for numeric features. We use the select_dtypes
    method on the DataFrame to find the number of unique values in each column and
    sort the resulting series in ascending order. For numeric columns, use this command:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 查找数值特征的唯一值个数。我们在 DataFrame 上使用 select_dtypes 方法，查找每列中的唯一值数量，并按升序排列结果序列。对于数值列，使用以下命令：
- en: numeric_variables.nunique().sort_values()
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: numeric_variables.nunique().sort_values()
- en: 'The output will be as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.20: Number of unique values for numeric features'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.20：数值特征的唯一值数量'
- en: '](img/image-AUNV0ACJ.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-AUNV0ACJ.jpg)'
- en: 'Figure 2.20: Number of unique values for numeric features'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20：数值特征的唯一值数量
- en: 'For categorical columns, use this command:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类列，使用以下命令：
- en: object_variables.nunique().sort_values()
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: object_variables.nunique().sort_values()
- en: 'The output will be as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.21: Number of unique values for categorical columns'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.21：分类列的唯一值数量'
- en: '](img/image-YC1T58GJ.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-YC1T58GJ.jpg)'
- en: 'Figure 2.21: Number of unique values for categorical columns'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：分类列的唯一值数量
- en: Note
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2YlSmFt.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2YlSmFt。
- en: You can also run this example online at https://packt.live/31hnuIr. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在 https://packt.live/31hnuIr 在线运行此示例。您必须执行整个 Notebook 才能得到预期的结果。
- en: For the numeric variables, we can see that the top nine have significantly fewer
    unique values than the remaining rows, and it's likely that these are categorical
    variables. However, we must keep in mind that it is possible that some of them
    might just be continuous variables with a low range of rounded-up values. Also,
    month and day would not be considered categorical variables here.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值变量，我们可以看到前九个变量具有显著较少的唯一值，远少于其余行，因此这些变量很可能是分类变量。然而，我们必须记住，某些变量可能只是具有较低范围的四舍五入值的连续变量。另外，月份和日期在这里不应视为分类变量。
- en: 'Exercise 2.08: Calculating Category Value Counts'
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.08：计算分类值的计数
- en: 'For columns with categorical values, it would be useful to see what the unique
    values (categories) of the feature are, along with what the frequencies of these
    categories are, that is, how often does each distinct value occur in the dataset.
    Let''s find the number of occurrences of each 0 to 4 label and NaN values for
    the injuries_description categorical variable. This exercise is a continuation
    of Exercise 2.07: Identifying Data Types for Categorical Variables:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有分类值的列，查看特征的唯一值（类别）及其频率是非常有用的，即每个不同值在数据集中出现的频率。让我们找出 injuries_description
    分类变量中每个 0 到 4 标签和 NaN 值的出现次数。此练习是练习 2.07：识别分类变量的数据类型的延续：
- en: 'Use the value_counts() function on the injuries_description column to find
    the frequency of each category. Using value_counts gives us the frequencies of
    each value in decreasing order in the form of a pandas series:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 value_counts() 函数对 injuries_description 列进行操作，找出每个类别的频率。使用 value_counts 会以降序的形式返回每个值的频率，结果为
    pandas 系列：
- en: counts = data.injuries_description.value_counts(dropna=False)
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: counts = data.injuries_description.value_counts(dropna=False)
- en: counts
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 计数
- en: 'The output should be as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 2.22: Frequency of each category'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.22：每个类别的频率'
- en: '](img/image-UW9LQJF8.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-UW9LQJF8.jpg)'
- en: 'Figure 2.22: Frequency of each category'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22：每个类别的频率
- en: 'Sort the values in increasing order of the ordinal variable. If we want the
    frequencies in the order of the values themselves, we can reset the index to give
    us a DataFrame and sort values by the index (that is, the ordinal variable):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 按顺序变量的升序排序值。如果我们希望按值本身的频率排序，我们可以重置索引以得到一个 DataFrame，并按索引排序（即按顺序变量）：
- en: counts.reset_index().sort_values(by='index')
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: counts.reset_index().sort_values(by='index')
- en: 'The output will be as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 2.23: Sorted values'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.23：排序后的值'
- en: '](img/image-TGI2TTM4.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-TGI2TTM4.jpg)'
- en: 'Figure 2.23: Sorted values'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.23：排序后的值
- en: Note
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Yn5URj.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2Yn5URj。
- en: You can also run this example online at https://packt.live/314dYIr. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/314dYIr 上在线运行此示例。你必须执行整个 Notebook 才能得到所需结果。
- en: 'Exercise 2.09: Plotting a Pie Chart'
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.09：绘制饼图
- en: 'Since our target variable in our sample data is categorical, the example in
    Exercise 2.06: Plotting a Bar Chart, showed us one way of visualizing how the
    categorical values are distributed (using a bar chart). Another plot that can
    make it easy to see how each category functions as a fraction of the overall dataset
    is a pie chart. Let''s plot a pie chart to visualize the distribution of the discrete
    values of the damage_description variable. This exercise is a continuation of
    Exercise 2.08, Calculating Category Value Counts:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们样本数据中的目标变量是分类变量，在练习 2.06：绘制柱状图中，我们展示了可视化分类值分布的一种方法（使用柱状图）。另一个可以清楚显示每个类别如何作为整体数据集的一部分的方法是饼图。我们将绘制一个饼图来可视化
    damage_description 变量的离散值分布。此练习是练习 2.08：计算类别值计数的延续：
- en: 'Format the data into the form that needs to be plotted. Here, we run value_counts()
    over the column and sort the series by index:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据格式化为需要绘制的形式。在这里，我们对列运行 value_counts() 并按索引排序系列：
- en: counts = data.damage_description.value_counts()
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: counts = data.damage_description.value_counts()
- en: counts = counts.sort_index()
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: counts = counts.sort_index()
- en: 'Plot the pie chart. The plt.pie() category plots the pie chart using the count
    data. We will use the same three steps for plotting as described in Exercise 2.06:
    Plotting a Bar Chart:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制饼图。plt.pie() 类别绘制饼图并使用计数数据。我们将使用在练习 2.06：绘制柱状图中描述的相同三个步骤进行绘制：
- en: fig, ax = plt.subplots(figsize=(10,10))
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: fig, ax = plt.subplots(figsize=(10,10))
- en: slices = ax.pie(counts, \
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: slices = ax.pie(counts, \
- en: labels=counts.index, \
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: labels=counts.index, \
- en: colors = ['white'], \
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: colors = ['white'], \
- en: 'wedgeprops = {''edgecolor'': ''black''})'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 'wedgeprops = {''edgecolor'': ''black''})'
- en: patches = slices[0]
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: patches = slices[0]
- en: hatches = ['/', '\\', '|', '-', '+', 'x', 'o', 'O', '\.', '*']
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: hatches = ['/', '\\', '|', '-', '+', 'x', 'o', 'O', '\.', '*']
- en: 'for patch in range(len(patches)):'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个补丁执行循环，范围为补丁数量：
- en: patches[patch].set_hatch(hatches[patch])
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: patches[patch].set_hatch(hatches[patch])
- en: plt.title('Pie chart showing counts for\ndamage_description '\
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title('饼图显示\ndamage_description 的计数 '\
- en: '''categories'')'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '''类别'')'
- en: plt.show()
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 2.24: Pie chart showing counts for damage_description categories'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.24：显示 damage_description 类别计数的饼图'
- en: '](img/image-5PCI6LVT.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-5PCI6LVT.jpg)'
- en: 'Figure 2.24: Pie chart showing counts for damage_description categories'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24：饼图显示损害描述类别的计数
- en: Note
  id: totrans-405
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/37Ovj9s.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/37Ovj9s。
- en: You can also run this example online at https://packt.live/37OvotM. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/37OvotM 在线运行这个示例。你必须执行整个Notebook才能获得预期的结果。
- en: 'Figure 2.24 tells us the relative number of items in each of the five damage
    description categories. Note that it would be good practice to do the extra work
    to change the uninformative labels to the categories—recall from the EDA discussion
    that:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24展示了五个损害描述类别中每个类别的相对项目数量。请注意，最好做额外的工作将无意义的标签更改为类别标签——回忆一下从EDA讨论中提到的：
- en: 0 = NONE
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 0 = 无
- en: 1 = LIMITED (roughly corresponding to less than $1 million)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 1 = 有限（大致对应于低于100万美元）
- en: 2 = MODERATE (~$1 to $5 million)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 2 = 中等（约100万到500万美元）
- en: 3 = SEVERE (~>$5 to $24 million)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 3 = 严重（约500万到2400万美元）
- en: 4 = EXTREME (~$25 million or more)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 4 = 极端（约2500万美元或更多）
- en: In addition, while the pie chart gives us a quick visual impression of which
    are the largest and smallest categories, we get no idea of the actual quantities,
    so adding those labels would increase the value of the chart. You can use the
    code in the repository for this book to update the chart.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然饼图能给我们一个快速的视觉印象，告诉我们哪些是最大和最小的类别，但它并没有提供实际的数量信息，因此添加这些标签将提升图表的价值。你可以使用本书中仓库里的代码来更新图表。
- en: Continuous Data
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续数据
- en: 'Continuous variables can take any number of values and are usually integer
    (for example, number of deaths) or float data types (for example, the height of
    a mountain). It''s useful to get an idea of the basic statistics of the values
    in the feature: the minimum, maximum, and percentile values we see from the output
    of the describe() function gives us a fair estimate of this.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量可以取任何数量的值，通常是整数（例如，死亡人数）或浮动数据类型（例如，山的高度）。了解特征值的基本统计信息是非常有用的：从describe()函数的输出中得到的最小值、最大值和百分位数值可以为我们提供一个合理的估计。
- en: However, for continuous variables, it is also very useful to see how the values
    are distributed in the range they operate in. Since we cannot simply find the
    counts of individual values, instead, we order the values in ascending order,
    group them into evenly-sized intervals, and find the counts for each interval.
    This gives us the underlying frequency distribution and plotting this gives us
    a histogram, which allows us to examine the shape, central values, and amount
    of variability in the data.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于连续变量，查看其在所处范围内的分布情况也是非常有用的。由于我们不能简单地找到单个值的计数，因此我们将值按升序排列，分组为均匀大小的区间，并计算每个区间的计数。这为我们提供了底层的频率分布，并绘制这个分布会得到一个直方图，这让我们能够检查数据的形状、中心值和变异性。
- en: Histograms give us an easy view of the data that we're looking at. They tell
    us about the behavior of the values at a glance in terms of the underlying distribution
    (for example, a normal or exponential distribution), the presence of outliers,
    skewness, and more.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图为我们提供了一个便捷的视图，让我们可以一眼看出数据的表现，包括底层分布（例如，正态分布或指数分布）、异常值、偏斜程度等。
- en: Note
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is easy to get confused between a bar chart and a histogram. The major difference
    is that a histogram is used to plot continuous data that has been binned to visualize
    the frequency distribution, while bar charts can be used for a variety of other
    use cases, including to represent categorical variables as we have done. Additionally,
    with histograms, the number of bins is something we can vary, so the range of
    values in a bin is determined by the number of bins, as is the height of the bars
    in the histogram. In a bar chart, the width of the bars does not generally convey
    meaning, and the height is usually a property of the category, like a count.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易将条形图和直方图混淆。主要的区别在于，直方图用于绘制已分箱的连续数据，用以可视化频率分布，而条形图可以用于多种其他用途，包括表示分类变量，正如我们所做的那样。此外，在直方图中，箱数是可以变化的，因此箱内值的范围由箱数决定，直方图中条形的高度也是如此。在条形图中，条形的宽度通常不具有传达意义，而高度通常是类别的属性，比如计数。
- en: One of the most common frequency distributions is a Gaussian (or normal) distribution.
    This is a symmetric distribution that has a bell-shaped curve, which indicates
    that the values near the middle of the range have the highest occurrences in the
    dataset with a symmetrically decreasing frequency of occurrences as we move away
    from the middle. You almost certainly have seen examples of Gaussian distributions,
    because many natural and man-made processes generate values that vary nearly like
    the Gaussian distribution. Thus, it is extremely common to see data compared to
    the Gaussian distribution.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的频率分布之一是高斯（或正态）分布。这是一个对称分布，具有钟形曲线，表示数据集中中间范围附近的值出现的频率最高，随着远离中间，出现的频率对称性地减少。你几乎肯定见过高斯分布的例子，因为许多自然和人为过程生成的值几乎呈现高斯分布。因此，数据与高斯分布的比较是非常常见的。
- en: 'It is a probability distribution and the area under the curve equals one, as
    shown in Figure 2.25:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个概率分布，曲线下方的面积等于 1，如图 2.25 所示：
- en: '![Figure 2.25: Gaussian (normal) distribution'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.25：高斯（正态）分布'
- en: '](img/image-5FZD4VW6.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-5FZD4VW6.jpg)'
- en: 'Figure 2.25: Gaussian (normal) distribution'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.25：高斯（正态）分布
- en: A symmetric distribution like normal distribution can be characterized entirely
    by two parameters—the mean (µ) and the standard deviation (σ). In Figure 2.25,
    the mean is at 7.5, for example. However, there are significant amounts of real
    data that do not follow a normal distribution and may be asymmetric. The asymmetry
    of data is often referred to as a skew.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 像正态分布这样的对称分布可以通过两个参数完全描述——均值（µ）和标准差（σ）。例如，在图 2.25 中，均值为 7.5。然而，许多实际数据并不遵循正态分布，可能是非对称的。数据的非对称性通常被称为偏斜。
- en: Skewness
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏度
- en: A distribution is said to be skewed if it is not symmetric in nature, and skewness
    measures the asymmetry of a variable about its mean. The value can be positive
    or negative (or undefined). In the former case, the tail is on the right-hand
    side of the distribution, while the latter indicates that the tail is on the left-hand
    side.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个分布不对称，则称为偏斜，偏度衡量的是变量相对于其均值的非对称性。其值可以为正、负（或未定义）。在前一种情况下，尾部位于分布的右侧，而后一种情况则表示尾部位于左侧。
- en: However, it must be noted that a thick and short tail would have the same effect
    on the value of skewness as a long, thin tail.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，必须注意的是，粗短的尾部与细长的尾部对偏度值的影响是相同的。
- en: Kurtosis
  id: totrans-430
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 峰度
- en: Kurtosis is a measure of the tailedness of the distribution of a variable and
    is used to measure the presence of outliers in one tail versus the other. A high
    value of kurtosis indicates a fatter tail and the presence of outliers. In a similar
    way to the concept of skewness, kurtosis also describes the shape of the distribution.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 峰度是衡量变量分布尾部程度的指标，用于衡量一个尾部相对于另一个尾部的异常值情况。高峰度值表示尾部较胖，并且存在异常值。与偏度概念类似，峰度也描述了分布的形状。
- en: 'Exercise 2.10: Plotting a Histogram'
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.10：绘制直方图
- en: 'Let''s plot the histogram for the eq_primary feature using the Seaborn library.
    This exercise is a continuation of Exercise 2.09, Plotting a Pie Chart:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Seaborn 库绘制 eq_primary 特征的直方图。这个练习是练习 2.09，绘制饼图的延续：
- en: 'Use plt.figure() to initiate the plotting:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 plt.figure() 来初始化绘图：
- en: plt.figure(figsize=(10,7))
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10,7))
- en: 'sns.distplot() is the primary command that we will use to plot the histogram.
    The first parameter is the one-dimensional data over which to plot the histogram,
    while the bins parameter defines the number and size of the bins. Use this as
    follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: sns.distplot() 是我们用来绘制直方图的主要命令。第一个参数是一维数据，它决定了要绘制直方图的数据，而 bins 参数定义了箱子的数量和大小。使用方法如下：
- en: sns.distplot(data.eq_primary.dropna(), \
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: sns.distplot(data.eq_primary.dropna(), \
- en: bins=np.linspace(0,10,21))
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: bins=np.linspace(0,10,21))
- en: 'Display the plot using plt.show():'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 plt.show() 显示图表：
- en: plt.show()
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.26: Histogram for the example primary feature'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.26：示例主特征的直方图'
- en: '](img/image-DME453BH.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-DME453BH.jpg)'
- en: 'Figure 2.26: Histogram for the example primary feature'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26：示例主特征的直方图
- en: The plot gives us a normed (or normalized) histogram, which means that the area
    under the bars of the histogram equals unity. Additionally, the line over the
    histogram is the kernel density estimate, which gives us an idea of what the probability
    distribution for the variable would look like.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 图表为我们提供了一个标准化（或归一化）的直方图，这意味着直方图柱形下方的面积等于 1。此外，直方图上方的线是核密度估计，它能给我们一个变量概率分布的大致形态。
- en: Note
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2BwZrdj.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考 https://packt.live/2BwZrdj。
- en: You can also run this example online at https://packt.live/3fMSxj2\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行此示例，网址为 https://packt.live/3fMSxj2。你必须执行整个 Notebook 才能获得期望的结果。
- en: From the plot, we can see that the values of eq_primary lie mostly between 5
    and 8, which means that most earthquakes had a magnitude with a moderate to high
    value, with barely any earthquakes having a low or very high magnitude.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中，我们可以看到 `eq_primary` 的值大多介于 5 和 8 之间，这意味着大多数地震的震级处于中等至高值范围，几乎没有地震的震级是低值或极高值。
- en: 'Exercise 2.11: Computing Skew and Kurtosis'
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.11：计算偏度和峰度
- en: 'Let''s calculate the skew and kurtosis values for all of the features in the
    dataset using the core pandas functions available to us. This exercise is a continuation
    of Exercise 2.10, Plotting a Histogram:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们使用 pandas 中的核心函数来计算数据集中所有特征的偏度和峰度值。此练习是练习 2.10: 绘制直方图的延续：'
- en: 'Use the .skew() DataFrame method to calculate the skew for all features and
    then sort the values in ascending order:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 .skew() DataFrame 方法计算所有特征的偏度，然后按升序排列这些值：
- en: data.skew().sort_values()
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: data.skew().sort_values()
- en: 'The output will be as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.27: Skew values for all the features in the dataset'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.27: 数据集中所有特征的偏度值'
- en: '](img/image-V0JJI54Q.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-V0JJI54Q.jpg)'
- en: 'Figure 2.27: Skew values for all the features in the dataset'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.27: 数据集中所有特征的偏度值'
- en: 'Use the .kurt() DataFrame method to calculate the kurtosis for all features:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 .kurt() DataFrame 方法计算所有特征的峰度：
- en: data.kurt()
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: data.kurt()
- en: 'The output will be as follows:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.28: Kurtosis values for all the features in the dataset'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.28: 数据集中所有特征的峰度值'
- en: '](img/image-WUUXAE1Y.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-WUUXAE1Y.jpg)'
- en: 'Figure 2.28: Kurtosis values for all the features in the dataset'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.28: 数据集中所有特征的峰度值'
- en: Here, we can see that the kurtosis values for some variables deviate significantly
    from 0\. This means that these columns have a long tail. But the values that are
    at the tail end of these variables (which indicate the number of people dead,
    injured, and the monetary value of damage), in our case, may be outliers that
    we may need to pay special attention to. Larger values might, in fact, indicate
    an additional force that added to the devastation caused by an earthquake, that
    is, a tsunami.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们可以看到，某些变量的峰度值与 0 的偏差较大。这意味着这些列具有长尾分布。但是，这些变量尾部的值（表示死亡人数、受伤人数和损失金额），在我们的案例中，可能是我们需要特别关注的异常值。较大的值实际上可能表示有额外的力量加剧了地震带来的破坏，即海啸。
- en: Note
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Yklmh0.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考 https://packt.live/2Yklmh0。
- en: You can also run this example online at https://packt.live/37PcMdj. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行此示例，网址为 https://packt.live/37PcMdj。你必须执行整个 Notebook 才能获得期望的结果。
- en: 'Activity 2.02: Representing the Distribution of Values Visually'
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 2.02：可视化表示值的分布
- en: 'In this activity, we will implement what we learned in the previous section
    by creating different plots such as histograms and pie charts. Furthermore, we
    will calculate the skew and kurtosis for the features of the dataset. Here, will
    use the same dataset we used in Activity 2.01: Summary Statistics and Missing
    Values, that is, House Prices: Advanced Regression Techniques. We''ll use different
    types of plots to visually represent the distribution of values for this dataset.
    This activity is a continuation of Activity 2.01: Summary Statistics and Missing
    Values:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '在此活动中，我们将通过创建不同的图表，如直方图和饼图，来实现上一节中学到的内容。此外，我们将计算数据集特征的偏度和峰度。这里我们将使用在活动 2.01:
    总结统计与缺失值中使用的相同数据集，即“房价：高级回归技巧”数据集。我们将使用不同类型的图表来可视化该数据集的值分布。本活动是活动 2.01: 总结统计与缺失值的延续：'
- en: 'The steps to be performed are as follows:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Plot a histogram using Matplotlib for the target variable, SalePrice.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Matplotlib 绘制目标变量 SalePrice 的直方图。
- en: 'The output will be as follows:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 2.29: Histogram for the target variable'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.29：目标变量的直方图'
- en: '](img/image-5YLC5971.jpg)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-5YLC5971.jpg)'
- en: 'Figure 2.29: Histogram for the target variable'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.29：目标变量的直方图
- en: Find the number of unique values within each column having an object type.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 找出每个列中具有对象类型的唯一值的数量。
- en: Create a DataFrame representing the number of occurrences for each categorical
    value in the HouseStyle column.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个数据框，表示 HouseStyle 列中每个类别值的出现次数。
- en: Plot a pie chart representing these counts.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制表示这些计数的饼图。
- en: 'The output will be as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 2.30: Pie chart representing the counts'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.30：表示计数的饼图'
- en: '](img/image-IYKYADF1.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-IYKYADF1.jpg)'
- en: 'Figure 2.30: Pie chart representing the counts'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.30：表示计数的饼图
- en: Find the number of unique values within each column having a number type.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 找出每个列中具有数字类型的唯一值的数量。
- en: Plot a histogram using seaborn for the LotArea variable.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 seaborn 绘制 LotArea 变量的直方图。
- en: 'The output will be as follows:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 2.31: Histogram for the LotArea variable'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.31：LotArea 变量的直方图'
- en: '](img/image-JBM6D62T.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JBM6D62T.jpg)'
- en: 'Figure 2.31: Histogram for the LotArea variable'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.31：LotArea 变量的直方图
- en: Calculate the skew and kurtosis values for the values in each column.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每列值的偏斜度和峰度值。
- en: 'The output for skew values will be:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 偏斜度值的输出将如下所示：
- en: '![Figure 2.32: Skew values for each column'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.32：每列的偏斜值'
- en: '](img/image-8QWHUC43.jpg)'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-8QWHUC43.jpg)'
- en: 'Figure 2.32: Skew values for each column'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.32：每列的偏斜值
- en: 'The output for kurtosis values will be:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 对峰度值的输出将如下所示：
- en: '![Figure 2.33: Kurtosis values for each column'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.33：每列的峰度值'
- en: '](img/image-XPECYU25.jpg)'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-XPECYU25.jpg)'
- en: 'Figure 2.33: Kurtosis values for each column'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.33：每列的峰度值
- en: Note
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以通过此链接找到。
- en: We have seen how to look into the nature of data in more detail, in particular,
    by beginning to understand the distribution of the data using histograms or density
    plots, relative counts of data using pie charts, as well as inspecting the skew
    and kurtosis of the variables as a first step to finding potentially problematic
    data, outliers, and so on.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何更详细地了解数据的性质，特别是通过开始理解数据的分布，使用直方图或密度图、数据的相对计数使用饼图，以及检查变量的偏斜度和峰度，作为发现潜在问题数据、异常值等的第一步。
- en: By now, you should have a comfort level handling various statistical measures
    of data such as summary statistics, counts, and the distribution of values. Using
    tools such as histograms and density plots, you can explore the shape of datasets,
    and augment that understanding by calculating statistics such as skew and kurtosis.
    You should be developing some intuition for some flags that warrant further investigation,
    such as large skew or kurtosis values.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经能够熟练处理各种数据统计量，例如摘要统计、计数和数值分布。使用直方图和密度图等工具，你可以探索数据集的形状，并通过计算如偏斜度和峰度等统计量来增强对数据集的理解。你应该逐渐培养一些直觉，识别出需要进一步调查的标志，如较大的偏斜度或峰度值。
- en: Relationships within the Data
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据中的关系
- en: 'There are two reasons why it is important to find relationships between variables
    in the data:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找数据中变量之间关系的重要性有两个原因：
- en: Establishing which features are potentially important can be deemed essential,
    since finding ones that have a strong relationship with the target variable will
    aid in the feature selection process.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 确定哪些特征可能重要是至关重要的，因为找到与目标变量有强关系的特征有助于特征选择过程。
- en: Finding relationships between different features themselves can be useful since
    variables in the dataset are usually never completely independent of every other
    variable and this can affect our modeling in a number of ways.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找不同特征之间的关系也很有用，因为数据集中的变量通常不是完全独立的，这会以多种方式影响我们的建模。
- en: Now, there are a number of ways in which we can visualize these relationships,
    and this really depends on the types of variable we are trying to find the relationship
    between, and how many we are considering as part of the equation or comparison.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有多种方式可以可视化这些关系，这主要取决于我们试图找到关系的变量类型，以及我们在方程或比较中考虑的变量数量。
- en: Relationship between Two Continuous Variables
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 两个连续变量之间的关系
- en: Establishing a relationship between two continuous variables is basically seeing
    how one varies as the value of the other is increased. The most common way to
    visualize this would be to use a scatter plot, in which we take each variable
    along a single axis (the X and Y axes in a two-dimensional plane when we have
    two variables) and plot each data point using a marker in the X-Y plane. This
    visualization gives us a good idea of whether any kind of relationship exists
    between the two variables at all.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 建立两个连续变量之间的关系基本上是观察其中一个变量在另一个变量增加时如何变化。最常见的可视化方法是使用散点图，我们将每个变量沿单一轴（当有两个变量时是二维平面中的
    X 和 Y 轴）绘制，并在 X-Y 平面中使用标记绘制每个数据点。这种可视化方式可以很好地帮助我们了解这两个变量之间是否存在某种关系。
- en: If we want to quantize the relationship between the two variables, however,
    the most common method is to find the correlation between them. If the target
    variable is continuous and it has a high degree of correlation with another variable,
    this is an indication that the feature would be an important part of the model.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想量化两个变量之间的关系，最常见的方法是找出它们之间的相关性。如果目标变量是连续的，并且与另一个变量有很高的相关性，这表明该特征将在模型中占据重要地位。
- en: Pearson's Coefficient of Correlation
  id: totrans-510
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: 'Pearson''s Coefficient of Correlation is a correlation coefficient that is
    commonly used to show the linear relationship between a pair of variables. The
    formula returns a value between -1 and +1, where:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数是常用的相关系数，用于展示一对变量之间的线性关系。该公式返回一个介于 -1 和 +1 之间的值，其中：
- en: +1 indicates a strong positive relationship
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: +1 表示强正相关
- en: -1 indicates a strong negative relationship
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: -1 表示强负相关
- en: 0 indicates no relationship at all
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 0 表示完全没有关系
- en: It's also useful to find correlations between pairs of features themselves.
    In some models, highly correlated features can cause issues, including coefficients
    that vary strongly with small changes in data or modal parameters. In the extreme
    case, perfectly correlated features (such as X2 = 2.5 * X1) cause some models,
    including linear regression, to return undefined coefficients (values of Inf).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 找出特征对之间的相关性也非常有用。在某些模型中，高度相关的特征可能会引发问题，包括系数在数据或模型参数发生微小变化时剧烈波动。在极端情况下，完全相关的特征（例如
    X2 = 2.5 * X1）会导致某些模型（包括线性回归）返回未定义的系数（如 Inf）。
- en: Note
  id: totrans-516
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When fitting a linear model, having features that are highly correlated to one
    another can result in an unpredictable and widely varying model. This is because
    the coefficients of each feature in a linear model can be interpreted as the unit
    change in the target variable, keeping all other features constant. When a set
    of features is not independent (that is, are correlated), however, we cannot determine
    the effect of the independent changes on the target variable due to each feature,
    resulting in widely varying coefficients.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合线性模型时，特征之间的高度相关性可能会导致模型不可预测且变化较大。这是因为线性模型中每个特征的系数可以解释为在保持其他特征不变的情况下，目标变量的单位变化。然而，当一组特征不是独立的（即它们是相关的）时，我们无法确定每个特征对目标变量的独立变化影响，导致系数大幅波动。
- en: To find the pairwise correlation for every numeric feature in a DataFrame with
    every other feature, we can use the .corr() function on the DataFrame.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找 DataFrame 中每个数值特征与其他特征之间的配对相关性，我们可以使用 DataFrame 上的 `.corr()` 函数。
- en: 'Exercise 2.12: Plotting a Scatter Plot'
  id: totrans-519
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.12：绘制散点图
- en: 'Let''s plot a scatter plot between the primary earthquake magnitude on the
    X axis and the corresponding number of injuries on the Y axis. This exercise is
    a continuation of Exercise 2.11, Computing Skew and Kurtosis:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来绘制一个散点图，X 轴为主要地震震级，Y 轴为相应的受伤人数。此练习是练习 2.11（计算偏度和峰度）的延续：
- en: 'Filter out the null values. Since we know that there are null values in both
    columns, let''s first filter the data to include only the non-null rows:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 筛选出非空值。由于我们知道两列中都有空值，首先我们筛选出只包含非空行的数据：
- en: data_to_plot = data[~pd.isnull(data.injuries) \
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: data_to_plot = data[~pd.isnull(data.injuries) \
- en: '& ~pd.isnull(data.eq_primary)]'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '& ~pd.isnull(data.eq_primary)]'
- en: 'Create and display the scatter plot. We will use Matplotlib''s plt.scatter(x=...,
    y=...) command as the primary command for plotting the data. The x and y parameters
    state which feature is to be considered along which axis. They take a single-dimensional
    data structure such as a list, a tuple, or a pandas series. We can also send the
    scatter function more parameters that define, say, the icon to use to plot an
    individual data point. For example, to use a red cross as the icon, we would need
    to send the parameters marker=''x'', c=''r'':'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并显示散点图。我们将使用 Matplotlib 的 plt.scatter(x=..., y=...) 命令作为绘制数据的主要命令。x 和 y 参数指定在哪个轴上考虑哪个特征。它们接受单维数据结构，如列表、元组或
    pandas 系列。我们还可以向 scatter 函数发送更多参数，以定义例如要使用的图标来绘制单个数据点。例如，要使用红色十字作为图标，我们需要发送参数
    marker='x'，c='r'：
- en: plt.figure(figsize=(12,9))
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(12,9))
- en: plt.scatter(x=data_to_plot.eq_primary, y=data_to_plot.injuries)
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(x=data_to_plot.eq_primary, y=data_to_plot.injuries)
- en: plt.xlabel('Primary earthquake magnitude')
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('主要地震震级')
- en: plt.ylabel('No. of injuries')
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('受伤人数')
- en: plt.show()
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.34: Scatter plot'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.34: 散点图'
- en: '](img/image-OA11WG07.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-OA11WG07.jpg)'
- en: 'Figure 2.34: Scatter plot'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 2.34: 散点图'
- en: From the plot, we can infer that although there doesn't appear to be a trend
    between the number of people who were injured and the earthquake magnitude, there
    is an increasing number of earthquakes with large injury counts as the magnitude
    increases. However, for the majority of earthquakes, there does not seem to be
    a relationship.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以推断，尽管看不出受伤人数与地震震级之间有趋势，但随着震级增加，受伤人数较多的地震数量有所增加。然而，对于大多数地震，似乎没有明显的关系。
- en: Note
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/314eupR.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 https://packt.live/314eupR。
- en: You can also run this example online at https://packt.live/2YWtbsm. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在 https://packt.live/2YWtbsm 上在线运行此示例。为了获得期望的结果，您必须执行整个笔记本。
- en: 'Exercise 2.13: Plotting a Correlation Heatmap'
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '练习 2.13: 绘制相关性热图'
- en: Let's plot a correlation heatmap between all the numeric variables in our dataset
    using seaborn's sns.heatmap() function on the inter-feature correlation values
    in the dataset. This exercise is a continuation of Exercise 2.12, Plotting a Scatter
    Plot.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 seaborn 的 sns.heatmap() 函数在数据集中的特征间相关性值上绘制一个相关性热图。这是练习 2.12 绘制散点图的延续。
- en: 'The optional parameters passed to the sns.heatmap() function are square and
    cmap, which indicate that the plot should be such that each pixel is square and
    specify which color scheme to use, respectively:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 sns.heatmap() 函数的可选参数为 square 和 cmap，分别表示绘制的每个像素为正方形，并指定使用的颜色方案：
- en: 'Plot a basic heatmap with all the features:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制包含所有特征的基本热图：
- en: plt.figure(figsize = (12,10))
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize = (12,10))
- en: sns.heatmap(data.corr(), square=True, cmap="YlGnBu")
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: sns.heatmap(data.corr(), square=True, cmap="YlGnBu")
- en: plt.show()
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.35: Correlation heatmap'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.35: 相关性热图'
- en: '](img/image-KJ0HRKHR.jpg)'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-KJ0HRKHR.jpg)'
- en: 'Figure 2.35: Correlation heatmap'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 2.35: 相关性热图'
- en: We can see from the color bar on the right of the plot that the minimum value,
    around -0.2, is the lightest shade, which is a misrepresentation of the correlation
    values, which vary from -1 to 1.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从图右侧的色条中看到，最小值约为 -0.2，是最浅的色调，这是对相关性值的误表示，实际上相关性值的范围是从 -1 到 1。
- en: 'Plot a subset of features in a more customized heatmap. We will specify the
    upper and lower limits using the vmin and vmax parameters and plot the heatmap
    again with annotations specifying the pairwise correlation values on a subset
    of features. We will also change the color scheme to one that can be better interpreted—while
    the neutral white will represent no correlation, increasingly darker shades of
    blue and red will represent higher positive and negative correlation values, respectively:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在更定制的热图中绘制特征子集。我们将使用 vmin 和 vmax 参数指定上下限，并在特定特征子集上重新绘制带有注释的热图，指定成对相关性值。我们还将更改颜色方案，以便更好地解释—中性的白色表示无相关性，越来越深的蓝色和红色表示更高的正相关和负相关值：
- en: feature_subset = ['focal_depth', 'eq_primary', 'eq_mag_mw', \
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 特征子集 = ['震源深度', '主要地震震级', '震级 (MW)', \
- en: '''eq_mag_ms'', ''eq_mag_mb'', ''intensity'', \'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '''震级 (MS)'', ''震级 (MB)'', ''强度'', \'
- en: '''latitude'', ''longitude'', ''injuries'', \'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '''纬度'', ''经度'', ''受伤人数'', \'
- en: '''damage_millions_dollars'',''total_injuries'', \'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '''damage_millions_dollars'',''total_injuries'', \'
- en: '''total_damage_millions_dollars'']'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '''total_damage_millions_dollars'']'
- en: plt.figure(figsize = (12,10))
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize = (12,10))
- en: sns.heatmap(data[feature_subset].corr(), square=True, \
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: sns.heatmap(data[feature_subset].corr(), square=True, \
- en: annot=True, cmap="RdBu", vmin=-1, vmax=1)
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: annot=True, cmap="RdBu", vmin=-1, vmax=1)
- en: plt.show()
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.36: Customized correlation heatmap'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.36：自定义相关性热图](img/image-8A67TY8S.jpg)'
- en: '](img/image-Y1Y2KFEB.jpg)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-Y1Y2KFEB.jpg)'
- en: 'Figure 2.36: Customized correlation heatmap'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.36：自定义相关性热图
- en: Note
  id: totrans-564
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Z1lPUB.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2Z1lPUB。
- en: You can also run this example online at https://packt.live/2YntBc8\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/2YntBc8 上在线运行这个示例。你必须执行整个笔记本才能得到预期的结果。
- en: Now, while we can calculate the value of correlation, this only gives us an
    indication of a linear relationship. To better judge whether there's a possible
    dependency, we could plot a scatter plot between pairs of features, which is mostly
    useful when the relationship between the two variables is not known, and visualizing
    how the data points are scattered or distributed could give us an idea of whether
    (and how) the two may be related.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，虽然我们可以计算相关性值，但这仅仅给出了线性关系的一个指示。为了更好地判断是否存在可能的依赖关系，我们可以绘制特征对之间的散点图，这在变量之间的关系未知时尤其有用，且可视化数据点如何散布或分布可以让我们初步判断这两个变量是否可能有关联（以及如何关联）。
- en: Using Pairplots
  id: totrans-568
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用成对图
- en: A pairplot is useful for visualizing multiple relationships between pairs of
    features at once and can be plotted using Seaborn's .pairplot() function. In the
    following exercise, we will create a pairplot and visualize relations between
    the features in a dataset.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 成对图对于一次性可视化多个特征对之间的关系非常有用，可以使用 Seaborn 的 .pairplot() 函数绘制。在接下来的练习中，我们将创建一个成对图，并可视化数据集中特征之间的关系。
- en: 'Exercise 2.14: Implementing a Pairplot'
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.14：实现一个成对图
- en: 'In this exercise, we will look at a pairplot between the features having the
    highest pairwise correlation in the dataset. This exercise is a continuation of
    Exercise 2.13, Plotting a Correlation Heatmap:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将查看数据集中具有最高成对相关性的特征之间的成对图。此练习是练习 2.13《绘制相关性热图》的延续：
- en: 'Define a list having the subset of features on which to create the pairplot:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个列表，其中包含要创建成对图的特征子集：
- en: feature_subset = ['focal_depth', 'eq_primary', 'eq_mag_mw', \
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: feature_subset = ['focal_depth', 'eq_primary', 'eq_mag_mw', \
- en: '''eq_mag_ms'', ''eq_mag_mb'', ''intensity'',]'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '''eq_mag_ms'', ''eq_mag_mb'', ''intensity'',]'
- en: Create the pairplot using seaborn. The arguments sent to the plotting function
    are kind='scatter', which indicates that we want each individual plot between
    the pair of variables in the grid to be represented as a scatter plot, and diag_kind='kde',
    which indicates that we want the plots along the diagonal (where both the features
    in the pair are the same) to be a kernel density estimate.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 seaborn 创建成对图。传递给绘图函数的参数是 kind='scatter'，这表示我们希望网格中每一对变量之间的单独图像显示为散点图，diag_kind='kde'，这表示我们希望对角线（即特征对相同的地方）上的图形是一个核密度估计。
- en: 'It should also be noted here that the plots symmetrically across the diagonal
    from one another will essentially be the same, just with the axes reversed:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，这里对角线对称的图将本质上是相同的，只是坐标轴被反转了：
- en: sns.pairplot(data[feature_subset].dropna(), kind ='scatter', \
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: sns.pairplot(data[feature_subset].dropna(), kind ='scatter', \
- en: diag_kind='kde')
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: diag_kind='kde')
- en: plt.show()
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.37: Pairplot between the features having the highest pairwise correlation'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.37：具有最高成对相关性的特征之间的成对图](img/image-Y1Y2KFEB.jpg)'
- en: '](img/image-8A67TY8S.jpg)'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-8A67TY8S.jpg)'
- en: 'Figure 2.37: Pairplot between the features having the highest pairwise correlation'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.37：具有最高成对相关性的特征之间的成对图
- en: We have successfully visualized a pairplot to look at the features that have
    high correlation between them within a dataset.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地通过成对图可视化了数据集中具有高相关性的特征之间的关系。
- en: Note
  id: totrans-585
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Ni11T0.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2Ni11T0。
- en: You can also run this example online at https://packt.live/3eol7aj. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/3eol7aj 上在线运行这个示例。你必须执行整个笔记本才能得到预期的结果。
- en: Relationship between a Continuous and a Categorical Variable
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 连续变量与分类变量之间的关系
- en: 'A common way to view the relationship between two variables when one is categorical
    and the other is continuous is to use a bar plot or a box plot:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 查看一个分类变量和一个连续变量之间关系的常见方法是使用条形图或箱线图：
- en: A bar plot helps compare the value of a variable for a discrete set of parameters
    and is one of the most common types of plots. Each bar represents a categorical
    value and the height of the bar usually represents an aggregated value of the
    continuous variable over that category (such as average, sum, or count of the
    values of the continuous variable in that category).
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图有助于比较离散参数集的变量值，是最常见的图表类型之一。每个条形代表一个分类值，条形的高度通常代表该类别下连续变量的聚合值（例如平均值、总和或该类别下连续变量值的计数）。
- en: A box plot is a rectangle drawn to represent the distribution of the continuous
    variable for each discrete value of the categorical variable. It not only allows
    us to visualize outliers efficiently but also allows us to compare the distribution
    of the continuous variable across categories of the categorical variable. The
    lower and upper edges of the rectangle represent the first and third quartiles,
    respectively, the line down through the middle represents the median value, and
    the points (or fliers) above and below the rectangle represent outlier values.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图是用来表示每个分类变量的离散值对应的连续变量分布的矩形。它不仅能有效地可视化离群值，还能帮助我们比较分类变量不同类别之间的连续变量分布。矩形的上下边缘分别代表第一四分位数和第三四分位数，中间的线代表中位数，矩形上下方的点（或异常值）表示离群值。
- en: 'Exercise 2.15: Plotting a Bar Chart'
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.15：绘制条形图
- en: 'Let''s visualize the total number of tsunamis created by earthquakes of each
    intensity level using a bar chart. This exercise is a continuation of Exercise
    2.14, Implementing a Pairplot:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用条形图可视化由不同强度级别的地震产生的海啸总数。这个练习是练习 2.14 的延续，内容为：实现一个配对图（Pairplot）：
- en: 'Preprocess the flag_tsunami variable. Before we can use the flag_tsunami variable,
    we need to preprocess it to convert the No values to zeros and the Tsu values
    to ones. This will give us the binary target variable. To do this, we set the
    values in the column using the .loc operator, with : indicating that values need
    to be set for all rows, and the second parameter specifying the name of the column
    for which values are to be set:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '对 flag_tsunami 变量进行预处理。在使用 flag_tsunami 变量之前，我们需要对其进行预处理，将 No 值转换为零，Tsu 值转换为一。这样就可以得到二进制目标变量。为此，我们使用
    .loc 操作符设置列中的值，: 表示对所有行设置值，第二个参数指定要设置值的列名：'
- en: data.loc[:,'flag_tsunami'] = data.flag_tsunami\
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: data.loc[:,'flag_tsunami'] = data.flag_tsunami\
- en: '.apply(lambda t: int(str(t) == ''Tsu''))'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '.apply(lambda t: int(str(t) == ''Tsu''))'
- en: 'Remove all rows having null intensity values from the data we want to plot:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们要绘制的数据中删除所有具有空强度值的行：
- en: subset = data[~pd.isnull(data.intensity)][['intensity',\
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: subset = data[~pd.isnull(data.intensity)][['intensity',\
- en: '''flag_tsunami'']]'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '''flag_tsunami'']]'
- en: 'Find the total number of tsunamis for each intensity level and display the
    DataFrame. To get the data in a format by means of which a bar plot can be visualized,
    we will need to group the rows by each intensity level, and then sum over the
    flag_tsunami values to get the total number of tsunamis for each intensity level:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 查找每个强度级别的海啸总数并显示 DataFrame。为了以条形图可视化这些数据，我们需要按强度级别对行进行分组，然后对 flag_tsunami 值求和，以得到每个强度级别的海啸总数：
- en: data_to_plot = subset.groupby('intensity').sum()
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: data_to_plot = subset.groupby('intensity').sum()
- en: data_to_plot
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: data_to_plot
- en: 'The output will be as follows:'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.38: Total number of tsunamis for each intensity level'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.38：每个强度级别的海啸总数'
- en: '](img/image-TZAVF3XT.jpg)'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-TZAVF3XT.jpg)'
- en: 'Figure 2.38: Total number of tsunamis for each intensity level'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.38：每个强度级别的海啸总数
- en: 'Plot the bar chart, using Matplotlib''s plt.bar(x=..., height=...) method,
    which takes two arguments, one specifying the x values at which bars need to be
    drawn, and the second specifying the height of each bar. Both of these are one-dimensional
    data structures that must have the same length:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Matplotlib 的 plt.bar(x=..., height=...) 方法绘制条形图，该方法需要两个参数，第一个指定绘制条形图的 x 值，第二个指定每个条形的高度。这两个参数都是一维数据结构，且必须具有相同的长度：
- en: plt.figure(figsize=(12,9))
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(12,9))
- en: plt.bar(x=data_to_plot.index, height=data_to_plot.flag_tsunami)
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: plt.bar(x=data_to_plot.index, height=data_to_plot.flag_tsunami)
- en: plt.xlabel('Earthquake intensity')
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('地震强度')
- en: plt.ylabel('No. of tsunamis')
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('海啸数量')
- en: plt.show()
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.39: Bar chart'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.39: 条形图'
- en: '](img/image-R4RR092D.jpg)'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-R4RR092D.jpg)'
- en: 'Figure 2.39: Bar chart'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.39: 条形图'
- en: From this plot, we can see that as the earthquake intensity increases, the number
    of tsunamis caused also increases, but beyond an intensity of 9, the number of
    tsunamis seems to suddenly drop.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们可以看到，随着地震强度的增加，海啸数量也随之增加，但当强度超过 9 时，海啸数量似乎突然下降。
- en: Think about why this could be happening. Perhaps it's just that there are fewer
    earthquakes with an intensity that high, and hence fewer tsunamis. Or it could
    be an entirely independent factor; maybe high-intensity earthquakes have historically
    occurred on land and couldn't trigger a tsunami. Explore the data to find out.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 思考一下为什么会发生这种情况。也许只是因为强度这么高的地震较少，因此海啸也就更少。或者，这可能是一个完全独立的因素；也许高强度地震历史上发生在陆地上，无法触发海啸。请探索数据来找出原因。
- en: Note
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3enFjsZ.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请访问 https://packt.live/3enFjsZ。
- en: You can also run this example online at https://packt.live/2V5apxV. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址：https://packt.live/2V5apxV。你必须执行整个笔记本才能得到预期的结果。
- en: 'Exercise 2.16: Visualizing a Box Plot'
  id: totrans-622
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '练习 2.16: 可视化箱型图'
- en: 'In this exercise, we''ll plot a box plot that represents the variation in eq_primary
    over those countries with at least 100 earthquakes. This exercise is a continuation
    of Exercise 2.15, Plotting a Bar Chart:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将绘制一个箱型图，表示发生至少100次地震的国家中 eq_primary 的变化。此练习是练习 2.15“绘制条形图”的延续：
- en: 'Find countries with over 100 earthquakes. We will find the value counts for
    all the countries in the dataset. Then, we''ll create a series comprising only
    those countries having a count greater than 100:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 查找发生超过100次地震的国家。我们将计算数据集中所有国家的频次计数。然后，我们将创建一个仅包含那些频次大于100的国家的系列：
- en: country_counts = data.country.value_counts()
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: country_counts = data.country.value_counts()
- en: top_countries = country_counts[country_counts > 100]
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: top_countries = country_counts[country_counts > 100]
- en: top_countries
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: top_countries
- en: 'The output will be as follows:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.40: Countries with over 100 earthquakes'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.40: 超过100次地震的国家'
- en: '](img/image-85T9MVBS.jpg)'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-85T9MVBS.jpg)'
- en: 'Figure 2.40: Countries with over 100 earthquakes'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.40: 超过100次地震的国家'
- en: 'Subset the DataFrame to filter in only those rows having countries in the preceding
    set. To filter the rows, we use the .isin() method on the pandas series to select
    those rows containing a value in the array-like object passed as a parameter:'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 子集筛选数据框，只保留那些国家属于前述集合的行。要筛选行，我们使用 .isin() 方法在 pandas 系列上选择那些包含传递为参数的类数组对象中的值的行：
- en: subset = data[data.country.isin(top_countries.index)]
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: subset = data[data.country.isin(top_countries.index)]
- en: 'Create and display the box plot. The primary command for plotting the data
    is sns.boxplot(x=..., y=..., data=..., order=). The x and y parameters are the
    names of the columns in the DataFrame to be plotted on each axis—the former is
    assumed to be the categorical variable and the latter the continuous. The data
    parameter takes the DataFrame from which to take the data and order takes a list
    of category names that indicates the order in which to display the categories
    on the X axis:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并显示箱型图。绘制数据的主要命令是 sns.boxplot(x=..., y=..., data=..., order=)。x 和 y 参数是数据框中要绘制在每个轴上的列名——前者被认为是分类变量，后者是连续变量。data
    参数接受数据框，并且 order 参数接受一个类别名称列表，指示在 X 轴上显示类别的顺序：
- en: plt.figure(figsize=(15, 15))
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(15, 15))
- en: sns.boxplot(x='country', y="eq_primary", data=subset, \
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: sns.boxplot(x='country', y="eq_primary", data=subset, \
- en: order=top_countries.index)
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: order=top_countries.index)
- en: plt.show()
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.41: Box plot'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.41: 箱型图'
- en: '](img/image-Y7EBC9P1.jpg)'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-Y7EBC9P1.jpg)'
- en: 'Figure 2.41: Box plot'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.41: 箱型图'
- en: Note
  id: totrans-643
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2zQHPZw.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请访问 https://packt.live/2zQHPZw。
- en: You can also run this example online at https://packt.live/3hPAzhN. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址：https://packt.live/3hPAzhN。你必须执行整个笔记本才能得到预期的结果。
- en: Relationship Between Two Categorical Variables
  id: totrans-646
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 两个分类变量之间的关系
- en: When we are looking at only a pair of categorical variables to find a relationship
    between them, the most intuitive way to do this is to divide the data on the basis
    of the first category, and then subdivide it further on the basis of the second
    categorical variable and look at the resultant counts to find the distribution
    of data points. While this might seem confusing, a popular way to visualize this
    is to use stacked bar charts. As in a regular bar chart, each bar would represent
    a categorical value. But each bar would again be subdivided into color-coded categories
    that would provide an indication of what fraction of the data points in the primary
    category fall into each subcategory (that is, the second category). The variable
    with a larger number of categories is usually considered the primary category.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们仅查看一对分类变量以寻找它们之间的关系时，最直观的方法是基于第一个类别划分数据，然后根据第二个分类变量进一步细分数据，查看结果计数以了解数据点的分布。虽然这可能看起来有些困惑，但一种常见的可视化方法是使用堆叠条形图。与常规条形图一样，每个条形图表示一个分类值。但每个条形图会再次被细分为颜色编码的类别，这些类别表示主类别中有多少数据点属于每个子类别（即第二个类别）。通常，类别数量较多的变量被认为是主类别。
- en: 'Exercise 2.17: Plotting a Stacked Bar Chart'
  id: totrans-648
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 2.17：绘制堆叠条形图
- en: 'In this exercise, we''ll plot a stacked bar chart that represents the number
    of tsunamis that occurred for each intensity level. This exercise is a continuation
    of Exercise 2.16, Visualizing a Box Plot :'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将绘制一个堆叠条形图，表示每个强度级别发生的海啸数量。这个练习是练习 2.16（可视化箱形图）的延续：
- en: 'Find the number of data points that fall into each grouped value of intensity
    and flag_tsunami:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 查找每个分组值的强度和flag_tsunami的数据点数量：
- en: grouped_data = data.groupby(['intensity', \
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: grouped_data = data.groupby(['intensity', \
- en: '''flag_tsunami'']).size()'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '''flag_tsunami'']).size()'
- en: grouped_data
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: grouped_data
- en: 'The output will be as follows:'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.42: Data points falling into each grouped value of intensity and
    flag_tsunami'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.42：每个分组值的强度和flag_tsunami的数据点'
- en: '](img/image-G6BXFRIT.jpg)'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-G6BXFRIT.jpg)'
- en: 'Figure 2.42: Data points falling into each grouped value of intensity and flag_tsunami'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.42：每个分组值的强度和flag_tsunami的数据点
- en: 'Use the .unstack() method on the resultant DataFrame to get the level-1 index
    (flag_tsunami) as a column:'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`.unstack()`方法在结果 DataFrame 上获取 level-1 索引（flag_tsunami）作为列：
- en: data_to_plot = grouped_data.unstack()
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: data_to_plot = grouped_data.unstack()
- en: data_to_plot
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: data_to_plot
- en: 'The output will be as follows:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.43: The level-1 index'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.43：level-1 索引'
- en: '](img/image-43W4BPAB.jpg)'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-43W4BPAB.jpg)'
- en: 'Figure 2.43: The level-1 index'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.43：level-1 索引
- en: 'Create the stacked bar chart. We first use the sns.set() function to indicate
    that we want to use seaborn as our visualization library. Then, we can easily
    use the native .plot() function in pandas to plot a stacked bar chart by passing
    the kind=''bar'' and stacked=True arguments:'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 创建堆叠条形图。我们首先使用sns.set()函数来指示我们想使用seaborn作为可视化库。然后，我们可以轻松地使用pandas中的.native .plot()函数，通过传递kind='bar'和stacked=True参数来绘制堆叠条形图：
- en: sns.set()
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: sns.set()
- en: data_to_plot.plot(kind='bar', stacked=True, figsize=(12,8))
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: data_to_plot.plot(kind='bar', stacked=True, figsize=(12,8))
- en: plt.show()
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output will be as follows:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.44: A stacked bar chart'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.44：堆叠条形图'
- en: '](img/image-JSR92G61.jpg)'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JSR92G61.jpg)'
- en: 'Figure 2.44: A stacked bar chart'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.44：堆叠条形图
- en: Note
  id: totrans-673
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/37SnqA8.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 https://packt.live/37SnqA8。
- en: You can also run this example online at https://packt.live/3dllvVx. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址为 https://packt.live/3dllvVx。你必须执行整个 Notebook 才能获得所需的结果。
- en: 'The plot now lets us visualize and interpret the fraction of earthquakes that
    caused tsunamis at each intensity level. In Exercise 2.15: Plotting a Bar Chart,
    we saw the number of tsunamis drop for earthquakes having an intensity of greater
    than 9\. From this plot, we can now confirm that this was primarily because the
    number of earthquakes themselves dropped beyond level 10; the fraction of tsunamis
    even increased for level 11.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 该图现在让我们能够可视化和解释每个强度级别导致海啸的地震所占的比例。在练习 2.15：绘制条形图中，我们看到对于强度大于9的地震，海啸的数量减少了。从这张图中，我们现在可以确认，这主要是因为地震本身的数量在超过10级之后减少了；甚至在11级时，海啸的比例有所增加。
- en: 'Activity 2.03: Relationships within the Data'
  id: totrans-677
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 2.03：数据中的关系
- en: 'In this activity, we will revise what we learned in the previous section about
    relationships between data. We will use the same dataset we used in Activity 2.01:
    Summary Statistics and Missing Values, that is, House Prices: Advanced Regression
    Techniques. We''ll use different plots to highlight relationships between values
    in this dataset. This activity is a continuation of Activity 2.01: Summary Statistics
    and Missing Values:'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个活动中，我们将复习上一节关于数据关系的知识。我们将使用在活动 2.01: 总结统计和缺失值中使用的相同数据集，即《房价：高级回归技术》。我们将使用不同的图表来突出数据集中的变量关系。这个活动是活动
    2.01: 总结统计和缺失值的延续：'
- en: 'The steps to be performed are as follows:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Plot the correlation heatmap for the dataset.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制数据集的相关性热图。
- en: 'The output should be similar to the following:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于以下内容：
- en: '![Figure 2.45: Correlation Heatmap for the Housing dataset'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.45：房屋数据集的相关性热图](img/image-RV8XI78X.jpg)'
- en: '](img/image-KZM5Q1AT.jpg)'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-KZM5Q1AT.jpg)'
- en: 'Figure 2.45: Correlation Heatmap for the Housing dataset'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.45：房屋数据集的相关性热图
- en: 'Plot a more compact heatmap having annotations for correlation values using
    the following subset of features:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下特征子集绘制一个更紧凑的热图，并附加相关值的注释：
- en: feature_subset = ['GarageArea','GarageCars','GarageCond', \
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: feature_subset = ['GarageArea','GarageCars','GarageCond', \
- en: '''GarageFinish'',''GarageQual'',''GarageType'', \'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '''GarageFinish'',''GarageQual'',''GarageType'', \'
- en: '''GarageYrBlt'',''GrLivArea'',''LotArea'', \'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '''GarageYrBlt'',''GrLivArea'',''LotArea'', \'
- en: '''MasVnrArea'',''SalePrice'']'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '''MasVnrArea'',''SalePrice'']'
- en: 'The output should be similar to the following:'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于以下内容：
- en: '![Figure 2.46: Correlation heatmap for selected variables of the Housing dataset'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.46：房屋数据集选择变量的相关性热图](img/image-SYRP2X30.jpg)'
- en: '](img/image-R3PJ6CKC.jpg)'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-R3PJ6CKC.jpg)'
- en: 'Figure 2.46: Correlation heatmap for selected variables of the Housing dataset'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.46：房屋数据集选择变量的相关性热图
- en: Display the pairplot for the same subset of features, with the KDE plot on the
    diagonals and the scatter plot elsewhere.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 显示相同特征子集的配对图，直方图在对角线上，散点图在其他位置。
- en: 'The output will be as follows:'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.47: Pairplot for the same subset of features'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.47：相同特征子集的配对图](img/image-R3PJ6CKC.jpg)'
- en: '](img/image-SYRP2X30.jpg)'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-SYRP2X30.jpg)'
- en: 'Figure 2.47: Pairplot for the same subset of features'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.47：相同特征子集的配对图
- en: 'Create a boxplot to show the variation in SalePrice for each category of GarageCars:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个箱型图，以显示不同GarageCars类别中SalePrice的变化：
- en: 'The output will be as follows:'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.48: Boxplot showing variation in SalePrice for each category of
    GarageCars'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.48：展示不同GarageCars类别中SalePrice变化的箱型图](img/image-KZM5Q1AT.jpg)'
- en: '](img/image-RV8XI78X.jpg)'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-RV8XI78X.jpg)'
- en: 'Figure 2.48: Boxplot showing variation in SalePrice for each category of GarageCars'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.48：展示不同GarageCars类别中SalePrice变化的箱型图
- en: 'Plot a line graph using seaborn to show the variation in SalePrice for older
    and more recently built homes:'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 使用seaborn绘制折线图，展示较旧和较新建的房屋的SalePrice变化：
- en: 'The output will be as follows:'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.49: Line graph showing the variation in SalePrice'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.49：展示SalePrice变化的折线图](img/image-0972CWFE.jpg)'
- en: for older to more recently built homes
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 针对较旧和较新建的房屋
- en: '](img/image-0972CWFE.jpg)'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-0972CWFE.jpg)'
- en: 'Figure 2.49: Line graph showing the variation in SalePrice for older to more
    recently built homes'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.49：展示较旧和较新建的房屋SalePrice变化的折线图
- en: Note
  id: totrans-710
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以通过此链接找到。
- en: You have learned how to use more advanced methods from the seaborn package to
    visualize large numbers of variables at once, using charts such as the correlation
    heatmap, pairplot, and boxplots. With boxplots, you learned how to visualize the
    range of one variable segmented across another, categorical variable. The boxplot
    further directly visualizes the quantiles and outliers, making it a powerful tool
    in your EDA toolkit. You have also created some preliminary line and scatter plots
    that are helpful in visualizing continuous data that trends over time or some
    other variable.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何使用seaborn包中的更高级方法来可视化大量变量，使用相关性热图、配对图和箱型图等图表。通过箱型图，你学会了如何可视化一个变量在另一个分类变量中的范围。箱型图还直接可视化了四分位数和异常值，使其成为你EDA工具包中的强大工具。你还创建了一些初步的折线图和散点图，这对于可视化随时间或其他变量变化的连续数据非常有帮助。
- en: Summary
  id: totrans-713
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by talking about why data exploration is an important
    part of the modeling process and how it can help in not only preprocessing the
    dataset for the modeling process but also help us engineer informative features
    and improve model accuracy. This chapter focused on not only gaining a basic overview
    of the dataset and its features but also gaining insights by creating visualizations
    that combine several features. We looked at how to find the summary statistics
    of a dataset using core functionality from pandas. We looked at how to find missing
    values and talked about why they're important while learning how to use the Missingno
    library to analyze them and the pandas and scikit-learn libraries to impute the
    missing values. Then, we looked at how to study the univariate distributions of
    variables in the dataset and visualize them for both categorical and continuous
    variables using bar charts, pie charts, and histograms. Lastly, we learned how
    to explore relationships between variables, and about how they can be represented
    using scatter plots, heatmaps, box plots, and stacked bar charts, to name but
    a few.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先讨论了为什么数据探索是建模过程中的重要部分，以及它如何帮助我们不仅为建模过程进行数据预处理，还能帮助我们工程化有意义的特征并提高模型的准确性。本章不仅侧重于对数据集及其特征的基本概述，还通过创建结合多个特征的可视化图表来获得深入见解。我们介绍了如何使用pandas的核心功能找到数据集的摘要统计信息。我们讨论了如何查找缺失值，并讲解了它们的重要性，同时学习了如何使用Missingno库来分析缺失值，以及如何使用pandas和scikit-learn库来填补缺失值。接着，我们学习了如何研究数据集中变量的单变量分布，并使用条形图、饼图和直方图等方法将其可视化，适用于分类变量和连续变量。最后，我们学习了如何探索变量之间的关系，并了解了如何使用散点图、热力图、箱线图和堆叠条形图等方式来展示它们。
- en: In the following chapters, we will start exploring supervised machine learning
    algorithms. Now that we have an idea of how to explore a dataset that we have,
    we can proceed to the modeling phase. The next chapter will introduce regression,
    a class of algorithms that are primarily used to build models for continuous target variables.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将开始探索有监督的机器学习算法。现在，我们已经对如何探索现有数据集有了一定的了解，可以进入建模阶段。下一章将介绍回归，这是一类主要用于构建连续目标变量模型的算法。
