- en: 'Chapter 10: Advanced Training Techniques'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章：高级训练技巧
- en: In the previous chapter, you learned when and how to scale training jobs using
    features such as **Pipe mode** and **distributed training**, as well as alternatives
    to **S3** for dataset storage.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何使用**Pipe模式**和**分布式训练**等特性来扩展训练任务，以及使用替代的**S3**存储数据集。
- en: In this chapter, we'll conclude our exploration of training techniques. In the
    first part of the chapter, you'll learn how to slash down your training costs
    with **managed spot training**, how to squeeze every drop of accuracy from your
    models with **automatic model tuning**, and how to crack models open with **SageMaker
    Debugger**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将结束对训练技巧的探索。在本章的第一部分，你将学习如何通过**托管点训练**大幅降低训练成本，如何通过**自动模型调优**从模型中挤出每一滴精度，并且如何使用**SageMaker
    Debugger**拆解模型。
- en: 'In the second part of the chapter, we''ll introduce two new SageMaker capabilities
    that help you build more efficient workflows and higher quality models: **SageMaker
    Feature Store** and **SageMaker Clarify**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将介绍两个新的SageMaker功能，帮助你构建更高效的工作流和更高质量的模型：**SageMaker Feature Store**和**SageMaker
    Clarify**。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Optimizing training costs with managed spot training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用托管点训练优化训练成本
- en: Optimizing hyperparameters with automatic model tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动模型调优优化超参数
- en: Exploring models with SageMaker Debugger
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Debugger探索模型
- en: Managing features and building datasets with SageMaker Feature Store
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Feature Store管理特征和构建数据集
- en: Detecting bias and explaining predictions with SageMaker Clarify
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Clarify检测偏差并解释预测
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS free tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个AWS账户来运行本章中包含的示例。如果你还没有账户，请访问[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)来创建一个。你还应该了解AWS免费层（[https://aws.amazon.com/free/](https://aws.amazon.com/free/)），它允许你在一定的使用限制内免费使用许多AWS服务。
- en: You will need to install and configure the AWS **Command-Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置AWS **命令行界面**（**CLI**）（[https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)）。
- en: You will need a working `pandas`, `numpy`, and more).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要一个可用的`pandas`、`numpy`等库。
- en: Code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中包含的代码示例可在GitHub上获取，链接为[https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装Git客户端来访问它们（[https://git-scm.com/](https://git-scm.com/)）。
- en: Optimizing training costs with managed spot training
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用托管点训练优化训练成本
- en: In the previous chapter, we trained the **image classification** algorithm on
    the **ImageNet** dataset. The job ran for a little less than 4 hours. At about
    $290 per hour, this job cost us roughly $1,160\. That's a lot of money… but is
    it really?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们在**ImageNet**数据集上训练了**图像分类**算法。这个任务运行了不到4小时。按照每小时$290计算，这个任务大约花费了我们$1,160。那是一大笔钱……但真的那么贵吗？
- en: Comparing costs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较成本
- en: 'Before you throw your arms up the air yelling "*What is he thinking?*", please
    consider how much it would cost your organization to own and run this training
    cluster:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在你抛开双手，喊出“*他在想什么？*”之前，请考虑一下让你的组织拥有并运行这个训练集群需要花费多少成本：
- en: A back-of-the-envelope calculation for capital expenditure (servers, storage,
    GPUs, 100 Gbit/s networking equipment) says at least $1.5M. As far as operational
    expenditure is concerned, hosting costs won't be cheap, as each equivalent server
    will require 4-5 kW of power. That's enough to fill one rack at your typical hosting
    company, so even if high-density racks are available, you'll need several. Add
    bandwidth, cross connects, and so on, and my gut feeling says it would cost about
    $15K per month (much more in certain parts of the world).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个粗略的资本支出计算（包括服务器、存储、GPU、100 Gbit/s的网络设备）至少需要150万美元。就运营支出而言，托管成本不会便宜，因为每台等效的服务器需要4-5千瓦的电力。这足够填满你典型托管公司的一排机架，因此即使使用高密度机架，你也需要多个。再加上带宽、跨连接等，我的直觉告诉我每月大约需要1.5万美元（在某些地区可能更多）。
- en: We would need to add hardware support contracts (say, 10% per year, so $150K).
    Depreciating this cluster over 5 years, total monthly costs would be ($1.5M +
    60*$15K + 5*$150K)/60 = $52.5K. Let's round it to $55K to account for labor costs
    for server maintenance and so on.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要添加硬件支持合同（比如，每年10%，即15万美元）。将这个集群的折旧期设为5年，总月成本为($1.5M + 60*$15K + 5*$150K)/60
    = 52.5K美元。为了计算维护服务器等的人工成本，我们将其四舍五入到55K美元。
- en: Using conservative estimates, this spend is equivalent to 190 hours of training
    with the large $290-an-hour cluster we've used for our ImageNet example. As we
    will see later in this chapter, managed spot training routinely delivers savings
    of 70%. So, now the spend would be equivalent to about 633 hours of ImageNet training
    per month.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用保守估计，这笔开支相当于使用我们为ImageNet示例所用的大型每小时$290集群进行190小时的训练。正如我们在本章稍后将看到的，托管的Spot训练通常能提供70%的节省。因此，现在这笔开支相当于每月大约633小时的ImageNet训练。
- en: This amounts to 87% usage (633/720) month in, month out, and it's very unlikely
    you'd keep your training cluster that busy. Add downtime, accelerated depreciation
    caused by hardware innovation, hardware insurance costs, the opportunity cost
    of not investing $1.5M in other ventures, and so on, and the business case for
    physical infrastructure gets worse by the minute.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每月87%的使用率（633/720），而且很不可能你会让你的训练集群保持如此繁忙。再加上停机时间、硬件创新带来的加速折旧、硬件保险费用、未将150万美元投资于其他项目的机会成本等等，物理基础设施的商业案例每分钟都在变得更差。
- en: Financials matter, but the worst thing is that you'd only have one cluster.
    What if a potential business opportunity required another one? Would you spend
    another $1.5M? If not, would you have to time-share the existing cluster? Of course,
    only you could decide what's best for your organization. Just make sure that you
    look at the big picture.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 财务问题固然重要，但最糟糕的是你只能拥有一个集群。如果一个潜在的商业机会需要另一个集群怎么办？你会再花150万美元吗？如果不会，是否需要共享现有的集群？当然，只有你能决定什么对你的组织最有利。只要确保你从全局角度来看问题。
- en: Now, let's see how you can easily enjoy that 70% cost reduction.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何轻松享受70%的成本降低。
- en: Understanding Amazon EC2 Spot Instances
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解Amazon EC2 Spot实例
- en: At any given time, **Amazon** **EC2** has more capacity than needed. This allows
    customers to add on-demand capacity to their platforms whenever they need to.
    On-demand instances may be created explicitly using an API call, or automatically
    if **Auto Scaling** is configured. Once a customer has acquired an on-demand instance,
    they will keep it until they decide to release it, either explicitly or automatically.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时候，**Amazon** **EC2**的容量都超过了实际需求。这使得客户可以根据需要随时向他们的平台添加按需容量。可以通过API调用显式创建按需实例，或者在配置了**Auto
    Scaling**的情况下自动创建。一旦客户获得了按需实例，他们将保留它，直到他们决定释放它，无论是显式释放还是自动释放。
- en: '**Spot Instances** are a simple way to tap into this unused capacity and to
    enjoy very significant discounts (50-70% are typical). You can request them in
    the same way, and they behave the same too. The only difference is that should
    AWS need the capacity to build on-demand instances, your Spot Instance may be
    reclaimed. It will receive an interruption notification two minutes before being
    forcefully terminated.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spot实例**是利用这些未使用容量并享受非常显著折扣（通常为50-70%）的简单方法。您可以以相同的方式请求它们，它们的行为也相同。唯一的区别是，如果AWS需要容量来构建按需实例，您的Spot实例可能会被回收。在被强制终止之前，它会收到两分钟的中断通知。'
- en: This isn't as bad as it sounds. Depending on regions and instance families,
    Spot Instances may not be reclaimed very often, and customers routinely keep them
    for days, if not more. In addition, you can architecture your application for
    this requirement, for example, by running stateless workloads on Spot Instances
    and relying on managed services for data storage. The cost benefit is too good
    to pass!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不像听起来那么糟糕。根据区域和实例系列的不同，抢占式实例可能不会被频繁回收，客户通常可以将它们保留几天甚至更长时间。此外，您可以为此需求架构您的应用程序，例如，在抢占式实例上运行无状态工作负载，并依赖托管服务进行数据存储。成本优势非常明显，不容错过！
- en: 'Going to the `p3dn.24xlarge` for the last three months, where the spot price
    has been 60-70% cheaper than the on-demand price:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 查看过去三个月`p3dn.24xlarge`的情况，抢占式价格比按需价格便宜60-70%：
- en: '![Figure 10.1 – Viewing the spot price of p3dn.24xlarge'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.1 – 查看 p3dn.24xlarge 的抢占式价格'
- en: '](img/B17705_10_1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_1.jpg)'
- en: Figure 10.1 – Viewing the spot price of p3dn.24xlarge
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 查看 p3dn.24xlarge 的抢占式价格
- en: 'These are EC2 prices, but the same discount rates apply to SageMaker prices.
    Discounts vary across instance types, regions, and even availability zones. You
    can use the `describe-spot-price-history` API to collect this information programmatically
    and use it in your workflows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是EC2的价格，但相同的折扣率也适用于SageMaker价格。折扣因实例类型、区域，甚至可用区而异。您可以使用`describe-spot-price-history`
    API以编程方式收集这些信息，并将其用于工作流中：
- en: https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-price-history.html
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-spot-price-history.html
- en: Now, let's see what this means for SageMaker.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这对SageMaker意味着什么。
- en: Understanding managed spot training
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解托管抢占式训练
- en: 'Training with Spot Instances is available in all SageMaker configurations:
    single-instance training, distributed training, built-in algorithms, frameworks,
    and your own algorithms.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用抢占式实例进行训练在所有SageMaker配置中都可用：单实例训练、分布式训练、内置算法、框架以及您自己的算法。
- en: Setting a couple of estimator parameters is all it takes. You don't need to
    worry about handling notifications and interruptions. SageMaker automatically
    does it for you.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 只需设置几个估算器参数即可。您无需担心处理通知和中断，SageMaker会自动为您处理。
- en: If a training job is interrupted, SageMaker regains adequate spot capacity and
    relaunches the training job. If the algorithm uses checkpointing, training resumes
    from the latest checkpoint. If not, the job restarts from the beginning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练作业被中断，SageMaker会恢复足够的抢占式容量并重新启动训练作业。如果算法使用了检查点，训练将从最新的检查点继续。如果没有，作业将从头开始。
- en: 'How much work is required to implement checkpointing depends on the algorithm
    you''re using:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实现检查点所需的工作量取决于您使用的算法：
- en: The three built-in algorithms for computer vision and **XGBoost** support checkpointing.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个用于计算机视觉和**XGBoost**的内置算法支持检查点功能。
- en: All other built-in algorithms don't. You can still train them with Spot Instances.
    However, the maximum running time is limited to 60 minutes to minimize potential
    waste. If your training job takes longer than 60 minutes, you should try scaling
    it. If that's not enough, you'll have to use on-demand instances.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有其他内置算法则没有此功能。您仍然可以使用抢占式实例进行训练。然而，最大运行时间限制为60分钟，以减少潜在的浪费。如果您的训练作业超过60分钟，您应该考虑进行扩展。如果仍然不够，您将不得不使用按需实例。
- en: The **deep learning containers** for **TensorFlow**, **PyTorch**, **Apache**
    **MXNet**, and **Hugging Face** come with built-in checkpointing, and you don't
    need to modify your training script.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习容器**适用于**TensorFlow**、**PyTorch**、**Apache** **MXNet**和**Hugging Face**，并内置了检查点功能，您无需修改训练脚本。'
- en: If you use other frameworks or your own custom code, you need to implement checkpointing.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用其他框架或自己的自定义代码，您需要实现检查点功能。
- en: During training, checkpoints are saved inside the training container. The default
    path is `/opt/ml/checkpoints`, and you can customize it with an estimator parameter.
    SageMaker also automatically persists these checkpoints to a user-defined S3 path.
    If your training job is interrupted and relaunched, checkpoints are automatically
    copied inside the container. Your code can check for their presence and load the
    appropriate one to resume training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，检查点会保存在训练容器内。默认路径为`/opt/ml/checkpoints`，您可以通过估算器参数进行自定义。SageMaker还会自动将这些检查点持久化到用户定义的S3路径。如果您的训练作业被中断并重新启动，检查点会自动复制到容器内。您的代码可以检查它们的存在，并加载适当的检查点来继续训练。
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that checkpointing is available even when you train with on-demand
    instances. This may come in handy if you'd like to store checkpoints in S3 for
    further inspection or for incremental training. The only restriction is that checkpointing
    is not available with **Local mode**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使使用按需实例进行训练，检查点功能仍然可用。如果你希望将检查点存储在S3中以便进一步检查或进行增量训练，这将非常有用。唯一的限制是**本地模式**不支持检查点功能。
- en: Last but not least, checkpointing does slow down jobs, especially for large
    models. However, this is a small price to pay to avoid restarting long-running
    jobs from scratch.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，检查点功能确实会拖慢任务的速度，尤其是对于大模型来说。然而，这是一个值得支付的小代价，以避免从头开始重新启动长时间运行的任务。
- en: Now, let's add managed spot training to the **object detection** job we ran
    in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training Computer
    Vision Models*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将托管点训练添加到我们在[*第5章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)中运行的**目标检测**任务中，*训练计算机视觉模型*。
- en: Using managed spot training with object detection
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用托管点训练进行目标检测
- en: Switching from on-demand training to managed spot training is very simple. We
    just have to set the maximum duration of the training job, including any time
    spent waiting for Spot Instances to be available.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从按需训练切换到托管点训练非常简单。我们只需设置训练任务的最大持续时间，包括等待Spot实例可用的时间。
- en: 'We set a maximum running time of 2 hours, plus 8 hours for any spot delay.
    If either one of these bounds is exceeded, the job will be terminated automatically.
    This is helpful in killing runaway jobs that last much longer than expected or
    jobs that are stuck waiting for spot instances:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了2小时的最大运行时间，加上8小时的任何点延迟。如果超出了这两个限制，任务将自动终止。这对于终止运行时间过长或因为等待点实例而卡住的任务非常有帮助：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We train with the same configuration as before: Pipe mode and `dist_sync` mode.
    As the first epoch completes, the training log tells us that checkpointing is
    active. A new checkpoint is saved automatically each time the validation metric
    improves:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与之前相同的配置进行训练：管道模式和`dist_sync`模式。当第一个epoch完成时，训练日志告诉我们检查点功能已经激活。每次验证指标改进时，都会自动保存一个新的检查点：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the training job is complete, the training log tells us how much we saved:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练任务完成，训练日志会告诉我们节省了多少：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Not only is this job 70% cheaper than its on-demand counterpart, but it's also
    less than half the price of our original single-instance job. This means that
    we could use more instances and accelerate our training job for the same budget.
    Indeed, managed spot training lets you optimize the duration of a job and its
    cost. Instead of complex capacity planning, you can set a training budget that
    fits your business requirements, and then grab as much infrastructure as possible.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务不仅比按需版本便宜70%，而且价格还不到我们原来单实例任务的一半。这意味着我们可以使用更多的实例，并以相同的预算加速训练任务。实际上，托管点训练让你能够优化任务的持续时间和成本。你可以根据业务需求设定训练预算，然后尽可能地获取基础设施。
- en: Let's try another example where we implement checkpointing in **Keras**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个例子，在**Keras**中实现检查点功能。
- en: Using managed spot training and checkpointing with Keras
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用托管点训练和Keras中的检查点功能
- en: In this example, we'll build a simple `Sequential` API in TensorFlow 2.1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将在TensorFlow 2.1中构建一个简单的`Sequential` API。
- en: Checkpointing with Keras
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras中的检查点功能
- en: 'Let''s first look at the Keras script itself. For the sake of brevity, only
    important steps are presented here. You can find the full code in the GitHub repository
    for this book:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来看一下Keras脚本本身。为了简洁起见，这里只展示了重要的步骤。你可以在本书的GitHub仓库中找到完整的代码：
- en: Using Script mode, we store dataset paths and hyperparameters.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用脚本模式，我们存储数据集路径和超参数。
- en: Then, we load the dataset and normalize pixel values to the [0,1] range. We
    also one-hot encode class labels.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载数据集并将像素值归一化到[0,1]范围内。我们还对类别标签进行独热编码。
- en: 'We build a `Sequential` model: two convolution blocks (`Conv2D` / `BatchNormalization`
    / `ReLU` / `MaxPooling2D` / `Dropout`), then two fully connected blocks (`Dense`
    / `BatchNormalization` / `ReLU` / `Dropout`), and finally, a `softmax` output
    layer for the 10 classes in the dataset.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建了一个`Sequential`模型：两个卷积块（`Conv2D` / `BatchNormalization` / `ReLU` / `MaxPooling2D`
    / `Dropout`），然后是两个全连接块（`Dense` / `BatchNormalization` / `ReLU` / `Dropout`），最后是一个用于数据集中10个类别的`softmax`输出层。
- en: 'We compile the model using the **categorical cross-entropy** loss function
    and the **Adam** optimizer:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用**分类交叉熵**损失函数和**Adam**优化器来编译模型：
- en: '[PRE3]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We define a Keras callback to checkpoint the model each time validation accuracy
    improves:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个 Keras 回调，每当验证准确率提高时就保存一个检查点：
- en: '[PRE4]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We train the model, adding the callback we just created:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练模型，添加我们刚刚创建的回调：
- en: '[PRE5]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When training is complete, we save the model in the **TensorFlow Serving**
    format, which is required to deploy on SageMaker:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，我们将模型保存为 **TensorFlow Serving** 格式，这是在 SageMaker 上部署时所需的格式：
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, let's look at our training notebook.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的训练笔记本。
- en: Training with managed spot training and checkpointing
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用托管的 spot 训练和检查点保存进行训练
- en: 'We use the same workflow as before:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前相同的工作流程：
- en: We download the Fashion-MNIST dataset and save it to a local directory. We upload
    the dataset to S3, and we define the S3 location where SageMaker should copy the
    checkpoints.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载 Fashion-MNIST 数据集并将其保存在本地目录。我们将数据集上传到 S3，并定义 SageMaker 应该将检查点复制到的 S3 位置。
- en: 'We configure a `TensorFlow` estimator, enabling managed spot training and passing
    the S3 output location for checkpoints. This time, we use an `ml.g4dn.xlarge`
    instance. This very cost-effective GPU instance ($0.822 in `eu-west-1`) is more
    than enough for a small model:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置一个 `TensorFlow` 估算器，启用托管的 spot 训练，并传递检查点的 S3 输出位置。这次，我们使用的是 `ml.g4dn.xlarge`
    实例。这种非常具成本效益的 GPU 实例（在 `eu-west-1` 区域的价格为 $0.822）足以应对一个小模型：
- en: '[PRE7]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We launch training as usual, and the job hits 93.11% accuracy. Training lasts
    289 seconds, and we're only billed for 87 seconds, thanks to a 69.9% discount.
    The total cost is 1.98 cents! Who said GPU training had to be costly?
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像往常一样启动训练，任务达到了93.11%的准确率。训练持续了289秒，我们只需为87秒支付费用， thanks to a 69.9%的折扣。总费用为1.98美分！谁说GPU训练必须昂贵？
- en: 'In the training log, we see that a checkpoint is created every time validation
    accuracy improves:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练日志中，我们看到每当验证准确率提高时，都会创建一个检查点：
- en: '[PRE8]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'While the job is running, we also see that checkpoints are copied to S3:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在任务运行时，我们还看到检查点被复制到 S3：
- en: '[PRE9]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If our spot job gets interrupted, SageMaker will copy checkpoints inside the
    container so that we can use them to resume training. This requires some logic
    in our Keras script to load the latest checkpoint. Let's see how to do this.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的 spot 任务被中断，SageMaker 会在容器内复制检查点，以便我们可以使用它们来恢复训练。这需要在我们的 Keras 脚本中添加一些逻辑，以加载最新的检查点。让我们看看如何实现。
- en: Resuming training from a checkpoint
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从检查点恢复训练
- en: 'This is a pretty simple process—look for checkpoints, and resume training from
    the latest one:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的过程——查找检查点，并从最新的检查点继续训练：
- en: 'We list the checkpoint directory:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们列出检查点目录：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If checkpoints are present, we find the most recent and its epoch number. Then,
    we load the model:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有检查点，我们会找到最新的一个以及它的 epoch 编号。然后，我们加载模型：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If no checkpoint is present, we build the model as usual:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有检查点，我们像往常一样构建模型：
- en: '[PRE12]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We compile the model, and we launch training, passing the number of the last
    epoch:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编译模型，启动训练，并传递最后一个 epoch 的编号：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How can we test this? There is no way to intentionally cause a spot interruption.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们怎么测试这个呢？没有办法故意造成一个 spot 中断。
- en: 'Here''s the trick: start a new training job with existing checkpoints in the
    `checkpoint_s3_uri` path, and increase the number of epochs. This will simulate
    resuming an interrupted job.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 诀窍是：用 `checkpoint_s3_uri` 路径中的现有检查点启动一个新的训练任务，并增加 epoch 的数量。这将模拟恢复一个中断的任务。
- en: Setting the number of epochs to 25 and keeping the checkpoints in `s3://sagemaker-eu-west-1-123456789012/keras2`
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将 epoch 数设置为 25，并将检查点保存在 `s3://sagemaker-eu-west-1-123456789012/keras2`
- en: '`fashion-mnist/checkpoints`, we launch the training job again.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`fashion-mnist/checkpoints`，我们再次启动训练任务。'
- en: 'In the training log, we see that the latest checkpoint is loaded and that training
    resumes at epoch 21:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练日志中，我们看到最新的检查点被加载，训练从第 21 个 epoch 继续：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also see that new checkpoints are created as validation accuracy improves,
    and they''re copied to S3:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到每当验证准确率提高时，新的检查点会被创建，并被复制到 S3：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, it's not difficult to set up checkpointing in SageMaker, and
    you should be able to do the same for other frameworks. Thanks to this, you can
    enjoy the deep discount provided by managed spot training without the risk of
    losing any work if an interruption occurs. Of course, you can use checkpointing
    on its own to inspect intermediate training results, or for incremental training.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在 SageMaker 中设置检查点并不困难，你应该也能在其他框架中做到这一点。得益于此，你可以享受由托管的按需训练提供的深度折扣，而在中断发生时也无需担心丢失任何工作。当然，你也可以单独使用检查点来检查中间训练结果，或用于增量训练。
- en: 'In the next section, we''re going to introduce another important feature: automatic
    model tuning.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍另一个重要特性：自动模型调优。
- en: Optimizing hyperparameters with automatic model tuning
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自动模型调优优化超参数
- en: Hyperparameters have a huge influence on the training outcome. Just like in
    **chaos theory**, tiny variations of a single hyperparameter can cause wild swings
    in accuracy. In most cases, the "why?" evades us, leaving us perplexed about what
    to try next.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数对训练结果有巨大的影响。就像**混沌理论**中所说的那样，一个单一超参数的微小变化就可能导致准确率的剧烈波动。在大多数情况下，我们无法解释“为什么？”，这让我们对接下来该尝试什么感到困惑。
- en: 'Over the years, several techniques have been devised to try to solve the problem
    of selecting optimal hyperparameters:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，已经设计了几种技术来尝试解决选择最佳超参数的问题：
- en: '**Manual search**: This means using our best judgment and experience to select
    the "best" hyperparameters. Let''s face it: this doesn''t really work, especially
    with deep learning and its horde of training and network architecture parameters.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**手动搜索**：这意味着依靠我们的最佳判断和经验来选择“最佳”超参数。说实话：这真的不起作用，尤其是在深度学习和众多训练及网络架构参数的情况下。'
- en: '**Grid search**: This entails systematically exploring the hyperparameter space,
    zooming in on hot spots, and repeating the process. This is much better than a
    manual search. However, this usually requires training hundreds of jobs. Even
    with scalable infrastructure, the time and dollar budgets can be significant.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**网格搜索**：这意味着系统地探索超参数空间，集中在热点区域，然后重复这一过程。这比手动搜索要好得多。然而，这通常需要训练成百上千的任务。即使有可扩展的基础设施，时间和预算仍然可能是巨大的。'
- en: '**Random search**: This refers to selecting hyperparameters at random. Unintuitive
    as it sounds, James Bergstra and Yoshua Bengio (of Turing Award fame) proved in
    2012 that this technique delivers better models than a grid search with the same
    compute budget'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机搜索**：指的是随机选择超参数。虽然听起来不合常理，但詹姆斯·伯格斯特拉和约书亚·本吉奥（图灵奖得主）在2012年证明，这一技术在相同计算预算下能比网格搜索交付更好的模型。'
- en: '[http://www.jmlr.org/papers/v13/bergstra12a.html](http://www.jmlr.org/papers/v13/bergstra12a.html)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.jmlr.org/papers/v13/bergstra12a.html](http://www.jmlr.org/papers/v13/bergstra12a.html)'
- en: '**Hyperparameter optimization** (HPO): This means using optimization techniques
    to select hyperparameters, such as **Bayesian optimization** and **Gaussian process
    regression**. With the same compute budget, HPO typically delivers results with
    10x fewer training epochs than other techniques.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**超参数优化**（HPO）：这意味着使用优化技术来选择超参数，例如**贝叶斯优化**和**高斯过程回归**。在相同的计算预算下，HPO通常能以比其他技术少10倍的训练周期交付结果。'
- en: Understanding automatic model tuning
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解自动模型调优
- en: SageMaker includes an **automatic model tuning** capability that lets you easily
    explore hyperparameter ranges and quickly optimize any training metric with a
    limited number of jobs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 包含一个**自动模型调优**功能，可以让你轻松探索超参数范围，并通过有限的任务数快速优化任何训练指标。
- en: 'Model tuning supports both random search and HPO. The former is an interesting
    baseline that helps you to check whether the latter is indeed overperforming.
    You can find a very detailed comparison in this excellent blog post:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 模型调优支持随机搜索和超参数优化（HPO）。前者是一个有趣的基准，帮助你检查后者是否确实表现更好。你可以在这篇精彩的博客文章中找到非常详细的比较：
- en: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-random-search-and-hyperparameter-scaling/
- en: Model tuning is completely agnostic to the algorithm you're using. It works
    with built-in algorithms, and the documentation lists the hyperparameters that
    can be tuned. It also works with all frameworks and custom containers, and hyperparameters
    are passed in the same way.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型调优对你使用的算法完全无关。它适用于内置算法，文档中列出了可以调优的超参数。它也适用于所有框架和自定义容器，且超参数传递方式相同。
- en: 'For each hyperparameter that we want to optimize, we have to define the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要优化的每一个超参数，我们需要定义以下内容：
- en: A name
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名称
- en: A type (parameters can either be an integer, continuous, or categorical)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种类型（参数可以是整数、连续的或分类的）
- en: A range of values to explore
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索的值范围
- en: A scaling type (linear, logarithmic, or reverse logarithmic, or auto)—this lets
    us control how a specific parameter range will be explored
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种缩放类型（线性、对数、反对数或自动）——这让我们控制特定参数范围的探索方式
- en: We also define the metric we want to optimize for. It can be any numerical value
    as long as it's visible in the training log and you can pass a regular expression
    to extract it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了要优化的指标。它可以是任何数值，只要它在训练日志中可见，并且您可以传递正则表达式来提取它。
- en: Then, we launch the tuning jobs, passing all of these parameters as well as
    the number of training jobs to run and their degree of parallelism. With Bayesian
    optimization, you'll get the best results with sequential jobs (no parallelism),
    as optimization can be applied after each job. Having said that, running a small
    number of jobs in parallel is acceptable. Random search has no restrictions on
    parallelism as jobs are completely unrelated.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们启动调优作业，传递所有这些参数以及要运行的训练作业数量和并行度。使用贝叶斯优化，您可以通过顺序作业（无并行）获得最佳结果，因为优化可以在每个作业后应用。话虽如此，运行少量并行作业是可以接受的。随机搜索对并行性没有限制，因为作业之间完全不相关。
- en: Calling the `deploy()` API on the tuner object deploys the best model. If tuning
    is is still in progress, it will deploy the best model so far, which can be useful
    for early testing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`deploy()` API 在调优器对象上部署最佳模型。如果调优仍在进行中，它将部署迄今为止的最佳模型，这对于早期测试非常有用。
- en: Let's run the first example with a built-in algorithm and learn about the model
    tuning API.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用内置算法运行第一个示例，并了解模型调优 API。
- en: Using automatic model tuning with object detection
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动模型调优进行目标检测
- en: 'We''re going to optimize our object detection job. Looking at the documentation,
    we can see the list of tunable hyperparameters:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将优化我们的目标检测作业。查看文档，我们可以看到可调超参数的列表：
- en: https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tuning.html
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tuning.html
- en: 'Let''s try to optimize the learning rate, momentum, and weight decay:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试优化学习率、动量和权重衰减：
- en: We set up the input channels using Pipe mode. There's no change here.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用管道模式设置输入通道。这里没有变化。
- en: 'We also configure the estimator as usual, setting up managed spot training
    to minimize costs. We''ll train on a single instance for maximum accuracy:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还像往常一样配置估算器，设置托管现货训练以最小化成本。我们将在单个实例上训练以获得最高准确度：
- en: '[PRE16]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We use the same hyperparameters as before:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用与之前相同的超参数：
- en: '[PRE17]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We define the three extra hyperparameters we want to tune. We explicitly set
    logarithmic scaling for the learning rate, to make sure that different orders
    of magnitude are explored:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了我们希望调优的三个额外超参数。我们明确设置学习率的对数缩放，以确保探索不同数量级：
- en: '[PRE18]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We set the metric to optimize for:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置要优化的指标：
- en: '[PRE19]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We put everything together, using the `HyperparameterTuner` object. We decide
    to run 30 jobs, with two jobs in parallel. We also enable early stopping to weed
    out low performing jobs, saving us time and money:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有内容整合在一起，使用`HyperparameterTuner`对象。我们决定运行30个作业，其中两个作业并行运行。我们还启用了早停，以淘汰表现较差的作业，从而节省时间和金钱：
- en: '[PRE20]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We launch training on the tuner object (not on the estimator) without waiting
    for it to complete:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在调优器对象（而不是估算器）上启动训练，而不等待它完成：
- en: '[PRE21]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At the moment, **SageMaker Studio** doesn''t provide a convenient view of tuning
    jobs. Instead, we can track progress in the **Hyperparameter tuning jobs** section
    of the SageMaker console, as shown in the following screenshot:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，**SageMaker Studio** 没有提供方便的调优作业查看界面。相反，我们可以在 SageMaker 控制台的 **超参数调优作业**
    部分跟踪进度，如下图所示：
- en: '![Figure 10.2 – Viewing tuning jobs in the SageMaker console'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2 – 在 SageMaker 控制台中查看调优作业](img/B17705_10_2.jpg)'
- en: '](img/B17705_10_2.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_2.jpg)'
- en: Figure 10.2 – Viewing tuning jobs in the SageMaker console
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 在 SageMaker 控制台中查看调优作业
- en: The job runs for 17 hours (wall time). 22 jobs completed and 8 stopped early.
    The total training time is 30 hours and 15 minutes. Applying the 70% spot discount,
    the total cost is 25.25 * $4.131 * 0.3 = $37.48.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 作业运行了17小时（壁钟时间）。22个作业完成，8个作业提前停止。总训练时间为30小时15分钟。应用70%的现货折扣，总成本为 25.25 * $4.131
    * 0.3 = $37.48。
- en: 'How well did this tuning job do? With default hyperparameters, our standalone
    training job reached a `0.2453`. Our tuning job hits `0.6337`, as shown in the
    following screenshot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个调优作业的表现如何？使用默认超参数，我们的独立训练作业达到了`0.2453`。我们的调优作业达到了`0.6337`，如下图所示：
- en: '![Figure 10.3 – Tuning job results'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.3 – 调优作业结果](img/B17705_10_3.jpg)'
- en: '](img/B17705_10_3.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_3.jpg)'
- en: Figure 10.3 – Tuning job results
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 调优作业结果
- en: 'The graph for validation mAP is shown in the next image. It tells me that we
    could probably train a little longer and get extra accuracy:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 验证 mAP 的图表显示在下一张图片中。它告诉我我们可能需要再训练一段时间，以获得更高的准确度：
- en: '![Figure 10.4 – Viewing the mAP metric'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4 – 查看mAP指标'
- en: '](img/B17705_10_4.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_4.jpg)'
- en: Figure 10.4 – Viewing the mAP metric
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 查看mAP指标
- en: One idea would be to launch a single training job with the best hyperparameters
    and let it run for more epochs. We could also resume the tuning job using `deploy()`
    on the tuner object and test our model just like any SageMaker model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用最佳超参数启动单个训练作业，并让它运行更多周期。我们也可以通过在调优器对象上使用`deploy()`来恢复调优作业，并像任何SageMaker模型一样测试我们的模型。
- en: As you can see, automatic model tuning is extremely powerful. By running a small
    number of jobs, we improved our metric by 158%! The cost is negligible compared
    to the time you would spend experimenting with other techniques.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，自动模型调优非常强大。通过运行少量作业，我们将指标提高了158%！与花费时间尝试其他技术相比，成本几乎可以忽略不计。
- en: In fact, running the same tuning job using the random strategy delivers a top
    accuracy of 0.52\. We would certainly need to run many more training jobs to even
    hope hitting 0.6315.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用随机策略运行相同的调优作业可以得到最高准确率0.52。我们肯定需要运行更多的训练作业才能希望达到0.6315。
- en: Let's now try to optimize the Keras example we used earlier in this chapter.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试优化我们在本章前面使用的Keras示例。
- en: Using automatic model tuning with Keras
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras进行自动模型调优
- en: Automatic model tuning can easily be used any algorithm on SageMaker, which
    of course includes all frameworks. Let's see how this works with Keras.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 自动模型调优可以轻松应用于SageMaker上的任何算法，当然也包括所有框架。让我们看看Keras是如何工作的。
- en: Earlier in this chapter, we trained our Keras CNN on the Fashion MNIST dataset
    for 20 epochs and reached a validation accuracy of 93.11%. Let's see if we can
    improve it with automatic model tuning. In the process, we'll also learn how to
    optimize for any metric present in the training log, not just metrics that are
    predefined in SageMaker.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，我们在Fashion MNIST数据集上训练了我们的Keras CNN，训练了20个周期，得到了93.11%的验证准确率。让我们看看能否通过自动模型调优来提高这个结果。在此过程中，我们还将学习如何优化训练日志中存在的任何指标，而不仅仅是SageMaker中预定义的指标。
- en: Optimizing on a custom metric
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在自定义指标上的优化
- en: 'Modifying our training script, we install the `keras-metrics` package ([https://github.com/netrack/keras-metrics](https://github.com/netrack/keras-metrics))
    and add the **precision**, **recall**, and **f1 score** metrics to the training
    log:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 修改我们的训练脚本，安装`keras-metrics`包（[https://github.com/netrack/keras-metrics](https://github.com/netrack/keras-metrics)），并将**精度**、**召回率**和**f1得分**指标添加到训练日志中：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After 20 epochs, the metrics now look like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 经过20个周期后，当前的指标如下所示：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we wanted to optimize on the f1 score, we would define the tuner metrics
    like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想优化f1得分，可以像这样定义调优器指标：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: That's all it takes. As long as a metric is printed in the training log, you
    can use it to tune models.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。只要在训练日志中打印出某个指标，你就可以用它来调优模型。
- en: Optimizing our Keras model
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化我们的Keras模型
- en: 'Now, let''s run our tuning job:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行我们的调优作业：
- en: 'We define the metrics for `HyperparameterTuner` like so, optimizing for accuracy
    and also displaying the f1 score:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像这样为`HyperparameterTuner`定义指标，优化准确度并同时显示f1得分：
- en: '[PRE25]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We define the parameter ranges to explore:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义要探索的参数范围：
- en: '[PRE26]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We use the same estimator (20 epochs with spot instances) and we define the
    tuner:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用相同的估算器（20个周期，使用Spot实例），并定义调优器：
- en: '[PRE27]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We launch the tuning job. While it''s running, we can use the **SageMaker SDK**
    to display the list of training jobs and their properties:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动调优作业。在作业运行时，我们可以使用**SageMaker SDK**来显示训练作业及其属性的列表：
- en: '[PRE28]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This prints out the table visible in the next screenshot:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出下一个截图中可见的表格：
- en: '![Figure 10.5 – Viewing information on a tuning job'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5 – 查看调优作业的信息'
- en: '](img/B17705_10_5.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_5.jpg)'
- en: Figure 10.5 – Viewing information on a tuning job
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 查看调优作业的信息
- en: The tuning job runs for 2 hours and 8 minutes (wall time). Top validation accuracy
    is 93.46% – a decent improvement over our baseline.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 调优作业运行了2小时8分钟（墙时）。最高验证准确率为93.46%——相比我们的基准，取得了不错的改进。
- en: We could certainly do better by training longer. However, the longer we train
    for, the more overfitting becomes a concern. We can alleviate it with early stopping,
    which can be implemented with a Keras callback. However, we should make sure that
    the job reports the metric for the best epoch, not for the last epoch. How can
    we display this in the training log? With another callback!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以通过训练更长时间来做得更好。然而，训练时间越长，过拟合的风险越大。我们可以通过早停法来减轻这一问题，这可以通过Keras回调实现。然而，我们应该确保作业报告的是最佳周期的指标，而不是最后一个周期的指标。我们该如何在训练日志中显示这个信息？通过另一个回调！
- en: Adding callbacks for early stopping
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为早期停止添加回调
- en: 'Adding a Keras callback for early stopping is very simple:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为早期停止添加一个 Keras 回调非常简单：
- en: 'We add a built-in callback for early stopping, based on validation accuracy:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加了一个基于验证准确率的早期停止内置回调：
- en: '[PRE29]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We add a custom callback to store validation accuracy at the end of each epoch,
    and to display the best one at the end of training:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加了一个自定义回调，以便在每个 epoch 结束时保存验证准确率，并在训练结束时显示最佳结果：
- en: '[PRE30]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We add these two callbacks to the training API:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这两个回调添加到训练 API 中：
- en: '[PRE31]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Testing with a few individual jobs, the last lines of the training log now
    look like this:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试几个单独的作业时，训练日志的最后几行现在看起来是这样的：
- en: '[PRE32]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the notebook, we update our metric definition in order to extract the best
    validation accuracy:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中，我们更新了指标定义，以便提取最佳验证准确率：
- en: '[PRE33]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Training for 60 epochs this time (about 3 hours wall time), top validation accuracy
    is now at 93.78%. It looks like this is as good as it gets by tweaking the learning
    rate and the batch size.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这次训练了 60 个 epochs（大约 3 小时的壁钟时间），当前的最高验证准确率为 93.78%。看起来通过调整学习率和批大小，已经达到了最佳效果。
- en: Using automatic model tuning for architecture search
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动模型调优进行架构搜索
- en: 'Our neural network has plenty more hyperparameters: number of convolution filters,
    dropout, and so on. Let''s try to optimize these as well:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络还有很多超参数：卷积滤波器的数量、dropout 等等。让我们也尝试优化这些参数：
- en: 'We modify our training script to add command-line parameters for the following
    network parameters, which are used by Keras layers in our model:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们修改训练脚本，添加命令行参数，以便为模型中 Keras 层使用的以下网络参数提供支持：
- en: '[PRE34]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you certainly guessed, the parameters let us set values for the number of
    convolution filters in each layer, the dropout value for convolution layers, and
    the dropout value for fully connected layers.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你猜测的那样，参数允许我们为每一层的卷积滤波器数量、卷积层的 dropout 值和全连接层的 dropout 值设置值。
- en: 'Accordingly, in the notebook, we define these hyperparameters and their ranges.
    For the learning rate and the batch size, we use narrow ranges centered on the
    optimal values discovered by the previous tuning job:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相应地，在笔记本中，我们定义了这些超参数及其范围。对于学习率和批大小，我们使用了以之前调优作业发现的最佳值为中心的窄范围：
- en: '[PRE35]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We launch the tuning job, running 50 jobs two at a time for 100 epochs.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动调优作业，运行 50 个作业，每次运行 2 个，总共训练 100 个 epochs。
- en: The tuning job runs for about 12 hours, for a total cost of about $15\. Top
    validation accuracy hits 94.09%. Compared to our baseline, automatic model tuning
    has improved the accuracy of our model by almost 1 percentage point – a very significant
    gain. If this model is used to predict 1 million samples a day, this translates
    to over 10,000 additional accurate predictions!
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 调优作业运行了大约 12 小时，总费用约为 15 美元。最高验证准确率达到了 94.09%。与我们的基线相比，自动模型调优提高了模型的准确性近 1 个百分点——这是一个非常显著的提升。如果这个模型每天用于预测
    100 万个样本，这将意味着多出 10,000 个准确的预测！
- en: In total, we've spent less about $50 on tuning our Keras model. Whatever business
    metric would be improved by the extra accuracy, it's fair to say that this spend
    would be recouped in no time. As many customers have told me, automatic model
    tuning pays for itself, and then some.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们在调优 Keras 模型上花费了不到 50 美元。无论是什么业务指标从额外的准确性中获益，可以公平地说，这笔支出很快就能收回。正如许多客户告诉我的那样，自动模型调优是自我盈利的，甚至会带来更多回报。
- en: This concludes our exploration of automatic model tuning, one of my favorite
    features in SageMaker. You can find more examples at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对自动模型调优的探索，这是 SageMaker 中我最喜欢的功能之一。你可以在 [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning)
    找到更多示例。
- en: Now, let's learn about SageMaker Debugger, and how it can help us to understand
    what's happening inside our models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解一下 SageMaker Debugger，以及它如何帮助我们理解模型内部发生了什么。
- en: Exploring models with SageMaker Debugger
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Debugger 探索模型
- en: SageMaker Debugger lets you configure *debugging rules* for your training job.
    These rules will inspect its internal state and check for specific unwanted conditions
    that could be developing during training. SageMaker Debugger includes a long list
    of built-in rules ([https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html)),
    and you can add your own written in Python.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger让您为训练作业配置*调试规则*。这些规则将检查作业的内部状态，并检测在训练过程中可能出现的特定不良条件。SageMaker
    Debugger包含一长串内置规则（[https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html)），并且您可以添加自定义的Python编写规则。
- en: In addition, you can save and inspect the model state (gradients, weights, and
    so on) as well as the training state (metrics, optimizer parameters, and so on).
    At each training step, the **tensors** storing these values may be saved in near-real-time
    in an S3 bucket, making it possible to visualize them while the model is training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以保存和检查模型状态（如梯度、权重等）以及训练状态（如指标、优化器参数等）。在每个训练步骤中，存储这些值的**张量**可以在接近实时的情况下保存到S3桶中，使得在模型训练过程中就可以对其进行可视化。
- en: Of course, you can select the tensor **collections** that you'd like to save,
    how often, and so on. Depending on the framework you use, different collections
    are available. You can find more information at [https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md).
    Last but not least, you can save either raw tensor data or tensor reductions to
    limit the amount of data involved. Reductions include min, max, median, and more.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您可以选择希望保存的张量**集合**，以及保存的频率等。根据您使用的框架，可用的集合有所不同。您可以在[https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md)中找到更多信息。最后但同样重要的是，您可以保存原始张量数据或张量的归约结果，以限制涉及的数据量。归约操作包括最小值、最大值、中位数等。
- en: If you are working with the built-in containers for supported versions of TensorFlow,
    PyTorch, Apache MXNet, or the built-in XGBoost algorithm, you can use SageMaker
    Debugger out of the box, without changing a line of code in your script. Yes,
    you read that right. All you have to do is add extra parameters to the estimator,
    as we will in the next examples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是支持版本的TensorFlow、PyTorch、Apache MXNet的内置容器，或者内置的XGBoost算法，您可以开箱即用地使用SageMaker
    Debugger，而无需在脚本中更改一行代码。没错，您没有看错。您只需向估算器中添加额外的参数，就像我们接下来在示例中展示的那样。
- en: With other versions, or with your own containers, minimal modifications are
    required. You can find the latest information and examples at [https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他版本，或者使用您自己的容器，仅需进行最小的修改。您可以在[https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger)找到最新的信息和示例。
- en: Debugging rules and saving tensors can be configured on the same training job.
    For clarity, we'll run two separate examples. First, let's use the XGBoost and
    Boston Housing example from [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 调试规则和保存张量可以在同一个训练作业中进行配置。为了清晰起见，我们将运行两个独立的示例。首先，让我们使用来自[*第4章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)的XGBoost和波士顿住房示例，*训练机器学习模型*。
- en: Debugging an XGBoost job
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试XGBoost作业
- en: 'First, we will configure several built-in rules, train our model, and check
    the status of all rules:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将配置几个内置规则，训练我们的模型，并检查所有规则的状态：
- en: 'Taking a look at the list of built-in rules, we decide to use `overtraining`
    and `overfit`. Each rule has extra parameters that we could tweak. We stick to
    defaults, and we configure the `Estimator` accordingly:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看内置规则列表后，我们决定使用`overtraining`和`overfit`。每个规则都有额外的参数，我们可以进行调整。我们保持默认设置，并相应地配置`Estimator`：
- en: '[PRE36]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We set hyperparameters and launch training without waiting for the training
    job to complete. The training log won''t be visible in the notebook, but it will
    still be available in **CloudWatch Logs**:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置超参数并启动训练，而无需等待训练作业完成。训练日志将不会在笔记本中显示，但仍然可以在**CloudWatch Logs**中查看：
- en: '[PRE37]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In addition to the training job, one debugging job per rule is running under
    the hood, and we can check their statuses:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了训练作业外，每个规则下还会运行一个调试作业，我们可以查看它们的状态：
- en: '[PRE38]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This tells us that the debugger jobs are running:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这告诉我们调试作业正在运行：
- en: '[PRE39]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Running the same cell once the training job is complete, we see that no rule
    was triggered:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练作业完成后重新运行相同的单元格时，我们看到没有任何规则被触发：
- en: '[PRE40]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Had a rule been triggered, we would get an error message, and the training job
    would be stopped. Inspecting tensors stored in S3 would help us understand what
    went wrong.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果触发了规则，我们会收到错误信息，训练任务将被停止。检查存储在S3中的张量有助于我们理解出了什么问题。
- en: Inspecting an XGBoost job
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查XGBoost任务
- en: 'Let''s configure a new training job that saves all tensor collections available
    for XGBoost:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们配置一个新的训练任务，保存XGBoost的所有张量集合：
- en: 'We configure the `Estimator`, passing a `DebuggerHookConfig` object. We save
    three tensor collections at each training step: metrics, feature importance, and
    average **SHAP** ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    values. These help us understand how each feature in a data sample contributes
    to increasing or decreasing the predicted value.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置`Estimator`，传入`DebuggerHookConfig`对象。在每个训练步骤中，我们保存三种张量集合：指标、特征重要性和平均**SHAP**（[https://github.com/slundberg/shap](https://github.com/slundberg/shap)）值。这些有助于我们理解数据样本中每个特征如何影响预测值的增减。
- en: 'For larger models and datasets, this could generate a lot of data, which would
    take a long time to load and analyze. We would either increase the save interval
    or save tensor reductions instead of full tensors:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于更大的模型和数据集，这可能会生成大量数据，加载和分析这些数据需要较长时间。我们可以增加保存间隔，或保存张量缩减值而非完整张量：
- en: '[PRE41]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once the training job has started, we can create a trial and load data that
    has already been saved. As this job is very short, we see all data within a minute
    or so:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练任务开始，我们可以创建一个试验并加载已保存的数据。由于该任务非常短，我们将在一分钟左右查看到所有数据：
- en: '[PRE42]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can list the name of all tensors that were saved:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以列出所有已保存张量的名称：
- en: '[PRE43]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can also list the name of all tensors in a given collection:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以列出给定集合中所有张量的名称：
- en: '[PRE44]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For each tensor, we can access training steps and values. Let''s plot feature
    information from the `average_shap` and `feature_importance` collections:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个张量，我们可以访问训练步骤和数值。让我们绘制来自`average_shap`和`feature_importance`集合的特征信息：
- en: '[PRE45]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We build the `average_shap` plot:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建`average_shap`图：
- en: '[PRE46]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can see it in the following screenshot – **dis**, **crim**, and **nox**
    have the largest average values:![Figure 10.6 – Plotting average SHAP values over
    time
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在以下截图中看到——**dis**、**crim**和**nox**的平均值最大：![图10.6 – 绘制SHAP值随时间变化的图
- en: '](img/B17705_10_6.jpg)'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_10_6.jpg)'
- en: Figure 10.6 – Plotting average SHAP values over time
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.6 – 绘制SHAP值随时间变化的图
- en: 'We build the `feature_importance/weight` plot:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们构建`feature_importance/weight`图：
- en: '[PRE47]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You can see it in the following screenshot – **crim**, **age**, and **dis**
    have the largest weights:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在以下截图中看到——**crim**、**age**和**dis**的权重最大：
- en: '![Figure 10.7 – Plotting feature weights over time'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 – 绘制特征权重随时间变化的图'
- en: '](img/B17705_10_7.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_7.jpg)'
- en: Figure 10.7 – Plotting feature weights over time
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 绘制特征权重随时间变化的图
- en: Now, let's use SageMaker Debugger on our Keras and Fashion-MNIST example.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Keras和Fashion-MNIST示例中使用SageMaker Debugger。
- en: Debugging and inspecting a Keras job
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试和检查Keras任务
- en: 'We can inspect and debug a Keras job using the following steps:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤检查和调试Keras任务：
- en: 'The default behavior in TensorFlow 2.x is eager mode, where gradients are not
    available. Hence, we disable eager mode in our script, which is the only modification
    required:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 2.x的默认行为是急切模式（eager mode），此时梯度不可用。因此，我们在脚本中禁用急切模式，这是唯一需要修改的地方：
- en: '[PRE48]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We start from the same estimator. The dataset has 70,000 samples (60,000 for
    training, plus 10,000 for validation). With 30 epochs and a batch size of 128,
    our training job will have about 16,400 steps (70,000 * 30 / 128). Saving tensors
    at each step feels like overkill. Let''s save them every 100 steps instead:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从相同的估计器开始。数据集包含70,000个样本（60,000个用于训练，10,000个用于验证）。通过30个epoch和批次大小128，我们的训练任务将有大约16,400个步骤（70,000
    * 30 / 128）。在每个步骤保存张量可能显得有些过头。我们改为每100步保存一次：
- en: '[PRE49]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Looking at the built-in rules available for TensorFlow, we decide to set up
    `poor_weight_initialization`, `dead_relu,` and `check_input_images`. We need to
    specify the index of channel information in the input tensor. It''s 4 for TensorFlow
    (batch size, height, width, and channels):'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看TensorFlow的内建规则后，我们决定设置`poor_weight_initialization`、`dead_relu`和`check_input_images`。我们需要指定输入张量中的通道信息索引。对于TensorFlow来说，它是4（批次大小、高度、宽度和通道）：
- en: '[PRE50]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Looking at the collections available for TensorFlow, we decide to save metrics,
    losses, outputs, weights, and gradients:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看TensorFlow的集合后，我们决定保存指标、损失、输出、权重和梯度：
- en: '[PRE51]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'As training starts, we see the rules being launched in the training log:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练开始时，我们在训练日志中看到规则被触发：
- en: '[PRE52]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'When training is complete, we check the status of the debugging rules:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，我们检查调试规则的状态：
- en: '[PRE53]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We create a trial using the same tensors saved in S3:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用保存在 S3 中的相同张量创建一个试验：
- en: '[PRE54]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s inspect the filters in the first convolution layer:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查第一层卷积层中的过滤器：
- en: '[PRE55]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As defined in our training script, the first convolution layer has 64 filters.
    Each one is 3x3 pixels, with a single channel (2D). Accordingly, gradients have
    the same shape.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的训练脚本中定义，第一层卷积层有64个过滤器。每个过滤器是3x3像素，具有单通道（2D）。因此，梯度具有相同的形状。
- en: 'We write a function to plot filter weights and gradients over time, and we
    plot weights in the last filter of the first convolution layer:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写一个函数来绘制过滤器权重和梯度随时间变化的图，并绘制第一层卷积层中最后一个过滤器的权重：
- en: '[PRE56]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You can see the graph in the following screenshot:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在以下截图中看到图表：
- en: '![Figure 10.8 – Plotting the weights of a convolution filter over time'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.8 – 绘制卷积过滤器随时间变化的权重'
- en: '](img/B17705_10_8.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_8.jpg)'
- en: Figure 10.8 – Plotting the weights of a convolution filter over time
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 绘制卷积过滤器随时间变化的权重
- en: As you can see, SageMaker Debugger makes it really easy to inspect training
    jobs. If you work with the built-in containers that support it, you don't need
    to modify your code. All configuration takes place in the estimator.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，SageMaker Debugger 使得检查训练作业变得非常简单。如果你使用内置的支持它的容器，你无需修改代码。所有配置都在估算器中完成。
- en: You can find additional examples at [https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples),
    including some advanced use cases such as real-time visualization and model pruning.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples)找到更多示例，包括一些高级用例，如实时可视化和模型修剪。
- en: This concludes the first part of the chapter, where we learned how to optimize
    the cost of training jobs with managed spot training, their accuracy with automatic
    model tuning, and how to inspect their internal state with SageMaker Debugger.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了，我们学习了如何通过托管的临时训练来优化训练作业的成本，通过自动模型调优来提高准确性，以及如何使用 SageMaker Debugger
    检查它们的内部状态。
- en: In the second part, we're going to dive into two advanced capabilities that
    will help us build better training workflows – SageMaker Feature Store and SageMaker
    Clarify.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们将深入探讨两项高级功能，帮助我们构建更好的训练工作流——SageMaker Feature Store 和 SageMaker Clarify。
- en: Managing features and building datasets with SageMaker Feature Store
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Feature Store 管理特征和构建数据集
- en: 'Until now, we''ve engineered our training and validation features in a notebook
    or in a SageMaker Processing script, before storing them as S3 objects. Then,
    we used these objects as-is to train and evaluate models. This is a perfectly
    reasonable workflow. However, the following questions may arise as your machine
    learning workflows grow and mature:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们一直在笔记本或 SageMaker Processing 脚本中工程化我们的训练和验证特征，然后将它们存储为 S3 对象。然后，我们直接使用这些对象来训练和评估模型。这是一个完全合理的工作流。然而，随着你的机器学习工作流的增长和成熟，可能会出现以下问题：
- en: How can we apply a well-defined schema to our features?
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何对特征应用明确定义的模式？
- en: How can we select a subset of our features to build different datasets?
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何选择特征的子集来构建不同的数据集？
- en: How can we store and manage different feature versions?
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何存储和管理不同版本的特征？
- en: How can we discover and reuse feature engineering by other teams?
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何发现并重用其他团队的特征工程？
- en: How can we access engineered features at prediction time?
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何在预测时访问工程化的特征？
- en: SageMaker Feature Store is designed to answer these questions. Let's add it
    to the classification training workflow we built with BlazingText and Amazon Reviews
    in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training Natural
    Language Processing Models*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Feature Store 被设计用来回答这些问题。让我们将其添加到我们与 BlazingText 和 Amazon Reviews
    一起构建的分类训练工作流中，见[*第6章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)，*训练自然语言处理模型*。
- en: Engineering features with SageMaker Processing
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Processing 工程化特征
- en: 'We can reuse our previous SageMaker Processing job almost as-is. The only difference
    is the output format of the engineered data. In the original job, we saved it
    as a plain text file according to the input format expected by BlazingText. This
    format is inconvenient for SageMaker Feature Store, as we need easy access to
    each column. CSV doesn''t work either as reviews contain commas, so we decide
    to use TSV instead:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎可以直接重用之前的SageMaker Processing任务。唯一的区别是工程数据的输出格式。在原始任务中，我们将其保存为纯文本文件，按照BlazingText期望的输入格式。此格式对于SageMaker
    Feature Store来说不太方便，因为我们需要轻松访问每一列。CSV格式也不行，因为评论中包含逗号，因此我们决定改用TSV格式：
- en: 'Accordingly, we add a few lines to our processing script:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们在处理脚本中添加了几行：
- en: '[PRE57]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Running our SageMaker Processing job as before, we now see two outputs: a plain
    text output for BlazingText (in case we wanted to train directly on the full dataset)
    and a TSV output that we''ll ingest in SageMaker Feature Store:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和之前一样运行我们的SageMaker Processing任务，我们现在看到两个输出：一个是BlazingText的纯文本输出（如果我们想直接对完整数据集进行训练），另一个是我们将摄取到SageMaker
    Feature Store中的TSV输出：
- en: '[PRE58]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s load the TSV file in a `pandas` dataframe and display the first few
    rows:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将TSV文件加载到`pandas`数据框中，并显示前几行：
- en: '[PRE59]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This prints out the table visible in the next image:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出下图所示的表格：
- en: '![Figure 10.9 – Viewing the first rows'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.9 – 查看前几行'
- en: '](img/B17705_10_9.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_9.jpg)'
- en: Figure 10.9 – Viewing the first rows
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 查看前几行
- en: Now, let's create a feature group where we'll ingest this data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个特征组，以便我们摄取这些数据。
- en: Creating a feature group
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建特征组
- en: A **feature group** is a resource that stores a collection of related features.
    Feature groups are organized in rows, which have a unique identifier and a timestamp.
    Each row contains key-value pairs, where each pair represents a feature name and
    a feature value.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征组**是一种资源，用于存储相关特征的集合。特征组按行组织，每行都有一个唯一标识符和时间戳。每行包含键值对，其中每一对代表一个特征名称和特征值。'
- en: 'First, let''s define the name of our feature group:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们定义特征组的名称：
- en: '[PRE60]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, we set the name of the feature that contains a unique identifier – `review_id`
    works perfectly here, and you could use any unique value present in your data
    source, such as a primary key:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置包含唯一标识符的特征名称——`review_id`在这里完全适用，你可以使用数据源中任何唯一的值，如主键：
- en: '[PRE61]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then, we add a timestamp column to all rows in our `pandas` dataframe. If your
    data source already contains a timestamp, you can reuse that value, either in
    the **float64** format or in the **UNIX** date/time format:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们为`pandas`数据框中的所有行添加了时间戳列。如果你的数据源已经包含时间戳，你可以重用该值，无论是**float64**格式还是**UNIX**日期/时间格式：
- en: '[PRE62]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Our dataframe now looks like the following picture:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们的数据框看起来如下所示：
- en: '![Figure 10.10 – Viewing timestamps'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 10.10 – 查看时间戳'
- en: '](img/B17705_10_10.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_10_10.jpg)'
- en: Figure 10.10 – Viewing timestamps
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.10 – 查看时间戳
- en: 'The next step is to define a schema for the feature group. We can either provide
    it explicitly in a JSON document or let SageMaker pick it up from the pandas dataframe.
    We use the second option:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是为特征组定义模式。我们可以将其显式提供为JSON文档，或者让SageMaker从pandas数据框中自动提取。我们使用第二种方式：
- en: '[PRE63]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We then load feature definitions:'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们加载特征定义：
- en: '[PRE64]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we create the feature group, passing the S3 location where features
    will be stored. This is where we''ll query them to build datasets. We enable the
    online store, which will give us low-latency access to features at prediction
    time. We also add a description and tags which make it easier to discover the
    feature group:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建特征组，传递将存储特征的S3位置。这是我们查询它们以构建数据集的地方。我们启用在线存储，这将使我们在预测时以低延迟访问特征。我们还添加了描述和标签，便于发现特征组：
- en: '[PRE65]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'After a few seconds, the feature group is ready and visible in SageMaker Studio,
    under **Components and registries** / **Feature Store**, as shown in the following
    screenshot:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，特征组准备就绪，并在SageMaker Studio中可见，位于**组件和注册表** / **特征存储**下，如下图所示：
- en: '![Figure 10.11 – Viewing a feature group'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.11 – 查看特征组'
- en: '](img/B17705_10_11.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_11.jpg)'
- en: Figure 10.11 – Viewing a feature group
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 查看特征组
- en: Now, let's ingest data.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始数据摄取。
- en: Ingesting features
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摄取特征
- en: 'SageMaker Feature Store lets us ingest data in three ways:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Feature Store允许我们通过三种方式摄取数据：
- en: Call the `PutRecord()` API to ingest a single record.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`PutRecord()` API以摄取单个记录。
- en: Call the `ingest()` API to upload the contents of a `pandas` dataframe.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`ingest()` API上传`pandas`数据框的内容。
- en: If we used **SageMaker Data Wrangler** for feature engineering, use an auto-generated
    notebook to create a feature group and ingest data.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用**SageMaker 数据 Wrangler**进行特征工程，可以使用自动生成的笔记本创建特征组并导入数据。
- en: 'We use the second option here, which is as simple as the following:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用第二个选项，它与以下代码一样简单：
- en: '[PRE66]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Once ingestion is complete, features are stored at the S3 location we specified,
    as well as in a dedicated low-latency backend. Let's use the former to build a
    dataset.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据导入完成，特征将存储在我们指定的 S3 位置以及专用的低延迟后端中。我们可以使用前者来构建数据集。
- en: Querying features to build a dataset
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询特征以构建数据集
- en: When we create the feature group, SageMaker automatically adds a new table for
    it in the **AWS Glue Data Catalog**. This makes it easy to use **Amazon Athena**
    to query data and build datasets on demand.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建特征组时，SageMaker 会自动在 **AWS Glue 数据目录** 中为其添加一个新表。这使得使用 **Amazon Athena**
    查询数据并按需构建数据集变得更加容易。
- en: 'Let''s say that we''d like to build a dataset that contains best-selling cameras
    with at least 1,000 reviews:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望构建一个包含至少有 1,000 条评论的畅销相机的数据集：
- en: 'First, we write an SQL query that computes the average rating for each camera,
    counts how many reviews each camera received, only keeps cameras with at least
    1,000 reviews, and orders cameras by descending average rating:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们编写一个 SQL 查询，计算每台相机的平均评分，统计每台相机收到的评论数，仅保留至少有 1,000 条评论的相机，并按平均评分降序排列相机：
- en: '[PRE67]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Then, we use Athena to query our feature group, store selected rows in a `pandas`
    dataframe, and display the first few rows:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 Athena 查询我们的特征组，将选中的行存储在 `pandas` 数据框中，并显示前几行：
- en: '[PRE68]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This prints out the table visible in the next image:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印出下一张图片中可见的表格：
- en: '![Figure 10.12 – Viewing query results'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.12 – 查看查询结果'
- en: '](img/B17705_10_12.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_12.jpg)'
- en: Figure 10.12 – Viewing query results
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 查看查询结果
- en: From then on, it's business as usual. We can save this dataframe to a CSV file
    and use it to train models. You'll find an end-to-end example in the GitHub repository.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，一切照常。我们可以将这个数据框保存为 CSV 文件，并用它来训练模型。你可以在 GitHub 仓库中找到一个完整的示例。
- en: Exploring other capabilities of SageMaker Feature Store
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 SageMaker Feature Store 的其他功能
- en: Over time, we could store different versions of the same feature – that is,
    several records with the same identifier but with different timestamps. This would
    allow us to retrieve earlier versions of a dataset – "time traveling" in our data
    with a simple SQL query.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们可以存储同一特征的不同版本——即具有相同标识符但时间戳不同的多个记录。这将允许我们通过简单的 SQL 查询来检索数据集的早期版本——在我们的数据中进行“时光旅行”。
- en: Last but not least, features are also available in the online store. We can
    retrieve individual records with the `GetRecord()` API and use features at prediction
    time whenever needed.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，功能也可以在在线商店中使用。我们可以通过 `GetRecord()` API 检索单个记录，并在预测时根据需要使用功能。
- en: Again, you'll find code samples for both capabilities in the GitHub repository.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你将在 GitHub 仓库中找到这两项功能的代码示例。
- en: To close this chapter, let's look at Amazon SageMaker Clarify, a capability
    that helps us build higher quality models by detecting potential bias present
    in datasets and models.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本章内容，让我们看看 Amazon SageMaker Clarify，这是一项通过检测数据集和模型中的潜在偏差，帮助我们构建更高质量模型的功能。
- en: Detecting bias in datasets and explaining predictions with SageMaker Clarify
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Clarify 检测数据集中的偏差并解释预测结果
- en: A **machine learning** (**ML**) model is only as good as the dataset it was
    built from. If a dataset is inaccurate or unfair in representing the reality it's
    supposed to capture, a corresponding model is very likely to learn this biased
    representation and perpetuate it in its predictions. As ML practitioners, we need
    to be aware of these problems, understand how they impact predictions, and limit
    that impact whenever possible.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）模型的好坏取决于其构建的 dataset。如果数据集不准确或无法公平地代表其应该捕捉的现实情况，那么相应的模型很可能会学习到这种偏差的表示，并在预测中延续这种偏差。作为机器学习实践者，我们需要意识到这些问题，理解它们如何影响预测，并尽可能地减少这种影响。'
- en: In this example, we'll work with the **Adult Data Set**, available at the **UCI
    Machine Learning Repository** ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml),
    Dua, D. and Graff, C., 2019). This dataset describes a binary classification task,
    where we try to predict if an individual earns less or more than $50,000 per year.
    Here, we'd like to check whether this dataset introduces gender bias or not. In
    other words, does it help us build models that predict equally well for men and
    women?
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用**成人数据集**，该数据集可在**UCI机器学习库**中找到（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)，Dua,
    D.和Graff, C.，2019）。这个数据集描述了一个二分类任务，我们尝试预测一个人是否年收入超过$50,000。这里，我们想检查这个数据集是否引入了性别偏差。换句话说，它是否有助于我们构建一个对男性和女性的预测效果一样好的模型？
- en: Note
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset you'll find in the GitHub repository has been slightly processed.
    The label column has been moved to the front as per XGBoost requirements. Categorical
    variables have been one-hot encoded.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 你在GitHub仓库中找到的数据集经过了轻微的处理。标签列已经根据XGBoost的要求被移到前面。类别变量已经进行了独热编码。
- en: Configuring a bias analysis with SageMaker Clarify
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Clarify配置偏差分析
- en: SageMaker Clarify computes pre-training and post-training metrics that help
    us understand how a model predicts.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify计算训练前和训练后的指标，帮助我们理解模型的预测情况。
- en: Post-training metrics obviously require a trained model, so we first train a
    binary classification model with XGBoost. It's nothing we haven't seen many times
    already, and you'll find the code in the GitHub repository. This model hits a
    validation AuC of 92.75%.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练指标显然需要一个已训练的模型，因此我们首先使用XGBoost训练一个二分类模型。这是我们已经看过很多次的内容，你可以在GitHub仓库中找到相关代码。这个模型的验证AuC达到了92.75%。
- en: 'Once training is complete, we can proceed with the bias analysis:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们就可以进行偏差分析：
- en: 'Bias analyses run as SageMaker Processing jobs. Accordingly, we create a `SageMakerClarifyProcessor`
    object with our infrastructure requirements. As the job is small-scale, we use
    a single instance. For larger jobs, we could use an increased instance count,
    and the analysis would automatically run on **Spark**:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏差分析作为SageMaker处理任务运行。因此，我们创建一个`SageMakerClarifyProcessor`对象，指定我们的基础设施需求。由于任务规模较小，我们使用一个实例。对于更大的任务，我们可以使用更多实例，并且分析将自动在**Spark**上运行：
- en: '[PRE69]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Then, we create a `DataConfig` object describing the dataset to analyze:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个`DataConfig`对象，描述要分析的数据集：
- en: '[PRE70]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Likewise, we create a `ModelConfig` object describing the model to analyze:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样地，我们创建一个`ModelConfig`对象，描述要分析的模型：
- en: '[PRE71]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Finally, we create a `BiasConfig` object describing the metrics to compute.
    The `label_values_or_threshold` defines the label value for the positive outcome
    (1, indicating a revenue higher than $50K). The `facet_name` defines the feature
    on which we'd like to run the analysis (`Sex_`), and `facet_values_or_threshold`
    defines the feature value for the potentially disadvantaged group (1, indicating
    women).
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建一个`BiasConfig`对象，描述要计算的指标。`label_values_or_threshold`定义了正向结果的标签值（1，表示年收入高于$50K）。`facet_name`定义了我们希望分析的特征（`Sex_`），而`facet_values_or_threshold`定义了潜在弱势群体的特征值（1，表示女性）。
- en: '[PRE72]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We're now ready to run the analysis.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好运行分析了。
- en: Running a bias analysis
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行偏差分析
- en: 'Putting everything together, we launch the analysis with the following:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起，我们使用以下命令启动分析：
- en: '[PRE73]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Once the analysis is complete, the results are visible in SageMaker Studio.
    A report is also generated and stored in S3 in HTML, PDF, and notebook format.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分析完成，结果将在SageMaker Studio中可见。报告也会生成并以HTML、PDF和Notebook格式存储在S3中。
- en: 'In **Experiments and trials**, we locate our SageMaker Clarify job, and we
    right-click on **Open trial details**. Selecting **Bias report**, we see bias
    metrics, as shown in the next screenshot:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在**实验和试验**中，我们找到我们的SageMaker Clarify任务，并右键点击**打开试验详情**。选择**偏差报告**，我们可以看到偏差指标，如下图所示：
- en: '![Figure 10.13 – Viewing bias metrics'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.13 – 查看偏差指标'
- en: '](img/B17705_10_13.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_13.jpg)'
- en: Figure 10.13 – Viewing bias metrics
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 查看偏差指标
- en: Analyzing bias metrics
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析偏差指标
- en: 'If you''d like to learn more about bias metrics, what they mean, and how they''re
    computed, I highly recommend these resources:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于偏差指标的信息、它们的含义以及它们是如何计算的，我强烈推荐以下资源：
- en: '[https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf)'
- en: '[https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf)'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf](https://pages.awscloud.com/rs/112-TZM-766/images/Amazon.AI.Fairness.and.Explainability.Whitepaper.pdf)'
- en: '[https://github.com/aws/amazon-sagemaker-clarify](https://github.com/aws/amazon-sagemaker-clarify)'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/aws/amazon-sagemaker-clarify](https://github.com/aws/amazon-sagemaker-clarify)'
- en: Let's look at two pre-training metrics, **Class Imbalance** (**CI**) and **Difference
    in Positive Proportions in Labels** (**DPL**), and one post-training metric, **Difference
    in Positive Proportions in Predicted Labels** (**DPPL**).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看两个训练前的度量标准，**类别不平衡**（**CI**）和**标签中正类比例差异**（**DPL**），以及一个训练后的度量标准，**预测标签中正类比例差异**（**DPPL**）。
- en: A non-zero value of CI indicates that the dataset is imbalanced. Here, the difference
    between the men fraction and the women fraction is 0.35\. Indeed, the men group
    is about two-thirds of the dataset, the women group is about one-third. This isn't
    a very severe imbalance, but we should also look at the proportion of positive
    labels for each class.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: CI的非零值表明数据集是不平衡的。这里，男性和女性比例的差异是0.35。确实，男性组大约占数据集的三分之二，女性组约占三分之一。这并不是一个非常严重的失衡，但我们也应该查看每个类别的正类标签比例。
- en: The DPL measures if each class has the same proportion of positive labels. In
    other words, does the dataset contain the same ratio of men and women earning
    $50K? The DPL is non-zero (0.20), which tells us that men have a higher ratio
    of $50K earners.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: DPL衡量每个类别是否具有相同的正类标签比例。换句话说，数据集中男性和女性赚取$50K的比例是否相同？DPL的值为非零（0.20），这告诉我们男性的$50K收入者比例更高。
- en: The DPPL is a post-training metric similar to the DPL. Its value (0.18) shows
    that the model unfortunately picked up the bias present in the dataset, only lightly
    reducing it. Indeed, the model predicts a more favorable outcome for men (over-predicting
    $50K earners) and a less favorable outcome for women (under-predicting 50K earners).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: DPPL是一个训练后度量，类似于DPL。它的值（0.18）表明模型不幸地拾取了数据集中的偏差，只是轻微地减少了它。实际上，模型为男性预测了一个更有利的结果（过度预测$50K收入者），而为女性预测了一个不太有利的结果（低估$50K收入者）。
- en: That's clearly a problem. Although the model has a rather nice validation AuC
    (92.75%), it doesn't predict both classes equally well.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一个问题。尽管模型有一个相当不错的验证AuC（92.75%），但它并没有同样好地预测两种类别。
- en: Before we dive into the data and try to mitigate this issue, let's run an explainability
    analysis.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入分析数据并尝试缓解这个问题之前，先进行一次可解释性分析。
- en: Running an explainability analysis
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行可解释性分析
- en: SageMaker Clarify can compute local and global SHAP ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    values. They help us understand feature importance, and how individual feature
    values contribute to a positive or negative outcome.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify可以计算局部和全局的SHAP值（[https://github.com/slundberg/shap](https://github.com/slundberg/shap)）。它们帮助我们理解特征的重要性，以及各个特征值如何影响正面或负面的结果。
- en: 'Bias analyses run as SageMaker Processing jobs, and the process is similar:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差分析作为SageMaker处理作业运行，过程类似：
- en: 'We create a `DataConfig` object describing the dataset to analyze:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`DataConfig`对象，描述要分析的数据集：
- en: '[PRE74]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We create a `SHAPConfig` object describing how we''d like to compute SHAP values
    – that is, which baseline to use (I use the test set where I removed labels),
    how many samples to use (twice the number of features plus 2048, a common default),
    and how to aggregate values:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`SHAPConfig`对象，描述我们希望如何计算SHAP值——即使用哪个基准（我使用了移除标签后的测试集），使用多少样本（特征数的两倍加2048，这是一个常见的默认值），以及如何聚合值：
- en: '[PRE75]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, we run the analysis:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们运行分析：
- en: '[PRE76]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Results in available in SageMaker Studio, under `Sex` feature is by far the
    most important, which confirms the bias analysis. Ethical considerations aside,
    this doesn't seem to make a lot of sense from a business perspective. Features
    such as education or capital gain should be more important.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在SageMaker Studio中查看，`Sex`特征是最重要的，这确认了偏差分析的结果。抛开伦理考虑不谈，从商业角度来看，这似乎并不太合理。像教育程度或资本收益这样的特征应该更为重要。
- en: '![Figure 10.14 – Viewing feature importance'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.14 – 查看特征重要性](https://example.org)'
- en: '](img/B17705_10_14.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_10_14.jpg)'
- en: Figure 10.14 – Viewing feature importance
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 查看特征重要性
- en: Local SHAP values have also been computed and stored in S3\. We could use them
    to understand how feature values impact the prediction of each individual sample.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 本地SHAP值也已经计算并存储在S3中。我们可以使用这些值来了解特征值如何影响每个单独样本的预测。
- en: Now, let's see how we can try to mitigate the bias we detected in our dataset.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何尝试缓解我们在数据集中检测到的偏差。
- en: Mitigating bias
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解偏差
- en: This dataset combines two problems. First, it contains more men than women.
    Second, the men group has a higher proportion of positive outcomes. The combination
    of these two problems leads to a situation where the dataset contains a disproportionately
    low number of women who earn more than $50K. This makes it harder for the model
    to learn in a fair way, and it tends to favor the majority class.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集结合了两个问题。首先，它包含更多的男性而非女性。其次，男性组的正向结果比例较高。这两个问题的结合导致数据集中$50K收入的女性数量比例异常低。这使得模型更难以公平地学习，并且倾向于偏向多数类。
- en: 'Bias mitigation techniques include the following:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差缓解技术包括以下内容：
- en: Undersampling the majority class by removing majority samples to rebalance the
    dataset
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过删除多数样本来对多数类进行下采样，以重新平衡数据集
- en: Oversampling the minority class by adding more samples through duplication of
    existing ones
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过复制现有样本来对少数类进行过采样，增加更多样本
- en: Adding synthetic samples to the minority class by generating new samples that
    have statistical properties similar to existing samples
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过生成与现有样本具有相似统计属性的新样本，向少数类添加合成样本
- en: Note
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Altering data shouldn't be done lightly, especially in organizations operating
    in regulated industries. This can have serious business, compliance, and legal
    consequences. Please make sure to get approval before doing this in production.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改数据不应轻率进行，尤其是在受监管行业中运作的组织中。这可能会产生严重的业务、合规性和法律后果。在生产环境中进行此操作之前，请务必获得批准。
- en: 'Let''s try a combined approach based on the **imbalanced-learn** open source
    library (https://imbalanced-learn.org). First, we''ll add synthetic samples to
    the minority class with the **Synthetic Minority Oversampling Technique** (**SMOTE**)
    algorithm, in order to match the ratio of $50K earners present in the majority
    samples. Then, we''ll undersample the majority class to match the number of samples
    of the minority class. The result will be a perfectly balanced dataset, where
    both classes have the same size and the same ratio of $50K earners. Let''s get
    started:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一种结合方法，基于**imbalanced-learn**开源库（https://imbalanced-learn.org）。首先，我们将使用**合成少数类过采样技术**（**SMOTE**）算法向少数类添加合成样本，以匹配多数类样本中$50K收入者的比例。接着，我们将对多数类进行下采样，使其样本数与少数类相匹配。结果将是一个完全平衡的数据集，两个类别的大小相同，$50K收入者的比例也相同。让我们开始吧：
- en: 'First, we need to compute the ratios for both classes:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要计算两个类别的比例：
- en: '[PRE77]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'This gives us the following result, showing that the majority class (class
    0) has a much larger ratio of $50k earners:'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这给出了以下结果，显示多数类（类别 0）拥有远高于$50K收入者的比例：
- en: '[PRE78]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Then, we generate synthetic minority samples:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们生成少数类的合成样本：
- en: '[PRE79]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Next, we rebuild the dataset with the original majority class and the rebalanced
    minority class:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用原始多数类和重新平衡的少数类重新构建数据集：
- en: '[PRE80]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Finally, we undersample the original majority class to rebalance ratios:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们对原始多数类进行下采样，以重新平衡比例：
- en: '[PRE81]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'We count both classes and compute their ratios again:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次计算两个类别的样本数，并重新计算它们的比例：
- en: '[PRE82]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'This displays the following results:'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显示了以下结果：
- en: '[PRE83]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Training with this rebalanced dataset, and using the same test set, we get a
    validation AuC of 92.95%, versus 92.75% for the original model. Running a new
    bias analysis, CI is zero, and the DPL and DPPL are close to zero.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这个重新平衡的数据集进行训练，并使用相同的测试集时，我们得到了92.95%的验证AuC，相比之下原始模型为92.75%。进行新的偏差分析时，CI为零，DPL和DPPL接近零。
- en: Not only have we built a model that predicts more fairly, but it's also a little
    bit more accurate. For once, it looks like we got the best of both worlds!
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅构建了一个预测更公平的模型，而且还稍微提高了准确性。难得的是，这次我们似乎做到了两全其美！
- en: Summary
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter concludes our exploration of training techniques. You learned about
    managed spot training, a simple way to slash training costs by 70% or more. You
    also saw how checkpointing helps to resume jobs that have been interrupted. Then,
    you learned about automatic model tuning, a great way to extract more accuracy
    from your models by exploring hyperparameter ranges. You learned about SageMaker
    Debugger, an advanced capability that automatically inspects training jobs for
    unwanted conditions and saves tensor collections to S3 for inspection and visualization.
    Finally, we discovered two capabilities that help you build higher quality workflows
    and models, SageMaker Feature Store and SageMaker Clarify.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了我们对训练技术的探索。你学习了受管训练（managed spot training），这是一种通过减少70％或更多的训练成本的简单方法。你还了解了检查点（checkpointing）如何帮助恢复被中断的任务。接着，你学习了自动模型调优（automatic
    model tuning），这是一种通过探索超参数范围从模型中提取更多准确性的有效方法。你了解了SageMaker调试器（SageMaker Debugger），这是一个高级功能，能够自动检查训练任务中的不良条件，并将张量集合保存到S3，以便检查和可视化。最后，我们发现了两个有助于你构建更高质量工作流和模型的功能，SageMaker特征存储（SageMaker
    Feature Store）和SageMaker Clarify。
- en: In the next chapter, we'll study model deployment in detail.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将详细学习模型部署。
