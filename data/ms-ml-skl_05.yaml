- en: Chapter 5. Nonlinear Classification and Regression with Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 使用决策树进行非线性分类和回归
- en: 'In the previous chapters we discussed generalized linear models, which relate
    a linear combination of explanatory variables to one or more response variables
    using a link function. You learned to use multiple linear regression to solve
    regression problems, and we used logistic regression for classification tasks.
    In this chapter we will discuss a simple, nonlinear model for classification and
    regression tasks: the decision tree. We''ll use decision trees to build an ad
    blocker that can learn to classify images on a web page as banner advertisements
    or page content. Finally, we will introduce ensemble learning methods, which combine
    a set of models to produce an estimator with better predictive performance than
    any of its component estimators.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了广义线性模型，它通过链接函数将解释变量的线性组合与一个或多个响应变量联系起来。你学习了如何使用多元线性回归来解决回归问题，并且我们使用了逻辑回归来处理分类任务。在这一章中，我们将讨论一个用于分类和回归任务的简单非线性模型：决策树。我们将使用决策树构建一个广告拦截器，它能够学习将网页上的图像分类为横幅广告或页面内容。最后，我们将介绍集成学习方法，这些方法结合多个模型来生成一个估算器，其预测性能优于任何单一模型。
- en: Decision trees
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are tree-like graphs that model a decision. They are analogous
    to the parlor game Twenty Questions. In Twenty Questions, one player, called the
    answerer, chooses an object but does not reveal the object to the other players,
    who are called questioners. The object should be a common noun, such as "guitar"
    or "sandwich", but not "1969 Gibson Les Paul Custom" or "North Carolina". The
    questioners must guess the object by asking as many as twenty questions that can
    be answered with `yes`, `no`, or `maybe`. An intuitive strategy for questioners
    is to ask questions of increasing specificity; asking "*is it a musical instrument?*"
    as the first question will not efficiently reduce the number of possibilities.
    The branches of a decision tree specify the shortest sequences of explanatory
    variables that can be examined in order to estimate the value of a response variable.
    To continue the analogy, in Twenty Questions the questioner and the answerers
    all have knowledge of the training data, but only the answerer knows the values
    of the features for the test instance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是类似树状的图形，用于建模决策过程。它们类似于游戏《二十个问题》。在《二十个问题》中，一名玩家，称为回答者，选择一个物体，但不向其他玩家透露物体是什么，这些其他玩家被称为提问者。物体应该是常见名词，比如“吉他”或“三明治”，而不是“1969年吉布森Les
    Paul Custom吉他”或“北卡罗来纳”。提问者必须通过提问最多二十个可以回答“是”、“否”或“也许”的问题来猜测物体。提问者的一个直观策略是逐步提高问题的具体性；作为第一个问题问“它是乐器吗？”不会有效减少可能的答案数量。决策树的分支指定了可以检查的最短解释变量序列，以估算响应变量的值。延续这个类比，在《二十个问题》游戏中，提问者和回答者都知道训练数据，但只有回答者知道测试实例的特征值。
- en: Decision trees are commonly learned by recursively splitting the set of training
    instances into subsets based on the instances' values for the explanatory variables.
    The following diagram depicts a decision tree that we will look at in more detail
    later in the chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通常通过递归地将训练实例集根据实例的解释变量值划分为子集来学习。以下图示展示了一个决策树，我们将在本章稍后更详细地讨论它。
- en: '![Decision trees](img/8365OS_05_01.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/8365OS_05_01.jpg)'
- en: Represented by boxes, the interior nodes of the decision tree test explanatory
    variables. These nodes are connected by edges that specify the possible outcomes
    of the tests. The training instances are divided into subsets based on the outcomes
    of the tests. For example, a node might test whether or not the value of an explanatory
    variable exceeds a threshold. The instances that pass the test will follow an
    edge to the node's right child, and the instances that fail the test will follow
    an edge to the node's left child. The children nodes similarly test their subsets
    of the training instances until a stopping criterion is satisfied. In classification
    tasks, the leaf nodes of the decision tree represent classes. In regression tasks,
    the values of the response variable for the instances contained in a leaf node
    may be averaged to produce the estimate for the response variable. After the decision
    tree has been constructed, making a prediction for a test instance requires only
    following the edges until a leaf node is reached.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的内部节点通过框表示，用来测试解释变量。这些节点通过边连接，边上指定了测试结果的可能性。训练实例根据测试结果被分为多个子集。例如，一个节点可能会测试某个解释变量的值是否超过某个阈值。通过测试的实例将沿着边连接到节点的右子节点，而未通过测试的实例将沿着边连接到节点的左子节点。子节点同样测试它们的训练实例子集，直到满足停止准则。在分类任务中，决策树的叶节点代表类别。在回归任务中，叶节点中包含的实例的响应变量值可能会被平均，以生成响应变量的估计值。构建完决策树后，为一个测试实例做预测只需要沿着边走，直到到达叶节点。
- en: Training decision trees
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练决策树
- en: Let's create a decision tree using an algorithm called **Iterative Dichotomiser
    3** (**ID3**). Invented by Ross Quinlan, ID3 was one of the first algorithms used
    to train decision trees. Assume that you have to classify animals as cats or dogs.
    Unfortunately, you cannot observe the animals directly and must use only a few
    attributes of the animals to make your decision. For each animal, you are told
    whether or not it likes to play fetch, whether or not it is frequently grumpy,
    and its favorite of three types of food.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一种名为**迭代二分法 3**（**ID3**）的算法来创建决策树。ID3由罗斯·昆兰发明，是最早用于训练决策树的算法之一。假设你需要将动物分类为猫或狗。不幸的是，你不能直接观察这些动物，只能通过它们的几个属性来做决定。对于每只动物，你会被告知它是否喜欢玩接飞盘，它是否经常生气，以及它最喜欢哪三种食物之一。
- en: To classify new animals, the decision tree will examine an explanatory variable
    at each node. The edge it follows to the next node will depend on the outcome
    of the test. For example, the first node might ask whether or not the animal likes
    to play fetch. If the animal does, we will follow the edge to the left child node;
    if not, we will follow the edge to the right child node. Eventually an edge will
    connect to a leaf node that indicates whether the animal is a cat or a dog.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分类新的动物，决策树将在每个节点检查一个解释变量。它将根据测试结果选择沿着哪条边到达下一个节点。例如，第一个节点可能会问动物是否喜欢玩接飞盘。如果喜欢，我们将沿着边走到左子节点；如果不喜欢，我们将沿着边走到右子节点。最终，一条边将连接到叶节点，指示该动物是猫还是狗。
- en: 'The following fourteen instances comprise our training data:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下十四个实例构成我们的训练数据：
- en: '| Training instance | Plays fetch | Is grumpy | Favorite food | Species |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 训练实例 | 玩接飞盘 | 是否生气 | 最喜欢的食物 | 物种 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | Yes | No | Bacon | Dog |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 否 | 培根 | 狗 |'
- en: '| 2 | No | Yes | Dog Food | Dog |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 否 | 是 | 狗粮 | 狗 |'
- en: '| 3 | No | Yes | Cat food | Cat |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 否 | 是 | 猫粮 | 猫 |'
- en: '| 4 | No | Yes | Bacon | Cat |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 否 | 是 | 培根 | 猫 |'
- en: '| 5 | No | No | Cat food | Cat |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 否 | 否 | 猫粮 | 猫 |'
- en: '| 6 | No | Yes | Bacon | Cat |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 否 | 是 | 培根 | 猫 |'
- en: '| 7 | No | Yes | Cat Food | Cat |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 否 | 是 | 猫粮 | 猫 |'
- en: '| 8 | No | No | Dog Food | Dog |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 否 | 否 | 狗粮 | 狗 |'
- en: '| 9 | No | Yes | Cat food | Cat |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 否 | 是 | 猫粮 | 猫 |'
- en: '| 10 | Yes | No | Dog Food | Dog |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 是 | 否 | 狗粮 | 狗 |'
- en: '| 11 | Yes | No | Bacon | Dog |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 是 | 否 | 培根 | 狗 |'
- en: '| 12 | No | No | Cat food | Cat |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 否 | 否 | 猫粮 | 猫 |'
- en: '| 13 | Yes | Yes | Cat food | Cat |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 是 | 是 | 猫粮 | 猫 |'
- en: '| 14 | Yes | Yes | Bacon | Dog |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 是 | 是 | 培根 | 狗 |'
- en: From this data we can see that cats are generally grumpier than the dogs. Most
    dogs play fetch and most cats refuse. Dogs prefer dog food and bacon, whereas
    cats only like cat food and bacon. The `is grumpy` and `plays fetch` explanatory
    variables can be easily converted to binary-valued features. The `favorite food`
    explanatory variable is a categorical variable that has three possible values;
    we will one-hot encode it. Recall from [Chapter 3](ch03.html "Chapter 3. Feature
    Extraction and Preprocessing"), *Feature Extraction and Preprocessing*, that one-hot
    encoding represents a categorical variable with as many binary-valued features
    as there are values for variable. Representing the categorical variable with a
    single integer-valued feature will encode an artificial order to its values. Since
    `favorite food` has three possible states, we will represent it with three binary-valued
    features. From this table, we can manually construct classification rules. For
    example, an animal that is grumpy and likes cat food must be a cat, while an animal
    that plays fetch and likes bacon must be a dog. Constructing these classification
    rules by hand for even a small data set is cumbersome. Instead, we will learn
    these rules by creating a decision tree.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数据我们可以看出，猫通常比狗更易怒。大多数狗喜欢玩接飞盘，而大多数猫拒绝。狗喜欢狗粮和培根，而猫只喜欢猫粮和培根。`is grumpy` 和 `plays
    fetch` 这两个解释变量可以轻松转换为二进制特征。`favorite food` 这个解释变量是一个类别变量，具有三个可能的值；我们将对其进行独热编码。回顾[第三章](ch03.html
    "第三章 特征提取与预处理")，*特征提取与预处理*，独热编码通过与变量值相同数量的二进制特征来表示类别变量。如果用单个整数值特征表示类别变量，会对其值编码一种人为的顺序。由于
    `favorite food` 有三个可能的状态，我们将用三个二进制特征来表示它。从这个表中，我们可以手动构建分类规则。例如，一个既易怒又喜欢猫粮的动物一定是猫，而一个喜欢玩接飞盘并喜欢培根的动物一定是狗。即使是对于一个小数据集，手动构建这些分类规则也非常繁琐。相反，我们将通过创建决策树来学习这些规则。
- en: Selecting the questions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择问题
- en: Like Twenty Questions, the decision tree will estimate the value of the response
    variable by testing the values of a sequence of explanatory variables. Which explanatory
    variable should be tested first? Intuitively, a test that produces subsets that
    contain all cats or all dogs is better than a test that produces subsets that
    still contain both cats and dogs. If the members of a subset are of different
    classes, we are still uncertain about how to classify the instance. We should
    also avoid creating tests that separate only a single cat or dog from the others;
    such tests are analogous to asking specific questions in the first few rounds
    of Twenty Questions. More formally, these tests can infrequently classify an instance
    and might not reduce our uncertainty. The tests that reduce our uncertainty about
    the classification the most are the best. We can quantify the amount of uncertainty
    using a measure called **entropy**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 像“二十个问题”一样，决策树通过测试一系列解释变量的值来估算响应变量的值。应该首先测试哪个解释变量？直观地说，产生包含所有猫或所有狗的子集的测试要优于产生仍包含猫和狗的子集的测试。如果子集中的成员属于不同的类别，我们仍然无法确定如何对实例进行分类。我们还应该避免创建只将一只猫或一只狗从其他动物中分离出来的测试；这种测试类似于在“二十个问题”的前几轮问特定问题。更正式地说，这些测试可能很少对实例进行分类，并且可能不会减少我们的不确定性。减少分类不确定性的测试是最好的。我们可以使用一种叫做**熵**的度量来量化不确定性的大小。
- en: Measured in bits, entropy quantifies the amount of uncertainty in a variable.
    Entropy is given by the following equation, where ![Selecting the questions](img/8365OS_05_30.jpg)
    is the number of outcomes and ![Selecting the questions](img/8365OS_05_31.jpg)
    is the probability of the outcome ![Selecting the questions](img/8365OS_05_32.jpg).
    Common values for ![Selecting the questions](img/8365OS_05_33.jpg) are `2`, ![Selecting
    the questions](img/8365OS_05_33a.jpg), and `10`. Because the log of a number less
    than one will be negative, the entire sum is negated to return a positive value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用比特（bits）来衡量，熵量化了变量中的不确定性。熵由以下方程给出，其中 ![选择问题](img/8365OS_05_30.jpg) 是结果的数量，![
- en: '![Selecting the questions](img/8365OS_05_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_02.jpg)'
- en: 'For example, a single toss of a fair coin has only two outcomes: heads and
    tails. The probability that the coin will land on heads is 0.5, and the probability
    that it will land on tails is 0.5\. The entropy of the coin toss is equal to the
    following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一次掷公平硬币只有两种结果：正面和反面。硬币正面朝上的概率是0.5，反面朝上的概率也是0.5。那么该掷投的熵等于以下公式：
- en: '![Selecting the questions](img/8365OS_05_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_03.jpg)'
- en: 'That is, only one bit is required to represent the two equally probable outcomes,
    heads and tails. Two tosses of a fair coin can result in four possible outcomes:
    heads and heads, heads and tails, tails and heads, and tails and tails. The probability
    of each outcome is *0.5 x 0.5 = 0.25*. The entropy of two tosses is equal to the
    following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，只需要一个比特就能表示两个同样可能的结果：正面和反面。两次掷公平硬币可以产生四种可能的结果：正正、正反、反正和反反。每个结果的概率是*0.5
    x 0.5 = 0.25*。两次掷投的熵等于以下公式：
- en: '![Selecting the questions](img/8365OS_05_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_04.jpg)'
- en: 'If the coin has the same face on both sides, the variable representing its
    outcome has 0 bits of entropy; that is, we are always certain of the outcome and
    the variable will never represent new information. Entropy can also be represented
    as a fraction of a bit. For example, an unfair coin has two different faces, but
    is weighted such that the faces are not equally likely to land in a toss. Assume
    that the probability that an unfair coin will land on heads is 0.8, and the probability
    that it will land on tails is 0.2\. The entropy of a single toss of this coin
    is equal to the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果硬币的两面是相同的，那么表示其结果的变量的熵为0比特；也就是说，我们对结果始终确定，变量永远不会代表新信息。熵也可以表示为比特的分数。例如，不公平硬币有两面不同，但其加权使得两面并非等可能出现。假设不公平硬币正面朝上的概率为0.8，反面朝上的概率为0.2。那么该硬币一次掷投的熵等于以下公式：
- en: '![Selecting the questions](img/8365OS_05_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_05.jpg)'
- en: The outcome of a single toss of an unfair coin can have a fraction of one bit
    of entropy. There are two possible outcomes of the toss, but we are not totally
    uncertain since one outcome is more frequent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一次不公平硬币的掷投结果可以有一个比特的分数熵。掷投有两个可能的结果，但我们并不完全不确定，因为其中一个结果更为常见。
- en: 'Let''s calculate the entropy of classifying an unknown animal. If an equal
    number of dogs and cats comprise our animal classification training data and we
    do not know anything else about the animal, the entropy of the decision is equal
    to one. All we know is that the animal could be either a cat or a dog; like the
    fair coin toss, both outcomes are equally likely. Our training data, however,
    contains six dogs and eight cats. If we do not know anything else about the unknown
    animal, the entropy of the decision is given by the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算分类一个未知动物的熵。如果我们的动物分类训练数据中狗和猫的数量相等，而我们对该动物一无所知，那么决策的熵为1。我们所知道的只是该动物可能是猫或狗；就像公平的掷硬币一样，两个结果的可能性是相等的。然而，我们的训练数据包含了六只狗和八只猫。如果我们对未知动物一无所知，那么决策的熵可以通过以下公式来表示：
- en: '![Selecting the questions](img/8365OS_05_06.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_06.jpg)'
- en: 'Since cats are more common, we are less uncertain about the outcome. Now let''s
    find the explanatory variable that will be most helpful in classifying the animal;
    that is, let''s find the explanatory variable that reduces the entropy the most.
    We can test the `plays fetch` explanatory variable and divide the training instances
    into animals that play fetch and animals that don''t. This produces the two following
    subsets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于猫更常见，我们对结果的确定性更高。现在让我们找出最有助于分类该动物的解释变量；也就是说，我们要找出能最大程度降低熵的解释变量。我们可以测试`玩接飞盘`这一解释变量，并将训练实例划分为玩接飞盘的动物和不玩接飞盘的动物。这会产生以下两个子集：
- en: '![Selecting the questions](img/8365OS_05_07.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_07.jpg)'
- en: 'Decision trees are often visualized as diagrams that are similar to flowcharts.
    The top box of the previous diagram is the root node; it contains all of our training
    instances and specifies the explanatory variable that will be tested. At the root
    node we have not eliminated any instances from the training set and the entropy
    is equal to approximately 0.985\. The root node tests the `plays fetch` explanatory
    variable. Recall that we converted this Boolean explanatory variable to a binary-valued
    feature. Training instances for which `plays fetch` is equal to zero follow the
    edge to the root''s left child, and training instances for animals that do play
    fetch follow the edge to the root''s right child node. The left child node contains
    a subset of the training data with seven cats and two dogs that do not like to
    play fetch. The entropy at this node is given by the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通常以类似流程图的图示方式呈现。上一个图示的顶端框是根节点；它包含我们所有的训练实例，并指定将要测试的解释变量。在根节点，我们还没有从训练集中排除任何实例，熵值大约等于0.985。根节点测试`plays
    fetch`这一解释变量。回想一下，我们将这个布尔型解释变量转换为二值特征。对于`plays fetch`等于零的训练实例，沿着左子树的边缘走；而对于喜欢玩接球的动物，则沿着右子树的边缘走。左子节点包含一个子集，包含七只不喜欢玩接球的猫和两只狗。该节点的熵值如下所示：
- en: '![Selecting the questions](img/8365OS_05_08.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_08.jpg)'
- en: 'The right child contains a subset with one cat and four dogs that do like to
    play fetch. The entropy at this node is given by the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 右子树包含一个子集，其中有一只猫和四只喜欢玩接球的狗。该节点的熵值如下所示：
- en: '![Selecting the questions](img/8365OS_05_09.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_09.jpg)'
- en: Instead of testing the `plays fetch` explanatory variable, we could test the
    `is grumpy` explanatory variable. This test produces the following tree. As with
    the previous tree, instances that fail the test follow the left edge, and instances
    that pass the test follow the right edge.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以测试`is grumpy`这一解释变量，而不是测试`plays fetch`。此测试产生了以下决策树。与之前的树一样，未通过测试的实例沿左边缘走，已通过测试的实例沿右边缘走。
- en: '![Selecting the questions](img/8365OS_05_10.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_10.jpg)'
- en: 'We could also divide the instances into animals that prefer cat food and animals
    that don''t to produce the following tree:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将实例划分为偏好猫粮的动物和不偏好猫粮的动物，以产生以下决策树：
- en: '![Selecting the questions](img/8365OS_05_11.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![选择问题](img/8365OS_05_11.jpg)'
- en: Information gain
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息增益
- en: 'Testing for the animals that prefer cat food resulted in one subset with six
    cats, zero dogs, and 0 bits of entropy and another subset with two cats, six dogs,
    and 0.811 bits of entropy. How can we measure which of these tests reduced our
    uncertainty about the classification the most? Averaging the entropies of the
    subsets may seem to be an appropriate measure of the reduction in entropy. In
    this example, the subsets produced by the cat food test have the lowest average
    entropy. Intuitively, this test seems to be effective, as we can use it to classify
    almost half of the training instances. However, selecting the test that produces
    the subsets with the lowest average entropy can produce a suboptimal tree. For
    example, imagine a test that produced one subset with two dogs and no cats and
    another subset with four dogs and eight cats. The entropy of the first subset
    is equal to the following (note that the second term is omitted because ![Information
    gain](img/8365OS_05_35.jpg) is undefined):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对偏好猫粮的动物进行测试，结果得到了两个子集，一个子集包含六只猫、零只狗和0比特的熵，另一个子集包含两只猫、六只狗和0.811比特的熵。我们如何衡量这些测试中，哪个减少了我们对分类的不确定性最多呢？对这些子集熵值的平均值似乎是衡量熵减少的一种合适方法。在这个例子中，猫粮测试产生的子集具有最低的平均熵。直观上，这个测试似乎是有效的，因为我们可以用它来分类几乎一半的训练实例。然而，选择产生最低平均熵的子集的测试可能会产生一个次优的决策树。例如，假设有一个测试，产生了一个子集，里面有两只狗和没有猫，另一个子集包含四只狗和八只猫。第一个子集的熵值等于以下（注意第二项被省略，因为![信息增益](img/8365OS_05_35.jpg)未定义）：
- en: '![Information gain](img/8365OS_05_12.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益](img/8365OS_05_12.jpg)'
- en: 'The entropy of the second subset is equal to the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个子集的熵值等于以下值：
- en: '![Information gain](img/8365OS_05_13.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益](img/8365OS_05_13.jpg)'
- en: The average of these subsets' entropies is only 0.459, but the subset containing
    most of the instances has almost one bit of entropy. This is analogous to asking
    specific questions early in Twenty Questions; we could get lucky and win within
    the first few attempts, but it is more likely that we will squander our questions
    without eliminating many possibilities. Instead, we will measure the reduction
    in entropy using a metric called **information gain**. Calculated with the following
    equation, information gain is the difference between the entropy of the parent
    node, ![Information gain](img/8365OS_05_36.jpg), and the weighted average of the
    children nodes' entropies. ![Information gain](img/8365OS_05_37.jpg) is the set
    of instances, and ![Information gain](img/8365OS_05_41.jpg) is the explanatory
    variable under test. ![Information gain](img/8365OS_05_38.jpg) is the value of
    attribute ![Information gain](img/8365OS_05_41.jpg) for instance ![Information
    gain](img/8365OS_05_46.jpg). ![Information gain](img/8365OS_05_39.jpg) is the
    number of instances for which attribute ![Information gain](img/8365OS_05_41.jpg)
    is equal to the value ![Information gain](img/8365OS_05_42.jpg). ![Information
    gain](img/8365OS_05_40.jpg) is the entropy of the subset of instances for which
    the value of the explanatory variable ![Information gain](img/8365OS_05_41.jpg)
    is ![Information gain](img/8365OS_05_42.jpg).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些子集的平均熵值仅为0.459，但包含大部分实例的子集几乎有一位的熵。这类似于在“二十个问题”中早早问一些具体问题；我们可能会幸运地在前几次就猜中，但更有可能的是我们会浪费问题而没有排除很多可能性。相反，我们将使用一种叫做**信息增益**的度量来衡量熵的减少。通过以下公式计算，信息增益是父节点的熵值与子节点熵值加权平均值之间的差异。![信息增益](img/8365OS_05_36.jpg)是实例的集合，![信息增益](img/8365OS_05_37.jpg)是正在测试的解释变量，![信息增益](img/8365OS_05_41.jpg)是实例![信息增益](img/8365OS_05_46.jpg)的属性值，![信息增益](img/8365OS_05_39.jpg)是属性![信息增益](img/8365OS_05_41.jpg)值等于![信息增益](img/8365OS_05_42.jpg)的实例数量，![信息增益](img/8365OS_05_40.jpg)是解释变量![信息增益](img/8365OS_05_41.jpg)值为![信息增益](img/8365OS_05_42.jpg)的实例子集的熵值。
- en: '![Information gain](img/8365OS_05_14.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益](img/8365OS_05_14.jpg)'
- en: The following table contains the information gains for all of the tests. In
    this case, the cat food test is still the best, as it increases the information
    gain the most.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格包含了所有测试的信息增益。在这种情况下，猫粮测试仍然是最好的，因为它能最大程度地增加信息增益。
- en: '| Test | Parent''s entropy | Child''s entropy | Child''s entropy | Weighted
    average | IG |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 父节点熵 | 子节点熵 | 子节点熵 | 加权平均 | 信息增益 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **plays fetch?** | 0.9852 | 0.7642 | 0.7219 | 0.7490 * 9/14 + 0.7219 * 5/14
    = 0.7491 | 0.2361 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| **会捡球吗？** | 0.9852 | 0.7642 | 0.7219 | 0.7490 * 9/14 + 0.7219 * 5/14 = 0.7491
    | 0.2361 |'
- en: '| **is grumpy?** | 0.9852 | 0.9183 | 0.8113 | 0.9183 * 6/14 + 0.8113 * 8/14
    = 0.85710.8572 | 0.1280 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| **是不是易怒？** | 0.9852 | 0.9183 | 0.8113 | 0.9183 * 6/14 + 0.8113 * 8/14 = 0.85710.8572
    | 0.1280 |'
- en: '| **favorite food = cat food** | 0.9852 | 0.8113 | 0 | 0.8113 * 8 /14 + 0.0
    * 6/14 = 0.4636 | 0.5216 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物 = 猫粮** | 0.9852 | 0.8113 | 0 | 0.8113 * 8 /14 + 0.0 * 6/14 = 0.4636
    | 0.5216 |'
- en: '| **favorite food = dog food** | 0.9852 | 0.8454 | 0 | 0.8454 * 11/14 + 0.0
    * 3/14 = 0.6642 | 0.3210 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物 = 狗粮** | 0.9852 | 0.8454 | 0 | 0.8454 * 11/14 + 0.0 * 3/14 = 0.6642
    | 0.3210 |'
- en: '| **favorite food = bacon** | 0.9852 | 0.9183 | 0.971 | 0.9183 * 9/14 + 0.9710
    * 5/14 = 0.9371 | 0.0481 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物 = 培根** | 0.9852 | 0.9183 | 0.971 | 0.9183 * 9/14 + 0.9710 * 5/14
    = 0.9371 | 0.0481 |'
- en: 'Now let''s add another node to the tree. One of the child nodes produced by
    the test is a leaf node that contains only cats. The other node still contains
    two cats and six dogs. We will add a test to this node. Which of the remaining
    explanatory variables reduces our uncertainty the most? The following table contains
    the information gains for all of the possible tests:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在树上添加另一个节点。由该测试生成的子节点之一是一个叶子节点，里面只有猫。另一个节点仍然包含两只猫和六只狗。我们将在此节点上添加一个测试。剩下的哪些解释变量能最大程度地减少我们的不确定性？以下表格包含了所有可能测试的信息增益：
- en: '| Test | Parent''s entropy | Child''s entropy | Child''s entropy | Weighted
    average | IG |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 父节点熵 | 子节点熵 | 子节点熵 | 加权平均 | 信息增益 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **plays fetch?** | 0.8113 | 1 | 0 | 1.0 * 4/8 + 0 * 4/8 = 0.5 | 0.3113 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| **会捡球吗？** | 0.8113 | 1 | 0 | 1.0 * 4/8 + 0 * 4/8 = 0.5 | 0.3113 |'
- en: '| **is grumpy?** | 0.8113 | 0 | 1 | 0.0 * 4/8 + 1 * 4/8 = 0.5 | 0.3113 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **是不是易怒？** | 0.8113 | 0 | 1 | 0.0 * 4/8 + 1 * 4/8 = 0.5 | 0.3113 |'
- en: '| **favorite food=dog food** | 0.8113 | 0.9710 | 0 | 0.9710 * 5/8 + 0.0 * 3/8
    = 0.6069 | 0.2044 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物=狗粮** | 0.8113 | 0.9710 | 0 | 0.9710 * 5/8 + 0.0 * 3/8 = 0.6069 |
    0.2044 |'
- en: '| **favorite food=bacon** | 0.8113 | 0 | 0.9710 | 0.0 * 3/8 + 0.9710 * 5/8
    = 0.6069 | 0.2044 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物=培根** | 0.8113 | 0 | 0.9710 | 0.0 * 3/8 + 0.9710 * 5/8 = 0.6069 |
    0.2044 |'
- en: 'All of the tests produce subsets with 0 bits of entropy, but the `is grumpy`
    and `plays fetch` tests produce the greatest information gain. ID3 breaks ties
    by selecting one of the best tests arbitrarily. We will select the `is grumpy`
    test, which splits its parent''s eight instances into a leaf node containing four
    dogs and a node containing two cats and two dogs. The following is a diagram of
    the current tree:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的测试都产生熵为0的子集，但 `性格烦躁` 和 `玩接球` 测试产生了最大的 信息增益。ID3通过任意选择最佳测试来打破平局。我们将选择 `性格烦躁`
    测试，它将父节点的八个实例分割成一个包含四只狗的叶节点和一个包含两只猫和两只狗的节点。以下是当前树的图示：
- en: '![Information gain](img/8365OS_05_15.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益](img/8365OS_05_15.jpg)'
- en: 'We will now select another explanatory variable to test the child node''s four
    instances. The remaining tests, `favorite food=bacon`, `favorite food=dog food`,
    and `plays fetch`, all produce a leaf node containing one dog or cat and a node
    containing the remaining animals. The remaining tests produce equal information
    gains, as shown in the following table:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将选择另一个解释变量来测试子节点的四个实例。剩下的测试项 `最喜欢的食物=培根`、`最喜欢的食物=狗粮` 和 `玩接球`，都会产生一个包含一只狗或一只猫的叶节点，以及一个包含剩余动物的节点。剩下的测试项产生相等的信息增益，如下表所示：
- en: '| Test | Parent''s Entropy | Child Entropy | Child Entropy | Weighted Average
    | Information Gain |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 父节点的熵 | 子节点的熵 | 子节点的熵 | 加权平均 | 信息增益 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **plays fetch?** | 1 | 0.9183 | 0 | 0.688725 | 0.311275 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **玩接球？** | 1 | 0.9183 | 0 | 0.688725 | 0.311275 |'
- en: '| **favorite food=dog food** | 1 | 0.9183 | 0 | 0.688725 | 0.311275 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物=狗粮** | 1 | 0.9183 | 0 | 0.688725 | 0.311275 |'
- en: '| **favorite food=bacon** | 1 | 0 | 0.9183 | 0.688725 | 0.311275 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **最喜欢的食物=培根** | 1 | 0 | 0.9183 | 0.688725 | 0.311275 |'
- en: 'We will arbitrarily select the `plays fetch` test to produce a leaf node containing
    one dog and a node containing two cats and a dog. Two explanatory variables remain;
    we can test for animals that like bacon, or we can test for animals that like
    dog food. Both of the tests will produce the same subsets and create a leaf node
    containing one dog and a leaf node containing two cats. We will arbitrarily choose
    to test for animals that like dog food. The following is a diagram of the completed
    decision tree:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将任意选择 `玩接球` 测试，产生一个包含一只狗的叶节点和一个包含两只猫和一只狗的节点。剩下两个解释变量，我们可以测试喜欢培根的动物，或者可以测试喜欢狗粮的动物。这两项测试会产生相同的子集，并创建一个包含一只狗的叶节点和一个包含两只猫的叶节点。我们将任意选择测试喜欢狗粮的动物。以下是完成的决策树图示：
- en: '![Information gain](img/8365OS_05_16.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益](img/8365OS_05_16.jpg)'
- en: 'Let''s classify some animals from the following test data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对以下测试数据中的一些动物进行分类：
- en: '| Testing instance | Plays fetch | Is grumpy | Favorite food | Species |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 测试实例 | 玩接球 | 性格烦躁 | 最喜欢的食物 | 物种 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | Yes | No | Bacon | Dog |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 否 | 培根 | 狗 |'
- en: '| 2 | Yes | Yes | Dog Food | Dog |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 是 | 是 | 狗粮 | 狗 |'
- en: '| 3 | No | Yes | Dog Food | Cat |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 否 | 是 | 狗粮 | 猫 |'
- en: '| 4 | No | Yes | Bacon | Cat |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 否 | 是 | 培根 | 猫 |'
- en: '| 5 | No | No | Cat food | Cat |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 否 | 否 | 猫粮 | 猫 |'
- en: Let's classify the first animal, which likes to plays fetch, is infrequently
    grumpy, and loves bacon. We will follow the edge to the root node's left child
    since the animal's favorite food is not cat food. The animal is not grumpy, so
    we will follow the edge to the second-level node's left child. This is a leaf
    node containing only dogs; we have correctly classified this instance. To classify
    the third test instance as a cat, we follow the edge to the root node's left child,
    follow the edge to the second-level node's right child, follow the edge to the
    third-level node's left child, and finally follow the edge to the fourth-level
    node's right child.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对第一只动物进行分类，它喜欢玩接球，性格不常烦躁，并且喜欢培根。我们将沿着边缘走到根节点的左子节点，因为这只动物最喜欢的食物不是猫粮。它不烦躁，因此我们将沿着边缘走到第二级节点的左子节点。这里是一个叶节点，只有狗；我们已正确分类该实例。为了将第三个测试实例分类为猫，我们沿着边缘走到根节点的左子节点，再走到第二级节点的右子节点，再走到第三级节点的左子节点，最后走到四级节点的右子节点。
- en: Congratulations! You've constructed a decision tree using the ID3 algorithm.
    Other algorithms can be used to train decision trees. **C4.5** is a modified version
    of ID3 that can be used with continuous explanatory variables and can accommodate
    missing values for features. C4.5 also can **prune** trees. Pruning reduces the
    size of a tree by replacing branches that classify few instances with leaf nodes.
    Used by scikit-learn's implementation of decision trees, **CART** is another learning
    algorithm that supports pruning.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经使用 ID3 算法构建了一个决策树。其他算法也可以用来训练决策树。**C4.5** 是 ID3 的改进版本，能够处理连续的解释变量，并且可以处理缺失的特征值。C4.5
    还可以对树进行**剪枝**。剪枝通过将分类较少实例的分支替换为叶节点来减少树的大小。由 scikit-learn 实现的决策树使用了 **CART**，这是一种支持剪枝的学习算法。
- en: Gini impurity
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基尼不纯度
- en: 'In the previous section, we built a decision tree by creating nodes that produced
    the greatest information gain. Another common heuristic for learning decision
    trees is **Gini impurity**, which measures the proportions of classes in a set.
    Gini impurity is given by the following equation, where ![Gini impurity](img/8365OS_05_45.jpg)
    is the number of classes, ![Gini impurity](img/8365OS_05_44.jpg) is the subset
    of instances for the node, and ![Gini impurity](img/8365OS_05_43.jpg) is the probability
    of selecting an element of class ![Gini impurity](img/8365OS_05_32.jpg) from the
    node''s subset:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们通过创建产生最大信息增益的节点构建了一个决策树。学习决策树的另一个常见启发式方法是**基尼不纯度**，它衡量一个集合中各类别的比例。基尼不纯度由以下公式给出，其中
    ![基尼不纯度](img/8365OS_05_45.jpg) 是类别的数量，![基尼不纯度](img/8365OS_05_44.jpg) 是该节点的实例子集，![基尼不纯度](img/8365OS_05_43.jpg)
    是从节点子集中选择类别 ![基尼不纯度](img/8365OS_05_32.jpg) 的元素的概率：
- en: '![Gini impurity](img/8365OS_05_17.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![基尼不纯度](img/8365OS_05_17.jpg)'
- en: 'Intuitively, Gini impurity is zero when all of the elements of the set are
    the same class, as the probability of selecting an element of that class is equal
    to one. Like entropy, Gini impurity is greatest when each class has an equal probability
    of being selected. The maximum value of Gini impurity depends on the number of
    possible classes, and it is given by the following equation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地讲，当集合中的所有元素都属于同一类别时，基尼不纯度为零，因为选择该类别元素的概率等于一。像熵一样，当每个类别被选择的概率相等时，基尼不纯度最大。基尼不纯度的最大值取决于可能的类别数量，它由以下公式给出：
- en: '![Gini impurity](img/8365OS_05_18.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![基尼不纯度](img/8365OS_05_18.jpg)'
- en: Our problem has two classes, so the maximum value of the Gini impurity measure
    will be equal to one half. scikit-learn supports learning decision trees using
    both information gain and Gini impurity. There are no firm rules to help you decide
    when to use one criterion or the other; in practice, they often produce similar
    results. As with many decisions in machine learning, it is best to compare the
    performances of models trained using both options.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的问题有两个类别，因此基尼不纯度的最大值将等于一半。scikit-learn 支持使用信息增益和基尼不纯度来学习决策树。没有明确的规则来帮助你决定何时使用其中一个标准；在实际应用中，它们通常会产生相似的结果。与机器学习中的许多决策一样，最好比较使用两种选项训练的模型的表现。
- en: Decision trees with scikit-learn
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 训练决策树
- en: 'Let''s use decision trees to create software that can block banner ads on web
    pages. This program will predict whether each of the images on a web page is an
    advertisement or article content. Images that are classified as being advertisements
    could then be hidden using Cascading Style Sheets. We will train a decision tree
    classifier using the *Internet Advertisements Data Set* from [http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements),
    which contains data for 3,279 images. The proportions of the classes are skewed;
    459 of the images are advertisements and 2,820 are content. Decision tree learning
    algorithms can produce biased trees from data with unbalanced class proportions;
    we will evaluate a model on the unaltered data set before deciding if it is worth
    balancing the training data by over- or under-sampling instances. The explanatory
    variables are the dimensions of the image, words from the containing page''s URL,
    words from the image''s URL, the image''s alt text, the image''s anchor text,
    and a window of words surrounding the image tag. The response variable is the
    image''s class. The explanatory variables have already been transformed into feature
    representations. The first three features are real numbers that encode the width,
    height, and aspect ratio of the images. The remaining features encode binary term
    frequencies for the text variables. In the following sample, we will grid search
    for the hyperparameter values that produce the decision tree with the greatest
    accuracy, and then evaluate the tree''s performance on a test set:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用决策树创建一个可以屏蔽网页上横幅广告的软件。该程序将预测网页上每张图像是否是广告或文章内容。被分类为广告的图像可以使用层叠样式表（CSS）隐藏。我们将使用来自[http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements)的*互联网广告数据集*来训练一个决策树分类器，该数据集包含3,279张图像的数据。类别的比例是倾斜的；459张图像是广告，2,820张是内容。决策树学习算法可能会根据不平衡类别比例的数据生成偏向的树；我们将在不修改数据集的情况下评估模型，然后决定是否值得通过过采样或欠采样实例来平衡训练数据。解释变量是图像的维度、包含页面的
    URL 中的词、图像 URL 中的词、图像的 alt 文本、图像的锚文本以及围绕图像标签的词窗口。响应变量是图像的类别。解释变量已经转换为特征表示。前三个特征是实数，表示图像的宽度、高度和宽高比。其余特征表示文本变量的二元词频。在接下来的示例中，我们将使用网格搜索寻找产生最高准确度的决策树超参数值，然后在测试集上评估该树的表现：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First we read the `.csv` file using pandas. The `.csv` does not have a header
    row, so we split the last column containing the response variable''s values from
    the features using its index:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 pandas 读取 `.csv` 文件。该 `.csv` 文件没有标题行，因此我们使用其索引将包含响应变量值的最后一列与特征分开：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We encoded the advertisements as the positive class and the content as the
    negative class. More than one quarter of the instances are missing at least one
    of the values for the image''s dimensions. These missing values are marked by
    whitespace and a question mark. We replaced the missing values with negative one,
    but we could have imputed the missing values; for instance, we could have replaced
    the missing height values with the average height value:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将广告编码为正类，将内容编码为负类。超过四分之一的实例至少缺少一个图像维度的值。这些缺失值用空格和问号标记。我们用负一替换了缺失值，但我们本可以填补缺失值；例如，我们可以用平均高度值替换缺失的高度值：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then split the data into training and test sets:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据拆分为训练集和测试集：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We created a pipeline and an instance of `DecisionTreeClassifier`. Then, we
    set the `criterion` keyword argument to `entropy` to build the tree using the
    information gain heuristic:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个管道和 `DecisionTreeClassifier` 的实例。然后，我们将 `criterion` 关键字参数设置为 `entropy`，以便使用信息增益启发式方法构建树：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we specified the hyperparameter space for the grid search:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定了网格搜索的超参数空间：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We set `GridSearchCV()` to maximize the model''s F1 score:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `GridSearchCV()` 设置为最大化模型的 F1 分数：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The classifier detected more than 80 percent of the ads in the test set, and
    approximately 92 percent of the images that it predicted were ads were truly ads.
    Overall, the performance is promising; in following sections, we will try to modify
    our model to improve its performance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该分类器在测试集中检测到超过 80% 的广告，并且大约 92% 的它预测为广告的图像确实是广告。总体而言，性能非常有前景；在接下来的章节中，我们将尝试修改模型以提高其性能。
- en: Tree ensembles
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树集成
- en: '**Ensemble learning** methods combine a set of models to produce an estimator
    that has better predictive performance than its individual components. A **random
    forest** is a collection of decision trees that have been trained on randomly
    selected subsets of the training instances and explanatory variables. Random forests
    usually make predictions by returning the mode or mean of the predictions of their
    constituent trees; scikit-learn''s implementations return the mean of the trees''
    predictions. Random forests are less prone to overfitting than decision trees
    because no single tree can learn from all of the instances and explanatory variables;
    no single tree can memorize all of the noise in the representation.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成学习**方法将一组模型组合起来，生成一个比其单个组件具有更好预测性能的估计器。**随机森林**是一个由在训练实例和解释变量的随机选择子集上训练的决策树集合组成的模型。随机森林通常通过返回其构成树预测的众数或均值来进行预测；scikit-learn的实现返回树预测的均值。与决策树相比，随机森林更不容易过拟合，因为没有单棵树可以从所有实例和解释变量中学习；没有单棵树可以记住表示中的所有噪声。'
- en: Let's update our ad blocker's classifier to use a random forest. It is simple
    to replace the `DecisionTreeClassifier` using scikit-learn's API; we simply replace
    the object with an instance of `RandomForestClassifier`. Like the previous example,
    we will grid search to find the values of the hyperparameters that produce the
    random forest with the best predictive performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新广告拦截器的分类器，改用随机森林。使用scikit-learn的API，替换`DecisionTreeClassifier`非常简单；我们只需将对象替换为`RandomForestClassifier`的实例。像之前的例子一样，我们将使用网格搜索来找到生成具有最佳预测性能的随机森林的超参数值。
- en: 'First, import the `RandomForestClassifier` class from the `ensemble` module:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从`ensemble`模块导入`RandomForestClassifier`类：
- en: '[PRE7]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, replace the `DecisionTreeClassifier` in the `pipeline` with an instance
    of `RandomForestClassifier` and update the hyperparameter space:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将`pipeline`中的`DecisionTreeClassifier`替换为`RandomForestClassifier`实例，并更新超参数空间：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Replacing the single decision tree with a random forest resulted in a significant
    reduction of the error rate; the random forest improves the precision and recall
    for ads to 0.97 and 0.83.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 用随机森林替代单一决策树显著降低了错误率；随机森林将广告的精确度和召回率分别提高到0.97和0.83。
- en: The advantages and disadvantages of decision trees
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树的优缺点
- en: The compromises associated with using decision trees are different than those
    of the other models we discussed. Decision trees are easy to use. Unlike many
    learning algorithms, decision trees do not require the data to have zero mean
    and unit variance. While decision trees can tolerate missing values for explanatory
    variables, scikit-learn's current implementation cannot. Decision trees can even
    learn to ignore explanatory variables that are not relevant to the task.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树所涉及的妥协与我们讨论的其他模型不同。决策树易于使用。与许多学习算法不同，决策树不要求数据具有零均值和单位方差。虽然决策树可以容忍解释变量的缺失值，但当前scikit-learn的实现无法做到这一点。决策树甚至可以学会忽略与任务无关的解释变量。
- en: Small decision trees can be easy to interpret and visualize with the `export_graphviz`
    function from scikit-learn's `tree` module. The branches of a decision tree are
    conjunctions of logical predicates, and they are easily visualized as flowcharts.
    Decision trees support multioutput tasks, and a single decision tree can be used
    for multiclass classification without employing a strategy like one-versus-all.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 小型决策树可以通过scikit-learn的`tree`模块中的`export_graphviz`函数轻松地进行解释和可视化。决策树的分支是逻辑谓词的结合，它们可以很容易地被可视化为流程图。决策树支持多输出任务，单一决策树可以用于多类分类，而无需采用像一对多这样的策略。
- en: Like the other models we discussed, decision trees are **eager learners**. Eager
    learners must build an input-independent model from the training data before they
    can be used to estimate the values of test instances, but can predict relatively
    quickly once the model has been built. In contrast, **lazy** **learners** such
    as the k-nearest neighbors algorithm defer all generalization until they must
    make a prediction. Lazy learners do not spend time training, but often predict
    slowly compared to eager learners.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们讨论的其他模型一样，决策树是**急切学习者**。急切学习者必须在可以用来估计测试实例的值之前，从训练数据中构建一个与输入无关的模型，但一旦模型构建完成，它们可以相对快速地做出预测。相比之下，**懒惰**
    **学习者**（如k近邻算法）则将所有的泛化推迟到必须做出预测时才进行。懒惰学习者不需要花时间训练，但与急切学习者相比，它们通常预测较慢。
- en: Decision trees are more prone to overfitting than many of the models we discussed,
    as their learning algorithms can produce large, complicated decision trees that
    perfectly model every training instance but fail to generalize the real relationship.
    Several techniques can mitigate over-fitting in decision trees. **Pruning** is
    a common strategy that removes some of the tallest nodes and leaves of a decision
    tree, but it is not currently implemented in scikit-learn. However, similar effects
    can be achieved by setting a maximum depth for the tree or by creating child nodes
    only when the number of training instances they will contain exceeds a threshold.
    The `DecisionTreeClassifier` and `DecisionTreeRegressor` classes provide keyword
    arguments to set these constraints. Creating a random forest can also reduce over-fitting.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树比我们讨论的许多模型更容易过拟合，因为它们的学习算法可能会产生庞大且复杂的决策树，完美地拟合每个训练实例，但却无法概括真实的关系。有几种技术可以缓解决策树中的过拟合问题。**修剪**是一种常见策略，通过去除决策树中一些最高的节点和叶子来减少过拟合，但在
    scikit-learn 中尚未实现这一功能。然而，通过为树设置最大深度，或者仅在它们将包含的训练实例数量超过某个阈值时才创建子节点，也能达到类似效果。`DecisionTreeClassifier`和`DecisionTreeRegressor`类提供了设置这些约束的关键字参数。创建随机森林也可以减少过拟合。
- en: Efficient decision tree learning algorithms like ID3 are **greedy**. They learn
    efficiently by making locally optimal decisions, but are not guaranteed to produce
    the globally optimal tree. ID3 constructs a tree by selecting a sequence of explanatory
    variables to test. Each explanatory variable is selected because it reduces the
    uncertainty in the node more than the other variables. It is possible, however,
    that locally suboptimal tests are required in order to find the globally optimal
    tree.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的决策树学习算法，如 ID3，是**贪心的**。它们通过做出局部最优决策来高效学习，但并不能保证产生全局最优的树。ID3通过选择一系列解释变量来构建树。每个解释变量之所以被选择，是因为它比其他变量更能减少节点的不确定性。然而，为了找到全局最优树，可能需要进行局部次优的测试。
- en: In our toy examples, the size of the tree did not matter since we retained all
    of nodes. In a real application, however, the tree's growth could be limited by
    pruning or similar mechanisms. Pruning trees with different shapes can produce
    trees with different performances. In practice, locally optimal decisions that
    are guided by the information gain or Gini impurity heuristics often result in
    an acceptable decision trees.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，树的大小并不重要，因为我们保留了所有节点。然而，在实际应用中，树的增长可能会受到修剪或类似机制的限制。修剪具有不同形状的树可以产生具有不同性能的树。在实践中，通常由信息增益或基尼不纯度启发式方法指导的局部最优决策往往能产生一个可接受的决策树。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter we learned about simple nonlinear models for classification
    and regression called decision trees. Like the parlor game Twenty Questions, decision
    trees are composed of sequences of questions that examine a test instance. The
    branches of a decision tree terminate in leaves that specify the predicted value
    of the response variable. We discussed how to train decision trees using the ID3
    algorithm, which recursively splits the training instances into subsets that reduce
    our uncertainty about the value of the response variable. We also discussed ensemble
    learning methods, which combine the predictions from a set of models to produce
    an estimator with better predictive performance. Finally, we used random forests
    to predict whether or not an image on a web page is a banner advertisement. In
    the next chapter, we will introduce our first unsupervised learning task: clustering.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了用于分类和回归的简单非线性模型——决策树。就像益智游戏“二十个问题”一样，决策树由一系列用于检查测试实例的问题组成。决策树的分支终止于叶子，叶子指定了响应变量的预测值。我们讨论了如何使用
    ID3 算法训练决策树，该算法通过递归地将训练实例划分为子集，从而减少我们对响应变量值的不确定性。我们还讨论了集成学习方法，它通过结合一组模型的预测，生成具有更好预测性能的估算器。最后，我们使用随机森林预测网页上的图像是否为横幅广告。在下一章中，我们将介绍我们的第一个无监督学习任务：聚类。
