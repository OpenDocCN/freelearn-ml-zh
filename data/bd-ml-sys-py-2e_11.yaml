- en: Chapter 11. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 降维
- en: Garbage in, garbage out—throughout the book, we saw this pattern also holds
    true when applying machine learning methods to training data. Looking back, we
    realize that the most interesting machine learning challenges always involved
    some sort of feature engineering, where we tried to use our insight into the problem
    to carefully crafted additional features that the machine learner hopefully picks
    up.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾进，垃圾出——在本书中，我们看到，当将机器学习方法应用于训练数据时，这一模式同样成立。回顾过去，我们意识到，最有趣的机器学习挑战总是涉及某种特征工程，在这些挑战中，我们尝试利用对问题的洞察力，精心设计额外的特征，期望机器学习器能够识别并利用这些特征。
- en: 'In this chapter, we will go in the opposite direction with dimensionality reduction
    involving cutting away features that are irrelevant or redundant. Removing features
    might seem counter-intuitive at first thought, as more information should always
    be better than less information. Also, even if we had redundant features in our
    dataset, would not the learning algorithm be able to quickly figure it out and
    set their weights to 0? The following are several good reasons that are still
    in practice for trimming down the dimensions as much as possible:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将走向相反的方向，进行降维，去除那些无关或冗余的特征。移除特征乍一看似乎是反直觉的，因为更多的信息通常应该比更少的信息更好。此外，即使我们的数据集中有冗余特征，难道学习算法不会迅速识别并将它们的权重设为0吗？以下是一些仍然适用于尽可能减少维度的合理理由：
- en: Superfluous features can irritate or mislead the learner. This is not the case
    with all machine learning methods (for example, Support Vector Machines love high
    dimensional spaces). However, most of the models feel safer with fewer dimensions.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多余的特征可能会干扰或误导学习器。并非所有机器学习方法都会出现这种情况（例如，支持向量机喜欢高维空间）。然而，大多数模型在较少的维度下感觉更安全。
- en: Another argument against high dimensional feature spaces is that more features
    mean more parameters to tune and a higher risk to overfit.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个反对高维特征空间的理由是，更多的特征意味着更多的参数需要调整，也增加了过拟合的风险。
- en: The data we retrieved to solve our task might have just artificially high dimensionality,
    whereas the real dimension might be small.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为了解决任务而检索的数据可能具有人工高维性，而实际的维度可能很小。
- en: Fewer dimensions = faster training = more parameter variations to try out in
    the same time frame = better end result.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的维度 = 更快的训练 = 在相同的时间框架内可以尝试更多的参数变化 = 更好的最终结果。
- en: Visualization—if we want to visualize the data we are restricted to two or three
    dimensions.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化——如果我们想要可视化数据，我们只能在二维或三维中进行。
- en: So, here we will show how to get rid of the garbage within our data while keeping
    the real valuable part of it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这里我们将展示如何去除数据中的垃圾，同时保留其中真正有价值的部分。
- en: Sketching our roadmap
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制我们的路线图
- en: Dimensionality reduction can be roughly grouped into feature selection and feature
    extraction methods. We already employed some kind of feature selection in almost
    every chapter when we invented, analyzed, and then probably dropped some features.
    In this chapter, we will present some ways that use statistical methods, namely
    correlation and mutual information, to be able to do so in vast feature spaces.
    Feature extraction tries to transform the original feature space into a lower-dimensional
    feature space. This is especially useful when we cannot get rid of features using
    selection methods, but still we have too many features for our learner. We will
    demonstrate this using **principal component analysis** (**PCA**), **linear discriminant
    analysis** (**LDA**), and **multidimensional scaling** (**MDS**).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 降维大致可以分为特征选择和特征提取方法。在本书的几乎每一章中，我们都使用了一种特征选择方法，无论是在发明、分析还是可能丢弃某些特征时。在本章中，我们将介绍一些利用统计方法的方式，即相关性和互信息，来实现这一目标，尤其是在特征空间非常大的情况下。特征提取试图将原始特征空间转换为低维特征空间。这在我们无法通过选择方法去除特征，但仍然拥有过多特征以供学习器使用时尤其有用。我们将通过**主成分分析**（**PCA**）、**线性判别分析**（**LDA**）和**多维尺度分析**（**MDS**）来演示这一点。
- en: Selecting features
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: If we want to be nice to our machine learning algorithm, we provide it with
    features that are not dependent on each other, yet highly dependent on the value
    to be predicted. This means that each feature adds salient information. Removing
    any of the features will lead to a drop in performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想对机器学习算法友好，我们应该提供那些彼此独立、但与待预测值高度相关的特征。这意味着每个特征都提供了显著的信息。移除任何特征都会导致性能下降。
- en: If we have only a handful of features, we could draw a matrix of scatter plots
    (one scatter plot for every feature pair combination). Relationships between the
    features could then be easily spotted. For every feature pair showing an obvious
    dependence, we would then think of whether we should remove one of them or better
    design a newer, cleaner feature out of both.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有少数几个特征，可以绘制一个散点图矩阵（每对特征组合画一个散点图）。特征之间的关系可以很容易地被发现。对于每一对明显依赖的特征，我们就会考虑是否应该去除其中一个，或者更好地设计一个新的、更干净的特征。
- en: Most of the time, however, we have more than a handful of features to choose
    from. Just think of the classification task where we had a bag of words to classify
    the quality of an answer, which would require a 1,000 by 1,000 scatter plot. In
    this case, we need a more automated way to detect overlapping features and to
    resolve them. We will present two general ways to do so in the following subsections,
    namely filters and wrappers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数情况下，我们需要从多个特征中进行选择。想一想分类任务中，我们有一个词袋来分类答案的质量，这将需要一个 1000×1000 的散点图。在这种情况下，我们需要一种更自动化的方法来检测重叠特征并解决它们。我们将在以下小节中介绍两种常见的方法，即过滤器和包装器。
- en: Detecting redundant features using filters
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用过滤器检测冗余特征
- en: 'Filters try to clean up the feature forest independent of any later used machine
    learning method. They rely on statistical methods to find which of the features
    are redundant or irrelevant. In case of redundant features, it keeps only one
    per redundant feature group. Irrelevant features will simply be removed. In general,
    the filter works as depicted in the following workflow:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器试图独立于任何后续使用的机器学习方法清理特征集。它们依赖统计方法来查找哪些特征是冗余的或无关的。在冗余特征的情况下，它只保留每个冗余特征组中的一个。无关的特征则会被直接移除。一般来说，过滤器按以下工作流程进行：
- en: '![Detecting redundant features using filters](img/2772OS_11_06.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![使用过滤器检测冗余特征](img/2772OS_11_06.jpg)'
- en: Correlation
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关性
- en: Using correlation, we can easily see linear relationships between pairs of features.
    In the following graphs, we can see different degrees of correlation, together
    with a potential linear dependency plotted as a red-dashed line (fitted 1-dimensional
    polynomial). The correlation coefficient ![Correlation](img/2772OS_11_14.jpg)
    at the top of the individual graphs is calculated using the common Pearson correlation
    coefficient (Pearson `r` value) by means of the `pearsonr()` function of `scipy.stat`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相关性，我们可以轻松看到特征对之间的线性关系。在以下图表中，我们可以看到不同程度的相关性，并且有一个潜在的线性依赖关系通过红色虚线（拟合的 1 维多项式）显示出来。每个单独图表顶部的相关系数
    ![相关性](img/2772OS_11_14.jpg) 是通过 `scipy.stat` 库的 `pearsonr()` 函数计算的常见 Pearson
    相关系数（Pearson `r` 值）。
- en: 'Given two equal-sized data series, it returns a tuple of the correlation coefficient
    value and the p-value. The p-value describes how likely it is that the data series
    has been generated by an uncorrelated system. In other words, the higher the p-value,
    the less we should trust the correlation coefficient:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个相同大小的数据系列，它返回相关系数值和 p 值的元组。p 值描述了数据系列是否可能由一个不相关的系统生成。换句话说，p 值越高，我们就越不应该信任相关系数：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the first case, we have a clear indication that both series are correlated.
    In the second case, we still have a clearly non-zero ![Correlation](img/2772OS_11_15.jpg)
    value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个案例中，我们明显看到两个系列是相关的。在第二个案例中，我们仍然看到一个明显非零的 ![相关性](img/2772OS_11_15.jpg) 值。
- en: 'However, the p-value of 0.84 tells us that the correlation coefficient is not
    significant and we should not pay too close attention to it. Have a look at the
    following graphs:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，0.84 的 p 值告诉我们，相关系数并不显著，我们不应过于关注它。请查看以下图表：
- en: '![Correlation](img/2772OS_11_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![相关性](img/2772OS_11_01.jpg)'
- en: In the first three cases that have high correlation coefficients, we would probably
    want to throw out either ![Correlation](img/2772OS_11_16.jpg) or ![Correlation](img/2772OS_11_17.jpg)
    because they seem to convey similar, if not the same, information.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前三个具有高相关系数的案例中，我们可能会希望抛弃 ![相关性](img/2772OS_11_16.jpg) 或 ![相关性](img/2772OS_11_17.jpg)，因为它们似乎传递了相似的，甚至是相同的信息。
- en: In the last case, however, we should keep both features. In our application,
    this decision would, of course, be driven by this p-value.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在最后一个案例中，我们应该保留两个特征。在我们的应用中，当然，这个决定将由 p 值来驱动。
- en: 'Although, it worked nicely in the preceding example, reality is seldom nice
    to us. One big disadvantage of correlation-based feature selection is that it
    only detects linear relationships (a relationship that can be modelled by a straight
    line). If we use correlation on a non-linear data, we see the problem. In the
    following example, we have a quadratic relationship:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在前面的例子中表现得很好，现实通常对我们并不友好。基于相关性的特征选择的一个大缺点是，它只能检测线性关系（即可以用直线建模的关系）。如果我们在非线性数据上使用相关性，就会发现问题。在以下的例子中，我们有一个二次关系：
- en: '![Correlation](img/2772OS_11_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![相关性](img/2772OS_11_02.jpg)'
- en: Although, the human eye immediately sees the relationship between X[1] and X[2]
    in all but the bottom-right graph, the correlation coefficient does not. It's
    obvious that correlation is useful to detect linear relationships, but fails for
    everything else. Sometimes, it already helps to apply simple transformations to
    get a linear relationship. For instance, in the preceding plot, we would have
    got a high correlation coefficient if we had drawn X[2] over X[1] squared. Normal
    data, however, does not often offer this opportunity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人眼可以立刻看到X[1]与X[2]之间的关系，除了右下角的图表外，相关系数却看不出这种关系。显然，相关性对于检测线性关系很有用，但对于其他类型的关系却无能为力。有时候，应用简单的变换就能获得线性关系。例如，在前面的图中，如果我们把X[2]绘制在X[1]的平方上，我们会得到一个较高的相关系数。然而，正常数据并不总是能提供这样的机会。
- en: Luckily, for non-linear relationships, mutual information comes to the rescue.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于非线性关系，互信息来拯救我们。
- en: Mutual information
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 互信息
- en: When looking at the feature selection, we should not focus on the type of relationship
    as we did in the previous section (linear relationships). Instead, we should think
    in terms of how much information one feature provides (given that we already have
    another).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行特征选择时，我们不应像在上一节中那样关注关系的类型（线性关系）。相反，我们应该从一个特征提供多少信息的角度来思考（前提是我们已经拥有另一个特征）。
- en: To understand this, let's pretend that we want to use features from `house_size`,
    `number_of_levels`, and `avg_rent_price` feature set to train a classifier that
    outputs whether the house has an elevator or not. In this example, we intuitively
    see that knowing `house_size` we don't need to know `number_of_levels` anymore,
    as it contains, somehow, redundant information. With `avg_rent_price`, it's different
    because we cannot infer the value of rental space simply from the size of the
    house or the number of levels it has. Thus, it would be wise to keep only one
    of them in addition to the average price of rental space.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，我们假设要使用`house_size`（房屋面积）、`number_of_levels`（楼层数）和`avg_rent_price`（平均租金）特征集来训练一个分类器，该分类器判断房屋是否有电梯。在这个例子中，我们直观地认为，知道了`house_size`后，我们不再需要知道`number_of_levels`，因为楼层数在某种程度上包含了冗余信息。而对于`avg_rent_price`，情况不同，因为我们无法仅通过房屋的大小或楼层数来推断租金。因此，最好只保留其中一个特征，外加租金的平均值。
- en: Mutual information formalizes the aforementioned reasoning by calculating how
    much information two features have in common. However, unlike correlation, it
    does not rely on a sequence of data, but on the distribution. To understand how
    it works, we have to dive a bit into information entropy.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息通过计算两个特征之间有多少信息是共享的，来形式化上述推理。然而，与相关性不同，互信息不依赖于数据的顺序，而是依赖于数据的分布。为了理解它是如何工作的，我们需要稍微了解一下信息熵。
- en: 'Let''s assume we have a fair coin. Before we flip it, we will have maximum
    uncertainty as to whether it will show heads or tails, as both have an equal probability
    of 50 percent. This uncertainty can be measured by means of Claude Shannon''s
    information entropy:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个公平的硬币。在我们抛掷硬币之前，关于它是正面还是反面我们将面临最大的未知，因为正反两面的概率各为50%。这种不确定性可以通过Claude
    Shannon的信息熵来度量：
- en: '![Mutual information](img/2772OS_11_18.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_18.jpg)'
- en: 'In our fair coin case, we have two cases: Let ![Mutual information](img/2772OS_11_19.jpg)
    be the case of head and ![Mutual information](img/2772OS_11_20.jpg) the case of
    tail with ![Mutual information](img/2772OS_11_21.jpg).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们公平硬币的例子中，有两种情况：让![互信息](img/2772OS_11_19.jpg)代表正面，![互信息](img/2772OS_11_20.jpg)代表反面，![互信息](img/2772OS_11_21.jpg)。
- en: 'Thus, it concludes to:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，得出结论：
- en: '![Mutual information](img/2772OS_11_22.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_22.jpg)'
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: For convenience, we can also use `scipy.stats.entropy([0.5, 0.5], base=2)`.
    We set the base parameter to `2` to get the same result as earlier. Otherwise,
    the function will use the natural logarithm via `np.log()`. In general, the base
    does not matter (as long as you use it consistently).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们也可以使用`scipy.stats.entropy([0.5, 0.5], base=2)`。我们将base参数设置为`2`，以便得到与之前相同的结果。否则，函数将使用自然对数（通过`np.log()`）。一般来说，基数无关紧要（只要你始终如一地使用它）。
- en: 'Now, imagine we knew upfront that the coin is actually not that fair with heads
    having a chance of 60 percent showing up after flipping:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们事先知道这枚硬币其实并不公平，抛掷后正面朝上的概率是60%：
- en: '![Mutual information](img/2772OS_11_23.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_23.jpg)'
- en: 'We see that this situation is less uncertain. The uncertainty will decrease
    the farther away we get from 0.5 reaching the extreme value of 0 for either 0
    percent or 100 percent of heads showing up, as we can see in the following graph:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这种情况的不确定性较小。随着我们离0.5越来越远，不确定性会减少，达到极端值0，即正面朝上的概率为0%或100%，如以下图表所示：
- en: '![Mutual information](img/2772OS_11_03.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_03.jpg)'
- en: We will now modify entropy ![Mutual information](img/2772OS_11_24.jpg) by applying
    it to two features instead of one in such a way that it measures how much uncertainty
    is removed from X when we learn about Y. Then, we can catch how one feature reduces
    the uncertainty of another.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将熵应用于两个特征而非一个来修改熵！[互信息](img/2772OS_11_24.jpg)，从而衡量当我们了解Y时，X的不确定性减少了多少。然后，我们可以捕捉到一个特征如何减少另一个特征的不确定性。
- en: For example, without having any further information about the weather, we are
    totally uncertain whether it's raining outside or not. If we now learn that the
    grass outside is wet, the uncertainty has been reduced (we will still have to
    check whether the sprinkler had been turned on).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在没有任何关于天气的进一步信息的情况下，我们完全不确定外面是否在下雨。如果我们现在得知外面的草地是湿的，那么不确定性就减少了（我们仍然需要检查洒水器是否开启了）。
- en: 'More formally, mutual information is defined as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，互信息被定义为：
- en: '![Mutual information](img/2772OS_11_25.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_25.jpg)'
- en: This looks a bit intimidating, but is really not more than sums and products.
    For instance, the calculation of ![Mutual information](img/2772OS_11_26.jpg) is
    done by binning the feature values and then calculating the fraction of values
    in each bin. In the following plots, we have set the number of bins to ten.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来有点让人害怕，但其实不过是求和和乘积。例如，计算![互信息](img/2772OS_11_26.jpg)是通过对特征值进行分箱，然后计算每个箱中值的比例。在以下图表中，我们将箱的数量设置为十个。
- en: 'In order to restrict mutual information to the interval [0,1], we have to divide
    it by their added individual entropy, which gives us the normalized mutual information:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将互信息限制在区间[0,1]内，我们必须将其除以它们各自的熵之和，这样就得到了归一化互信息：
- en: '![Mutual information](img/2772OS_11_27.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_27.jpg)'
- en: 'The nice thing about mutual information is that unlike correlation, it does
    not look only at linear relationships, as we can see in the following graphs:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息的一个优点是，与相关性不同，它不仅仅关注线性关系，正如我们在以下图表中所看到的：
- en: '![Mutual information](img/2772OS_11_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_04.jpg)'
- en: 'As we can see, mutual information is able to indicate the strength of a linear
    relationship. The following diagram shows that, it also works for squared relationships:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，互信息能够指示线性关系的强度。下图展示了它同样适用于平方关系：
- en: '![Mutual information](img/2772OS_11_05.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![互信息](img/2772OS_11_05.jpg)'
- en: So, what we would have to do is to calculate the normalized mutual information
    for all feature pairs. For every pair having too high value (we would have to
    determine what this means), we would then drop one of them. In case of regression,
    we could drop this feature that has too low mutual information with the desired
    result value.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们需要做的是计算所有特征对的归一化互信息。对于每对具有过高值的特征（我们需要确定这意味着什么），我们将删除其中一个。在回归的情况下，我们可以删除与目标结果值互信息过低的特征。
- en: This might work for a not too-big set of features. At some point, however, this
    procedure can be really expensive, because the amount of calculation grows quadratically
    (as we are computing the mutual information between feature pairs).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能适用于特征集不太大的情况。然而，到了某个阶段，这个过程可能会非常昂贵，因为计算量会呈二次方增长（因为我们在计算特征对之间的互信息）。
- en: Another huge disadvantage of filters is that they drop features that seem to
    be not useful in isolation. More often than not, there are a handful of features
    that seem to be totally independent of the target variable, yet when combined
    together they rock. To keep these, we need wrappers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器的另一个重大缺点是，它们会丢弃那些在单独使用时似乎没有用的特征。实际上，往往有一些特征看起来与目标变量完全独立，但当它们结合起来时，却非常有效。为了保留这些特征，我们需要使用包装器。
- en: Asking the model about the features using wrappers
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用包装器向模型询问特征
- en: 'While filters can help tremendously in getting rid of useless features, they
    can go only so far. After all the filtering, there might still be some features
    that are independent among themselves and show some degree of dependence with
    the result variable, but yet they are totally useless from the model''s point
    of view. Just think of the following data that describes the XOR function. Individually,
    neither `A` nor `B` would show any signs of dependence on `Y`, whereas together
    they clearly do:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管过滤器可以在去除无用特征方面起到很大作用，但它们的效果也是有限的。经过所有的过滤后，仍然可能会有一些特征彼此独立，并且与结果变量之间有某种程度的依赖性，但从模型的角度来看，它们仍然是完全无用的。试想一下下面描述异或（XOR）函数的数据。单独来看，`A`
    和 `B` 都不会显示出与 `Y` 之间的任何依赖性，而它们一起时却明显存在依赖关系：
- en: '| A | B | Y |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| A | B | Y |'
- en: '| --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 0 | 0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: 'So, why not ask the model itself to give its vote on the individual features?
    This is what wrappers do, as we can see in the following process chart:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不直接让模型本身来为每个特征投票呢？这就是包装器的作用，正如我们在以下过程图中所看到的那样：
- en: '![Asking the model about the features using wrappers](img/2772OS_11_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![使用包装器向模型询问特征](img/2772OS_11_07.jpg)'
- en: Here, we pushed the calculation of feature importance to the model training
    process. Unfortunately (but understandably), feature importance is not determined
    as a binary, but as a ranking value. So, we still have to specify where to make
    the cut, what part of the features are we willing to take, and what part do we
    want to drop?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将特征重要性的计算推送到了模型训练过程中。不幸的是（但可以理解），特征重要性并不是以二进制方式确定的，而是以排名值的形式给出的。因此，我们仍然需要指定切割点，决定我们愿意保留哪些特征，哪些特征我们想要丢弃？
- en: Coming back to scikit-learn, we find various excellent wrapper classes in the
    `sklearn.feature_selection` package. A real workhorse in this field is `RFE`,
    which stands for recursive feature elimination. It takes an estimator and the
    desired number of features to keep as parameters and then trains the estimator
    with various feature sets as long as it has found a subset of features that is
    small enough. The `RFE` instance itself pretends to be like an estimator, thereby,
    indeed, wrapping the provided estimator.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 scikit-learn，我们可以在 `sklearn.feature_selection` 包中找到多种优秀的包装器类。在这个领域中，一个非常强大的工具是
    `RFE`，即递归特征消除。它接收一个估算器和要保留的特征数量作为参数，然后用各种特征集训练估算器，直到它找到一个足够小的特征子集。`RFE` 实例本身看起来就像一个估算器，实际上就是包装了提供的估算器。
- en: 'In the following example, we create an artificial classification problem of
    100 samples using datasets'' convenient `make_classification()` function. It lets
    us specify the creation of 10 features, out of which only three are really valuable
    to solve the classification problem:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用数据集方便的 `make_classification()` 函数创建了一个人工分类问题，包含 100 个样本。该函数允许我们指定创建
    10 个特征，其中只有 3 个特征对于解决分类问题是非常有价值的：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The problem in real-world scenarios is, of course, how can we know the right
    value for `n_features_to_select`? Truth is, we can't. However, most of the time
    we can use a sample of the data and play with it using different settings to quickly
    get a feeling for the right ballpark.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的问题当然是，如何知道 `n_features_to_select` 的正确值呢？事实上，我们无法得知这个值。然而，大多数时候我们可以利用数据的一个样本，并通过不同的设置进行尝试，快速感受出大致的合适范围。
- en: 'The good thing is that we don''t have to be that exact using wrappers. Let''s
    try different values for `n_features_to_select` to see how `support_` and `ranking_`
    change:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们在使用包装器时不必非常精确。让我们尝试不同的 `n_features_to_select` 值，看看 `support_` 和 `ranking_`
    是如何变化的：
- en: '| n_features_to_select | support_ | ranking_ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| n_features_to_select | support_ | ranking_ |'
- en: '| --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | [False False False True False False False False False False] | [ 6 3
    5 1 10 7 9 8 2 4] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1 | [False False False True False False False False False False] | [ 6 3
    5 1 10 7 9 8 2 4] |'
- en: '| 2 | [False False False True False False False False True False] | [5 2 4
    1 9 6 8 7 1 3] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 2 | [False False False True False False False False True False] | [5 2 4
    1 9 6 8 7 1 3] |'
- en: '| 3 | [False True False True False False False False True False] | [4 1 3 1
    8 5 7 6 1 2] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 3 | [假 真 假 真 假 假 假 假 真 假] | [4 1 3 1 8 5 7 6 1 2] |'
- en: '| 4 | [False True False True False False False False True True] | [3 1 2 1
    7 4 6 5 1 1] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 4 | [假 真 假 真 假 假 假 假 真 真] | [3 1 2 1 7 4 6 5 1 1] |'
- en: '| 5 | [False True True True False False False False True True] | [2 1 1 1 6
    3 5 4 1 1] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 5 | [假 真 真 真 假 假 假 假 真 真] | [2 1 1 1 6 3 5 4 1 1] |'
- en: '| 6 | [ True True True True False False False False True True] | [1 1 1 1 5
    2 4 3 1 1] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 6 | [ 真 真 真 真 假 假 假 假 真 真] | [1 1 1 1 5 2 4 3 1 1] |'
- en: '| 7 | [ True True True True False True False False True True] | [1 1 1 1 4
    1 3 2 1 1] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 7 | [ 真 真 真 真 假 真 假 假 真 真] | [1 1 1 1 4 1 3 2 1 1] |'
- en: '| 8 | [ True True True True False True False True True True] | [1 1 1 1 3 1
    2 1 1 1] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 8 | [ 真 真 真 真 假 真 假 真 真 真] | [1 1 1 1 3 1 2 1 1 1] |'
- en: '| 9 | [ True True True True False True True True True True] | [1 1 1 1 2 1
    1 1 1 1] |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 9 | [ 真 真 真 真 假 真 真 真 真 真] | [1 1 1 1 2 1 1 1 1 1] |'
- en: '| 10 | [ True True True True True True True True True True] | [1 1 1 1 1 1
    1 1 1 1] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 10 | [ 真 真 真 真 真 真 真 真 真 真] | [1 1 1 1 1 1 1 1 1 1] |'
- en: We see that the result is very stable. Features that have been used when requesting
    smaller feature sets keep on getting selected when letting more features in. At
    last, we rely on our train/test set splitting to warn us when we go the wrong
    way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到结果非常稳定。当请求较小特征集时使用的特征，在允许更多特征进入时依然会被选择。最终，我们依赖于训练/测试集的划分，提醒我们何时走偏。
- en: Other feature selection methods
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他特征选择方法
- en: There are several other feature selection methods that you will discover while
    reading through machine learning literature. Some even don't look like being a
    feature selection method because they are embedded into the learning process (not
    to be confused with the aforementioned wrappers). Decision trees, for instance,
    have a feature selection mechanism implanted deep in their core. Other learning
    methods employ some kind of regularization that punishes model complexity, thus
    driving the learning process towards good performing models that are still "simple".
    They do this by decreasing the less impactful features importance to zero and
    then dropping them (L1-regularization).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读机器学习文献时，你会发现还有其他几种特征选择方法。有些方法甚至看起来不像特征选择方法，因为它们被嵌入到学习过程中（不要与前面提到的包装器方法混淆）。例如，决策树在其核心中深深植入了特征选择机制。其他学习方法则采用某种正则化，惩罚模型复杂度，从而推动学习过程向着表现良好的“简单”模型发展。它们通过将影响较小的特征重要性降为零，然后将其丢弃（L1正则化）来实现这一点。
- en: So watch out! Often, the power of machine learning methods has to be attributed
    to their implanted feature selection method to a great degree.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，要小心！通常，机器学习方法的强大功能很大程度上要归功于它们植入的特征选择方法。
- en: Feature extraction
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: At some point, after we have removed redundant features and dropped irrelevant
    ones, we, often, still find that we have too many features. No matter what learning
    method we use, they all perform badly and given the huge feature space we understand
    that they actually cannot do better. We realize that we have to cut living flesh;
    we have to get rid of features, for which all common sense tells us that they
    are valuable. Another situation when we need to reduce the dimensions and feature
    selection does not help much is when we want to visualize data. Then, we need
    to have at most three dimensions at the end to provide any meaningful graphs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些时候，在我们移除冗余特征并丢弃不相关特征后，通常仍然会发现特征过多。无论使用什么学习方法，它们的表现都很差，并且考虑到庞大的特征空间，我们理解它们实际上做得更好是不可行的。我们意识到必须“割肉”，必须剔除那些所有常识告诉我们它们是有价值的特征。另一个需要减少维度而特征选择方法帮助不大的情况是当我们想要可视化数据时。此时，我们需要最终只有最多三维，才能提供任何有意义的图表。
- en: Enter feature extraction methods. They restructure the feature space to make
    it more accessible to the model or simply cut down the dimensions to two or three
    so that we can show dependencies visually.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 引入特征提取方法。它们重构特征空间，使其对模型更易接入，或简单地将维度降低到二或三，以便我们可以通过可视化显示依赖关系。
- en: Again, we can distinguish feature extraction methods as being linear or non-linear
    ones. Also, as seen before in the *Selecting features* section, we will present
    one method for each type (principal component analysis as a linear and non-linear
    version of multidimensional scaling). Although, they are widely known and used,
    they are only representatives for many more interesting and powerful feature extraction
    methods.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以将特征提取方法区分为线性方法和非线性方法。另外，正如在 *选择特征* 部分中所见，我们将为每种类型介绍一种方法（主成分分析作为线性方法，以及多维尺度法的非线性版本）。尽管它们被广泛知晓和使用，但它们仅是许多更有趣且强大的特征提取方法的代表。
- en: About principal component analysis
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于主成分分析
- en: '**Principal component analysis** (**PCA**) is often the first thing to try
    out if you want to cut down the number of features and do not know what feature
    extraction method to use. PCA is limited as it''s a linear method, but chances
    are that it already goes far enough for your model to learn well enough. Add to
    this the strong mathematical properties it offers and the speed at which it finds
    the transformed feature space and is later able to transform between original
    and transformed features; we can almost guarantee that it also will become one
    of your frequently used machine learning tools.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）通常是你想要减少特征数量且不知道使用哪种特征提取方法时首先要尝试的内容。PCA 是一种线性方法，因此有其局限性，但它可能已经足够使你的模型学习得足够好。加上它所提供的强大数学性质以及它在找到变换后的特征空间的速度，以及之后在原始特征和变换特征之间的转换速度；我们几乎可以保证，它也会成为你常用的机器学习工具之一。'
- en: 'Summarizing it, given the original feature space, PCA finds a linear projection
    of itself in a lower dimensional space that has the following properties:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，给定原始特征空间，PCA 会在一个低维空间中找到其线性投影，并具有以下特性：
- en: The conserved variance is maximized.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化保留的方差。
- en: The final reconstruction error (when trying to go back from transformed features
    to original ones) is minimized.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化最终的重构误差（即试图从变换后的特征恢复到原始特征时的误差）。
- en: As PCA simply transforms the input data, it can be applied both to classification
    and regression problems. In this section, we will use a classification task to
    discuss the method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PCA只是对输入数据进行变换，它可以同时应用于分类和回归问题。在本节中，我们将使用分类任务来讨论这一方法。
- en: Sketching PCA
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绘制PCA
- en: 'PCA involves a lot of linear algebra, which we do not want to go into. Nevertheless,
    the basic algorithm can be easily described as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 涉及很多线性代数知识，我们不打算深入探讨。然而，基本的算法可以简单地描述如下：
- en: Center the data by subtracting the mean from it.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从数据中减去均值来对数据进行中心化。
- en: Calculate the covariance matrix.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵。
- en: Calculate the eigenvectors of the covariance matrix.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征向量。
- en: If we start with ![Sketching PCA](img/2772OS_11_28.jpg) features, then the algorithm
    will return a transformed feature space again with ![Sketching PCA](img/2772OS_11_28.jpg)
    dimensions (we gained nothing so far). The nice thing about this algorithm, however,
    is that the eigenvalues indicate how much of the variance is described by the
    corresponding eigenvector.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从 ![绘制PCA](img/2772OS_11_28.jpg) 特征开始，那么算法将返回一个变换后的特征空间，依然具有 ![绘制PCA](img/2772OS_11_28.jpg)
    维度（到目前为止我们并没有获得任何新东西）。然而，这个算法的优点在于，特征值表示对应特征向量所描述的方差量。
- en: Let's assume we start with ![Sketching PCA](img/2772OS_11_29.jpg) features and
    we know that our model does not work well with more than ![Sketching PCA](img/2772OS_11_30.jpg)
    features. Then, we simply pick the ![Sketching PCA](img/2772OS_11_30.jpg) eigenvectors
    with the highest eigenvalues.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从 ![绘制PCA](img/2772OS_11_29.jpg) 特征开始，并且我们知道模型在超过 ![绘制PCA](img/2772OS_11_30.jpg)
    特征时表现不好。那么，我们只需选择具有最大特征值的 ![绘制PCA](img/2772OS_11_30.jpg) 特征向量。
- en: Applying PCA
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用PCA
- en: 'Let''s consider the following artificial dataset, which is visualized in the
    following left plot diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下人工数据集，并在左侧的图表中进行可视化：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Applying PCA](img/2772OS_11_08.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![应用PCA](img/2772OS_11_08.jpg)'
- en: 'Scikit-learn provides the `PCA` class in its decomposition package. In this
    example, we can clearly see that one dimension should be enough to describe the
    data. We can specify this using the `n_components` parameter:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了 `PCA` 类在其分解包中。在这个例子中，我们可以清楚地看到，使用一个维度就足够描述数据。我们可以通过 `n_components`
    参数来指定这一点：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Also, here we can use the `fit()` and `transform()` methods of `pca` (or its
    `fit_transform()` combination) to analyze the data and project it in the transformed
    feature space:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这里我们可以使用`pca`的`fit()`和`transform()`方法（或其`fit_transform()`组合）来分析数据，并将其投影到变换后的特征空间：
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we have specified, `Xtrans` contains only one dimension. You can see the
    result in the preceding right plot diagram. The outcome is even linearly separable
    in this case. We would not even need a complex classifier to distinguish between
    both classes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所指定，`Xtrans`仅包含一个维度。你可以在前面的右侧图中看到结果。在这种情况下，结果甚至是线性可分的。我们甚至不需要复杂的分类器来区分这两个类别。
- en: 'To get an understanding of the reconstruction error, we can have a look at
    the variance of the data that we have retained in the transformation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解重构误差，我们可以查看在变换中保留下来的数据的方差：
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This means that after going from two to one dimension, we are still left with
    96 percent of the variance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在从二维降到一维之后，我们仍然保留了96%的方差。
- en: 'Of course, it''s not always this simple. Oftentimes, we don''t know what number
    of dimensions is advisable upfront. In that case, we leave `n_components` parameter
    unspecified when initializing `PCA` to let it calculate the full transformation.
    After fitting the data, `explained_variance_ratio_` contains an array of ratios
    in decreasing order: The first value is the ratio of the basis vector describing
    the direction of the highest variance, the second value is the ratio of the direction
    of the second highest variance, and so on. After plotting this array, we quickly
    get a feel of how many components we would need: the number of components immediately
    before the chart has its elbow is often a good guess.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不总是如此简单。通常情况下，我们并不知道预先应该选择多少个维度。在这种情况下，我们在初始化`PCA`时不指定`n_components`参数，让它计算完整的变换。拟合数据后，`explained_variance_ratio_`包含一个按降序排列的比率数组：第一个值是描述最大方差方向的基向量的比率，第二个值是描述第二大方差方向的比率，依此类推。绘制这个数组后，我们很快就能感觉出需要多少个主成分：图表上肘部之前的成分数通常是一个不错的估计。
- en: Tip
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Plots displaying the explained variance over the number of components is called
    a Scree plot. A nice example of combining a Scree plot with a grid search to find
    the best setting for the classification problem can be found at [http://scikit-learn.sourceforge.net/stable/auto_examples/plot_digits_pipe.html](http://scikit-learn.sourceforge.net/stable/auto_examples/plot_digits_pipe.html).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 显示每个主成分方差解释度的图称为碎石图。将碎石图与网格搜索结合以找到分类问题的最佳设置的一个很好的例子可以在[http://scikit-learn.sourceforge.net/stable/auto_examples/plot_digits_pipe.html](http://scikit-learn.sourceforge.net/stable/auto_examples/plot_digits_pipe.html)找到。
- en: Limitations of PCA and how LDA can help
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA的局限性以及LDA如何提供帮助
- en: Being a linear method, PCA has, of course, its limitations when we are faced
    with data that has non-linear relationships. We won't go into details here, but
    it's sufficient to say that there are extensions of PCA, for example, Kernel PCA,
    which introduces a non-linear transformation so that we can still use the PCA
    approach.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种线性方法，PCA在面对具有非线性关系的数据时，当然也有其局限性。我们在这里不深入讨论，但可以简单地说，PCA有一些扩展方法，例如核PCA，它引入了非线性变换，使我们仍然可以使用PCA方法。
- en: 'Another interesting weakness of PCA, which we will cover here, is when it''s
    being applied to special classification problems. Let''s replace `good = (x1 >
    5) | (x2 > 5)` with `good = x1 > x2` to simulate such a special case and we quickly
    see the problem:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的另一个有趣的弱点是，当它应用于特殊的分类问题时，我们将在这里讨论这一点。让我们将`good = (x1 > 5) | (x2 > 5)`替换为`good
    = x1 > x2`来模拟这种特殊情况，并且我们很快就能看到问题所在：
- en: '![Limitations of PCA and how LDA can help](img/2772OS_11_09.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![PCA的局限性以及LDA如何提供帮助](img/2772OS_11_09.jpg)'
- en: Here, the classes are not distributed according to the axis with the highest
    variance, but the second highest variance. Clearly, PCA falls flat on its face.
    As we don't provide PCA with any cues regarding the class labels, it cannot do
    any better.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，类别并不是按照方差最大的轴进行分布，而是按照第二大方差的轴分布。显然，PCA在这种情况下失效了。由于我们没有为PCA提供任何关于类别标签的提示，它无法做得更好。
- en: '**Linear Discriminant Analysis** (**LDA**) comes to the rescue here. It''s
    a method that tries to maximize the distance of points belonging to different
    classes, while minimizing the distances of points of the same class. We won''t
    give any more details regarding how in particular the underlying theory works,
    just a quick tutorial on how to use it:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）在这里派上了用场。它是一种方法，旨在最大化不同类别之间点的距离，同时最小化同一类别点之间的距离。我们不会详细说明底层理论的具体工作原理，只提供一个如何使用它的简要教程：'
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'That''s all. Note that in contrast to the previous PCA example, we provide
    the class labels to the `fit_transform()` method. Thus, PCA is an unsupervised
    feature extraction method, whereas LDA is a supervised one. The result looks as
    expected:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些。请注意，与之前的PCA示例不同，我们将类别标签提供给了`fit_transform()`方法。因此，PCA是一种无监督特征提取方法，而LDA是有监督的。结果如预期所示：
- en: '![Limitations of PCA and how LDA can help](img/2772OS_11_10.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![PCA的局限性及LDA如何提供帮助](img/2772OS_11_10.jpg)'
- en: So, why then consider PCA at all and not simply use LDA? Well, it's not that
    simple. With an increasing number of classes and fewer samples per class, LDA
    does not look that well any more. Also, PCA seems to be not as sensitive to different
    training sets as LDA. So, when we have to advise which method to use, we can only
    suggest a clear "it depends".
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么还要考虑PCA，而不是直接使用LDA呢？其实，事情并没有那么简单。随着类别数量的增加和每个类别样本的减少，LDA的效果就不那么理想了。此外，PCA似乎对不同训练集的敏感度不如LDA。因此，当我们需要建议使用哪种方法时，我们只能明确地说“取决于具体情况”。
- en: Multidimensional scaling
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多维尺度法
- en: Although, PCA tries to use optimization for retained variance, **multidimensional
    scaling** (**MDS**) tries to retain the relative distances as much as possible
    when reducing the dimensions. This is useful when we have a high-dimensional dataset
    and want to get a visual impression.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PCA试图通过优化保持方差，**多维尺度法**（**MDS**）则尽可能保留相对距离，以减少维度。这在我们处理高维数据集并希望获得可视化印象时非常有用。
- en: 'MDS does not care about the data points themselves; instead, it''s interested
    in the dissimilarities between pairs of data points and interprets these as distances.
    The first thing the MDS algorithm is doing is, therefore, taking all the ![Multidimensional
    scaling](img/2772OS_11_28.jpg) datapoints of dimension ![Multidimensional scaling](img/2772OS_11_31.jpg)
    and calculates a distance matrix using a distance function ![Multidimensional
    scaling](img/2772OS_11_32.jpg), which measures the (most of the time, Euclidean)
    distance in the original feature space:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: MDS不关心数据点本身，而是关注数据点对之间的异质性，并将其解释为距离。因此，MDS算法首先做的事情是，取所有![多维尺度法](img/2772OS_11_28.jpg)的维度![多维尺度法](img/2772OS_11_31.jpg)的数据点，并使用距离函数![多维尺度法](img/2772OS_11_32.jpg)计算距离矩阵，该函数衡量原始特征空间中的（大多数情况下是欧几里得）距离：
- en: '![Multidimensional scaling](img/2772OS_11_33.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![多维尺度法](img/2772OS_11_33.jpg)'
- en: Now, MDS tries to position the individual datapoints in the lower dimensional
    space such that the new distance there resembles the distances in the original
    space as much as possible. As MDS is often used for visualization, the choice
    of the lower dimension is most of the time two or three.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，MDS尝试将各个数据点放置到低维空间中，以使该空间中的新距离尽可能接近原始空间中的距离。由于MDS通常用于可视化，低维度的选择通常是二维或三维。
- en: 'Let''s have a look at the following simple data consisting of three datapoints
    in five-dimensional space. Two of the datapoints are close by and one is very
    distinct and we want to visualize this in three and two dimensions:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下简单的数据，包含三个五维空间中的数据点。两个数据点较为接近，另一个则非常不同，我们想要在三维和二维中可视化这一点：
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the `MDS` class in scikit-learn''s `manifold` package, we first specify
    that we want to transform `X` into a three-dimensional Euclidean space:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`manifold`包中的`MDS`类，我们首先指定希望将`X`转换为三维欧几里得空间：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To visualize it in two dimensions, we would need to set `n_components` accordingly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在二维中可视化，我们需要相应地设置`n_components`。
- en: 'The results can be seen in the following two graphs. The triangle and circle
    are both close together, whereas the star is far away:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以在以下两张图中看到。三角形和圆形点非常接近，而星形点则远离它们：
- en: '![Multidimensional scaling](img/2772OS_11_13.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![多维尺度法](img/2772OS_11_13.jpg)'
- en: 'Let''s have a look at the slightly more complex Iris dataset. We will use it
    later to contrast LDA with PCA. The Iris dataset contains four attributes per
    flower. With the preceding code, we would project it into three-dimensional space
    while keeping the relative distances between the individual flowers as much as
    possible. In the previous example, we did not specify any metric, so `MDS` will
    default to Euclidean. This means that flowers that were "different" according
    to their four attributes should also be far away in the MDS-scaled three-dimensional
    space and flowers that were similar should be near together now, as shown in the
    following diagram:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下稍微复杂一些的鸢尾花数据集。稍后我们将用它来对比LDA和PCA。鸢尾花数据集每朵花包含四个属性。使用之前的代码，我们将其投影到三维空间，同时尽可能保持个别花朵之间的相对距离。在之前的例子中，我们没有指定任何度量，因此`MDS`将默认使用欧几里得距离。这意味着，依据四个属性“不同”的花朵，应该在MDS缩放后的三维空间中保持较远的距离，而相似的花朵则应该聚集在一起，正如下图所示：
- en: '![Multidimensional scaling](img/2772OS_11_11.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![多维缩放](img/2772OS_11_11.jpg)'
- en: 'Reducing the dimensional reduction to three and two dimensions with PCA instead,
    we see the expected bigger spread of the flowers belonging to the same class,
    as shown in the following diagram:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将PCA用于将维度减少到三维和二维后，我们可以看到同一类别的花朵在图中呈现出预期的更大分布，如下图所示：
- en: '![Multidimensional scaling](img/2772OS_11_12.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![多维缩放](img/2772OS_11_12.jpg)'
- en: Of course, using MDS requires an understanding of the individual feature's units;
    maybe we are using features that cannot be compared using the Euclidean metric.
    For instance, a categorical variable, even when encoded as an integer (0= circle,
    1= star, 2= triangle, and so on), cannot be compared using Euclidean (is circle
    closer to star than to triangle?).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用MDS需要理解各个特征的单位；也许我们使用的特征无法通过欧几里得度量来比较。例如，一个类别变量，即使它被编码为整数（0=圆形，1=星形，2=三角形，等等），也无法通过欧几里得距离来比较（圆形与星形比与三角形更接近吗？）。
- en: However, once we are aware of this issue, MDS is a useful tool that reveals
    similarities in our data that otherwise would be difficult to see in the original
    feature space.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦我们意识到这个问题，MDS就是一个有用的工具，它能够揭示数据中的相似性，这在原始特征空间中是难以察觉的。
- en: Looking a bit deeper into MDS, we realize that it's not a single algorithm,
    but rather a family of different algorithms, of which we have used just one. The
    same was true for PCA. Also, in case you realize that neither PCA nor MDS solves
    your problem, just look at the many other manifold learning algorithms that are
    available in the scikit-learn toolkit.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地看MDS，我们意识到它并不是单一的算法，而是不同算法的一家族，而我们只使用了其中一个。PCA也是如此。此外，如果你发现PCA或MDS都无法解决你的问题，可以查看scikit-learn工具包中提供的许多其他流形学习算法。
- en: However, before you get overwhelmed by the many different algorithms, it's always
    best to start with the simplest one and see how far you get with it. Then, take
    the next more complex one and continue from there.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在你被众多不同算法淹没之前，最好的方法是从最简单的一个开始，看看它能带你多远。然后，再尝试下一个更复杂的算法，并从那里继续。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You learned that sometimes you can get rid of complete features using feature
    selection methods. We also saw that in some cases, this is not enough and we have
    to employ feature extraction methods that reveal the real and the lower-dimensional
    structure in our data, hoping that the model has an easier game with it.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你学到了有时可以通过特征选择方法去除完整的特征。我们也看到，在某些情况下，这还不够，我们必须使用特征提取方法来揭示数据中的真实和低维结构，希望模型能够更轻松地处理它。
- en: For sure, we only scratched the surface of the huge body of available dimensionality
    reduction methods. Still, we hope that we got you interested in this whole field,
    as there are lots of other methods waiting for you to be picked up. At the end,
    feature selection and extraction is an art, just like choosing the right learning
    method or training model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们只是触及了可用的维度减少方法这一巨大领域的表面。尽管如此，我们希望能激发你对这个领域的兴趣，因为还有许多其他方法等待你去发掘。最终，特征选择和提取是一门艺术，就像选择合适的学习方法或训练模型一样。
- en: The next chapter covers the use of Jug, a little Python framework to manage
    computations in a way that takes advantage of multiple cores or multiple machines.
    You will also learn about AWS, the Amazon Cloud.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍Jug的使用，这是一个小型的Python框架，用于以多核或多机器的方式管理计算。你还将了解AWS，亚马逊云服务。
