- en: 'Chapter 4: Data Preparation at Scale Using Amazon SageMaker Data Wrangler and
    Processing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：使用 Amazon SageMaker Data Wrangler 和 Processing 进行大规模数据准备
- en: So far, we've identified our dataset and explored both manual and automated
    labeling. Now it's time to turn our attention to preparing the data for training.
    Data scientists are familiar with the steps of feature engineering, such as **scaling
    numeric features**, **encoding categorical features**, and **dimensionality reduction**.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经确定了我们的数据集并探索了手动和自动标记。现在，我们需要将注意力转向为训练准备数据。数据科学家熟悉特征工程步骤，例如**缩放数值特征**、**编码分类特征**和**降维**。
- en: As motivation, let's consider our weather dataset. What if our input dataset
    is imbalanced or not really representative of the data we'll encounter in production?
    Our model will not be as accurate as we'd like, and the consequences can be profound.
    Some facial recognition systems have been trained on datasets weighted toward
    white faces, with distressing consequences ([https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为动机，让我们考虑我们的天气数据集。如果我们的输入数据集不平衡或并不真正代表我们在生产中遇到的数据，会怎样呢？我们的模型可能不会像我们希望的那样准确，后果可能非常严重。一些面部识别系统在偏向白人脸的数据集上进行了训练，产生了令人不安的后果([https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/?web=1&wdLOR=cB09A9880-DF39-442C-A728-B00E70AF1CA9))。
- en: We need to understand what input features are affecting the model. That's important
    from a business standpoint as well as a legal or regulatory standpoint. Consider
    a model that predicts operational outages for an application. Understanding why
    outages happen is perhaps more valuable than predicting when an outage will occur
    – is the problem in our application or due to some external factor such as a network
    hiccup? Then, in some industries such as financial services, we cannot use a model
    without being able to demonstrate that it doesn't violate regulations against
    discriminatory lending, say.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '我们需要了解哪些输入特征正在影响模型。这在商业角度和法律或监管角度都很重要。考虑一个预测应用程序操作中断的模型。了解中断发生的原因可能比预测中断发生的时间更有价值——问题是出在我们的应用程序中，还是由于一些外部因素，如网络故障？然后，在某些行业，如金融服务行业，如果我们不能证明模型没有违反歧视性贷款等规定，我们就不能使用该模型。 '
- en: The smaller version of our dataset (covering 1 month) is about 5 GB of data.
    We can analyze that dataset on a modern workstation without too much difficulty.
    But what about the full dataset, which is closer to 500 GB? If we want to prepare
    the full dataset, we need to work with horizontally scalable cluster computing
    frameworks. Furthermore, activities such as encoding categorical variables can
    take quite some time if we use inefficient processing frameworks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的小版本（涵盖1个月）大约有5 GB的数据。我们可以在现代工作站上分析这个数据集而不会遇到太多困难。但完整的数据集，接近500 GB，怎么办呢？如果我们想准备完整的数据集，我们需要与水平可扩展的集群计算框架合作。此外，如果我们使用低效的处理框架，编码分类变量等活动可能需要相当长的时间。
- en: In this chapter, we'll look at the challenges involved in data preparation when
    processing a large dataset and examining the **SageMaker** features that help
    us with large-scale feature engineering.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨在处理大型数据集时数据准备所面临的挑战，并检查**SageMaker**中帮助我们进行大规模特征工程的功能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Visual data preparation with Data Wrangler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Data Wrangler 进行可视数据准备
- en: Bias detection and explainability with Data Wrangler
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Data Wrangler 进行偏差检测和可解释性
- en: Data preparation at scale with SageMaker Processing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Processing 进行大规模数据准备
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 运行本章中包含的示例需要 AWS 账户。如果您尚未设置数据科学环境，请参阅[*第二章*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*，数据科学环境*，其中将指导您完成设置过程。
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 书中包含的代码示例可在 GitHub 上找到：[https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter04)。您需要安装
    Git 客户端才能访问它们（[https://git-scm.com/](https://git-scm.com/)）。
- en: The code for this chapter is in the `CH04` folder of the GitHub repository.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于 GitHub 仓库的 `CH04` 文件夹中。
- en: Visual data preparation with Data Wrangler
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Data Wrangler 进行可视数据准备
- en: Let's start small with our 1-month dataset. Working with a small dataset is
    a good way to get familiar with the data before diving into more scalable techniques.
    SageMaker Data Wrangler gives us an easy way to construct a data flow, a series
    of data preparation steps powered by a visual interface.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的 1 个月数据集开始小规模操作。在深入研究更可扩展的技术之前，使用小数据集熟悉数据是一个好方法。SageMaker Data Wrangler
    以一种简单的方式为我们提供了构建数据流的方法，这是一系列由可视化界面驱动的数据准备步骤。
- en: In the rest of this section, we'll use Data Wrangler to inspect and transform
    data, and then export the Data Wrangler steps into a reusable flow.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将使用 Data Wrangler 检查和转换数据，然后将 Data Wrangler 步骤导出为可重用的流程。
- en: Data inspection
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据检查
- en: 'Let''s get started with Data Wrangler for data inspection, where we look at
    the properties of our data and determine how to prepare it for model training.
    Begin by adding a new flow in SageMaker Studio; go to the **File** menu, then
    **New**, then **Flow**. After the flow starts up and connects to Data Wrangler,
    we need to import our data. The following screenshot shows the data import step
    in Data Wrangler:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据检查的 Data Wrangler 开始，在这里我们查看数据的属性并确定如何为模型训练做准备。首先在 SageMaker Studio 中添加一个新的流程；转到**文件**菜单，然后选择**新建**，再选择**流程**。在流程启动并连接到
    Data Wrangler 后，我们需要导入我们的数据。以下截图显示了 Data Wrangler 中的数据导入步骤：
- en: '![Figure 4.1 – Import data source in Data Wrangler'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – 在 Data Wrangler 中导入数据源'
- en: '](img/B17249_04_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_01.jpg)'
- en: Figure 4.1 – Import data source in Data Wrangler
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 在 Data Wrangler 中导入数据源
- en: Because our dataset consists of multiple small JSON files scattered in date-partitioned
    folders, we'll use `PrepareData.ipynb` notebook walks you through creating a `Glue`
    database and table and registering the partitions in the section called `Glue
    Catalog`. Once that's done, click on **Athena** to start importing the small dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集由多个散布在日期分区文件夹中的小 JSON 文件组成，我们将使用 `PrepareData.ipynb` 笔记本，它将指导你创建一个 `Glue`
    数据库和表，并在名为 `Glue Catalog` 的部分中注册分区。完成这些后，点击**Athena**开始导入小数据集。
- en: 'On the next screen, specify the database you created in the notebook. Enter
    the following query to import 1 month''s worth of data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一屏幕上，指定你在笔记本中创建的数据库。输入以下查询以导入一个月的数据：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the import step in Data Wrangler:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 Data Wrangler 中的导入步骤：
- en: '![Figure 4.2 – Athena import into Data Wrangler'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – Athena 导入到 Data Wrangler'
- en: '](img/B17249_04_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_02.jpg)'
- en: Figure 4.2 – Athena import into Data Wrangler
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – Athena 导入到 Data Wrangler
- en: Run the query and click on **Import dataset**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 运行查询并点击**导入数据集**。
- en: 'Now we''re ready to perform some analysis and transformation. Click the **+**
    symbol next to the last box in the data flow and select **Add analysis**. You''ll
    now have a screen where you can choose one of the available analyses, as you can
    see in the following screenshot:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备进行一些分析和转换。点击数据流中最后一个框旁边的**+**符号，选择**添加分析**。你现在将有一个屏幕，你可以选择一个可用的分析，如下面的截图所示：
- en: '![Figure 4.3 – Data analysis configuration'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – 数据分析配置'
- en: '](img/B17249_04_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_03.jpg)'
- en: Figure 4.3 – Data analysis configuration
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 数据分析配置
- en: 'Start with a **Table summary** step, which shows some statistical properties
    of numeric features, as you can see in the following screenshot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个**表摘要**步骤开始，它显示了数值特征的某些统计属性，如下面的截图所示：
- en: '![Figure 4.4 – Table summary'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – 表摘要'
- en: '](img/B17249_04_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_04.jpg)'
- en: Figure 4.4 – Table summary
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 表摘要
- en: 'Next, let''s try a scatter plot to help us visualize the distribution of the
    measurement values. Set the `y` axis to `value`, the `x` axis to `aggdate`, color
    by `country`, and facet by `parameter`. We can see in the following preview chart
    that the value for nitrogen dioxide is relatively steady over time, while the
    value for carbon monoxide shows more variability for some countries:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试一个散点图来帮助我们可视化测量值的分布。将 `y` 轴设置为 `value`，`x` 轴设置为 `aggdate`，按 `country`
    着色，并按 `parameter` 分面。我们可以在以下预览图表中看到，二氧化氮的值随时间相对稳定，而一氧化碳的值在一些国家显示出更多的变化性：
- en: '![Figure 4.5 – Scatter plot showing measurement values by date, color-coded
    by country, and faceted'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5 – 通过日期显示测量值，按国家着色，并按参数分面的散点图'
- en: by parameter](img/B17249_04_05.jpg)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过参数](img/B17249_04_05.jpg)
- en: Figure 4.5 – Scatter plot showing measurement values by date, color-coded by
    country, and faceted by parameter
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 通过日期显示测量值，按国家着色，并按参数分面的散点图
- en: Feel free to add more scatter plots or try a histogram. We'll explore the bias
    report and quick mode in the *Bias detection and explainability with Data Wrangler
    and Clarify* section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随意添加更多散点图或尝试直方图。我们将在“使用 Data Wrangler 和 Clarify 进行偏差检测和可解释性”部分探索偏差报告和快速模式。
- en: Now that we've done some basic data inspection, we move on to data transformation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经进行了一些基本的数据检查，我们将继续进行数据转换。
- en: Data transformation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'In this section, we will convert the data from the raw format into a format
    usable for model training. Recall the basic format of our raw data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把原始数据格式转换为可用于模型训练的格式。回顾一下我们原始数据的基本格式：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We''ll perform the following steps using Data Wrangler:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Data Wrangler 执行以下步骤：
- en: Scale numeric values.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模数值。
- en: Encode categorical values.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分类值进行编码。
- en: Add features related to the date (for example, day of the week, day in a month).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加与日期相关的特征（例如，星期几，月份中的某一天）。
- en: Drop unwanted columns (`source name`, `coordinates`, `averaging period`, `attribution`,
    `units`, and `location`). These columns are either redundant (for example, the
    important part of the location is in the city and country columns) or not usable
    as features.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除不需要的列（`source name`、`coordinates`、`averaging period`、`attribution`、`units`
    和 `location`）。这些列要么是冗余的（例如，位置的重要部分在城市和国家列中），要么不能作为特征使用。
- en: 'Back to the *Preparation* part of the flow, click the **+** symbol next to
    the last box in the data flow panel and select **Add Transform**. You''ll see
    a preview of the dataset and a list of the available transforms as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 返回流程的 *准备* 部分，点击数据流面板中最后一个框旁边的 **+** 符号，并选择 **添加转换**。您将看到数据集的预览和可用转换的列表，如下所示：
- en: '![Figure 4.6 – Data transformations in Data Wrangler'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – Data Wrangler 中的数据转换'
- en: '](img/B17249_04_06.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_06.jpg)'
- en: Figure 4.6 – Data transformations in Data Wrangler
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – Data Wrangler 中的数据转换
- en: 'For our first transformation, select `sourcetype` as the column, set **Output
    Style** to **Columns**, and add a prefix for the new column names:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一次转换，选择 `sourcetype` 作为列，将 **输出样式** 设置为 **列**，并为新列名添加前缀：
- en: '![Figure 4.7 – One-hot encoding in Data Wrangler'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – Data Wrangler 中的独热编码'
- en: '](img/B17249_04_07.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_07.jpg)'
- en: Figure 4.7 – One-hot encoding in Data Wrangler
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – Data Wrangler 中的独热编码
- en: When you're done setting up the transformation, click **Preview** and then **Add**
    to add the transform. You can now add additional transformations to drop the unwanted
    columns, scale the numeric columns, and featurize the date. You can also provide
    your own custom code if you like.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成设置转换后，点击 **预览**，然后点击 **添加** 以添加转换。现在您可以添加额外的转换来删除不需要的列，缩放数值列，并特征化日期。如果您愿意，还可以提供自己的自定义代码。
- en: Exporting the flow
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出流程
- en: Data Wrangler is very handy when we want to quickly explore a dataset. But we
    can also export the results of a flow into Amazon SageMaker **Feature Store**,
    generate a **SageMaker pipeline**, create a Data Wrangler job, or generate **Python**
    code. We will not use these capabilities now, but feel free to experiment with
    them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要快速探索数据集时，Data Wrangler 非常方便。但我们也可以将流程的结果导出到 Amazon SageMaker **特征存储库**，生成
    **SageMaker 管道**，创建 Data Wrangler 作业，或生成 **Python** 代码。我们现在不会使用这些功能，但请随意尝试它们。
- en: Bias detection and explainability with Data Wrangler and Clarify
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Data Wrangler 和 Clarify 进行偏差检测和可解释性
- en: Now that we've done some initial work in exploring and preparing our data, let's
    do a sanity check on our input data. While bias can mean many things, one particular
    symptom is a dataset that has many more samples of one type of data than another,
    which will affect our model's performance. We'll use Data Wrangler to see if our
    input data is imbalanced and understand which features are most important to our
    model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对数据进行了初步的探索和准备之后，现在让我们对我们的输入数据进行一次合理性检查。虽然偏差可以意味着许多事情，但一个特定的症状是数据集中一种类型的数据样本比另一种类型多得多，这将影响我们模型的表现。我们将使用
    Data Wrangler 来查看我们的输入数据是否不平衡，并了解哪些特征对我们模型最重要。
- en: To begin, add an analysis to the flow. Choose `mobile` column as the label,
    with `1` as the predicted value. Choose `city` as the column to use for bias analysis,
    then click **Check for bias**. In this scenario, we want to determine whether
    our dataset is somehow imbalanced with respect to the city and whether the data
    was collected at a mobile station. If the quality of data from mobile sources
    is inferior to non-mobile sources, it'd be good to know if the mobile sources
    are unevenly distributed among cities.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，向流程中添加一个分析。选择`mobile`列作为标签，`1`作为预测值。选择`city`列用于偏差分析，然后点击**检查偏差**。在这种情况下，我们想确定我们的数据集是否在某种程度上与城市不平衡，以及数据是否在移动站收集。如果来自移动源的数据质量低于非移动源，了解移动源在城市间的分布是否均匀将是有益的。
- en: Next, we'll examine **feature importance**. Feature importance is one aspect
    of model explainability. We want to understand which parts of the dataset are
    most important to model behavior. Another aspect, which we'll visit in [*Chapter
    11*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*, Monitoring Production Models
    with Amazon SageMaker Model Monitor and Clarify*, in the *Monitor bias drift and
    feature importance drift using Amazon SageMaker Clarify* section, is understanding
    which features contributed to a specific inference.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查**特征重要性**。特征重要性是模型可解释性的一个方面。我们想了解数据集中哪些部分对模型行为最重要。另一个方面，我们将在[*第 11
    章*](B17249_11_Final_JM_ePub.xhtml#_idTextAnchor210)*，使用 Amazon SageMaker Model
    Monitor 和 Clarify 监控生产模型*部分探讨，是了解哪些特征对特定推理做出了贡献。
- en: 'Add another analysis in the last step of the flow. Select `value` column (Data
    Wrangler will infer that this is a regression problem). Preview and create the
    analysis. You should see a screen that looks similar to the following screenshot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在流程的最后一步添加另一个分析。选择`value`列（Data Wrangler 将推断这是一个回归问题）。预览并创建分析。您应该看到一个类似于以下屏幕截图的屏幕：
- en: '![Figure 4.8 – Feature importance generated by Data Wrangler'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – Data Wrangler 生成的特征重要性'
- en: '](img/B17249_04_08.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_04_08.jpg)'
- en: Figure 4.8 – Feature importance generated by Data Wrangler
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – Data Wrangler 生成的特征重要性
- en: This analysis generates a random forest model, evaluates performance using a
    test set with 30% of the data, and calculates a **Gini importance score** for
    each feature. As you can see in *Figure 4.8*, the city and day of the month are
    the most important features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此分析生成一个随机森林模型，使用包含 30% 数据的测试集评估性能，并为每个特征计算一个**Gini 重要性分数**。如图 4.8 所示，城市和月份是最重要的特征。
- en: So far we've used Data Wrangler for visual inspection and transformation. Now,
    we'll look at how to handle larger datasets using SageMaker Processing.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用 Data Wrangler 进行了视觉检查和转换。现在，我们将探讨如何使用 SageMaker Processing 处理更大的数据集。
- en: Data preparation at scale with SageMaker Processing
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Processing 进行大规模数据准备
- en: Now let's turn our attention to preparing the entire dataset. At 500 GB, it's
    too large to process using `sklearn` on a single EC2 instance. We will write a
    SageMaker processing job that uses **Spark ML** for data preparation. (Alternatively,
    you can use **Dask**, but at the time of writing, SageMaker Processing does not
    provide a Dask container out of the box.)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向准备整个数据集。由于有 500 GB，它太大，无法使用单个 EC2 实例上的 `sklearn` 进行处理。我们将编写一个使用
    **Spark ML** 进行数据准备的 SageMaker 处理作业。（或者，您也可以使用 **Dask**，但在撰写本文时，SageMaker Processing
    并未提供开箱即用的 Dask 容器。）
- en: The `Processing Job` part of this chapter's notebook walks you through launching
    the processing job. Note that we'll use a cluster of 15 EC2 instances to run the
    job (if you need limits raised, you can contact AWS support).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本章笔记本的`Processing Job`部分将指导您启动处理作业。请注意，我们将使用 15 个 EC2 实例的集群来运行作业（如果您需要提高限制，可以联系
    AWS 支持）。
- en: Also note that up until now, we've been working with the uncompressed JSON version
    of the data. This format containing thousands of small JSON files is not ideal
    for Spark processing as the `OpenAQ` dataset also includes a `gzip` is not a preferred
    compression format as it is not splittable; if you have a choice, use the Snappy
    compression format.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，到目前为止，我们一直在使用数据的未压缩JSON版本。这种包含数千个小JSON文件的格式对于Spark处理来说并不理想，因为`OpenAQ`数据集还包括一个`gzip`，它不是一个首选的压缩格式，因为它不可分割；如果您有选择，请使用Snappy压缩格式。
- en: 'We will use the gzipped Parquet version of our data for the larger data preparation
    job:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们数据的gzip Parquet版本进行较大的数据准备工作：
- en: 'First, we will define the `processor` class, using `7200` seconds (2 hours).
    Two hours is more than sufficient to process at least one of the 8 tables in the
    Parquet dataset. If you want to process all eight of them, change the timeout
    to 3 hours and make an adjustment in the `preprocess.py` script:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义`processor`类，使用`7200`秒（2小时）。两小时足以处理Parquet数据集中的至少一张表。如果您想处理所有八张表，请将超时时间更改为3小时，并在`preprocess.py`脚本中进行调整：
- en: '[PRE2]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we''ll set the Spark configuration, following the formulas defined in
    an EMR blog ([https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/](https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/)):'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置Spark配置，遵循EMR博客中定义的公式（[https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/](https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/))：
- en: '[PRE3]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we''ll launch the job. We need to include a JSON `serde` class:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将启动作业。我们需要包含一个JSON `serde`类：
- en: '[PRE4]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The processing script, `CH04/scripts/preprocess.py`, walks through several steps,
    which we'll explain in the subsequent sections.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 处理脚本`CH04/scripts/preprocess.py`会经过几个步骤，我们将在后续章节中解释。
- en: Loading the dataset
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'We will load one or more of the Parquet table sets from S3\. If you want to
    process more than one, modify the `get_tables` function to return more table names
    in the list as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从S3加载一个或多个Parquet表集。如果您想处理多个，请修改`get_tables`函数以返回列表中的更多表名，如下所示：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The next step in the processing script is dropping unnecessary columns from
    the dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 处理脚本中的下一步是删除数据集中的不必要的列。
- en: Drop columns
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除列
- en: 'We''ll repeat most of the steps we did in Data Wrangler using **PySpark**.
    We need to drop some columns that we don''t want, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重复在Data Wrangler中执行的大部分步骤，使用**PySpark**。我们需要删除一些我们不想要的列，如下所示：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Converting data types
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换数据类型
- en: 'We''ll convert the `mobile` field to an integer:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`mobile`字段转换为整数：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Scaling numeric fields
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放数值字段
- en: 'We''ll use the Spark ML standard scaler to transform the `value` field:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Spark ML标准缩放器来转换`value`字段：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Featurizing the date
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征化日期
- en: 'The date by itself isn''t that useful, so we''ll extract several new features
    from it indicating the day, month, quarter, and year:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 日期本身并不那么有用，因此我们将从中提取几个新特征，表示日期、月份、季度和年份：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Simulating labels for air quality
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟空气质量标签
- en: 'Although we used ground truth in [*Chapter 3*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*,
    Data Labeling with Amazon SageMaker Ground Truth*, for labeling, for the sake
    of this demonstration we''ll use a simple heuristic to assign these labels instead:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在[*第3章*](B17249_03_Final_JM_ePub.xhtml#_idTextAnchor052)*，使用Amazon SageMaker
    Ground Truth进行数据标注*中使用了地面真实数据，但为了演示的目的，我们将使用一个简单的启发式方法来分配这些标签：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Encoding categorical variables
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码分类变量
- en: 'Now we''ll encode the categorical features. Most of these features have fairly
    high cardinality, so we''ll perform ordinal encoding here and learn embeddings
    later in our training process. We will only use one-hot encoding for the parameter,
    which only has seven possible choices:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对分类特征进行编码。这些特征中的大多数都具有相当高的基数，因此我们将在这里执行顺序编码，并在训练过程中稍后学习嵌入。我们只将对参数使用独热编码，它只有七个可能的选择：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Splitting and saving the dataset
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割和保存数据集
- en: 'After some final cleanup of the dataset, we can split the dataset into train,
    validation, and test sets, and save them to S3:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集进行一些最后的清理后，我们可以将数据集分为训练集、验证集和测试集，并将它们保存到S3：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this section, we saw how to use a SageMaker Processing job to perform data
    preparation on a larger dataset using Apache Spark. In the field, many datasets
    are large enough to require a distributed processing framework, and now you understand
    how to integrate a Spark job into your SageMaker workflow.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用SageMaker Processing作业在更大的数据集上使用Apache Spark进行数据准备。在实际应用中，许多数据集足够大，需要分布式处理框架，现在你了解了如何将Spark作业集成到你的SageMaker工作流程中。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we tackled feature engineering for a large (~ 500 GB) dataset.
    We looked at challenges including scalability, bias, and explainability. We saw
    how to use SageMaker Data Wrangler, Clarify, and Processing jobs to explore and
    prepare data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们处理了一个大型（约500 GB）数据集的特征工程问题。我们探讨了包括可扩展性、偏差和可解释性在内的挑战。我们了解了如何使用SageMaker
    Data Wrangler、Clarify和Processing作业来探索和准备数据。
- en: While there are many ways to use these tools, we recommend using Data Wrangler
    for interactive exploration of small to mid-sized datasets. For processing large
    datasets in their entirety, switch to programmatic use of processing jobs using
    the Spark framework to take advantage of parallel processing. (At the time of
    writing, Data Wrangler does not support running on multiple instances, but you
    can run a processing job on multiple instances.) You can always export a Data
    Wrangler flow as a starting point.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有多种使用这些工具的方法，但我们建议使用Data Wrangler进行小到中等规模数据集的交互式探索。对于处理整个大型数据集，切换到使用Spark框架的编程方式来利用并行处理。
    （在撰写本文时，Data Wrangler不支持在多个实例上运行，但你可以在多个实例上运行一个处理作业。）你始终可以将Data Wrangler流程导出作为起点。
- en: If your dataset is many terabytes, consider running a Spark job directly in
    **EMR** or Glue and invoking SageMaker using the SageMaker Spark SDK. EMR and
    Glue have optimized Spark runtimes and more efficient integration with S3 storage.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集达到数个TB，考虑直接在**EMR**或Glue上运行Spark作业，并使用SageMaker Spark SDK调用SageMaker。EMR和Glue对Spark运行时进行了优化，并与S3存储有更高效的集成。
- en: At this point, we have our data ready for model training. In the next chapter,
    we'll explore using Amazon SageMaker Feature Store to help us manage prepared
    feature data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的数据已经准备好用于模型训练。在下一章中，我们将探讨使用Amazon SageMaker Feature Store来帮助我们管理准备好的特征数据。
