- en: Recommending Movies with Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras进行电影推荐
- en: Recommendation systems are an invaluable tool. They are able to increase both
    customer experience and a company's profitability. Such systems work by recommending
    items that users will probably like, based on other items they have already liked.
    For example, when shopping for a smartphone on Amazon, accessories for that specific
    smartphone will be recommended. This improves the customer's experience (as they
    do not need to search for accessories), while it also increases Amazon's profits
    (for example, if the user did not know that there are accessories available for
    sale).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一种宝贵的工具。它们能够提升客户体验并增加公司的盈利能力。此类系统通过基于用户已喜欢的其他物品，推荐用户可能喜欢的物品。例如，在亚马逊上购买智能手机时，系统会推荐该手机的配件。这样既提高了客户体验（因为他们无需再寻找配件），也增加了亚马逊的盈利（例如，如果用户并不知道有配件在售）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Demystifying recommendation systems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解密推荐系统
- en: Neural recommendation systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络推荐系统
- en: Using Keras for movie recommendations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras进行电影推荐
- en: In this chapter, we will utilize the MovieLens dataset (available at [http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip))
    in order to create a movie recommendation system using the Keras deep learning
    framework and ensemble learning techniques.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用MovieLens数据集（可在[http://files.grouplens.org/datasets/movielens/ml-latest-small.zip](http://files.grouplens.org/datasets/movielens/ml-latest-small.zip)下载），利用Keras深度学习框架和集成学习技术创建一个电影推荐系统。
- en: 'We would like to thank the GroupLens members for giving us permission to use
    their data in this book. For more information about the data, please read the
    following relevant paper:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢GroupLens团队授权我们在本书中使用他们的数据。有关数据的更多信息，请阅读以下相关论文：
- en: 'F. Maxwell Harper and Joseph A. Konstan. 2015. *The MovieLens Datasets: History
    and Context*. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4,
    Article 19 (December 2015), 19 pages.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'F. Maxwell Harper 和 Joseph A. Konstan. 2015. *The MovieLens Datasets: History
    and Context*. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4,
    Article 19 (2015年12月)，第19页。'
- en: '[The paper is available at: http://dx.doi.org/10.1145/2827872](http://dx.doi.org/10.1145/2827872)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文可在以下链接获取：http://dx.doi.org/10.1145/2827872](http://dx.doi.org/10.1145/2827872)'
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，了解Python的约定和语法也是必需的。最后，熟悉NumPy库将大大帮助读者理解一些自定义算法实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter12](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter12)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter12](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter12)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2NXZqVE](http://bit.ly/2NXZqVE).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码是如何执行的：[http://bit.ly/2NXZqVE](http://bit.ly/2NXZqVE)。
- en: Demystifying recommendation systems
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密推荐系统
- en: 'Although the inner workings of recommendation systems may seem intimidating
    at first, they are actually quite intuitive. Let''s take an example of various
    movies and users. Each user has the option to rate a movie on a scale of 1 to
    5\. The recommendation system will try to find users with similar preferences
    to a new user, and will then recommend movies that the new user will probably
    like, as similar users also like them. Let''s take the following simple example,
    consisting of four users and six movies:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管推荐系统的内部机制一开始看起来可能令人畏惧，但其实它们非常直观。让我们以一些电影和用户为例。每个用户可以根据1到5的评分标准评价电影。推荐系统会尝试找到与新用户兴趣相似的其他用户，并根据这些相似用户喜欢的电影，向新用户推荐可能喜欢的电影。我们来看一个简单的例子，包含四个用户和六部电影：
- en: '| **User** | **Interstellar** | **2001: A Space Odyssey** | **The Matrix**
    | **Full Metal Jacket** | **Jarhead** | **Top Gun** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **用户** | **星际穿越** | **2001太空漫游** | **黑客帝国** | **全金属外壳** | **海湾战争** | **壮志凌云**
    |'
- en: '| U0 | 5 | 4 |  | 2 | 1 |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| U0 | 5 | 4 |  | 2 | 1 |  |'
- en: '| U1 |  | 1 |  | 4 | 4 | 3 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| U1 |  | 1 |  | 4 | 4 | 3 |'
- en: '| U2 | 4 |  | 4 |  |  | 1 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| U2 | 4 |  | 4 |  |  | 1 |'
- en: '| U3 |  | 4 | 5 | 5 | 4 |  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| U3 |  | 4 | 5 | 5 | 4 |  |'
- en: Ratings for each movie from each user
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每部电影每个用户的评分
- en: 'As is evident, each user has rated a number of movies, although not all users
    watched the same movies and each user liked different movies. If we want to recommend
    a movie to **user two** (**U2**), we must first find the most similar users. We
    can then make predictions in a **k-Nearest Neighbor** (**k-NN**) fashion, using
    the *K* most similar users. Of course, we can see that the user probably likes
    sci-fi films, but we need a quantitative method to measure it. If we treat each
    user''s preferences as a vector, we have four vectors of six elements. We can
    then compute the cosine between any two vectors. If the vectors align perfectly,
    the cosine will be 1, indicating a perfect equality. If the vectors are completely
    opposite, it will be -1, indicating a perfect disagreement between the two users''
    preferences. The only problem that arises is the fact that not all movies have
    been rated by each user. We can fill empty entries with zeros, in order to compute
    the cosine similarities. The following graph shows the cosine similarities between
    the users:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，每个用户都评分了若干部电影，尽管并非所有用户都观看了相同的电影，并且每个用户的喜好各不相同。如果我们想向**用户二**（**U2**）推荐一部电影，我们必须首先找到最相似的用户。然后，我们可以通过**k-最近邻**（**k-NN**）的方式，使用*K*个最相似的用户来进行预测。当然，我们可以看到该用户可能喜欢科幻电影，但我们需要一种量化的方法来衡量这一点。如果我们将每个用户的偏好看作一个向量，我们就有四个六维的向量。然后，我们可以计算任意两个向量之间的余弦值。如果两个向量完全对齐，余弦值为
    1，表示完全相同。如果向量完全相反，余弦值为 -1，表示两个用户的偏好完全相反。唯一的问题是，并非所有用户都评分了每部电影。为了计算余弦相似度，我们可以将空缺项填充为零。下图显示了用户之间的余弦相似度：
- en: '![](img/387799cc-8a62-41af-ad61-84416098f610.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/387799cc-8a62-41af-ad61-84416098f610.png)'
- en: Cosine similarities between users
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用户之间的余弦相似度
- en: 'We notice that users U0 and U3 exhibit a high level of similarity with U2\.
    The problem is that U0 also exhibits high similarity with U1, although their ratings
    are complete opposites. This is due to the fact that we fill any non-rated movie
    with 0, meaning all users who have not watched a movie agree that they do not
    like it. This can be remedied by first subtracting the mean of each user''s ratings
    from their ratings. This normalizes the values and centers them around 0\. Following
    this, we assign 0 to any movie the user has not yet rated. This indicates that
    the user is indifferent toward this movie and the user''s mean rating is not altered.
    By computing the centered cosine similarity, we get the following values:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，U0 和 U3 与 U2 展现出较高的相似度。问题是，U0 也与 U1 展现出较高的相似度，尽管他们的评分完全相反。这是因为我们将任何未评分的电影填充为
    0，这意味着所有未观看电影的用户都同意他们不喜欢这部电影。这可以通过首先从每个用户的评分中减去其平均值来解决。这样可以将值归一化并将其集中在 0 附近。接下来，对于用户尚未评分的任何电影，我们将其赋值为
    0。这表示用户对该电影没有偏好，并且用户的平均评分不会被改变。通过计算居中余弦相似度，我们得到以下值：
- en: '![](img/814c6255-6afd-4803-a455-f18e7dc4b21e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/814c6255-6afd-4803-a455-f18e7dc4b21e.png)'
- en: Centered cosine similarities between users
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 用户之间的居中余弦相似度
- en: 'We can now see that U2 is similar to U0 and U3, while U1 and U0 are quite dissimilar.
    In order to compute a prediction about movies that U2 has not seen, but that the
    nearest *K* neighbors have seen, we will compute the weighted average for each
    movie, using the cosine similarities as weights. We only do this for movies that
    all similar users have rated, but that the target user has not rated yet. This
    gives us the following predicted ratings. If we were to recommend a single movie
    to U2, we would recommend *2001: A Space Odyssey*, a sci-fi film, as we speculated
    earlier:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，U2 与 U0 和 U3 相似，而 U1 和 U0 则相差较大。为了计算 U2 未看过的电影的预测评分，但最近的*K*个邻居已经看过，我们将使用余弦相似度作为权重，计算每部电影的加权平均值。我们只对所有相似用户已经评分，但目标用户尚未评分的电影进行此操作。这为我们提供了以下预测评分。如果我们要向
    U2 推荐一部电影，我们将推荐*2001：太空漫游*，一部科幻电影，正如我们之前所推测的：
- en: '| **Interstellar** | **2001: A Space Odyssey** | **The Matrix** | **Full Metal
    Jacket** | **Jarhead** | **Top Gun** |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **星际穿越** | **2001：太空漫游** | **黑客帝国** | **全金属外壳** | **瓶中信** | **壮志凌云** |'
- en: '| - | 4.00 | - | 3.32 | 2.32 | - |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| - | 4.00 | - | 3.32 | 2.32 | - |'
- en: Predicted ratings for user U2
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: U2 的预测评分
- en: This recommendation method is called **collaborative filtering**. When we search
    for similar users, as we did in this small example, it is called **user-user filtering**.
    We can also apply this method to search for similar items by transposing the ratings
    table. This is called **item-item filtering**, and it usually performs better
    in real-world applications. This is due to the fact that items usually belong
    to more well-defined categories, when compared to users. For example, a movie
    can be an action movie, a thriller, a documentary, or a comedy with little overlap
    between the genres. A user may like a certain mix of those categories; thus, it
    is easier to find similar movies, rather than similar users.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种推荐方法被称为**协同过滤**。当我们像这个小示例一样寻找相似用户时，这称为**用户-用户过滤**。我们也可以将这种方法应用于通过转置评分表来寻找相似项，这被称为**物品-物品过滤**，在实际应用中通常表现得更好。这是因为物品通常属于更明确的类别，相较于用户。例如，一部电影可以是动作片、惊悚片、纪录片或喜剧片，类型之间几乎没有重叠。一个用户可能喜欢这些类别的某种混合；因此，找到相似的电影比找到相似的用户要容易。
- en: Neural recommendation systems
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络推荐系统
- en: Instead of explicitly defining similarity metrics, we can utilize deep learning
    techniques in order to learn good representations and mappings of the feature
    space. There are a number of ways to employ neural networks in order to build
    recommendation systems. In this chapter, we will present two of the simplest ways
    to do so in order to demonstrate the ability to incorporate ensemble learning
    into the system. The most important piece that we will utilize in our networks
    is the embedding layer. These layer types accept an integer index as input and
    map it to an n-dimensional space. For example, a two-dimensional mapping could
    map 1 to [0.5, 0.5]. Utilizing these layers, we will be able to feed the user's
    index and the movie's index to our network, and the network will predict the rating
    for the specific user-movie combination.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用深度学习技术，而不是显式定义相似度度量，来学习特征空间的良好表示和映射。神经网络有多种方法可以用于构建推荐系统。在本章中，我们将展示两种最简单的方法，以展示如何将集成学习融入到系统中。我们将在网络中使用的最重要部分是嵌入层。这些层类型接受整数索引作为输入，并将其映射到n维空间。例如，二维映射可以将1映射到[0.5,
    0.5]。通过这些层，我们将能够将用户的索引和电影的索引输入到网络中，网络将预测特定用户-电影组合的评分。
- en: 'The first architecture that we will test consists of two embedding layers,
    where we will multiply their outputs using a dot product, in order to predict
    the user''s rating of the movie. The architecture is depicted in the following
    diagram. Although it is not a traditional neural network, we will utilize backpropagation
    in order to train the parameters of the two embedding layers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试的第一个架构由两个嵌入层组成，在这两个嵌入层的输出上进行点积操作，以预测用户对电影的评分。该架构如下图所示。虽然它不是传统的神经网络，但我们将利用反向传播来训练这两个嵌入层的参数：
- en: '![](img/813302e0-f9b1-4198-94ec-2dabd06b1515.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/813302e0-f9b1-4198-94ec-2dabd06b1515.png)'
- en: Simple dot product architecture
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的点积架构
- en: 'The second architecture is a more traditional neural network. Instead of relying
    on a predefined operation to combine the outputs of the embedding layers (the
    dot product), we will allow the network to find the optimal way to combine them.
    Instead of a dot product, we will feed the output of the embedding layers to a
    series of fully-connected (**dense**) layers. The architecture is depicted in
    the following diagram:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个架构是一个更传统的神经网络。我们将不再依赖预定义的操作来结合嵌入层的输出（点积操作），而是允许网络找到将它们结合的最佳方式。我们将不使用点积，而是将嵌入层的输出馈送到一系列全连接（**密集**）层。该架构如下图所示：
- en: '![](img/5598ed5a-7097-49cd-8f2e-b1dfc95f9543.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5598ed5a-7097-49cd-8f2e-b1dfc95f9543.png)'
- en: The fully connected architecture
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接架构
- en: In order to train the networks, we will utilize the Adam optimizer, and we will
    use the **mean squared error** (**MSE**) as a loss function. Our goal will be
    to predict the ratings of movies for any given user as accurately as possible.
    As the embedding layers have a predetermined output dimension, we will utilize
    a number of networks with different dimensions in order to create a stacking ensemble.
    Each individual network will be a separate base learner, and a relatively simple
    machine learning algorithm will be utilized in order to combine the individual
    predictions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练网络，我们将使用 Adam 优化器，并使用 **均方误差**（**MSE**）作为损失函数。我们的目标是尽可能准确地预测任何给定用户的电影评分。由于嵌入层具有预定的输出维度，我们将使用具有不同维度的多个网络来创建堆叠集成。每个单独的网络将是一个独立的基础学习器，并将使用相对简单的机器学习算法来组合各个预测。
- en: Using Keras for movie recommendations
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 进行电影推荐
- en: 'In this section, we will utilize Keras as a deep learning framework in order
    to build our models. Keras can easily be installed by using either `pip` (`pip
    install keras`) or `conda` (`conda install -c conda-forge keras`). In order to
    build the neural networks, we must first understand our data. The MovieLens dataset
    consists of almost 100,000 samples and 4 different variables:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Keras 作为深度学习框架来构建我们的模型。Keras 可以通过 `pip`（`pip install keras`）或 `conda`（`conda
    install -c conda-forge keras`）轻松安装。为了构建神经网络，我们首先需要理解我们的数据。MovieLens 数据集包含了近 100,000
    个样本和 4 个不同的变量：
- en: '`userId`: A numeric index corresponding to a specific user'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`userId`：与特定用户对应的数字索引'
- en: '`movieId`: A numeric index corresponding to a specific movie'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movieId`：与特定电影对应的数字索引'
- en: '`rating`: A value between 0 and 5'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rating`：一个介于 0 和 5 之间的值'
- en: '`timestamp`: The specific time when the user rated the movie'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestamp`：用户评分电影的具体时间'
- en: 'A sample from the dataset is depicted in the following table. As is evident,
    the dataset is sorted by the `userId` column. This can potentially create overfitting
    problems in our models. Thus, we will shuffle the data before any split happens.
    Furthermore, we will not utilize the `timestamp` variable in our models, as we
    do not care about the order in which the movies were rated:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的一个示例如下表所示。显然，数据集是按照 `userId` 列排序的。这可能会导致我们的模型出现过拟合问题。因此，我们将在数据分割之前对数据进行洗牌。此外，我们不会在模型中使用
    `timestamp` 变量，因为我们并不关心电影评分的顺序：
- en: '| **userId** | **movieId** | **rating** | **timestamp** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **userId** | **movieId** | **rating** | **timestamp** |'
- en: '| 1 | 1 | 4 | 964982703 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 4 | 964982703 |'
- en: '| 1 | 3 | 4 | 964981247 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3 | 4 | 964981247 |'
- en: '| 1 | 6 | 4 | 964982224 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 4 | 964982224 |'
- en: '| 1 | 47 | 5 | 964983815 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 47 | 5 | 964983815 |'
- en: '| 1 | 50 | 5 | 964982931 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 50 | 5 | 964982931 |'
- en: A sample from the dataset
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集示例
- en: 'By looking at the distribution of ratings on the following graph, we can see
    that most movies were rated at 3.5, which is above the middle of the rating scale
    (2.5). Furthermore, the distribution shows a left tail, indicating that most users
    are generous with their ratings. Indeed, the first quartile of the ratings spans
    from 0.5 to 3, while the other 75% of the ratings lie in the 3-5 range. In other
    words, a user only rates 1 out of 4 movies with a value of less than 3:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看下图中评分的分布情况，我们可以看到大多数电影的评分为 3.5，超过了评分范围的中间值（2.5）。此外，分布图显示出左偏尾，表明大多数用户给出的评分都比较慷慨。事实上，评分的第一四分位数范围是从
    0.5 到 3，而其余 75% 的评分则在 3 到 5 的范围内。换句话说，用户只有在评分低于 3 的电影中，才会选择 1 部电影：
- en: '![](img/c8b2bbe4-1cbb-4474-80e1-e82bd00c6c6c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8b2bbe4-1cbb-4474-80e1-e82bd00c6c6c.png)'
- en: Ratings' distribution
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 评分分布
- en: Creating the dot model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建点模型
- en: 'Our first model will consist of two embedding layers, one for the movie index
    and one for the user index, as well as their dot product. We will use the `keras.layers`
    package, which contains the necessary layer implementations, as well as the `Model`
    implementation from the `keras.models` package. The layers that we will utilize
    are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个模型将包括两个嵌入层，一个用于电影索引，另一个用于用户索引，以及它们的点积。我们将使用 `keras.layers` 包，它包含了所需的层实现，以及
    `keras.models` 包中的 `Model` 实现。我们将使用的层如下：
- en: The`Input` layer, which is responsible for creating Keras tensors from more
    conventional Python data types
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Input` 层，负责将更传统的 Python 数据类型转换为 Keras 张量'
- en: The `Embedding` layer, which is the implementation of embedding layers
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Embedding` 层，这是嵌入层的实现'
- en: The `Flatten` layer, which transforms any Keras n-dimensional tensor to a single
    dimensional tensor
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Flatten` 层，将任何 Keras n 维张量转换为一维张量'
- en: The `Dot` layer, which implements the dot product
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dot` 层，实现点积'
- en: 'Furthermore, we will utilize `train_test_split` and `metrics` from `sklearn`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用 `train_test_split` 和 `sklearn` 的 `metrics`：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Apart from setting the random seed of `numpy`, we define a function that loads
    and preprocesses our data. We read the data from the `.csv` file, drop the timestamp,
    and shuffle the data by utilizing the shuffle function of `pandas`. Furthermore,
    we create a train/test split of 80%/20%. We then re-map the dataset''s indices
    in order to have consecutive integers as indices:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设置 `numpy` 的随机种子外，我们定义了一个函数来加载和预处理数据。我们从 `.csv` 文件中读取数据，去除时间戳，并利用 `pandas`
    的 shuffle 函数打乱数据。此外，我们创建了一个 80%/20% 的训练集/测试集划分。然后，我们重新映射数据集的索引，使其成为连续的整数索引：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In order to create the network, we first define the movie part of the input.
    We create an `Input` layer, which will act as the interface to our `pandas` dataset
    by accepting its data and transforming it into Keras tensors. Following this,
    the layer''s output is fed into the `Embedding` layer, in order to map the integer
    to a five-dimensional space. We define the number of possible indices as `n_movies`
    (first parameter), and the number of features as `fts` (second parameter). Finally,
    we flatten the output. The same process is repeated for the user part:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建网络，我们首先定义输入的电影部分。我们创建一个 `Input` 层，它将作为我们 `pandas` 数据集的接口，通过接收数据并将其转换为 Keras
    张量。接着，层的输出被输入到 `Embedding` 层，用于将整数映射到五维空间。我们将可能的索引数量定义为 `n_movies`（第一个参数），特征的数量定义为
    `fts`（第二个参数）。最后，我们展平输出。用户部分重复相同的过程：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we define the dot product layer, with the two flattened embeddings
    as inputs. We then define `Model` by specifying the `user_in` and `movie_in` (`Input`)
    layers as inputs, and the `prod` (`Dot`) layer as an output. After defining the
    model, Keras needs to compile it in order to create the computational graph. During
    compilation, we define the optimizer and loss functions:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义点积层，以两个展平的嵌入向量作为输入。然后，我们通过指定 `user_in` 和 `movie_in`（`Input`）层作为输入，`prod`（`Dot`）层作为输出，来定义
    `Model`。在定义模型后，Keras 需要对其进行编译，以创建计算图。在编译过程中，我们定义优化器和损失函数：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'By calling `model.summary()`, we can see that the model has around 52,000 trainable
    parameters. All of these parameters are in the `Embedding` layers. This means
    that the network will only learn how to map the user and movie indices to the
    five-dimensional space. The function''s output is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `model.summary()`，我们可以看到模型大约有 52,000 个可训练参数。所有这些参数都在 `Embedding` 层中。这意味着网络将只学习如何将用户和电影的索引映射到五维空间。函数的输出如下：
- en: '![](img/6e947f22-661c-49ef-b9bb-98dfbef6a3e4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e947f22-661c-49ef-b9bb-98dfbef6a3e4.png)'
- en: The model's summary
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的摘要
- en: 'Finally, we fit the model to our train set and evaluate it on the test set.
    We train the network for ten epochs in order to observe how it behaves, as well
    as how much time it needs to train itself. The following code depicts the training
    progress of the network:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将模型拟合到训练集，并在测试集上评估它。我们训练网络十个周期，以观察其行为，以及它需要多少时间来训练。以下代码展示了网络的训练进度：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Take a look at the following screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的截图：
- en: '![](img/610b0fd4-a499-4dd9-b633-43eb6bd20209.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/610b0fd4-a499-4dd9-b633-43eb6bd20209.png)'
- en: Training progress of the dot product network
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 点积网络的训练进度
- en: The model is able to achieve an MSE of 1.28 on the test set. In order to improve
    the model, we could increase the number of features each `Embedding` layer is
    able to learn, but the main limitation is the dot product layer. Instead of increasing
    the number of features, we will give the model the freedom to choose how to combine
    the two layers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在测试集上能够达到 1.28 的均方误差（MSE）。为了提高模型的性能，我们可以增加每个 `Embedding` 层能够学习的特征数量，但主要的限制是点积层。我们不会增加特征数量，而是让模型自由选择如何组合这两层。
- en: Creating the dense model
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建密集模型
- en: 'In order to create the dense model, we will substitute the `Dot` layer with
    a series of `Dense` layers. `Dense` layers are classic neurons, where each neuron
    gets, as input, all the outputs from the previous layer. In our case, as we have
    two `Embedding` layers, we must first concatenate them using the `Concatenate`
    layer, and then feed them to the first `Dense` layer. These two layers are also
    included in the `keras.layers` package. Thus, our model definition will now look
    like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建密集模型，我们将用一系列`Dense`层替代`Dot`层。`Dense`层是经典的神经元，每个神经元都会接收来自上一层的所有输出作为输入。在我们的例子中，由于我们有两个`Embedding`层，我们首先需要使用`Concatenate`层将它们连接起来，然后将其传递给第一个`Dense`层。这两层也包含在`keras.layers`包中。因此，我们的模型定义现在将如下所示：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'By adding these three `Dense` layers, we have increased the number of trainable
    parameters from almost 52,000 to almost 57,200 (an increase of 10%). Furthermore,
    each step now needs almost 210 microseconds, which increased from 144 us (a 45%
    increase), as is evident from the training progression and summary, as depicted
    in the following diagrams:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加这三个`Dense`层，我们将可训练参数的数量从接近52,000增加到接近57,200（增加了10%）。此外，现在每一步的时间需要大约210微秒，较之前的144微秒增加了45%，这一点从训练进度和总结中可以明显看出，具体表现如以下图所示：
- en: '![](img/6a1c2efe-311b-48ad-9b1b-c8c71eb3c77a.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a1c2efe-311b-48ad-9b1b-c8c71eb3c77a.png)'
- en: Summary of the dense model
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 密集模型的总结
- en: '![](img/52afae2e-d3ca-4544-8b37-e560cdcff7c3.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52afae2e-d3ca-4544-8b37-e560cdcff7c3.png)'
- en: Training progression of the dense model
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 密集模型的训练进度
- en: Nonetheless, the model now achieves an MSE 0.77 , which is 60% of the original
    dot-product model. Thus, as this model outperforms the previous model, we will
    utilize this architecture for our stacking ensemble. Moreover, as each network
    has a higher degree of freedom, it has a higher probability of diversifying from
    other base learners.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，该模型现在的均方误差为0.77，约为原始点积模型的60%。因此，由于该模型表现优于之前的模型，我们将利用此架构构建我们的堆叠集成模型。此外，由于每个网络具有更高的自由度，它具有更高的概率与其他基础学习器进行多样化。
- en: Creating a stacking ensemble
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建堆叠集成模型
- en: In order to create our stacking ensemble, we will utilize three dense networks,
    with embeddings consisting of 5, 10, and 15 features as base learners. We will
    train all networks on the original train set and utilize them to make predictions
    on the test set. Furthermore, we will train a Bayesian ridge regression as a meta
    learner. In order to train the regression, we will use all but the last 1,000
    samples of the test set. Finally, we will evaluate the stacking ensemble on these
    last 1,000 samples.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的堆叠集成模型，我们将使用三个密集网络，其中嵌入层包含5、10和15个特征作为基础学习器。我们将在原始训练集上训练所有网络，并利用它们在测试集上进行预测。此外，我们将训练一个贝叶斯岭回归模型作为元学习器。为了训练回归模型，我们将使用测试集中的所有样本，除了最后的1,000个样本。最后，我们将在这最后的1,000个样本上评估堆叠集成模型。
- en: 'First, we will create a function that creates and trains a dense network with
    *n* number of embedding features, as well as a function that accepts a model as
    input and return its predictions on the test set:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个函数，用于创建和训练一个具有*n*个嵌入特征的密集网络，以及一个接受模型作为输入并返回其在测试集上预测结果的函数：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will then create and train our base learners and meta learner in order to
    predict on the test set. We combine all three models'' predictions in a single
    array:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建并训练我们的基础学习器和元学习器，以便对测试集进行预测。我们将三种模型的预测结果组合成一个数组：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we train the meta learner on all but the last 1,000 test samples and
    evaluate the base learners, as well as the whole ensemble, on these last 1,000
    samples:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在除了最后1,000个测试样本之外的所有样本上训练元学习器，并在这最后的1,000个样本上评估基础学习器以及整个集成模型：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The results are depicted in the following table. As is evident, the ensemble
    is able to outperform the individual base learners on unseen data, achieving a
    lower MSE than any individual base learner:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如以下表所示。从中可以看出，集成模型能够在未见数据上超越单独的基础学习器，达到了比任何单一基础学习器更低的均方误差（MSE）：
- en: '| **Model** | **MSE** |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **均方误差（MSE）** |'
- en: '| Base Learner 5 | 0.7609 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 基础学习器 5 | 0.7609 |'
- en: '| Base Learner 10 | 0.7727 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 基础学习器 10 | 0.7727 |'
- en: '| Base Learner 15 | 0.7639 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 基础学习器 15 | 0.7639 |'
- en: '| Ensemble | 0.7596 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 集成模型 | 0.7596 |'
- en: Results for individual base learners and the ensemble
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 单独基础学习器和集成模型的结果
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we briefly presented the concept of recommendation systems
    and how collaborative filtering works. We then presented how neural networks can
    be utilized in order to avoid explicitly defining rules that dictate how unrated
    items would be rated by a user, using embedding layers and dot products. Following
    that, we showed how the performance of these models can be improved if we allow
    the networks to learn how to combine the embedding layers themselves. This gives
    the models considerably higher degrees of freedom without drastically increasing
    the number of parameters, leading to considerable increases in performance. Finally,
    we showed how the same architecture—with variable numbers of embedding features—can
    be utilized in order to create base learners for a stacking ensemble. In order
    to combine the base learners, we utilized a Bayesian ridge regression, which resulted
    in better results than any individual base learner.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们简要介绍了推荐系统的概念以及协同过滤是如何工作的。然后，我们展示了如何利用神经网络来避免明确地定义规则，来决定用户对未评级项目的评分，使用嵌入层和点积。接着，我们展示了如何通过允许网络学习如何自行组合嵌入层，从而提高这些模型的性能。这使得模型拥有更高的自由度，而不会显著增加参数数量，从而显著提高了性能。最后，我们展示了如何利用相同的架构——具有不同数量嵌入特征——来创建堆叠集成的基学习器。为了组合这些基学习器，我们采用了贝叶斯岭回归，这比任何单一的基学习器都取得了更好的结果。
- en: This chapter serves as an introduction to the concept of using ensemble learning
    techniques for deep recommendation systems, rather than a fully detailed guide.
    There are many more options that can lead to considerable improvements in the
    system. For example, the usage of user descriptions (rather than indices), additional
    information about each movie (such as genre), and different architectures, can
    all greatly contribute to performance improvements. Still, all these concepts
    can greatly benefit from the usage of ensemble learning techniques, which this
    chapter adequately demonstrates.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本章作为使用集成学习技术来构建深度推荐系统的概述，而非完全详细的指南。其实还有很多其他选项可以显著提高系统的性能。例如，使用用户描述（而非索引）、每部电影的附加信息（如类型）以及不同的架构，都能大大提升性能。不过，所有这些概念都可以通过使用集成学习技术来获益，本章已充分展示了这一点。
- en: In the next and final chapter, we will use ensemble learning techniques in order
    to cluster data from the World Happiness Report as we try to uncover patterns
    in the data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的最后一章中，我们将使用集成学习技术来对《世界幸福报告》中的数据进行聚类，以期发现数据中的模式。
