- en: 'Chapter 4: Training Machine Learning Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：训练机器学习模型
- en: In the previous chapter, you learned how Amazon SageMaker Autopilot makes it
    easy to build, train, and optimize models automatically, without writing a line
    of machine learning code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了 Amazon SageMaker Autopilot 如何让你轻松构建、训练和自动优化模型，而无需编写一行机器学习代码。
- en: For problem types that are not supported by SageMaker Autopilot, the next best
    option is to use one of the algorithms already implemented in SageMaker and to
    train it on your dataset. These algorithms are referred to as **built-in algorithms**,
    and they cover many typical machine learning problems, from classification to
    time series to anomaly detection.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SageMaker Autopilot 不支持的问题类型，下一最佳选择是使用 SageMaker 中已实现的算法之一，并在你的数据集上进行训练。这些算法被称为
    **内置算法**，它们涵盖了许多典型的机器学习问题，从分类到时间序列到异常检测。
- en: 'In this chapter, you will learn about built-in algorithms for supervised and
    unsupervised learning, what type of problems you can solve with them, and how
    to use them with the SageMaker SDK:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习适用于监督学习和无监督学习的内置算法，了解你可以用它们解决什么类型的问题，以及如何使用 SageMaker SDK 来使用它们：
- en: Discovering the built-in algorithms in Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现 Amazon SageMaker 中的内置算法
- en: Training and deploying models with built-in algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置算法训练和部署模型
- en: Using the SageMaker SDK with built-in algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker SDK 与内置算法
- en: Working with more built-in algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多内置算法
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you don't already have one, please point your browser to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 AWS 账户才能运行本章中的示例。如果你还没有账户，请访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建一个。你还应该熟悉 AWS 免费套餐 ([https://aws.amazon.com/free/](https://aws.amazon.com/free/))，它让你在某些使用限制内免费使用许多
    AWS 服务。
- en: You will need to install and configure the AWS Command-Line Interface (CLI)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置 AWS 命令行界面（CLI） ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))。
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory, but
    strongly encouraged as it includes many projects that we will need (Jupyter, `pandas`,
    `numpy`, and more).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个工作中的 Python 3.x 环境。安装 Anaconda 发行版 ([https://www.anaconda.com/](https://www.anaconda.com/))
    不是强制性的，但强烈推荐，因为它包含了我们将需要的许多项目（Jupyter，`pandas`，`numpy` 等）。
- en: Code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中包含的代码示例可在 GitHub 上找到：[https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装
    Git 客户端才能访问它们 ([https://git-scm.com/](https://git-scm.com/))。
- en: Discovering the built-in algorithms in Amazon SageMaker
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现 Amazon SageMaker 中的内置算法
- en: Built-in algorithms are machine learning algorithms implemented, and in some
    cases invented, by Amazon ([https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)).
    They let you quickly train and deploy your own models without writing a line of
    machine learning code. Indeed, since the training and prediction code is readily
    available, you don't have to worry about implementing it, and you can focus on
    the machine learning problem at hand. As usual with SageMaker, infrastructure
    is fully managed, saving you even more time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 内置算法是由亚马逊实现的机器学习算法，在某些情况下是由亚马逊发明的 ([https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html))。它们让你在无需编写一行机器学习代码的情况下，快速训练和部署自己的模型。实际上，由于训练和预测代码是现成的，你无需担心实现它，能够专注于眼前的机器学习问题。像往常一样，SageMaker
    完全管理基础设施，节省你更多的时间。
- en: In this section, you'll learn about the built-in algorithms for traditional
    machine learning problems. Algorithms for computer vision and natural language
    processing will be covered in the next two chapters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解传统机器学习问题的内置算法。计算机视觉和自然语言处理相关的算法将在接下来的两章中介绍。
- en: Supervised learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有监督学习
- en: 'Supervised learning focuses on problems that require a labeled dataset, such
    as regression or classification:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督学习专注于需要标记数据集的问题，如回归或分类：
- en: '**Linear Learner** builds linear models to solve regression problems, as well
    as classification problems (binary or multi-class).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性学习器**构建线性模型来解决回归问题以及分类问题（包括二分类和多分类）。'
- en: '**Factorization Machines** builds linear models to solve regression problems,
    as well as classification problems (binary or multi-class). Factorization machines
    are a generalization of linear models, and they''re a good fit for high-dimension,
    sparse datasets, such as user-item interaction matrices in recommendation problems.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因式分解机**构建线性模型来解决回归问题以及分类问题（包括二分类和多分类）。因式分解机是线性模型的推广，特别适用于高维稀疏数据集，如推荐系统中的用户-物品交互矩阵。'
- en: '**K-nearest neighbors** (**KNN**) builds non-parametric models for regression
    and classification problems.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K最近邻**（**KNN**）构建非参数模型用于回归和分类问题。'
- en: '**XGBoost** builds models for regression, classification, and ranking problems.
    XGBoost is possibly the most widely used machine learning algorithm used today,
    and SageMaker uses the open source implementation available at [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XGBoost**构建回归、分类和排序问题的模型。XGBoost可能是当前使用最广泛的机器学习算法，SageMaker使用的是[https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)提供的开源实现。'
- en: '**DeepAR** builds forecasting models for multivariate time series. DeepAR is
    an Amazon-invented algorithm based on **Recurrent Neural Networks**, and you can
    read more about it at [https://arxiv.org/abs/1704.04110](https://arxiv.org/abs/1704.04110).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepAR**构建多变量时间序列的预测模型。DeepAR是基于**循环神经网络**的亚马逊发明算法，您可以通过[https://arxiv.org/abs/1704.04110](https://arxiv.org/abs/1704.04110)了解更多信息。'
- en: '**Object2Vec** learns low-dimension embeddings from general-purpose high-dimensional
    objects. Object2Vec is an algorithm invented by Amazon.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Object2Vec**从通用高维对象中学习低维嵌入。Object2Vec是由亚马逊发明的算法。'
- en: '**BlazingText** builds text classification models. This algorithm was invented
    by Amazon, and you can read more about it at [https://dl.acm.org/doi/10.1145/3146347.3146354](https://dl.acm.org/doi/10.1145/3146347.3146354).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BlazingText**构建文本分类模型。这个算法由亚马逊发明，您可以通过[https://dl.acm.org/doi/10.1145/3146347.3146354](https://dl.acm.org/doi/10.1145/3146347.3146354)了解更多信息。'
- en: Unsupervised learning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'Unsupervised learning doesn''t require a labeled dataset, and includes problems
    such as clustering or anomaly detection:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习不需要标记数据集，涉及的问题包括聚类或异常检测：
- en: '**K-means** builds clustering models. SageMaker uses a modified version of
    the web-scale k-means clustering algorithm ([https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K均值**构建聚类模型。SageMaker使用了一个修改版的网页规模K均值聚类算法（[https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)）。'
- en: '**Principal Component Analysis** (**PCA**) builds dimensionality reduction
    models.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）构建降维模型。'
- en: '**Random Cut Forest** builds anomaly detection models.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机切割森林**构建异常检测模型。'
- en: '**IP Insights** builds models to identify usage patterns for IPv4 addresses.
    This comes in handy for monitoring, cybersecurity, and so on.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP洞察**构建模型以识别IPv4地址的使用模式。这对于监控、网络安全等领域非常有用。'
- en: '**BlazingText** computes word vectors, a very useful representation for natural
    language processing tasks.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BlazingText**计算词向量，这对于自然语言处理任务非常有用。'
- en: We'll cover some of these algorithms in detail in the rest of this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本章剩余部分将详细介绍其中一些算法。
- en: A word about scalability
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于可扩展性
- en: Before we dive into training and deploying models with the algorithms, you may
    wonder why you should use them instead of their counterparts in well-known libraries
    such as `scikit-learn` and `R`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解如何使用这些算法进行模型训练和部署之前，您可能会想知道为什么要使用它们，而不是使用像`scikit-learn`和`R`等著名库中的算法。
- en: First, these algorithms have been implemented and tuned by Amazon teams, who
    are not exactly newcomers to machine learning! A lot of effort has been put into
    making sure that these algorithms run as fast as possible on AWS infrastructure,
    no matter what type of instance you use. In addition, many of these algorithms
    support **distributed training** out of the box, letting you split model training
    across a cluster of fully managed instances.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这些算法已经由亚马逊团队实现并调优，亚马逊团队可不是机器学习的新人！大量的工作已经投入到确保这些算法在 AWS 基础设施上尽可能快速运行，无论你使用什么类型的实例。此外，许多算法支持**分布式训练**，让你可以将模型训练分布到一个完全托管的实例集群中。
- en: 'Thanks to this, benchmarks indicate that these algorithms are generally 10
    times better than competing implementations. In many cases, they are also much
    more cost-effective. You can learn more about this at the following URLs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，基准测试表明，这些算法通常比竞争对手的实现要好 10 倍。在许多情况下，它们也更具成本效益。你可以在以下网址了解更多信息：
- en: 'AWS Tel Aviv Summit 2018: *Speed Up Your Machine Learning Workflows with Built-In
    Algorithms*: [https://www.youtube.com/watch?v=IeIUr78OrE0](https://www.youtube.com/watch?v=IeIUr78OrE0)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 特拉维夫峰会 2018：*通过内置算法加速你的机器学习工作流程*：[https://www.youtube.com/watch?v=IeIUr78OrE0](https://www.youtube.com/watch?v=IeIUr78OrE0)
- en: '*Elastic Machine Learning Algorithms in Amazon*, Liberty et al., SIGMOD''20:
    SageMaker: [https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker](https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*亚马逊中的弹性机器学习算法*，Liberty 等，SIGMOD''20：SageMaker：[https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker](https://www.amazon.science/publications/elastic-machine-learning-algorithms-in-amazon-sagemaker)'
- en: Of course, these algorithms benefit from all the features present in SageMaker,
    as you will find out by the end of the book.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些算法也受益于 SageMaker 中的所有功能，正如你在本书的结尾部分会发现的那样。
- en: Training and deploying models with built-in algorithms
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内置算法训练和部署模型
- en: Amazon SageMaker lets you train and deploy models in many different configurations.
    Although it encourages best practices, it is a modular service that lets you do
    things your own way.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 让你以多种不同配置训练和部署模型。虽然它鼓励最佳实践，但它是一个模块化的服务，允许你按照自己的方式操作。
- en: In this section, we'll first look at a typical end-to-end workflow, where we
    use SageMaker from data upload all the way to model deployment. Then, we'll discuss
    alternative workflows, and how you can cherry-pick the features that you need.
    Finally, we will take a look under the hood, and see what happens from an infrastructure
    perspective when we train and deploy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先看看一个典型的端到端工作流程，展示如何从数据上传到模型部署，使用 SageMaker。然后，我们将讨论替代工作流程，以及如何根据需要挑选所需的功能。最后，我们将深入了解，从基础设施的角度看，当我们训练和部署时会发生什么。
- en: Understanding the end-to-end workflow
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解端到端工作流程
- en: 'Let''s look at a typical SageMaker workflow. You''ll see it again and again
    in our examples, as well as in the AWS notebooks available on GitHub ([https://github.com/awslabs/amazon-sagemaker-examples/](https://github.com/awslabs/amazon-sagemaker-examples/)):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个典型的 SageMaker 工作流程。你会在我们的示例中反复看到它，也会在 GitHub 上提供的 AWS 笔记本中看到它（[https://github.com/awslabs/amazon-sagemaker-examples/](https://github.com/awslabs/amazon-sagemaker-examples/)）：
- en: '`protobuf` ([https://developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers)).'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`protobuf` ([https://developers.google.com/protocol-buffers](https://developers.google.com/protocol-buffers))。'
- en: '**Configure the training job**: This is where you select the algorithm that
    you want to train with, set hyperparameters, and define infrastructure requirements
    for the training job.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置训练任务**：在这里，你可以选择想要训练的算法，设置超参数，并定义训练任务的基础设施要求。'
- en: '**Launch the training job**: This is where we pass the location of your dataset
    in S3\. Training takes place on managed infrastructure, created and provisioned
    automatically according to your requirements. Once training is complete, the **model
    artifact** is saved in S3\. The training infrastructure is terminated automatically,
    and you only pay for what you used.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动训练任务**：在这里，我们传入你在 S3 中的数据集位置。训练发生在托管的基础设施上，系统会根据你的需求自动创建和配置。一旦训练完成，**模型工件**会被保存在
    S3 中。训练基础设施会自动终止，你只需为实际使用的部分付费。'
- en: '**Deploy the model**: You can deploy a model either on a **real-time HTTPS
    endpoint** for live prediction or for **batch transform**. Again, you simply need
    to define infrastructure requirements.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署模型**：你可以将模型部署到 **实时 HTTPS 端点** 进行实时预测，或部署到 **批量转换** 中。同样，你只需要定义基础设施要求。'
- en: '**Predict data**: Either invoking a real-time endpoint or a batch transformer.
    As you would expect, infrastructure is managed here too. For production, you would
    also monitor the quality of data and predictions.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测数据**：可以调用实时端点或批量转换器。正如你所期望的，这里的基础设施也已托管。对于生产环境，你还需要监控数据和预测的质量。'
- en: '**Clean up!**: This involves taking the endpoint down, to avoid unnecessary
    charges.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**清理！**：这涉及关闭端点，以避免不必要的费用。'
- en: Understanding this workflow is critical in being productive with Amazon SageMaker.
    Fortunately, the SageMaker SDK has simple APIs that closely match these steps,
    so you shouldn't be confused about which one to use, or when to use it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个工作流程对于提高 Amazon SageMaker 的生产力至关重要。幸运的是，SageMaker SDK 提供了与这些步骤高度匹配的简单 API，因此你不必担心该使用哪个
    API，或者何时使用它。
- en: Before we start looking at the SDK, let's consider alternative workflows that
    could make sense in your business and technical environments.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始查看 SDK 之前，让我们考虑一些在你的业务和技术环境中可能有意义的替代工作流。
- en: Using alternative workflows
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用替代工作流
- en: Amazon SageMaker is a modular service that lets you work your way. Let's first
    consider a workflow where you would train on SageMaker and deploy on your own
    server, whatever the reasons may be.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 是一个模块化的服务，允许你按自己的方式工作。让我们首先考虑一个工作流，其中你会在 SageMaker 上进行训练，并在你自己的服务器上进行部署，无论原因是什么。
- en: Exporting a model
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出模型
- en: 'Steps 1-3 would be the same as in the previous example, and then you would
    do the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1-3 与前面的示例相同，然后你可以做如下操作：
- en: Download the training artifact from S3, which is materialized as a `model.tar.gz`
    file.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 S3 下载训练工件，它以 `model.tar.gz` 文件的形式存在。
- en: Extract the model stored in the artifact.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取存储在工件中的模型。
- en: 'On your own server, load the model with the appropriate machine learning library:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你自己的服务器上，使用适当的机器学习库加载模型：
- en: '`fastText` implementation available at [https://fasttext.cc/](https://fasttext.cc/).'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fastText` 实现可以在 [https://fasttext.cc/](https://fasttext.cc/) 获取。'
- en: '**For all other models**: Use **Apache MXNet** ([https://mxnet.apache.org/](https://mxnet.apache.org/)).'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于所有其他模型**：使用 **Apache MXNet** ([https://mxnet.apache.org/](https://mxnet.apache.org/))。'
- en: Now, let's see how you could import an existing model and deploy it on SageMaker.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何将现有模型导入并部署到 SageMaker 上。
- en: Importing a model
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导入模型
- en: 'The steps are equally simple:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤同样简单：
- en: Package your model in a model artifact (`model.tar.gz`).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的模型打包成模型工件（`model.tar.gz`）。
- en: Upload the artifact to an S3 bucket.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将工件上传到 S3 桶。
- en: Register the artifact as a SageMaker model.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将工件注册为 SageMaker 模型。
- en: Deploy the model and predict.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型并进行预测。
- en: This is just a quick look. We'll run full examples for both workflows in [*Chapter
    11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine Learning
    Models*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个快速的概览。我们将在 [*第11章*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237)，*部署机器学习模型*
    中为这两个工作流运行完整的示例。
- en: Using fully managed infrastructure
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用完全托管的基础设施
- en: All SageMaker jobs run on managed infrastructure. Let's take a look under the
    hood and see what happens when we train and deploy models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 SageMaker 作业都在托管基础设施上运行。让我们看看背后的工作原理，看看训练和部署模型时发生了什么。
- en: Packaging algorithms in Docker containers
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将算法打包到 Docker 容器中
- en: 'All SageMaker algorithms must be packaged in **Docker** containers. Don''t
    worry, you don''t need to know much about Docker in order to use SageMaker. If
    you''re not familiar with it, I would recommend going through this tutorial to
    understand key concepts and tools: [https://docs.docker.com/get-started/](https://docs.docker.com/get-started/).
    It''s always good to know a little more than actually required!'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 SageMaker 算法必须打包在 **Docker** 容器中。别担心，使用 SageMaker 并不需要你对 Docker 有太多了解。如果你不熟悉
    Docker，建议你阅读这个教程，以了解关键概念和工具：[https://docs.docker.com/get-started/](https://docs.docker.com/get-started/)。多了解一点总是有好处的！
- en: As you would expect, built-in algorithms are pre-packaged, and containers are
    readily available for training and deployment. They are hosted in **Amazon Elastic
    Container Registry** (**ECR**), AWS' Docker registry service ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/)).
    As ECR is a region-based service, you will find a collection of containers in
    each region where SageMaker is available.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所预期的，内置算法已经打包好，并且容器随时可用于训练和部署。它们托管在**Amazon Elastic Container Registry**（**ECR**）中，这是
    AWS 的 Docker 注册服务 ([https://aws.amazon.com/ecr/](https://aws.amazon.com/ecr/))。由于
    ECR 是一个基于区域的服务，您会在每个 SageMaker 可用的区域找到一系列容器。
- en: You can find the list of built-in algorithm containers at [https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html).
    For instance, the name of the container for the Linear Learner algorithm in the
    eu-west-1 region is `438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest`.
    These containers can only be pulled to SageMaker managed instances, so you won't
    be able to run them on your local machine.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)
    找到内置算法容器的列表。例如，在 eu-west-1 区域，Linear Learner 算法的容器名称是 `438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest`。这些容器只能拉取到
    SageMaker 管理的实例中，因此您无法在本地计算机上运行它们。
- en: Now let's look at the underlying infrastructure.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下底层基础设施。
- en: Creating the training infrastructure
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建训练基础设施
- en: When you launch a training job, SageMaker fires up infrastructure according
    to your requirements (instance type and instance count).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动训练作业时，SageMaker 会根据您的需求（实例类型和实例数量）启动基础设施。
- en: Once a training instance is in service, it pulls the appropriate training container
    from ECR. Hyperparameters are applied to the algorithm, which also receives the
    location of your dataset. By default, the algorithm then copies the full dataset
    from S3 and starts training. If distributed training is configured, SageMaker
    automatically distributes dataset batches to the different instances in the cluster.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练实例投入使用，它会从 ECR 拉取适当的训练容器。超参数会应用到算法上，算法也会接收到数据集的位置。默认情况下，算法会从 S3 复制完整的数据集并开始训练。如果配置了分布式训练，SageMaker
    会自动将数据集批次分发到集群中的不同实例。
- en: Once training is complete, the model is packaged in a model artifact saved in
    S3\. Then, the training infrastructure is shut down automatically. Logs are available
    in **Amazon CloudWatch Logs**. Last but not least, you're only charged for the
    exact amount of training time.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，模型会被打包成模型工件并保存在 S3 中。然后，训练基础设施会自动关闭。日志可以在**Amazon CloudWatch Logs**中查看。最后但同样重要的是，您只会按实际的训练时间收费。
- en: Creating the prediction infrastructure
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建预测基础设施
- en: When you launch a deployment job, SageMaker once again creates infrastructure
    according to your requirements.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动部署作业时，SageMaker 会根据您的需求再次创建基础设施。
- en: Let's focus on real-time endpoints for now, and not on batch transform.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们暂时专注于实时终端节点，而不关注批量转换。
- en: Once an endpoint instance is in service, it pulls the appropriate prediction
    container from ECR and loads your model from S3\. Then, the HTTPS endpoint is
    provisioned and is ready for prediction within minutes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦终端节点实例投入使用，它会从 ECR 拉取适当的预测容器，并从 S3 加载您的模型。然后，HTTPS 端点会被配置好，并在几分钟内准备好进行预测。
- en: If you configured the endpoint with several instances, load balancing and high
    availability are set up automatically. If you configured **Auto Scaling**, this
    is applied as well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您配置了多个实例的终端节点，负载均衡和高可用性将自动设置。如果您配置了**自动扩展**，也会应用此设置。
- en: As you would expect, an endpoint stays up until it's deleted explicitly, either
    in the AWS Console or with a SageMaker API call. In the meantime, you will be
    charged for the endpoint, so **please make sure to delete endpoints that you don't
    need!**
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所预期的，终端节点会保持开启状态，直到显式删除它，无论是在 AWS 控制台中，还是通过 SageMaker API 调用。在此期间，您将为终端节点付费，因此**请确保删除不再需要的终端节点！**
- en: Now that we understand the big picture, let's start looking at the SageMaker
    SDK, and how we can use it to train and deploy models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了大致情况，让我们开始了解 SageMaker SDK，看看如何使用它来训练和部署模型。
- en: Using the SageMaker SDK with built-in algorithms
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内置算法的 SageMaker SDK
- en: Being familiar with the SageMaker SDK is important to making the most of SageMaker.
    You can find its documentation at [https://sagemaker.readthedocs.io](https://sagemaker.readthedocs.io).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉SageMaker SDK对于充分利用SageMaker至关重要。您可以在[https://sagemaker.readthedocs.io](https://sagemaker.readthedocs.io)找到其文档。
- en: Walking through a simple example is the best way to get started. In this section,
    we'll use the Linear Learner algorithm to train a regression model on the Boston
    Housing dataset ([https://www.kaggle.com/c/boston-housing](https://www.kaggle.com/c/boston-housing)).
    We'll proceed very slowly, leaving no stone unturned. Once again, these concepts
    are essential, so please take your time, and make sure you understand every step
    fully.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个简单的示例入手是最好的开始方式。在本节中，我们将使用Linear Learner算法在波士顿房价数据集上训练回归模型（[https://www.kaggle.com/c/boston-housing](https://www.kaggle.com/c/boston-housing)）。我们将非常慢地进行，每一步都不遗漏。再次强调，这些概念非常重要，请花时间理解每一个步骤。
- en: Reminder
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒
- en: 'I recommend that you follow along and run the code available in the companion
    GitHub repository. Every effort has been made to check all code samples present
    in the text. However, for those of you who have an electronic version, copying
    and pasting may have unpredictable results: formatting issues, weird quotes, and
    so on.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您跟随并运行伴随GitHub仓库中的代码。我们已尽一切努力检查文中所有代码示例。然而，对于电子版的用户，复制粘贴可能会导致不可预测的结果：格式问题、奇怪的引号等。
- en: Preparing data
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: Built-in algorithms expect the dataset to be in a certain format, such as **CSV**,
    **protobuf**, or **libsvm**. Supported formats are listed in the algorithm documentation.
    For instance, Linear Learner supports CSV and RecordIO-wrapped protobuf ([https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 内置算法要求数据集必须符合特定格式，例如**CSV**、**protobuf**或**libsvm**。支持的格式可以在算法文档中找到。例如，Linear
    Learner支持CSV和RecordIO包装的protobuf（[https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-input_output)）。
- en: 'Our input dataset is already in the repository in CSV format, so let''s use
    that. The dataset preparation will be extremely simple, and we''ll run it manually:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入数据集已经以CSV格式存储在仓库中，因此我们将使用它。数据集准备将极为简单，我们将手动运行它：
- en: 'Using `pandas`, we load the CSV dataset with pandas:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`，我们用pandas加载CSV数据集：
- en: '[PRE0]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we print the shape of the dataset:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们打印数据集的形状：
- en: '[PRE1]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It contains 506 samples and 13 columns:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它包含506个样本和13列：
- en: '[PRE2]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we display the first 5 lines of the dataset:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们显示数据集的前5行：
- en: '[PRE3]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This prints out the table visible in the following figure. For each house,
    we see 12 features, and a target attribute (`medv`) set to the median value of
    the house in thousands of dollars:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下图中可见的表格。对于每个房子，我们可以看到12个特征，以及一个目标属性（`medv`），该属性是房子的中位数价格（以千美元为单位）：
- en: '![Figure 4.1 – Viewing the dataset'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.1 – 查看数据集'
- en: '](img/B17705_04_1.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_1.jpg)'
- en: Figure 4.1 – Viewing the dataset
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.1 – 查看数据集
- en: 'Reading the algorithm documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html)),
    we see that *Amazon SageMaker requires that a CSV file doesn''t have a header
    record and that the target variable is in the first column*. Accordingly, we move
    the `medv` column to the front of the dataframe:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读算法文档（[https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html)），我们看到*Amazon
    SageMaker要求CSV文件没有头部记录，且目标变量必须位于第一列*。因此，我们将`medv`列移动到数据框的最前面：
- en: '[PRE4]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A bit of `scikit-learn` magic helps split the dataframe up into two parts –
    90% for training, and 10% for validation:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些`scikit-learn`的魔法帮助我们将数据框分为两部分——90%用于训练，10%用于验证：
- en: '[PRE5]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We save these two splits to individual CSV files, without either an index or
    a header:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这两个拆分保存为单独的CSV文件，不包含索引或头部：
- en: '[PRE6]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now need to upload these two files to S3\. We could use any bucket, and
    here we''ll use the default bucket conveniently created by SageMaker in the region
    we''re running in. We can find its name with the `sagemaker.Session.default_bucket()`
    API:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要将这两个文件上传到S3。我们可以使用任何桶，在这里我们将使用SageMaker在我们运行的区域自动创建的默认桶。我们可以通过`sagemaker.Session.default_bucket()`
    API查找其名称：
- en: '[PRE7]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we use the `sagemaker.Session.upload_data()` API to upload the two
    **CSV** files to the default bucket. Here, the training and validation datasets
    are made of a single file each, but we could upload multiple files if needed.
    For this reason, **we must upload the datasets under different S3 prefixes**,
    so that their files won''t be mixed up:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `sagemaker.Session.upload_data()` API 将两个 **CSV** 文件上传到默认存储桶。这里，训练和验证数据集每个由一个文件组成，但如果需要，我们也可以上传多个文件。因此，**我们必须将数据集上传到不同的
    S3 前缀下**，以避免文件混淆：
- en: '[PRE8]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The two S3 paths look like this. Of course, the account number in the default
    bucket name will be different:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这两个 S3 路径如下所示。自然，默认存储桶名称中的账户号码会有所不同：
- en: '[PRE9]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that data is ready in S3, we can configure the training job.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好在 S3 中，我们可以配置训练任务。
- en: Configuring a training job
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置训练任务
- en: The `Estimator` object (`sagemaker.estimator.Estimator`) is the cornerstone
    of model training. It lets you select the appropriate algorithm, define your training
    infrastructure requirements, and more.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`Estimator` 对象（`sagemaker.estimator.Estimator`）是模型训练的基石。它让你选择适当的算法，定义你的训练基础设施要求等。'
- en: 'The SageMaker SDK also includes algorithm-specific estimators, such as `sagemaker.LinearLearner`
    or `sagemaker.PCA`. I generally find them less flexible than the generic estimator
    (no CSV support, for one thing), and I don''t recommend using them. Using the
    `Estimator` object also lets you reuse your code across examples, as we will see
    in the next sections:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker SDK 还包括特定算法的估算器，例如 `sagemaker.LinearLearner` 或 `sagemaker.PCA`。我通常认为它们不如通用估算器灵活（例如不支持
    CSV 格式），并且不推荐使用它们。使用 `Estimator` 对象还可以让你在不同示例之间重用代码，正如我们在接下来的章节中所看到的：
- en: 'Earlier in this chapter, we learned that SageMaker algorithms are packaged
    in Docker containers. Using `boto3` and the `image_uris.retrieve()` API, we can
    easily find the name of the Linear Learner algorithm in the region we''re running:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章之前，我们学习了 SageMaker 算法是打包在 Docker 容器中的。通过使用 `boto3` 和 `image_uris.retrieve()`
    API，我们可以轻松找到我们运行的区域中 Linear Learner 算法的名称：
- en: '[PRE10]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we know the name of the container, we can configure our training job
    with the `Estimator` object. In addition to the container name, we also pass the
    IAM role that SageMaker instances will use, the instance type and instance count
    to use for training, as well as the output location for the model. `Estimator`
    will generate a training job automatically, and we could also set our own prefix
    with the `base_job_name` parameter:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们知道了容器的名称，我们可以使用 `Estimator` 对象来配置我们的训练任务。除了容器名称外，我们还需要传递 SageMaker 实例将使用的
    IAM 角色、用于训练的实例类型和实例数量，以及模型的输出位置。`Estimator` 会自动生成一个训练任务，我们还可以通过 `base_job_name`
    参数设置自己的前缀：
- en: '[PRE11]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: SageMaker supports plenty of different instance types, with some differences
    across AWS regions. You can find the full list at [https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html](https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html).
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SageMaker 支持多种不同类型的实例，不同的 AWS 区域之间有所差异。你可以在 [https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html](https://docs.aws.amazon.com/sagemaker/latest/dg/instance-types-az.html)
    查看完整列表。
- en: 'Which one should we use here? Looking at the Linear Learner documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances)),
    we see that *you can train the Linear Learner algorithm on single- or multi-machine
    CPU and GPU instances*. Here, we''re working with a tiny dataset, so let''s select
    the smallest training instance available in our region: `ml.m5.large`.'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们应该选择哪个呢？查看 Linear Learner 文档 ([https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html#ll-instances))，我们看到
    *你可以在单机或多机 CPU 和 GPU 实例上训练 Linear Learner 算法*。在这里，我们处理的是一个小数据集，因此让我们选择我们所在区域中最小的训练实例：`ml.m5.large`。
- en: Checking the pricing page ([https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/)),
    we see that this instance costs $0.128 per hour in the eu-west-1 region (the one
    I'm using for this job).
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看定价页面 ([https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/))，我们看到这个实例在
    eu-west-1 区域的费用是每小时 $0.128（我为此任务使用的区域）。
- en: Next, we have to set `predictor_type`. It defines the type of problem that Linear
    Learner is training on (regression, binary classification, or multiclass classification).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置 `predictor_type`。它定义了 Linear Learner 训练的目标问题类型（回归、二分类或多分类）。
- en: 'Taking a deeper look, we see that the default value for `mini_batch_size` is
    1000: this isn''t going to work well with our 506-sample dataset, so let''s set
    it to 32\. We also learn that the `normalize_data` parameter is set to true by
    default, which makes it unnecessary to normalize data ourselves:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更深入地查看，我们发现`mini_batch_size`的默认值是1000：对于我们506个样本的数据集，这个值并不适合，因此我们将其设置为32。我们还发现`normalize_data`参数默认设置为true，这使得我们不需要自己进行数据标准化：
- en: '[PRE12]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s define the data channels: a channel is a named source of data passed
    to a SageMaker estimator. All built-in algorithms need at least a training channel,
    and many also accept additional channels for validation and testing. Here, we
    have two channels, which both provide data in CSV format. The `TrainingInput()`
    API lets us define their location, their format, whether they are compressed,
    and so on:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义数据通道：通道是传递给SageMaker估算器的命名数据源。所有内置算法至少需要一个训练通道，许多算法还接受用于验证和测试的额外通道。在这里，我们有两个通道，它们都提供CSV格式的数据。`TrainingInput()`
    API让我们定义数据的位置、格式、是否压缩等：
- en: '[PRE13]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By default, data served by a channel will be fully copied to each training instance,
    which is fine for small datasets. We'll study alternatives in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Advanced Training Techniques*.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 默认情况下，通道提供的数据会完全复制到每个训练实例，这对于小数据集来说没问题。在[*第10章*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206)《高级训练技术》中，我们将研究其他方法。
- en: Everything is now ready for training, so let's launch our job.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，接下来我们启动训练任务。
- en: Launching a training job
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动训练任务
- en: 'All it takes is one line of code:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 只需要一行代码：
- en: 'We simply pass a Python dictionary containing the two channels to the `fit()`
    API:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需将包含两个通道的Python字典传递给`fit()` API：
- en: '[PRE14]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Immediately, the training job starts:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练任务立即开始：
- en: '[PRE15]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As soon as the job is launched, it appears in the **SageMaker components and
    registries** | **Experiments and trials** panel. There, you can see all job metadata:
    the location of the dataset, hyperparameters, and more.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦任务启动，它就会出现在**SageMaker组件和注册表** | **实验和试验**面板中。在那里，你可以看到所有任务的元数据：数据集的位置、超参数等。
- en: 'The training log is visible in the notebook, and it''s also stored in Amazon
    CloudWatch Logs, under the `/aws/sagemaker/TrainingJobs` prefix. Here are the
    first few lines, showing the infrastructure being provisioned, as explained earlier,
    in the *Using fully managed infrastructure* section:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练日志在笔记本中可见，同时也存储在Amazon CloudWatch Logs中，路径为`/aws/sagemaker/TrainingJobs`。这里是前几行，显示了基础设施的配置过程，如前面在*使用完全托管的基础设施*部分所解释的：
- en: '[PRE16]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'At the end of the training log, we see information on the **mean square error**
    (**MSE**) and loss metrics:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练日志的末尾，我们看到有关**均方误差**（**MSE**）和损失指标的信息：
- en: '[PRE17]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once training is complete, the model is copied automatically to S3, and SageMaker
    tells us how long the job took:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，模型会自动复制到S3，SageMaker会告诉我们训练任务花费的时间：
- en: '[PRE18]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Looking at the output location in our S3 bucket, we see the model artifact:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看S3存储桶中的输出位置，我们看到了模型工件：
- en: '[PRE19]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We'll see in [*Chapter 11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237),
    *Deploying Machine Learning Models*, what's inside that artifact, and how to deploy
    the model outside of SageMaker. For now, let's deploy it to a real-time endpoint.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第11章*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237)，《部署机器学习模型》中，了解该工件内部的内容以及如何在SageMaker外部部署模型。现在，我们将模型部署到实时端点。
- en: Deploying a model
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型
- en: 'This is my favorite part of SageMaker; we only need one line of code to deploy
    a model to an **HTTPS endpoint**:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我最喜欢的SageMaker部分；我们只需要一行代码就能将模型部署到**HTTPS端点**：
- en: 'It''s good practice to create identifiable and unique endpoint names. We could
    also let SageMaker create one for us during deployment:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最好的做法是创建可识别且唯一的端点名称。我们也可以在部署时让SageMaker为我们创建一个端点名称：
- en: '[PRE20]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, the endpoint name is `linear-learner-demo-29-08-37-25`.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，端点名称是`linear-learner-demo-29-08-37-25`。
- en: 'We deploy the model using the `deploy()` API. As this is a test endpoint, we
    use the smallest endpoint instance available, `ml.t2.medium`. In the eu-west-1
    region, this will only cost us $0.07 per hour:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`deploy()` API部署模型。由于这是一个测试端点，我们使用可用的最小端点实例`ml.t2.medium`。在欧盟西部1区，这将仅花费我们每小时0.07美元：
- en: '[PRE21]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When the endpoint is created, we can see it in the **SageMaker components and
    registries** | **Endpoints** panel in SageMaker Studio.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当端点创建完成后，我们可以在**SageMaker组件和注册表** | **端点**面板中看到它：
- en: 'A few minutes later, the endpoint is in service. We can use the `predict()`
    API to send it a CSV sample for prediction. We set serialization using built-in
    functions:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，端点开始服务。我们可以使用 `predict()` API 发送一个 CSV 样本进行预测。我们使用内置函数设置序列化：
- en: '[PRE22]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The prediction output tells us that this house should cost $30,173:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测输出告诉我们，这栋房子的预计价格为 $30,173：
- en: '[PRE23]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can also predict multiple samples at a time:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还可以一次预测多个样本：
- en: '[PRE24]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now the prediction output is as follows:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在预测输出如下：
- en: '[PRE25]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When we're done working with the endpoint, **we shouldn't forget to delete it
    to avoid unnecessary charges**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成端点的工作时，**我们不应忘记删除它以避免不必要的费用**。
- en: Cleaning up
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理工作
- en: 'Deleting an endpoint is as simple as calling the `delete_endpoint()` API:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 删除端点就像调用 `delete_endpoint()` API 一样简单：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At the risk of repeating myself, the topics covered in this section are extremely
    important, so please make sure you''re completely familiar with them, as we''ll
    constantly use them in the rest of the book. Please spend some time reading the
    service and SDK documentation as well:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 再次重申，本节涵盖的主题非常重要，请确保您完全熟悉它们，因为我们将在本书的其余部分经常使用它们。请花些时间阅读服务和 SDK 文档：
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)'
- en: '[https://sagemaker.readthedocs.io](https://sagemaker.readthedocs.io)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://sagemaker.readthedocs.io](https://sagemaker.readthedocs.io)'
- en: Now let's explore other built-in algorithms. You'll see that the workflow and
    the code are very similar!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索其他内置算法。您将看到工作流程和代码非常相似！
- en: Working with more built-in algorithms
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用更多内置算法：
- en: 'In the rest of this chapter, we will run more examples with built-in algorithms,
    both in supervised and unsupervised mode. This will help you become very familiar
    with the SageMaker SDK and learn how to solve actual machine learning problems.
    The following list shows some of these algorithms:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将使用内置算法运行更多示例，包括监督和无监督模式。这将帮助您非常熟悉 SageMaker SDK，并学习如何解决实际的机器学习问题。以下列表显示了其中一些算法：
- en: Classification with XGBoost
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 XGBoost 进行分类：
- en: Recommendation with Factorization Machines
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因子分解机进行推荐：
- en: Dimensionality reduction with PCA
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PCA 进行降维：
- en: Anomaly detection with Random Cut Forest
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Random Cut Forest 进行异常检测：
- en: Regression with XGBoost
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 XGBoost 进行回归：
- en: 'Let''s train a model on the Boston Housing dataset with the **XGBoost** algorithm
    ([https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)). As we will
    see in [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending
    Machine Learning Services Using Built-In Frameworks* , SageMaker also supports
    XGBoost scripts:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 **XGBoost** 算法在波士顿房屋数据集上训练模型（[https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)）。正如我们将在
    [*第 7 章*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130) 中看到的，*使用内置框架扩展机器学习服务*，SageMaker
    也支持 XGBoost 脚本：
- en: We reuse the dataset preparation steps from the previous examples.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重复了前面示例中的数据集准备步骤。
- en: 'We find the name of the XGBoost container. As several versions are supported,
    we select the latest one (1.3.1 at the time of writing):'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们找到 XGBoost 容器的名称。由于支持多个版本，我们选择最新版本（写作时为 1.3.1）：
- en: '[PRE27]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We configure the `Estimator` function. The code is strictly identical to the
    code used with `LinearLearner`:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置 `Estimator` 函数。代码与使用 `LinearLearner` 时完全相同：
- en: '[PRE28]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Taking a look at the hyperparameters ([https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)),
    we see that the only required one is `num_round`. As it''s not obvious which value
    to set, we''ll go for a large value, and we''ll also define the `early_stopping_rounds`
    parameter in order to avoid overfitting. Of course, we need to set the objective
    for a regression problem:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看超参数（[https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)），我们看到唯一需要的是
    `num_round`。因为不明确应设置哪个值，我们将选择一个较大的值，并且还会定义 `early_stopping_rounds` 参数以避免过拟合。当然，我们需要为回归问题设置目标：
- en: '[PRE29]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We define the training input, just like in the previous example:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义训练输入，就像前面的示例一样：
- en: '[PRE30]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then launch the training job:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们启动训练作业：
- en: '[PRE31]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The job only ran for 22 rounds, meaning that **early stopping** was triggered.
    Looking at the training log, we see that round #12 was actually the best one,
    with a **root mean square error** (**RMSE**) of 2.43126:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务只运行了22轮，这意味着触发了**早停**。从训练日志中看到，第12轮实际上是最佳的，**均方根误差**（**RMSE**）为2.43126：
- en: '[PRE32]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Deploying still takes one line of code:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署仍然只需要一行代码：
- en: '[PRE33]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once the model is deployed, we use the `predict()` API again to send it a CSV
    sample:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型部署完成，我们再次使用`predict()` API发送一个CSV样本：
- en: '[PRE34]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The result tells us that this house should cost $23,754:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果告诉我们，这套房子应该值23,754美元：
- en: '[PRE35]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we delete the endpoint when we''re done:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当我们完成任务时，删除端点：
- en: '[PRE36]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, the SageMaker workflow is pretty simple and makes it easy to
    experiment quickly with different algorithms without having to rewrite all your
    code.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，SageMaker的工作流程非常简单，使我们能够快速地尝试不同的算法，而无需重写所有代码。
- en: Let's move on to the Factorization Machines algorithm. In the process, we will
    learn about the highly efficient RecordIO-wrapped protobuf format.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讲解因式分解机算法。在这个过程中，我们将了解高效的RecordIO封装protobuf格式。
- en: Recommendation with Factorization Machines
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用因式分解机进行推荐
- en: Factorization Machines is a generalization of linear models ([https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)).
    It's well-suited for high-dimension sparse datasets, such as user-item interaction
    matrices for recommendation.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因式分解机是线性模型的广义化（[https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)）。它非常适合高维稀疏数据集，如推荐系统中的用户-物品交互矩阵。
- en: In this example, we're going to train a recommendation model based on the **MovieLens**
    dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将基于**MovieLens**数据集训练一个推荐模型（[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)）。
- en: The dataset exists in several versions. To minimize training times, we'll use
    the 100k version. It contains 100,000 ratings (integer values from 1 to 5) assigned
    by 943 users to 1,682 movies. The dataset is already split for training and validation.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集有多个版本。为了减少训练时间，我们将使用100k版本。它包含943个用户对1,682部电影的100,000条评分（评分值从1到5）。数据集已经划分为训练集和验证集。
- en: As you know by now, training and deploying with SageMaker is very simple. Most
    of the code will be identical to the two previous examples, which is great! This
    lets us focus on understanding and preparing data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在所知道的，使用SageMaker进行训练和部署非常简单。大部分代码将与之前的两个示例完全相同，这很棒！这样我们可以专注于理解和准备数据。
- en: Understanding sparse datasets
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解稀疏数据集
- en: 'Imagine building a matrix to store this dataset. It would have 943 lines (one
    per user) and 1,682 columns (one per movie). Cells would store the ratings. The
    following diagram shows a basic example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 想象构建一个矩阵来存储这个数据集。它会有943行（每行代表一个用户）和1,682列（每列代表一部电影）。单元格中存储评分。下图展示了一个基本的示例：
- en: '![Figure 4.2 – Sparse matrix'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 稀疏矩阵'
- en: '](img/B17705_04_2.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_04_2.jpg)'
- en: Figure 4.2 – Sparse matrix
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 稀疏矩阵
- en: Hence, the matrix would have 943*1,682=1,586,126 cells. However, as only 100,000
    ratings are present, 93.69% of cells would be empty. Storing our dataset this
    way would be extremely inefficient. It would needlessly consume RAM, storage,
    and network bandwidth to store and transfer lots of zero values!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，矩阵将有943*1,682=1,586,126个单元格。然而，由于只有100,000条评分，93.69%的单元格将是空的。以这种方式存储数据集将非常低效，浪费RAM、存储和网络带宽来存储和传输大量零值！
- en: 'In fact, things are much worse, as the algorithm expects the input dataset
    to look like in the following diagram:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，情况更糟，因为算法希望输入数据集看起来像下图所示：
- en: '![Figure 4.3 – Sparse matrix'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3 – 稀疏矩阵'
- en: '](img/B17705_04_3.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_04_3.jpg)'
- en: Figure 4.3 – Sparse matrix
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 稀疏矩阵
- en: 'Why do we need to store data this way? The answer is simple: Factorization
    Machines is a **supervised learning** algorithm, so we need to train it on labeled
    samples.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要以这种方式存储数据？答案很简单：因式分解机是一个**监督学习**算法，所以我们需要用带标签的样本来训练它。
- en: Looking at the preceding diagram, we see that each line represents a movie review.
    The matrix on the left stores its one-hot encoded features (users and movies),
    and the vector on the right stores its label. For instance, the last line tells
    us that user 4 has given movie 5 a "5" rating.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示可以看到，每一行代表一个电影评分。左边的矩阵存储它的独热编码特征（用户和电影），而右边的向量存储它的标签。例如，最后一行告诉我们，用户4给电影5打了“5”分。
- en: The size of this matrix is 100,000 lines by 2,625 columns (943 movies plus 1,682
    movies). The total number of cells is 262,500,000, which are only 0.076% full
    (200,000 / 262,500,000). If we used a 32-bit value for each cell, we would need
    almost a gigabyte of memory to store this matrix. This is horribly inefficient
    but still manageable.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的大小是 100,000 行和 2,625 列（943 部电影加 1,682 部电影）。总共有 262,500,000 个单元格，其中只有 0.076%
    被填充（200,000 / 262,500,000）。如果我们为每个单元格使用一个 32 位的值，我们需要接近一 GB 的内存来存储这个矩阵。这非常低效，但仍然可以管理。
- en: 'Just for fun, let''s do the same exercise for the largest version of MovieLens,
    which has 25 million ratings, 62,000 movies, and 162,000 users. The matrix would
    have 25 million lines and 224,000 columns, for a total of 5,600,000,000,000 cells.
    Yes, that''s 5.6 trillion cells, and although they would be 99.999% empty, we
    would still need over 20 terabytes of RAM to store them. Ouch. If that''s not
    bad enough, consider recommendation models with millions of users and products:
    the numbers are mind-boggling!'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了好玩，让我们以 MovieLens 的最大版本为例，它有 2500 万条评分，62,000 部电影和 162,000 名用户。这个矩阵将有 2500
    万行和 224,000 列，总共 5,600,000,000,000 个单元格。是的，这是 5.6 万亿个单元格，尽管它们有 99.999% 是空的，但我们仍然需要超过
    20 TB 的内存来存储它们。哎呀。如果这还不够糟糕，想想推荐模型，它们有数百万的用户和产品：这些数字令人咋舌！
- en: Instead of using a plain matrix, we'll use a `SciPy` has exactly the object
    we need, named `lil_matrix` ([https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html)).
    This will help us to get rid of all these nasty zeros.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会使用普通的矩阵，而是使用一个 `SciPy` 提供的对象，名为 `lil_matrix` （[https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html)）。这将帮助我们去除所有这些讨厌的零。
- en: Understanding protobuf and RecordIO
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 protobuf 和 RecordIO
- en: So how will we pass this sparse matrix to the SageMaker algorithm? As you would
    expect, we're going to serialize the object and store it in S3\. We're not going
    to use Python serialization, however. Instead, we're going to use `protobuf` ([https://developers.google.com/protocol-buffers/](https://developers.google.com/protocol-buffers/)),
    a popular and efficient serialization mechanism.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将这个稀疏矩阵传递给 SageMaker 算法呢？正如你所预期的，我们将序列化这个对象并将其存储在 S3 中。但我们不会使用 Python
    的序列化方法。相反，我们将使用 `protobuf` （[https://developers.google.com/protocol-buffers/](https://developers.google.com/protocol-buffers/)），这是一种流行且高效的序列化机制。
- en: 'In addition, we''re going to store the protobuf-encoded data in a record format
    called **RecordIO** ([https://mxnet.apache.org/api/faq/recordio/](https://mxnet.apache.org/api/faq/recordio/)).
    Our dataset will be stored as a sequence of records in a single file. This has
    the following benefits:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将把 protobuf 编码的数据存储在一种名为 **RecordIO** 的记录格式中（[https://mxnet.apache.org/api/faq/recordio/](https://mxnet.apache.org/api/faq/recordio/)）。我们的数据集将作为一系列记录存储在单个文件中。这有以下几个好处：
- en: 'A single file is easier to move around: who wants to deal with thousands of
    individual files that can get lost or corrupted?'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个文件更容易移动：谁想处理成千上万的单独文件，这些文件可能会丢失或损坏呢？
- en: A sequential file is faster to read, which makes the training process more efficient.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序文件读取速度更快，这使得训练过程更加高效。
- en: A sequence of records is easy to split for distributed training.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列记录便于分割用于分布式训练。
- en: Don't worry if you're not familiar with protobuf and RecordIO. The SageMaker
    SDK includes utility functions that hide their complexity.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉 protobuf 和 RecordIO，不用担心。SageMaker SDK 包含隐藏其复杂性的实用函数。
- en: Building a Factorization Machines model on MovieLens
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 MovieLens 上构建因式分解机模型
- en: 'We will begin building the model using the following steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤开始构建模型：
- en: 'In a Jupyter notebook, we first download and extract the MovieLens dataset:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Jupyter notebook 中，我们首先下载并提取 MovieLens 数据集：
- en: '[PRE37]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As the dataset is ordered by user ID, we shuffle it as a precaution. Then,
    we take a look at the first few lines:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据集是按用户 ID 排序的，我们进行了一次洗牌作为预防措施。然后，我们查看前几行：
- en: '[PRE38]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We see four columns: the user ID, the movie ID, the rating, and a timestamp
    (which we''ll ignore in our model):'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到了四列：用户 ID、电影 ID、评分和时间戳（我们将在模型中忽略时间戳）：
- en: '[PRE39]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We define sizing constants:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了大小常数：
- en: '[PRE40]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s write a function to load a dataset into a sparse matrix. Based
    on the previous explanation, we go through the dataset line by line. In the X
    matrix, we set the appropriate user and movie columns to `1`. We also store the
    rating in the Y vector:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个函数，将数据集加载到稀疏矩阵中。根据之前的解释，我们逐行遍历数据集。在 X 矩阵中，我们将相应的用户和电影列设置为 `1`。我们还将评分存储在
    Y 向量中：
- en: '[PRE41]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We then process the training and test datasets:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们处理训练集和测试集：
- en: '[PRE42]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We check that the shapes are what we expect:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查形状是否符合预期：
- en: '[PRE43]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This displays the dataset shapes:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示数据集的形状：
- en: '[PRE44]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s write a function that converts a dataset to RecordIO-wrapped `protobuf`,
    and uploads it to an S3 bucket. We first create an in-memory binary stream with
    `io.BytesIO()`. Then, we use the lifesaving `write_spmatrix_to_sparse_tensor()`
    function to write the sample matrix and the label vector to that buffer in `protobuf`
    format. Finally, we use `boto3` to upload the buffer to S3:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个函数，将数据集转换为 RecordIO 封装的 `protobuf` 格式，并将其上传到 S3 桶中。我们首先使用 `io.BytesIO()`
    创建一个内存中的二进制流。然后，我们使用至关重要的 `write_spmatrix_to_sparse_tensor()` 函数，将样本矩阵和标签向量以 `protobuf`
    格式写入该缓冲区。最后，我们使用 `boto3` 将缓冲区上传到 S3：
- en: '[PRE45]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Had our data been stored in a `numpy` array instead of `lilmatrix`, we would
    have used the `write_numpy_to_dense_tensor()` function instead. It has the same
    effect.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们的数据存储在 `numpy` 数组中，而不是 `lilmatrix`，我们将使用 `write_numpy_to_dense_tensor()`
    函数。它的效果是一样的。
- en: 'We apply this function to both datasets, and we store their S3 paths:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将此函数应用于两个数据集，并存储它们的 S3 路径：
- en: '[PRE46]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Taking a look at the S3 bucket in a terminal, we see that the training dataset
    only takes 5.5 MB. The combination of sparse matrix, protobuf, and RecordIO has
    paid off:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端查看 S3 桶时，我们看到训练数据集仅占用 5.5 MB。稀疏矩阵、protobuf 和 RecordIO 的结合取得了效果：
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'What comes next is SageMaker business as usual. We find the name of the Factorization
    Machines container, configure the `Estimator` function, and set the hyperparameters:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的过程是 SageMaker 的常规操作。我们找到分解机容器的名称，配置 `Estimator` 函数，并设置超参数：
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Looking at the documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html)),
    we see that the required hyperparameters are `feature_dim`, `predictor_type`,
    and `num_factors`. The default setting for `epochs` is `1`, which feels a little
    low, so we use `10` instead.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查阅文档（[https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-hyperparameters.html)），我们看到所需的超参数为
    `feature_dim`、`predictor_type` 和 `num_factors`。`epochs` 的默认设置为 `1`，这感觉有点低，因此我们将其设置为
    `10`。
- en: 'We then launch the training job. Did you notice that we didn''t configure training
    inputs? We''re simply passing the location of the two `protobuf` files. As `protobuf`
    is the default format for Factorization Machines (as well as other built-in algorithms),
    we can save a step:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们启动训练作业。你有没有注意到我们没有配置训练输入？我们只是传递了两个 `protobuf` 文件的位置。由于 `protobuf` 是分解机（以及其他内置算法）的默认格式，我们可以省略这一步：
- en: '[PRE49]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once the job is over, we deploy the model to a real-time endpoint:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业完成后，我们将模型部署到实时端点：
- en: '[PRE50]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We''ll now send samples to the endpoint in JSON format (https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html#fm-inputoutput).
    For this purpose, we write a custom serializer to convert input data to JSON.
    The default JSON deserializer will be used automatically since we set the content
    type to `''application/json''`:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将以 JSON 格式（https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html#fm-inputoutput）将样本发送到端点。为此，我们编写一个自定义序列化器，将输入数据转换为
    JSON。由于我们将内容类型设置为 `'application/json'`，默认的 JSON 反序列化器将自动使用：
- en: '[PRE51]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We send the first three samples of the test set for prediction:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们发送测试集的前三个样本进行预测：
- en: '[PRE52]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The prediction looks like this:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测结果如下：
- en: '[PRE53]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Using this model, we could fill all the empty cells in the recommendation matrix.
    For each user, we would simply predict the score of all movies, and store, say,
    the top 50 movies. That information would be stored in a backend, and the corresponding
    metadata (title, genre, and so on) would be displayed to the user in a frontend
    application.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个模型，我们可以填充推荐矩阵中的所有空白单元。对于每个用户，我们只需预测所有电影的评分，并存储，例如，排名前 50 的电影。这些信息将存储在后台，前端应用程序中将显示相应的元数据（如标题、类型等）。
- en: 'Finally, we delete the endpoint:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们删除端点：
- en: '[PRE54]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: So far, we've only used supervised learning algorithms. In the next section,
    we'll move on to unsupervised learning with Principal Component Analysis.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅使用了监督学习算法。在接下来的部分，我们将转向使用主成分分析进行无监督学习。
- en: Using Principal Component Analysis
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用主成分分析
- en: '`protobuf` dataset built in the Factorization Machines example. Its 2,625 columns
    are a good candidate for dimensionality reduction! We will use PCA by taking the
    following steps:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在因子分解机示例中建立的`protobuf`数据集。它的2,625列非常适合降维！我们将采用PCA，步骤如下：
- en: 'Starting from the processed dataset, we configure `Estimator` for PCA. By now,
    you should (almost) be able to do this with your eyes closed:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从处理过的数据集开始，我们为PCA配置`Estimator`。现在，你应该（几乎）可以闭着眼睛做到这一点：
- en: '[PRE55]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We then set the hyperparameters. The required ones are the initial number of
    features, the number of principal components to compute, and the batch size:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们设置超参数。必需的是初始特征数、计算主成分的数量以及批处理大小：
- en: '[PRE56]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We train and deploy the model:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练并部署模型：
- en: '[PRE57]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then, we predict the first test sample, using the same serialization code as
    in the previous example:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们预测第一个测试样本，使用与前一个示例相同的序列化代码：
- en: '[PRE58]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This prints out the 64 principal components of the test sample. In real life,
    we typically would process the dataset with this model, save the results, and
    use them to train a regression model:'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出测试样本的64个主要成分。在实际生活中，我们通常会使用这个模型处理数据集，保存结果，并用它们来训练回归模型：
- en: '[PRE59]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Don't forget to delete the endpoint when you're done. Then, let's run one more
    unsupervised learning example to conclude this chapter!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成时别忘了删除端点。接着，让我们运行另一个无监督学习的例子来结束这一章节！
- en: Detecting anomalies with Random Cut Forest
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林检测异常
- en: '**Random Cut Forest** (**RCF**) is an unsupervised learning algorithm for anomaly
    detection ([https://proceedings.mlr.press/v48/guha16.pdf](https://proceedings.mlr.press/v48/guha16.pdf)).
    We''re going to apply it to a subset of the household electric power consumption
    dataset ([https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)),
    available in the GitHub repository for this book. Data is aggregated hourly over
    a period of a little less than a year (just under 8,000 values):'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林** (**RCF**) 是一种用于异常检测的无监督学习算法（[https://proceedings.mlr.press/v48/guha16.pdf](https://proceedings.mlr.press/v48/guha16.pdf)）。我们将把它应用于家庭电力消耗数据集的一个子集（[https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)），该数据集可在本书的
    GitHub 仓库中获取。数据在接近一年的时间内按小时聚合（大约不到8,000个值）：'
- en: 'In a Jupyter notebook, we load the dataset with `pandas`, and we display the
    first few lines:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter笔记本中，我们使用`pandas`加载数据集，并显示前几行：
- en: '[PRE60]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'As shown in the following screenshot, the dataset has three columns – an hourly
    timestamp, the power consumption value (in kilowatt-hours), and the client ID:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如下截图所示，数据集有三列 - 每小时时间戳、功耗值（以千瓦时计算）、客户ID：
- en: '![Figure 4.4 – Viewing the columns'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.4 – 查看列'
- en: '](img/B17705_04_4.jpg)'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_4.jpg)'
- en: Figure 4.4 – Viewing the columns
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.4 – 查看列
- en: 'Using `matplotlib`, we plot the dataset to get a quick idea of what it looks
    like:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`，我们绘制数据集以快速了解其外观：
- en: '[PRE61]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The plot is shown in the following diagram. We see three time series corresponding
    to three different clients:'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了绘图结果。我们看到三个时间序列对应于三个不同的客户：
- en: '![Figure 4.5 – Viewing the dataset'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.5 – 查看数据集'
- en: '](img/B17705_04_5.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_5.jpg)'
- en: Figure 4.5 – Viewing the dataset
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.5 – 查看数据集
- en: 'There are two issues with this dataset. First, it contains several time series:
    RCF can only train a model on a single series. Second, RCF requires `pandas` –
    we only keep the `"client_12"` time series, we multiply its values by 100, and
    cast them to the integer type:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个数据集存在两个问题。首先，它包含多个时间序列：RCF只能在单个序列上训练模型。其次，RCF需要`pandas` - 我们只保留了`"client_12"`时间序列，将其值乘以100，并转换为整数类型：
- en: '[PRE62]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The following diagram shows the first lines of the transformed dataset:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了转换后数据集的前几行：
- en: '![Figure 4.6 – The values of the first lines'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.6 – 第一行的值'
- en: '](img/B17705_04_6.jpg)'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_6.jpg)'
- en: Figure 4.6 – The values of the first lines
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.6 – 第一行的值
- en: We plot it again to check that it looks as expected. Note the large drop right
    after step 2000, highlighted by a box in the following screenshot. This is clearly
    an anomaly, and hopefully, our model will catch it:![Figure 4.7 – Viewing a single
    time series
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次绘制它以检查它的预期外观。请注意在第2000步后的大幅下降，这在下图的框中得到了突出显示。这显然是一个异常，希望我们的模型能够捕捉到它：![图
    4.7 – 查看单个时间序列
- en: '](img/B17705_04_7.jpg)'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_7.jpg)'
- en: Figure 4.7 – Viewing a single time series
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.7 – 查看单一时间序列
- en: 'As in the previous examples, we save the dataset to a CSV file, which we upload
    to S3:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与前面的示例一样，我们将数据集保存到 CSV 文件中，并上传到 S3：
- en: '[PRE63]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Then, we define the `label_size=1`). Even though the training channel never
    has labels, we still need to tell RCF.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义 `label_size=1`）。即使训练通道从未包含标签，我们仍然需要告诉 RCF。
- en: 'Second, the only `ShardedByS3Key`. This policy splits the dataset across the
    different instances in the training cluster, instead of sending them a full copy.
    We won''t run distributed training here, but we need to set that policy nonetheless:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，唯一的 `ShardedByS3Key`。该策略将数据集跨训练集群中的不同实例进行拆分，而不是发送完整副本。我们不会在这里运行分布式训练，但仍然需要设置该策略：
- en: '[PRE64]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The rest is business as usual: train and deploy! Once again, we reuse the code
    for the previous examples, and it''s almost unchanged:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其余部分照常：训练并部署！我们再次重用前面示例中的代码，几乎没有改变：
- en: '[PRE65]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'After a few minutes, the model is deployed. We convert the input time series
    to a Python list, and we send it to the endpoint for prediction. We use CSV and
    JSON, respectively, for serialization and deserialization:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，模型已被部署。我们将输入的时间序列转换为 Python 列表，然后发送到端点进行预测。我们分别使用 CSV 和 JSON 进行序列化和反序列化：
- en: '[PRE66]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The response contains the anomaly score for each value in the time series.
    It looks like this:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 响应包含时间序列中每个值的异常分数。它看起来是这样的：
- en: '[PRE67]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We then convert this response to a Python list, and we then compute its mean
    and its standard deviation:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将此响应转换为 Python 列表，并计算其均值和标准差：
- en: '[PRE68]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We plot a subset of the time series and the corresponding scores. Let''s focus
    on the "[2000:2500]" interval, as this is where we saw a large drop. We also plot
    a line representing the mean plus three standard deviations (99.7% of the score
    distribution) – any score largely exceeding the line is likely to be an anomaly:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们绘制时间序列的一个子集以及相应的分数。让我们集中关注 "[2000:2500]" 区间，因为这是我们看到大幅下降的地方。我们还绘制了代表均值加三倍标准差（得分分布的99.7%）的线——任何大大超过该线的得分都可能是异常：
- en: '[PRE69]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The drop is clearly visible in the following diagram:'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下降在以下图表中显而易见：
- en: '![Figure 4.8 – Zooming in on an anomaly'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.8 – 聚焦于异常'
- en: '](img/B17705_04_8.jpg)'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_8.jpg)'
- en: Figure 4.8 – Zooming in on an anomaly
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.8 – 聚焦于异常
- en: 'As you can see on the following score plot, its score is sky high! Beyond a
    doubt, this value is an anomaly:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如下图所示，其得分非常高！毫无疑问，这个值是一个异常：
- en: '![Figure 4.9 – Viewing anomaly scores'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.9 – 查看异常分数'
- en: '](img/B17705_04_9.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_04_9.jpg)'
- en: Figure 4.9 – Viewing anomaly scores
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.9 – 查看异常分数
- en: Exploring other intervals of the time series, we could certainly find more.
    Who said machine learning wasn't fun?
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索时间序列的其他区间，我们肯定能找到更多。谁说机器学习不有趣？
- en: 'Finally, we delete the endpoint:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们删除端点：
- en: '[PRE70]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Having gone through five complete examples, you should now be familiar with
    built-in algorithms, the SageMaker workflow, and the SDK. To fully master these
    topics, I would recommend experimenting with your datasets and running additional
    examples available at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 经过五个完整示例后，你现在应该对内置算法、SageMaker 工作流以及 SDK 有了熟悉。为了更深入地掌握这些主题，我建议你尝试自己的数据集，并运行
    [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms)
    中的其他示例。
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As you can see, built-in algorithms are a great way to quickly train and deploy
    models without having to write any machine learning code.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，内置算法是快速训练和部署模型的好方法，无需编写任何机器学习代码。
- en: In this chapter, you learned about the SageMaker workflow, and how to implement
    it with a handful of APIs from the SageMaker SDK, without ever worrying about
    infrastructure.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了 SageMaker 工作流，并学习了如何通过 SageMaker SDK 中的一些 API 实现它，而无需担心基础设施。
- en: 'You learned how to work with data in CSV and RecordIO-wrapped protobuf format,
    the latter being the preferred format for large-scale training on bulky datasets.
    You also learned how to build models with important algorithms for supervised
    and unsupervised learning: Linear Learner, XGBoost, Factorization Machines, PCA,
    and Random Cut Forest.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了如何处理CSV格式和RecordIO封装的protobuf格式数据，后者是大规模训练庞大数据集时推荐的格式。你还学会了如何使用重要的算法来构建监督学习和无监督学习模型：线性学习器（Linear
    Learner）、XGBoost、因式分解机（Factorization Machines）、主成分分析（PCA）和随机切割森林（Random Cut Forest）。
- en: In the next chapter, you will learn how to use additional built-in algorithms
    to build computer vision models.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用额外的内置算法来构建计算机视觉模型。
