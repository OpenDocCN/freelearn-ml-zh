- en: Stacking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠
- en: '**Stacking** is the second ensemble learning technique that we will study.
    Together with voting, it belongs to the non-generative methods class, as they
    both use individually trained classifiers as base learners.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**是我们将要研究的第二种集成学习技术。与投票一起，它属于非生成方法类别，因为它们都使用单独训练的分类器作为基础学习器。'
- en: In this chapter, we will present the main ideas behind stacking, its strengths
    and weaknesses, and how to select base learners. Furthermore, we will go through
    the processes of implementing stacking for both regression and classification
    problems with scikit-learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍堆叠的主要思想、优缺点，以及如何选择基础学习器。此外，我们还将介绍如何使用 scikit-learn 实现回归和分类问题的堆叠过程。
- en: 'The main topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主要主题如下：
- en: The methodology of stacking and using a meta-learner to combine predictions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠的方法论及使用元学习器组合预测
- en: The motivation behind using stacking
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用堆叠的动机
- en: The strengths and weaknesses of stacking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠的优缺点
- en: Selecting base learners for an ensemble
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择集成的基础学习器
- en: Implementing stacking for regression and classification problems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现堆叠回归和分类问题
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解 Python 的约定和语法。最后，熟悉 NumPy 库将大大帮助读者理解一些自定义算法的实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter04)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter04)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2XJgyD2](http://bit.ly/2XJgyD2).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，查看代码的实际应用：[http://bit.ly/2XJgyD2](http://bit.ly/2XJgyD2)。
- en: Meta-learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元学习
- en: '**Meta-learning** is a broad machine learning term. It has a number of meanings,
    but it generally entails utilizing metadata for a specific problem in order to
    solve it. Its applications range from solving a problem more efficiently, to designing
    entirely new learning algorithms. It is a growing research field that has recently
    yielded impressive results by designing novel deep learning architectures.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**元学习**是一个广泛的机器学习术语。它有多重含义，但通常指的是利用特定问题的元数据来解决该问题。它的应用范围从更高效地解决问题，到设计全新的学习算法。它是一个日益发展的研究领域，最近通过设计新颖的深度学习架构取得了令人瞩目的成果。'
- en: Stacking
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠
- en: Stacking is a form of meta-learning. The main idea is that we use base learners
    in order to generate metadata for the problem's dataset and then utilize another
    learner called a meta-learner, in order to process the metadata. Base learners
    are considered to be level 0 learners, while the meta learner is considered a
    level 1 learner. In other words, the meta learner is stacked on top of the base
    learners, hence the name stacking.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠是一种元学习形式。其主要思想是，我们使用基础学习器生成问题数据集的元数据，然后利用另一种学习器——元学习器，来处理这些元数据。基础学习器被视为0级学习器，而元学习器则被视为1级学习器。换句话说，元学习器堆叠在基础学习器之上，因此得名堆叠。
- en: 'A more intuitive way to describe the ensemble is to present an analogy with
    voting. In voting, we combined a number of base learners'' predictions in order
    to increase their performance. In stacking, instead of explicitly defining the
    combination rule, we train a model that learns how to best combine the base learners''
    predictions. The meta-learner''s input dataset consists of the base learners''
    predictions (metadata), as shown in figure:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更直观的集成描述方式是通过投票类比。在投票中，我们结合多个基础学习器的预测，以提高它们的性能。在堆叠中，我们不是明确地定义组合规则，而是训练一个模型，学习如何最好地结合基础学习器的预测。元学习器的输入数据集由基础学习器的预测（元数据）组成，如下图所示：
- en: '![](img/8becb6b8-de8a-4fb7-9718-921f8c998776.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8becb6b8-de8a-4fb7-9718-921f8c998776.png)'
- en: Stacking ensemble data flow, from original data to the base learners, creating
    metadata for the meta-learner
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠集成数据流，从原始数据到基础学习器，生成元学习器的元数据
- en: Creating metadata
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建元数据
- en: As mentioned earlier, we need metadata in order to both train and operate our
    ensemble. During the operation phase, we simply pass the data from our base learners.
    On the other hand, the training phase is a little more complicated. We want our
    meta-learner to discover strengths and weaknesses between our base learners. Although
    some would argue that we could train the base learners on the train set, predict
    on it, and use the predictions in order to train our meta-learner, this would
    induce variance. Our meta-learner would discover the strengths and weaknesses
    of data that has already been seen (by the base learners). As we want to generate
    models with decent predictive (out-of-sample) performance, instead of descriptive
    (in-sample) capabilities, another approach must be utilized.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要元数据来训练和操作我们的集成模型。在操作阶段，我们仅需传递基本学习器的数据。另一方面，训练阶段稍微复杂一些。我们希望元学习器能够发现基本学习器之间的优缺点。尽管有人认为我们可以在训练集上训练基本学习器，对其进行预测，并使用这些预测来训练我们的元学习器，但这会引入方差。我们的元学习器将发现已经被基本学习器“看到”的数据的优缺点。由于我们希望生成具有良好预测（样本外）性能的模型，而不是描述性（样本内）能力，因此必须采用另一种方法。
- en: 'Another approach would be to split our training set into a base learner train
    set and a meta-learner train (validation) set. This way, we would still retain
    a true test set where we can measure the ensemble''s performance. The drawback
    of this approach is that we must donate some of the instances to the validation
    set. Furthermore, both the validation set size and the train set size will be
    smaller than the original train set size. Thus, the preferred approach is to utilize
    **K-fold cross validation**. For each *K*, the base learners will be trained on
    the *K*-1 folds and predict on the *K*th fold, generating 100/*K* percent of the
    final training metadata. By repeating the process *K* times, one for each fold,
    we will have generated metadata for the whole training dataset. The process is
    depicted in the following diagram. The final result is a set of metadata for the
    whole dataset, where the metadata is generated on out-of-sample data (from the
    perspective of the base learners, for each fold):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将训练集分为基本学习器训练集和元学习器训练（验证）集。这样，我们仍然可以保留一个真实的测试集，用于衡量集成模型的表现。此方法的缺点是我们必须将部分实例分配给验证集。此外，验证集的大小和训练集的大小都会小于原始训练集的大小。因此，首选方法是使用**K折交叉验证**。对于每个*K*，基本学习器将在*K*-1个折上进行训练，并在第*K*个折上进行预测，生成最终训练元数据的100/*K*百分比。通过将该过程重复*K*次，每次针对一个折，我们将为整个训练数据集生成元数据。该过程在以下图表中有所展示。最终结果是为整个数据集生成的元数据，其中元数据是基于样本外数据生成的（从基本学习器的角度来看，对于每个折）：
- en: '![](img/63d4726b-b178-4827-aadd-36539dd7fe8d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63d4726b-b178-4827-aadd-36539dd7fe8d.png)'
- en: Creating metadata with five-fold cross-validation
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五折交叉验证创建元数据
- en: Deciding on an ensemble's composition
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决定集成模型的组成
- en: We described stacking as an advanced form of voting. Similarly to voting (and
    most ensemble learning techniques for that matter), stacking is dependent on the
    diversity of its base learners. If the base learners exhibit the same characteristics
    and performance throughout the problem's domain, it will be difficult for the
    meta-learner to dramatically improve their collective performance. Furthermore,
    a complex meta-learner will be needed. If the base learners are diverse and exhibit
    different performance characteristics in different domains of the problem, even
    a simple meta-learner will be able to greatly improve their collective performance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将堆叠描述为一种高级的投票形式。与投票（以及大多数集成学习技术）类似，堆叠依赖于基本学习器的多样性。如果基本学习器在问题的整个领域中表现相同，那么元学习器将很难显著提升它们的集体表现。此外，可能需要一个复杂的元学习器。如果基本学习器具有多样性，并且在问题的不同领域中表现出不同的性能特征，即使是一个简单的元学习器也能大大提升它们的集体表现。
- en: Selecting base learners
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择基本学习器
- en: 'It is generally a good idea to mix different learning algorithms, in order
    to capture both linear and non-linear relationships between the features themselves,
    as well as the target variable. Take, for example, the following dataset, which
    exhibits both linear and non-linear relationships between the feature (*x*) and
    the target variable (*y*). It is evident that neither a single linear nor a single
    non-linear regression will be able to fully model the data. A stacking ensemble
    with a linear and non-linear regression will be able to greatly outperform either
    of the two models. Even without stacking, by hand-crafting a simple rule, (for
    example "use the linear model if x is in the spaces [0, 30] or [60, 100], else
    use the non-linear") we can greatly outperform the two models:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，混合不同的学习算法是个好主意，以便捕捉特征之间以及特征与目标变量之间的线性和非线性关系。例如，考虑以下数据集，其中特征（*x*）与目标变量（*y*）之间既有线性关系也有非线性关系。显然，单一的线性回归或单一的非线性回归都无法完全建模数据。而使用线性和非线性回归的堆叠集成将大大超越这两种模型。即使不使用堆叠，通过手工制定一个简单的规则（例如“如果
    x 在 [0, 30] 或 [60, 100] 区间内，使用线性模型，否则使用非线性模型”），我们也能大大超过这两个模型：
- en: '![](img/5e1ab97e-f28c-4e60-b92a-d9550621d653.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e1ab97e-f28c-4e60-b92a-d9550621d653.png)'
- en: Combination of x=5 and x-squared for the example dataset
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例数据集中的 x=5 和 x 的平方的组合
- en: Selecting the meta-learner
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择元学习器
- en: 'Generally, the meta-learner should be a relatively simple machine learning
    algorithm, in order to avoid overfitting. Furthermore, additional steps should
    be taken in order to regularize the meta-learner. For example, if a decision tree
    is used, then the tree''s maximum depth should be limited. If a regression model
    is used, a regularized regression (such as elastic net or ridge regression) should
    be preferred. If there is a need for more complex models in order to increase
    the ensemble''s predictive performance, a multi-level stack could be used, in
    which the number of models and each individual model''s complexity reduces as
    the stack''s level increases:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，元学习器应该是一个相对简单的机器学习算法，以避免过拟合。此外，还应采取额外的步骤来正则化元学习器。例如，如果使用决策树，则应限制树的最大深度。如果使用回归模型，应该首选正则化回归（如弹性网或岭回归）。如果需要更复杂的模型以提高集成的预测性能，可以使用多级堆叠，其中每个层级的模型数量和每个模型的复杂度会随着堆叠层级的增加而减少：
- en: '![](img/7206e732-7010-4481-a15a-9667fd20e787.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7206e732-7010-4481-a15a-9667fd20e787.png)'
- en: Level stacking ensemble. Each level has simpler models than the previous level
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 层级堆叠集成。每一层的模型比上一层更简单。
- en: Another really important characteristic of the meta-learner should be the ability
    to handle correlated inputs and especially to not make any assumptions about the
    independence of features from one another, as naive Bayes classifiers do. The
    inputs to the meta-learner (metadata) will be highly correlated. This happens
    because all base learners are trained to predict the same target. Thus, their
    predictions will come from an approximation of the same function. Although the
    predicted values will vary, they will be close to each other.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习器的另一个非常重要的特性是能够处理相关输入，特别是不能像朴素贝叶斯分类器那样对特征间的独立性做出假设。元学习器的输入（元数据）将高度相关。这是因为所有基学习器都被训练来预测相同的目标。因此，它们的预测将来自对相同函数的近似。尽管预测值会有所不同，但它们会非常接近。
- en: Python implementation
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实现
- en: Although scikit-learn does implement most ensemble methods that we cover in
    this book, stacking is not one of them. In this section, we will implement custom
    stacking solutions for both regression and classification problems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 scikit-learn 实现了本书中涵盖的大多数集成方法，但堆叠（stacking）并不包括在内。在这一部分，我们将为回归和分类问题实现自定义的堆叠解决方案。
- en: Stacking for regression
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归的堆叠
- en: Here, we will try to create a stacking ensemble for the diabetes regression
    dataset. The ensemble will consist of a 5-neighbor **k-Nearest Neighbors** (**k-NN**),
    a decision tree limited to a max depth of four, and a ridge regression (a regularized
    form of least squares regression). The meta-learner will be a simple **Ordinary
    Least Squares** (**OLS**) linear regression.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将尝试为糖尿病回归数据集创建一个堆叠集成。该集成将包含一个 5 邻居的**k-最近邻**（**k-NN**）、一个最大深度限制为四的决策树，以及一个岭回归（最小二乘回归的正则化形式）。元学习器将是一个简单的**普通最小二乘法**（**OLS**）线性回归。
- en: 'First, we have to import the required libraries and data. Scikit-learn provides
    a convenient method to split data into K-folds, with the `KFold` class from the `sklearn.model_selection`
    module. As in previous chapters, we use the first 400 instances for training and
    the remaining instances for testing:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所需的库和数据。Scikit-learn提供了一个便捷的方法，可以使用`sklearn.model_selection`模块中的`KFold`类将数据拆分为K个子集。与之前的章节一样，我们使用前400个实例进行训练，剩余的实例用于测试：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the following code, we instantiate the base and meta-learners. In order
    to have ease of access to the individual base learners later on, we store each
    base learner in a list, called `base_learners`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们实例化了基础学习器和元学习器。为了方便后续访问每个基础学习器，我们将每个基础学习器存储在一个名为`base_learners`的列表中：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After instantiating our learners, we need to create the metadata for the training
    set. We split the training set into five folds by first creating a `KFold` object,
    specifying the number of splits (K) with `KFold(n_splits=5)`, and then calling
    `KF.split(train_x)`. This, in turn, returns a generator for the train and test
    indices of the five splits. For each of these splits, we use the data indicated
    by `train_indices` (four folds) to train our base learners and create metadata
    on the data corresponding to `test_indices`. Furthermore, we store the metadata
    for each classifier in the `meta_data` array and the corresponding targets in
    the `meta_targets` array. Finally, we transpose `meta_data` in order to get a
    (instance, feature) shape:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化我们的学习器之后，我们需要为训练集创建元数据。我们通过首先创建一个`KFold`对象，指定分割数（K），即`KFold(n_splits=5)`，然后调用`KF.split(train_x)`将训练集拆分成五个子集。这将返回一个生成器，用于获取这五个子集的训练集和测试集索引。对于每个拆分，我们使用`train_indices`（四个子集）对应的数据来训练我们的基础学习器，并为与`test_indices`对应的数据创建元数据。此外，我们将每个分类器的元数据存储在`meta_data`数组中，将相应的目标存储在`meta_targets`数组中。最后，我们转置`meta_data`，以获得一个（实例，特征）的形状：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For the test set, we do not need to split it into folds. We simply train the
    base learners on the whole train set and predict on the test set. Furthermore,
    we evaluate each base learner and store the evaluation metrics, in order to compare
    them with the ensemble''s performance. As this is a regression problem, we use
    R-squared and **Mean Squared Error** (**MSE**) as evaluation metrics:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试集，我们不需要将其拆分成多个子集。我们仅需在整个训练集上训练基础学习器，并在测试集上进行预测。此外，我们会评估每个基础学习器并存储评估指标，以便与集成模型的表现进行比较。由于这是一个回归问题，我们使用R平方和**均方误差**（**MSE**）作为评估指标：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, that we have the metadata for both the train and test sets, we can train
    our meta-learner on the train set and evaluate on the test set:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们已经获得了训练集和测试集的元数据，我们就可以在训练集上训练我们的元学习器，并在测试集上进行评估：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We get the following output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下输出：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As is evident, r-squared has improved by over 16% from the best base learner
    (ridge regression), while MSE has improved by almost 20%. This is a considerable
    improvement.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，R平方从最佳基础学习器（岭回归）提高了超过16%，而MSE几乎提高了20%。这是一个相当可观的改进。
- en: Stacking for classification
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类任务的堆叠方法
- en: Stacking is a viable method for both regression and classification. In this
    section, we will use it to classify the breast cancer dataset. Again, we will
    use three base learners. A 5-neighbor k-NN, a decision tree limited to a max depth
    of 4, and a simple neural network with 1 hidden layer of 100 neurons. For the
    meta-learner, we utilize a simple logistic regression.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠方法既适用于回归问题，也适用于分类问题。在这一节中，我们将使用堆叠方法对乳腺癌数据集进行分类。我们依然会使用三个基础学习器：一个5邻居的k-NN，一个最大深度为4的决策树，和一个带有1个隐藏层、100个神经元的简单神经网络。对于元学习器，我们使用一个简单的逻辑回归模型。
- en: 'Again, we load the required libraries and split the data into a train and test
    set:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 再次加载所需的库，并将数据拆分为训练集和测试集：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We instantiate the base learners and the meta-learner. Note that `MLPClassifier` has
    a `hidden_layer_sizes =(100,)` parameter, which specifies the number of neurons
    for each hidden layer. Here, we have a single layer of 100 neurons:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了基础学习器和元学习器。请注意，`MLPClassifier`具有一个`hidden_layer_sizes =(100,)`参数，用来指定每个隐藏层的神经元数量。这里，我们只有一个隐藏层，包含100个神经元：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Again, using `KFolds`, we split the train set into five folds in order to train
    on four folds and generate metadata for the remaining fold, repeated five times.
    Note that we use `learner.predict_proba(train_x[test_indices])[:,0]` in order
    to get the predicted probability that the instance belongs to in the first class.
    Given that we have only two classes, this is sufficient. For *N* classes, we would
    have to either save *N*-1 features or simply use `learner.predict`, in order to
    save the predicted class:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，使用`KFolds`，我们将训练集拆分成五个折叠，以便在四个折叠上进行训练并为剩余的折叠生成元数据，重复五次。请注意，我们使用`learner.predict_proba(train_x[test_indices])[:,0]`来获取实例属于第一类的预测概率。鉴于我们只有两个类别，这已经足够了。如果是*N*个类别，我们必须保存*N*-1个特征，或者简单地使用`learner.predict`，以便保存预测的类别：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we train the base classifiers on the train set and create metadata for
    the test set, as well as evaluating their accuracy with `metrics.accuracy_score(test_y,
    learner.predict(test_x))`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在训练集上训练基础分类器，并为测试集创建元数据，同时使用`metrics.accuracy_score(test_y, learner.predict(test_x))`评估它们的准确度：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we fit the meta-learner on the train metadata, evaluate its performance
    on the test data, and print both the ensemble''s and the individual learner''s
    accuracy:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在训练元数据上拟合元学习器，评估其在测试数据上的表现，并打印出集成模型和单个学习器的准确度：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The final output is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出如下：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we can see that the meta-learner was only able to improve the ensemble''s
    performance by 1%, compared to the best performing base learner. If we try to
    utilize the `learner.predict` method to generate our metadata, we see that the
    ensemble actually under performs, compared to the neural network:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，元学习器仅能将集成模型的表现提高1%，与表现最好的基础学习器相比。如果我们尝试利用`learner.predict`方法生成元数据，我们会发现集成模型实际上表现不如神经网络：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Creating a stacking regressor class for scikit-learn
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 scikit-learn 创建一个堆叠回归器类
- en: 'We can utilize the preceding code in order to create a reusable class that
    orchestrates the ensemble''s training and prediction. All scikit-learn classifiers
    use the standard `fit(x, y)` and `predict(x)` methods, in order to train and predict
    respectively. First, we import the required libraries and declare the class and
    its constructor. The constructor''s argument is a list of lists of scikit-learn
    classifiers. Each sub-list contains the level''s learners. Thus, it is easy to
    construct a multi-level stacking ensemble. For example, a three-level ensemble
    can be constructed with `StackingRegressor([ [l11, l12, l13],[ l21, l22], [l31]
    ])`. We create a list of each stacking level''s size (the number of learners)
    and also create deep copies of the base learners. The classifier in the last list
    is considered to be the meta-learner:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用前面的代码来创建一个可重用的类，用于协调集成模型的训练和预测。所有 scikit-learn 分类器都使用标准的`fit(x, y)`和`predict(x)`方法，分别用于训练和预测。首先，我们导入所需的库，并声明类及其构造函数。构造函数的参数是一个包含
    scikit-learn 分类器子列表的列表。每个子列表包含该层的学习器。因此，构建一个多层堆叠集成模型非常容易。例如，可以使用`StackingRegressor([
    [l11, l12, l13], [ l21, l22], [l31] ])`来构建一个三层集成模型。我们创建一个包含每个堆叠层大小（学习器数量）的列表，并且还创建基础学习器的深拷贝。最后一个列表中的分类器被视为元学习器：
- en: All of the following code, up to (not including) Section 5 (comment labels), is
    part of the `StackingRegressor` class. It should be properly indented if it is
    copied to a Python editor.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有代码，直到（不包括）第5节（注释标签），都是`StackingRegressor`类的一部分。如果将其复制到Python编辑器中，应正确缩进。
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In following the constructor definition, we define the `fit` function. The
    only difference from the simple stacking script we presented in the preceding
    section is that instead of creating metadata for the meta-learner, we create a
    list of metadata, one for each stacking level. We save the metadata and targets
    in the `meta_data, meta_targets` lists and use `data_z, target_z` as the corresponding
    variables for each level. Furthermore, we train the level''s learners on the metadata
    of the previous level. We initialize the metadata lists with the original training
    set and targets:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟随构造函数定义的过程中，我们定义了`fit`函数。与我们在前一部分展示的简单堆叠脚本的唯一区别在于，我们不再为元学习器创建元数据，而是为每个堆叠层创建一个元数据列表。我们将元数据和目标保存到`meta_data,
    meta_targets`列表中，并使用`data_z, target_z`作为每个层的对应变量。此外，我们在上一层的元数据上训练该层的学习器。我们使用原始训练集和目标初始化元数据列表：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we define the `predict` function, which creates metadata for each
    level for the provided test set, using the same logic as was used in `fit` (storing
    each level''s metadata). The function returns the metadata for each level, as
    they are also the predictions of each level. The ensemble''s output can be accessed
    with `meta_data[-1]`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了`predict`函数，该函数为提供的测试集创建每个层的元数据，使用与`fit`中相同的逻辑（存储每个层的元数据）。该函数返回每个层的元数据，因为它们也是每个层的预测结果。集成输出可以通过`meta_data[-1]`访问：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we instantiate `StackingRegressor` with the same meta-learner and base learners
    as our regression example, we can see that it performs exactly the same! In order
    to access intermediate predictions, we must access the level''s index plus one,
    as the data in `meta_data[0]` is the original test data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用与回归示例中相同的元学习器和基础学习器实例化`StackingRegressor`，我们可以看到它的表现完全相同！为了访问中间预测，我们必须访问该层的索引加一，因为`meta_data[0]`中的数据是原始的测试数据：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The results match with our previous example''s result:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与我们之前示例中的结果一致：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In order to further clarify the relationships between the `meta_data` and `self.learners`
    lists, we graphically depict their interactions as follows. We initialize `meta_data[0]`
    for the sake of code simplicity. While it can be misleading to store the actual
    input data in the `meta_data` list, it avoids the need to handle the first level
    of base learners in a different way than the rest:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步澄清`meta_data`与`self.learners`列表之间的关系，我们通过图示方式展示它们的交互关系。为了代码简洁，我们初始化了`meta_data[0]`。虽然将实际输入数据存储在`meta_data`列表中可能会误导，但它避免了需要以不同于其他层的方式处理基础学习器第一层：
- en: '![](img/90f001b3-9e06-4847-8dc0-7ed5beb2952f.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90f001b3-9e06-4847-8dc0-7ed5beb2952f.png)'
- en: The relationships between each level of meta_data and self.learners
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层`meta_data`与`self.learners`之间的关系
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we presented an ensemble learning method called stacking (or
    stacked generalization). It can be seen as a more advanced method of voting. We
    first presented the basic concept of stacking, how to properly create the metadata,
    and how to decide on the ensemble's composition. We presented one regression and
    one classification implementation for stacking. Finally, we presented an implementation
    of an ensemble class  (implemented similarly to scikit-learn classes), which makes
    it easier to use multi-level stacking ensembles. The following are some key points
    to remember from this chapter.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一种名为堆叠（或堆叠泛化）的集成学习方法。它可以视为一种更高级的投票方法。我们首先介绍了堆叠的基本概念，如何正确创建元数据，以及如何决定集成的组成。我们为堆叠提供了回归和分类的实现。最后，我们展示了一个集成类的实现（类似于`scikit-learn`类的实现），使得多层堆叠集成更易于使用。以下是本章的一些关键要点：
- en: '**Stacking** can consist of many **levels**. Each level generates **metadata**
    for the next. You should create each level''s metadata by splitting the train
    set into **K folds** and iteratively **train on K-1 folds**, while creating **metadata
    for the Kth fold**. After creating the metadata, you should train the current
    level on the whole train set. Base learners must be diverse. The meta-learner
    should be a relatively simple algorithm that is resistant to overfitting. If possible,
    try to induce regularization in the meta-learner. For example, limit the maximum
    depth if you use a decision tree or use a regularized regression. The meta-learner
    should be able to handle correlated inputs relatively well. You should not be
    afraid to **add under-performing models** to the ensemble, as long as they introduce
    new information to the metadata (that is, they handle the dataset differently
    from the other models). In the next chapter, we will introduce the first generative
    ensemble method, Bagging.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**可以由多个**层**组成。每一层都会为下一层生成**元数据**。你应该通过将训练集划分为**K折**并迭代地**在K-1折上训练**，同时为**第K折**创建**元数据**来创建每一层的元数据。创建元数据后，你应该在整个训练集上训练当前层。基础学习器必须具有多样性。元学习器应该是一个相对简单的算法，并能抵抗过拟合。如果可能的话，尽量在元学习器中引入正则化。例如，如果使用决策树，则限制其最大深度，或使用正则化回归。元学习器应该能够相对较好地处理相关输入。你不应该害怕**将表现不佳的模型**添加到集成中，只要它们为元数据引入了新的信息（即，它们以不同于其他模型的方式处理数据集）。在下一章中，我们将介绍第一个生成式集成方法——袋装（Bagging）。'
