- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Vision-Based Defect Detection System – Machines Can See Now!
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于视觉的缺陷检测系统 – 机器现在也能“看”了！
- en: '**Computer Vision** (**CV**) is a field of artificial intelligence concerned
    with giving machines the ability to analyze and extract meaningful information
    from digital images, videos, and other visual input, as well as take actions or
    make recommendations based on the extracted information. Decades of research in
    the field of CV have led to the development of powerful **Machine Learning** (**ML**)-based
    vision algorithms that are capable of classifying images into some pre-defined
    categories, detecting objects from images, understanding written content from
    digital images, and detecting actions being performed in videos. Such vision algorithms
    have given businesses and organizations the ability to analyze large amounts of
    digital content (images and videos) and also automate processes to make instant
    decisions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机视觉**（**CV**）是人工智能的一个领域，它关注于赋予机器分析并从数字图像、视频和其他视觉输入中提取有意义信息的能力，以及根据提取的信息采取行动或做出推荐。在计算机视觉领域数十年的研究导致了基于机器学习（**ML**）的强大视觉算法的发展，这些算法能够将图像分类到一些预定义的类别中，从图像中检测对象，从数字图像中理解书面内容，以及检测视频中正在执行的动作。这样的视觉算法赋予了企业和组织分析大量数字内容（图像和视频）的能力，并自动化流程以做出即时决策。'
- en: CV-based algorithms have changed the way we interact with smart devices in our
    day-to-day life – for example, we can now unlock smartphones by just showing our
    face, and photo editing apps today can make us look younger or older. Another
    important use case of applying CV-based ML algorithms is defect detection. ML
    algorithms can be leveraged to analyze visual input and detect defects in product
    images, which can be quite useful for manufacturing industries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计算机视觉（CV）的算法已经改变了我们日常与智能设备互动的方式 – 例如，我们现在只需展示我们的面部就可以解锁智能手机，而现在的照片编辑应用可以使我们看起来更年轻或更老。应用基于CV的机器学习（ML）算法的另一个重要用例是缺陷检测。机器学习算法可以用来分析视觉输入并在产品图像中检测缺陷，这对制造业来说非常有用。
- en: In this chapter, we will develop a real-world defect detection solution using
    deep learning on Google Cloud. We will also see how to deploy our vision-based
    defect detection model as a Vertex AI endpoint so that it can be utilized for
    online prediction.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Google Cloud上的深度学习开发一个实际的缺陷检测解决方案。我们还将了解如何部署我们的基于视觉的缺陷检测模型作为Vertex
    AI端点，以便它可以用于在线预测。
- en: 'This chapter covers the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主要内容：
- en: Vision-based defect detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于视觉的缺陷检测
- en: Deploying a vision model to a Vertex AI endpoint
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将视觉模型部署到Vertex AI端点
- en: Getting online predictions from a vision model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从视觉模型获取在线预测
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code samples used in this chapter can be found at the following GitHub
    address: [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的代码示例可以在以下GitHub地址找到：[https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16).
- en: Vision-based defect detection
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于视觉的缺陷检测
- en: CV is capable nowadays of detecting visual defects on object surfaces or inconsistencies
    in their designs (such as dents or scratches on a car body), by just analyzing
    their digital photographs or videos. The manufacturing industry can leverage CV
    algorithms to automatically detect and remove low-quality and defected products
    from being packed and reaching customers. There are many possible ways to detect
    defects within digital content using CV-based algorithms. One simple idea is to
    solve defect detection as a classification problem, where a vision model can be
    trained to classify images such as *good* or *defected*. A more complex defect
    detection system will also locate the exact area of an image with a defect. The
    problem of identifying and locating visual defects can be solved using object-detection
    algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的计算机视觉（CV）能够通过分析物体的数字照片或视频来检测物体表面的视觉缺陷或设计上的不一致性（例如汽车车身上的凹痕或划痕）。制造业可以利用计算机视觉算法自动检测并移除低质量或存在缺陷的产品，防止其被包装并送达客户。使用基于CV的算法在数字内容中检测缺陷有许多可能的方法。一个简单的想法是将缺陷检测视为一个分类问题，其中可以训练一个视觉模型来分类图像，如“良好”或“缺陷”。一个更复杂的缺陷检测系统还将定位图像中存在缺陷的确切区域。可以使用目标检测算法来解决识别和定位视觉缺陷的问题。
- en: In this section, we will build and train a simple defect detection system step
    by step. In this example, we will use ML classification as a mechanism to detect
    visually defected products. Let’s explore the example.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步构建和训练一个简单的缺陷检测系统。在这个例子中，我们将使用机器学习分类作为检测视觉缺陷产品的机制。让我们探索这个例子。
- en: Dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: For this experiment, we have downloaded an open source dataset from Kaggle.
    The dataset has over a thousand colored images of glass bangles. These images
    contain bangles of different sizes and colors and can be classified into three
    major categories, based on their manufacturing quality and damages – good, defected,
    and broken. Defected bangles may have a manufacturing defect such as invariable
    width or improper circular shape, while broken bangles would have some piece of
    the circle missing. In this experiment, we will utilize some of these images to
    train an ML classification model and test it on a few of the unseen samples. Images
    are already separated into the aforementioned categories, so there’s no need for
    manual data annotation. The dataset used in this experiment can be downloaded
    from the Kaggle link provided in the Jupyter Notebook corresponding to this experiment.
    The GitHub location of the code samples is presented at the beginning of this
    chapter in the *Technical* *requirements* section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们从 Kaggle 下载了一个开源数据集。该数据集包含超过一千张彩色玻璃手镯的图像。这些图像包含不同大小和颜色的手镯，可以根据其制造质量和损坏情况分为三大类——好的、有缺陷的和破损的。有缺陷的手镯可能存在制造缺陷，如恒定的宽度或不当的圆形形状，而破损的手镯则可能缺少圆形的一部分。在这个实验中，我们将利用这些图像中的某些图像来训练一个机器学习分类模型，并在一些未见过的样本上进行测试。图像已经按照上述类别分开，因此不需要手动数据标注。本实验中使用的数据集可以从与该实验对应的
    Jupyter Notebook 中提供的 Kaggle 链接下载。代码样本的 GitHub 位置在本章的 *技术要求* 部分开头给出。
- en: We have already downloaded and extracted the dataset into the same directory
    as our Jupyter Notebook. Now, we can start looking at some of the image samples.
    Let’s get into the coding part now.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将数据集下载并解压到了与我们的 Jupyter Notebook 相同的目录中。现在，我们可以开始查看一些图像样本了。现在让我们进入编码部分。
- en: Importing useful libraries
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入有用的库
- en: 'The first step is to import useful Python packages for our experiment. Here,
    `cv2` refers to the OpenCV library, which has lots of prebuilt functionalities
    for dealing with images and other CV tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是导入实验中需要的有用 Python 包。在这里，`cv2` 指的是 OpenCV 库，它为处理图像和其他 CV 任务提供了许多预构建的功能：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, let’s look at the dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查看数据集。
- en: Loading and verifying data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据并进行验证
- en: 'Now, let’s load all the image file paths into three separate lists, one for
    each category – good, defected, and broken. Keeping three separate lists will
    make it easier to keep track of image labels. Let’s also print the exact number
    of images within each category:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将所有图像文件路径加载到三个单独的列表中，每个类别一个列表——好的、有缺陷的和破损的。保留三个单独的列表将使跟踪图像标签更容易。我们还将打印出每个类别中图像的确切数量：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have 520 good-quality images, 244 defected images, and 316 broken bangle
    images in total. Next, let’s verify a few samples from each category by plotting
    them using `matplotlib`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总共有 520 张高质量图像、244 张有缺陷的图像和 316 张破损的手镯图像。接下来，让我们通过使用 `matplotlib` 绘制来验证每个类别的一些样本：
- en: Checking few samples
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查一些样本
- en: In this step, we will randomly choose a few image paths from each of the previously
    discussed lists and plot them with their category name as their title.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将从之前讨论的每个列表中随机选择一些图像路径，并使用它们的类别名称作为标题进行绘制。
- en: 'Let’s plot a few good bangle images:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一些好的手镯图像：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, we will plot a few random defective bangle pieces:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将绘制一些随机的有缺陷的手镯碎片：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we will also plot some broken bangle images in a similar way so that
    we can see all three categories visually and learn more about the data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将以类似的方式绘制一些破损的手镯图像，这样我们就可以直观地看到所有三个类别，并更多地了解数据：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output of the preceding scripts is shown in *Figure 16**.1* where each row
    represents a few random samples from each of the aforementioned categories. The
    category name is present in the title of images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前述脚本的输出显示在 *图 16.1* 中。其中每一行代表上述每个类别的一些随机样本。类别名称出现在图像的标题中。
- en: '![Figure 16.1 – Bangle image samples from each category – good, defected, and
    broken](img/B17792_16_1.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.1 – 各个类别（好的、有缺陷的和破损的）的手镯图像样本](img/B17792_16_1.jpg)'
- en: Figure 16.1 – Bangle image samples from each category – good, defected, and
    broken
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 – 每个类别的手镯图像样本 – 好的、有缺陷的和破损的
- en: As shown in the preceding figure, good bangles are the ones that look like a
    perfect circle with an even width, defected bangles may have some uneven width
    or surface, while broken bangles are easily distinguishable, as they are missing
    some part of their circular shape. So, differentiating between good and defected
    bangles can be a bit challenging, but differentiating both of these categories
    from the broken bangles should be easier for the algorithm. Let’s see how it goes
    in the next few sections.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，好的手镯看起来像是一个完美的圆形，宽度均匀，有缺陷的手镯可能有一些不均匀的宽度或表面，而破损的手镯很容易辨认，因为它们缺少圆形的一部分。因此，区分好的手镯和有缺陷的手镯可能有点挑战性，但算法区分这两种类别与破损的手镯应该更容易。让我们看看在接下来的几节中情况会如何。
- en: Data preparation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: In this step, we will prepare data for our **TensorFlow** (**TF**)-based deep
    learning model. A TF-based model requires all the inputs to have the same shape,
    so the first step would be to make all the input images the same shape. We will
    also downgrade the quality of images a bit so that they are more memory-efficient
    while reading them and performing calculations over them during training. We have
    to keep one thing in mind – we can’t degrade the data quality significantly such
    that it becomes hard for the model to find any visual clues to detect defected
    surfaces. For this experiment, we will resize each image to a 200x200 resolution.
    Secondly, as we are only concerned about finding defects, the color of the image
    is not important to us. So, we will convert all the colored images into grayscale
    images, as it reduces the images’ channels from 3 to 1; thus, the image size becomes
    a third of its original size. Finally, we will convert them into NumPy arrays,
    as TF models require the input tensors to be NumPy arrays.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将为基于**TensorFlow**（**TF**）的深度学习模型准备数据。基于TF的模型要求所有输入具有相同的形状，因此第一步将是使所有输入图像具有相同的形状。我们还将稍微降低图像的质量，以便在读取它们和在训练期间对它们进行计算时更加内存高效。我们必须记住一件事
    – 我们不能显著降低数据质量，以至于模型难以找到任何视觉线索来检测有缺陷的表面。对于这个实验，我们将每个图像调整到200x200的分辨率。其次，因为我们只关心寻找缺陷，所以图像的颜色对我们来说并不重要。因此，我们将所有彩色图像转换为灰度图像，因为这样可以减少图像的通道从3到1；因此，图像大小变为原始大小的三分之一。最后，我们将它们转换为NumPy数组，因为TF模型要求输入张量是NumPy数组。
- en: 'The following are the pre-processing steps that we will follow for each image
    in the dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将为数据集中的每一张图片执行的预处理步骤：
- en: Reading the colored image using the OpenCV library
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenCV库读取彩色图像
- en: Resizing an image to a fixed size (200 x 200)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整到固定大小（200 x 200）
- en: Converting image to grayscale (black and white)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度（黑白）
- en: Converting the list of images into NumPy arrays
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像列表转换为NumPy数组
- en: Adding one channel dimensions, as required by convolutional layers
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加一个通道维度，这是卷积层所要求的
- en: 'Let’s first follow these steps for good bangle images:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先按照以下步骤处理好的手镯图像：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Similarly, we will pre-process the defected bangle images:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将预处理有缺陷的手镯图像：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we will follow the same pre-processing steps for the broken bangle
    images. We will also print the final shapes of NumPy arrays for each category
    of images:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将遵循相同的预处理步骤来处理破损的手镯图像。我们还将打印每个图像类别的最终NumPy数组形状：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each image array now will have the shape of 200x200x1, and the first dimension
    will represent the total number of images. The following is the output of the
    previous script:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像数组现在将具有200x200x1的形状，第一个维度将代表图像的总数。以下是前一个脚本的输出：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, let’s split the data into training and test partitions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将数据分成训练和测试分区。
- en: Splitting data into train and test
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据分割为训练和测试
- en: As our data is now ready and compatible with TF-model format, we just need to
    perform one last important step of splitting the data into two partitions – train
    and test. The *train* partition will be shown to the model for learning purposes
    during training of the model, and the *test* partition will be kept separate and
    will not contribute in the model parameters’ update. Once the model is trained,
    we will check how well it performs on the unseen test partition of data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据现在已准备好且与TF模型格式兼容，我们只需执行最后一步重要步骤，即将数据分成两个分区——训练和测试。*训练*分区将在模型训练期间用于学习目的，而*测试*分区将保持分开，不会对模型参数的更新做出贡献。一旦模型训练完成，我们将检查它在未见过的测试数据分区上的表现如何。
- en: 'As our test data should also have a significant number of samples from each
    category, we will divide each of the image categories into train and test partitions
    and, finally, merge all the train and test partitions together. We will utilize
    the first 75% of the images from each category array for training and the rest
    of the 25% images for testing purposes, as shown in the following snippet:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的测试数据也应该包含每个类别的样本，我们将每个图像类别分成训练和测试分区，最后将所有训练和测试分区合并在一起。我们将利用每个类别数组中的前75%的图像用于训练，其余的25%图像用于测试，如下面的代码片段所示：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The previously defined code also prints the shape of the train and test partitions
    just for data verification purposes. The following is the output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的代码还打印出训练和测试分区的形状，仅用于数据验证目的。以下为输出：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Our training and test data is almost ready now; we just need corresponding labels
    for model training. In the next step, we will create label arrays for both partitions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练和测试数据现在几乎准备好了；我们只需要为模型训练创建相应的标签。在下一步中，我们将为两个分区创建标签数组。
- en: Final preparation of training and testing data
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和测试数据的最终准备
- en: Now that we have three pairs of train and test partitions (one for each category),
    let’s combine them into a single pair of train and test partitions for model training
    and testing purpose. After concatenating these NumPy arrays, we will also perform
    reshuffling so that images from each of the categories are well-mixed. This is
    important, as during training we will only send small batches to the model, so
    for smooth training of the model, each batch should have samples from all the
    classes. As test data is kept separate, there is no need to shuffle it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了三对训练和测试分区（每个类别一对），让我们将它们合并成一对训练和测试分区，用于模型训练和测试。在连接这些NumPy数组之后，我们还将进行重新洗牌，以确保每个类别的图像都混合得很好。这是很重要的，因为在训练过程中，我们只会向模型发送小批量数据，所以为了模型的平稳训练，每个批次都应该包含来自所有类别的样本。由于测试数据是分开的，所以不需要洗牌。
- en: Additionally, we also need to create corresponding label arrays as well. As
    ML algorithms only support numeric data, we need to encode our output categories
    into some numeric values. We can represent our three categories with three numbers
    – 0, 1, and 2.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要创建相应的标签数组。由于机器学习算法只支持数值数据，我们需要将我们的输出类别编码成一些数值。我们可以用三个数字来表示我们的三个类别——0、1和2。
- en: 'We will use the following label mapping rule to encode our categories:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下标签映射规则来编码我们的类别：
- en: '**Good – 0**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**良好 - 0**'
- en: '**Defected – 1**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺陷 - 1**'
- en: '**Broken –** **2**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损坏 -** **2**'
- en: 'The following code snippet concatenates all the training partitions into a
    single train partition and also creates a label array, using the aforementioned
    mapping rule:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段将所有训练分区连接成一个单一的训练分区，并使用前面提到的映射规则创建一个标签数组：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similarly, we will concatenate all the test partitions into a single test partition.
    We will also create a label array for our test partition, which will help us to
    check the accuracy metrics of our trained model. Here, we also print the final
    shapes of the train and test partitions, as shown in the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将所有测试分区连接成一个单一的测试分区。我们还将为我们的测试分区创建一个标签数组，这将帮助我们检查训练模型的准确度指标。在这里，我们也会打印出训练和测试分区的最终形状，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of this script shows that we have 810 training images and 270 test
    images in total, as shown in the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本的输出显示，我们总共有810张训练图像和270张测试图像，如下面的输出所示：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As discussed before, it is very important to shuffle our training partition,
    ensuring that images from each category are well-mixed and each batch will have
    a good variety. The important thing to keep in mind while shuffling is that we
    also need to shuffle the label array accordingly to avoid any data label mismatches.
    For this purpose, we have defined a Python function that shuffles two given arrays
    in unison:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，确保我们的训练分区被充分打乱是非常重要的，以确保每个类别的图像都充分混合，并且每个批次都将有一个良好的多样性。在打乱时需要注意的重要事情是，我们还需要相应地打乱标签数组，以避免任何数据标签不匹配。为此，我们定义了一个Python函数，该函数可以同时打乱两个给定的数组：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our training and testing dataset is now all set. We can now move to the model
    architecture.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练和测试数据集现在都已准备就绪。我们现在可以转向模型架构。
- en: TF model architecture
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF模型架构
- en: In this section, we will define a model architecture for our TF-based deep learning
    classification model. As we are dealing with images in this experiment, we will
    utilize **Convolutional Neural Network** (**CNN**) layers to learn and extract
    important features from training images. CNNs have proved to be quite useful in
    the field of CV. In general, a few CNN layers are stacked on top of each other
    to extract low-level (minor details) and high-level (big shape-related) feature
    information. We will also create a CNN-based feature extraction architecture in
    a similar way, combining it with a few fully connected layers. The final fully
    connected layer should have three neurons to generate output for each category,
    and a *sigmoid* activation layer to have that output in the form of a probability
    distribution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义基于TF的深度学习分类模型的模型架构。由于我们在本实验中处理图像，我们将利用**卷积神经网络**（**CNN**）层来从训练图像中学习和提取重要特征。CNN在计算机视觉领域已被证明非常有用。一般来说，几个CNN层堆叠在一起以提取低级（细节）和高级（形状相关）的特征信息。我们也将以类似的方式创建一个基于CNN的特征提取架构，并结合几个全连接层。最终的完全连接层应该有三个神经元来为每个类别生成输出，并使用一个**sigmoid**激活层将输出以概率分布的形式呈现。
- en: A convolutional block
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个卷积块
- en: 'We will now define a reusable convolutional block that we can use repeatedly
    to create our final model architecture. In the convolutional block, we will have
    each convolutional layer followed by the layers of **Batch Normalization** (**BN**),
    **ReLU** activation, a max-pooling layer, and a dropout layer. Here, the layers
    of BN and dropout are for regularization purposes to ensure the smooth learning
    of our TF model. The following Python snippet defines our convolutional block:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将定义一个可重复使用的卷积块，我们可以重复使用它来创建我们的最终模型架构。在卷积块中，我们将有每个卷积层后面跟着**批归一化**（**BN**）、**ReLU**激活、一个最大池化层和一个dropout层。在这里，BN和dropout层是为了正则化目的，以确保我们的TF模型平稳学习。以下Python代码片段定义了我们的卷积块：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let’s define our complete model architecture by making use of this convolutional
    block.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过使用这个卷积块来定义我们的完整模型架构。
- en: TF model definition
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF模型定义
- en: 'We can now define our final TF model architecture, which can utilize a few
    iterations of the convolutional block defined in the previous step. We will first
    define an input layer to tell the model about the size of the input images that
    it will be expecting during training. As discussed earlier, each image in our
    dataset has a size of 200x200x1\. The following Python code defines the convolution-based
    feature extraction part of our network:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义我们的最终TF模型架构，该架构可以利用之前步骤中定义的卷积块进行几次迭代。我们首先定义一个输入层，告诉模型在训练期间期望的输入图像的大小。如前所述，我们数据集中的每个图像大小为200x200x1。以下Python代码定义了我们的网络基于卷积的特征提取部分：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we will use a `Flatten()` layer to bring all the features into a single
    dimension and apply fully connected layers to refine these features. Finally,
    we use another fully connected layer with three neurons, followed by a `softmax`
    activation, to generate probabilistic output for three classes. We then define
    our model object with input and output layers and print out a summary of the model
    for reference:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用一个`Flatten()`层将所有特征汇总到一个单一维度，并应用全连接层来细化这些特征。最后，我们使用另一个具有三个神经元的全连接层，后面跟着一个`softmax`激活，为三个类别生成概率输出。然后我们定义我们的模型对象，包括输入和输出层，并打印出模型的摘要以供参考：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This snippet prints the model summary, which looks something similar to what
    is shown in *Figure 16**.2* (for a complete summary, check out the Jupyter Notebook):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段打印出模型摘要，其外观类似于*图16**.2*（对于完整的摘要，请参阅Jupyter Notebook）：
- en: '![Figure 16.2: The model Summary for the TF-based defect detection architecture](img/B17792_16_2.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2：基于TF的缺陷检测架构的模型摘要](img/B17792_16_2.jpg)'
- en: 'Figure 16.2: The model Summary for the TF-based defect detection architecture'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：基于TF的缺陷检测架构的模型摘要
- en: Our TF model graph is now ready, so we can now compile and fit our model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的TF模型图现在已准备就绪，因此我们可以现在编译并拟合我们的模型。
- en: Compiling the model
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'In this step, we can define the appropriate loss function, optimization algorithm,
    and metrics. As we have a multi-label classification problem, we will utilize
    *categorical cross-entropy* as a loss function. We will utilize the *Adam* optimizer
    with its default values and `''accuracy''` as a metric:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们可以定义适当的损失函数、优化算法和度量标准。由于我们有一个多标签分类问题，我们将利用*分类交叉熵*作为损失函数。我们将使用默认值的*Adam*优化器，并将`'accuracy'`作为度量标准：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now, we are all set to start training our model on the previously curated dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好在之前整理的数据集上开始训练我们的模型。
- en: Training the model
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Now, we are all set to launch the training of our model, as our data and model
    object are both set. We plan to train our model for 50 epochs with a batch size
    of 64\. After each epoch, we will keep checking the model’s loss and accuracy
    on training and testing partitions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好启动模型的训练，因为我们的数据和模型对象都已设置。我们计划以64个批次的规模训练模型50个epoch。在每个epoch之后，我们将持续检查模型在训练和测试分区上的损失和准确率：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The training logs with loss and accuracy values will look similar to the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志中包含损失和准确率值，其外观将类似于以下内容：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once the training is complete, we can start verifying the results of our model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们就可以开始验证我们模型的成果。
- en: Plotting the training progress
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制训练进度
- en: In this step, we will utilize the `history` variable defined in the previous
    step to plot the progress of the training loss, test loss, train accuracy, and
    test accuracy with progressing epochs. These graphs can help us understand whether
    our model training is going in the right direction or not. Also, we can check
    the ideal number of epochs required to get reasonable accuracy on test sets.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将利用之前步骤中定义的`history`变量来绘制训练损失、测试损失、训练准确率和测试准确率随epoch进展的进度图。这些图表可以帮助我们了解我们的模型训练是否正在正确的方向上进行。我们还可以检查获得合理测试集准确率所需的理想epoch数量。
- en: 'Let’s first plot the training and validation loss of the model with an epoch
    number on the *X* axis and a loss value on the *Y* axis:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先绘制一个图表，将epoch编号放在*X*轴上，损失值放在*Y*轴上，以显示模型的训练和验证损失：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output of this snippet is shown in *Figure 16**.3*. We can see in the figure
    that training and validation loss decrease as the training progresses, which tells
    us that our model training is going in the right direction and our model is learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段的输出显示在*图16.3*中。从图中我们可以看到，随着训练的进行，训练和验证损失在下降，这告诉我们我们的模型训练正在正确的方向上进行，并且我们的模型正在学习。
- en: '![Figure 16.3 – Training and validation loss](img/B17792_16_3.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图16.3 – 训练和验证损失](img/B17792_16_3.jpg)'
- en: Figure 16.3 – Training and validation loss
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 – 训练和验证损失
- en: 'Next, we can also plot the accuracy of the model on training and test partitions
    with the progress of training, as shown in following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还可以绘制模型在训练和测试分区上的准确率随训练进度变化的情况，如下面的代码所示：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The resulting plot can be seen in *Figure 16**.4*. We can see that training
    accuracy keeps increasing and reaches close to 100% as the model starts overfitting,
    while the validation accuracy increases to around 70% and keeps fluctuating around
    that. It means that our current setup is capable of achieving around 70% accuracy
    on the test set. This accuracy value can be improved further by either increasing
    the capacity of our network (by adding a few more layers), or by improving the
    way we extract features from the images. For our experiment, this accuracy is
    satisfactory, and we will move forward with it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表可以在*图16.4*中看到。我们可以看到，随着模型开始过拟合，训练准确率持续增加并接近100%，而验证准确率增加到约70%并围绕该值波动。这意味着我们当前的设置能够在测试集上实现约70%的准确率。通过增加网络的容量（添加几个更多层），或者改进我们从图像中提取特征的方式，我们可以进一步提高这个准确率值。对于我们的实验，这个准确率是可以接受的，我们将继续使用它。
- en: '![Figure 16.4 – Training and validation accuracy with the progress of training
    epochs](img/B17792_16_4.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图16.4 – 随着训练epoch的推移的训练和验证准确率](img/B17792_16_4.jpg)'
- en: Figure 16.4 – Training and validation accuracy with the progress of training
    epochs
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4 – 随着训练epoch的推移的训练和验证准确率
- en: As our model training is now complete, we can start making predictions on unseen
    data. Let’s first check the results on our test set.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型训练现在已经完成，我们可以开始对未见数据做出预测。让我们首先检查测试集上的结果。
- en: Results
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: Here comes the interesting part where we check the TF-model results on our unseen
    test set. Before checking the numbers, let’s first plot a few test images along
    with their true labels and model outputs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来检查未见测试集上的TF模型结果。在检查数字之前，让我们首先绘制一些测试图像及其真实标签和模型输出。
- en: Checking the results on a few random test images
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查几个随机测试图像的结果
- en: 'Let’s first visually verify the results of the model by choosing a few random
    images from our test set, making predictions on them, and plotting them with model
    predictions along with actual labels. We will put the label and prediction information
    in the image titles, as shown in the following Python snippet:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先通过从测试集中选择一些随机图像，对它们进行预测，并将模型预测与实际标签一起绘制出来来验证模型的输出结果。我们将在图像标题中放入标签和预测信息，如下面的Python代码片段所示：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output of this snippet is shown in *Figure 16**.4*. We can see that the
    model is slightly confused between class 0 and class 1 but easily identifies class
    2, due to big clues on the shape. As a reminder, class 0 here represents good
    bangles, class 1 represents defected bangles, and class 2 represents broken bangles.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出结果展示在*图16**.4*中。我们可以看到，模型在类别0和类别1之间有些混淆，但由于形状上的大线索，很容易识别类别2。提醒一下，这里的类别0代表好的手镯，类别1代表有缺陷的手镯，类别2代表破损的手镯。
- en: '![Figure 16.5 – The model results for the classification of good, defected,
    and broken bangles](img/B17792_16_5.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图16.5 – 对良好、有缺陷和破损手镯进行分类的模型结果](img/B17792_16_5.jpg)'
- en: Figure 16.5 – The model results for the classification of good, defected, and
    broken bangles
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 – 对良好、有缺陷和破损手镯进行分类的模型结果
- en: As per *Figure 16**.5*, our model is doing a good job of identifying class 2
    but is slightly confused between class 0 and class 1, due to very tiny visual
    clues. Next, let’s check the metrics on the entire test set to get a sense of
    the quality of our classifier.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图16**.5*所示，我们的模型在识别类别2方面做得很好，但在类别0和类别1之间有些混淆，这是由于非常微小的视觉线索。接下来，让我们检查整个测试集上的指标，以了解我们分类器的质量。
- en: Classification report
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类报告
- en: 'To generate the classification report on our entire test set, we first need
    to generate model outputs for the entire test set. We will choose the class with
    maximum probability as the model prediction (note that choosing the output class
    like that may not be the best option when we have highly imbalanced datasets).
    The following is the Python code that generates the model output on the entire
    test set:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成整个测试集的分类报告，我们首先需要为整个测试集生成模型输出。我们将选择概率最大的类别作为模型预测（注意，当我们有高度不平衡的数据集时，选择输出类别可能不是最佳选择）。以下是在整个测试集上生成模型输出的Python代码：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can now print the classification report for our model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以打印出我们模型的分类报告：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following is the output of the classification report:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为分类报告的输出：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The classification report indicates that our model has a F1 score of around
    0.65 for class 0 and class 1, and 0.71 for class 2\. As suspected, the model is
    doing a better job at identifying broken bangle images. Thus, the recall of the
    model for the `Broken` class is very good, around 92%. Overall, our model is doing
    a decent job but has potential for improvement.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告表明，我们的模型在类别0和类别1上的F1分数约为0.65，在类别2上的分数约为0.71。正如所料，模型在识别破损手镯图像方面做得更好。因此，模型对“破损”类别的召回率非常好，约为92%。总的来说，我们的模型表现不错，但仍有改进的潜力。
- en: 'Improving the accuracy of this model can be a good exercise for you. The following
    are some hints that may help to increase the overall accuracy of the model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 提高这个模型的准确率可以是一个很好的练习。以下是一些可能有助于提高模型整体准确率的提示：
- en: Work with better resolution (something better than 200x200)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更高分辨率的图像（比200x200更好的分辨率）
- en: A deeper model (more and bigger CNN layers to get better features)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度模型（更多和更大的CNN层以获得更好的特征）
- en: Data augmentation (make the model more robust)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强（使模型更健壮）
- en: A better network (better feature extraction with an attention mechanism or any
    other feature extraction strategy)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的网络（使用注意力机制或其他特征提取策略进行更好的特征提取）
- en: 'Finally, let’s print the confusion matrix:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们打印出混淆矩阵：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is the output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: A confusion matrix can help to determine what kind of mistakes our model makes.
    In other words, when it classifies class 0 incorrectly, which class does our model
    confuse it with?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以帮助我们确定模型犯了什么类型的错误。换句话说，当模型错误地将类别0分类时，它会将其混淆为哪个类别？
- en: With this, our exercise of training a custom TF-based model for the task of
    defect detection is complete. We now have an average-performing trained TF model.
    Now, let’s see how can we deploy this model on Google Vertex AI.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们为缺陷检测任务训练自定义TF模型的练习就完成了。现在，我们有一个平均性能的已训练TF模型。现在，让我们看看如何将此模型部署到Google Vertex
    AI。
- en: Deploying a vision model to a Vertex AI endpoint
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将视觉模型部署到Vertex AI端点
- en: In the previous section, we completed our experiment of training a TF-based
    vision model to identify detects from product images. We now have a trained model
    that can identify defected or broken bangle images. To make this model usable
    in downstream applications, we need to deploy it to an endpoint so that we can
    query that endpoint, getting outputs for new input images on demand. There are
    certain things that are important to consider while deploying a model, such as
    expected traffic, expected latency, and expected cost. Based on these factors,
    we can choose the best infrastructure to deploy our models. If there are strict
    low-latency requirements, we can deploy our model to machines with accelerators
    (such as **Graphical Processing Units** (**GPUs**) or **Tensor Processing Units**
    (**TPUs**)). Conversely, we don’t have the necessity of online or on-demand predictions,
    so we don’t need to deploy our model to an endpoint. Offline batch-prediction
    requests can be handled without even deploying the model to an endpoint.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们完成了基于TF的视觉模型训练实验，用于从产品图像中识别缺陷。现在我们有一个可以识别有缺陷或破损手镯图像的模型。为了使该模型在下游应用中使用，我们需要将其部署到端点，以便我们可以查询该端点，按需获取新输入图像的输出。在部署模型时，有一些重要的事项需要考虑，例如预期的流量、预期的延迟和预期的成本。基于这些因素，我们可以选择最佳的基础设施来部署我们的模型。如果对低延迟有严格的要求，我们可以将模型部署到具有加速器（如**图形处理单元（GPU**）或**张量处理单元（TPU**））的机器上。相反，如果我们没有在线或按需预测的必要性，那么我们不需要将模型部署到端点。离线批量预测请求可以在不部署模型到端点的情况下处理。
- en: Saving model to Google Cloud Storage (GCS)
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型保存到Google Cloud Storage (GCS)
- en: 'In our case, we are interested in deploying our model to a Vertex AI endpoint,
    just to see how it works. The very first step is to save our trained TF model
    into GCS:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们感兴趣的是将我们的模型部署到Vertex AI端点，只是为了看看它的工作原理。第一步是将我们的训练好的TF模型保存到GCS：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Once the model is saved, the next step would be to upload this model artifact
    to the Vertex AI Model Registry. Once our model is uploaded to the Model Registry,
    it can be deployed easily, either by using the Google Cloud console UI or Python
    scripts.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型保存后，下一步就是将此模型工件上传到Vertex AI模型注册表。一旦我们的模型上传到模型注册表，就可以轻松部署，无论是使用Google Cloud控制台UI还是Python脚本。
- en: Uploading the TF model to the Vertex Model Registry
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将TF模型上传到Vertex模型注册表
- en: In this step, we will utilize the Vertex AI SDK to upload our TF model to the
    Model Registry. Alternatively, we can also use the console UI to do the same.
    For this purpose, we will be required to provide a model display name, a region
    to upload the model to, the URI of the saved model artifact, and a serving container
    image. Note that the serving container images must have dependencies installed
    for appropriate versions of frameworks, such as TF. Google Cloud provides a list
    of prebuilt serving containers that can be readily used to serve the models. In
    our case, we will also use a prebuilt container that supports TF 2.11 for serving.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用Vertex AI SDK将我们的TF模型上传到模型注册表。或者，我们也可以使用控制台UI来完成同样的操作。为此，我们需要提供模型显示名称、上传模型到的区域、保存模型工件的URI以及服务容器镜像。请注意，服务容器镜像必须安装了框架的适当版本的依赖项，例如TF。Google
    Cloud提供了一系列预构建的服务容器，可以立即用于模型服务。在我们的案例中，我们也将使用支持TF 2.11的服务预构建容器。
- en: 'Let’s set up some configurations:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一些配置：
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, we can go ahead and upload the model artifact to the Model Registry, as
    shown in the following Python code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续上传模型工件到模型注册表，如下面的Python代码所示：
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is the output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We have now successfully uploaded our TF model to the Registry. We can now locate
    our model, using either the model display name or the model resource ID, as per
    the previous output. Next, we need an endpoint to deploy our model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经成功将我们的 TF 模型上传到注册表。我们可以使用模型显示名称或模型资源 ID 来定位我们的模型，如前所述。接下来，我们需要一个端点来部署我们的模型。
- en: Creating a Vertex AI endpoint
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Vertex AI 端点
- en: In this step, we will create a Vertex AI endpoint that will be used to serve
    our model prediction requests. Again, the endpoints can also be created using
    the Google Cloud console UI, but here, we will do it programmatically with the
    Vertex AI SDK.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们将创建一个 Vertex AI 端点，用于处理我们的模型预测请求。同样，端点也可以使用 Google Cloud 控制台 UI 来创建，但在这里，我们将使用
    Vertex AI SDK 进行编程操作。
- en: 'Here is the sample function that can be leveraged to create an endpoint:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个可以用来创建端点的示例函数：
- en: '[PRE34]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s use this function to create an endpoint for our model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数来为我们的模型创建一个端点：
- en: '[PRE35]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This snippet gives us the following output, and it confirms that the endpoint
    has been created (note that this endpoint is currently empty without any model):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段给出以下输出，并确认端点已创建（请注意，此端点目前为空，没有任何模型）：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can use the following command to list all the endpoints within a region
    to verify whether it has been successfully created:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令列出区域内的所有端点，以验证其是否已成功创建：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This command gives the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令给出以下输出：
- en: '[PRE38]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Our endpoint is now set, so we can go ahead and deploy a model now.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的端点现在已设置，因此我们可以继续部署模型。
- en: Deploying a model to the Vertex AI endpoint
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型部署到 Vertex AI 端点
- en: 'We now have our TF model in the Model Registry, and we have also created a
    Vertex AI endpoint. We are now all set to deploy our model to this endpoint. This
    action can also be performed using the Google Cloud console UI, but here, we will
    do it programmatically using the Vertex AI SDK. First, let’s get the details of
    our model from the Registry:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在模型注册表中有了我们的 TF 模型，并且我们也创建了一个 Vertex AI 端点。我们现在已经准备好将我们的模型部署到这个端点上了。此操作也可以使用
    Google Cloud 控制台 UI 来执行，但在这里，我们将使用 Vertex AI SDK 进行编程操作。首先，让我们从注册表中获取我们模型的详细信息：
- en: '[PRE39]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, let’s deploy our model to the endpoint:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的模型部署到端点上：
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As soon as the execution of this snippet is complete without any errors, our
    model should be ready to accept requests for online predictions. This deployed
    model information should be visible in the Google Cloud console UI as well. We
    can also verify the deployed model by using the following function:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦此代码片段执行完成且没有错误，我们的模型应该准备好接受在线预测的请求。部署的模型信息也应显示在 Google Cloud 控制台 UI 中。我们还可以使用以下函数来验证部署的模型：
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This should print the resources related to the deployed model. The output will
    look something similar to the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打印出与部署模型相关的资源。输出将类似于以下内容：
- en: '[PRE42]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We have now verified that our TF model is successfully deployed. Now, we are
    all set to call this endpoint for predictions. In the next section, we will see
    how an endpoint with a deployed model can serve online predictions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经验证了我们的 TF 模型已成功部署。现在，我们已经准备好调用此端点进行预测。在下一节中，我们将了解具有部署模型的端点如何提供在线预测。
- en: Getting online predictions from a vision model
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视觉模型获取在线预测
- en: In the previous section, we deployed our custom TF-based model to a Vertex AI
    Endpoint so that we could embed it into any downstream application, querying it
    for on-demand or online predictions. In this section, we will see how we can call
    this endpoint for online predictions programmatically using Python. However, the
    prediction requests can also be made by using a `curl` command and sending a JSON
    file with input data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将基于 TF 的自定义模型部署到 Vertex AI 端点，以便我们可以将其嵌入到任何下游应用程序中，进行按需或在线预测。在本节中，我们将了解如何使用
    Python 编程方式调用此端点进行在线预测。然而，预测请求也可以通过使用 `curl` 命令并发送包含输入数据的 JSON 文件来实现。
- en: There are a few things to consider while making prediction requests; the most
    important part is pre-processing the input data accordingly. In the first section,
    when we trained our model, we did some pre-processing on our image dataset to
    make it compatible with the model. Similarly, while requesting the predictions,
    we should follow the exact same data preparation steps. Otherwise, either the
    model request will fail, due to an incompatible input format, or it will give
    bad results, due to training-serving skew. We already have pre-processed test
    images, so we can pass them directly within the prediction requests.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在制作预测请求时，有一些事情需要考虑；最重要的是相应地预处理输入数据。在第一部分，当我们训练模型时，我们对图像数据集进行了一些预处理，使其与模型兼容。同样，在请求预测时，我们也应该遵循完全相同的数据准备步骤。否则，由于输入格式不兼容，模型请求可能会失败，或者由于训练-服务偏差，它可能会给出不良的结果。我们已经有预处理的测试图像，因此我们可以直接在预测请求中传递它们。
- en: 'As we will pass the input image to the request JSON, we will need to encode
    it into a JSON serializable format. So, we will send our input image to the model
    encode with the `base64` format. Check out the following snippet for an example
    payload creation of a single test image:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将输入图像传递到请求JSON中，我们需要将其编码为JSON可序列化格式。因此，我们将以`base64`格式将我们的输入图像发送到模型进行编码。查看以下示例代码，以创建单个测试图像的负载示例：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s set some configurations to call the endpoint:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置一些配置来调用端点：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can now go ahead and make a prediction request to our Vertex AI endpoint.
    Note that our parameters dictionary is empty, which means that we will get the
    raw model predictions as a result. In a more customized setting, we can also pass
    some parameters, such as *thresholds*, to perform minor post-processing on the
    model predictions accordingly. Check out the following Python code to request
    predictions:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续向我们的Vertex AI端点发送预测请求。请注意，我们的参数字典是空的，这意味着我们将得到原始模型预测结果。在一个更定制化的设置中，我们也可以传递一些参数，例如*阈值*，以对模型预测进行相应的后处理。查看以下Python代码以请求预测：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Similarly, we can pass a number of images in the `instances` variable and get
    an on-demand or online prediction result from an endpoint.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以在`instances`变量中传递多个图像，并从端点获取按需或在线预测结果。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we created an end-to-end vision-based solution to detect visual
    defects from images. We saw how CNN-based deep learning architectures can be used
    to extract useful features from images and then use those features for tasks such
    as classification. After training and testing our model, we went ahead and deployed
    it to a Vertex AI endpoint, allowing it to serve online or on-demand prediction
    requests for any number of downstream applications.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们创建了一个端到端基于视觉的解决方案，用于从图像中检测视觉缺陷。我们看到了如何使用基于CNN的深度学习架构从图像中提取有用的特征，然后使用这些特征进行分类等任务。在训练和测试我们的模型之后，我们继续将其部署到Vertex
    AI端点，使其能够为任何数量的下游应用提供在线或按需预测请求。
- en: After completing this chapter, you should be confident about how to approach
    vision-based problems and how to utilize ML to solve them. You should now be able
    to train your own vision-based classification models to solve real-world business
    problems. After completing the second section on deploying a custom model to a
    Vertex AI endpoint and the third section on getting online prediction from a Vertex
    endpoint, you should now be able to make your custom vision models usable for
    any downstream business application, by deploying them to Google Vertex AI. We
    hope this chapter was a good learning experience, with a hands-on real-world example.
    The next chapter will also present a hands-on example of a real-world, NLP-related
    use case.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，你应该对如何处理基于视觉的问题以及如何利用机器学习来解决这些问题充满信心。你现在应该能够训练自己的基于视觉的分类模型来解决现实世界的商业问题。在完成将自定义模型部署到Vertex
    AI端点的第二部分和从Vertex端点获取在线预测的第三部分后，你现在应该能够通过将它们部署到Google Vertex AI，使你的自定义视觉模型适用于任何下游商业应用。我们希望本章是一个良好的学习体验，包含了一个动手的实战示例。下一章也将展示一个与真实世界NLP相关的实战用例。
