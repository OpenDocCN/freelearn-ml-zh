- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: 'Designing Databricks: Day One'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计Databricks：第一天
- en: “Design is not just what it looks like and feels like. Design is how it works.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “设计不仅仅是看起来和感觉如何。设计是如何运作的。”
- en: '- Steve Jobs'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '- 史蒂夫·乔布斯'
- en: 'This chapter will introduce concepts and topics that engineers, data scientists,
    and people in similar roles should know to set themselves up for success in the
    Databricks Data Intelligence Platform. When setting up your data and AI platform,
    in our case, Databricks, there are always best practices to follow. We share those
    in this chapter to give you a better understanding of the setup options and their
    impacts; these can be strategic decisions that impact the entire data product
    workflow, as well as simply matters of preference. We start by explaining Databrick’s
    general architecture and key terminology, then cover the most important decisions
    to be made during platform setup, and conclude with code examples and configurations
    to download the data for our example projects. We also introduce a variety of
    platform features and components throughout the chapter, which we will cover in
    more detail throughout the rest of this book. Here is what you will learn as part
    of this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍工程师、数据科学家以及类似角色应该了解的概念和主题，以便在Databricks数据智能平台上取得成功。在设置数据和AI平台时，以我们为例，Databricks，总有最佳实践需要遵循。我们在本章中分享这些最佳实践，以便您更好地理解设置选项及其影响；这些可能是影响整个数据产品工作流程的战略决策，也可能是简单的偏好问题。我们首先解释Databricks的一般架构和关键术语，然后介绍平台设置期间需要做出的最重要的决策，并以代码示例和配置来下载示例项目的数据。我们还在本章中介绍了各种平台功能和组件，这些将在本书的其余部分进行更详细的介绍。以下是本章中将学习的内容：
- en: 'Here is what you will learn about as part of this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章中将学习的内容：
- en: Planning your platform
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划您的平台
- en: Defining a workspace
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义工作区
- en: Selecting the metastore
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择元数据存储
- en: Discussing data preparation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论数据准备
- en: Planning to create features
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划创建特征
- en: Modeling in Databricks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks中的建模
- en: Applying learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用学习
- en: Planning your platform
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划您的平台
- en: This section covers topics for discussion before and during the DI Platform
    setup process. The role of the data team often determines the platform setup.
    One of Databricks’ ideal attributes is that the technology stack is unified, making
    the setup and collaboration between teams more straightforward. The data team
    reporting structure frequently determines the border where one role ends and another
    begins, rather than the actual data product workflow. Luckily, we do not have
    to worry because the DI Platform serves data engineers, scientists, and analysts
    alike.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '本节涵盖了在DI平台设置前后需要讨论的主题。数据团队的角色通常决定了平台设置。Databricks的理想属性之一是其技术栈统一，这使得团队设置和协作更加直接。数据团队的报告结构通常决定了角色结束和开始的地方，而不是实际的数据产品工作流程。幸运的是，我们不必担心，因为DI平台服务于数据工程师、科学家和分析人员。 '
- en: In *Figure 2**.1*, you can see an end-to-end lakehouse architecture and the
    components in Databricks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图2.1**中，您可以查看端到端的湖屋架构以及Databricks中的组件。
- en: '![Figure 2.1 – Overview of a lakehouse architecture and how Databricks DI Platform
    fits this paradigm](img/B16865_02_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 湖屋架构概述以及Databricks DI平台如何适应这一范式](img/B16865_02_01.jpg)'
- en: Figure 2.1 – Overview of a lakehouse architecture and how Databricks DI Platform
    fits this paradigm
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 湖屋架构概述以及Databricks DI平台如何适应这一范式
- en: The DI Platform consists of one or more Databricks accounts. Most of the time,
    companies only have one. However, there are situations where companies require
    extra environment isolation, and having separate accounts for development, staging,
    and production is an option. Discussion about multiple accounts for levels of
    isolation is outside of this book's scope, but if you have questions or want to
    know more, please check out the resources in *Further reading*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DI平台由一个或多个Databricks账户组成。大多数情况下，公司只有一个账户。然而，在某些情况下，公司需要额外的环境隔离，并为开发、预生产和生产设置单独的账户是一个选择。关于多个账户用于隔离级别的讨论超出了本书的范围，但如果您有疑问或想了解更多信息，请查看**进一步阅读**中的资源。
- en: '![Figure 2.2 – Visual representation of the environment isolation options](img/B16865_02_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 环境隔离选项的视觉表示](img/B16865_02_02.jpg)'
- en: Figure 2.2 – Visual representation of the environment isolation options
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 环境隔离选项的视觉表示
- en: We separate our environments using different catalogs. Most of this book’s project
    work occurs using the `ml_in_action` catalog. For the production version of some
    models, we use the `ml_in_prod` catalog. Setting up multiple workspaces is another
    way to separate environments. We recommend using documentation and your company
    policies to guide your isolation setup. Let’s move on to what precisely a workspace
    is in the context of Databricks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的目录来分离我们的环境。本书的大部分项目工作都使用`ml_in_action`目录。对于某些模型的生成版本，我们使用`ml_in_prod`目录。设置多个工作区是另一种分离环境的方法。我们建议使用文档和您的公司政策来指导您的隔离设置。让我们继续讨论在Databricks环境中工作区究竟是什么。
- en: Defining a workspace
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义工作区
- en: 'It’s important to know that Databricks uses the word **workspace** to refer
    to two distinct components: an instance of Databricks (meaning your hosted Databricks
    deployment that you access via your unique URL address) and the folder environment
    for accessing your work products, like notebooks, queries, and dashboards.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道Databricks使用“工作区”一词来指代两个不同的组件：Databricks的一个实例（意味着您通过唯一的URL地址访问的托管Databricks部署）以及访问您的工作成果（如笔记本、查询和仪表板）的文件夹环境。
- en: 'Let’s go through the two components:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一了解这两个组件：
- en: '**Workspace as an instance**: A Databricks account can have multiple workspaces
    attached to it, meaning instances of the DI Platform are deployed and often accessible
    from a browser, as mentioned previously, but are also accessible via an SDK or
    a REST API.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作区作为一个实例**：一个Databricks账户可以连接多个工作区，这意味着DI平台的实例已部署，通常可以通过浏览器访问，如前所述，但也可以通过SDK或REST
    API访问。'
- en: '`Workspace` folder to store their MLFlow experiments or Terraform states for
    pipeline deployment. You can also create and store notebooks outside source control
    in your home and project folders.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`工作区`文件夹用于存储他们的MLFlow实验或Terraform状态，以便进行管道部署。您还可以在您的个人和项目文件夹中创建和存储笔记本，而不在源代码控制之外。'
- en: We now have a clearer understanding of a workspace. Now let’s discuss why we
    choose **Unity Catalog** (**UC**) as our preferred metastore.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对工作区有了更清晰的理解。现在让我们讨论为什么我们选择**Unity Catalog**（**UC**）作为我们首选的元数据存储。
- en: Selecting the metastore
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择元数据存储
- en: A metastore is a system that stores metadata for a data platform and can be
    thought of as the top-level container of objects. It registers a variety of information
    about databases, tables, views, **User-Defined Functions** (**UDFs**), and other
    data assets. Metadata includes details such as storage location and the permissions
    that govern access to each asset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据存储是一个存储数据平台元数据的系统，可以将其视为对象的顶层容器。它注册有关数据库、表、视图、**用户定义函数**（**UDFs**）和其他数据资产的各种信息。元数据包括诸如存储位置和每个资产访问权限的详细信息。
- en: 'Two types of metastores are natively available in the DI Platform: **Unity
    Catalog** (**UC**) and the **Hive Metastore** (**HMS**). UC has a three-level
    namespace consisting of a catalog, a database (also called a schema), and a table
    name. In contrast, the HMS only uses a two-level namespace containing just a database
    and table name. A metastore is required for your Databricks Workspace instance,
    as this is the component that organizes and governs data access. Deciding on the
    right metastore is an early decision in your DI Platform journey, and we recommend
    Unity Catalog. Let’s talk about why.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DI平台中本机提供两种类型的元数据存储：**Unity Catalog**（**UC**）和**Hive Metastore**（**HMS**）。UC由一个包含目录、数据库（也称为模式）和表名的三级命名空间组成。相比之下，HMS只使用一个包含数据库和表名的两级命名空间。元数据存储对于您的Databricks工作区实例是必需的，因为这是组织和控制数据访问的组件。选择正确的元数据存储是您DI平台之旅的早期决策之一，我们建议使用Unity
    Catalog。让我们谈谈为什么。
- en: Notice in *Figure 2**.3* that you can have multiple workspaces assigned to the
    same metastore. **Access Controls** and **User Management** are scoped to the
    account level, as shown in the figure. A UC **Metastore**, a group of catalogs,
    is scoped to a region with precisely one metastore per region. Within the region,
    you can easily share **Data**, **Features**, **Volumes** access, **Functions**,
    and **Models**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在*图2.3*中，您可以拥有多个分配给同一元数据存储的工作区。**访问控制**和**用户管理**的范围是账户级别，如图所示。一个UC元数据存储，一组目录，范围在一个区域，每个区域恰好有一个元数据存储。在区域内，您可以轻松共享**数据**、**特征**、**卷**访问、**函数**和**模型**。
- en: '![Figure 2.3 – The Design of Unity Catalog with multiple workspaces](img/B16865_02_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – 具有多个工作区的Unity Catalog设计](img/B16865_02_03.jpg)'
- en: Figure 2.3 – The Design of Unity Catalog with multiple workspaces
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 具有多个工作区的Unity Catalog设计
- en: Unity Catalog is more than a group of data assets. UC also tracks who has accessed
    assets, which makes auditing a simple exercise. Using UC allows companies to administer
    privileges and secure data and objects easily while being able to share them between
    various workspaces. Securely sharing between environments is one of the reasons
    why we recommend using the UC metastore.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Unity Catalog不仅仅是数据资产的一组。UC还跟踪谁访问了资产，这使得审计变得简单。使用UC允许公司轻松管理权限和确保数据及对象的安全，同时能够在不同的工作区之间共享它们。在环境之间安全共享是为什么我们推荐使用UC元数据存储的原因之一。
- en: The HMS design is less centralized than that of UC. For example, historically,
    workspaces have been created as data and code isolation, meaning there is a separate
    workspace for separate isolation levels. This design often required a centralized
    model registry workspace in addition to development, staging, and production workspaces.
    If not using UC, each workspace requires its own HMS and user and group management.
    In contrast, UC governs all assets at the account level rather than the individual
    workspace level; see *Figure 2**.3*, *Figure 2**.4*, and *Further reading*. The
    centralized governance model provides the ability to integrate multiple workspaces
    more seamlessly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: HMS设计比UC更少集中化。例如，从历史上看，工作区作为数据和代码隔离被创建，这意味着有单独的工作区用于不同的隔离级别。这种设计通常需要在开发、测试和生成工作区之外，还有一个集中的模型注册表工作区。如果不使用UC，每个工作区都需要自己的HMS和用户组管理。相比之下，UC在账户级别而不是单个工作区级别管理所有资产；参见*图2.3*、*图2.4*和*进一步阅读*。集中的治理模型提供了更无缝地集成多个工作区的能力。
- en: '![Figure 2.4 – UC governs all assets under catalogs, including databases, tables,
    volumes, functions, and models](img/B16865_02_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – UC管理目录下的所有资产，包括数据库、表、卷、函数和模型](img/B16865_02_04.jpg)'
- en: Figure 2.4 – UC governs all assets under catalogs, including databases, tables,
    volumes, functions, and models
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – UC管理目录下的所有资产，包括数据库、表、卷、函数和模型
- en: Deciding on your metastore does not have to be a permanent choice. However,
    migrating later could become a headache. UC is continually improving and integrating
    with new Databricks features. The list of reasons to choose UC over HMS continues
    to grow, and our recommendation is to begin with and stick with UC.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定您的元数据存储时，不必是一个永久的决定。然而，稍后迁移可能会变得头疼。UC持续改进并与新的Databricks功能集成。选择UC而不是HMS的原因列表持续增长，我们的建议是从UC开始并坚持使用UC。
- en: To determine whether Unity Catalog is the right choice for you and your company,
    you can check out the shrinking list of limitations for choosing UC in *Further
    reading*. As UC continues to expand in capability, it is the path of the future.
    Specifically, for machine learning, there is an integration with the Feature Engineering
    client and the new Model Registry. Using the UC Model Registry for model sharing
    and governing is simpler. We will cover more about the Model Registry in Unity
    Catalog and Databricks Feature Engineering Client in *Chapters 5, 6*, and *7*,
    but if you’re curious and eager to learn more now, you can check out Manage model
    lifecycle in Unity Catalog in the Further reading section. Given the ever-growing
    number of reasons to use UC, all project code in this book will use UC.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定Unity Catalog是否适合您和您的公司，您可以在*进一步阅读*中查看选择UC的限制列表。随着UC在功能上的持续扩展，它是未来的道路。特别是对于机器学习，它与特征工程客户端和新的模型注册表集成。使用UC模型注册表进行模型共享和管理更简单。我们将在第5、6和7章中详细介绍Unity
    Catalog中的模型注册表和Databricks特征工程客户端，但如果您现在就好奇并渴望了解更多，您可以在*进一步阅读*部分查看“在Unity Catalog中管理模型生命周期”。鉴于使用UC的理由不断增多，本书中的所有项目代码都将使用UC。
- en: Defining where the data lives, and cloud object storage
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义数据存储的位置和云对象存储
- en: All data products start with data, and so how we make the data accessible for
    data teams is another important early design choice. Databricks is a cloud-based
    platform that connects to cloud object storage – **Azure Data Lake Storage** (**ADLS**),
    Amazon **Simple Storage Service** (**S3**), or **Google Cloud Storage** (**GCS**).
    There is a separation of compute and storage. Databricks orchestrates compute;
    the data is in cloud object storage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据产品都始于数据，因此我们如何使数据对数据团队可访问是另一个重要的早期设计选择。Databricks是一个基于云的平台，可以连接到云对象存储 –
    **Azure Data Lake Storage** (**ADLS**), Amazon **Simple Storage Service** (**S3**),
    或 **Google Cloud Storage** (**GCS**)。计算和存储是分离的。Databricks负责计算；数据存储在云对象存储中。
- en: Originally, data had to be in cloud object storage before being utilized on
    the DI Platform. Now, **Query Federation** allows customers to query their data
    no matter where it resides (see the documentation for any possible limitations)
    without first worrying about ingestion and data engineering from that remote system.
    However, that is data in memory, not persistent data. You can land your data in
    cloud storage in various ways. There are many documentation sources and external
    tools for the actual landing of data in cloud storage. These may depend on your
    cloud service provider of choice. Despite best practices of storing data in your
    cloud storage, using the **Databricks File System** (**DBFS**) to store the data
    for this book’s example projects is also possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，数据必须在DI平台被利用之前存储在云对象存储中。现在，**查询联邦**允许客户无论数据存储在哪里都可以查询他们的数据（请参阅文档了解任何可能的限制），无需首先担心从远程系统进行摄取和数据工程。然而，这是内存中的数据，而不是持久数据。您可以通过多种方式将数据存放在云存储中。有许多文档资源和外部工具可用于在云存储中实际存放数据。这些可能取决于您选择的云服务提供商。尽管存储数据在您的云存储中是最佳实践，但使用**Databricks文件系统（DBFS**）来存储本书示例项目中的数据也是可能的。
- en: DBFS is a shared filesystem provided by Databricks that all users of a given
    workspace can access. Any data stored in DBFS is potentially accessible to all
    users, regardless of their group, role, or permissions. Therefore, only non-sensitive
    and non-production data you are willing to share openly across your organization
    should be in DBFS. An example of non-sensitive data would be the publicly available
    *Kaggle* datasets. This lack of governance is why we recommend storing data in
    Databricks volumes, where you can apply governance. We will cover more on volumes
    in the last section of this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DBFS是由Databricks提供的共享文件系统，所有给定工作区的用户都可以访问。存储在DBFS中的任何数据都可能对所有用户可访问，无论他们的组、角色或权限如何。因此，只有您愿意在组织内公开共享的非敏感和非生产数据才应该存放在DBFS中。非敏感数据的例子包括公开可用的*Kaggle*数据集。正是由于缺乏治理，我们建议将数据存储在Databricks卷中，在那里您可以应用治理。我们将在本章的最后部分更详细地介绍卷。
- en: When it comes to cloud storage, the optimal format for structured data is almost
    always Delta, which we talked about in detail in [*Chapter 1*](B16865_01.xhtml#_idTextAnchor016).
    When a table is stored in the Delta format, we refer to it as a Delta table. You
    can choose tables to be “managed” or “external” tables. We use both types of tables
    in this book (the choice is justified when required). Please see the resources
    in *Further reading* for more information on the two types of tables.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到云存储时，结构化数据的最佳格式几乎总是Delta，我们在[*第一章*](B16865_01.xhtml#_idTextAnchor016)中详细讨论了这一点。当一个表以Delta格式存储时，我们将其称为Delta表。您可以选择将表设置为“管理”或“外部”表。在这本书中我们使用这两种类型的表（当需要时，选择是合理的）。请参阅*进一步阅读*资源，了解更多关于两种类型表的信息。
- en: Discussing source control
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论源控制
- en: Whether or not to use a source control is usually not the question. The question
    is how should someone use it? Databricks has a few features that can aid in source
    control.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 是否使用源控制通常不是问题。问题是某人应该如何使用它？Databricks有一些功能可以帮助源控制。
- en: The first is the version history that is built into notebooks. The history of
    changes for each notebook is tracked even before submitting it to a remote repository
    using Git. Version tracking is beneficial as we are not always ready to make a
    Git commit but still want to track progress and collaborate. It’s also a game
    changer if you accidentally pull someone’s code into your working remote branch
    and forget to push your code before it. The notebook’s history will keep your
    edited copy so you can simply roll back in time and restore all your work!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项是笔记本中内置的版本历史记录。每个笔记本的更改历史在提交到远程仓库（使用Git）之前就被跟踪了。版本跟踪的好处在于我们并不总是准备好进行Git提交，但仍然想要跟踪进度和协作。如果您不小心将某人的代码拉入您的工作远程分支，并且忘记在之前推送您的代码，这也会是一个变革性的变化。笔记本的历史记录将保留您的编辑副本，这样您就可以简单地回滚时间并恢复所有工作！
- en: The second feature is the ease of connecting notebooks and files in your workspace
    to a remote Git repository. Historically, saving Jupyter notebooks to remote repositories
    was a technical nightmare for code reviews, sharing, and diffs. The Databricks
    code repository integration allows Databricks notebooks to contain multiple languages
    (Python, Scala, SQL) and track them as nicely as a typical Python file. This ability
    to track notebooks in source as a standard file is an improvement for data engineers
    and scientists wanting to review notebooks compared to previously converting files
    to Python and losing all output and images. The days of setting up hooks to automatically
    save your notebook as a regular Python file every time you save your notebook
    are over.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个特点是轻松将工作空间中的笔记本和文件连接到远程Git仓库。历史上，将Jupyter笔记本保存到远程仓库对于代码审查、共享和差异比较来说是一个技术噩梦。Databricks代码仓库集成允许Databricks笔记本包含多种语言（Python、Scala、SQL）并像典型的Python文件一样跟踪它们。这种将笔记本作为标准文件在源中跟踪的能力，对于想要审查笔记本的数据工程师和科学家来说是一个改进，与之前将文件转换为Python并丢失所有输出和图像相比。每次保存笔记本时自动将笔记本保存为常规Python文件的钩子设置的日子已经过去了。
- en: Of course, you can store standard file formats such as markdown, delimiter separated,
    JSON, or YML for a whole reproducibility approach. Note that we do not recommend
    keeping data under repos unless it’s a data sample for testing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以存储标准文件格式，如markdown、分隔符分隔、JSON或YML，以实现整个可重复性方法。请注意，除非是测试用的数据样本，否则我们不推荐在仓库中保留数据。
- en: Within a repository, how a team defines the expected folder structure for each
    project is generally less important than the consistent use of that structure.
    However, defining your project structure is still important. We recommend reading
    through *Big Book of MLOps* (Joseph Bradley, Rafi Kurlansik, Matthew Thomson,
    and Niall Turbitt, 2023, *Big Book of MLOps, second edition*, [https://www.databricks.com/resources/ebook/the-big-book-of-mlops](https://www.databricks.com/resources/ebook/the-big-book-of-mlops))
    to determine the best structure for your team or organization. As we will see
    in future chapters, MLflow and repositories are essential for **reproducible research**.
    In the world of data science and machine learning specifically, we want to ensure
    the reproduction of models and experiments.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个仓库中，团队如何定义每个项目的预期文件夹结构通常不如该结构的持续使用重要。然而，定义你的项目结构仍然很重要。我们建议阅读*《MLOps大全书》*（Joseph
    Bradley，Rafi Kurlansik，Matthew Thomson和Niall Turbitt，2023，*《MLOps大全书，第二版》*，[https://www.databricks.com/resources/ebook/the-big-book-of-mlops](https://www.databricks.com/resources/ebook/the-big-book-of-mlops)）以确定最适合你团队或组织的结构。正如我们将在未来的章节中看到，MLflow和仓库对于**可重复研究**至关重要。在数据科学和机器学习的特定领域，我们希望确保模型和实验的可重复性。
- en: Discussing data preparation
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论数据准备
- en: Generally, the first step for any data science project is to explore and prepare
    the data. We will refer to this process as moving the data from “Bronze” to “Silver”
    layers in reference to the Medallion architecture methodology. You might think
    of this type of data transformation exclusively as a data engineering task, but
    it’s also essential for data science and machine learning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，任何数据科学项目的第一步是探索和准备数据。我们将这个过程称为根据Medallion架构方法将数据从“Bronze”层移动到“Silver”层。你可能认为这种类型的数据转换仅是数据工程任务，但它对于数据科学和机器学习也是至关重要的。
- en: 'If you aren’t familiar with this architecture terminology, the **Medallion
    architecture** is a data design pattern used to organize data logically in a warehouse.
    This architecture is also commonly called “multi-hop” architecture. It aims to
    incrementally and progressively improve the structure and quality of data as it
    flows through each layer. The Medallion architecture has three layers: Bronze,
    Silver, and Gold, listed as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不太熟悉这种架构术语，**Medallion架构**是一种用于在仓库中逻辑组织数据的数据设计模式。这种架构也通常被称为“多跳”架构。它的目标是随着数据流过每一层而逐步和渐进地改进数据的结构和质量。Medallion架构有三个层次：Bronze、Silver和Gold，如下所示：
- en: The **Bronze** layer is the raw data layer. It contains all the raw, unprocessed
    data ingested from the source systems. This data still needs to be cleaned or
    transformed.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bronze**层是原始数据层。它包含从源系统摄取的所有原始、未处理的数据。这些数据仍需要被清理或转换。'
- en: The **Silver** layer is the validated data layer. It contains data that has
    been cleaned and is subject to various validation and transformation steps. This
    data is ready to be used for analysis and modeling.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**银**层是经过验证的数据层。它包含经过清理且已接受各种验证和转换步骤的数据。这些数据可用于分析和建模。'
- en: The **Gold** layer is the enriched data layer. It is the highest level and contains
    data enriched with additional information, such as business intelligence metrics
    and key performance indicators, to meet the requirements of the business users.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金**层是增强数据层。它是最高级别，包含添加了额外信息的数据，如商业智能指标和关键绩效指标（KPIs），以满足业务用户的需求。'
- en: The Medallion architecture is a flexible and customizable architecture that
    can meet the specific needs of each organization. The Medallion architecture is
    compatible with the concept of Data Mesh. Data Mesh is an architectural and organizational
    paradigm to ensure value from data. Lakehouse and Data Mesh are complementary,
    paradigms. See *Further reading* for blog posts on leveraging a data mesh with
    the DI Platform. This distributed data architecture enables organizations to unlock
    the value of their data by making it accessible and usable by everyone in the
    organization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Medallion架构是一种灵活且可定制的架构，可以满足每个组织的特定需求。Medallion架构与Data Mesh概念兼容。Data Mesh是一种架构和组织范式，旨在确保数据的价值。Lakehouse和Data
    Mesh是互补的范式。参见*进一步阅读*部分，了解如何利用DI平台利用数据网格的博客文章。这种分布式数据架构使组织能够通过使组织中的每个人都能访问和使用数据来释放数据的价值。
- en: 'Communication and collaboration are vital for the data preparation process.
    This step involves identifying and correcting errors, filling in missing values,
    and resolving inconsistencies in the data. The actions you take should be discussed
    as a team and documented. This is especially important when working collaboratively
    across data teams because data engineers and data scientists often have different
    perspectives on how data should be prepared. For example, we have seen situations
    where an engineer imputed all the missing values in a column with a zero. The
    rationalization made sense; many zeros were already in the column, making the
    KPIs’ values come out correctly. However, from a modeling perspective, missing
    data differs from zeros, especially if there are already zeros in the dataset.
    The approach of replacing nulls with zeros is not necessarily incorrect; it simply
    needs to be discussed with the downstream consumers of the data. One helpful communication
    tool is the column tagging functionality from the Databricks Catalog UI. See *Figure
    2**.5* for an example:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通信和协作对于数据准备过程至关重要。这一步骤涉及识别和纠正错误、填补缺失值以及解决数据中的不一致性。你所采取的行动应该作为团队讨论并记录下来。当跨数据团队协作时，这一点尤为重要，因为数据工程师和数据科学家往往对数据应该如何准备有不同的看法。例如，我们见过工程师在一个列中用零插补所有缺失值的情况。这种合理化是有道理的；列中已经有很多零了，这使得关键绩效指标（KPIs）的值正确显示。然而，从建模的角度来看，缺失数据与零不同，特别是如果数据集中已经存在零的话。用零替换空值的方法并不一定是不正确的；它只需要与数据下游的消费者进行讨论。一个有用的沟通工具是Databricks目录UI中的列标记功能。参见*图2.5*的示例：
- en: '![Figure 2.5 – Example of how to use tagging in a catalog to communicate the
    transformation performed on a column to all table users](img/B16865_02_05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – 如何在目录中使用标记来向所有表用户传达对列进行的转换](img/B16865_02_05.jpg)'
- en: Figure 2.5 – Example of how to use tagging in a catalog to communicate the transformation
    performed on a column to all table users
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – 在目录中使用标记的示例，以向所有表用户传达对列进行的转换
- en: This implementation of an incorrect imputation method also serves as an example
    of wanting to go back and reprocess history. Luckily, using the Medallion architecture
    methodology is a saving grace. In the situation mentioned previously, the chosen
    imputation would only be present in the Silver and Gold data layers. Meanwhile,
    the Bronze layer still contains the original raw data, so the source of truth
    is not lost, and reprocessing is possible.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不正确的插补方法的实现也作为一个想要回溯和重新处理历史的例子。幸运的是，使用Medallion架构方法论是一种救赎。在之前提到的情况下，所选的插补方法只会出现在银和金数据层中。同时，青铜层仍然包含原始的原始数据，因此真相来源并未丢失，重新处理是可能的。
- en: One of the ways that the Databricks Platform boosts productivity and collaboration
    is the feature of real-time collaboration support for notebooks. This feature
    allows two or more people to simultaneously see and edit a notebook. The ability
    to pair-program virtually during the pandemic was a lifesaver for many. We’re
    big fans of people who have worked remotely for much of our careers. Collaborative
    editing of a notebook is much easier than sharing code via video call. While there
    are many options for reviewing code, historically, reviewing code in notebooks
    has been difficult, particularly when committing notebooks to source control.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks平台提高生产力和协作的一种方式是支持笔记本的实时协作功能。这个功能允许两个人或更多人同时查看和编辑一个笔记本。在疫情期间，能够在虚拟上结对编程对许多人来说是一个救星。我们非常欣赏那些在我们职业生涯的大部分时间里远程工作的人。与通过视频通话共享代码相比，笔记本的协作编辑要容易得多。虽然有许多代码审查的选项，但历史上，在笔记本中审查代码一直很困难，尤其是在将笔记本提交到源控制时。
- en: After completing transformations, documentation is easy using markdown in notebooks.
    Even if the resulting notebook is not not itself designated for production ETL,
    documenting the how and why of your data transformations is important for all
    downstream users. To read more about Databricks notebooks, see the documentation
    in *Further reading*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 完成转换后，使用笔记本中的Markdown进行文档编写变得容易。即使结果笔记本本身并未指定用于生产ETL，但记录数据转换的如何和为什么对所有下游用户来说都很重要。要了解更多关于Databricks笔记本的信息，请参阅*进一步阅读*中的文档。
- en: Planning to create features
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计划创建功能
- en: A data engineer may build Gold tables from Silver tables for consumption by
    the business. At the same time, a data scientist is building features from the
    same Silver tables for models. If we aren’t careful, two people working separately
    without communication can create different versions of the same metrics. When
    architecting your unified DI Platform, be sure to think about reusability and
    maintainability. For this reason, with features specifically, the features-as-code
    approach is our recommendation. Features-as-code refers to the software development
    practice *everything is code*, with a focus on creating a repository of reusable
    code to define features rather than features stored in tables.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师可能从银表构建金表以供业务使用。同时，数据科学家正在从相同的银表中构建特征以用于模型。如果我们不小心，两个没有沟通的独立工作者可能会创建相同指标的不同的版本。在架构你的统一DI平台时，务必考虑可重用性和可维护性。因此，对于功能而言，我们推荐功能即代码的方法。功能即代码是指软件开发实践*一切皆代码*，重点是创建一个可重用代码的仓库来定义特征，而不是将特征存储在表中。
- en: You can implement features-as-code in various ways. Initially, we mainly focus
    on function reusability. You can place functions you execute in multiple notebooks
    or scripts in a folder within the repository root directory. In the *Applying
    our learning* section, you will see this is where we store functions even when
    not calculating a feature per se. We call these the utils. You will be referencing
    the `mlia_utils` notebook throughout the example projects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过多种方式实现功能即代码。最初，我们主要关注函数的可重用性。你可以将你在多个笔记本或脚本中执行的功能放置在仓库根目录下的一个文件夹中。在*应用我们的学习*部分，你会看到即使不是在计算特征本身，我们也会在这里存储函数。我们称之为utils。在整个示例项目中，你将引用`mlia_utils`笔记本。
- en: You can find the `mlia_utils` functions in the root folder of the GitHub repository
    ([https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action](https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action)).
    We walk through pulling the GitHub repository into Databricks in the *Applying
    our learning* section. In it, you will find Python files containing useful functions
    we will use in the projects. It is best practice to save, share, and track functions
    so that the metrics and features we calculate are consistent. Note the empty `__init__.py`
    file is also in the `utils` folder. Having an `__init__.py` file is required.
    With this structure, we can use all functions as imports, for example, from `mlia_utils.rag_funcs`
    import `extract_doc_text`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub仓库的根目录中找到`mlia_utils`函数（[https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action](https://github.com/PacktPublishing/Databricks-Lakehouse-ML-In-Action)）。我们在*应用我们的学习*部分中介绍了将GitHub仓库拉入Databricks的过程。在其中，你会找到包含我们将用于项目的有用函数的Python文件。最佳实践是保存、共享和跟踪函数，以确保我们计算出的指标和特征是一致的。注意，空的`__init__.py`文件也在`utils`文件夹中。拥有一个`__init__.py`文件是必需的。有了这种结构，我们可以将所有函数作为导入使用，例如，从`mlia_utils.rag_funcs`导入`extract_doc_text`。
- en: Features-as-code is not only a way to reduce duplicative work by reusing functions.
    It is also a great way to ensure consistent business metrics. For example, online
    advertising often has complex calculations for the different types of revenue.
    Therefore, if other people or teams calculate business-critical metrics differently,
    it will be hard to establish the true metric value. Instead, you can often avoid
    this confusion by providing executive-approved functions for use. We will talk
    about this again in *Chapters 5* and *6*. In addition to features being centrally
    located and thus easier to find, Databricks offers easy ways to document your
    data assets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特征即代码不仅是一种通过重用函数来减少重复工作的方法。它也是一种确保一致业务指标的好方法。例如，在线广告通常对不同类型的收入有复杂的计算。因此，如果其他人或团队以不同的方式计算业务关键指标，将很难确定真正的指标值。相反，你通常可以通过提供经高管批准的函数来避免这种混淆。我们将在
    *第 5 章* 和 *第 6 章* 中再次讨论这个问题。除了特征集中存储且更容易找到之外，Databricks 还提供了轻松记录你的数据资产的方法。
- en: Creating a business process requiring the team to document tables and functions
    with descriptions makes current and previous efforts more discoverable. In the
    Databricks Catalog UI, you should see *AI-generated* suggestions to fill in your
    table and column descriptions, so you don’t have to start from scratch. Another
    great way to document transformations performed on a data table would be using
    tags. Tags can help with documentation and communication. Recall the example of
    missing data being imputed (*Figure 2**.5*).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个需要团队记录表格和函数描述的业务流程，可以使当前和以往的努力更容易被发现。在 Databricks 目录 UI 中，你应该会看到 *AI 生成*
    的建议来填写你的表格和列描述，这样你就不必从头开始。记录在数据表上执行转换的另一种好方法是使用标签。标签可以帮助进行文档和沟通。回想一下缺失数据被插补的例子（*图
    2**.5*）。
- en: Focusing on ML features more specifically, you will learn how to store and serve
    feature functions to simplify your final deployment process. You will create an
    on-demand feature function and use it in your model. We also will show you how
    to leverage saved feature tables to create training sets. If you want to jump
    ahead right away, see [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244), where we
    cover topics such as on-demand feature functions, feature lookups, syncing to
    the online store, and the Databricks Feature Engineering client.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地关注 ML 特征，你将学习如何存储和提供特征函数以简化你的最终部署过程。你将创建一个按需特征函数并在你的模型中使用它。我们还将向你展示如何利用保存的特征表创建训练集。如果你想立即跳过，请参阅
    [*第 5 章*](B16865_05.xhtml#_idTextAnchor244)，其中我们涵盖了按需特征函数、特征查找、同步到在线商店以及 Databricks
    特征工程客户端等主题。
- en: Modeling in Databricks
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Databricks 中的建模
- en: After features have been created and stored as feature tables, we create training
    sets and focus on model training. We will cover modeling in terms of leveraging
    Databricks to facilitate the model lifecycle in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    In [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), we’ll discuss the Unity Catalog
    Registry and how to use it to track an enormous amount of information from the
    associated experiments, in addition to details such as model lineage. You can
    register multiple versions of a model at every stage and can give these different
    versions aliases, such as **champion** and **challenger**, or a more specific
    alias referring to versions A and B in an A/B test. See aliasing in *Figure 2**.6*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建并存储为特征表的特征之后，我们创建训练集并专注于模型训练。我们将讨论如何利用 Databricks 促进模型生命周期的建模（[*第 6 章*](B16865_06.xhtml#_idTextAnchor297)）。在第
    [*第 7 章*](B16865_07.xhtml#_idTextAnchor325) 中，我们将讨论 Unity 目录注册表及其如何用于跟踪关联实验的大量信息，包括模型血缘等细节。你可以在每个阶段注册多个版本的模型，并可以为这些不同版本提供别名，例如
    **冠军** 和 **挑战者**，或者更具体的别名，用于 A/B 测试中的版本 A 和 B。参见 *图 2**.6* 中的别名示例。
- en: '![Figure 2.6 – A user can alias a model with specific names for A/B testing
    or multi-armed bandits](img/B16865_02_06.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 用户可以为 A/B 测试或多臂老虎机游戏为模型指定特定名称进行别名设置](img/B16865_02_06.jpg)'
- en: Figure 2.6 – A user can alias a model with specific names for A/B testing or
    multi-armed bandits
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 用户可以为 A/B 测试或多臂老虎机游戏为模型指定特定名称进行别名设置
- en: In [*Chapter 7*](B16865_07.xhtml#_idTextAnchor325), we demonstrate how to trigger
    a testing script to test every model before having a human review it. Testing
    models is an efficient practice to reduce the time to production when used consistently
    and with intention. We suggest defining the criteria for successfully transitioning
    models/code through isolation environments (from development to stage to production).
    Clearly defined environments are one of the practices that enable you to create
    clear and consistent model quality expectations across all models. Be sure to
    consult *The Big Book of MLOps* on best practices for isolation and model promotion.
    No matter where your environment is, it is beneficial to incorporate logging into
    model experimentation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 7 章*](B16865_07.xhtml#_idTextAnchor325)中，我们演示了如何触发测试脚本以在人工审查之前测试每个模型。当持续且有意地使用时，测试模型是一种提高将模型/代码过渡到生产环境时间的有效实践。我们建议定义成功通过隔离环境（从开发到预生产再到生产）的模型/代码的准则。明确定义的环境是使您能够跨所有模型创建清晰和一致模型质量预期的实践之一。务必查阅*《MLOps
    大全》*以了解隔离和模型提升的最佳实践。无论您的环境在哪里，将日志记录纳入模型实验都是有益的。
- en: 'We discuss logging in the ML context rather than the software development sense.
    Logging for ML is focused on reproducible research and is also known as experiment
    tracking. It is common practice to track the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在机器学习上下文中讨论日志记录，而不是软件开发意义上的日志记录。机器学习中的日志记录侧重于可重复研究，也称为实验跟踪。跟踪以下内容是常见做法：
- en: The input data used to train a model
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型所使用的输入数据
- en: The parameters used to train a model
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型所使用的参数
- en: The accuracy and speed performance of a model during training and inference
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在训练和推理过程中的准确性和速度性能
- en: The errors that occur during training and inference
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和推理过程中发生的错误
- en: The runtime environment of a model
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的运行环境
- en: When using MLflow, you have access to a powerful feature called automatic logging,
    or autologging. Autologging is excellent because it makes it easy to track the
    parameters, metrics, and artifacts of your machine learning experiments without
    explicit instructions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 MLflow 时，您可以使用一个名为自动记录或自动记录的强大功能。自动记录非常出色，因为它使得跟踪机器学习实验的参数、指标和工件变得容易，无需显式指令。
- en: Note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Auto logging only tracks flavors supported by MLflow. Custom *pyfunc* models
    are not supported. For more information, check *Further reading*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 自动记录仅跟踪 MLflow 支持的风味。不支持自定义 *pyfunc* 模型。更多信息，请参阅*进一步阅读*。
- en: MLflow auto logging logs parameter values and models for each run in a single
    experiment. Every time you train and evaluate your model, MLflow logs your standard
    metrics and parameters. If you have custom metrics to track with your models,
    you can also easily add them. We demonstrate tracking custom metrics in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297), when we log parameters for the streaming
    transactions model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 自动记录在单个实验中记录每个运行的参数值和模型。每次您训练和评估模型时，MLflow 都会记录您的标准指标和参数。如果您有与模型一起跟踪的自定义指标，也可以轻松添加。我们在[*第
    6 章*](B16865_06.xhtml#_idTextAnchor297)中演示了跟踪自定义指标，当时我们记录了流式事务模型的参数。
- en: An admin can enable auto logging for all notebooks attached to interactive clusters
    at the workspace level. At the cluster level, you can add `spark.databricks.mlflow.autologging.enabled=true`
    to the advanced section of your cluster configuration to turn on auto logging
    with a cluster scope. It is less common but possible to enable auto logging within
    a notebook scope by adding `mlflow.autolog()` to a Python cell in a notebook.
    Be sure to check the list of modeling flavors supported by autolog.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员可以在工作空间级别为所有附加到交互式集群的笔记本启用自动记录。在集群级别，您可以将 `spark.databricks.mlflow.autologging.enabled=true`
    添加到集群配置的高级部分以在集群范围内开启自动记录。在笔记本范围内启用自动记录虽然不常见但可行，通过在笔记本中的一个 Python 单元中添加 `mlflow.autolog()`
    实现。务必检查自动记录支持的建模风味列表。
- en: By default, MLflow saves the tracked items in the managed folder in DBFS (which
    will be in UC in the future). You can also set `artifact_location` to point to
    a volume path, which is what we do in the example projects. You also have the
    option to set the location to another cloud storage location, although doing so
    eliminates the ability to see your experiments in the MLflow UI.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，MLflow 将跟踪项保存在 DBFS 中的托管文件夹中（未来将在 UC 中）。您也可以设置 `artifact_location` 以指向卷路径，这是我们示例项目中所做的。您还可以选择将位置设置为另一个云存储位置，尽管这样做将消除在
    MLflow UI 中查看实验的能力。
- en: The MLflow UI makes it incredibly easy to compare each trail; see *Figures 2.7*
    and *2.8*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: The MLflow UI makes it incredibly easy to compare each trail; see *Figures 2.7*
    and *2.8*.
- en: '![Figure 2.7 – Comparison of experiment runs using the MLflow UI](img/B16865_02_07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7 – 使用MLflow UI比较实验运行](img/B16865_02_07.jpg)'
- en: Figure 2.7 – Comparison of experiment runs using the MLflow UI
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 使用MLflow UI比较实验运行
- en: There are numerous options for visualizing the results of experiments tracked
    using MLflow.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: There are numerous options for visualizing the results of experiments tracked
    using MLflow.
- en: '![Figure 2.8 – Graphically comparing parameters and model performance in the
    MLflow UI](img/B16865_02_08.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8 – 在MLflow UI中图形化比较参数和模型性能](img/B16865_02_08.jpg)'
- en: Figure 2.8 – Graphically comparing parameters and model performance in the MLflow
    UI
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – 在MLflow UI中图形化比较参数和模型性能
- en: We’ve examined how to compare parameters and model performance in your experiment
    using the MLflow UI. Next, we’ll look at how to use model monitoring (Lakehouse
    monitoring) to keep track of your model’s performance over time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何使用MLflow UI比较实验中的参数和模型性能。接下来，我们将看看如何使用模型监控（Lakehouse监控）来跟踪模型随时间的变化性能。
- en: Monitoring data and models
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控数据和模型
- en: When we think about model monitoring and how to implement it, it becomes less
    about the actual model itself and more about the model’s input and output. For
    this reason, Databricks Lakehouse Monitoring focuses on monitoring a model’s input
    and output, which is simply data. The computation of table metrics occurs in the
    background using serverless compute, or as we like to call it, managed compute.
    Fully managed compute abstracts away the complexities and optimization so users
    focus on which tables to monitor, known as primary tables, rather than how. Lakehouse
    Monitoring is currently in public preview, meaning not all information is ready
    for release. For the latest on this feature, check out the Lakehouse Monitoring
    product page. We demonstrate how to use Lakehouse Monitoring in *Chapters 4* *and
    7*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们思考模型监控及其实现方式时，它就不再仅仅是关于模型本身，而是更多地关注模型的输入和输出。因此，Databricks Lakehouse监控专注于监控模型的输入和输出，这仅仅是数据。表指标的计算是在后台使用无服务器计算完成的，或者正如我们喜欢称之为的，托管计算。完全托管的计算抽象掉了复杂性和优化，因此用户只需关注要监控哪些表，即主表，而不是如何操作。Lakehouse监控目前处于公开预览阶段，这意味着并非所有信息都准备好发布。有关此功能的最新信息，请查看Lakehouse监控产品页面。我们在第4章和第7章中演示了如何使用Lakehouse监控。
- en: We’ve touched on a wide variety of topics so far, from the earliest design decisions
    when setting up your Databricks Data Intelligence Platform to the key topics we’ll
    cover throughout the rest of this book. Now let’s dive into the example projects.
    Get ready to follow along in your own Databricks workspace as you work through
    setting up your workspace.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涉及了许多主题，从设置Databricks数据智能平台时的早期设计决策，到本书余下部分我们将涉及的关键主题。现在让我们深入到示例项目中。准备好在您自己的Databricks工作区中跟随操作，您将在设置工作区时进行操作。
- en: Applying our learning
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: This chapter’s *Applying our learning* section focuses on getting your Databricks
    workspace set up and ready for each project we’ll be working through. We’ll also
    go over getting set up in Kaggle so that you can download the datasets we will
    use throughout the rest of this book. Let’s get started!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的“应用我们的学习”部分专注于设置您的Databricks工作区，使其为我们将要工作的每个项目做好准备。我们还将介绍如何在Kaggle上设置，以便您可以下载本书余下部分我们将使用的数据集。让我们开始吧！
- en: Technical requirements
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before we begin setting up a workspace, please review the technical requirements
    needed to complete the hands-on work in this chapter:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始设置工作区之前，请查看完成本章动手实践所需的技术要求：
- en: We utilize a Python package, `opendatasets`, to download the data we need from
    the Kaggle API easily.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Python包，`opendatasets`，从Kaggle API轻松下载所需数据。
- en: We use the Databricks Labs Python library, `dbldatagen`, to generate synthetic
    data.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用Databricks Labs Python库，`dbldatagen`，来生成合成数据。
- en: To use the Kaggle API, you must download your credential file, `kaggle.json`.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用Kaggle API，您必须下载您的凭证文件，`kaggle.json`。
- en: A GitHub account is beneficial for connecting Databricks and the code repository
    for the book ([https://github.com/PacktPublishing/Databricks-ML-In-Action](https://github.com/PacktPublishing/Databricks-ML-In-Action)).
    In addition to a GitHub account, it is ideal to fork the book repository into
    your GitHub account. You will see that each chapter has a folder, and each project
    has a folder under the chapters. We will refer to the notebooks by name throughout
    the project work.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub 账户对于连接 Databricks 和本书的代码仓库（[https://github.com/PacktPublishing/Databricks-ML-In-Action](https://github.com/PacktPublishing/Databricks-ML-In-Action)）很有帮助。除了
    GitHub 账户外，将本书仓库分叉到您的 GitHub 账户也是理想的。您会看到每个章节都有一个文件夹，每个项目都在章节下有文件夹。在整个项目工作中，我们将通过名称引用笔记本。
- en: 'We will use the Databricks Secrets API to save both Kaggle and OpenAI credentials.
    The Secrets API requires the Databricks CLI. We will walk through this setup.
    However, you will need to create a **personal access token** (**PAT**) on your
    own for the configuration step: [https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 Databricks Secrets API 来保存 Kaggle 和 OpenAI 凭证。Secrets API 需要使用 Databricks
    CLI。我们将逐步介绍此设置。但是，您需要在配置步骤中自己创建一个**个人访问令牌**（**PAT**）：[https://docs.databricks.com/en/dev-tools/auth/pat.html](https://docs.databricks.com/en/dev-tools/auth/pat.html)
- en: 'The compute clusters we use are as follows (they vary slightly depending on
    your data cloud):'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用的计算集群如下（它们根据您的数据云略有不同）：
- en: Single-node CPU configuration
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单节点 CPU 配置
- en: '![Figure 2.9 – Single-node CPU cluster configuration, DBR ML 14.2](img/B16865_02_09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – 单节点 CPU 集群配置，DBR ML 14.2](img/B16865_02_09.jpg)'
- en: Figure 2.9 – Single-node CPU cluster configuration, DBR ML 14.2
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 单节点 CPU 集群配置，DBR ML 14.2
- en: This will work for most workloads in this book.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将适用于本书中的大多数工作负载。
- en: Setting up your workspace
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置您的开发环境
- en: As defined previously, the workspace discussed in this section refers to the
    deployment instance. Here, we will discuss workspace setup recommendations, project
    setup files, and download instructions for each dataset used throughout this book.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，本节中讨论的工作空间指的是部署实例。在这里，我们将讨论工作空间设置建议、项目设置文件以及本书中使用的每个数据集的下载说明。
- en: There is comprehensive documentation on deploying your workspace for the first
    time. If you do not already have a Databricks account and deployed workspace,
    then you have a couple of places to start from. One method is going into your
    cloud account and activating Databricks through the marketplaces. Another method
    is to begin on the Databricks website. For more advanced users, consider using
    Terraform. Given the amount of documentation and the ever-changing world of technology,
    we leave the exercise of activating a workspace up to you.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首次部署工作空间有详细的文档说明。如果您还没有 Databricks 账户和已部署的工作空间，那么您有几个起点。一种方法是进入您的云账户并通过市场激活
    Databricks。另一种方法是直接从 Databricks 网站开始。对于更高级的用户，考虑使用 Terraform。鉴于大量的文档和不断变化的技术世界，我们将激活工作空间的练习留给您自己。
- en: Once we have a workspace deployed, we can begin setting it up. Generally, we
    start with user groups and governance. The experience of setting up Unity Catalog
    is frequently updated for simplicity. Therefore, we recommend you watch the latest
    video documentation on how to do so (see *Further reading*). The process is the
    same, regardless of the data persona using the platform. Please be sure to complete
    metastore and governance setup before going forward.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们部署了工作空间，我们就可以开始设置它。通常，我们从用户组和治理开始。设置 Unity Catalog 的体验经常更新以简化。因此，我们建议您观看最新的视频文档了解如何进行（见*进一步阅读*）。无论使用平台的数据角色如何，过程都是相同的。请确保在继续之前完成元存储和治理设置。
- en: Kaggle setup
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle 设置
- en: 'You will need a Kaggle account to download the Kaggle datasets we’ll be using,
    which require an API token for authentication. There is an official Kaggle API,
    but, there are numerous other ways to connect to Kaggle to download data and interact
    with the Kaggle site as well. All methods require downloading your API credentials
    file, `kaggle.json`, from the Kaggle website. Before downloading data, you need
    to make your credentials accessible. Here are three methods for accomplishing
    this:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个 Kaggle 账户来下载我们将要使用的 Kaggle 数据集，这些数据集需要 API 令牌进行身份验证。存在官方的 Kaggle API，但还有许多其他方式可以连接到
    Kaggle 下载数据以及与 Kaggle 网站交互。所有方法都需要从 Kaggle 网站下载您的 API 凭证文件，`kaggle.json`。在下载数据之前，您需要使您的凭证可访问。以下有三种实现此目的的方法：
- en: '`kaggle.json` file to your project folder. If you choose to do this, be aware
    that your credentials are viewable to others, even if only admins. Also, add `kaggle.json`
    to your `.gitignore` file to prevent committing your credentials to the repository
    and ensure you do not commit your credentials to a Git repository.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `kaggle.json` 文件添加到您的项目文件夹。如果您选择这样做，请注意，您的凭据对其他人可见，即使只有管理员。此外，将 `kaggle.json`
    添加到您的 `.gitignore` 文件中，以防止将凭据提交到仓库，并确保您不会将凭据提交到 Git 仓库。
- en: '`.gitignore` file to prevent committing your credentials to the repository.
    However, the ability to remove other users’ access may not be in your control,
    depending on your role. Furthermore, in general, admins can see all files.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.gitignore` 文件以防止将凭据提交到仓库。然而，根据您的角色，您可能无法控制删除其他用户的访问权限。此外，通常管理员可以查看所有文件。'
- en: '![Figure 2.10 – Passing user credentials to the notebook](img/B16865_02_10.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![**图 2.10 – 将用户凭据传递给笔记本**](img/B16865_02_10.jpg)'
- en: Figure 2.10 – Passing user credentials to the notebook
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.10 – 将用户凭据传递给笔记本**'
- en: '**Option 3**: Use Databricks secrets to store and retrieve your username and
    token, for optimal security, as shown in *Figure 2**.11*. This is the method we
    use for downloading images.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选项 3**：使用 Databricks 机密存储和检索您的用户名和令牌，以实现最佳安全性，如**图 2.11**所示。这是我们用于下载图片的方法。'
- en: '![Figure 2.11 – Using secrets to store and retrieve your username and token](img/B16865_02_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![**图 2.11 – 使用机密存储和检索您的用户名和令牌**](img/B16865_02_11.jpg)'
- en: Figure 2.11 – Using secrets to store and retrieve your username and token
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.11 – 使用机密存储和检索您的用户名和令牌**'
- en: This code is in global_setup.py, but you could also put it in the notebook itself
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码位于 `global_setup.py` 中，但您也可以将其放在笔记本本身中
- en: '`o``pendatasets` library to paste your credentials in at the time of download.
    This is a safe way to download data, so we demonstrate this with the Favorita
    Sales data.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `o``pendatasets` 库在下载时粘贴您的凭据。这是一种安全的数据下载方式，因此我们使用 Favorita 销售数据来演示这一点。
- en: We’ll will walk through `global-setup.py` later. The last section of the file
    is setting your Kaggle credentials. We recommend setting up a secret scope with
    your credentials. We will show you how to set this up once you have a cluster
    running, so there is no need to jump around. Simply download your Kaggle credentials
    for now.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后介绍 `global-setup.py`。文件的最后一部分是设置您的 Kaggle 凭据。我们建议设置一个包含凭据的秘密范围。一旦您有一个集群运行，我们将向您展示如何设置，因此无需四处跳跃。现在，只需下载您的
    Kaggle 凭据即可。
- en: Setting up our GitHub repository
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置我们的 GitHub 仓库
- en: The first thing is to pull the code you will work with throughout the book from
    the GitHub repository.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事是从 GitHub 仓库拉取您将在整本书中使用的代码。
- en: '![Figure 2.12 – Setting up a Git repository: Workspace > Repos > Home folder
    > Add > Repo'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![**图 2.12 – 设置 Git 仓库：工作区 > 仓库 > 主文件夹 > 添加 > 仓库**](img/B16865_02_12.jpg)'
- en: '](img/B16865_02_12.jpg)![Figure 2.12 – Setting up a Git repository: Workspace
    > Repos > Home folder > Add > Repo](img/B16865_02_13.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16865_02_12.jpg)![**图 2.12 – 设置 Git 仓库：工作区 > 仓库 > 主文件夹 > 添加 > 仓库**](img/B16865_02_13.jpg)'
- en: 'Figure 2.12 – Setting up a Git repository: Workspace > Repos > Home folder
    > Add > Repo'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.12 – 设置 Git 仓库：工作区 > 仓库 > 主文件夹 > 添加 > 仓库**'
- en: Navigate to your fork of the Book’s GitHub repository, as mentioned in the *Technical
    requirements* section under the *Applying our learning* section. You can copy
    and paste the HTTPS link into the **Add Repo** screen’s URL section, shown in
    *Figure 2**.12*. Next, you will link your GitHub account. If you are unsure how
    to do this, follow the documentation linked in *Further reading* titled *About
    personal access tokens* and *Set up Databricks Repos*. Once your repository is
    ready, you can create a cluster if no one has done so yet.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到在“应用我们的学习”部分下的“技术要求”部分中提到的书的 GitHub 仓库分支。您可以将 HTTPS 链接复制并粘贴到**添加仓库**屏幕的 URL
    部分，如**图 2.12**所示。接下来，您将链接您的 GitHub 账户。如果您不确定如何操作，请遵循**进一步阅读**部分中链接的文档，标题为**关于个人访问令牌**和**设置
    Databricks 仓库**。一旦您的仓库准备就绪，如果还没有人这样做，您就可以创建一个集群。
- en: Creating compute
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建计算
- en: 'We provide the cluster configurations we use for this project in the *Technical
    requirements* section. You can use the same configuration if you like. There are
    several options to choose from when creating a new cluster configuration. It might
    seem complicated for new users, but stay calm when trying to choose the right
    one. We highly recommend the *Best Practices for Cluster Configuration* linked
    in *Further reading* for guidance, especially if you are responsible for setting
    up compute for one or more teams. Let’s talk about some of the compute options
    as they relate to ML and this book:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *技术要求* 部分提供了本项目使用的集群配置。如果您愿意，可以使用相同的配置。创建新的集群配置时，有多种选项可供选择。对于新用户来说，这可能会显得复杂，但在尝试选择正确的配置时请保持冷静。我们强烈推荐在
    *进一步阅读* 链接中的 *集群配置最佳实践* 作为指导，特别是如果您负责为一或多个团队设置计算资源。让我们讨论一些与机器学习和本书相关的计算选项：
- en: '**Multi-node versus single-node**: Multi-node is excellent for distributed
    projects (think Spark). Single-node is suitable for projects or workloads that
    are performed on the driver (think scikit-learn or pandas).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多节点与单节点**：多节点非常适合分布式项目（例如 Spark）。单节点适用于在驱动程序上执行的项目或工作负载（例如 scikit-learn 或
    pandas）。'
- en: '**Access mode**: Some cluster configurations support Unity Catalog, and some
    don’t. Choose a cluster that supports UC for the projects in this book.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问模式**：某些集群配置支持 Unity Catalog，而有些则不支持。为本书中的项目选择支持 UC 的集群。'
- en: '`pyenv`). You will install libraries both at the cluster level and in a couple
    of project notebooks. You can see the **Libraries** tab in *Figure 2**.13*. Simply
    click **Install new** to install a new library.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (`pyenv`)。您将在集群级别和几个项目笔记本中安装库。您可以在 *图 2.13* 中的 **库** 选项卡中看到。只需点击 **安装新库** 即可安装新库。
- en: '![Figure 2.13 – Adding libraries to the Machine Learning in Action (MLIA) cluster
    configuration](img/B16865_02_14.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.13 – 向《机器学习实战》（MLIA）集群配置中添加库](img/B16865_02_14.jpg)'
- en: Figure 2.13 – Adding libraries to the Machine Learning in Action (MLIA) cluster
    configuration
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 – 向《机器学习实战》（MLIA）集群配置中添加库
- en: This is the ideal time to install the libraries you will need. Install via `PyPI
    opendatasets`, `dbldatagen`, `databricks-feature-engineering`, and `mlflow-skinny[databricks]>=2.5.0`.
    These libraries are used across multiple notebooks throughout the book.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是安装所需库的理想时机。通过 `PyPI opendatasets`、`dbldatagen`、`databricks-feature-engineering`
    和 `mlflow-skinny[databricks]>=2.5.0` 进行安装。这些库在本书的多个笔记本中都会用到。
- en: '**Photon acceleration**: Photon is an acceleration engine that speeds up ETL
    and SQL workloads. Photon currently is not advantageous for standard ML modeling.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Photon 加速**：Photon 是一个加速引擎，可以加快 ETL 和 SQL 工作负载。目前，Photon 对于标准机器学习建模并没有优势。'
- en: '**VM types**: There are many VMs to pick from. You can choose VMs by family
    from the drop-down list. You can start with a VM in the *General Purpose* group
    if you need more clarification or are just starting.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟机类型**：有众多虚拟机可供选择。您可以从下拉列表中选择按家族分类的虚拟机。如果您需要更多澄清或刚开始，可以从 *通用目的* 组的虚拟机开始。'
- en: '**Min and max workers**: The rule of thumb is to start with a few max workers
    and increase the number of workers as your workload increases. Keep in mind that
    your cluster will autoscale for you. However, we still recommend starting smaller
    and growing out for only the more compute-heavy examples, such as certain notebooks
    in the Multilabel Image Classification deep learning project.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小和最大工作节点数**：一般建议从少量最大工作节点开始，随着工作负载的增加逐渐增加工作节点数。请记住，您的集群会自动为您进行扩展。然而，我们仍然建议对于仅计算密集型示例（例如多标签图像分类深度学习项目中的某些笔记本），只从小规模开始，并逐步扩展。'
- en: You now have your development environment set up. You are ready to lock down
    your credentials for safe use in your code.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已设置好开发环境。您可以为代码中的安全使用锁定凭证。
- en: Setting up the Databricks CLI and secrets
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Databricks CLI 和密钥
- en: The Databricks CLI is the command line interface for Databricks. We recommend
    using the web terminal from your cluster to install Databricks CLI as well as
    create Databricks secrets. As we mentioned earlier in the chapter, there are other
    options to get access to Kaggle datasets, but we walk you through the steps to
    set up secrets here. Please see the documentation in *Further reading* for more
    details on the CLI, installation, usage, and secrets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks CLI是Databricks的命令行界面。我们建议使用你的集群的Web终端来安装Databricks CLI以及创建Databricks秘密。正如我们在本章前面提到的，有其他选项可以访问Kaggle数据集，但我们在本节中引导你设置秘密。请参阅*进一步阅读*中的文档以获取有关CLI、安装、使用和秘密的更多详细信息。
- en: Go to the **Apps** tab of the compute cluster you set up in the last section.
    You can refer to *Figure 2**.13* to see the location of the **Apps** tab. Apps
    are only available while the cluster is, so it may be greyed out initially. You
    will have to start your cluster to proceed.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往你在上一节中设置的计算集群的**应用**标签页。你可以参考*图2.13*来查看**应用**标签的位置。应用仅在集群运行时可用，因此最初可能为灰色。你必须启动你的集群才能继续。
- en: Select the web terminal.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Web终端。
- en: Install the latest version of the CLI. `curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh
    |` `sudo sh`
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装CLI的最新版本。`curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh
    | sudo sh`
- en: Check your Databricks version to be sure it’s greater than `0.2`. We had to
    point to the updated version in the location installed by curl. `/usr/local/bin/databricks-v`.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查你的Databricks版本，确保它大于`0.2`。我们必须指向curl安装的位置的更新版本。`/usr/local/bin/databricks-v`
- en: 'Next, you need to configure the connection. You need your PAT for this:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你需要配置连接。你需要你的PAT来完成此操作：
- en: '[PRE0]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a secret scope for storing credentials related to this book:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为存储与本书相关的凭据创建一个秘密作用域：
- en: '[PRE1]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a secret for storing your Kaggle username:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个秘密来存储你的Kaggle用户名：
- en: '[PRE2]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a secret for storing your Kaggle API key:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个秘密来存储你的Kaggle API密钥：
- en: '[PRE3]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Last, list your secrets to make sure everything works as expected:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，列出你的秘密以确保一切按预期工作：
- en: '[PRE4]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In [*Chapter 8*](B16865_08.xhtml#_idTextAnchor384), we’ll create another scope
    to hold an OpenAI API key, but for now we just need the Kaggle credentials. Now
    that we have our secrets set up, let's get our codebase ready!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B16865_08.xhtml#_idTextAnchor384)中，我们将创建另一个作用域来存储OpenAI API密钥，但到目前为止我们只需要Kaggle凭据。现在我们已经设置了秘密，让我们准备好我们的代码库！
- en: Setting up your code base
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置你的代码库
- en: We use a setup file to help keep variables consistent across multiple project
    notebooks. You will run the setup file each time you run the project notebooks
    using a magic command, `%run`. This command brings everything into the memory
    of your notebook session. The `global-setup.py` file has numerous components to
    it. Let’s walk through each section. Feel free to edit the file to fit your needs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用设置文件来帮助保持多个项目笔记本中变量的一致性。每次你使用魔法命令`%run`运行项目笔记本时，你都会运行设置文件。这个命令将所有内容带入你的笔记本会话的内存中。`global-setup.py`文件有许多组件。让我们逐一介绍每个部分。你可以自由地编辑文件以适应你的需求。
- en: Note
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'It’s possible you’ll receive an error message: `py4j.security.Py4JSecurityException:
    Method public scala.collection.immutable.Map com.databricks.backend.common.rpc.CommandContext.tags()
    is not whitelisted on class` `class com.databricks.backend.common.rpc.CommandContext`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '你可能会收到一个错误消息：`py4j.security.Py4JSecurityException: Method public scala.collection.immutable.Map
    com.databricks.backend.common.rpc.CommandContext.tags() is not whitelisted on
    class` `class com.databricks.backend.common.rpc.CommandContext`'
- en: This is because you are on a shared compute cluster. You can simply hardcode
    `current_user` to your username.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为你在一个共享的计算集群上。你可以简单地将`current_user`硬编码到你的用户名中。
- en: Passing variables via widgets
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过小部件传递变量
- en: Widgets pass variables to notebooks similarly to how command-line arguments
    pass variables to Python scripts. The code block in *Figure 2**.14* creates the
    widgets needed to pass variables from the `Run` command to the `global-setup.py`
    file using Databricks Utilities or `dbutils`. You can read more about the `dbutils`
    capabilities in the Databricks Utilities documentation in *Further reading*. These
    widgets create, pass, and access parameters. The arguments are the variable name,
    default value, and verbose name in respective order.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 小部件将变量传递到笔记本中，类似于命令行参数将变量传递到Python脚本中。*图2.14*中的代码块创建了所需的小部件，用于使用Databricks实用工具或`dbutils`将`Run`命令中的变量传递到`global-setup.py`文件。你可以在*进一步阅读*中的Databricks实用工具文档中了解更多关于`dbutils`的功能。这些小部件创建、传递和访问参数。参数的顺序是变量名、默认值和详细名称。
- en: '![](img/B16865_02_15.jpg)![](img/B16865_02_16.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16865_02_15.jpg)![](img/B16865_02_16.jpg)'
- en: Figure 2.14 – Creating widgets for accepting notebook-specific variables
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 创建接受笔记本特定变量的小部件
- en: You can pass each variable while running the file by adding a single line cell
    with appropriate parameters at the top of the notebook, as shown in *Figure 2**.1**5*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在笔记本顶部添加带有适当参数的单行单元格来在运行文件时传递每个变量，如图*图2**.1**5*所示。
- en: '![Figure 2.15 – Running project-specific variables in our global setup file](img/B16865_02_17.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图2.15 – 在我们的全局设置文件中运行项目特定变量](img/B16865_02_17.jpg)'
- en: Figure 2.15 – Running project-specific variables in our global setup file
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – 在我们的全局设置文件中运行项目特定变量
- en: Running `global-setup.py` saves all the variables defined in the script in memory
    for easy reference.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`global-setup.py`将脚本中定义的所有变量保存在内存中，以便于引用。
- en: Checking for compatibility
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查兼容性
- en: Next, in `global-setup.py`, we run checks for compatibility between the code
    base and the cluster attached to the notebook.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在`global-setup.py`中，我们运行代码库和笔记本附加的集群之间的兼容性检查。
- en: 'The compatibility code block checks the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容性代码块检查以下内容：
- en: A project name was submitted as a variable.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个项目名称被提交为一个变量。
- en: The cluster is configured with an ML runtime and meets the minimum version.
    To be sure all features in the code are available in the runtime used, we set
    a minimum.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群配置了ML运行时并满足最低版本要求。为了确保代码中的所有功能都在使用的运行时中可用，我们设置了最低版本。
- en: Once all checks pass, we assign a user and paths.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有检查都通过，我们分配用户和路径。
- en: Setting a default catalog and project-specific database
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置默认目录和项目特定数据库
- en: This book provides code that uses the Unity Catalog catalog. Your default catalog
    is set based on your environment. If you do not set the environment or you set
    it to `dev`, then the catalog is named `ml_in_action`. When the environment is
    `prod`, the catalog is `ml_in_prod`. The default name for the database is always
    the project name. However, you can provide a different name if you desire by entering
    a project variable for the database name.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本书提供了使用Unity Catalog目录的代码。您的默认目录基于您的环境设置。如果您没有设置环境或将其设置为`dev`，则目录名为`ml_in_action`。当环境为`prod`时，目录为`ml_in_prod`。数据库的默认名称始终是项目名称。但是，如果您希望提供不同的名称，可以通过输入数据库名称的项目变量来实现。
- en: '![Figure 2.16 – Using the defined variables to set the default with retries](img/B16865_02_18.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16 – 使用定义的变量通过重试设置默认值](img/B16865_02_18.jpg)'
- en: Figure 2.16 – Using the defined variables to set the default with retries
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 使用定义的变量通过重试设置默认值
- en: We want to be sure that the catalog and database are set to the notebooks’ defaults.
    Occasionally, with parallel execution, this command can fail during initialization;
    therefore, we add a few retries to work around this issue, as shown in *Figure
    2**.16*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要确保目录和数据库设置为笔记本的默认值。偶尔，在并行执行时，此命令在初始化过程中可能会失败；因此，我们添加了一些重试来解决这个问题，如*图2**.16*所示。
- en: Granting permissions
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 授予权限
- en: Now that we’ve set our catalog and database defaults, we can grant permissions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了目录和数据库的默认值，我们可以授予权限。
- en: '![](img/B16865_02_19.jpg)![](img/B16865_02_19_01.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16865_02_19.jpg)![](img/B16865_02_19_01.jpg)'
- en: Figure 2.17 – Granting permissions to the catalog and database
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – 授予目录和数据库权限
- en: We grant the group `account users` permission. If you do not want to make your
    assets available to others, remove this or comment it out.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们授予`account users`组权限。如果您不希望将您的资产提供给其他人，请删除此权限或将其注释掉。
- en: Important note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Be sure to use tick marks around your group name or email address when granting
    permission. If you use single quotes instead, you will get an error message.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在授予权限时，务必在您的组名或电子邮件地址周围使用勾号。如果您使用单引号，您将收到错误消息。
- en: The catalog and database are ready for tables. However, not all data we use
    in machine learning goes into a table. For other data, files, and objects, we
    have volumes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 目录和数据库已准备好用于表格。然而，我们用于机器学习的并非所有数据都进入表格。对于其他数据、文件和对象，我们有卷。
- en: Setting up volumes
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置卷
- en: Volumes are views of cloud object storage. We create project-specific volumes.
    Use them for path-based access to structured or unstructured data. Volumes sit
    under a database in a catalog and are used to manage and provide access to data
    files. You can govern access to volumes using `GRANT` statements. Volumes provide
    scalable file-based storage without sacrificing governance. Often, we use unstructured,
    semi-structured, or non-tabular data in machine learning. Images are a good example
    of unstructured, non-tabular data that we will use for the Multilabel Image Classification
    project. To work with these images, the Multilabel Image Classification project
    uses volumes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 卷是云对象存储的视图。我们创建特定于项目的卷。使用它们进行基于路径的访问，以访问结构化或非结构化数据。卷位于目录下的数据库中，用于管理和提供对数据文件的访问。你可以使用
    `GRANT` 语句来管理对卷的访问。卷提供可扩展的基于文件的存储，而不牺牲治理。我们通常在机器学习中使用非结构化、半结构化或非表格数据。图像是我们将在多标签图像分类项目中使用的非结构化、非表格数据的良好示例。为了处理这些图像，多标签图像分类项目使用卷。
- en: '![](img/B16865_02_20.jpg)![Figure 2.18 – EndpointApiClient class](img/B16865_02_20_01.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16865_02_20.jpg)![图 2.18 – EndpointApiClient 类](img/B16865_02_20_01.jpg)'
- en: Figure 2.18 – EndpointApiClient class
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18 – EndpointApiClient 类
- en: Starting the projects
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始项目
- en: 'We have planned our platform and set up our workspace environment. Next, let’s
    work through each project. In GitHub, you will see that each chapter has a folder
    containing folders corresponding to each project. When we refer to the notebooks
    by name, we assume you are in the appropriate chapter and project folder. For
    example, this chapter has the first notebook:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经规划了我们的平台并设置了我们的工作区环境。接下来，让我们逐一处理每个项目。在 GitHub 中，你会看到每个章节都有一个包含对应每个项目文件夹的文件夹。当我们按名称引用笔记本时，我们假设你处于适当的章节和项目文件夹中。例如，本章的第一个笔记本是：
- en: '[PRE5]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We refer to the notebook by only the filename itself, `CH2-01-Downloading_Sales_Forecast_Data`.
    Let’s jump into the first project.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只通过文件名本身来引用笔记本，`CH2-01-Downloading_Sales_Forecast_Data`。让我们开始第一个项目。
- en: 'Project: Favorita store sales – time series forecasting'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：Favorita 店铺销售 – 时间序列预测
- en: 'Recall from [*Chapter 1*](B16865_01.xhtml#_idTextAnchor016) that we use a Kaggle-provided
    dataset to forecast sales. In this chapter, we download our data from the Kaggle
    website. To follow along in your workspace, please open the following notebook:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 [*第 1 章*](B16865_01.xhtml#_idTextAnchor016)，我们使用 Kaggle 提供的数据集来预测销售。在本章中，我们从
    Kaggle 网站下载我们的数据。为了在你的工作区中跟随，请打开以下笔记本：
- en: '`CH2-01-Downloading_Sales_Forecast_Data`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH2-01-Downloading_Sales_Forecast_Data`'
- en: In the notebook, and as well as the code here in *Figures 2.19 and 2.20*, we
    set our path and download our data from Kaggle.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，以及在此处 *图 2.19 和 2.20* 中的代码中，我们设置了我们的路径并从 Kaggle 下载了我们的数据。
- en: First, we designate `raw_data_path` to store the files.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定 `raw_data_path` 来存储文件。
- en: '![Figure 2.19 – Setting the path for our volume](img/B16865_02_21.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.19 – 设置我们的卷路径](img/B16865_02_21.jpg)'
- en: Figure 2.19 – Setting the path for our volume
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19 – 设置我们的卷路径
- en: In the following code block (*Figure 2**.20*), we use the Python package `opendatasets`,
    a library specifically created to download data from the Kaggle API. You can find
    more information in the *Further* *reading* section.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块（*图 2.20*）中，我们使用 Python 包 `opendatasets`，这是一个专门创建来从 Kaggle API 下载数据的库。你可以在
    *进一步* *阅读* 部分找到更多信息。
- en: '![Figure 2.20 – Downloading Favorita data from opendatasets](img/B16865_02_22.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.20 – 从 opendatasets 下载 Favorita 数据](img/B16865_02_22.jpg)'
- en: Figure 2.20 – Downloading Favorita data from opendatasets
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20 – 从 opendatasets 下载 Favorita 数据
- en: That is all for the *Favorita Store Sales* project in this chapter! Now, we
    can focus on generating data for our `Streaming` `Transactions` project.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章关于 *Favorita 店铺销售* 项目的内容就到这里！现在，我们可以专注于为我们的 `流式` `事务` 项目生成数据。
- en: 'Project: Streaming Transactions'
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：流式事务
- en: Your goal with the Streaming Transactions project is to build a model to classify
    transactions. The dataset consists of JSON-formatted transactions with `Transaction`,
    `timestamp`, `Label`, `Amount`, and `CustomerID`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在流式事务项目中，你的目标是构建一个用于分类事务的模型。数据集由具有 `Transaction`、`timestamp`、`Label`、`Amount`
    和 `CustomerID` 的 JSON 格式的事务组成。
- en: 'In later chapters, you will add a product column to demonstrate schema evolution.
    In this chapter, you’ll create the first version of transaction data used throughout
    the rest of the book. To follow along in your workspace, please open the following
    notebook:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，你将添加一个产品列以展示模式演变。在本章中，你将创建本书其余部分使用的第一个版本的事务数据。为了在你的工作区中跟随，请打开以下笔记本：
- en: '`CH2-01-Generating_Records_Using_DBKS_Labs_Datagen`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH2-01-Generating_Records_Using_DBKS_Labs_Datagen`'
- en: You can run each cell of the notebook as we work through them or run them all
    at once. After the setup commands, we set notebook variables to establish the
    number of rows generated per batch of transactions (`nRows`), the number of positively
    labeled rows per batch (`nPositiveRows`), the path to the volume where you will
    store the JSON dataset (`destination_path`), a temporary path (`temp_path`), and
    the number of seconds between each batch of data you generate (`sleepIntervalSeconds`).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在我们处理它们时运行笔记本中的每个单元格，或者一次性运行它们。在设置命令之后，我们设置笔记本变量以确定每批交易生成的行数（`nRows`）、每批正标签行数（`nPositiveRows`）、您将存储JSON数据集的卷路径（`destination_path`）、临时路径（`temp_path`）以及您生成每批数据之间的秒数（`sleepIntervalSeconds`）。
- en: The following code block accesses the value of the `Reset` widget. Any data
    already written to the volume will be deleted if the widget is set to `True` (its
    default value).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块访问`Reset`小部件的值。如果小部件设置为`True`（其默认值），则已写入卷中的任何数据将被删除。
- en: '![Figure 2.21 – Checking the Reset widget](img/B16865_02_23.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图2.21 – 检查Reset小部件](img/B16865_02_23.jpg)'
- en: Figure 2.21 – Checking the Reset widget
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 – 检查Reset小部件
- en: Next, we set the parameter values used in the data generator to create the transactions.
    We set the minimum and maximum values for each `CustomerID`. We also create a
    dictionary of product types and set `min`, `max`, `mean`, `alpha`, and `beta`
    variables, which you use to generate random transaction amounts according to a
    distribution.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置数据生成器中用于创建交易的参数值。我们为每个`CustomerID`设置最小值和最大值。我们还创建了一个产品类型字典，并设置了`min`、`max`、`mean`、`alpha`和`beta`变量，您可以使用这些变量根据分布生成随机的交易金额。
- en: '![](img/B16865_02_24.jpg)![](img/B16865_02_25.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16865_02_24.jpg)![](img/B16865_02_25.jpg)'
- en: Figure 2.22 – Dictionaries to hold variables for use within the define_specs
    function
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 – 保存用于define_specs函数中使用的变量的字典
- en: With the variables set, we build out the functions to create the transaction
    data, starting with the `define_specs` function. The function accepts as input
    a product type (defined in the dictionary in *Figure 2**.22*), a positive or negative
    label, and a timestamp; it returns a dollar amount for the transaction. *Figure
    2**.23* shows a portion of the code; the rest is in the accompanying notebook.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 变量设置完毕后，我们构建创建交易数据的函数，从`define_specs`函数开始。该函数接受产品类型（在图2.22中的字典中定义）作为输入，一个正标签或负标签，以及一个时间戳；它返回交易的金额。图2.23显示了代码的一部分；其余部分在配套的笔记本中。
- en: '![ Figure 2.23 – Defining the define_specs function to generate transaction
    records](img/B16865_02_26.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图2.23 – 定义define_specs函数以生成交易记录](img/B16865_02_26.jpg)'
- en: Figure 2.23 – Defining the define_specs function to generate transaction records
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 – 定义define_specs函数以生成交易记录
- en: Next, we write a function to generate a single record by calling `define_specs`
    and including the current timestamp.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个函数，通过调用`define_specs`并包含当前时间戳来生成单个记录。
- en: '![Figure 2.24 – Defining a function to generate a single transaction record](img/B16865_02_27.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24 – 定义一个生成单个交易记录的函数](img/B16865_02_27.jpg)'
- en: Figure 2.24 – Defining a function to generate a single transaction record
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24 – 定义一个生成单个交易记录的函数
- en: 'We then build `generateRecordSet` to generate the `recordCount` number of records
    in each batch. Notice that in this notebook, we’re using the `None` product type,
    so the records generated will only have four features: `CustomerID`, `TransactionTimestamp`,
    `Amount`, and `Label` (this will be important in the next chapter!).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建`generateRecordSet`以生成每批中的`recordCount`个记录。请注意，在这个笔记本中，我们使用的是`None`产品类型，因此生成的记录将只有四个特征：`CustomerID`、`TransactionTimestamp`、`Amount`和`Label`（这在下一章中将非常重要！）。
- en: '![ Figure 2.25 – The generateRecordSet function creates a record for each product
    and each label. Each record contains nRows transactions](img/B16865_02_28.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图2.25 – generateRecordSet函数为每个产品和每个标签创建一个记录。每个记录包含nRows笔交易](img/B16865_02_28.jpg)'
- en: Figure 2.25 – The generateRecordSet function creates a record for each product
    and each label. Each record contains nRows transactions
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 – generateRecordSet函数为每个产品和每个标签创建一个记录。每个记录包含nRows笔交易
- en: Finally, we write a function to generate a set of data, convert the data to
    a DataFrame, and write it out as one JSON file to a temporary path. Then, we move
    that file to the final volume destination.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编写一个函数来生成一组数据，将数据转换为DataFrame，并将其作为单个JSON文件写入临时路径。然后，我们将该文件移动到最终卷目标位置。
- en: '![Figure 2.26 – The writeJsonFile function generates a set of records](img/B16865_02_29.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图2.26 – writeJsonFile函数生成一组记录](img/B16865_02_29.jpg)'
- en: Figure 2.26 – The writeJsonFile function generates a set of records
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.26 – writeJsonFile函数生成一组记录
- en: The set contains amounts generated as integers, so we divide by 100 to turn
    the amounts into dollars and type float. The function writes out the JSON file
    to a `temp` directory and then moves the single file to the final directory.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该集合包含作为整数的生成金额，因此我们将它们除以100，将金额转换为美元并输入浮点数。该函数将JSON文件写入`temp`目录，然后将单个文件移动到最终目录。
- en: With everything set up, create the dataset with the code provided. Feel free
    to increase the iterations to build a larger dataset. Then, move on to the next
    project!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好一切后，使用提供的代码创建数据集。您可以自由增加迭代次数以构建更大的数据集。然后，继续下一个项目！
- en: 'Project: Retrieval-Augmented Generation Chatbot'
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：检索增强生成聊天机器人
- en: 'The RAG Chatbot project will ingest PDF documents to build the knowledge base
    for the chatbot. We use a volume to store the PDFs. To follow along in your workspace,
    please open the following notebook:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: RAG聊天机器人项目将摄入PDF文档以构建聊天机器人的知识库。我们使用体积来存储PDF文件。要在您的空间中跟随，请打开以下笔记本：
- en: '`CH2-01-Downloading_PDF_Documents`'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH2-01-Downloading_PDF_Documents`'
- en: Files can be uploaded to a volume directly in the Databricks console via the
    user interface, as shown in *Figure 2**.27*; however, this project uses the code
    provided in the notebook to download and save the data to a volume programmatically.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 文件可以直接在Databricks控制台的用户界面中上传到体积，如图*图2**.27*所示；然而，此项目使用笔记本中提供的代码以编程方式下载并保存数据到体积中。
- en: '![Figure 2.27 – Manually uploading documents into a volume](img/B16865_02_30.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图2.27 – 手动将文档上传到体积中](img/B16865_02_30.jpg)'
- en: Figure 2.27 – Manually uploading documents into a volume
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.27 – 手动将文档上传到体积中
- en: The code for this chapter begins with setup cells and helper functions, and
    in *Figure* *2**.28* we designate `library_folder` where we will save the PDFs
    we download.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码从设置单元格和辅助函数开始，并在*图* *2**.28*中指定了`library_folder`，我们将在此处保存下载的PDF文件。
- en: '![Figure 2.28 – Designating the library folder to hold the files for this project](img/B16865_02_31.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图2.28 – 指定用于存储此项目文件的库文件夹](img/B16865_02_31.jpg)'
- en: Figure 2.28 – Designating the library folder to hold the files for this project
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.28 – 指定用于存储此项目文件的库文件夹
- en: We are using open articles published on the **Arxiv** page that relate to **Generative
    AI** (**GenAI**) and how it can impact human labor markets and economics. We pass
    the URLs to be used as documents for our chatbot and load these files into our
    volume.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用在**Arxiv**页面上发布的与**生成式AI**（**GenAI**）及其如何影响人力市场和经济学相关的开放文章。我们将URL传递给用作聊天机器人文档，并将这些文件加载到我们的体积中。
- en: '![Figure 2.29 – Download PDF files and save them to our volume](img/B16865_02_32.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图2.29 – 下载PDF文件并将它们保存到我们的体积中](img/B16865_02_32.jpg)'
- en: Figure 2.29 – Download PDF files and save them to our volume
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.29 – 下载PDF文件并将它们保存到我们的体积中
- en: 'Now that we have the documents downloaded and, they are ready to be processed
    for our chatbot. With that completed, we can move on to our final project: **Multilabel**
    **Image Classification**.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载了文档，它们准备好被处理以用于我们的聊天机器人。完成此操作后，我们可以继续我们的最终项目：**多标签** **图像分类**。
- en: 'Project: Multilabel Image Classification'
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目：多标签图像分类
- en: 'The MIC project ingests images into Delta tables to fine-tune a pre-trained
    model from the *Restnet* family to improve its accuracy. We will programmatically
    download the images from Kaggle and save the data to a volume. To follow along
    in your workspace, please open the `CH2-01-Downloading_Images` notebook:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: MIC项目将图像摄入Delta表以微调来自*Restnet*家族的预训练模型以提高其准确性。我们将以编程方式从Kaggle下载图像并将数据保存到体积中。要在您的空间中跟随，请打开`CH2-01-Downloading_Images`笔记本：
- en: '![](img/B16865_02_33.jpg)![](img/B16865_02_34.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16865_02_33.jpg)![](img/B16865_02_34.jpg)'
- en: Figure 2.30 – Downloading data from Kaggle using Databricks magic commands
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.30 – 使用Databricks魔法命令从Kaggle下载数据
- en: Now we create the volume folder and unzip the images for our classification
    project into our volumes. It will take around one hour (as it contains 80K images!)
    to extract the images from ZIP to `Volumes`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建体积文件夹，并将我们的分类项目的图像解压缩到我们的体积中。提取图像到`Volumes`需要大约一个小时（因为它包含80K个图像！）。
- en: '![Figure 2.31 – Unzipping images into the volumes for this project](img/B16865_02_35.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图2.31 – 将图像解压缩到本项目的体积中](img/B16865_02_35.jpg)'
- en: Figure 2.31 – Unzipping images into the volumes for this project
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.31 – 将图像解压缩到本项目的卷中
- en: We have downloaded or generated all four datasets, and they are ready to be
    brought into our Bronze layer in the next chapter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已下载或生成了所有四个数据集，它们已准备好在下一章被引入我们的青铜层。
- en: Summary
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered a wide range of setup decisions, options, and processes
    for planning your Data Intelligence Platform. We took you through an overview
    of the main components of the DI Platform, from early design choices to important
    features that we will dive into further in upcoming chapters. You also learned
    how to set up your workspace and project code base. We hope you feel more comfortable
    with the basics of the platform. With Databricks ready and the project data downloaded,
    we are now ready to get into what it means to build out the Bronze data layer.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了规划您的数据智能平台的各种设置决策、选项和流程。我们带您了解了 DI 平台的主要组件概述，从早期设计选择到我们将在后续章节中深入探讨的重要功能。您还学习了如何设置您的工作空间和项目代码库。我们希望您对平台的基础知识感到更加自在。随着
    Databricks 准备就绪，项目数据已下载，我们现在准备深入了解构建青铜数据层意味着什么。
- en: In [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123), we cover the essentials
    of building out the Bronze data layer within the Databricks Intelligence Platform.
    We will format our data into the most optimized format, learn about schema evolution,
    change data capture using Delta, and more.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123) 中，我们介绍了在 Databricks 智能平台内构建青铜数据层的要点。我们将数据格式化为最优化格式，了解模式演变、使用
    Delta 进行变更数据捕获以及更多内容。
- en: Questions
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: The following questions are meant to solidify key points to remember and tie
    the content back to your own experience.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题旨在巩固需要记住的关键点，并将内容与您的个人经验联系起来。
- en: How do Databricks runtimes enable stability?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Databricks 运行时如何实现稳定性？
- en: How can we make our data more discoverable?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何使我们的数据更具可发现性？
- en: What are some common steps needed to set up a Databricks workspace?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 Databricks 工作空间需要哪些常见步骤？
- en: Answers
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: After putting thought into the questions, compare your answers to ours.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考这些问题后，比较您的答案与我们的答案。
- en: Databricks runtimes enable stability by providing a consistent set of libraries.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Databricks 运行时通过提供一组一致的库来实现稳定性。
- en: Utilizing the built-in functionality for metadata, such as table and column
    descriptions, makes our data more discoverable.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用内置的元数据功能，例如表和列描述，使我们的数据更具可发现性。
- en: Some common steps for setting up a workspace are activating Databricks through
    the marketplace and setting up user groups and governance.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置工作空间的一些常见步骤是通过市场激活 Databricks 并设置用户组和治理。
- en: Further reading
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In this chapter, we identified specific libraries, technical features, and
    options. Please take a look at these resources to delve deeper into the areas
    that interest you most:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们确定了特定的库、技术特性和选项。请查看这些资源，深入了解您最感兴趣的领域：
- en: '*What is Unity* *Catalog?*: [https://docs.databricks.com/data-governance/unity-catalog/index.html](https://docs.databricks.com/data-governance/unity-catalog/index.html)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是 Unity 目录？*: [https://docs.databricks.com/data-governance/unity-catalog/index.html](https://docs.databricks.com/data-governance/unity-catalog/index.html)'
- en: '*Lakehouse Monitoring* *demo*: [https://youtu.be/3TLBZSKeYTk?t=560](https://youtu.be/3TLBZSKeYTk?t=560)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*湖屋监控* *演示*: [https://youtu.be/3TLBZSKeYTk?t=560](https://youtu.be/3TLBZSKeYTk?t=560)'
- en: '*UC has a more centralized method of managing the model lifecycle than* *HMS*:
    [https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*UC 比* *HMS* *有更集中的模型生命周期管理方法*: [https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)'
- en: '*Share Models across* *workspaces*: [https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html](https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在* *工作空间间* *共享模型*: [https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html](https://docs.databricks.com/en/machine-learning/manage-model-lifecycle/multiple-workspaces.html)'
- en: '*In-depth UC setup on* *Azure*: [https://youtu.be/itGKRVHdNPo](https://youtu.be/itGKRVHdNPo)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Azure 上的 Unity 设置* *深入探讨*: [https://youtu.be/itGKRVHdNPo](https://youtu.be/itGKRVHdNPo)'
- en: '*Connecting external HMS to* *UC*: [https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api](https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接外部HMS到* *UC*: [https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api](https://www.databricks.com/blog/extending-databricks-unity-catalog-open-apache-hive-metastore-api)'
- en: '*Unity Catalog* *limitations*: [https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations](https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Unity Catalog* *限制*: [https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations](https://docs.databricks.com/en/data-governance/unity-catalog/index.html#unity-catalog-limitations)'
- en: '*Best practices: Cluster configuration | Select Cloud in the* *dropdown*: [https://docs.databricks.com/clusters/cluster-config-best-practices.html](https://docs.databricks.com/clusters/cluster-config-best-practices.html)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最佳实践：集群配置 | 在* *下拉菜单中选择云*: [https://docs.databricks.com/clusters/cluster-config-best-practices.html](https://docs.databricks.com/clusters/cluster-config-best-practices.html)'
- en: '*Databricks* *Notebooks*: [https://docs.databricks.com/en/notebooks/index.html](https://docs.databricks.com/en/notebooks/index.html)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks* *笔记本*: [https://docs.databricks.com/en/notebooks/index.html](https://docs.databricks.com/en/notebooks/index.html)'
- en: '*Databricks Autologging | Select Cloud in the* *dropdown*: [https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management](https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks Autologging | 在* *下拉菜单中选择云*: [https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management](https://docs.databricks.com/mlflow/databricks-autologging.html#security-and-data-management)'
- en: '*Kaggle API* *GitHub*: [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggle API* *GitHub*: [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)'
- en: '*Lakehouse Monitoring product* *page*: [https://www.databricks.com/product/machine-learning/lakehouse-monitoring](https://www.databricks.com/product/machine-learning/lakehouse-monitoring)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*湖仓监控产品* *页面*: [https://www.databricks.com/product/machine-learning/lakehouse-monitoring](https://www.databricks.com/product/machine-learning/lakehouse-monitoring)'
- en: '*System* *Tables*: [https://www.databricks.com/resources/demos/tutorials/governance/system-tables](https://www.databricks.com/resources/demos/tutorials/governance/system-tables)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统* *表*: [https://www.databricks.com/resources/demos/tutorials/governance/system-tables](https://www.databricks.com/resources/demos/tutorials/governance/system-tables)'
- en: '*Opendatasets Python* *package*: [https://pypi.org/project/opendatasets/](https://pypi.org/project/opendatasets/)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Opendatasets Python* *包*: [https://pypi.org/project/opendatasets/](https://pypi.org/project/opendatasets/)'
- en: '*Kaggle* *API*: [https://www.kaggle.com/docs/api](https://www.kaggle.com/docs/api)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggle* *API*: [https://www.kaggle.com/docs/api](https://www.kaggle.com/docs/api)'
- en: '*GitHub*: [https://github.com/](https://github.com/)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*GitHub*: [https://github.com/](https://github.com/)'
- en: '*Databricks ML in Action GitHub* *Repository*: [https://github.com/PacktPublishing/Databricks-ML-In-Action](https://github.com/PacktPublishing/Databricks-ML-In-Action)'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks ML in Action GitHub* *仓库*: [https://github.com/PacktPublishing/Databricks-ML-In-Action](https://github.com/PacktPublishing/Databricks-ML-In-Action)'
- en: '*Databricks Secrets* *API*: [https://docs.databricks.com/en/security/secrets/secrets.html](https://docs.databricks.com/en/security/secrets/secrets.html)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks Secrets* *API*: [https://docs.databricks.com/en/security/secrets/secrets.html](https://docs.databricks.com/en/security/secrets/secrets.html)'
- en: '*Databricks* *CLI*: [https://docs.databricks.com/en/dev-tools/cli/index.html](https://docs.databricks.com/en/dev-tools/cli/index.html)'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks* *CLI*: [https://docs.databricks.com/en/dev-tools/cli/index.html](https://docs.databricks.com/en/dev-tools/cli/index.html)'
- en: '*Databricks* *Utilities*: [https://docs.databricks.com/en/dev-tools/databricks-utils.html](https://docs.databricks.com/en/dev-tools/databricks-utils.html)'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Databricks* *实用工具*: [https://docs.databricks.com/en/dev-tools/databricks-utils.html](https://docs.databricks.com/en/dev-tools/databricks-utils.html)'
- en: '*Workspace* *libraries*: [https://docs.databricks.com/en/libraries/workspace-libraries.html](https://docs.databricks.com/en/libraries/workspace-libraries.html)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作区* *库*: [https://docs.databricks.com/en/libraries/workspace-libraries.html](https://docs.databricks.com/en/libraries/workspace-libraries.html)'
- en: '*Data Mesh and the DI Platforms Blog Posts*: [https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html),
    [https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html](https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据网格和DI平台博客文章*: [https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html](https://www.databricks.com/blog/2022/10/10/databricks-lakehouse-and-data-mesh-part-1.html),
    [https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html](https://www.databricks.com/blog/2022/10/19/building-data-mesh-based-databricks-lakehouse-part-2.html)'
- en: '*Short YouTube video on managed vs external tables in* *UC*: [https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA](https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于托管表与外部表比较的短YouTube视频* *UC*: [https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA](https://youtu.be/yt9vax_PH58?si=dVJRZHAOnrEUBdkA)'
- en: '*Query* *Federation*: [https://docs.databricks.com/en/query-federation/index.html](https://docs.databricks.com/en/query-federation/index.html)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*查询* *联邦*: [https://docs.databricks.com/en/query-federation/index.html](https://docs.databricks.com/en/query-federation/index.html)'
- en: '*Centralized model registry workspace for* *HMS*: [https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html](https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HMS的集中式模型注册工作区* [https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html](https://docs.databricks.com/applications/machine-learning/manage-model-lifecycle/multiple-workspaces.html)'
- en: '*Manage model lifecycle in Unity* *Catalog*: [https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Unity中管理模型生命周期* *目录*: [https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html)'
- en: '*Terraform* *https*: [https://github.com/databricks/terraform-provider-databricks](https://github.com/databricks/terraform-provider-databricks)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Terraform* *https*: [https://github.com/databricks/terraform-provider-databricks](https://github.com/databricks/terraform-provider-databricks)'
- en: '*Widgets*: [https://docs.databricks.com/notebooks/widgets.html](https://docs.databricks.com/notebooks/widgets.html)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*小部件* [https://docs.databricks.com/notebooks/widgets.html](https://docs.databricks.com/notebooks/widgets.html)'
- en: '*Kaggle API* *GitHub*: [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kaggle API* *GitHub*: [https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api).'
