- en: Chapter 2. Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：线性回归
- en: In this chapter you will learn how to use linear models in regression problems.
    First, we will examine simple linear regression, which models the relationship
    between a response variable and single explanatory variable. Next, we will discuss
    multiple linear regression, a generalization of simple linear regression that
    can support more than one explanatory variable. Then, we will discuss polynomial
    regression, a special case of multiple linear regression that can effectively
    model nonlinear relationships. Finally, we will discuss how to train our models
    by finding the values of their parameters that minimize a cost function. We will
    work through a toy problem to learn how the models and learning algorithms work
    before discussing an application with a larger dataset.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何在回归问题中使用线性模型。首先，我们将研究简单线性回归，它建立了一个响应变量与单个解释变量之间的关系。接下来，我们将讨论多重线性回归，它是简单线性回归的推广，支持多个解释变量。然后，我们将讨论多项式回归，这是一种多重线性回归的特例，能够有效地建模非线性关系。最后，我们将讨论如何通过找到使成本函数最小化的参数值来训练我们的模型。在讨论更大数据集的应用之前，我们将通过一个玩具问题来学习这些模型和学习算法的工作原理。
- en: Simple linear regression
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: In the previous chapter you learned that training data is used to estimate the
    parameters of a model in supervised learning problems. Past observations of explanatory
    variables and their corresponding response variables comprise the training data.
    The model can be used to predict the value of the response variable for values
    of the explanatory variable that have not been previously observed. Recall that
    the goal in regression problems is to predict the value of a continuous response
    variable. In this chapter, we will examine several example linear regression models.
    We will discuss the training data, model, learning algorithm, and evaluation metrics
    for each approach. To start, let's consider **simple linear regression**. Simple
    linear regression can be used to model a linear relationship between one response
    variable and one explanatory variable. Linear regression has been applied to many
    important scientific and social problems; the example that we will consider is
    probably not one of them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解到训练数据用于估计监督学习问题中模型的参数。过去对解释变量及其相应响应变量的观察构成了训练数据。该模型可以用于预测在先前未观察到的解释变量值下响应变量的值。回顾一下，回归问题的目标是预测连续响应变量的值。在本章中，我们将研究几个线性回归模型的示例。我们将讨论每种方法的训练数据、模型、学习算法和评估指标。首先，让我们考虑**简单线性回归**。简单线性回归可用于建立一个响应变量与一个解释变量之间的线性关系。线性回归已被应用于许多重要的科学和社会问题；我们将要考虑的这个例子可能并非其中之一。
- en: 'Suppose you wish to know the price of a pizza. You might simply look at a menu.
    This, however, is a machine learning book, so we will use simple linear regression
    instead to predict the price of a pizza based on an attribute of the pizza that
    we can observe. Let''s model the relationship between the size of a pizza and
    its price. First, we will write a program with scikit-learn that can predict the
    price of a pizza given its size. Then, we will discuss how simple linear regression
    works and how it can be generalized to work with other types of problems. Let''s
    assume that you have recorded the diameters and prices of pizzas that you have
    previously eaten in your pizza journal. These observations comprise our training
    data:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想知道比萨饼的价格。你可能会直接查看菜单。然而，这本书是一本机器学习书籍，所以我们将使用简单线性回归来预测比萨饼的价格，基于我们可以观察到的比萨饼的某个属性。让我们建模比萨饼大小与价格之间的关系。首先，我们将编写一个使用scikit-learn的程序，可以根据比萨饼的大小预测它的价格。然后，我们将讨论简单线性回归是如何工作的，以及它如何被推广到解决其他类型的问题。假设你已经在比萨饼日志中记录了你之前吃过的比萨饼的直径和价格。这些观察数据构成了我们的训练数据：
- en: '| Training instance | Diameter (in inches) | Price (in dollars) |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| 训练实例 | 直径（英寸） | 价格（美元） |'
- en: '| --- | --- | --- |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 6 | 7 |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 7 |'
- en: '| 2 | 8 | 9 |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8 | 9 |'
- en: '| 3 | 10 | 13 |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 13 |'
- en: '| 4 | 14 | 17.5 |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 14 | 17.5 |'
- en: '| 5 | 18 | 18 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 18 | 18 |'
- en: 'We can visualize our training data by plotting it on a graph using `matplotlib`:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`matplotlib`将训练数据绘制在图表上来可视化它：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding script produces the following graph. The diameters of the pizzas
    are plotted on the *x* axis and the prices are plotted on the *y* axis.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的脚本产生了以下图形。比萨饼的直径绘制在*x*轴上，价格绘制在*y*轴上。
- en: '![Simple linear regression](img/8365OS_02_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![简单线性回归](img/8365OS_02_01.jpg)'
- en: 'We can see from the graph of the training data that there is a positive relationship
    between the diameter of a pizza and its price, which should be corroborated by
    our own pizza-eating experience. As the diameter of a pizza increases, its price
    generally increases too. The following pizza-price predictor program models this
    relationship using linear regression. Let''s review the following program and
    discuss how linear regression works:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练数据的图形中，我们可以看到比萨的直径与其价格之间存在正相关关系，这也应该得到我们自己吃比萨经验的验证。随着比萨直径的增大，其价格通常也会增加。以下的比萨价格预测程序使用线性回归来建模这种关系。让我们回顾以下程序，并讨论线性回归是如何工作的：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Simple linear regression assumes that a linear relationship exists between the
    response variable and explanatory variable; it models this relationship with a
    linear surface called a hyperplane. A hyperplane is a subspace that has one dimension
    less than the ambient space that contains it. In simple linear regression, there
    is one dimension for the response variable and another dimension for the explanatory
    variable, making a total of two dimensions. The regression hyperplane therefore,
    has one dimension; a hyperplane with one dimension is a line.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归假设响应变量与解释变量之间存在线性关系；它用一个称为超平面的线性面来建模这个关系。超平面是一个子空间，其维度比包含它的空间少一个维度。在简单线性回归中，响应变量有一个维度，解释变量有另一个维度，总共有两个维度。因此，回归超平面只有一个维度；一个一维的超平面就是一条直线。
- en: The `sklearn.linear_model.LinearRegression` class is an **estimator**. Estimators
    predict a value based on the observed data. In scikit-learn, all estimators implement
    the `fit()` and `predict()` methods. The former method is used to learn the parameters
    of a model, and the latter method is used to predict the value of a response variable
    for an explanatory variable using the learned parameters. It is easy to experiment
    with different models using scikit-learn because all estimators implement the
    `fit` and `predict` methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.linear_model.LinearRegression` 类是一个**估计器**。估计器根据观察到的数据预测一个值。在scikit-learn中，所有估计器都实现了
    `fit()` 和 `predict()` 方法。前者用于学习模型的参数，后者用于利用学习到的参数预测响应变量的值。使用scikit-learn很容易进行不同模型的实验，因为所有估计器都实现了
    `fit` 和 `predict` 方法。'
- en: 'The `fit` method of `LinearRegression` learns the parameters of the following
    model for simple linear regression:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinearRegression` 的 `fit` 方法学习以下简单线性回归模型的参数：'
- en: '![Simple linear regression](img/8365OS_02_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![简单线性回归](img/8365OS_02_02.jpg)'
- en: '![Simple linear regression](img/8365OS_02_31.jpg) is the predicted value of
    the response variable; in this example, it is the predicted price of the pizza.
    ![Simple linear regression](img/8365OS_02_32.jpg) is the explanatory variable.
    The intercept term ![Simple linear regression](img/8365OS_02_33.jpg) and coefficient
    ![Simple linear regression](img/8365OS_02_34.jpg) are parameters of the model
    that are learned by the learning algorithm. The line plotted in the following
    figure models the relationship between the size of a pizza and its price. Using
    this model, we would expect the price of an 8-inch pizza to be about $7.33, and
    the price of a 20-inch pizza to be $18.75.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![简单线性回归](img/8365OS_02_31.jpg) 是响应变量的预测值；在这个例子中，它是比萨的预测价格。 ![简单线性回归](img/8365OS_02_32.jpg)
    是解释变量。截距项 ![简单线性回归](img/8365OS_02_33.jpg) 和系数 ![简单线性回归](img/8365OS_02_34.jpg)
    是模型的参数，这些参数由学习算法学习得出。下图中的直线描绘了比萨的大小与价格之间的关系。使用这个模型，我们预计一个8英寸比萨的价格约为7.33美元，而一个20英寸比萨的价格约为18.75美元。'
- en: '![Simple linear regression](img/8365OS_02_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![简单线性回归](img/8365OS_02_03.jpg)'
- en: Using training data to learn the values of the parameters for simple linear
    regression that produce the best fitting model is called **ordinary least squares**
    or **linear least** **squares**. "In this chapter we will discuss methods for
    approximating the values of the model's parameters and for solving them analytically.
    First, however, we must define what it means for a model to fit the training data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据学习简单线性回归的参数值，以生成最佳拟合模型的过程称为**普通最小二乘法**或**线性最小二乘法**。在本章中，我们将讨论近似模型参数值和解析解法的方法。首先，我们必须定义什么是模型拟合训练数据。
- en: Evaluating the fitness of a model with a cost function
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型适应度的代价函数
- en: Regression lines produced by several sets of parameter values are plotted in
    the following figure. How can we assess which parameters produced the best-fitting
    regression line?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由若干参数值集生成的回归线在下图中绘制。我们如何评估哪些参数生成了最佳拟合的回归线？
- en: '![Evaluating the fitness of a model with a cost function](img/8365OS_02_04.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型拟合度的代价函数](img/8365OS_02_04.jpg)'
- en: A **cost** **function**, also called a **loss function,** is used to define
    and measure the error of a model. The differences between the prices predicted
    by the model and the observed prices of the pizzas in the training set are called
    **residuals** or **training errors**. Later, we will evaluate a model on a separate
    set of test data; the differences between the predicted and observed values in
    the test data are called **prediction** **errors** or **test errors**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**代价** **函数**，也称为**损失函数**，用于定义和衡量模型的误差。模型预测的价格与训练集中比萨实际价格之间的差异被称为**残差**或**训练误差**。稍后，我们将在一个单独的测试数据集上评估模型；预测值与测试数据中观察到的值之间的差异被称为**预测**
    **误差**或**测试误差**。'
- en: 'The residuals for our model are indicated by the vertical lines between the
    points for the training instances and regression hyperplane in the following plot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的残差由以下图中训练实例的点和回归超平面之间的垂直线表示：
- en: '![Evaluating the fitness of a model with a cost function](img/8365OS_02_05.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型拟合度的代价函数](img/8365OS_02_05.jpg)'
- en: 'We can produce the best pizza-price predictor by minimizing the sum of the
    residuals. That is, our model fits if the values it predicts for the response
    variable are close to the observed values for all of the training examples. This
    measure of the model''s fitness is called the **residual** **sum of squares**
    cost function. Formally, this function assesses the fitness of a model by summing
    the squared residuals for all of our training examples. The residual sum of squares
    is calculated with the formula in the following equation, where ![Evaluating the
    fitness of a model with a cost function](img/8365OS_02_35.jpg) is the observed
    value and ![Evaluating the fitness of a model with a cost function](img/8365OS_02_36.jpg)
    is the predicted value:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过最小化残差的总和来生成最佳的比萨价格预测器。也就是说，当模型对所有训练样本的响应变量预测值接近观察到的实际值时，我们的模型就得到了拟合。这种衡量模型拟合度的方式被称为**残差**
    **平方和**代价函数。严格来说，这个函数通过对所有训练样本的残差平方求和来评估模型的拟合度。残差平方和通过以下公式计算，其中 ![评估模型拟合度的代价函数](img/8365OS_02_35.jpg)
    为观察值，![评估模型拟合度的代价函数](img/8365OS_02_36.jpg) 为预测值：
- en: '![Evaluating the fitness of a model with a cost function](img/8365OS_02_06.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型拟合度的代价函数](img/8365OS_02_06.jpg)'
- en: 'Let''s compute the residual sum of squares for our model by adding the following
    two lines to the previous script:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在前一个脚本中添加以下两行来计算我们模型的残差平方和：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have a cost function, we can find the values of our model's parameters
    that minimize it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了代价函数，可以找到最小化该函数的模型参数值。
- en: Solving ordinary least squares for simple linear regression
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解普通最小二乘法以进行简单线性回归
- en: 'In this section, we will work through solving ordinary least squares for simple
    linear regression. Recall that simple linear regression is given by the following
    equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将解决简单线性回归的普通最小二乘法问题。回顾一下，简单线性回归由以下方程给出：
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![解普通最小二乘法以进行简单线性回归](img/8365OS_02_02.jpg)'
- en: Also, recall that our goal is to solve the values of ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_34.jpg) and ![Solving ordinary
    least squares for simple linear regression](img/8365OS_02_33.jpg) that minimize
    the cost function. We will solve ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_34.jpg) first. To do so, we will calculate the **variance**
    of ![Solving ordinary least squares for simple linear regression](img/8365OS_02_32.jpg)
    and **covariance** of ![Solving ordinary least squares for simple linear regression](img/8365OS_02_32.jpg)
    and ![Solving ordinary least squares for simple linear regression](img/8365OS_02_31.jpg).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，回想一下，我们的目标是求解使成本函数最小化的![解决普通最小二乘法的简单线性回归](img/8365OS_02_34.jpg) 和![解决普通最小二乘法的简单线性回归](img/8365OS_02_33.jpg)
    的值。我们将首先求解![解决普通最小二乘法的简单线性回归](img/8365OS_02_34.jpg)。为此，我们将计算![解决普通最小二乘法的简单线性回归](img/8365OS_02_32.jpg)
    的**方差**和![解决普通最小二乘法的简单线性回归](img/8365OS_02_32.jpg) 与![解决普通最小二乘法的简单线性回归](img/8365OS_02_31.jpg)
    的**协方差**。
- en: 'Variance is a measure of how far a set of values is spread out. If all of the
    numbers in the set are equal, the variance of the set is zero. A small variance
    indicates that the numbers are near the mean of the set, while a set containing
    numbers that are far from the mean and each other will have a large variance.
    Variance can be calculated using the following equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 方差是衡量一组值分布范围的指标。如果该组中的所有数值相等，则方差为零。小方差表示这些数值接近该组的均值，而一组包含远离均值且彼此差异较大的数值则具有较大的方差。方差可以通过以下方程计算：
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_07.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![解决普通最小二乘法的简单线性回归](img/8365OS_02_07.jpg)'
- en: 'In the preceding equation, ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_37.jpg) is the mean of ![Solving ordinary least squares
    for simple linear regression](img/8365OS_02_32.jpg), ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_38.jpg) is the value of ![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_32.jpg) for
    the ![Solving ordinary least squares for simple linear regression](img/8365OS_02_39.jpg)
    training instance, and ![Solving ordinary least squares for simple linear regression](img/8365OS_02_40.jpg)
    is the number of training instances. Let''s calculate the variance of the pizza
    diameters in our training set:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，![解决普通最小二乘法的简单线性回归](img/8365OS_02_37.jpg) 是![解决普通最小二乘法的简单线性回归](img/8365OS_02_32.jpg)
    的均值，![解决普通最小二乘法的简单线性回归](img/8365OS_02_38.jpg) 是![解决普通最小二乘法的简单线性回归](img/8365OS_02_32.jpg)
    在![解决普通最小二乘法的简单线性回归](img/8365OS_02_39.jpg) 训练实例中的值，![解决普通最小二乘法的简单线性回归](img/8365OS_02_40.jpg)
    是训练实例的数量。让我们计算训练集中的比萨直径的方差：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'NumPy also provides the `var` method to calculate variance. The `ddof` keyword
    parameter can be used to set Bessel''s correction to calculate the sample variance:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy还提供了`var`方法来计算方差。可以使用`ddof`关键字参数设置贝塞尔修正来计算样本方差：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Covariance is a measure of how much two variables change together. If the value
    of the variables increase together, their covariance is positive. If one variable
    tends to increase while the other decreases, their covariance is negative. If
    there is no linear relationship between the two variables, their covariance will
    be equal to zero; the variables are linearly uncorrelated but not necessarily
    independent. Covariance can be calculated using the following formula:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差是衡量两个变量共同变化程度的指标。如果变量的值一起增加，则它们的协方差为正；如果一个变量倾向于增加而另一个减少，则它们的协方差为负。如果两个变量之间没有线性关系，则它们的协方差为零；这些变量是线性不相关的，但不一定是独立的。协方差可以通过以下公式计算：
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_08.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![解决普通最小二乘法的简单线性回归](img/8365OS_02_08.jpg)'
- en: 'As with variance, ![Solving ordinary least squares for simple linear regression](img/8365OS_02_38.jpg)
    is the diameter of the ![Solving ordinary least squares for simple linear regression](img/8365OS_02_39.jpg)
    training instance, ![Solving ordinary least squares for simple linear regression](img/8365OS_02_37.jpg)
    is the mean of the diameters, ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_41.jpg) is the mean of the prices, ![Solving ordinary
    least squares for simple linear regression](img/8365OS_02_35.jpg) is the price
    of the ![Solving ordinary least squares for simple linear regression](img/8365OS_02_39.jpg)
    training instance, and ![Solving ordinary least squares for simple linear regression](img/8365OS_02_40.jpg)
    is the number of training instances. Let''s calculate the covariance of the diameters
    and prices of the pizzas in the training set:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与方差类似，![求解普通最小二乘法用于简单线性回归](img/8365OS_02_38.jpg)是![求解普通最小二乘法用于简单线性回归](img/8365OS_02_39.jpg)训练实例的直径，![求解普通最小二乘法用于简单线性回归](img/8365OS_02_37.jpg)是直径的平均值，![求解普通最小二乘法用于简单线性回归](img/8365OS_02_41.jpg)是价格的平均值，![求解普通最小二乘法用于简单线性回归](img/8365OS_02_35.jpg)是![求解普通最小二乘法用于简单线性回归](img/8365OS_02_39.jpg)训练实例的价格，而![求解普通最小二乘法用于简单线性回归](img/8365OS_02_40.jpg)是训练实例的数量。我们来计算训练集中比萨饼直径和价格的协方差：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have calculated the variance of our explanatory variable and the
    covariance of the response and explanatory variables, we can solve ![Solving ordinary
    least squares for simple linear regression](img/8365OS_02_34.jpg) using the following
    formula:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了自变量的方差和响应与自变量的协方差，我们可以使用以下公式求解![求解普通最小二乘法用于简单线性回归](img/8365OS_02_34.jpg)：
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_09.jpg)![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_10.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![求解普通最小二乘法用于简单线性回归](img/8365OS_02_09.jpg)![求解普通最小二乘法用于简单线性回归](img/8365OS_02_10.jpg)'
- en: 'Having solved ![Solving ordinary least squares for simple linear regression](img/8365OS_02_34.jpg),
    we can solve ![Solving ordinary least squares for simple linear regression](img/8365OS_02_33.jpg)
    using the following formula:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在求解了![求解普通最小二乘法用于简单线性回归](img/8365OS_02_34.jpg)后，我们可以使用以下公式求解![求解普通最小二乘法用于简单线性回归](img/8365OS_02_33.jpg)：
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_11.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![求解普通最小二乘法用于简单线性回归](img/8365OS_02_11.jpg)'
- en: 'In the preceding formula, ![Solving ordinary least squares for simple linear
    regression](img/8365OS_02_41.jpg) is the mean of ![Solving ordinary least squares
    for simple linear regression](img/8365OS_02_31.jpg) and ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_37.jpg) is the mean of ![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_32.jpg). ![Solving
    ordinary least squares for simple linear regression](img/8365OS_02_42.jpg) are
    the coordinates of the centroid, a point that the model must pass through. We
    can use the centroid and the value of ![Solving ordinary least squares for simple
    linear regression](img/8365OS_02_34.jpg) to solve for ![Solving ordinary least
    squares for simple linear regression](img/8365OS_02_33.jpg) as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，![求解普通最小二乘法用于简单线性回归](img/8365OS_02_41.jpg)是![求解普通最小二乘法用于简单线性回归](img/8365OS_02_31.jpg)的平均值，而![求解普通最小二乘法用于简单线性回归](img/8365OS_02_37.jpg)是![求解普通最小二乘法用于简单线性回归](img/8365OS_02_32.jpg)的平均值。![求解普通最小二乘法用于简单线性回归](img/8365OS_02_42.jpg)是重心的坐标，这是模型必须经过的点。我们可以使用重心和![求解普通最小二乘法用于简单线性回归](img/8365OS_02_34.jpg)的值来求解![求解普通最小二乘法用于简单线性回归](img/8365OS_02_33.jpg)，如下所示：
- en: '![Solving ordinary least squares for simple linear regression](img/8365OS_02_12.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![求解普通最小二乘法用于简单线性回归](img/8365OS_02_12.jpg)'
- en: Now that we have solved the values of the model's parameters that minimize the
    cost function, we can plug in the diameters of the pizzas and predict their prices.
    For instance, an 11-inch pizza is expected to cost around $12.70, and an 18-inch
    pizza is expected to cost around $19.54\. Congratulations! You used simple linear
    regression to predict the price of a pizza.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了能够最小化成本函数的模型参数的值，我们可以代入比萨饼的直径并预测其价格。例如，一款11英寸的比萨饼预计价格为12.70美元，而一款18英寸的比萨饼预计价格为19.54美元。恭喜你！你使用了简单线性回归来预测比萨饼的价格。
- en: Evaluating the model
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'We have used a learning algorithm to estimate a model''s parameters from the
    training data. How can we assess whether our model is a good representation of
    the real relationship? Let''s assume that you have found another page in your
    pizza journal. We will use the entries on this page as a test set to measure the
    performance of our model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了学习算法从训练数据中估算模型的参数。我们如何评估我们的模型是否能良好地表示实际关系呢？假设你在披萨日志中找到了另一页记录。我们将使用这页中的条目作为测试集来衡量模型的表现：
- en: '| Test Instance | Diameter (in inches) | Observed price (in dollars) | Predicted
    price (in dollars) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 测试实例 | 直径（英寸） | 观察到的价格（美元） | 预测的价格（美元） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 8 | 11 | 9.7759 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8 | 11 | 9.7759 |'
- en: '| 2 | 9 | 8.5 | 10.7522 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9 | 8.5 | 10.7522 |'
- en: '| 3 | 11 | 15 | 12.7048 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 11 | 15 | 12.7048 |'
- en: '| 4 | 16 | 18 | 17.5863 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 18 | 17.5863 |'
- en: '| 5 | 12 | 11 | 13.6811 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 12 | 11 | 13.6811 |'
- en: Several measures can be used to assess our model's predictive capabilities.
    We will evaluate our pizza-price predictor using **r-squared**. R-squared measures
    how well the observed values of the response variables are predicted by the model.
    More concretely, r-squared is the proportion of the variance in the response variable
    that is explained by the model. An r-squared score of one indicates that the response
    variable can be predicted without any error using the model. An r-squared score
    of one half indicates that half of the variance in the response variable can be
    predicted using the model. There are several methods to calculate r-squared. In
    the case of simple linear regression, r-squared is equal to the square of the
    Pearson product moment correlation coefficient, or Pearson's *r*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用多种方法来评估我们模型的预测能力。我们将使用**r平方**来评估我们的披萨价格预测器。R平方衡量模型预测响应变量观察值的准确度。更具体地说，r平方是模型解释的响应变量方差的比例。r平方值为1表示模型可以毫无误差地预测响应变量。r平方值为0.5表示模型能够预测响应变量方差的一半。计算r平方有几种方法。在简单线性回归的情况下，r平方等于皮尔逊积矩相关系数的平方，或皮尔逊的*r*。
- en: Using this method, r-squared must be a positive number between zero and one.
    This method is intuitive; if r-squared describes the proportion of variance in
    the response variable explained by the model, it cannot be greater than one or
    less than zero. Other methods, including the method used by scikit-learn, do not
    calculate r-squared as the square of Pearson's *r*, and can return a negative
    r-squared if the model performs extremely poorly. We will follow the method used
    by scikit-learn to calculate r-squared for our pizza-price predictor.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，r平方应该是一个介于零和一之间的正数。这种方法是直观的；如果r平方描述了模型解释的响应变量方差比例，它不能大于1或小于0。其他方法，包括scikit-learn使用的方法，并不会将r平方计算为皮尔逊*r*的平方，如果模型表现极差，它可能返回负的r平方。我们将遵循scikit-learn使用的方法来计算我们披萨价格预测器的r平方值。
- en: First, we must measure the total sum of the squares. ![Evaluating the model](img/8365OS_02_35.jpg)
    is the observed value of the response variable for the ![Evaluating the model](img/8365OS_02_39.jpg)
    test instance, and ![Evaluating the model](img/8365OS_02_41.jpg) is the mean of
    the observed values of the response variable
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须计算总平方和。![评估模型](img/8365OS_02_35.jpg)是第![评估模型](img/8365OS_02_39.jpg)个测试实例的响应变量观察值，且![评估模型](img/8365OS_02_41.jpg)是响应变量观察值的均值。
- en: '![Evaluating the model](img/8365OS_02_13.jpg)![Evaluating the model](img/8365OS_02_14.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型](img/8365OS_02_13.jpg)![评估模型](img/8365OS_02_14.jpg)'
- en: Next, we must find the residual sum of the squares. Recall that this is also
    our cost function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须计算残差平方和。请记住，这也是我们的成本函数。
- en: '![Evaluating the model](img/8365OS_02_06.jpg)![Evaluating the model](img/8365OS_02_15.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型](img/8365OS_02_06.jpg)![评估模型](img/8365OS_02_15.jpg)'
- en: 'Finally, we can find r-squared using the following formula:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下公式来计算r平方：
- en: '![Evaluating the model](img/8365OS_02_16.jpg)![Evaluating the model](img/8365OS_02_17.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型](img/8365OS_02_16.jpg)![评估模型](img/8365OS_02_17.jpg)'
- en: 'An r-squared score of **0.6620** indicates that a large proportion of the variance
    in the test instances'' prices is explained by the model. Now, let''s confirm
    our calculation using scikit-learn. The `score` method of `LinearRegression` returns
    the model''s r-squared value, as seen in the following example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**0.6620**的r平方值表示模型能够解释测试实例价格变化的大部分方差。现在，让我们使用scikit-learn来验证我们的计算。`LinearRegression`的`score`方法返回模型的r平方值，如下例所示：'
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Multiple linear regression
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: We have trained and evaluated a model to predict the price of a pizza. While
    you are eager to demonstrate the pizza-price predictor to your friends and co-workers,
    you are concerned by the model's imperfect r-squared score and the embarrassment
    its predictions could cause you. How can we improve the model?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练并评估了一个模型，用来预测比萨的价格。当你迫不及待想要向朋友和同事展示这个比萨价格预测器时，你却因模型不完美的r平方得分以及预测可能带来的尴尬而感到担忧。我们如何改进这个模型？
- en: 'Recalling your personal pizza-eating experience, you might have some intuitions
    about the other attributes of a pizza that are related to its price. For instance,
    the price often depends on the number of toppings on the pizza. Fortunately, your
    pizza journal describes toppings in detail; let''s add the number of toppings
    to our training data as a second explanatory variable. We cannot proceed with
    simple linear regression, but we can use a generalization of simple linear regression
    that can use multiple explanatory variables called multiple linear regression.
    Formally, multiple linear regression is the following model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下你个人的吃比萨经验，你可能对比萨的其他特征有一些直觉，这些特征与比萨的价格相关。例如，价格通常取决于比萨上的配料数量。幸运的是，你的比萨日志详细描述了配料；让我们把配料数量作为第二个解释变量，加入我们的训练数据。我们不能继续进行简单线性回归，但可以使用简单线性回归的推广方法——多元线性回归，它可以使用多个解释变量。严格来说，多元线性回归是以下模型：
- en: '![Multiple linear regression](img/8365OS_02_18.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![多元线性回归](img/8365OS_02_18.jpg)'
- en: this edit makes no sense. change to "Where simple linear regression uses a single
    explanatory variable with a single coefficient, multiple linear regression uses
    a coefficient for each of an arbitrary number of explanatory variables.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个编辑没有意义。更改为：“简单线性回归使用一个解释变量和一个系数，而多元线性回归为每个任意数量的解释变量使用一个系数。”
- en: '![Multiple linear regression](img/8365OS_02_19.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![多元线性回归](img/8365OS_02_19.jpg)'
- en: 'For simple linear regression, this is equivalent to the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单线性回归，这相当于以下内容：
- en: '![Multiple linear regression](img/8365OS_02_20.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![多元线性回归](img/8365OS_02_20.jpg)'
- en: '![Multiple linear regression](img/8365OS_02_43.jpg) is a column vector of the
    values of the response variables for the training examples. ![Multiple linear
    regression](img/8365OS_02_34.jpg) is a column vector of the values of the model''s
    parameters. ![Multiple linear regression](img/8365OS_02_44.jpg), called the design
    matrix, is an ![Multiple linear regression](img/8365OS_02_45.jpg) dimensional
    matrix of the values of the explanatory variables for the training examples. ![Multiple
    linear regression](img/8365OS_02_46.jpg) is the number of training examples and
    ![Multiple linear regression](img/8365OS_02_40.jpg) is the number of explanatory
    variables. Let''s update our pizza training data to include the number of toppings
    with the following values:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![多元线性回归](img/8365OS_02_43.jpg) 是训练实例中响应变量的值的列向量。![多元线性回归](img/8365OS_02_34.jpg)
    是模型参数值的列向量。![多元线性回归](img/8365OS_02_44.jpg)，称为设计矩阵，是训练实例中解释变量值的![多元线性回归](img/8365OS_02_45.jpg)
    维矩阵。![多元线性回归](img/8365OS_02_46.jpg) 是训练实例的数量，![多元线性回归](img/8365OS_02_40.jpg) 是解释变量的数量。让我们更新我们的比萨训练数据，将配料数量加入，值如下：'
- en: '| Training Example | Diameter (in inches) | Number of toppings | Price (in
    dollars) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 训练实例 | 直径（英寸） | 配料数量 | 价格（美元） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 6 | 2 | 7 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 2 | 7 |'
- en: '| 2 | 8 | 1 | 9 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8 | 1 | 9 |'
- en: '| 3 | 10 | 0 | 13 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 0 | 13 |'
- en: '| 4 | 14 | 2 | 17.5 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 14 | 2 | 17.5 |'
- en: '| 5 | 18 | 0 | 18 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 18 | 0 | 18 |'
- en: 'We must also update our test data to include the second explanatory variable,
    as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须更新我们的测试数据，加入第二个解释变量，如下所示：
- en: '| Test Instance | Diameter (in inches) | Number of toppings | Price (in dollars)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 测试实例 | 直径（英寸） | 配料数量 | 价格（美元） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 8 | 2 | 11 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8 | 2 | 11 |'
- en: '| 2 | 9 | 0 | 8.5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9 | 0 | 8.5 |'
- en: '| 3 | 11 | 2 | 15 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 11 | 2 | 15 |'
- en: '| 4 | 16 | 2 | 18 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 2 | 18 |'
- en: '| 5 | 12 | 0 | 11 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 12 | 0 | 11 |'
- en: 'Our learning algorithm must estimate the values of three parameters: the coefficients
    for the two features and the intercept term. While one might be tempted to solve
    ![Multiple linear regression](img/8365OS_02_34.jpg) by dividing each side of the
    equation by ![Multiple linear regression](img/8365OS_02_44.jpg), division by a
    matrix is impossible. Just as dividing a number by an integer is equivalent to
    multiplying by the inverse of the same integer, we can multiply ![Multiple linear
    regression](img/8365OS_02_34.jpg) by the inverse of ![Multiple linear regression](img/8365OS_02_44.jpg)
    to avoid matrix division. Matrix inversion is denoted with a superscript -1\.
    Only square matrices can be inverted. ![Multiple linear regression](img/8365OS_02_44.jpg)
    is not likely to be a square; the number of training instances will have to be
    equal to the number of features for it to be so. We will multiply ![Multiple linear
    regression](img/8365OS_02_44.jpg) by its transpose to yield a square matrix that
    can be inverted. Denoted with a superscript ![Multiple linear regression](img/8365OS_02_47.jpg),
    the transpose of a matrix is formed by turning the rows of the matrix into columns
    and vice versa, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的学习算法必须估计三个参数的值：两个特征的系数和截距项。虽然有人可能会试图通过将方程两边除以 ![多元线性回归](img/8365OS_02_44.jpg)
    来求解 ![多元线性回归](img/8365OS_02_34.jpg)，但矩阵除法是不可能的。就像除以一个整数等同于乘以该整数的倒数一样，我们可以通过将 ![多元线性回归](img/8365OS_02_34.jpg)
    乘以 ![多元线性回归](img/8365OS_02_44.jpg) 的逆矩阵来避免矩阵除法。矩阵求逆用上标 -1 来表示。只有方阵才能求逆。![多元线性回归](img/8365OS_02_44.jpg)
    很可能不是方阵；训练实例的数量必须等于特征的数量，才有可能是方阵。我们将把 ![多元线性回归](img/8365OS_02_44.jpg) 乘以其转置矩阵，得到一个可以求逆的方阵。矩阵的转置用上标
    ![多元线性回归](img/8365OS_02_47.jpg) 来表示，矩阵的转置是将矩阵的行变为列，列变为行，形式如下：
- en: '![Multiple linear regression](img/8365OS_02_51.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![多元线性回归](img/8365OS_02_51.jpg)'
- en: 'To recap, our model is given by the following formula:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的模型由以下公式给出：
- en: '![Multiple linear regression](img/8365OS_02_19.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![多元线性回归](img/8365OS_02_19.jpg)'
- en: 'We know the values of ![Multiple linear regression](img/8365OS_02_43.jpg) and
    ![Multiple linear regression](img/8365OS_02_44.jpg) from our training data. We
    must find the values of ![Multiple linear regression](img/8365OS_02_34.jpg), which
    minimize the cost function. We can solve ![Multiple linear regression](img/8365OS_02_34.jpg)
    as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练数据中知道 ![多元线性回归](img/8365OS_02_43.jpg) 和 ![多元线性回归](img/8365OS_02_44.jpg)
    的值。我们必须找到 ![多元线性回归](img/8365OS_02_34.jpg) 的值，以最小化代价函数。我们可以如下所示求解 ![多元线性回归](img/8365OS_02_34.jpg)：
- en: '![Multiple linear regression](img/8365OS_02_21.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![多元线性回归](img/8365OS_02_21.jpg)'
- en: 'We can solve ![Multiple linear regression](img/8365OS_02_34.jpg) using NumPy,
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 NumPy 来求解 ![多元线性回归](img/8365OS_02_34.jpg)，如下所示：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'NumPy also provides a least squares function that can solve the values of the
    parameters more compactly:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 还提供了一个最小二乘函数，可以更简洁地求解参数的值：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s update our pizza-price predictor program to use the second explanatory
    variable, and compare its performance on the test set to that of the simple linear
    regression model:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的比萨价格预测程序，使用第二个解释变量，并将其在测试集上的表现与简单线性回归模型进行比较：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It appears that adding the number of toppings as an explanatory variable has
    improved the performance of our model. In later sections, we will discuss why
    evaluating a model on a single test set can provide inaccurate estimates of the
    model's performance, and how we can estimate its performance more accurately by
    training and testing on many partitions of the data. For now, however, we can
    accept that the multiple linear regression model performs significantly better
    than the simple linear regression model. There may be other attributes of pizzas
    that can be used to explain their prices. What if the relationship between these
    explanatory variables and the response variable is not linear in the real world?
    In the next section, we will examine a special case of multiple linear regression
    that can be used to model nonlinear relationships.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来将配料数量作为一个解释变量已提高了我们模型的性能。在后续章节中，我们将讨论为什么仅通过单一测试集来评估模型可能会提供不准确的性能估计，以及如何通过在多个数据分区上进行训练和测试来更准确地估计其性能。不过，目前我们可以接受，多元线性回归模型的表现明显优于简单线性回归模型。或许比萨的其他属性也可以用来解释其价格。如果这些解释变量与响应变量之间的关系在现实世界中不是线性的怎么办？在下一节中，我们将探讨多元线性回归的一个特例，该特例可以用来建模非线性关系。
- en: Polynomial regression
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式回归
- en: 'In the previous examples, we assumed that the real relationship between the
    explanatory variables and the response variable is linear. This assumption is
    not always true. In this section, we will use **polynomial regression**, a special
    case of multiple linear regression that adds terms with degrees greater than one
    to the model. The real-world curvilinear relationship is captured when you transform
    the training data by adding polynomial terms, which are then fit in the same manner
    as in multiple linear regression. For ease of visualization, we will again use
    only one explanatory variable, the pizza''s diameter. Let''s compare linear regression
    with polynomial regression using the following datasets:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们假设自变量与响应变量之间的关系是线性的。这个假设并不总是成立。在本节中，我们将使用**多项式回归**，它是多元线性回归的一种特例，通过在模型中添加大于一的多项式项。通过添加多项式项转换训练数据，能够捕捉到实际的曲线关系，之后使用与多元线性回归相同的方式进行拟合。为了便于可视化，我们将再次仅使用一个解释变量——披萨的直径。让我们通过以下数据集将线性回归与多项式回归进行比较：
- en: '| Training Instance | Diameter (in inches) | Price (in dollars) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 训练实例 | 直径（英寸） | 价格（美元） |'
- en: '| --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 6 | 7 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 7 |'
- en: '| 2 | 8 | 9 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8 | 9 |'
- en: '| 3 | 10 | 13 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 13 |'
- en: '| 4 | 14 | 17.5 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 14 | 17.5 |'
- en: '| 5 | 18 | 18 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 18 | 18 |'
- en: '| Testing Instance | Diameter (in inches) | Price (in dollars) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 测试实例 | 直径（英寸） | 价格（美元） |'
- en: '| --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 6 | 7 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 7 |'
- en: '| 2 | 8 | 9 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 8 | 9 |'
- en: '| 3 | 10 | 13 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 13 |'
- en: '| 4 | 14 | 17.5 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 14 | 17.5 |'
- en: '**Quadratic regression**, or regression with a second order polynomial, is
    given by the following formula:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**二次回归**，即使用二次多项式进行回归，公式如下：'
- en: '![Polynomial regression](img/8365OS_02_22.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/8365OS_02_22.jpg)'
- en: 'We are using only one explanatory variable, but the model now has three terms
    instead of two. The explanatory variable has been transformed and added as a third
    term to the model to capture the curvilinear relationship. Also, note that the
    equation for polynomial regression is the same as the equation for multiple linear
    regression in vector notation. The `PolynomialFeatures` transformer can be used
    to easily add polynomial features to a feature representation. Let''s fit a model
    to these features, and compare it to the simple linear regression model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅使用一个解释变量，但现在模型有三个项，而不是两个。解释变量经过转换，并作为第三项添加到模型中，以捕捉曲线关系。此外，注意多项式回归的公式与多元线性回归的向量表示形式相同。`PolynomialFeatures`转换器可以用来轻松地将多项式特征添加到特征表示中。让我们将模型拟合到这些特征上，并与简单线性回归模型进行比较：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the output of the preceding script:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述脚本的输出：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The simple linear regression model is plotted with the solid line in the following
    figure. Plotted with a dashed line, the quadratic regression model visibly fits
    the training data better.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归模型用实线表示，绘制在下图中。用虚线表示的二次回归模型明显更好地拟合了训练数据。
- en: '![Polynomial regression](img/8365OS_02_23.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/8365OS_02_23.jpg)'
- en: 'The r-squared score of the simple linear regression model is 0.81; the quadratic
    regression model''s r-squared score is an improvement at 0.87\. While quadratic
    and cubic regression models are the most common, we can add polynomials of any
    degree. The following figure plots the quadratic and cubic models:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归模型的R平方值为0.81；二次回归模型的R平方值提高到0.87。虽然二次和三次回归模型是最常见的，我们也可以添加任意阶数的多项式。下图展示了二次回归和三次回归模型：
- en: '![Polynomial regression](img/8365OS_02_24.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/8365OS_02_24.jpg)'
- en: 'Now, let''s try an even higher-order polynomial. The plot in the following
    figure shows a regression curve created by a ninth-degree polynomial:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一个更高阶的多项式。下图中的回归曲线是由一个九次方多项式创建的：
- en: '![Polynomial regression](img/8365OS_02_25.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![多项式回归](img/8365OS_02_25.jpg)'
- en: The ninth-degree polynomial regression model fits the training data almost exactly!
    The model's r-squared score, however, is -0.09\. We created an extremely complex
    model that fits the training data exactly, but fails to approximate the real relationship.
    This problem is called **over-fitting**. The model should induce a general rule
    to map inputs to outputs; instead, it has memorized the inputs and outputs from
    the training data. As a result, the model performs poorly on test data. It predicts
    that a 16 inch pizza should cost less than $10, and an 18 inch pizza should cost
    more than $30\. This model exactly fits the training data, but fails to learn
    the real relationship between size and price.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 九次多项式回归模型几乎完美地拟合了训练数据！然而，模型的 R 方值为 -0.09。我们创建了一个非常复杂的模型，完美地拟合了训练数据，但未能逼近真实关系。这个问题称为
    **过拟合**。该模型应该生成一个通用规则，将输入映射到输出；然而，它只是记住了训练数据中的输入和输出。因此，模型在测试数据上的表现较差。它预测一个 16
    英寸的比萨饼应该少于 10 美元，而一个 18 英寸的比萨饼应该超过 30 美元。该模型完美拟合了训练数据，但未能学习到大小与价格之间的真实关系。
- en: Regularization
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: '**Regularization** is a collection of techniques that can be used to prevent
    over-fitting. Regularization adds information to a problem, often in the form
    of a penalty against complexity, to a problem. Occam''s razor states that a hypothesis
    with the fewest assumptions is the best. Accordingly, regularization attempts
    to find the simplest model that explains the data.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**是一组可以防止过拟合的技术。正则化通过向问题中添加信息，通常是对复杂性进行惩罚，从而对问题进行调整。奥卡姆剃刀原理指出，假设越少的假设越好。因此，正则化旨在找到最简单的模型来解释数据。'
- en: 'scikit-learn provides several regularized linear regression models. **Ridge
    regression**, also known as **Tikhonov regularization**, penalizes model parameters
    that become too large. Ridge regression modifies the residual sum of the squares
    cost function by adding the L2 norm of the coefficients, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了几种正则化的线性回归模型。**岭回归**，也称为 **提霍诺夫正则化**，惩罚变得过大的模型参数。岭回归通过将系数的 L2
    范数添加到残差平方和代价函数中来进行调整，具体如下：
- en: '![Regularization](img/8365OS_02_26.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![正则化](img/8365OS_02_26.jpg)'
- en: '![Regularization](img/8365OS_02_48.jpg) is a hyperparameter that controls the
    strength of the penalty. **Hyperparameters** are parameters of the model that
    are not learned automatically and must be set manually. As ![Regularization](img/8365OS_02_48.jpg)
    increases, the penalty increases, and the value of the cost function increases.
    When ![Regularization](img/8365OS_02_48.jpg) is equal to zero, ridge regression
    is equal to linear regression.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![正则化](img/8365OS_02_48.jpg) 是一个超参数，用于控制惩罚的强度。**超参数**是模型的参数，不会自动学习，必须手动设置。当
    ![正则化](img/8365OS_02_48.jpg) 增加时，惩罚增强，代价函数的值也增加。当 ![正则化](img/8365OS_02_48.jpg)
    等于零时，岭回归等同于线性回归。'
- en: 'scikit-learn also provides an implementation of the **Least Absolute Shrinkage
    and Selection Operator** (**LASSO**). LASSO penalizes the coefficients by adding
    their L1 norm to the cost function, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 还提供了 **最小绝对收缩和选择算子**（**LASSO**）的实现。LASSO 通过将其 L1 范数添加到代价函数中来惩罚系数，具体如下：
- en: '![Regularization](img/8365OS_02_27.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![正则化](img/8365OS_02_27.jpg)'
- en: The LASSO produces sparse parameters; most of the coefficients will become zero,
    and the model will depend on a small subset of the features. In contrast, ridge
    regression produces models in which most parameters are small but nonzero. When
    explanatory variables are correlated, the LASSO will shrink the coefficients of
    one variable toward zero. Ridge regression will shrink them more uniformly. Finally,
    scikit-learn provides an implementation of **elastic net** regularization, which
    linearly combines the L1 and L2 penalties used by the LASSO and ridge regression.
    That is, the LASSO and ridge regression are both special cases of the elastic
    net method in which the hyperparameter for either the L1 or L2 penalty is equal
    to zero.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO 会产生稀疏的参数；大多数系数将变为零，模型将依赖于特征的一个小子集。与此不同的是，岭回归会产生模型，其中大多数参数虽然小，但不为零。当解释变量之间存在相关性时，LASSO
    会将一个变量的系数收缩到零。岭回归则会更均匀地收缩它们。最后，scikit-learn 提供了 **弹性网** 正则化的实现，该方法线性地结合了 LASSO
    和岭回归使用的 L1 和 L2 惩罚。也就是说，LASSO 和岭回归是弹性网方法的特例，其中 L1 或 L2 惩罚的超参数之一为零。
- en: Applying linear regression
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用线性回归
- en: We have worked through a toy problem to learn how linear regression models relationships
    between explanatory and response variables. Now we'll use a real data set and
    apply linear regression to an important task. Assume that you are at a party,
    and that you wish to drink the best wine that is available. You could ask your
    friends for recommendations, but you suspect that they will drink any wine, regardless
    of its provenance. Fortunately, you have brought pH test strips and other tools
    to measure various physicochemical properties of wine—it is, after all, a party.
    We will use machine learning to predict the quality of the wine based on its physicochemical
    attributes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个简单的问题学习了如何使用线性回归模型建立解释变量与响应变量之间的关系。现在，我们将使用一个真实的数据集，并应用线性回归来解决一个重要任务。假设你正在参加一个聚会，想要喝到最好喝的葡萄酒。你可以向朋友们请求推荐，但你怀疑他们会喝任何葡萄酒，无论它的来源如何。幸运的是，你带了
    pH 测试条和其他工具来测量葡萄酒的各种物理化学属性——毕竟，这是一个聚会。我们将使用机器学习根据葡萄酒的物理化学属性来预测其质量。
- en: The UCI Machine Learning Repository's Wine data set measures eleven physicochemical
    attributes, including the pH and alcohol content, of 1,599 different red wines.
    Each wine's quality has been scored by human judges. The scores range from zero
    to ten; zero is the worst quality and ten is the best quality. The data set can
    be downloaded from [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine).
    We will approach this problem as a regression task and regress the wine's quality
    onto one or more physicochemical attributes. The response variable in this problem
    takes only integer values between 0 and 10; we could view these as discrete values
    and approach the problem as a multiclass classification task. In this chapter,
    however, we will view the response variable as a continuous value.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库的葡萄酒数据集测量了 1,599 种不同红酒的十一种物理化学属性，包括 pH 值和酒精含量。每种酒的质量已由人工评审员打分，分数范围从零到十；零表示最差质量，十表示最佳质量。数据集可以从[https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)下载。我们将把这个问题视为一个回归任务，并将葡萄酒的质量回归到一个或多个物理化学属性上。这个问题中的响应变量仅取
    0 到 10 之间的整数值；我们可以将其视为离散值，并将问题作为多类分类任务来处理。然而，在本章中，我们将把响应变量视为一个连续值。
- en: Exploring the data
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索数据
- en: '| Fixed acidity | Volatile acidity | Citric acidity | Residual sugar | Chlorides
    | Free sulfur dioxide | Total sulfur dioxide | Density | pH | Sulphates | Alcohol
    | Quality |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 固定酸度 | 挥发酸度 | 柠檬酸度 | 残留糖分 | 氯化物 | 游离二氧化硫 | 总二氧化硫 | 密度 | pH | 硫酸盐 | 酒精 | 质量
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 7.4 | 0.7 | 0 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 7.4 | 0.7 | 0 | 1.9 | 0.076 | 11 | 34 | 0.9978 | 3.51 | 0.56 | 9.4 | 5 |'
- en: '| 7.8 | 0.88 | 0 | 2.6 | 0.098 | 25 | 67 | 0.9968 | 3.2 | 0.68 | 9.8 | 5 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 7.8 | 0.88 | 0 | 2.6 | 0.098 | 25 | 67 | 0.9968 | 3.2 | 0.68 | 9.8 | 5 |'
- en: '| 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15 | 54 | 0.997 | 3.26 | 0.65 | 9.8 | 5
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 7.8 | 0.76 | 0.04 | 2.3 | 0.092 | 15 | 54 | 0.997 | 3.26 | 0.65 | 9.8 | 5
    |'
- en: '| 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17 | 60 | 0.998 | 3.16 | 0.58 | 9.8 |
    6 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 11.2 | 0.28 | 0.56 | 1.9 | 0.075 | 17 | 60 | 0.998 | 3.16 | 0.58 | 9.8 |
    6 |'
- en: scikit-learn is intended to be a tool to build machine learning systems; its
    capabilities to explore data are impoverished compared to those of packages such
    as SPSS Statistics or the R language. We will use pandas, an open source data
    analysis library for Python, to generate descriptive statistics from the data;
    we will use these statistics to inform some of the design decisions of our model.
    pandas introduces Python to some concepts from R such as the dataframe, a two-dimensional,
    tabular, and heterogeneous data structure. Using pandas for data analysis is the
    topic of several books; we will use only a few basic methods in the following
    examples.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 是一个用于构建机器学习系统的工具；与 SPSS Statistics 或 R 语言等软件包相比，它在数据探索方面的能力较为贫弱。我们将使用
    pandas，这是一个开源的 Python 数据分析库，用于从数据中生成描述性统计信息；我们将使用这些统计信息来指导模型的一些设计决策。pandas 将一些来自
    R 的概念引入了 Python，例如数据框（dataframe），它是一个二维、表格化且异质的数据结构。使用 pandas 进行数据分析是多本书籍的主题；在以下示例中，我们将只使用一些基本方法。
- en: 'First, we will load the data set and review some basic summary statistics for
    the variables. The data is provided as a `.csv` file. Note that the fields are
    separated by semicolons rather than commas):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载数据集并查看一些基本的摘要统计信息。数据以 `.csv` 文件提供。请注意，字段是用分号而不是逗号分隔的。
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `pd.read_csv()` function is a convenience utility that loads the `.csv`
    file into a dataframe. The `Dataframe.describe()` method calculates summary statistics
    for each column of the dataframe. The preceding code sample shows the summary
    statistics for only the last four columns of the dataframe. Note the summary for
    the quality variable; most of the wines scored five or six. Visualizing the data
    can help indicate if relationships exist between the response variable and the
    explanatory variables. Let''s use `matplotlib` to create some scatter plots. Consider
    the following code snippet:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`pd.read_csv()`函数是一个方便的工具，可以将`.csv`文件加载到数据框中。`Dataframe.describe()`方法计算数据框中每一列的汇总统计信息。前面的代码示例只显示了数据框最后四列的汇总统计信息。注意质量变量的汇总情况；大多数酒的评分是五分或六分。可视化数据有助于表明响应变量与解释变量之间是否存在关系。我们可以使用`matplotlib`来创建一些散点图。请参考以下代码片段：'
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding code snippet is shown in the following figure:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码片段的输出结果如下图所示：
- en: '![Exploring the data](img/8365OS_02_28.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![探索数据](img/8365OS_02_28.jpg)'
- en: 'A weak positive relationship between the alcohol content and quality is visible
    in the scatter plot in the preceding figure; wines that have high alcohol content
    are often high in quality. The following figure reveals a negative relationship
    between volatile acidity and quality:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前图的散点图中，酒精含量与质量之间呈现出微弱的正相关关系；高酒精含量的酒往往质量较高。下图则揭示了挥发性酸度与质量之间的负相关关系：
- en: '![Exploring the data](img/8365OS_02_29.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![探索数据](img/8365OS_02_29.jpg)'
- en: These plots suggest that the response variable depends on multiple explanatory
    variables; let's model the relationship with multiple linear regression. How can
    we decide which explanatory variables to include in the model? `Dataframe.corr()`
    calculates a pairwise correlation matrix. The correlation matrix confirms that
    the strongest positive correlation is between the alcohol and quality, and that
    quality is negatively correlated with volatile acidity, an attribute that can
    cause wine to taste like vinegar. To summarize, we have hypothesized that good
    wines have high alcohol content and do not taste like vinegar. This hypothesis
    seems sensible, though it suggests that wine aficionados may have less sophisticated
    palates than they claim.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表表明响应变量依赖于多个解释变量；我们来用多元线性回归建模这种关系。我们如何决定在模型中包含哪些解释变量呢？`Dataframe.corr()`计算一对一的相关矩阵。相关矩阵确认了酒精含量和质量之间存在最强的正相关关系，而质量与挥发性酸度呈负相关，挥发性酸度是导致酒品有醋味的属性。总结来说，我们假设优质的酒具有较高的酒精含量，并且不带有醋味。这个假设似乎有道理，尽管它暗示了酒类爱好者的味觉可能比他们声称的要不那么精细。
- en: Fitting and evaluating the model
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟合和评估模型
- en: 'Now we will split the data into training and testing sets, train the regressor,
    and evaluate its predictions:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将数据分为训练集和测试集，训练回归器，并评估其预测结果：
- en: '[PRE14]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: First, we loaded the data using pandas and separated the response variable from
    the explanatory variables. Next, we used the `train_test_split` function to randomly
    partition the data into training and test sets. The proportions of the data for
    both partitions can be specified using keyword arguments. By default, 25 percent
    of the data is assigned to the test set. Finally, we trained the model and evaluated
    it on the test set.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用pandas加载了数据，并将响应变量与解释变量分开。接着，我们使用`train_test_split`函数将数据随机分割成训练集和测试集。可以使用关键字参数指定两个分区的数据比例。默认情况下，25%的数据被分配给测试集。最后，我们训练了模型并在测试集上进行了评估。
- en: 'The r-squared score of 0.35 indicates that 35 percent of the variance in the
    test set is explained by the model. The performance might change if a different
    75 percent of the data is partitioned to the training set. We can use cross-validation
    to produce a better estimate of the estimator''s performance. Recall from chapter
    one that each cross-validation round trains and tests different partitions of
    the data to reduce variability:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: r平方值为0.35，表示测试集中的35%的方差由模型解释。如果将不同的75%的数据分配到训练集，模型的表现可能会有所变化。我们可以使用交叉验证来更好地估计估计器的性能。回顾第一章，每一轮交叉验证都训练并测试数据的不同分割，以减少变异性：
- en: '[PRE15]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `cross_val_score` helper function allows us to easily perform cross-validation
    using the provided data and estimator. We specified a five-fold cross validation
    using the `cv` keyword argument, that is, each instance will be randomly assigned
    to one of the five partitions. Each partition will be used to train and test the
    model. `cross_val_score` returns the value of the estimator's `score` method for
    each round. The r-squared scores range from 0.13 to 0.36! The mean of the scores,
    0.29, is a better estimate of the estimator's predictive power than the r-squared
    score produced from a single train / test split.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`cross_val_score` 辅助函数允许我们使用提供的数据和估计器轻松执行交叉验证。我们使用 `cv` 关键字参数指定了五折交叉验证，即每个实例将随机分配到五个分区之一。每个分区将用于训练和测试模型。
    `cross_val_score` 返回每轮估计器 `score` 方法的值。 R平方分数的范围为0.13到0.36！分数的均值0.29比从单一训练/测试拆分产生的R平方分数更好地估计了估计器的预测能力。'
- en: 'Let''s inspect some of the model''s predictions and plot the true quality scores
    against the predicted scores:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些模型的预测结果，并将真实质量分数与预测分数绘制在一起：
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following figure shows the output of the preceding code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了前述代码的输出：
- en: '![Fitting and evaluating the model](img/8365OS_02_30.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![拟合和评估模型](img/8365OS_02_30.jpg)'
- en: As expected, few predictions exactly match the true values of the response variable.
    The model is also better at predicting the qualities of average wines, since most
    of the training data is for average wines.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 预期地，很少有预测能完全匹配响应变量的真实值。由于大部分训练数据是关于普通葡萄酒的，该模型在预测普通葡萄酒的质量时表现更好。
- en: Fitting models with gradient descent
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度下降拟合模型
- en: 'In the examples in this chapter, we analytically solved the values of the model''s
    parameters that minimize the cost function with the following equation:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的示例中，我们通过以下方程解析地求解了模型参数的值，该方程最小化了成本函数的值：
- en: '![Fitting models with gradient descent](img/8365OS_02_21.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![使用梯度下降拟合模型](img/8365OS_02_21.jpg)'
- en: Recall that ![Fitting models with gradient descent](img/8365OS_02_44.jpg) is
    the matrix of the values of the explanatory variables for each training example.
    The dot product of ![Fitting models with gradient descent](img/8365OS_02_49.jpg)
    results in a square matrix with dimensions ![Fitting models with gradient descent](img/8365OS_02_50.jpg),
    where ![Fitting models with gradient descent](img/8365OS_02_40.jpg) is equal to
    the number of explanatory variables. The computational complexity of inverting
    this square matrix is nearly cubic in the number of explanatory variables. While
    the number of explanatory variables has been small in this chapter's examples,
    this inversion can be prohibitively costly for problems with tens of thousands
    of explanatory variables, which we will encounter in the following chapters. Furthermore,
    ![Fitting models with gradient descent](img/8365OS_02_49.jpg) cannot be inverted
    if its determinant is equal to zero. In this section, we will discuss another
    method to efficiently estimate the optimal values of the model's parameters called
    **gradient descent**. Note that our definition of a good fit has not changed;
    we will still use gradient descent to estimate the values of the model's parameters
    that minimize the value of the cost function.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，![使用梯度下降拟合模型](img/8365OS_02_44.jpg) 是每个训练示例的解释变量值的矩阵。 ![使用梯度下降拟合模型](img/8365OS_02_49.jpg)
    的点积结果是一个维度为 ![使用梯度下降拟合模型](img/8365OS_02_50.jpg) 的方阵，其中 ![使用梯度下降拟合模型](img/8365OS_02_40.jpg)
    等于解释变量的数量。反转这个方阵的计算复杂度几乎是解释变量数量的立方。虽然本章的示例中解释变量数量较少，但在后续章节中遇到数万个解释变量的问题时，这种反转可能成本高昂。此外，如果
    ![使用梯度下降拟合模型](img/8365OS_02_49.jpg) 的行列式为零，它就无法反转。在本节中，我们将讨论另一种有效估算模型参数最优值的方法，称为**梯度下降**。请注意，我们对良好拟合的定义没有改变；我们仍将使用梯度下降估算模型参数的值，以最小化成本函数的值。
- en: Gradient descent is sometimes described by the analogy of a blindfolded man
    who is trying to find his way from somewhere on a mountainside to the lowest point
    of the valley. He cannot see the topography, so he takes a step in the direction
    with the steepest decline. He then takes another step, again in the direction
    with the steepest decline. The sizes of his steps are proportional to the steepness
    of the terrain at his current position. He takes big steps when the terrain is
    steep, as he is confident that he is still near the peak and that he will not
    overshoot the valley's lowest point. The man takes smaller steps as the terrain
    becomes less steep. If he were to continue taking large steps, he may accidentally
    step over the valley's lowest point. He would then need to change direction and
    step toward the lowest point of the valley again. By taking decreasingly large
    steps, he can avoid stepping back and forth over the valley's lowest point. The
    blindfolded man continues to walk until he cannot take a step that will decrease
    his altitude; at this point, he has found the bottom of the valley.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降有时通过一个类比来描述：假设一个蒙着眼睛的人正试图从山坡上的某个地方走到山谷的最低点。他无法看到地形，因此他每次都朝着坡度最陡的方向迈出一步。接着他再迈出一步，依然朝着坡度最陡的方向。每一步的大小与当前位置的坡度成正比。当地形陡峭时，他迈出较大的一步，因为他相信自己仍然接近山顶，并且不会超过山谷的最低点。当地形变得不那么陡峭时，他会迈出较小的步伐。如果他继续迈出大步，可能会不小心越过山谷的最低点。此时他需要改变方向，重新朝着山谷的最低点迈进。通过逐步减小步伐，他可以避免反复跨越山谷的最低点。蒙眼的人继续走，直到他再也无法迈出一步使自己的高度降低；此时，他就找到了山谷的底部。
- en: 'Formally, gradient descent is an optimization algorithm that can be used to
    estimate the local minimum of a function. Recall that we are using the residual
    sum of squares cost function, which is given by the following equation:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，梯度下降是一种优化算法，可以用来估计函数的局部最小值。回想一下，我们使用的是残差平方和（RSS）代价函数，其公式如下：
- en: '![Fitting models with gradient descent](img/8365OS_02_06.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![使用梯度下降拟合模型](img/8365OS_02_06.jpg)'
- en: We can use gradient descent to find the values of the model's parameters that
    minimize the value of the cost function. Gradient descent iteratively updates
    the values of the model's parameters by calculating the partial derivative of
    the cost function at each step. The calculus required to compute the partial derivative
    of the cost function is beyond the scope of this book, and is also not required
    to work with scikit-learn. However, having an intuition for how gradient descent
    works can help you use it effectively.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用梯度下降来找到模型参数的值，从而最小化代价函数的值。梯度下降通过计算代价函数在每一步的偏导数，迭代更新模型参数的值。计算代价函数的偏导数所需的微积分超出了本书的范围，而且在使用
    scikit-learn 时也不需要了解这些内容。然而，理解梯度下降如何工作可以帮助你更有效地使用它。
- en: It is important to note that gradient descent estimates the local minimum of
    a function. A three-dimensional plot of the values of a convex cost function for
    all possible values of the parameters looks like a bowl. The bottom of the bowl
    is the sole local minimum. Non-convex cost functions can have many local minima,
    that is, the plots of the values of their cost functions can have many peaks and
    valleys. Gradient descent is only guaranteed to find the local minimum; it will
    find a valley, but will not necessarily find the lowest valley. Fortunately, the
    residual sum of the squares cost function is convex.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，梯度下降估计的是函数的局部最小值。对于一个凸的代价函数，其值在所有可能参数的取值下的三维图形看起来像一个碗。碗的底部是唯一的局部最小值。非凸代价函数可能有多个局部最小值，也就是说，它们的代价函数值的图形可能会有多个峰值和谷值。梯度下降只保证找到局部最小值；它会找到一个谷底，但不一定是最低的那个谷底。幸运的是，残差平方和代价函数是凸的。
- en: An important hyperparameter of gradient descent is the learning rate, which
    controls the size of the blindfolded man's steps. If the learning rate is small
    enough, the cost function will decrease with each iteration until gradient descent
    has converged on the optimal parameters. As the learning rate decreases, however,
    the time required for gradient descent to converge increases; the blindfolded
    man will take longer to reach the valley if he takes small steps than if he takes
    large steps. If the learning rate is too large, the man may repeatedly overstep
    the bottom of the valley, that is, gradient descent could oscillate around the
    optimal values of the parameters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的一个重要超参数是学习率，它控制盲人行走的步伐大小。如果学习率足够小，代价函数将随着每次迭代而减少，直到梯度下降收敛到最优参数。然而，随着学习率的减小，梯度下降收敛所需的时间也会增加；如果盲人迈小步，他将比迈大步花更多时间到达山谷。如果学习率过大，盲人可能会反复越过山谷的底部，也就是说，梯度下降可能在参数的最优值附近震荡。
- en: There are two varieties of gradient descent that are distinguished by the number
    of training instances that are used to update the model parameters in each training
    iteration. **Batch gradient descent**, which is sometimes called only gradient
    descent, uses all of the training instances to update the model parameters in
    each iteration. **Stochastic Gradient Descent** (**SGD**), in contrast, updates
    the parameters using only a single training instance in each iteration. The training
    instance is usually selected randomly. Stochastic gradient descent is often preferred
    to optimize cost functions when there are hundreds of thousands of training instances
    or more, as it will converge more quickly than batch gradient descent. Batch gradient
    descent is a deterministic algorithm, and will produce the same parameter values
    given the same training set. As a stochastic algorithm, SGD can produce different
    parameter estimates each time it is run. SGD may not minimize the cost function
    as well as gradient descent because it uses only single training instances to
    update the weights. Its approximation is often close enough, particularly for
    convex cost functions such as residual sum of squares.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降有两种类型，它们通过每次训练迭代中用于更新模型参数的训练实例数量来区分。**批量梯度下降**（有时简称为梯度下降）使用所有训练实例来更新模型参数。**随机梯度下降**（**SGD**）则通过每次迭代仅使用一个训练实例来更新参数。训练实例通常是随机选择的。由于随机梯度下降比批量梯度下降收敛得更快，因此当训练实例数达到数十万或更多时，通常会选择使用它来优化代价函数。批量梯度下降是一个确定性算法，给定相同的训练集，它会产生相同的参数值。作为一种随机算法，SGD每次运行时可能会产生不同的参数估计值。由于SGD每次仅使用单个训练实例来更新权重，它可能无法像梯度下降那样有效地最小化代价函数。尽管如此，它的逼近通常足够接近，特别是对于像残差平方和这样的凸代价函数。
- en: 'Let''s use stochastic gradient descent to estimate the parameters of a model
    with scikit-learn. `SGDRegressor` is an implementation of SGD that can be used
    even for regression problems with hundreds of thousands or more features. It can
    be used to optimize different cost functions to fit different linear models; by
    default, it will optimize the residual sum of squares. In this example, we will
    predict the prices of houses in the Boston Housing data set from 13 explanatory
    variables:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用随机梯度下降来估计使用 scikit-learn 构建的模型的参数。`SGDRegressor` 是一种 SGD 的实现，甚至可以用于具有数十万或更多特征的回归问题。它可以用来优化不同的代价函数，以拟合不同的线性模型；默认情况下，它将优化残差平方和。在这个例子中，我们将预测波士顿住房数据集中基于
    13 个解释变量的房价：
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'scikit-learn provides a convenience function for loading the data set. First,
    we split the data into training and testing sets using `train_test_split`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一个便捷的函数来加载数据集。首先，我们使用 `train_test_split` 将数据分为训练集和测试集：
- en: '[PRE18]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we scaled the features using `StandardScaler`, which we will describe
    in detail in the next chapter:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `StandardScaler` 对特征进行标准化，这将在下一章中详细介绍：
- en: '[PRE19]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we trained the estimator, and evaluated it using cross validation
    and the test set. The following is the output of the script:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练了估计器，并通过交叉验证和测试集进行了评估。以下是脚本的输出：
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter we discussed three cases of linear regression. We worked through
    an example of simple linear regression, which models the relationship between
    a single explanatory variable and a response variable using a line. We then discussed
    multiple linear regression, which generalizes simple linear regression to model
    the relationship between multiple explanatory variables and a response variable.
    Finally, we described polynomial regression, a special case of multiple linear
    regression that models non-linear relationships between explanatory variables
    and a response variable. These three models can be viewed as special cases of
    the generalized linear model, a framework for model linear relationships, which
    we will discuss in more detail in [Chapter 4](ch04.html "Chapter 4. From Linear
    Regression to Logistic Regression"), *From Linear Regression to Logistic Regression*.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了三种线性回归的案例。我们通过一个简单线性回归的例子，展示了如何使用一条直线来建模单一解释变量与响应变量之间的关系。接着，我们讨论了多元线性回归，它将简单线性回归推广到多个解释变量与响应变量之间关系的建模。最后，我们描述了多项式回归，这是多元线性回归的一个特例，用于建模解释变量与响应变量之间的非线性关系。这三种模型可以视为广义线性模型的特例，广义线性模型是一个用于建模线性关系的框架，我们将在[第4章](ch04.html
    "Chapter 4. From Linear Regression to Logistic Regression")《从线性回归到逻辑回归》中详细讨论。
- en: We assessed the fitness of models using the residual sum of squares cost function
    and discussed two methods to learn the values of a model's parameters that minimize
    the cost function. First, we solved the values of the model's parameters analytically.
    We then discussed gradient descent, a method that can efficiently estimate the
    optimal values of the model's parameters even when the model has a large number
    of features. The features in this chapter's examples were simple measurements
    of their explanatory variables; it was easy to use them in our models. In the
    next chapter, you will learn to create features for different types of explanatory
    variables, including categorical variables, text, and images.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用残差平方和成本函数来评估模型的拟合程度，并讨论了两种方法来学习能够最小化成本函数的模型参数值。首先，我们通过解析方法求解了模型参数的值。接着，我们讨论了梯度下降法，这是一种能够高效估计模型参数最优值的方法，即使模型有大量特征时也能应用。在本章的例子中，特征是解释变量的简单测量，因此很容易将它们应用到我们的模型中。在下一章，你将学习如何为不同类型的解释变量创建特征，包括分类变量、文本和图像。
