- en: '*Chapter 7*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 7 章*'
- en: Topic Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you''ll be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将能够：
- en: Perform basic cleaning techniques for textual data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为文本数据执行基本的清理技术
- en: Evaluate latent Dirichlet allocation models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估隐含狄利克雷分配模型
- en: Execute non-negative matrix factorization models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行非负矩阵分解模型
- en: Interpret the results of topic models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释主题模型的结果
- en: Identify the best topic model for the given scenario
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为给定场景识别最佳主题模型
- en: In this chapter, we will see how topic modeling provides insights into the underlying
    structure of documents.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何通过主题建模深入了解文档的潜在结构。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Topic modeling is one facet of **natural language processing** (**NLP**), the
    field of computer science exploring the relationship between computers and human
    language, which has been increasing in popularity with the increased availability
    of textual datasets. NLP can deal with language in almost any form, including
    text, speech, and images. Besides topic modeling, sentiment analysis, object character
    recognition, and lexical semantics are noteworthy NLP algorithms. Nowadays, the
    data being collected and needing analysis less frequently comes in standard tabular
    forms and more frequently coming in less structured forms, including documents,
    images, and audio files. As such, successful data science practitioners need to
    be fluent in methodologies used for handling these diverse datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是**自然语言处理**（**NLP**）的一个方面，这是计算机科学领域探索计算机与人类语言关系的一个领域，随着文本数据集的增加可用性的增加而日益流行。NLP可以处理几乎任何形式的语言，包括文本、语音和图像。除了主题建模外，情感分析、对象字符识别和词汇语义是值得注意的NLP算法。如今，收集和需要分析的数据很少是以标准表格形式出现，而更频繁地以较少结构化的形式出现，包括文档、图像和音频文件。因此，成功的数据科学从业者需要精通处理这些多样化数据集的方法论。
- en: 'Here is a demonstration of identifying words in a text and assigning them to
    topics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，演示如何识别文本中的单词并将其分配给主题：
- en: '![Figure 7.1: Example of identifying words in a text and assigning them to
    topics](img/C12626_07_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1：示例，识别文本中的单词并将它们分配给主题](img/C12626_07_01.jpg)'
- en: 'Figure 7.1: Example of identifying words in a text and assigning them to topics'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.1：示例，识别文本中的单词并将它们分配给主题
- en: Your immediate question is probably *what are topics*? Let's answer that question
    with an example. You could imagine, or perhaps have noticed, that on days when
    major events take place, such as national elections, natural disasters, or sport
    championships, the posts on social media websites tend to focus on those events.
    Posts generally reflect, in some way, the day's events, and they will do so in
    varying ways. Posts can and will have a number of divergent viewpoints. If we
    had tweets about the World Cup final, the topics of those tweets could cover divergent
    viewpoints, ranging from the quality of the refereeing to fan behavior. In the
    United States, the president delivers an annual speech in mid to late January
    called the State of the Union. With sufficient numbers of social media posts,
    we would be able to infer or predict high-level reactions – topics – to the speech
    from the social media community by grouping posts using the specific keywords
    contained in them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能立即问的问题是*什么是主题*？让我们通过一个例子来回答这个问题。你可以想象，或者可能已经注意到，在发生重大事件的日子，例如国家选举、自然灾害或体育冠军赛，社交媒体网站上的帖子往往会集中在这些事件上。帖子通常以某种方式反映当天的事件，并且会以不同的方式这样做。帖子可以并且将会有许多不同的观点。如果我们有关于世界杯决赛的推文，这些推文的主题可能涵盖各种不同的观点，从裁判的裁判质量到球迷行为。在美国，总统在每年的一月中旬到晚些时候发表一次称为国情咨文的年度演讲。通过足够数量的社交媒体帖子，我们将能够通过使用其中包含的特定关键词对帖子进行分组来推断或预测社交媒体社区对演讲的高级反应
    - 主题。
- en: Topic Models
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题模型
- en: Topic models fall into the unsupervised learning bucket because, almost always,
    the topics being identified are not known in advance. So, no target exists on
    which we can perform regression or classification modeling. In terms of unsupervised
    learning, topic models most resemble clustering algorithms, specifically k-means
    clustering. You'll recall that, in k-means clustering, the number of clusters
    is established first and then the model assigns each data point to one of the
    predetermined number of clusters. The same is generally true in topic models.
    We select the number of topics at the start and then the model isolates the words
    that form that number of topics. This is a great jumping-off point for a high-level
    topic modeling overview.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型属于无监督学习类别，因为几乎总是无法事先知道要识别的主题。因此，没有目标可以进行回归或分类建模。在无监督学习中，主题模型最像聚类算法，特别是k-means聚类。你会记得，在k-means聚类中，首先确定聚类的数量，然后模型将每个数据点分配到预定的某个聚类中。主题模型通常也是如此。我们在开始时选择主题的数量，然后模型会提取出构成这些主题的词汇。这是一个很好的出发点，用于对主题建模进行高层次的概述。
- en: 'Before that, let''s check that the correct environment and libraries are installed
    and ready for use. The following table lists the required libraries and their
    main purposes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，我们先检查一下是否安装了正确的环境和库，并准备好使用。下表列出了所需的库及其主要用途：
- en: '![Figure 7.2: Table showing different libraries and their use](img/C12626_07_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2：显示不同库及其用途的表格](img/C12626_07_02.jpg)'
- en: 'Figure 7.2: Table showing different libraries and their use'
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.2：显示不同库及其用途的表格
- en: 'Exercise 27: Setting Up the Environment'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 27：设置环境
- en: 'To check whether the environment is ready for topic modeling, we will perform
    several steps. The first of which involves loading all the libraries that will
    be needed in this chapter:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查环境是否准备好进行主题建模，我们将执行几个步骤。其中第一步是加载本章将需要的所有库：
- en: Note
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If any or all of these libraries are not currently installed, install the required
    packages via the command line using `pip`. For example, `pip install langdetect`,
    if not installed already.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些库中有任何一个或全部未安装，请通过命令行使用`pip`安装所需的软件包。例如，若未安装，运行`pip install langdetect`。
- en: Open a new Jupyter notebook.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter笔记本。
- en: 'Import the requisite libraries:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that not all of these packages are used for cleaning the data; some of
    them are used in the actual modeling, but it is nice to import all of the required
    libraries at once, so let's take care of all library importing now.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，并非所有这些库都用于清理数据；其中一些库用于实际建模，但一次性导入所有所需的库是很方便的，所以我们现在就将所有库的导入处理好。
- en: 'Libraries not yet installed will return the following error:![Figure 7.3: Library
    not installed error](img/C12626_07_03.jpg)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尚未安装的库将返回以下错误：![图 7.3：库未安装错误](img/C12626_07_03.jpg)
- en: 'Figure 7.3: Library not installed error'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.3：库未安装错误
- en: If this error is returned, install the relevant libraries via the command line
    as previously discussed. Once successfully installed, rerun the library import
    process using `import`.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果返回此错误，请按照之前讨论的方法通过命令行安装相关库。成功安装后，使用`import`重新运行库导入过程。
- en: 'Certain textual data cleaning and preprocessing processes require word dictionaries
    (more to come). In this step, we will install two of these dictionaries. If the
    `nltk` library is imported, the following code can be executed:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 某些文本数据清理和预处理过程需要词典（更多内容稍后介绍）。在此步骤中，我们将安装其中的两个词典。如果导入了`nltk`库，可以执行以下代码：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_07_04.jpg)'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_04.jpg)'
- en: 'Figure 7.4: Importing libraries and downloading dictionaries'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.4：导入库并下载词典
- en: 'Run the `matplotlib` magic and specify inline so that the plots print inside
    the notebook:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`matplotlib`魔法命令并指定inline，以便图表显示在笔记本中：
- en: '[PRE2]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The notebook and environment are now set and ready for data loading.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本和环境已设置好，准备好加载数据。
- en: A High-Level Overview of Topic Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主题模型的高层次概述
- en: When it comes to analyzing large volumes of potentially related text data, topic
    models are one go-to approach. By related, we mean that the documents ideally
    come from the same source. That is, survey results, tweets, and newspaper articles
    would not normally be analyzed simultaneously in the same model. It is certainly
    possible to analyze them together, but the results are liable to be very vague
    and therefore meaningless. To run any topic model, the only data requirement is
    the documents themselves. No additional data, meta or otherwise, is required.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析大量可能相关的文本数据时，主题模型是一个常用方法。所谓相关，是指文档理想情况下来自相同来源。也就是说，调查结果、推文和新闻文章通常不会在同一个模型中同时进行分析。当然，也可以将它们一起分析，但结果可能非常模糊，因此没有意义。运行任何主题模型的唯一数据要求是文档本身。不需要额外的数据，无论是元数据还是其他类型的数据。
- en: In the simplest terms, topic models identify the abstract topics (also known
    as themes) in a collection of documents, referred to as a **corpus**, using the
    words contained in the documents. That is, if a sentence contains the words salary,
    employee, and meeting, it would be safe to assume that that sentence is about,
    or that its topic is, work. It is of note that the documents making up the corpus
    need not be documents as traditionally defined – think letters or contracts. A
    document could be anything containing text, including tweets, news headlines,
    or transcribed speech.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，主题模型通过分析文档中的词汇，识别一组文档（称为**语料库**）中的抽象主题（也称为主题）。也就是说，如果一句话中包含“薪水”、“员工”和“会议”等词汇，就可以推测这句话的主题是与工作相关的。需要注意的是，构成语料库的文档不一定是传统意义上的文档——可以是信件或合同等。文档可以是任何包含文本的内容，包括推文、新闻标题或转录的演讲。
- en: 'Topic models assume that words in the same document are related and use that
    assumption to define abstract topics by finding groups of words that repeatedly
    appear in close proximity. In this way, these models are classic pattern recognition
    algorithms where the patterns being detected are made up of words. The general
    topic modeling algorithm has four main steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型假设同一文档中的词汇是相关的，并利用这一假设通过寻找反复出现在相近位置的词汇组来定义抽象主题。通过这种方式，这些模型属于经典的模式识别算法，其中被检测到的模式是由词汇组成的。一般的主题建模算法包含四个主要步骤：
- en: Determine the number of topics.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定主题的数量。
- en: Scan the documents and identify co-occurring words or phrases.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描文档并识别共现的词汇或短语。
- en: Auto-learn groups (or clusters) of words characterizing the documents.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动学习表征文档的词汇组（或集群）。
- en: Output abstract topics characterizing the corpus as word groupings.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出表征语料库的抽象主题，作为词汇组合。
- en: As step 1 notes, the number of topics needs to be selected before fitting the
    model. Selecting an appropriate number of topics can be tricky, but, as is the
    case with most machine learning models, this parameter can be optimized by fitting
    several models using different numbers of topics and selecting the best model
    based on some performance metric. We'll dive into this process again later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如步骤 1 所示，在拟合模型之前需要选择主题的数量。选择合适的主题数量可能会有些棘手，但与大多数机器学习模型一样，这个参数可以通过拟合多个使用不同主题数量的模型，并基于某些性能指标选择最佳模型来优化。稍后我们会再深入讨论这个过程。
- en: 'The following is the generic topic modeling workflow:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通用主题建模工作流程：
- en: '![Figure 7.5: The generic topic modeling workflow](img/C12626_07_05.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5: 通用主题建模工作流程](img/C12626_07_05.jpg)'
- en: 'Figure 7.5: The generic topic modeling workflow'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 7.5: 通用主题建模工作流程'
- en: It is important to select the best number of topics possible, as this parameter
    can majorly impact topic coherence. This is because the model finds groups of
    words that best fit the corpus under the constraint of a predefined number of
    topics. If the number of topics is too high, the topics become inappropriately
    narrow. Overly specific topics are referred to as **over-cooked**. Likewise, if
    the number of topics is too low, the topics become generic and vague. These types
    of topics are considered **under-cooked**. Over-cooked and under-cooked topics
    can sometimes be fixed by decreasing or increasing the number of topics, respectively.
    A frequent and unavoidable result of topic models is that, frequently, at least
    one topic will be problematic.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的主题数非常重要，因为这一参数会对主题的连贯性产生重大影响。这是因为模型会在预定义的主题数限制下，找到最适合语料库的词组。如果主题数过高，主题会变得过于狭窄。过于具体的主题被称为**过度加工**。同样地，如果主题数过低，主题会变得过于通用和模糊。这些类型的主题被认为是**加工不足**。过度加工和加工不足的主题有时可以通过分别减少或增加主题数来修正。话题模型的一个常见且不可避免的结果是，通常至少有一个话题会出现问题。
- en: 'A key aspect of topic models is that they do not produce specific one-word
    or one-phrase topics, but rather collections of words, each of which represents
    an abstract topic. Recall the imaginary sentence about work from before. The topic
    model built to identify the topics of some hypothetical corpus to which that sentence
    belongs would not return the word work as a topic. It would instead return a collection
    of words, such as paycheck, employee, and boss; words that describe the topic
    and from which the one-word or one-phrase topic could be inferred. This is because
    topic models understand word **proximity**, not context. The model has no idea
    what paycheck, employee, and boss mean; it only knows that these words, generally,
    whenever they appear, appear in close proximity to one another:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 话题模型的一个关键方面是，它们不会生成特定的单词或短语主题，而是生成一组词汇，每个词汇代表一个抽象的主题。回想一下之前关于工作的假设句子。构建出来的主题模型识别该句子所属的假设语料库的主题时，不会返回单词“work”作为主题。它会返回一组词汇，例如“paycheck”（薪水）、“employee”（员工）和“boss”（老板）；这些词汇描述了主题，并可以推断出一个单词或短语主题。这是因为话题模型理解的是词汇的**接近度**，而不是上下文。模型并不理解“paycheck”、“employee”和“boss”的含义；它只知道这些词汇通常在一起出现并且彼此接近：
- en: '![Figure 7.6: Inferring topics from word groupings](img/C12626_07_06.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6：从词组中推断主题](img/C12626_07_06.jpg)'
- en: 'Figure 7.6: Inferring topics from word groupings'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.6：从词组中推断主题
- en: Topic models can be used to predict the topic(s) belonging to unseen documents,
    but, if you are going to make predictions, it is important to recognize that topic
    models only know the words used to train them. That is, if the unseen documents
    have words that were not in the training data, the model will not be able to process
    those words even if they link to one of the topics identified in the training
    data. Because of this fact, topic models tend to be used more for exploratory
    analysis and inference more so than for prediction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 话题模型可以用来预测未见文档的主题，但如果你打算做预测，重要的是要认识到，话题模型只知道用来训练它们的词汇。也就是说，如果未见文档中包含训练数据中没有的词汇，模型将无法处理这些词汇，即使它们与训练数据中已识别的某个主题相关。因此，话题模型更多地用于探索性分析和推断，而不是预测。
- en: Each topic model outputs two matrices. The first matrix contains words against
    topics. It lists each word related to each topic with some quantification of the
    relationship. Given the number of words being considered by the model, each topic
    is only going to be described by a relatively small number of words. Words can
    either be assigned to one topic or to multiple topics at differing quantifications.
    Whether words are assigned to one or multiple topics depends on the algorithm.
    Similarly, the second matrix contains documents against topics. It lists each
    document related to a topic with some quantification of how related each document
    is to that topic.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每个话题模型输出两个矩阵。第一个矩阵包含词汇与主题的关系。它列出了每个词汇与每个主题的关联，并对这种关系进行量化。考虑到模型正在考虑的词汇数量，每个主题只会由相对较少的词汇描述。词汇可以被分配给一个主题，或者分配给多个主题，并在不同的量化下表示。词汇是被分配给一个主题还是多个主题，取决于算法。同样，第二个矩阵包含文档与主题的关系。它列出了与某个主题相关的每个文档，并对每个文档与该主题的关联程度进行量化。
- en: When discussing topic modeling, it is important to continually reinforce the
    fact that the word groups representing topics are not related conceptually, only
    by proximity. The frequent proximity of certain words in the documents is enough
    to define topics because of an assumption stated previously, which is that all
    words in the same document are related. However, this assumption may either not
    be true or the words may be too generic to form coherent topics. Interpreting
    abstract topics involves balancing the innate characteristics of text data with
    the generated word groupings. Text data, and language in general, is highly variable,
    complex, and has context, which means any generalized result needs to be consumed
    cautiously. This is not to downplay or invalidate the results of the model. Given
    thoroughly cleaned documents and an appropriate number of topics, word groupings,
    as we will see, can be a good guide as to what is contained in a corpus and can
    effectively be incorporated into larger data systems.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论主题建模时，重要的是要不断强调，代表主题的词汇组之间并不是概念上的关联，而仅仅是通过词汇的接近度来关联的。文档中某些单词的频繁接近足以定义主题，这是因为前面提到的假设——同一文档中的所有单词都是相关的。然而，这个假设可能并不成立，或者这些单词可能过于泛化，无法形成连贯的主题。解释抽象的主题涉及将文本数据的固有特性与生成的词汇组进行平衡。文本数据和语言一般来说是高度可变的、复杂的，并且具有上下文，这意味着任何普遍的结果都需要谨慎对待。这并不是贬低或否定模型的结果。在彻底清理过的文档和适当数量的主题下，词汇组，如我们将看到的，能够很好地指导语料库的内容，并且可以有效地融入到更大的数据系统中。
- en: We have discussed some of the limitations of topic models already, but there
    are some additional points to consider. The noisy nature of text data can lead
    topic models to assign words unrelated to one of the topics to that topic. Again,
    consider the sentence about work from before. The word meeting could appear in
    the word grouping representing the topic work. It is also possible that the word
    long could be in that group, but the word long is not directly related to work.
    Long may be in the group because it frequently appears in close proximity to the
    word meeting. Therefore, long would probably be considered to be spuriously correlated
    to work and should probably be removed from the topic grouping, if possible. Spuriously
    correlated words in word groupings can cause significant problems when analyzing
    data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了一些主题模型的局限性，但还有一些额外的注意事项。文本数据的噪声特性可能会导致主题模型将与某个主题无关的单词错误地分配给该主题。再考虑之前提到的关于工作的句子。单词meeting可能出现在代表“工作”主题的词汇组中。也有可能单词long也会出现在该组中，但单词long与工作没有直接关系。long之所以可能出现在该组，是因为它常常出现在meeting旁边。因此，long可能会被认为与工作虚假相关，并且如果可能的话，应该从该主题词汇组中移除。词汇组中的虚假相关词可能会在分析数据时引起严重问题。
- en: This is not necessarily a flaw in the model, it is, instead, a characteristic
    of the model that, given noisy data, could extract quirks from the data that might
    negatively impact the results. Spurious correlations could be the result of how,
    where, or when the data was collected. If the documents were collected only in
    some specific geographic region, words associated with that region could be incorrectly,
    albeit accidentally, linked to one or many of the word groupings output from the
    model. Note that, with additional words in the word group, we could be attaching
    more documents to that topic than should be attached. It should be straightforward
    that, if we shrink the number of words belonging to a topic, then that topic will
    be assigned to fewer documents. Keep in mind that this is not a bad thing. We
    want each word grouping to contain only words that make sense so that we assign
    the appropriate topics to the appropriate documents.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这不一定是模型的缺陷，而是模型的一个特性，在面对噪声数据时，可能会从数据中提取出一些特征，这些特征可能会对结果产生负面影响。虚假的相关性可能是由于数据的收集方式、地点或时间造成的。如果文档仅在某些特定的地理区域内收集，那么与该区域相关的词汇可能会不正确地、尽管是偶然地，链接到模型输出的一个或多个词汇组。请注意，随着词汇组中单词数量的增加，我们可能会将更多的文档归类到该主题中，超出应归类的范围。显然，如果我们减少属于某一主题的词汇数量，那么该主题将会被分配给更少的文档。记住，这并不是什么坏事。我们希望每个词汇组只包含有意义的单词，从而将合适的主题分配给合适的文档。
- en: There are many topic modeling algorithms, but perhaps the two best known are
    **Latent Dirichlet Allocation** and **Non-Negative Matrix Factorization**. We
    will discuss both in detail later on.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多主题建模算法，但也许最为人熟知的两种是**潜在狄利克雷分配（Latent Dirichlet Allocation）**和**非负矩阵分解（Non-Negative
    Matrix Factorization）**。我们稍后将详细讨论这两种算法。
- en: Business Applications
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 商业应用
- en: Despite its limitations, topic modeling can provide actionable insights that
    drive business value if used correctly and in the appropriate context. Let's now
    review one of the biggest applications of topic models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在局限性，但如果在正确的上下文中正确使用，主题建模可以提供可操作的洞察，推动商业价值。现在，让我们回顾一下主题模型的一个重要应用。
- en: One of the use cases is exploratory data analysis on new text data where the
    underlying structure of the dataset is unknown. This is the equivalent to plotting
    and computing summary statistics for an unseen dataset featuring numeric and categorical
    variables whose characteristics need to be understood before more sophisticated
    analyses can be reasonably performed. With the results of topic modeling, the
    usability of this dataset in future modeling exercises is ascertainable. For example,
    if the topic model returns clear and distinct topics, then that dataset would
    be a great candidate for further clustering-type analyses.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个使用案例是对新文本数据进行探索性数据分析，其中数据集的底层结构未知。这相当于为一个未见过的数据集绘制图表并计算总结统计量，该数据集包含需要理解其特征的数值和类别变量，才能在进行更复杂的分析之前做出合理的判断。通过主题建模的结果，可以确定此数据集在未来建模工作中的可用性。例如，如果主题模型返回了清晰且
    distinct 的主题，那么该数据集将是进一步聚类分析的理想候选对象。
- en: Effectively, what determining topics does is to create an additional variable
    that can be used to sort, categorize, and/or chunk data. If our topic model returns
    cars, farming, and electronics as abstract topics, we could filter our large text
    dataset down to just the documents with farming as a topic. Once filtered, we
    could perform further analyses, including sentiment analysis, another round of
    topic modeling, or any other analysis we could think up. Beyond defining the topics
    present in a corpus, topic modeling returns a lot of other information indirectly
    that could also be used to break a large dataset down and understand its characteristics.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，确定主题的作用是创建一个额外的变量，用于排序、分类和/或分块数据。如果我们的主题模型返回了汽车、农业和电子产品作为抽象主题，我们可以将大量的文本数据集过滤到只有农业作为主题的文档。一旦过滤完毕，我们可以进行进一步的分析，包括情感分析、另一次主题建模，或者任何我们能想到的分析。除了定义语料库中存在的主题外，主题建模还会间接返回许多其他信息，这些信息也可以用于分解大型数据集并理解其特征。
- en: 'A representation of sorting documents is shown here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了文档排序的表示：
- en: '![Figure 7.7: Sorting/categorizing documents](img/C12626_07_07.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7：文档排序/分类](img/C12626_07_07.jpg)'
- en: 'Figure 7.7: Sorting/categorizing documents'
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.7：文档排序/分类
- en: Among those characteristics is topic prevalence. Think about performing an analysis
    on an open response survey that is designed to gauge the response to a product.
    We could imagine the topic model returning topics in the form of sentiment. One
    group of words might be good, excellent, recommend, and quality, while the other
    might be garbage, broken, poor, and disappointing. Given this style of survey,
    the topics themselves may not be that surprising, but what would be interesting
    is that we could count the number of documents containing each topic. From the
    counts, we could say things like x-percent of the survey respondents had a positive
    reaction to the product, while only y-percent of the respondents had a negative
    reaction. Essentially, what we would have created is a rough version of a sentiment
    analysis.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个特征是主题的普遍性。想象一下对一个开放式问卷调查进行分析，这个问卷调查旨在评估对某个产品的反馈。我们可以设想主题模型返回的主题形式为情感。一组词可能是“好”，“优秀”，“推荐”和“质量”，而另一组则可能是“垃圾”，“损坏”，“差”和“失望”。鉴于这种类型的调查，主题本身可能不会太令人惊讶，但有趣的是，我们可以统计每个主题包含的文档数量。从统计结果中，我们可以得出这样的结论：例如，有x百分比的调查参与者对该产品有正面反应，而只有y百分比的参与者有负面反应。实质上，我们所创建的是情感分析的粗略版本。
- en: Currently, the most frequent use for a topic model is as a component of a recommendation
    engine. The emphasis today is on personalization – delivering products to customers
    that are specifically designed and curated for individual customers. Take websites,
    news or otherwise, devoted to the propagation of articles. Companies such as Yahoo
    and Medium need customers to keep reading in order to stay in business, and one
    way to keep customers reading is to feed them articles that they would be more
    inclined to read. This is where topic modeling comes in. Using a corpus made up
    of articles previously read by an individual, a topic model would essentially
    tell us what types of articles said subscriber likes to read. The company could
    then go to its inventory and find articles with similar topics and send them to
    the individual either via their account page or email. This is custom curation
    to facilitate simplicity and ease of use while also maintaining engagement.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，主题模型最常见的用途是作为推荐引擎的一个组件。如今的重点是个性化——为客户提供专门设计和策划的产品。以网站为例，无论是新闻网站还是其他类型的网站，它们的目的是传播文章。像雅虎和Medium这样的公司需要客户持续阅读才能维持生意，而让客户继续阅读的一种方法是向他们推送他们更倾向于阅读的文章。这就是主题建模的用武之地。通过使用一个由个体之前阅读的文章组成的语料库，主题模型基本上会告诉我们该订阅者喜欢阅读什么类型的文章。公司然后可以查找其库存中的相关文章，并通过用户的账户页面或电子邮件将其发送给该个体。这就是定制策划，以简化使用并保持用户的持续参与。
- en: Before we get into prepping data for our model, let's quickly load and explore
    the data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始为模型准备数据之前，让我们快速加载并浏览一下数据。
- en: 'Exercise 28: Data Loading'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 28：数据加载
- en: In this exercise we will load the data from a dataset and format it. The dataset
    for this and all the subsequent exercises in this chapter comes from the Machine
    Learning Repository, hosted by the University of California, Irvine (UCI). To
    find the data, go to [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将从一个数据集加载数据并进行格式化。本章中的所有练习的数据集均来自加利福尼亚大学尔湾分校（UCI）托管的机器学习库。要找到数据，请访问[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038)。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This data is downloaded from [https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms).
    It can be accessed at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038).
    Nuno Moniz and LuÃs Torgo (2018), Multi-Source Social Feedback of Online News
    Feeds, CoRR UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据下载自[https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)。可以通过[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Exercise27-Exercise%2038)访问。Nuno
    Moniz 和 Luís Torgo（2018），《在线新闻源的多源社交反馈》，CoRR UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚尔湾：加利福尼亚大学信息与计算机科学学院。
- en: 'This is the only file that is required for this exercise. Once downloaded and
    saved locally, the data can be loaded into the notebook:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是进行此练习所需的唯一文件。下载并保存在本地后，数据可以加载到笔记本中：
- en: Note
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Execute the exercises of this chapter in the same notebook.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个笔记本中执行本章的练习。
- en: 'Define the path to the data and load it using `pandas`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据的路径并使用`pandas`加载它：
- en: '[PRE3]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Add the file on the same path where you have opened your notebook.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将文件添加到与您打开的笔记本相同的路径中。
- en: 'Examine the data briefly by executing the following code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下代码简要检查数据：
- en: '[PRE4]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This user-defined function returns the shape of the data (number of rows and
    columns), the column names, and the first two rows of the data.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个用户定义的函数返回数据的形状（行数和列数）、列名和数据的前两行。
- en: '![](img/C12626_07_08.jpg)'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_08.jpg)'
- en: 'Figure 7.8: Raw data'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.8：原始数据
- en: This is a much larger dataset in terms of features than is needed to run the
    topic models.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个在特征上比运行主题模型所需的数据集要大得多的数据集。
- en: 'Notice that one of the columns, named `Topic`, actually contains the information
    that topic models are trying to ascertain. Let''s briefly look at the provided
    topic data, so that when we finally generate our own topics, the results can be
    compared. Run the following line to print the unique topic values and their number
    of occurrences:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，其中一列名为`Topic`，实际上包含了主题模型试图确定的信息。让我们简要查看提供的主题数据，以便在最终生成我们自己的主题时，能够进行比较。运行以下代码行，打印唯一的主题值及其出现次数：
- en: '[PRE5]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we extract the headline data and transform the extracted data into a list
    object. Print the first five elements of the list and the list length to confirm
    that the extraction was successful:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们提取标题数据并将提取的数据转换为一个列表对象。打印列表中的前五个元素以及列表长度，以确认提取是否成功：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 7.9: A list of headlines](img/C12626_07_09.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9：标题列表](img/C12626_07_09.jpg)'
- en: 'Figure 7.9: A list of headlines'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.9：标题列表
- en: With the data now loaded and correctly formatted, let's talk about textual data
    cleaning and then jump into some actual cleaning and preprocessing. For instructional
    purposes, the cleaning process will initially be built and executed on only one
    headline. Once we have established the process and tested it on the example headline,
    we will go back and run the process on every headline.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已加载并正确格式化，让我们讨论文本数据清洗，然后进行一些实际的清洗和预处理。为了教学目的，清洗过程最初将在一个标题上构建和执行。建立并测试了该过程之后，我们将返回并在每个标题上运行该过程。
- en: Cleaning Text Data
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清洗文本数据
- en: 'A key component of all successful modeling exercises is a clean dataset that
    has been appropriately and sufficiently preprocessed for the specific data type
    and analysis being performed. Text data is no exception, as it is virtually unusable
    in its raw form. It does not matter what algorithm is being run: if the data isn''t
    properly prepared, the results will be at best meaningless and at worst misleading.
    As the saying goes, *"garbage in, garbage out."* For topic modeling, the goal
    of data cleaning is to isolate the words in each document that could be relevant
    by removing everything that could be obstructive.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所有成功建模工作的一个关键组成部分是清洗过的数据集，该数据集已针对特定数据类型和分析方法进行了适当且充分的预处理。文本数据也不例外，因为它以原始形式几乎无法使用。无论运行什么算法：如果数据未经过适当准备，结果充其量是毫无意义，最坏的情况则可能是误导性的。正如俗话所说，*“垃圾进，垃圾出。”*
    对于主题建模来说，数据清洗的目标是通过移除可能妨碍分析的内容，来提取每个文档中可能相关的词汇。
- en: Data cleaning and preprocessing is almost always specific to the dataset, meaning
    that each dataset will require a unique set of cleaning and preprocessing steps
    selected to specifically handle the issues in the dataset being worked on. With
    text data, cleaning and preprocessing steps can include language filtering, removing
    URLs and screen names, lemmatizing, and stop word removal, among others. In the
    forthcoming exercises, a dataset featuring news headlines is cleaned for topic
    modeling.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗和预处理几乎总是特定于数据集的，这意味着每个数据集都需要选择一套独特的清洗和预处理步骤，专门处理该数据集中的问题。对于文本数据，清洗和预处理步骤可能包括语言过滤、移除网址和用户名、词形还原、去除停用词等。在接下来的练习中，我们将清洗一个包含新闻标题的数据集，以进行主题建模。
- en: Data Cleaning Techniques
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据清洗技术
- en: To reiterate a previous point, the goal of cleaning text for topic modeling
    is to isolate the words in each document that could be relevant to finding the
    abstract topics of the corpus. This means removing common words, short words (generally
    more common), numbers, and punctuation. No hard and fast process exists for cleaning
    data, so it is important to understand the typical problem points in the type
    of data being cleaned and to do extensive exploratory work.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下之前的观点，清洗文本数据以进行主题建模的目标是提取每个文档中可能与发现语料库的抽象主题相关的词汇。这意味着需要去除常见词、短词（通常较为常见）、数字和标点符号。没有固定的清洗流程，因此理解数据类型中的典型问题点并进行广泛的探索性工作是非常重要的。
- en: Let's now discuss some of the text data cleaning techniques that we will employ.
    One of the first things that needs to be done when doing any modeling task involving
    text is to determine the language(s) of the text. In this dataset, most of the
    headlines are English, so we will remove the non-English headlines for simplicity.
    Building models on non-English text data requires additional skill sets, the least
    of which is fluency in the particular language being modeled.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一些我们将要使用的文本数据清理技术。进行任何涉及文本的建模任务时，首先需要做的一件事是确定文本的语言。在这个数据集中，大多数标题是英语，因此为了简化处理，我们将删除非英语的标题。构建基于非英语文本数据的模型需要额外的技能，最基本的是流利掌握所处理的语言。
- en: The next crucial step in data cleaning is to remove all elements of the documents
    that are either not relevant to word-based models or are potential sources of
    noise that could obscure the results. Elements needing removal could include website
    addresses, punctuation, numbers, and stop words. Stop words are basically simple,
    everyday words, including we, are, and the. It is important to note that there
    is no definitive dictionary of stop words; instead, every dictionary varies slightly.
    Despite the differences, each dictionary represents a number of common words that
    are assumed to be topic agnostic. Topic models try to identify words that are
    both frequent and infrequent enough to be descriptive of an abstract topic.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理的下一个关键步骤是删除文档中所有与基于单词的模型无关的元素，或那些可能成为噪声源，进而影响结果的元素。需要删除的元素可能包括网站地址、标点符号、数字和停用词。停用词基本上是一些简单的日常用词，包括
    we、are 和 the。需要注意的是，没有一个权威的停用词词典；相反，每个词典都会有所不同。尽管存在差异，每个词典都表示一些常见的单词，这些单词被认为与话题无关。主题模型试图识别那些既频繁又不太频繁的单词，以便能够描述一个抽象的主题。
- en: The removal of website addresses has a similar motivation. Specific website
    addresses will appear very rarely, but even if one specific website address appears
    enough to be linked to a topic, website addresses are not interpretable in the
    same way as words. Removing irrelevant information from the documents reduces
    the amount of noise that could either prevent model convergence or obscure results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 删除网站地址有类似的动机。特定的网站地址出现的频率非常低，但即使一个特定的网站地址出现足够多，足以与某个话题相关联，网站地址的解释方式也不同于单词。从文档中移除不相关的信息，可以减少可能阻碍模型收敛或模糊结果的噪声。
- en: '**Lemmatization**, like language detection, is an important component of all
    modeling activities involving text. It is the process of reducing words to their
    base form as a way to group words that should all be the same but are not. Consider
    the words running, runs, and ran. All three of these words have the base form
    of run. A great aspect of lemmatizing is that it looks at all the words in a sentence,
    or in other words it considers the context, before determining how to alter each
    word. Lemmatization, like most of the preceding cleaning techniques, simply reduces
    the amount of noise in the data, so that we can identify clean and interpretable
    topics.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**词形还原**，与语言检测一样，是所有涉及文本的建模活动中一个重要的组成部分。它是将单词还原为其基本形式的过程，以便将应该相同但实际不同的单词归为一类。考虑单词
    running、runs 和 ran，它们的基本形式都是 run。词形还原的一个好处是，它在决定如何处理每个单词之前，会考虑整个句子的所有单词，换句话说，它会考虑上下文。像大多数前述的清理技术一样，词形还原只是减少数据中的噪声，从而使我们能够识别干净且可解释的主题。'
- en: Now, with basic knowledge of textual cleaning techniques, let's apply it to
    real-world data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，掌握了基本的文本清理技术后，让我们将其应用于实际数据。
- en: 'Exercise 29: Cleaning Data Step by Step'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 29：逐步清理数据
- en: 'In this exercise, we will learn how to implement some key techniques for cleaning
    text data. Each technique will be explained as we work through the exercise. After
    every cleaning step, the example headline is output using `print`, so we can watch
    the evolution from raw data to modeling-ready data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何实现一些清理文本数据的关键技术。每个技术将在我们进行练习时进行解释。在每一步清理操作后，使用`print`输出示例标题，便于我们观察从原始数据到适合建模的数据的演变：
- en: 'Select the fifth headline as the example on which we will build and test the
    cleaning process. The fifth headline is not a random choice; it was selected because
    it contains specific problems that will be addressed during the cleaning process:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择第五个标题作为我们构建和测试清理过程的示例。第五个标题不是随意选择的，它被选中是因为它包含了在清理过程中将要解决的特定问题：
- en: '[PRE8]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.10: The fifth headline](img/C12626_07_10.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.10：第五个标题](img/C12626_07_10.jpg)'
- en: 'Figure 7.10: The fifth headline'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.10：第五个标题
- en: 'Use the `langdetect` library to detect the language of each headline. If the
    language is anything other than English ("`en`"), remove that headline from the
    dataset:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`langdetect`库来检测每个标题的语言。如果语言不是英语（"en"），则从数据集中删除该标题：
- en: '[PRE9]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12626_07_11.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_11.jpg)'
- en: 'Figure 7.11: Detected language'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.11：检测到的语言
- en: 'Split the string containing the headline into pieces, called **tokens**, using
    the white spaces. The returned object is a list of words and numbers that make
    up the headline. Breaking the headline string into tokens makes the cleaning and
    preprocessing process simpler:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将包含标题的字符串使用空格分割成片段，称为**tokens**。返回的对象是构成标题的单词和数字的列表。将标题字符串拆分成 tokens 会使清理和预处理过程更简单：
- en: '[PRE10]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12626_07_12.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_12.jpg)'
- en: 'Figure 7.12: String split using white spaces'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.12：使用空格分割字符串
- en: 'Identify all URLs using a regular expression search for tokens containing `http://`
    or `https://`. Replace the URLs with the `''URL''` string:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式查找包含`http://`或`https://`的 tokens 来识别所有 URL。将 URL 替换为`'URL'`字符串：
- en: '[PRE11]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.13: URLs replaced with the URL string](img/C12626_07_13.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.13：将 URLs 替换为 URL 字符串](img/C12626_07_13.jpg)'
- en: 'Figure 7.13: URLs replaced with the URL string'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.13：将 URLs 替换为 URL 字符串
- en: 'Replace all punctuation and newline symbols (`\n`) with empty strings using
    regular expressions:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式将所有标点符号和换行符（`\n`）替换为空字符串：
- en: '[PRE12]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.14: Punctuation replaced with newline character](img/C12626_07_14.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.14：将标点符号替换为换行符](img/C12626_07_14.jpg)'
- en: 'Figure 7.14: Punctuation replaced with newline character'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.14：将标点符号替换为换行符
- en: 'Replace all numbers with empty strings using regular expressions:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式将所有数字替换为空字符串：
- en: '[PRE13]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.15: Numbers replaced with empty strings](img/C12626_07_15.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.15：将数字替换为空字符串](img/C12626_07_15.jpg)'
- en: 'Figure 7.15: Numbers replaced with empty strings'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.15：将数字替换为空字符串
- en: 'Change all uppercase letters to lowercase. Converting everything to lowercase
    is not a mandatory step, but it does help reduce complexity. With everything lowercase,
    there is less to keep track of and therefore less chance of error:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有大写字母转换为小写字母。将所有内容转换为小写并不是强制性步骤，但它有助于减少复杂性。将所有内容转换为小写后，跟踪的内容更少，因此出错的机会也更小：
- en: '[PRE14]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.16: Uppercase letters converted to lowercase](img/C12626_07_16.jpg)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.16：将大写字母转换为小写字母](img/C12626_07_16.jpg)'
- en: 'Figure 7.16: Uppercase letters converted to lowercase'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.16：将大写字母转换为小写字母
- en: 'Remove the `"URL"` string that was added as a placeholder in *step 4*. The
    previously added `"URL"` string is not actually needed for modeling. If it seems
    harmless to leave it in, consider that the `"URL"` string could appear naturally
    in a headline and we do not want to artificially boost its number of appearances.
    Also, the `"URL"` string does not appear in every headline, so by leaving it in
    we could be unintentionally creating a connection between the `"URL"` strings
    and a topic:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除*步骤 4*中作为占位符添加的`"URL"`字符串。之前添加的`"URL"`字符串实际上对于建模来说并不需要。如果认为它留在里面无害，考虑到`"URL"`字符串可能自然出现在标题中，而我们不希望人为地增加它出现的频率。此外，`"URL"`字符串并非每个标题中都会出现，因此如果将其留在里面，可能会无意中创建`"URL"`字符串与某一主题之间的关联：
- en: '[PRE15]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.17: String URL removed](img/C12626_07_17.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.17：字符串 URL 被移除](img/C12626_07_17.jpg)'
- en: 'Figure 7.17: String URL removed'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.17：移除字符串 URL
- en: 'Load in the `stopwords` dictionary from `nltk` and print it:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`nltk`加载`stopwords`字典并打印出来：
- en: '[PRE16]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.18: List of stop words](img/C12626_07_18.jpg)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.18：停用词列表](img/C12626_07_18.jpg)'
- en: 'Figure 7.18: List of stop words'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.18：停用词列表
- en: Before using the dictionary, it is important to reformat the words to match
    the formatting of our headlines. That involves confirming that everything is lowercase
    and without punctuation.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用字典之前，重要的是要重新格式化单词，以符合我们标题的格式。这包括确认所有内容都是小写且没有标点符号。
- en: 'Now that we have correctly formatted the `stopwords` dictionary, use it to
    remove all stop words from the headline:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经正确格式化了`stopwords`字典，使用它来删除标题中的所有停用词：
- en: '[PRE17]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.19: Stop words removed from the headline](img/C12626_07_19.jpg)'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.19：从标题中移除停用词](img/C12626_07_19.jpg)'
- en: 'Figure 7.19: Stop words removed from the headline'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.19：从标题中移除停用词
- en: 'Perform lemmatization by defining a function that can be applied to each headline
    individually. Lemmatizing requires loading the wordnet dictionary:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过定义一个可以单独应用于每个标题的函数来执行词形还原。词形还原需要加载WordNet字典：
- en: '[PRE18]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.20: Output after performing lemmatization](img/C12626_07_20.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.20：执行词形还原后的输出](img/C12626_07_20.jpg)'
- en: 'Figure 7.20: Output after performing lemmatization'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.20：执行词形还原后的输出
- en: 'Remove all words with a length of four of less from the list of tokens. The
    assumption around this step is that short words are, in general, more common and
    therefore will not drive the types of insights we are looking to extract from
    the topic models. This is the final step of data cleaning and preprocessing:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标记列表中移除所有长度为四个或更短的单词。该步骤的假设是，短单词通常更为常见，因此不会对我们希望从主题模型中提取的见解产生重要影响。这是数据清理和预处理的最后一步：
- en: '[PRE19]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.21: Headline number five post-cleaning](img/C12626_07_21.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.21：标题五号清理后的结果](img/C12626_07_21.jpg)'
- en: 'Figure 7.21: Headline number five post-cleaning'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.21：标题五号清理后的结果
- en: Now that we have worked through the cleaning and preprocessing steps individually
    on one headline, we need to apply those steps to every one of the nearly 100,000
    headlines. Applying the steps to every headline the way we applied them to the
    single headline, which is in a manual way, is not feasible.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经逐个标题完成了清理和预处理步骤，我们需要将这些步骤应用到近100,000个标题上。将这些步骤像对单个标题那样手动应用到每个标题上是不可行的。
- en: 'Exercise 30: Complete Data Cleaning'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 30：完整数据清理
- en: 'In this exercise, we will consolidate steps 2 through 12 from *Exercise 29*,
    *Cleaning Data Step-by-Step*, into one function that we can apply to every headline.
    The function will take one headline in string format as an input and output one
    cleaned headline as a list of tokens. The topic models require that documents
    be formatted as strings instead of as lists of tokens, so, in *step 4*, the lists
    of tokens are converted back into strings:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将把*练习29*中的第2步到第12步（*逐步清理数据*）合并成一个可以应用于每个标题的函数。该函数将接受一个字符串格式的标题作为输入，并输出一个以标记列表形式呈现的清理后标题。主题模型要求文档以字符串格式而非标记列表的形式呈现，因此，在*第4步*中，标记列表将被转换回字符串：
- en: 'Define a function that contains all the individual steps of the cleaning process
    from *Exercise 29*, *Cleaning Data Step-by-step*:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含*练习29*中清理过程所有单独步骤的函数（*逐步清理数据*）：
- en: '[PRE20]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Execute the function on each headline. The `map` function in Python is a nice
    way to apply a user-defined function to each element of a list. Convert the `map`
    object to a list and assign it to the `clean` variable. The `clean` variable is
    a list of lists:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个标题上执行该函数。Python中的`map`函数是将用户定义的函数应用于列表中每个元素的一个好方法。将`map`对象转换为列表，并将其赋值给`clean`变量。`clean`变量是一个列表的列表：
- en: '[PRE21]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In `do_headline_cleaning`, `None` is returned if the language of the headline
    is detected as being any language other than English. The elements of the final
    cleaned list should only be lists, not `None`, so remove all `None` types. Use
    `print` to display the first five cleaned headlines and the length of the `clean`
    variable:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`do_headline_cleaning`中，如果检测到标题的语言不是英语，则返回`None`。最终清理后的列表元素应仅为列表，而不是`None`，因此需要移除所有`None`类型。使用`print`显示前五个清理后的标题和`clean`变量的长度：
- en: '[PRE22]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_07_22.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_22.jpg)'
- en: 'Figure 7.22: Headline and its length'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.22：标题及其长度
- en: 'For every individual headline, concatenate the tokens using a white space separator.
    The headlines should now be an unstructured collection of words, nonsensical to
    the human reader, but ideal for topic modeling:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个单独的标题，使用空格分隔符连接标记。此时，标题应为一个非结构化的单词集合，尽管对人类读者来说毫无意义，但对主题建模来说是理想的：
- en: '[PRE23]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The cleaned headlines should resemble the following:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 清理后的标题应类似于以下内容：
- en: '![](img/C12626_07_23.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_23.jpg)'
- en: 'Figure 7.23: Headlines cleaned for modeling'
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.23：用于建模的清理后的标题
- en: To recap, what the cleaning and preprocessing work effectively does is strip
    out the noise from the data, so that the model can hone in on elements of the
    data that could actually drive insights. For example, words that are agnostic
    to any topic (or stop words) should not be informing topics, but, by accident
    alone if left in, could be informing topics. In an effort to avoid what we could
    call "*fake signals*," we remove those words. Likewise, since topic models cannot
    discern context, punctuation is irrelevant and is therefore removed. Even if the
    model could find the topics without removing the noise from the data, the uncleaned
    data could have thousands to millions of extra words and random characters to
    parse, depending on the number of documents in the corpus, which could significantly
    increase the computational demands. So, data cleaning is an integral part of topic
    modeling.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，清理和预处理工作实际做的事情是从数据中剔除噪声，使得模型能够集中于那些实际上可能提供洞察的数据元素。例如，任何与特定主题无关的词（或停用词）不应该影响主题的生成，但如果不小心留下，它们可能会影响主题的生成。为了避免所谓的“*假信号*”，我们移除了这些词。同样，由于主题模型无法识别上下文，标点符号是无关的，因此也被移除。即使模型在没有清理噪声数据的情况下能够找到主题，未清理的数据可能会有成千上万甚至更多的额外词汇和随机字符需要解析，具体取决于语料库中文档的数量，这可能会显著增加计算需求。因此，数据清理是主题建模的一个重要部分。
- en: 'Activity 15: Loading and Cleaning Twitter Data'
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 15：加载并清理 Twitter 数据
- en: In this activity, we will load and clean Twitter data for modeling done in subsequent
    activities. Our usage of the headline data is ongoing, so let's complete this
    activity in a separate Jupyter notebook, but with all the same requirements and
    imported libraries.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们将加载并清理 Twitter 数据，以便在后续活动中进行建模。我们对标题数据的使用仍在进行中，因此我们将在一个单独的 Jupyter notebook
    中完成此活动，但所有要求和导入的库都保持一致。
- en: The goal is to take the raw tweet data, clean it, and produce the same output
    that we did in *Step 4* of the previous exercise. That output should be a list
    whose length is similar to the number of rows in the raw data file. The length
    is similar to, meaning potentially not equal to, the number of rows, because tweets
    can get dropped in the cleaning process for reasons including that the tweet might
    not be in English. Each element of the list should represent one tweet and should
    contain just the words in the tweet that might be relevant to topic formation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是处理原始推文数据，清理它，并生成与前一个练习中*步骤 4*相同的输出。该输出应该是一个列表，列表长度类似于原始数据文件中的行数。长度类似，意味着可能不等于行数，因为在清理过程中可能会丢弃一些推文，原因包括推文可能不是英文的。列表中的每个元素应代表一条推文，且仅包含可能与主题形成相关的词汇。
- en: 'Here are the steps to complete the activity:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动的步骤如下：
- en: Import the necessary libraries.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Load the LA Times health Twitter data (latimeshealth.txt) from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17)加载
    LA Times 健康推特数据（latimeshealth.txt）。
- en: Note
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is downloaded from [https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter](https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter).
    We have uploaded it on GitHub and it can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17).
    Karami, A., Gangopadhyay, A., Zhou, B., & Kharrazi, H. (2017). Fuzzy approach
    topic discovery in health and medical corpora. International Journal of Fuzzy
    Systems. UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此数据集下载自[https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter](https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter)。我们已将其上传至
    GitHub，并可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson07/Activity15-Activity17)下载。Karami,
    A., Gangopadhyay, A., Zhou, B., & Kharrazi, H. (2017). 模糊方法在健康与医学语料库中的主题发现。《国际模糊系统杂志》。UCI
    机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学欧文分校：加利福尼亚大学信息与计算机科学学院。
- en: Run a quick exploratory analysis to ascertain data size and structure.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行快速的探索性分析，确定数据的大小和结构。
- en: Extract the tweet text and convert it to a list object.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取推文文本并转换为列表对象。
- en: Write a function to perform language detection, tokenization on whitespaces,
    and replace screen names and URLs with `SCREENNAME` and `URL`, respectively. The
    function should also remove punctuation, numbers, and the SCREENNAME and URL replacements.
    Convert everything to lowercase, except SCREENNAME and URL. It should remove all
    stop words, perform lemmatization, and keep words with five or more letters.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数来执行语言检测、基于空格的分词、将屏幕名称和网址替换为 `SCREENNAME` 和 `URL`。该函数还应删除标点、数字以及屏幕名称和网址的替换。将所有内容转换为小写，但屏幕名称和网址除外。它应删除所有停用词，执行词形还原，并保留五个或更多字母的单词。
- en: Apply the function defined in *step 5* to every tweet.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将在 *步骤5* 中定义的函数应用于每个推文。
- en: Remove elements of the output list equal to `None`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除输出列表中等于 `None` 的元素。
- en: Turn the elements of each tweet back into a string. Concatenate using white
    space.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个推文的元素转换回字符串。使用空格连接。
- en: Keep the notebook open for future modeling.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持笔记本开放以备将来建模。
- en: Note
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities in this chapter need to be performed in the same notebook.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本章中的所有活动都需要在同一个笔记本中执行。
- en: 'The output will be as follows:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/C12626_07_24.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_24.jpg)'
- en: 'Figure 7.24: Tweets cleaned for modeling'
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.24：用于建模的已清理推文
- en: Note
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 357.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第357页找到。
- en: Latent Dirichlet Allocation
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: In 2003, David Biel, Andrew Ng, and Michael Jordan published their article on
    the topic modeling algorithm known as **Latent Dirichlet Allocation** (**LDA**).
    LDA is a generative probabilistic model. This means that we assume the process,
    which is articulated in terms of probabilities, by which the data was generated
    is known and then work backward from the data to the parameters that generated
    it. In this case, it is the topics that generated the data that are of interest.
    The process discussed here is the most basic form of LDA, but for learning, it
    is also the most comprehensible.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 2003年，David Biel、Andrew Ng和Michael Jordan发表了关于主题建模算法的文章，称为**潜在狄利克雷分配**（**LDA**）。LDA是一种生成概率模型。这意味着我们假定已知生成数据的过程，该过程以概率形式表达，并且从数据反推生成数据的参数。在这种情况下，我们感兴趣的是生成数据的主题。这里讨论的过程是LDA的最基本形式，但对于学习来说也是最易理解的。
- en: 'For each document in the corpus, the assumed generative process is:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语料库中的每个文档，假定生成过程如下：
- en: Select ![A picture containing tableware
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 ![包含餐具的图片
- en: Description automatically generated](img/C12626_07_Formula_01.png), where ![A
    picture containing music
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/C12626_07_Formula_01.png)，其中![包含音乐的图片
- en: Description automatically generated](img/C12626_07_Formula_02.png) is the number
    of words.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/C12626_07_Formula_02.png)是单词的数量。
- en: Select ![A drawing of a face
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 ![一个人脸的绘图
- en: Description automatically generated](img/C12626_07_Formula_03.png), where ![](img/C12626_07_Formula_04.png)
    is the distribution of topics.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/C12626_07_Formula_03.png)，其中 ![](img/C12626_07_Formula_04.png)
    是主题的分布。
- en: For each of the ![A picture containing music
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一个 ![包含音乐的图片
- en: Description automatically generated](img/C12626_07_Formula_05.png) words ![](img/C12626_07_Formula_06.png),
    select topic ![A drawing of a face
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/C12626_07_Formula_05.png)单词 ![](img/C12626_07_Formula_06.png)，选择主题
    ![一个人脸的绘图
- en: Description automatically generated](img/C12626_07_Formula_07.png), and select
    word ![](img/C12626_07_Formula_08.png) from ![A picture containing clipart
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/C12626_07_Formula_07.png)，并从 ![包含剪贴画的图片
- en: Description automatically generated](img/C12626_07_Formula_09.png).
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/C12626_07_Formula_09.png)。
- en: 'Let''s go through the generative process in a bit more detail. The preceding
    three steps repeat for every document in the corpus. The initial step is to choose
    the number of words in the document by sampling from, in most cases, the *Poisson*
    distribution. It is important to note that, because N is independent of the other
    variables, the randomness associated with its generation is mostly ignored in
    the derivation of the algorithm. Coming after the selection of *N* is the generation
    of the topic mixture or distribution of topics, which is unique to each document.
    Think of this as a per-document list of topics with probabilities representing
    the amount of the document represented by each topic. Consider three topics: A,
    B, and C. An example document could be 100% topic A, 75% topic B, and 25% topic
    C, or an infinite number of other combinations. Lastly, the specific words in
    the document are selected via a probability statement conditioned on the selected
    topic and the distribution of words for that topic. Note that documents are not
    really generated in this way, but it is a reasonable proxy.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地回顾一下生成过程。前面提到的三个步骤会为语料库中的每个文档重复执行。初始步骤是通过从大多数情况下的*泊松*分布中抽样来选择文档中的单词数。需要注意的是，由于N与其他变量无关，因此在推导算法时，生成N的随机性大多被忽略。选择了*N*后，接下来是生成主题混合或每个文档独有的主题分布。可以将其看作是每个文档的主题列表，列表中的概率表示每个主题在文档中所占的比例。考虑三个主题：A、B和C。例如，一个文档可能是100%的主题A、75%的主题B和25%的主题C，或者是其他无数的组合。最后，文档中的特定单词是通过基于所选主题和该主题的单词分布的概率条件来选择的。需要注意的是，文档并不是以这种方式生成的，但这种方式是一个合理的代理。
- en: 'This process can be thought of as a distribution over distributions. A document
    is selected from the collection (distribution) of documents, and from that document
    one topic is selected, via the multinomial distribution, from the probability
    distribution of topics for that document generated by the Dirichlet distribution:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以看作是在分布上的分布。从文档集合（分布）中选择一个文档，并从该文档中通过多项分布选择一个主题，这个主题来自由狄利克雷分布生成的该文档的主题概率分布：
- en: '![Figure 7.25: Graphical representation of LDA](img/C12626_07_25.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.25：LDA的图形表示](img/C12626_07_25.jpg)'
- en: 'Figure 7.25: Graphical representation of LDA'
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.25：LDA的图形表示
- en: 'The most straightforward way to build the formula representing the LDA solution
    is through a graphical representation. This particular representation is referred
    to as a plate notation graphical model, as it uses plates to represent the two
    iterative steps in the process. You will recall that the generative process was
    executed for every document in the corpus, so the outermost plate, labeled M,
    represents iterating over each document. Similarly, the iteration over words in
    *step 3* is represented by the innermost plate of the diagram, labeled *N*. The
    circles represent the parameters, distributions, and results. The shaded circle,
    labeled *w*, is the selected word, which is the only known piece of data and,
    as such, is used to reverse-engineer the generative process. Besides *w*, the
    other four variables in the diagram are defined as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 构建表示LDA解的公式最直接的方法是通过图形表示。这种特殊的表示法称为板符号图模型，因为它使用板块来表示过程中的两个迭代步骤。你可能还记得，生成过程会对语料库中的每个文档执行，因此最外层的板块，标记为M，表示对每个文档进行迭代。同样，*第3步*中的单词迭代则通过图表中最内层的板块表示，标记为*N*。圆圈表示参数、分布和结果。带阴影的圆圈，标记为*w*，表示选定的单词，它是唯一已知的数据，因此用来反推生成过程。除了*w*之外，图中的其他四个变量定义如下：
- en: '![A picture containing scissors, tool'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![包含剪刀和工具的图片'
- en: 'Description automatically generated](img/C12626_07_Formula_10.png): Hyperparameter
    for the topic-document Dirichlet distribution'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_10.png)：主题-文档狄利克雷分布的超参数'
- en: '![A picture containing furniture'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![包含家具的图片'
- en: 'Description automatically generated](img/C12626_07_Formula_11.png): Distribution
    of words for each topic'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_11.png)：每个主题的单词分布'
- en: '![](img/C12626_07_Formula_12.png): This is the latent variable for the topic'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/C12626_07_Formula_12.png)：这是主题的潜在变量'
- en: '![](img/C12626_07_Formula_13.png): This is the latent variable for the distribution
    of topics for each document'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/C12626_07_Formula_13.png)：这是每个文档主题分布的潜在变量'
- en: '![A picture containing scissors, tool'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含剪刀和工具的图片'
- en: Description automatically generated](img/C12626_07_Formula_14.png) and ![A picture
    containing furniture
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_07_Formula_14.png)和 ![A picture containing furniture
- en: Description automatically generated](img/C12626_07_Formula_15.png) control the
    frequency of topics in documents and of words in topics. If ![A picture containing
    scissors, tool
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 控制文档中主题频率和主题中单词频率的 ![A picture containing scissors, tool
- en: Description automatically generated](img/C12626_07_Formula_16.png) increases,
    the documents become increasingly similar as the number of topics in each document
    increases. On the other hand, if ![A picture containing scissors, tool
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每个文档中主题数量的增加，文档之间的相似性也逐渐增加。另一方面，如果 ![A picture containing scissors, tool
- en: Description automatically generated](img/C12626_07_Formula_17.png) decreases,
    the documents become increasingly dissimilar as the number of topics in each document
    decreases. The ![A picture containing furniture
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每个文档中主题数量的减少，文档之间的相似性逐渐减小。 ![A picture containing furniture
- en: Description automatically generated](img/C12626_07_Formula_18.png) parameter
    behaves similarly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_07_Formula_18.png) 参数表现相似。
- en: Variational Inference
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变分推断
- en: The big issue with LDA is that the evaluation of the conditional probabilities,
    the distributions, is unmanageable, so instead of computing them directly, the
    probabilities are approximated. Variational inference is one of the simpler approximation
    algorithms, but it has an extensive derivation that requires significant knowledge
    of probability. In order to spend more time on the application of LDA, this section
    will give some high-level details on how variational inference is applied in this
    context, but will not fully explore the algorithm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的一个大问题是条件概率（即分布）的评估是不可管理的，因此不直接计算这些概率，而是进行近似。变分推断是一种简单的近似算法，但它有一个复杂的推导过程，需要对概率有深入的理解。为了能更多地关注LDA的应用，本节将简要介绍变分推断在此背景下的应用，但不会全面探讨该算法。
- en: Let's take a moment to work through the variational inference algorithm intuitively.
    Start by randomly assigning each word in each document in the corpus to one of
    the topics. Then, for each document and each word in each document separately,
    calculate two proportions. Those proportions would be the proportion of words
    in the document that are currently assigned to the topic, ![](img/C12626_07_Formula_19.png),
    and the proportion of assignments across all documents of a specific word to the
    topic, ![](img/C12626_07_Formula_20.png). Multiply the two proportions and use
    the resulting proportion to assign the word to a new topic. Repeat this process
    until a steady state, where topic assignments are not changing significantly,
    is reached. These assignments are then used to estimate the within-document topic
    mixture and the within-topic word mixture.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间直观地讲解变分推断算法。首先，随机地将语料库中每个文档中的每个单词分配给一个主题。然后，针对每个文档中的每个单词，分别计算两个比例。这些比例分别是：文档中当前分配给该主题的单词比例，![](img/C12626_07_Formula_19.png)，以及在所有文档中，特定单词分配给该主题的比例，![](img/C12626_07_Formula_20.png)。将这两个比例相乘，使用结果比例为该单词分配一个新的主题。重复此过程，直到达到稳态，即主题分配不再发生显著变化为止。然后，这些分配将被用来估算文档内的主题混合和主题内的单词混合。
- en: '![Figure 7.26: The variational inference process](img/C12626_07_26.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.26：变分推断过程](img/C12626_07_26.jpg)'
- en: 'Figure 7.26: The variational inference process'
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.26：变分推断过程
- en: The thought process behind variational inference is that, if the actual distribution
    is intractable, then a simpler distribution, let's call it the variational distribution,
    very close to initial distribution, which is tractable, should be found so that
    inference becomes possible.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断背后的思维过程是，如果实际分布是不可解的，那么应该找到一个简单的分布，称为变分分布，它非常接近可解的初始分布，以便使推断成为可能。
- en: 'To start, select a family of distributions, *q*, conditioned on new variational
    parameters. The parameters are optimized so that the original distribution, which
    is actually the posterior distribution for those people familiar with Bayesian
    statistics, and the variational distribution are as close as possible. The variational
    distribution will be close enough to the original posterior distribution to be
    used as a proxy, making any inference done on it applicable to the original posterior
    distribution. The generic formula for the family of distributions, *q*, is as
    follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择一个分布族 *q*，并基于新的变分参数进行条件化。优化这些参数，使得原始分布（对于熟悉贝叶斯统计的人来说，实际上是后验分布）与变分分布尽可能接近。变分分布将足够接近原始的后验分布，可以作为代理，从而使得基于该分布进行的任何推断都适用于原始的后验分布。分布族
    *q* 的通用公式如下：
- en: '![Figure 7.27: Formula for the family of distributions, q](img/C12626_07_27.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.27：分布族的公式，q](img/C12626_07_27.jpg)'
- en: 'Figure 7.27: Formula for the family of distributions, q'
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.27：分布族的公式，q
- en: There is a large collection of potential variational distributions that can
    be used as an approximation for the posterior distribution. An initial variational
    distribution is selected from the collection, which acts as the starting point
    for an optimization process that iteratively moves closer and closer to the optimal
    distribution. The optimal parameters are the parameters of the distribution that
    best approximate the posterior.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量潜在的变分分布可以用作后验分布的近似。首先从这些分布中选择一个初始变分分布，作为优化过程的起点，该过程会迭代地不断逼近最优分布。最优参数是能够最好地近似后验分布的分布参数。
- en: The similarity of the two distributions is measured using the **Kullback-Leibler**
    (KL) divergence. KL divergence is also known as relative entropy. Again, finding
    the best variational distribution, *q*, for the original posterior distribution,
    *p*, requires minimizing the KL divergence. The default method for finding the
    parameters that minimize the divergence is the iterative fixed-point method, which
    we will not dig into here.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分布的相似度通过 **Kullback-Leibler**（KL）散度来衡量。KL 散度也被称为相对熵。同样，找到最佳的变分分布 *q*，以近似原始后验分布
    *p*，需要最小化 KL 散度。找到最小化散度的参数的默认方法是迭代的固定点方法，我们在这里不深入讨论。
- en: Once the optimal distribution has been identified, which means the optimal parameters
    have been identified, it can be leveraged to produce the output matrices and do
    any required inference.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦识别出最优分布（这意味着最优参数已被确定），就可以利用它来生成输出矩阵并进行任何必要的推断。
- en: Bag of Words
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型
- en: Text cannot be passed directly into any machine learning algorithm; it first
    needs to be encoded numerically. A straightforward way of working with text in
    machine learning is via a bag-of-words model, which removes all information around
    the order of the words and focuses strictly on the degree of presence, meaning
    count or frequency, of each word. The Python `sklearn` library can be leveraged
    to transform the cleaned vector created in the previous exercise into the structure
    that the LDA model requires. Since LDA is a probabilistic model, we do not want
    to do any scaling or weighting of the word occurrences; instead, we opt to input
    just the raw counts.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 文本不能直接传递给任何机器学习算法；首先需要将其进行数值编码。在机器学习中处理文本的一种简单方法是通过词袋模型，该模型去除词语顺序的信息，严格关注每个词的出现程度，即计数或频率。可以利用
    Python 的 `sklearn` 库，将前一个练习中创建的清理向量转换为 LDA 模型所需的结构。由于 LDA 是一个概率模型，我们不希望对词语出现次数进行任何缩放或加权；相反，我们选择输入原始计数。
- en: The input of the bag-of-words model will be the list of cleaned strings that
    were returned from *Exercise 4*, *Complete Data Cleaning*. The output will be
    the document number, the word as its numeric encoding, and the count of times
    that word appears in that document. These three items will be presented as a tuple
    and an integer. The tuple will be something like (0, 325), where 0 is the document
    number and 325 is the numerically encoded word. Note that 325 would be the encoding
    of that word across all documents. The integer would then be the count. The bag-of-words
    models we will be running in this chapter are from `sklearn` and are called `CountVectorizer`
    and `TfIdfVectorizer`. The first model returns the raw counts and the second returns
    a scaled value, which we will discuss a bit later.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型的输入将是从*练习 4*（*完整数据清理*）返回的清理后的字符串列表。输出将是文档编号、单词的数字编码及该单词在文档中出现的次数。这三项将以元组和整数的形式呈现。元组类似于（0，325），其中0是文档编号，325是数字编码的单词。请注意，325是该单词在所有文档中的编码。然后，整数表示该单词的出现次数。我们将在本章运行的词袋模型来自`sklearn`，分别为`CountVectorizer`和`TfIdfVectorizer`。第一个模型返回原始计数，第二个模型返回一个缩放值，我们稍后会讨论。
- en: A critical note is that the results of both topic models being covered in this
    chapter can vary from run to run, even when the data is the same, because of randomness.
    Neither the probabilities in LDA nor the optimization algorithms are deterministic,
    so do not be surprised if your results differ slightly from the results shown
    from here on out.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的说明是，本章介绍的两个主题模型的结果可能会有所不同，即使数据相同，也会因为随机性而有所变化。无论是LDA中的概率还是优化算法都不是确定性的，因此，如果您的结果与这里展示的结果略有不同，也不必惊讶。
- en: 'Exercise 31: Creating a Bag-of-Words Model Using the Count Vectorizer'
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 31：使用计数向量化器创建词袋模型
- en: In this exercise, we will run the `CountVectorizer` in `sklearn` to convert
    our previously created cleaned vector of headlines into a bag-of-words data structure.
    In addition, we will define some variables that will be used through the modeling
    process.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将运行`CountVectorizer`（在`sklearn`中）将我们之前创建的清洗过的标题向量转换为词袋数据结构。此外，我们还将定义一些变量，这些变量将在建模过程中使用。
- en: 'Define `number_words`, `number_docs`, and `number_features`. The first two
    variables control the visualization of the LDA results. More to come later. The
    `number_features` variable controls the number of words that will be kept in the
    feature space:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`number_words`、`number_docs`和`number_features`。前两个变量控制LDA结果的可视化，稍后会详细介绍。`number_features`变量控制将在特征空间中保留的单词数：
- en: '[PRE24]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Run the count vectorizer and print the output. There are three crucial inputs,
    which are `max_df`, `min_df`, and `max_features`. These parameters further filter
    the number of words in the corpus down to those that will most likely influence
    the model. Words that only appear in a small number of documents are too rare
    to be attributed to any topic, so `min_df` is used to throw away words that appear
    in less than the specified number of documents. Words that appear in too many
    documents are not specific enough to be linked to specific topics, so `max_df`
    is used to throw away words that appear in more than the specified percentage
    of documents. Lastly, we do not want to overfit the model, so the number of words
    used to fit the model is limited to the most frequently occurring specified number
    (`max_features`) of words:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行计数向量化器并打印输出。有三个关键输入参数，分别是`max_df`、`min_df`和`max_features`。这些参数进一步筛选语料库中的单词，保留那些最有可能影响模型的单词。那些只出现在少数文档中的单词太稀有，无法归属到任何一个主题中，因此`min_df`用于丢弃在指定文档数以下出现的单词。而那些出现在太多文档中的单词则不够具体，无法与特定主题关联，因此`max_df`用于丢弃在超过指定百分比文档中出现的单词。最后，为了避免过拟合模型，限制用于拟合模型的单词数量为出现频率最高的指定数量单词（`max_features`）：
- en: '[PRE25]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.28: The bag-of-words data structure](img/C12626_07_28.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.28：词袋数据结构](img/C12626_07_28.jpg)'
- en: 'Figure 7.28: The bag-of-words data structure'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.28：词袋数据结构
- en: 'Extract the feature names and the words from the vectorizer. The model is only
    fed the numerical encodings of the words, so having the feature names vector to
    merge with the results will make interpretation easier:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取特征名称和向量化器中的单词。模型仅接受单词的数字编码，因此将特征名称向量与结果合并将使解释更加容易：
- en: '[PRE26]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Perplexity
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 困惑度
- en: 'Models generally have metrics that can be leveraged to evaluate their performance.
    Topic models are no different, although performance in this case has a slightly
    different definition. In regression and classification, predicted values can be
    compared to actual values from which clear measures of performance can be calculated.
    With topic models, prediction is less reliable, because the model only knows the
    words it was trained on and new documents may not contain any of those words,
    despite featuring the same topics. Due to that difference, topic models are evaluated
    using a metric specific to language models called **perplexity**. Perplexity,
    abbreviated to PP, measures the number of different equally most probable words
    that can follow any given word on average. Let''s consider two words as an example:
    the and announce. The word the can preface an enormous number of equally most
    probable words, while the number of equally most probable words that can follow
    the word announce is significantly less – albeit still a large number.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通常具有可以用来评估其性能的指标。主题模型也不例外，尽管在这种情况下，性能的定义略有不同。在回归和分类中，预测值可以与实际值进行比较，从中可以计算出明确的性能指标。而在主题模型中，预测的可靠性较差，因为模型只知道它训练时使用的单词，而新文档可能不包含这些单词，尽管它们可能涉及相同的主题。由于这一差异，主题模型使用一种特定于语言模型的指标来评估，即**困惑度**。困惑度（Perplexity，简称PP）衡量的是在给定单词后，平均而言，有多少个不同的最可能的单词可以跟随它。我们可以用两个单词作为例子：the
    和 announce。单词 the 后可以接大量的同样最可能的单词，而单词 announce 后可以接的同样最可能的单词则要少得多——尽管它们的数量仍然很大。
- en: 'The idea is that words that, on average, can be followed by a smaller number
    of equally most probable words are more specific and can be more tightly tied
    to topics. As such, lower perplexity scores imply better language models. Perplexity
    is very similar to entropy, but perplexity is typically used because it is easier
    to interpret. As we will see momentarily, it can be used to select the optimal
    number of topics. With *m* being the number of words in the sequence of words,
    perplexity is defined as:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路是：那些平均可以被更少数量的同样最可能的单词跟随的词语更具特异性，并且可以与主题更紧密地关联。因此，较低的困惑度得分意味着更好的语言模型。困惑度与熵非常相似，但通常使用困惑度，因为它更易于解释。正如我们接下来将看到的，它可以用来选择最佳的主题数。当
    *m* 是单词序列中的单词数时，困惑度定义为：
- en: '![](img/C12626_07_29.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_29.jpg)'
- en: 'Figure 7.29: Formula of perplexity'
  id: totrans-271
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.29：困惑度公式
- en: 'Exercise 32: Selecting the Number of Topics'
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 32：选择主题数
- en: As stated previously, LDA has two required inputs. The first are the documents
    themselves and the second is the number of topics. Selecting an appropriate number
    of topics can be very tricky. One approach to finding the optimal number of topics
    is to search over several numbers of topics and select the number of topics that
    corresponds to the smallest perplexity score. In machine learning, this approach
    is referred to as grid search.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LDA 有两个必需的输入。第一个是文档本身，第二个是主题数。选择合适的主题数非常具有挑战性。寻找最佳主题数的一种方法是对多个主题数进行搜索，并选择对应于最小困惑度得分的主题数。在机器学习中，这种方法称为网格搜索（grid
    search）。
- en: 'In this exercise, we use the perplexity scores for LDA models fit on varying
    numbers of topics to determine the number of topics with which to move forward.
    Keep in mind that the original dataset had the headlines sorted into four topics.
    Let''s see whether this approach returns four topics:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们使用不同主题数下拟合的 LDA 模型的困惑度得分，来确定最终采用的主题数。请记住，原始数据集中的标题已经被分类成四个主题。让我们看看这种方法是否返回了四个主题：
- en: 'Define a function that fits an LDA model on various numbers of topics and computes
    the perplexity score. Return two items: a data frame that has the number of topics
    with its perplexity score and the number of topics with the minimum perplexity
    score as an integer:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数根据不同的主题数拟合 LDA 模型并计算困惑度得分。返回两个项目：一个数据框，包含主题数及其困惑度得分；一个整数，表示最小困惑度得分对应的主题数：
- en: '[PRE27]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Execute the function defined in *step 1*. The `ntopics` input is a list of
    numbers of topics that can be of any length and contain any values. Print out
    the data frame:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行在*步骤 1* 中定义的函数。`ntopics` 输入是一个包含任何值的主题数列表，可以具有任意长度。打印出数据框：
- en: '[PRE28]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.30: Data frame containing number of topics and perplexity score](img/C12626_07_30.jpg)'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.30：包含主题数和困惑度得分的数据框](img/C12626_07_30.jpg)'
- en: 'Figure 7.30: Data frame containing number of topics and perplexity score'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.30：包含主题数量和困惑度得分的数据框
- en: 'Plot the perplexity scores as a function of the number of topics. This is just
    another way to view the results contained in the data frame from *step 2*:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将困惑度得分绘制为主题数量的函数。这只是查看*步骤2*数据框中结果的另一种方式：
- en: '[PRE29]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The plot looks as follows:'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表如下所示：
- en: '![Figure 7.31: Line plot view of perplexity as a function of the number of
    topics](img/C12626_07_31.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.31：困惑度作为主题数量函数的折线图](img/C12626_07_31.jpg)'
- en: 'Figure 7.31: Line plot view of perplexity as a function of the number of topics'
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.31：困惑度作为主题数量函数的折线图
- en: As the data frame and plot show, the optimal number of topics using perplexity
    is three. Having the number of topics set to four yielded the second-lowest perplexity,
    so, while the results did not exactly match with the information contained in
    the original dataset, the results are close enough to engender confidence in the
    grid search approach to identifying the optimal number of topics. There could
    be several reasons that the grid search returned three instead of four, which
    we will dig into in future exercises.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 正如数据框和图表所示，使用困惑度（perplexity）得到的最佳主题数量是三个。将主题数量设置为四时得到的困惑度为第二低，因此，尽管结果与原始数据集中的信息不完全匹配，但结果足够接近，足以增强对网格搜索方法的信心，用于确定最佳主题数量。网格搜索返回三个而不是四个的原因可能有很多，未来的练习中我们将深入探讨这些原因。
- en: 'Exercise 33: Running Latent Dirichlet Allocation'
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习33：运行潜在狄利克雷分配（Latent Dirichlet Allocation）
- en: 'In this exercise, we implement LDA and examine the results. LDA outputs two
    matrices. The first is the topic-document matrix and the second is the word-topic
    matrix. We will look at these matrices as returned from the model and as nicely
    formatted tables that are easier to digest:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们实现LDA并检查结果。LDA输出两个矩阵，第一个是主题-文档矩阵，第二个是词-主题矩阵。我们将查看这些矩阵，分别以模型返回的原始形式以及易于理解的格式化表格呈现：
- en: 'Fit an LDA model using the optimal number of topics found in *Exercise 32*,
    *Selecting the Number of Topics*:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*练习32*中找到的最佳主题数量拟合LDA模型，*选择主题数量*：
- en: '[PRE30]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.32: LDA model](img/C12626_07_32.jpg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.32：LDA模型](img/C12626_07_32.jpg)'
- en: 'Figure 7.32: LDA model'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.32：LDA模型
- en: 'Output the topic-document matrix and its shape to confirm that it aligns with
    the number of topics and the number of documents. Each row of the matrix is the
    per-document distribution of topics:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出主题-文档矩阵及其形状，以确认其与主题数量和文档数量一致。矩阵的每一行是每个文档的主题分布：
- en: '[PRE31]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.33: Topic-document matrix and its dimensions](img/C12626_07_33.jpg)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.33：主题-文档矩阵及其维度](img/C12626_07_33.jpg)'
- en: 'Figure 7.33: Topic-document matrix and its dimensions'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.33：主题-文档矩阵及其维度
- en: 'Output the word-topic matrix and its shape to confirm that it aligns with the
    number of features (words) specified in *Exercise 31*, *Creating Bag-of-Words
    Model Using the Count Vectorizer*, and the number of topics input. Each row is
    basically the counts (although not counts exactly) of assignments to that topic
    of each word, but those quasi-counts can be transformed into the per-topic distribution
    of words:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出词-主题矩阵及其形状，以确认其与*练习31*中指定的特征数量（词语）以及输入的主题数量一致。每一行基本上是每个词分配到该主题的计数（虽然不完全是计数），但这些准计数可以转化为每个主题的词分布：
- en: '[PRE32]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 7.34: Word-topic matrix and its dimensions](img/C12626_07_34.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.34：词-主题矩阵及其维度](img/C12626_07_34.jpg)'
- en: 'Figure 7.34: Word-topic matrix and its dimensions'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.34：词-主题矩阵及其维度
- en: 'Define a function that formats the two output matrices into easy-to-read tables:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将两个输出矩阵格式化为易于阅读的表格：
- en: '[PRE33]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The function may be tricky to navigate, so let's walk through it. Start by creating
    the *W* and *H* matrices, which includes converting the assignment counts of *W*
    into the per-topic distribution of words. Then, iterate over the topics. Inside
    each iteration, identify the top words and documents associated with each topic.
    Convert the results into two data frames.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数可能比较复杂，因此让我们一步步分析。首先创建*W*和*H*矩阵，包括将*W*的分配计数转化为每个主题的词分布。然后，遍历各个主题。在每次迭代中，识别与每个主题相关的顶级词汇和文档。将结果转换为两个数据框。
- en: 'Execute the function defined in *step 4*:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行*步骤4*中定义的函数：
- en: '[PRE34]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Print out the word-topic data frame. It shows the top-10 words, by distribution
    value, that are associated with each topic. From this data frame, we can identify
    the abstract topics that the word groupings represent. More on abstract topics
    to come:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出单词-主题数据框。它展示了与每个主题相关的前十个单词，按分布值排序。从这个数据框中，我们可以识别出单词分组所代表的抽象主题。更多关于抽象主题的内容请参见后续：
- en: '[PRE35]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_07_35.jpg)'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_35.jpg)'
- en: 'Figure 7.35: Word-topic table'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.35：单词-主题表
- en: 'Print out the topic-document data frame. It shows the 10 documents to which
    each topic is most closely related. The values are from the per-document distribution
    of topics:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出主题-文档数据框。它展示了与每个主题最相关的10篇文档。数据来源于每篇文档的主题分布：
- en: '[PRE36]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.36: Topic-document table](img/C12626_07_36.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.36：主题-文档表](img/C12626_07_36.jpg)'
- en: 'Figure 7.36: Topic-document table'
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.36：主题-文档表
- en: The results of the word-topic data frame show that the abstract topics are Barack
    Obama, the economy, and Microsoft. What is interesting is that the word grouping
    describing the economy contains references to Palestine. All four topics specified
    in the original dataset are represented in the word-topic data frame output, but
    not in the fully distinct manner expected. We could be facing one of two problems.
    First, the topic referencing both the economy and Palestine could be under-cooked,
    which means increasing the number of topics may fix the issue. The other potential
    problem is that LDA does not handle correlated topics well. In *Exercise 35*,
    *Trying Four Topics*, we will try expanding the number of topics, which will give
    us a better idea of why one of the word groupings is seemingly a mixture of topics.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 单词-主题数据框的结果显示，抽象主题是巴拉克·奥巴马、经济和微软。有趣的是，描述经济的单词分组中包含了关于巴勒斯坦的提及。原始数据集中指定的四个主题在单词-主题数据框输出中都有体现，但并不是以预期的完全独立的方式呈现。我们可能面临两种问题。首先，涉及经济和巴勒斯坦的主题可能还不成熟，这意味着增加主题的数量可能会解决这个问题。另一个潜在的问题是，LDA对于相关主题的处理较差。在*练习
    35*，*尝试四个主题*中，我们将尝试扩展主题的数量，这将帮助我们更好地理解为什么某个单词分组似乎是多个主题的混合。
- en: 'Exercise 34: Visualize LDA'
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 34：可视化LDA
- en: Visualization is a helpful tool for exploring the results of topic models. In
    this exercise, we will look at three different visualizations. Those visualizations
    are basic histograms and specialty visualizations using t-SNE and PCA.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是探索主题模型结果的有用工具。在本练习中，我们将查看三种不同的可视化图表。这些可视化图表包括基本的直方图以及使用t-SNE和PCA的专业可视化。
- en: 'To create some of the visualizations, we are going to use the `pyLDAvis` Library.
    This library is flexible enough to take in topic models built using several different
    frameworks. In this case, we will use the `sklearn` framework. This visualization
    tool returns a histogram showing the words that are the most closely related to
    each topic and a biplot, frequently used in PCA, where each circle corresponds
    to a topic. From the biplot, we know how prevalent each topic is across the whole
    corpus, which is reflected by the area of the circle, and the similarity of the
    topics, which is reflected by the closeness of the circles. The ideal scenario
    is to have the circles spread throughout the plot and be of reasonable size. That
    is, we want the topics to be distinct and to appear consistently across the corpus:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一些可视化图表，我们将使用`pyLDAvis`库。这个库足够灵活，可以处理使用不同框架构建的主题模型。在这种情况下，我们将使用`sklearn`框架。这个可视化工具会返回一个直方图，展示与每个主题最相关的单词，以及一个二元图，常用于PCA，其中每个圆圈代表一个主题。从二元图中，我们可以了解每个主题在整个语料库中的流行度，圆圈的大小反映了这一点；圆圈的接近度反映了主题之间的相似性。理想的情况是，圆圈在图中均匀分布，且大小适中。也就是说，我们希望主题之间是独立的，并且在语料库中一致地出现：
- en: 'Run and display `pyLDAvis`. This plot is interactive. Clicking on each circle
    updates the histogram to show the top words related to that specific topic. The
    following is one view of this interactive plot:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行并显示`pyLDAvis`。这个图表是交互式的。点击每个圆圈会更新直方图，显示与该主题最相关的词汇。以下是这个交互式图表的一个视图：
- en: '[PRE37]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The plot looks as follows:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图表如下所示：
- en: '![Figure 7.37: A histogram and biplot for the LDA model](img/C12626_07_37.jpg)'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.37：LDA模型的直方图和二元图](img/C12626_07_37.jpg)'
- en: 'Figure 7.37: A histogram and biplot for the LDA model'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.37：LDA模型的直方图和二元图
- en: 'Define a function that fits a t-SNE model and then plots the results:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，拟合t-SNE模型并绘制结果：
- en: '[PRE38]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The function starts by filtering down the topic-document matrix using an input
    threshold value. There are tens of thousands of headlines and any plot incorporating
    all the headlines is going to be difficult to read and therefore not helpful.
    So, this function only plots a document if one of the distribution values is greater
    than or equal to the input threshold value. Once the data is filtered down, we
    run t-SNE, where the number of components is two, so we can plot the results in
    two dimensions. Then, create a vector with an indicator of which topic is most
    related to each document. This vector will be used to color-code the plot by topic.
    To understand the distribution of topics across the corpus and the impact of threshold
    filtering, the function returns the length of the topic vector as well as the
    topics themselves with the number of documents to which that topic has the largest
    distribution value. The last step of the function is to create and return the
    plot.
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数首先通过输入阈值过滤主题-文档矩阵。由于有数万个标题，包含所有标题的任何图形都会难以阅读，因此不具备实用性。因此，只有当分布值大于或等于输入阈值时，函数才会绘制文档。一旦数据经过过滤，我们运行t-SNE，其中组件数为2，因此可以在二维中绘制结果。接下来，创建一个向量，指示与每个文档最相关的主题。该向量将用于按主题为图形着色。为了理解语料库中主题的分布情况以及阈值过滤的影响，该函数返回主题向量的长度，以及每个主题与最大分布值相关联的文档数。函数的最后一步是创建并返回图形。
- en: 'Execute the function:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行该函数：
- en: '[PRE39]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.38: t-SNE plot with metrics around the distribution of the topics
    across the corpus](img/C12626_07_38.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图7.38：带有关于主题分布度量的t-SNE图](img/C12626_07_38.jpg)'
- en: 'Figure 7.38: t-SNE plot with metrics around the distribution of the topics
    across the corpus'
  id: totrans-336
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.38：带有关于主题分布度量的t-SNE图
- en: The visualizations show that the LDA model with three topics is producing good
    results overall. In the biplot, the circles are of a medium size, which suggests
    that the topics appear consistently across the corpus and the circles have good
    spacing. The t-SNE plot shows clear clusters supporting the separation between
    the circles represented in the biplot. The only glaring issue, which was previously
    discussed, is that one of the topics has words that do not seem to belong to that
    topic. In the next exercise, let's rerun the LDA using four topics.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化结果显示，使用三主题的LDA模型总体表现良好。在双变量图中，圆圈的大小适中，表明主题在语料库中出现的一致性较高，并且圆圈之间的间距良好。t-SNE图展示了清晰的聚类，支持双变量图中圆圈之间的分隔。唯一显著的问题是，之前已讨论过的，某个主题包含了看起来与该主题不相关的词语。在下一个练习中，我们将使用四个主题重新运行LDA。
- en: 'Exercise 35: Trying Four Topics'
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习35：尝试四个主题
- en: 'In this exercise, LDA is run with the number of topics set to four. The motivation
    for doing this is to try and solve what might be an under-cooked topic from the
    three-topic LDA model that has words related to both Palestine and the economy.
    We will run through the steps first and then explore the results at the end:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，LDA模型的主题数被设置为四。这样做的动机是尝试解决三主题LDA模型中的一个可能不成熟的主题，该主题包含了与巴勒斯坦和经济相关的词汇。我们将首先按照步骤进行，然后在最后探索结果：
- en: 'Run an LDA model with the number of topics equal to four:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个LDA模型，主题数设置为四：
- en: '[PRE40]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12626_07_39.jpg)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_39.jpg)'
- en: 'Figure 7.39: LDA model'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.39：LDA模型
- en: 'Execute the `get_topics` function, defined in the preceding code, to produce
    the more readable word-topic and topic-document tables:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行前面代码中定义的`get_topics`函数，生成更易读的词-主题和主题-文档表：
- en: '[PRE41]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Print the word-topic table:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印词-主题表：
- en: '[PRE42]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output is as follows:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.40: The word-topic table using the four-topic LDA model](img/C12626_07_40.jpg)'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图7.40：使用四主题LDA模型的词-主题表](img/C12626_07_40.jpg)'
- en: 'Figure 7.40: The word-topic table using the four-topic LDA model'
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.40：使用四主题LDA模型的词-主题表
- en: 'Print the document-topic table:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印文档-主题表：
- en: '[PRE43]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.41: The document-topic table using the four-topic LDA model](img/C12626_07_41.jpg)'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图7.41：使用四主题LDA模型的文档-主题表](img/C12626_07_41.jpg)'
- en: 'Figure 7.41: The document-topic table using the four-topic LDA model'
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.41：使用四主题LDA模型的文档-主题表
- en: 'Display the results of the LDA model using `pyLDAvis`:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pyLDAvis`显示LDA模型结果：
- en: '[PRE44]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The plot is as follows:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图形如下：
- en: '![Figure 7.42: A histogram and biplot describing the four-topic LDA model](img/C12626_07_42.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图7.42：描述四主题LDA模型的直方图和双图](img/C12626_07_42.jpg)'
- en: 'Figure 7.42: A histogram and biplot describing the four-topic LDA model'
  id: totrans-361
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.42：描述四主题LDA模型的直方图和双图
- en: Looking at the word-topic table, we see that the four topics found by this model
    align with the four topics specified in the original dataset. Those topics are
    Barack Obama, Palestine, Microsoft, and the economy. The question now is, why
    did the model built using four topics have a higher perplexity score than the
    model with three topics? That answer comes from the visualization produced in
    *step 5*. The biplot has circles of reasonable size, but two of those circles
    are quite close together, which suggests that those two topics, Microsoft and
    the economy, are very similar. In this case, the similarity actually makes intuitive
    sense. Microsoft is a major global company that impacts and is impacted by the
    economy. A next step, if we were to make one, would be to run the t-SNE plot to
    check whether the clusters in the t-SNE plot overlap. Let's now apply our knowledge
    of LDA to another dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 看着单词-主题表格，我们发现该模型找到的四个主题与原始数据集中指定的四个主题一致。这些主题分别是巴拉克·奥巴马、巴勒斯坦、微软和经济。现在的问题是，为什么使用四个主题建立的模型比使用三个主题的模型具有更高的困惑度得分？答案可以从*步骤5*中生成的可视化中得出。双图中有大小合适的圆圈，但其中两个圆圈非常接近，这表明这两个主题——微软和经济——非常相似。在这种情况下，这种相似性其实是合乎直觉的。微软是一个全球性的大公司，既受到经济的影响，也影响着经济。如果我们继续下一步，可能会运行t-SNE图，以检查t-SNE图中的聚类是否重叠。现在，让我们将LDA的知识应用到另一个数据集上。
- en: 'Activity 16: Latent Dirichlet Allocation and Health Tweets'
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动16：潜在狄利克雷分配与健康推文
- en: In this activity, we apply LDA to the health tweets data loaded and cleaned
    in *Activity 15*, *Loading and Cleaning Twitter Data*. Remember to use the same
    notebook used in *Activity 15*, *Loading and Cleaning Twitter Data*. Once the
    steps have been executed, discuss the results of the model. Do these word groupings
    make sense?
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将LDA应用于*活动15*中加载并清理过的健康推文数据，*加载和清理Twitter数据*。记得使用与*活动15*中相同的笔记本。执行完这些步骤后，讨论模型的结果。这些单词组合是否合理？
- en: For this activity, let's imagine that we are interested in acquiring a high-level
    understanding of the major public health topics. That is, what people are talking
    about in the world of health. We have collected some data that could shed light
    on this inquiry. The easiest way to identify the major topics in the dataset,
    as we have discussed, is topic modeling.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，让我们假设我们有兴趣获得关于主要公共卫生话题的高层次理解。也就是说，了解人们在健康领域讨论的内容。我们收集了一些数据，可以为这个问题提供一些线索。如我们所讨论的，识别数据集中主要话题的最简单方法是主题建模。
- en: 'Here are the steps to complete the activity:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成此活动的步骤：
- en: Specify the `number_words`, `number_docs`, and `number_features` variables.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`number_words`、`number_docs`和`number_features`变量。
- en: Create a bag-of-words model and assign the feature names to another variable
    for use later on.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个词袋模型，并将特征名称分配给另一个变量以供以后使用。
- en: Identify the optimal number of topics.
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定最优的主题数。
- en: Fit the LDA model using the optimal number of topics.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最优的主题数拟合LDA模型。
- en: Create and print the word-topic table.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并打印单词-主题表格。
- en: Print the document-topic table.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印文档-主题表格。
- en: Create a biplot visualization.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个双图可视化。
- en: Keep the notebook open for future modeling.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持笔记本开启以便进行未来的建模。
- en: 'The output will be as follows:'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/C12626_07_43.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_43.jpg)'
- en: 'Figure 7.43: A histogram and biplot for the LDA model trained on health tweets'
  id: totrans-377
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.43：用于健康推文的LDA模型训练的直方图和双图
- en: Note
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 360.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第360页找到。
- en: Bag-of-Words Follow-Up
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型后续处理
- en: When running LDA models, the count vectorizer bag-of-words model was used, but
    that is not the only bag-of-words model. Term Frequency – Inverse Document Frequency
    (TF-IDF) is similar to the count vectorizer used in the LDA algorithm, except
    that instead of returning raw counts, TF-IDF returns a weight that reflects the
    importance of a given word to a document in the corpus. The key component of this
    weighting scheme is that there is a normalization component for how frequently
    a given word appears across the entire corpus. Consider the word have.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 LDA 模型时，使用了计数向量器词袋模型，但这不是唯一的词袋模型。词频 – 逆文档频率（TF-IDF）类似于 LDA 算法中使用的计数向量器，不同之处在于，TF-IDF
    返回的是一个权重，而不是原始计数，反映了给定单词在语料库中文档中的重要性。这种加权方案的关键组成部分是，对于给定单词在整个语料库中出现的频率有一个归一化组件。考虑单词
    "have"。
- en: The word have may occur several times in a single document, suggesting that
    it could be important for isolating the topic of that document, but have will
    occur in many documents, if not most, in the corpus, potentially rendering it
    unless for isolating topics. Essentially, this scheme goes one step further than
    just returning the raw counts of the words in the document in an effort to initially
    identify words that may help identify abstract topics. The TF-IDF vectorizer is
    executed using `sklearn` via `TfidfVectorizer`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 单词 "have" 可能在单个文档中出现多次，表明它可能对区分该文档的主题很重要，但 "have" 会出现在许多文档中，如果不是大多数的话，在语料库中，可能因此使其无法用于区分主题。本质上，这个方案比仅返回文档中单词的原始计数更进一步，旨在初步识别可能有助于识别抽象主题的单词。TF-IDF
    向量化器通过 `sklearn` 使用 `TfidfVectorizer` 执行。
- en: 'Exercise 36: Creating a Bag-of-Words Using TF-IDF'
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 36：使用 TF-IDF 创建词袋模型
- en: 'In this exercise, we will create a bag-of-words using TF-IDF:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 TF-IDF 创建一个词袋模型：
- en: 'Run the TF-IDF vectorizer and print out the first few rows:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 TF-IDF 向量化器并打印出前几行：
- en: '[PRE45]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_07_44.jpg)'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_44.jpg)'
- en: 'Figure 7.44: Output of the TF-IDF vectorizer'
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.44：TF-IDF 向量化器的输出
- en: 'Return the feature names, the actual words in the corpus dictionary, to use
    when analyzing the output. You recall that we did the same thing when we ran the
    `CountVectorizer` in *Exercise 31*, *Creating Bag-of-Words Model Using the Count
    Vectorizer*:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回特征名称，即语料库字典中的实际单词，用于分析输出时使用。你还记得我们在 *练习 31* 中运行 `CountVectorizer` 时做的同样事情，*使用计数向量器创建词袋模型*：
- en: '[PRE46]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'A section of the output is as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '[PRE47]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Non-Negative Matrix Factorization
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非负矩阵分解
- en: Unlike LDA, non-negative matrix factorization (NMF) is not a probabilistic model.
    It is instead, as the name implies, an approach involving linear algebra. Using
    matrix factorization as an approach to topic modeling was introduced by Daniel
    D. Lee and H. Sebastian Seung in 1999\. The approach falls into the decomposition
    family of models that includes PCA, the modeling technique introduced in *Chapter
    4*, *An Introduction to Dimensionality Reduction & PCA*.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LDA 不同，非负矩阵分解（NMF）不是一个概率模型。相反，正如其名称所示，它是一种涉及线性代数的方法。使用矩阵分解作为主题建模的方法是由 Daniel
    D. Lee 和 H. Sebastian Seung 在 1999 年提出的。该方法属于分解类模型，包括 *第4章* 中介绍的 PCA，*降维与 PCA
    入门*。
- en: The major differences between PCA and NMF are that PCA requires components to
    be orthogonal while allowing them to be either positive or negative. NMF requires
    matrix components be non-negative, which should make sense if you think of this
    requirement in the context of the data. Topics cannot be negatively related to
    documents and words cannot be negatively related to topics. If you are not convinced,
    try to interpret a negative weight associating a topic with a document. It would
    be something like, topic T makes up -30% of document D; but what does that even
    mean? It is nonsensical, so NMF has non-negative requirements for every part of
    matrix factorization.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 和 NMF 之间的主要区别在于，PCA 要求组件是正交的，同时允许它们是正数或负数。而 NMF 要求矩阵组件是非负的，如果你在数据的上下文中思考这个要求，应该能理解这一点。主题与文档之间不能有负相关，单词与主题之间也不能有负相关。如果你不信服，试着解读一个负权重将一个主题与文档关联起来。比如，主题
    T 占文档 D 的 -30%；但这意味着什么呢？这是没有意义的，因此 NMF 对矩阵分解的每一部分都有非负的要求。
- en: 'Let''s define the matrix to be factorized, call it *X*, as a term-document
    matrix where the rows are words and the columns are documents. Each element of
    matrix X is either the number of occurrences of word *i* (the ![](img/C12626_07_Formula_21.png)
    row) in document *j* (the ![](img/C12626_07_Formula_22.png) column) or some other
    quantification of the relationship between word i and document j. The matrix,
    X, is naturally a sparse matrix as most elements in the term-document matrix will
    be zero, since each document only contains a limited number of words. There will
    be more on creating this matrix and deriving the quantifications later:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义要分解的矩阵，称之为 *X*，作为术语-文档矩阵，其中行是词，列是文档。矩阵 X 的每个元素是词 *i*（![](img/C12626_07_Formula_21.png)
    行）在文档 *j*（![](img/C12626_07_Formula_22.png) 列）中出现的次数，或者是词 i 与文档 j 之间关系的其他量化表示。矩阵
    X 本质上是一个稀疏矩阵，因为术语-文档矩阵中的大多数元素都是零，因为每个文档只包含有限数量的词。稍后会有更多关于创建此矩阵和推导量化方法的内容：
- en: '![Figure 7.45: The matrix factorization ](img/C12626_07_45.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.45：矩阵分解](img/C12626_07_45.jpg)'
- en: 'Figure 7.45: The matrix factorization'
  id: totrans-399
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.45：矩阵分解
- en: The matrix factorization takes the form ![A picture containing furniture
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解的形式为 ![包含家具的图片
- en: Description automatically generated](img/C12626_07_Formula_23.png), where the
    two component matrices, *W* and *H*, represent the topics as collections of words
    and the topic weights for each document, respectively. More specifically, ![](img/C12626_07_Formula_24.png)
    is a word by topic matrix, ![A picture containing furniture
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_23.png)，其中两个分量矩阵 *W* 和 *H* 分别表示将话题作为词集合和每个文档的主题权重。更具体地，![](img/C12626_07_Formula_24.png)
    是一个词-话题矩阵，![包含家具的图片'
- en: Description automatically generated](img/C12626_07_Formula_25.png) is a topic
    by document matrix, and, as stated earlier, ![A picture containing furniture
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_25.png)是一个由文档矩阵组成的话题，正如前面所述，![包含家具的图片'
- en: Description automatically generated](img/C12626_07_Formula_26.png) is a word
    by document matrix. A nice way to think of this factorization is as a weighted
    sum of word groupings defining abstract topics. The equivalency symbol in the
    formula for the matrix factorization is an indicator that the factorization *WH*
    is an approximation and thus the product of those two matrices will not reproduce
    the original term-document matrix exactly. The goal, as it was with LDA, is to
    find the approximation that is closest to the original matrix. Like X, both *W*
    and *H* are sparse matrices as each topic is only related to a few words and each
    document is a mixture of only a small number of topics – one topic in many cases.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_26.png)是一个词-文档矩阵。将此分解方式视为一个加权和的词组集合定义抽象话题是一种很好的思路。矩阵分解公式中的等价符号表示分解
    *WH* 是一种近似，因此这两个矩阵的乘积不会完全再现原始术语-文档矩阵。目标，如同在 LDA 中一样，是找到最接近原始矩阵的近似值。像 X 一样，*W*
    和 *H* 都是稀疏矩阵，因为每个话题只与少数几个词相关，每个文档通常只包含少量话题——在许多情况下只有一个话题。'
- en: Frobenius Norm
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Frobenius 范数
- en: 'The goal of solving NMF is the same as that of LDA: find the best approximation.
    To measure the distance between the input matrix and the approximation, NMF can
    use virtually any distance measure, but the standard is the Frobenius norm, also
    known as the Euclidean norm. The Frobenius norm is the sum of the element-wise
    squared errors mathematically expressed as ![A picture containing object'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 求解 NMF 的目标与 LDA 相同：找到最佳近似。为了衡量输入矩阵与近似值之间的距离，NMF 可以使用几乎任何距离度量，但标准的是 Frobenius
    范数，也称为欧几里得范数。Frobenius 范数是元素逐项平方误差的总和，数学表达式为 ![包含物体的图片
- en: Description automatically generated](img/C12626_07_Formula_27.png).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_27.png)。'
- en: With the measure of distance selected, the next step is defining the objective
    function. The minimization of the Frobenius norm will return the best approximation
    of the original term-document matrix and thus the most reasonable topics. Note
    that the objective function is minimized with respect to *W* and *H* so that both
    matrices are non-negative. It is expressed as ![A picture containing object
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择了距离度量之后，下一步是定义目标函数。最小化 Frobenius 范数将返回原始术语-文档矩阵的最佳近似，从而得到最合理的话题。请注意，目标函数是针对
    *W* 和 *H* 最小化的，以确保这两个矩阵都是非负的。其表达式为 ![包含物体的图片
- en: Description automatically generated](img/C12626_07_Formula_28.png).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动生成的描述](img/C12626_07_Formula_28.png)。'
- en: Multiplicative Update
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乘法更新
- en: 'The optimization algorithm used to solve NMF by Lee and Seung in their 1999
    paper is the Multiplicative Update algorithm and it is still one of the most commonly
    used solutions. It will be implemented in the exercises and activities later in
    the chapter. The update rules, for both *W* and *H*, are derived by expanding
    the objective function and taking the partial derivatives with respect to *W*
    and *H*. The derivatives are not difficult but do require fairly extensive linear
    algebra knowledge and are time-consuming, so let''s skip the derivatives and just
    state the updates. Note that, in the update rules, *i* is the current iteration
    and *T* means the transpose of the matrix. The first update rule is as follows:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Lee 和 Seung 在其 1999 年的论文中提出的优化算法用于求解 NMF，这就是乘法更新算法，至今仍是最常用的解决方案之一。它将在本章后续的练习和活动中实现。*W*
    和 *H* 的更新规则是通过展开目标函数，并对 *W* 和 *H* 求偏导数得出的。这些导数并不复杂，但需要相当深入的线性代数知识，并且计算量较大，因此我们跳过导数部分，直接给出更新规则。注意，在更新规则中，*i*
    表示当前迭代，*T* 表示矩阵的转置。第一个更新规则如下：
- en: '![](img/C12626_07_46.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_46.jpg)'
- en: 'Figure 7.46: First update rule'
  id: totrans-412
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.46：第一个更新规则
- en: 'The second update rule is as follows:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个更新规则如下：
- en: '![](img/C12626_07_47.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_47.jpg)'
- en: 'Figure 7.47: Second update rule'
  id: totrans-415
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.47：第二个更新规则
- en: '*W* and *H* are updated iteratively until the algorithm converges. The objective
    function can also be shown to be non-increasing. That is, with each iterative
    update of *W* and *H*, the objective function gets closer to the minimum. Note
    that the multiplicative update optimizer, if the update rules are reorganized,
    is a rescaled gradient descent algorithm.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '*W* 和 *H* 会通过迭代更新，直到算法收敛。目标函数也可以证明是非递增的。也就是说，每次迭代更新 *W* 和 *H* 后，目标函数都会更接近最小值。注意，如果重新组织更新规则，乘法更新优化器实际上是一个缩放版的梯度下降算法。'
- en: The final component of building a successful NMF algorithm is initializing the
    *W* and *H* component matrices so that the multiplicative update works quickly.
    A popular approach to initializing matrices is Singular Value Decomposition (SVD),
    which is a generalization of Eigen decomposition. In the implementation of NMF
    done in the forthcoming exercises, the matrices are initialized via non-negative
    Double Singular Value Decomposition, which is basically a more advanced version
    of SVD that is strictly non-negative. The full details of these initialization
    algorithms are not important to understanding NMF. Just note that initialization
    algorithms are used as a starting point for the optimization algorithms and can
    drastically speed up convergence.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 构建成功的 NMF 算法的最后一个组件是初始化 *W* 和 *H* 组件矩阵，以便乘法更新能够快速工作。初始化矩阵的一个常用方法是奇异值分解（SVD），它是特征分解的推广。在接下来的练习中实现的
    NMF 中，矩阵是通过非负双重奇异值分解（Double Singular Value Decomposition）进行初始化的，这基本上是 SVD 的一种更高级版本，且严格非负。对于理解
    NMF，这些初始化算法的具体细节并不重要。只需要注意，初始化算法作为优化算法的起点，能够显著加速收敛过程。
- en: 'Exercise 37: Non-negative Matrix Factorization'
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 37：非负矩阵分解
- en: 'In this exercise, we fit the NMF algorithm and output the same two result tables
    we did with LDA previously. Those tables are the word-topic table, which shows
    the top-10 words associated with each topic, and the document-topic table, which
    shows the top-10 documents associated with each topic. There are two additional
    parameters in the NMF algorithm function that we have not previously discussed,
    which are `alpha` and `l1_ratio`. If an overfit model is of concern, these parameters
    control how (`l1_ratio`) and the extent to which (`alpha`) regularization is applied
    to the objective function. More details can be found in the documentation for
    the scikit-learn library ([https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)):'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们拟合了 NMF 算法，并输出了与之前使用 LDA 得到的相同的两个结果表。这些表格是单词-主题表，显示与每个主题相关的前 10 个单词，以及文档-主题表，显示与每个主题相关的前
    10 个文档。NMF 算法函数中有两个我们之前没有讨论的额外参数，分别是 `alpha` 和 `l1_ratio`。如果担心模型过拟合，这些参数控制正则化应用到目标函数的方式（`l1_ratio`）和程度（`alpha`）。更多详细信息可以在
    scikit-learn 库的文档中找到（[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)）：
- en: 'Define the NMF model and call the `fit` function using the output of the TF-IDF
    vectorizer:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 NMF 模型，并使用 TF-IDF 向量化器的输出调用 `fit` 函数：
- en: '[PRE48]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_07_48.jpg)'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_07_48.jpg)'
- en: 'Figure 7.48: Defining the NMF model'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.48：定义 NMF 模型
- en: 'Run the `get_topics` functions to produce the two output tables:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`get_topics`函数生成两个输出表：
- en: '[PRE49]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print the `W` table:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`W`表：
- en: '[PRE50]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.49: The word-topic table containing probabilities](img/C12626_07_49.jpg)'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.49：包含概率的词汇-主题表](img/C12626_07_49.jpg)'
- en: 'Figure 7.49: The word-topic table containing probabilities'
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.49：包含概率的词汇-主题表
- en: 'Print the `H` table:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`H`表：
- en: '[PRE51]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '![Figure 7.50: The document-topic table containing probabilities](img/C12626_07_50.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.50：包含概率的文档-主题表](img/C12626_07_50.jpg)'
- en: 'Figure 7.50: The document-topic table containing probabilities'
  id: totrans-435
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.50：包含概率的文档-主题表
- en: The word-topic table contains word groupings that suggest the same abstract
    topics that the four-topic LDA model produced in *Exercise 35*, *Trying four topics*.
    However, the interesting part of the comparison is that some of the individual
    words contained in these groupings are new or in a new place in the grouping.
    This is not surprising given that the methodologies are distinct. Given the alignment
    with the topics specified in the original dataset, we have shown that both of
    these methodologies are effective tools for extracting the underlying topic structure
    of the corpus.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇-主题表包含的词汇分组表明与*练习 35*中使用四主题 LDA 模型得到的抽象主题相同。然而，这次比较中有趣的部分是，一些包含在这些分组中的单词是新的，或者在分组中的位置发生了变化。考虑到两种方法论的不同，这是不足为奇的。鉴于与原始数据集中的主题对齐，我们已经证明这两种方法都是提取语料库潜在主题结构的有效工具。
- en: 'Exercise 38: Visualizing NMF'
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 38：可视化 NMF
- en: 'The purpose of this exercise is to visualize the results of NMF. Visualizing
    the results gives insights into the distinctness of the topics and the prevalence
    of each topic in the corpus. In this exercise, we do the visualizing using t-SNE,
    which was discussed fully in *Chapter 6*, *t-Distributed Stochastic Neighbor Embedding
    (t-SNE)*:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的目的是可视化 NMF 的结果。通过可视化结果，可以深入了解各主题的独特性以及每个主题在语料库中的普遍性。在本练习中，我们使用 t-SNE 进行可视化，t-SNE
    已在*第 6 章*中详细讨论过，*t-分布随机邻居嵌入（t-SNE）*：
- en: 'Run `transform` on the cleaned data to get the topic-document allocations.
    Print both the shape and an example of the data:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对清理后的数据运行`transform`，以获取主题-文档分配。打印数据的形状和示例：
- en: '[PRE52]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.51: Shape and example of data](img/C12626_07_51.jpg)'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 7.51：数据的形状和示例](img/C12626_07_51.jpg)'
- en: 'Figure 7.51: Shape and example of data'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.51：数据的形状和示例
- en: 'Run the `plot_tsne` function to fit a t-SNE model and plot the results:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`plot_tsne`函数以拟合 t-SNE 模型并绘制结果：
- en: '[PRE53]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The plot looks as follows:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图形如下：
- en: '![Figure 7.52: t-SNE plot with metrics summarizing the topic distribution across
    the corpus](img/C12626_07_52.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.52：t-SNE 图，带有总结语料库中主题分布的指标](img/C12626_07_52.jpg)'
- en: 'Figure 7.52: t-SNE plot with metrics summarizing the topic distribution across
    the corpus'
  id: totrans-448
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7.52：t-SNE 图，带有总结语料库中主题分布的指标
- en: The t-SNE plot, with no threshold specified, shows some topic overlap and a
    clear discrepancy in the topic frequency across the corpus. These two facts explain
    why, when using perplexity, the optimal number of topics is three. There seems
    to be some correlation between topics that the model can't fully accommodate.
    Even with the correlation between topics, the model is finding the topics it should
    when the number of topics is set to four.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 图，未指定阈值，显示了部分主题重叠，并且跨语料库的主题频率有明显的差异。这两个事实解释了为什么在使用困惑度时，最佳的主题数是三个。似乎有一些主题之间的相关性，而模型未能完全处理这些相关性。即使有这些主题之间的相关性，当主题数设置为四时，模型仍能找出应该有的主题。
- en: To recap, NMF is a non-probabilistic topic model that seeks to answer the same
    question LDA is trying to answer. It uses a popular concept of linear algebra
    known as matrix factorization, which is the process of breaking a large and intractable
    matrix down into smaller and more easily interpretable matrices that can be leveraged
    to answer many questions about the data. Remember that the non-negative requirement
    is not rooted in mathematics, but in the data itself. It does not make sense for
    the components of any documents to be negative. In many cases, NMF does not perform
    as well as LDA, because LDA incorporates prior distributions that add an extra
    layer of information to help inform the topic word groupings. However, we know
    that there are cases, especially when the topics are highly correlated, when NMF
    is the better performer. One of those cases was the headline data on which all
    the exercises were based.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，NMF（非负矩阵分解）是一个非概率性主题模型，旨在回答与LDA（潜在狄利克雷分配）相同的问题。它使用了一个在线性代数中非常流行的概念——矩阵分解，这是将一个庞大且难以处理的矩阵分解成较小且更容易解释的矩阵的过程，这些矩阵可以帮助回答有关数据的许多问题。请记住，非负要求并不是源于数学，而是源于数据本身。任何文档的组件不应该是负数。在许多情况下，NMF的表现不如LDA，因为LDA结合了先验分布，为主题词分组提供了额外的信息层。然而，我们知道在某些情况下，特别是当主题高度相关时，NMF可能表现得更好。这些情况之一就是所有练习所基于的头条数据。
- en: 'Activity 17: Non-Negative Matrix Factorization'
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动17：非负矩阵分解
- en: This activity is the summation of the topic modeling analysis done on the health
    Twitter data loaded and cleaned in activity one, and on which LDA was done in
    activity two. The execution of NMF is straightforward and requires limited coding,
    so I suggest taking this opportunity to play with the parameters of the model
    while thinking about the limitations and benefits of NMF.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动是对健康Twitter数据进行主题建模分析的总结，这些数据在第一项活动中被加载并清理过，并且在第二项活动中使用了LDA。执行NMF非常简单，需要的编码很少，因此我建议利用这个机会调整模型参数，同时思考NMF的局限性和优点。
- en: 'Here are the steps to complete the activity:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动的步骤如下：
- en: Create the appropriate bag-of-words model and output the feature names as another
    variable.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建适当的词袋模型，并将特征名称输出为另一个变量。
- en: Define and fit the NMF algorithm using the number of topics (`n_components`)
    value from activity two.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第二项活动中的主题数量（`n_components`）值定义并拟合NMF算法。
- en: Get the topic-document and word-topic result tables. Take a few minutes to explore
    the word groupings and try to define the abstract topics. Can you quantify the
    meanings of the word groupings? Do the word groupings make sense? Are the results
    similar to those produced using LDA?
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取主题-文档和词-主题结果表。花几分钟时间探索词语分组，并尝试定义抽象的主题。你能量化这些词语分组的含义吗？这些词语分组是否合理？结果是否与使用LDA时的结果相似？
- en: Adjust the model parameters and rerun *step 3* and *step 4*. How do the results
    change?
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整模型参数并重新运行*第3步*和*第4步*。结果如何变化？
- en: 'The output will be as follows:'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/C12626_07_53.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_07_53.jpg)'
- en: 'Figure 7.53: The word-topic table with probabilities'
  id: totrans-460
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7.53：带有概率的词-主题表
- en: Note
  id: totrans-461
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 364.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第364页找到。
- en: Summary
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'When faced with the task of extracting information from an as yet unseen large
    collection of documents, topic modeling is a great approach, as it provides insights
    into the underlying structure of the documents. That is, topic models find word
    groupings using proximity, not context. In this chapter, we have learned how to
    apply two of the most common and most effective topic modeling algorithms: latent
    Dirichlet allocation and non-negative matrix factorization. We should now feel
    comfortable cleaning raw text documents using several different techniques; techniques
    that can be utilized in many other modeling scenarios. We continued by learning
    how to convert the cleaned corpus into the appropriate data structure of per-document
    raw word counts or word weights by applying bag-of-words models. The main focus
    of the chapter was fitting the two topic models, including optimizing the number
    of topics, converting the output to easy-to-interpret tables, and visualizing
    the results. With this information, you should be able to apply fully functioning
    topic models to derive value and insights for your business.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 当面临从尚未见过的大量文档集合中提取信息的任务时，主题建模是一个很好的方法，因为它提供了对文档底层结构的洞察。也就是说，主题模型通过接近度而非上下文来寻找词语分组。在这一章中，我们学习了如何应用两种最常见且最有效的主题建模算法：潜在狄利克雷分配（LDA）和非负矩阵分解（NMF）。现在我们应该能够通过多种不同的技术来清理原始文本文档；这些技术可以应用于许多其他建模场景。我们继续学习如何通过应用词袋模型将清理后的语料库转换为每篇文档的原始词频或词权重的适当数据结构。本章的主要焦点是拟合这两种主题模型，包括优化主题数、将输出转换为易于解释的表格，并可视化结果。有了这些信息，你应该能够应用功能完备的主题模型，为你的业务提取价值和洞察。
- en: In the next chapter, we will change directions entirely. We will deep dive into
    market basket analysis.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将完全改变方向。我们将深入探讨市场篮子分析。
