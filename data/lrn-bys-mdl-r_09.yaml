- en: Chapter 9. Bayesian Modeling at Big Data Scale
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。大数据规模下的贝叶斯建模
- en: When we learned the principles of Bayesian inference in [Chapter 3](part0030.xhtml#aid-SJGS2
    "Chapter 3. Introducing Bayesian Inference"), *Introducing Bayesian Inference*,
    we saw that as the amount of training data increases, contribution to the parameter
    estimation from data overweighs that from the prior distribution. Also, the uncertainty
    in parameter estimation decreases. Therefore, you may wonder why one needs Bayesian
    modeling in large-scale data analysis. To answer this question, let us look at
    one such problem, which is building recommendation systems for e-commerce products.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第3章](part0030.xhtml#aid-SJGS2 "第3章。介绍贝叶斯推理")中学习了贝叶斯推理原理，即《介绍贝叶斯推理》时，我们发现随着训练数据量的增加，数据对参数估计的贡献超过了先验分布的贡献。此外，参数估计的不确定性也会降低。因此，你可能想知道为什么在大规模数据分析中需要贝叶斯建模。为了回答这个问题，让我们看看这样一个问题，即为电子商务产品构建推荐系统。
- en: In a typical e-commerce store, there will be millions of users and tens of thousands
    of products. However, each user would have purchased only a small fraction (less
    than 10%) of all the products found in the store in their lifetime. Let us say
    the e-commerce store is collecting users' feedback for each product sold as a
    rating on a scale of 1 to 5\. Then, the store can create a user-product rating
    matrix to capture the ratings of all users. In this matrix, rows would correspond
    to users and columns would correspond to products. The entry of each cell would
    be the rating given by the user (corresponding to the row) to the product (corresponding
    to the column). Now, it is easy to see that although the overall size of this
    matrix is huge, only less than 10% entries would have values since every user
    would have bought only less than 10% products from the store. So, this is a highly
    sparse dataset. Whenever there is a machine learning task where, even though the
    overall data size is huge, the data is highly sparse, overfitting can happen and
    one should rely on Bayesian methods (reference 1 in the *References* section of
    this chapter). Also, many models such as Bayesian networks, Latent Dirichlet allocation,
    and deep belief networks are built on the Bayesian inference paradigm.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的电子商务商店中，可能会有数百万用户和数万种产品。然而，每个用户在其一生中可能只购买过商店中所有产品的一小部分（不到10%）。让我们假设电子商务商店正在收集每个销售产品的用户反馈，以1到5分的评分尺度进行。然后，商店可以创建一个用户-产品评分矩阵来捕捉所有用户的评分。在这个矩阵中，行对应于用户，列对应于产品。每个单元格的值将是用户（对应于行）对产品（对应于列）给出的评分。现在，很容易看出，尽管这个矩阵的整体大小很大，但只有不到10%的条目会有值，因为每个用户只从商店购买了不到10%的产品。所以，这是一个高度稀疏的数据集。每当存在一个机器学习任务，尽管整体数据量很大，但数据高度稀疏时，可能会发生过拟合，并且应该依赖于贝叶斯方法（参考本章“参考文献”部分的第1条）。此外，许多模型，如贝叶斯网络、潜在狄利克雷分配和深度信念网络，都是建立在贝叶斯推理范式之上的。
- en: When these models are trained on a large dataset, such as text corpora from
    Reuters, then the underlying problem is large-scale Bayesian modeling. As it is,
    Bayesian modeling is computationally intensive since we have to estimate the whole
    posterior distribution of parameters and also do model averaging of the predictions.
    The presence of large datasets will make the situation even worse. So what are
    the computing frameworks that we can use to do Bayesian learning at a large scale
    using R? In the next two sections, we will discuss some of the latest developments
    in this area.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些模型在大数据集上训练，例如来自路透社的文本语料库时，那么潜在的问题是大规模贝叶斯建模。实际上，贝叶斯建模是计算密集型的，因为我们必须估计参数的整个后验分布，并且还要对预测进行模型平均。大数据集的存在将使情况变得更糟。那么，我们能够使用哪些计算框架在R中进行大规模贝叶斯学习呢？在接下来的两个部分中，我们将讨论这个领域的一些最新发展。
- en: Distributed computing using Hadoop
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Hadoop进行分布式计算
- en: In the last decade, tremendous progress was made in distributed computing when
    two research engineers from Google developed a computing paradigm called the **MapReduce**
    framework and an associated distributed filesystem called Google File System (reference
    2 in the *References* section of this chapter). Later on, Yahoo developed an open
    source version of this distributed filesystem named **Hadoop** that became the
    hallmark of Big Data computing. Hadoop is ideal for processing large amounts of
    data, which cannot fit into the memory of a single large computer, by distributing
    the data into multiple computers and doing the computation on each node locally
    from the disk. An example would be extracting relevant information from log files,
    where typically the size of data for a month would be in the order of terabytes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，当两位来自 Google 的研究工程师开发了一种名为 **MapReduce** 框架的计算范式以及一个相关的分布式文件系统 Google
    文件系统（本章 *参考文献* 部分的参考 2）时，分布式计算取得了巨大的进步。后来，Yahoo 开发了一个名为 **Hadoop** 的开源分布式文件系统版本，这成为了大数据计算的一个标志。Hadoop
    通过将数据分布到多台计算机并在每个节点上从磁盘本地执行计算，非常适合处理无法适应单个大型计算机内存的大量数据。一个例子是从日志文件中提取相关信息，通常一个月的数据量在千兆字节级别。
- en: To use Hadoop, one has to write programs using MapReduce framework to parallelize
    the computing. A Map operation splits the data into multiple key-value pairs and
    sends it to different nodes. At each of those nodes, a computation is done on
    each of the key-value pairs. Then, there is a shuffling operation where all the
    pairs with the same value of key are brought together. After this, a Reduce operation
    sums up all the results corresponding to the same key from the previous computation
    step. Typically, these MapReduce operations can be written using a high-level
    language called **Pig**. One can also write MapReduce programs in R using the
    **RHadoop** package, which we will describe in the next section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Hadoop，必须使用 MapReduce 框架编写程序以并行化计算。Map 操作将数据分割成多个键值对，并发送到不同的节点。在每个节点上，对每个键值对进行计算。然后，有一个洗牌操作，将具有相同键值的所有键值对聚集在一起。之后，Reduce
    操作将前一步计算中对应相同键的所有结果求和。通常，这些 MapReduce 操作可以使用称为 **Pig** 的高级语言编写。也可以使用 **RHadoop**
    包在 R 中编写 MapReduce 程序，我们将在下一节中描述。
- en: RHadoop for using Hadoop from R
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 R 使用 RHadoop 操作 Hadoop
- en: RHadoop is a collection of open source packages using which an R user can manage
    and analyze data stored in the **Hadoop Distributed File System** (**HDFS**).
    In the background, RHadoop will translate these as MapReduce operations in Java
    and run them on HDFS.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 是一系列开源包的集合，R 用户可以使用这些包管理和分析存储在 **Hadoop 分布式文件系统**（**HDFS**）中的数据。在后台，RHadoop
    将这些操作转换为 Java 中的 MapReduce 操作并在 HDFS 上运行。
- en: 'The various packages in RHadoop and their uses are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RHadoop 中的各种包及其用途如下：
- en: '**rhdfs**: Using this package, a user can connect to an HDFS from R and perform
    basic actions such as read, write, and modify files.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rhdfs**：使用此包，用户可以从 R 连接到 HDFS 并执行基本操作，如读取、写入和修改文件。'
- en: '**rhbase**: This is the package to connect to a HBASE database from R and to
    read, write, and modify tables.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rhbase**：这是连接到 HBASE 数据库并读取、写入和修改表的包。'
- en: '**plyrmr**: Using this package, an R user can do the common data manipulation
    tasks such as the slicing and dicing of datasets. This is similar to the function
    of packages such as **plyr** or **reshape2**.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**plyrmr**：使用此包，R 用户可以执行常见的数据操作任务，如数据集的切片和切块。这与 **plyr** 或 **reshape2** 等包的功能类似。'
- en: '**rmr2**: Using this package, a user can write MapReduce functions in R and
    execute them in an HDFS.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rmr2**：使用此包，用户可以在 R 中编写 MapReduce 函数并在 HDFS 中执行它们。'
- en: Unlike the other packages discussed in this book, the packages associated with
    RHadoop are not available from CRAN. They can be downloaded from the GitHub repository
    at [https://github.com/RevolutionAnalytics](https://github.com/RevolutionAnalytics)
    and are installed from the local drive.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中讨论的其他包不同，与 RHadoop 相关的包不可从 CRAN 获取。它们可以从 GitHub 仓库 [https://github.com/RevolutionAnalytics](https://github.com/RevolutionAnalytics)
    下载，并从本地驱动器安装。
- en: 'Here is a sample MapReduce code written using the rmr2 package to count the
    number of words in a corpus (reference 3 in the *References* section of this chapter):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用 rmr2 包编写的 MapReduce 代码示例，用于统计语料库中的单词数量（本章 *参考文献* 部分的参考 3）：
- en: 'The first step involves loading the `rmr2` library:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是加载 `rmr2` 库：
- en: '[PRE0]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The second step involves writing the Map function. This function takes each
    line in the text document and splits it into words. Each word is taken as a token.
    The function emits key-value pairs where each distinct word is a *key* and *value
    = 1*:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步涉及编写 Map 函数。这个函数将文本文档中的每一行分割成单词。每个单词被视为一个标记。该函数输出键值对，其中每个不同的单词是一个 *键*，*值
    = 1*：
- en: '[PRE1]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The third step involves writing a reduce function. This function groups all
    the same *key* from different mappers and sums their *value*. Since, in this case,
    each word is a *key* and the *value = 1*, the output of the reduce will be the
    count of the words:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三步涉及编写 Reduce 函数。这个函数将来自不同 Mapper 的相同 *键* 进行分组并求和它们的 *值*。由于在这种情况下，每个单词都是一个
    *键*，且 *值 = 1*，Reduce 的输出将是单词的计数：
- en: '[PRE2]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The fourth step involves writing a word count function combining the map and
    reduce functions and executing this function on a file named `hdfs.data` stored
    in the HDFS containing the input text:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四步涉及编写一个结合 Map 和 Reduce 函数的词频统计函数，并在名为 `hdfs.data` 的文件上执行此函数，该文件存储在包含输入文本的
    HDFS 中：
- en: '[PRE3]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The fifth step involves getting the output file from HDFS and printing the
    top five lines:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第五步涉及从 HDFS 获取输出文件并打印前五行：
- en: '[PRE4]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Spark – in-memory distributed computing
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark – 内存分布式计算
- en: One of the issues with Hadoop is that after a MapReduce operation, the resulting
    files are written to the hard disk. Therefore, when there is a large data processing
    operation, there would be many read and write operations on the hard disk, which
    makes processing in Hadoop very slow. Moreover, the network latency, which is
    the time required to shuffle data between different nodes, also contributes to
    this problem. Another disadvantage is that one cannot make real-time queries from
    the files stored in HDFS. For machine learning problems, during training phase,
    the MapReduce will not persist over iterations. All this makes Hadoop not an ideal
    platform for machine learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 的问题之一是在 MapReduce 操作之后，结果文件被写入硬盘。因此，当进行大量数据处理操作时，硬盘上会有许多读写操作，这使得 Hadoop
    的处理速度非常慢。此外，网络延迟，即在不同节点之间洗牌数据所需的时间，也加剧了这个问题。另一个缺点是，无法从存储在 HDFS 中的文件中进行实时查询。对于机器学习问题，在训练阶段，MapReduce
    不会在迭代中持久化。所有这些都使得 Hadoop 不是一个理想的机器学习平台。
- en: 'A solution to this problem was invented at Berkeley University''s AMP Lab in
    2009\. This came out of the PhD work of Matei Zaharia, a Romanian born computer
    scientist. His paper *Resilient Distributed Datasets: A Fault-Tolerant Abstraction
    for In-Memory Cluster Computing* (reference 4 in the *References* section of this
    chapter) gave rise to the Spark project that eventually became a fully open source
    project under Apache. Spark is an in-memory distributed computing framework that
    solves many of the problems of Hadoop mentioned earlier. Moreover, it supports
    more type of operations that just MapReduce. Spark can be used for processing
    iterative algorithms, interactive data mining, and streaming applications. It
    is based on an abstraction called **Resilient Distributed Datasets** (**RDD**).
    Similar to HDFS, it is also fault-tolerant.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '2009 年，加州大学伯克利分校的 AMP 实验室发明了这种解决方案。这是罗马尼亚出生的计算机科学家 Matei Zaharia 博士的博士论文 *Resilient
    Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing*（本章“参考文献”部分的第
    4 条参考文献）的成果，该论文催生了 Spark 项目，最终成为 Apache 下的一个完全开源项目。Spark 是一个内存分布式计算框架，解决了之前提到的许多
    Hadoop 问题。此外，它支持比 MapReduce 更多的操作类型。Spark 可以用于处理迭代算法、交互式数据挖掘和流式应用。它基于一个称为 **Resilient
    Distributed Datasets**（**RDD**）的抽象。类似于 HDFS，它也是容错的。'
- en: Spark is written in a language called Scala. It has interfaces to use from Java
    and Python and from the recent version 1.4.0; it also supports R. This is called
    SparkR, which we will describe in the next section. The four classes of libraries
    available in Spark are SQL and DataFrames, Spark Streaming, MLib (machine learning),
    and GraphX (graph algorithms). Currently, SparkR supports only SQL and DataFrames;
    others are definitely in the roadmap. Spark can be downloaded from the Apache
    project page at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Starting from 1.4.0 version, SparkR is included in Spark and no separate download
    is required.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用一种名为Scala的语言编写的。它具有从Java和Python使用的接口，从最近的1.4.0版本开始；它还支持R。这被称为SparkR，我们将在下一节中描述。Spark中可用的四个库类别是SQL和DataFrames、Spark
    Streaming、MLib（机器学习）和GraphX（图算法）。目前，SparkR仅支持SQL和DataFrames；其他肯定在路线图中。Spark可以从Apache项目页面[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载。从1.4.0版本开始，SparkR包含在Spark中，无需单独下载。
- en: SparkR
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR
- en: 'Similar to RHadoop, SparkR is an R package that allows R users to use Spark
    APIs through the `RDD` class. For example, using SparkR, users can run jobs on
    Spark from RStudio. SparkR can be evoked from RStudio. To enable this, include
    the following lines in your `.Rprofile` file that R uses at startup to initialize
    the environments:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与RHadoop类似，SparkR是一个R包，允许R用户通过`RDD`类使用Spark API。例如，使用SparkR，用户可以从RStudio上运行Spark作业。SparkR可以从RStudio中调用。为了启用此功能，请在R启动时初始化环境的`.Rprofile`文件中包含以下行：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once this is done, start RStudio and enter the following commands to start
    using SparkR:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，启动RStudio并输入以下命令以开始使用SparkR：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As mentioned, as of the latest version 1.5 when this chapter is in writing,
    SparkR supports limited functionalities of R. This mainly includes data slicing
    and dicing and summary stat functions. The current version does not support the
    use of contributed R packages; however, it is planned for a future release. On
    machine learning, currently SparkR supports the `glm( )` function. We will do
    an example in the next section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当本章撰写时，SparkR支持R的有限功能。这主要包括数据切片和切块以及汇总统计函数。当前版本不支持使用贡献的R包；然而，计划在未来的版本中实现。在机器学习方面，当前SparkR支持`glm()`函数。我们将在下一节中做一个示例。
- en: Linear regression using SparkR
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行线性回归
- en: 'In the following example, we will illustrate how to use SparkR for machine
    learning. For this, we will use the same dataset of energy efficiency measurements
    that we used for linear regression in [Chapter 5](part0041.xhtml#aid-173721 "Chapter 5. Bayesian
    Regression Models"), *Bayesian Regression Models*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将说明如何使用SparkR进行机器学习。为此，我们将使用与[第5章](part0041.xhtml#aid-173721 "第5章。贝叶斯回归模型")中线性回归相同的能源效率测量数据集，*贝叶斯回归模型*：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Computing clusters on the cloud
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云计算集群
- en: In order to process large datasets using Hadoop and associated R packages, one
    needs a cluster of computers. In today's world, it is easy to get using cloud
    computing services provided by Amazon, Microsoft, and others. One needs to pay
    only for the amount of CPU and storage used. No need for upfront investments on
    infrastructure. The top four cloud computing services are AWS by Amazon, Azure
    by Microsoft, Compute Cloud by Google, and Bluemix by IBM. In this section, we
    will discuss running R programs on AWS. In particular, you will learn how to create
    an AWS instance; install R, RStudio, and other packages in that instance; develop
    and run machine learning models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Hadoop和相关R包处理大型数据集，需要一个计算机集群。在当今世界，使用亚马逊、微软和其他人提供的云计算服务很容易。只需支付使用的CPU和存储量。无需在基础设施上进行前期投资。前四大云计算服务是亚马逊的AWS、微软的Azure、谷歌的Compute
    Cloud和IBM的Bluemix。在本节中，我们将讨论在AWS上运行R程序。特别是，你将学习如何创建AWS实例；在该实例中安装R、RStudio和其他包；开发和运行机器学习模型。
- en: Amazon Web Services
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊网络服务
- en: Popularly known as AWS, Amazon Web Services started as an internal project in
    Amazon in 2002 to meet the dynamic computing requirements to support their e-commerce
    business. This grew as an **infrastructure as a service** and in 2006 Amazon launched
    two services to the world, **Simple Storage Service** (**S3**) and **Elastic Computing
    Cloud** (**EC2**). From there, AWS grew at incredible pace. Today, they have more
    than 40 different types of services using millions of servers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 广为人知的 AWS，即亚马逊网络服务，始于 2002 年亚马逊的一个内部项目，旨在满足支持其电子商务业务的动态计算需求。这作为一个**基础设施即服务**项目发展起来，2006
    年亚马逊向世界推出了两项服务，**简单存储服务**（**S3**）和**弹性计算云**（**EC2**）。从那时起，AWS 以惊人的速度增长。如今，他们拥有超过
    40 种不同类型的服务，使用数百万台服务器。
- en: Creating and running computing instances on AWS
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 AWS 上创建和运行计算实例
- en: The best place to learn how to set up an AWS account and start using EC2 is
    the freely available e-book from Amazon Kindle store named *Amazon Elastic Compute
    Cloud (EC2) User Guide* (reference 6 in the *References* section of this chapter).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何设置 AWS 账户并开始使用 EC2 的最佳地方是亚马逊 Kindle 商店中免费提供的电子书，名为 *Amazon Elastic Compute
    Cloud (EC2) 用户指南*（参考本章节 *参考文献* 部分的第 6 条）。
- en: 'Here, we only summarize the essential steps involved in the process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们仅总结涉及此过程的基本步骤：
- en: Create an AWS account.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 AWS 账户。
- en: Sign in to the AWS management console ([https://aws.amazon.com/console/](https://aws.amazon.com/console/)).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录 AWS 管理控制台 ([https://aws.amazon.com/console/](https://aws.amazon.com/console/))。
- en: Click on the EC2 service.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 EC2 服务。
- en: Choose **Amazon Machine Instance (AMI)**.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **Amazon Machine Instance (AMI**)。
- en: Choose an instance type.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择实例类型。
- en: Create a public-private key-pair.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建公私钥对。
- en: Configure instance.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置实例。
- en: Add storage.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加存储。
- en: Tag instance.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记实例。
- en: Configure a security group (policy specifying who can access the instance).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置安全组（指定谁可以访问实例的策略）。
- en: Review and launch the instance.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查并启动实例。
- en: Log in to your instance using SSH (from Linux/Ubuntu), Putty (from Windows),
    or a browser using the private key provided at the time of configuring security
    and the IP address given at the time of launching. Here, we are assuming that
    the instance you have launched is a Linux instance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SSH（从 Linux/Ubuntu）、Putty（从 Windows）或使用配置安全时提供的私钥和启动时给出的 IP 地址通过浏览器登录到您的实例。这里，我们假设您启动的实例是一个
    Linux 实例。
- en: Installing R and RStudio
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 R 和 RStudio
- en: 'To install R and RStudio, you need to be an authenticated user. So, create
    a new user and give the user administrative privilege (sudo). After that, execute
    the following steps from the Ubuntu shell:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 R 和 RStudio，您需要成为认证用户。因此，创建一个新用户并授予用户管理员权限（sudo）。之后，从 Ubuntu shell 执行以下步骤：
- en: Edit the `/etc/apt/sources.list` file.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑 `/etc/apt/sources.list` 文件。
- en: 'Add the following line at the end:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在末尾添加以下行：
- en: '[PRE8]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Get the keys for the repository to run:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取运行存储库的密钥：
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Update the package list:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新包列表：
- en: '[PRE10]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Install the latest version of R:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 R 的最新版本：
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Install gdebi to install Debian packages from the local disk:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 gdebi 以从本地磁盘安装 Debian 软件包：
- en: '[PRE12]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Download the RStudio package:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 RStudio 软件包：
- en: '[PRE13]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Install RStudio:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 RStudio：
- en: '[PRE14]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once the installation is completed successfully, RStudio running on your AWS
    instance can be accessed from a browser. For this, open a browser and enter the
    URL `<your.aws.ip.no>:8787`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功完成后，运行在您的 AWS 实例上的 RStudio 可以通过浏览器访问。为此，请打开浏览器并输入 URL `<your.aws.ip.no>:8787`。
- en: If you are able to use your RStudio running on the AWS instance, you can then
    install other packages such as rhdfs, rmr2, and more from RStudio, build any machine
    learning models in R, and run them on the AWS cloud.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够使用运行在 AWS 实例上的 RStudio，那么你可以从 RStudio 安装其他包，例如 rhdfs、rmr2 等，并在 R 中构建任何机器学习模型，然后在
    AWS 云上运行它们。
- en: Apart from R and RStudio, AWS also supports Spark (and hence SparkR). In the
    following section, you will learn how to run Spark on an EC2 cluster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 R 和 RStudio，AWS 还支持 Spark（因此也支持 SparkR）。在下一节中，您将学习如何在 EC2 集群上运行 Spark。
- en: Running Spark on EC2
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 EC2 上运行 Spark
- en: 'You can launch and manage Spark clusters on Amazon EC2 using the `spark-ec2`
    script located in the `ec2` directory of Spark in your local machine. To launch
    a Spark cluster on EC2, use the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用位于您本地机器 Spark 的 `ec2` 目录中的 `spark-ec2` 脚本来在 Amazon EC2 上启动和管理 Spark 集群。要在
    EC2 上启动 Spark 集群，请按照以下步骤操作：
- en: Go to the `ec2` directory in the Spark folder in your local machine.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的本地机器 Spark 文件夹中的 `ec2` 目录下。
- en: 'Run the following command:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令：
- en: '[PRE15]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, `<keypair>` is the name of the keypair you used for launching the EC2
    service mentioned in the *Creating and running computing instances on AWS* section
    of this chapter. The `<key-file>` is the path in your local machine where the
    private key has been downloaded and kept. The number of worker nodes is specified
    by `<num-slaves>`.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`<keypair>` 是您用于启动本章“在 AWS 上创建和运行计算实例”部分中提到的 EC2 服务的密钥对名称。`<key-file>` 是您本地机器上私钥已下载并保存的路径。工作节点数由
    `<num-slaves>` 指定。
- en: 'To run your programs in the cluster, first SSH into the cluster using the following
    command:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在集群上运行您的程序，请首先使用以下命令通过 SSH 连接到集群：
- en: '[PRE16]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After logging into the cluster, you can use Spark as you use on the local machine.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 登录到集群后，您可以使用与在本地机器上使用相同的方式使用 Spark。
- en: More details on how to use Spark on EC2 can be found in the Spark documentation
    and AWS documentation (references 5, 6, and 7 in the *References* section of the
    chapter).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 文档和 AWS 文档中可以找到有关如何在 EC2 上使用 Spark 的更多详细信息（章节“参考文献”部分的第 5、6 和 7 条）。
- en: Microsoft Azure
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Microsoft Azure
- en: Microsoft Azure has full support for R and Spark. Microsoft had bought Revolution
    Analytics, a company that started building and supporting an enterprise version
    of R. Apart from this, Azure has a machine learning service where there are APIs
    for some Bayesian machine learning models as well. A nice video tutorial of how
    to launch instances on Azure and how to use their machine learning as a service
    can be found at the Microsoft Virtual Academy website (reference 8 in the *References*
    section of the chapter).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Azure 对 R 和 Spark 提供了全面支持。Microsoft 收购了 Revolution Analytics 公司，该公司开始构建并支持
    R 的企业版。除此之外，Azure 还有一个机器学习服务，其中包含一些贝叶斯机器学习模型的 API。有关如何在 Azure 上启动实例以及如何使用其机器学习服务的视频教程可以在
    Microsoft Virtual Academy 网站上找到（章节“参考文献”部分的第 8 条）。
- en: IBM Bluemix
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM Bluemix
- en: Bluemix has full support for R through the full set of R libraries available
    on their instances. IBM also has integration of Spark into their cloud services
    in their roadmap plans. More details can be found at their documentation page
    (reference 9 in the *References* section of the chapter).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Bluemix 通过其实例上可用的完整 R 库集对 R 完全支持。IBM 在其路线图计划中也包含了将 Spark 集成到其云服务中。更多详细信息可以在他们的文档页面上找到（章节“参考文献”部分的第
    9 条）。
- en: Other R packages for large scale machine learning
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他用于大规模机器学习的 R 包
- en: 'Apart from RHadoop and SparkR, there are several other native R packages specifically
    built for large-scale machine learning. Here, we give a brief overview of them.
    Interested readers should refer to *CRAN Task View: High-Performance and Parallel
    Computing with R* (reference 10 in the *References* section of the chapter).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 RHadoop 和 SparkR 之外，还有几个专门为大规模机器学习构建的本地 R 包。在这里，我们简要概述它们。感兴趣的读者应参考 *CRAN
    任务视图：使用 R 进行高性能和并行计算*（章节“参考文献”部分的第 10 条）。
- en: Though R is single-threaded, there exists several packages for parallel computation
    in R. Some of the well-known packages are **Rmpi** (R version of the popular message
    passing interface), **multicore**, **snow** (for building R clusters), and **foreach**.
    From R 2.14.0, a new package called **parallel** started shipping with the base
    R. We will discuss some of its features here.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 R 是单线程的，但存在几个用于 R 的并行计算包。其中一些知名的包是 **Rmpi**（流行的消息传递接口的 R 版本）、**multicore**、**snow**（用于构建
    R 集群）和 **foreach**。从 R 2.14.0 版本开始，一个新的名为 **parallel** 的包开始随基础 R 一起发货。我们将在下面讨论其一些功能。
- en: The parallel R package
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行 R 包
- en: The **parallel** package is built on top of the multicore and snow packages.
    It is useful for running a single program on multiple datasets such as K-fold
    cross validation. It can be used for parallelizing in a single machine over multiple
    CPUs/cores or across several machines. For parallelizing across a cluster of machines,
    it evokes MPI (message passing interface) using the Rmpi package.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**并行**包建立在多核和 snow 包之上。它适用于在多个数据集上运行单个程序，例如 K 折交叉验证。它可以用于在单个机器上的多个 CPU/核心或跨多台机器进行并行化。对于跨机器集群的并行化，它通过
    Rmpi 包调用 MPI（消息传递接口）。'
- en: We will illustrate the use of parallel package with the simple example of computing
    a square of numbers in the list 1:100000\. This example will not work in Windows
    since the corresponding R does not support the multicore package. It can be tested
    on any Linux or OS X platform.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过计算列表 1:100000 的数字平方的简单示例来说明并行包的使用。此示例在 Windows 上无法工作，因为相应的 R 不支持多核包。它可以在任何
    Linux 或 OS X 平台上进行测试。
- en: 'The sequential way of performing this operation is to use the `lapply` function
    as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作的顺序方法是使用`lapply`函数，如下所示：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Using the `mclapply` function of the parallel package, this computation can
    be achieved in much less time:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用并行包中的`mclapply`函数，可以在更短的时间内完成这个计算：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If the dataset is so large that it needs a cluster of computers, we can use
    the `parLapply` function to run the program over a cluster. This needs the Rmpi
    package:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集非常大，需要使用计算机集群，我们可以使用`parLapply`函数在集群上运行程序。这需要Rmpi包：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The foreach R package
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: The foreach R package
- en: 'This is a new looping construct in R that can be executed in parallel across
    multicores or clusters. It has two important operators: `%do%` for repeatedly
    doing a task and `%dopar%` for executing tasks in parallel.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在R中的一种新的循环结构，可以在多核或集群上并行执行。它有两个重要的运算符：`%do%`用于重复执行任务，`%dopar%`用于并行执行任务。
- en: 'For example, the squaring function we discussed in the previous section can
    be implemented using a single line command using the foreach package:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在前一个章节中讨论的平方函数中使用foreach包的单行命令来实现：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will also do an example of quick sort using the `foreach` function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用`foreach`函数的例子来演示快速排序：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: These packages are still undergoing a lot of development. They have not yet
    been used in a large way for Bayesian modeling. It is easy to use them for Bayesian
    inference applications such as Monte Carlo simulations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包仍在进行大量开发。它们还没有被广泛用于贝叶斯建模。它们很容易用于贝叶斯推断应用，如蒙特卡洛模拟。
- en: Exercises
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Revisit the classification problem in [Chapter 6](part0049.xhtml#aid-1ENBI1
    "Chapter 6. Bayesian Classification Models"), *Bayesian Classification Models*.
    Repeat the same problem using the `glm()` function of SparkR.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回顾[第6章](part0049.xhtml#aid-1ENBI1 "第6章。贝叶斯分类模型")中的分类问题，*贝叶斯分类模型*。使用SparkR的`glm()`函数重复相同的问题。
- en: Revisit the linear regression problem, we did in this chapter, using SparkR.
    After creating the AWS instance, repeat this problem using RStudio server on AWS.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用SparkR回顾本章中我们做的线性回归问题。在创建AWS实例后，使用AWS上的RStudio服务器重复这个问题。
- en: References
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '"MapReduce Implementation of Variational Bayesian Probabilistic Matrix Factorization
    Algorithm". In: IEEE Conference on Big Data. pp 145-152\. 2013'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '"变分贝叶斯概率矩阵分解算法的MapReduce实现"。在：IEEE大数据会议。第145-152页。2013年'
- en: 'Dean J. and Ghemawat S. "MapReduce: Simplified Data Processing on Large Clusters".
    Communications of the ACM 51 (1). 107-113'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Dean J. and Ghemawat S. "MapReduce: Simplified Data Processing on Large Clusters".
    Communications of the ACM 51 (1). 107-113'
- en: '[https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R](https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R)'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R](https://github.com/jeffreybreen/tutorial-rmr2-airline/blob/master/R/1-wordcount.R)'
- en: 'Chowdhury M., Das T., Dave A., Franklin M.J., Ma J., McCauley M., Shenker S.,
    Stoica I., and Zaharia M. "Resilient Distributed Datasets: A Fault-Tolerant Abstraction
    for In-Memory Cluster Computing". NSDI 2012\. 2012'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chowdhury M., Das T., Dave A., Franklin M.J., Ma J., McCauley M., Shenker S.,
    Stoica I.和Zaharia M. "弹性分布式数据集：内存集群计算的容错抽象"。NSDI 2012。2012年
- en: '*Amazon Elastic Compute Cloud (EC2) User Guide*, Kindle e-book by Amazon Web
    Services, updated April 9, 2014'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*亚马逊弹性计算云（EC2）用户指南*，亚马逊网络服务Kindle电子书，更新于2014年4月9日'
- en: Spark documentation for AWS at [http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark文档在AWS上的说明，请参阅[http://spark.apache.org/docs/latest/ec2-scripts.html](http://spark.apache.org/docs/latest/ec2-scripts.html)
- en: AWS documentation for Spark at [http://aws.amazon.com/elasticmapreduce/details/spark/](http://aws.amazon.com/elasticmapreduce/details/spark/)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS上的Spark文档，请参阅[http://aws.amazon.com/elasticmapreduce/details/spark/](http://aws.amazon.com/elasticmapreduce/details/spark/)
- en: Microsoft Virtual Academy website at [http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning](http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微软虚拟学院网站，请参阅[http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning](http://www.microsoftvirtualacademy.com/training-courses/getting-started-with-microsoft-azure-machine-learning)
- en: IBM Bluemix Tutorial at [http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html](http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IBM Bluemix教程，请参阅[http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html](http://www.ibm.com/developerworks/cloud/bluemix/quick-start-bluemix.html)
- en: CRAN Task View for contributed packages in R at [https://cran.r-project.org/web/views/HighPerformanceComputing.html](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CRAN Task View for contributed packages in R at [https://cran.r-project.org/web/views/HighPerformanceComputing.html](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this last chapter of the book, we covered various frameworks to implement
    large-scale machine learning. These are very useful for Bayesian learning too.
    For example, to simulate from a posterior distribution, one could run a Gibbs
    sampling over a cluster of machines. We learned how to connect to Hadoop from
    R using the RHadoop package and how to use R with Spark using SparkR. We also
    discussed how to set up clusters in cloud services such as AWS and how to run
    Spark on them. Some of the native parallelization frameworks such as parallel
    and foreach functions were also covered.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章中，我们介绍了各种实现大规模机器学习的框架。这些框架对贝叶斯学习也非常有用。例如，为了从后验分布中进行模拟，可以在机器集群上运行Gibbs抽样。我们学习了如何使用RHadoop包从R连接到Hadoop，以及如何使用SparkR与Spark一起使用R。我们还讨论了如何在AWS等云服务中设置集群，以及如何在它们上运行Spark。还涵盖了某些本地并行化框架，如parallel和foreach函数。
- en: The overall aim of this book was to introduce readers to the area of Bayesian
    modeling using R. Readers should have gained a good grasp of theory and concepts
    behind Bayesian machine learning models. Since the examples were mainly given
    for the purposes of illustration, I urge readers to apply these techniques to
    real-world problems to appreciate the subject of Bayesian inference more deeply.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要目的是向读者介绍使用R进行贝叶斯建模的领域。读者应该已经对贝叶斯机器学习模型背后的理论和概念有了很好的理解。由于示例主要是为了说明目的，我敦促读者将这些技术应用于实际问题，以更深入地理解贝叶斯推断的主题。
