- en: Chapter 4. Logistic Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章. 逻辑回归
- en: 'In this chapter, another supervised method is introduced: classification. We
    will introduce the simplest classifier, the Logistic Regressor, which shares the
    same foundations as the Linear Regressor, but it targets classification problems.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，介绍了另一种监督方法：分类。我们将介绍最简单的分类器，逻辑回归，它与线性回归有相同的基础，但针对分类问题。
- en: 'In the following chapter, you''ll find:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你会找到：
- en: A formal and mathematical definition of the classification problem, for both
    binary and multiclass problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分类问题（包括二分类和多分类问题）的正式和数学定义
- en: How to evaluate classifier performances—that is, their metrics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估分类器的性能——即它们的指标
- en: The math behind Logistic Regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归背后的数学
- en: A revisited formula for SGD, specifically built for Logistic Regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为逻辑回归专门构建的SGD重访公式
- en: The multiclass case, with Multiclass Logistic Regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多分类情况，使用多分类逻辑回归
- en: Defining a classification problem
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义分类问题
- en: 'Although the name Logistic Regression suggests a regression operation, the
    goal of Logistic Regression is classification. In a very rigorous world such as
    statistics, why is this technique ambiguously named? Simple, the name is not wrong
    at all, and it makes perfect sense: it just requires a bit of an introduction
    and investigation. After that you''ll fully understand why it''s named Logistic
    Regression, and you''ll no longer think that it''s a wrong name.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然逻辑回归这个名字暗示了回归操作，但逻辑回归的目标是分类。在一个非常严谨的世界，比如统计学，为什么这个技术会有歧义的名字？简单地说，这个名字根本没错，它完全合理：它只需要一点介绍和调查。之后，你就会完全理解为什么它被称为逻辑回归，你也不再认为这个名字是错误的。
- en: First, let's introduce what a classification problem is, what a classifier is,
    how it operates, and what its output is.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍什么是分类问题，什么是分类器，它是如何运作的，以及它的输出是什么。
- en: In the previous chapter, we presented regression as the operation of estimating
    a continuous value in a target variable; mathematically speaking, the predicted
    variable is a real number in the range (*−∞*, *+∞*). Classification, instead,
    predicts a class, that is, an index in a finite set of classes. The simplest case
    is named binary classification, and the output is typically a Boolean value (`true`/`false`).
    If the class is `true` the sample is typically called a *positive sample*; otherwise
    it's a *negative sample*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将回归描述为在目标变量中估计一个连续值的操作；从数学上讲，预测变量是在范围(*−∞*, *+∞*)内的一个实数。相反，分类预测一个类别，即一个有限类别集合中的索引。最简单的情况称为二分类，输出通常是布尔值（`true`/`false`）。如果类别是`true`，则该样本通常被称为*正样本*；否则，它是一个*负样本*。
- en: 'To state some examples, here are some questions that refer to a binary classification
    problem:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举一些例子，以下是一些涉及二分类问题的疑问：
- en: Is this email spam?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这封电子邮件是垃圾邮件吗？
- en: Is my house worth at least $200,000?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的房子价值至少20万美元吗？
- en: Is the banner/email clicked/opened by the user?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户是否点击/打开横幅/电子邮件？
- en: Is the current document about finance?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这份文档是关于金融的吗？
- en: Is there a person in the image? Is it a man or a woman?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中有人吗？是男性还是女性？
- en: Tip
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Putting a threshold on the output of a regression problem, to determine whether
    the value is greater or lower than a fixed threshold, is actually a binary classification
    problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题的输出上设置一个阈值，以确定该值是否大于或小于一个固定的阈值，实际上是一个二分类问题。
- en: When the output can have multiple values (that is, the predicted label is a
    categorical variable), the classification is named a multiclass one. Usually,
    the possible labels are named levels or classes, and the list of them should be
    finite and known in advance (or else it will be an unsupervised problem, not a
    supervised one).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出可以有多个值时（即预测标签是分类变量），这种分类被称为多分类。通常，可能的标签被称为级别或类别，它们的列表应该是有限的，并且事先已知（否则它将是一个无监督问题，而不是监督问题）。
- en: 'Examples of multiclass classification problems are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多分类分类问题的例子包括：
- en: Which kind of flower is this?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是什么花？
- en: What's the primary topic of this webpage?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个网页页面的主要主题是什么？
- en: Which kind of network attack am I experiencing?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我正在经历哪种类型的网络攻击？
- en: Which digit/letter is drawn in the image?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中画的是哪个数字/字母？
- en: 'Formalization of the problem: binary classification'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题的形式化：二分类
- en: 'Let''s start now with the simplest type of classification: the **binary classification**.
    Don''t worry; in a few pages things are going to be more complex when we focus
    on the multiclass classification.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从最简单的分类类型开始：**二元分类**。不要担心；在接下来的几页中，当我们专注于多类分类时，事情将会变得更加复杂。
- en: 'Formally, the generic observation is an *n*-dimensional feature vector (*x[i]*)
    paired with its label: the generic *i*-th can be written as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，一般观测是一个 *n*- 维特征向量 (*x[i]*) 与其标签配对：一般 *i*- 维可以写成：
- en: '![Formalization of the problem: binary classification](img/00056.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![问题的形式化：二元分类](img/00056.jpeg)'
- en: 'The model underneath the classifier is a function and is called a **classification
    function**, which can be either linear or non linear. The form of the function
    is the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器底下的模型是一个函数，被称为**分类函数**，它可以是线性的或非线性的。函数的形式如下：
- en: '![Formalization of the problem: binary classification](img/00057.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![问题的形式化：二元分类](img/00057.jpeg)'
- en: During the prediction task, the classification function is applied to a new
    feature vector, and the output of the classifier represents the class to which
    the input sample is classified, that is, the predicted label. A perfect classifier
    predicts, for every possible input, the correct class `y`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测任务期间，分类函数应用于新的特征向量，分类器的输出代表输入样本被分类到的类别，即预测标签。一个完美的分类器对每个可能的输入都预测正确的类别 `y`。
- en: The feature vector *x* should comprise numbers. If you're dealing with categorical
    features (such as gender, membership, and words), you should be able to take that
    variable to one or more numeric variables (usually binary). We'll see more about
    this point later on in the book, in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*, which is devoted to data preparation
    of variables into the most suitable form for regression.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量 *x* 应该包含数字。如果你处理的是分类特征（例如性别、成员资格和单词），你应该能够将那个变量转换为一个或多个数字变量（通常是二进制）。我们将在本书后面的[第5章](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "第5章。数据准备")中看到更多关于这一点，该章节专门讨论将变量准备成回归最合适形式的数据准备。
- en: 'To have a visual understanding of what''s going on, let''s consider now a binary
    classification problem, where every feature has two dimensions (a 2-D problem).
    Let''s first define the input dataset; here the `make_classifier` method of the
    Scikit-learn library comes in very handy. It creates a dummy dataset for classification,
    providing the number of classes, the dimensionality of the problem, and the number
    of observations as parameters. Additionally, you should specify that each feature
    is informative (and there are no redundancies) and each class is composed of a
    single cluster of points:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有一个直观的理解，让我们考虑一个二元分类问题，其中每个特征都有两个维度（一个二维问题）。让我们首先定义输入数据集；在这里，Scikit-learn库的`make_classifier`方法非常有用。它通过提供类别数量、问题维度和观测数量作为参数来创建一个用于分类的虚拟数据集。此外，您应指定每个特征都是信息性的（并且没有冗余），并且每个类别由一个点的单一簇组成：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Formalization of the problem: binary classification](img/00058.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![问题的形式化：二元分类](img/00058.jpeg)'
- en: Assessing the classifier's performance
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估分类器的性能
- en: 'To understand if a classifier is a good one, or equivalently, to identify the
    classifier with the best performance in the classification task, we need to define
    some metrics. There is no single metric since the classification goal can be different—for
    example, the correctness or completeness of a defined label, minimization of the
    number of misclassifications, correct ordering in respect of the likelihood of
    having a certain label, and quite a few others. All the measures can be derived
    from the classification matrix after having applied a cost matrix: the outcome
    highlights which errors are more expensive and which are not so expensive in terms
    of results.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解一个分类器是否是一个好的分类器，或者等价地，为了在分类任务中识别具有最佳性能的分类器，我们需要定义一些指标。由于分类目标可以不同——例如，定义标签的正确性或完整性、最小化错误分类的数量、根据拥有某个标签的可能性进行正确的排序，以及许多其他指标——因此没有单一的指标。所有这些指标都可以在应用成本矩阵后从分类矩阵中推导出来：结果突出了哪些错误更昂贵，哪些错误不那么昂贵。
- en: 'Metrics exposed here can be used for both binary and multiclass classification.
    Although it is not a measure of performance, let''s start from the confusion matrix,
    the simplest metric that gives us a visual impact of the correct classifications
    and the misclassification errors for each class. On the rows there are the true
    labels, on the column the predicted one. Let''s also create a dummy label set
    and a predicted set for the following experiments. In our example the original
    labels are six `0` and four `1`; the classifier misclassified entries are two
    `0` and one `1`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里公开的度量标准可以用于二进制和多类分类。尽管这不是性能的度量标准，但让我们从混淆矩阵开始，这是最简单的度量标准，它给我们提供了正确分类和每个类别的误分类错误的视觉影响。在行中有真实标签，在列中有预测标签。让我们也为以下实验创建一个虚拟标签集和预测集。在我们的例子中，原始标签是六个
    `0` 和四个 `1`；分类器误分类的条目是两个 `0` 和一个 `1`：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s now create the confusion matrix for this experiment:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在为这个实验创建混淆矩阵：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From this matrix we can extract some evidence:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个矩阵中我们可以提取一些证据：
- en: The number of samples is `10` (the sum of the whole matrix).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本数量为 `10`（整个矩阵的总和）。
- en: The number of samples labeled `0` in the original is `6`; `1`s are `4` (the
    sum for the lines). These numbers are named support.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始数据中标记为 `0` 的样本数量为 `6`；`1` 的数量为 `4`（线的总和）。这些数字被称为支持。
- en: The number of samples labeled `0` in the predicted dataset is `5`; `1`s are
    `5` (the sum as columns).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测数据集中标记为 `0` 的样本数量为 `5`；`1` 的数量为 `5`（列的总和）。
- en: Correct classifications are `7` (the sum of the diagonal).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确分类的数量为 `7`（对角线元素的总和）。
- en: Misclassifications are `3` (the sum of all numbers not on the diagonal).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误分类的数量为 `3`（所有不在对角线上的数字的总和）。
- en: A perfect classification example would have had all the numbers on the diagonal,
    and `0` elsewhere.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美的分类示例应该是在对角线上有所有数字，其他地方都是 `0`。
- en: 'This matrix can also be represented graphically, using a heatmap. This is a
    very impactful representation, especially when dealing with multiclass problems:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵也可以用热图来图形化表示。这是一种非常有影响力的表示方式，尤其是在处理多类问题时：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Assessing the classifier''s performance](img/00059.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![评估分类器的性能](img/00059.jpeg)'
- en: The first measure we're going to explore to evaluate the classifier's performance
    is accuracy. Accuracy is the percentage of correct classifications, over the total
    number of samples. You can derive this error measure directly from the confusion
    matrix by dividing the sum over the diagonal by the sum of the elements in the
    matrix. The best possible accuracy is `1.0` and the worst one is `0.0`. In the
    preceding example, accuracy amounts to *7/10 = 0.7*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的第一个度量标准来评估分类器的性能是准确度。准确度是正确分类的百分比，占总样本数的比例。你可以直接从混淆矩阵中推导出这个错误度量，通过将对角线上的总和除以矩阵中元素的总和。最佳准确度为
    `1.0`，最差为 `0.0`。在先前的例子中，准确度为 *7/10 = 0.7*。
- en: 'Using Python, this becomes:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python，这会变成：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Another very popular measure is precision. It considers only one label and counts
    the percentage of correct classifications on that label. While considering our
    label "1", the precision is the number in the bottom right of the confusion matrix,
    divided by the sum of the elements in the second column—that is, *3/5=0.6*. Values
    are bounded between 0 and 1, where 1 is the best possible result and 0 the worst.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行的度量标准是精确度。它只考虑一个标签，并计算在该标签上正确分类的百分比。当我们考虑标签“1”时，精确度就是混淆矩阵右下角的数字除以第二列元素的总和——即
    *3/5=0.6*。值介于0和1之间，其中1是最佳结果，0是最差结果。
- en: 'Note that this function in Scikit-learn expects a binary input, where only
    the class under examination is marked as `true` (this is sometime named a *class
    indicator*). To extract a precision score for each label, you should then make
    each class a binary vector:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Scikit-learn 中的这个函数期望二进制输入，其中只有被检查的类别被标记为 `true`（有时被称为 *类别指示器*）。为了提取每个标签的精确度分数，你应该将每个类别转换为二进制向量：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Paired with precision you''ll frequently find another error measure, recall.
    If precision is about the quality of what you got (that is, the quality of the
    results marked with the label `1`), recall is about the quality of what you could
    have gotten—that is, how many instances of `1` you''ve been able to extract properly.
    Also, here, this measure is class-based, and to compute the recall score for class
    `1` you should divide the bottom right number in the confusion matrix by the sum
    of the second line, that is, *3/4=0.75*. Recall is bounded `0` and `1`; the best
    score is `1` and means that all the instances of "1" in the original dataset have
    been correctly classified as "1"; a score equal to `0` means that no "1"s have
    been classified properly:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确度经常相伴的另一个错误度量是召回率。精确度关乎你得到的结果的质量（即，标记为`1`的结果的质量），而召回率关乎你能得到的结果的质量——即，你正确提取的`1`的实例数量。此外，这里的度量是基于类别的，要计算类别`1`的召回率分数，你应该将混淆矩阵右下角的数字除以第二行的总和，即*3/4=0.75*。召回率的范围是`0`到`1`；最佳分数是`1`，意味着原始数据集中所有`1`的实例都被正确分类为`1`；分数等于`0`意味着没有`1`被正确分类：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Precision and recall are two metrics that indicate how well the classifier performed
    on a class. Merge their score, using a harmonic average, and you'll get the comprehensive
    f1-score, helping you to figure out at a glance the performance on both error
    measures.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率是两个指标，表明分类器在某个类别上的表现如何。将它们的分数合并，使用调和平均数，你将得到全面的f1分数，帮助你一眼看出在两个错误度量上的表现。
- en: 'Mathematically:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上：
- en: '![Assessing the classifier''s performance](img/00060.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![评估分类器的性能](img/00060.jpeg)'
- en: 'In Python this is easier:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中这更容易：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In conclusion, if there are so many error scores, which is the best to use?
    The solution is not very easy, and often it is better to have and evaluate the
    classifier on all of them. How can we do that? Is it a long function to write?
    No, Scikit-learn here comes to help us here, providing a method to compute all
    these scores for each class (this is really handy). Here is how it works:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果有这么多错误分数，哪个是最好的？解决方案并不简单，通常最好是拥有并评估所有这些分类器。我们如何做到这一点？这是一个需要写很长的函数吗？不，Scikit-learn在这里帮助我们，提供了一个方法来计算每个类别的所有这些分数（这真的很有用）。以下是它是如何工作的：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Assessing the classifier''s performance](img/00061.jpeg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![评估分类器的性能](img/00061.jpeg)'
- en: Defining a probability-based approach
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义基于概率的方法
- en: Let's gradually introduce how logistic regression works. We said that it's a
    classifier, but its name recalls a regressor. The element we need to join the
    pieces is the probabilistic interpretation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步介绍逻辑回归是如何工作的。我们说它是一个分类器，但它的名字让人联想到回归器。我们需要将各个部分连接起来的元素是概率解释。
- en: 'In a binary classification problem, the output can be either "0" or "1". What
    if we check the probability of the label belonging to class "1"? More specifically,
    a classification problem can be seen as: given the feature vector, find the class
    (either 0 or 1) that maximizes the conditional probability:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类问题中，输出可以是`0`或`1`。如果我们检查标签属于类别`1`的概率呢？更具体地说，一个分类问题可以看作：给定特征向量，找到最大化条件概率的类别（要么是0，要么是1）：
- en: '![Defining a probability-based approach](img/00062.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00062.jpeg)'
- en: 'Here''s the connection: if we compute a probability, the classification problem
    *looks like* a regression problem. Moreover, in a binary classification problem,
    we just need to compute the probability of membership of class "1", and therefore
    it looks like a well-defined regression problem. In the regression problem, classes
    are no longer "1" or "0" (as strings), but 1.0 and 0.0 (as the probability of
    belonging to class "1").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是联系：如果我们计算一个概率，分类问题看起来就像回归问题。此外，在二元分类问题中，我们只需要计算属于类别`1`的成员概率，因此它看起来像是一个定义良好的回归问题。在回归问题中，类别不再是`1`或`0`（作为字符串），而是1.0和0.0（作为属于类别`1`的概率）。
- en: 'Let''s now try fitting a multiple linear regressor on a dummy classification
    problem, using a probabilistic interpretation. We reuse the same dataset we created
    earlier in this chapter, but first we split the dataset into train and test sets,
    and we convert the `y` vector to floating point values:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试使用概率解释来拟合一个多重线性回归器在虚拟分类问题上。我们重用本章早期创建的相同数据集，但首先我们将数据集分为训练集和测试集，并将`y`向量转换为浮点值：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Defining a probability-based approach](img/00063.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00063.jpeg)'
- en: Here, with these few methods, we split the datasets into two folds, (train and
    test) and we converted all the numbers in the *y* array to floating point. In
    the last cell, we effectively check the operation. Now, if *y = 1.0*, it means
    that the relative observation is 100% class "1"; *y = 0.0* implies that the observation
    is 0% class "1". Since it's a binary classification task, it implies that it's
    also 100% class "0" (note that the percentages here refer to probability).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用这些方法中的几种，将数据集分为两个部分（训练集和测试集），并将*y*数组中的所有数字转换为浮点数。在最后一个单元格中，我们有效地检查了操作。现在，如果*y
    = 1.0*，这意味着相对观察结果是100%属于类别“1”；*y = 0.0*表示观察结果是0%属于类别“1”。由于这是一个二元分类任务，这也意味着它是100%属于类别“0”（注意，这里的百分比指的是概率）。
- en: 'Let''s now proceed with the regression:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行回归：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Defining a probability-based approach](img/00064.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00064.jpeg)'
- en: 'The output—that is, the prediction of the regressor—should be the probability
    of belonging to class 1\. As you can see in the last cell output, that''s not
    a proper probability, since it contains values below 0 and greater than 1\. The
    simplest idea here is clipping results between 0 and 1, and putting a threshold
    at `0.5`: if the value is *>0.5*, then the predicted class is "1"; otherwise the
    predicted class is "0".'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输出——即回归器的预测——应该是属于类别1的概率。正如你在最后一个单元格的输出中看到的，这不是一个合适的概率，因为它包含小于0和大于1的值。这里最简单的方法是将结果裁剪到0和1之间，并将阈值设置为`0.5`：如果值大于*0.5*，则预测的类别是“1”；否则预测的类别是“0”。
- en: This procedure works, but we can do better. We've seen how easy it is to transit
    from a classification problem to a regression one, and then go back with predicted
    values to predicted classes. With this process in mind, let's again start the
    analysis, digging further in its core algorithm while introducing some changes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是有效的，但我们可以做得更好。我们已经看到从分类问题过渡到回归问题很容易，然后再用预测值回到预测类别。考虑到这个过程，我们再次开始分析，深入其核心算法的同时引入一些变化。
- en: 'In our dummy problem, we applied the linear regression model to estimate the
    probability of the observation belonging to class "1". The regression model was
    (as we''ve seen in the previous chapter):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的假设问题中，我们应用了线性回归模型来估计观察结果属于类别“1”的概率。回归模型如下（如我们在上一章中看到的）：
- en: '![Defining a probability-based approach](img/00065.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00065.jpeg)'
- en: 'Now, we''ve seen that the output is not a proper probability. To be a probability,
    we need to do the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看到输出不是一个合适的概率。要成为一个概率，我们需要做以下事情：
- en: Bound the output between 0.0 and 1.0 (clipping).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出限制在0.0和1.0之间（裁剪）。
- en: If the prediction is equal to the threshold (we chose 0.5 previously), the probability
    should be 0.5 (symmetry).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果预测等于阈值（我们之前选择了0.5），则概率应该是0.5（对称性）。
- en: To have both conditions `true`, the best we could do is to send the output of
    the regressor through a sigmoid curve, or an S-shaped curve. A sigmoid generically
    maps values in R (the field of real numbers) to values in the range `[0,1]`, and
    its value when mapping `0` is `0.5`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了同时满足这两个条件`true`，我们最好的办法是将回归器的输出通过一个sigmoid曲线，或者S形曲线。sigmoid函数通常将实数域R中的值映射到[0,1]范围内的值，并且当映射`0`时，其值为`0.5`。
- en: On the basis of such a hypothesis, we can now write (for the first time) the
    formula underneath the logistic regression algorithm.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的假设基础上，我们现在可以（第一次）写下逻辑回归算法下面的公式。
- en: '![Defining a probability-based approach](img/00066.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00066.jpeg)'
- en: Note also that the weight `W[0]` (the bias weight) will take care of the misalignment
    of the central point of the sigmoid (it's in 0, whereas the threshold is in 0.5).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，权重`W[0]`（偏差权重）将负责sigmoid的中心点与阈值的错位（它在0，而阈值在0.5）。
- en: 'That''s all. That''s the logistic regression algorithm. There is just one thing
    missing: why logistic? What''s the *σ* function?'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。这就是逻辑回归算法。只是还缺了一点：为什么是逻辑？*σ*函数是什么？
- en: 'Well, the answer to both questions is trivial: the standard choice of sigma
    is the logistic function, also named the inverse-logit function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这两个问题的答案都很简单：标准的选择是sigma函数，也称为逆对数函数：
- en: '![Defining a probability-based approach](img/00067.jpeg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00067.jpeg)'
- en: Although there are infinite functions that satisfy the sigmoid constraints,
    the logistic has been chosen because it's continuous, easily differentiable, and
    quick to compute. If the results are not satisfactory, always consider that, by
    introducing a couple of parameters, you can change the steepness and the center
    of the function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有无限多个函数满足sigmoid约束，但选择逻辑函数是因为它是连续的，易于微分，并且计算速度快。如果结果不满意，始终考虑通过引入几个参数，你可以改变函数的陡度和中心。
- en: 'The sigmoid function is quickly drawn:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在快速绘制sigmoid函数：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Defining a probability-based approach](img/00068.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![定义基于概率的方法](img/00068.jpeg)'
- en: You can immediately see that, for a very low **t**, the function tends to the
    value **0**; for a very high **t**, the function tends to be **1**, and, in the
    center, where **t** is **0**, the function is **0.5**. Exactly the sigmoid function
    we were looking for.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以立即看到，对于非常低的 **t**，函数趋向于 **0**；对于非常高的 **t**，函数趋向于 **1**，而在中心，**t** 为 **0**
    时，函数是 **0.5**。这正是我们寻找的sigmoid函数。
- en: More on the logistic and logit functions
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于逻辑和logit函数的信息
- en: 'Now, why did we use the inverse of the logit function? Isn''t there anything
    better than that? The answer to this question comes from statistics: we''re dealing
    with probabilities, and the logit function is a great fit. In statistics, the
    logit function applied to a probability, returns the log-odds:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为什么我们使用logit函数的逆函数？难道没有比这更好的方法吗？这个问题的答案来自统计学：我们处理的是概率，而logit函数是一个很好的匹配。在统计学中，logit函数应用于概率，返回对数几率：
- en: '![More on the logistic and logit functions](img/00069.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![更多关于逻辑和logit函数的信息](img/00069.jpeg)'
- en: This function transforms numbers from range `[0,1]` to numbers in (*−∞*, *+∞*).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将范围 `[0,1]` 内的数字转换为 (*−∞*, *+∞*) 范围内的数字。
- en: 'Now, let''s see if you can intuitively understand the logic behind the selection
    of the inverse-logit function as the sigmoid function for the logistic regression.
    Let''s first write down the probabilities for both classes, according to this
    logistic regression equation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看你是否可以直观地理解选择逆logit函数作为逻辑回归的sigmoid函数的逻辑。让我们首先写下两个类别的概率，根据这个逻辑回归方程：
- en: '![More on the logistic and logit functions](img/00070.jpeg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![更多关于逻辑和logit函数的信息](img/00070.jpeg)'
- en: 'Let''s now compute the log-odds:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在计算对数几率：
- en: '![More on the logistic and logit functions](img/00071.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![更多关于逻辑和logit函数的信息](img/00071.jpeg)'
- en: 'However, not surprisingly, that''s also the **logit** function, applied to
    the probability of getting a "1":'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不出所料，这也是 **logit** 函数，应用于得到“1”的概率：
- en: '![More on the logistic and logit functions](img/00072.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![更多关于逻辑和logit函数的信息](img/00072.jpeg)'
- en: 'The chain of our reasoning is finally closed, and here''s why logistic regression
    is based on, as the definition implies, the logistic function. Actually, logistic
    regression is a model of the big category of the GLM: the generalized linear model.
    Each model has a different function, a different formulation, a different operative
    hypothesis, and not, surprisingly, a different goal.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推理的链条最终闭合了，这就是为什么逻辑回归基于，正如定义所暗示的，逻辑函数。实际上，逻辑回归是广义线性模型（GLM）的大类别中的一个模型：广义线性模型。每个模型都有不同的函数，不同的公式，不同的操作假设，并且不出所料，不同的目标。
- en: Let's see some code
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们看看一些代码
- en: 'First, we start with the dummy dataset we created at the beginning of the chapter.
    Creating and fitting a logistic regressor classifier is really easy: thanks to
    Scikit-learn, it just requires a couple of lines of Python code. As for regressors,
    to train the model you need to call the `fit` method, whereas for predicting the
    class you just need to call the `predict` method:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从本章开头创建的虚拟数据集开始。创建和拟合逻辑回归分类器非常简单：多亏了Scikit-learn，这只需要几行Python代码。对于回归器，要训练模型，你需要调用
    `fit` 方法，而对于预测类别，你只需要调用 `predict` 方法：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Let''s see some code](img/00073.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![让我们看看一些代码](img/00073.jpeg)'
- en: 'Note that here we''re not making a regression operation; that''s why the label
    vector must comprise integers (or class indexes). The report shown at the bottom
    shows a very accurate prediction: all the scores are close to 1 for all classes.
    Since we have `33` samples in the test set, `0.97` means just one case misclassified.
    That''s almost perfect in this dummy example!'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里我们并没有进行回归操作；这就是为什么标签向量必须包含整数（或类别索引）。底部显示的报告显示了一个非常准确的预测：所有类别的分数都接近 1。由于测试集中有
    `33` 个样本，`0.97` 意味着只有一个案例被错误分类。在这个示例中这几乎是完美的！
- en: 'Now, let''s try to dig under the hood even more. First, we would like to check
    the decision boundary of the classifier: which part of the bidimensional space
    has points being classified as "1"; and where are the "0"s? Let''s see how you
    can visually see the decision boundary here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地探索一下。首先，我们想检查分类器的决策边界：二维空间中哪些部分被分类为“1”；“0”在哪里？让我们看看你如何在这里直观地看到决策边界：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Let''s see some code](img/00074.jpeg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![让我们看看一些代码](img/00074.jpeg)'
- en: 'The separation is almost vertical. "1"s are on the left (yellow) side; "0"s
    on the right (red). From the earlier screenshot, you can immediately perceive
    the misclassification: it''s pretty close to the boundary. Therefore, its probability
    of belonging to class "1" will be very close to 0.5.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔几乎垂直。数字“1”在左侧（黄色）一侧；“0”在右侧（红色）一侧。从早期的截图，你可以立即感知到误分类：它非常接近边界。因此，它属于类别“1”的概率将非常接近0.5。
- en: 'Let''s now see the bare probabilities and the weight vector. To compute the
    probability, you need to use the `predict_proba` method of the classifier. It
    returns two values for each observation: the first is the probability of being
    of class "0"; the second the probability for class "1". Since we''re interested
    in class "1", here we just select the second value for all the observations:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看裸概率和权重向量。要计算概率，你需要使用分类器的`predict_proba`方法。它为每个观测值返回两个值：第一个是该观测值属于类别“0”的概率；第二个是类别“1”的概率。由于我们感兴趣的是类别“1”，因此我们只选择所有观测值的第二个值：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Let''s see some code](img/00075.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![让我们看看一些代码](img/00075.jpeg)'
- en: 'In the screenshot, pure yellow and pure red are where the predicted probability
    is very close to 1 and 0 respectively. The black dot is the origin *(0,0)* of
    the Cartesian bidimensional space, and the arrow is the representation of the
    weight vector of the classifier. As you can see, it''s orthogonal to the decision
    boundary, and it''s *pointing* toward the "1" class. The weight vector is actually
    the model itself: if you need to store it in a file, consider that it''s just
    a couple of floating point numbers and nothing more.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在截图上，纯黄色和纯红色是预测概率非常接近1和0的地方。黑色点是笛卡尔二维空间的起点 *(0,0)*，箭头是分类器权重向量的表示。正如你所见，它与决策边界正交，并且它是指向“1”类别的。权重向量实际上是模型本身：如果你需要将其存储在文件中，请考虑它只是几个浮点数，没有其他东西。
- en: 'Lastly, I''d want to focus on speed. Let''s now see how much time the classifier
    takes to train and to predict the labels:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想关注一下速度。现在让我们看看分类器训练和预测标签需要多少时间：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Although timings are computer-specific (here we're training it and predicting
    using the full 100-point dataset), you can see that Logistic Regression is a very
    fast technique both during training and when predicting the class and the probability
    for all classes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然时间取决于计算机（在这里我们使用完整的100点数据集进行训练和预测），但你可以看到逻辑回归在训练和预测类别以及所有类别的概率时都非常快。
- en: Pros and cons of logistic regression
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归的优缺点
- en: 'Logistic regression is a very popular algorithm because of the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归之所以非常流行，是因为以下原因：
- en: 'It''s linear: it''s the equivalent of the linear regression for classification.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是线性的：它是分类中线性回归的等价物。
- en: It's very simple to understand, and the output can be the most likely class,
    or the probability of membership.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它非常容易理解，输出可以是最可能的类别，或者成员的概率。
- en: 'It''s simple to train: it has very few coefficients (one coefficient for each
    feature, plus one bias). This makes the model very small to store (you just need
    to store a vector of weights).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练很简单：它具有非常少的系数（每个特征一个系数，加上一个偏置）。这使得模型非常小，便于存储（你只需要存储一个权重向量）。
- en: 'It''s computationally efficient: using some special tricks (see later in the
    chapter), it can be trained very quickly.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在计算上效率很高：使用一些特殊的技巧（本章后面将介绍），它可以非常快地训练。
- en: It has an extension for multiclass classification.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有用于多类别分类的扩展。
- en: 'Unfortunately, it''s not a perfect classifier and has some drawbacks:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，它不是一个完美的分类器，存在一些缺点：
- en: 'It''s often not very performant, compared to most advanced algorithms, because
    it tends to underfit (no flexibility: the boundary has to be a line or a hyperplane)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与大多数高级算法相比，它通常表现不佳，因为它往往欠拟合（没有灵活性：边界必须是一条线或超平面）
- en: 'It''s linear: if the problem is non-linear, there is no way to properly fit
    this classifier onto the dataset'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是线性的：如果问题是非线性的，就没有办法将这个分类器适当地拟合到数据集上
- en: Revisiting gradient descent
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾梯度下降
- en: 'In the previous chapter, we introduced the gradient descent technique to speed
    up processing. As we''ve seen with Linear Regression, the fitting of the model
    can be made in two ways: closed form or iterative form. Closed form gives the
    best possible solution in one step (but it''s a very complex and time-demanding
    step); iterative algorithms, instead, reach the minima step by step with few calculations
    for each update and can be stopped at any time.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了梯度下降技术来加速处理。正如我们在线性回归中看到的，模型的拟合可以通过两种方式完成：闭式或迭代式。闭式在一步内给出最佳可能解（但这是一个非常复杂且耗时的一步）；相反，迭代算法通过逐步计算每个更新的少量计算来达到最小值，并且可以在任何时候停止。
- en: 'Gradient descent is a very popular choice for fitting the Logistic Regression
    model; however, it shares its popularity with Newton''s methods. Since Logistic
    Regression is the base of the iterative optimization, and we''ve already introduced
    it, we will focus on it in this section. Don''t worry, there is no winner or any
    best algorithm: all of them can reach the very same model eventually, following
    different paths in the coefficients'' space.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是拟合逻辑回归模型的一个非常流行的选择；然而，它与牛顿方法共享其流行度。由于逻辑回归是迭代优化的基础，并且我们已经介绍了它，因此我们将重点放在这一节上。不用担心，没有赢家或任何最佳算法：它们最终都可以达到完全相同的模型，只是在系数空间中遵循不同的路径。
- en: 'First, we should compute the derivate of the loss function. Let''s make it
    a bit longer, and let''s start deriving the logistic function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该计算损失函数的导数。让我们让它更长一些，并开始推导逻辑函数：
- en: '![Revisiting gradient descent](img/00076.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00076.jpeg)'
- en: 'Its first-order derivative is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它的一阶导数如下：
- en: '![Revisiting gradient descent](img/00077.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00077.jpeg)'
- en: 'This is another reason why logistic regression used the logistic function:
    its derivate is computationally light. Now, let''s assume that the training observations
    are independent. Computing the likelihood, with respect to the set of weights,
    is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是逻辑回归使用逻辑函数的另一个原因：它的导数计算量小。现在，假设训练观察值是独立的。关于权重的似然计算如下：
- en: '![Revisiting gradient descent](img/00078.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00078.jpeg)'
- en: Note that, in the last row, we used a trick based on the fact that *y[i]* can
    be either 0 or 1\. If *y[i]=1*, only the first factor of the multiplication is
    computed; otherwise it's the second factor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在最后一行，我们使用了一个基于事实的技巧，即 *y[i]* 可以是 0 或 1。如果 *y[i]=1*，则只计算乘法的第一因子；否则是第二因子。
- en: 'Let''s now compute the log-likelihood: it will make things easier:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算对数似然：这将使事情变得更容易：
- en: '![Revisiting gradient descent](img/00079.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00079.jpeg)'
- en: 'Now, we have two considerations to make. First: the SGD works with one point
    at a time; therefore, the log-likelihood, step-by-step, is just a function of
    one point. Hence, we can remove the sum over all the points, and name *(x,y)*
    the point under observation. Second, we need to maximize the likelihood: to do
    so, we need to extract its partial derivative with respect to the generic *k*-th
    coefficient of *W*.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两点需要考虑。首先：SGD一次只处理一个点；因此，对数似然，逐步，只是单个点的函数。因此，我们可以移除对所有点的求和，并将 *(x,y)*
    命名为观察点。其次，我们需要最大化似然：为了做到这一点，我们需要提取其对 *W* 的通用 *k*-th 系数的偏导数。
- en: 'The math here becomes a bit complex; therefore we will just write the last
    result (this is the thinking we will use in our model). Deriving and understanding
    the equations in the middle is given to the reader as homework:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的数学变得有些复杂；因此我们只写最后的结果（这是我们将在模型中使用的思考）。推导和理解中间的方程留给读者作为作业：
- en: '![Revisiting gradient descent](img/00080.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00080.jpeg)'
- en: 'Since we''re trying to maximize the likelihood (and its log version), the right
    formula for updating the weights is the Stochastic Gradient Ascent:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们试图最大化似然（及其对数版本），更新权重的正确公式是随机梯度上升：
- en: '![Revisiting gradient descent](img/00081.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00081.jpeg)'
- en: 'That''s the generic formula. In our case, the update step for each coefficient
    composing *W* is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通用的公式。在我们的情况下，组成 *W* 的每个系数的更新步骤如下：
- en: '![Revisiting gradient descent](img/00082.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![重新审视梯度下降](img/00082.jpeg)'
- en: Here, *(x,y)* is the (stochastic) random observation chosen for the update step,
    and the learning step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*(x,y)* 是为更新步骤和学习步骤选择的（随机）随机观察值。
- en: To see a real example of what the SGD produces, check the last section of this
    chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看SGD产生的真实示例，请查看本章的最后部分。
- en: Multiclass Logistic Regression
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类逻辑回归
- en: 'The extension to Logistic Regression, for classifying more than two classes,
    is Multiclass Logistic Regression. Its foundation is actually a generic approach:
    it doesn''t just work for Logistic Regressors, it also works with other binary
    classifiers. The base algorithm is named **One-vs-rest**, or **One-vs-all**, and
    it''s simple to grasp and apply.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 将逻辑回归扩展到分类超过两个类别的多类逻辑回归。其基础实际上是一种通用方法：它不仅适用于逻辑回归器，还适用于其他二分类器。基本算法命名为**One-vs-rest**，或**One-vs-all**，它简单易懂且易于应用。
- en: 'Let''s describe it with an example: we have to classify three kinds of flowers
    and, given some features, the possible outputs are three classes: `f1`, `f2`,
    and `f3`. That''s not what we''ve seen so far; in fact, this is not a binary classification
    problem. Instead, it seems very easy to break down this problem into three simpler
    problems:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来描述它：我们必须对三种花卉进行分类，并且给定一些特征，可能的输出是三个类别：`f1`、`f2`和`f3`。这并不是我们之前看到的；事实上，这并不是一个二元分类问题。相反，这个问题似乎很容易分解成三个更简单的问题：
- en: '**Problem #1**: Positive examples (that is, the ones that get the label "1")
    are `f1`; negative examples are all the others'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题 #1**：正例（即被标记为“1”的例子）是`f1`；负例是所有其他例子'
- en: '**Problem #2**: Positive examples are `f2`; negative examples are `f1` and
    `f3`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题 #2**：正例是`f2`；负例是`f1`和`f3`'
- en: '**Problem #3**: Positive examples are `f3`; negative examples are `f1` and
    `f2`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题 #3**：正例是`f3`；负例是`f1`和`f2`'
- en: For all three problems, we can use a binary classifier, as Logistic Regressor,
    and, unsurprisingly, the first classifier will output *P(y = f1|x)*; the second
    and the third will output respectively *P(y = f2|x) and P(y = f3|x)*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有三个问题，我们可以使用二分类器，如逻辑回归器，并且不出所料，第一个分类器将输出*P(y = f1|x)*；第二个和第三个将分别输出*P(y =
    f2|x)和P(y = f3|x)*。
- en: To make the final prediction, we just need to select the classifier that emitted
    the highest probability. Having trained three classifiers, the feature space is
    not divided in two subplanes, but according to the decision boundary of the three
    classifiers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出最终预测，我们只需要选择发出最高概率的分类器。训练了三个分类器后，特征空间不是分为两个子平面，而是根据三个分类器的决策边界来划分。
- en: 'The approach of One-vs-all is very convenient, in fact:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: One-vs-all方法非常方便，事实上：
- en: The number of classifiers to fit is exactly the same as the number of classes.
    Therefore, the model will be composed by *N* (where *N* is the number of classes)
    weight vectors.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要拟合的分类器数量正好等于类别的数量。因此，模型将由*N*（其中*N*是类别的数量）权重向量组成。
- en: Moreover, this operation is embarrassingly parallel and the training of the
    *N* classifiers can be made simultaneously, using multiple threads (up to *N*
    threads).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，这个操作是令人尴尬的并行，*N*个分类器的训练可以同时进行，使用多个线程（最多*N*个线程）。
- en: If the classes are balanced, the training time for each classifier is similar,
    and the predicting time is the same (even for unbalanced classes).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果类别平衡，每个分类器的训练时间相似，预测时间也相同（即使对于不平衡的类别）。
- en: 'For a better understanding, let''s make a multiclass classification example,
    creating a dummy three-class dataset, splitting it as training and test sets,
    training a Multiclass Logistic Regressor, applying it on the training set, and
    finally visualizing the boundaries:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们通过一个多类分类的例子来说明，创建一个虚拟的三类数据集，将其分为训练集和测试集，训练一个多类逻辑回归器，将其应用于训练集，并最终可视化边界：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Multiclass Logistic Regression](img/00083.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑回归](img/00083.jpeg)'
- en: '[PRE17]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Multiclass Logistic Regression](img/00084.jpeg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑回归](img/00084.jpeg)'
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Multiclass Logistic Regression](img/00085.jpeg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑回归](img/00085.jpeg)'
- en: On this dummy dataset, the classifier has achieved a perfect classification
    (precision, recall, and f1-score are all 1.0). In the last picture, you can see
    that the decision boundaries define three areas, and create a non-linear division.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个虚拟数据集上，分类器实现了完美的分类（精确度、召回率和f1分数都是1.0）。在最后一张图片中，你可以看到决策边界定义了三个区域，并创建了一个非线性划分。
- en: 'Finally, let''s observe the first feature vector, its original label, and its
    predicted label (both reporting class "0"):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们观察第一个特征向量、其原始标签和其预测标签（都报告为类别“0”）：
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To get its probabilities to belong to each of the three classes, you can simply
    apply the `predict_proba` method (exactly as in the binary case), and the classifier
    will output the three probabilities. Of course, their sum is 1.0, and the highest
    value is, naturally, one for class "0".
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到属于三个类别中每一个的概率，你可以简单地应用`predict_proba`方法（与二分类情况完全相同），分类器将输出三个概率。当然，它们的总和是1.0，自然，对于类别“0”的最高值是1。
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: An example
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个例子
- en: We now look at a practical example, containing what we've seen so far in this
    chapter.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来看一个实际例子，其中包含了本章到目前为止所看到的内容。
- en: Our dataset is an artificially created one, composed of 10,000 observations
    and 10 features, all of them informative (that is, no redundant ones) and labels
    "0" and "1" (binary classification). Having all the informative features is not
    an unrealistic hypothesis in machine learning, since usually the feature selection
    or feature reduction operation selects non-related features.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集是一个人工创建的，由10,000个观测值和10个特征组成，所有这些特征都是信息性的（也就是说，没有冗余的），标签为“0”和“1”（二元分类）。在机器学习中拥有所有信息性特征不是一个不切实际的假设，因为通常特征选择或特征减少操作会选择非相关特征。
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now, we'll show you how to use different libraries, and different modules, to
    perform the classification task, using logistic regression. We won't focus here
    on how to measure the performance, but on how the coefficients can compose the
    model (what we've named in the previous chapters).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将向您展示如何使用不同的库和不同的模块来执行分类任务，使用逻辑回归。我们不会关注如何衡量性能，而是关注系数如何构成模型（我们在前几章中命名过）。
- en: 'As a first step, we will use Statsmodel. After having loaded the right modules,
    we need to add an additional feature to the input set in order to have the bias
    weight `W[0]`. After that, training the model is really simple: we just need to
    instantiate a `logit` object and use its `fit` method. Statsmodel will train the
    model and will show whether it was able to train a model (*Optimization terminated
    successfully*) or it failed:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将使用Statsmodel。在加载正确的模块后，我们需要向输入集添加一个额外的特征，以便有偏置权重`W[0]`。之后，训练模型就非常简单了：我们只需要实例化一个`logit`对象并使用其`fit`方法。Statsmodel将训练模型，并显示它是否能够训练一个模型（*优化成功终止*）或失败：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To get a detailed insight into the model, use the method summary:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解模型，请使用summary方法：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![An example](img/00086.jpeg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![一个例子](img/00086.jpeg)'
- en: 'Two tables are returned: the first one is about the dataset and model performances;
    the second is about the weights of the model. Statsmodel provides a lot of information
    on the model; some of it has been shown in [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression*, about a trained regressor. Here, instead, we have a brief description
    of the information shown for the classifier:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 返回了两个表格：第一个关于数据集和模型性能；第二个关于模型的权重。Statsmodel提供了关于模型的大量信息；其中一些已经在[第二章](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "第二章. 接近简单线性回归")中展示过，*接近简单线性回归*，关于一个训练好的回归器。在这里，我们简要描述了对于分类器显示的信息：
- en: '**Converged**: This tells whether the classification model has reached convergence
    while being trained. Use the model only if this parameter is `true`.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收敛**：这表明分类模型在训练过程中是否已达到收敛。只有当此参数为`true`时，才使用该模型。'
- en: '**Log-Likelihood**: This is the logarithm of the likelihood. It''s what we
    previously named.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数似然**：这是似然的对数。这是我们之前命名的。'
- en: '**LL-Null**: This is the Log-Likelihood when only the intercept is used as
    a predictor.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LL-Null**：这是仅使用截距作为预测因子时的对数似然。'
- en: '**LLR p-value**: This is the chi-squared probability of getting a log-likelihood
    ratio statistically greater than LLR. Basically, it shows how the model is better
    than guessing with a constant value. LLR is the log-likelihood ratio, that is,
    the logarithm of the likelihood of the null model (intercept only), divided by
    the likelihood of the alternate model (full model).'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLR p值**：这是得到一个统计上大于LLR的对数似然比的概率。基本上，它显示了模型相对于使用常数猜测的优越性。LLR是对数似然比，即零模型（仅截距）的似然的对数除以备择模型（完整模型）的似然。'
- en: '**Pseudo R-squared**: This can be seen as the proportion of the total variability
    unexplained by the model. It''s computed as *1-Log-likelihood/LL-Null*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伪R平方**：这可以看作是模型未解释的总变异的比例。它计算为*1-Log-似然/LL-Null*。'
- en: 'As for the coefficient table, there is one line for each coefficient: `const`
    is the weight associated to the intercept term (that is, the bias weight); `x1`,
    `x2`, … `x10` are the weights associated to the 10 features composing the model.
    For each of them there are a few values:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 至于系数表，每行对应一个系数：`const`是与截距项（即偏差权重）关联的权重；`x1`、`x2`、… `x10`是与模型中组成的10个特征关联的权重。对于每个权重，都有一些值：
- en: '**Coef**: This is the weight in the model associated to that feature.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系数**: 这是与该特征关联的模型中的权重。'
- en: '**Std err**: This is the standard error of the coefficient, that is its (predicted)
    standard deviation (across all observations) divided by the square root of the
    sample size.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准误差**: 这是系数的标准误差，即其（预测的）标准差（在所有观测值中）除以样本大小的平方根。'
- en: '**Z**: This is the ratio between the standard error and the coefficient (it''s
    the stat t-value).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z**: 这是标准误差与系数之间的比率（即统计t值）。'
- en: '**P>|z|**: This is the probability of obtaining a t-value greater than z, while
    sampling from the same population.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P>|z|**: 这是从同一总体中采样时获得大于z的t值的概率。'
- en: '**[95.0% Conf. Int.]**: This is the interval where, with 95% confidence, the
    real value of the coefficient is. It is computed as *coefficient +/- 1.96 * std
    err*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[95.0% 置信区间]**: 这是一个区间，在这个区间内，我们有95%的信心认为系数的真实值就在这里。它是通过计算 *系数 +/- 1.96 *
    标准误差* 得出的。'
- en: 'An alternate method to obtain the same result (often used when the model contains
    a small number of features) is to write down the formula involved in the regression.
    This is possible thanks to the Statsmodel formula API, which makes the fitting
    operation similar to what you would use in R. We first need to name the features,
    then we write down the formula (using the names we set), and lastly we fit the
    model. With this method, the intercept term is automatically added to the model.
    Its output, then, is the same as the preceding output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 获取相同结果的另一种方法（通常在模型包含少量特征时使用）是写下回归中涉及的公式。这得益于Statsmodel公式API，它使得拟合操作类似于你在R中使用的操作。我们首先需要命名特征，然后写下公式（使用我们设定的名称），最后拟合模型。使用这种方法，截距项会自动添加到模型中。因此，其输出与前面的输出相同：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s change our approach, and let''s now fully implement the stochastic gradient
    descent formula. Each piece of the formula has a function, and the `main` function
    is optimization. With respect to the linear regression, here the big difference
    is the `loss` function, which is the logistic (that is, the sigmoid):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变我们的方法，现在完全实现随机梯度下降公式。公式中的每一部分都有一个函数，而`main`函数是优化。与线性回归相比，这里的主要区别是`loss`函数，即逻辑函数（即Sigmoid）：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The coefficients produced with the stochastic gradient descent approach are
    the same as the ones Statsmodels derived previously. The code implementation,
    as seen before, is not best optimized; though reasonably efficient at working
    out the solution, it''s just an instructive way to understand how SGD works under
    the hood in the logistic regression task. Try to play around, checking the relation
    between the number of iterations, alpha, eta, and the final outcome: you''ll understand
    how these parameters are connected, as well as how to select the best settings.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机梯度下降法产生的系数与Statsmodels之前推导出的系数相同。如之前所见，代码实现并未进行最佳优化；尽管在求解解决方案方面相当高效，但它仅仅是一种了解随机梯度下降在逻辑回归任务中底层工作原理的指导性方法。尝试调整迭代次数、alpha、eta和最终结果之间的关系：你会理解这些参数是如何相互关联的，以及如何选择最佳设置。
- en: 'Finally, we switch to the Scikit-learn library, and its implementation of Logistic
    Regression. Scikit-learn has two implementations: one based on the *classic* solution
    of the logistic regression optimization, and the other one based on a quick SGD
    implementation. We''ll explore them both.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们转向Scikit-learn库及其逻辑回归的实现。Scikit-learn有两种实现：一种基于逻辑回归优化的*经典*解决方案，另一种基于快速随机梯度下降的实现。我们将探索这两种方法。
- en: 'First, we start with the classic Logistic Regression implementation. The training
    is really simple, and just requires a couple of parameters. We will set its parameters
    to the extreme, so the solution is not regularized (C is very high) and the stopping
    criterion on tolerance is very low. We do that in this example to get the same
    weights in the model; in a real experiment, these parameters will guide hyperparameter
    optimization. For more information about regularization, please refer to [Chapter
    6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6 "Chapter 6. Achieving
    Generalization"), *Achieving Generalization*:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从经典的逻辑回归实现开始。训练过程非常简单，只需要几个参数。我们将将其参数设置到极端，以便解决方案不受正则化（C值非常高）和容忍度停止标准的限制。我们在本例中这样做是为了获得模型中的相同权重；在真实实验中，这些参数将指导超参数优化。有关正则化的更多信息，请参阅[第6章](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "第6章。实现泛化")，*实现泛化*：
- en: '[PRE26]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![An example](img/00087.jpeg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/00087.jpeg)'
- en: 'As the last model, we try the Scikit-learn implementation of the SGD. Getting
    the same weights is really tricky, since the model is really complex, and the
    parameters should be optimized for performance, not for obtaining the same result
    as for the closed form approach. So, use this example to understand the coefficients
    in the model, but not for training a real-world model:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一个模型，我们尝试了Scikit-learn的SGD实现。由于模型非常复杂，获得相同的权重非常困难，因为参数应该针对性能进行优化，而不是为了获得与闭式方法相同的结果。因此，使用本例来理解模型中的系数，而不是用于训练真实世界的模型：
- en: '[PRE27]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![An example](img/00088.jpeg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/00088.jpeg)'
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We've seen in this chapter how to build a binary classifier based on Linear
    Regression and the logistic function. It's fast, small, and very effective, and
    can be trained using an incremental technique based on SGD. Moreover, with very
    little effort (the One-vs-Rest approach), the Binary Logistic Regressor can become
    multiclass.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中看到了如何基于线性回归和对数函数构建二元分类器。它速度快，体积小，非常有效，可以使用基于SGD的增量技术进行训练。此外，只需付出很少的努力（使用一对一余方法），二元逻辑回归器就可以变成多类。
- en: 'In the next chapter, we will focus on how to prepare data: to obtain the maximum
    from the supervised algorithm, the input dataset must be carefully cleaned and
    normalized. In fact, real world datasets can have missing data, errors, and outliers,
    and variables can be categorical and with different ranges of values. Fortunately,
    some popular algorithms deal with these problems, transforming the dataset in
    the best way possible for the machine learning algorithm.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点关注如何准备数据：为了从监督算法中获得最大效果，输入数据集必须经过仔细的清洗和归一化。实际上，现实世界的数据集可能存在缺失数据、错误和异常值，变量可以是分类的，并且具有不同的值范围。幸运的是，一些流行的算法可以处理这些问题，以最佳方式转换数据集，使其适合机器学习算法。
