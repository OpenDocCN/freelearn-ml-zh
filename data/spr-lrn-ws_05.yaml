- en: 5\. Classification Techniques
  id: totrans-0
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5\. 分类技术
- en: Overview
  id: totrans-1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces classification problems, classification using linear
    and logistic regression, K-nearest neighbors, and decision trees. You will also
    be briefly introduced to artificial neural networks as a type of classification
    technique.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了分类问题，使用线性回归和逻辑回归、K最近邻和决策树进行分类。你还将简要了解人工神经网络作为一种分类技术。
- en: By the end of this chapter, you will be able to implement logistic regression
    and explain how it can be used to classify data into specific groups or classes.
    You will also be able to use the k-nearest neighbors algorithm for classification
    and decision trees for data classification, including the ID3 algorithm. Additionally,
    you will be able to identify the entropy within data and explain how decision
    trees such as ID3 aim to reduce entropy.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够实现逻辑回归并解释如何使用它将数据分类到特定的组或类别中。你还将能够使用K最近邻算法进行分类，使用决策树进行数据分类，包括ID3算法。此外，你将能够识别数据中的熵，并解释决策树（如ID3）如何通过减少熵来进行分类。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapters, we began our supervised machine learning journey
    using regression techniques, predicting the continuous variable output on a given
    set of input data. We will now turn to the other type of machine learning problem:
    classification. Recall that classification tasks aim to classify given input data
    into two or more specified number of classes.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们通过回归技术开始了有监督机器学习之旅，预测给定输入数据集上的连续变量输出。现在我们将转向另一类机器学习问题：分类。回想一下，分类任务的目的是将给定的输入数据分类到两个或更多指定的类别中。
- en: So, while regression is a task of estimating a continuous value for given input
    data (for example, estimating the price of a house given its location and dimensions
    as input data), classification is about predicting a (discrete) label for given
    input data. For example, a well-known machine learning classification task is
    the spam detection of emails, where the task is to predict whether a given email
    is spam or not_spam. Here, spam and not_spam are the labels for this task and
    the input data is the email, or rather the textual data contained in the different
    fields of the email, such as subject, body, and receiver. The textual data would
    be preprocessed into numerical features in order to be usable for a classification
    model. Because there are only two labels in this task, it is known as a binary
    classification task. And if there are more than two labels in a classification
    task, it is called a multiclass classification task.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，回归是估计给定输入数据的连续值的任务（例如，根据房子的地点和尺寸估计房价），而分类则是预测给定输入数据的（离散）标签。例如，一个著名的机器学习分类任务是电子邮件的垃圾邮件检测，任务是预测给定的电子邮件是否为垃圾邮件或非垃圾邮件。在这里，垃圾邮件和非垃圾邮件是这个任务的标签，输入数据是电子邮件，或者更准确地说，是电子邮件中不同字段的文本数据，如主题、正文和收件人。文本数据将经过预处理，转化为数值特征，以便用于分类模型。因为这个任务只有两个标签，所以它被称为二分类任务。如果分类任务中有两个以上的标签，则称为多类分类任务。
- en: There are various kinds of classification models with different learning algorithms,
    each having their pros and cons. But essentially, all models are trained using
    a labeled dataset and, once trained, can predict labels for unlabeled data samples.
    In this chapter, we will extend the concepts learned in Chapter 3, Linear Regression,
    and Chapter 4, Autoregression, and will apply them to a dataset labeled with classes,
    rather than continuous values, as output. We will discuss some of the well-known
    classification models and apply them to some example labeled datasets.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种类型的分类模型，采用不同的学习算法，每种方法都有其优缺点。但本质上，所有模型都使用带标签的数据集进行训练，训练完成后，可以对未标记的数据样本进行标签预测。在本章中，我们将扩展第三章《线性回归》和第四章《自回归》中学到的概念，并将其应用于带有类别标签的数据集，而不是连续值作为输出。我们将讨论一些著名的分类模型，并将其应用于一些示例标签数据集。
- en: Ordinary Least Squares as a Classifier
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 普通最小二乘法作为分类器
- en: We covered ordinary least squares (OLS) as linear regression in the context
    of predicting continuous variable output in the previous chapter, but it can also
    be used to predict the class that a set of data is a member of. OLS-based classifiers
    are not as powerful as other types of classifiers that we will cover in this chapter,
    but they are particularly useful in understanding the process of classification.
    To recap, an OLS-based classifier is a non-probabilistic, linear binary classifier.
    It is non-probabilistic because it does not generate any confidence over the prediction
    such as, for example, logistic regression. It is a linear classifier as it has
    a linear relationship with respect to its parameters/coefficient.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将最小二乘法（OLS）作为线性回归应用于预测连续变量输出，但它也可以用于预测一组数据属于哪个类别。基于最小二乘法的分类器虽然不如本章后续介绍的其他分类器强大，但它们在理解分类过程时尤其有用。回顾一下，基于最小二乘法的分类器是一个非概率性的线性二分类器。它之所以是非概率性的，是因为它不会像逻辑回归那样对预测生成置信度。它是一个线性分类器，因为它与其参数/系数之间具有线性关系。
- en: Now, let's say we had a fictional dataset containing two separate groups, Xs
    and Os, as shown in Figure 5.1\. We could construct a linear classifier by first
    using OLS linear regression to fit the equation of a straight line to the dataset.
    For any value that lies above the line, the X class would be predicted, and for
    any value beneath the line, the O class would be predicted. Any dataset that can
    be separated by a straight line is known as linearly separable (as in our example),
    which forms an important subset of data types in machine learning problems. The
    straight line, in this case, would be called the decision boundary. More generally,
    the decision boundary is defined as the hyperplane separating the data. In this
    case, the decision boundary is linear. There could be cases where a decision boundary
    can be non-linear. Datasets such as the one in our example can be learned by linear
    classifiers such as an OLS-based classifier, or support vector machines (SVMs)
    with linear kernels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有一个虚构的数据集，其中包含两个不同的组，X和O，如图 5.1所示。我们可以通过首先使用最小二乘法线性回归来拟合一条直线方程，从而构造一个线性分类器。对于任何位于直线之上的值，将预测为X类，而对于任何位于直线之下的值，将预测为O类。任何能够被直线分开的数据集都被称为线性可分的（如我们的例子所示），这是机器学习问题中一个重要的数据类型子集。在这种情况下，直线将被称为决策边界。更一般地说，决策边界被定义为将数据分开的超平面。在这种情况下，决策边界是线性的。也可能存在决策边界是非线性的情况。像我们例子中的数据集，可以通过线性分类器（如基于OLS的分类器）或具有线性核的支持向量机（SVM）进行学习。
- en: 'However, this does not mean that a linear model can only have a linear decision
    boundary. A linear classifier/model is a model that is linear with respect to
    the parameters/weights (β) of the model, but not necessarily with respect to inputs
    (x). Depending on the input, a linear model may have a linear or non-linear decision
    boundary. As mentioned before, examples of linear models include OLS, SVM, and
    logistic regression, while examples of non-linear models include KNN, random forest,
    decision tree, and ANN. We will cover more of these models in the later parts
    of this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着线性模型只能有线性的决策边界。线性分类器/模型是指与模型的参数/权重（β）具有线性关系的模型，但不一定与输入（x）具有线性关系。根据输入的不同，线性模型可能会有线性或非线性的决策边界。如前所述，线性模型的例子包括最小二乘法、支持向量机（SVM）和逻辑回归，而非线性模型的例子包括KNN、随机森林、决策树和人工神经网络（ANN）。我们将在本章后续部分介绍更多这些模型：
- en: '![Figure 5.1: OLS as a classifier'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：最小二乘法作为分类器'
- en: '](img/image-1IET8G37.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-1IET8G37.jpg)'
- en: 'Figure 5.1: OLS as a classifier'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：最小二乘法作为分类器
- en: 'Exercise 5.01: Ordinary Least Squares as a Classifier'
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.01：最小二乘法作为分类器
- en: 'This exercise contains a contrived example of using OLS as a classifier. In
    this exercise, we will use a completely fictional dataset, and test how the OLS
    model fares as a classifier. In order to implement OLS, we will use the LinearRegression
    API of sklearn. The dataset is composed of manually selected x and y values for
    a scatterplot which are approximately divided into two groups. The dataset has
    been specifically designed for this exercise, to demonstrate how linear regression
    can be used as a classifier, and this is available in the accompanying code files
    for this book, as well as on GitHub, at https://packt.live/3a7oAY8:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习包含了一个使用最小二乘法（OLS）作为分类器的设计示例。在本练习中，我们将使用一个完全虚构的数据集，并测试最小二乘法模型作为分类器的效果。为了实现OLS，我们将使用sklearn的LinearRegression
    API。数据集由手动选择的x和y值组成，用于散点图，这些值大致被分为两组。该数据集专门为本练习设计，旨在演示如何将线性回归用作分类器，相关代码文件随书附赠，并且也可以在GitHub上找到，网址为
    https://packt.live/3a7oAY8：
- en: 'Import the required packages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: import matplotlib.pyplot as plt
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: import matplotlib.lines as mlines
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.lines as mlines
- en: import numpy as np
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pandas as pd
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: from sklearn.linear_model import LinearRegression
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.linear_model import LinearRegression
- en: from sklearn.model_selection import train_test_split
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.model_selection import train_test_split
- en: 'Load the linear_classifier.csv dataset into a pandas DataFrame:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将linear_classifier.csv数据集加载到pandas DataFrame中：
- en: df = pd.read_csv('../Datasets/linear_classifier.csv')
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv('../Datasets/linear_classifier.csv')
- en: df.head()
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.2: First five rows'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：前五行'
- en: '](img/image-EV57YN82.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-EV57YN82.jpg)'
- en: 'Figure 5.2: First five rows'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：前五行
- en: Looking through the dataset, each row contains a set of x, y coordinates, as
    well as the label corresponding to which class the data belongs to, either a cross
    (x) or a circle (o).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览数据集时，每一行包含一组x、y坐标，以及与数据所属类别相关的标签，类别可以是叉号（x）或圆圈（o）。
- en: 'Produce a scatterplot of the data with the marker for each point as the corresponding
    class label:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制数据的散点图，并用对应的类别标签作为每个点的标记：
- en: plt.figure(figsize=(10, 7))
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: 'for label, label_class in df.groupby(''labels''):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'for label, label_class in df.groupby(''labels''):'
- en: plt.scatter(label_class.values[:,0], label_class.values[:,1], \
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(label_class.values[:,0], label_class.values[:,1], \
- en: label=f'Class {label}', marker=label, c='k')
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: label=f'类 {label}', marker=label, c='k')
- en: plt.legend()
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend()
- en: plt.title("Linear Classifier");
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title("线性分类器");
- en: 'We''ll get the following scatterplot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下散点图：
- en: '![Figure 5.3 Scatterplot of a linear classifier'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3 线性分类器的散点图'
- en: '](img/image-OPFECQO9.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-OPFECQO9.jpg)'
- en: Figure 5.3 Scatterplot of a linear classifier
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 线性分类器的散点图
- en: 'In order to impartially evaluate the model, we should split the training dataset
    into a training and a test set. We make that train/test split in the ratio 60:40
    in the following step:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公正地评估模型，我们应将训练数据集分为训练集和测试集。我们将在接下来的步骤中按照60:40的比例进行训练/测试数据的划分：
- en: df_train, df_test = train_test_split(df.copy(), test_size=0.4, \
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: df_train, df_test = train_test_split(df.copy(), test_size=0.4, \
- en: random_state=12)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: random_state=12)
- en: 'Using the scikit-learn LinearRegression API from the previous chapter, fit
    a linear model to the x, y coordinates of the training dataset and print out the
    linear equation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一章中的scikit-learn LinearRegression API，将一个线性模型拟合到训练数据集的x、y坐标，并打印出线性方程：
- en: Fit a linear regression model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合线性回归模型
- en: model = LinearRegression()
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: model = LinearRegression()
- en: model.fit(df_train.x.values.reshape((-1, 1)), \
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(df_train.x.values.reshape((-1, 1)), \
- en: df_train.y.values.reshape((-1, 1)))
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: df_train.y.values.reshape((-1, 1)))
- en: Print out the parameters
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打印出参数
- en: print(f'y = {model.coef_[0][0]}x + {model.intercept_[0]}')
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'y = {model.coef_[0][0]}x + {model.intercept_[0]}')
- en: 'The output will be as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: y = 1.2718120805369124x + 8.865771812080538
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: y = 1.2718120805369124x + 8.865771812080538
- en: Note
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Throughout the exercises and activities in this chapter, owing to randomization,
    there could be a minor variation in the outputs presented here and those that
    you might obtain.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的练习和活动中，由于随机化的原因，输出结果可能会与你得到的结果略有不同。
- en: 'Plot the fitted trendline over the test dataset:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制拟合的趋势线于测试数据集上：
- en: Plot the trendline
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制趋势线
- en: trend = model.predict(np.linspace(0, 10).reshape((-1, 1)))
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: trend = model.predict(np.linspace(0, 10).reshape((-1, 1)))
- en: plt.figure(figsize=(10, 7))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: 'for label, label_class in df_test.groupby(''labels''):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'for label, label_class in df_test.groupby(''labels''):'
- en: plt.scatter(label_class.values[:,0], label_class.values[:,1], \
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(label_class.values[:,0], label_class.values[:,1], \
- en: label=f'Class {label}', marker=label, c='k')
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: label=f'类 {label}', marker=label, c='k')
- en: plt.plot(np.linspace(0, 10), trend, c='k', label='Trendline')
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(np.linspace(0, 10), trend, c='k', label='趋势线')
- en: plt.legend()
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend()
- en: plt.title("Linear Classifier");
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title("线性分类器");
- en: 'The output will be as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.4: Scatterplot with trendline'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：带趋势线的散点图'
- en: '](img/image-POXLL967.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-POXLL967.jpg)'
- en: 'Figure 5.4: Scatterplot with trendline'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：带趋势线的散点图
- en: 'With the fitted trendline, the classifier can then be applied. For each row
    in the test dataset, determine whether the x, y point lies above or below the
    linear model (or trendline). If the point lies below the trendline, the model
    predicts the o class; if above the line, the x class is predicted. Include these
    values as a column of predicted labels:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用拟合的趋势线后，可以应用分类器。对于测试数据集中的每一行，判断 x, y 点是否位于线性模型（或趋势线）之上或之下。如果点位于趋势线以下，模型预测为
    o 类；如果位于趋势线以上，预测为 x 类。将这些值作为预测标签列：
- en: Make predictions
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做出预测
- en: y_pred = model.predict(df_test.x.values.reshape((-1, 1)))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred = model.predict(df_test.x.values.reshape((-1, 1)))
- en: pred_labels = []
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: pred_labels = []
- en: 'for _y, _y_pred in zip(df_test.y, y_pred):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 'for _y, _y_pred in zip(df_test.y, y_pred):'
- en: 'if _y < _y_pred:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'if _y < _y_pred:'
- en: pred_labels.append('o')
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: pred_labels.append('o')
- en: 'else:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: pred_labels.append('x')
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: pred_labels.append('x')
- en: df_test['Pred Labels'] = pred_labels
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: df_test['预测标签'] = pred_labels
- en: df_test.head()
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: df_test.head()
- en: 'The output will be as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.5: First five rows'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5：前五行数据'
- en: '](img/image-MBL2A6EJ.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-MBL2A6EJ.jpg)'
- en: 'Figure 5.5: First five rows'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：前五行数据
- en: 'Plot the points with the corresponding ground truth labels. For those points
    where the labels were correctly predicted, plot the corresponding class. For those
    incorrect predictions, plot a diamond:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制具有相应真实标签的点。对于那些标签被正确预测的点，绘制相应的类别。对于那些错误预测的点，绘制一个菱形：
- en: plt.figure(figsize=(10, 7))
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: 'for idx, label_class in df_test.iterrows():'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'for idx, label_class in df_test.iterrows():'
- en: 'if label_class.labels != label_class[''Pred Labels'']:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 'if label_class.labels != label_class[''预测标签'']:'
- en: label = 'D'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: label = 'D'
- en: s=70
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: s=70
- en: 'else:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: label = label_class.labels
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: label = label_class.labels
- en: s=50
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: s=50
- en: plt.scatter(label_class.values[0], label_class.values[1], \
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(label_class.values[0], label_class.values[1], \
- en: label=f'Class {label}', marker=label, c='k', s=s)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: label=f'类 {label}', marker=label, c='k', s=s)
- en: plt.plot(np.linspace(0, 10), trend, c='k', label='Trendline')
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(np.linspace(0, 10), trend, c='k', label='趋势线')
- en: plt.title("Linear Classifier");
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title("线性分类器");
- en: incorrect_class = mlines.Line2D([], [], color='k', marker='D', \
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: incorrect_class = mlines.Line2D([], [], color='k', marker='D', \
- en: markersize=10, \
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: markersize=10, \
- en: label='Incorrect Classification');
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: label='错误分类');
- en: plt.legend(handles=[incorrect_class]);
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend(handles=[incorrect_class]);
- en: 'The output will be as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.6: Scatterplot showing incorrect predictions'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6：显示错误预测的散点图'
- en: '](img/image-JAYZIZCL.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JAYZIZCL.jpg)'
- en: 'Figure 5.6: Scatterplot showing incorrect predictions'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：显示错误预测的散点图
- en: We can see that, in this plot, the linear classifier made two incorrect predictions
    in this completely fictional dataset, one at x = 1, and another at x = 3.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在这个图中，线性分类器在这个完全虚构的数据集中做出了两个错误的预测，一个发生在 x = 1，另一个发生在 x = 3。
- en: Note
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3hT3Fwy.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/3hT3Fwy。
- en: You can also run this example online at https://packt.live/3fECHai. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/3fECHai 在线运行这个例子。你必须执行整个 Notebook 才能获得预期结果。
- en: But what if our dataset is not linearly separable and we cannot classify the
    data using a straight-line model, which is very frequently the case. Furthermore,
    the preceding approach doesn't give us a measure of confidence regarding the predictions.
    To cope with these challenges, we turn to other classification methods, many of
    which use different models, but the process logically flows from our simplified
    linear classifier model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们的数据集不是线性可分的，且我们无法使用直线模型对数据进行分类，这种情况非常常见。此外，前面的方式并没有给出关于预测的置信度度量。为了应对这些挑战，我们转向其他分类方法，其中许多使用不同的模型，但过程在逻辑上是从我们简化的线性分类器模型延续下来的。
- en: Logistic Regression
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'The logistic, or logit, model is a linear model that has been effectively used
    for classification tasks in a number of different domains. Recalling the definition
    of the OLS model from the previous section, the logistic regression model takes
    as input a linear combination of the input features. In this section, we will
    use it to classify images of handwritten digits. In understanding the logistic
    model, we also take an important step in understanding the operation of a particularly
    powerful machine learning model – artificial neural networks. So, what exactly
    is the logistic model? Like the OLS model, which is composed of a linear or straight-line
    function, the logistic model is composed of the standard logistic function, which,
    in mathematical terms, looks something like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型，或称为 logit 模型，是一种线性模型，已被有效应用于多个领域的分类任务。回顾上一节中 OLS 模型的定义，逻辑回归模型以输入特征的线性组合作为输入。在本节中，我们将使用它来分类手写数字图像。在理解逻辑回归模型时，我们也迈出了理解一个非常强大的机器学习模型——人工神经网络的关键一步。那么，逻辑回归模型究竟是什么呢？就像由线性或直线函数组成的
    OLS 模型一样，逻辑回归模型由标准的逻辑函数组成，在数学上，它看起来大致如下：
- en: '![Figure 5.7: Logistic function'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：逻辑函数'
- en: '](img/image-4DAJB21N.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-4DAJB21N.jpg)'
- en: 'Figure 5.7: Logistic function'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：逻辑函数
- en: In practical terms, when trained, this function returns the probability of the
    input information belonging to a particular class or group. In the preceding equation,
    x is the input feature vector (an array of numbers, each representing a feature
    of the input data), β1 is the parameter vector of the model that has to be learned
    by training the model, β0 is the bias term or offset term (yet another parameter)
    that helps the model to deal with any constant value offsets in the relationship
    between input (x) and output (y), and p(x) is the output probability of the data
    sample x belonging to a certain class. For example, if we have two classes, A
    and B, then p(x) is the probability of class A and 1-p(x) is the probability of
    class B.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际的角度来看，经过训练后，这个函数返回输入信息属于特定类别或组的概率。在前面的方程中，x 是输入特征向量（一个数字数组，每个数字代表输入数据的一个特征），β1
    是模型的参数向量，通过训练模型来学习，β0 是偏置项或偏置项（另一个参数），它帮助模型处理输入（x）和输出（y）之间的常数偏移值关系，p(x) 是数据样本
    x 属于某个类别的输出概率。例如，如果我们有两个类别 A 和 B，那么 p(x) 是类别 A 的概率，1-p(x) 是类别 B 的概率。
- en: 'So, how did we arrive at the logistic function? Well, the logistic regression
    model arises from the desire to model the log of odds in favor of a data point
    to belong to class A of the two classes (A and B) via linear functions in x. The
    model has the following form:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是如何得出逻辑函数的呢？逻辑回归模型的产生源于希望通过 x 的线性函数来建模数据点属于两个类别（A 和 B）中类别 A 的概率对数。该模型具有以下形式：
- en: '![Figure 5.8: Logistic function for the logistic regression model'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8：逻辑回归模型的逻辑函数'
- en: '](img/image-AXTYXOGW.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-AXTYXOGW.jpg)'
- en: 'Figure 5.8: Logistic function for the logistic regression model'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：逻辑回归模型的逻辑函数
- en: 'We are considering the case of binary classification here, with just two classes,
    A and B, although we could easily extend the discussion to multiclass classification
    as well using the one-versus-all classification trick. More on that will be discussed
    in a subsequent section. But for now, because we know there are only two classes,
    we know that:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里考虑的是二分类的情况，只有两个类别 A 和 B，尽管我们也可以通过一对多分类的技巧轻松扩展讨论到多分类问题。稍后会在后续章节中进一步讨论这个问题。但现在，由于我们知道只有两个类别，我们知道：
- en: '![Figure 5.9: Summation of the probability distribution'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：概率分布的求和'
- en: '](img/image-QDW5FLB3.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-QDW5FLB3.jpg)'
- en: 'Figure 5.9: Summation of the probability distribution'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：概率分布的求和
- en: 'Using the preceding two equations, we can get:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的两个方程式，我们可以得到：
- en: '![Figure 5.10: Logistic function for the logistic regression model with binary
    classification'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10：用于二分类的逻辑回归模型的逻辑函数'
- en: '](img/image-WKXQ6KDU.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-WKXQ6KDU.jpg)'
- en: 'Figure 5.10: Logistic function for the logistic regression model with binary
    classification'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：用于二分类的逻辑回归模型的逻辑函数
- en: 'And now, if we consider class A as our target class, we can replace p(class=A)
    with y (target output):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们将类别 A 视为我们的目标类别，我们可以将 p(类别=A) 替换为 y（目标输出）：
- en: '![Figure 5.11: Logistic function for the logistic regression model by replacing
    p(class=A)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：通过替换 p(类别=A)得到的逻辑回归模型的逻辑函数'
- en: '](img/image-H061BHFV.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-H061BHFV.jpg)'
- en: 'Figure 5.11: Logistic function for the logistic regression model by replacing
    p(class=A)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：通过替换 p(class=A) 得到的逻辑回归模型的逻辑函数
- en: The left-hand side of the preceding equation is popularly known as log-odds,
    as it is the logarithm of the odds ratio, which is the ratio of the probability
    of class A to the probability of class B. So, why is this important? For a linear
    model such as logistic regression, the fact that the log-odds of this model is
    linear with respect to the input x implies the linearity of the model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程的左侧通常被称为对数几率（log-odds），因为它是几率比的对数，几率比是类 A 的概率与类 B 的概率之比。那么，为什么这很重要呢？对于像逻辑回归这样的线性模型，模型的对数几率与输入
    x 之间的线性关系意味着模型的线性性。
- en: 'By rearranging the preceding equation slightly, we get the logistic function:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过稍微重新排列上述方程，我们得到逻辑函数：
- en: '![Figure 5.12: Logistic function'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12：逻辑函数'
- en: '](img/image-E9QCK09Q.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-E9QCK09Q.jpg)'
- en: 'Figure 5.12: Logistic function'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：逻辑函数
- en: 'Notice the exponents of e, that is, β0 + β1x, and that this relationship is
    a linear function of the two training parameters or weights, β0 and β1, as well
    as the input feature vector, x. If we were to assume β0 = 0 and β1 = 1, and plot
    the logistic function over the range (-6, 6), we would get the following result:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 e 的指数部分，即 β0 + β1x，并且这种关系是两个训练参数或权重 β0 和 β1，以及输入特征向量 x 的线性函数。如果我们假设 β0 =
    0 和 β1 = 1，并在范围 (-6, 6) 内绘制逻辑函数，我们将得到如下结果：
- en: '![Figure 5.13: Logistic function curve'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13：逻辑函数曲线'
- en: '](img/image-2O7HWM1T.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-2O7HWM1T.jpg)'
- en: 'Figure 5.13: Logistic function curve'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：逻辑函数曲线
- en: Note
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The sigmoid curve centers around the point x = -β0, so, if β0 is nonzero, the
    curve would not center around the point x=0, as shown in the preceding figure.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: S 型曲线围绕点 x = -β0 中心，因此，如果 β0 不为零，曲线将不会围绕点 x = 0 中心，如前图所示。
- en: Examining Figure 5.13, we notice some important aspects of classification. The
    first thing to note is that, if we look at the probability values on the y axis
    at the extremes of the function, the values are almost at zero when x = -6 and
    at one when x = 6\. While it looks like the values are in fact 0 and 1, this is
    not exactly the case. The logistic function approaches zero and one at these extremes
    and will only equal zero and one when x is at a positive or negative infinity.
    In practical terms, what this means is that the logistic function will never return
    a probability of greater than or equal to one, or less than or equal to zero,
    which is perfect for a classification task. In any event, we could never have
    a probability of greater than one since, by definition, a probability of one is
    a certainty of an event occurring. Likewise, we cannot have a probability of less
    than zero since, by definition, a probability of zero is a certainty of the event
    not occurring. The fact that the logistic function approaches but never equals
    one or zero means that there is always some uncertainty in the outcome or the
    classification.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图 5.13，我们注意到分类的一些重要方面。首先要注意的是，如果我们查看函数两端的 y 轴上的概率值，当 x = -6 时，值几乎为零，而当 x =
    6 时，值接近一。虽然看起来这些值实际上是 0 和 1，但实际情况并非如此。逻辑函数在这些极限点趋近于零和一，只有当 x 处于正无穷或负无穷时，才会等于零或一。从实际角度看，这意味着逻辑函数永远不会返回大于或等于
    1 的概率，或者小于或等于 0 的概率，这对于分类任务来说是完美的。无论如何，我们永远不会有大于 1 的概率，因为根据定义，概率为 1 表示事件发生的确定性。同样，我们也不能有小于
    0 的概率，因为根据定义，概率为 0 表示事件不发生的确定性。逻辑函数趋近但永远不会等于 1 或 0，这意味着结果或分类中总会存在一定的不确定性。
- en: The final aspect to notice regarding the logistic function is that at x = 0,
    the probability is 0.5, which, if we were to get this result, would indicate that
    the model is equally uncertain about the outcome of the corresponding class; that
    is, it really has no idea.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于逻辑函数的最后一个方面是，当 x = 0 时，概率为 0.5。如果我们得到这个结果，那么说明模型对对应类别的结果同样不确定；也就是说，它实际上并没有任何明确的判断。
- en: Note
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is very important to correctly understand and interpret the probability information
    provided by classification models such as logistic regression. Consider this probability
    score as the chance of the input information belonging to a particular class given
    the variability in the information provided by the training data. One common mistake
    is to use this probability score as an objective measure of whether the model
    can be trusted regarding its prediction; unfortunately, this isn't necessarily
    the case. For example, a model can provide a probability of 99.99% that some data
    belongs to a particular class and might still be absolutely wrong.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正确理解和解释分类模型（如逻辑回归）提供的概率信息非常重要。将这个概率分数视为给定训练数据的变异性后，输入信息属于某一特定类别的概率。一个常见的错误是将这个概率分数作为判断模型预测是否可靠的客观指标；然而，这并不一定是正确的。例如，一个模型可能提供99.99%的概率，认为某些数据属于特定类别，但它仍然可能完全错误。
- en: 'What we do use the probability value for is selecting the predicted class by
    the classifier. Between the model outputting the probability and us deciding the
    predicted class lies the probability threshold value. We need to decide a threshold
    value, τ, between 0 and 1, such that the two classes (say, A and B) can then be
    defined as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用概率值的目的是选择分类器的预测类别。在模型输出概率与我们决定预测类别之间，存在一个概率阈值。我们需要决定一个阈值τ，它位于0和1之间，从而可以定义两个类别（例如，A和B），具体定义如下：
- en: Data samples with a model output probability between 0 and τ belong to class
    A.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出概率介于0和τ之间的数据样本属于类别A。
- en: Data samples with a model output probability between τ and 1 belong to class
    B.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出概率介于τ和1之间的数据样本属于类别B。
- en: Now, say we had a model that was to predict whether some set of data belonged
    to class A or class B, and we decided the threshold to be 0.5 (which is actually
    a very common choice). If the logistic model returned a probability of 0.7, then
    we would return class B as the predicted class for the model. If the probability
    was only 0.2, the predicted class for the model would be class A.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个模型，它用于预测某一组数据属于类别A还是类别B，并且我们决定阈值为0.5（这实际上是一个非常常见的选择）。如果逻辑回归模型返回的概率是0.7，那么我们会将类别B作为模型的预测类别。如果概率只有0.2，那么模型的预测类别将是类别A。
- en: 'Exercise 5.02: Logistic Regression as a Classifier – Binary Classifier'
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '练习 5.02: 逻辑回归作为分类器 – 二元分类器'
- en: 'For this exercise, we will be using a sample of the famous MNIST dataset (available
    at http://yann.lecun.com/exdb/mnist/ or on GitHub at https://packt.live/3a7oAY8),
    which is a sequence of images of handwritten code digits, 0 through 0, with corresponding
    labels. The MNIST dataset is comprised of 60,000 training samples and 10,000 test
    samples, where each sample is a grayscale image with a size of 28 x 28 pixels.
    In this exercise, we will use logistic regression to build a classifier. The first
    classifier we will build is a binary classifier, where we will determine whether
    the image is a handwritten 0 or a 1:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次练习，我们将使用著名的MNIST数据集样本（可通过http://yann.lecun.com/exdb/mnist/ 或在GitHub上的https://packt.live/3a7oAY8获取）。该数据集包含从0到9的手写数字图像及其对应的标签。MNIST数据集包括60,000个训练样本和10,000个测试样本，每个样本都是28
    x 28像素的灰度图像。在本次练习中，我们将使用逻辑回归来构建一个分类器。我们首先要构建的是一个二元分类器，用于判断图像是手写数字0还是1：
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    import statements:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本次练习，我们需要导入一些依赖库。请执行以下导入语句：
- en: import struct
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: import struct
- en: import numpy as np
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import gzip
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: import gzip
- en: import urllib.request
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: import urllib.request
- en: import matplotlib.pyplot as plt
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from array import array
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: from array import array
- en: from sklearn.linear_model import LogisticRegression
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.linear_model import LogisticRegression
- en: 'We will also need to download the MNIST datasets. You will only need to do
    this once, so after this step, feel free to comment out or remove these cells.
    Download the image data, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要下载MNIST数据集。你只需做这一步一次，因此在此步骤之后，可以随意注释掉或删除这些代码块。请按照以下方式下载图像数据：
- en: request = \
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: request = \
- en: urllib.request.urlopen('http://yann.lecun.com/exdb'\
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: urllib.request.urlopen('http://yann.lecun.com/exdb'\
- en: '''/mnist/train-images-idx3-ubyte.gz'')'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '''/mnist/train-images-idx3-ubyte.gz'')'
- en: 'with open(''../Datasets/train-images-idx3-ubyte.gz'', ''wb'') as f:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''../Datasets/train-images-idx3-ubyte.gz'', ''wb'') as f:'
- en: f.write(request.read())
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(request.read())
- en: request = \
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: request = \
- en: urllib.request.urlopen('http://yann.lecun.com/exdb'\
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: urllib.request.urlopen('http://yann.lecun.com/exdb'\
- en: '''/mnist/t10k-images-idx3-ubyte.gz'')'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '''/mnist/t10k-images-idx3-ubyte.gz'')'
- en: 'with open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''wb'') as f:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''wb'') as f:'
- en: f.write(request.read())
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(request.read())
- en: 'Download the corresponding labels for the data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据的相应标签：
- en: request = \
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: request = \
- en: urllib.request.urlopen('http://yann.lecun.com/exdb'\
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: urllib.request.urlopen('http://yann.lecun.com/exdb'\
- en: '''/mnist/train-labels-idx1-ubyte.gz'')'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '''/mnist/train-labels-idx1-ubyte.gz'')'
- en: 'with open(''../Datasets/train-labels-idx1-ubyte.gz'', ''wb'') as f:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''../Datasets/train-labels-idx1-ubyte.gz'', ''wb'') as f:'
- en: f.write(request.read())
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(request.read())
- en: request = \
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: request = \
- en: urllib.request.urlopen('http://yann.lecun.com/exdb'\
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: urllib.request.urlopen('http://yann.lecun.com/exdb'\
- en: '''/mnist/t10k-labels-idx1-ubyte.gz'')'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '''/mnist/t10k-labels-idx1-ubyte.gz'')'
- en: 'with open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''wb'') as f:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'with open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''wb'') as f:'
- en: f.write(request.read())
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: f.write(request.read())
- en: 'Once all the files have been successfully downloaded, unzip the files in the
    local directory using the following command (for Windows):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有文件都成功下载，请使用以下命令解压缩本地目录中的文件（适用于 Windows）：
- en: '!ls *.gz #!dir *.gz for windows'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '!ls *.gz #!dir *.gz for windows'
- en: 'The output will be as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: t10k-images-idx3-ubyte.gz train-images-idx3-ubyte.gz
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: t10k-images-idx3-ubyte.gz train-images-idx3-ubyte.gz
- en: t10k-labels-idx1-ubyte.gz train-images-idx1-ubyte.gz
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: t10k-labels-idx1-ubyte.gz train-images-idx1-ubyte.gz
- en: Note
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For Linux and macOS, check out the files in the local directory using the !ls
    *.gz command.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Linux 和 macOS，请使用 !ls *.gz 命令检查本地目录中的文件。
- en: 'Load the downloaded data. Don''t worry too much about the exact details of
    reading the data, as these are specific to the MNIST dataset:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 加载下载的数据。无需过多担心数据读取的具体细节，因为这些是针对 MNIST 数据集的特定内容：
- en: 'with gzip.open(''../Datasets/train-images-idx3-ubyte.gz'', ''rb'') as f:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/train-images-idx3-ubyte.gz'', ''rb'') as f:'
- en: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
- en: img = np.array(array("B", f.read())).reshape((size, rows, cols))
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: img = np.array(array("B", f.read())).reshape((size, rows, cols))
- en: 'with gzip.open(''../Datasets/train-labels-idx1-ubyte.gz'', ''rb'') as f:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/train-labels-idx1-ubyte.gz'', ''rb'') as f:'
- en: magic, size = struct.unpack(">II", f.read(8))
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size = struct.unpack(">II", f.read(8))
- en: labels = np.array(array("B", f.read()))
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: labels = np.array(array("B", f.read()))
- en: 'with gzip.open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''rb'') as f:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''rb'') as f:'
- en: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
- en: img_test = np.array(array("B", f.read()))\
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: img_test = np.array(array("B", f.read()))\
- en: .reshape((size, rows, cols))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: .reshape((size, rows, cols))
- en: 'with gzip.open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''rb'') as f:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''rb'') as f:'
- en: magic, size = struct.unpack(">II", f.read(8))
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size = struct.unpack(">II", f.read(8))
- en: labels_test = np.array(array("B", f.read()))
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: labels_test = np.array(array("B", f.read()))
- en: 'As always, having a thorough understanding of the data is key, so create an
    image plot of the first 10 images in the training sample. Notice the grayscale
    images and the fact that the corresponding labels are the digits 0 through 9:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，彻底理解数据至关重要，因此请创建训练样本中前 10 张图像的图像图。注意这些是灰度图像，并且对应的标签是 0 到 9 的数字：
- en: 'for i in range(10):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: plt.subplot(2, 5, i + 1)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(2, 5, i + 1)
- en: plt.imshow(img[i], cmap='gray');
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(img[i], cmap='gray');
- en: plt.title(f'{labels[i]}');
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title(f'{labels[i]}');
- en: plt.axis('off')
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off')
- en: 'The output will be as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.14: Training images'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14：训练图像'
- en: '](img/image-AAWUQ18Q.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-AAWUQ18Q.jpg)'
- en: 'Figure 5.14: Training images'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：训练图像
- en: 'As the initial classifier is aiming to classify either images of zeros or images
    of ones, we must first select these samples from the dataset:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于初始分类器旨在分类数字 0 或数字 1 的图像，我们必须首先从数据集中选择这些样本：
- en: samples_0_1 = np.where((labels == 0) | (labels == 1))[0]
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: samples_0_1 = np.where((labels == 0) | (labels == 1))[0]
- en: images_0_1 = img[samples_0_1]
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: images_0_1 = img[samples_0_1]
- en: labels_0_1 = labels[samples_0_1]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: labels_0_1 = labels[samples_0_1]
- en: samples_0_1_test = np.where((labels_test == 0) | (labels_test == 1))
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: samples_0_1_test = np.where((labels_test == 0) | (labels_test == 1))
- en: images_0_1_test = img_test[samples_0_1_test]\
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: images_0_1_test = img_test[samples_0_1_test]\
- en: .reshape((-1, rows * cols))
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: .reshape((-1, rows * cols))
- en: labels_0_1_test = labels_test[samples_0_1_test]
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: labels_0_1_test = labels_test[samples_0_1_test]
- en: Visualize one sample from the 0 selection and another from the handwritten 1
    digits to ensure that we have correctly allocated the data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化从 0 选择的一个样本和从手写数字 1 中选取的另一个样本，以确保我们已正确分配数据。
- en: 'Here is the code for 0:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 0 的代码：
- en: sample_0 = np.where((labels == 0))[0][0]
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: sample_0 = np.where((labels == 0))[0][0]
- en: plt.imshow(img[sample_0], cmap='gray');
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(img[sample_0], cmap='gray');
- en: 'The output will be as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.15: First handwritten image'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15：第一张手写图像'
- en: '](img/image-DZL45SCU.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-DZL45SCU.jpg)'
- en: 'Figure 5.15: First handwritten image'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：第一张手写图片
- en: 'Here is the code for 1:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是1的代码：
- en: sample_1 = np.where((labels == 1))[0][0]
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: sample_1 = np.where((labels == 1))[0][0]
- en: plt.imshow(img[sample_1], cmap='gray');
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(img[sample_1], cmap='gray');
- en: 'The output will be as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.16: Second handwritten image'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.16：第二张手写图片'
- en: '](img/image-7G19D7FE.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-7G19D7FE.jpg)'
- en: 'Figure 5.16: Second handwritten image'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16：第二张手写图片
- en: 'We are almost at the stage where we can start building the model. However,
    as each sample is an image and has data in a matrix format, we must first rearrange
    each of the images. The model needs the images to be provided in vector form,
    that is, all the information for each image is stored in one row. Execute this
    as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎到达了可以开始构建模型的阶段。然而，由于每个样本是一个图片，并且数据采用矩阵格式，因此我们必须先重新排列每张图片。模型需要以向量形式提供图片，即每张图片的所有信息都存储在一行中。按照如下步骤执行：
- en: images_0_1 = images_0_1.reshape((-1, rows * cols))
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: images_0_1 = images_0_1.reshape((-1, rows * cols))
- en: images_0_1.shape
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: images_0_1.shape
- en: 'Now, we can build and fit the logistic regression model with the selected images
    and labels:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用选定的图片和标签来构建并拟合逻辑回归模型：
- en: model = LogisticRegression(solver='liblinear')
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: model = LogisticRegression(solver='liblinear')
- en: model.fit(X=images_0_1, y=labels_0_1)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(X=images_0_1, y=labels_0_1)
- en: 'The output will be as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.17: Logistic regression model'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.17：逻辑回归模型'
- en: '](img/image-3G4O0GPR.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-3G4O0GPR.jpg)'
- en: 'Figure 5.17: Logistic regression model'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17：逻辑回归模型
- en: Note how the scikit-learn API calls for logistic regression are consistent with
    that of linear regression. There is an additional argument, solver, which specifies
    the type of optimization process to be used. We have provided this argument here
    with the default value to suppress a future warning in this version of scikit-learn
    that requires solver to be specified. The specifics of the solver argument are
    beyond the scope of this chapter and have only been included to suppress the warning
    message.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，scikit-learn API 对逻辑回归的调用与线性回归的一致。还有一个额外的参数 `solver`，它指定了要使用的优化过程类型。我们在这里提供了该参数的默认值，以抑制
    scikit-learn 在该版本中要求指定 `solver` 的警告。`solver` 参数的具体内容超出了本章的范围，添加它的目的是为了抑制警告消息。
- en: 'Check the performance of this model against the corresponding training data:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 检查该模型在相应训练数据上的表现：
- en: model.score(X=images_0_1, y=labels_0_1)
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=images_0_1, y=labels_0_1)
- en: 'The output will be as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '1.0'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: In this example, the model was able to predict the training labels with 100% accuracy.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，模型能够以100%的准确率预测训练标签。
- en: 'Display the first two predicted labels for the training data using the model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型显示训练数据的前两个预测标签：
- en: model.predict(images_0_1) [:2]
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: model.predict(images_0_1) [:2]
- en: 'The output will be as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: array([0, 1], dtype=uint8)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: array([0, 1], dtype=uint8)
- en: 'How is the logistic regression model making the classification decisions? Look
    at some of the probabilities produced by the model for the training set:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型是如何做出分类决策的？来看一下模型为训练集生成的一些概率：
- en: model.predict_proba(images_0_1)[:2]
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: model.predict_proba(images_0_1)[:2]
- en: 'The output will be as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: array([[9.99999999e-01, 9.89532857e-10],
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: array([[9.99999999e-01, 9.89532857e-10],
- en: '[4.56461358e-09, 9.99999995e-01]])'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[4.56461358e-09, 9.99999995e-01]])'
- en: We can see that, for each prediction made, there are two probability values.
    For the prediction of each image, the first value is the probability that it is
    an image of digit 0, and the second value is the probability of digit 1\. These
    two values add up to 1\. We can see that, in the first example, the prediction
    probability is 0.9999999 for digit 0 and, hence, the prediction is digit 0\. Similarly,
    the inverse is true for the second example.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于每个预测，都会有两个概率值。对于每张图片的预测，第一个值是该图片是数字0的概率，第二个值是数字1的概率。这两个值相加为1。我们可以看到，在第一个例子中，数字0的预测概率为0.9999999，因此，预测为数字0。同理，第二个例子的情况也相反。
- en: Note
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The probabilities should ideally add up to 1 but, due to computational limits
    and truncation errors, it is almost 1.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，这两个概率应该相加为1，但由于计算限制和截断误差，它们几乎等于1。
- en: 'Compute the performance of the model against the test set to check its performance
    against data that it has not seen:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的表现，以检查其在未见数据上的表现：
- en: model.score(X=images_0_1_test, y=labels_0_1_test)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=images_0_1_test, y=labels_0_1_test)
- en: 'The output will be as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '0.9995271867612293'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9995271867612293'
- en: Note
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to Chapter 7, Model Evaluation, for better methods of objectively measuring
    the model's performance.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅第7章《模型评估》，了解更好的客观评估模型性能的方法。
- en: We can see here that logistic regression is a powerful classifier that is able
    to distinguish between handwritten samples of 0 and 1.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，逻辑回归是一种强大的分类器，能够区分手写的0和1样本。
- en: Note
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3dqqEvH.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参见 [https://packt.live/3dqqEvH](https://packt.live/3dqqEvH)。
- en: You can also run this example online at https://packt.live/3hT6FJm. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址是 [https://packt.live/3hT6FJm](https://packt.live/3hT6FJm)。你必须执行整个Notebook才能获得期望的结果。
- en: Now that we have trained a logistic regression model on a binary classification
    problem, let's extend the model to multiple classes. Essentially, we will be using
    the same dataset and, instead of classifying into just two classes or digits,
    0 and 1, we classify into all 10 classes, or digits 0–9\. In essence, multiclass
    classification for logistic regression works as one-versus-all classification.
    That is, for classification into the 10 classes, we will be training 10 binary
    classifiers. Each classifier will have 1 digit as the first class, and all the
    other 9 digits as the second class. In this way, we get 10 binary classifiers
    that are then collectively used to make predictions. In other words, we get the
    prediction probabilities from each of the 10 binary classifiers and the final
    output digit/class is one whose classifier gave the highest probability.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在一个二分类问题上训练了一个逻辑回归模型，让我们将模型扩展到多个类。实际上，我们将使用相同的数据集，而不是只将其分类为两个类别或数字0和1，而是将其分类为所有10个类别，或数字0-9。实际上，逻辑回归的多类分类是通过一对多分类来实现的。也就是说，对于分类为10个类别，我们将训练10个二分类器。每个分类器将有一个数字作为第一类，其他9个数字作为第二类。通过这种方式，我们得到了10个二分类器，然后共同用于做出预测。换句话说，我们从每个二分类器中获取预测概率，最终的输出数字/类别是其分类器给出最高概率的那个。
- en: 'Exercise 5.03: Logistic Regression – Multiclass Classifier'
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '练习 5.03: 逻辑回归 – 多类分类器'
- en: 'In the previous exercise, we examined using logistic regression to classify
    between one of two groups. Logistic regression, however, can also be used to classify
    a set of input information to k different groups and it is this multiclass classifier
    we will be investigating in this exercise. The process for loading the MNIST training
    and test data is identical to the previous exercise:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的练习中，我们研究了使用逻辑回归对两个组别进行分类。然而，逻辑回归也可以用于将输入信息分类为k个不同的组，这就是我们将在本练习中探讨的多类分类器。加载MNIST训练和测试数据的过程与前一个练习完全相同：
- en: 'Import the required packages:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: import struct
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: import struct
- en: import numpy as np
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import gzip
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: import gzip
- en: import urllib.request
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: import urllib.request
- en: import matplotlib.pyplot as plt
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from array import array
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: from array import array
- en: from sklearn.linear_model import LogisticRegression
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.linear_model import LogisticRegression
- en: 'Load the training/test images and the corresponding labels:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 加载训练/测试图像及其对应的标签：
- en: 'with gzip.open(''../Datasets/train-images-idx3-ubyte.gz'', ''rb'') as f:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/train-images-idx3-ubyte.gz'', ''rb'') as f:'
- en: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
- en: img = np.array(array("B", f.read()))\
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: img = np.array(array("B", f.read()))\
- en: .reshape((size, rows, cols))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: .reshape((size, rows, cols))
- en: 'with gzip.open(''../Datasets/train-labels-idx1-ubyte.gz'', ''rb'') as f:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/train-labels-idx1-ubyte.gz'', ''rb'') as f:'
- en: magic, size = struct.unpack(">II", f.read(8))
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size = struct.unpack(">II", f.read(8))
- en: labels = np.array(array("B", f.read()))
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: labels = np.array(array("B", f.read()))
- en: 'with gzip.open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''rb'') as f:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''rb'') as f:'
- en: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
- en: img_test = np.array(array("B", f.read()))\
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: img_test = np.array(array("B", f.read()))\
- en: .reshape((size, rows, cols))
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: .reshape((size, rows, cols))
- en: 'with gzip.open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''rb'') as f:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''rb'') as f:'
- en: magic, size = struct.unpack(">II", f.read(8))
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size = struct.unpack(">II", f.read(8))
- en: labels_test = np.array(array("B", f.read()))
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: labels_test = np.array(array("B", f.read()))
- en: 'Visualize a sample of the data:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据的一个样本：
- en: 'for i in range(10):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: plt.subplot(2, 5, i + 1)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(2, 5, i + 1)
- en: plt.imshow(img[i], cmap='gray');
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(img[i], cmap='gray');
- en: plt.title(f'{labels[i]}');
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title(f'{labels[i]}');
- en: plt.axis('off')
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off')
- en: 'The output will be as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.18: Sample data'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.18：示例数据'
- en: '](img/image-DQ7T1V0S.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-DQ7T1V0S.jpg)'
- en: 'Figure 5.18: Sample data'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18：示例数据
- en: 'Given that the training data is so large, we will select a subset of the overall
    data to reduce the training time as well as the system resources required for
    the training process:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于训练数据量非常大，我们将选择一部分数据来减少训练时间及所需的系统资源：
- en: 'np.random.seed(0) # Give consistent random numbers'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'np.random.seed(0) # 提供一致的随机数'
- en: selection = np.random.choice(len(img), 5000)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: selection = np.random.choice(len(img), 5000)
- en: selected_images = img[selection]
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images = img[selection]
- en: selected_labels = labels[selection]
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: selected_labels = labels[selection]
- en: Note that, in this example, we are using data from all 10 classes, not just
    classes 0 and 1, so we are making this example a multiclass classification problem.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本示例中，我们使用了来自所有 10 个类别的数据，而不仅仅是类别 0 和 1，因此我们将此示例视为一个多类分类问题。
- en: 'Again, reshape the input data in vector form for later use:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 再次将输入数据重塑为向量形式，以便后续使用：
- en: selected_images = selected_images.reshape((-1, rows * cols))
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images = selected_images.reshape((-1, rows * cols))
- en: selected_images.shape
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images.shape
- en: 'The output will be as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: (5000, 784)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: (5000, 784)
- en: 'The next cell is intentionally commented out. Leave this code commented out
    for the moment:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个单元格故意被注释掉了。暂时保留这段代码注释：
- en: selected_images = selected_images / 255.0
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: selected_images = selected_images / 255.0
- en: img_test = img_test / 255.0
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: img_test = img_test / 255.0
- en: 'Construct the logistic model. There are a few extra arguments, as follows:
    the lbfgs value for solver is geared up for multiclass problems, with additional
    max_iter iterations required for converging on a solution. The multi_class argument
    is set to multinomial to calculate the loss over the entire probability distribution:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 构建逻辑回归模型。该模型有一些额外的参数如下：求解器的 lbfgs 值适用于多类别问题，且需要更多的 max_iter 迭代才能收敛。multi_class
    参数设置为 multinomial，以便计算整个概率分布的损失：
- en: model = LogisticRegression(solver='lbfgs', \
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: model = LogisticRegression(solver='lbfgs', \
- en: multi_class='multinomial', \
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: multi_class='multinomial', \
- en: max_iter=500, tol=0.1)
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: max_iter=500, tol=0.1)
- en: model.fit(X=selected_images, y=selected_labels)
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(X=selected_images, y=selected_labels)
- en: 'The output will be as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.19: Logistic regression model'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.19：逻辑回归模型'
- en: '](img/image-KTEK5125.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-KTEK5125.jpg)'
- en: 'Figure 5.19: Logistic regression model'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19：逻辑回归模型
- en: Note
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to the documentation at https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
    for more information on the arguments.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 详细信息请参阅文档：https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html。
- en: 'Determine the accuracy score against the training set:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 确定训练集的准确度评分：
- en: model.score(X=selected_images, y=selected_labels)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=selected_images, y=selected_labels)
- en: 'The output will be as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '1.0'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: 'Determine the first two predictions for the training set and plot the images
    with the corresponding predictions:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 确定训练集前两个预测结果，并绘制带有相应预测的图像：
- en: model.predict(selected_images)[:2]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: model.predict(selected_images)[:2]
- en: 'The output will be as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: array([4, 1], dtype-uint8)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: array([4, 1], dtype=uint8)
- en: 'Show the images for the first two samples of the training set to see whether
    we are correct:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 显示训练集前两个样本的图像，以查看我们是否正确：
- en: plt.subplot(1, 2, 1)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(1, 2, 1)
- en: plt.imshow(selected_images[0].reshape((28, 28)), cmap='gray');
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(selected_images[0].reshape((28, 28)), cmap='gray');
- en: plt.axis('off');
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off');
- en: plt.subplot(1, 2, 2)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(1, 2, 2)
- en: plt.imshow(selected_images[1].reshape((28, 28)), cmap='gray');
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(selected_images[1].reshape((28, 28)), cmap='gray');
- en: plt.axis('off');
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off');
- en: 'The output will be as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.20: Plotting the two selected images'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.20：绘制两个选定图像'
- en: '](img/image-7TD8CXXH.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-7TD8CXXH.jpg)'
- en: 'Figure 5.20: Plotting the two selected images'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20：绘制两个选定图像
- en: 'Again, print out the probability scores provided by the model for the first
    sample of the training set. Confirm that there are 10 different values for each
    of the 10 classes in the set:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 再次打印出模型为训练集中的第一个样本提供的概率得分。确认每个类别有 10 个不同的值：
- en: model.predict_proba(selected_images)[0]
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: model.predict_proba(selected_images)[0]
- en: 'The output will be as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.21: Array of predicted values'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.21：预测值数组'
- en: '](img/image-JOG6A5G8.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JOG6A5G8.jpg)'
- en: 'Figure 5.21: Array of predicted values'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21：预测值数组
- en: Notice that, in the probability array of the first sample, the fifth (index
    four) sample is the highest probability, thereby indicating a prediction of 4.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在第一个样本的概率数组中，第五个（索引四）样本的概率最高，这表明预测结果为4。
- en: 'Compute the accuracy of the model against the test set. This will provide a
    reasonable estimate of the model''s in the wild performance, as it has never seen
    the data in the test set. It is expected that the accuracy rate of the test set
    will be slightly lower than the training set, given that the model has not been
    exposed to this data:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的准确率。这将为模型在实际环境中的表现提供一个合理的估计，因为模型从未见过测试集中的数据。由于模型没有接触过这些数据，预计测试集的准确率会略低于训练集：
- en: model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)
- en: 'The output will be as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '0.878'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '0.878'
- en: When checked against the test set, the model produced an accuracy level of 87.8%.
    When applying a test set, a performance drop is expected, as this is the very
    first time the model has seen these samples; while, during training, the training
    set was repeatedly shown to the model.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上检查时，模型的准确率为87.8%。当应用测试集时，性能下降是预期中的，因为这是模型第一次看到这些样本；而在训练过程中，训练集数据是反复呈现给模型的。
- en: 'Find the cell with the commented-out code, as shown in Step 4\. Uncomment the
    code in this cell:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 找到带有注释掉的代码的单元格，如第4步所示。取消注释该单元格中的代码：
- en: selected_images = selected_images / 255.0
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images = selected_images / 255.0
- en: img_test = img_test / 255.0
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: img_test = img_test / 255.0
- en: This cell simply scales all the image values to between 0 and 1\. Grayscale
    images are comprised of pixels with values between and including 0–255, where
    0 is black and 255 is white.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单元格仅仅是将所有图像的值缩放到0到1之间。灰度图像的像素值范围在0到255之间，其中0代表黑色，255代表白色。
- en: Click Restart & Run-All to rerun the entire notebook.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“重启并运行全部”以重新运行整个Notebook。
- en: 'Find the training set error:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 找到训练集的误差：
- en: model.score(X=selected_images, y=selected_labels)
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=selected_images, y=selected_labels)
- en: 'We''ll get the following score:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下得分：
- en: '0.986'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '0.986'
- en: 'Find the test set error:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 找到测试集的误差：
- en: model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)
- en: 'We''ll get the following score:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下得分：
- en: '0.9002'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9002'
- en: Note
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2B1CNKe.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2B1CNKe](https://packt.live/2B1CNKe)。
- en: You can also run this example online at https://packt.live/3fQU4Vd. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，网址是 [https://packt.live/3fQU4Vd](https://packt.live/3fQU4Vd)。你必须执行整个Notebook才能得到预期的结果。
- en: What effect did normalizing the images have on the overall performance of the
    system? The training error is worse! We went from 100% accuracy in the training
    set to 98.6%. Yes, there was a reduction in the performance of the training set,
    but an increase in the test set accuracy from 87.8% to 90.02%. The test set performance
    is of more interest, as the model has not seen this data before, and so it is
    a better representation of the performance that we could expect once the model
    is in the field. So, why do we get a better result?
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 正常化图像对系统整体性能有何影响？训练误差更大了！我们从训练集的100%准确率下降到98.6%。是的，训练集的性能有所下降，但测试集的准确率从87.8%上升到了90.02%。测试集的性能更为重要，因为模型之前没有见过这些数据，因此它更能代表我们期望模型在实际应用中的表现。那么，为什么会有更好的结果呢？
- en: Recall what we discussed about normalization and data scaling methods in Chapter
    2, Exploratory Data Analysis and Visualization. And now let's review Figure 5.13,
    and notice the shape of the curve as it approaches -6 and +6\. The curve saturates
    or flattens at almost 0 and almost 1, respectively. So, if we use an image (or
    x values) of between 0 and 255, the class probability defined by the logistic
    function is well within this flat region of the curve. Predictions within this
    region are unlikely to change much, as they will need to have very large changes
    in x values for any meaningful change in y. Scaling the images to be between 0
    and 1 initially puts the predictions closer to p(x) = 0.5, and so, changes in
    x can have a bigger impact on the value for y. This allows for more sensitive
    predictions and results in getting a couple of predictions in the training set
    wrong, but more in the test set right. It is recommended, for your logistic regression
    models, that you scale the input values to be between either 0 and 1 or -1 and
    1 prior to training and testing.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们在第二章《探索性数据分析与可视化》中讨论的关于归一化和数据缩放方法的内容。现在让我们回顾一下图5.13，并注意曲线在接近-6和+6时的形状。曲线在接近0和接近1时分别饱和或变平。所以，如果我们使用的是0到255之间的图像（或x值），由逻辑函数定义的类概率会落在曲线的平坦区域内。位于此区域内的预测不太可能发生太大变化，因为它们需要非常大的x值变化才能使y发生有意义的变化。将图像缩放到0和1之间，最初会将预测值接近p(x)
    = 0.5，因此，x的变化对y的值影响更大。这可以实现更灵敏的预测，并导致在训练集上犯几个错误，但在测试集上更多的正确预测。建议在进行逻辑回归模型的训练和测试之前，将输入值缩放到0到1或-1到1之间。
- en: 'The following function is one way of scaling values of a NumPy array between
    0 and 1:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数是一种将NumPy数组的值缩放到0和1之间的方法：
- en: 'def scale_input(x):'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`scale_input(x)`函数：
- en: normalized = (x-min(x))/(max(x)-min(x))
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '`normalized = (x - min(x)) / (max(x) - min(x))`'
- en: return normalized
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 返回`normalized`
- en: The preceding method of scaling is called min-max scaling, as it is based on
    scaling with respect to the minimum and maximum values of the array. Z-scaling
    and mean scaling are other well-known scaling methods.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 上述缩放方法被称为最小-最大缩放（min-max scaling），因为它是基于数组的最小值和最大值进行缩放的。Z缩放和均值缩放是其他著名的缩放方法。
- en: 'Thus, we have successfully solved a multiclass classification problem using
    the logistic regression model. Let''s now proceed toward an activity where, similar
    to Exercise 5.02: Logistic Regression as a Classifier – Binary Classifier, we
    will solve a binary classification problem. This time, however, we will use a
    simpler model – a linear regression classifier.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功地使用逻辑回归模型解决了一个多类分类问题。现在让我们进行一个类似于练习5.02：“逻辑回归作为分类器 – 二分类器”的活动，这次我们将解决一个二分类问题。不过，这次我们将使用一个更简单的模型——线性回归分类器。
- en: 'Activity 5.01: Ordinary Least Squares Classifier – Binary Classifier'
  id: totrans-393
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动5.01：普通最小二乘法分类器 – 二分类器
- en: In this activity, we will build a two-class OLS (linear regression)-based classifier
    using the MNIST dataset to classify between two digits, 0 and 1.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将构建一个基于OLS（线性回归）的二类分类器，使用MNIST数据集对数字0和1进行分类。
- en: 'The steps to be performed are as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的步骤如下：
- en: 'Import the required dependencies:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的依赖：
- en: import struct
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`struct`
- en: import numpy as np
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`numpy`为`np`
- en: import gzip
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`gzip`
- en: import urllib.request
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`urllib.request`
- en: import matplotlib.pyplot as plt
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 导入`matplotlib.pyplot`为`plt`
- en: from array import array
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 从数组库导入`array`
- en: from sklearn.linear_model import LinearRegression
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 从`sklearn.linear_model`导入`LinearRegression`
- en: Load the MNIST data into memory.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 将MNIST数据加载到内存中。
- en: Visualize a sample of the data.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据样本。
- en: Construct a linear classifier model to classify the digits 0 and 1\. The model
    we are going to create is to determine whether the samples are either the digits
    0 or 1\. To do this, we first need to select only those samples.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个线性分类器模型，用于分类数字0和1。我们将创建的模型是用来确定样本是否为数字0或1。为此，我们首先需要选择这些样本。
- en: Visualize the selected information with images of one sample of 0 and one sample
    of 1.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 使用0和1的一个样本图像可视化所选信息。
- en: In order to provide the image information to the model, we must first flatten
    the data out so that each image is 1 x 784 pixels in shape.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将图像信息提供给模型，我们必须首先将数据展平，使得每个图像的形状为1 x 784个像素。
- en: Let's construct the model; use the LinearRegression API and call the fit function.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型；使用`LinearRegression` API并调用`fit`函数。
- en: Determine the accuracy against the training set.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 确定训练集上的准确性。
- en: Determine the label predictions for each of the training samples, using a threshold
    of 0.5\. Values greater than 0.5 classify as 1; values less than, or equal to,
    0.5 classify as 0.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 使用0.5的阈值确定每个训练样本的标签预测值。大于0.5的值分类为1；小于或等于0.5的值分类为0。
- en: Compute the classification accuracy of the predicted training values versus
    the ground truth.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 计算预测训练值与实际值之间的分类准确率。
- en: Compare the performance against the test set.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与测试集进行性能比较。
- en: Note
  id: totrans-414
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以通过此链接找到。
- en: 'An interesting point to note here is that the test set performance here is
    worse than that in Exercise 5.02: Logistic Regression as a Classifier – Binary
    Classifier. The dataset is exactly the same in both cases, but the models are
    different. And, as expected, the linear regression classifier, being a simpler
    model, leads to poorer test set performance compared to a stronger, logistic regression
    model.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的点需要注意，那就是在这里的测试集表现比练习5.02：逻辑回归作为分类器——二元分类器的表现更差。两种情况的数据集完全相同，但模型不同。正如预期的那样，线性回归分类器作为一个更简单的模型，其测试集表现比更强大的逻辑回归模型要差。
- en: Select K Best Feature Selection
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择K最佳特征选择
- en: 'Now that we have established how to train and test the linear regression and
    logistic regression models on the MNIST dataset, we will now solve another multiclass
    classification problem on a different dataset using the logistic regression model.
    As a prerequisite for the next exercise, let''s quickly discuss a particular kind
    of feature selection method – select k best feature selection. In this method,
    we select features according to the k highest scores. The scores are derived based
    on a scoring function, which takes in the input feature (X) and target (y), and
    returns scores for each feature. An example of such a function could be a function
    that computes the ANOVA F-value between label (y) and feature (X). An implementation
    of this scoring function is available with scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif.
    The features are then sorted based on the decreasing order of scores, and we choose
    the top k features out of this ordered list. An implementation of the select k
    best feature selection method is available with scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html.
    Furthermore, the following is an example code to demonstrate how this method is
    used in scikit-learn:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了如何在MNIST数据集上训练和测试线性回归与逻辑回归模型，接下来我们将使用逻辑回归模型在另一个数据集上解决一个多类分类问题。作为下一个练习的前提条件，让我们快速讨论一种特定的特征选择方法——选择K最佳特征选择方法。在这种方法中，我们根据K个最高分数来选择特征。分数是基于评分函数得出的，该函数输入特征（X）和目标（y），并返回每个特征的分数。一个这样的函数的例子可能是计算标签（y）与特征（X）之间的ANOVA
    F值的函数。此评分函数的实现可以通过scikit-learn获得：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif。然后，特征会根据分数的降序排列，我们从这个排序列表中选择前K个特征。选择K最佳特征选择方法的实现也可以通过scikit-learn获得：https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html。此外，以下是一个示例代码，展示如何在scikit-learn中使用此方法：
- en: '>>> from sklearn.datasets import load_digits'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> 从sklearn.datasets导入load_digits'
- en: '>>> from sklearn.feature_selection import SelectKBest, chi2'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> 从sklearn.feature_selection导入SelectKBest和chi2'
- en: '>>> X, y = load_digits(return_X_y=True)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X, y = load_digits(return_X_y=True)'
- en: '>>> X.shape'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X.shape'
- en: (1797, 64)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: (1797, 64)
- en: '>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)'
- en: '>>> X_new.shape'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '>>> X_new.shape'
- en: (1797, 20)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: (1797, 20)
- en: And now we move on to our next exercise, where we solve a multiclass classification problem.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入下一个练习，解决一个多类分类问题。
- en: 'Exercise 5.04: Breast Cancer Diagnosis Classification Using Logistic Regression'
  id: totrans-428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.04：使用逻辑回归进行乳腺癌诊断分类
- en: In this exercise, we will be using the Breast Cancer Diagnosis dataset (available
    at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)
    or on GitHub at https://packt.live/3a7oAY8). This dataset is a part of the UCI
    Machine Learning Repository (https://archive.ics.uci.edu/ml/index.php). The dataset
    contains characteristics of the cell nuclei present in the digitized image of
    a Fine Needle Aspirate (FNA) of a breast mass, with the labels of malignant and
    benign for each cell nucleus. Characteristics are features (30 in total), such
    as the mean radius, radius error, worst radius, mean texture, texture error, and
    worst texture of the cell nuclei. In this exercise, we will use the features provided
    in the dataset to classify between malignant and benign cells.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用乳腺癌诊断数据集（可在 https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
    下载，也可以在 GitHub 上找到 https://packt.live/3a7oAY8）。该数据集是 UCI 机器学习库的一部分（https://archive.ics.uci.edu/ml/index.php）。数据集包含数字化的乳腺肿块细针穿刺（FNA）图像中细胞核的特征，每个细胞核都标有恶性或良性的标签。特征包括（共30个），例如细胞核的平均半径、半径误差、最差半径、平均纹理、纹理误差和最差纹理等。我们将使用数据集提供的这些特征来对恶性细胞和良性细胞进行分类。
- en: 'The steps to be performed are as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 需要执行的步骤如下：
- en: 'Import the required packages. For this exercise, we will require the pandas
    package to load the data, the Matplotlib package for plotting, and scikit-learn
    for creating the logistic regression model. Import all the required packages and
    relevant modules for these tasks:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包。对于这个练习，我们需要 pandas 包来加载数据，Matplotlib 包来绘图，以及 scikit-learn 来创建逻辑回归模型。导入所有必需的包和相关模块：
- en: import pandas as pd
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import matplotlib.pyplot as plt
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from sklearn.linear_model import LogisticRegression
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.linear_model import LogisticRegression
- en: from sklearn.feature_selection import SelectKBest
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.feature_selection import SelectKBest
- en: from sklearn.model_selection import train_test_split
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.model_selection import train_test_split
- en: 'Load the Breast Cancer Diagnosis dataset using pandas and examine the first
    five rows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 加载乳腺癌诊断数据集并检查前五行：
- en: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
- en: df.head()
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.22: Top five rows of the breast cancer dataset'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.22：乳腺癌数据集的前五行'
- en: '](img/image-5FNGGWXB.jpg)'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-5FNGGWXB.jpg)'
- en: 'Figure 5.22: Top five rows of the breast cancer dataset'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22：乳腺癌数据集的前五行
- en: 'Additionally, dissect the dataset into input (X) and output (y) variables:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将数据集分解为输入（X）和输出（y）变量：
- en: X, y = df[[c for c in df.columns if c != 'diagnosis']], df.diagnosis
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: X, y = df[[c for c in df.columns if c != 'diagnosis']], df.diagnosis
- en: 'The next step is feature engineering. We use scikit-learn''s select k best
    features sub-module under its feature selection module. Basically, this examines
    the power of each feature against the target output based on a scoring function.
    You can read about the details here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是特征工程。我们使用 scikit-learn 的 SelectKBest 特征选择模块。基本上，这个方法根据得分函数检查每个特征对目标输出的影响力。你可以在这里阅读更多细节：
    https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html：
- en: '"""'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: restricting to 2 best features so that
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 限制为两个最佳特征，以便
- en: we can visualize them on a plot
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图表中可视化它们
- en: '"""'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: skb_model = SelectKBest(k=2)
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: skb_model = SelectKBest(k=2)
- en: X_new = skb_model.fit_transform(X, y)
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: X_new = skb_model.fit_transform(X, y)
- en: get the k - best column names
  id: totrans-453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取 k 个最佳列名称
- en: 'mask = skb_model.get_support() #list of booleans'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 'mask = skb_model.get_support() # 布尔值列表'
- en: 'selected_features = [] # The list of your K best features'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 'selected_features = [] # 你的 K 个最佳特征列表'
- en: 'for bool, feature in zip(mask, df.columns):'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 'for bool, feature in zip(mask, df.columns):'
- en: 'if bool:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 'if bool:'
- en: selected_features.append(feature)
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: selected_features.append(feature)
- en: print(selected_features)
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: print(selected_features)
- en: 'The output will be as follows:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[''worst perimeter'', ''worst concave points'']'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '[''worst perimeter'', ''worst concave points'']'
- en: 'And now let''s visualize how these two most important features correlate with
    the target (diagnosis) and how well they separate the two classes of diagnosis:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化这两个最重要的特征与目标（诊断）的相关性，以及它们如何很好地区分这两类诊断：
- en: 'markers = {''benign'': {''marker'': ''o''}, \'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 'markers = {''benign'': {''marker'': ''o''}, \'
- en: '''malignant'': {''marker'': ''x''},}'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '''malignant'': {''marker'': ''x''},}'
- en: plt.figure(figsize=(10, 7))
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: 'for name, group in df.groupby(''diagnosis''):'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, group in df.groupby(''diagnosis''):'
- en: plt.scatter(group[selected_features[0]], \
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(group[selected_features[0]], \
- en: group[selected_features[1]], label=name, \
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: group[selected_features[1]], label=name, \
- en: marker=markers[name]['marker'],)
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: marker=markers[name]['marker'],)
- en: plt.title(f'Diagnosis Classification {selected_features[0]} vs \
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title(f'诊断分类 {selected_features[0]} 与 \
- en: '{selected_features[1]}'');'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '{selected_features[1]}'');'
- en: plt.xlabel(selected_features[0]);
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel(selected_features[0]);
- en: plt.ylabel(selected_features[1]);
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel(selected_features[1]);
- en: plt.legend();
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend();
- en: 'The output will be as follows:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.23: Scatterplot for feature selection'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.23：特征选择的散点图'
- en: '](img/image-E134S58Q.jpg)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-E134S58Q.jpg)'
- en: 'Figure 5.23: Scatterplot for feature selection'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23：特征选择的散点图
- en: 'Before we can construct the model, we must first convert the diagnosis values
    into labels that can be used within the model. Replace the benign diagnosis string
    with the value 0, and the malignant diagnosis string with the value 1:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型之前，我们必须先将诊断值转换为可以在模型中使用的标签。将良性诊断字符串替换为值 0，将恶性诊断字符串替换为值 1：
- en: diagnoses = ['benign', 'malignant',]
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: diagnoses = ['良性', '恶性',]
- en: output = [diagnoses.index(diag) for diag in df.diagnosis]
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: output = [diagnoses.index(diag) for diag in df.diagnosis]
- en: 'Also, in order to impartially evaluate the model, we should split the training
    dataset into a training and a validation set:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，为了公平地评估模型，我们应将训练数据集分为训练集和验证集：
- en: train_X, valid_X, \
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: train_X, valid_X, \
- en: train_y, valid_y = train_test_split(df[selected_features], output, \
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: train_y, valid_y = train_test_split(df[selected_features], output, \
- en: test_size=0.2, random_state=123)
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: test_size=0.2, random_state=123)
- en: 'Create the model using the selected_features and the assigned diagnosis labels:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 selected_features 和分配的诊断标签创建模型：
- en: model = LogisticRegression(solver='liblinear')
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: model = LogisticRegression(solver='liblinear')
- en: model.fit(df[selected_features], output)
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(df[selected_features], output)
- en: 'The output will be as follows:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
- en: intercept_scaling=1, l1_ratio=None, max_iter=100,
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: intercept_scaling=1, l1_ratio=None, max_iter=100,
- en: multi_class='warn', n_jobs=None, penalty='l2',
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: multi_class='warn', n_jobs=None, penalty='l2',
- en: random_state=None, solver='liblinear', tol=0.0001,
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: random_state=None, solver='liblinear', tol=0.0001,
- en: verbose=0,
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: verbose=0,
- en: warm_start=False)
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: warm_start=False)
- en: 'Compute the accuracy of the model against the validation set:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型在验证集上的准确性：
- en: model.score(valid_X, valid_y)
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(valid_X, valid_y)
- en: 'The output will be as follows:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '0.9385964912280702'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9385964912280702'
- en: 'Construct another model using a random choice of selected_features and compare
    performance:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机选择的 selected_features 构建另一个模型并比较性能：
- en: 'selected_features = [''mean radius'', # List features here \'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 'selected_features = [''mean radius'', # 在此列出特征 \'
- en: '''mean texture'', ''compactness error'']'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '''mean texture'', ''compactness error'']'
- en: train_X, valid_X, \
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: train_X, valid_X, \
- en: train_y, valid_y = train_test_split(df[selected_features], output, \
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: train_y, valid_y = train_test_split(df[selected_features], output, \
- en: test_size=0.2, random_state=123)
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: test_size=0.2, random_state=123)
- en: model = LogisticRegression(solver='liblinear')
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: model = LogisticRegression(solver='liblinear')
- en: model.fit(train_X, train_y)
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(train_X, train_y)
- en: model.score(valid_X, valid_y)
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(valid_X, valid_y)
- en: 'The output will be as follows:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '0.8859649122807017'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '0.8859649122807017'
- en: This reduced accuracy shows that indeed, using the two most important features
    renders a more powerful model than using three randomly chosen features.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这个降低的准确性表明，确实，使用两个最重要的特征比使用三个随机选择的特征能更强大。
- en: 'Construct another model using all the available information and compare performance:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有可用信息构建另一个模型并比较性能：
- en: selected_features = [feat for feat in df.columns \
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: selected_features = [feat for feat in df.columns \
- en: 'if feat != ''diagnosis'' # List features here'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 feat != ''diagnosis'' # 在此列出特征'
- en: ']'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ']'
- en: train_X, valid_X, \
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: train_X, valid_X, \
- en: train_y, valid_y = train_test_split(df[selected_features], output, \
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: train_y, valid_y = train_test_split(df[selected_features], output, \
- en: test_size=0.2, random_state=123)
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: test_size=0.2, random_state=123)
- en: model = LogisticRegression(solver='liblinear')
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: model = LogisticRegression(solver='liblinear')
- en: model.fit(train_X, train_y)
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(train_X, train_y)
- en: model.score(valid_X, valid_y)
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(valid_X, valid_y)
- en: 'The output will be as follows:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '0.9824561403508771'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9824561403508771'
- en: Note
  id: totrans-524
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2YWxjIN.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考 https://packt.live/2YWxjIN。
- en: You can also run this example online at https://packt.live/2Bx8NWt. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是 https://packt.live/2Bx8NWt。你必须执行整个 Notebook 才能得到预期的结果。
- en: This improvement in performance by using all the features shows that even those
    features that are not among the most important ones do still play a role in improving
    model performance.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用所有特征来提高性能表明，即使是那些并非最重要的特征，也仍然在提高模型性能方面发挥了作用。
- en: Classification Using K-Nearest Neighbors
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 K-近邻分类
- en: 'Now that we are comfortable with creating multiclass classifiers using logistic
    regression and are getting reasonable performance with these models, we will turn
    our attention to another type of classifier: the K-nearest neighbors (KNN) classifier.
    KNN is a non-probabilistic, non-linear classifier. It does not predict the probability
    of a class. Also, as it does not learn any parameters, there is no linear combination
    of parameters and, thus, it is a non-linear model:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了使用逻辑回归创建多类分类器，并且这些模型的性能也不错，我们将把注意力转向另一种类型的分类器：K-近邻（KNN）分类器。KNN 是一种非概率的非线性分类器。它不会预测类别的概率。而且，由于它不会学习任何参数，因此没有参数的线性组合，因而它是一个非线性模型：
- en: '![Figure 5.24: Visual representation of KNN'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.24：KNN 的可视化表示'
- en: '](img/image-8IYKILZY.jpg)'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-8IYKILZY.jpg)'
- en: 'Figure 5.24: Visual representation of KNN'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：KNN 的可视化表示
- en: Figure 5.24 represents the workings of a KNN classifier. The two different symbols,
    X and O, represent data points belonging to two different classes. The solid circle
    at the center is the test point requiring classification, the inner dotted circle
    shows the classification process where k=3, while the outer dotted circle shows
    the classification process where k=5\. What we mean here is that, if k=3, we only
    look at the three data points nearest to the test point, which gives us the impression
    of that dotted circle encompassing those three nearest data points.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24 展示了 KNN 分类器的工作原理。两种不同的符号，X 和 O，代表属于两个不同类别的数据点。中心的实心圆是需要分类的测试点，内层虚线圆显示了
    k=3 时的分类过程，而外层虚线圆显示了 k=5 时的分类过程。这里的意思是，如果 k=3，我们只看离测试点最近的三个数据点，这给人一种虚线圆圈包含这三个最近数据点的感觉。
- en: KNN is one of the simplest "learning" algorithms available for data classification.
    The use of learning in quotation marks is explicit, as KNN doesn't really learn
    from the data and encode these learnings in parameters or weights like other methods,
    such as logistic regression. KNN uses instance-based or lazy learning in that
    it simply stores or memorizes all the training samples and the corresponding classes.
    It derives its name, k-nearest neighbors, from the fact that, when a test sample
    is provided to the algorithm for class prediction, it uses a majority vote of
    the k-nearest points to determine the corresponding class. If we look at Figure
    5.24 and if we assume k=3, the nearest three points lie within the inner dotted
    circle, and, in this case, the classification would be a hollow circle (O).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是最简单的数据分类“学习”算法之一。这里使用引号中的“学习”是有明确指示的，因为 KNN 并不像其他方法（如逻辑回归）那样从数据中学习并通过参数或权重对这些学习进行编码。KNN
    使用基于实例的学习或懒学习，因为它仅仅存储或记住所有的训练样本及其相应的类别。它的名字“K-近邻”来源于这样一个事实：当测试样本提供给算法进行分类预测时，它通过对
    k 个最近数据点的多数投票来决定相应的类别。如果我们看图 5.24，并假设 k=3，最近的三个数据点位于内层虚线圆内，在这种情况下，分类结果将是一个空心圆（O）。
- en: If, however, we were to take k=5, the nearest five points lie within the outer
    dotted circle and the classification would be a cross (X) (three crosses to two
    hollow circles). So, how do we select k? Academically, we should plot the KNN
    model performance (error) as a function of k. Look for an elbow in this plot,
    and the moment when an increase in k does not change the error significantly;
    this means that we have found an optimal value for k. More practically, the choice
    of k depends on the data, with larger values of k reducing the effect of noise
    on the classification, but thereby making boundaries between classes less distinct.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们取 k=5，则最近的五个点位于外部虚线圆内，分类结果为叉号（X）（三个叉号与两个空心圆）。那么，我们如何选择 k 呢？从学术角度来看，我们应该将
    KNN 模型的性能（误差）绘制成 k 的函数。查看图中是否有肘部（elbow）点，并观察当 k 增加时，误差是否不再显著变化；这意味着我们已经找到了 k 的最优值。更实际地说，k
    的选择取决于数据，较大的 k 值能够减少噪声对分类的影响，但也会使类别之间的边界变得不那么明显。
- en: 'The preceding figure highlights a few characteristics of KNN classification
    that should be considered:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 上图突出了 KNN 分类中应考虑的一些特性：
- en: 'As mentioned previously, the selection of k is quite important. In this simple
    example, switching k from three to five flipped the class prediction due to the
    proximity of both classes. As the final classification is taken by a majority
    vote, it is often useful to use odd numbers of k to ensure that there is a winner
    in the voting process. If an even value of k is selected, and a tie in the vote
    occurs, then there are a number of different methods available for breaking the
    tie, including:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，k的选择非常重要。在这个简单的例子中，由于两类样本的接近，k从3切换到5使得分类预测发生了变化。由于最终分类是通过多数投票确定的，因此使用奇数值的k通常是有益的，以确保投票过程中有赢家。如果选择偶数值的k，并且发生投票平局，那么可以使用多种方法来打破平局，包括：
- en: Reducing k by one until the tie is broken
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 将k减1，直到打破平局
- en: Selecting the class on the basis of the smallest Euclidean distance to the nearest point
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 基于最小欧几里得距离选择最近点的类别
- en: Applying a weighting function to bias the test point toward those neighbors
    that are closer
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 应用加权函数使测试点更倾向于靠近的邻居
- en: KNN models have the ability to form extremely complex non-linear boundaries,
    which can be advantageous in classifying images or datasets with highly non-linear
    boundaries. Considering that, in Figure 5.24, the test point changes from a hollow
    circle classification to a cross with an increase in k, we can see here that a
    complex boundary could be formed.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: KNN模型能够形成极其复杂的非线性边界，这在对具有高度非线性边界的图像或数据集进行分类时具有优势。考虑到在图5.24中，随着k的增加，测试点从空心圆分类变为交叉点，我们可以看到这里可能会形成一个复杂的边界。
- en: KNN models can be highly sensitive to local features in the data, given that
    the classification process is only really dependent on the nearby points.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类过程实际上只依赖于附近的点，KNN模型对数据中的局部特征非常敏感。
- en: As KNN models memorize all the training information to make predictions, they
    can struggle with generalizing to new, unseen data.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KNN模型会记住所有训练信息以进行预测，因此它们在泛化到新的、未见过的数据时可能会遇到困难。
- en: There is another variant of KNN, which, rather than specifying the number of
    nearest neighbors, specifies the size of the radius around the test point at which
    to look. This method, known as the radius neighbors classification, will not be
    considered in this chapter, but, in understanding KNN, you will also develop an
    understanding of the radius neighbors classification and how to use the model
    through scikit-learn.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: KNN还有一个变种，它不是指定最近邻的数量，而是指定围绕测试点的半径大小来查找邻居。这种方法称为半径邻居分类，本章不予讨论，但在理解KNN的同时，你也将理解半径邻居分类，并学习如何通过scikit-learn使用该模型。
- en: Note
  id: totrans-545
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Our explanation of KNN classification and the next exercise examines modeling
    data with two features or two dimensions, as it enables simpler visualization
    and a greater understanding of the KNN modeling process. And then we will classify
    a dataset with a greater number of dimensions in Activity 5.02: KNN Multiclass
    Classifier, wherein we''ll classify MNIST using KNN. Remember, just because there
    are too many dimensions to plot, this doesn''t mean it cannot be classified with
    N dimensions.'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对KNN分类的解释以及接下来的练习将研究具有两个特征或两个维度的数据建模，因为这可以简化可视化并更好地理解KNN建模过程。接下来，我们将在活动5.02：KNN多类分类器中对具有更多维度的数据集进行分类，在那里我们将使用KNN对MNIST数据集进行分类。记住，尽管有太多维度以至于无法绘制图像，但这并不意味着它不能用N维度进行分类。
- en: To allow visualization of the KNN process, we will turn our attention in the
    following exercise to the Breast Cancer Diagnosis dataset. This dataset is provided
    as part of the accompanying code files for this book.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于可视化KNN过程，我们将在接下来的练习中关注乳腺癌诊断数据集。该数据集作为本书附带的代码文件的一部分提供。
- en: 'Exercise 5.05: KNN Classification'
  id: totrans-548
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习5.05：KNN分类
- en: 'In this exercise, we will be using the KNN classification algorithm to build
    a model on the Breast Cancer Diagnosis dataset and evaluate its performance by
    calculating its accuracy:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用KNN分类算法，在乳腺癌诊断数据集上构建模型，并通过计算其准确率来评估模型的表现：
- en: 'For this exercise, we need to import pandas, Matplotlib, and the KNeighborsClassifier
    and train_test_split sub-modules of scikit-learn. We will use the shorthand notation
    KNN for quick access:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习中，我们需要导入pandas、Matplotlib以及scikit-learn中的KNeighborsClassifier和train_test_split子模块。我们将使用简写KNN以便快速访问：
- en: import pandas as pd
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import matplotlib.pyplot as plt
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from sklearn.neighbors import KNeighborsClassifier as KNN
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier as KNN
- en: from sklearn.model_selection import train_test_split
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.model_selection import train_test_split
- en: 'Load the Breast Cancer Diagnosis dataset and examine the first five rows:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 加载乳腺癌诊断数据集并查看前五行：
- en: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
- en: df.head()
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.25: First five rows'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.25：前五行'
- en: '](img/image-DU1QBBKL.jpg)'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-DU1QBBKL.jpg)'
- en: 'Figure 5.25: First five rows'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25：前五行
- en: 'At this stage, we need to choose the most appropriate features from the dataset
    for use with the classifier. We could simply select all 30 features. However,
    as this exercise is designed to allow visualization of the KNN process, we will
    arbitrarily only select the mean radius and worst radius. Construct a scatterplot
    for mean radius versus worst radius for each of the classes in the dataset with
    the corresponding diagnosis type:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，我们需要从数据集中选择最合适的特征用于分类器。我们可以简单地选择所有 30 个特征。然而，由于这个练习是为了展示 KNN 过程，我们将任意选择平均半径和最差半径。构建一个散点图，展示每个类别的平均半径与最差半径的关系，并标明对应的诊断类型：
- en: 'markers = {''benign'': {''marker'': ''o'', ''facecolor'': ''g'', \'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 'markers = {''良性'': {''marker'': ''o'', ''facecolor'': ''g'', \'
- en: '''edgecolor'': ''g''}, \'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '''edgecolor'': ''g''}, \'
- en: '''malignant'': {''marker'': ''x'', ''facecolor'': ''r'', \'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '''恶性'': {''marker'': ''x'', ''facecolor'': ''r'', \'
- en: '''edgecolor'': ''r''},}'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '''edgecolor'': ''r''},}'
- en: plt.figure(figsize=(10, 7))
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: 'for name, group in df.groupby(''diagnosis''):'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, group in df.groupby(''diagnosis''):'
- en: plt.scatter(group['mean radius'], group['worst radius'], \
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(group['mean radius'], group['worst radius'], \
- en: label=name, marker=markers[name]['marker'], \
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: label=name, marker=markers[name]['marker'], \
- en: facecolors=markers[name]['facecolor'], \
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: facecolors=markers[name]['facecolor'], \
- en: edgecolor=markers[name]['edgecolor'])
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: edgecolor=markers[name]['edgecolor'])
- en: plt.title('Breast Cancer Diagnosis Classification Mean Radius '\
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title('乳腺癌诊断分类 平均半径 '\
- en: '''vs Worst Radius'');'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '''与最差半径'');'
- en: plt.xlabel('Mean Radius');
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('平均半径');
- en: plt.ylabel('Worst Radius');
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('最差半径');
- en: plt.legend();
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend();
- en: 'The output will be as follows:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.26: Scatterplot of cancer data'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.26：癌症数据的散点图'
- en: '](img/image-7W4CN645.jpg)'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-7W4CN645.jpg)'
- en: 'Figure 5.26: Scatterplot of cancer data'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26：癌症数据的散点图
- en: 'Before actually going into training a model, let''s split the training dataset
    further into a training and a validation set in the ratio 80:20 to be able to
    impartially evaluate the model performance later using the validation set:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际开始训练模型之前，我们先将训练数据集按 80:20 的比例拆分为训练集和验证集，以便稍后使用验证集公正地评估模型性能：
- en: train_X, valid_X, \
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: train_X, valid_X, \
- en: train_y, valid_y = train_test_split(df[['mean radius', \
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: train_y, valid_y = train_test_split(df[['mean radius', \
- en: '''worst radius'']], \'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '''最差半径'']], \'
- en: df.diagnosis, test_size=0.2, \
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: df.diagnosis, test_size=0.2, \
- en: random_state=123)
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: random_state=123)
- en: 'Construct a KNN classifier model with k = 3 and fit it to the training data:'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 KNN 分类器模型，k = 3，并将其拟合到训练数据上：
- en: model = KNN(n_neighbors=3)
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: model = KNN(n_neighbors=3)
- en: model.fit(X=train_X, y=train_y)
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(X=train_X, y=train_y)
- en: 'The output will be as follows:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.27: K Neighbor classifier'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.27：K 邻近分类器'
- en: '](img/image-W5ZP4LBR.jpg)'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-W5ZP4LBR.jpg)'
- en: 'Figure 5.27: K Neighbor classifier'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27：K 邻近分类器
- en: 'Check the performance of the model against the validation set:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型在验证集上的表现：
- en: model.score(X=valid_X, y=valid_y)
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=valid_X, y=valid_y)
- en: 'The output will show the performance score:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将显示性能得分：
- en: '0.9385964912280702'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9385964912280702'
- en: As we can see, the accuracy is over 93% on the validation set. Next, by means
    of an exercise, we will try to understand what decision boundaries are formed
    by the KNN model during the training process. We will draw the boundaries in the
    exercise.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，验证集上的准确率超过了 93%。接下来，通过一个练习，我们将尝试理解 KNN 模型在训练过程中形成的决策边界。我们将在练习中绘制这些边界。
- en: Note
  id: totrans-600
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3dovRUH.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此特定部分的源代码，请参见 https://packt.live/3dovRUH。
- en: You can also run this example online at https://packt.live/2V5hYEP. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/2V5hYEP 上在线运行此示例。你必须执行整个 Notebook 才能获得期望的结果。
- en: 'Exercise 5.06: Visualizing KNN Boundaries'
  id: totrans-603
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.06：可视化 KNN 边界
- en: 'To visualize the decision boundaries produced by the KNN classifier, we need
    to sweep over the prediction space, that is, the minimum and maximum values for
    the mean radius and worst radius, and determine the classifications made by the
    model at those points. Once we have this sweep, we can then plot the classification
    decisions made by the model:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化KNN分类器生成的决策边界，我们需要遍历预测空间，即“平均半径”和“最差半径”的最小值和最大值，并确定模型在这些点上的分类结果。一旦完成遍历，我们就可以绘制模型所做的分类决策：
- en: 'Import all the relevant packages. We will also need NumPy for this exercise:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所有相关的包。我们还需要使用NumPy来完成此练习：
- en: import numpy as np
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pandas as pd
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import matplotlib.pyplot as plt
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from matplotlib.colors import ListedColormap
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: from matplotlib.colors import ListedColormap
- en: from sklearn.neighbors import KNeighborsClassifier as KNN
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neighbors import KNeighborsClassifier as KNN
- en: 'Load the dataset into a pandas DataFrame:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集加载到pandas DataFrame中：
- en: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
- en: df.head()
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.28: First five rows'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.28：前五行数据](img/image-WKLMXVYM.jpg)'
- en: '](img/image-A9HFJUOT.jpg)'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-A9HFJUOT.jpg)'
- en: 'Figure 5.28: First five rows'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.28：前五行数据
- en: 'While we could use the diagnosis strings to create the model in the previous
    exercise, in plotting the decision boundaries, it would be more useful to map
    the diagnosis to separate integer values. To do this, create a list of the labels
    for later reference and iterate through this list, replacing the existing label
    with the corresponding index in the list:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用诊断字符串来创建之前练习中的模型，但在绘制决策边界时，将诊断映射为不同的整数值会更加有用。为此，首先创建一个标签列表以供后续参考，并通过遍历该列表，将现有标签替换为列表中对应的索引：
- en: labelled_diagnoses = ['benign', 'malignant',]
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: labelled_diagnoses = ['良性', '恶性',]
- en: 'for idx, label in enumerate(labelled_diagnoses):'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 'for idx, label in enumerate(labelled_diagnoses):'
- en: df.diagnosis = df.diagnosis.replace(label, idx)
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: df.diagnosis = df.diagnosis.replace(label, idx)
- en: df.head()
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.29: First five rows'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.29：前五行数据](img/image-MBA2EF5A.jpg)'
- en: '](img/image-MBA2EF5A.jpg)'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-MBA2EF5A.jpg)'
- en: 'Figure 5.29: First five rows'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29：前五行数据
- en: Notice the use of the enumerate function in the for loop definition. When iterating
    through the for loop, the enumerate function provides the index of the value in
    the list as well as the value itself through each iteration. We assign the index
    of the value to the idx variable and the value to label. Using enumerate in this
    way provides an easy way to replace the species strings with a unique integer
    label.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意在for循环定义中使用了enumerate函数。在遍历for循环时，enumerate函数提供了列表中值的索引以及每次迭代中的值。我们将值的索引赋给idx变量，将值赋给label。以这种方式使用enumerate提供了一种简单的方法来将物种字符串替换为唯一的整数标签。
- en: 'Construct a KNN classification model, again using three nearest neighbors and
    fit to the mean radius and worst radius with the newly labeled diagnosis data:'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个KNN分类模型，同样使用三个最近邻，并拟合“平均半径”和“最差半径”以及新标签化的诊断数据：
- en: model = KNN(n_neighbors=3)
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: model = KNN(n_neighbors=3)
- en: model.fit(X=df[['mean radius', 'worst radius']], y=df.diagnosis)
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(X=df[['mean radius', 'worst radius']], y=df.diagnosis)
- en: 'The output will be as follows:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.30: K-neighbors classifier'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.30：K-邻居分类器](img/image-A9HFJUOT.jpg)'
- en: '](img/image-WKLMXVYM.jpg)'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-WKLMXVYM.jpg)'
- en: 'Figure 5.30: K-neighbors classifier'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.30：K-邻居分类器
- en: 'To visualize our decision boundaries, we need to create a mesh or range of
    predictions across the information space, that is, all possible combinations of
    values of mean radius and worst radius. Starting with 1 unit less than the minimum
    for both the mean radius and worst radius, and finishing at 1 unit more than the
    maximum for mean radius and worst radius, use the arange function of NumPy to
    create a range of values between these limits in increments of 0.1 (spacing):'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化我们的决策边界，我们需要在信息空间内创建一个网格或预测范围，即所有可能的“平均半径”和“最差半径”的值的组合。从“平均半径”和“最差半径”的最小值减去1单位开始，最终在“平均半径”和“最差半径”的最大值上加1单位，使用NumPy的arange函数在这些范围内按0.1（间隔）创建一系列值：
- en: spacing = 0.1
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: spacing = 0.1
- en: mean_radius_range = np.arange(df['mean radius'].min() - 1, \
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: mean_radius_range = np.arange(df['mean radius'].min() - 1, \
- en: df['mean radius'].max() + 1, spacing)
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: df['mean radius'].max() + 1, spacing)
- en: worst_radius_range = np.arange(df['worst radius'].min() - 1, \
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: worst_radius_range = np.arange(df['worst radius'].min() - 1, \
- en: df['worst radius'].max() + 1, spacing)
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: df['worst radius'].max() + 1, spacing)
- en: 'Use the NumPy meshgrid function to combine the two ranges in a grid:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NumPy 的 meshgrid 函数将两个范围合并成网格：
- en: Create the mesh
  id: totrans-642
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网格
- en: xx, yy = np.meshgrid(mean_radius_range, worst_radius_range)
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: xx, yy = np.meshgrid(mean_radius_range, worst_radius_range)
- en: 'Check out xx:'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 xx：
- en: xx
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: xx
- en: 'The output will be as follows:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.31: Array of meshgrid xx values'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.31: 网格 xx 值数组'
- en: '](img/image-VYES63YM.jpg)'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-VYES63YM.jpg)'
- en: 'Figure 5.31: Array of meshgrid xx values'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.31: 网格 xx 值数组'
- en: 'Check out yy:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 yy：
- en: yy
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: yy
- en: 'The output will be as follows:'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.32: Array of meshgrid yy values'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.32: 网格 yy 值数组'
- en: '](img/image-0N35YYON.jpg)'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-0N35YYON.jpg)'
- en: 'Figure 5.32: Array of meshgrid yy values'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.32: 网格 yy 值数组'
- en: 'Concatenate the mesh into a single NumPy array using np.c_:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 np.c_ 将网格拼接成一个 NumPy 数组：
- en: 'pred_x = np.c_[xx.ravel(), yy.ravel()] # Concatenate the results'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 'pred_x = np.c_[xx.ravel(), yy.ravel()] # 拼接结果'
- en: pred_x
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: pred_x
- en: 'The output will be as follows:'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.33: Array of predicted values'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.33: 预测值数组'
- en: '](img/image-5ZTF8FOS.jpg)'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-5ZTF8FOS.jpg)'
- en: 'Figure 5.33: Array of predicted values'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.33: 预测值数组'
- en: While this function call looks a little mysterious, it simply concatenates the
    two separate arrays together (refer to https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html)
    and is shorthand for concatenate.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个函数调用看起来有点神秘，但它其实只是将两个独立的数组拼接在一起（参考 https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html），是拼接的简写形式。
- en: 'Produce the class predictions for the mesh:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网格的类别预测：
- en: pred_y = model.predict(pred_x).reshape(xx.shape)
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: pred_y = model.predict(pred_x).reshape(xx.shape)
- en: pred_y
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: pred_y
- en: 'The output will be as follows:'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.34: Array of predicted y values'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.34: 预测的 y 值数组'
- en: '](img/image-N9PE4LC8.jpg)'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-N9PE4LC8.jpg)'
- en: 'Figure 5.34: Array of predicted y values'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.34: 预测的 y 值数组'
- en: 'To consistently visualize the boundaries, we will need two sets of consistent
    colors; a lighter set of colors for the decision boundaries, and a darker set
    of colors for the points of the training set themselves. Create two color maps
    using ListedColormaps:'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 为了一致地可视化边界，我们需要两组一致的颜色；一组较浅的颜色用于决策边界，另一组较深的颜色用于训练集的点。使用 ListedColormaps 创建两个色图：
- en: Create color maps
  id: totrans-672
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建色图
- en: cmap_light = ListedColormap(['#6FF6A5', '#F6A56F',])
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: cmap_light = ListedColormap(['#6FF6A5', '#F6A56F',])
- en: cmap_bold = ListedColormap(['#0EE664', '#E6640E',])
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: cmap_bold = ListedColormap(['#0EE664', '#E6640E',])
- en: 'To highlight the decision boundaries, first plot the training data according
    to the diagnosis types, using the cmap_bold color scheme and different markers
    for each of the different diagnosis types:'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出决策边界，首先根据诊断类型绘制训练数据，使用 cmap_bold 色图，并为每种不同的诊断类型使用不同的标记：
- en: 'markers = {''benign'': {''marker'': ''o'', ''facecolor'': ''g'', \'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 'markers = {''良性'': {''marker'': ''o'', ''facecolor'': ''g'', \'
- en: '''edgecolor'': ''g''}, \'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '''edgecolor'': ''g''}, \'
- en: '''malignant'': {''marker'': ''x'', ''facecolor'': ''r'', \'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '''恶性'': {''marker'': ''x'', ''facecolor'': ''r'', \'
- en: '''edgecolor'': ''r''},}'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '''edgecolor'': ''r''},}'
- en: plt.figure(figsize=(10, 7))
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: 'for name, group in df.groupby(''diagnosis''):'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, group in df.groupby(''diagnosis''):'
- en: diagnoses = labelled_diagnoses[name]
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: diagnoses = labelled_diagnoses[name]
- en: plt.scatter(group['mean radius'], group['worst radius'], \
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(group['mean radius'], group['worst radius'], \
- en: c=cmap_bold.colors[name], \
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: c=cmap_bold.colors[name], \
- en: label=labelled_diagnoses[name], \
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: label=labelled_diagnoses[name], \
- en: marker=markers[diagnoses]['marker'])
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: marker=markers[diagnoses]['marker'])
- en: plt.title('Breast Cancer Diagnosis Classification Mean Radius '\
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title('乳腺癌诊断分类平均半径 '\
- en: '''vs Worst Radius'');'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '''vs 最差半径'');'
- en: plt.xlabel('Mean Radius');
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('平均半径');
- en: plt.ylabel('Worst Radius');
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('最差半径');
- en: plt.legend();
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend();
- en: 'The output will be as follows:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.35: Scatterplot with highlighted decision boundaries'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.35: 突出显示决策边界的散点图'
- en: '](img/image-TD3KES3D.jpg)'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-TD3KES3D.jpg)'
- en: 'Figure 5.35: Scatterplot with highlighted decision boundaries'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.35: 突出显示决策边界的散点图'
- en: 'Using the prediction mesh made previously, plot the decision boundaries in
    addition to the training data:'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先前制作的预测网格，绘制决策边界以及训练数据：
- en: plt.figure(figsize=(10, 7))
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10, 7))
- en: plt.pcolormesh(xx, yy, pred_y, cmap=cmap_light);
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: plt.pcolormesh(xx, yy, pred_y, cmap=cmap_light);
- en: plt.scatter(df['mean radius'], df['worst radius'], c=df.diagnosis, cmap=cmap_bold,
    edgecolor='k', s=20);
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: plt.scatter(df['mean radius'], df['worst radius'], c=df.diagnosis, cmap=cmap_bold,
    edgecolor='k', s=20);
- en: plt.title('Breast Cancer Diagnosis Decision Boundaries Mean Radius '\
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title('乳腺癌诊断决策边界平均半径 '\
- en: '''vs Worst Radius'');'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '''vs 最差半径'');'
- en: plt.xlabel('Mean Radius');
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('平均半径');
- en: plt.ylabel('Worst Radius');
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('最差半径');
- en: plt.text(15, 12, 'Benign', ha='center',va='center', \
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: plt.text(15, 12, '良性', ha='center',va='center', \
- en: size=20,color='k');
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: size=20,color='k');
- en: plt.text(15, 30, 'Malignant', ha='center',va='center', \
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: plt.text(15, 30, '恶性', ha='center',va='center', \
- en: size=20,color='k');
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: size=20,color='k');
- en: 'The output will be as follows:'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.36: The decision boundary'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.36：决策边界'
- en: '](img/image-1N1P8LQH.jpg)'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-1N1P8LQH.jpg)'
- en: 'Figure 5.36: The decision boundary'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.36：决策边界
- en: Note
  id: totrans-712
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3dpxPnY.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 https://packt.live/3dpxPnY。
- en: You can also run this example online at https://packt.live/3drmBPE. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/3drmBPE 在线运行此示例。你必须执行整个 Notebook 才能得到期望的结果。
- en: We have thus both trained a KNN classifier and also understood how the knn decision
    boundaries are formed. Next, we will train a KNN multiclass classifier for a different
    dataset and evaluate its performance.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们既训练了一个 KNN 分类器，又理解了 knn 决策边界是如何形成的。接下来，我们将训练一个 KNN 多类分类器，应用于不同的数据集，并评估其性能。
- en: 'Activity 5.02: KNN Multiclass Classifier'
  id: totrans-716
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 5.02：KNN 多类分类器
- en: In this activity, we will use the KNN model to classify the MNIST dataset into
    10 different digit-based classes.
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将使用 KNN 模型将 MNIST 数据集分类为 10 个不同的数字类。
- en: 'The steps to be performed are as follows:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的步骤如下：
- en: 'Import the following packages:'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 导入以下包：
- en: import struct
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `struct`
- en: import numpy as np
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `numpy` 为 `np`
- en: import gzip
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `gzip`
- en: import urllib.request
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `urllib.request`
- en: import matplotlib.pyplot as plt
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `matplotlib.pyplot` 为 `plt`
- en: from array import array
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `array` 导入 `array`
- en: from sklearn.neighbors import KNeighborsClassifier as KNN
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `sklearn.neighbors` 导入 `KNeighborsClassifier` 为 `KNN`
- en: Load the MNIST data into memory; first the training images, then the training
    labels, then the test images, and, finally, the test labels.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 将 MNIST 数据加载到内存中；首先是训练图像，然后是训练标签，然后是测试图像，最后是测试标签。
- en: Visualize a sample of the data.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据的一个样本。
- en: Construct a KNN classifier, with three nearest neighbors to classify the MNIST
    dataset. Again, to save processing power, randomly sample 5,000 images for use
    in training.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 KNN 分类器，使用三个最近邻来分类 MNIST 数据集。同样，为了节省处理能力，随机采样 5,000 张图像用于训练。
- en: In order to provide the image information to the model, we must first flatten
    the data out such that each image is 1 x 784 pixels in shape.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将图像信息提供给模型，我们必须首先将数据展平，使得每个图像的形状为 1 x 784 像素。
- en: Build the KNN model with k=3 and fit the data to the model. Note that, in this
    activity, we are providing 784 features or dimensions to the model, not just 2.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 k=3 构建 KNN 模型，并将数据拟合到模型中。请注意，在本次活动中，我们为模型提供了 784 个特征或维度，而不仅仅是 2 个。
- en: Determine the score against the training set.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 确定在训练集上的得分。
- en: Display the first two predictions for the model against the training data.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 显示模型在训练数据上的前两个预测。
- en: Compare the performance against the test set.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 将性能与测试集进行比较。
- en: 'The output will be as follows:'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '0.9376'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9376'
- en: Note
  id: totrans-737
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以通过此链接找到。
- en: If we compare the preceding test set performance with that in Exercise 5.03,
    Logistic Regression – Multiclass Classifier, we see that for the exact same dataset,
    the knn model outperforms the logistic regression classifier regarding this task.
    This doesn't necessarily mean that knn always outperforms logistic regression,
    but it does so for this task, for this dataset.
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将前面的测试集性能与第 5.03 题中的逻辑回归——多类分类器的表现进行比较，我们会发现，在相同的数据集上，knn 模型在此任务中优于逻辑回归分类器。这并不意味着
    knn 总是优于逻辑回归，但在这个任务和这个数据集上，它确实表现得更好。
- en: Classification Using Decision Trees
  id: totrans-740
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用决策树进行分类
- en: Another powerful classification method that we will be examining in this chapter
    is decision trees, which have found particular use in applications such as natural
    language processing, for example. There are a number of different machine learning
    algorithms that fall within the overall umbrella of decision trees, such as Iterative
    Dichotomiser 3 (ID3) and Classification and Regression Tree (CART). In this chapter,
    we will investigate the use of the ID3 method in classifying categorical data,
    and we will use the scikit-learn CART implementation as another method of classifying
    the dataset. So, what exactly are decision trees?
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究另一种强大的分类方法——决策树。决策树在许多应用中都有广泛的使用，例如自然语言处理等。本章将探讨ID3方法在分类类别数据中的应用，同时使用scikit-learn中的CART实现作为另一种分类数据集的方法。那么，决策树到底是什么呢？
- en: 'As the name suggests, decision trees are a learning algorithm that apply a
    sequential series of decisions based on input information to make the final classification.
    Recalling your childhood biology class, you may have used a process similar to
    decision trees in the classification of different types of animals via dichotomous
    keys. Just like the dichotomous key example shown, decision trees aim to classify
    information following the result of a number of decision or question steps:'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，决策树是一种学习算法，基于输入信息按顺序做出一系列决策，以做出最终分类。回想你小时候的生物课，你可能通过二分法分类工具（dichotomous
    keys）来分类不同类型的动物，类似于决策树的过程。就像上面展示的二分法工具的例子一样，决策树旨在根据一系列决策或问题步骤的结果对信息进行分类：
- en: '![Figure 5.37: Animal classification using the dichotomous key'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.37：使用二分法分类动物'
- en: '](img/image-VAU88P55.jpg)'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-VAU88P55.jpg)'
- en: 'Figure 5.37: Animal classification using the dichotomous key'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.37：使用二分法分类动物
- en: 'Depending upon the decision tree algorithm being used, the implementation of
    the decision steps may vary slightly, but we will be considering the implementation
    of the ID3 algorithm specifically. The ID3 algorithm aims to classify the data
    on the basis of each decision providing the largest information gain. To further
    understand this design, we also need to understand two additional concepts: entropy
    and information gain.'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的决策树算法不同，决策步骤的实现可能略有差异，但我们将专门讨论ID3算法的实现。ID3算法的目的是根据每个决策提供的最大信息增益来对数据进行分类。为了更好地理解这一设计，我们还需要理解两个额外的概念：熵和信息增益。
- en: Note
  id: totrans-747
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The ID3 algorithm was first proposed by the Australian researcher Ross Quinlan
    in 1985 (https://doi.org/10.1007/BF00116251).
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: ID3算法最早由澳大利亚研究员罗斯·昆兰（Ross Quinlan）在1985年提出（https://doi.org/10.1007/BF00116251）。
- en: 'Entropy: In simple terms, entropy shows the degree of uncertainty of the signal.
    For example, if a football (soccer) game is 5 minutes from finishing and if the
    score is 5-0, then we would say that the game has a low entropy, or, in other
    words, we are almost certain that the team with 5 goals will win. However, if
    the score is 1-1, then the game will be considered to have a high entropy (uncertainty).
    In the context of information theory, entropy is the average rate at which information
    is provided by a random source of data. Mathematically speaking, this entropy
    is defined as:'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 熵：简单来说，熵表示信号的不确定性程度。例如，如果一场足球比赛（soccer）距离结束还有5分钟，而比分是5-0，我们就会说这场比赛的熵很低，换句话说，我们几乎可以确定5个进球的队伍将获胜。然而，如果比分是1-1，那么这场比赛的熵将被认为很高（不确定性）。在信息论的背景下，熵是由随机数据源提供信息的平均速率。从数学角度看，这个熵定义为：
- en: '![Figure 5.38: Entropy equation'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.38：熵方程'
- en: '](img/image-75ECNLSY.jpg)'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-75ECNLSY.jpg)'
- en: 'Figure 5.38: Entropy equation'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.38：熵方程
- en: In this scenario, when the random source of data produces a probability value
    of around 0.5, the event carries more information, as the final outcome is relatively
    uncertain compared to when the data source produces an extreme (high or low) probability
    value.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当数据的随机源产生约0.5的概率值时，事件携带更多的信息，因为与数据源产生极端（高或低）概率值时相比，最终结果相对不确定。
- en: 'Information gain: This quantifies the amount of uncertainty reduced if we have
    prior information about a variable a (the variable will be a feature in the case
    of machine learning models). In other words, how much information can variable
    a provide regarding an event. Given a dataset S, and an attribute to observe a,
    the information gain is defined mathematically as:'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益：这是量化如果我们事先知道变量a的信息（在机器学习模型中，变量a通常是一个特征）时，可以减少的不确定性。换句话说，变量a可以提供关于某个事件多少信息。给定数据集S和要观察的属性a，信息增益在数学上定义为：
- en: '![Figure 5.39: Information gain equation'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.39：信息增益方程'
- en: '](img/image-LO6IJIR6.jpg)'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-LO6IJIR6.jpg)'
- en: 'Figure 5.39: Information gain equation'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.39：信息增益方程
- en: The information gain of dataset S, for attribute a, is equal to the entropy
    of S minus the entropy of S conditional on attribute a, or the entropy of dataset
    S minus the ratio of number of elements in set t to the total number of elements
    in source S, times the entropy of t, where t is one of the categories in attribute
    a.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集S的属性a的信息增益等于S的熵减去S在属性a条件下的熵，或者S的数据集熵减去集合t中元素数与源S中元素总数的比例，再乘以t的熵，其中t是属性a中的某个类别。
- en: If at first you find the mathematics here a little daunting, don't worry, for
    it is far simpler than it seems. To clarify the ID3 process, we will walk through
    the process using the same dataset as was provided by Quinlan in the original
    paper.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这里的数学有点难理解，不用担心，它比看起来要简单得多。为了更好地理解ID3过程，我们将使用与Quinlan在原始论文中提供的相同数据集，逐步讲解该过程。
- en: 'Exercise 5.07: ID3 Classification'
  id: totrans-760
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习5.07：ID3分类
- en: 'In this exercise, we will be performing ID3 classification on a dataset. In
    the original paper, Quinlan provided a small dataset of 10 weather observation
    samples labeled with either P to indicate that the weather was suitable for, say,
    a Saturday morning game of cricket, or baseball for our North American friends,
    or, if the weather was not suitable for a game, N. The example dataset described
    in the paper will be created in the exercise:'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将对一个数据集执行ID3分类。在原始论文中，Quinlan提供了一个包含10个天气观察样本的小数据集，标记为P表示天气适合，例如，适合周六早晨的板球比赛（或者对于我们北美的朋友来说，是棒球），如果天气不适合比赛，则标记为N。论文中描述的示例数据集将在本练习中创建：
- en: 'Import the required packages:'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: import pandas as pd
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import matplotlib.pyplot as plt
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: 'In a Jupyter notebook, create a pandas DataFrame of the following training
    set:'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，创建一个如下的pandas DataFrame训练集：
- en: df = pd.DataFrame()
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.DataFrame()
- en: df['Outlook'] = ['sunny', 'sunny', 'overcast', 'rain', 'rain', \
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: df['Outlook'] = ['晴天', '晴天', '阴天', '雨天', '雨天', \
- en: '''rain'', ''overcast'', ''sunny'', ''sunny'', ''rain'', \'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '''雨天''，''阴天''，''晴天''，''晴天''，''雨天''，\'
- en: '''sunny'', ''overcast'', ''overcast'', ''rain'']'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '''晴天''，''阴天''，''阴天''，''雨天'']'
- en: df['Temperature'] = ['hot', 'hot', 'hot', 'mild', 'cool', 'cool', \
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: df['Temperature'] = ['炎热', '炎热', '炎热', '温和', '凉爽', '凉爽', \
- en: '''cool'', ''mild'', ''cool'', ''mild'', ''mild'', \'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: '''凉爽''，''温和''，''凉爽''，''温和''，''温和''，\'
- en: '''mild'', ''hot'', ''mild'',]'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '''温和''，''炎热''，''温和''，]'
- en: df['Humidity'] = ['high', 'high', 'high', 'high', 'normal', \
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: df['Humidity'] = ['高', '高', '高', '高', '正常', \
- en: '''normal'', ''normal'', ''high'', ''normal'', \'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '''正常'', ''正常'', ''高'', ''正常'', \'
- en: '''normal'', ''normal'', ''high'', ''normal'', ''high'']'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '''正常'', ''正常'', ''高'', ''正常'', ''高'']'
- en: df['Windy'] = ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', \
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: df['Windy'] = ['弱', '强', '弱', '弱', '弱', '强', \
- en: '''Strong'', ''Weak'', ''Weak'', ''Weak'',''Strong'', ''Strong'', \'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '''强''，''弱''，''弱''，''弱''，''强''，''强''，\'
- en: '''Weak'', ''Strong'']'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '''弱''，''强'']'
- en: df['Decision'] = ['N', 'N', 'P', 'P', 'P', 'N', 'P', 'N', 'P', \
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: df['Decision'] = ['N', 'N', 'P', 'P', 'P', 'N', 'P', 'N', 'P', \
- en: '''P'',''P'', ''P'', ''P'', ''N'']'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: '''P'',''P'', ''P'', ''P'', ''N'']'
- en: df
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: df
- en: 'The output will be as follows:'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.40: pandas DataFrame'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.40：pandas DataFrame'
- en: '](img/image-G05OF88R.jpg)'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-G05OF88R.jpg)'
- en: 'Figure 5.40: pandas DataFrame'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.40：pandas DataFrame
- en: 'In the original paper, the ID3 algorithm starts by taking a small sample of
    the training set at random and fitting the tree to this window. This can be a
    useful method for large datasets, but given that ours is quite small, we will
    simply start with the entire training set. The first step is to calculate the
    entropy for the Decision column, where there are two possible values, or classes,
    P and N:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，ID3算法通过随机选取训练集中的小样本并将树拟合到这个窗口开始。这对于大数据集可能是一个有用的方法，但考虑到我们的数据集相当小，我们将直接从整个训练集开始。第一步是计算决策列的熵，这里有两个可能的值或类别，P和N：
- en: Probability of P
  id: totrans-788
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P的概率
- en: p_p = len(df.loc[df.Decision == 'P']) / len(df)
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: p_p = len(df.loc[df.Decision == 'P']) / len(df)
- en: Probability of N
  id: totrans-790
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N的概率
- en: p_n = len(df.loc[df.Decision == 'N']) / len(df)
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: p_n = len(df.loc[df.Decision == 'N']) / len(df)
- en: entropy_decision = -p_n * np.log2(p_n) - p_p * np.log2(p_p)
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision = -p_n * np.log2(p_n) - p_p * np.log2(p_p)
- en: print(f'H(S) = {entropy_decision:0.4f}')
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'H(S) = {entropy_decision:0.4f}')
- en: 'The output will be as follows:'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: H(S) = 0.94403
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: H(S) = 0.94403
- en: 'We will need to repeat this calculation, so wrap it in a function:'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重复这个计算，因此将其包装成一个函数：
- en: 'def f_entropy_decision(data):'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 'def f_entropy_decision(data):'
- en: p_p = len(data.loc[data.Decision == 'P']) / len(data)
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: p_p = len(data.loc[data.Decision == 'P']) / len(data)
- en: p_n = len(data.loc[data.Decision == 'N']) / len(data)
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: p_n = len(data.loc[data.Decision == 'N']) / len(data)
- en: return -p_n * np.log2(p_n) - p_p * np.log2(p_p)
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 -p_n * np.log2(p_n) - p_p * np.log2(p_p)
- en: 'The next step is to calculate which attribute provides the highest information
    gain out of Outlook, Temperature, Humidity, and Windy. Starting with the Outlook
    parameter, determine the probability of each decision given sunny, overcast, and
    rainy conditions. We need to evaluate the following equation:'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算哪个属性在 Outlook、Temperature、Humidity 和 Windy 中提供了最高的信息增益。从 Outlook 参数开始，确定在晴天、阴天和雨天情况下每个决策的概率。我们需要评估以下方程：
- en: '![Figure 5.41: Information gain'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.41：信息增益'
- en: '](img/image-FV5142IQ.jpg)'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-FV5142IQ.jpg)'
- en: 'Figure 5.41: Information gain'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.41：信息增益
- en: 'Construct this equation in Python using the pandas groupby method:'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas groupby 方法在 Python 中构建这个方程：
- en: 'IG_decision_Outlook = entropy_decision # H(S)'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 'IG_decision_Outlook = entropy_decision # H(S)'
- en: Create a string to print out the overall equation
  id: totrans-807
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个字符串以打印出整体方程
- en: overall_eqn = 'Gain(Decision, Outlook) = Entropy(Decision)'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: overall_eqn = 'Gain(Decision, Outlook) = Entropy(Decision)'
- en: '"""Iterate through the values for outlook and compute the probabilities and
    entropy values'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '"""迭代遍历 outlook 的值，并计算每个决策的概率和熵值'
- en: '"""'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '"""'
- en: 'for name, Outlook in df.groupby(''Outlook''):'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 name, Outlook in df.groupby(''Outlook''):'
- en: num_p = len(Outlook.loc[Outlook.Decision == 'P'])
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: num_p = len(Outlook.loc[Outlook.Decision == 'P'])
- en: num_n = len(Outlook.loc[Outlook.Decision != 'P'])
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: num_n = len(Outlook.loc[Outlook.Decision != 'P'])
- en: num_Outlook = len(Outlook)
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: num_Outlook = len(Outlook)
- en: print(f'p(Decision=P|Outlook={name}) = {num_p}/{num_Outlook}')
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'p(Decision=P|Outlook={name}) = {num_p}/{num_Outlook}')
- en: print(f'p(Decision=N|Outlook={name}) = {num_n}/{num_Outlook}')
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'p(Decision=N|Outlook={name}) = {num_n}/{num_Outlook}')
- en: print(f'p(Outlook={name}) = {num_Outlook}/{len(df)}')
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'p(Outlook={name}) = {num_Outlook}/{len(df)}')
- en: print(f'Entropy(Decision|Outlook={name}) = '\
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'Entropy(Decision|Outlook={name}) = '\
- en: f'-{num_p}/{num_Outlook}.log2({num_p}/{num_Outlook}) - '\
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: f'-{num_p}/{num_Outlook}.log2({num_p}/{num_Outlook}) - '\
- en: f'{num_n}/{num_Outlook}.log2({num_n}/{num_Outlook})')
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: f'{num_n}/{num_Outlook}.log2({num_n}/{num_Outlook})')
- en: entropy_decision_outlook = 0
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision_outlook = 0
- en: '# Cannot compute log of 0 so add checks'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '# 不能计算0的对数，因此需要添加检查'
- en: 'if num_p != 0:'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 num_p != 0:'
- en: entropy_decision_outlook -= (num_p / num_Outlook) \
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision_outlook -= (num_p / num_Outlook) \
- en: '* np.log2(num_p / num_Outlook)'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '* np.log2(num_p / num_Outlook)'
- en: '# Cannot compute log of 0 so add checks'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '# 不能计算0的对数，因此需要添加检查'
- en: 'if num_n != 0:'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 num_n != 0:'
- en: entropy_decision_outlook -= (num_n / num_Outlook) \
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision_outlook -= (num_n / num_Outlook) \
- en: '* np.log2(num_n / num_Outlook)'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '* np.log2(num_n / num_Outlook)'
- en: IG_decision_Outlook -= (num_Outlook / len(df)) \
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: IG_decision_Outlook -= (num_Outlook / len(df)) \
- en: '* entropy_decision_outlook'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: '* entropy_decision_outlook'
- en: print()
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: print()
- en: overall_eqn += f' - p(Outlook={name}).'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: overall_eqn += f' - p(Outlook={name}).'
- en: overall_eqn += f'Entropy(Decision|Outlook={name})'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: overall_eqn += f'Entropy(Decision|Outlook={name})'
- en: print(overall_eqn)
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: print(overall_eqn)
- en: print(f'Gain(Decision, Outlook) = {IG_decision_Outlook:0.4f}')
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'Gain(Decision, Outlook) = {IG_decision_Outlook:0.4f}')
- en: 'The output will be as follows:'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.42: Entropy and gain probabilities'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.42：熵和增益概率'
- en: '](img/image-VBI0GUB5.jpg)'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-VBI0GUB5.jpg)'
- en: 'Figure 5.42: Entropy and gain probabilities'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.42：熵和增益概率
- en: 'The final gain equation for Outlook can be rewritten as:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: Outlook的最终增益方程可以重新写为：
- en: '![Figure 5.43: Equation of information gain'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.43：信息增益的方程'
- en: '](img/image-D3ND1FJM.jpg)'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-D3ND1FJM.jpg)'
- en: 'Figure 5.43: Equation of information gain'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.43：信息增益的方程
- en: 'We need to repeat this process quite a few times, so wrap it in a function
    for ease of use later:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重复这个过程好几次，因此将其包装成一个方便以后使用的函数：
- en: 'def IG(data, column, ent_decision=entropy_decision):'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 'def IG(data, column, ent_decision=entropy_decision):'
- en: IG_decision = ent_decision
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: IG_decision = ent_decision
- en: 'for name, temp in data.groupby(column):'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 name, temp in data.groupby(column):'
- en: p_p = len(temp.loc[temp.Decision == 'P']) / len(temp)
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: p_p = len(temp.loc[temp.Decision == 'P']) / len(temp)
- en: p_n = len(temp.loc[temp.Decision != 'P']) / len(temp)
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: p_n = len(temp.loc[temp.Decision != 'P']) / len(temp)
- en: entropy_decision = 0
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision = 0
- en: 'if p_p != 0:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 p_p != 0:'
- en: entropy_decision -= (p_p) * np.log2(p_p)
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision -= (p_p) * np.log2(p_p)
- en: 'if p_n != 0:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '如果 p_n != 0:'
- en: entropy_decision -= (p_n) * np.log2(p_n)
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision -= (p_n) * np.log2(p_n)
- en: IG_decision -= (len(temp) / len(df)) * entropy_decision
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: IG_decision -= (len(temp) / len(df)) * entropy_decision
- en: return IG_decision
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: return IG_decision
- en: 'Repeat this process for each of the other columns to compute the corresponding
    information gain:'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 对其他每一列重复这个过程，计算相应的信息增益：
- en: 'for col in df.columns[:-1]:'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 'for col in df.columns[:-1]:'
- en: print(f'Gain(Decision, {col}) = {IG(df, col):0.4f}')
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'Gain(Decision, {col}) = {IG(df, col):0.4f}')
- en: 'The output will be as follows:'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: Gain(Decision, Outlook) = 0.2467
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Outlook) = 0.2467
- en: Gain (Decision, Temperature) = 0.0292
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Temperature) = 0.0292
- en: Gain(Decision, Humidity) = 0.1518
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Humidity) = 0.1518
- en: Gain(Decision, Windy) = 0.0481
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Windy) = 0.0481
- en: 'This information provides the first decision of the tree. We want to split
    on the maximum information gain, so we split on Outlook. Look at the data splitting
    on Outlook:'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息提供了决策树的第一个决策。我们希望按最大信息增益进行拆分，所以我们选择在Outlook上进行拆分。看一下根据Outlook拆分后的数据：
- en: 'for name, temp in df.groupby(''Outlook''):'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, temp in df.groupby(''Outlook''):'
- en: print('-' * 15)
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: print(name)
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: print(name)
- en: print('-' * 15)
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: print(temp)
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: print(temp)
- en: print('-' * 15)
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: 'The output will be as follows:'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.44: Information gain'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.44：信息增益'
- en: '](img/image-8CX99TQ4.jpg)'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-8CX99TQ4.jpg)'
- en: 'Figure 5.44: Information gain'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.44：信息增益
- en: 'Notice that all the overcast records have a decision of P. This provides our
    first terminating leaf of the decision tree. If it is overcast, we are going to
    play, while if it is rainy or sunny, there is a chance we will not play. The decision
    tree so far can be represented as in the following figure:'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有多云记录的决策为P。这为我们的决策树提供了第一个终止叶子节点。如果是多云，我们将进行游戏，而如果是雨天或阳光明媚，则有可能不进行游戏。目前为止，决策树可以表示为以下图示：
- en: '![Figure 5.45: Decision tree'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.45：决策树'
- en: '](img/image-VQ3LN987.jpg)'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-VQ3LN987.jpg)'
- en: 'Figure 5.45: Decision tree'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.45：决策树
- en: Note
  id: totrans-881
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This figure was created manually for reference and is not contained in, or obtained
    from, the accompanying source code.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 该图是手动创建的，仅供参考，并不包含在附带的源代码中，也没有从中获得。
- en: 'We now repeat this process, splitting by information gain until all the data
    is allocated and all branches of the tree terminate. First, remove the overcast
    samples, as they no longer provide any additional information:'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们重复这个过程，通过信息增益进行拆分，直到所有数据被分配完并且所有树的分支都终止。首先，移除多云的样本，因为它们不再提供任何额外的信息：
- en: df_next = df.loc[df.Outlook != 'overcast']
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: df_next = df.loc[df.Outlook != 'overcast']
- en: df_next
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: df_next
- en: 'The output will be as follows:'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.46: Data after removing the overcast samples'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.46：移除多云样本后的数据'
- en: '](img/image-QF0P7RNV.jpg)'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-QF0P7RNV.jpg)'
- en: 'Figure 5.46: Data after removing the overcast samples'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.46：移除多云样本后的数据
- en: 'Now, we will turn our attention to the sunny samples and will rerun the gain
    calculations to determine the best way to split the sunny information:'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将关注阳光样本，并重新运行增益计算，以确定最佳的阳光信息拆分方式：
- en: df_sunny = df_next.loc[df_next.Outlook == 'sunny']
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: df_sunny = df_next.loc[df_next.Outlook == 'sunny']
- en: 'Recompute the entropy for the sunny samples:'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 对阳光样本重新计算熵值：
- en: entropy_decision = f_entropy_decision(df_sunny)
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision = f_entropy_decision(df_sunny)
- en: entropy_decision
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision
- en: 'The output will be as follows:'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '0.9709505944546686'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9709505944546686'
- en: 'Run the gain calculations for the sunny samples:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 对阳光样本进行增益计算：
- en: 'for col in df_sunny.columns[1:-1]:'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 'for col in df_sunny.columns[1:-1]:'
- en: print(f'Gain(Decision, {col}) = \
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'Gain(Decision, {col}) = \
- en: '{IG(df_sunny, col, entropy_decision):0.4f}'')'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '{IG(df_sunny, col, entropy_decision):0.4f}'')'
- en: 'The output will be as follows:'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: Gain(Decision, Temperature) = 0.8281
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Temperature) = 0.8281
- en: Gain(Decision, Humidity) = 0.9710
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Humidity) = 0.9710
- en: Gain(Decision, Windy) = 0.6313
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Windy) = 0.6313
- en: 'Again, we select the largest gain, which is Humidity. Group the data by Humidity:'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 再次选择最大增益，这次是湿度。根据湿度分组数据：
- en: 'for name, temp in df_sunny.groupby(''Humidity''):'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, temp in df_sunny.groupby(''Humidity''):'
- en: print('-' * 15)
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: print(name)
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: print(name)
- en: print('-' * 15)
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: print(temp)
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: print(temp)
- en: print('-' * 15)
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: 'The output will be as follows:'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.47: After grouping data according to humidity'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.47：根据湿度分组后的数据'
- en: '](img/image-JBOR08DM.jpg)'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JBOR08DM.jpg)'
- en: 'Figure 5.47: After grouping data according to humidity'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.47：根据湿度分组后的数据
- en: 'We can see here that we have two terminating leaves in that when the Humidity
    is high, there is a decision not to play, and, vice versa, when the Humidity is
    normal, there is the decision to play. So, updating our representation of the
    decision tree, we have:'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当湿度较高时，决策是“不玩”，而当湿度正常时，决策是“玩”。因此，更新后的决策树表示为：
- en: '![Figure 5.48: Decision tree with two values'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.48：具有两个值的决策树'
- en: '](img/image-SK44MT7R.jpg)'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-SK44MT7R.jpg)'
- en: 'Figure 5.48: Decision tree with two values'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.48：具有两个值的决策树
- en: 'So, the last set of data that requires classification is the rainy outlook
    data. Extract only the rain data and rerun the entropy calculation:'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最后一组需要分类的数据是雨天预报数据。提取仅包含降雨数据的部分并重新运行熵计算：
- en: df_rain = df_next.loc[df_next.Outlook == 'rain']
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: df_rain = df_next.loc[df_next.Outlook == 'rain']
- en: entropy_decision = f_entropy_decision(df_rain)
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision = f_entropy_decision(df_rain)
- en: entropy_decision
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: entropy_decision
- en: 'The output will be as follows:'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '0.9709505944546686'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9709505944546686'
- en: 'Repeat the gain calculation with the rain subset:'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 重复对雨天子集的增益计算：
- en: 'for col in df_rain.columns[1:-1]:'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 'for col in df_rain.columns[1:-1]:'
- en: print(f'Gain(Decision, {col}) = \
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'Gain(Decision, {col}) = \
- en: '{IG(df_rain, col, entropy_decision):0.4f}'')'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '{IG(df_rain, col, entropy_decision):0.4f}'')'
- en: 'The output will be as follows:'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: Gain(Decision, Temperature) = 0.6313
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Temperature) = 0.6313
- en: Gain(Decision,Humidity) = 0.6313
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision,Humidity) = 0.6313
- en: Gain(Decision, Windy) = 0.9710
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: Gain(Decision, Windy) = 0.9710
- en: 'Again, splitting on the attribute with the largest gain value requires splitting
    on the Windy values. So, group the remaining information by Windy:'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，基于最大增益值的划分需要基于“Windy”值进行划分。因此，将剩余信息按“Windy”进行分组：
- en: 'for name, temp in df_rain.groupby(''Windy''):'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, temp in df_rain.groupby(''Windy''):'
- en: print('-' * 15)
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: print(name)
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: print(name)
- en: print('-' * 15)
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: print(temp)
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: print(temp)
- en: print('-' * 15)
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: print('-' * 15)
- en: 'The output will be as follows:'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.49: Data grouped according to Windy'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.49：按Windy分组的数据'
- en: '](img/image-CDMZY3B5.jpg)'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-CDMZY3B5.jpg)'
- en: 'Figure 5.49: Data grouped according to Windy'
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.49：按Windy分组的数据
- en: 'Finally, we have all the terminating leaves required to complete the tree,
    as splitting on Windy provides two sets, all of which indicate either play (P)
    or no-play (N) values. Our complete decision tree is as follows:'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有了所有终止叶节点，完成树的构建，因为基于“Windy”属性的划分提供了两个集合，其中每个集合都指示“玩”（P）或“不玩”（N）值。我们的完整决策树如下：
- en: '![Figure 5.50: Final decision tree'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.50：最终决策树'
- en: '](img/image-DJO7JT0K.jpg)'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-DJO7JT0K.jpg)'
- en: 'Figure 5.50: Final decision tree'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.50：最终决策树
- en: Note
  id: totrans-949
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: To access the source code for this specific section, please refer to https://packt.live/37Rh7fX.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 https://packt.live/37Rh7fX。
- en: You can also run this example online at https://packt.live/3hTz4Px. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在线运行此示例，网址是 https://packt.live/3hTz4Px。您必须执行整个笔记本，以获得所需的结果。
- en: Decision trees, very much like KNN models, are discriminative models. Discriminative
    models are the models that aim to maximize the conditional probability of the
    class of data given the features. The opposite of discriminative models is generative
    models, which learn the joint probability of data classes and features and, hence,
    learn the distribution of data to generate artificial samples.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，类似于KNN模型，是判别模型。判别模型旨在最大化给定特征的数据类别的条件概率。与判别模型相对的是生成模型，生成模型学习数据类别和特征的联合概率，从而学习数据的分布以生成人工样本。
- en: 'So, how do we make predictions with unseen information in the case of a decision
    tree? Simply follow the tree. Look at the decision being made at each node and
    apply the data from the unseen sample. The prediction will then end up being the
    label specified at the terminating leaf. Let''s say we had a weather forecast
    for the upcoming Saturday and we wanted to predict whether we were going to play
    or not. The weather forecast is as follows:'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在决策树的情况下，我们如何利用未见过的信息进行预测呢？只需沿着树走。查看每个节点处做出的决策，并应用未见样本的数据。预测最终会是终止叶节点所指定的标签。假设我们有一个即将到来的星期六的天气预报，并且我们想预测是否能去玩。天气预报如下：
- en: '![Figure 5.51: Weather forecast for the upcoming Saturday'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.51：即将到来的星期六的天气预报'
- en: '](img/image-98TZ8U3V.jpg)'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-98TZ8U3V.jpg)'
- en: 'Figure 5.51: Weather forecast for the upcoming Saturday'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.51：即将到来的星期六的天气预报
- en: 'The decision tree for this would be as follows (the dashed circles indicate
    selected leaves in the tree):'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: 该决策树的结构如下（虚线圈表示树中选择的叶子）：
- en: '![Figure 5.52: Making a new prediction using a decision tree'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.52：使用决策树进行新预测'
- en: '](img/image-EI6A0WZI.jpg)'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-EI6A0WZI.jpg)'
- en: 'Figure 5.52: Making a new prediction using a decision tree'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.52：使用决策树进行新预测
- en: Now, hopefully, you have a reasonable understanding of the underlying concept
    of decision trees and the process of making sequential decisions. With the principles
    of decision trees in our toolkit, we will now look at applying a more complicated
    model using the functionality provided in scikit-learn.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，希望你对决策树的基本概念和顺序决策过程有了合理的理解。掌握了决策树的原理后，我们将继续探讨如何使用 scikit-learn 提供的功能应用更复杂的模型。
- en: Classification and Regression Tree
  id: totrans-962
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分类与回归树
- en: The scikit-learn decision tree methods implement the CART method, which provides
    the ability to use decision trees in both classification and regression problems.
    CART differs from ID3 in that the decisions are made by comparing the values of
    features against a calculated value. More precisely, we can see that in the ID3
    algorithm, a decision is made based on the value of the feature that is present
    in the dataset. This serves the purpose well when data is categorical; however,
    once data becomes continuous, this method does not work well. In such cases, CART
    is used, which calculates the threshold value for comparison with a feature value.
    And because, in such comparisons, there can only be two possible outcomes – (a)
    the feature value is greater than (or equal to) the threshold value or, (b) the
    feature value is less than (or equal to) the threshold value – hence, CART results
    in binary trees.
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的决策树方法实现了 CART 方法，它提供了在分类和回归问题中使用决策树的能力。CART 与 ID3 的不同之处在于，决策是通过将特征值与计算得到的值进行比较来做出的。更准确地说，我们可以看到，在
    ID3 算法中，决策是基于数据集中存在的特征值做出的。这种方法在数据是分类数据时表现良好；然而，一旦数据变为连续数据，这种方法就不再有效。在这种情况下，使用
    CART，它计算与特征值进行比较的阈值。而且，因为在这种比较中，只有两种可能的结果——（a）特征值大于（或等于）阈值，或者，（b）特征值小于（或等于）阈值——因此，CART
    结果为二叉树。
- en: 'On the contrary, ID3 creates multiway trees because, as mentioned earlier,
    in ID3, the decision is made based on existing feature values and if the feature
    is categorical, then the tree is going to branch into potentially as many branches
    as the number of categories. Another difference between ID3 and CART is, as opposed
    to ID3, which uses information gain as the metric to find the best split, CART
    uses another measure called the gini impurity measure. Mathematically, you will
    recall that we defined entropy as:'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，ID3 创建多叉树，因为如前所述，在 ID3 中，决策是基于现有特征值做出的，如果特征是分类的，则树会根据类别的数量分支成多个分支。ID3 与 CART
    之间的另一个区别是，ID3 使用信息增益作为度量标准来找到最佳切分，而 CART 使用另一个度量，称为基尼不纯度度量。从数学上讲，我们可以回忆起我们定义的熵为：
- en: '![Figure 5.53: Definition of entropy'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.53：熵的定义'
- en: '](img/image-16MC7PX3.jpg)'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-16MC7PX3.jpg)'
- en: 'Figure 5.53: Definition of entropy'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.53：熵的定义
- en: 'And so, gini impurity is defined as:'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基尼不纯度定义为：
- en: '![Figure 5.54: Definition of gini impurity'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.54：基尼不纯度的定义'
- en: '](img/image-OTNB03MJ.jpg)'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-OTNB03MJ.jpg)'
- en: 'Figure 5.54: Definition of gini impurity'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.54：基尼不纯度的定义
- en: 'Conceptually, this is a measure of the following: if we randomly pick a data
    point in our dataset and if we randomly classify (label) it according to the class
    distribution in the dataset, then what is the probability of classifying the data
    point incorrectly?'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这衡量的是以下内容：如果我们随机选择数据集中的一个数据点，并根据数据集中的类别分布随机对其进行分类（标记），那么将数据点分类错误的概率是多少？
- en: Having discussed the CART- and ID3-based decision tree methodologies, let's
    now solve a classification problem using the CART methodology.
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了基于 CART 和 ID3 的决策树方法后，接下来我们将使用 CART 方法解决一个分类问题。
- en: 'Exercise 5.08: Breast Cancer Diagnosis Classification Using a CART Decision
    Tree'
  id: totrans-974
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 5.08：使用 CART 决策树进行乳腺癌诊断分类
- en: 'In this exercise, we will classify the Breast Cancer Diagnosis data using scikit-learn''s
    decision tree classifier, which can be used in both classification and regression problems:'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 scikit-learn 的决策树分类器对乳腺癌诊断数据进行分类，这可以应用于分类和回归问题：
- en: 'Import the required packages:'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: import numpy as np
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pandas as pd
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import matplotlib.pyplot as plt
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 matplotlib.pyplot 为 plt
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 sklearn.tree 的 DecisionTreeClassifier
- en: from sklearn.model_selection import train_test_split
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 sklearn.model_selection 的 train_test_split
- en: 'Load the Breast Cancer dataset:'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 加载乳腺癌数据集：
- en: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv('../Datasets/breast-cancer-data.csv')
- en: df.head()
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.55: First five rows'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.55：前五行'
- en: '](img/image-5FNGGWXB.jpg)'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-5FNGGWXB.jpg)'
- en: 'Figure 5.55: First five rows'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.55：前五行
- en: 'Before actually going into training a model, let''s further split the training
    dataset into a training and a validation set in the ratio 70:30 to be able to
    impartially evaluate the model performance later using the validation set:'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际开始训练模型之前，我们将训练数据集进一步分割为训练集和验证集，比例为 70:30，以便稍后使用验证集公平地评估模型性能：
- en: train_X, valid_X, \
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: train_X, valid_X, \
- en: train_y, valid_y = train_test_split(df[set(df.columns)\
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: train_y, valid_y = train_test_split(df[set(df.columns)\
- en: -{'diagnosis'}], df.diagnosis, \
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: -{'diagnosis'}], df.diagnosis, \
- en: test_size=0.3, random_state=123)
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: test_size=0.3, random_state=123)
- en: 'Fit the model to the training data and check the corresponding accuracy:'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型拟合到训练数据并检查相应的准确率：
- en: model = DecisionTreeClassifier()
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: model = DecisionTreeClassifier()
- en: model = model.fit(train_X, train_y)
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: model = model.fit(train_X, train_y)
- en: model.score(train_X, train_y)
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(train_X, train_y)
- en: 'The output will be as follows:'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '1.0'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: Our model achieves 100% accuracy on the training set.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在训练集上达到了 100% 的准确率。
- en: 'Check the performance against the test set:'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 检查在测试集上的表现：
- en: model.score(valid_X, valid_y)
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(valid_X, valid_y)
- en: 'The output accuracy should be smaller than 1, ideally:'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 输出准确率应小于 1，理想情况下为：
- en: '0.9415204678362573'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9415204678362573'
- en: 'One of the great things about decision trees is that we can visually represent
    the model and see exactly what is going on. Install the required dependency:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个优点是我们可以直观地表示模型并查看发生了什么。安装所需的依赖：
- en: '!conda install python-graphviz'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: '!conda install python-graphviz'
- en: 'Import the graphing package:'
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 导入绘图包：
- en: import graphviz
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 graphviz
- en: from sklearn.tree import export_graphviz
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 sklearn.tree 的 export_graphviz
- en: 'Plot the model:'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制模型：
- en: dot_data = export_graphviz(model, out_file=None)
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: dot_data = export_graphviz(model, out_file=None)
- en: graph = graphviz.Source(dot_data)
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: graph = graphviz.Source(dot_data)
- en: graph
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: graph
- en: 'The output will be as follows:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.56: Decisions of the CART decision tree'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.56：CART 决策树的决策'
- en: '](img/image-0D99OWZV.jpg)'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-0D99OWZV.jpg)'
- en: 'Figure 5.56: Decisions of the CART decision tree'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.56：CART 决策树的决策
- en: 'This figure illustrates the decisions of the CART decision tree in the scikit-learn
    model. The first line of the node is the decision that is made at that step. The
    first node, X[1] <= 16.795, indicates that the training data is split on column
    1 on the basis of being less than or equal to 16.795\. Those samples with values
    on column 1 less than 16.795 (of which there are 254) are then further dissected
    on column 25\. Similarly, samples with values on column 1 greater than or equal
    to 16.795 (of which there are 144) are then further dissected on column 28\. This
    decision/branching process continues until the terminating condition is reached.
    The terminating condition can be defined in several ways. Some of them are as
    follows:'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 此图展示了 scikit-learn 模型中 CART 决策树的决策过程。节点的第一行是该步骤所做的决策。第一个节点 X[1] <= 16.795，表示训练数据在第
    1 列上按小于或等于 16.795 进行分割。那些第 1 列值小于 16.795 的样本（共 254 个）会进一步在第 25 列上进行划分。同样，第 1 列值大于等于
    16.795 的样本（共 144 个）会在第 28 列上进一步划分。此决策/分支过程一直持续，直到达到终止条件。终止条件可以通过多种方式定义。以下是其中一些：
- en: The tree has been exhausted and all terminating leaves have been constructed/found.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 树已被遍历，所有终止叶节点已构建/找到。
- en: Impurity (the measure of the different number of classes that the elements in
    a node belong to) at a particular node is below a given threshold.
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 纯度（衡量一个节点内元素所属不同类别的数量）在某一特定节点处低于给定阈值。
- en: The number of elements at a particular node is lower than a threshold number
    of elements.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 某一特定节点的元素数量低于阈值数量。
- en: Note
  id: totrans-1022
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/31btfY5.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参见 https://packt.live/31btfY5。
- en: You can also run this example online at https://packt.live/37PJTO4\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/37PJTO4 上在线运行此示例。你必须执行整个 Notebook 才能得到预期的结果。
- en: Before we move on to the next topic, let's perform a binary classification task
    using the CART decision tree on the MNIST digits dataset. The task is to classify
    images of digits 0 and 1 into digits (or classes) 0 and 1.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一个主题之前，我们先使用 CART 决策树对 MNIST 数字数据集执行一个二分类任务。任务是将数字 0 和 1 的图像分类为数字（或类别）0
    和 1。
- en: 'Activity 5.03: Binary Classification Using a CART Decision Tree'
  id: totrans-1026
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 5.03：使用 CART 决策树进行二分类
- en: 'In this activity, we will build a CART Decision Tree-based classifier using
    the MNIST dataset to classify between two digits: 0 and 1.'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将使用 MNIST 数据集构建基于 CART 决策树的分类器，用于对两个数字进行分类：0 和 1。
- en: 'The steps to be performed are as follows:'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的步骤如下：
- en: 'Import the required dependencies:'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的依赖：
- en: import struct
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: import struct
- en: import numpy as np
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import pandas as pd
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import gzip
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: import gzip
- en: import urllib.request
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: import urllib.request
- en: import matplotlib.pyplot as plt
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from array import array
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: from array import array
- en: from sklearn.model_selection import train_test_split
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.model_selection import train_test_split
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.tree import DecisionTreeClassifier
- en: Load the MNIST data into memory.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 将 MNIST 数据加载到内存中。
- en: Visualize a sample of the data.
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据样本。
- en: Construct a CART Decision Tree classifier model to classify the digits 0 and
    1\. The model we are going to create is to determine whether the samples are either
    the digits 0 or 1\. To do this, we first need to select only those samples.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个 CART 决策树分类器模型，用于分类数字 0 和 1。我们要创建的模型是确定样本是数字 0 还是 1。为此，我们首先需要选择这些样本。
- en: Visualize the selected information with images of one sample of 0 and one sample
    of 1.
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数字 0 和数字 1 的一个样本图像，来可视化所选信息。
- en: In order to provide the image information to the model, we must first flatten
    the data out so that each image is 1 x 784 pixels in shape.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将图像信息提供给模型，我们必须首先将数据展平，使得每个图像变为 1 x 784 像素的形状。
- en: Construct the model; use the DecisionTreeClassifier API and call the fit function.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型；使用 DecisionTreeClassifier API 并调用 fit 函数。
- en: Determine the training set accuracy.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 确定训练集的准确率。
- en: Compare the performance against the test set.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 比较在测试集上的表现。
- en: 'The output will be as follows:'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '0.9962174940898345'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9962174940898345'
- en: Note
  id: totrans-1049
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 本次活动的解决方案可以通过此链接找到。
- en: 'An interesting point to note here is that the test set performance here is
    much better than that in Activity 5.01: Ordinary Least Squares Classifier – Binary
    Classifier. The dataset is exactly the same in both cases, but the models are
    different. This demonstrates the fact that the CART decision trees-based model
    performs better than the OLS-based model on this binary classification task.'
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的点需要注意的是，测试集的表现比在活动 5.01：普通最小二乘分类器 – 二分类器中的表现要好得多。两个情况下的数据集完全相同，但模型不同。这表明基于
    CART 决策树的模型在此二分类任务中比基于 OLS 的模型表现得更好。
- en: Now that we have acquired an understanding of decision trees for classification,
    we will next discuss one of the most popular and powerful types of machine learning
    model that is widely used in the industry as well as in academia – artificial
    neural networks.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了用于分类的决策树，接下来我们将讨论一种在工业界和学术界广泛使用的最流行且强大的机器学习模型之一——人工神经网络。
- en: Artificial Neural Networks
  id: totrans-1053
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: 'The final type of classification model that we will be studying is Artificial
    Neural Networks (ANNs). Firstly, this class of model is inspired by how the human
    brain functions. More specifically, we try to mathematically emulate the interconnected-neurons
    architecture, hence the name – neural networks. Essentially, an artificial neural
    network architecture looks something like that shown in Figure 5.57:'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究的最终分类模型类型是人工神经网络（ANNs）。首先，这种模型的灵感来自于人类大脑的功能。更具体来说，我们试图在数学上模拟互联神经元的结构，因此得名“神经网络”。本质上，人工神经网络架构看起来像图
    5.57 所示：
- en: '![Figure 5.57: Neural network architecture example'
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.57：神经网络架构示例'
- en: '](img/image-7ULGS8TJ.jpg)'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-7ULGS8TJ.jpg)'
- en: 'Figure 5.57: Neural network architecture example'
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.57：神经网络架构示例
- en: To the extreme left is the input data X, expanded into the N0 different feature
    dimensions. This example has two hidden layers, h1 and h2, having N1 and N2 number
    of neurons, respectively. Wait, what is a neuron? The nomenclature is derived
    from the human brain analogy, and a neuron in the context of an artificial neural
    network is essentially a node in the network/graph. And finally, in the figure,
    there is the output layer, Y, which consists of the N number of classes for the
    example of a multiclass classification task. Each arrow in this figure represents
    a network weight or parameter. As you can see, these models can therefore have
    a large number of arrows/parameters, which essentially makes them complex and
    powerful. And the way these weights come into play is, for example, h11 is the
    weighted sum of all the input features, x1, x2 … xN0, passed through an activation
    function.
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 极左侧是输入数据 X，扩展到 N0 个不同的特征维度。这个例子有两个隐藏层，h1 和 h2，分别有 N1 和 N2 个神经元。等等，什么是神经元？这个命名来源于类比人脑，人工神经网络中的神经元本质上是网络/图中的一个节点。最后，在图中，有输出层
    Y，它包含了 N 个类别，用于多类分类任务的示例。图中的每一条箭头表示一个网络权重或参数。正如你所看到的，这些模型因此可能拥有大量的箭头/参数，这使得它们既复杂又强大。这些权重的作用方式是，例如，h11
    是所有输入特征 x1, x2 … xN0 的加权和，通过激活函数处理后得出。
- en: 'Wait, what then is an activation function? In neural networks, inside each
    neuron or node is an implicit non-linear function. This helps make the model non-linear
    (hence complex), and if we remove these non-linearities, then the several hidden
    layers will collapse (by virtue of a series of matrix multiplications) resulting
    in an extremely simple linear model. This linear model would imply that the output
    class of data can be represented as the weighted sum of input features, which
    is absolutely not the case with ANNs. Popular non-linear activation functions
    used in neural networks are sigmoid, tanh (hyperbolic tangent), and Rectified
    Linear Unit (ReLU) In fact, if we use sigmoid as the activation function and omit
    all the hidden layers and restrict the number of classes to two, we get the following
    neural network:'
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么是激活函数？在神经网络中，每个神经元或节点内部都有一个隐式的非线性函数。这个函数有助于让模型变得非线性（从而更加复杂），如果去掉这些非线性，那么几个隐藏层将会坍塌（由于一系列矩阵乘法的结果），最终变成一个极其简单的线性模型。这个线性模型意味着数据的输出类别可以表示为输入特征的加权和，而这在人工神经网络（ANN）中显然不是这样的。神经网络中常用的非线性激活函数有
    sigmoid、tanh（双曲正切）和修正线性单元（ReLU）。事实上，如果我们使用 sigmoid 作为激活函数，且省略所有隐藏层，并将类别数限制为两个，我们就得到了以下的神经网络：
- en: '![Figure 5.58: Neural network binary classifier with no hidden layers'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.58: 无隐藏层的神经网络二分类器'
- en: '](img/image-NQSFR1X7.jpg)'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-NQSFR1X7.jpg)'
- en: 'Figure 5.58: Neural network binary classifier with no hidden layers'
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.58: 无隐藏层的神经网络二分类器'
- en: Does this look familiar? This model is precisely the same as our logistic regression
    model! First, we take the weighted sum of all the input features x1, x2 …. xN0,
    and then apply the sigmoid or logistic function in order to get the final output.
    This output is then compared with the ground truth label to compute the loss.
    And, similar to linear regression models as discussed in the previous chapter,
    neural networks use gradient descent to derive the optimal set of weights or parameters
    by minimizing the loss. Although, since a neural network model is much more complex
    than a linear regression model, the way the parameters are updated in the former
    is much more sophisticated than the latter, and a technique called backpropagation
    is used to do so. Mathematical details of backpropagation are beyond the scope
    of this chapter, but we encourage readers to read further on that.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很熟悉吗？这个模型实际上和我们的逻辑回归模型完全相同！首先，我们取所有输入特征 x1, x2 …. xN0 的加权和，然后应用 sigmoid
    或逻辑函数来获得最终输出。然后，这个输出与真实标签进行比较，计算损失。而且，类似于前一章讨论的线性回归模型，神经网络通过梯度下降来导出最优的权重或参数集，目的是最小化损失。尽管如此，由于神经网络模型比线性回归模型复杂得多，前者更新参数的方式远比后者精密，采用了叫做反向传播（backpropagation）的方法。反向传播的数学细节超出了本章的范围，但我们鼓励读者进一步阅读相关内容。
- en: 'Exercise 5.09: Neural Networks – Multiclass Classifier'
  id: totrans-1064
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '练习 5.09: 神经网络 – 多类分类器'
- en: 'Neural networks can be used for multiclass classification and are by no means
    restricted just to binary classification. In this exercise, we will be investigating
    a 10-class classification problem, in other words, the MNIST digits classification
    task. The process for loading the MNIST training and test data is identical to
    the previous exercises:'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以用于多类分类任务，并不限于二分类。在这个练习中，我们将研究一个10类分类问题，换句话说，就是MNIST数字分类任务。加载MNIST训练和测试数据的过程与之前的练习完全相同：
- en: 'Import the required packages:'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: import struct
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: import struct
- en: import numpy as np
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: import gzip
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: import gzip
- en: import urllib.request
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: import urllib.request
- en: import matplotlib.pyplot as plt
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from array import array
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: from array import array
- en: from sklearn.neural_network import MLPClassifier
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.neural_network import MLPClassifier
- en: 'Load the training/test images and the corresponding labels:'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 加载训练/测试图像及其对应的标签：
- en: 'with gzip.open(''../Datasets/train-images-idx3-ubyte.gz'', ''rb'') as f:'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/train-images-idx3-ubyte.gz'', ''rb'') as f:'
- en: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
- en: img = np.array(array("B", f.read())).reshape((size, rows, cols))
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: img = np.array(array("B", f.read())).reshape((size, rows, cols))
- en: 'with gzip.open(''../Datasets/train-labels-idx1-ubyte.gz'', ''rb'') as f:'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/train-labels-idx1-ubyte.gz'', ''rb'') as f:'
- en: magic, size = struct.unpack(">II", f.read(8))
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size = struct.unpack(">II", f.read(8))
- en: labels = np.array(array("B", f.read()))
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: labels = np.array(array("B", f.read()))
- en: 'with gzip.open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''rb'') as f:'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/t10k-images-idx3-ubyte.gz'', ''rb'') as f:'
- en: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size, rows, cols = struct.unpack(">IIII", f.read(16))
- en: img_test = np.array(array("B", f.read()))\
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: img_test = np.array(array("B", f.read()))\
- en: .reshape((size, rows, cols))
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: .reshape((size, rows, cols))
- en: 'with gzip.open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''rb'') as f:'
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 'with gzip.open(''../Datasets/t10k-labels-idx1-ubyte.gz'', ''rb'') as f:'
- en: magic, size = struct.unpack(">II", f.read(8))
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: magic, size = struct.unpack(">II", f.read(8))
- en: labels_test = np.array(array("B", f.read()))
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: labels_test = np.array(array("B", f.read()))
- en: 'Visualize a sample of the data:'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据样本：
- en: 'for i in range(10):'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: 'for i in range(10):'
- en: plt.subplot(2, 5, i + 1)
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(2, 5, i + 1)
- en: plt.imshow(img[i], cmap='gray');
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(img[i], cmap='gray');
- en: plt.title(f'{labels[i]}');
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: plt.title(f'{labels[i]}');
- en: plt.axis('off')
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off')
- en: 'The output will be as follows:'
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.59: Sample data'
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.59：示例数据'
- en: '](img/image-55PRZNGR.jpg)'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-55PRZNGR.jpg)'
- en: 'Figure 5.59: Sample data'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.59：示例数据
- en: 'Given that the training data is so large, we will select a subset of the overall
    data to reduce the training time as well as the system resources required for
    the training process:'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练数据量非常大，我们将选择一个数据子集，以减少训练时间和所需的系统资源：
- en: 'np.random.seed(0) # Give consistent random numbers'
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 'np.random.seed(0) # 设置一致的随机数'
- en: selection = np.random.choice(len(img), 5000)
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: selection = np.random.choice(len(img), 5000)
- en: selected_images = img[selection]
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images = img[selection]
- en: selected_labels = labels[selection]
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: selected_labels = labels[selection]
- en: 'Again, reshape the input data in vector form for later use:'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 再次将输入数据重塑为向量形式，以供后续使用：
- en: selected_images = selected_images.reshape((-1, rows * cols))
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images = selected_images.reshape((-1, rows * cols))
- en: selected_images.shape
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images.shape
- en: 'The output will be as follows:'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: (5000, 784)
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: (5000, 784)
- en: 'Next, we normalize the image data. We scale all the image values between 0
    and 1\. Originally, grayscale images are comprised of pixels with values between
    and including 0 to 255, where 0 is black and 255 is white. Normalization is important
    because it helps the gradient descent algorithm perform effectively. Unnormalized
    data is more prone to diminishing/exploding values of gradients during weight
    updates and will, therefore, lead to negligible weight updates:'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对图像数据进行归一化处理。我们将所有的图像值缩放到0到1之间。原始的灰度图像包含像素值在0到255之间（包括0和255），其中0表示黑色，255表示白色。归一化非常重要，因为它有助于梯度下降算法的有效运行。未经归一化的数据在权重更新过程中更容易出现梯度值衰减或爆炸，从而导致权重更新极小：
- en: selected_images = selected_images / 255.0
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: selected_images = selected_images / 255.0
- en: img_test = img_test / 255.0
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: img_test = img_test / 255.0
- en: 'Construct the neural network (or the multilayer perceptron) model. There are
    a few extra arguments, as follows: the sgd value for solver tells the model to
    use stochastic gradient descent, with additional max_iter iterations required
    to converge on a solution. The hidden_layer_sizes argument essentially describes
    the model architecture, in other words, how many hidden layers there are and how
    many neurons there are in each hidden layer. For example, (20, 10, 5) would mean
    3 hidden layers, with 20, 10, and 5 neurons in them, respectively. The learning_rate_init
    argument gives the initial learning rate for the gradient descent algorithm:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 构建神经网络（或多层感知器）模型。此模型有几个额外的参数，如下所示：solver的sgd值告诉模型使用随机梯度下降，并且需要额外的max_iter迭代才能收敛到解决方案。hidden_layer_sizes参数本质上描述了模型的架构，换句话说，就是有多少隐藏层，每个隐藏层中有多少个神经元。例如，(20,
    10, 5)表示3个隐藏层，分别有20、10和5个神经元。learning_rate_init参数给出了梯度下降算法的初始学习率：
- en: model = MLPClassifier(solver='sgd', hidden_layer_sizes=(100,), \
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: model = MLPClassifier(solver='sgd', hidden_layer_sizes=(100,), \
- en: max_iter=1000, random_state=1, \
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: max_iter=1000, random_state=1, \
- en: learning_rate_init=.01)
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: learning_rate_init=.01)
- en: model.fit(X=selected_images, y=selected_labels)
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: model.fit(X=selected_images, y=selected_labels)
- en: 'The output will be as follows:'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.60: Neural network model'
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.60：神经网络模型'
- en: '](img/image-01EI7LF3.jpg)'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-01EI7LF3.jpg)'
- en: 'Figure 5.60: Neural network model'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.60：神经网络模型
- en: Note
  id: totrans-1120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to the documentation at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier
    for more information on the arguments.
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 有关参数的更多信息，请参阅文档：https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier
- en: 'Determine the accuracy score against the training set:'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 确定训练集的准确率得分：
- en: model.score(X=selected_images, y=selected_labels)
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=selected_images, y=selected_labels)
- en: 'The output will be as follows:'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '1.0'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '1.0'
- en: 'Determine the first two predictions for the training set and plot the images
    with the corresponding predictions:'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: 确定训练集的前两个预测并绘制图像以及对应的预测结果：
- en: model.predict(selected_images)[:2]
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: model.predict(selected_images)[:2]
- en: 'The output will be as follows:'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: array([4, 1], dtype=uint8)
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: array([4, 1], dtype=uint8)
- en: 'Show the images for the first two samples of the training set to see whether
    we are correct:'
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 显示训练集中前两个样本的图像，查看我们的预测是否正确：
- en: plt.subplot(1, 2, 1)
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(1, 2, 1)
- en: plt.imshow(selected_images[0].reshape((28, 28)), cmap='gray');
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(selected_images[0].reshape((28, 28)), cmap='gray');
- en: plt.axis('off');
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off');
- en: plt.subplot(1, 2, 2)
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: plt.subplot(1, 2, 2)
- en: plt.imshow(selected_images[1].reshape((28, 28)), cmap='gray');
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: plt.imshow(selected_images[1].reshape((28, 28)), cmap='gray');
- en: plt.axis('off');
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: plt.axis('off');
- en: 'The output will be as follows:'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![ Figure 5.61: Sample images from the training dataset'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: '![ 图 5.61：来自训练数据集的样本图像'
- en: '](img/image-JSOB0XP8.jpg)'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JSOB0XP8.jpg)'
- en: 'Figure 5.61: Sample images from the training dataset'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.61：来自训练数据集的样本图像
- en: 'Again, print out the probability scores provided by the model for the first
    sample of the training set. Confirm that there are 10 different values for each
    of the 10 classes in the set:'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次打印出模型为训练集的第一个样本提供的概率分数。确认每个类别都有10个不同的值，分别对应10个类别：
- en: model.predict_proba(selected_images)[0]
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: model.predict_proba(selected_images)[0]
- en: 'The output will be as follows:'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.62: Array of predicted class probability'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.62：预测类别概率数组'
- en: '](img/image-GXHG6HCZ.jpg)'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-GXHG6HCZ.jpg)'
- en: 'Figure 5.62: Array of predicted class probability'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.62：预测类别概率数组
- en: Notice that, in the probability array of the first sample, the fifth (digit
    4) number is the highest probability, thus indicating a prediction of 4.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在第一个样本的概率数组中，第五个（数字4）数字的概率是最高的，因此预测为4。
- en: 'Compute the accuracy of the model against the test set. This will provide a
    reasonable estimate of the model''s in the wild performance, as it has never seen
    the data in the test set. It is expected that the accuracy rate of the test set
    will be slightly lower than the training set, given that the model has not been
    exposed to this data:'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的准确率。这将提供模型在实际应用中的合理表现估计，因为它从未见过测试集中的数据。预计测试集的准确率会略低于训练集，因为模型没有接触过这些数据：
- en: model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: model.score(X=img_test.reshape((-1, rows * cols)), y=labels_test)
- en: 'The output will be as follows:'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '0.9384'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9384'
- en: If we compare these training and test set scores (1 and 0.9384) for the neural
    network model with those for the logistic regression model (0.986 and 0.9002)
    as obtained in Exercise 5.03, Logistic Regression – Multiclass Classifier, we
    can see that the neural network model expectedly outperforms the logistic regression
    model. This happens because there are many more parameters to be learned in a
    neural network compared to a logistic regression model, making neural networks
    more complex and hence powerful. Conversely, if we build a neural network binary
    classifier with no hidden layers and using sigmoidal activation functions, it
    essentially becomes the same as a logistic regression model.
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将这些神经网络模型的训练集和测试集得分（1 和 0.9384）与在练习 5.03《逻辑回归——多类分类器》中获得的逻辑回归模型得分（0.986 和
    0.9002）进行比较，可以看到神经网络模型的表现超出了逻辑回归模型的预期。这是因为神经网络需要学习的参数比逻辑回归模型多得多，导致神经网络更复杂，从而更强大。相反，如果我们构建一个没有隐藏层且使用
    sigmoidal 激活函数的神经网络二分类器，它本质上就与逻辑回归模型相同。
- en: Note
  id: totrans-1153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2NjfiyX.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见 https://packt.live/2NjfiyX。
- en: You can also run this example online at https://packt.live/3dowv4z. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/3dowv4z 上在线运行这个示例。你必须执行整个笔记本以获得期望的结果。
- en: Before we conclude this chapter, let's work out a last classification task using
    neural networks, this time, on the Breast Cancer Diagnosis classification dataset.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们通过神经网络完成最后一个分类任务，这次我们将使用乳腺癌诊断分类数据集。
- en: 'Activity 5.04: Breast Cancer Diagnosis Classification Using Artificial Neural
    Networks'
  id: totrans-1157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 5.04：使用人工神经网络进行乳腺癌诊断分类
- en: In this activity, we will be using the Breast Cancer Diagnosis dataset (available
    at https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
    or on GitHub at https://packt.live/3a7oAY8). This dataset is a part of the UCI
    Machine Learning Repository (https://archive.ics.uci.edu/ml/index.php). The dataset
    contains characteristics of the cell nuclei present in the digitized image of
    a fine needle aspirate (FNA) of a breast mass, with the labels malignant and benign
    for each cell nucleus. Characteristics are features (30 in total) such as the
    mean radius, radius error, worst radius, mean texture, texture error, and worst
    texture of the cell nuclei. In this activity, we will use the features provided
    in the dataset to classify between malignant and benign cells.
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用乳腺癌诊断数据集（可通过 https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29
    或 GitHub 上的 https://packt.live/3a7oAY8 获取）。该数据集是 UCI 机器学习库的一部分（https://archive.ics.uci.edu/ml/index.php）。该数据集包含数字化图像中乳腺肿块细针穿刺（FNA）样本中细胞核的特征，并为每个细胞核标注了恶性和良性标签。特征包括
    30 个项目，如细胞核的平均半径、半径误差、最差半径、平均纹理、纹理误差和最差纹理等。在本活动中，我们将使用数据集中提供的特征来区分恶性细胞和良性细胞。
- en: 'The steps to be performed are as follows:'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: 'Import the required packages. For this activity, we will require the pandas
    package for loading the data, the matplotlib package for plotting, and scikit-learn
    for creating the neural network model, as well as to split the dataset into training
    and test sets. Import all the required packages and relevant modules for these
    tasks:'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的包。对于本活动，我们需要 pandas 包来加载数据，matplotlib 包来绘图，scikit-learn 包来创建神经网络模型，并且将数据集划分为训练集和测试集。导入所有所需的包和相关模块以完成这些任务：
- en: import pandas as pd
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 pandas 作为 pd
- en: import matplotlib.pyplot as plt
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 matplotlib.pyplot 作为 plt
- en: from sklearn.neural_network import MLPClassifier
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.neural_network 导入 MLPClassifier
- en: from sklearn.model_selection import train_test_split
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.model_selection 导入 train_test_split
- en: from sklearn import preprocessing
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn 导入 preprocessing
- en: Load the Breast Cancer Diagnosis dataset using pandas and examine the first
    five rows.
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 加载乳腺癌诊断数据集，并检查前五行数据。
- en: 'The next step is feature engineering. Different columns of this dataset have
    different scales of magnitude, hence, before constructing and training a neural
    network model, we normalize the dataset. For this, we use the MinMaxScaler API
    from sklearn, which normalizes each column''s values between 0 and 1, as discussed
    in the Logistic Regression section of this chapter (see Exercise 5.03, Logistic
    Regression – Multiclass Classifier): https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是特征工程。这个数据集的不同列具有不同的量级，因此，在构建和训练神经网络模型之前，我们需要对数据集进行归一化处理。为此，我们使用 sklearn
    的 MinMaxScaler API，它将每列的值归一化到 0 和 1 之间，正如本章的逻辑回归部分所讨论的那样（见练习 5.03，逻辑回归——多类分类器）：https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html。
- en: Before we can construct the model, we must first convert the diagnosis values
    into labels that can be used within the model. Replace the benign diagnosis string
    with the value 0, and the malignant diagnosis string with the value 1.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建模型之前，必须首先将诊断值转换为模型中可以使用的标签。将良性诊断字符串替换为值 0，将恶性诊断字符串替换为值 1。
- en: Also, in order to impartially evaluate the model, we should split the training
    dataset into a training and a validation set.
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了公正地评估模型，我们应该将训练数据集分为训练集和验证集。
- en: Create the model using the normalized dataset and the assigned diagnosis labels.
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归一化数据集和分配的诊断标签来创建模型。
- en: Compute the accuracy of the model against the validation set.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型在验证集上的准确性。
- en: 'The output will be similar to the following:'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '0.9824561403508771'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '0.9824561403508771'
- en: Note
  id: totrans-1174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以通过此链接找到。
- en: 'If we compare this validation set accuracy result with the result(s) from Activity
    5.02: KNN Multiclass Classifier, we find artificial neural networks to be performing
    better than the logistic regression model on the exact same dataset. This is also
    expected, as the former is a more complex and powerful type of machine learning
    model than the latter.'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个验证集的准确度结果与活动 5.02：KNN 多类分类器的结果进行比较，我们会发现人工神经网络在相同数据集上比逻辑回归模型表现得更好。这也是预期的，因为前者是比后者更复杂且更强大的机器学习模型。
- en: Summary
  id: totrans-1177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: We covered a number of powerful and extremely useful classification models in
    this chapter, starting with the use of OLS as a classifier, and then we observed
    a significant increase in performance through the use of the logistic regression
    classifier. We then moved on to memorizing models, such as KNN, which, while simple
    to fit, was able to form complex non-linear boundaries in the classification process,
    even with images as input information into the model. Thereafter, we discussed
    decision trees and the ID3 algorithm. We saw how decision trees, like KNN models,
    memorize the training data using rules to make predictions with quite a high degree
    of accuracy. Finally, we concluded our introduction to classification problems
    with one of the most powerful classification models – artificial neural networks.
    We briefly covered the basics of a feedforward neural network and also showed
    through an exercise how it outperformed the logistic regression model on a classification
    task.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们涵盖了多个强大且极其有用的分类模型，从使用 OLS 作为分类器开始，然后我们通过使用逻辑回归分类器观察到性能的显著提升。接着我们进入了记忆型模型，如
    KNN，虽然 KNN 模型易于拟合，但它能够在分类过程中形成复杂的非线性边界，即便是将图像作为输入信息也能做到。随后，我们讨论了决策树和 ID3 算法。我们看到，像
    KNN 模型一样，决策树通过规则来记忆训练数据，从而以相当高的准确度做出预测。最后，我们以其中一个最强大的分类模型——人工神经网络，结束了对分类问题的介绍。我们简要介绍了前馈神经网络的基础知识，并通过一个练习展示了它在分类任务中优于逻辑回归模型的表现。
- en: In the next chapter, we will be extending what we have learned in this chapter.
    It will cover ensemble techniques, including boosting, and the very effective
    random forest model.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将扩展本章所学的内容，涵盖集成技术，包括提升方法，以及非常有效的随机森林模型。
