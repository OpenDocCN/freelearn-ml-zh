- en: 3D Head Pose Estimation Using AAM and POSIT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AAM和POSIT进行3D头部姿态估计
- en: A good computer vision algorithm can't be complete without great, robust capabilities,
    as well as wide generalization and a solid math foundation. All these features
    accompany the work mainly developed by Timothy Cootes with Active Appearance Models.
    This chapter will teach you how to create an **Active Appearance Model** (**AAM**)
    of your own using OpenCV as well as how to use it to search for the closest position
    your model is located at in a given frame. Besides, you will learn how to use
    the POSIT algorithm and how to fit your 3D model in the *posed* image. With all
    these tools, you will be able to track a 3D model in a video, in real time--ain't
    it great? Although the examples focus on head pose, virtually any deformable model
    could use the same approach.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的计算机视觉算法不能没有强大的、鲁棒的特性，以及广泛的泛化能力和坚实的数学基础。所有这些特性都伴随着Timothy Cootes主要开发的活动外观模型的工作。本章将教你如何使用OpenCV创建自己的**活动外观模型**（**AAM**），以及如何使用它来搜索模型在给定帧中的最接近位置。此外，你将学习如何使用POSIT算法以及如何将你的3D模型拟合到*姿态*图像中。有了所有这些工具，你将能够实时跟踪视频中的3D模型——这不是很棒吗？尽管示例侧重于头部姿态，但实际上任何可变形模型都可以使用相同的方法。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Active Appearance Models overview
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活动外观模型概述
- en: Active Shape Models overview
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 活动外观模型概述
- en: Model instantiation--playing with the Active Appearance Model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型实例化--与活动外观模型玩耍
- en: AAM search and fitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AAM搜索和拟合
- en: POSIT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: POSIT
- en: 'The following list has an explanation of the terms that you will come across
    in the chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表解释了你在本章中会遇到的一些术语：
- en: '**Active Appearance Model** (**AAM**): This is an object model containing statistical
    information of its shape and texture. It is a powerful way of capturing shape
    and texture variation from objects.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动外观模型**（**AAM**）：这是一个包含其形状和纹理统计信息的对象模型。它是从对象中捕获形状和纹理变化的一种强大方式。'
- en: '**Active Shape Model** (**ASM**): This is a statistical model of the shape
    of an object. It is very useful for learning shape variation.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动形状模型**（**ASM**）：这是一个对象的形状的统计模型。它对于学习形状变化非常有用。'
- en: '**Principal Component Analysis** (**PCA**): This is an orthogonal linear transformation
    that transforms the data to a new coordinate system, such that the greatest variance
    by any projection of the data comes to lie on the first coordinate (called the
    first principal component), the second greatest variance on the second coordinate,
    and so on. This procedure is often used in dimensionality reduction. When reducing
    the dimension of the original problem, one can use a faster fitting algorithm.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）：这是一种正交线性变换，将数据转换到新的坐标系中，使得数据通过任何投影的最大方差都落在第一个坐标（称为第一个主成分）上，第二个最大方差落在第二个坐标上，依此类推。这个程序通常用于降维。在降低原始问题的维度时，可以使用更快的拟合算法。'
- en: '**Delaunay Triangulation (DT)**: For a set of *P* points in a plane, it is
    a triangulation such that no point in *P* is inside the circumcircle of any triangle
    in the triangulation. It tends to avoid skinny triangles. The triangulation is
    required for texture mapping.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Delaunay三角剖分（DT）**：对于平面上一组*P*个点，它是一种三角剖分，使得*P*中的任何点都不在三角剖分中任何三角形的外接圆内。它倾向于避免瘦三角形。这种三角剖分对于纹理映射是必需的。'
- en: '**Affine transformation**: This is any transformation that can be expressed
    in the form of a matrix multiplication followed by a vector addition. This can
    be used for texture mapping.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仿射变换**：这是任何可以用矩阵乘法后跟向量加法表示的变换。这可以用于纹理映射。'
- en: '**Pose from Orthography and Scaling with Iterations** (**POSIT**): This is
    a computer vision algorithm that performs 3D pose estimation.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从正写法和迭代缩放中提取姿态**（**POSIT**）：这是一个进行3D姿态估计的计算机视觉算法。'
- en: Active Appearance Models overview
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动外观模型概述
- en: In few words, Active Appearance Models are a nice model parameterization of
    combined texture and shape, coupled to an efficient search algorithm that can
    tell exactly where and how a model is located in a picture frame. In order to
    do this, we will start with the *Active Shape Models* section and see that they
    are more closely related to landmark positions. A Principal Component Analysis
    and some hands-on experience will be better described in the following sections.
    Then, we will be able to get some help from OpenCV's Delaunay functions and learn
    some triangulation. From that, we will evolve to applying piecewise affine warps
    in the triangle texture warping section, where we can get information from an
    object's texture.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，活动外观模型是结合纹理和形状的优良模型参数化，与一个高效的搜索算法相结合，该算法可以精确地告诉模型在图片框架中的位置和姿态。为了做到这一点，我们将从“活动形状模型”部分开始，并看到它们与地标位置更为紧密相关。主成分分析和一些实践经验将在以下部分中更好地描述。然后，我们将能够利用OpenCV的Delaunay函数和了解一些三角剖分。从那以后，我们将发展到在三角形纹理变形部分应用分片仿射变形，在那里我们可以从物体的纹理中获取信息。
- en: As we get enough background to build a good model, we can play with the techniques
    in the model instantiation section. We will then be able to solve the inverse
    problem through AAM search and fitting. These, by themselves, are already very
    useful algorithms for 2D and maybe even 3D image matching. However, when one is
    able to get it to work, why not bridge it to **POSIT** (**Pose from Orthography
    and Scaling with Iterations**), another rock-solid algorithm for 3D model fitting?
    Diving into the POSIT section will give us enough background to work with it in
    OpenCV, and you will then learn how to couple a head model to it, in the following
    section. This way, we can use a 3D model to fit the already matched 2D frame.
    If you want to know where this will take us, it is just a matter of combining
    AAM and POSIT in a frame-by-frame fashion to get real-time 3D tracking by detection
    for deformable models! These details will be covered in the tracking from the
    webcam or video file section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们收集到足够的背景来构建一个好的模型时，我们可以在模型实例化部分玩转这些技术。然后，我们可以通过AAM搜索和拟合来解决逆问题。这些本身就是非常有用的算法，用于2D甚至可能是3D图像匹配。然而，当一个人能够让它工作的时候，为什么不将其与**POSIT**（**通过迭代进行正射投影和缩放姿态**）联系起来，这是另一个用于3D模型拟合的稳固算法呢？深入研究POSIT部分将为我们提供足够的信息，以便在OpenCV中使用它，你将在以下部分中学习如何将其与头模型结合。这样，我们可以使用3D模型来拟合已经匹配的2D框架。如果你想知道这将把我们带到哪里，那只是将AAM和POSIT逐帧结合，以获得变形模型的实时3D检测跟踪！这些细节将在从网络摄像头或视频文件进行跟踪的部分中介绍。
- en: 'It is said that a picture is worth a thousand words; imagine if we get *N*
    pictures. This way, what we previously mentioned is easily tracked in the following
    screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 据说“一图胜千言”；想象一下如果我们得到*N*张图片。这样，我们之前提到的内容在以下截图中将很容易追踪：
- en: '![](img/image_06_001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_001.png)'
- en: Overview of the chapter algorithms
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 章节算法概述
- en: Given an image (upper-left image in the preceding screenshot), we can use an
    Active Appearance search algorithm to find the 2D pose of the human head. The
    top-right figure in the screenshot shows a previously trained Active Appearance
    model used in the search algorithm. After a pose has been found, POSIT can be
    applied to extend the result to a 3D pose. If the procedure is applied to a video
    sequence, 3D tracking by detection will be obtained.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张图像（前一张截图中的左上角图像），我们可以使用活动外观搜索算法来找到人头部的2D姿态。截图右上角的图像显示了用于搜索算法的先前训练好的活动外观模型。找到姿态后，可以使用POSIT来扩展结果到3D姿态。如果将此过程应用于视频序列，将获得检测的3D跟踪。
- en: Active Shape Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动形状模型
- en: 'As mentioned earlier, AAMs require a shape model, and this role is played by
    Active Shape Models (ASMs). In the upcoming sections, we will create an ASM that
    is a statistical model of shape variation. The shape model is generated through
    the combination of shape variations. A training set of labeled images is required,
    as described in the article *Active Shape Models--Their Training and Application*,
    by Timothy Cootes. In order to build a face-shape model, several images marked
    with points on key positions of a face are required to outline the main features.
    The following screenshot shows such an example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AAMs需要形状模型，而这个角色由主动形状模型（ASMs）扮演。在接下来的章节中，我们将创建一个ASM，它是一个形状变化的统计模型。形状模型是通过形状变化的组合生成的。需要一组标记的图像训练集，如Timothy
    Cootes在文章《主动形状模型--它们的训练和应用》中所述。为了构建人脸形状模型，需要几幅在人脸关键位置标记点的图像来描绘主要特征。以下截图显示了这样一个例子：
- en: '![](img/image_06_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_002.jpg)'
- en: There are 76 landmarks on a face, which are taken from the **MUCT** dataset.
    These landmarks are usually marked up by hand, and they outline several face features
    such as mouth contour, nose, eyes, eyebrows, and face shape, since they are easier
    to track.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸上有76个标记点，这些点来自**MUCT**数据集。这些标记点通常手动标记，它们描绘了几个面部特征，如嘴巴轮廓、鼻子、眼睛、眉毛和面部形状，因为它们更容易追踪。
- en: '**Procrustes Analysis**: A form of statistical shape analysis used to analyze
    the distribution of a set of shapes. Procrustes superimposition is performed by
    optimally translating, rotating, and uniformly scaling the objects.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Procrustes分析**：一种用于分析形状集分布的统计形状分析方法。Procrustes叠加通过最优地平移、旋转和均匀缩放对象来实现。'
- en: 'If we have the previously mentioned set of images, we can generate a statistical
    model of shape variation. Since the labeled points on an object describe the shape
    of that object, we will first align all the sets of points into a coordinate frame
    using Procrustes Analysis, if required, and represent each shape by a vector,
    *x*. Then, we will apply Principal Component Analysis to the data. We can then
    approximate any example using the following formula:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有前面提到的图像集，我们可以生成形状变化的统计模型。由于物体上的标记点描述了该物体的形状，我们将首先使用Procrustes分析将所有点集对齐到一个坐标系中，如果需要的话，然后通过一个向量*x*来表示每个形状。然后，我们将对数据进行主成分分析。然后我们可以使用以下公式来近似任何示例：
- en: '*x = x + Ps bs*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*x = x + Ps bs*'
- en: In the preceding formula, *x* is the mean shape, *Ps* is a set of orthogonal
    modes of variation, and *bs* is a set of shape parameters. Well, in order to understand
    this better, we will create a simple application in the rest of this section,
    which will show us how to deal with PCA and shape models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*x*是平均形状，*Ps*是一组正交变化模式，*bs*是一组形状参数。好吧，为了更好地理解这一点，我们将在本节的其余部分创建一个简单的应用程序，它将向我们展示如何处理PCA和形状模型。
- en: 'Why use PCA at all? Because PCA is going to really help us when it comes to
    reducing the number of parameters of our model. We will also see how much that
    helps when searching for it in a given image later in this chapter. The following
    is said about PCA ([http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么一定要使用PCA呢？因为PCA在减少我们模型参数数量时将真正帮助我们。我们还将看到，在本章稍后搜索给定图像时它能帮我们多少。以下是对PCA的描述（[http://en.wikipedia.org/wiki/Principal_component_analysis](http://en.wikipedia.org/wiki/Principal_component_analysis)）：
- en: PCA can supply the user with a lower-dimensional picture, a *shadow* of this
    object when viewed from its (in some sense) most informative viewpoint. This is
    done by using only the first few principal components so that the dimensionality
    of the transformed data is reduced.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）可以为用户提供一个低维度的图像，从其（在某种意义上）最有信息量的视角看，这是该对象的一个“影子”。这是通过仅使用前几个主成分来实现的，从而降低了变换数据的维度。
- en: 'This becomes clear when we see the following figure:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看到以下图时，这一点变得清晰：
- en: '![](img/image_06_003.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_003.png)'
- en: 'Image source: [h t t p ://e n . w i k i p e d i a . o r g /w i k i /F i l e
    :G a u s s i a n S c a t t e r P C A . p n g](http://en.wikipedia.org/wiki/File:GaussianScatterPCA.png)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[h t t p ://e n . w i k i p e d i a . o r g /w i k i /F i l e :G a u s
    s i a n S c a t t e r P C A . p n g](http://en.wikipedia.org/wiki/File:GaussianScatterPCA.png)
- en: The preceding figure shows the PCA of a multivariate Gaussian distribution centered
    at *(2,3)*. The vectors shown are the eigenvectors of the covariance matrix, shifted
    so their tails are at the mean.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了以*(2,3)*为中心的多变量高斯分布的PCA。显示的向量是协方差矩阵的特向量，它们的尾巴被移到了均值处。
- en: This way, if we wanted to represent our model with a single parameter, taking
    the direction from the eigenvector that points to the upper-right part of the
    screenshot would be a good idea. Besides, by varying the parameter a bit, we can
    extrapolate data and get values similar to the ones we are looking for.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，如果我们想用一个单参数来表示我们的模型，从指向截图右上角的方向的特向量取方向会是一个好主意。此外，通过稍微改变参数，我们可以外推数据并获得类似我们正在寻找的值。
- en: Getting the feel of PCA
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解PCA
- en: In order to get a feeling of how PCA could help us with our face model, we will
    start with an Active Shape Model and test some parameters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解PCA如何帮助我们构建面部模型，我们将从一个主动形状模型开始，并测试一些参数。
- en: Since face detection and tracking have been studied for a while, several face
    databases are available online for research purposes. We will use a couple of
    samples from the IMM database.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于面部检测和跟踪已经被研究了一段时间，因此在线上可以找到几个用于研究目的的面部数据库。我们将使用IMM数据库中的几个样本。
- en: First, let's understand how the PCA class works in OpenCV. We can conclude from
    the documentation that the PCA class is used to compute a special basis for a
    set of vectors, which consist of eigenvectors of the covariance matrix computed
    from the input set of vectors. This class can also transform vectors to and from
    the new coordinate space, using project and **backproject** methods. This new
    coordinate system can be quite accurately approximated by taking just the first
    few of its components. This means, we can represent the original vector from a
    high-dimensional space with a much shorter vector consisting of the projected
    vector's coordinates in the subspace.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解PCA类在OpenCV中的工作原理。我们可以从文档中得出结论，PCA类用于计算一组向量的特殊基，这些基是来自输入向量集的协方差矩阵的主元向量。此类还可以使用投影和**反向投影**方法将向量转换到新坐标系中。这个新坐标系可以通过仅取其前几个分量来相当准确地近似。这意味着，我们可以用一个由投影向量在子空间中的坐标组成的更短的向量来表示来自高维空间的原向量。
- en: Since we want a parameterization in terms of a few scalar values, the main method
    we will use from the class is the backproject method. It takes principal component
    coordinates of projected vectors and reconstructs the original ones. We could
    retrieve the original vectors if we retained all the components, but the difference
    will be very small if we just use a couple of components; that's one of the reasons
    for using PCA. Since we want some variability around the original vectors, our
    parameterized scalars will be able to extrapolate the original data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望用几个标量值来参数化，我们将主要使用类中的反向投影方法。它接受投影向量的主成分坐标并重建原始向量。如果我们保留了所有分量，我们可以检索原始向量，但如果只使用几个分量，差异将非常小；这就是使用PCA的原因之一。由于我们希望在原始向量周围有一些可变性，我们的参数化标量将能够外推原始数据。
- en: Besides, the PCA class can transform vectors to and from the new coordinate
    space, defined by the basis. Mathematically, it means that we compute projection
    of the vector to a subspace formed by a few eigenvectors corresponding to the
    dominant eigenvalues of the covariance matrix, as one can see from the documentation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PCA类可以将向量转换到由基定义的新坐标系中，并从该坐标系转换回来。从数学上讲，这意味着我们计算向量的投影，该投影是协方差矩阵的主元向量形成的子空间，正如文档中所示。
- en: 'Our approach will be annotating our face images with landmarks yielding a training
    set for our **Point Distribution Model** (**PDM**). If we have *k*-aligned landmarks
    in two dimensions, our shape description will look like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法是将我们的面部图像标注上地标，从而得到用于我们**点分布模型**（**PDM**）的训练集。如果我们有两个维度的*k*个对齐地标，我们的形状描述将看起来像这样：
- en: '*X = { x1, y1, x2, y2, ..., xk, yk}*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*X = { x1, y1, x2, y2, ..., xk, yk}*'
- en: It's important to note that we need consistent labeling across all image samples.
    So, for instance, if the left part of the mouth is landmark number *3* in the
    first image, it will need to be number *3* in all other images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们需要对所有图像样本进行一致的标注。例如，如果第一幅图中嘴巴的左侧是地标编号*3*，那么在其他所有图像中它也必须是编号*3*。
- en: These sequences of landmarks will now form the shape outlines, and a given training
    shape can be defined as a vector. We generally assume this scattering is Gaussian
    in this space, and we use PCA to compute normalized eigenvectors and eigenvalues
    of the covariance matrix across all training shapes. Using the top-center eigenvectors,
    we will create a matrix of dimensions *2k * m*, which we will call *P*. This way,
    each eigenvector describes a principal mode of variation along the set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征点的序列将现在形成形状轮廓，给定的训练形状可以定义为向量。我们通常假设在这个空间中这种散布是高斯分布的，我们使用PCA来计算所有训练形状协方差矩阵的归一化特征向量和特征值。使用顶部的中心特征向量，我们将创建一个*2k
    * m*维度的矩阵，我们将称之为*P*。这样，每个特征向量描述了沿集合的变异的主模式。
- en: 'Now, we can define a new shape through the following equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下方程定义一个新的形状：
- en: '*X'' = X'' + Pb*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*X'' = X'' + Pb*'
- en: Here, *X'* is the mean shape across all training images--we just average each
    of the landmarks--and *b* is a vector of scaling values for each principal component.
    This leads us to create a new shape modifying the value of *b*. It's common to
    set *b* to vary within three standard deviations so that the generated shape can
    fall inside the training set.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X'*是所有训练图像的平均形状——我们只是平均每个特征点——而*b*是每个主成分的缩放值向量。这导致我们创建一个新的形状，通过修改*b*的值来实现。通常将*b*设置为在三个标准差内变化，以便生成的形状可以落在训练集内。
- en: 'The following screenshot shows point-annotated mouth landmarks for three different
    pictures:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了三张不同图片的点标注嘴部特征点：
- en: '![](img/image_06_004.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_004.png)'
- en: As can be seen in the preceding screenshot, the shapes are described by their
    landmark sequences. One could use a program such as *GIMP* or *ImageJ* as well
    as building a simple application in OpenCV in order to annotate the training images.
    We will assume the user has completed this process and saved the points as sequences
    of *x* and *y* landmark positions for all training images in a text file, which
    will be used in our PCA analysis. We will then add two parameters to the first
    line of this file, which is the number of training images and the number of read
    columns. So, for *k* 2D points, this number will be *2*k*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个截图所示，形状是通过其特征点序列来描述的。可以使用像*GIMP*或*ImageJ*这样的程序，也可以在OpenCV中构建一个简单的应用程序来标注训练图像。我们假设用户已经完成了这个过程，并将点保存为所有训练图像的*x*和*y*特征点位置的序列，这些序列将被用于我们的PCA分析。然后，我们将在这个文件的第一个行添加两个参数，即训练图像的数量和读取列的数量。因此，对于*k*个2D点，这个数字将是*2*k*。
- en: 'In the following data, we have an instance of this file, which was obtained
    through the annotation of three images from IMM database, in which *k* is equal
    to 5:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下数据中，我们有一个通过标注IMM数据库中的三个图像获得的该文件实例，其中*k*等于5：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have annotated images, let''s turn this data into our shape model.
    First, load this data into a matrix. This will be achieved through the `loadPCA`
    function. The following code snippet shows the use of the `loadPCA` function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经标注了图像，让我们将此数据转换为我们的形状模型。首先，将此数据加载到矩阵中。这将通过`loadPCA`函数实现。以下代码片段显示了`loadPCA`函数的使用：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that our matrix is created in the `pcaset = Mat::eye(rows,cols,CV_64F)`
    line and that enough space is allocated for *2*k* values. After the two `for`
    loops load the data into the matrix, the PCA constructor is called with the data,
    an empty matrix, that could be our precomputed mean vector, if we wish to make
    it only once. We also indicate that our vectors will be stored as matrix rows
    and that we wish to keep the same number of given rows as the number of components,
    though we could use just a few ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的矩阵是在`pcaset = Mat::eye(rows,cols,CV_64F)`这一行创建的，并且为*2*k*个值分配了足够的空间。在两个`for`循环将数据加载到矩阵之后，我们使用数据、一个空矩阵（如果愿意，可以是我们预先计算的平均向量）调用PCA构造函数，如果只想计算一次。我们还指出，我们的向量将被存储为矩阵行，并且我们希望保持与成分数量相同的给定行数，尽管我们也可以只使用少数几个。
- en: 'Now that we have filled our PCA object with our training set, it has everything
    it needs to back project our shape according to the parameters. We do so by invoking
    `PCA.backproject`, passing the parameters as a row vector, and receiving the back
    projected vector into the second argument:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经用我们的训练集填充了PCA对象，它拥有了根据参数回投影形状所需的一切。我们通过调用`PCA.backproject`，将参数作为行向量传递，并将回投影向量作为第二个参数接收来实现这一点：
- en: '![](img/image_06_005.jpg)![](img/image_06_006.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_005.jpg)![图片](img/image_06_006.jpg)'
- en: The two previous screenshots show two different shape configurations according
    to the selected parameters chosen from the slider. The yellow and green shapes
    show training data, while the red one reflects the shape generated from the chosen
    parameters. A sample program can be used to experiment with Active Shape Models,
    as it allows the user to try different parameters for the model. One is able to
    note that varying only the first two scalar values through the slider (which correspond
    to the first and second modes of variation), we can achieve a shape that is very
    close to the trained ones. This variability will help us when searching for a
    model in AAM, since it provides interpolated shapes. We will discuss triangulation,
    texturing, AAM, and AAM search in the following sections.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 两个之前的截图显示了根据从滑块选择的参数选择的两个不同的形状配置。黄色和绿色的形状表示训练数据，而红色形状反映了从所选参数生成的形状。可以使用一个示例程序来实验Active
    Shape Models，因为它允许用户尝试为模型尝试不同的参数。可以注意到，通过滑动条仅改变前两个标量值（对应于第一和第二种变化模式），我们可以得到一个非常接近训练形状的形状。这种可变性将帮助我们寻找AAM中的模型，因为它提供了插值形状。我们将在以下章节中讨论三角剖分、纹理、AAM和AAM搜索。
- en: Triangulation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三角剖分
- en: 'As the shape we are looking for might be distorted, such as an open mouth for
    instance, we are required to map our texture back to a mean shape and then apply
    PCA to this normalized texture. In order to do this, we will use triangulation.
    The concept is very simple: we will create triangles including our annotated points
    and then map from one triangle to another. OpenCV comes with a handy class called
    `Subdiv2D`, which deals with Delaunay Triangulation. You can just consider this
    a good triangulation that will avoid skinny triangles.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们寻找的形状可能会发生扭曲，例如开口的嘴巴，我们需要将我们的纹理映射回一个平均形状，然后对这种归一化纹理应用PCA。为了做到这一点，我们将使用三角剖分。这个概念非常简单：我们将创建包含我们的标注点的三角形，然后从一个三角形映射到另一个三角形。OpenCV附带一个名为`Subdiv2D`的便捷类，用于处理Delaunay三角剖分。你可以将其视为一种好的三角剖分，它将避免瘦三角形。
- en: In mathematics and computational geometry, a Delaunay Triangulation for a set
    *P* of points in a plane is a triangulation DT(P) such that no point in *P* is
    inside the circumcircle of any triangle in DT(P). Delaunay Triangulations maximize
    the minimum angle of all the angles of the triangles in the triangulation; they
    tend to avoid skinny triangles. The triangulation is named after Boris Delaunay
    for his work on this topic from 1934 onwards.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学和计算几何中，对于平面上点集*P*的Delaunay三角剖分是一个三角剖分DT(P)，其中*P*中的任何点都不在DT(P)中任何三角形的外接圆内。Delaunay三角剖分最大化了三角剖分中所有三角形的最小角度；它们倾向于避免瘦三角形。这个三角剖分是以Boris
    Delaunay的名字命名的，因为他从1934年开始在这个领域的工作。
- en: 'After a Delaunay subdivision has been created, one will use the `insert` member
    function to populate points into the subdivision. The following lines of code
    will elucidate what a direct use of triangulation would be like:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了一个Delaunay细分之后，将使用`insert`成员函数将点填充到细分中。以下代码行将阐明直接使用三角剖分会是什么样的：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that our points are going to be inside a rectangular frame that is passed
    as a parameter to `Subdiv2D`. In order to create a subdivision, we need to instantiate
    the `Subdiv2D` class, as seen earlier. Then, in order to create the triangulation,
    we need to insert points using the insert method from `Subdiv2D`. This happens
    inside the `for` loop in the preceding code. Note that the points should already
    have been initialized, since they are the ones we''ll usually be using as inputs.
    The following diagram shows what the triangulation could look like:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的点将位于一个矩形框架内，该框架作为参数传递给`Subdiv2D`。为了创建一个细分，我们需要实例化`Subdiv2D`类，如前所述。然后，为了创建三角剖分，我们需要使用`Subdiv2D`的`insert`方法插入点。这发生在前述代码中的`for`循环内。注意，点应该已经初始化，因为它们是我们通常用作输入的点。以下图显示了三角剖分可能的样子：
- en: '![](img/image_06_007.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_007.png)'
- en: This diagram is the output of the preceding code for a set of points that yield
    the triangulation using Delaunay algorithm.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此图是前述代码对一组使用Delaunay算法生成三角剖分的点的输出。
- en: 'In order to iterate through all the triangles from a given subdivision, one
    can use the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了遍历给定细分中的所有三角形，可以使用以下代码：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Given a subdivision, we will initialize its `triangleList` through a `Vec6f`
    vector, which will save space for each set of three points, which can be obtained
    iterating `triangleList`, as shown in the preceding `for` loop.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个细分，我们将通过一个`Vec6f`向量初始化其`triangleList`，这将为每组三个点节省空间，可以通过遍历`triangleList`获得，如前述`for`循环所示。
- en: Triangle texture warping
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三角纹理扭曲
- en: 'Now that we''ve been able to iterate through the triangles of a subdivision,
    we are able to warp one triangle from an original annotated image into a generated
    distorted one. This is useful for mapping the texture from the original shape
    to a distorted one. The following piece of code will guide the process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够遍历细分三角形的各个部分，我们能够将原始标注图像中的一个三角形扭曲成生成的扭曲图像。这对于将纹理从原始形状映射到扭曲形状非常有用。以下代码片段将指导这个过程：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code assumes we have the triangle vertices packed in the `srcTri`
    array and the destination one packed in the `dstTri` array. The 2x3 `warp_mat`
    matrix is used to get the Affine transformation from the source triangles to the
    destination ones. More information can be quoted from OpenCV''s *cvGetAffineTransform*
    documentation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码假设我们已经有三角形的顶点打包在`srcTri`数组中，目标顶点打包在`dstTri`数组中。2x3的`warp_mat`矩阵用于从源三角形到目标三角形的仿射变换。更多信息可以参考OpenCV的`cvGetAffineTransform`文档：
- en: 'The `cvGetAffineTransform` function calculates the matrix of an affine transform
    in the following way:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`cvGetAffineTransform`函数以以下方式计算仿射变换的矩阵：'
- en: '![](img/image_06_008.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_008.png)'
- en: In the preceding equation, destination *(i)* is equal to (*xi',yi'*), source
    *(i)* is equal to (*xi, yi*), and *i* is equal to *0, 1, 2*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，目标*(i)*等于(*xi',yi'*)，源*(i)*等于(*xi, yi*)，而*i*等于*0, 1, 2*。
- en: After retrieving the affine matrix, we can apply the Affine transformation to
    the source image. This is done through the `warpAffine` function. Since we don't
    want to do it in the entire image, we want to focus on our triangle, a mask can
    be used for this task. This way, the last line copies only the triangle from our
    original image with the mask we just created, which was made through a `cvFillConvexPoly`
    call.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索到仿射矩阵后，我们可以将仿射变换应用于源图像。这是通过`warpAffine`函数完成的。由于我们不想在整个图像上执行此操作，我们只想关注我们的三角形，因此可以使用掩码来完成此任务。这样，最后一行只复制我们原始图像中的三角形，使用我们刚刚创建的掩码，该掩码是通过`cvFillConvexPoly`调用来创建的。
- en: 'The following screenshot shows the result of applying this procedure to every
    triangle in an annotated image. Note that the triangles are mapped back to the
    alignment frame, which faces toward the viewer. This procedure is used to create
    the statistical texture of the AAM:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将此过程应用于标注图像中每个三角形的结果。请注意，三角形被映射回面向观察者的对齐框架。此过程用于创建AAM的统计纹理：
- en: '![](img/image_06_009.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_009.png)'
- en: The preceding screenshot shows the result of warping all the mapped triangles
    in the left image to a mean reference frame.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了将左图中所有映射的三角形扭曲到平均参考框架的结果。
- en: Model Instantiation - playing with the AAM
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型实例化 - 玩转AAM
- en: 'An interesting aspect of AAMs is their ability to easily interpolate the model
    that we trained our images on. We can get used to their amazing representational
    power through the adjustment of a couple of shape or model parameters. As we vary
    shape parameters, the destination of our warp changes according to the trained
    shape data. On the other hand, while appearance parameters are modified, the texture
    on the base shape is modified. Our warp transforms will take every triangle from
    the base shape to the modified destination shape so that we can synthesize a closed
    mouth on top of an open mouth, as shown in the following screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: AAMs的一个有趣方面是它们能够轻松地插值我们训练图像上的模型。我们可以通过调整几个形状或模型参数来习惯它们的惊人表达能力。当我们改变形状参数时，我们的扭曲目标会根据训练的形状数据变化。另一方面，当外观参数被修改时，基础形状上的纹理也会被修改。我们的扭曲变换将把基础形状中的每个三角形转换到修改后的目标形状，这样我们就可以在张开嘴的上方合成一个闭嘴，如下面的截图所示：
- en: '![](img/image_06_010.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_06_010.jpg)'
- en: This preceding screenshot shows a synthesized closed mouth obtained through
    Active Appearance Model instantiation on top of another image. It shows how one
    could combine a smiling mouth with an admired face, extrapolating the trained
    images.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了通过在另一图像上对Active Appearance Model进行实例化而获得的合成闭嘴。它显示了如何将微笑的嘴巴与受人钦佩的面孔结合在一起，外推训练图像。
- en: The preceding screenshot was obtained by changing only three parameters for
    shape and three for the texture, which is the goal of AAMs. A sample application
    has been developed and is available at [http://www.packtpub.com/](http://www.packtpub.com/)
    for you to try out AAM. Instantiating a new model is just a question of sliding
    the equation parameters, as defined in the *Getting the feel of PCA* section.
    You should note that AAM search and fitting rely on this flexibility to find the
    best match for a given captured frame of our model in a different position from
    the trained ones. We will see this in the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图是通过仅更改形状和纹理的三个参数获得的，这是 AAM 的目标。已经开发了一个示例应用程序，可在 [http://www.packtpub.com/](http://www.packtpub.com/)
    上试用 AAM。实例化新模型只是滑动方程参数的问题，如 *获取 PCA 感觉* 部分中定义的那样。你应该注意，AAM 搜索和拟合依赖于这种灵活性，以找到与训练位置不同的给定捕获帧的最佳匹配。我们将在下一节中看到这一点。
- en: AAM search and fitting
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AAM 搜索和拟合
- en: 'With our fresh, new combined shape and texture model, we have found a nice
    way to describe how a face could change not only in shape, but also in appearance.
    Now, we want to find which set of *p* shape and *λ* appearance parameters will
    bring our model as close as possible to a given input image *I(x)*. We could naturally
    calculate the error between our instantiated model and the given input image in
    the coordinate frame of *I(x)*, or map the points back to the base appearance
    and calculate the difference there. We are going to use the latter approach. This
    way, we want to minimize the following function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们新鲜、新的组合形状和纹理模型，我们找到了一种描述面部如何不仅在形状上，而且在外观上发生变化的好方法。现在，我们想要找到哪组 *p* 形状和 *λ*
    外观参数将使我们的模型尽可能接近给定的输入图像 *I(x)*。我们可以在 *I(x)* 的坐标系中自然地计算我们的实例化模型和给定输入图像之间的误差，或者将点映射回基础外观并计算那里的差异。我们将采用后一种方法。这样，我们想要最小化以下函数：
- en: '![](img/image_06_011.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_011.png)'
- en: In the preceding equation, *S0* denotes the set of pixels *x* is equal to *(x,y)T*
    that lie inside the AAMs base mesh, *A0(x)* is our base mesh texture, *Ai(x)*
    is appearance images from PCA, and *W(x;p)* is the warp that takes pixels from
    the input image back to the base mesh frame.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*S0* 表示像素 *x* 等于 *(x,y)T* 且位于 AAM 基础网格内部的像素集，*A0(x)* 是我们的基础网格纹理，*Ai(x)*
    是来自 PCA 的外观图像，而 *W(x;p)* 是将像素从输入图像返回到基础网格框架的变形。
- en: 'Several approaches have been proposed for this minimization through years of
    studying. The first idea was to use an additive approach, in which *∆pi* and *∆λi*
    were calculated as linear functions of the error image and then the shape parameter
    *p* and appearance *λ* were updated as *pi ← pi + ∆pi* and *λi ← λi + ∆λi*, in
    the iteration. Although convergence can occur sometimes, the delta doesn''t always
    depend on current parameters, and this might lead to divergence. Another approach,
    which was studied based on the gradient descent algorithms, was very slow, so
    another way of finding convergence was sought. Instead of updating the parameters,
    the whole warp could be updated. This way, a compositional approach was proposed
    by Ian Mathews and Simon Baker in a famous paper called *Active Appearance Models
    Revisited*. More details can be found in the paper, but the important contribution
    it gave to fitting was that it brought the most intensive computation to a pre-compute
    step, as seen in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多年的研究，已经提出了几种用于这种最小化的方法。最初的想法是使用一种加性方法，其中 *∆pi* 和 *∆λi* 被计算为误差图像的线性函数，然后形状参数
    *p* 和外观 *λ* 通过迭代更新为 *pi ← pi + ∆pi* 和 *λi ← λi + ∆λi*。尽管有时可以发生收敛，但增量并不总是依赖于当前参数，这可能会导致发散。另一种基于梯度下降算法的研究方法非常慢，因此寻求另一种找到收敛的方法。不是更新参数，而是整个变形可以更新。这种方式，Ian
    Mathews 和 Simon Baker 在一篇名为 *Active Appearance Models Revisited* 的著名论文中提出了一种组合方法。更多细节可以在论文中找到，但它在拟合方面的重要贡献是将最密集的计算带到了预计算步骤，正如以下截图所示：
- en: '![](img/image_06_012.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_012.png)'
- en: 'Note that the update occurs in terms of a compositional step as seen in step
    (9) (see the previous screenshot). Equations (40) and (41) from the paper can
    be seen in the following screenshots:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，更新是通过组合步骤发生的，正如步骤（9）所示（参见前面的截图）。以下截图显示了论文中的方程（40）和（41）：
- en: '![](img/image_06_013.png)![](img/image_06_014.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_013.png)![](img/image_06_014.png)'
- en: 'Although the algorithm just mentioned will mostly converge very well from a
    position near the final one, this might not be the case when there''s a big difference
    in rotation, translation, or scale. We can bring more information to the convergence
    through the parameterization of a global 2D similarity transform. This is equation
    *42* in the paper and is shown as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面提到的算法将从接近最终位置的位置开始大多数情况下都会很好地收敛，但在旋转、平移或比例有较大差异的情况下可能并非如此。我们可以通过全局2D相似变换的参数化来引入更多信息以促进收敛。这是论文中的方程*42*，如下所示：
- en: '![](img/image_06_015.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_015.png)'
- en: 'In the preceding equation, the four parameters *q* = (*a*, *b*, *t[x]*, *t[y]*)
    have the following interpretations. The &filig;rst pair (*a*, *b*) is related
    to the scale *k* and rotation *θ: a* is equal to k *cos θ - 1* and *b = k sin
    θ*. The second pair (*t[x]*, *t[y]*) is the *x* and *y* translations, as proposed
    in the *Active Appearance Models Revisited* paper.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，四个参数*q* = (*a*, *b*, *t[x]*, *t[y]*)有以下解释。第一对(*a*, *b*)与比例*k*和旋转*θ*相关：*a*等于*k*
    *cos θ - 1*，而*b = k sin θ*。第二对(*t[x]*, *t[y]*)是*x*和*y*平移，如*Active Appearance Models
    Revisited*论文中提出的那样。
- en: With a bit more of math transformations, you can finally use the preceding algorithm
    to find the best image fit with a global 2D transform.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更多的数学变换，您最终可以使用前面的算法通过全局2D变换找到最佳图像拟合。
- en: 'As the warp compositional algorithm has several performance advantages, we
    will use the one described in the AAM Revisited paper: the *inverse compositional
    project-out algorithm*. Remember that in this method, the effect of appearance
    variation during fitting can be precomputed, or projected out, improving AAM fitting
    performance.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于组合变换算法具有几个性能优势，我们将使用AAM Revisited论文中描述的算法：*逆组合投影算法*。记住，在这个方法中，拟合过程中外观变化的影响可以预先计算或投影出去，从而提高AAM拟合性能。
- en: 'The following screenshot shows convergence for different images from the MUCT
    dataset using the inverse compositional project-out AAM fitting algorithm:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了使用逆组合投影和我们的AAM拟合算法在MUCT数据集的不同图像上的收敛情况：
- en: '![](img/image_06_016.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_016.jpg)'
- en: The preceding screenshot shows successful convergences, over faces outside the
    AAM training set-using the inverse compositional project, out AAM fitting algorithm.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了使用逆组合投影和我们的AAM拟合算法在AAM训练集外部面上成功收敛的情况。
- en: POSIT
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: POSIT
- en: After we have found the 2D position of our landmark points, we can derive the
    3D pose of our model using the POSIT. The pose *P* of a 3D object is defined as
    the 3 x 3 rotation matrix *R* and the 3D translation vector *T*; hence, *P* is
    equal to *[ R | T ]*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们找到我们的特征点的2D位置后，我们可以使用POSIT推导出我们模型的3D姿态。3D对象的位置*P*定义为3 x 3旋转矩阵*R*和3D平移向量*T*；因此，*P*等于*[
    R | T ]*。
- en: Most of this section is based on the *OpenCV POSIT* tutorial by Javier Barandiaran.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的大部分内容基于Javier Barandiaran的*OpenCV POSIT*教程。
- en: As the name implies, POSIT uses the **Pose from Orthography and Scaling** (**POS**)
    algorithm in several iterations, so it is an acronym for POS with iterations.
    The hypothesis for its working is that we can detect and match in the image four
    or more non-coplanar feature points of the object and that we know their relative
    geometry on the object.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，POSIT在多次迭代中使用**从正射投影和缩放中获取姿态**（**POS**）算法，因此它是POS的迭代缩写。其工作假设是我们可以检测和匹配图像中对象四个或更多非共面的特征点，并且我们知道它们在对象上的相对几何形状。
- en: The main idea of the algorithm is that we can find a good approximation to the
    object pose, supposing that all the model points are in the same plane, since
    their depths are not very different from one another if compared to the distance
    from the camera to a face. After the initial pose is obtained, the rotation matrix
    and translation vector of the object are found by solving a linear system. Then,
    the approximate pose is iteratively used to better compute scaled orthographic
    projections of the feature points, followed by POS application to these projections
    instead of the original ones. For more information, you can refer to the paper
    by DeMenton, *Model-Based Object Pose in 25 Lines of Code*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的主要思想是，假设所有模型点都在同一平面上，我们可以找到一个很好的对象姿态近似，因为它们的深度彼此之间并不非常不同，与相机到人脸的距离相比。在获得初始姿态后，通过求解线性系统找到对象的旋转矩阵和平移向量。然后，迭代地使用近似的姿态来更好地计算特征点的缩放正交投影，随后对这些投影而不是原始投影应用POS。有关更多信息，您可以参考DeMenton的论文，*25行代码中的基于模型的物体姿态*。
- en: Diving into POSIT
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入 POSIT
- en: In order for POSIT to work, you need at least four non-coplanar 3D model points
    and their respective matchings in the 2D image. We will add a termination criteria
    to that, since POSIT is an iterative algorithm, which generally is a number of
    iterations or a distance parameter. We will then call the `cvPOSIT` function,
    included in `calib3d_c.h`, which yields the rotation matrix and the translation
    vector.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 POSIT 能够工作，你需要至少四个非共面的 3D 模型点和它们在 2D 图像中的对应匹配。我们将为此添加一个终止条件，因为 POSIT 是一个迭代算法，通常是一定次数的迭代或距离参数。然后我们将调用包含在
    `calib3d_c.h` 中的 `cvPOSIT` 函数，该函数会得到旋转矩阵和平移向量。
- en: 'As an example, we will follow the tutorial from Javier Barandiaran, which uses
    POSIT to obtain the pose of a cube. The model is created with four points. It
    is initialized with the following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将遵循 Javier Barandiaran 的教程，该教程使用 POSIT 获取一个立方体的姿态。模型是通过四个点创建的。它使用以下代码初始化：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that the model itself is created with the `cvCreatePOSITObject` method,
    which returns a `CvPOSITObject` method that will be used in the `cvPOSIT` function.
    Be aware that the pose will be calculated referring to the first model point,
    which makes it a good idea to put it at the origin.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型本身是通过 `cvCreatePOSITObject` 方法创建的，该方法返回一个 `CvPOSITObject` 方法，该方法将在 `cvPOSIT`
    函数中使用。请注意，姿态计算将参照第一个模型点，这使得将其放置在原点是一个好主意。
- en: 'We then need to put the 2D image points in another vector. Remember that they
    must be put in the array in the same order that the model points were inserted
    in; this way, the *i^(th)* 2D image point matches the *i^(th)* 3D model point.
    A catch here is that the origin for the 2D image points is located at the center
    of the image, which might require you to translate them. You can insert the following
    2D image points (of course, they will vary according to the user''s matching):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将 2D 图像点放入另一个向量中。记住，它们必须按照模型点插入的顺序放入数组中；这样，第 *i* 个 2D 图像点与第 *i* 个 3D
    模型点匹配。这里的一个难点是，2D 图像点的原点位于图像中心，这可能需要你进行平移。你可以插入以下 2D 图像点（当然，它们将根据用户的匹配而变化）：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, you only need to allocate memory for the matrixes and create termination
    criteria, followed by a call to `cvPOSIT`, as shown in the following code snippet:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你只需要为矩阵分配内存，创建终止条件，然后调用 `cvPOSIT`，如下面的代码片段所示：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After the iterations, `cvPOSIT` will store the results in `rotation_matrix`
    and `translation_vector`. The following screenshot shows the inserted `srcImagePoints`
    with white circles as well as a coordinate axis showing the rotation and translation
    results:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代完成后，`cvPOSIT` 将结果存储在 `rotation_matrix` 和 `translation_vector` 中。以下截图显示了插入的
    `srcImagePoints` 以及显示旋转和平移结果的坐标轴：
- en: '![](img/image_06_017.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_017.png)'
- en: 'With reference to the preceding screenshot, let''s see the following input
    points and results of running the POSIT algorithm:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 参考前面的截图，让我们看看以下输入点和运行 POSIT 算法的结果：
- en: The white circles show input points, while the coordinate axes show the resulting
    model pose.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白色圆圈显示输入点，而坐标轴显示结果模型姿态。
- en: Make sure you use the focal length of your camera as obtained through a calibration
    process. You might want to check one of the calibration procedures available in
    the *Camera calibration* section in [Chapter 7](https://www.packtpub.com/sites/default/files/downloads/NaturalFeatureTrackingforAugmentedReality.pdf),
    *Natural Feature Tracking for Augmented Reality*. The current implementation of
    POSIT will only allow square pixels, so there won't be room for focal length in
    the *x* and *y* axes.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你使用通过校准过程获得的相机焦距。你可能想检查 [第 7 章](https://www.packtpub.com/sites/default/files/downloads/NaturalFeatureTrackingforAugmentedReality.pdf)
    中 *相机校准* 部分提供的校准过程之一，*自然特征跟踪增强现实*。当前 POSIT 的实现将只允许正方形像素，因此在 *x* 和 *y* 轴上不会有焦距的空间。
- en: 'Expect the rotation matrix in the following format:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望以下格式的旋转矩阵：
- en: '[rot[0] rot[1] rot[2]]'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[rot[0] rot[1] rot[2]]'
- en: '[rot[3] rot[4] rot[5]]'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[rot[3] rot[4] rot[5]]'
- en: '[rot[6] rot[7] rot[8]]'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[rot[6] rot[7] rot[8]]'
- en: 'The translation vector will be in the following format:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平移向量将具有以下格式：
- en: '[trans[0]]'
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[trans[0]]'
- en: '[trans[1]]'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[trans[1]]'
- en: '[trans[2]]'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[trans[2]]'
- en: POSIT and head model
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: POSIT 和头部模型
- en: 'In order to use POSIT as a tool for head pose, you will need to use a 3D head
    model. There is one available from the Institute of Systems and Robotics of the
    University of Coimbra and can be found at [http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp](http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp).
    Note that the model can be obtained from where it says:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将POSIT用作头部姿态的工具，你需要使用一个3D头部模型。可以从科英布拉大学系统与机器人研究所获得一个，可以在[http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp](http://aifi.isr.uc.pt/Downloads/OpenGL/glAnthropometric3DModel.cpp)找到。请注意，模型可以从它所说的位置获得：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The model can be seen in the following screenshot:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以在以下截图中看到：
- en: '![](img/image_06_018.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_018.png)'
- en: The preceding screenshot shows a 58-point 3D head model available for POSIT.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了可用于POSIT的58点3D头部模型。
- en: 'In order to get POSIT to work, the point corresponding to the 3D head model
    must be matched accordingly. Note that at least four non-coplanar 3D points and
    their corresponding 2D projections are required for POSIT to work, so these must
    be passed as parameters, pretty much as described in the *Diving into POSIT* section.
    Note that this algorithm is linear in terms of the number of matched points. The
    following screenshot shows how matching should be done:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使POSIT工作，必须相应地匹配对应于3D头部模型的点。请注意，至少需要四个非共面的3D点和它们对应的2D投影才能使POSIT工作，因此这些必须作为参数传递，基本上就像在*深入POSIT*部分所描述的那样。请注意，这个算法在匹配点数方面是线性的。以下截图显示了匹配应该如何进行：
- en: '![](img/image_06_019.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_06_019.png)'
- en: The preceding screenshot shows the correctly matched points of a 3D head model
    and an AAM mesh.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图显示了3D头部模型和AAM网格正确匹配的点。
- en: Tracking from webcam or video file
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从摄像头或视频文件进行跟踪
- en: 'Now that all the tools have been assembled to get 6 degrees of freedom head
    tracking, we can apply it to a camera stream or video file. OpenCV provides the
    `VideoCapture` class that can be used in the following manner (see the *Accessing
    the webcam* section in [Chapter 1](03913e76-ec18-4a31-875e-dfceca32d26f.xhtml),
    *Cartoonifier and Skin Changer for Raspberry Pi*, for more details):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有工具都已组装完毕，以实现6自由度的头部跟踪，我们可以将其应用于相机流或视频文件。OpenCV提供了`VideoCapture`类，可以按以下方式使用（有关更多详细信息，请参阅[第1章](03913e76-ec18-4a31-875e-dfceca32d26f.xhtml)的*访问摄像头*部分，*Raspberry
    Pi的卡通化皮肤变换器*）：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The algorithm works like this. A video capture is initialized through `VideoCapture
    cap(0)` so that the default webcam is used. Now that we have video capture working,
    we also need to load our trained Active Appearance Model, which will occur in
    the `loadPreviouslyTrainedAAM` pseudocode mapping. We will also load the 3D head
    model for POSIT and the mapping of landmark points to 3D head points in our mapping
    variable.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的工作原理如下。通过`VideoCapture cap(0)`初始化视频捕获，以便使用默认的摄像头。现在我们已经使视频捕获工作，我们还需要加载我们训练好的活动外观模型，这将在`loadPreviouslyTrainedAAM`伪代码映射中发生。我们还将加载用于POSIT的3D头部模型以及我们的映射变量中地标点到3D头部点的映射。
- en: After everything we need has been loaded, we will need to initialize the algorithm
    from a known pose, which is a known 3D position, known rotation, and a known set
    of AAM parameters. This could be made automatically through OpenCV's highly documented
    Haar features classifier face detector (more details in the *Face Detection* section
    of [Chapter 4](3f3bb6f3-e501-4491-b1f0-6cd7506a7b80.xhtml), *Non-rigid Face Tracking*,
    or in OpenCV's cascade classifier documentation), or we could manually initialize
    the pose from a previously annotated frame. A brute-force approach, which would
    be to run an AAM fitting for every rectangle, could also be used, since it would
    be very slow only during the first frame. Note that by initialization, we mean
    finding the 2D landmarks of the AAM through their parameters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载了所有需要的工具之后，我们需要从一个已知的姿态初始化算法，这是一个已知的3D位置、已知的旋转和一组已知的AAM参数。这可以通过OpenCV高度文档化的Haar特征分类器人脸检测器自动完成（更多细节请参阅[第4章](3f3bb6f3-e501-4491-b1f0-6cd7506a7b80.xhtml)的*人脸检测*部分，*非刚性人脸跟踪*，或在OpenCV的级联分类器文档中），或者我们可以从先前标注的帧中手动初始化姿态。一种暴力方法，即对每个矩形运行AAM拟合，也可以使用，因为它在第一帧中会非常慢。请注意，我们所说的初始化是指通过它们的参数找到AAM的2D特征点。
- en: When everything is loaded, we can iterate through the main loop delimited by
    the `while` loop. In this loop, we first query the next grabbed frame, and we
    then run an Active Appearance Model fit so that we can find landmarks on the next
    frame. Since the current position is very important at this step, we pass it as
    a parameter to the pseudocode function `performAAMSearch(pose,aam)`. If we find
    the current pose, which is signaled through error image convergence, we will get
    the next landmark positions, so we can provide them to POSIT. This happens in
    the following line, `applyPOSIT(new2DPose, headModel, mapping)`, where the new
    2D pose is passed as a parameter, as also our previously loaded `headModel` and
    the mapping. After that, we can render any 3D model in the obtained pose like
    a coordinate axis or an augmented reality model. As we have landmarks, more interesting
    effects can be obtained through model parameterization, such as opening a mouth
    or changing eyebrow position.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当一切加载完成后，我们可以遍历由`while`循环定义的主循环。在这个循环中，我们首先查询下一个捕获的帧，然后运行一个主动外观模型拟合，以便我们可以在下一帧上找到特征点。由于当前位置在这一步非常重要，所以我们将其作为参数传递给伪代码函数`performAAMSearch(pose,aam)`。如果我们找到当前姿态，这通过错误图像收敛来表示，我们将获得下一个特征点位置，因此我们可以将它们提供给POSIT。这发生在以下行，`applyPOSIT(new2DPose,
    headModel, mapping)`，其中新的2D姿态作为参数传递，以及我们之前加载的`headModel`和映射。之后，我们可以以坐标轴或增强现实模型的方式渲染获得的姿态中的任何3D模型。由于我们有特征点，可以通过模型参数化获得更多有趣的效果，例如张开嘴巴或改变眉毛位置。
- en: As this procedure relies on the previous pose for the next estimation, we could
    accumulate errors and diverge from head position. A workaround could be to reinitialize
    the procedure every time it happens, checking a given error image threshold. Another
    factor to pay attention to is the use of filters when tracking, since jittering
    can occur. A simple mean filter for each of the translation and rotation coordinates
    can give reasonable results.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此过程依赖于前一个姿态进行下一个估计，我们可能会累积误差并偏离头部位置。一种解决方案是在每次发生时重新初始化此过程，检查给定的错误图像阈值。另一个需要注意的因素是在跟踪时使用过滤器，因为可能会发生抖动。对于每个平移和旋转坐标的简单均值滤波器可以给出合理的结果。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how Active Appearance Models can be combined with
    the POSIT algorithm in order to obtain a 3D head pose. An overview on how to create,
    train, and manipulate AAMs has been given, and you can use this background for
    any other field, such as medical, imaging, or industry. Besides dealing with AAMs,
    we got familiar with Delaunay subdivisions and learned how to use such an interesting
    structure as a triangulated mesh. We also showed you how to perform texture mapping
    in the triangles using OpenCV functions. Another interesting topic was approached
    in AAM fitting. Although only the inverse compositional project-out algorithm
    was described, we could easily obtain the results of years of research by simply
    using its output.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何将主动外观模型（AAM）与POSIT算法结合以获得3D头部姿态。我们概述了如何创建、训练和操作AAM，并且你可以将这些背景知识应用于任何其他领域，例如医学、成像或工业。除了处理AAM之外，我们还熟悉了Delaunay细分，并学习了如何使用这种有趣的结构作为三角网格。我们还展示了如何使用OpenCV函数在三角形上执行纹理映射。在AAM拟合中，我们还探讨了一个有趣的话题。尽管只描述了逆合成投影算法，但我们可以通过简单地使用其输出轻松地获得多年研究的结果。
- en: After enough theory and practice of AAMs, we dived into the details of POSIT
    in order to couple 2D measurements to 3D ones, explaining how to fit a 3D model
    using matchings between model points. We concluded the chapter by showing how
    to use all the tools in an online face tracker by detection, which yields 6 degrees
    of freedom head pose-3 degrees for rotation, and 3 for translation. The complete
    code for this chapter can be downloaded from [http://www.packtpub.com/](http://www.packtpub.com/).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在对AAM有足够的理论和实践之后，我们深入探讨了POSIT的细节，以便将2D测量与3D测量相结合，解释了如何通过模型点之间的匹配来拟合3D模型。我们通过展示如何通过检测使用在线人脸追踪器中的所有工具来结束本章，这产生了6个自由度的头部姿态——3个用于旋转，3个用于平移。本章的完整代码可以从[http://www.packtpub.com/](http://www.packtpub.com/)下载。
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Active Appearance Models, T.F. Cootes, G. J. Edwards, and C. J. Taylor, ECCV,
    2:484-498, 1998* ([http://www.cs.cmu.edu/~efros/courses/AP06/Papers/cootes-eccv-98.pdf](http://www.cs.cmu.edu/~efros/courses/AP06/Papers/cootes-eccv-98.pdf))'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主动外观模型，T.F. Cootes, G. J. Edwards, 和 C. J. Taylor, ECCV, 2:484-498, 1998*
    ([http://www.cs.cmu.edu/~efros/courses/AP06/Papers/cootes-eccv-98.pdf](http://www.cs.cmu.edu/~efros/courses/AP06/Papers/cootes-eccv-98.pdf))'
- en: '*Active Shape Models-Their Training and Application, T.F. Cootes, C.J. Taylor,
    D.H. Cooper, and J. Graham, Computer Vision and Image Understanding, (61): 38-59,
    1995* ([http://www.wiau.man.ac.uk/~bim/Papers/cviu95.pdf](http://www.wiau.man.ac.uk/~bim/Papers/cviu95.pdf))'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主动形状模型及其训练与应用，T.F. Cootes，C.J. Taylor，D.H. Cooper，和J. Graham，计算机视觉与图像理解，第61卷，第38-59页，1995年，（[http://www.wiau.man.ac.uk/~bim/Papers/cviu95.pdf](http://www.wiau.man.ac.uk/~bim/Papers/cviu95.pdf))* '
- en: '*The MUCT Landmarked Face Database, S. Milborrow, J. Morkel, and F. Nicolls,
    Pattern Recognition Association of South Africa, 2010* ([http://www.milbo.org/muct/](http://www.milbo.org/muct/))'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MUCT标记人脸数据库，S. Milborrow，J. Morkel，和F. Nicolls，南非模式识别协会，2010年，（[http://www.milbo.org/muct/](http://www.milbo.org/muct/))* '
- en: '*The IMM Face Database - An Annotated Dataset of 240 Face Images, Michael M.
    Nordstrom, Mads Larsen, Janusz Sierakowski, and Mikkel B.**Stegmann, Informatics
    and Mathematical Modeling, Technical University of Denmark, 2004,* ([http://www2.imm.dtu.dk/~aam/datasets/datasets.html](http://www2.imm.dtu.dk/~aam/datasets/datasets.html))'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IMM人脸数据库 - 240张人脸图像的注释数据集，Michael M. Nordstrom，Mads Larsen，Janusz Sierakowski，以及Mikkel
    B.**Stegmann，丹麦技术大学信息与数学建模系，2004年，（[http://www2.imm.dtu.dk/~aam/datasets/datasets.html](http://www2.imm.dtu.dk/~aam/datasets/datasets.html))* '
- en: '*Sur la sphère vide, B. Delaunay, Izvestia Akademii Nauk SSSR, Otdelenie Matematicheskikh
    i Estestvennykh Nauk, 7:793-800, 1934*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于空球面，B. Delaunay，苏联科学院院刊，数学和自然科学部，第7卷，第793-800页，1934年*'
- en: '*Active Appearance Models for Facial Expression Recognition and Monocular Head
    Pose Estimation Master Thesis, P. Martins, 2008*'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于面部表情识别和单目头部姿态估计的主动外观模型，P. Martins，硕士论文，2008年*'
- en: '*Active Appearance Models Revisited, International Journal of Computer Vision,
    Vol. 60, No. 2, pp. 135 - 164, I. Mathews and S. Baker, November, 2004* ([http://www.ri.cmu.edu/pub_files/pub4/matthews_iain_2004_2/matthews_iain_2004_2.pdf](http://www.ri.cmu.edu/pub_files/pub4/matthews_iain_2004_2/matthews_iain_2004_2.pdf))'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重新审视主动外观模型，国际计算机视觉杂志，第60卷，第2期，第135-164页，I. Mathews和S. Baker，2004年11月，（[http://www.ri.cmu.edu/pub_files/pub4/matthews_iain_2004_2/matthews_iain_2004_2.pdf](http://www.ri.cmu.edu/pub_files/pub4/matthews_iain_2004_2/matthews_iain_2004_2.pdf))* '
- en: '*POSIT Tutorial, Javier Barandiaran* ([http://opencv.willowgarage.com/wiki/Posit](http://opencv.willowgarage.com/wiki/Posit))'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*POSIT教程，Javier Barandiaran，（[http://opencv.willowgarage.com/wiki/Posit](http://opencv.willowgarage.com/wiki/Posit))* '
- en: '*Model-Based Object Pose in 25 Lines of Code, International Journal of Computer
    Vision, 15, pp. 123-141, Dementhon and L.S Davis, 1995* ([http://www.cfar.umd.edu/~daniel/daniel_papersfordownload/Pose25Lines.pdf](http://www.cfar.umd.edu/~daniel/daniel_papersfordownload/Pose25Lines.pdf))'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*25行代码实现基于模型的物体姿态，国际计算机视觉杂志，第15卷，第123-141页，Dementhon和L.S Davis，1995年，（[http://www.cfar.umd.edu/~daniel/daniel_papersfordownload/Pose25Lines.pdf](http://www.cfar.umd.edu/~daniel/daniel_papersfordownload/Pose25Lines.pdf))* '
