- en: Scala for Dimensionality Reduction and Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala 用于降维和聚类
- en: In the previous chapters, we saw several examples of supervised learning, covering
    both classification and regression. We performed supervised learning techniques
    on structured and labelled data. However, as we mentioned previously, with the
    rise of cloud computing, IoT, and social media, unstructured data is increasing
    unprecedentedly. Collectively, more than 80% of this data is unstructured and
    which most of them are unlabeled.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了几个监督学习的例子，包括分类和回归。我们在结构化和标记数据上执行了监督学习技术。然而，正如我们之前提到的，随着云计算、物联网和社交媒体的兴起，非结构化数据正在以前所未有的速度增加。总的来说，超过80%的数据是非结构化的，其中大部分是无标签的。
- en: Unsupervised learning techniques, such as clustering analysis and dimensionality
    reduction, are two of the key applications in data-driven research and industry
    settings for finding hidden structures in unstructured datasets. There are many
    clustering algorithms being proposed for this, such as k-means, bisecting k-means,
    and the Gaussian mixture model. However, these algorithms cannot perform with
    high-dimensional input datasets and often suffer from the *curse of dimensionality*.
    So, reducing the dimensionality using algorithms like **principal component analysis**
    (**PCA**) and feeding the latent data is a useful technique for clustering billions
    of data points.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术，如聚类分析和降维，是数据驱动研究和工业环境中寻找非结构化数据集中隐藏结构的关键应用。为此，提出了许多聚类算法，如k-means、二分k-means和高斯混合模型。然而，这些算法不能在高维输入数据集上高效运行，并且经常遭受*维度灾难*。因此，使用主成分分析（**PCA**）等算法降低维度，并输入潜在数据，是聚类数十亿数据点的一种有用技术。
- en: In this chapter, we will use a genetic variant (one kind of genomic data) to
    cluster the population according to their predominant ancestry, also called geographic
    ethnicity. We will evaluate the clustering analysis result, followed by the dimensionality
    reduction technique, to avoid the curse of dimensionality.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一种基因变体（一种基因组数据）根据他们的主要血统（也称为地理种族）对人群进行聚类。我们将评估聚类分析结果，然后进行降维技术，以避免维度灾难。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Overview of unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习概述
- en: Learning clustering—clustering geographic ethnicity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习聚类——聚类地理种族
- en: Dimensionality reduction with PCA
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA进行降维
- en: Clustering with reduced dimensional data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用降维数据进行聚类
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Scala 2.11.x和Java 1.8.x已安装并配置在您的机器上。
- en: 'The code files of this chapters can be found on GitHub:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter05)'
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用：
- en: '[http://bit.ly/2ISwb3o](http://bit.ly/2ISwb3o)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2ISwb3o](http://bit.ly/2ISwb3o)'
- en: Overview of unsupervised learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习概述
- en: 'In unsupervised learning, an input set is provided to the system during the
    training phase. In contrast to supervised learning, the input objects are not
    labeled with their class. Although in classification analysis the training dataset
    is labeled, we do not always have that advantage when we collect data in the real
    world, but still we want to find important values or hidden structures of the
    data. In NeuralIPS'' 2016, Facebook AI Chief Yann LeCun introduced the *cake analogy*:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，在训练阶段向系统提供一个输入集。与监督学习相反，输入对象没有标记其类别。虽然在分类分析中训练数据集是标记的，但在现实世界中收集数据时，我们并不总是有这种优势，但我们仍然希望找到数据的重要值或隐藏结构。在2016年的NeuralIPS上，Facebook
    AI首席科学家Yann LeCun介绍了*蛋糕类比*：
- en: '"If intelligence was a cake, unsupervised learning would be the cake, supervised
    learning would be the icing on the cake, and reinforcement learning would be the
    cherry on the cake. We know how to make the icing and the cherry, but we don''t
    know how to make the cake."'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: “如果智能是一块蛋糕，无监督学习就是蛋糕本身，监督学习就是蛋糕上的糖霜，强化学习就是蛋糕上的樱桃。我们知道如何制作糖霜和樱桃，但我们不知道如何制作蛋糕。”
- en: 'In order to create such a cake, several unsupervised learning tasks, including
    clustering, dimensionality reduction, anomaly detection, and association rule
    mining, are used. If unsupervised learning algorithms help find previously unknown
    patterns in a dataset without needing a label, we can learn the following analogy
    for this chapter:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这样的蛋糕，需要使用包括聚类、维度约简、异常检测和关联规则挖掘在内的几个无监督学习任务。如果无监督学习算法能够在不需要标签的情况下帮助在数据集中找到先前未知的模式，我们可以为这一章学习以下类比：
- en: K-means is a popular clustering analysis algorithm for grouping similar data
    points together
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means是一种流行的聚类分析算法，用于将相似数据点分组在一起
- en: A dimensionality reduction algorithm, such as PCA, helps find the most relevant
    features in a dataset
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度约简算法，如PCA，有助于在数据集中找到最相关的特征
- en: In this chapter, we'll discuss these two techniques for cluster analysis with
    a practical example.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过实际示例讨论这两种聚类分析技术。
- en: Clustering analysis
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析
- en: Clustering analysis and dimensionality reduction are the two most popular examples
    of unsupervised learning, which we will discuss throughout this chapter with examples.
    Suppose you have a large collection of legal MP3 files in your computer or smartphones.
    In such a case, how could you possibly group songs together if you do not have
    direct access to their metadata?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析和维度约简是无监督学习的两个最流行的例子，我们将在本章中通过示例进行讨论。假设您在电脑或智能手机中有大量法律MP3文件。在这种情况下，如果没有直接访问它们的元数据，您如何将歌曲分组在一起？
- en: One possible approach could be to mix various ML techniques, but clustering
    is often the best solution. This is because we can develop a clustering model
    in order to automatically group similar songs and organize them into your favorite
    categories, such as country, rap, or rock.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法可能是混合各种机器学习技术，但聚类通常是最佳解决方案。这是因为我们可以开发一个聚类模型，以自动将相似的歌曲分组并组织到您最喜欢的类别中，例如乡村、说唱或摇滚。
- en: Although the data points are not labeled, we can still do the necessary feature
    engineering and group similar objects together, which is commonly referred to
    as clustering.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据点没有标签，我们仍然可以进行必要的特征工程并将相似的对象分组在一起，这通常被称为聚类。
- en: A cluster refers to a collection of data points grouped together based on certain
    similarity measures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是指根据某些相似性度量将数据点分组在一起的一组数据点。
- en: However, this is not easy for a human. Instead, a standard approach is to define
    a similarity measure between two objects and then look for any cluster of objects
    that is more similar to each other than it is to the objects in the other clusters.
    Once we have done the clustering of the data points (that is, the MP3 files) and
    the validation is completed, we know the pattern of the data (that is, what type
    of MP3 files fall in which group).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这对人类来说并不容易。相反，一种标准的方法是定义两个对象之间的相似性度量，然后寻找任何对象簇，这些对象簇之间的相似性比它们与其他簇中的对象之间的相似性更大。一旦我们对数据点（即MP3文件）进行了聚类（即验证完成），我们就知道了数据的模式（即哪种类型的MP3文件属于哪个组）。
- en: 'The left-hand side diagram shows all the **MP3 tracks in a playlist**, which
    are scattered. The right-hand side part shows how the MP3 are clustered based
    on genre:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧图显示了播放列表中所有的**MP3曲目**，它们是分散的。右侧部分显示了基于流派如何对MP3进行聚类：
- en: '![](img/5a19db36-cb05-4e82-92d5-44ed0b747360.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a19db36-cb05-4e82-92d5-44ed0b747360.png)'
- en: Clustering analysis algorithms
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析算法
- en: 'The goal of a clustering algorithm is to group a similar set of unlabeled data
    points together to discover underlying patterns. Here are some of the algorithms
    that have been proposed and used for clustering analysis:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的目标是将一组相似的无标签数据点分组在一起，以发现潜在的模式。以下是一些已经提出并用于聚类分析算法的算法：
- en: K-means
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means
- en: Bisecting k-means
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分k-means
- en: '**Gaussian mixture model** (**GMM**)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（**GMM**）'
- en: '**Power iteration clustering** (**PIC**)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类**（**PIC**）'
- en: '**Latent Dirichlet Allocation** (**LDA**)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）'
- en: Streaming k-means
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式k-means
- en: K-means, bisecting k-means, and GMM are the most widely used. They will be covered
    in detail to show a quick-start clustering analysis. However, we will also look
    at an example based on only k-means.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: K-means、二分k-means和GMM是最广泛使用的。我们将详细说明，以展示快速入门的聚类分析。然而，我们还将查看仅基于k-means的示例。
- en: K-means for clustering analysis
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means聚类分析
- en: K-means looks for a fixed number *k* of clusters (that is, the number of centroids), partitions
    the data points into *k* clusters, and allocates every data point to the nearest
    cluster by keeping the centroids as small as possible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: K-means寻找一个固定的簇数*k*（即质心的数量），将数据点划分为*k*个簇，并通过尽可能保持质心最小来将每个数据点分配到最近的簇。
- en: A centroid is an imaginary or real location that represents the center of the
    cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 质心是一个想象中的或实际的位置，代表簇的中心。
- en: 'K-means computes the distance (usually the Euclidean distance) between data
    points and the center of the *k* clusters by minimizing the cost function, called
    **within-cluster sum of squares** (**WCSS**). The k-means algorithm proceeds by
    alternating between two steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: K-means通过最小化成本函数，称为**簇内平方和**（**WCSS**），来计算数据点到**k**个簇中心的距离（通常是欧几里得距离）。k-means算法通过交替进行以下两个步骤进行：
- en: '**Cluster assignment step**: Each data point is assigned to the cluster whose
    mean has the least-squared Euclidean distance, yielding the lowest WCSS'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇分配步骤**：每个数据点被分配到具有最小平方欧几里得距离的簇，从而产生最低的WCSS'
- en: '**Centroid update step**: The new means of the observations in the new clusters
    are calculated and used as the new centroids'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质心更新步骤**：计算新簇中观测值的新均值，并将其用作新的质心'
- en: 'The preceding steps can be represented in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤可以用以下图表表示：
- en: '![](img/df03a615-1cf4-4351-b4b6-7939864b274e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df03a615-1cf4-4351-b4b6-7939864b274e.png)'
- en: 'The k-means algorithm is done when the centroids have stabilized or when the
    predefined number of iterations have been iterated. Although k-means uses Euclidean
    distance, there are other ways to calculate the distance too, for example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当质心稳定或达到预定义的迭代次数时，k-means算法完成。尽管k-means使用欧几里得距离，但还有其他计算距离的方法，例如：
- en: The **Chebyshev distance** can be used to measure the distance by considering
    only the most notable dimensions
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切比雪夫距离**可以用来通过仅考虑最显著的维度来测量距离'
- en: The **Hamming distance** algorithm can identify the difference between two strings
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汉明距离**算法可以识别两个字符串之间的差异'
- en: To make the distance metric scale-undeviating, **Mahalanobis distance** can
    be used to normalize the covariance matrix
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使距离度量不受尺度影响，可以使用**马氏距离**来归一化协方差矩阵
- en: The **Manhattan distance** is used to measure the distance by considering only
    axis-aligned directions
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿距离**通过仅考虑轴对齐方向来测量距离'
- en: The **Minkowski distance** algorithm is used to make the Euclidean distance,
    Manhattan distance, and Chebyshev distance
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**闵可夫斯基距离**算法用于生成欧几里得距离、曼哈顿距离和切比雪夫距离'
- en: The **haversine distance** is used to measure the great-circle distances between
    two points on a sphere from the location, that is, longitudes and latitudes
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**哈夫曼距离**用于测量球面上两点之间的球面距离，即经纬度'
- en: Bisecting k-means
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分k-means
- en: Bisecting k-means can be thought of as a combination of k-means and hierarchical
    clustering, which starts with all the data points in a single cluster. Then, it
    randomly picks a cluster to split, which returns two sub-clusters using basic
    k-means. This is called the **bisecting step**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 二分k-means可以看作是k-means和层次聚类的组合，它从单个簇中的所有数据点开始。然后，它随机选择一个簇进行分割，使用基本的k-means返回两个子簇。这被称为**二分步骤**。
- en: The bisecting k-means algorithm is based on a paper titled *A Comparison of
    Document Clustering Techniques* by *Michael Steinbach et al.*, *KDD Workshop on
    Text Mining, 2000*, which has been extended to fit with Spark MLlib.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 二分k-means算法基于一篇题为“*A Comparison of Document Clustering Techniques*”的论文，由Michael
    Steinbach等人撰写，发表于2000年的KDD文本挖掘研讨会，该论文已被扩展以适应Spark MLlib。
- en: Then, the bisecting step is iterated for a predefined number of times (usually
    set by the user/developer), and all the splits are collected that produce the
    cluster with the highest similarity. These steps are continued until the desired
    number of clusters is reached. Although bisecting k-means is faster than regular
    k-means, it produces different clustering because bisecting k-means initializes
    clusters randomly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对二分步骤进行预定义的次数迭代（通常由用户/开发者设置），并收集产生具有最高相似度的所有分割。这些步骤一直持续到达到所需的簇数。尽管二分k-means比常规k-means更快，但它产生的聚类不同，因为二分k-means随机初始化簇。
- en: Gaussian mixture model
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: GMM is a probabilistic model with a strong assumption that all the data points
    are generated from a mixture of a finite number of Gaussian distributions with
    unknown parameters. So, it is a distribution-based clustering algorithm too, which
    is based on an expectation-maximization approach.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: GMM是一种概率模型，它强假设所有数据点都是由有限数量的高斯分布的混合生成的，且参数未知。因此，它也是一种基于分布的聚类算法，该算法基于期望最大化方法。
- en: 'GMM can also be considered as a generalized k-means where the model parameters
    are optimized iteratively to fit the model better to the training dataset. The
    overall process can be written in a three-step pseudocode:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 也可以被视为一种广义的 k-means，其中模型参数通过迭代优化以更好地拟合训练数据集。整个过程可以用以下三步伪代码表示：
- en: '**Objective function**: Compute and maximize the log-likelihood using **expectation-maximization**
    (**EM**)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标函数**：使用**期望最大化**（**EM**）计算并最大化对数似然'
- en: '**EM step**: This EM step consists of two sub-steps called expectation and
    maximization:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EM 步骤**：这个 EM 步骤包括两个子步骤，称为期望和最大化：'
- en: '**Step E**: Compute the posterior probability of the nearer data points'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 E**：计算最近数据点的后验概率'
- en: '**Step M**: Update and optimize the model parameters for fitting mixture-of-Gaussian
    models'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 M**：更新和优化模型参数以拟合高斯混合模型'
- en: '**Assignment**: Perform soft assignment during *step E*'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分配**：在*步骤 E*期间进行软分配'
- en: 'The preceding steps can be visualized very naively as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤可以非常直观地表示如下：
- en: '![](img/6e09fc2f-8686-40aa-acb1-18c837d64212.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e09fc2f-8686-40aa-acb1-18c837d64212.png)'
- en: Other clustering analysis algorithms
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他聚类分析算法
- en: The other clustering algorithms includes PIC, which is used to cluster the nodes
    of a graph based on the given pairwise similarities, such as edge. The LDA is
    used often in text clustering use cases, such as topic modeling.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其他聚类算法包括 PIC，它用于根据给定的成对相似度（如边）对图中的节点进行聚类。LDA 在文本聚类用例中经常被使用，如主题建模。
- en: 'On the other hand, streaming k-means is similar to k-means but applicable for
    streaming data. For example, when we want to estimate the clusters dynamically
    so that the clustering assignment will be updated when new data arrives, using
    streaming k-means is a good option. For a more detailed discussion with examples,
    interested readers can refer to the following links:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，流式 k-means 与 k-means 类似，但适用于流数据。例如，当我们想要动态估计簇，以便在新的数据到达时更新聚类分配时，使用流式 k-means
    是一个好的选择。对于更详细的讨论和示例，感兴趣的读者可以参考以下链接：
- en: Spark ML-based clustering algorithms ([https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html))
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Spark ML 的聚类算法 ([https://spark.apache.org/docs/latest/ml-clustering.html](https://spark.apache.org/docs/latest/ml-clustering.html))
- en: Spark MLlib-based clustering algorithms ([https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html))
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Spark MLlib 的聚类算法 ([https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html))
- en: Clustering analysis through examples
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例进行聚类分析
- en: 'One of the most important tasks in clustering analysis is the analysis of genomic
    profiles to attribute individuals to specific ethnic populations, or the analysis
    of nucleotide haplotypes for diseases susceptibility. Human ancestry from Asia,
    Europe, Africa, and the Americas can be separated based on their genomic data.
    Research has shown that the Y chromosome lineage can be geographically localized,
    forming the evidence for clustering the human alleles of the human genotypes.
    According to National Cancer Institute ([https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant](https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant)):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类分析中，最重要的任务之一是对基因组图谱进行分析，以将个体归入特定的种族群体，或者对疾病易感性进行核苷酸单倍型分析。根据亚洲、欧洲、非洲和美洲的基因组数据，可以区分人类祖先。研究表明，Y染色体谱系可以在地理上定位，这为将人类基因型的等位基因进行聚类提供了证据。根据国家癌症研究所（[https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant](https://www.cancer.gov/publications/dictionaries/genetics-dictionary/def/genetic-variant)）：
- en: '"Genetic variants are an alteration in the most common DNA nucleotide sequence.
    The term variant can be used to describe an alteration that may be benign, pathogenic,
    or of unknown significance. The term variant is increasingly being used in place
    of the term mutation."'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: “遗传变异是DNA最常见核苷酸序列的改变。变异一词可以用来描述可能良性、致病或意义未知的改变。变异一词越来越多地被用来代替突变。”
- en: A better understanding of genetic variations assists us in finding correlating
    population groups, identifying patients who are predisposed to common diseases,
    and solving rare diseases. In short, the idea is to cluster geographic ethnic
    groups based on their genetic variants. However, before going into this any further,
    let's get to know the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地理解遗传变异有助于我们找到相关的种群群体，识别易患常见疾病的患者，以及解决罕见疾病。简而言之，想法是根据遗传变异将地理民族群体进行聚类。然而，在进一步探讨之前，让我们先了解数据。
- en: Description of the dataset
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: 'The data from the 1,000 Genomes Project is a large catalog of human genetic
    variants. This project is meant for determining genetic variants with frequencies
    above 1% in the populations that were studied. The third phase of the 1,000 Genomes
    Project finished in September 2014, covering 2,504 individuals from 26 populations
    and 84,4000,000 million genetic variants. The population samples are grouped into
    five super-population groups, according to their predominant ancestry:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1,000基因组项目的数据是人类遗传变异的大型目录。该项目旨在确定在研究人群中频率超过1%的遗传变异。1,000基因组项目的第三阶段于2014年9月完成，涵盖了来自26个种群和8,440,000,000个遗传变异的2,504个个体。根据其主要的血统，种群样本被分为五个超级种群群体：
- en: East Asian (CHB, JPT, CHS, CDX, and KHV)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚洲东部（CHB、JPT、CHS、CDX和KHV）
- en: European (CEU, TSI, FIN, GBR, and IBS)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧洲地区（CEU、TSI、FIN、GBR和IBS）
- en: African (YRI, LWK, GWD, MSL, ESN, ASW, and ACB)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲地区（YRI、LWK、GWD、MSL、ESN、ASW和ACB）
- en: American (MXL, PUR, CLM, and PEL)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国地区（MXL、PUR、CLM和PEL）
- en: South Asian (GIH, PJL, BEB, STU, and ITU)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 南亚地区（GIH、PJL、BEB、STU和ITU）
- en: Each genotype comprises 23 chromosomes and a separate panel file that contains
    sample and population information. The data in **Variant Call Format** (**VCF**)
    as well the panel file can be downloaded from [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个基因型由23条染色体和一个包含样本和种群信息的单独的PANEL文件组成。**变异调用格式**（**VCF**）中的数据以及PANEL文件可以从[ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/)下载。
- en: Preparing the programming environment
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备编程环境
- en: 'Since the third release of the 1,000 Genome Project contributes about 820 GB
    of data, using scalable software and hardware is required to process them. To
    do so, we will use a software stack that consists of the following components:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于1,000基因组项目的第三次发布贡献了大约820 GB的数据，因此需要使用可扩展的软件和硬件来处理它们。为此，我们将使用以下组件组成的软件栈：
- en: '**ADAM**: This can be used to achieve the scalable genomics-data-analytics
    platform with support for the VCF file format so that we can transform genotype-based
    RDD into Spark DataFrames.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ADAM**：这可以用来实现支持VCF文件格式的可扩展基因组数据分析平台，从而将基于基因型的RDD转换为Spark DataFrame。'
- en: '**Sparkling Water**:H20 is an AI platform for machine learning and a web-based
    data-processing UI with support for programming languages such as Java, Python,
    and R. In short, Sparkling Water equals H2O plus Spark.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sparkling Water**：H20是一个机器学习AI平台，以及一个支持Java、Python和R等编程语言的基于Web的数据处理UI。简而言之，Sparkling
    Water等于H2O加上Spark。'
- en: Spark-ML-based k-means is trained for clustering analysis.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Spark-ML的k-means用于聚类分析。
- en: For this example, we need to use multiple technology and software stacks, such
    as Spark, H2O, and Adam. Before using H20, make sure that your laptop has at least
    16 GB RAM and sufficient storage space. I will develop this solution as a Maven
    project.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们需要使用多个技术和软件栈，例如Spark、H2O和Adam。在使用H20之前，请确保您的笔记本电脑至少有16 GB的RAM和足够的存储空间。我将把这个解决方案作为一个Maven项目来开发。
- en: 'Let''s define the properties tag on the `pom.xml` file for the Maven-friendly
    project:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`pom.xml`文件上定义属性标签，以适应Maven友好的项目：
- en: '[PRE0]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once you create a Maven project on Eclipse (from an IDE or using the `mvn install` command),
    all the required dependencies will be downloaded!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在Eclipse上创建了一个Maven项目（从一个IDE或使用`mvn install`命令），所有必需的依赖项都将被下载！
- en: Clustering geographic ethnicity
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类地理民族
- en: '24 VCF files contribute around 820 GB of data, which will impose a great computational
    challenge. To overcome this, use the genetic variants from the smallest chromosome,
    Y. The size of the VCF file for this is around 160 MB. Let''s get started by creating
    `SparkSession`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 24个VCF文件贡献了大约820 GB的数据，这将带来巨大的计算挑战。为了克服这一点，使用最小的染色体Y中的遗传变异。这个VCF文件的大小大约为160
    MB。让我们通过创建`SparkSession`开始：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s show Spark the path of both VCF and the PANEL file:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们向Spark展示VCF和PANEL文件的路由：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We process the PANEL file using Spark to access the target population data
    and identify the population groups. First, we create a set of `populations` that
    we want to form clusters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Spark 处理 PANEL 文件，以访问目标人群数据并识别人群组。首先，我们创建一组我们想要形成聚类的 `populations`：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we need to create a map between the sample ID and the given population
    so that we can filter out the samples we are not interested in:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建样本 ID 和给定人群之间的映射，以便我们可以过滤掉我们不感兴趣的样本：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The panel file produces the sample ID of all individuals, population groups,
    ethnicity, super-population groups, and genders:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 面板文件生成了所有个体的样本 ID、人群组、民族、超人群组和性别：
- en: '![](img/5d3126f4-ea27-4581-8734-c56bb5f15441.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d3126f4-ea27-4581-8734-c56bb5f15441.png)'
- en: Check out the details of the panel file at [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看面板文件的详细信息：[ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel)。
- en: 'Then, load the ADAM genotypes and filter the genotypes so that we are left
    with only those in the populations we are interested in:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，加载 ADAM 基因型并过滤基因型，以便我们只留下对我们感兴趣的人群中的那些：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, convert the `Genotype` objects into our own `SampleVariant` objects to
    conserve memory. Then, the `genotype` object is converted into a `SampleVariant`
    object that contains the data that needs to be processed further:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将 `Genotype` 对象转换为我们的 `SampleVariant` 对象以节省内存。然后，将 `genotype` 对象转换为包含需要进一步处理的数据的
    `SampleVariant` 对象：
- en: '**Sample ID**: To uniquely identify a particular sample'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本 ID**：用于唯一标识特定的样本'
- en: '**Variant ID**: To uniquely identify a particular genetic variant'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变异 ID**：用于唯一标识特定的遗传变异'
- en: '**Alternate alleles count**: Needed when the sample differs from the reference
    genome'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**替代等位基因计数**：当样本与参考基因组不同时需要'
- en: 'The signature that prepares a `SampleVariant` is given as follows, which takes
    `sampleID`, `variationId`, and the `alternateCount` objects:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 准备 `SampleVariant` 的签名如下，它接受 `sampleID`、`variationId` 和 `alternateCount` 对象：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we have to find the `variantID` from the genotype file. A `varitantId`
    is a String type that consists of a name, and the start and end positions in the
    chromosome:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须从基因型文件中找到 `variantID`。`varitantId` 是一个由名称、染色体中的起始和结束位置组成的字符串类型：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we have the `variantID`, we should hunt for the `alternateCount`. In the
    genotype file, objects for which an allele reference is present, would be the
    genetic alternates:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 `variantID`，我们就应该寻找 `alternateCount`。在基因型文件中，具有等位基因参考的对象将是遗传替代物：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we will construct a `SampleVariant` object. For this, we need to intern
    sample IDs as they will be repeated a lot in a VCF file:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将构建一个 `SampleVariant` 对象。为此，我们需要将样本 ID 内部化，因为它们在 VCF 文件中会重复很多次：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we need is to prepare `variantsRDD`. First, we have to group the variants
    by sample ID so that we can process the variants sample by sample. Then, we can
    get the total number of samples to be used to find the variants that are missing
    for some samples. Finally, we have to group the variants by variant ID and filter
    out those variants that are missing from some samples:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要准备 `variantsRDD`。首先，我们必须按样本 ID 对变异进行分组，以便我们可以逐个处理变异。然后，我们可以获取用于查找某些样本缺失变异的总样本数。最后，我们必须按变异
    ID 对变异进行分组，并过滤掉某些样本缺失的变异：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s map `variantId` with the count of samples with an alternate count
    of greater than zero. Then, we filter out the variants that are not in our desired
    frequency range. The objective here is to reduce the number of dimensions in the
    dataset to make it easier to train the model:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 `variantId` 与具有大于零的替代计数的样本数量进行映射。然后，我们过滤掉不在我们期望的频率范围内的变异。这里的目的是减少数据集的维度数量，使其更容易训练模型：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The total number of samples (or individuals) has been determined. Now, before
    grouping them using their variant IDs, we can filter out less significant variants.
    Since we have more than 84 million genetic variants, filtering would help us deal
    with the curse of dimensionality.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 样本总数（或个体数）已经确定。现在，在根据变异 ID 对它们进行分组之前，我们可以过滤掉不太重要的变异。由于我们有超过 8400 万个遗传变异，过滤可以帮助我们处理维度诅咒。
- en: 'The specified range is arbitrary and was chosen because it includes a reasonable
    number of variants, but not too many. To be more specific, for each variant, the
    frequency for alternate alleles has been calculated, and variants with fewer than
    12 alternate alleles have been excluded, leaving about 3,000,000 variants in the
    analysis (for 23 chromosome files):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 指定的范围是任意的，因为它包括合理数量的变体，但不是太多。更具体地说，对于每个变体，已经计算了等位基因的频率，并且排除了具有少于 12 个等位基因的变体，从而在分析中留下了大约
    3,000,000 个变体（对于 23 个染色体文件）：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once we have `filteredVariantsBySampleId`, we need to sort the variants for
    each sample ID. Each sample should now have the same number of sorted variants:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 `filteredVariantsBySampleId`，我们需要对每个样本 ID 的变体进行排序。每个样本现在应该具有相同数量的排序变体：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All items in the RDD should now have the same variants in the same order. The
    final task is to use `sortedVariantsBySampleId` to construct an RDD of a row that
    contains the region and the alternate count:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 中的所有项现在都应该具有相同的变体，并且顺序相同。最终任务是使用 `sortedVariantsBySampleId` 来构建一个包含区域和等位基因计数的行
    RDD：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Therefore, we can just use the first one to construct our header for the training
    DataFrame:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们只需使用第一个来构建我们的训练 DataFrame 的标题：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Well done! We have our RDD and the `StructType` header. Now, we can play with
    the Spark machine learning algorithm with minimal adjustment/conversion.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们有了我们的 RDD 和 `StructType` 标题。现在，我们可以用最小的调整/转换来玩 Spark 机器学习算法。
- en: Training the k-means algorithm
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 k-means 算法
- en: 'Once we have the `rowRDD` and the header, we need to construct the rows of
    our schema DataFrame from the variants using the header and `rowRDD`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 `rowRDD` 和标题，我们需要使用标题和 `rowRDD` 从变体中构建我们的模式 DataFrame 的行：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding `show()` method should show a snapshot of the training dataset
    that contains the features and the `label` columns (that is, `Region`):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 `show()` 方法应该显示包含特征和 `label` 列（即 `Region`）的训练数据集快照：
- en: '![](img/b82f5318-3021-49a0-b8ed-ba8d1cdc66cc.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b82f5318-3021-49a0-b8ed-ba8d1cdc66cc.png)'
- en: 'In the preceding DataFrame, only a few `feature` columns and the `label` column
    are shown so that it fits on the page. Since the training would be unsupervised,
    we need to drop the `label` column (that is, `Region`):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 DataFrame 中，只显示了少数 `feature` 列和 `label` 列，以便它适合页面。由于训练将是无监督的，我们需要删除 `label`
    列（即 `Region`）：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding `show()` method shows the following snapshot of the training
    dataset for k-means. Note that there is no `label` column (that is, `Region`):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 `show()` 方法显示了以下 k-means 的训练数据集快照。注意，没有 `label` 列（即 `Region`）：
- en: '![](img/084d8df1-14cb-450a-a607-23c68c8d0adc.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/084d8df1-14cb-450a-a607-23c68c8d0adc.png)'
- en: 'In [Chapter 1](33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml), *Introduction to
    Machine Learning with Scala*, and [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml),
    *Scala for Regression Analysis*, we saw that Spark expects two columns (`features`
    and `label`) for supervised training. However, for the unsupervised training,
    only a single column containing the features is required. Since we dropped the
    `label` column, we now need to amalgamate the entire `variable` column into a
    single `features` column. For this, we will use the `VectorAssembler()` transformer.
    Let''s select the columns to be embedded into a vector space:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 1 章](33fe7442-ce44-4a18-bac6-0e08e9b1ae1e.xhtml)，《使用 Scala 的机器学习入门》，和[第
    2 章](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml)，《Scala 回归分析》，我们了解到 Spark 预期用于监督训练有两个列（`features`
    和 `label`）。然而，对于无监督训练，只需要包含特征的单一列。由于我们删除了 `label` 列，我们现在需要将整个 `variable` 列合并为一个单一的
    `features` 列。为此，我们将使用 `VectorAssembler()` 转换器。让我们选择要嵌入到向量空间中的列：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we will instantiate the `VectorAssembler()` transformer by specifying
    the input columns and the output column:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过指定输入列和输出列来实例化 `VectorAssembler()` 转换器：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s see what the feature vectors for the k-means look like:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 k-means 的特征向量是什么样的：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding line shows the assembled vectors, which can be used as the feature
    vectors for the k-means model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的行显示了组装的向量，这些向量可以用作 k-means 模型的特征向量：
- en: '![](img/7e18458d-a0b4-4da2-8f8e-212bd5351897.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e18458d-a0b4-4da2-8f8e-212bd5351897.png)'
- en: 'Finally, we are ready to train the k-means algorithm and evaluate the clustering
    by computing **WCSS**:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好训练 k-means 算法并通过计算 **WCSS** 来评估聚类：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following is the **WCSS** value for `k = 5`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 **WCSS** 值为 `k = 5`：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We managed to apply k-means to cluster genetic variants. However, we saw that
    the WCSS was high because k-means was unable to separate the non-linearity among
    different correlated and high-dimensional features. This is because genomic sequencing
    datasets are very high dimensional due to a huge number of genetic variants.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地将k-means应用于聚类遗传变异。然而，我们注意到WCSS很高，因为k-means无法分离不同相关的高维特征之间的非线性。这是因为基因组测序数据集由于大量的遗传变异而具有非常高的维度。
- en: In the next section, we will see how we can use dimensionality-reduction techniques,
    such as PCA, to reduce the dimensionality of the input data before feeding it
    to k-means in order to get better clustering quality.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用降维技术，如PCA，在将数据输入到k-means之前降低输入数据的维度，以获得更好的聚类质量。
- en: Dimensionality reduction
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Since humans are visual creatures, understanding a high dimensional dataset
    (even with more than three dimensions) is impossible. Even for a machine (or say,
    our machine learning algorithm), it's difficult to model the non-linearity from
    correlated and high-dimensional features. Here, the dimensionality reduction technique
    is a savior.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类是视觉生物，理解高维数据集（甚至超过三个维度）是不可能的。即使是对于机器（或者说，我们的机器学习算法），也很难从相关的高维特征中建模非线性。在这里，降维技术是一个救星。
- en: Statistically, dimensionality reduction is the process of reducing the number
    of random variables to find a low-dimensional representation of the data while
    preserving as much information as possible**.**
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学的角度来看，降维是减少随机变量的数量，以找到数据的一个低维表示，同时尽可能保留尽可能多的信息**。**
- en: 'The overall step in PCA can be visualized naively in the following diagram:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的整体步骤可以在以下图表中直观地表示：
- en: '![](img/02ea50cd-5589-46e8-8bf3-388c3ed9c326.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02ea50cd-5589-46e8-8bf3-388c3ed9c326.png)'
- en: PCA and **singular-value decomposition** (**SVD**) are the most popular algorithms
    for dimensionality reduction. Technically, PCA is a statistical technique that's
    used to emphasize variation and extract the most significant patterns (that is,
    features) from a dataset, which is not only useful for clustering but also for
    classification and visualization.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）和**奇异值分解**（**SVD**）是降维中最受欢迎的算法。从技术上讲，PCA是一种用于强调变异并从数据集中提取最显著模式（即特征）的统计技术，这不仅对聚类有用，对分类和可视化也有帮助。
- en: Principal component analysis with Spark ML
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Spark ML的主成分分析
- en: 'Spark-ML-based PCA can be used to project vectors to a low-dimensional space
    to reduce the dimensionality of genetic variant features before feeding them into
    the k-means model. The following example shows how to project 6D feature vectors
    into 4D principal components from the following feature vector:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Spark-ML的PCA可以用来将向量投影到低维空间，在将它们输入到k-means模型之前降低遗传变异特征的维度。以下示例展示了如何将以下特征向量投影到4维主成分：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that we have a feature DataFrame with a 6D-feature vector, it can be fed
    into the PCA model:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个具有6维特征向量的特征DataFrame，它可以被输入到PCA模型中：
- en: '![](img/0069126d-2e94-400f-a8e4-9bf63f6f3ab0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0069126d-2e94-400f-a8e4-9bf63f6f3ab0.png)'
- en: 'First, we have to instantiate the PCA model by setting the necessary parameters,
    as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须通过设置必要的参数来实例化PCA模型，如下所示：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To distinguish the original features from the principal component-based features,
    we set the output column name as `pcaFeatures` using the `setOutputCol()` method.
    Then, we set the dimension of the PCA (that is, the number of principal components).
    Finally, we fit the DataFrame to make the transformation. A model can be loaded
    from older data but will have an empty vector for `explainedVariance`. Now, let''s
    show the resulting features:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分原始特征和基于主成分的特征，我们使用`setOutputCol()`方法将输出列名设置为`pcaFeatures`。然后，我们设置PCA的维度（即主成分的数量）。最后，我们将DataFrame拟合以进行转换。可以从旧数据中加载模型，但`explainedVariance`将会有一个空向量。现在，让我们展示生成的特征：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code produces a feature DataFrame with 4D feature vectors as
    principal components using the PCA:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用PCA生成一个具有4维特征向量的特征DataFrame，作为主成分：
- en: '![](img/7b6c797c-957d-49ee-a048-f66fd3b659e2.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7b6c797c-957d-49ee-a048-f66fd3b659e2.png)'
- en: Similarly, we can transform the assembled DataFrame (that is, `assembleDF`)
    in the previous step and the top five principle components. You can adjust the
    number of principal components, though.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以将上一步组装的DataFrame（即`assembleDF`）和前五个主成分进行转换。你可以调整主成分的数量。
- en: 'Finally, to avoid any ambiguity, we renamed the `pcaFeatures` column to `features`:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了避免任何歧义，我们将 `pcaFeatures` 列重命名为 `features`：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding lines of code show the embedded vectors, which can be used as
    the feature vectors for the k-means model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码行显示了嵌入的向量，这些向量可以用作 k-means 模型的特征向量：
- en: '![](img/79031b6c-40ce-4ec1-b5cc-a8369cfc143c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79031b6c-40ce-4ec1-b5cc-a8369cfc143c.png)'
- en: 'The preceding screenshot shows the top five principal components as the most
    important features. Excellent—everything went smoothly. Finally, we are ready
    to train the k-means algorithm and evaluate clustering by computing WCSS:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了前五个主成分作为最重要的特征。太好了——一切顺利。最后，我们准备训练 k-means 算法并通过计算 WCSS 来评估聚类：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This time, the WCSS is slightly lower (compared to the previous value, which
    was `59.34564329865`):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，WCSS 略微降低（与之前的值 `59.34564329865` 相比）：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Normally, we set the number of `k` (that is, `5`) randomly and computed the
    WCSS. However, this way, we cannot always set the optimal number of clusters.
    In order to find an optimal value, researchers have come up with two techniques,
    called the elbow method and silhouette analysis, which we'll look at in the following
    subsection.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们随机设置 `k` 的数量（即 `5`）并计算 WCSS。然而，这种方法并不能总是设置最佳聚类数量。为了找到一个最佳值，研究人员提出了两种技术，称为肘部方法和轮廓分析，我们将在下一小节中探讨。
- en: Determining the optimal number of clusters
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定最佳聚类数量
- en: 'Sometimes, assuming the number of clusters naively and before starting the
    training may not be a good idea. If the assumption is too far from the optimal
    number of clusters, the model performs poorly because of the overfitting or underfitting
    issue that''s introduced. So, determining the number of optimal clusters is a
    separate optimization problem. There are two popular techniques to tackle this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，在开始训练之前天真地假设聚类数量可能不是一个好主意。如果假设与最佳聚类数量相差太远，模型会因为引入的过拟合或欠拟合问题而表现不佳。因此，确定最佳聚类数量是一个独立的优化问题。有两种流行的技术来解决此问题：
- en: The heuristic approach, called the **elbow method**
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被称为**肘部方法**的启发式方法
- en: '**Silhouette analysis**, to observe the separation distance between predicted
    clusters'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮廓分析**，用于观察预测聚类的分离距离'
- en: The elbow method
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 肘部方法
- en: 'We start by setting the `k` value to `2` and running the k-means algorithm
    on the same dataset by increasing `k` and observing the value of WCSS. As expected,
    a drastic drop should occur in the cost function (that is, WCSS values) at some
    point. However, after the drastic fall, the value of WCSS becomes marginal with
    the increasing value of `k`. As suggested by the elbow method, we can pick the
    optimal value of `k` after the last big drop of WCSS:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将 `k` 值设置为 `2`，并在相同的数据集上运行 k-means 算法，通过增加 `k` 并观察 WCSS 的值。正如预期的那样，成本函数（即
    WCSS 值）在某一点应该会有一个急剧下降。然而，在急剧下降之后，随着 `k` 值的增加，WCSS 的值变得微不足道。正如肘部方法所建议的，我们可以在 WCSS
    的最后一次大幅下降后选择 `k` 的最佳值：
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let''s see the WCSS values for a different number of clusters, such as
    between `2` and `20`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看不同数量聚类（例如 `2` 到 `20`）的 WCSS 值：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As shown in the preceding code, we calculated the cost function, WCSS, as a
    function of a number of clusters for the k-means algorithm, and applied them to
    the Y chromosome genetic variants from the selected population groups. It can
    be observed that a big drop occurs when `k = 5` (which is not a drastic drop,
    though). Therefore, we chose a number of clusters to be 10.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们计算了成本函数 WCSS 作为聚类数量的函数，并将其应用于所选种群组的 Y 染色体遗传变异。可以观察到，当 `k = 5` 时会出现一个大的下降（尽管不是急剧下降）。因此，我们选择聚类数量为
    10。
- en: The silhouette analysis
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮廓分析
- en: Analyzing the silhouette is carried out by observing the separation distance
    between predicted clusters. Drawing a silhouette plot will show the distance between
    a data point from its neighboring clusters, and then we can visually inspect a
    number of clusters so that similar data points get well-separated.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察预测聚类的分离距离来分析轮廓。绘制轮廓图将显示数据点与其邻近聚类之间的距离，然后我们可以通过视觉检查多个聚类，以便相似的数据点得到良好的分离。
- en: 'The silhouette score, which is used to measure the clustering quality, has
    a range of [-1, 1]. Evaluate the clustering quality by computing the silhouette
    score:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓得分，用于衡量聚类质量，其范围为 [-1, 1]。通过计算轮廓得分来评估聚类质量：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We get the following output:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE32]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As shown in the preceding code, the height value of the silhouette is generated
    with `k = 2`, which is `0.9175803927739566`. However, this suggests that genetic
    variants should be clustered in two groups. The elbow method suggested `k = 5`
    as the optimal number of clusters.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，轮廓的高度值是通过`k = 2`生成的，为`0.9175803927739566`。然而，这表明遗传变异应该分为两组。肘部方法建议`k
    = 5`作为最佳聚类数量。
- en: 'Let''s find out the silhouette using the squared Euclidean distance, as shown
    in the following code block:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用平方欧几里得距离来找出轮廓，如下面的代码块所示：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The silhouette with the squared Euclidean distance for `k = 2` is `0.9175803927739566`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`k = 2`的平方欧几里得距离轮廓值为`0.9175803927739566`。'
- en: It has been found that the bisecting k-means algorithm can result in better
    cluster assignment for data points, converging to the global minima. On the other
    hand, k-means often gets stuck in the local minima. Please note that you might
    observe different values of the preceding parameters depending on your machine's
    hardware configuration and the random nature of the dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 已经发现，分割k均值算法可以对数据点的聚类分配产生更好的结果，收敛到全局最小值。另一方面，k均值算法往往陷入局部最小值。请注意，根据您的机器硬件配置和数据集的随机性，您可能会观察到前面参数的不同值。
- en: Interested readers should also refer to the Spark-MLlib-based clustering techniques
    at [https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)
    for more insights.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者还应参考基于Spark-MLlib的聚类技术[https://spark.apache.org/docs/latest/mllib-clustering.html](https://spark.apache.org/docs/latest/mllib-clustering.html)，以获得更多见解。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed some clustering analysis techniques, such as k-means,
    bisecting k-means, and GMM. We saw a step-by-step example of how to cluster ethnic
    groups based on their genetic variants. In particular, we used the PCA for dimensionality
    reduction, k-means for clustering, and H2O and ADAM for handling large-scale genomics
    datasets. Finally, we learned about the elbow and silhouette methods for finding
    the optimal number of clusters.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些聚类分析方法，如k均值、分割k均值和GMM。我们看到了如何根据遗传变异对族群进行聚类的逐步示例。特别是，我们使用了PCA进行降维，k均值进行聚类，以及H2O和ADAM处理大规模基因组数据集。最后，我们学习了肘部和轮廓方法来寻找最佳聚类数量。
- en: Clustering is the key to most data-driven applications. Readers can try to apply
    clustering algorithms on higher-dimensional datasets, such as gene expression
    or miRNA expression, in order to cluster similar and correlated genes. A great
    resource is the gene expression cancer RNA-Seq dataset, which is open source.
    This dataset can be downloaded from the UCI machine learning repository at [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是大多数数据驱动应用的关键。读者可以尝试在更高维度的数据集上应用聚类算法，例如基因表达或miRNA表达，以聚类相似和相关的基因。一个很好的资源是基因表达癌症RNA-Seq数据集，它是开源的。此数据集可以从UCI机器学习存储库下载，网址为[https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)。
- en: In the next chapter, we will discuss item-based collaborative filtering approaches
    for the recommender system. We'll learn how to develop a book recommendation system.
    Technically, it will be a model-based recommendation engine with Scala and Spark.
    We will see how we can interoperate between ALS and matrix factorization.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论推荐系统中的基于物品的协同过滤方法。我们将学习如何开发一个图书推荐系统。技术上，它将是一个基于Scala和Spark的模型推荐引擎。我们将看到如何实现ALS和矩阵分解之间的互操作。
