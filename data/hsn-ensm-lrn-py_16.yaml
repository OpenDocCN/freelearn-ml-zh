- en: Evaluating Sentiment on Twitter
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Twitter 上评估情感
- en: Twitter is a highly popular social network with over 300 million monthly active
    users. The platform has been developed around short posts (limited to a number
    of characters; currently, the limit is 280 characters). The posts themselves are
    called tweets. On average, 6000 tweets are tweeted every second, which equates
    to around 200 billion tweets per year. This constitutes a huge amount of data
    that contains an equal amount of information. As is obvious, it is not possible
    to analyze this volume of data by hand. Thus, automated solutions have been employed,
    both by Twitter and third parties. One of the hottest topics involves a tweet's
    sentiment, or how the user feels about the topic that they tweets. Sentiment analysis
    comes in many flavors. The most common approach is a positive or negative classification
    of each tweet. Other approaches involve a more complex analysis of positive and
    negative emotions, such as anger, disgust, fear, happiness, sadness, and surprise.
    In this chapter, we will briefly present some sentiment analysis tools and practices.
    Following this, we will cover the basics of building a classifier that leverages
    ensemble learning techniques in order to classify tweets. Finally, we will see
    how we can classify tweets in real time by using Twitter's API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 是一个非常受欢迎的社交网络，拥有超过 3 亿月活跃用户。该平台围绕简短的帖子（字符数量有限，目前限制为 280 个字符）开发。帖子本身称为推文。平均每秒发布
    6000 条推文，相当于每年约 2000 亿条推文。这构成了一个庞大的数据量，包含了大量信息。显然，手动分析如此大量的数据是不可能的。因此，Twitter
    和第三方都采用了自动化解决方案。最热门的话题之一是推文的情感分析，或者说用户对他们发布的主题的情感。情感分析有很多种形式。最常见的方法是对每条推文进行正面或负面分类。其他方法则涉及更复杂的正负面情感分析，如愤怒、厌恶、恐惧、快乐、悲伤和惊讶等。在本章中，我们将简要介绍一些情感分析工具和实践。接下来，我们将介绍构建一个利用集成学习技术进行推文分类的分类器的基础知识。最后，我们将看到如何通过使用
    Twitter 的 API 实时分类推文。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Sentiment analysis tools
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析工具
- en: Getting Twitter data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取 Twitter 数据
- en: Creating a model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型
- en: Classifying tweets in real time
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时分类推文
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解 Python 的约定和语法。最后，熟悉 NumPy 库将极大帮助读者理解一些自定义算法实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter11)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2XSLQ5U](http://bit.ly/2XSLQ5U).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用：[http://bit.ly/2XSLQ5U](http://bit.ly/2XSLQ5U)。
- en: Sentiment analysis tools
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析工具
- en: Sentiment analysis can be implemented in a number of ways. The easiest to both
    implement and understand are lexicon-based approaches. These methods leverage
    the use of lists (lexicons) of polarized words and expressions. Given a sentence,
    these methods count the number of positive and negative words and expressions.
    If there are more positive words/expressions, the sentence is labeled as positive.
    If there are more negative than positive words/expressions, the sentence is labeled
    as negative. If the number of positive and negative words/expressions are equal,
    the sentence is labeled as neutral. Although this approach is relatively easy
    to code and does not require any training, it has two major disadvantages. First,
    it does not take into account interactions between words. For example, *not bad*,
    which is actually a positive expression, can be classified as negative, as it
    is composed of two negative words. Even if the expression is included in the lexicon
    under positive, the expression *not that bad* may not be included. The second
    disadvantage is that the whole process relies on good and complete lexicons. If
    the lexicon omits certain words, the results can be very poor.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析可以通过多种方式实现。最容易实现和理解的方法是基于词典的方法。这些方法利用了极性单词和表达的词典列表。给定一个句子，这些方法会计算正面和负面单词及表达的数量。如果正面单词/表达的数量更多，则该句子被标记为正面。如果负面单词/表达比正面更多，则该句子被标记为负面。如果正面和负面单词/表达的数量相等，则该句子被标记为中性。虽然这种方法相对容易编码，并且不需要任何训练，但它有两个主要缺点。首先，它没有考虑单词之间的相互作用。例如，*not
    bad*，实际上是一个正面的表达，但可能被分类为负面，因为它由两个负面单词组成。即使该表达在词典中被归为正面，表达*not that bad*也可能没有包含在内。第二个缺点是整个过程依赖于良好和完整的词典。如果词典遗漏了某些单词，结果可能会非常糟糕。
- en: Another approach is to train a machine learning model in order to classify sentences.
    In order to do so, a training dataset has to be created, where a number of sentences
    are labeled as positive or negative by human experts. This process indirectly
    uncovers a hidden problem in (and also indicates the difficulty of) sentiment
    analysis. Human analysts agree on 80% to 85% of the cases. This is partly due
    to the subjective nature of many expressions. For example, the sentence *Today
    the weather is nice, yesterday it was bad*, can be either positive, negative,
    or neutral. This depends on intonation. Assuming that the bold word is intonated,
    *Today the weather is **nice**, yesterday it was bad* is positive. *Today the
    weather is nice, yesterday it was **bad** *is negative, while *Today the weather
    is nice, yesterday it was bad* is actually neutral (a simple observation of a
    change in the weather).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是训练一个机器学习模型来分类句子。为此，必须创建一个训练数据集，其中一些句子由人工专家标记为正面或负面。这个过程间接揭示了情感分析中的一个隐藏问题（也表明了其难度）。人类分析师在80%到85%的情况下达成一致。这部分是由于许多表达的主观性。例如，句子*今天天气很好，昨天很糟糕*，可以是正面、负面或中性。这取决于语调。假设**粗体**部分有语调，*今天天气很好，昨天很糟糕*是正面的，*今天天气很好，昨天很糟糕*是负面的，而*今天天气很好，昨天很糟糕*实际上是中性的（只是简单地观察天气变化）。
- en: You can read more about the problem of disagreement between human analysts in
    sentiment classification at: [https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview](https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此链接阅读更多关于人类分析师在情感分类中分歧的问题：[https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview](https://www.lexalytics.com/lexablog/sentiment-accuracy-quick-overview)。
- en: 'In order to create machine learning features from text data, usually, n-grams
    are created. N-grams are sequences of *n* words extracted from each sentence.
    For example, the sentence "Hello there, kids" contains the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从文本数据中创建机器学习特征，通常会创建n-grams。N-grams是从每个句子中提取的*n*个词的序列。例如，句子"Hello there, kids"包含以下内容：
- en: '1-grams: "Hello", "there,", "kids"'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1-grams: "Hello"，"there,"，"kids"'
- en: '2-grams: "Hello there,”, "there, kids"'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2-grams: "Hello there,"，"there, kids"'
- en: '3-grams: "Hello there, kids"'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3-grams: "Hello there, kids"'
- en: 'In order to create numeric features for a dataset, a single feature is created
    for each unique N-gram. For each instance, the feature''s value depends on the
    number of times it appears in the sentence. For example, consider the following
    toy dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为数据集创建数值特征，为每个唯一的N-gram创建一个特征。对于每个实例，特征的值取决于它在句子中出现的次数。例如，考虑以下玩具数据集：
- en: '| **Sentence** | **Polarity** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **句子** | **极性** |'
- en: '| My head hurts | Positive |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 我的头很痛 | 正面 |'
- en: '| The food was good food | Negative |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 食物很好吃 | 负面 |'
- en: '| The sting hurts | Positive |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 刺痛很严重 | 正面 |'
- en: '| That was a good time | Negative |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 那是一个很棒的时光 | 负面 |'
- en: A sentiment toy dataset
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个情感玩具数据集
- en: 'Assume that we will only use 1-grams (unigrams). The unique unigrams contained
    in the dataset are: "My", "head", "hurts", "The", "food", "was", "good", "sting",
    "That", "a", and "time". Thus, each instance has 11 features. Each feature corresponds
    to a single *n*-gram (in our case, a unigram). Each feature’s value equals the
    number of appearances of the corresponding *n*-gram in the instance. The final
    dataset is depicted in the following table:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们只使用1-gram（单字）。数据集中包含的唯一单字有：“My”，“head”，“hurts”，“The”，“food”，“was”，“good”，“sting”，“That”，“a”和“time”。因此，每个实例有11个特征。每个特征对应一个单元词（在本例中是单字）。每个特征的值等于该单元词在该实例中的出现次数。最终的数据集如下所示：
- en: '| **My** | **Head** | **Hurts** | **The** | **Food** | **Was** | **Good** |
    **Sting** | **That** | **A** | **Time** | **Polarity** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **我的** | **头** | **疼** | **这** | **食物** | **很好** | **刺痛** | **那** | **一**
    | **时间** | **极性** |'
- en: '| 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Positive |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 正面 |'
- en: '| 0 | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 0 | 0 | 0 | Negative |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 负面 |'
- en: '| 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | Positive |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 正面 |'
- en: '| 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 | 1 | Negative |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 1 | 1 | 负面 |'
- en: The extracted features dataset
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的特征数据集
- en: 'Usually, each instance is normalized, so each feature represents the relative
    frequency, rather than the absolute frequency (count), of each *n*-gram. This
    method is called **Term Frequency** (**TF**). The TF dataset is depicted as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个实例会被归一化，因此每个特征表示的是每个单元词的相对频率，而不是绝对频率（计数）。这种方法被称为**词频**（**TF**）。TF数据集如下所示：
- en: '| **My** | **Head** | **Hurts** | **The** | **Food** | **Was** | **Good** |
    **Sting** | **That** | **A** | **Time** | **Polarity** |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **我的** | **头** | **疼** | **这** | **食物** | **很好** | **刺痛** | **那** | **一**
    | **时间** | **极性** |'
- en: '| 0.33 | 0.33 | 0.33 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Positive |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 0.33 | 0.33 | 0.33 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 正面 |'
- en: '| 0 | 0 | 0 | 0.2 | 0.4 | 0.2 | 0.2 | 0 | 0 | 0 | 0 | Negative |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0.2 | 0.4 | 0.2 | 0.2 | 0 | 0 | 0 | 0 | 负面 |'
- en: '| 0 | 0 | 0.33 | 0.33 | 0 | 0 | 0 | 0.33 | 0 | 0 | 0 | Positive |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0.33 | 0.33 | 0 | 0 | 0 | 0.33 | 0 | 0 | 0 | 正面 |'
- en: '| 0 | 0 | 0 | 0 | 0 | 0.2 | 0.2 | 0 | 0.2 | 0.2 | 0.2 | Negative |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 0 | 0.2 | 0.2 | 0 | 0.2 | 0.2 | 0.2 | 负面 |'
- en: The TF dataset
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: TF数据集
- en: 'In the English language, some terms exhibit a really high frequency, while
    contributing little towards the expression’s sentiment. In order to account for
    this fact, **Inverse Document Frequency** (**IDF**) is employed. IDF puts more
    emphasis on infrequent terms. For *N* instances with *K* unique unigrams, the
    IDF of unigram *u*, which is present in *M* instances, is calculated as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语中，一些词语的出现频率非常高，但对表达情感的贡献很小。为了考虑这一事实，采用了**逆文档频率**（**IDF**）。IDF更加关注不常见的词语。对于*N*个实例和*K*个唯一的单词，单词*u*的IDF值计算公式如下：
- en: '![](img/cfb7ea6d-9315-4036-8d83-4203240902dd.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfb7ea6d-9315-4036-8d83-4203240902dd.png)'
- en: 'The following table depicts the IDF-transformed dataset:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了IDF转换后的数据集：
- en: '| **My** | **Head** | **Hurts** | **The** | **Food** | **Was** | **Good** |
    **Sting** | **That** | **A** | **Time** | **Polarity** |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **我的** | **头** | **疼** | **这** | **食物** | **很好** | **刺痛** | **那** | **一**
    | **时间** | **极性** |'
- en: '| 0.6 | 0.6 | 0.3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Positive |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 0.6 | 0.6 | 0.3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 正面 |'
- en: '| 0 | 0 | 0 | 0.3 | 0.6 | 0.3 | 0.3 | 0 | 0 | 0 | 0 | Negative |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0.3 | 0.6 | 0.3 | 0.3 | 0 | 0 | 0 | 0 | 负面 |'
- en: '| 0 | 0 | 0.3 | 0.3 | 0 | 0 | 0 | 0.6 | 0 | 0 | 0 | Positive |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0.3 | 0.3 | 0 | 0 | 0 | 0.6 | 0 | 0 | 0 | 正面 |'
- en: '| 0 | 0 | 0 | 0 | 0 | 0.3 | 0.3 | 0 | 0.6 | 0.6 | 0.6 | Negative |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 0 | 0.3 | 0.3 | 0 | 0.6 | 0.6 | 0.6 | 负面 |'
- en: The IDF dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: IDF数据集
- en: Stemming
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取
- en: Stemming is another practice usually utilized in sentiment analysis. It is the
    process of reducing words to their root. This lets us handle words that originate
    from the same root as a single unigram. For example, *love*, *loving*, and *loved*
    will be all handled as the same unigram, *love*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是情感分析中常用的另一种做法。它是将单词还原为词根的过程。这使得我们可以将来源于相同词根的单词作为同一个单元词处理。例如，*love*、*loving*和*loved*都会作为相同的单元词，*love*来处理。
- en: Getting Twitter data
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Twitter数据
- en: 'There are a number of ways to gather Twitter data. From web scraping to using
    custom libraries, each one has different advantages and disadvantages. For our
    implementation, as we also need sentiment labeling, we will utilize the `Sentiment140`
    dataset ([http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)).
    The reason that we do not collect our own data is mostly due to the time we would
    need to label it. In the last section of this chapter, we will see how we can
    collect our own data and analyze it in real time. The dataset consists of 1.6
    million tweets, containing the following 6 fields:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 收集 Twitter 数据有多种方式。从网页抓取到使用自定义库，每种方式都有不同的优缺点。对于我们的实现，由于我们还需要情感标注，我们将使用 `Sentiment140`
    数据集（[http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip)）。我们不收集自己的数据，主要是因为需要标注数据的时间。在本章的最后部分，我们将看到如何收集自己的数据并实时分析。该数据集包含160万条推文，包含以下6个字段：
- en: The tweet's polarity
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文的情感极性
- en: A numeric ID
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字 ID
- en: The date it was tweeted
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文的日期
- en: The query used to record the tweet
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于记录推文的查询
- en: The user's name
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户的名字
- en: The tweet's text content
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文的文本内容
- en: 'For our models, we will only need the tweet''s text and polarity. As can be
    seen in the following graph, there are 800,000 positive (with a polarity 4) and
    800,000 negative (with a polarity 0) tweets:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们只需要推文的文本和情感极性。如以下图表所示，共有80万个正面推文（情感极性为4）和80万个负面推文（情感极性为0）：
- en: '![](img/3b782be9-bd85-4c47-a9ef-d13a8eaf4a44.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b782be9-bd85-4c47-a9ef-d13a8eaf4a44.png)'
- en: Polarity distribution
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 情感极性分布
- en: 'Here, we can also verify the statement we made earlier about word frequencies.
    The following graph depicts the 30 most common words in the dataset. As is evident,
    none of them bears any sentiment. Thus, an IDF transform would be more beneficial
    to our models:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还可以验证我们之前关于单词频率的说法。以下图表展示了数据集中最常见的30个单词。显然，它们没有表现出任何情感。因此，IDF 转换对我们的模型将更有帮助：
- en: '![](img/58641b42-29bc-42aa-b6f0-89246483f46d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58641b42-29bc-42aa-b6f0-89246483f46d.png)'
- en: The 30 most common words in the dataset and the number of occurrences of each
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中最常见的30个单词及其出现次数
- en: Creating a model
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: 'The most important step in sentiment analysis (as is the case with most machine
    learning problems) is the preprocessing of our data. The following table contains
    10 tweets, randomly sampled from the dataset:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析中最重要的步骤（就像大多数机器学习问题一样）是数据的预处理。以下表格包含从数据集中随机抽取的10条推文：
- en: '| **id** | **text** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **id** | **文本** |'
- en: '| 44 | @JonathanRKnight Awww I soo wish I was there to see... |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 44 | @JonathanRKnight 哎呀，我真希望我能在那儿看到... |'
- en: '| 143873 | Shaking stomach flipping........god i hate thi... |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 143873 | 胃部翻腾……天啊，我讨厌这个... |'
- en: '| 466449 | why do they refuse to put nice things in our v... |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 466449 | 为什么他们拒绝把好东西放进我们的 v... |'
- en: '| 1035127 | @KrisAllenmusic visit here |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1035127 | @KrisAllenmusic 访问这里 |'
- en: '| 680337 | Rafa out of Wimbledon Love Drunk by BLG out S... |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 680337 | Rafa 退出温布尔登，因 BLG 感情失控... |'
- en: '| 31250 | It''s official, printers hate me Going to sul... |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 31250 | 官方宣布，打印机讨厌我，准备沉沦... |'
- en: '| 1078430 | @_Enigma__ Good to hear |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 1078430 | @_Enigma__ 很高兴听到这个 |'
- en: '| 1436972 | Dear Photoshop CS2\. i love you. and i miss you! |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1436972 | 亲爱的 Photoshop CS2\. 我爱你，我想你！ |'
- en: '| 401990 | my boyfriend got in a car accident today ! |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 401990 | 我的男朋友今天出了车祸！ |'
- en: '| 1053169 | Happy birthday, Wisconsin! 161 years ago, you ... |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 1053169 | 生日快乐，威斯康星州！161年前，你... |'
- en: An outline of 10 random samples from the dataset
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的10个随机样本大纲
- en: We can immediately make the following observations. First, there are references
    to other users, for example, `@KrisAllenmusic`. These references do not provide
    any information about the tweet's sentiment. Thus, during preprocessing, we will
    remove them. Second, there are numbers and punctuation. These also do not contribute
    to the tweet’s sentiment, so they must also be removed. Third, some letters are
    capitalized while others are not. As capitalization does not alter the word’s
    sentiment, we can choose to either convert all letters to lowercase or to convert
    them to uppercase. This ensures that words such as *LOVE*, *love*, and *Love*
    will be handled as the same unigram. If we sample more tweets, we can identify
    more problems. There are hashtags (such as `#summer`), which also do not contribute
    to the tweet’s sentiment. Furthermore, there are URL links (for example [https://www.packtpub.com/eu/](https://www.packtpub.com/eu/))
    and HTML attributes (such as `&amp` which corresponds to `&`). These will also
    be removed during our preprocessing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即得出以下观察结果。首先，有对其他用户的引用，例如`@KrisAllenmusic`。这些引用并没有提供有关推文情感的信息。因此，在预处理过程中，我们将删除它们。其次，有数字和标点符号。这些也没有贡献推文的情感，因此它们也必须被删除。第三，部分字母是大写的，而其他字母则不是。由于大小写不会改变单词的情感，我们可以选择将所有字母转换为小写或大写。这确保像*LOVE*、*love*和*Love*这样的词将被视为相同的单元词。如果我们再取样更多推文，可以识别出更多问题。有话题标签（例如`#summer`），这些同样不贡献推文的情感。此外，还有网址链接（例如[https://www.packtpub.com/eu/](https://www.packtpub.com/eu/)）和HTML属性（如`&amp`对应`&`）。这些在预处理中也将被删除。
- en: 'In order to preprocess our data, first, we must import the required libraries.
    We will use pandas; Python''s built-in regular expressions library, `re`; `punctuation`
    from `string`; and the **Natural Language Toolkit** (**NLTK**). The `nltk` library
    can be easily installed either through `pip` or `conda` as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行预处理，首先，我们必须导入所需的库。我们将使用pandas，Python内置的正则表达式库`re`，`string`中的`punctuation`，以及**自然语言工具包**（**NLTK**）。可以通过`pip`或`conda`轻松安装`nltk`库，方法如下：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After loading the libraries, we load the data, change the polarity from *[0,
    4]* to *[0, 1]*, and discard all fields except for the text content and the polarity:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 加载完库后，我们加载数据，将极性从*[0, 4]*更改为*[0, 1]*，并丢弃除了文本内容和极性之外的所有字段：
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As we saw earlier, many words do not contribute to a tweet''s sentiment, although
    they frequently appear in text. Search engines handle this by removing such words,
    which are called stop words. NLTK has a list of the most common stop words that
    we are going to utilize. Furthermore, as there are a number of stop words that
    are contractions (such as "you''re" and "don''t") and tweets frequently omit single
    quotes in contractions, we expand the list in order to include contractions without
    single quotes (such as "dont"):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，许多单词并不对推文的情感产生影响，尽管它们在文本中经常出现。搜索引擎通过去除这些单词来处理此问题，这些单词被称为停用词。NLTK提供了最常见的停用词列表，我们将利用该列表。此外，由于有一些停用词是缩写词（如"you're"和"don't"），而且推文中通常省略缩写词中的单引号，因此我们将扩展该列表，以包括没有单引号的缩写词（如"dont"）。
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then define two distinct functions. The first function, `clean_string`,
    cleans the tweet by removing all elements we discussed earlier (such as references,
    hashtags, and so on). The second function removes all punctuation or stop word
    and stems each word, by utilizing NLTK''s `PorterStemmer`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了两个不同的函数。第一个函数`clean_string`通过删除我们之前讨论过的所有元素（如引用、话题标签等）来清理推文。第二个函数通过使用NLTK的`PorterStemmer`去除所有标点符号或停用词，并对每个单词进行词干化处理：
- en: '[PRE3]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we would like to compare the performance of the ensemble with the base learners
    themselves, we will define a function that will evaluate any given classifier.
    The two most important factors that will define our dataset are the n-grams we
    will use and the number of features. Scikit-learn has an implementation of an
    IDF feature extractor, the `TfidfVectorizer` class. This allows us to only utilize
    the top *M* most frequent features, as well as define the n-gram range we will
    use, through the `max_features` and `ngram_range` parameters. It creates sparse
    arrays of features, which saves a great deal of memory, but the results must be
    converted to normal arrays before they can be processed by scikit-learn''s classifiers.
    This is achieved by calling the `toarray()` function. Our `check_features_ngrams` function
    accepts the number of features, a tuple of minimum and maximum n-grams, and a
    list of named classifiers (a name, classifier tuple). It extracts the required
    features from the dataset and passes them to the nested `check_classifier`. This
    function trains and evaluates each classifier, as well as exports the results
    to the specified file, `outs.txt`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望比较集成模型与基学习器本身的性能，我们将定义一个函数，用于评估任何给定的分类器。定义我们数据集的两个最重要因素是我们将使用的 n-gram
    和特征数量。Scikit-learn 提供了一个 IDF 特征提取器实现，即 `TfidfVectorizer` 类。这使得我们可以仅使用 *M* 个最常见的特征，并通过
    `max_features` 和 `ngram_range` 参数定义我们将使用的 n-gram 范围。它创建了稀疏特征数组，这节省了大量内存，但结果必须在被
    scikit-learn 分类器处理之前转换为普通数组。这可以通过调用 `toarray()` 函数来实现。我们的 `check_features_ngrams`
    函数接受特征数量、最小和最大 n-gram 的元组，以及命名分类器的列表（名称，分类器元组）。它从数据集中提取所需的特征，并将其传递给嵌套的 `check_classifier`。该函数训练并评估每个分类器，并将结果导出到指定的文件
    `outs.txt`：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The results are depicted in the following diagram. As is evident, as we increase
    the number of features, the accuracy increases for all classifiers. Furthermore,
    if the number of features is relatively small, unigrams outperform combinations
    of unigrams and bigrams/trigrams. This is due to the fact that the most frequent
    expressions are not sentimental. Finally, although voting exhibits a relatively
    satisfactory performance, it is not able to outperform logistic regression:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下面的图表所示。如图所示，随着特征数量的增加，所有分类器的准确率都有所提高。此外，如果特征数量相对较少，单一的 unigram 优于 unigram
    与 bigram/trigram 的组合。这是因为最常见的表达式往往没有情感色彩。最后，尽管投票法的表现相对令人满意，但仍未能超过逻辑回归：
- en: '![](img/0300f491-87fe-4535-8f10-53273d2ff456.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0300f491-87fe-4535-8f10-53273d2ff456.png)'
- en: Results of voting and base learners
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 投票法与基学习器的结果
- en: Classifying tweets in real time
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时分类推文
- en: We can use our model in order to classify tweets in real time using Twitter’s
    API. In order to simplify things, we will make use of a very popular wrapper library
    for the API, `tweepy` ([https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy)).
    Installation is easily achieved with `pip install tweepy`. The first step to accessing
    Twitter programmatically is to generate relevant credentials. This is achieved
    by navigating to [https://apps.twitter.com/](https://apps.twitter.com/) and selecting
    Create an app. The application process is straightforward and should be accepted
    quickly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用我们的模型通过 Twitter 的 API 实时分类推文。为了简化操作，我们将使用一个非常流行的 API 封装库 `tweepy`（[https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy)）。安装可以通过
    `pip install tweepy` 很容易地完成。通过编程访问 Twitter 的第一步是生成相关的凭证。这可以通过访问 [https://apps.twitter.com/](https://apps.twitter.com/)
    并选择“创建应用”来实现。申请过程简单，通常很快就会被接受。
- en: 'Using tweepy''s `StreamListener`, we will define a class that listens for incoming
    tweets, and as soon as they arrive, it classifies them and prints the original
    text and predicted polarity. First, we will load the required libraries. As a
    classifier, we will utilize the voting ensemble we trained earlier. First, we
    load the required libraries. We need the `json` library, as tweets are received
    in the JSON format; parts of the `tweepy` library; as well as the scikit-learn
    components we utilized earlier. Furthermore, we store our API keys in variables:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 tweepy 的 `StreamListener`，我们将定义一个监听器类，当推文到达时，它会立即对其进行分类，并打印原始文本和预测的极性。首先，我们将加载所需的库。作为分类器，我们将使用之前训练的投票集成模型。首先，加载所需的库。我们需要
    `json` 库，因为推文以 JSON 格式接收；还需要部分 `tweepy` 库以及之前使用过的 scikit-learn 组件。此外，我们将 API 密钥存储在变量中：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then proceed to create and train our `TfidfVectorizer` and `VotingClassifier`
    with 30,000 features and n-grams in the *[1, 3]* range:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建并训练我们的`TfidfVectorizer`和`VotingClassifier`，使用30,000个特征和范围为*[1, 3]*的n-gram：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then proceed with defining our `StreamClassifier` class, responsible for
    listening for incoming tweets and classifying them as they arrive. It inherits
    the `StreamListener` class from `tweepy`. By overriding the `on_data` function,
    we are able to process tweets as they arrive through the stream. The tweets arrive
    in JSON format, so we first parse them with `json.loads(data)`, which returns
    a dictionary, and then extract the text using the `"text"` key. We can then extract
    the features using the fitted `vectorizer` and utilize the features in order to
    predict its polarity:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`StreamClassifier`类，负责监听到达的推文并对其进行分类。它继承自`tweepy`的`StreamListener`类。通过重写`on_data`函数，我们可以在推文通过流到达时对其进行处理。推文以JSON格式到达，因此我们首先使用`json.loads(data)`解析它们，返回一个字典，然后使用`"text"`键提取文本。我们可以使用拟合好的`vectorizer`提取特征，并利用这些特征预测其情感：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we instantiate `StreamClassifier`, passing as arguments, the trained
    voting ensemble and `TfidfVectorizer` and authenticate using the `OAuthHandler`.
    In order to start the stream, we instantiate a `Stream` object with the `OAuthHandler`
    and `StreamClassifier` objects as parameters and define the keywords we want to
    track with `filter(track=[''Trump''])`. In this case, we track tweets that contain
    the keyword `''Trump''` as shown here:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实例化`StreamClassifier`，并将训练好的投票集成和`TfidfVectorizer`作为参数传入，使用`OAuthHandler`进行身份验证。为了启动流，我们实例化一个`Stream`对象，将`OAuthHandler`和`StreamClassifier`对象作为参数，并定义我们想要追踪的关键字`filter(track=['Trump'])`。在这个例子中，我们追踪包含关键字“特朗普”的推文，如下所示：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That''s it! The preceding code now tracks any tweet containing the keyword
    Trump and predicts its sentiment in real time. The following table depicts some
    simple tweets that were classified:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！前面的代码现在可以实时追踪任何包含“特朗普”关键字的推文，并预测其情感。下表显示了一些简单的推文及其分类结果：
- en: '| **Text** | **Polarity** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **文本** | **情感** |'
- en: '| **RT @BillyBaldwin**: Only two things funnier than my brothers impersonation
    of Trump. Your daughters impersonation of being an honest, decent… | Negative
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **RT @BillyBaldwin**: 比我兄弟模仿特朗普还要好笑的只有两件事。你女儿模仿一个诚实正直的... | 消极 |'
- en: '| **RT @danpfeiffer**: This is a really important article for Democrats to
    read. Media reports of Trump’s malfeasance is only the start. It''s the… | Positive
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **RT @danpfeiffer**: 这是民主党人必须阅读的一篇非常重要的文章。媒体报道特朗普的不当行为只是开始。这是... | 积极 |'
- en: '| **RT @BillKristol**: "In other words, Trump had backed himself, not Mexico,
    into a corner. They had him. He had to cave. And cave he did. He go… | Positive
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **RT @BillKristol**: "换句话说，特朗普把自己逼到了死角，而不是墨西哥。他们抓住了他。他不得不妥协。而且他确实妥协了。他去...
    | 积极 |'
- en: '| **RT @SenJeffMerkley**: That Ken Cuccinelli started today despite not being
    nominated is unacceptable. Trump is doing an end run around the Sen… | Negative
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **RT @SenJeffMerkley**: 尽管没有被提名，肯·库奇内利今天还是开始工作了，这是无法接受的。特朗普正在绕过参议院... | 消极
    |'
- en: Example of tweets being classified
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 推文分类示例
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the possibility of using ensemble learning in
    order to classify tweets. Although a simple logistic regression can outperform
    ensemble learning techniques, it is an interesting introduction to the realm of
    natural language processing and the techniques that are used in order to preprocess
    the data and extract useful features. In summary, we introduced the concepts of
    n-grams, IDF feature extraction, stemming, and stop word removal. We discussed
    the process of cleaning the data, as well as training a voting classifier and
    using it to classify tweets in real time using Twitter's API.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了使用集成学习对推文进行分类的可能性。虽然简单的逻辑回归可能优于集成学习技术，但它是自然语言处理领域的一个有趣入门，并且涉及到数据预处理和特征提取的技术。总的来说，我们介绍了n-gram、IDF特征提取、词干化和停用词移除的概念。我们讨论了清理数据的过程，并且训练了一个投票分类器，使用Twitter的API进行实时推文分类。
- en: In the next chapter, we will see how ensemble learning can be utilized in the
    design of recommender systems, with the aim of recommending movies to a specific
    user.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何在推荐系统的设计中利用集成学习，目的是向特定用户推荐电影。
