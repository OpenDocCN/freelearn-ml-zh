- en: '*Chapter 2*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*'
- en: Exploratory Data Analysis and Visualization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析与可视化
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of the chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Explain the importance of data exploration and communicate the summary statistics
    of a dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释数据探索的重要性并传达数据集的总结性统计信息
- en: Visualize patterns in missing values in data and be able to replace null values
    appropriately
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据中缺失值的模式，并能够适当替换空值
- en: Identify continuous features and categorical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别连续特征和类别特征
- en: Visualize distributions of values across individual variables
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化单个变量值的分布
- en: Describe and analyze relationships between different types of variables using
    correlation and visualizations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用相关性和可视化分析描述不同类型变量之间的关系
- en: This chapter takes us through how to perform exploration and analysis on a new
    dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带领我们进行对新数据集的探索与分析。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Say we have a problem statement that involves predicting whether a particular
    earthquake caused a tsunami or not. How do we decide what model to use? What do
    we know about the data we have? Nothing! But if we don't know and understand our
    data, chances are we'll end up building a model that's not very interpretable
    or reliable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个问题陈述，涉及预测某次地震是否引发了海啸。我们如何决定使用什么模型？我们知道我们拥有的数据是什么吗？什么都不知道！但如果我们不了解数据，最终可能会构建一个既难以解释又不可靠的模型。
- en: When it comes to data science, it's important to have a thorough understanding
    of the data we're dealing with, in order to generate features that are highly
    informative and, consequently, to build accurate and powerful models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，深入理解我们处理的数据非常重要，这样可以生成高度信息量的特征，从而构建准确且强大的模型。
- en: In order to gain this understanding, we perform an exploratory analysis on the
    data to see what the data can tell us about the relationships between the features
    and the target variable. Getting to know our data will even help us interpret
    the model we build and identify ways we can improve its accuracy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这种理解，我们对数据进行探索性分析，看看数据能告诉我们特征与目标变量之间的关系。了解数据甚至有助于我们解释构建的模型，并找出改善其准确性的方法。
- en: The approach we take to achieve this is to allow the data to reveal its structure
    or model, which helps gain some new, often unsuspected, insight into the data.
    Let's learn more about this approach.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现这一目标的方法是让数据揭示其结构或模型，这有助于我们获得一些新的、常常是意想不到的数据洞察。让我们深入了解这种方法。
- en: Exploratory Data Analysis (EDA)
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: '**Exploratory data analysis** (**EDA**) is defined as an approach to analyzing
    datasets to summarize their main characteristics, often with visual methods.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析**（**EDA**）被定义为一种分析数据集的方法，旨在总结其主要特征，通常采用可视化方法。'
- en: 'The purpose of EDA is to:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: EDA的目的：
- en: Discover patterns within a dataset
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现数据集中的模式
- en: Spot anomalies
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现异常
- en: Form hypotheses about the behavior of data
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据的行为形成假设
- en: Validate assumptions
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证假设
- en: Everything from basic summary statistics to complex visualizations help us gain
    an intuitive understanding of the data itself, which is highly important when
    it comes to forming new hypotheses about the data and uncovering what parameters
    affect the target variable. Often, discovering how the target variable varies
    across a single feature gives us an indication of how important a feature might
    be, and a variation across a combination of several features helps us come up
    with ideas for new informative features to engineer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础的总结性统计数据到复杂的可视化图形，都帮助我们对数据本身形成直观的理解，这在形成新的假设和揭示哪些参数影响目标变量时非常重要。通常，通过发现目标变量如何随单一特征的变化而变化，可以帮助我们判断某个特征的重要性，而多个特征组合的变化则帮助我们构思新的有用特征。
- en: Most exploration and visualization is intended to understand the relationship
    between the features and the target variable. This is because we want to find
    out what relationships exist (or don't exist) between the data we have and the
    values we want to predict.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数探索和可视化的目的是理解特征与目标变量之间的关系。因为我们想要找出我们拥有的数据和我们想要预测的值之间存在哪些（或不存在的）关系。
- en: A very basic domain knowledge is usually necessary to be able to understand
    both the problem statement itself as well as what the data is telling us. In this
    chapter, we'll look at the ways we can get to know more about the data we have
    by analyzing the features we have.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一些非常基本的领域知识通常是必要的，以便能够理解问题陈述本身以及数据所传达的信息。在本章中，我们将探索通过分析我们拥有的特征，了解更多关于数据的方式。
- en: 'EDA can tell us about:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: EDA 可以告诉我们：
- en: Features that are unclean, have missing values, or have outliers
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不干净、缺失值或存在异常值的特征
- en: Features that are informative and are a good indicator of the target
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有信息价值并且是目标的良好指示器的特征
- en: The kind of relationships features have with the target
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征与目标之间的关系类型
- en: Further features that the data might need that we don't already have
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可能需要的其他特征，而这些特征我们现在还没有
- en: Edge cases you might need to account for separately
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能需要单独处理的边缘情况
- en: Filters you might need to apply on the dataset
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能需要在数据集上应用的过滤器
- en: The presence of incorrect or fake data points
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误或虚假的数据点的存在
- en: Now that we've looked at why EDA is important and what it can tell us, let's
    talk about what exactly EDA involves. EDA can involve anything from looking at
    basic summary statistics to visualizing complex trends over multiple variables.
    However, even simple statistics and plots can be powerful tools, as they may reveal
    important facts about the data that could change our modeling perspective. When
    we see plots representing data, we are able to easily detect trends and patterns,
    compared to just raw data and numbers. These visualizations further allow us to
    ask questions such as "How?" and "Why?", and form hypotheses about the dataset
    that can be validated by further visualizations. This is a continuous process
    that leads to a deeper understanding of the data. This chapter will introduce
    you to some of the basic tools that can be used to explore any dataset while keeping
    in mind the ultimate problem statement.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了为什么 EDA 很重要以及它能告诉我们什么，接下来让我们讨论一下 EDA 具体包括什么内容。EDA 可以包括从查看基本的摘要统计到可视化多个变量之间复杂趋势的任何操作。然而，即使是简单的统计数据和图表也可以是强大的工具，因为它们可能揭示出数据中的重要事实，这些事实可能会改变我们建模的视角。当我们看到表示数据的图表时，我们能够轻松地检测到趋势和模式，相比于仅仅处理原始数据和数字。这些可视化进一步让我们能够提出类似“如何？”和“为什么？”的问题，并对数据集形成假设，这些假设可以通过进一步的可视化来验证。这是一个持续的过程，能够帮助我们更深入地理解数据。本章将向你介绍一些可以用来探索任何数据集的基本工具，同时牢记最终的问题陈述。
- en: We'll start by walking through some basic summary statistics and how to interpret
    them, followed by a section on finding, analyzing, and dealing with missing values.
    Then we'll look at univariate relationships, that is, distributions and the behavior
    of individual variables. This will be followed by the final section on exploring
    relationships between variables. In this chapter, you will be introduced to types
    of plots that can be used to gain a basic overview of the dataset and its features,
    as well as how to gain insights by creating visualizations that combine several
    features, and we'll then work through some examples on how they can be used.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一些基本的摘要统计量开始，介绍如何解读它们，接着是关于查找、分析和处理缺失值的部分。然后我们将研究单变量关系，即单个变量的分布和行为。最后，我们会探讨变量之间关系的探索部分。本章将向你介绍可以用来获得数据集及其特征基本概览的图表类型，以及如何通过创建结合多个特征的可视化来获得见解，我们还将通过一些示例展示它们如何使用。
- en: 'The dataset that we will use for our exploratory analysis and visualizations
    has been taken from the *Significant Earthquake Database* from NOAA, available
    as a public dataset on Google BigQuery (`table ID: ''bigquery-public-data.noaa_significant_earthquakes.earthquakes''`).
    We will be using a subset of the columns available, the metadata for which is
    available at [https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table](https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table),
    and loading it into a pandas DataFrame to perform the exploration. We''ll primarily
    be using Matplotlib for most of our visualizations, along with Seaborn and Missingno
    for some. It is to be noted, however, that Seaborn merely provides a wrapper over
    Matplotlib''s functionalities, so anything that is plotted using Seaborn can also
    be plotted using Matplotlib. We''ll try to keep things interesting by mixing up
    visualizations from both libraries.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于探索性分析和可视化的数据集来自NOAA的*重大地震数据库*，该数据集作为公共数据集可在Google BigQuery上获取（`表ID：'bigquery-public-data.noaa_significant_earthquakes.earthquakes'`）。我们将使用其中部分列，相关元数据可以在[https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table](https://console.cloud.google.com/bigquery?project=packt-data&folder&organizationId&p=bigquery-public-data&d=noaa_significant_earthquakes&t=earthquakes&page=table)查看，并将其加载到pandas
    DataFrame中以进行探索。我们主要使用Matplotlib来进行大多数可视化，同时也会使用Seaborn和Missingno进行部分可视化。然而需要注意的是，Seaborn只是对Matplotlib功能的封装，因此任何使用Seaborn绘制的图表都可以使用Matplotlib绘制。我们会通过混合使用两个库的可视化方式，来保持事情的趣味性。
- en: 'The exploration and analysis will be conducted keeping in mind a sample problem
    statement: *Given the data we have, we want to predict whether an earthquake caused
    a tsunami or not*. This will be a classification problem (more on this in *Chapter
    4*, *Classification*) where the target variable is the `flag_tsunami` column.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和分析将以一个示例问题为基础进行：*给定我们拥有的数据，我们希望预测地震是否引发了海啸*。这将是一个分类问题（更多内容请见*第4章*，*分类*），其中目标变量是`flag_tsunami`列。
- en: 'Exercise 10: Importing Libraries for Data Exploration'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 10：导入数据探索所需的库
- en: 'Before we begin, let''s first import the required libraries, which we will
    be using for most of our data manipulations and visualizations:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，首先导入我们将用于大多数数据操作和可视化的必要库：
- en: 'In a Jupyter notebook, import the following libraries:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中，导入以下库：
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `%matplotlib inline` command allows Jupyter to display the plots inline
    within the notebook itself.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`%matplotlib inline`命令允许Jupyter在Notebook中直接显示图表。'
- en: 'We can also read in the metadata containing the data types for each column,
    which are stored in the form of a JSON file. Do this using the following command.
    This command opens the file in readable format and uses the `json` library to
    read the file into a dictionary:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以读取包含每列数据类型的元数据，这些数据类型以JSON文件的形式存储。可以使用以下命令完成此操作。该命令将以可读格式打开文件，并使用`json`库将文件读入字典：
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let's get started.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。
- en: Summary Statistics and Central Values
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 汇总统计与中心值
- en: In order to find out what our data really looks like, we use a technique known
    as **data profiling**. This is defined as the process of examining the data available
    from an existing information source (for example, a database or a file) and collecting
    statistics or informative summaries about that data. The goal is to make sure
    that you understand your data well and are able to identify any challenges that
    the data may pose early on in the project, which is done by summarizing the dataset
    and assessing its structure, content, and quality.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解我们的数据到底是什么样子，我们使用一种叫做**数据分析**的技术。数据分析被定义为检查现有信息源（例如数据库或文件）中的数据，并收集关于该数据的统计信息或信息摘要的过程。目标是确保你充分理解你的数据，并能够尽早识别数据可能带来的挑战，这通过总结数据集并评估其结构、内容和质量来实现。
- en: 'Data profiling includes collecting descriptive statistics and data types. Here
    are a few commands that are commonly used to get a summary of a dataset:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析包括收集描述性统计和数据类型。以下是一些常用的命令，可以用来获取数据集的总结信息：
- en: '`data.info()`: This command tells us how many non-null values there are there
    in each column, along with the data type of the values (non-numeric types are
    represented as `object` types).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.info()`：此命令告诉我们每列中有多少非空值，并显示每列值的数据类型（非数值类型以 `object` 类型表示）。'
- en: '`data.describe()`: This gives us basic summary statistics for all the numerical
    columns in the DataFrame, such as the count of non-null values, minimum and maximum,
    the mean and standard deviation, and the quarter-wise percentiles for all numerical
    features. If there are any string-type features, it does not include a summary
    of those.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.describe()`：该命令为 DataFrame 中所有数值列提供基本的总结统计数据，例如非空值的计数、最小值和最大值、均值和标准差，以及所有数值特征的四分位数百分位数。如果有任何字符串类型的特征，则不包括这些特征的总结。'
- en: '`data.head()` and `data.tail()`: These commands display the first five and
    last five rows of the DataFrame respectively. While the previous commands give
    us a general idea of the dataset, it is a good idea to get a closer look at the
    actual data itself, which can be done using these commands.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.head()` 和 `data.tail()`：这两个命令分别显示 DataFrame 的前五行和后五行数据。虽然前面的命令可以给我们一个数据集的总体概览，但更深入地了解实际数据本身是一个好主意，可以使用这些命令来完成。'
- en: Standard Deviation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准差
- en: The standard deviation represents how widespread the distribution of the values
    of *x* are.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差表示 *x* 的值分布的广泛程度。
- en: 'For a set of numerical values, xi, the standard deviation is given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组数值 xi，标准差由以下公式给出：
- en: '![Figure 2.1: Standard deviation equation](img/C12622_02_01.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1：标准差公式](img/C12622_02_01.jpg)'
- en: 'Figure 2.1: Standard deviation equation'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.1：标准差公式
- en: Here, 𝝈 is the standard deviation, *N* is the number of data points, and 𝝁 is
    the mean.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，𝝈 是标准差，*N* 是数据点的数量，𝝁 是均值。
- en: 'Say we have a set of 10 values, *x = [0,1,1,2,3,4,2,2,0,1]*. The mean, 𝝁, will
    be the sum of these values, divided by 10\. That is, 𝝁 = 1.6:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组 10 个值，*x = [0,1,1,2,3,4,2,2,0,1]*。均值 𝝁 将是这些值的总和除以 10。也就是说，𝝁 = 1.6：
- en: '![Figure 2.2: Mean square values for x](img/C12622_02_02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2：x 的均方值](img/C12622_02_02.jpg)'
- en: 'Figure 2.2: Mean square values for x'
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.2：x 的均方值
- en: Then, standard deviation = sqrt(14.4/10) = 1.2.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，标准差 = sqrt(14.4/10) = 1.2。
- en: Percentiles
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 百分位数
- en: For a set of values, the *n**th* percentile is equal to the value that is greater
    than *n%* of values in the set. For example, the 50*th* percentile is the value
    in the dataset that has as many values greater than it as it does that are less
    than it. Additionally, the fiftieth percentile of a dataset is also known as its
    median, and the twenty-fifth and seventy-fifth percentiles are also known as the
    lower and upper quartiles.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组值，*n**th* 百分位数是大于该值的数值所占比例为 *n%* 的数据点。例如，第 50*th* 百分位数是数据集中大于该值和小于该值的元素数量相同的值。此外，数据集的第
    50 百分位数也称为其中位数，第 25 百分位数和第 75 百分位数也称为下四分位数和上四分位数。
- en: 'Say we have the same set of 10 values as earlier, *x = [0,1,1,2,3,4,2,2,0,1]*.
    Let''s first sort this list of values. Upon sorting, we have *x = [0,0,1,1,1,2,2,2,3,4]*.
    To find the twenty-fifth percentile, let''s first calculate the index at which
    the value occurs: *i = (p/100) * n)*, where *p = 25* and *n = 10*. Then, *i =
    2.5*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有与之前相同的 10 个值，*x = [0,1,1,2,3,4,2,2,0,1]*。我们先对这个值列表进行排序。排序后，得到 *x = [0,0,1,1,1,2,2,2,3,4]*。为了找到第
    25 百分位数，我们首先计算该值所在的索引：*i = (p/100) * n)*，其中 *p = 25*，*n = 10*。然后，*i = 2.5*。
- en: Since *i* is not an integer, we round it up to 3 and take the third element
    in the list as the twenty-fifth percentile. The twenty-fifth percentile in the
    given list would then be *1*, which is the third element in our sorted list.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *i* 不是整数，我们将其四舍五入为 3，并取排序列表中的第三个元素作为第 25 百分位数。给定列表中的第 25 百分位数是 *1*，即我们排序后的列表中的第三个元素。
- en: 'Exercise 11: Summary Statistics of Our Dataset'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 11：我们数据集的总结统计
- en: 'In this exercise, we will use the summary statistics functions we read about
    previously to get a basic idea of our dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用之前了解过的总结统计函数，获取我们数据集的基本情况：
- en: 'Read the earthquakes data into a `data` pandas DataFrame and use the `dtyp`
    dictionary we read using the `json` library in the previous exercise to specify
    the data types of each column in the CSV:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将地震数据读入一个 `data` pandas DataFrame，并使用在上一练习中通过 `json` 库读取的 `dtyp` 字典来指定 CSV 中每一列的数据类型：
- en: '[PRE2]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use the `data.info()` function to get an overview of the dataset:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `data.info()` 函数来获取数据集的概览：
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.3: Overview of the dataset](img/C12622_02_03.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.3：数据集概览](img/C12622_02_03.jpg)'
- en: 'Figure 2.3: Overview of the dataset'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.3：数据集概览
- en: 'Print the first five and last five rows of the dataset. The first five rows
    are printed as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据集的前五行和最后五行。前五行打印如下：
- en: '[PRE4]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will be as shown here:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.4: The first five rows](img/C12622_02_04.jpg)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.4：前五行](img/C12622_02_04.jpg)'
- en: 'Figure 2.4: The first five rows'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.4：前五行
- en: 'The last five rows are printed as follows:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后五行打印如下：
- en: '[PRE5]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will be as shown here:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.5: The last five rows](img/C12622_02_05.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.5：最后五行](img/C12622_02_05.jpg)'
- en: 'Figure 2.5: The last five rows'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.5：最后五行
- en: We can see in these outputs that there are 28 columns, but not all of them are
    displayed. Only the first 10 and last 10 columns are displayed, with the ellipses
    representing the fact that there are columns in between that are not displayed.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在这些输出中看到，共有28列，但并不是所有列都显示出来。只显示了前10列和最后10列，省略号表示中间还有其他未显示的列。
- en: 'Use `data.describe()` to find the summary statistics of the dataset. Run `data.describe().T`:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `data.describe()` 查找数据集的摘要统计信息。运行 `data.describe().T`：
- en: '[PRE6]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, `.T` indicates that we're taking a transpose of the DataFrame to which
    it is applied, that is, turning the columns into rows and vice versa. Applying
    it to the `describe()` function allows us to see the output more easily with each
    row in the transposed DataFrame now corresponding to the statistics for a single
    feature.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`.T` 表示我们正在对应用的 DataFrame 进行转置操作，即将列转换为行，反之亦然。将其应用于 `describe()` 函数，可以使我们更容易地查看输出，因为转置后的
    DataFrame 中每一行现在都对应于单个特征的统计数据。
- en: 'We should get an output like this:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们应该得到如下输出：
- en: '![Figure 2.6: Summary statistics](img/C12622_02_06.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6：摘要统计](img/C12622_02_06.jpg)'
- en: 'Figure 2.6: Summary statistics'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.6：摘要统计
- en: Notice here that the `describe()` function only shows the statistics for columns
    with numerical values. This is because we cannot calculate the statistics for
    the columns having non-numerical values.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的 `describe()` 函数仅显示具有数值的列的统计信息。这是因为我们无法为具有非数值的列计算统计信息。
- en: Missing Values
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺失值
- en: When there is no value (that is, a null value) recorded for a particular feature
    in a data point, we say the data is missing. Having missing values in a real dataset
    is inevitable; no dataset is ever perfect. However, it is important to understand
    why the data is missing, and if there is a factor that has affected the loss of
    data. Appreciating and recognizing this allows us to handle the remaining data
    in an appropriate manner. For example, if the data is missing randomly, then it's
    highly likely that the remaining data is still representative of the population.
    However, if the missing data is not random in nature and we assume that it is,
    it could bias our analysis and subsequent modeling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当某个数据点的特征没有记录值（即缺失值）时，我们称数据缺失。在真实的数据集中出现缺失值是不可避免的；没有数据集是完美的。然而，理解为什么数据会缺失，以及是否有某种因素影响了数据丢失是很重要的。理解和认识到这一点可以帮助我们以合适的方式处理剩余数据。例如，如果数据缺失是随机的，那么剩余数据很可能仍能代表总体。然而，如果缺失的数据不是随机的，我们却假设它是随机的，可能会偏向我们的分析和后续建模。
- en: 'Let''s look at the common reasons (or mechanisms) for missing data:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下缺失数据的常见原因（或机制）：
- en: '**Missing Completely at Random** (**MCAR**): Values in a dataset are said to
    be MCAR if there is no correlation whatsoever between the value missing and any
    other recorded variable or external parameter. This means that the remaining data
    is still representative of the population, though this is rarely the case and
    taking missing data to be completely random is usually an unrealistic assumption.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全随机缺失**（**MCAR**）：如果数据集中的缺失值与任何其他记录的变量或外部参数之间没有任何相关性，则该值被认为是MCAR。这意味着剩余的数据仍然能够代表总体，尽管这种情况很少发生，并且将缺失数据视为完全随机通常是一个不现实的假设。'
- en: For example, in a study that involves determining the reason for obesity among
    K12 children, MCAR is when the parents forgot to take their kids to the clinic
    for the study.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在一项研究中，研究K12儿童肥胖的原因，MCAR是父母忘记带孩子去诊所参加研究的情况。
- en: '**Missing at Random** (**MAR**): If the case where the data is missing is related
    to the data that was recorded rather than the data that was not, then the data
    is said to be MAR. Since it''s unfeasible to statistically verify whether data
    is MAR, we''d have to depend on whether it''s a reasonable possibility or not.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机缺失** (**MAR**)：如果数据缺失的情况与已记录的数据相关，而不是与未记录的数据相关，那么数据被认为是 MAR。由于无法通过统计方法验证数据是否为
    MAR，我们只能依赖于其是否为合理的可能性。'
- en: Using the K12 study, missing data in this case is due to parents moving to a
    different city, hence the children had to leave the study; *missingness* has nothing
    to do with the study itself.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 K12 研究中，缺失数据是由于父母搬到其他城市，导致孩子不得不退出研究；*缺失性*与研究本身无关。
- en: '**Missing Not at Random** (**MNAR**): Data that is neither MAR nor MCAR is
    said to be MNAR. This is the case of a non-ignorable non-response, that is, the
    value of the variable that''s missing is related to the reason it is missing.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失非随机** (**MNAR**)：既不是 MAR 也不是 MCAR 的数据被称为 MNAR。这是一个不可忽略的非响应情况，也就是说，缺失的变量值与其缺失的原因相关。'
- en: Continuing with the example of the case study, data would be MNAR if the parents
    were offended by the nature of the study and did not want their children to be
    bullied, so they withdrew their kids from the study.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 继续以案例研究为例，如果父母对研究的性质感到反感，不希望孩子受到欺负，因此他们将孩子从研究中撤出，那么数据将是 MNAR。
- en: Finding Missing Values
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找缺失值
- en: So, now that we know why it's important to familiarize ourselves with the reasons
    behind why our data is missing, let's talk about how we can find these missing
    values in a dataset. For a pandas DataFrame, this is most commonly done using
    the `.isnull()` method on a DataFrame to create a mask of the null values (that
    is, a DataFrame of Boolean values) indicating where the null values exist—a `True`
    value at any position indicates a null value, while a `False` value indicates
    the existence of a valid value at that position.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了熟悉数据缺失原因的重要性，接下来我们来讨论如何在数据集中找到这些缺失值。对于 pandas DataFrame，通常使用 `.isnull()`
    方法创建空值掩码（即一个布尔值的 DataFrame），用以指示空值的位置——在任意位置的 `True` 值表示空值，而 `False` 值表示该位置存在有效值。
- en: Note
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `.isnull()` method can be used interchangeably with the `.isna()` method
    for pandas DataFrames. Both these methods do exactly the same thing—the reason
    there are two methods to do the same thing is because pandas DataFrames were originally
    based on R DataFrames, and hence have reproduced much of the syntax and ideas
    in the latter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`.isnull()` 方法可以与 `.isna()` 方法互换使用。对于 pandas DataFrame，这两个方法的功能完全相同——之所以有两个方法实现相同的功能，是因为
    pandas DataFrame 最初是基于 R DataFrame 的，因此复用了很多 R 中的语法和思想。'
- en: 'It may not be immediately obvious whether the missing data is random or not:
    discovering the nature of missing values across features in a dataset is possible
    through two common visualization techniques:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缺失是否随机可能不会立刻显现：通过两种常见的可视化技术，可以发现数据集中各特征之间缺失值的性质：
- en: '**Nullity matrix**: This is a data-dense display that lets us quickly visualize
    the patterns in data completion. It gives us a quick glance at how the null values
    within a feature (and across features) are distributed, how many there are, and
    how often they appear with other features.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空值矩阵**：这是一种数据密集的展示方式，能帮助我们快速可视化数据补全中的模式。它让我们一眼看到每个特征（以及跨特征）的空值分布、数量以及它们与其他特征出现的频率。'
- en: '**Nullity-correlation heatmap**: This heatmap visually describes the nullity
    relationship (or a data completeness relationship) between each pair of features,
    that is, it measures how strongly the presence or absence of one variable affects
    the presence of another.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空值相关热图**：该热图形象地描述了每对特征之间的空值关系（或数据完整性关系），即它衡量了一个变量的存在或缺失对另一个变量存在的影响强度。'
- en: 'Akin to regular correlation, nullity correlation values range from -1 to 1:
    the former indicating that one variable appears when the other definitely does
    not, and the latter indicating the simultaneous presence of both variables. A
    value of 0 implies that one variable having a null value has no effect on the
    other being null.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与常规相关性类似，空值相关性值的范围从 -1 到 1：前者表示一个变量出现时另一个变量肯定不出现，后者则表示两个变量同时存在。值为 0 表示一个变量的空值对另一个变量为空没有影响。
- en: 'Exercise 12: Visualizing Missing Values'
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 12：可视化缺失值
- en: 'Let''s analyze the nature of the missing values by first looking at the count
    and percentage of missing values for each feature, then plotting a nullity matrix
    and correlation heatmap using the `missingno` library in Python:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来分析缺失值的性质，首先查看每个特征的缺失值数量和百分比，然后使用 Python 中的 `missingno` 库绘制空值矩阵和相关性热图：
- en: Calculate the count and percentage of missing values in each column and arrange
    these in decreasing order. We will use the `.isnull()` function on the DataFrame
    to get a mask. The count of null values in each column can then be found using
    the `.sum()` function over the mask DataFrame. Similarly, the fraction of null
    values can be found using `.mean()` over the mask DataFrame and multiplied by
    100 to convert it into a percentage.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每列缺失值的数量和百分比，并按降序排列。我们将使用 `.isnull()` 函数在 DataFrame 上获取掩码。每列的空值数量可以通过对掩码 DataFrame
    使用 `.sum()` 函数来获得。同样，空值的比例可以通过对掩码 DataFrame 使用 `.mean()` 函数得到，并乘以 100 转换为百分比。
- en: 'Then, we combine the total and percentage of null values into a single DataFrame
    using the `pd.concat()` function, and subsequently sort the rows by percentage
    of missing values and print the DataFrame:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们使用 `pd.concat()` 函数将空值的总数和百分比合并成一个单一的 DataFrame，并按空值的百分比对行进行排序，最后打印出 DataFrame：
- en: '[PRE7]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will be as follows:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.7: Count and percentage of missing values in each column](img/C12622_02_07.jpg)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.7：每列缺失值的数量和百分比](img/C12622_02_07.jpg)'
- en: 'Figure 2.7: Count and percentage of missing values in each column'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.7：每列缺失值的数量和百分比
- en: Here, we can see that the `state`, `total_damage_millions_dollars`, and `damage_millions_dollars`
    columns have over 90% missing values, which means that data for less than 10%
    of data points in the dataset are available for these columns. On the other hand,
    `year`, `flag_tsunami`, `country`, and `region_code` have no missing values.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到 `state`、`total_damage_millions_dollars` 和 `damage_millions_dollars`
    列的缺失值超过 90%，这意味着数据集中这些列中不到 10% 的数据点是可用的。另一方面，`year`、`flag_tsunami`、`country` 和
    `region_code` 列没有缺失值。
- en: 'Plot the nullity matrix. First, we find the list of columns that have any null
    values in them using the `.any()` function on the mask DataFrame from the previous
    step. Then, we use the `missingno` library to plot the nullity matrix for a random
    sample of 500 data points from our dataset, for only those columns that have missing
    values:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制空值矩阵。首先，我们使用 `.any()` 函数在上一步的掩码 DataFrame 上查找包含任何空值的列列表。然后，我们使用 `missingno`
    库绘制空值矩阵，针对数据集中随机抽取的 500 个数据点，仅绘制那些包含缺失值的列：
- en: '[PRE8]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output will be as follows:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.8: The nullity matrix](img/C12622_02_08.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.8：空值矩阵](img/C12622_02_08.jpg)'
- en: 'Figure 2.8: The nullity matrix'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.8：空值矩阵
- en: Here, black lines represent non-nullity while the white lines indicate the presence
    of a null value in that column. At a glance, `location_name` appears to be completely
    populated (we know from the previous step that there is, in fact, only one missing
    value in this column), while `latitude` and `longitude` seem mostly complete,
    but spottier.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，黑色线条代表非空值，而白色线条则表示该列中存在空值。通过一眼看去，`location_name` 似乎完全填充（我们从之前的步骤知道，这一列实际上只有一个缺失值），而
    `latitude` 和 `longitude` 列大多完整，但有一些空白。
- en: The spark line at the right summarizes the general shape of the data completeness
    and points out the rows with the maximum and minimum nullity in the dataset.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 右侧的火花线总结了数据完整性的总体形状，并指出数据集中空值最多和最少的行。
- en: 'Plot the nullity correlation heatmap. We will plot the nullity correlation
    heatmap using the `missingno` library for our dataset, for only those columns
    that have missing values:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制空值相关性热图。我们将使用 `missingno` 库绘制数据集的空值相关性热图，仅针对那些包含空值的列：
- en: '[PRE9]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be as follows:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.9: The nullity correlation heatmap](img/C12622_02_09.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9：空值相关性热图](img/C12622_02_09.jpg)'
- en: 'Figure 2.9: The nullity correlation heatmap'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.9：空值相关性热图
- en: Here, we can also see some boxes labeled `injuries` and `total_injuries`, which
    tells us that there are a few records that have one or the other, but not both.
    These types of cases will require special attention—if the correlation between
    the values of the variables themselves is high, it means that having both is not
    a value and one of the two can be dropped.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还可以看到一些标记为`injuries`和`total_injuries`的框，这告诉我们有一些记录分别包含这两个值中的一个，但不同时包含这两个值。这类情况需要特别关注——如果这些变量之间的相关性较高，那么拥有两个值并没有意义，两个值中的一个可以被删除。
- en: Imputation Strategies for Missing Values
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缺失值的插补策略
- en: 'There are multiple ways of dealing with missing values in a column. The simplest
    way is to simply delete rows having missing values; however, this can result in
    the loss of valuable information from other columns. Another option is to impute
    the data, that is, replace the missing values with a valid value inferred from
    the known part of the data. The common ways in which this can be done are listed
    here:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 处理列中缺失值的方法有多种。最简单的方法是直接删除缺失值所在的行；然而，这样做可能会导致丢失其他列中的有价值信息。另一个选项是插补数据，即用从已知部分数据推断出的有效值替代缺失值。常见的插补方法列举如下：
- en: Create a new value that is distinct from the other values to replace the missing
    values in the column so as to differentiate those rows altogether. Then, use a
    non-linear machine learning algorithm (such as ensemble models or support vectors)
    that can separate the values out.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的值，该值不同于其他值，用来替代列中的缺失值，以便完全区分这些行。然后，使用非线性机器学习算法（如集成模型或支持向量机）来区分这些值。
- en: Use an appropriate central value from the column (mean, median, or mode) to
    replace the missing values.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用列中的适当中心值（均值、中位数或众数）来替换缺失值。
- en: Use a model (such as a K-nearest neighbors or a Gaussian mixture model) to learn
    the best value with which to replace the missing values.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型（例如K近邻或高斯混合模型）来学习替换缺失值的最佳值。
- en: 'Python has a few functions that are useful for replacing null values in a column
    with a static value. One way to do this is using the inherent pandas `.fillna(0)`
    function: there is no ambiguity in imputation here—the static value with which
    to substitute the null data point in the column is the argument being passed to
    the function (the value in the brackets).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Python有一些函数对于用静态值替换列中的空值非常有用。实现这一功能的一种方法是使用内建的pandas `.fillna(0)`函数：在插补中没有歧义——用来替代列中空数据点的静态值即为传递给函数的参数（括号中的值）。
- en: 'However, if the number of null values in a column is significant and it''s
    not immediately obvious what the appropriate central value is that can be used
    to replace each null value, then we can either delete the rows having null values
    or delete the column altogether from the modeling perspective, as it may not add
    any significant value. This can be done by using the `.dropna()` function on the
    DataFrame. The parameters that can be passed to the function are:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果列中空值的数量较多，并且无法立即明确可以用来替换每个空值的适当中心值，那么我们可以选择删除包含空值的行，或从建模的角度完全删除该列，因为它可能不会带来任何显著的价值。这可以通过在DataFrame上使用`.dropna()`函数来完成。可以传递给该函数的参数包括：
- en: '`axis`: This defines whether to drop rows or columns, which is determined by
    assigning the parameter a value of 0 or 1 respectively.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axis`：此参数定义了是删除行还是删除列，通过将参数分别赋值为0或1来确定。'
- en: '`how`: A value of `all` or `any` can be assigned to this parameter to indicate
    whether the row/column should contain all null values to drop the column, or whether
    to drop the column if there is at least one null value.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`how`：可以将`all`或`any`的值赋给此参数，以指示行/列是否应包含所有空值以删除该列，或者是否至少有一个空值时删除该列。'
- en: '`thresh`: This defines the minimum number of null values the row/column should
    have in order to be dropped.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thresh`：此参数定义了行/列必须具有的最小空值数量，才会被删除。'
- en: Additionally, if an appropriate replacement for a null value for a categorical
    feature cannot be determined, a possible alternative to deleting the column is
    to create a new category in the feature that can represent the null values.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果无法为分类特征的空值确定合适的替代值，可以考虑在特征中创建一个新的类别来表示空值，而不是删除该列。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: If it is immediately obvious how a null value for a column can be replaced from
    an intuitive understanding or domain knowledge, then we can replace the value
    on the spot. In many cases, however, such inferences become more obvious at later
    stages in the exploration process. In these cases, we can substitute null values
    as and when we find an appropriate way to do so.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果从直观理解或领域知识上立即能够看出如何替换列的空值，那么我们可以当场替换这些值。然而，在许多情况下，这些推断会在探索过程的后期变得更加明显。在这些情况下，我们可以根据找到的合适方法，随时替换空值。
- en: 'Exercise 13: Imputation Using pandas'
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 13：使用 pandas 填充
- en: Let's look at missing values and replace them with zeros in time-based (continuous)
    features having at least one null value (month, day, hour, minute, and second).
    We do this because for cases where we do not have recorded values, it would be
    safe to assume that the events take place at the beginning of the time duration.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看缺失值，并将它们替换为零，针对那些具有至少一个空值的基于时间的（连续）特征（如月份、日期、小时、分钟和秒）。我们这样做是因为对于没有记录值的情况，可以安全地假设事件发生在时间段的开始。
- en: 'Create a list containing the names of the columns whose values we want to impute:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个列表，包含我们想要填充值的列名：
- en: '[PRE10]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Impute the null values using `.fillna()`. We will replace the missing values
    in these columns with `0` using the inherent pandas `.fillna()` function and pass
    `0` as an argument to the function:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.fillna()` 填充空值。我们将使用 pandas 的内建 `.fillna()` 函数，将这些列中的缺失值替换为 `0`，并将 `0`
    作为参数传递给函数：
- en: '[PRE11]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Use the `.info()` function to view null value counts for the imputed columns:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.info()` 函数查看填充列的空值计数：
- en: '[PRE12]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will be as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.10: Null value counts](img/C12622_02_10.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.10：空值计数](img/C12622_02_10.jpg)'
- en: 'Figure 2.10: Null value counts'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.10：空值计数
- en: As we can see now, all values for our features in the DataFrame are now non-null.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，我们可以看到，数据框中所有特征的值都已经是非空值。
- en: 'Exercise 14: Imputation Using scikit-learn'
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 14：使用 scikit-learn 填充
- en: 'Let''s replace the null values in the description-related categorical features
    using scikit-learn''s `SimpleImputer` class. In *Exercise 12: Visualizing Missing
    Values*, we saw that almost all of these features comprised more than 50% of null
    values in the data. Replacing these null values with a central value might bias
    any model we try to build using the features, deeming them irrelevant. Let''s
    instead replace the null values with a separate category, having value `NA`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 scikit-learn 的 `SimpleImputer` 类，替换与描述相关的分类特征中的空值。在*练习 12：可视化缺失值*中，我们看到几乎所有这些特征中超过
    50% 的值都是空值。将这些空值替换为中心值可能会对我们试图构建的模型产生偏差，使它们变得不相关。因此，我们将空值替换为一个单独的类别，值为 `NA`：
- en: 'Create a list containing the names of the columns whose values we want to impute:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个列表，包含我们想要填充值的列名：
- en: '[PRE13]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create an object of the `SimpleImputer` class. Here, we first create an `imp`
    object of the `SimpleImputer` class and initialize it with parameters that represent
    how we want to impute the data. The parameters we will pass to initialize the
    object are:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `SimpleImputer` 类的对象。在这里，我们首先创建一个 `imp` 对象，并使用表示如何填充数据的参数初始化它。我们将传递给对象初始化的参数包括：
- en: '`missing_values`: This is the placeholder for the missing values, that is,
    all occurrences of the values in the `missing_values` parameter will be imputed.'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`missing_values`：这是缺失值的占位符，即所有出现在 `missing_values` 参数中的值将被填充。'
- en: '`strategy`: This is the imputation strategy, which can be one of `mean`, `median`,
    `most_frequent` (that is, the mode), or `constant`. While the first three can
    only be used with numeric data and will replace missing values using the specified
    central value along each column, the last one will replace missing values with
    a constant as per the `fill_value` parameter.'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`strategy`：这是填充策略，可以是 `mean`、`median`、`most_frequent`（即众数）或 `constant`。前三者只能用于数值数据，并会通过指定的中心值替换每列中的缺失值，而最后一个则会根据
    `fill_value` 参数，用常数替换缺失值。'
- en: '`fill_value`: This specifies the value with which to replace all occurrences
    of `missing_values`. If left to the default, the imputed value will be `0` when
    imputing numerical data and the `missing_value` string for strings or object data
    types:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`fill_value`：指定用来替换所有 `missing_values` 的值。如果保持默认设置，当填充数值数据时，填充值将为 `0`，而字符串或对象数据类型将使用
    `missing_value` 字符串。'
- en: '[PRE14]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Perform the imputation. We will use `imp.fit_transform()` to actually perform
    the imputation. It takes the DataFrame with null values as input and returns the
    imputed DataFrame:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行填充。我们将使用 `imp.fit_transform()` 实际执行填充。它将带有空值的 DataFrame 作为输入，并返回填充后的 DataFrame：
- en: '[PRE15]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use the `.info()` function to view null value counts for the imputed columns:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.info()` 函数查看填充列的空值计数：
- en: '[PRE16]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output will be as follows:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.11: The null value counts](img/C12622_02_11.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.11：空值计数](img/C12622_02_11.jpg)'
- en: 'Figure 2.11: The null value counts'
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.11：空值计数
- en: 'Exercise 15: Imputation Using Inferred Values'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 15：使用推断值进行填充
- en: 'Let''s replace the null values in the continuous `damage_millions_dollars`
    feature with information from the categorical `damage_description` feature. Although
    we may not know the exact dollar amount that was incurred, the categorical feature
    gives us information on the range of the amount that was incurred due to damage
    from the earthquake:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用来自类别 `damage_description` 特征的信息替换连续的 `damage_millions_dollars` 特征中的空值。尽管我们可能不知道确切的损失金额，但类别特征为我们提供了因地震造成的损失金额的范围信息：
- en: 'Find how many rows have null `damage_millions_dollars` values, and how many
    of those have non-null `damage_description` values:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出有多少行的 `damage_millions_dollars` 值为空，以及其中有多少行的 `damage_description` 值不为空：
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be as follows:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/C12622_02_12.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12622_02_12.jpg)'
- en: 'Figure 2.12: Count of rows with null values'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.12：包含空值的行数
- en: As we can see, 3,849 of 5,594 null values can be easily substituted with the
    help of another variable.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，5,594 个空值中有 3,849 个可以通过另一个变量轻松替换。
- en: 'Find the mean `damage_millions_dollars` value for each category. Since each
    of the categories in `damage_description` represent a range of values, we find
    the mean `damage_millions_dollars` value for each category from the non-null values
    already available. These provide a reasonable estimate for the most likely value
    for that category:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出每个类别的平均 `damage_millions_dollars` 值。由于 `damage_description` 中的每个类别代表一系列值，我们从已存在的非空值中找到每个类别的平均
    `damage_millions_dollars` 值。这些为该类别提供了合理的最可能值估算：
- en: '[PRE18]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output will be as follows:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.13: The mean damage_millions_dollars value for each category](img/C12622_02_13.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.13：每个类别的平均 damage_millions_dollars 值](img/C12622_02_13.jpg)'
- en: 'Figure 2.13: The mean damage_millions_dollars value for each category'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.13：每个类别的平均 damage_millions_dollars 值
- en: Store the mean values as a dictionary. In this step, we will convert the DataFrame
    containing the mean values to a dictionary (a Python `dict` object) so that accessing
    them is convenient.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将平均值存储为字典。在此步骤中，我们将包含平均值的 DataFrame 转换为字典（Python `dict` 对象），这样访问它们就更加方便。
- en: 'Additionally, since the value for the newly created `NA` category (the imputed
    value in the previous exercise) was `NaN` and the value for the `0` category was
    absent (no rows had `damage_description` equal to `0` in the dataset), we explicitly
    added these values in the dictionary as well:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，由于新创建的 `NA` 类别（在前一个练习中的填充值）的值为 `NaN`，并且 `0` 类别的值缺失（数据集中没有 `damage_description`
    等于 `0` 的行），我们也明确地将这些值添加到字典中：
- en: '[PRE19]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output will be as follows:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.14: The dictionary of mean values](img/C12622_02_14.jpg)'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.14：平均值字典](img/C12622_02_14.jpg)'
- en: 'Figure 2.14: The dictionary of mean values'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.14：平均值字典
- en: 'Create a series of replacement values. For each value in the `damage_description`
    column, we map the categorical value onto the mean value using the `map` function.
    The `.map()` function is used to map the keys in the column to the corresponding
    values for each element from the `replacement_values` dictionary:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一系列替代值。对于 `damage_description` 列中的每个值，我们使用 `map` 函数将类别值映射到平均值。`.map()` 函数用于将列中的键映射到
    `replacement_values` 字典中每个元素的对应值：
- en: '[PRE20]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Replace null values in the column. We do this by using `np.where` as a ternary
    operator: the first argument is the mask, the second is the series from which
    to take the value if the mask is positive, and the third is the series from which
    to take the value if the mask is negative.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替换列中的空值。我们通过使用 `np.where` 作为三元运算符来实现：第一个参数是掩码，第二个是如果掩码为正时从中获取值的系列，第三个是如果掩码为负时从中获取值的系列。
- en: 'This ensures that the array returned by `np.where` only replaces the null values
    in `damage_millions_dollars` with values from the `imputed_values` series:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这确保了 `np.where` 返回的数组仅将 `damage_millions_dollars` 中的空值替换为 `imputed_values` 序列中的值：
- en: '[PRE21]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Use the `.info()` function to view null value counts for the imputed columns:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.info()` 函数查看填充列的空值计数：
- en: '[PRE22]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output will be as follows:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.15: The null value counts](img/C12622_02_15.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.15：空值计数](img/C12622_02_15.jpg)'
- en: 'Figure 2.15: The null value counts'
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.15：空值计数
- en: We can see that, after replacement, there are no null values in the `damage_millions_dollars`
    column.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，替换后 `damage_millions_dollars` 列中没有空值。
- en: 'Activity 2: Summary Statistics and Missing Values'
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 2：汇总统计与缺失值
- en: 'In this activity, we''ll revise some of the summary statistics and missing
    value exploration we have looked at thus far in this chapter. We will be using
    a new dataset, taken from Kaggle''s *House Prices: Advanced Regression Techniques*
    competition (available at [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    or on GitHub at [https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)).
    While the Earthquakes dataset used in the exercises is aimed at solving a classification
    problem (when the target variable has only discrete values), the dataset we will
    use in the activities will be aimed at solving a regression problem (when the
    target variable takes on a range of continuous values). We''ll use pandas functions
    to generate summary statistics and visualize missing values using a nullity matrix
    and nullity correlation heatmap.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '在本活动中，我们将回顾本章中到目前为止看到的一些汇总统计和缺失值分析。我们将使用一个新的数据集，来自 Kaggle 的 *House Prices:
    Advanced Regression Techniques* 比赛（可以在 [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    或 GitHub 上的 [https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)
    获取）。虽然在练习中使用的地震数据集是针对解决分类问题的（当目标变量只有离散值时），我们将在活动中使用的数据集将用于解决回归问题（当目标变量包含连续值的范围时）。我们将使用
    pandas 函数生成汇总统计，并通过空值矩阵和空值相关热图可视化缺失值。'
- en: 'The steps to be performed are as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Read the data.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据。
- en: Use pandas' `.info()` and `.describe()` methods to view the summary statistics
    of the dataset.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `.info()` 和 `.describe()` 方法查看数据集的汇总统计信息。
- en: Find the total count and total percentage of missing values in each column of
    the DataFrame and display them for columns having at least one null value, in
    descending order of missing percentages.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找每列的缺失值总数和缺失值百分比，并按缺失百分比的降序显示至少有一个空值的列。
- en: Plot the nullity matrix and nullity correlation heatmap.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制空值矩阵和空值相关热图。
- en: Delete the columns having more than 80% of values missing.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除缺失值超过 80% 的列。
- en: Replace null values in the `FireplaceQu` column with `NA` values.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `FireplaceQu` 列中的空值替换为 `NA` 值。
- en: Note
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 307.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 307 页找到。
- en: Distribution of Values
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值的分布
- en: In this section, we'll look at how individual variables behave—what kind of
    values they take, what the distribution across those values is, and how those
    distributions can be represented visually.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看各个变量的行为——它们取什么值，这些值的分布如何，以及如何以可视化方式表示这些分布。
- en: Target Variable
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标变量
- en: The target variable can either have values that are continuous (in the case
    of a regression problem) or discrete (as in the case of a classification problem).
    The problem statement we're looking at in this chapter involves predicting whether
    or not an earthquake caused a tsunami, that is, the `flag_tsunami` variable, which
    takes on two discrete values only—making it a classification problem.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量可以是连续值（回归问题的情况）或离散值（分类问题的情况）。本章讨论的问题是预测地震是否引发海啸，也就是 `flag_tsunami` 变量，它只有两个离散值——因此是一个分类问题。
- en: One way of visualizing how many earthquakes resulted in tsunamis and how many
    didn't is a bar chart, where each bar represents a single discrete value of the
    variable, and the height of the bars is equal to the count of the data points
    having the corresponding discrete value. This gives us a good comparison of the
    absolute counts of each category.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化有多少地震引发了海啸，以及有多少没有引发海啸的一种方法是柱状图，其中每个柱子代表一个离散值，柱子的高度等于具有相应离散值的数据点的计数。这为我们提供了每个类别的绝对计数的良好比较。
- en: 'Exercise 16: Plotting a Bar Chart'
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习16：绘制柱状图
- en: 'Let''s look at how many of the earthquakes in our dataset resulted in a tsunami.
    We will do this by using the `value_counts()` method over the column and directly
    using the `.plot(kind=''bar'')` function on the returned pandas series. Follow
    these steps:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看数据集中有多少地震引发了海啸。我们将通过对列使用`value_counts()`方法，并直接对返回的pandas系列使用`.plot(kind='bar')`函数来完成。按照以下步骤操作：
- en: 'Use `plt.figure()` to initiate the plotting:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plt.figure()` 初始化绘图：
- en: '[PRE23]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, type in our primary plotting command:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，输入我们的主要绘图命令：
- en: '[PRE24]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Set the display parameters and display the plot:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置显示参数并显示图表：
- en: '[PRE25]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be as follows:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.16: Bar chart showing how many earthquakes resulted in a tsunami](img/C12622_02_16.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16：柱状图显示有多少地震引发了海啸](img/C12622_02_16.jpg)'
- en: 'Figure 2.16: Bar chart showing how many earthquakes resulted in a tsunami'
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.16：柱状图显示有多少地震引发了海啸
- en: From this bar plot, we can see that most of the earthquakes did not result in
    tsunamis, and that less than one-third of the earthquakes did. This shows us that
    the dataset is slightly imbalanced.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个柱状图中，我们可以看到大多数地震没有引发海啸，而且不到三分之一的地震引发了海啸。这显示出数据集略微失衡。
- en: 'Let''s look more closely at what these Matplotlib commands do:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看一下这些Matplotlib命令的作用：
- en: '`plt.figure(figsize=(8,6))`: This command defines how big our plot should be,
    by providing width and height values. This is always the first command before
    any plotting command is written.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plt.figure(figsize=(8,6))`：此命令定义了我们的图表大小，通过提供宽度和高度的值。这是所有绘图命令之前的第一条命令。'
- en: '`plt.xlabel()` and `plt.ylabel()`: These commands take a string as input, and
    allow us to specify what the labels for the *X* and *Y* axes on the plot should
    be.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plt.xlabel()` 和 `plt.ylabel()`：这些命令接受字符串作为输入，允许我们指定图表中*X*轴和*Y*轴的标签。'
- en: '`plt.show()`: This is the final command written when plotting that displays
    the plot inline within the Jupyter notebook.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plt.show()`：这是绘图时写入的最后一条命令，它将图表以内联方式显示在Jupyter Notebook中。'
- en: Categorical Data
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别数据
- en: 'Categorical variables are ones that take discrete values representing different
    categories or levels of observation that can either be string objects, or integer
    values. For example, our target variable, `flag_tsunami`, is a categorical variable
    having two categories: `Tsu` and `No`.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量是那些具有离散值，表示不同类别或观察水平的变量，可以是字符串对象或整数值。例如，我们的目标变量`flag_tsunami`是一个类别变量，具有两个类别：`Tsu`和`No`。
- en: 'Categorical variables can be of two types:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 类别变量可以分为两种类型：
- en: '`location_name`. The values that this variable takes cannot be said to be ordered,
    that is, one location is not *greater* than the other. Similarly, more examples
    of such a variable would be color, types of footwear, ethnicity type, and so on.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`location_name`。这个变量的值不能说是有序的，也就是说，一个地点并不*大于*另一个地点。类似的变量示例还包括颜色、鞋类类型、种族类型等。'
- en: '`damage_description`, since each value represents an increasing value of damage
    incurred. Another example could be day of the week, which would have values from
    Monday to Sunday, which have some order associated with them and we know that
    Thursday comes after Wednesday but before Friday.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`damage_description`，因为每个值表示逐渐增加的损害值。另一个例子可以是星期几，其值从星期一到星期天，具有一定的顺序关系，我们知道星期四在星期三之后，但在星期五之前。'
- en: Although ordinal variables can be represented by object data types, they are
    often represented as numerical data types as well, often making it difficult to
    differentiate between them and continuous variables.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管有序变量可以通过对象数据类型表示，但它们通常也表示为数值数据类型，这通常使得它们与连续变量之间的区分变得困难。
- en: One of the major challenges faced when dealing with categorical variables in
    a dataset is high cardinality, that is, a large number of categories or distinct
    values with each value appearing a relatively small number of times. For example,
    `location_name` has a large number of unique values, with each value occurring
    a small fraction of times in the dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据集中的类别变量时面临的主要挑战之一是高基数，即大量的类别或不同的值，其中每个值在数据集中的出现次数相对较少。例如，`location_name`具有大量的唯一值，每个值在数据集中的出现频率较低。
- en: Additionally, non-numerical categorical variables will always require some form
    of preprocessing to be converted into a numerical format so that they can be ingested
    for training by a machine learning model. It can be a challenge to encode categorical
    variables numerically without losing out on contextual information that despite
    being easy for humans to interpret (due to domain knowledge or otherwise just
    plain common sense), would be hard for a computer to automatically understand.
    For example, a geographical feature such as country or location name by itself
    would give no indication of the geographical proximity of different values, but
    that might just be an important feature—what if earthquakes that occur at locations
    in South-East Asia trigger more tsunamis than those that occur in Europe? There
    would be no way of capturing that information by merely numerically encoding the
    feature.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非数值型的类别变量总是需要某种形式的预处理，将其转换为数值格式，以便机器学习模型能够读取并进行训练。在没有丢失上下文信息的情况下，如何将类别变量编码为数值型是一个挑战。尽管这些信息对于人类来说（由于领域知识或常识）非常容易理解，但计算机却很难自动理解。例如，像国家或地点名称这样的地理特征本身并不能表明不同值之间的地理接近性，但这可能是一个重要特征——如果发生在东南亚地区的地震比欧洲地区的地震触发更多海啸呢？仅仅通过数值编码特征，无法捕获这些信息。
- en: 'Exercise 17: Datatypes for Categorical Variables'
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 17：类别变量的数据类型
- en: 'Let''s find which variables in our Earthquake dataset are categorical and which
    are continuous. As we now know, categorical variables can also have numerical
    values, so having a numeric data type doesn''t guarantee that a variable is continuous:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出地震数据集中哪些变量是类别型的，哪些是连续型的。正如我们现在所知道的，类别变量也可以具有数值值，因此，拥有数值数据类型并不意味着变量就是连续型的：
- en: 'Find all the columns that are numerical and object types. We use the `.select_dtypes()`
    method on the DataFrame to create a subset DataFrame having numeric (`np.number`)
    and categorical (`np.object`) columns, and then print the column names for each.
    For numeric columns, use this:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有数值型和对象型的列。我们在 DataFrame 上使用 `.select_dtypes()` 方法，创建一个包含数值型（`np.number`）和类别型（`np.object`）列的子集
    DataFrame，然后打印每个列的列名。对于数值列，请使用以下方法：
- en: '[PRE26]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output will be as follows:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.17: All columns that are numerical ](img/C12622_02_17.jpg)'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.17：所有数值型的列](img/C12622_02_17.jpg)'
- en: 'Figure 2.17: All columns that are numerical'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.17：所有数值型的列
- en: 'For categorical columns, use this:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于类别列，请使用以下方法：
- en: '[PRE27]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output will be as follows:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.18: All columns that are object types](img/C12622_02_18.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.18：所有对象类型的列](img/C12622_02_18.jpg)'
- en: 'Figure 2.18: All columns that are object types'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.18：所有对象类型的列
- en: Here, it is evident that the columns of object type are categorical variables.
    To differentiate between the categorical and continuous variables from the numeric
    columns, let's see how many unique values there are for each of these features.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里显然可以看出，对象类型的列是类别变量。为了区分数值列中的类别变量和连续变量，让我们查看这些特征的唯一值数量。
- en: 'Find the number of unique values for numeric features. We use the `select_dtypes`
    method on the DataFrame to find the number of unique values in each column and
    sort the resulting series in ascending order. For numeric columns, use this:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出数值特征的唯一值数量。我们在 DataFrame 上使用 `select_dtypes` 方法，找到每一列的唯一值数量，并将结果序列按升序排序。对于数值列，请使用以下方法：
- en: '[PRE28]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output will be as follows:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.19: Number of unique values for numeric features](img/C12622_02_19.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.19：数值特征的唯一值数量](img/C12622_02_19.jpg)'
- en: 'Figure 2.19: Number of unique values for numeric features'
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.19：数值特征的唯一值数量
- en: 'For categorical columns, use this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类别列，请使用以下方法：
- en: '[PRE29]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output will be as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.20: Number of unique values for categorical columns](img/C12622_02_20.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.20：类别列的唯一值数量](img/C12622_02_20.jpg)'
- en: 'Figure 2.20: Number of unique values for categorical columns'
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.20：分类列的唯一值数量
- en: For the numeric variables, we can see that the top nine have significantly fewer
    unique values than the remaining rows, and it's likely that these are categorical
    variables. However, we must keep in mind that it is possible that some of them
    might just be continuous variables with a low range of rounded-up values. Also,
    `month` and `day` would not be considered categorical variables here.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值型变量，我们可以看到前九个变量的唯一值显著少于其余行，这些变量很可能是分类变量。然而，我们必须记住，其中一些可能只是具有较小范围的四舍五入值的连续变量。另外，`month`
    和 `day` 在这里不会被视为分类变量。
- en: 'Exercise 18: Calculating Category Value Counts'
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 18：计算类别值计数
- en: 'For columns with categorical values, it would be useful to see what the unique
    values (categories) of the feature are, along with what the frequencies of these
    categories are, that is, how much does each distinct value occur in the dataset.
    Let''s find the number of occurrences of each `0` to `4` label and `NaN` values
    for the `injuries_description` categorical variable:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有分类值的列，查看该特征的唯一值（类别）以及这些类别的频率将非常有用，也就是说，每个不同的值在数据集中出现的次数。我们来找出 `injuries_description`
    分类变量中每个 `0` 到 `4` 标签和 `NaN` 值的出现次数：
- en: 'Use the `value_counts()` function on the `injuries_description` column to find
    the frequency of each category. Using `value_counts` gives us the frequencies
    of each value in decreasing order in the form of a pandas series:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 `injuries_description` 列使用 `value_counts()` 函数来找出每个类别的频率。使用 `value_counts`
    会以降序形式返回每个值的频率，并以 pandas 系列的形式显示：
- en: '[PRE30]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output should be as follows:'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 2.21: Frequency of each category](img/C12622_02_21.jpg)'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.21：每个类别的频率](img/C12622_02_21.jpg)'
- en: 'Figure 2.21: Frequency of each category'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.21：每个类别的频率
- en: 'Sort the values in increasing order of the ordinal variable. If we want the
    frequencies in the order of the values themselves, we can reset the index to give
    us a DataFrame and sort values by the index (that is, the ordinal variable):'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照顺序变量的升序排序这些值。如果我们希望按值本身的顺序显示频率，我们可以重置索引，从而得到一个 DataFrame，并按索引（即顺序变量）排序值：
- en: '[PRE31]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Figure 2.22: Sorted values](img/C12622_02_22.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.22：排序后的值](img/C12622_02_22.jpg)'
- en: 'Figure 2.22: Sorted values'
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.22：排序后的值
- en: 'Exercise 19: Plotting a Pie Chart'
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 19：绘制饼图
- en: 'Since our target variable in our sample data is categorical, the example in
    *Exercise 16: Plotting a Bar Chart* showed us one way of visualizing how the categorical
    values are distributed (using a bar chart). Another plot that can make it easy
    to see how each category functions as a fraction of the overall dataset is a pie
    chart. Let''s plot a pie chart to visualize the distribution of the discrete values
    of the `damage_description` variable:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们示例数据中的目标变量是分类的，*练习 16：绘制条形图* 中的示例展示了可视化分类值分布的一种方式（使用条形图）。另一种可以帮助我们轻松查看每个类别在整个数据集中所占比例的图表是饼图。让我们绘制一个饼图来可视化
    `damage_description` 变量的离散值分布：
- en: 'Format the data into the form that needs to be plotted. Here, we run `value_counts()`
    over the column and sort the series by index:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据格式化成需要绘制的形式。在这里，我们对该列使用 `value_counts()` 并按索引排序系列：
- en: '[PRE32]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the pie chart. The `plt.pie()` category plots the pie chart using the
    count data. We will use the same three steps for plotting as described in *Exercise
    16: Plotting a Bar Chart*:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制饼图。`plt.pie()` 分类函数使用计数数据绘制饼图。我们将按照 *练习 16：绘制条形图* 中描述的相同三步进行绘图：
- en: '[PRE33]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output will be:'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将是：
- en: '![Figure 2.23: Pie chart showing counts for damage_description categories](img/C12622_02_23.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.23：显示 `damage_description` 类别计数的饼图](img/C12622_02_23.jpg)'
- en: 'Figure 2.23: Pie chart showing counts for damage_description categories'
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.23：显示 `damage_description` 类别计数的饼图
- en: Continuous Data
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续数据
- en: 'Continuous variables can take any number of values and are usually integer
    (for example, number of deaths) or float data types (for example, the height of
    a mountain). It''s useful to get an idea of the basic statistics of the values
    in the feature: the minimum, maximum, and percentile values we see from the output
    of the `describe()` function gives us a fair estimate of this.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 连续变量可以取任意数量的值，通常是整数（例如，死亡人数）或浮动数据类型（例如，山脉的高度）。了解特征中值的基本统计信息是非常有用的：`describe()`
    函数的输出显示的最小值、最大值和百分位数给我们提供了一个合理的估算。
- en: However, for continuous variables, it is also very useful to see how the values
    are distributed in the range they operate in. Since we cannot simply find the
    counts of individual values, instead we order the values in ascending order, group
    them into evenly-sized intervals, and find the counts for each interval. This
    gives us the underlying frequency distribution, and plotting this gives us a histogram,
    which allows us to examine the shape, central values, and amount of variability
    in the data.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于连续变量来说，了解其在操作范围内的分布情况也非常有用。由于我们不能简单地计算各个值的计数，我们会将值按升序排列，将其分组为等间隔的区间，然后计算每个区间的计数。这为我们提供了底层的频率分布，绘制该分布便能得到直方图，从而让我们查看数据的形态、中心值以及变异性。
- en: Histograms give us an easy view of the data that we're looking at. They tell
    us about the behavior of the values at a glance in terms of the underlying distribution
    (for example, a normal or exponential distribution), the presence of outliers,
    skewness, and more.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图为我们提供了一个简洁的视角，帮助我们了解正在查看的数据。它们让我们一目了然地看到数据的行为，揭示了底层分布（例如正态分布或指数分布）、异常值、偏度等信息。
- en: Note
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is easy to get confused between a bar chart and a histogram. The major difference
    is that a histogram is used to plot continuous data that has been binned to visualize
    the frequency distribution, while bar charts can be used for a variety of other
    use cases, including to represent categorical variables as we have done. Additionally,
    it is not just the height of the bar that indicates the frequency of that bin,
    but also the width of the bin itself, which is not the case in a bar chart.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易将条形图与直方图混淆。主要的区别在于，直方图用于绘制已被分组的连续数据以可视化频率分布，而条形图可以用于多种其他用途，包括表示我们之前处理的类别变量。此外，条形图中的条形高度表示该箱子的频率，但直方图中的宽度也会影响频率的表示，这在条形图中并不适用。
- en: One of the most common frequency distributions is a Gaussian (or normal) distribution.
    This is a symmetric distribution that has a bell-shaped curve, which indicates
    that the values near the middle of the range have the highest occurrences in the
    dataset with a symmetrically decreasing frequency of occurrences as we move away
    from the middle.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的频率分布之一是高斯（或正态）分布。这是一种对称分布，具有钟形曲线，表示接近中间值的范围在数据集中出现频率最高，随着远离中间部分，频率对称性地减少。
- en: It is a probability distribution and the area under the curve equals one.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个概率分布，曲线下的面积等于一。
- en: '![Figure 2.24: Normal distribution](img/C12622_02_24.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24：正态分布](img/C12622_02_24.jpg)'
- en: 'Figure 2.24: Normal distribution'
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.24：正态分布
- en: '**Skewness**'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏度**'
- en: A distribution is said to be skewed if it is not symmetric in nature, and skewness
    measures the asymmetry of a variable about its mean. The value can be positive
    or negative (or undefined). In the former case, the tail is on the right-hand
    side of the distribution, while the latter indicates that the tail is on the left-hand
    side.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个分布不是对称的，我们称其为偏斜的，偏度衡量的是变量相对于其均值的非对称性。偏度的值可以是正值、负值（或未定义）。在前一种情况下，尾巴位于分布的右侧，而后一种情况则表示尾巴位于左侧。
- en: However, it must be noted that a thick and short tail would have the same effect
    on the value of skewness as a long, thin tail.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须注意的是，厚而短的尾部对偏度的影响与长而细的尾部相同。
- en: '**Kurtosis**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**峰度**'
- en: Kurtosis is a measure of the *tailedness* of the distribution of a variable
    and is used to measure the presence of outliers in one tail versus the other.
    A high value of kurtosis indicates a fatter tail and the presence of outliers.
    In a similar way to the concept of skewness, kurtosis also describes the shape
    of the distribution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 峰度是衡量变量分布的*尾部形态*的一个指标，用来衡量一侧尾部是否存在异常值。较高的峰度值表示尾部较厚，且存在异常值。与偏度的概念类似，峰度也描述了分布的形态。
- en: 'Exercise 20: Plotting a Histogram'
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习20：绘制直方图
- en: 'Let''s plot the histogram for the `eq_primary` feature using the Seaborn library:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Seaborn库绘制`eq_primary`特征的直方图：
- en: 'Use `plt.figure()` to initiate the plotting:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plt.figure()`来启动绘图：
- en: '[PRE34]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`sns.distplot()` is the primary command that we will use to plot the histogram.
    The first parameter is the one-dimensional data over which to plot the histogram,
    the bins parameter defines the number and size of the bins. Use this as follows:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sns.distplot()`是我们用来绘制直方图的主要命令。第一个参数是用来绘制直方图的单维数据，bins参数定义了箱子的数量和大小。使用方式如下：'
- en: '[PRE35]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Display the plot using `plt.show()`:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plt.show()` 显示图表：
- en: '[PRE36]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output will be as follows:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.25: Histogram for the eq_primary feature](img/C12622_02_25.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.25：eq_primary 特征的直方图](img/C12622_02_25.jpg)'
- en: 'Figure 2.25: Histogram for the eq_primary feature'
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.25：eq_primary 特征的直方图
- en: The plot gives us a normed (or normalized) histogram, which means that the area
    under the bars of the histogram equals unity. Additionally, the line over the
    histogram is the **kernel density estimate**, which gives us an idea of what the
    probability distribution for the variable would look like.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 该图给出了一个标准化（或归一化）直方图，这意味着直方图下方的区域总和为1。此外，直方图上的曲线是**核密度估计**，它给我们提供了变量的概率分布的形态。
- en: From the plot, we can see that the values of `eq_primary` lie mostly between
    5 and 8, which means that most earthquakes had a magnitude with a moderate to
    high value, with barely any earthquakes having a low or very high magnitude.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，`eq_primary` 的值大多位于 5 到 8 之间，这意味着大多数地震的震级是中等到高值，几乎没有地震震级非常低或非常高。
- en: 'Exercise 21: Skew and Kurtosis'
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 21：偏度与峰度
- en: 'Let''s calculate the skew and kurtosis values for all the features in the dataset
    using the core pandas functions available to us:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 pandas 核心函数计算数据集中所有特征的偏度和峰度值：
- en: 'Use the `.skew()` DataFrame method to calculate the skew for all features and
    then sort the values in ascending order:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.skew()` 数据框方法计算所有特征的偏度，然后按升序排序值：
- en: '[PRE37]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output will be:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将是：
- en: '![](img/C12622_02_26.jpg)'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12622_02_26.jpg)'
- en: 'Figure 2.26: Skew values for all the features in the dataset'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.26：数据集中所有特征的偏度值
- en: 'Use the `.kurt()` DataFrame method to calculate the kurtosis for all features:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.kurt()` 数据框方法计算所有特征的峰度：
- en: '[PRE38]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将是：
- en: '![Figure 2.27: Kurtosis values for all the features in the dataset](img/C12622_02_27.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.27：数据集中所有特征的峰度值](img/C12622_02_27.jpg)'
- en: 'Figure 2.27: Kurtosis values for all the features in the dataset'
  id: totrans-325
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.27：数据集中所有特征的峰度值
- en: Here, we can see that the kurtosis values for some variables deviate significantly
    from 0\. This means that these columns have a long tail. But the values that are
    at the tail end of these variables (which indicate the number of people dead,
    injured, and the monetary value of damage), in our case, may be outliers that
    we may need to pay special attention to. Larger values might, in fact, indicate
    an additional force that added to the devastation caused by an earthquake, that
    is, a tsunami.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到某些变量的峰度值显著偏离0。这意味着这些列具有长尾。但是，这些变量尾部的值（即表示死亡、受伤人数以及损失金额的数值）在我们的案例中，可能是离群值，我们可能需要特别关注它们。较大的值可能实际上表示额外的因素，增加了由地震引起的破坏，即海啸。
- en: 'Activity 3: Visually Representing the Distribution of Values'
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 3：可视化表示值的分布
- en: 'In this activity, we will revise what we learned in the previous section about
    different types of data. We will use the same dataset we used in *Activity 2:
    Summary Statistics and Missing Values*, that is, *House Prices: Advanced Regression
    Techniques* (available at [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    or on GitHub at [https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)).
    We''ll use different types of plots to visually represent the distribution of
    values for this dataset.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将复习上一节关于不同类型数据的内容。我们将使用与*活动 2：摘要统计和缺失值*中相同的数据集，即 *房价：高级回归技术*（可以在[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    或在 GitHub 上找到 [https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)）。我们将使用不同类型的图表来可视化表示该数据集的值分布。
- en: 'The steps to be performed are as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Plot a histogram using Matplotlib for the target variable, `SalePrice`.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Matplotlib 绘制目标变量 `SalePrice` 的直方图。
- en: Find the number of unique values within each column having an object type.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出每个对象类型列中唯一值的数量。
- en: Create a DataFrame representing the number of occurrences for each categorical
    value in the `HouseStyle` column.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据框，表示 `HouseStyle` 列中每个类别值的出现次数。
- en: Plot a pie chart representing these counts.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个饼图，表示这些计数。
- en: Find the number of unique values within each column having a number type.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找每个具有数字类型的列中唯一值的数量。
- en: Plot a histogram using Seaborn for the `LotArea` variable.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Seaborn绘制`LotArea`变量的直方图。
- en: Calculate the skew and kurtosis values for the values in each column.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每列值的偏度和峰度值。
- en: Note
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 312.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第312页找到。
- en: Relationships within the Data
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中的关系
- en: 'There are two reasons why it is important to find relationships between variables
    in the data:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 找到数据中变量之间关系的重要性有两个原因：
- en: Finding which features are potentially important can be deemed essential, since
    finding ones that have a strong relationship with the target variable will aid
    in the feature selection process.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出哪些特征可能是重要的可以被认为是至关重要的，因为找到与目标变量有强烈关系的特征将有助于特征选择过程。
- en: Finding relationships between different features themselves can be useful, since
    variables in the dataset are usually never completely independent of every other
    variable and this can affect our modeling in a number of ways.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到不同特征之间的关系是有用的，因为数据集中的变量通常不可能完全独立于其他所有变量，而这可能会以多种方式影响我们的建模。
- en: Now, there are a number of ways we can visualize these relationships, and this
    really depends on the types of variable we are trying to find the relationship
    between, and how many we are considering as part of the equation or comparison.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有许多方法可以可视化这些关系，具体方法取决于我们试图找到关系的变量类型，以及我们考虑作为方程或比较的一部分的变量数量。
- en: Relationship between Two Continuous Variables
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两个连续变量之间的关系
- en: To find a relationship between two continuous variables is basically to see
    how one varies as the value of the other is increased. The most common way to
    visualize this would be using a scatter plot, in which we take each variable along
    a single axis (the *X* and *Y* axes in a two-dimensional plane when we have two
    variables) and plot each data point using a marker in the *X-Y* plane. This visualization
    gives us a good idea of whether any kind of relationship exists between the two
    variables at all.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 找到两个连续变量之间的关系，基本上是看一个变量的值增加时另一个变量如何变化。最常见的可视化方法是使用散点图，其中我们将每个变量沿一个轴（当我们有两个变量时，在二维平面中的*X*和*Y*轴）绘制，并使用标记在*X-Y*平面中绘制每个数据点。这种可视化能够很好地展示这两个变量之间是否存在某种关系。
- en: If we want to quantize the relationship between the two variables, however,
    the most common method is to find the correlation between them. If the target
    variable is continuous and it has a high degree of correlation with another variable,
    this is an indication that the feature would be an important part of the model.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想量化两个变量之间的关系，最常用的方法是找到它们之间的相关性。如果目标变量是连续的，并且与另一个变量高度相关，这表明该特征在模型中是一个重要部分。
- en: '**Pearson''s Coefficient of Correlation**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**皮尔逊相关系数**'
- en: '**Pearson''s Coefficient of Correlation** is a correlation coefficient that
    is commonly used to show the linear relationship between a pair of variables.
    The formula returns a value between -1 and +1, where:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**皮尔逊相关系数**是一种常用的相关系数，用来显示一对变量之间的线性关系。公式返回一个介于-1和+1之间的值，其中：'
- en: +1 indicates a strong positive relationship
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +1 表示强正相关
- en: -1 indicates a strong negative relationship
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -1 表示强负相关
- en: 0 indicates no relationship at all
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示没有关系
- en: It's also useful to find correlations between pairs of features themselves.
    Although the presence of highly correlated features wouldn't worsen the model,
    they wouldn't necessarily make any model better, either. For the sake of simplicity,
    it is always better to keep only one from a set of highly correlated features.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，找到特征对之间的相关性也很有用。尽管高度相关的特征的存在不会使模型变差，但它们也不一定会使任何模型变得更好。为了简化起见，最好从一组高度相关的特征中只保留一个。
- en: Note
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When fitting a linear model, having features that are highly correlated to each
    other can result in an unpredictable and widely varying model. This is because
    the coefficients of each feature in a linear model can be interpreted as the unit
    change in the target variable, keeping all other features constant. When a set
    of features are not independent (that is, are correlated), however, we cannot
    determine the effect of the independent changes to the target variable due to
    each feature, resulting in widely varying coefficients.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合线性模型时，特征之间高度相关可能会导致模型不可预测且变化幅度较大。这是因为线性模型中每个特征的系数可以解释为在保持其他特征不变的情况下，目标变量的单位变化。然而，当一组特征不独立（即存在相关性）时，我们无法确定每个特征对目标变量的独立变化所造成的影响，导致系数变化幅度较大。
- en: To find the pairwise correlation for every numeric feature in a DataFrame with
    every other feature, we can use the `.corr()` function on the DataFrame.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到DataFrame中每个数值特征与其他特征的成对相关性，可以在DataFrame上使用`.corr()`函数。
- en: 'Exercise 22: Plotting a Scatter Plot'
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 22：绘制散点图
- en: 'Let''s plot a scatter plot between the primary earthquake magnitude on the
    *X* axis and the corresponding number of injuries on the *Y* axis:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制主震震中震级（*X*轴）与对应的受伤人数（*Y*轴）之间的散点图：
- en: 'Filter out null values. Since we know that there are null values in both columns,
    let''s first filter the data to include only the non-null rows:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉空值。由于我们知道两列中都有空值，首先让我们过滤数据，只保留非空行：
- en: '[PRE39]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create and display the scatter plot. We will use Matplotlib''s `plt.scatter(x=...,
    y=...)` as the primary command for plotting the data. The `x` and `y` parameters
    state which feature is to be considered along which axis. They take a single-dimensional
    data structure such as a list, a tuple, or a pandas series. We can also send the
    `scatter` function more parameters that define, say, the icon to use to plot an
    individual data point. For example, to use a red cross as the icon, we would need
    to send the following parameters: `marker=''x'', c=''r''`:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并显示散点图。我们将使用Matplotlib的`plt.scatter(x=..., y=...)`作为绘制数据的主要命令。`x`和`y`参数指定哪个特征应沿哪个轴绘制。它们接受单一维度的数据结构，如列表、元组或pandas系列。我们还可以向`scatter`函数传递更多参数，例如指定绘制单个数据点时使用的图标。例如，要使用红色的交叉符号作为图标，我们需要传递以下参数：`marker='x',
    c='r'`：
- en: '[PRE40]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output will be as follows:'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将如下所示：
- en: '![](img/C12622_02_28.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12622_02_28.jpg)'
- en: 'Figure 2.28: Scatter plot'
  id: totrans-364
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.28：散点图
- en: From the plot, we can infer that although there doesn't seem to be a trend between
    the number of people who were injured and the earthquake magnitude, there are
    an increasing number of earthquakes with large injury counts as the magnitude
    increases. However, for the majority of earthquakes, there does not seem to be
    a relationship.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们可以推断出，虽然受伤人数和震中震级之间似乎没有明显的趋势，但随着震级的增加，受伤人数较多的地震数量也在增加。然而，对于大多数地震而言，似乎没有明显的关系。
- en: 'Exercise 23: Correlation Heatmap'
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 23：相关性热力图
- en: Let's plot a correlation heatmap between all the numeric variables in our dataset
    using Seaborn's `sns.heatmap()` function on the inter-feature correlation values
    in the dataset.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Seaborn的`sns.heatmap()`函数绘制数据集中所有数值变量之间的相关性热力图，该函数基于数据集中的特征间相关性值。
- en: 'The optional parameters passed to the `sns.heatmap()` function are `square`
    and `cmap`, which indicate that the plot should be such that each pixel is square
    and specify which color scheme to use, respectively:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`sns.heatmap()`函数的可选参数是`square`和`cmap`，它们分别表示绘制的图表中每个像素应为正方形，并指定要使用的颜色方案：
- en: 'Plot a basic heatmap with all the features:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个包含所有特征的基本热力图：
- en: '[PRE41]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output will be:'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将是：
- en: '![](img/C12622_02_29.jpg)'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12622_02_29.jpg)'
- en: 'Figure 2.29: Correlation heatmap'
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.29：相关性热力图
- en: We can see from the color bar on the right of the plot that the minimum value,
    around `-0.2`, is the lightest shade, which is a misrepresentation of the correlation
    values, which vary from -1 to 1.
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图表右侧的颜色条中我们可以看到，最小值大约为`-0.2`，对应的颜色是最浅的，这在某种程度上误导了相关性值的表示，实际相关性值范围应该是从-1到1。
- en: 'Plot a subset of features in a more customized heatmap. We will specify the
    upper and lower limits using the `vmin` and `vmax` parameters, and plot the heatmap
    again with annotations specifying the pairwise correlation values on a subset
    of features. We will also change the color scheme to one that can be better interpreted—while
    the neutral white will represent no correlation, increasingly darker shades of
    blue and red will represent higher positive and negative correlation values respectively:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个更自定义的热图中绘制特征的子集。我们将使用 `vmin` 和 `vmax` 参数指定上下限，并使用带有注释的热图重新绘制，注释显示特征对之间的相关系数值。我们还将更改颜色方案，使其更易于解读——中性色白色表示无相关性，而越来越深的蓝色和红色分别表示更高的正相关性和负相关性：
- en: '[PRE42]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output will be as follows:'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12622_02_30.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12622_02_30.jpg)'
- en: 'Figure 2.30: Customized correlation heatmap'
  id: totrans-379
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.30：自定义的相关性热图
- en: Now, while we can calculate the value of correlation, this only gives us an
    indication of a linear relationship. To better judge whether there's a possible
    dependency, we could plot a scatter plot between pairs of features, which is mostly
    useful when the relationship between the two variables is not known and visualizing
    how the data points are scattered or distributed could give us an idea of whether
    (and how) the two may be related.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，虽然我们可以计算相关系数的值，但这仅仅提供了线性关系的指示。为了更好地判断是否存在可能的依赖关系，我们可以绘制特征对之间的散点图，这在变量之间的关系不明确时特别有用，因为通过可视化数据点的分布，我们可以大致判断这两个变量是否（以及如何）相关。
- en: 'Exercise 24: Pairplot'
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 24：Pairplot
- en: 'A pairplot is useful for visualizing multiple relationships between pairs of
    features at once, and can be plotted using Seaborn''s `.pairplot()` function.
    In this exercise, we will look at a pairplot between the features having the highest
    pair-wise correlation in the dataset:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: pairplot 对于同时可视化多个特征对之间的关系非常有用，可以使用 Seaborn 的 `.pairplot()` 函数绘制。在这个练习中，我们将查看数据集中具有最高成对相关性的特征之间的
    pairplot：
- en: 'Define a list having the subset of features on which to create the pairplot:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含要创建 pairplot 的特征子集的列表：
- en: '[PRE43]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create the pairplot using Seaborn. The arguments sent to the plotting function
    are: `kind=''scatter''`, which indicates that we want each individual plot between
    the pair of variables in the grid to be represented as a scatter plot, and `diag_kind=''kde''`,
    which indicates that we want the plots along the diagonal (where both the features
    in the pair are the same) to be a kernel density estimate.'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Seaborn 创建 pairplot。传递给绘图函数的参数是：`kind='scatter'`，表示我们希望网格中每一对变量之间的单独图形以散点图的形式展示；`diag_kind='kde'`，表示我们希望对角线上的图形（即两变量相同的位置）为核密度估计图。
- en: 'It should also be noted here that the plots symmetrically across the diagonal
    from each other will essentially be the same, just with the axes reversed:'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，还应该注意，对角线对称的图形本质上是相同的，只不过坐标轴被反转了：
- en: '[PRE44]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output will be as follows:'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12622_02_31.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12622_02_31.jpg)'
- en: 'Figure 2.31: Pairplot between the features having the highest pair-wise correlation'
  id: totrans-390
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.31：具有最高成对相关性的特征之间的 pairplot
- en: Relationship between a Continuous and a Categorical Variable
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续型变量与类别型变量之间的关系
- en: A common way to see the relationship between two variables when one is categorical
    and the other is continuous can be using a bar plot or a box plot.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个变量是类别型变量，另一个是连续型变量时，查看它们之间关系的一种常见方法是使用条形图或箱线图。
- en: A bar plot helps compare the value of a variable for a discrete set of parameters
    and is one of the most common types of plots. Each bar represents a categorical
    value and the height of the bar usually represents an aggregated value of the
    continuous variable over that category (such as average, sum, or count of the
    values of the continuous variable in that category).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条形图有助于比较一个变量在一组离散参数中的值，是最常见的图形类型之一。每根条形代表一个类别值，条形的高度通常表示该类别下连续变量的聚合值（如平均值、总和或该类别中连续变量的值的计数）。
- en: A box plot is a rectangle drawn to represent the distribution of the continuous
    variable for each discrete value of the categorical variable. It not only allows
    us to visualize outliers efficiently but also allows us to compare the distribution
    of the continuous variable across categories of the categorical variable. The
    lower and upper edges of the rectangle represent the first and third quartiles
    respectively, the line down through the middle represents the median value, and
    the points (or fliers) above and below the rectangle represent outlier values.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 箱型图是一个矩形，用来表示每个离散类别变量的连续变量的分布。它不仅能够有效地展示异常值，还可以让我们比较不同类别变量下连续变量的分布。矩形的下边缘和上边缘分别代表第一和第三四分位数，矩形中间的线代表中位数，而矩形上下的点（或离群点）代表异常值。
- en: 'Exercise 25: Bar Chart'
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 25：柱状图
- en: 'Let''s visualize the total number of tsunamis created by earthquakes of each
    intensity level using a bar chart:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用柱状图来可视化每个强度级别的地震所造成的海啸总数：
- en: 'Preprocess the `flag_tsunami` variable. Before we can use the `flag_tsunami`
    variable, we need to preprocess it to convert the `No` values to zeros and the
    `Tsu` values to ones. This will give us the binary target variable. To do this,
    we set the values in the column using the `.loc` operator, with `:` indicating
    that values need to be set for all rows, and the second parameter specifying the
    name of the column for which values are to be set:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理`flag_tsunami`变量。在使用`flag_tsunami`变量之前，我们需要预处理它，将`No`值转换为零，将`Tsu`值转换为一。这将为我们提供二元目标变量。为此，我们使用`.loc`操作符设置列中的值，`:`表示需要为所有行设置值，第二个参数指定要设置值的列名称：
- en: '[PRE45]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Remove all rows having null `intensity` values from the data we want to plot:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除所有`intensity`值为null的行，去掉我们要绘制的数据：
- en: '[PRE46]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Find the total number of tsunamis for each `intensity` level and display the
    DataFrame. To get the data in a format using which a bar plot can be visualized,
    we will need to group the rows by each intensity level, and then sum over the
    `flag_tsunami` values to get the total number of tsunamis for each intensity level:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找每个`intensity`级别的海啸总数并显示数据框。为了将数据转换为可视化柱状图的格式，我们需要按每个强度级别对行进行分组，然后对`flag_tsunami`值进行求和，以获得每个强度级别的海啸总数：
- en: '[PRE47]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output will be as follows:'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.32: Total number of tsunamis for each intensity level](img/C12622_02_32.jpg)'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.32：每个强度级别的海啸总数](img/C12622_02_32.jpg)'
- en: 'Figure 2.32: Total number of tsunamis for each intensity level'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.32：每个强度级别的海啸总数
- en: 'Plot the bar chart, using Matplotlib''s `plt.bar(x=..., height=...)` method,
    which takes two arguments, one specifying the `x` values at which bars need to
    be drawn, and the second specifying the height of each bar. Both of these are
    one-dimensional data structures that must have the same length:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Matplotlib的`plt.bar(x=..., height=...)`方法绘制柱状图，该方法需要两个参数，一个指定绘制柱形的位置（`x`值），另一个指定每个柱形的高度。这两者都是一维数据结构，必须具有相同的长度：
- en: '[PRE48]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be as follows:'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.33: Bar chart](img/C12622_02_33.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.33：柱状图](img/C12622_02_33.jpg)'
- en: 'Figure 2.33: Bar chart'
  id: totrans-410
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.33：柱状图
- en: From this plot, we can see that as the earthquake intensity increases, the number
    of tsunamis caused also increases, but beyond an intensity of 9, the number of
    tsunamis seems to suddenly drop.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们可以看到，随着地震强度的增加，造成的海啸数量也增加，但在强度超过9之后，海啸的数量似乎突然下降。
- en: Think about why this could be happening. Perhaps it's just that there are fewer
    earthquakes with an intensity that high, and hence fewer tsunamis. Or it could
    be an entirely independent factor; maybe high-intensity earthquakes have historically
    occurred on land and couldn't trigger a tsunami. Explore the data to find out.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 想想为什么会发生这种情况。或许只是因为高强度的地震较少，因此海啸也较少。或者可能是完全独立的因素；也许高强度的地震历史上发生在陆地上，无法引发海啸。探索数据以找出原因。
- en: 'Exercise 26: Box Plot'
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 26：箱型图
- en: 'In this exercise, we''ll plot a box plot that represents the variation in `eq_primary`
    over the countries with at least 100 earthquakes:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将绘制一个箱型图，表示具有至少100次地震的国家中`eq_primary`的变化：
- en: 'Find countries with over 100 earthquakes. We will find the value counts for
    all the countries in the dataset. Then, we''ll create a series comprising only
    those countries having a count greater than 100:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找发生超过100次地震的国家。我们将查找数据集中所有国家的计数值。然后，我们将创建一个仅包含计数大于100的国家的系列：
- en: '[PRE49]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output will be as follows:'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.34: Countries with over 100 earthquakes](img/C12622_02_34.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.34：地震超过 100 次的国家](img/C12622_02_34.jpg)'
- en: 'Figure 2.34: Countries with over 100 earthquakes'
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.34：地震超过 100 次的国家
- en: 'Subset the DataFrame to filter in only those rows having countries in the preceding
    set. To filter the rows, we use the `.isin()` method on the pandas series to select
    those rows containing a value in the array-like object passed as a parameter:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 DataFrame 进行子集化，筛选出前面集合中包含的国家的行。为了筛选这些行，我们使用 `.isin()` 方法在 pandas 系列上选择包含传递为参数的类似数组对象中的值的行：
- en: '[PRE50]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create and display the box plot. The primary command for plotting the data
    is `sns.boxplot(x=..., y=..., data=..., order=)`. The `x` and `y` parameters are
    the names of the columns in the DataFrame to be plotted on each axis—the former
    is assumed to be the categorical variable and the latter the continuous. The `data`
    parameter takes the DataFrame from which to take the data and `order` takes a
    list of category names that indicates the order in which to display the categories
    on the *X* axis:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并显示箱形图。绘制数据的主要命令是 `sns.boxplot(x=..., y=..., data=..., order=)`。`x` 和 `y`
    参数是 DataFrame 中要绘制在每个坐标轴上的列名——前者假定是类别变量，后者是连续变量。`data` 参数指定要从中获取数据的 DataFrame，`order`
    参数接受一个类别名称列表，指示在 *X* 轴上显示类别的顺序：
- en: '[PRE51]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output will be as follows:'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.35: Box plot](img/C12622_02_35.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.35：箱形图](img/C12622_02_35.jpg)'
- en: 'Figure 2.35: Box plot'
  id: totrans-426
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.35：箱形图
- en: Relationship between Two Categorical Variables
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 两个类别变量之间的关系
- en: When we are looking at only a pair of categorical variables to find a relationship
    between them, the most intuitive way to do this is to divide the data on the basis
    of the first category, and then subdivide it further on the basis of the second
    categorical variable and look at the resultant counts to find the distribution
    of data points. While this might seem confusing, a popular way to visualize this
    is using stacked bar charts. As in a regular bar chart, each bar would represent
    a categorical value. But each bar would again be subdivided into color-coded categories
    that would provide an indication of what fraction of the data points in the primary
    category fall into each subcategory (that is, the second category). The variable
    with a larger number of categories is usually considered the primary category.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们只关注一对类别变量以查找它们之间的关系时，最直观的方式是基于第一个类别将数据进行划分，然后再根据第二个类别变量进一步细分，查看结果的计数以找到数据点的分布。虽然这可能看起来有些混乱，但一种常见的可视化方式是使用堆叠条形图。与常规的条形图一样，每个条形图表示一个类别值。但每个条形图将再次被细分为颜色编码的子类别，这可以指示在主类别中有多少数据点落入每个子类别（即第二个类别）。类别数较多的变量通常被认为是主类别。
- en: 'Exercise 27: Stacked Bar Chart'
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 27：堆叠条形图
- en: 'In this exercise, we''ll plot a stacked bar chart that represents the number
    of tsunamis that occurred for for each intensity level:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将绘制一个堆叠条形图，表示每个强度级别发生的海啸数量：
- en: 'Find the number of data points that fall into each grouped value of `intensity`
    and `flag_tsunami`:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找落入每个 `intensity` 和 `flag_tsunami` 分组值中的数据点数量：
- en: '[PRE52]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be as follows:'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![](img/C12622_02_36.jpg)'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12622_02_36.jpg)'
- en: 'Figure 2.36: Data points falling into each grouped value of intensity and flag_tsunami'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.36：每个分组的强度和 flag_tsunami 中落入的数据点
- en: 'Use the `.unstack()` method on the resultant DataFrame to get the level 1 index
    (`flag_tsunami`) as a column:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对结果 DataFrame 使用 `.unstack()` 方法，将一级索引（`flag_tsunami`）作为列：
- en: '[PRE53]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output will be as follows:'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.37: The level 1 index](img/C12622_02_37.jpg)'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.37：一级索引](img/C12622_02_37.jpg)'
- en: 'Figure 2.37: The level 1 index'
  id: totrans-440
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.37：一级索引
- en: 'Create the stacked bar chart. We first use the `sns.set()` function to indicate
    that we want to use Seaborn as our visualization library. Then, we can easily
    use the native `.plot()` function in pandas to plot a stacked bar chart by passing
    the `kind=''bar''` and `stacked=True` arguments:'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建堆叠条形图。我们首先使用 `sns.set()` 函数来指示我们希望使用 Seaborn 作为可视化库。然后，我们可以轻松地使用 pandas 中的原生
    `.plot()` 函数，通过传递 `kind='bar'` 和 `stacked=True` 参数来绘制堆叠条形图：
- en: '[PRE54]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output will be as follows:'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 2.38: A stacked bar chart](img/C12622_02_38.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.38：堆叠条形图](img/C12622_02_38.jpg)'
- en: 'Figure 2.38: A stacked bar chart'
  id: totrans-445
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.38：堆叠条形图
- en: 'The plot now lets us visualize and interpret the fraction of earthquakes that
    caused tsunamis at each intensity level. In *Exercise 25: Bar Chart*, we saw the
    number of tsunamis drop for earthquakes having intensity greater than 9\. From
    this plot, we can now confirm that this was primarily because the number of earthquakes
    themselves dropped beyond level 10; the fraction of tsunamis even increased for
    level 11.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，该图表让我们能够可视化和解释每个强度级别导致海啸的地震比例。在*练习 25: 条形图*中，我们看到大于9级的地震所导致的海啸数量有所下降。从这个图中，我们可以确认，主要是因为超过10级的地震数量减少了；而11级的地震所导致的海啸比例甚至有所增加。'
- en: 'Activity 4: Relationships Within the Data'
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '活动 4: 数据内的关系'
- en: 'In this activity, we will revise what we learned in the previous section about
    relationships between data. We will use the same dataset we used in *Activity
    2: Summary Statistics and Missing Values*, that is, *House Prices: Advanced Regression
    Techniques* (available at [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    or on GitHub at [https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)).
    We''ll use different plots to highlight relationships between values in this dataset.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '在本次活动中，我们将回顾上一节中关于数据关系的学习内容。我们将使用与*活动 2: 汇总统计和缺失值*相同的数据集，即*房价：高级回归技术*（可在[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)或GitHub上的[https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)找到）。我们将使用不同的图表来突出显示数据集中的值之间的关系。'
- en: 'The steps to be performed are as follows:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的步骤如下：
- en: Plot the correlation heatmap for the dataset.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制数据集的相关性热图。
- en: 'Plot a more compact heatmap having annotations for correlation values using
    the following subset of features:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个更紧凑的热图，使用以下特征子集并带有相关性值的注释：
- en: '[PRE55]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Display the pairplot for the same subset of features, with the KDE plot on the
    diagonals and scatter plot elsewhere.
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示相同特征子集的配对图，KDE图位于对角线位置，散点图位于其他地方。
- en: Create a boxplot to show the variation in `SalePrice` for each category of `GarageCars`.
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个箱型图，显示`SalePrice`在每个`GarageCars`类别中的变化。
- en: Plot a line graph using Seaborn to show the variation in `SalePrice` for older
    and more recently built flats.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Seaborn绘制折线图，显示旧公寓和新建公寓的`SalePrice`变化。
- en: Note
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 319.
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本次活动的解决方案可以在第319页找到。
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we started by talking about why data exploration is an important
    part of the modeling process and how it can help in not only preprocessing the
    dataset for the modeling process, but also help us engineer informative features
    and improve model accuracy. This chapter focused on not only gaining a basic overview
    of the dataset and its features, but also gaining insights by creating visualizations
    that combine several features.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先讨论了为什么数据探索是建模过程中的重要部分，它不仅有助于对数据集进行预处理，还能帮助我们设计具有信息量的特征并提高模型的准确性。本章不仅侧重于对数据集及其特征的基本概览，还通过创建结合多个特征的可视化来获得洞见。
- en: We looked at how to find the summary statistics of a dataset using core functionality
    from pandas. We looked at how to find missing values and talked about why they're
    important, while learning how to use the Missingno library to analyze them and
    the pandas and scikit-learn libraries to impute the missing values.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了如何使用pandas的核心功能找到数据集的汇总统计数据。我们还研究了如何发现缺失值，并讨论了它们的重要性，同时学习如何使用Missingno库分析这些缺失值，以及使用pandas和scikit-learn库填补缺失值。
- en: Then, we looked at how to study the univariate distributions of variables in
    the dataset and visualize them for both categorical and continuous variables using
    bar charts, pie charts, and histograms. Lastly, we learned how to explore relationships
    between variables, and about how they can be represented using scatter plots,
    heatmaps, box plots, and stacked bar charts, to name a few.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了如何研究数据集中变量的单变量分布，并通过条形图、饼图和直方图等可视化方式展示这些分布。最后，我们学习了如何探索变量之间的关系，并了解到它们可以通过散点图、热图、箱型图、堆积条形图等形式进行表示。
- en: 'In the following chapters, we will start exploring supervised machine learning
    algorithms. Now that we have an idea of how to explore a dataset that we have,
    we can proceed to the modeling phase. The next chapter will introduce regression:
    a class of algorithms that are primarily used to build models for continuous target
    variables.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将开始探索监督式机器学习算法。现在我们已经了解了如何探索我们拥有的数据集，我们可以进入建模阶段。下一章将介绍回归：一种主要用于构建连续目标变量模型的算法类别。
