- en: '*Chapter 4*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*'
- en: Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Implement logistic regression and explain how it can be used to classify data
    into specific groups or classes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现逻辑回归，并解释如何将其用于将数据分类为特定组或类别
- en: Use the K-nearest neighbors clustering algorithm for classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K最近邻聚类算法进行分类
- en: Use decision trees for data classification, including the ID3 algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行数据分类，包括ID3算法
- en: Describe the concept of entropy within data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述数据中的熵概念
- en: Explain how decision trees such as ID3 aim to reduce entropy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释决策树（如ID3）如何旨在减少熵
- en: Use decision trees for data classification
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行数据分类
- en: This chapter introduces classification problems, classification using linear
    and logistic regression, K-nearest neighbors classification, and decision trees.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了分类问题，线性回归和逻辑回归分类，K最近邻分类和决策树。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapter, we began our supervised machine learning journey using
    regression techniques, predicting the continuous variable output given a set of
    input data. We will now turn to the other sub-type of machine learning problems
    that we previously described: classification problems. Recall that classification
    tasks aim to predict, given a set of input data, which one of a specified number
    of groups of classes data belongs to.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们开始了使用回归技术的监督学习之旅，预测给定一组输入数据时的连续变量输出。现在，我们将转向我们之前描述的另一类机器学习问题：分类问题。回想一下，分类任务的目标是根据一组输入数据，预测数据属于指定数量的类别中的哪一类。
- en: In this chapter, we will extend the concepts learned in *Chapter 3*, *Regression
    Analysis*, and will apply them to a dataset labeled with classes, rather than
    continuous values, as output.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将扩展在*第3章*《回归分析》中学到的概念，并将其应用于标注有类别而非连续值作为输出的数据集。
- en: Linear Regression as a Classifier
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将线性回归作为分类器
- en: We covered linear regression in the context of predicting continuous variable
    output in the previous chapter, but it can also be used to predict the class that
    a set of data is a member of. Linear regression classifiers are not as powerful
    as other types of classifiers that we will cover in this chapter, but they are
    particularly useful in understanding the process of classification. Let's say
    we had a fictional dataset containing two separate groups, Xs and Os, as shown
    in *Figure 4.1*. We could construct a linear classifier by first using linear
    regression to fit the equation of a straight line to the dataset. For any value
    that lies above the line, the *X* class would be predicted, and for any value
    beneath the line, the *O* class would be predicted. Any dataset that can be separated
    by a straight line is known as linearly separable, which forms an important subset
    of data types in machine learning problems. While this may not be particularly
    helpful in the context of a linear regression-based classifier, it often is in
    the case of other classifiers, such as **support vector machines** (**SVM**),
    decision trees, and linear neural network-based classifiers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们在预测连续变量输出的上下文中讨论了线性回归，但它也可以用于预测一组数据属于哪个类别。线性回归分类器不如我们将在本章中讨论的其他类型的分类器强大，但它们在理解分类过程时特别有用。假设我们有一个虚构的数据集，其中包含两个独立的组，X和O，如*图
    4.1*所示。我们可以通过首先使用线性回归拟合一条直线的方程来构建一个线性分类器。对于任何位于直线之上的值，将预测为*X*类别，而对于位于直线下方的任何值，将预测为*O*类别。任何可以通过一条直线分隔的数据集被称为线性可分，这构成了机器学习问题中的一个重要数据子集。尽管在基于线性回归的分类器中，这可能并不特别有用，但在其他分类器中，如**支持向量机**（**SVM**）、决策树和基于线性神经网络的分类器中，这通常是很有帮助的。
- en: '![Figure 4.1: Linear regression as a classifier](img/C12622_04_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1：将线性回归作为分类器](img/C12622_04_01.jpg)'
- en: 'Figure 4.1: Linear regression as a classifier'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.1：将线性回归作为分类器
- en: 'Exercise 36: Linear Regression as a Classifier'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 36：将线性回归用作分类器
- en: This exercise contains a contrived example of using linear regression as a classifier.
    In this exercise, we will use a completely fictional dataset, and test how linear
    regression fares as a classifier. The dataset is composed of manually selected
    *x* and *y* values for a scatterplot that are approximately divided into two groups.
    The dataset has been specifically designed for this exercise, to demonstrate how
    linear regression can be used as a classifier, and this is available in the accompanying
    code files for this book, as well as on GitHub, at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习包含了一个使用线性回归作为分类器的构造示例。在本练习中，我们将使用一个完全虚构的数据集，并测试线性回归作为分类器的效果。数据集由手动选择的*x*和*y*值组成，这些值大致分为两组。该数据集专门为本练习设计，旨在展示如何将线性回归作为分类器使用，数据集在本书的附带代码文件中以及GitHub上的[https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python)可以找到。
- en: 'Load the `linear_classifier.csv` dataset into a pandas DataFrame:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`linear_classifier.csv`数据集加载到pandas DataFrame中：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will be as follows:'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.2: First five rows](img/C12622_04_02.jpg)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.2：前五行](img/C12622_04_02.jpg)'
- en: 'Figure 4.2: First five rows'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.2：前五行
- en: Looking through the dataset, each row contains a set of *x, y* coordinates,
    as well as the label corresponding to which class the data belongs to, either
    a cross (**x**) or a circle (**o**).
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 浏览数据集，每行包含一组*x, y*坐标以及对应的标签，指示数据属于哪个类别，可能是叉号(**x**)或圆圈(**o**)。
- en: 'Produce a scatterplot of the data with the marker for each point as the corresponding
    class label:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制数据的散点图，每个点的标记为对应的类别标签：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We''ll get the following scatterplot:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到如下的散点图：
- en: '![Figure 4.3 Scatterplot of a linear classifier](img/C12622_04_03.jpg)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.3 线性分类器的散点图](img/C12622_04_03.jpg)'
- en: Figure 4.3 Scatterplot of a linear classifier
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.3 线性分类器的散点图
- en: 'Using the scikit-learn `LinearRegression` API from the previous chapter, fit
    a linear model to the *x*, *y* coordinates of the dataset and print out the linear
    equation:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一章中的scikit-learn `LinearRegression` API，拟合线性模型到数据集的*x*、*y*坐标，并打印出线性方程：
- en: '[PRE2]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output will be:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.4: Output of model fitting](img/C12622_04_04.jpg)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.4：模型拟合的输出](img/C12622_04_04.jpg)'
- en: 'Figure 4.4: Output of model fitting'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.4：模型拟合的输出
- en: 'Plot the fitted trendline over the dataset:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集上绘制拟合的趋势线：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.5: Scatterplot with trendline](img/C12622_04_05.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.5：带趋势线的散点图](img/C12622_04_05.jpg)'
- en: 'Figure 4.5: Scatterplot with trendline'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.5：带趋势线的散点图
- en: 'With the fitted trendline, the classifier can then be applied. For each row
    in the dataset, determine whether the *x, y* point lies above or below the linear
    model (or trendline). If the point lies below the trendline, the model predicts
    the **o** class, if above the line, the **x** class is predicted. Include these
    values as a column of predicted labels:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过拟合的趋势线，可以应用分类器。对于数据集中的每一行，判断*x, y*点是位于线性模型（或趋势线）之上还是之下。如果点位于趋势线之下，则模型预测**o**类；如果位于线之上，则预测**x**类。将这些值作为预测标签的一列包含在内：
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will be as follows:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.6: First five rows](img/C12622_04_06.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.6：前五行](img/C12622_04_06.jpg)'
- en: 'Figure 4.6: First five rows'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.6：前五行
- en: 'Plot the points with the corresponding ground truth labels. For those points
    where the labels were correctly predicted, plot the corresponding class. For those
    incorrect predictions, plot a diamond:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制带有相应真实标签的点。对于那些标签被正确预测的点，绘制对应的类别；对于错误预测的点，绘制一个菱形标记：
- en: '[PRE5]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will be as follows:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.7: Scatterplot showing incorrect predictions](img/C12622_04_07.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7：显示错误预测的散点图](img/C12622_04_07.jpg)'
- en: 'Figure 4.7: Scatterplot showing incorrect predictions'
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.7：显示错误预测的散点图
- en: We can see that, in this plot, the linear classifier made two incorrect predictions
    in this completely fictional dataset, one at *x = 1*, another at *x = 3*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在这个图中，线性分类器在这个完全虚构的数据集上做出了两次错误预测，一个是在*x = 1*时，另一个是在*x = 3*时。
- en: But what if our dataset is not linearly separable and we cannot classify the
    data using a straight line model, which is very frequently the case. In this scenario,
    we turn to other classification methods, many of which use different models, but
    the process logically flows from our simplified linear classifier model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们的数据集不是线性可分的，无法使用直线模型对数据进行分类，那该怎么办呢？这种情况非常常见。在这种情况下，我们会转向其他分类方法，其中许多方法使用不同的模型，但这一过程逻辑上是从我们简化的线性分类模型延伸出来的。
- en: Logistic Regression
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'The **logistic** or **logit** model is one such non-linear model that has been
    effectively used for classification tasks in a number of different domains. In
    this section, we will use it to classify images of hand-written digits. In understanding
    the logistic model, we also take an important step in understanding the operation
    of a particularly powerful machine learning model, **artificial neural networks**.
    So, what exactly is the logistic model? Like the linear model, which is composed
    of a linear or straight-line function, the logistic model is composed of the standard
    logistic function, which, in mathematical terms, looks something like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑**或**对数几率**模型就是一种非线性模型，已经在许多不同领域的分类任务中得到了有效应用。在本节中，我们将用它来分类手写数字的图像。在理解逻辑模型的过程中，我们也迈出了理解一种特别强大的机器学习模型——**人工神经网络**的关键一步。那么，逻辑模型到底是什么呢？像线性模型一样，线性模型由一个线性或直线函数组成，而逻辑模型则由标准的逻辑函数组成，数学上看起来大致是这样的：'
- en: '![Figure 4.8: Logistic function](img/C12622_04_08.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8：逻辑函数](img/C12622_04_08.jpg)'
- en: 'Figure 4.8: Logistic function'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.8：逻辑函数
- en: In practical terms, when trained, this function returns the probability of the
    input information belonging to a particular class or group.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际角度来看，当经过训练后，这个函数返回输入信息属于某一特定类别或组的概率。
- en: 'Say we would like to predict whether a single entry of data belongs to one
    of two groups. As in the previous example, in linear regression, this would equate
    to *y* being either zero or one, and *x* can take a value between ![](img/C12622_Formula_04_02.png)
    and ![](img/C12622_Formula_04_03.png):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想预测某一数据项是否属于两个组中的一个。就像之前的例子中，在线性回归中，这等价于 *y* 要么为零，要么为一，而 *x* 可以取值范围在 ![](img/C12622_Formula_04_02.png)
    和 ![](img/C12622_Formula_04_03.png) 之间：
- en: '![Figure 4.9: Equation for y](img/C12622_04_09.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9：y 的方程](img/C12622_04_09.jpg)'
- en: 'Figure 4.9: Equation for y'
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.9：y 的方程
- en: 'A range of zero to one and ![](img/C12622_Formula_04_02.png) to ![](img/C12622_Formula_04_03.png)
    are significantly different; to improve this, we will calculate the odds ratio,
    which will then vary from greater than zero to less than ![](img/C12622_Formula_04_03.png),
    which is a step in the right direction:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从零到一的范围与 ![](img/C12622_Formula_04_02.png) 到 ![](img/C12622_Formula_04_03.png)
    的差异非常大；为了改进这一点，我们将计算赔率比，这样它就会从大于零的数值变化到小于 ![](img/C12622_Formula_04_03.png) 的数值，这就是朝着正确方向迈出的一步：
- en: '![Figure 4.10: Odds ratio](img/C12622_04_10.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10：赔率比](img/C12622_04_10.jpg)'
- en: 'Figure 4.10: Odds ratio'
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.10：赔率比
- en: 'We can use the mathematical relationships of the natural log to reduce this
    even further. As the odds ratio approaches zero, ![](img/C12622_Formula_04_04.png)ss
    approaches ![](img/C12622_Formula_04_02.png); similarly, as the odds ratio approaches
    one, ![](img/C12622_Formula_04_04.png) approaches ![](img/C12622_Formula_04_03.png).
    This is exactly what we want; that is, for the two classification options to be
    as far apart as possible:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用自然对数的数学关系进一步简化这一过程。当赔率比接近零时，![](img/C12622_Formula_04_04.png)ss 接近 ![](img/C12622_Formula_04_02.png)；同样，当赔率比接近一时，![](img/C12622_Formula_04_04.png)
    接近 ![](img/C12622_Formula_04_03.png)。这正是我们想要的；也就是说，两个分类选项尽可能远离。
- en: '![Figure 4.11: Natural log of classified points](img/C12622_04_11.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11：分类点的自然对数](img/C12622_04_11.jpg)'
- en: 'Figure 4.11: Natural log of classified points'
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.11：分类点的自然对数
- en: 'With a little bit of equation re-arranging we get the logistic function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过稍微调整方程，我们得到了逻辑函数：
- en: '![Figure 4.12: Logistic function](img/C12622_04_12.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12：逻辑函数](img/C12622_04_12.jpg)'
- en: 'Figure 4.12: Logistic function'
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.12：逻辑函数
- en: 'Notice the exponents of *e*, that is, ![](img/C12622_Formula_04_05.png), and
    that this relationship is a linear function with two training parameters or *weights*,
    ![](img/C12622_Formula_04_06.png) and ![](img/C12622_Formula_04_02.png), as well
    as the input feature, *x*. If we were to plot the logistic function over the range
    *(-6, 6)*, we would get the following result:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 *e* 的指数，即 ![](img/C12622_Formula_04_05.png)，并且这个关系是一个线性函数，具有两个训练参数或 *权重*，![](img/C12622_Formula_04_06.png)
    和 ![](img/C12622_Formula_04_02.png)，以及输入特征 *x*。如果我们将逻辑函数绘制在 *(-6, 6)* 范围内，我们会得到以下结果：
- en: '![Figure 4.13: Logistic function curve](img/C12622_04_13.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13：逻辑函数曲线](img/C12622_04_13.jpg)'
- en: 'Figure 4.13: Logistic function curve'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.13：逻辑函数曲线
- en: Examining *Figure 4.13*, we can see some features that are important for a classification
    task. The first thing to note is that, if we look at the probability values on
    the *y* axis at the extremes of the function, the values are almost at zero when
    *x = -6* and at one when *x = 6*. While it looks like the values are in fact zero
    and one, this is not exactly the case. The logistic function approaches zero and
    one at these extremes and will only equal zero and one when *x* is at a positive
    or negative infinity. In practical terms, what this means is that the logistic
    function will never return a probability of one or greater or less than or equal
    to zero, which is perfect for a classification task. We can never have a probability
    of greater than one, as, by definition, a probability of one is a certainty of
    an event occurring. Likewise, we cannot have a probability of less than zero,
    as, by definition, a probability of zero is a certainty of the event not occurring.
    The fact that the logistic function approaches but never equals one or zero means
    that there is always some uncertainty in the outcome or the classification.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查 *图 4.13*，我们可以看到一些对分类任务很重要的特征。首先需要注意的是，如果我们查看函数两端的 *y* 轴上的概率值，在 *x = -6*
    时，概率值几乎为零，而在 *x = 6* 时，概率值接近 1。虽然看起来这些值实际上是零和一，但实际情况并非如此。逻辑函数在这些极值处接近零和一，只有当 *x*
    达到正无穷或负无穷时，它才会等于零或一。从实际角度来看，这意味着逻辑函数永远不会返回大于一的概率或小于等于零的概率，这对于分类任务来说是完美的。我们永远不能有大于一的概率，因为根据定义，概率为一意味着事件发生是确定的。同样，我们不能有小于零的概率，因为根据定义，概率为零意味着事件不发生是确定的。逻辑函数接近但永远不等于一或零，意味着结果或分类总是存在某种不确定性。
- en: The final feature to notice about the logistic function is that at *x = 0*,
    the probability is 0.5, which, if we were to get this result, would indicate that
    the model is equally uncertain about the outcome of the corresponding class; that
    is, it really has no idea. Typically, this is the default position at the start
    of training, and, as the model is exposed to training data, it becomes more confident
    in its decisions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数的最后一个特点是，在 *x = 0* 时，概率为 0.5，如果我们得到这个结果，这意味着模型对相应类别的结果具有相等的不确定性；也就是说，它完全没有把握。通常，这是训练开始时的默认状态，随着模型接触到训练数据，它对决策的信心逐渐增加。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is very important to correctly understand and interpret the probability information
    provided by classification models such as linear regression. Consider this probability
    score as the chance of the input information belonging to a particular class given
    the variability in the information provided by the training data. One common mistake
    is to use this probability score as an objective measure of whether the model
    can be trusted about its prediction; unfortunately, this isn't necessarily the
    case. *A model can provide a probability of 99.99% that some data belongs to a
    particular class and still be 99.99% wrong*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正确理解和解释分类模型（如线性回归）提供的概率信息非常重要。可以将这个概率得分视为在给定训练数据所提供的信息的变动性下，输入信息属于某一特定类别的可能性。一个常见的错误是将这个概率得分作为衡量模型对预测是否可靠的客观标准；不幸的是，这并不总是准确的。*一个模型可以提供
    99.99% 的概率，认为某些数据属于某个特定类别，但它仍然可能有 99.99% 的错误*。
- en: What we do use the probability value for is selecting the predicted class by
    the classifier. Say we had a model that was to predict whether some set of data
    belonged to class A or class B. If the logistic model returned a probability of
    0.7 for class A, then we would return class A as the predicted class for the model.
    If the probability was only 0.2, the predicted class for the model would be class
    B.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用概率值的目的是选择分类器预测的类别。假设我们有一个模型用于预测某些数据集是否属于类 A 或类 B。如果逻辑回归模型为类 A 返回的概率为 0.7，那么我们将返回类
    A 作为模型的预测类别。如果概率仅为 0.2，则模型的预测类别为类 B。
- en: 'Exercise 37: Logistic Regression as a Classifier – Two-Class Classifier'
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 37：逻辑回归作为分类器 – 二类分类器
- en: 'For this exercise, we will be using a sample of the famous MNIST dataset (available
    at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/) or on
    GitHub at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python)),
    which is a sequence of images of handwritten postcode digits, zero through nine,
    with corresponding labels. The MNIST dataset is comprised of 60,000 training samples
    and 10,000 test samples, where each sample is a grayscale image with a size of
    28 x 28 pixels. In this exercise, we will use logistic regression to build a classifier.
    The first classifier we will build is a two-class classifier, where we will determine
    whether the image is a handwritten zero or a one:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用著名的 MNIST 数据集的一个样本（可以在 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
    或 GitHub 上的 [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python)
    找到），它是一个包含手写邮政编码数字（从零到九）及相应标签的图像序列。MNIST 数据集包含 60,000 个训练样本和 10,000 个测试样本，每个样本都是大小为
    28 x 28 像素的灰度图像。在本练习中，我们将使用逻辑回归来构建一个分类器。我们将构建的第一个分类器是一个二类分类器，用来判断图像是手写的零还是一：
- en: 'For this exercise, we will need to import a few dependencies. Execute the following
    import statements:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们需要导入一些依赖项。执行以下导入语句：
- en: '[PRE6]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will also need to download the MNIST datasets. You will only need to do
    this once, so after this step, feel free to comment out or remove these cells.
    Download the image data, as follows:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要下载 MNIST 数据集。你只需要执行此操作一次，因此在此步骤之后，可以随意注释或删除这些代码单元。下载图像数据，具体如下：
- en: '[PRE7]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Download the corresponding labels for the data:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据的相应标签：
- en: '[PRE8]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once all the files have been successfully downloaded, check out the files in
    the local directory using the following command for Windows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有文件成功下载，使用以下命令检查本地目录中的文件（适用于 Windows）：
- en: '[PRE9]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be as follows:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.14: Files in directory](img/C12622_04_14.jpg)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.14：目录中的文件](img/C12622_04_14.jpg)'
- en: 'Figure 4.14: Files in directory'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.14：目录中的文件
- en: Note
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For Linux and macOS, check out the files in the local directory using the `!ls
    *.gz` command.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 Linux 和 macOS，使用 `!ls *.gz` 命令查看本地目录中的文件。
- en: 'Load the downloaded data. Don''t worry too much about the exact details of
    reading the data, as these are specific to the MNIST dataset:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载下载的数据。无需过于担心读取数据的具体细节，因为这些是 MNIST 数据集特有的：
- en: '[PRE10]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As always, having a thorough understanding of the data is key, so create an
    image plot of the first 10 images in the training sample. Notice the grayscale
    images and that the corresponding labels are the digits zero through nine:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一如既往，彻底理解数据至关重要，因此，创建训练样本中前 10 张图像的图像图。注意灰度图像以及相应的标签，它们是数字零到九：
- en: '[PRE11]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.15: Training images](img/C12622_04_15.jpg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.15：训练图像](img/C12622_04_15.jpg)'
- en: 'Figure 4.15: Training images'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.15：训练图像
- en: 'As the initial classifier is aiming to classify either images of zeros or images
    of ones, we must first select these samples from the dataset:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于初始分类器旨在分类零图像或一图像，我们必须首先从数据集中选择这些样本：
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Visualize one sample from the zero selection and another from the handwritten
    one digits to ensure we have correctly allocated the data.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化一个来自零选择的样本和另一个来自手写数字一的样本，以确保我们已正确分配数据。
- en: 'Here''s the code for zero:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是数字零的代码：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be as follows:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.16: First handwritten image](img/C12622_04_16.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.16：第一张手写图像](img/C12622_04_16.jpg)'
- en: 'Figure 4.16: First handwritten image'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.16：第一张手写图像
- en: 'Here''s the code for one:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是一个的代码：
- en: '[PRE14]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will be as follows:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.17: Second handwritten image](img/C12622_04_17.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.17：第二张手写图像](img/C12622_04_17.jpg)'
- en: 'Figure 4.17: Second handwritten image'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.17：第二张手写图像
- en: 'We are almost at the stage where we can start building the model, however,
    as each sample is an image and has data in a matrix format, we must first re-arrange
    each of the images. The model needs the images to be provided in vector form,
    that is, all the information for each image is stored in one row. Do that as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们几乎可以开始构建模型了，然而，由于每个样本都是图像并且数据是矩阵格式，我们必须首先重新排列每张图像。模型需要图像以向量形式提供，即每张图像的所有信息存储在一行中。按以下步骤操作：
- en: '[PRE15]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can build and fit the logistic regression model with the selected images
    and labels:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用选择的图像和标签构建并拟合逻辑回归模型：
- en: '[PRE16]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output will be:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.18: Logistic Regression Model](img/C12622_04_18.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.18：逻辑回归模型](img/C12622_04_18.jpg)'
- en: 'Figure 4.18: Logistic Regression Model'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.18：逻辑回归模型
- en: Note how the scikit-learn API calls for logistic regression are consistent with
    that of linear regression. There is an additional argument, `solver`, which specifies
    the type of optimization process to be used. We have provided this argument here
    with the default value to suppress a future warning in this version of scikit-learn
    that requires `solver` to be specified. The specifics of the `solver` argument
    are out of scope for this chapter and has only been included to suppress the warning
    message.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，scikit-learn 的逻辑回归 API 调用与线性回归的一致性。这里有一个额外的参数 `solver`，它指定要使用的优化过程类型。我们在这里提供了该参数的默认值，以抑制在当前版本的
    scikit-learn 中要求指定 `solver` 参数的未来警告。`solver` 参数的具体细节超出了本章的讨论范围，仅为抑制警告信息而包含。
- en: 'Check the performance of this model against the corresponding training data:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查此模型在相应训练数据上的表现：
- en: '[PRE17]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We''ll get the following output:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到如下输出：
- en: '![Figure 4.19: Model score](img/C12622_04_19.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.19：模型得分](img/C12622_04_19.jpg)'
- en: 'Figure 4.19: Model score'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.19：模型得分
- en: In this example, the model was able to predict the training labels with 100%
    accuracy.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，模型能够以 100% 的准确率预测训练标签。
- en: 'Display the first two predicted labels for the training data using the model:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型显示训练数据的前两个预测标签：
- en: '[PRE18]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output will be:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.20: The first two labels the model predicted](img/C12622_04_20.jpg)'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.20：模型预测的前两个标签](img/C12622_04_20.jpg)'
- en: 'Figure 4.20: The first two labels the model predicted'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.20：模型预测的前两个标签
- en: 'How is the logistic regression model making the classification decisions? Look
    at some of the probabilities produced by the model for the training set:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑回归模型是如何做出分类决策的？观察模型为训练集产生的一些概率值：
- en: '[PRE19]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output will be as follows:'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.21: Array of probabilities](img/C12622_04_21.jpg)'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.21：概率数组](img/C12622_04_21.jpg)'
- en: 'Figure 4.21: Array of probabilities'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.21：概率数组
- en: We can see that, for each prediction made, there are two probability values.
    The first corresponding to the probability of the class being zero, the second
    the probability of the class being one, both of which add up to one. We can see
    that, in the first example, the prediction probability is 0.9999999 for class
    zero and thus the prediction is class zero. Similarly, the inverse is true for
    the second example.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，对于每个预测，都会有两个概率值。第一个是类为零的概率，第二个是类为一的概率，二者加起来为一。我们可以看到，在第一个例子中，预测概率为 0.9999999（类零），因此预测为类零。同样地，第二个例子则相反。
- en: 'Compute the performance of the model against the test set to check its performance
    against data that it has not seen:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的表现，以检查其在未见过的数据上的性能：
- en: '[PRE20]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output will be:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.22: Model score](img/C12622_04_22.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.22：模型得分](img/C12622_04_22.jpg)'
- en: 'Figure 4.22: Model score'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.22：模型得分
- en: Note
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to *Chapter 6*, *Model Evaluation*, for better methods of objectively
    measuring the model performance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*第6章*，*模型评估*，了解更好的客观衡量模型性能的方法。
- en: We can see here that logistic regression is a powerful classifier that is able
    to distinguish between hand-written samples of zero and one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，逻辑回归是一个强大的分类器，能够区分手写的零和一。
- en: 'Exercise 38: Logistic Regression – Multiclass Classifier'
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 38：逻辑回归——多类分类器
- en: 'In the previous exercise, we examined using logistic regression to classify
    between one of two groups. Logistic regression, however, can also be used to classify
    a set of input information to *k* different groups and it is this multiclass classifier
    we will be investigating in this exercise. The process for loading the MNIST training
    and test data is identical to the previous exercise:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习中，我们使用逻辑回归对两类进行分类。然而，逻辑回归也可以用于将一组输入信息分类到 *k* 个不同的组，这就是我们在本练习中要研究的多类分类器。加载
    MNIST 训练和测试数据的过程与之前的练习相同：
- en: 'Load the training/test images and the corresponding labels:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练/测试图像及其对应的标签：
- en: '[PRE21]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Given that the training data is so large, we will select a subset of the overall
    data to reduce the training time as well as the system resources required for
    the training process:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于训练数据量较大，我们将选择整体数据的一个子集，以减少训练时间及训练过程中所需的系统资源：
- en: '[PRE22]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that, in this example, we are using data from all 10 classes, not just
    classes zero and one, so we are making this example a multiclass classification
    problem.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在此示例中，我们使用的是所有 10 个类别的数据，而不仅仅是零类和一类，因此我们将此示例设置为多类分类问题。
- en: 'Again, reshape the input data in vector form for later use:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次将输入数据重塑为向量形式，以便后续使用：
- en: '[PRE23]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be as follows:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.23: Reshaping the data](img/C12622_04_23.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.23：数据重塑](img/C12622_04_23.jpg)'
- en: 'Figure 4.23: Reshaping the data'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.23：数据重塑
- en: 'The next cell is intentionally commented out. Leave this code commented out
    for the moment:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一单元格故意被注释掉。暂时保持此代码为注释：
- en: '[PRE24]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Construct the logistic model. There are a few extra arguments, as follows:
    the `lbfgs` value for `solver` is geared up for multiclass problems, with additional
    `max_iter` iterations required for converging on a solution. The `multi_class`
    argument is set to `multinomial` to calculate the loss over the entire probability
    distribution:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建逻辑回归模型。这里有一些额外的参数，如下所示：`solver` 的 `lbfgs` 值适用于多类问题，需要额外的 `max_iter` 迭代次数来收敛到解。`multi_class`
    参数设置为 `multinomial`，以计算整个概率分布的损失：
- en: '[PRE25]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be as follows:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.24: Logistic regression model](img/C12622_04_24.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.24：逻辑回归模型](img/C12622_04_24.jpg)'
- en: 'Figure 4.24: Logistic regression model'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.24：逻辑回归模型
- en: Note
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Refer to the documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
    for more information on the arguments.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有关参数的更多信息，请参阅文档：[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)。
- en: 'Determine the accuracy score against the training set:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定训练集的准确性评分：
- en: '[PRE26]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output will be:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将是：
- en: '![Figure 4.25: Model score](img/C12622_04_25.jpg)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.25：模型评分](img/C12622_04_25.jpg)'
- en: 'Figure 4.25: Model score'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.25：模型评分
- en: 'Determine the first two predictions for the training set and plot the images
    with the corresponding predictions:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定训练集的前两个预测值，并绘制相应预测的图像：
- en: '[PRE27]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure 4.26: Model score predicted values](img/C12622_04_26.jpg)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.26：模型评分预测值](img/C12622_04_26.jpg)'
- en: 'Figure 4.26: Model score predicted values'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.26：模型评分预测值
- en: 'Show the images for the first two samples of the training set to see whether
    we are correct:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示训练集前两个样本的图像，查看我们的判断是否正确：
- en: '[PRE28]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output will be as follows:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.27: Images plotted using prediction](img/C12622_04_27.jpg)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.27：使用预测绘制的图像](img/C12622_04_27.jpg)'
- en: 'Figure 4.27: Images plotted using prediction'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.27：使用预测绘制的图像
- en: 'Again, print out the probability scores provided by the model for the first
    sample of the training set. Confirm that there are 10 different values for each
    of the 10 classes in the set:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次打印出模型为训练集第一个样本提供的概率分数。确认每个类别有 10 个不同的值：
- en: '[PRE29]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output will be as follows:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.28: Array of predicted values](img/C12622_04_28.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.28：预测值数组](img/C12622_04_28.jpg)'
- en: 'Figure 4.28: Array of predicted values'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.28：预测值数组
- en: Notice that, in the probability array of the first sample, the fifth (index
    four) sample is the highest probability, thus indicating a prediction of four.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在第一个样本的概率数组中，第五个（索引为四）样本的概率最高，因此表示预测为四。
- en: 'Compute the accuracy of the model against the test set. This will provide a
    reasonable estimate of the model''s *in the wild* performance, as it has never
    seen the data in the test set. It is expected that the accuracy rate of the test
    set will be slightly lower than the training set, given that the model has not
    been exposed to this data:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的准确性。这将提供一个合理的估计值，表示模型在*实际环境*中的表现，因为它从未见过测试集中的数据。考虑到模型没有接触过这些数据，测试集的准确率预计会稍微低于训练集：
- en: '[PRE30]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output will be as follows:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.29: Model score'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.29：模型得分'
- en: '](img/C12622_04_29.jpg)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12622_04_29.jpg)'
- en: 'Figure 4.29: Model score'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.29：模型得分
- en: When checked against the test set, the model produced accuracy of 87.8%. When
    applying a test set, a performance drop is expected, as this is the very first
    time the model has seen these samples; while, during training, the training set
    was repeatedly shown to the model.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在测试集上检查时，模型的准确率为87.8%。应用测试集时，性能下降是可以预期的，因为这是模型第一次接触这些样本；而在训练过程中，训练集已经多次呈现给模型。
- en: 'Find the cell with the commented-out code, as shown in *step four*. Uncomment
    the code in this cell:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到包含注释掉的代码的单元格，如*步骤四*所示。取消注释该单元格中的代码：
- en: '[PRE31]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This cell simply scales all the image values to between zero and one. Grayscale
    images are comprised of pixels with values between and including 0 – 255, where
    0 is black and 255 is white.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个单元格只是将所有图像值缩放到零和一之间。灰度图像由像素组成，这些像素的值在0到255之间，包括0（黑色）和255（白色）。
- en: Click **Restart & Run-All** to rerun the entire notebook.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**重新启动并运行全部**以重新运行整个笔记本。
- en: 'Find the training set error:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到训练集误差：
- en: '[PRE32]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We''ll get the following score:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下得分：
- en: '![Figure 4.30: Training set model score](img/C12622_04_30.jpg)'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.30：训练集模型得分](img/C12622_04_30.jpg)'
- en: 'Figure 4.30: Training set model score'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.30：训练集模型得分
- en: 'Find the test set error:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到测试集误差：
- en: '[PRE33]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We''ll get the following score:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下得分：
- en: '![Figure 4.31: Test set model score](img/C12622_04_31.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.31：测试集模型得分](img/C12622_04_31.jpg)'
- en: 'Figure 4.31: Test set model score'
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.31：测试集模型得分
- en: What effect did normalizing the images have on the overall performance of the
    system? The training error is worse! We went from 100% accuracy in the training
    set to 98.6%. Yes, there was a reduction in the performance of the training set,
    but an increase in the test set from 87.8% accuracy to 90.02%. The test set performance
    is of more interest, as the model has not seen this data before, and so it is
    a better representation of the performance than we could expect once the model
    is in the field. So, why do we get a better result? Again, review *Figure 4.13*,
    and notice the shape of the curve as it approaches and . The curve saturates or
    flattens at almost zero and almost one. So, if we use an image (or *x* values)
    of between 0 and 255, the class probability defined by the logistic function is
    well within this flat region of the curve. Predictions within this region are
    highly unlikely to change very much, as they will need to have very large changes
    in *x* values for any meaningful change in *y*. Scaling the images to be between
    zero and one initially puts the predictions closer to *p(x) = 0.5*, and so, changes
    in *x* can have a bigger impact on the value for *y*. This allows for more sensitive
    predictions and results in getting a couple of predictions in the training set
    wrong, but more in the test set right. It is recommended, for your logistic regression
    models, that you scale the input values to be between either zero and one or negative-one
    and one prior to training and testing.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化图像对系统整体性能有什么影响？训练误差变得更糟！我们从训练集的100%准确率下降到了98.6%。是的，训练集的性能有所下降，但测试集的准确率却从87.8%提高到了90.02%。测试集的性能更为重要，因为模型之前没有见过这些数据，因此它能更好地代表模型在实际应用中的表现。那么，为什么我们会得到更好的结果呢？再次查看*图
    4.13*，注意曲线接近零和接近一时的形状。曲线在接近零和接近一时趋于饱和或平坦。因此，如果我们使用0到255之间的图像（或*x*值），由逻辑函数定义的类别概率将位于曲线的平坦区域内。位于该区域内的预测变化很小，因为要产生任何有意义的变化，需要*x*值发生非常大的变化。将图像缩放到零和一之间，最初会将预测值拉近*p(x)
    = 0.5*，因此，*x*的变化会对*y*值产生更大的影响。这允许更敏感的预测，并导致训练集中的一些预测错误，但测试集中的更多预测是正确的。建议在训练和测试之前，将输入值缩放到零和一之间，或者负一和一之间，用于你的逻辑回归模型。
- en: 'The following function will scale values of a NumPy array between negative-one
    and one with a mean of approximately zero:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将把 NumPy 数组中的值缩放到-1到1之间，均值大约为零：
- en: '[PRE34]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Activity 11: Linear Regression Classifier – Two-Class Classifier'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 11：线性回归分类器 – 二分类器
- en: 'In this activity, we will build a two-class linear regression-based classifier
    using the MNIST dataset to classify between two digits: zero and one.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将使用 MNIST 数据集构建一个基于线性回归的二分类器，用于区分数字零和数字一。
- en: 'The steps to be performed are as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: 'Import the required dependencies:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的依赖项：
- en: '[PRE35]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Load the MNIST data into memory.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 MNIST 数据加载到内存中。
- en: Visualize a sample of the data.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化一组数据样本。
- en: Construct a linear classifier model to classify the digits zero and one. The
    model we are going to create is to determine whether the samples are either the
    digits zero or one. To do this, we first need to select only those samples.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个线性分类器模型来分类数字零和数字一。我们将创建的模型是用来判断样本是数字零还是数字一。为此，我们首先需要仅选择这些样本。
- en: Visualize the selected information with images of one sample of zero and one
    sample of one.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用零样本和一样本的图像可视化选择的信息。
- en: In order to provide the image information to the model, we must first flatten
    the data out so that each image is 1 x 784 pixels in shape.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了向模型提供图像信息，我们必须先将数据展平，使每个图像的形状为 1 x 784 像素。
- en: Let's construct the model; use the `LinearRegression` API and call the `fit`
    function.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建模型；使用 `LinearRegression` API 并调用 `fit` 函数。
- en: Determine the R2 score against the training set.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集的 R2 分数。
- en: Determine the label predictions for each of the training samples, using a threshold
    of 0.5\. Values greater than 0.5 classify as one; values less than or equal to
    0.5 classify as zero.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用阈值 0.5 来确定每个训练样本的标签预测。大于 0.5 的值分类为一；小于或等于 0.5 的值分类为零。
- en: Compute the classification accuracy of the predicted training values versus
    the ground truth.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测训练值与实际值之间的分类准确率。
- en: Compare the performance against the test set.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与测试集的性能进行比较。
- en: Note
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 352.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第 352 页找到。
- en: 'Activity 12: Iris Classification Using Logistic Regression'
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 12：使用逻辑回归进行虹膜分类
- en: 'In this activity, we will be using the well-known Iris Species dataset (available
    at [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)
    or on GitHub at [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python))
    created in 1936 by botanist, Ronald Fisher. The dataset contains sepal and petal
    length and width measurements for three different iris flower species: iris setosa,
    iris versicolor, and iris virginica. In this activity, we will use the measurements
    provided in the dataset to classify the different flower species.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将使用著名的虹膜物种数据集（可在 [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)
    或 GitHub 上的 [https://github.com/TrainingByPackt/Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Supervised-Learning-with-Python)
    获取），该数据集由植物学家罗纳德·费舍尔于 1936 年创建。数据集包含三种不同虹膜花卉物种的萼片和花瓣的长度和宽度测量值：虹膜变色花、虹膜佛罗伦萨和虹膜维尔吉尼卡。在这个活动中，我们将使用数据集中提供的测量值来分类不同的花卉物种。
- en: 'The steps to be performed are as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: 'Import the required packages. For this activity, we will require the pandas
    package for loading the data, the Matplotlib package for plotting, and scikit-learn
    for creating the logistic regression model. Import all the required packages and
    relevant modules for these tasks:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的软件包。在本次活动中，我们将需要 pandas 包来加载数据，Matplotlib 包来绘图，以及 scikit-learn 包来创建逻辑回归模型。导入所有必需的软件包和相关模块以完成这些任务：
- en: '[PRE36]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Load the iris dataset using pandas and examine the first five rows.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 加载虹膜数据集并检查前五行。
- en: The next step is feature engineering. We need to select the most appropriate
    features that will provide the most powerful classification model. Plot a number
    of different features versus the allocated species classifications, for example,
    sepal length versus petal length and species. Visually inspect the plots and look
    for any patterns that could indicate separation between each of the species.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是特征工程。我们需要选择最合适的特征，以提供最强大的分类模型。绘制多个不同的特征与分配的物种分类之间的关系图，例如，萼片长度与花瓣长度及物种。通过可视化检查这些图形，查找可能表明不同物种之间分离的模式。
- en: 'Select the features by writing the column names in the following list:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在以下列表中编写列名来选择特征：
- en: '[PRE37]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Before we can construct the model, we must first convert the `species` values
    into labels that can be used within the model. Replace the `Iris-setosa` species
    string with the value `0`, the `Iris-versicolor` species string with the value
    `1`, and the `Iris-virginica` species string with the value `2`.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建模型之前，我们必须先将`species`值转换为可以在模型中使用的标签。将`Iris-setosa`物种字符串替换为值`0`，将`Iris-versicolor`物种字符串替换为值`1`，将`Iris-virginica`物种字符串替换为值`2`。
- en: Create the model using the `selected_features` and the assigned `species` labels.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`selected_features`和分配的`species`标签创建模型。
- en: Compute the accuracy of the model against the training set.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在训练集上的准确性。
- en: Construct another model using your second choice `selected_features` and compare
    the performance.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第二选择的`selected_features`构建另一个模型，并比较其性能。
- en: Construct another model using all available information and compare the performance.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有可用信息构建另一个模型，并比较其性能。
- en: Note
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 357.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第357页找到。
- en: Classification Using K-Nearest Neighbors
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用K近邻分类
- en: 'Now that we are comfortable with creating multiclass classifiers using logistic
    regression and are getting reasonable performance with these models, we will turn
    our attention to another type of classifier: the **K-nearest neighbors** (**K-NN**)
    clustering method of classification. This is a handy method, as it can be used
    in both supervised classification problems as well as in unsupervised problems.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了使用逻辑回归创建多类分类器，并且在这些模型中取得了合理的性能，接下来我们将注意力转向另一种分类器：**K-最近邻**（**K-NN**）聚类方法。这是一种非常实用的方法，因为它不仅可以应用于监督分类问题，还可以应用于无监督问题。
- en: '![Figure 4.32: Visual representation of K-NN](img/C12622_04_32.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图4.32：K-NN的可视化表示](img/C12622_04_32.jpg)'
- en: 'Figure 4.32: Visual representation of K-NN'
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.32：K-NN的可视化表示
- en: The solid circle approximately in the center is the test point requiring classification,
    while the inner circle shows the classification process where *K=3* and the outer
    circle where *K=5*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 大约位于中心的实心圆是需要分类的测试点，而内圆表示分类过程，其中*K=3*，外圆表示*K=5*。
- en: K-NN is one of the simplest "learning" algorithms available for data classification.
    The use of learning in quotation marks is explicit, as K-NN doesn't really learn
    from the data and encode these learnings in parameters or weights like other methods,
    such as logistic regression. K-NN uses instance-based or lazy learning in that
    it simply stores or memorizes all the training samples and the corresponding classes.
    It derives its name, K-nearest neighbors, from the fact that, when a test sample
    is provided to the algorithm for class prediction, it uses a majority vote of
    the *K*-nearest points to determine the corresponding class. If we look at *Figure
    4.35*, the solid circle is the test point to be classified. If we use *K=3*, the
    nearest three points lie within the inner dotted circle, and, in this case, the
    classification would be a hollow circle. If, however we were to take *K=5*, the
    nearest five points lie within the outer dotted circle and the classification
    would be a cross (three crosses to two hollow circles).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN是数据分类中最简单的“学习”算法之一。这里使用“学习”一词是明确的，因为K-NN并不像其他方法（例如逻辑回归）那样从数据中学习并将这些学习结果编码为参数或权重。K-NN使用基于实例或懒惰学习，它只是存储或记住所有训练样本及其对应的类别。当一个测试样本提供给算法进行分类时，它通过对*K*个最近点的多数投票来确定对应的类别，从而得名K-最近邻。如果我们查看*图4.35*，实心圆表示需要分类的测试点。如果我们使用*K=3*，最近的三个点位于内虚线圆内，在这种情况下，分类结果将是空心圆。然而，如果我们使用*K=5*，最近的五个点位于外虚线圆内，分类结果将是交叉标记（三个交叉标记对两个空心圆）。
- en: 'This figure highlights a few characteristics of K-NN classification that should
    be considered:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 该图突出显示了K-NN分类的一些特征，这些特征应当加以考虑：
- en: 'The selection of *K* is quite important. In this simple example, switching
    *K* from three to five flipped the class prediction due to the proximity of both
    classes. As the final classification is taken by a majority vote, it is often
    useful to use odd numbers of *K* to ensure that there is a winner in the voting
    process. If an even value of *K* is selected, and a tie in the vote occurs, then
    there are a number of different methods available for breaking the tie, including:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K* 的选择非常重要。在这个简单的例子中，将 *K* 从三改为五，由于两个类别的接近性，导致了分类预测的翻转。由于最终的分类是通过多数投票决定的，因此通常使用奇数的
    *K* 值以确保投票中有一个获胜者。如果选择了偶数的 *K* 值并且投票发生了平局，那么有多种方法可以用来打破平局，包括：'
- en: Reducing *K* by one until the tie is broken
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 *K* 减少一个直到平局被打破
- en: Selecting the class on the basis of the smallest Euclidean distance to the nearest
    point
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于最近点的最小欧几里得距离选择类别
- en: Applying a weighting function to bias the test point toward those neighbors
    which are closer
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用加权函数以偏向距离较近的邻居
- en: K-NN models have the ability to form extremely complex non-linear boundaries,
    which can be advantageous in classifying images or datasets with interesting boundaries.
    Considering that, in *Figure 4.35*, the test point changes from a hollow circle
    classification to a cross with an increase in *K*, we can see here that a complex
    boundary could be formed.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-NN 模型具有形成极其复杂的非线性边界的能力，这在对图像或具有有趣边界的数据集进行分类时可能具有优势。考虑到在 *图 4.35* 中，随着 *K*
    的增加，测试点的分类从空心圆变为十字，我们可以看到此处可能形成复杂的边界。
- en: K-NN models can be highly sensitive to local features in the data, given that
    the classification process is only really dependent on the nearby points.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-NN 模型对数据中的局部特征非常敏感，因为分类过程实际上仅依赖于邻近的点。
- en: As K-NN models memorize all the training information to make predictions, they
    can struggle with generalizing to new, unseen data.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 K-NN 模型将所有训练信息都记住以进行预测，因此它们在对新的、未见过的数据进行泛化时可能会遇到困难。
- en: There is another variant of K-NN, which, rather than specifying the number of
    nearest neighbors, specifies the size of the radius around the test point at which
    to look. This method, known as the **radius neighbors classification**, will not
    be considered in this chapter, but in understanding K-NN, you will also develop
    an understanding of radius neighbors classification and how to use the model through
    scikit-learn.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN 还有一种变体，它不是指定最近邻的数量，而是指定测试点周围的半径大小以进行查找。这种方法称为 **半径邻居分类**，本章将不考虑这种方法，但在理解
    K-NN 的过程中，你也会对半径邻居分类有一定了解，并学习如何通过 scikit-learn 使用该模型。
- en: Note
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Our explanation of K-NN classification and the next exercise examines modeling
    data with two features or two dimensions as it enables simple visualization and
    a greater understanding of the K-NN modeling process. K-NN classification, like
    linear and logistic regression, is not limited to use in only two dimensions;
    it can be applied in *N* dimensional datasets as well. This will be examined in
    further detail in *Activity 13: K-NN Multiclass Classifier*, wherein we''ll classify
    MNIST using K-NN. Remember, just because there are too many dimensions to plot,
    it doesn''t mean it cannot be classified with *N* dimensions.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 K-NN 分类的解释以及接下来的练习将重点研究具有两个特征或两个维度的数据建模，因为这样可以简化可视化并更好地理解 K-NN 建模过程。像线性回归和逻辑回归一样，K-NN
    分类并不限于仅用于二维数据集；它也可以应用于 *N* 维数据集。我们将在 *活动 13：K-NN 多分类分类器* 中进一步详细探讨这一点，在该活动中，我们将使用
    K-NN 对 MNIST 数据集进行分类。请记住，仅仅因为无法绘制过多维度的图形，并不意味着它不能在 *N* 维数据集中进行分类。
- en: 'Exercise 39: K-NN Classification'
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 39：K-NN 分类
- en: 'To allow visualization of the K-NN process, we will turn our attention in this
    exercise to a different dataset, the well-known Iris dataset. This dataset is
    provided as part of the accompanying code files for this book:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许可视化 K-NN 过程，本练习将把重点转移到另一个数据集——著名的 Iris 数据集。该数据集作为本书附带代码文件的一部分提供：
- en: 'For this exercise, we need to import pandas, Matplotlib, and the `KNeighborsClassifier`
    of scikit-learn. We will use the shorthand notation `KNN` for quick access:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本练习，我们需要导入 pandas、Matplotlib 以及 scikit-learn 的 `KNeighborsClassifier`。我们将使用简写符号
    `KNN` 以便快速访问：
- en: '[PRE38]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the Iris dataset and examine the first five rows:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 Iris 数据集并查看前五行：
- en: '[PRE39]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.33: First five rows](img/C12622_04_33.jpg)'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.33：前五行](img/C12622_04_33.jpg)'
- en: 'Figure 4.33: First five rows'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.33：前五行
- en: 'At this stage, we need to choose the most appropriate features from the dataset
    for use with the classifier. We could simply select all four (sepal and petal,
    length and width), however, as this exercise is designed to allow visualization
    of the K-NN process, we will only select sepal length and petal width. Construct
    a scatterplot for sepal length versus petal width for each of the classes in the
    dataset with the corresponding species:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，我们需要从数据集中选择最合适的特征用于分类器。我们可以简单地选择所有四个（萼片和花瓣的长度和宽度），但由于这个练习旨在允许可视化 K-NN
    过程，我们只选择萼片长度和花瓣宽度。为数据集中的每个类构建萼片长度与花瓣宽度的散点图，并标注相应的物种：
- en: '[PRE40]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output will be as follows:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.34: Scatterplot of iris data](img/C12622_04_34.jpg)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.34: 鸢尾花数据的散点图](img/C12622_04_34.jpg)'
- en: 'Figure 4.34: Scatterplot of iris data'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.34: 鸢尾花数据的散点图'
- en: 'Looking at this graph, we can see that the species are reasonably well separated
    by the petal width, with the greatest similarity between the Iris versicolor and
    the Iris virginica species. There are a couple of Iris virginica species points
    that lie within the Iris versicolor cluster. As a test point for later use, select
    one of these points at the boundary—sample 134:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这张图中可以看出，花瓣宽度在某种程度上合理地分开了这些物种，特别是鸢尾花杂色和鸢尾花维吉尼亚物种之间具有最大的相似性。在鸢尾花维吉尼亚物种的群集中，有几个点位于鸢尾花杂色群集内。作为稍后使用的测试点，选择边界样本
    134：
- en: '[PRE41]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output will be:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.35: Boundary sample](img/C12622_04_35.jpg)'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.35: 边界样本](img/C12622_04_35.jpg)'
- en: 'Figure 4.35: Boundary sample'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.35: 边界样本'
- en: 'Plot the data again, highlighting the location of the test sample/point:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次绘制数据，突出显示测试样本/点的位置：
- en: '[PRE42]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output will be as follows:'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![Figure 4.36: Scatterplot with the test sample](img/C12622_04_36.jpg)'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.36: 带有测试样本的散点图](img/C12622_04_36.jpg)'
- en: 'Figure 4.36: Scatterplot with the test sample'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.36: 带有测试样本的散点图'
- en: 'Construct a K-NN classifier model with *K = 3* and fit it to the training data:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *K = 3* 构建 K-NN 分类器模型，并将其拟合到训练数据中：
- en: '[PRE43]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output will be:'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![Figure 4.37: K Neighbor classifier](img/C12622_04_37.jpg)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.37: K 近邻分类器](img/C12622_04_37.jpg)'
- en: 'Figure 4.37: K Neighbor classifier'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.37: K 近邻分类器'
- en: 'Check the performance of the model against the training set:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型在训练集上的性能：
- en: '[PRE44]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output will show the performance score:'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将显示性能评分：
- en: '![](img/C12622_04_38.jpg)'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12622_04_38.jpg)'
- en: 'Figure 4.38: Model score'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.38: 模型得分'
- en: As the accuracy is over 97% on the test set, the next step will be to check
    the test sample.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于测试集的准确率超过了97%，接下来的步骤将是检查测试样本。
- en: 'Predict the species of the test sample:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测测试样本的物种：
- en: '[PRE45]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output will be as follows:'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![Figure 4.39: Predicted test sample](img/C12622_04_39.jpg)'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.39: 预测的测试样本](img/C12622_04_39.jpg)'
- en: 'Figure 4.39: Predicted test sample'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.39: 预测的测试样本'
- en: 'Verify it with the actual species of the sample:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用实际样本的物种来验证它：
- en: '[PRE46]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output will be:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![Figure 4.40: Species of the sample](img/C12622_04_40.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.40: 样本的物种](img/C12622_04_40.jpg)'
- en: 'Figure 4.40: Species of the sample'
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.40: 样本的物种'
- en: This prediction is clearly incorrect but is not surprising given the location
    of the test point on the boundary. What would be helpful would be to know where
    the boundary for the model is drawn. We will draw this boundary in the next exercise.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测显然是错误的，但考虑到测试点位于边界上，这并不奇怪。有用的是知道模型的边界在哪里。我们将在下一个练习中绘制这个边界。
- en: 'Exercise 40: Visualizing K-NN Boundaries'
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习 40: 可视化 K-NN 边界'
- en: To visualize the decision boundaries produced by the K-NN classifier, we need
    to sweep over the prediction space, that is, the minimum and maximum values for
    petal width and sepal length, and determine the classifications made by the model
    at those points. Once we have this sweep, we can then plot the classification
    decisions made by the model.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化 K-NN 分类器生成的决策边界，我们需要在预测空间上进行扫描，即萼片宽度和长度的最小和最大值，并确定模型在这些点上的分类。一旦完成扫描，我们可以绘制模型所做的分类决策。
- en: 'Import all the relevant packages. We will also need NumPy for this exercise:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有相关的包。对于这个练习，我们还需要使用 NumPy：
- en: '[PRE47]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Load the Iris dataset into a pandas DataFrame:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鸢尾花数据集加载到 pandas 的 DataFrame 中：
- en: '[PRE48]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be:'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![Figure 4.41: First five rows](img/C12622_04_41.jpg)'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.41: 前五行](img/C12622_04_41.jpg)'
- en: 'Figure 4.41: First five rows'
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.41: 前五行'
- en: 'While we could use the species strings to create the model in the previous
    exercise, in plotting the decision boundaries, it would be more useful to map
    the species to separate integer values. To do this, create a list of the labels
    for later reference and iterate through this list, replacing the existing label
    with the corresponding index in the list:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们可以在之前的练习中使用物种字符串来创建模型，但在绘制决策边界时，将物种映射到单独的整数值会更有用。为此，创建一个标签列表以供后续参考，并遍历该列表，用对应的索引替换现有的标签：
- en: '[PRE49]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output will be as follows:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.42: First five rows](img/C12622_04_42.jpg)'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.42：前五行](img/C12622_04_42.jpg)'
- en: 'Figure 4.42: First five rows'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.42：前五行
- en: Notice the use of the `enumerate` function in the `for` loop definition. When
    iterating through the `for` loop, the `enumerate` function provides the index
    of the value in the list as well as the value itself through each iteration. We
    assign the index of the value to the `idx` variable and the value to `label`.
    Using `enumerate` in this way provides an easy way to replace the species strings
    with a unique integer label.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意在`for`循环定义中使用了`enumerate`函数。在迭代`for`循环时，`enumerate`函数在每次迭代中提供列表中值的索引和该值本身。我们将值的索引赋给`idx`变量，将值赋给`label`。以这种方式使用`enumerate`提供了一种简便的方法来用唯一的整数标签替换物种字符串。
- en: 'Construct a K-NN classification model, again using three nearest neighbors
    and fit to the sepal length and petal width with the newly labeled species data:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个K-NN分类模型，仍然使用三个最近邻，并将其拟合到新的物种标签数据的萼片长度和花瓣宽度上：
- en: '[PRE50]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output will be as follows:'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.43: K Neighbors classifier](img/C12622_04_43.jpg)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.43：K邻近分类器](img/C12622_04_43.jpg)'
- en: 'Figure 4.43: K Neighbors classifier'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.43：K邻近分类器
- en: 'To visualize our decision boundaries, we need to create a mesh or range of
    predictions across the information space, that is, all values of sepal length
    and petal width. Starting with 1mm less than the minimum for both the petal width
    and sepal length, and finishing at 1mm more than the maximum for petal width and
    sepal length, use the `arange` function of NumPy to create a range of values between
    these limits in increments of 0.1 (spacing):'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了可视化我们的决策边界，我们需要在信息空间内创建一个网格或预测范围，即所有萼片长度和花瓣宽度的值。从比花瓣宽度和萼片长度的最小值低1毫米开始，直到比它们的最大值高1毫米，使用NumPy的`arange`函数以0.1（间距）为增量创建这些限制之间的值范围：
- en: '[PRE51]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Use the NumPy `meshgrid` function to combine the two ranges into a grid:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NumPy的`meshgrid`函数将两个范围合并为一个网格：
- en: '[PRE52]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Check out `xx`:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看`xx`：
- en: '[PRE53]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output will be:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.44: Array of meshgrid xx values](img/C12622_04_44.jpg)'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.44：meshgrid xx 值数组](img/C12622_04_44.jpg)'
- en: 'Figure 4.44: Array of meshgrid xx values'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.44：meshgrid xx 值数组
- en: 'Check out `yy`:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 查看`yy`：
- en: '[PRE54]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output will be:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.45: Array of meshgrid yy values](img/C12622_04_45.jpg)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.45：meshgrid yy 值数组](img/C12622_04_45.jpg)'
- en: 'Figure 4.45: Array of meshgrid yy values'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.45：meshgrid yy 值数组
- en: 'Concatenate the mesh into a single NumPy array using `np.c_`:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.c_`将mesh拼接成一个单独的NumPy数组：
- en: '[PRE55]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We''ll get the following output:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 4.46: Array of predicted values](img/C12622_04_46.jpg)'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.46：预测值数组](img/C12622_04_46.jpg)'
- en: 'Figure 4.46: Array of predicted values'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.46：预测值数组
- en: While this function call looks a little mysterious, it simply concatenates the
    two separate arrays together (refer to [https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html))
    and is shorthand for concatenate.
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然这个函数调用看起来有些神秘，但它仅仅是将两个独立的数组连接在一起（参见[https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html](https://docs.scipy.org/doc/numpy/reference/generated/numpy.c_.html)），并且是`concatenate`的简写形式。
- en: 'Produce the class predictions for the mesh:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为网格生成类别预测：
- en: '[PRE56]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We''ll get the following output:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 4.47: Array of predicted y values](img/C12622_04_47.jpg)'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.47：预测y值数组](img/C12622_04_47.jpg)'
- en: 'Figure 4.47: Array of predicted y values'
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.47：预测y值数组
- en: 'To consistently visualize the boundaries, we will need two sets of consistent
    colors; a lighter set of colors for the decision boundaries and a darker set of
    colors for the points of the training set themselves. Create two `ListedColormaps`:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了持续可视化边界，我们需要两组一致的颜色：一组较浅的颜色用于决策边界，一组较深的颜色用于训练集点本身。创建两个`ListedColormaps`：
- en: '[PRE57]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To highlight the decision boundaries, first plot the training data according
    to the iris species, using the `cmap_bold` color scheme and different markers
    for each of the different species:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了突出显示决策边界，首先根据鸢尾花物种绘制训练数据，使用`cmap_bold`颜色方案，并为每个不同物种使用不同的标记：
- en: '[PRE58]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output will be as follows:'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12622_04_48.jpg)'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12622_04_48.jpg)'
- en: 'Figure 4.48: Scatterplot with highlighted decision boundaries'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.48：散点图与突出显示的决策边界
- en: 'Using the prediction mesh made previously, plot the decision boundaries in
    addition to the training data:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用之前创建的预测网格，绘制决策边界并与训练数据一起展示：
- en: '[PRE59]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output will be as follows:'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.49: The decision boundaries](img/C12622_04_49.jpg)'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图4.49：决策边界](img/C12622_04_49.jpg)'
- en: 'Figure 4.49: The decision boundaries'
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.49：决策边界
- en: Note
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 4.49* has been modified for grayscale print and has additional labels,
    indicating the prediction boundaries of the classes.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.49*已修改为灰度打印，并增加了额外的标签，指示类别的预测边界。'
- en: 'Activity 13: K-NN Multiclass Classifier'
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动13：K-NN多类别分类器
- en: In this activity, we will use the K-NN model to classify the MNIST dataset into
    10 different digit-based classes.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们将使用K-NN模型将MNIST数据集分类为10个不同的数字类别。
- en: 'The steps to be performed are as follows:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的步骤如下：
- en: 'Import the following packages:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下包：
- en: '[PRE60]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Load the MNIST data into memory. First the training images, then the training
    labels, then the test images, and finally, the test labels.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将MNIST数据加载到内存中。首先是训练图像，然后是训练标签，再然后是测试图像，最后是测试标签。
- en: Visualize a sample of the data.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据样本。
- en: Construct a K-NN classifier, with three nearest neighbors to classify the MNIST
    dataset. Again, to save processing power, randomly sample 5,000 images for use
    in training.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个K-NN分类器，使用三个最近邻来分类MNIST数据集。同样，为了节省处理能力，随机抽取5,000张图像用于训练。
- en: In order to provide the image information to the model, we must first flatten
    the data out so that each image is 1 x 784 pixels in shape.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将图像信息提供给模型，我们必须先将数据展平，使每个图像的形状为1 x 784像素。
- en: Build the three-neighbor KNN model and fit the data to the model. Note that,
    in this activity, we are providing 784 features or dimensions to the model, not
    simply 2.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建三邻居KNN模型并将数据拟合到模型中。请注意，在本活动中，我们为模型提供了784个特征或维度，而不仅仅是2个。
- en: Determine the score against the training set.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定与训练集的评分。
- en: Display the first two predictions for the model against the training data.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型在训练数据上的前两个预测结果。
- en: Compare the performance against the test set.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较与测试集的性能。
- en: Note
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 360.
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解答可以在第360页找到。
- en: Classification Using Decision Trees
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用决策树进行分类
- en: The final classification method that we will be examining in this chapter is
    **decision trees**, which have found particular use in applications such as natural
    language processing. There are a number of different machine learning algorithms
    that fall within the overall umbrella of decision trees, such as ID3, CART, and
    the powerful random forest classifiers (covered in *Chapter 5*, *Ensemble Modeling*).
    In this chapter, we will investigate the use of the ID3 method in classifying
    categorical data, and we will use the scikit-learn CART implementation as another
    means of classifying the Iris dataset. So, what exactly are decision trees?
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将要研究的最终分类方法是**决策树**，它在自然语言处理等应用中得到了广泛应用。决策树下有多种不同的机器学习算法，例如ID3、CART以及强大的随机森林分类器（在*第5章*，*集成建模*中介绍）。在本章中，我们将研究ID3方法在分类类别数据中的应用，并使用scikit-learn中的CART实现作为另一种分类鸢尾花数据集的方式。那么，究竟什么是决策树呢？
- en: 'As the name suggests, decision trees are a learning algorithm that apply a
    sequential series of decisions based on input information to make the final classification.
    Recalling your childhood biology class, you may have used a process similar to
    decision trees in the classification of different types of animals via dichotomous
    keys. Just like the example dichotomous key shown in *Figure 4.50*, decision trees
    aim to classify information following the result of a number of decision or question
    steps:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如名称所示，决策树是一种学习算法，它通过一系列基于输入信息的决策来进行最终分类。回想一下你小时候的生物学课，你可能使用过类似决策树的过程，通过二分法键来分类不同类型的动物。就像*图4.50*所示的二分法键，决策树旨在根据一系列决策或提问步骤的结果来分类信息：
- en: '![Figure 4.50: Animal classification using dichotomous key](img/C12622_04_50.jpg)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.50：使用二分法键进行动物分类](img/C12622_04_50.jpg)'
- en: 'Figure 4.50: Animal classification using dichotomous key'
  id: totrans-385
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.50：使用二分法键进行动物分类
- en: 'Depending upon the decision tree algorithm being used, the implementation of
    the decision steps may vary slightly, but we will be considering the implementation
    of the ID3 algorithm specifically. The **Iterative Dichotomiser 3** (**ID3**)
    algorithm aims to classify the data on the basis of each decision providing the
    largest information gain. To further understand this design, we also need to understand
    two additional concepts: entropy and information gain.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所使用的决策树算法，决策步骤的实现可能会略有不同，但我们将特别考虑 ID3 算法的实现。**迭代二分法 3**（**ID3**）算法旨在根据每个决策提供的最大信息增益来对数据进行分类。为了进一步理解这一设计，我们还需要理解两个额外的概念：熵和信息增益。
- en: Note
  id: totrans-387
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The ID3 algorithm was first proposed by the Australian researcher Ross Quinlan
    in 1985 ([https://doi.org/10.1007/BF00116251](https://doi.org/10.1007/BF00116251)).
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ID3 算法最早由澳大利亚研究员 Ross Quinlan 于 1985 年提出（[https://doi.org/10.1007/BF00116251](https://doi.org/10.1007/BF00116251)）。
- en: '**Entropy**: In the context of information theory, entropy is the average rate
    at which information is provided by a random source of data. Mathematically speaking,
    this entropy is defined as:'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**熵**：在信息论的背景下，熵是随机数据源提供信息的平均速率。从数学角度来看，这个熵定义为：'
- en: '![Figure 4.51: Entropy equation](img/C12622_04_51.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.51：熵方程](img/C12622_04_51.jpg)'
- en: 'Figure 4.51: Entropy equation'
  id: totrans-391
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.51：熵方程
- en: In this scenario, when the random source of data produces a low probability
    value, the event carries more information, as it is unexpected compared to when
    a high-probability event occurs.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当随机数据源产生低概率值时，事件携带的信息更多，因为与发生高概率事件时相比，它是不可预见的。
- en: '**Information gain**: Somewhat related to entropy is the amount of information
    gained about a random variable by observing another random variable. Given a dataset,
    *S*, and an attribute to observe, *a*, the information gain is defined mathematically
    as:'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息增益**：与熵相关的概念是通过观察另一个随机变量获得关于某个随机变量的信息量。给定一个数据集 *S* 和一个观察的属性 *a*，信息增益在数学上定义为：'
- en: '![Figure 4.52: Information gain equation](img/C12622_04_52.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.52：信息增益方程](img/C12622_04_52.jpg)'
- en: 'Figure 4.52: Information gain equation'
  id: totrans-395
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.52：信息增益方程
- en: The information gain of dataset *S*, for attribute *a*, is equal to the entropy
    of *S* minus the entropy of *S* conditional on attribute *a*, or the entropy of
    dataset *S* minus the proportion of elements in *t* to the elements in *S*, times
    the entropy of *t*, where *t* is one of the categories in attribute *a*.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 *S* 对于属性 *a* 的信息增益等于 *S* 的熵减去条件于属性 *a* 的 *S* 的熵，或者说，数据集 *S* 的熵减去 *t* 中元素的比例与
    *S* 中元素的比例，再乘以 *t* 的熵，其中 *t* 是属性 *a* 中的某个类别。
- en: If at first you find the mathematics here a little daunting, don't worry, for
    it is far simpler than it seems. To clarify the ID3 process, we will walk through
    the process using the same dataset as was provided by Quinlan in the original
    paper.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一开始觉得这里的数学有些令人畏惧，别担心，它比看起来简单得多。为了澄清 ID3 过程，我们将使用 Quinlan 在原始论文中提供的相同数据集来演示这个过程。
- en: 'Exercise 41: ID3 Classification'
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 41：ID3 分类
- en: In the original paper, Quinlan provided a small dataset of 10 weather observation
    samples labeled with either **P** to indicate that the weather was suitable for,
    say, a Saturday morning game of cricket, or baseball for our North American friends.
    If the weather was not suitable for a game, label **N** was provided. The example
    dataset described in the paper will be created in the exercise.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，Quinlan 提供了一个包含 10 个天气观察样本的小数据集，每个样本被标记为 **P**，表示天气适合进行某种活动，比如说星期六早上的板球比赛，或者北美朋友们喜欢的棒球比赛。如果天气不适合比赛，则标记为
    **N**。论文中描述的示例数据集将在练习中创建。
- en: 'In a Jupyter notebook, create a pandas DataFrame of the following training
    set:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Jupyter notebook 中，创建以下训练集的 pandas DataFrame：
- en: '[PRE61]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output will be as follows:'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.53: Pandas DataFrame](img/C12622_04_53.jpg)'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.53：Pandas DataFrame](img/C12622_04_53.jpg)'
- en: 'Figure 4.53: Pandas DataFrame'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.53：Pandas DataFrame
- en: 'In the original paper, the ID3 algorithm starts by taking a small sample of
    the training set at random and fitting the tree to this window. This can be a
    useful method for large datasets, but given that ours is quite small, we will
    simply start with the entire training set. The first step is to calculate the
    entropy for the `P` and `N`:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始论文中，ID3 算法首先随机选择一个小的训练集样本，并将树拟合到这个窗口。对于大数据集，这可能是一个有用的方法，但考虑到我们的数据集相对较小，我们将直接从整个训练集开始。第一步是计算`P`和`N`的熵：
- en: '[PRE62]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We''ll get the following output:'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 4.54: Entropy decision](img/C12622_04_54.jpg)'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.54：熵决策](img/C12622_04_54.jpg)'
- en: 'Figure 4.54: Entropy decision'
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.54：熵决策
- en: 'We will need to repeat this calculation, so wrap it into a function:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重复此计算，因此将其封装成一个函数：
- en: '[PRE63]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The next step is to calculate which attribute provides the highest information
    gain out of `groupby` method:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是计算哪一个属性通过`groupby`方法提供了最高的信息增益：
- en: '[PRE64]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output will be as follows:'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.56: Probabilities entropies and gains](img/C12622_04_56.jpg)'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.56：概率熵和增益](img/C12622_04_56.jpg)'
- en: 'Figure 4.56: Probabilities entropies and gains'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.56：概率熵和增益
- en: 'The final gain equation for **Outlook** can be re-written as:'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**前景**的最终增益方程可以重新写为：'
- en: '![Figure 4.57: Equation of information gain](img/C12622_04_57.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.57：信息增益的方程](img/C12622_04_57.jpg)'
- en: 'Figure 4.57: Equation of information gain'
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.57：信息增益的方程
- en: 'We need to repeat this process quite a few times, so wrap it in a function
    for ease of use later:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重复这个过程好几次，因此将其封装成一个函数，以便后续使用：
- en: '[PRE65]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Repeat this process for each of the other columns to compute the corresponding
    information gain:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其他每一列重复此过程，以计算相应的信息增益：
- en: '[PRE66]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We''ll get the following output:'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 4.58: Gains](img/C12622_04_58.jpg)'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.58：增益](img/C12622_04_58.jpg)'
- en: 'Figure 4.58: Gains'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.58：增益
- en: 'This information provides the first decision of the tree. We want to split
    on the maximum information gain, thus we split on **Outlook**. Look at the data
    splitting on **Outlook**:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些信息为树提供了第一个决策。我们希望根据最大的信息增益进行拆分，因此我们在**前景**上进行拆分。查看基于**前景**的数据拆分：
- en: '[PRE67]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output will be as follows:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.59: Information gain](img/C12622_04_59.jpg)'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.59：信息增益](img/C12622_04_59.jpg)'
- en: 'Figure 4.59: Information gain'
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.59：信息增益
- en: 'Notice that all the overcast records have a decision of **P**. This provides
    our first terminating leaf of the decision tree. If it is overcast, we are going
    to play, while if it is rainy or sunny, there is a chance we will not play. The
    decision tree so far can be represented as in the following figure:'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意到所有阴天记录的决策都是**P**。这为我们的决策树提供了第一个终止叶节点。如果是阴天，我们将会玩，而如果是雨天或晴天，则有可能我们不玩。到目前为止的决策树可以表示为下图：
- en: '![Figure 4.60: Decision tree](img/C12622_04_60.jpg)'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.60：决策树](img/C12622_04_60.jpg)'
- en: 'Figure 4.60: Decision tree'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.60：决策树
- en: Note
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This figure was created manually for reference and is not contained in or obtained
    from the accompanying source code.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该图是手动创建的用于参考，并未包含在随附的源代码中。
- en: 'We now repeat this process, splitting by information gain until all the data
    is allocated and all branches of the tree terminate. First, remove the overcast
    samples, as they no longer provide any additional information:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们重复这个过程，根据信息增益进行拆分，直到所有数据都被分配，决策树的所有分支终止。首先，移除阴天样本，因为它们不再提供任何额外信息：
- en: '[PRE68]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We''ll get the following output:'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 4.61: Data after removing the overcast samples](img/C12622_04_61.jpg)'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.61：移除阴天样本后的数据](img/C12622_04_61.jpg)'
- en: 'Figure 4.61: Data after removing the overcast samples'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.61：移除阴天样本后的数据
- en: 'Now, we will turn our attention to the sunny samples and will rerun the gain
    calculations to determine the best way to split the sunny information:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将注意力转向晴天样本，并重新运行增益计算以确定最佳的晴天信息拆分方式：
- en: '[PRE69]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Recompute the entropy for the sunny samples:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算晴天样本的熵：
- en: '[PRE70]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output will be:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.62: Entropy decision](img/C12622_04_62.jpg)'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.62：熵决策](img/C12622_04_62.jpg)'
- en: 'Figure 4.62: Entropy decision'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.62：熵决策
- en: 'Run the gain calculations for the sunny samples:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对晴天样本运行增益计算：
- en: '[PRE71]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output will be as follows:'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.63: Gains](img/C12622_04_63.jpg)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.63：增益](img/C12622_04_63.jpg)'
- en: 'Figure 4.63: Gains'
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.63：增益
- en: 'Again, we select the largest gain, which is **Humidity**. Group the data by
    **Humidity**:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们选择增益最大的属性，即**湿度**。根据**湿度**对数据进行分组：
- en: '[PRE72]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output will be:'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.64: After grouping data according to humidity](img/C12622_04_64.jpg)'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.64: 根据湿度分组后的数据](img/C12622_04_64.jpg)'
- en: 'Figure 4.64: After grouping data according to humidity'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.64: 根据湿度分组后的数据'
- en: 'We can see here that we have two terminating leaves in that when the **Humidity**
    is high there is a decision not to play, and, vice versa, when the **Humidity**
    is normal, there is the decision to play. So, updating our representation of the
    decision tree, we have:'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，当**湿度**高时，决策是“不玩”，而当**湿度**正常时，决策是“玩”。因此，更新我们对决策树的表示，我们得到：
- en: '![Figure 4.65: Decision tree with two values](img/C12622_04_65.jpg)'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.65: 拥有两个值的决策树](img/C12622_04_65.jpg)'
- en: 'Figure 4.65: Decision tree with two values'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.65: 拥有两个值的决策树'
- en: 'So, the last set of data that requires classification is the rainy outlook
    data. Extract only the **rain** data and rerun the entropy calculation:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，最后需要分类的数据是雨天预报数据。只提取**雨天**数据并重新运行熵计算：
- en: '[PRE73]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The output will be:'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.66: Entropy decision](img/C12622_04_66.jpg)'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.66: 熵决策](img/C12622_04_66.jpg)'
- en: 'Figure 4.66: Entropy decision'
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.66: 熵决策'
- en: 'Repeat the gain calculation with the **rain** subset:'
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对**雨天**子集重复计算增益：
- en: '[PRE74]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The output will be:'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.67: Gains](img/C12622_04_67.jpg)'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.67: 增益](img/C12622_04_67.jpg)'
- en: 'Figure 4.67: Gains'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.67: 增益'
- en: 'Again, splitting on the attribute with the largest gain value requires splitting
    on the **Windy** values. So, group the remaining information by **Windy**:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，基于最大增益值的属性进行分割时，需要根据**风速**值进行分割。所以，按**风速**对剩余信息进行分组：
- en: '[PRE75]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output will be:'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '![Figure 4.68: Data grouped according to Windy](img/C12622_04_68.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.68: 根据风速分组的数据](img/C12622_04_68.jpg)'
- en: 'Figure 4.68: Data grouped according to Windy'
  id: totrans-476
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.68: 根据风速分组的数据'
- en: 'Finally, we have all the terminating leaves required to complete the tree,
    as splitting on **Windy** provides two sets, all of which indicate either play
    (**P**) or no-play (**N**) values. Our complete decision tree is as follows:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了完成树所需的所有终止叶子节点，因为根据**风速**进行分割得到两个集合，这些集合都表示“玩”（**P**）或“不玩”（**N**）的值。我们的完整决策树如下：
- en: '![Figure 4.69: Final decision tree](img/C12622_04_69.jpg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.69: 最终决策树](img/C12622_04_69.jpg)'
- en: 'Figure 4.69: Final decision tree'
  id: totrans-479
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.69: 最终决策树'
- en: 'We can see here that decision trees, very much like K-NN models, consume the
    entire training set to construct the model. So, how do we make predictions with
    unseen information? Simply follow the tree. Look at the decision being made at
    each node and apply the data from the unseen sample. The prediction will then
    end up being the label specified at the terminating leaf. Let''s say we had a
    weather forecast for the upcoming Saturday and we wanted to predict whether we
    were going to play or not. The weather forecast is as follows:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，决策树与K-NN模型非常相似，都是利用整个训练集来构建模型。那么，如何用未见过的信息进行预测呢？只需要沿着树走，查看每个节点的决策，并应用未见样本的数据。最终的预测结果将是终止叶子节点指定的标签。假设我们有即将到来的周六的天气预报，想预测我们是否会进行游戏。天气预报如下：
- en: '![Figure 4.70: Weather forecast for upcoming Saturday](img/C12622_04_70.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.70: 即将到来的周六的天气预报](img/C12622_04_70.jpg)'
- en: 'Figure 4.70: Weather forecast for upcoming Saturday'
  id: totrans-482
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.70: 即将到来的周六的天气预报'
- en: 'The decision tree for this would be as follows (the dashed circles indicate
    selected leaves in the tree):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 该决策树如下所示（虚线圆圈表示树中选择的叶子节点）：
- en: '![Figure 4.71: Making a new prediction using a decision tree](img/C12622_04_71.jpg)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.71: 使用决策树进行新预测](img/C12622_04_71.jpg)'
- en: 'Figure 4.71: Making a new prediction using a decision tree'
  id: totrans-485
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.71: 使用决策树进行新预测'
- en: Now, hopefully, you have a reasonable understanding of the underlying concept
    of decision trees and the process of making sequential decisions. We have covered
    one of the first decision tree methodologies in this exercise, but there are certainly
    more available, and many of the more modern methods, such as random forests, do
    not suffer as much from overfitting (see *Chapter 5*, *Ensemble Modeling*, for
    more information) as ID3\. With the principles of decision trees in our toolkit,
    we will now look at applying a more complicated model using the functionality
    provided in scikit-learn.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，希望你已经对决策树的基本概念和序列决策过程有了合理的理解。在本练习中，我们介绍了其中一种初步的决策树方法，但实际上还有许多其他方法，而且许多更现代的方法，例如随机森林，比
    ID3 更不容易发生过拟合（有关更多信息，请参见*第 5 章*，*集成建模*）。在掌握了决策树的基本原理后，我们现在将探讨如何使用 scikit-learn
    提供的功能应用更复杂的模型。
- en: The scikit-learn decision tree methods implement the **CART** (**Classification
    and Regression Tree**) method, which provides the ability to use decision trees
    in both classification and regression problems. CART differs from ID3 in that
    the decisions are made by comparing the values of features against a calculated
    value, for example, for the Iris dataset, *is the petal width less than x mm*?
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的决策树方法实现了 **CART**（**分类与回归树**）方法，这提供了在分类和回归问题中使用决策树的能力。CART 与 ID3
    的不同之处在于，决策是通过将特征值与计算出的值进行比较来做出的，例如，对于 Iris 数据集，*花瓣宽度是否小于 x 毫米*？
- en: 'Exercise 42: Iris Classification Using a CART Decision Tree'
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 42：使用 CART 决策树进行 Iris 分类
- en: 'In this exercise, we will classify the Iris data using scikit-learn''s decision
    tree classifier, which can be used in both classification and regression problems:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 scikit-learn 的决策树分类器对 Iris 数据进行分类，该分类器可以应用于分类和回归问题：
- en: 'Import the required packages:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE76]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Load the Iris dataset:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 Iris 数据集：
- en: '[PRE77]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The output will be as follows:'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.72: First five rows](img/C12622_04_72.jpg)'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.72：前五行数据](img/C12622_04_72.jpg)'
- en: 'Figure 4.72: First five rows'
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.72：前五行数据
- en: 'Take a random sample of 10 rows to use for testing. Decision trees can overfit
    the training data, so this will provide an independent measure of the accuracy
    of the tree:'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机抽取 10 行数据用于测试。决策树可能会过拟合训练数据，因此这将提供一个独立的测量指标来评估树的准确性：
- en: '[PRE78]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Fit the model to the training data and check the corresponding accuracy:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到训练数据并检查相应的准确性：
- en: '[PRE79]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The output will be as follows:'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.73: Output of the model score](img/C12622_04_73.jpg)'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.73：模型得分输出](img/C12622_04_73.jpg)'
- en: 'Figure 4.73: Output of the model score'
  id: totrans-503
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.73：模型得分输出
- en: Our model achieves 100% accuracy on the training set.
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的模型在训练集上的准确率为 100%。
- en: 'Check the performance against the test set:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查测试集上的表现：
- en: '[PRE80]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output will be as follows:'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 4.74: Output of the model score using df_test](img/C12622_04_74.jpg)'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.74：使用 df_test 输出的模型得分](img/C12622_04_74.jpg)'
- en: 'Figure 4.74: Output of the model score using df_test'
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.74：使用 df_test 输出的模型得分
- en: 'One of the great things about decision trees is that we can visually represent
    the model and see exactly what is going on. Install the required dependency:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策树的一个优点是我们可以直观地表示模型，并准确看到模型的运作方式。安装所需的依赖包：
- en: '[PRE81]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Import the graphing package:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入图形绘制包：
- en: '[PRE82]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Plot the model:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制模型图：
- en: '[PRE83]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'We''ll get the following output:'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到如下输出：
- en: '![Figure 4.75: Decisions of the CART decision tree](img/C12622_04_75.jpg)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.75：CART 决策树的决策](img/C12622_04_75.jpg)'
- en: 'Figure 4.75: Decisions of the CART decision tree'
  id: totrans-518
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.75：CART 决策树的决策
- en: This figure illustrates the decisions of the CART decision tree in the scikit-learn
    model. The first line of the node is the decision that is made at each step. The
    first node *X[2] <= 2.45* indicates that the training data is split on column
    two (petal length) on the basis of being less than or equal to 2.45\. Those samples
    with a petal length of less than 2.45 (of which there are 46) are all of the iris
    setosa class, and, as such, all of a `gini` (a metric similar to information gain)
    of zero. If the petal length is greater than 2.45, the next decision is whether
    the petal width (column three) is less than or equal to 1.75mm. This decision/branching
    process continues until the tree has been exhausted and all terminating leaves
    have been constructed.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了在 scikit-learn 模型中 CART 决策树的决策过程。每个节点的第一行表示在每个步骤做出的决策。第一个节点 *X[2] <= 2.45*
    表示训练数据在第二列（花瓣长度）上进行划分，基于是否小于或等于 2.45。花瓣长度小于 2.45 的样本（共有 46 个）全部属于鸢尾花的 setosa 类，因此它们的
    `gini` 值（类似于信息增益的度量）为零。如果花瓣长度大于 2.45，下一步的决策是花瓣宽度（第三列）是否小于或等于 1.75 毫米。这个决策/分支过程会一直持续，直到树被完全展开，所有终止叶子节点都已构建完成。
- en: Summary
  id: totrans-520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We covered a number of powerful and extremely useful classification models in
    this chapter, starting with the use of linear regression as a classifier, then
    we observed a significant performance increase through the use of the logistic
    regression classifier. We then moved on to **memorizing** models, such as K-NN,
    which, while simple to fit, was able to form complex non-linear boundaries in
    the classification process, even with images as input information into the model.
    We then finished our introduction to classification problems, looking at decision
    trees and the ID3 algorithm. We saw how decision trees, like K-NN models, memorize
    the training data using rules and decision gates to make predictions with quite
    a high degree of accuracy.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了许多强大且极其有用的分类模型，从使用线性回归作为分类器开始，然后通过使用逻辑回归分类器，我们观察到了显著的性能提升。接着，我们转向了 **记忆**
    型模型，比如 K-NN，尽管它简单易于拟合，但能够在分类过程中形成复杂的非线性边界，即使输入信息是图像数据。最后，我们介绍了决策树和 ID3 算法。我们看到，像
    K-NN 模型一样，决策树通过使用规则和决策门来记住训练数据，从而进行预测，并且具有相当高的准确性。
- en: In the next chapter, we will be extending what we have learned in this chapter.
    It will cover ensemble techniques, including boosting and the very effective random
    forest method.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将扩展本章所学内容，涵盖集成技术，包括提升方法和非常有效的随机森林方法。
