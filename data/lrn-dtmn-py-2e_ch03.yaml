- en: Predicting Sports Winners with Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树预测体育比赛胜者
- en: 'In this chapter, we will look at predicting the winner of sports matches using
    a different type of classification algorithm to the ones we have seen so far:
    **decision trees**. These algorithms have a number of advantages over other algorithms.
    One of the main advantages is that they are readable by humans, allowing for their
    use in human-driven decision making. In this way, decision trees can be used to
    learn a procedure, which could then be given to a human to perform if needed.
    Another advantage is that they work with a variety of features, including categorical,
    which we will see in this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用不同于我们之前所见的分类算法来预测体育比赛的胜者：**决策树**。这些算法相较于其他算法有许多优势。其中一个主要优势是它们可以被人类阅读，这使得它们在人类驱动的决策中得以应用。通过这种方式，决策树可以用来学习一个程序，如果需要的话，可以将其交给人类执行。另一个优势是它们可以处理各种特征，包括分类特征，我们将在本章中看到。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Using the pandas library for loading and manipulating data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas库加载数据和操作数据
- en: Decision trees for classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类的决策树
- en: Random forests to improve upon decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林改进决策树
- en: Using real-world datasets in data mining
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据挖掘中使用真实世界的数据集
- en: Creating new features and testing them in a robust framework
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建新特征并在稳健的框架中测试它们
- en: Loading the dataset
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: In this chapter, we will look at predicting the winner of games of the **National
    Basketball Association** (**NBA**). Matches in the NBA are often close and can
    be decided at the last minute, making predicting the winner quite difficult. Many
    sports share this characteristic, whereby the (generally) better team could be
    beaten by another team on the right day.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨预测**美国职业篮球联赛**（**NBA**）比赛胜者的方法。NBA的比赛往往非常接近，有时在最后一刻才能分出胜负，这使得预测胜者变得相当困难。许多运动都具备这一特征，即（通常）更好的队伍在正确的一天可能会被另一支队伍击败。
- en: Various research into predicting the winner suggests that there may be an upper
    limit to sports outcome prediction accuracy which, depending on the sport, is
    between 70 percent and 80 percent. There is a significant amount of research being
    performed into sports prediction, often through data mining or statistics-based
    methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对预测胜者的各种研究表明，体育结果预测的准确性可能存在上限，这取决于运动类型，通常在70%到80%之间。目前正在进行大量的体育预测研究，通常通过数据挖掘或基于统计的方法。
- en: In this chapter, we are going to have a look at an entry level basketball match
    prediction algorithm, using decision trees for determining whether a team will
    win a given match. Unfortunately, it doesn't quite make as much profit as the
    models that sports betting agencies use, which are often a bit more advanced,
    more complex, and ultimately, more accurate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一个入门级的篮球比赛预测算法，使用决策树来确定一支队伍是否会赢得一场特定的比赛。不幸的是，它并不像体育博彩机构使用的模型那样盈利，这些模型通常更先进、更复杂，最终也更准确。
- en: Collecting the data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据
- en: 'The data we will be using is the match history data for the NBA for the 2015-2016
    season. The website  [http://basketball-reference.com](http://basketball-reference.com/)
    contains a significant number of resources and statistics collected from the NBA
    and other leagues. To download the dataset, perform the following steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据是2015-2016赛季NBA的比赛历史数据。网站 [http://basketball-reference.com](http://basketball-reference.com/)
    包含了从NBA和其他联赛收集的大量资源和统计数据。要下载数据集，请执行以下步骤：
- en: Navigate to [http://www.basketball-reference.com/leagues/NBA_2016_games.html](http://www.basketball-reference.com/leagues/NBA_2016_games.html) 
    in your web browser.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的网络浏览器中导航到 [http://www.basketball-reference.com/leagues/NBA_2016_games.html](http://www.basketball-reference.com/leagues/NBA_2016_games.html)
- en: Click Share & more.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“分享”和更多选项。
- en: Click Get table as CSV (for Excel).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“获取表格为CSV（适用于Excel）”。
- en: Copy the data, including the heading, into a text file named `basketball.csv`.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据（包括标题）复制到名为 `basketball.csv` 的文本文件中。
- en: Repeat this process for the other months, except do not copy the heading.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程以获取其他月份的数据，但不要复制标题。
- en: This will give you a CSV file containing the results from each game of this
    season of the NBA. Your file should contain 1316 games and a total of 1317 lines
    in the file, including the header line.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您提供包含NBA本赛季每场比赛结果的CSV文件。您的文件应包含1316场比赛和文件中的总行数1317行，包括标题行。
- en: CSV files are text files where each line contains a new row and each value is
    separated by a comma (hence the name). CSV files can be created manually by typing
    into a text editor and saving with a `.csv` extension. They can be opened in any
    program that can read text files but can also be opened in Excel as a spreadsheet.
    Excel (and other spreadsheet programs) can usually convert a spreadsheet to CSV
    as well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件是文本文件，其中每行包含一个新行，每个值由逗号分隔（因此得名）。CSV 文件可以通过在文本编辑器中输入并保存为 `.csv` 扩展名来手动创建。它们可以在任何可以读取文本文件的程序中打开，也可以在
    Excel 中作为电子表格打开。Excel（以及其他电子表格程序）通常可以将电子表格转换为 CSV。
- en: We will load the file with the `pandas` library, which is an incredibly useful
    library for manipulating data. Python also contains a built-in library called
    `csv` that supports reading and writing CSV files. However, we will use pandas,
    which provides more powerful functions that we will use later in the chapter for
    creating new features.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `pandas` 库加载文件，这是一个用于操作数据的极其有用的库。Python 还包含一个名为 `csv` 的内置库，它支持读取和写入 CSV
    文件。然而，我们将使用 pandas，它提供了我们将在本章后面用于创建新特征的更强大的功能。
- en: 'For this chapter, you will need to install pandas. The easiest way to install
    it is to use Anaconda''s `conda` installer, as you did in [Chapter 1](lrn-dtmn-py-2e_ch03.html),
    *Getting Started with data mining to install scikit-learn*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要安装 pandas。最简单的方法是使用 Anaconda 的 `conda` 安装程序，就像您在 [第 1 章](lrn-dtmn-py-2e_ch03.html)
    中安装 scikit-learn 一样：
- en: '`$ conda install pandas` If you have difficulty in installing pandas, head
    to the project''s website at [http://pandas.pydata.org/getpandas.html](http://pandas.pydata.org/getpandas.html)
    and read the installation instructions for your system.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ conda install pandas` 如果你在安装 pandas 时遇到困难，请访问项目的网站 [http://pandas.pydata.org/getpandas.html](http://pandas.pydata.org/getpandas.html)
    并阅读适用于您系统的安装说明。'
- en: Using pandas to load the dataset
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 加载数据集
- en: The `pandas` library is a library for loading, managing, and manipulating data.
    It handles data structures behind-the-scenes and supports data analysis functions,
    such as computing the mean and grouping data by value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` 库是一个用于加载数据、管理和操作数据的库。它在幕后处理数据结构，并支持数据分析功能，如计算平均值和按值分组数据。'
- en: When doing multiple data mining experiments, you will find that you write many
    of the same functions again and again, such as reading files and extracting features.
    Each time this reimplementation happens, you run the risk of introducing bugs.
    Using a high-quality library such as `pandas` significantly reduces the amount
    of work needed to do these functions, and also gives you more confidence in using
    well-tested code to underly your own programs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行多次数据挖掘实验时，你会发现你反复编写许多相同的函数，例如读取文件和提取特征。每次这种重新实现都会带来引入错误的风险。使用像 `pandas` 这样的高质量库可以显著减少执行这些函数所需的工作量，同时也增加了你使用经过良好测试的代码来构建自己程序的信心。
- en: Throughout this book, we will be using pandas a lot, introducing use cases as
    we go and new functions as needed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将大量使用 pandas，在介绍用例的同时，根据需要引入新功能。
- en: 'We can load the dataset using the `read_csv` function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `read_csv` 函数加载数据集：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The result of this is a pandas **DataFrame**, and it has some useful functions
    that we will use later on. Looking at the resulting dataset, we can see some issues. Type
    the following and run the code to see the first five rows of the dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这的结果是一个 pandas **DataFrame**，它有一些我们将稍后使用的有用功能。查看生成的数据集，我们可以看到一些问题。输入以下内容并运行代码以查看数据集的前五行：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here''s the output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![](img/B06162OS_03_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162OS_03_01.png)'
- en: Just reading the data with no parameters resulted in quite a usable dataset,
    but it has some issues which we will address in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用无参数读取数据就得到了一个非常可用的数据集，但它有一些问题，我们将在下一节中解决。
- en: Cleaning up the dataset
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理数据集
- en: 'After looking at the output, we can see a number of problems:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看输出后，我们可以看到许多问题：
- en: The date is just a string and not a date object
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期只是一个字符串，而不是日期对象
- en: From visually inspecting the results, the headings aren't complete or correct
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从视觉检查结果来看，标题并不完整或不正确
- en: These issues come from the data and we could fix this by altering the data itself.
    However, in doing this, we could forget the steps we took or misapply them; that
    is, we can't replicate our results. As with the previous section where we used
    pipelines to track the transformations we made to a dataset, we will use pandas
    to apply transformations to the raw data itself.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题来自数据，我们可以通过改变数据本身来解决这个问题。然而，在这样做的时候，我们可能会忘记我们采取的步骤或者错误地应用它们；也就是说，我们无法复制我们的结果。就像在前面一节中我们使用管道来跟踪我们对数据集所做的转换一样，我们将使用pandas来对原始数据进行转换。
- en: 'The `pandas.read_csv` function has parameters to fix each of these issues,
    which we can specify when loading the file. We can also change the headings after
    loading the file, as shown in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas.read_csv`函数有参数可以解决这些问题，我们可以在加载文件时指定。我们还可以在加载文件后更改标题，如下面的代码所示：'
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The results have significantly improved, as we can see if we print out the
    resulting data frame:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显著提高，正如我们可以通过打印出结果数据帧所看到的那样：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/B06162OS_03_02.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162OS_03_02.png)'
- en: 'Even in well-compiled data sources such as this one, you need to make some
    adjustments. Different systems have different nuances, resulting in data files
    that are not quite compatible with each other. When loading a dataset for the
    first time, always check the data loaded (even if it''s a known format) and also
    check the data types of the data. In pandas, this can be done with the following
    code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在像这样良好编译的数据源中，你也需要做一些调整。不同的系统有不同的细微差别，导致数据文件之间并不完全兼容。在第一次加载数据集时，始终检查加载的数据（即使它是已知的格式），并检查数据的数据类型。在pandas中，可以通过以下代码完成：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have our dataset in a consistent format, we can compute a **baseline**,
    which is an easy way to get a good accuracy on a given problem. Any decent data
    mining solution should beat this baseline figure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据集格式化为一致的形式，我们可以计算一个**基线**，这是一种在给定问题中获得良好准确率的好方法。任何合格的数据挖掘解决方案都应该超过这个基线数值。
- en: For a product recommendation system, a good baseline is to simply *recommend
    the most popular product*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于产品推荐系统，一个好的基线是简单地**推荐最受欢迎的产品**。
- en: For a classification task, it can be to *always predict the most frequent task*,
    or alternatively applying a very simple classification algorithm like **OneR**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，可以是始终预测最频繁的任务，或者选择应用一个非常简单的分类算法，如**OneR**。
- en: 'For our dataset, each match has two teams: a home team and a visitor team.
    An obvious baseline for this task is 50 percent, which is our expected accuracy
    if we simply guessed a winner at random. In other words, choosing the predicted
    winning team randomly will (over time) result in an accuracy of around 50 percent.
    With a little domain knowledge, however, we can use a better baseline for this
    task, which we will see in the next section.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据集，每场比赛有两支球队：一支主队和一支客队。这个任务的明显基线是50%，如果我们随机猜测获胜者，这是我们预期的准确率。换句话说，随机选择预测获胜的球队（随着时间的推移）将导致大约50%的准确率。然而，如果我们有一点领域知识，我们可以为这个任务使用更好的基线，我们将在下一节中看到。
- en: Extracting new features
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取新特征
- en: We will now extract some features from this dataset by combining and comparing
    the existing data. First, we need to specify our class value, which will give
    our classification algorithm something to compare against to see if its prediction
    is correct or not. This could be encoded in a number of ways; however, for this
    application, we will specify our class as 1 if the home team wins and 0 if the
    visitor team wins. In basketball, the team with the most points wins. So, while
    the data set doesn't specify who wins directly, we can easily compute it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过组合和比较现有数据从这个数据集中提取一些特征。首先，我们需要指定我们的类别值，这将给我们的分类算法提供一些东西来比较，以判断其预测是否正确。这可以通过多种方式编码；然而，对于这个应用，如果我们主队获胜，我们将指定类别为1，如果客队获胜，我们将指定类别为0。在篮球中，得分最高的球队获胜。所以，尽管数据集没有直接指定谁获胜，我们仍然可以轻松地计算出结果。
- en: 'We can specify the data set by the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式指定数据集：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then copy those values into a NumPy array to use later for our scikit-learn
    classifiers. There is not currently a clean integration between pandas and scikit-learn,
    but they work nicely together through the use of NumPy arrays. While we will use
    pandas to extract features, we will need to extract the values to use them with
    scikit-learn:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后将这些值复制到一个NumPy数组中，以便稍后用于我们的scikit-learn分类器。目前pandas和scikit-learn之间没有干净的集成，但它们通过使用NumPy数组可以很好地协同工作。虽然我们将使用pandas来提取特征，但我们需要提取这些值以便与scikit-learn一起使用：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding array now holds our class values in a format that scikit-learn
    can read.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数组现在以scikit-learn可以读取的格式存储我们的类别值。
- en: 'By the way, the better baseline figure for sports prediction is to predict
    the home team in every game. Home teams are shown to have an advantage in nearly
    all sports across the world. How big is this advantage? Let''s have a look:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，体育预测更好的基线是预测每场比赛的主队。研究表明，主队在全球几乎所有体育项目中都有优势。这个优势有多大？让我们看看：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The resulting value, around 0.59, indicates that the home team wins 59 percent
    of games on average. This is higher than 50 percent from random chance and is
    a simple rule that applies to most sports.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 结果值，大约为0.59，表明主队平均赢得59%的比赛。这高于随机机会的50%，并且是适用于大多数体育的简单规则。
- en: We can also start creating some features to use in our data mining for the input
    values (the `X` array). While sometimes we can just throw the raw data into our
    classifier, we often need to derive continuous numerical or categorical features
    from our data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以开始创建一些特征，用于我们的数据挖掘输入值（即`X`数组）。虽然有时我们可以直接将原始数据扔到我们的分类器中，但我们通常需要从我们的数据中推导出连续的数值或分类特征。
- en: For our current dataset, we can't really use the features already present (in
    their current form) to do a prediction. We wouldn't know the scores of a game
    before we would need to predict the outcome of the game, so we can not use them
    as features. While this might sound obvious, it can be easy to miss.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们当前的数据库，我们实际上不能使用现有的特征（以它们当前的形式）来进行预测。在我们需要预测比赛结果之前，我们不会知道比赛的分，因此我们不能将它们用作特征。虽然这听起来可能很明显，但很容易忽略。
- en: The first two features we want to create to help us predict which team will
    win are whether either of those two teams won their previous game. This would
    roughly approximate which team is currently playing well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要创建的前两个特征是为了帮助我们预测哪支队伍会赢，即这两个队伍中的任何一个是否赢得了他们上一场比赛。这大致可以近似出哪支队伍目前表现良好。
- en: We will compute this feature by iterating through the rows in order and recording
    which team won. When we get to a new row, we look up whether the team won the
    last time we saw them.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过按顺序遍历行并记录哪支队伍获胜来计算这个特征。当我们到达新行时，我们会查看这支队伍上次我们看到他们时是否获胜。
- en: 'We first create a (default) dictionary to store the team''s last result:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个（默认）字典来存储球队的最后一次结果：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then create a new feature on our dataset to store the results of our new
    features:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在数据集上创建一个新的特征来存储我们新特征的成果：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The key of this dictionary will be the team and the value will be whether they
    won their previous game. We can then iterate over all the rows and update the
    current row with the team''s last result:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字典的键将是球队，值将是他们是否赢得了上一场比赛。然后我们可以遍历所有行，并更新当前行的球队的最后结果：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that the preceding code relies on our dataset being in chronological order.
    Our dataset is in order; however, if you are using a dataset that is not in order,
    you will need to replace `dataset.iterrows()` with `dataset.sort("Date").iterrows()`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的代码依赖于我们的数据集是按时间顺序排列的。我们的数据集是有序的；然而，如果你使用的数据集不是按顺序排列的，你需要将`dataset.iterrows()`替换为`dataset.sort("Date").iterrows()`。
- en: Those last two lines in the loop update our dictionary with either a 1 or a
    0, depending on which team won the *current* game. This information is used for
    the next game each team plays.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 循环中的最后两行根据哪支队伍赢得了*当前*比赛来更新我们的字典，是1还是0。这些信息用于下一场每支队伍比赛。
- en: 'After the preceding code runs, we will have two new features: `HomeLastWin`
    and `VisitorLastWin`. Have a look at the dataset using `dataset.head(6)` to see
    an example of a home team and a visitor team that won their recent game. Have
    a look at other parts of the dataset using the panda''s indexer:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码运行之后，我们将有两个新的特征：`HomeLastWin`和`VisitorLastWin`。使用`dataset.head(6)`查看数据集，以查看一个主队和一个客队最近赢得比赛的例子。使用pandas的索引器查看数据集的其他部分：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Currently, this gives a false value to all teams (including the previous year's
    champion!) when they are first seen. We could improve this feature using the previous
    year's data, but we will not do that in this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，当它们首次出现时，这会给所有队伍（包括上一年的冠军！）提供一个错误的值。我们可以使用上一年的数据来改进这个功能，但在这个章节中我们不会这么做。
- en: Decision trees
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are a class of supervised learning algorithms like a flow chart
    that consists of a sequence of nodes, where the values for a sample are used to
    make a decision on the next node to go to.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一类监督学习算法，类似于由一系列节点组成的流程图，其中样本的值用于在下一个节点上进行决策。
- en: 'The following example gives a very good idea of how decision trees are a class
    of supervised learning algorithms:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例很好地说明了决策树是如何成为一类监督学习算法的：
- en: '![](img/B06162OS_03_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162OS_03_03.jpg)'
- en: 'As with most classification algorithms, there are two stages to using them:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数分类算法一样，使用它们有两个阶段：
- en: The first stage is the **training** stage, where a tree is built using training
    data. While the nearest neighbor algorithm from the previous chapter did not have
    a training phase, it is needed for decision trees. In this way, the nearest neighbor
    algorithm is a lazy learner, only doing any work when it needs to make a prediction.
    In contrast, decision trees, like most classification methods, are eager learners,
    undertaking work at the training stage and therefore needing to do less in the
    predicting stage.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个阶段是**训练**阶段，在这个阶段使用训练数据构建树。虽然上一章的最近邻算法没有训练阶段，但决策树需要它。这样，最近邻算法是一种懒惰的学习者，只有在需要做出预测时才会进行任何工作。相比之下，决策树，像大多数分类方法一样，是积极的学习者，在训练阶段进行工作，因此在预测阶段需要做的工作更少。
- en: The second stage is the **predicting** stage, where the trained tree is used
    to predict the classification of new samples. Using the previous example tree,
    a data point of `["is raining", "very windy"]` would be classed as *bad weather*.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个阶段是**预测**阶段，在这个阶段使用训练好的树来预测新样本的分类。使用之前的示例树，数据点`["is raining", "very windy"]`会被归类为*恶劣天气*。
- en: There are many algorithms for creating decision trees. Many of these algorithms
    are iterative. They start at the base node and decide the best feature to use
    for the first decision, then go to each node and choose the next best feature,
    and so on. This process is stopped at a certain point when it is decided that
    nothing more can be gained from extending the tree further.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多创建决策树的算法。其中许多算法是迭代的。它们从基节点开始，决定第一个决策使用最佳特征，然后转到每个节点并选择下一个最佳特征，依此类推。当决定进一步扩展树无法获得更多收益时，这个过程会在某个点上停止。
- en: The `scikit-learn` package implements the **Classification and Regression Trees** (**CART**)
    algorithm as its default dDecision tree class, which can use both categorical
    and continuous features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`包实现了**分类和回归树**（**CART**）算法，作为其默认的决策树类，它可以使用分类和连续特征。'
- en: Parameters in decision trees
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的参数
- en: One of the most important parameters for a Decision Tree is the **stopping criterion**.
    When the tree building is nearly completed, the final few decisions can often
    be somewhat arbitrary and rely on only a small number of samples to make their
    decision. Using such specific nodes can result in trees that significantly overfit
    the training data. Instead, a stopping criterion can be used to ensure that the
    Decision Tree does not reach this exactness.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树来说，最重要的参数之一是**停止标准**。当树构建接近完成时，最后的几个决策往往可能有些随意，并且只依赖于少量样本来做出决策。使用这样的特定节点可能导致树显著过度拟合训练数据。相反，可以使用停止标准来确保决策树不会达到这种精确度。
- en: Instead of using a stopping criterion, the tree could be created in full and
    then trimmed. This trimming process removes nodes that do not provide much information
    to the overall process. This is known as **pruning** and results in a model that
    generally does better on new datasets because it hasn't overfitted the training
    data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用停止标准，树可以完全创建，然后进行修剪。这个过程会移除对整体过程提供信息不多的节点。这被称为**剪枝**，并导致模型在新数据集上通常表现更好，因为它没有过度拟合训练数据。
- en: 'The decision tree implementation in scikit-learn provides a method to stop
    the building of a tree using the following options:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的决策树实现提供了以下选项来停止树的构建：
- en: '`**min_samples_split**`: This specifies how many samples are needed in order
    to create a new node in the Decision Tree'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**min_samples_split**`：这指定了在决策树中创建新节点所需的样本数量'
- en: '`**min_samples_leaf**`: This specifies how many samples must be resulting from
    a node for it to stay'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**min_samples_leaf**`：这指定了节点必须有多少样本才能保持'
- en: The first dictates whether a decision node will be created, while the second
    dictates whether a decision node will be kept.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个决定了是否创建决策节点，而第二个决定了是否保留决策节点。
- en: 'Another parameter for decision trees is the criterion for creating a decision.
    **Gini impurity** and** information gain** are two popular options for this parameter:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的另一个参数是创建决策的标准。**基尼不纯度**和**信息增益**是该参数的两个流行选项：
- en: '**Gini impurity**: This is a measure of how often a decision node would incorrectly
    predict a sample''s class'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基尼不纯度**：这是一个衡量决策节点错误预测样本类别的频率的指标'
- en: '**Information gain**: This uses information-theory-based entropy to indicate
    how much extra information is gained by the decision node'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息增益**：这使用基于信息论熵来指示决策节点通过多少额外的信息'
- en: These parameter values do approximately the same thing--decide which rule and
    value to use to split a node into subnodes. The value itself is simply which metric
    to use to determine that split, however this can make a significant impact on
    the final models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数值大致执行相同的功能——决定使用哪个规则和值来分割节点成子节点。值本身只是确定分割时使用哪个指标，然而这可能会对最终模型产生重大影响。
- en: Using decision trees
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树
- en: 'We can import the `DecisionTreeClassifier` class and create a Decision Tree
    using scikit-learn:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以导入`DecisionTreeClassifier`类，并使用scikit-learn创建一个决策树：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We used 14 for our `random_state` again and will do so for most of the book.
    Using the same random seed allows for replication of experiments. However, with
    your experiments, you should mix up the random state to ensure that the algorithm's
    performance is not tied to the specific value.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用了14作为`random_state`，并在本书的大部分内容中都会这样做。使用相同的随机种子允许实验的可重复性。然而，在你的实验中，你应该混合随机状态，以确保算法的性能不会与特定值相关联。
- en: 'We now need to extract the dataset from our pandas data frame in order to use
    it with our `scikit-learn` classifier. We do this by specifying the columns we
    wish to use and using the values parameter of a view of the data frame. The following
    code creates a dataset using our last win values for both the home team and the
    visitor team:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要从我们的pandas数据框中提取数据集，以便与我们的`scikit-learn`分类器一起使用。我们通过指定我们希望使用的列并使用数据框视图的值参数来完成此操作。以下代码使用主队和客队最后一场胜利的值创建了一个数据集：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Decision trees are estimators, as introduced in [Chapter 2](lrn-dtmn-py-2e_ch03.html),
    *Classifying using **scikit-learn* *Estimators*, and therefore have `fit` and
    `predict` methods. We can also use the `cross_val_score` method to get the average
    score (as we did previously):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是估计量，如第2章中介绍的，*使用**scikit-learn**估计量进行分类*，因此具有`fit`和`predict`方法。我们还可以使用`cross_val_score`方法来获取平均得分（如我们之前所做的那样）：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This scores 59.4 percent: we are better than choosing randomly! However, we
    aren''t beating our other baseline of just choosing the home team. In fact, we
    are pretty much exactly the same. We should be able to do better. **Feature engineering**
    is one of the most difficult tasks in data mining, and choosing *good* **features**
    is key to getting good outcomes—more so than choosing the right algorithm!'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分是59.4%，我们比随机选择要好！然而，我们并没有打败我们仅选择主队的其他基线。事实上，我们几乎完全相同。我们应该能够做得更好。**特征工程**是数据挖掘中最困难的任务之一，选择*好的***特征**是获得良好结果的关键——比选择正确的算法更重要！
- en: Sports outcome prediction
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 体育比赛结果预测
- en: We may be able to do better by trying other features. We have a method for testing
    how accurate our models are. The `cross_val_score` method allows us to try new
    features.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能通过尝试其他特征来做得更好。我们有一种测试模型准确性的方法。`cross_val_score`方法允许我们尝试新的特征。
- en: 'There are many possible features we could use, but we will try the following
    questions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多可能的特征，但我们将尝试以下问题：
- en: Which team is considered better generally?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常认为哪个队伍更好？
- en: Which team won their last encounter?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个队伍赢得了他们上次相遇？
- en: We will also try putting the raw teams into the algorithm, to check whether
    the algorithm can learn a model that checks how different teams play against each
    other.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将尝试将原始队伍放入算法中，以检查算法是否能够学习一个模型，该模型可以检查不同队伍之间的比赛情况。
- en: Putting it all together
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合起来
- en: For the first feature, we will create a feature that tells us if the home team
    is generally better than the visitors. To do this, we will load the standings
    (also called a ladder in some sports) from the NBA in the previous season. A team
    will be considered better if it ranked higher in 2015 than the other team.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个特征，我们将创建一个特征，告诉我们主队通常是否比客队更好。为此，我们将从上一个赛季的NBA中加载排名（在某些运动中也称为梯形）。如果一个队在2015年的排名高于另一个队，则认为该队更好。
- en: 'To obtain the standings data, perform the following steps:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取排名数据，执行以下步骤：
- en: Navigate to [http://www.basketball-reference.com/leagues/NBA_2015_standings.html](http://www.basketball-reference.com/leagues/NBA_2015_standings.html)
    in your web browser.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的网络浏览器中导航到[http://www.basketball-reference.com/leagues/NBA_2015_standings.html](http://www.basketball-reference.com/leagues/NBA_2015_standings.html)。
- en: Select Expanded Standings to get a single list for the entire league.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择扩展排名以获取整个联盟的单个列表。
- en: Click on the Export link.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击导出链接。
- en: Copy the text and save it in a text/CSV file called `standings.csv` in your
    data folder.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制文本并将其保存为名为`standings.csv`的文本/CSV文件，存放在您的数据文件夹中。
- en: 'Back in your Jupyter Notebook, enter the following lines into a new cell. You''ll
    need to ensure that the file was saved into the location pointed to by the data_folder
    variable. The code is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的Jupyter Notebook中，将以下行输入到新的单元格中。您需要确保文件已保存到data_folder变量指向的位置。代码如下：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can view the ladder by just typing standings into a new cell and running
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需在新的单元格中键入“排名”并运行即可查看梯形。
- en: 'the code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 代码：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/B06162OS_03_04.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162OS_03_04.png)'
- en: 'Next, we create a new feature using a similar pattern to the previous feature.
    We iterate over the rows, looking up the standings for the home team and visitor
    team. The code is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用与之前特征类似的方式创建一个新的特征。我们遍历行，查找主队和客队的排名。代码如下：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we use the `cross_val_score` function to test the result. First, we extract
    the dataset:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`cross_val_score`函数来测试结果。首先，我们提取数据集：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we create a new `DecisionTreeClassifier` and run the evaluation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个新的`DecisionTreeClassifier`并运行评估：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This now scores 60.9 percent  even better than our previous result, and now
    better than just choosing the home team every time. Can we do better?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这现在得分为60.9%，甚至比我们之前的结果更好，现在比每次都选择主队更好。我们能做得更好吗？
- en: 'Next, let''s test which of the two teams won their last match against each
    other. While rankings can give some hints on who won (the higher ranked team is
    more likely to win), sometimes teams play better against other teams. There are
    many reasons for this--for example, some teams may have strategies or players
    that work against specific teams really well. Following our previous pattern,
    we create a dictionary to store the winner of the past game and create a new feature
    in our data frame. The code is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们测试两支球队中哪一支在最近一场比赛中赢得了对对方的胜利。虽然排名可以提供一些关于谁获胜的线索（排名更高的球队更有可能获胜），但有时球队对其他球队的表现会更好。这有很多原因——例如，一些球队可能有针对特定球队非常有效的策略或球员。遵循我们之前的模式，我们创建一个字典来存储过去比赛的获胜者，并在我们的数据框中创建一个新的特征。代码如下：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This feature works much like our previous rank-based feature. However, instead
    of looking up the ranks, this features creates a tuple called `teams`, and then
    stores the previous result in a dictionary. When those two teams play each other
    next, it recreates this tuple, and looks up the previous result. Our code doesn't
    differentiate between home games and visitor games, which might be a useful improvement
    to look at implementing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征与我们的上一个基于排名的特征非常相似。然而，这个特征不是查找排名，而是创建一个名为`teams`的元组，并将之前的结果存储在字典中。当这两支球队再次比赛时，它将重新创建这个元组，并查找之前的结果。我们的代码没有区分主场比赛和客场比赛，这可能是一个有用的改进，可以考虑实现。
- en: 'Next, we need to evaluate. The process is pretty similar to before, except
    we add the new feature into the extracted values:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要进行评估。这个过程与之前非常相似，只是我们将新特征添加到提取的值中：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This scores 62.2 percent. Our results are getting better and better.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这得分为62.2%，我们的结果越来越好。
- en: Finally, we will check what happens if we throw a lot of data at the Decision
    Tree, and see if it can learn an effective model anyway. We will enter the teams
    into the tree and check whether a Decision Tree can learn to incorporate that
    information.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将大量数据投入决策树，看看它是否仍然可以学习到一个有效的模型。我们将球队输入到树中，并检查决策树是否可以学习到这些信息。
- en: 'While decision trees are capable of learning from categorical features, the
    implementation in `scikit-learn` requires those features to be encoded as numbers
    and features, instead of string values. We can use the `LabelEncoder` **transformer**
    to convert the string-based team names into assigned integer values. The code
    is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然决策树能够从分类特征中学习，但`scikit-learn`中的实现要求这些特征被编码为数字和特征，而不是字符串值。我们可以使用`LabelEncoder`
    **转换器**将基于字符串的球队名称转换为分配的整数值。代码如下：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We should use the same transformer for encoding both the home team and visitor
    teams. This is so that the same team gets the same integer value as both a home
    team and visitor team. While this is not critical to the performance of this application,
    it is important and failing to do this may degrade the performance of future models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该使用相同的转换器来编码主队和客队。这样，同一个球队作为主队和客队都会得到相同的整数值。虽然这对这个应用程序的性能不是至关重要，但这是重要的，而且不做这件事可能会降低未来模型的性能。
- en: These integers can be fed into the Decision Tree, but they will still be interpreted
    as continuous features by `DecisionTreeClassifier`. For example, teams may be
    allocated integers from 0 to 16\. The algorithm will see teams 1 and 2 as being
    similar, while teams 4 and 10 will be very different--but this makes no sense
    as all. All of the teams are different from each other--two teams are either the
    same or they are not!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些整数可以输入到决策树中，但它们仍然会被`DecisionTreeClassifier`解释为连续特征。例如，球队可能被分配从0到16的整数。算法会将1号和2号球队视为相似，而4号和10号球队则非常不同——但这在逻辑上并不合理。所有的球队都是不同的——两个球队要么相同，要么不同！
- en: 'To fix this inconsistency, we use the `OneHotEncoder` **transformer** to encode
    these integers into a number of binary features. Each binary feature will be a
    single value for the feature. For example, if the NBA team Chicago Bulls is allocated
    as integer 7 by the `LabelEncoder`, then the seventh feature returned by the `OneHotEncoder`
    will be a 1 if the team is Chicago Bulls and 0 for all other features/teams. This
    is done for every possible value, resulting in a much larger dataset. The code
    is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了修复这种不一致性，我们使用`OneHotEncoder` **转换器**将这些整数编码成一系列二进制特征。每个二进制特征将代表一个特征的单个值。例如，如果NBA球队芝加哥公牛被`LabelEncoder`分配为整数7，那么`OneHotEncoder`返回的第七个特征将为1，如果球队是芝加哥公牛，而对于所有其他特征/球队则为0。这是对每个可能的值都这样做，结果是一个更大的数据集。代码如下：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we run the Decision Tree as before on the new dataset:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在新的数据集上运行决策树，就像之前一样：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This scores an accuracy of 62.8 percent. The score is better still, even though
    the information given is just the teams playing. It is possible that the larger
    number of features were not handled properly by the decision trees. For this reason,
    we will try changing the algorithm and see if that helps. Data mining can be an
    iterative process of trying new algorithms and features.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这达到了62.8%的准确率。尽管提供的信息仅仅是参赛的球队，分数仍然更好。可能是因为更多的特征没有被决策树正确处理。因此，我们将尝试更改算法，看看是否有所帮助。数据挖掘可能是一个尝试新算法和特征的过程。
- en: Random forests
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: A single Decision Tree can learn quite complex functions. However, decision
    trees are prone to overfitting--learning rules that work only for the specific
    training set and don't generalize well to new data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 单个决策树可以学习相当复杂的功能。然而，决策树容易过拟合——学习只对特定训练集有效的规则，并且对新数据泛化不好。
- en: One of the ways that we can adjust for this is to limit the number of rules
    that it learns. For instance, we could limit the depth of the tree to just three
    layers. Such a tree will learn the best rules for splitting the dataset at a global
    level, but won't learn highly specific rules that separate the dataset into highly
    accurate groups. This trade-off results in trees that may have a good generalization,
    but an overall slightly poorer performance on the training dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整的一种方式是限制它学习的规则数量。例如，我们可以将树的深度限制为仅仅三层。这样的树将在全局层面上学习到分割数据集的最佳规则，但不会学习到将数据集分割成高度精确组的特定规则。这种权衡导致的结果是，树可能具有良好的泛化能力，但在训练数据集上的整体性能略差。
- en: To compensate for this, we could create many of these *limited* decision trees
    and then ask each to predict the class value. We could take a majority vote and
    use that answer as our overall prediction. Random Forests is an algorithm developed
    from this insight.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补偿这一点，我们可以创建许多这些**有限的**决策树，并让每个树预测类别值。我们可以进行多数投票，并使用那个答案作为我们的整体预测。随机森林就是从这个洞察力发展而来的算法。
- en: There are two problems with the aforementioned procedure. The first problem
    is that building decision trees is largely deterministic—using the same input
    will result in the same output each time. We only have one training dataset, which
    means our input (and therefore the output) will be the same if we try to build
    multiple trees. We can address this by choosing a random subsample of our dataset,
    effectively creating new training sets. This process is called **bagging**  and it
    can be very effective in many situations in data mining.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述程序有两个问题。第一个问题是构建决策树在很大程度上是确定性的——使用相同的输入将每次都产生相同的输出。我们只有一个训练数据集，这意味着如果我们尝试构建多个树，我们的输入（因此输出）将是相同的。我们可以通过选择数据集的随机子样本来解决这个问题，从而有效地创建新的训练集。这个过程被称为**Bagging**，在许多数据挖掘情况中可以非常有效。
- en: The second problem we might run into with creating many decision trees from
    similar data is that the features that are used for the first few decision nodes
    in our tree will tend to be similar. Even if we choose random subsamples of our
    training data, it is still quite possible that the decision trees built will be
    largely the same. To compensate for this, we also choose a random subset of the
    features to perform our data splits on.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从相似数据中创建许多决策树的第二个问题可能是，我们树中的第一个几个决策节点所使用的特征将趋于相似。即使我们选择训练数据的随机子样本，仍然很可能构建的决策树在很大程度上是相同的。为了补偿这一点，我们还选择一个特征随机子集来执行我们的数据拆分。
- en: Then, we have randomly built trees using randomly chosen samples, using (nearly)
    randomly chosen features. This is a random forest and, perhaps unintuitively,
    this algorithm is very effective for many datasets, with little need to tune many
    parameters of the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用随机选择的样本和（几乎）随机选择的特征随机构建树。这是一个随机森林，也许不太直观，但这个算法对许多数据集非常有效，几乎不需要调整模型的许多参数。
- en: How do ensembles work?
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成是如何工作的？
- en: The randomness inherent in random forests may make it seem like we are leaving
    the results of the algorithm up to chance. However, we apply the benefits of averaging
    to nearly randomly built decision trees, resulting in an algorithm that reduces
    the variance of the result.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林固有的随机性可能会让人感觉我们是在将算法的结果留给机会。然而，我们通过将平均化的好处应用于几乎随机构建的决策树，从而得到一个减少结果方差的算法。
- en: '**Variance** is the error introduced by variations in the training dataset
    on the algorithm. Algorithms with a high variance (such as decision trees) can
    be greatly affected by variations to the training dataset. This results in models
    that have the problem of overfitting. In contrast, **bias** is the error introduced
    by assumptions in the algorithm rather than anything to do with the dataset, that
    is, if we had an algorithm that presumed that all features would be normally distributed,
    then our algorithm may have a high error if the features were not.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差**是指训练数据集在算法中引入的误差。具有高方差（如决策树）的算法会受到训练数据集变化的极大影响。这会导致模型存在过拟合的问题。相比之下，**偏差**是由算法中的假设引入的误差，而不是与数据集有关，也就是说，如果我们有一个假设所有特征都呈正态分布的算法，那么如果特征不是正态分布的，我们的算法可能会有很高的误差。'
- en: Negative impacts from bias can be reduced by analyzing the data to see if the
    classifier's data model matches that of the actual data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析数据以查看分类器的数据模型是否与实际数据匹配，可以减少偏差的负面影响。
- en: To use an extreme example, a classifier that always predicts true, regardless
    of the input, has a very high bias. A classifier that always predicts randomly
    would have a very high variance. Each classifier has a high degree of error but
    of a different nature.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个极端的例子来说，一个总是预测为真的分类器，无论输入如何，都具有非常高的偏差。一个总是随机预测的分类器将具有非常高的方差。每个分类器都有很高的错误率，但性质不同。
- en: By averaging a large number of decision trees, this variance is greatly reduced.
    This results, at least normally, in a model with a higher overall accuracy and
    better predictive power. The trade-offs are an increase in time and an increase
    in the bias of the algorithm.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过平均大量决策树，这种方差大大降低。这至少在正常情况下，会导致模型的整体准确率更高，预测能力更好。权衡的是时间增加和算法偏差的增加。
- en: In general, ensembles work on the assumption that errors in prediction are effectively
    random and that those errors are quite different from one classifier to another.
    By averaging the results across many models, these random errors are canceled
    out—leaving the true prediction. We will see many more ensembles in action throughout
    the rest of the book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，集成假设预测误差是有效随机的，并且这些误差在各个分类器之间相当不同。通过在许多模型之间平均结果，这些随机误差被抵消——留下真正的预测。我们将在本书的其余部分看到更多集成的实际应用。
- en: Setting parameters in Random Forests
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置随机森林的参数
- en: The Random Forest implementation in scikit-learn is called `RandomForestClassifier`,
    and it has a number of parameters. As Random Forests use many instances of `DecisionTreeClassifier`,
    they share many of the same parameters such as the `criterion` (Gini Impurity
    or Entropy/information gain), `max_features`, and `min_samples_split`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的随机森林实现被称为 `RandomForestClassifier`，并且它有许多参数。由于随机森林使用了许多 `DecisionTreeClassifier`
    的实例，它们共享许多相同的参数，例如 `criterion`（基尼不纯度或熵/信息增益）、`max_features` 和 `min_samples_split`。
- en: 'There are some new parameters that are used in the ensemble process:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成过程中使用了一些新的参数：
- en: '`n_estimators`: This dictates how many decision trees should be built. A higher
    value will take longer to run, but will (probably) result in a higher accuracy.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这决定了应该构建多少个决策树。更高的值将需要更长的时间来运行，但（可能）会导致更高的准确率。'
- en: '`oob_score`: If true, the method is tested using samples that aren''t in the
    random subsamples chosen for training the decision trees.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oob_score`：如果为真，则使用不在为训练决策树选择的随机子样本中的样本进行方法测试。'
- en: '`n_jobs`: This specifies the number of cores to use when training the decision
    trees in parallel.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_jobs`：这指定了在并行训练决策树时使用的核心数。'
- en: The `scikit-learn` package uses a library called **Joblib** for inbuilt parallelization.
    This parameter dictates how many cores to use. By default, only a single core
    is used--if you have more cores, you can increase this, or set it to -1 to use
    all cores.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 包使用名为 **Joblib** 的库来实现内置的并行化。此参数决定了使用多少核心。默认情况下，只使用单个核心——如果你有更多的核心，你可以增加这个值，或者将其设置为
    -1 以使用所有核心。'
- en: Applying random forests
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用随机森林
- en: 'Random forests in scikit-learn use the **Estimator** interface, allowing us
    to use almost the exact same code as before to do cross-fold validation:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的随机森林使用 **Estimator** 接口，允许我们使用几乎与之前完全相同的代码来进行交叉验证：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This results in an immediate benefit of 65.3 percent, up by 2.5 points by just
    swapping the classifier.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致立即受益65.3%，仅通过交换分类器就提高了2.5个百分点。
- en: 'Random forests, using subsets of the features, should be able to learn more
    effectively with more features than normal decision trees. We can test this by
    throwing more features at the algorithm and seeing how it goes:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征子集的随机森林应该能够比普通决策树更有效地学习，具有更多的特征。我们可以通过向算法投入更多特征来测试这一点，看看结果如何：
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This results in 63.3 percent—a drop in performance! One cause is the randomness
    inherent in random forests only chose some features to use rather than others.
    Further, there are many more features in  `X_teams` than in `X_lastwinner`, and
    having the extra features results in less relevant information being used. That
    said, don't get too excited by small changes in percentages, either up or down.
    Changing the random state value will have more of an impact on the accuracy than
    the slight difference between these feature sets that we just observed. Instead,
    you should run many tests with different random states, to get a good sense of
    the mean and spread of accuracy values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致63.3%——性能下降！一个原因是随机森林固有的随机性，它只选择了一些特征来使用，而不是其他特征。此外，`X_teams` 中的特征比 `X_lastwinner`
    中的特征要多得多，而额外的特征会导致使用更不相关的信息。尽管如此，也不要对百分比的小幅变化过于兴奋，无论是上升还是下降。改变随机状态值对准确性的影响将大于我们刚刚观察到的这些特征集之间微小的差异。相反，你应该运行许多具有不同随机状态的测试，以获得准确度值的平均值和分布的良好感觉。
- en: 'We can also try some other parameters using the `GridSearchCV` class, as we
    introduced in [Chapter 2](lrn-dtmn-py-2e_ch02.html), *Classifying using **scikit-learn
    Estimators*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试使用`GridSearchCV`类来尝试一些其他参数，正如我们在[第2章](lrn-dtmn-py-2e_ch02.html)中介绍的，*使用**scikit-learn
    Estimators**进行分类：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This has a much better accuracy of 67.4 percent!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这有更高的准确率，达到了67.4%！
- en: 'If we wanted to see the parameters used, we can print out the best model that
    was found in the grid search. The code is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想查看使用的参数，我们可以打印出网格搜索中找到的最佳模型。代码如下：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The result shows the parameters that were used in the best scoring model:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示了最佳得分模型中使用的参数：
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Engineering new features
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工程新特征
- en: In the previous few examples, we saw that changing the features can have quite
    a large impact on the performance of the algorithm. Through our small amount of
    testing, we had more than 10 percent variance just from the features.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的几个例子中，我们看到了改变特征可以对算法的性能产生相当大的影响。通过我们的小量测试，我们只从特征上就有超过10%的方差。
- en: 'You can create features that come from a simple function in pandas by doing
    something like this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过这样做来创建来自pandas中简单函数的特征：
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The feature_creator function must return a list of the feature''s value for
    each sample in the dataset. A common pattern is to use the dataset as a parameter:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`feature_creator`函数必须返回数据集中每个样本的特征值列表。一个常见的模式是将数据集作为参数使用：'
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can create those features more directly by setting all the values to a
    single default value, like 0 in the next line:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将所有值设置为单个默认值（如下一行中的0）来更直接地创建这些特征：
- en: '[PRE32]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can then iterate over the dataset, computing the features as you go. We
    used
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以遍历数据集，在遍历过程中计算特征。我们使用了
- en: 'this format in this chapter to create many of our features:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的这种格式来创建我们的大多数特征：
- en: '[PRE33]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Keep in mind that this pattern isn't very efficient. If you are going to do
    this, try all of your features at once.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这种模式效率并不高。如果你要这样做，尝试一次性使用所有特征。
- en: A common *best practice* is to touch every sample as little as possible, preferably
    only once.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的**最佳实践**是尽可能少地触摸每个样本，最好是只触摸一次。
- en: 'Some example features that you could try and implement are as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试实现的一些示例特征如下：
- en: How many days has it been since each team's previous match? Teams may be tired
    if they play too many games in a short time frame.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自从每支球队上次比赛以来过去了多少天？如果他们在短时间内打了很多场比赛，球队可能会感到疲劳。
- en: How many games of the last five did each team win? This will give a more stable
    form of the `HomeLastWin` and `VisitorLastWin` features we extracted earlier (and
    can be extracted in a very similar way).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过去五场比赛中，每支球队赢了多少场？这将给出一个更稳定的`HomeLastWin`和`VisitorLastWin`特征形式，这是我们之前提取的（并且可以以非常相似的方式提取）。
- en: Do teams have a good record when visiting certain other teams? For instance,
    one team may play well in a particular stadium, even if they are the visitors.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当球队访问某些其他球队时，他们的记录好吗？例如，一支球队可能在特定的球场上表现良好，即使他们是客队。
- en: If you are facing trouble extracting features of these types, check the pandasdocumentation
    at [http://pandas.pydata.org/pandas-docs/stable/](http://pandas.pydata.org/pandas-docs/stable/)
    for help. Alternatively, you can try an online forum such as Stack Overflow for
    assistance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你面临提取这些类型特征的问题，请查阅[pandas文档](http://pandas.pydata.org/pandas-docs/stable/)以获取帮助。或者，你可以尝试像Stack
    Overflow这样的在线论坛以获得帮助。
- en: More extreme examples could use player data to estimate the strength of each
    team's sides to predict who won. These types of complex features are used every
    day by gamblers and sports betting agencies to try to turn a profit by predicting
    the outcome of sports matches.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 更极端的例子可能会使用球员数据来估计每支球队的实力，以预测谁会赢。这些类型的复杂特征每天都在赌徒和体育博彩机构中使用，以尝试通过预测体育比赛的结果来获利。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we extended our use of scikit-learn's classifiers to perform
    classification and introduced the `pandas`library to manage our data. We analyzed
    real-world data on basketball results from the NBA, saw some of the problems that
    even well-curated data introduces, and created new features for our analysis.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们扩展了我们对scikit-learn分类器的使用，以执行分类，并介绍了`pandas`库来管理我们的数据。我们分析了NBA篮球比赛结果的真实世界数据，看到了即使是精心整理的数据也会引入的一些问题，并为我们的分析创建了新的特征。
- en: We saw the effect that good features have on performance and used an ensemble
    algorithm, random forests, to further improve the accuracy. To take these concepts
    further, try to create your own features and test them out. Which features perform
    better? If you have trouble coming up with features, think about what other datasets
    can be included. For example, if key players are injured, this might affect the
    results of a specific match and cause a better team to lose.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了好的特征对性能的影响，并使用集成算法，随机森林，进一步提高了准确性。为了将这些概念进一步深化，尝试创建自己的特征并测试它们。哪些特征表现更好？如果你在构思特征方面遇到困难，考虑可以包含哪些其他数据集。例如，如果关键球员受伤，这可能会影响特定比赛的结果，导致原本更强的队伍输掉比赛。
- en: In the next chapter, we will extend the affinity analysis that we performed
    in the first chapter to create a program to find similar books. We will see how
    to use algorithms for ranking and also use an approximation to improve the scalability
    of data mining.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把我们在第一章中进行的亲和度分析扩展，创建一个寻找相似书籍的程序。我们将看到如何使用排名算法，以及如何使用近似方法来提高数据挖掘的可扩展性。
