- en: Chapter 5. Divide and Conquer – Classification Using Decision Trees and Rules
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章. 分治法 – 使用决策树和规则进行分类
- en: While deciding between several job offers with various levels of pay and benefits,
    many people begin by making lists of pros and cons, and eliminate options based
    on simple rules. For instance, ''if I have to commute for more than an hour, I
    will be unhappy.'' Or, ''if I make less than $50k, I won't be able to support
    my family.'' In this way, the complex and difficult decision of predicting one's
    future happiness can be reduced to a series of simple decisions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定是否接受多个薪资和福利不同的工作邀请时，许多人通常通过列出利弊清单，并根据简单的规则排除选项。例如，“如果我需要通勤超过一个小时，我会感到不高兴。”或者，“如果我的收入低于50k美元，我将无法养活我的家人。”通过这种方式，预测未来幸福的复杂且艰难的决策可以简化为一系列简单的决策。
- en: This chapter covers decision trees and rule learners—two machine learning methods
    that also make complex decisions from sets of simple choices. These methods then
    present their knowledge in the form of logical structures that can be understood
    with no statistical knowledge. This aspect makes these models particularly useful
    for business strategy and process improvement.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讲解了决策树和规则学习器——这两种机器学习方法也从一系列简单的选择中做出复杂决策。这些方法然后以逻辑结构的形式呈现其知识，且无需统计学知识也能理解。这一特点使得这些模型在商业战略和流程改进方面特别有用。
- en: 'By the end of this chapter, you will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将学到：
- en: How trees and rules "greedily" partition data into interesting segments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树和规则如何“贪婪地”将数据划分为有趣的片段
- en: The most common decision tree and classification rule learners, including the
    C5.0, 1R, and RIPPER algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的决策树和分类规则学习器，包括C5.0、1R和RIPPER算法
- en: How to use these algorithms to perform real-world classification tasks, such
    as identifying risky bank loans and poisonous mushrooms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用这些算法执行现实世界中的分类任务，例如识别风险较高的银行贷款和有毒蘑菇
- en: We will begin by examining decision trees, followed by a look at classification
    rules. Then, we will summarize what we've learned by previewing later chapters,
    which discuss methods that use trees and rules as a foundation for more advanced
    machine learning techniques.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从研究决策树开始，随后探讨分类规则。然后，我们将通过预览后面的章节来总结所学内容，这些章节讨论了将树和规则作为基础的更先进的机器学习技术。
- en: Understanding decision trees
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Decision tree learners are powerful classifiers, which utilize a **tree structure**
    to model the relationships among the features and the potential outcomes. As illustrated
    in the following figure, this structure earned its name due to the fact that it
    mirrors how a literal tree begins at a wide trunk, which if followed upward, splits
    into narrower and narrower branches. In much the same way, a decision tree classifier
    uses a structure of branching decisions, which channel examples into a final predicted
    class value.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习器是强大的分类器，它们利用**树结构**来建模特征与潜在结果之间的关系。正如下图所示，这种结构得名于其形状类似于字面意义上的树木，它从粗大的树干开始，向上延伸时分成越来越细的枝条。决策树分类器以相同的方式使用分支决策的结构，将示例引导到最终的预测类别值。
- en: To better understand how this works in practice, let's consider the following
    tree, which predicts whether a job offer should be accepted. A job offer to be
    considered begins at the **root node**, where it is then passed through **decision
    nodes** that require choices to be made based on the attributes of the job. These
    choices split the data across **branches** that indicate potential outcomes of
    a decision, depicted here as yes or no outcomes, though in some cases there may
    be more than two possibilities. In the case a final decision can be made, the
    tree is terminated by **leaf nodes** (also known as **terminal nodes**) that denote
    the action to be taken as the result of the series of decisions. In the case of
    a predictive model, the leaf nodes provide the expected result given the series
    of events in the tree.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解其实际应用，假设我们考虑以下这棵树，它预测一个工作邀请是否应该被接受。一个待考虑的工作邀请从**根节点**开始，然后通过**决策节点**传递，根据工作的属性做出选择。这些选择将数据分割到**分支**上，指示决策的潜在结果，这里表现为“是”或“否”的结果，尽管在某些情况下可能有多个可能性。如果可以做出最终决定，树将通过**叶节点**（也叫**终端节点**）终止，叶节点表示一系列决策的结果应该采取的行动。在预测模型中，叶节点提供给定树中一系列事件后的预期结果。
- en: '![Understanding decision trees](img/B03905_05_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![理解决策树](img/B03905_05_01.jpg)'
- en: 'A great benefit of decision tree algorithms is that the flowchart-like tree
    structure is not necessarily exclusively for the learner''s internal use. After
    the model is created, many decision tree algorithms output the resulting structure
    in a human-readable format. This provides tremendous insight into how and why
    the model works or doesn''t work well for a particular task. This also makes decision
    trees particularly appropriate for applications in which the classification mechanism
    needs to be transparent for legal reasons, or in case the results need to be shared
    with others in order to inform future business practices. With this in mind, some
    potential uses include:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的一个巨大优势是，其流程图式的树结构并不一定仅供学习者内部使用。在模型创建之后，许多决策树算法会以人类可读的格式输出生成的结构。这为我们提供了对模型如何以及为何在某项任务中工作或不工作的深刻理解。这也使得决策树特别适用于那些需要因法律原因或为了向他人分享结果以指导未来业务实践而使分类机制透明的应用场景。考虑到这一点，一些潜在的应用包括：
- en: Credit scoring models in which the criteria that causes an applicant to be rejected
    need to be clearly documented and free from bias
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信贷评分模型，其中导致申请人被拒绝的标准需要明确记录，并且必须排除偏见。
- en: Marketing studies of customer behavior such as satisfaction or churn, which
    will be shared with management or advertising agencies
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于客户行为的营销研究，如满意度或流失率，这些研究将与管理层或广告公司共享。
- en: Diagnosis of medical conditions based on laboratory measurements, symptoms,
    or the rate of disease progression
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于实验室测量、症状或疾病进展速度的医疗状况诊断。
- en: Although the previous applications illustrate the value of trees in informing
    decision processes, this is not to suggest that their utility ends here. In fact,
    decision trees are perhaps the single most widely used machine learning technique,
    and can be applied to model almost any type of data—often with excellent out-of-the-box
    applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前述应用展示了树在决策过程中提供价值，但这并不意味着它们的效用就此结束。事实上，决策树可能是最广泛使用的机器学习技术之一，可以应用于几乎任何类型的数据——通常可以获得出色的开箱即用的应用效果。
- en: This said, in spite of their wide applicability, it is worth noting some scenarios
    where trees may not be an ideal fit. One such case might be a task where the data
    has a large number of nominal features with many levels or it has a large number
    of numeric features. These cases may result in a very large number of decisions
    and an overly complex tree. They may also contribute to the tendency of decision
    trees to overfit data, though as we will soon see, even this weakness can be overcome
    by adjusting some simple parameters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树有广泛的应用，但值得注意的是，在某些情况下，树可能不是理想选择。一个这样的例子是当数据包含大量具有多个级别的名义特征，或者数据包含大量数值特征时。这些情况可能导致决策数量非常庞大，并且树结构过于复杂。它们还可能导致决策树出现过拟合数据的倾向，尽管正如我们很快会看到的，甚至这种弱点也可以通过调整一些简单的参数来克服。
- en: Divide and conquer
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分治法
- en: Decision trees are built using a heuristic called **recursive partitioning**.
    This approach is also commonly known as **divide and conquer** because it splits
    the data into subsets, which are then split repeatedly into even smaller subsets,
    and so on and so forth until the process stops when the algorithm determines the
    data within the subsets are sufficiently homogenous, or another stopping criterion
    has been met.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是使用一种名为**递归划分**的启发式方法构建的。这种方法通常也被称为**分治法**，因为它将数据划分成子集，然后将这些子集反复划分成更小的子集，依此类推，直到算法判断子集内的数据足够同质化，或满足其他停止准则为止。
- en: To see how splitting a dataset can create a decision tree, imagine a bare root
    node that will grow into a mature tree. At first, the root node represents the
    entire dataset, since no splitting has transpired. Next, the decision tree algorithm
    must choose a feature to split upon; ideally, it chooses the feature most predictive
    of the target class. The examples are then partitioned into groups according to
    the distinct values of this feature, and the first set of tree branches are formed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何将数据集拆分以创建决策树，可以想象一个裸根节点，它将成长为一棵成熟的树。最初，根节点代表整个数据集，因为此时尚未进行拆分。接下来，决策树算法必须选择一个特征来进行拆分；理想情况下，它选择的是最能预测目标类别的特征。然后，根据该特征的不同值，示例数据被分成多个组，树的第一个分支就此形成。
- en: 'Working down each branch, the algorithm continues to divide and conquer the
    data, choosing the best candidate feature each time to create another decision
    node, until a stopping criterion is reached. Divide and conquer might stop at
    a node in a case that:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着每个分支往下，算法继续分治数据，每次选择最合适的特征来创建另一个决策节点，直到达到停止标准。分治法可能会在某个节点停止，情形如下：
- en: All (or nearly all) of the examples at the node have the same class
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点处的所有（或几乎所有）示例都属于同一类
- en: There are no remaining features to distinguish among the examples
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有剩余的特征可以用来区分各个示例
- en: The tree has grown to a predefined size limit
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该树已经增长到预设的大小限制
- en: 'To illustrate the tree building process, let''s consider a simple example.
    Imagine that you work for a Hollywood studio, where your role is to decide whether
    the studio should move forward with producing the screenplays pitched by promising
    new authors. After returning from a vacation, your desk is piled high with proposals.
    Without the time to read each proposal cover-to-cover, you decide to develop a
    decision tree algorithm to predict whether a potential movie would fall into one
    of three categories: **Critical Success**, **Mainstream Hit**, or **Box Office
    Bust**.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明树形结构的构建过程，让我们考虑一个简单的例子。假设你在一家好莱坞电影公司工作，负责决定公司是否应该继续制作那些有潜力的新人作家所提交的电影剧本。度假回来后，你的办公桌上堆满了提案。由于没有时间逐一阅读每份提案，你决定开发一个决策树算法，用来预测一部潜在电影是否会落入以下三类之一：**关键成功**、**主流热片**或**票房失败**。
- en: 'To build the decision tree, you turn to the studio archives to examine the
    factors leading to the success and failure of the company''s 30 most recent releases.
    You quickly notice a relationship between the film''s estimated shooting budget,
    the number of A-list celebrities lined up for starring roles, and the level of
    success. Excited about this finding, you produce a scatterplot to illustrate the
    pattern:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建决策树，你查阅了公司档案，分析了影响公司最近发布的30部电影成功与失败的因素。你很快注意到，电影的预估拍摄预算、一线明星的出演数量和电影的成功程度之间存在某种关系。对这一发现感到兴奋，你制作了一个散点图来展示这种模式：
- en: '![Divide and conquer](img/B03905_05_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![分治法](img/B03905_05_02.jpg)'
- en: 'Using the divide and conquer strategy, we can build a simple decision tree
    from this data. First, to create the tree''s root node, we split the feature indicating
    the number of celebrities, partitioning the movies into groups with and without
    a significant number of A-list stars:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分治策略，我们可以从这些数据中构建一个简单的决策树。首先，为了创建树的根节点，我们根据明星数量这一特征进行划分，将电影分为有无大量一线明星的两组：
- en: '![Divide and conquer](img/B03905_05_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![分治法](img/B03905_05_03.jpg)'
- en: 'Next, among the group of movies with a larger number of celebrities, we can
    make another split between movies with and without a high budget:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在具有较多明星的电影组中，我们可以再次进行划分，区分有无高预算的电影：
- en: '![Divide and conquer](img/B03905_05_04.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![分治法](img/B03905_05_04.jpg)'
- en: At this point, we have partitioned the data into three groups. The group at
    the top-left corner of the diagram is composed entirely of critically acclaimed
    films. This group is distinguished by a high number of celebrities and a relatively
    low budget. At the top-right corner, majority of movies are box office hits with
    high budgets and a large number of celebrities. The final group, which has little
    star power but budgets ranging from small to large, contains the flops.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个阶段，我们已经将数据划分为三组。图表左上角的组完全由获得好评的电影组成。这个组的特点是有大量的明星出演，且预算相对较低。在右上角，绝大多数电影都是票房热片，具有高预算和大量的明星阵容。最后一个组虽然没有太多明星，但预算从小到大不等，包含了票房失败的电影。
- en: If we wanted, we could continue to divide and conquer the data by splitting
    it based on the increasingly specific ranges of budget and celebrity count, until
    each of the currently misclassified values resides in its own tiny partition,
    and is correctly classified. However, it is not advisable to overfit a decision
    tree in this way. Though there is nothing to stop us from splitting the data indefinitely,
    overly specific decisions do not always generalize more broadly. We'll avoid the
    problem of overfitting by stopping the algorithm here, since more than 80 percent
    of the examples in each group are from a single class. This forms the basis of
    our stopping criterion.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，我们可以继续通过基于越来越具体的预算和名人数量范围来划分数据，直到每个当前错误分类的值都位于自己的小分区中，并且被正确分类。然而，不建议以这种方式过度拟合决策树。虽然没有什么可以阻止我们无限制地划分数据，但过于具体的决策并不总是能够更广泛地泛化。我们将通过在此停止算法来避免过拟合的问题，因为每个组中超过80%的示例都来自同一类。这构成了我们停止标准的基础。
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You might have noticed that diagonal lines might have split the data even more
    cleanly. This is one limitation of the decision tree's knowledge representation,
    which uses **axis-parallel splits**. The fact that each split considers one feature
    at a time prevents the decision tree from forming more complex decision boundaries.
    For example, a diagonal line could be created by a decision that asks, "is the
    number of celebrities is greater than the estimated budget?" If so, then "it will
    be a critical success."
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，斜线可能会更加干净地划分数据。这是决策树知识表示的一个局限性，它使用的是**轴对齐划分**。每次划分只考虑一个特征，这使得决策树无法形成更复杂的决策边界。例如，可以通过一个决策来创建一条斜线，询问：“名人数量是否大于预估预算？”如果是，那么“它将是一个关键性的成功”。
- en: Our model for predicting the future success of movies can be represented in
    a simple tree, as shown in the following diagram. To evaluate a script, follow
    the branches through each decision until the script's success or failure has been
    predicted. In no time, you will be able to identify the most promising options
    among the backlog of scripts and get back to more important work, such as writing
    an Academy Awards acceptance speech.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于预测电影未来成功的模型可以用一个简单的树表示，如下图所示。为了评估剧本，按照每个决策的分支，直到预测出剧本的成功或失败。很快，你将能够从一堆积压的剧本中识别出最有前途的选项，然后回到更重要的工作，如写奥斯卡颁奖典礼的获奖感言。
- en: '![Divide and conquer](img/B03905_05_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![分而治之](img/B03905_05_05.jpg)'
- en: Since real-world data contains more than two features, decision trees quickly
    become far more complex than this, with many more nodes, branches, and leaves.
    In the next section, you will learn about a popular algorithm to build decision
    tree models automatically.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界的数据包含超过两个特征，决策树很快就会变得比这复杂得多，包含更多的节点、分支和叶子。在接下来的部分中，你将学习一种流行的算法，它可以自动构建决策树模型。
- en: The C5.0 decision tree algorithm
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C5.0决策树算法
- en: There are numerous implementations of decision trees, but one of the most well-known
    implementations is the **C5.0 algorithm**. This algorithm was developed by computer
    scientist J. Ross Quinlan as an improved version of his prior algorithm, **C4.5**,
    which itself is an improvement over his **Iterative Dichotomiser 3** (**ID3**)
    algorithm. Although Quinlan markets C5.0 to commercial clients (see [http://www.rulequest.com/](http://www.rulequest.com/)
    for details), the source code for a single-threaded version of the algorithm was
    made publically available, and it has therefore been incorporated into programs
    such as R.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有很多实现方式，但最著名的一种实现是**C5.0算法**。该算法由计算机科学家J. Ross Quinlan开发，是他之前算法**C4.5**的改进版，而**C4.5**本身则是他**迭代二分法
    3**（**ID3**）算法的改进。尽管Quinlan将C5.0推向商业客户（详情见[http://www.rulequest.com/](http://www.rulequest.com/)），但该算法的单线程版本的源代码已经公开，因此被像R这样的程序所采用。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To further confuse matters, a popular Java-based open source alternative to
    C4.5, titled **J48**, is included in R's `RWeka` package. Because the differences
    among C5.0, C4.5, and J48 are minor, the principles in this chapter will apply
    to any of these three methods, and the algorithms should be considered synonymous.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人困惑的是，一个流行的基于Java的开源替代方案**J48**，它是C4.5的替代品，已包含在R的`RWeka`包中。由于C5.0、C4.5和J48之间的差异很小，本章中的原理将适用于这三种方法，且这些算法应被视为同义。
- en: 'The C5.0 algorithm has become the industry standard to produce decision trees,
    because it does well for most types of problems directly out of the box. Compared
    to other advanced machine learning models, such as those described in [Chapter
    7](ch07.html "Chapter 7. Black Box Methods – Neural Networks and Support Vector
    Machines"), *Black Box Methods – Neural Networks and Support Vector Machines*,
    the decision trees built by C5.0 generally perform nearly as well, but are much
    easier to understand and deploy. Additionally, as shown in the following table,
    the algorithm''s weaknesses are relatively minor and can be largely avoided:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0算法已成为生成决策树的行业标准，因为它对大多数类型的问题在开箱即用时表现出色。与其他先进的机器学习模型（如[第7章](ch07.html "Chapter
    7. Black Box Methods – Neural Networks and Support Vector Machines")中描述的*黑箱方法
    – 神经网络和支持向量机*）相比，C5.0生成的决策树通常表现几乎相同，但更易于理解和部署。此外，如下表所示，该算法的弱点相对较小，并且大多可以避免：
- en: '| Strengths | Weaknesses |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 弱点 |'
- en: '| --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: An all-purpose classifier that does well on most problems
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种多用途的分类器，能够在大多数问题上表现良好
- en: Highly automatic learning process, which can handle numeric or nominal features,
    as well as missing data
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度自动化的学习过程，能够处理数值型或名义型特征，以及缺失数据
- en: Excludes unimportant features
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除不重要的特征
- en: Can be used on both small and large datasets
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于小型和大型数据集
- en: Results in a model that can be interpreted without a mathematical background
    (for relatively small trees)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的模型可以在没有数学背景的情况下进行解释（对于相对较小的树）
- en: More efficient than other complex models
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比其他复杂模型更高效
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Decision tree models are often biased toward splits on features having a large
    number of levels
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树模型通常偏向于对具有大量层级的特征进行划分
- en: It is easy to overfit or underfit the model
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易出现过拟合或欠拟合问题
- en: Can have trouble modeling some relationships due to reliance on axis-parallel
    splits
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于依赖于轴对齐的划分，可能在建模某些关系时遇到困难
- en: Small changes in the training data can result in large changes to decision logic
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的微小变化可能会导致决策逻辑的巨大变化
- en: Large trees can be difficult to interpret and the decisions they make may seem
    counterintuitive
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型树可能难以解释，且它们做出的决策可能显得不合直觉
- en: '|'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: To keep things simple, our earlier decision tree example ignored the mathematics
    involved in how a machine would employ a divide and conquer strategy. Let's explore
    this in more detail to examine how this heuristic works in practice.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们之前的决策树示例忽略了机器如何运用分治策略的数学原理。让我们更详细地探讨这个问题，研究这种启发式方法在实践中的工作原理。
- en: Choosing the best split
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择最佳的划分
- en: The first challenge that a decision tree will face is to identify which feature
    to split upon. In the previous example, we looked for a way to split the data
    such that the resulting partitions contained examples primarily of a single class.
    The degree to which a subset of examples contains only a single class is known
    as **purity**, and any subset composed of only a single class is called **pure**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树面临的第一个挑战是识别应该在哪个特征上进行划分。在之前的示例中，我们寻找了一种划分数据的方法，使得划分后的数据主要包含单一类别的示例。一个示例子集仅包含单一类别的程度被称为**纯度**，任何仅由单一类别组成的子集都被称为**纯**。
- en: There are various measurements of purity that can be used to identify the best
    decision tree splitting candidate. C5.0 uses **entropy**, a concept borrowed from
    information theory that quantifies the randomness, or disorder, within a set of
    class values. Sets with high entropy are very diverse and provide little information
    about other items that may also belong in the set, as there is no apparent commonality.
    The decision tree hopes to find splits that reduce entropy, ultimately increasing
    homogeneity within the groups.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种纯度度量方法可以用来识别最佳的决策树划分候选。C5.0使用**熵**，这是一个借自信息论的概念，用于量化一个类值集合中的随机性或无序性。熵高的集合非常多样化，几乎不能提供关于其他可能属于该集合的项目的信息，因为没有明显的共同点。决策树希望找到减少熵的划分，从而最终增加组内的同质性。
- en: Typically, entropy is measured in **bits**. If there are only two possible classes,
    entropy values can range from 0 to 1\. For *n* classes, entropy ranges from 0
    to *log[2](n)*. In each case, the minimum value indicates that the sample is completely
    homogenous, while the maximum value indicates that the data are as diverse as
    possible, and no group has even a small plurality.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，熵以**比特**为单位进行度量。如果只有两个可能的类别，熵值的范围为0到1。对于*n*个类别，熵的范围是从0到*log[2](n)*。在每种情况下，最小值表示样本完全同质，而最大值表示数据尽可能多样化，且没有任何一个群体占据主导地位。
- en: 'In the mathematical notion, entropy is specified as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学符号中，熵被定义如下：
- en: '![Choosing the best split](img/B03905_05_06.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![选择最佳划分](img/B03905_05_06.jpg)'
- en: 'In this formula, for a given segment of data *(S)*, the term *c* refers to
    the number of class levels and *p[i]* refers to the proportion of values falling
    into class level *i*. For example, suppose we have a partition of data with two
    classes: red (60 percent) and white (40 percent). We can calculate the entropy
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，对于给定的数据片段*(S)*，术语*c*表示类别的数量，而*p[i]*表示属于类别级别*i*的值的比例。例如，假设我们有一个数据分区，其中包含两个类别：红色（60%）和白色（40%）。我们可以按如下方式计算熵：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can examine the entropy for all the possible two-class arrangements. If
    we know that the proportion of examples in one class is *x*, then the proportion
    in the other class is *(1 – x)*. Using the `curve()` function, we can then plot
    the entropy for all the possible values of *x*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查所有可能的二类排列的熵。如果我们知道一个类别中示例的比例为*x*，那么另一个类别的比例就是*(1 – x)*。通过使用`curve()`函数，我们可以绘制出所有可能的*x*值下的熵：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This results in the following figure:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生如下图所示的结果：
- en: '![Choosing the best split](img/B03905_05_07.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![选择最佳划分](img/B03905_05_07.jpg)'
- en: As illustrated by the peak in entropy at *x = 0.50*, a 50-50 split results in
    maximum entropy. As one class increasingly dominates the other, the entropy reduces
    to zero.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*x = 0.50*时熵的峰值所示，50-50的划分会导致最大熵。当一个类别逐渐主导另一个类别时，熵会减少到零。
- en: 'To use entropy to determine the optimal feature to split upon, the algorithm
    calculates the change in homogeneity that would result from a split on each possible
    feature, which is a measure known as **information gain**. The information gain
    for a feature *F* is calculated as the difference between the entropy in the segment
    before the split *(S[1])* and the partitions resulting from the split *(S[2])*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用熵来确定最佳划分特征，算法会计算在每个可能的特征上进行划分后同质性变化的量，这个度量被称为**信息增益**。特征*F*的信息增益是通过计算划分前的片段熵*(S[1])*与划分后分区的熵*(S[2])*之间的差值来得到的：
- en: '![Choosing the best split](img/B03905_05_08.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![选择最佳划分](img/B03905_05_08.jpg)'
- en: 'One complication is that after a split, the data is divided into more than
    one partition. Therefore, the function to calculate *Entropy(S[2])* needs to consider
    the total entropy across all of the partitions. It does this by weighing each
    partition''s entropy by the proportion of records falling into the partition.
    This can be stated in a formula as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复杂的地方是，划分后数据被分成了多个分区。因此，计算*Entropy(S[2])*的函数需要考虑所有分区的总熵。它通过根据每个分区中记录所占比例来加权每个分区的熵。可以用以下公式来表示：
- en: '![Choosing the best split](img/B03905_05_09.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![选择最佳划分](img/B03905_05_09.jpg)'
- en: In simple terms, the total entropy resulting from a split is the sum of the
    entropy of each of the *n* partitions weighted by the proportion of examples falling
    in the partition (*w[i]*).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，划分后的总熵是每个*n*个分区的熵之和，加权每个分区的示例比例（*w[i]*）。
- en: The higher the information gain, the better a feature is at creating homogeneous
    groups after a split on this feature. If the information gain is zero, there is
    no reduction in entropy for splitting on this feature. On the other hand, the
    maximum information gain is equal to the entropy prior to the split. This would
    imply that the entropy after the split is zero, which means that the split results
    in completely homogeneous groups.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益越高，特征在此特征上进行划分后，生成同质群体的效果越好。如果信息增益为零，则说明在该特征上进行划分不会减少熵。另一方面，最大信息增益等于划分前的熵。这意味着划分后的熵为零，表示该划分结果形成了完全同质的群体。
- en: The previous formulae assume nominal features, but decision trees use information
    gain for splitting on numeric features as well. To do so, a common practice is
    to test various splits that divide the values into groups greater than or less
    than a numeric threshold. This reduces the numeric feature into a two-level categorical
    feature that allows information gain to be calculated as usual. The numeric cut
    point yielding the largest information gain is chosen for the split.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的公式假设了名义特征，但决策树也使用信息增益对数值特征进行分裂。为此，一个常见的做法是测试不同的分裂方法，将值划分为大于或小于某个数值阈值的组。这将数值特征转换为一个二级类别特征，从而可以像往常一样计算信息增益。选择具有最大信息增益的数值切分点进行分裂。
- en: Note
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Though it is used by C5.0, information gain is not the only splitting criterion
    that can be used to build decision trees. Other commonly used criteria are **Gini
    index**, **Chi-Squared statistic**, and **gain ratio**. For a review of these
    (and many more) criteria, refer to Mingers J. *An Empirical Comparison of Selection
    Measures for Decision-Tree Induction*. Machine Learning. 1989; 3:319-342.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管C5.0使用了信息增益，但信息增益并不是构建决策树时可以使用的唯一分裂准则。其他常用的准则包括**基尼指数**、**卡方统计量**和**增益比**。有关这些（以及更多）准则的回顾，请参考
    Mingers J. *决策树归纳的选择度量的实证比较*。机器学习。1989; 3:319-342。
- en: Pruning the decision tree
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剪枝决策树
- en: A decision tree can continue to grow indefinitely, choosing splitting features
    and dividing the data into smaller and smaller partitions until each example is
    perfectly classified or the algorithm runs out of features to split on. However,
    if the tree grows overly large, many of the decisions it makes will be overly
    specific and the model will be overfitted to the training data. The process of
    **pruning** a decision tree involves reducing its size such that it generalizes
    better to unseen data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以无限地生长，选择分裂特征并将数据分割成越来越小的部分，直到每个示例都被完美分类，或者算法无法再找到特征进行分裂。然而，如果树过度生长，许多决策将变得过于具体，模型将过拟合训练数据。**剪枝**决策树的过程涉及缩小树的大小，以便它能够更好地泛化到未见过的数据。
- en: One solution to this problem is to stop the tree from growing once it reaches
    a certain number of decisions or when the decision nodes contain only a small
    number of examples. This is called **early stopping** or **pre-pruning** the decision
    tree. As the tree avoids doing needless work, this is an appealing strategy. However,
    one downside to this approach is that there is no way to know whether the tree
    will miss subtle, but important patterns that it would have learned had it grown
    to a larger size.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的一个方法是，当树达到一定的决策数量或决策节点仅包含少量示例时，停止树的生长。这被称为**早期停止**或**预剪枝**决策树。由于树避免了不必要的工作，这是一个有吸引力的策略。然而，这种方法的一个缺点是，无法知道树是否会错过那些微妙但重要的模式，这些模式如果树生长到更大规模时可能会学习到。
- en: An alternative, called **post-pruning**, involves growing a tree that is intentionally
    too large and pruning leaf nodes to reduce the size of the tree to a more appropriate
    level. This is often a more effective approach than pre-pruning, because it is
    quite difficult to determine the optimal depth of a decision tree without growing
    it first. Pruning the tree later on allows the algorithm to be certain that all
    the important data structures were discovered.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法，称为**后剪枝**，包括先生长一棵故意过大的树，并通过剪枝叶子节点将树的大小减少到一个更合适的水平。这通常比预剪枝更有效，因为在没有先生长树的情况下很难确定决策树的最优深度。稍后对树进行剪枝可以确保算法发现了所有重要的数据结构。
- en: Note
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The implementation details of pruning operations are very technical and beyond
    the scope of this book. For a comparison of some of the available methods, see
    Esposito F, Malerba D, Semeraro G. *A Comparative Analysis of Methods for Pruning
    Decision Trees*. IEEE Transactions on Pattern Analysis and Machine Intelligence.
    1997;19: 476-491.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝操作的实现细节非常技术性，超出了本书的范围。如需了解一些可用方法的比较，请参阅 Esposito F, Malerba D, Semeraro G.
    *决策树剪枝方法的比较分析*。IEEE模式分析与机器智能学报。1997;19: 476-491。'
- en: One of the benefits of the C5.0 algorithm is that it is opinionated about pruning—it
    takes care of many decisions automatically using fairly reasonable defaults. Its
    overall strategy is to post-prune the tree. It first grows a large tree that overfits
    the training data. Later, the nodes and branches that have little effect on the
    classification errors are removed. In some cases, entire branches are moved further
    up the tree or replaced by simpler decisions. These processes of grafting branches
    are known as **subtree raising** and **subtree replacement**, respectively.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0 算法的一个优点是它在修剪过程中有明确的方向——它会使用相当合理的默认设置自动做出许多决策。其整体策略是后期修剪树形结构。它首先生成一个过拟合训练数据的大树，然后删除那些对分类错误影响较小的节点和分支。在某些情况下，整个分支会被移动到树的更高位置，或被更简单的决策所替代。这些移植分支的过程分别被称为**子树提升**和**子树替换**。
- en: Balancing overfitting and underfitting a decision tree is a bit of an art, but
    if model accuracy is vital, it may be worth investing some time with various pruning
    options to see if it improves the performance on test data. As you will soon see,
    one of the strengths of the C5.0 algorithm is that it is very easy to adjust the
    training options.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡决策树的过拟合与欠拟合是一项艺术，但如果模型准确性至关重要，花时间调整不同的修剪选项，看看是否能提高测试数据的表现，是值得投入的。正如你将很快看到的，C5.0
    算法的一个优点是它非常容易调整训练选项。
- en: Example – identifying risky bank loans using C5.0 decision trees
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用 C5.0 决策树识别高风险银行贷款
- en: The global financial crisis of 2007-2008 highlighted the importance of transparency
    and rigor in banking practices. As the availability of credit was limited, banks
    tightened their lending systems and turned to machine learning to more accurately
    identify risky loans.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2007-2008年的全球金融危机突显了银行业务中透明度和严格性的重要性。由于信贷供应受到限制，银行收紧了贷款系统，并转向机器学习，以更准确地识别高风险贷款。
- en: Decision trees are widely used in the banking industry due to their high accuracy
    and ability to formulate a statistical model in plain language. Since government
    organizations in many countries carefully monitor lending practices, executives
    must be able to explain why one applicant was rejected for a loan while the others
    were approved. This information is also useful for customers hoping to determine
    why their credit rating is unsatisfactory.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树具有高准确性和用通俗语言制定统计模型的能力，因此在银行业得到了广泛应用。由于许多国家的政府组织严格监控贷款实践，银行高层必须能够解释为什么一个申请人被拒绝贷款，而其他申请人却被批准。这些信息对于希望了解为什么自己的信用评级不合格的客户也非常有用。
- en: It is likely that automated credit scoring models are employed to instantly
    approve credit applications on the telephone and web. In this section, we will
    develop a simple credit approval model using C5.0 decision trees. We will also
    see how the results of the model can be tuned to minimize errors that result in
    a financial loss for the institution.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化信用评分模型可能被用来在电话和网络上即时批准信用申请。在本节中，我们将使用 C5.0 决策树开发一个简单的信用批准模型。我们还将看到如何调整模型结果，以最小化可能导致机构经济损失的错误。
- en: Step 1 – collecting data
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 收集数据
- en: The idea behind our credit model is to identify factors that are predictive
    of higher risk of default. Therefore, we need to obtain data on a large number
    of past bank loans and whether the loan went into default, as well as information
    on the applicant.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的信用模型背后的理念是识别那些能够预测较高违约风险的因素。因此，我们需要获取大量过去银行贷款的数据，了解这些贷款是否发生了违约，以及有关申请人的信息。
- en: Data with these characteristics is available in a dataset donated to the UCI
    Machine Learning Data Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))
    by Hans Hofmann of the University of Hamburg. The dataset contains information
    on loans obtained from a credit agency in Germany.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 具有这些特征的数据可以在由汉斯·霍夫曼（Hans Hofmann）捐赠给 UCI 机器学习数据仓库的一个数据集中找到 ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))。该数据集包含来自德国一家信用机构的贷款信息。
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The dataset presented in this chapter has been modified slightly from the original
    in order to eliminate some preprocessing steps. To follow along with the examples,
    download the `credit.csv` file from Packt Publishing's website and save it to
    your R working directory.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的数据集与原始数据集略有修改，目的是消除一些预处理步骤。为了跟随示例操作，请从 Packt Publishing 的网站下载 `credit.csv`
    文件并将其保存到你的 R 工作目录中。
- en: The credit dataset includes 1,000 examples on loans, plus a set of numeric and
    nominal features indicating the characteristics of the loan and the loan applicant.
    A class variable indicates whether the loan went into default. Let's see whether
    we can determine any patterns that predict this outcome.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 信用数据集包括1,000个贷款实例，另外还有一组数值型和名义型特征，表示贷款和贷款申请者的特点。一个类别变量表示贷款是否违约。让我们看看是否能发现一些预测这一结果的模式。
- en: Step 2 – exploring and preparing the data
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2 – 探索和准备数据
- en: 'As we did previously, we will import data using the `read.csv()` function.
    We will ignore the `stringsAsFactors` option and, therefore, use the default value
    of `TRUE`, as the majority of the features in the data are nominal:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，我们将使用`read.csv()`函数导入数据。我们将忽略`stringsAsFactors`选项，因此使用默认值`TRUE`，因为数据中的大多数特征都是名义型的：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first several lines of output from the `str()` function are as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`str()`函数的前几行输出如下：'
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We see the expected 1,000 observations and 17 features, which are a combination
    of factor and integer data types.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到期望的1,000条观察数据和17个特征，这些特征是因子和整数数据类型的组合。
- en: 'Let''s take a look at the `table()` output for a couple of loan features that
    seem likely to predict a default. The applicant''s checking and savings account
    balance are recorded as categorical variables:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`table()`函数输出的几个可能预测违约的贷款特征。申请者的支票和储蓄账户余额被记录为分类变量：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The checking and savings account balance may prove to be important predictors
    of loan default status. Note that since the loan data was obtained from Germany,
    the currency is recorded in Deutsche Marks (DM).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 支票和储蓄账户余额可能是预测贷款违约状态的重要指标。请注意，由于贷款数据来自德国，因此货币记录为德国马克（DM）。
- en: 'Some of the loan''s features are numeric, such as its duration and the amount
    of credit requested:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 贷款的某些特征是数值型的，例如贷款的期限和请求的信用金额：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The loan amounts ranged from 250 DM to 18,420 DM across terms of 4 to 72 months
    with a median duration of 18 months and an amount of 2,320 DM.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 贷款金额从250 DM到18,420 DM不等，期限从4个月到72个月不等，贷款的中位数期限为18个月，金额为2,320 DM。
- en: 'The `default` vector indicates whether the loan applicant was unable to meet
    the agreed payment terms and went into default. A total of 30 percent of the loans
    in this dataset went into default:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`default`向量表示贷款申请者是否未能按约定的付款条款履约并进入违约。该数据集中有30%的贷款进入了违约状态：'
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A high rate of default is undesirable for a bank, because it means that the
    bank is unlikely to fully recover its investment. If we are successful, our model
    will identify applicants that are at high risk to default, allowing the bank to
    refuse credit requests.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 高违约率对银行来说是不利的，因为这意味着银行不太可能完全收回其投资。如果我们成功了，我们的模型将能够识别出高违约风险的申请者，从而允许银行拒绝这些申请的信用请求。
- en: Data preparation – creating random training and test datasets
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 创建随机的训练和测试数据集
- en: 'As we have done in the previous chapters, we will split our data into two portions:
    a training dataset to build the decision tree and a test dataset to evaluate the
    performance of the model on new data. We will use 90 percent of the data for training
    and 10 percent for testing, which will provide us with 100 records to simulate
    new applicants.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中所做的那样，我们将把数据分成两部分：一个用于构建决策树的训练数据集和一个用于评估模型在新数据上表现的测试数据集。我们将使用90%的数据用于训练，10%的数据用于测试，这将为我们提供100条记录来模拟新申请者。
- en: As prior chapters used data that had been sorted in a random order, we simply
    divided the dataset into two portions, by taking the first 90 percent of records
    for training, and the remaining 10 percent for testing. In contrast, the credit
    dataset is not randomly ordered, making the prior approach unwise. Suppose that
    the bank had sorted the data by the loan amount, with the largest loans at the
    end of the file. If we used the first 90 percent for training and the remaining
    10 percent for testing, we would be training a model on only the small loans and
    testing the model on the big loans. Obviously, this could be problematic.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章使用的数据是按随机顺序排序的不同，我们简单地将数据集分为两部分，取前90%的记录用于训练，剩余的10%用于测试。相比之下，信用数据集没有经过随机排序，因此采用之前的方法是不明智的。假设银行按贷款金额排序数据，最大贷款位于文件末尾。如果我们使用前90%的数据用于训练，剩余10%用于测试，我们将仅在小额贷款上训练模型，而在大额贷款上测试模型。显然，这可能会造成问题。
- en: We'll solve this problem by using a **random sample** of the credit data for
    training. A random sample is simply a process that selects a subset of records
    at random. In R, the `sample()` function is used to perform random sampling. However,
    before putting it in action, a common practice is to set a **seed** value, which
    causes the randomization process to follow a sequence that can be replicated later
    on if desired. It may seem that this defeats the purpose of generating random
    numbers, but there is a good reason for doing it this way. Providing a seed value
    via the `set.seed()` function ensures that if the analysis is repeated in the
    future, an identical result is obtained.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用信用数据的**随机样本**来解决这个问题。随机样本仅仅是一个随机选择记录子集的过程。在R中，`sample()`函数用于执行随机抽样。然而，在执行之前，一个常见的做法是设置**种子**值，这样可以确保随机化过程遵循一个可以在以后复制的序列。看起来这似乎违背了生成随机数的目的，但这样做是有原因的。通过`set.seed()`函数提供种子值可以确保如果将来重复分析，可以得到相同的结果。
- en: Tip
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You may wonder how a so-called random process can be seeded to produce an identical
    result. This is due to the fact that computers use a mathematical function called
    a **pseudorandom number generator** to create random number sequences that appear
    to act very random, but are actually quite predictable given knowledge of the
    previous values in the sequence. In practice, modern pseudorandom number sequences
    are virtually indistinguishable from true random sequences, but have the benefit
    that computers can generate them quickly and easily.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，怎么一个所谓的随机过程可以设置种子来产生相同的结果呢？这是因为计算机使用一种名为**伪随机数生成器**的数学函数来创建看似非常随机的随机数序列，但实际上只要知道序列中前一个值，它们是可以预测的。实际上，现代伪随机数序列与真正的随机序列几乎无法区分，但它们的优势在于计算机可以快速、轻松地生成这些序列。
- en: 'The following commands use the `sample()` function to select 900 values at
    random out of the sequence of integers from 1 to 1000\. Note that the `set.seed()`
    function uses the arbitrary value `123`. Omitting this seed will cause your training
    and testing split to differ from those shown in the remainder of this chapter:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令使用`sample()`函数从1到1000的整数序列中随机选择900个值。请注意，`set.seed()`函数使用了一个任意值`123`。如果省略这个种子值，你的训练和测试集划分将与本章其余部分所示的结果不同：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As expected, the resulting `train_sample` object is a vector of 900 random
    integers:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，生成的`train_sample`对象是一个包含900个随机整数的向量：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By using this vector to select rows from the credit data, we can split it into
    the 90 percent training and 10 percent test datasets we desired. Recall that the
    dash operator used in the selection of the test records tells R to select records
    that are not in the specified rows; in other words, the test data includes only
    the rows that are not in the training sample.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个向量从信用数据中选择行，我们可以将其分为我们所需的90%的训练数据集和10%的测试数据集。请记住，在选择测试记录时使用的破折号运算符告诉R选择那些不在指定行中的记录；换句话说，测试数据仅包含那些不在训练样本中的行。
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If all went well, we should have about 30 percent of defaulted loans in each
    of the datasets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，我们应该在每个数据集中都有大约30%的违约贷款：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This appears to be a fairly even split, so we can now build our decision tree.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一个相当均匀的划分，所以我们现在可以构建我们的决策树。
- en: Tip
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If your results do not match exactly, ensure that you ran the command `set.seed(123)`
    immediately prior to creating the `train_sample` vector.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的结果不完全匹配，请确保在创建`train_sample`向量之前立即运行了命令`set.seed(123)`。
- en: Step 3 – training a model on the data
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: We will use the C5.0 algorithm in the `C50` package to train our decision tree
    model. If you have not done so already, install the package with `install.packages("C50")`
    and load it to your R session, using `library(C50)`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`C50`包中的C5.0算法来训练我们的决策树模型。如果你还没有安装该包，可以通过`install.packages("C50")`来安装，并使用`library(C50)`将其加载到R会话中。
- en: The following syntax box lists some of the most commonly used commands to build
    decision trees. Compared to the machine learning approaches we used previously,
    the C5.0 algorithm offers many more ways to tailor the model to a particular learning
    problem, but more options are available. Once the `C50` package has been loaded,
    the `?C5.0Control` command displays the help page for more details on how to finely-tune
    the algorithm.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下语法框列出了构建决策树时最常用的一些命令。与我们之前使用的机器学习方法相比，C5.0算法提供了更多定制模型以适应特定学习问题的方式，但也提供了更多选项。一旦加载了`C50`包，`?C5.0Control`命令将显示帮助页面，以获得有关如何精细调整算法的更多细节。
- en: '![Step 3 – training a model on the data](img/B03905_05_10.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 3 – 在数据上训练模型](img/B03905_05_10.jpg)'
- en: 'For the first iteration of our credit approval model, we''ll use the default
    C5.0 configuration, as shown in the following code. The 17th column in `credit_train`
    is the `default` class variable, so we need to exclude it from the training data
    frame, but supply it as the target factor vector for classification:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们信用审批模型的第一次迭代，我们将使用默认的C5.0配置，如下所示的代码。`credit_train`的第17列是`default`类别变量，因此我们需要将其从训练数据框中排除，但作为分类的目标因子向量提供：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `credit_model` object now contains a C5.0 decision tree. We can see some
    basic data about the tree by typing its name:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`credit_model`对象现在包含一个C5.0决策树。我们可以通过输入它的名称来查看树的一些基本数据：'
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding text shows some simple facts about the tree, including the function
    call that generated it, the number of features (labeled `predictors`), and examples
    (labeled `samples`) used to grow the tree. Also listed is the tree size of 57,
    which indicates that the tree is 57 decisions deep—quite a bit larger than the
    example trees we've considered so far!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 上述文本显示了有关决策树的一些基本事实，包括生成该树的函数调用、特征数量（标记为`predictors`）和用于生成树的示例（标记为`samples`）。还列出了树的大小为57，表示该树有57个决策层级——比我们之前考虑的示例树要大得多！
- en: 'To see the tree''s decisions, we can call the `summary()` function on the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看树的决策，我们可以在模型上调用`summary()`函数：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Step 3 – training a model on the data](img/B03905_05_11.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 3 – 在数据上训练模型](img/B03905_05_11.jpg)'
- en: 'The preceding output shows some of the first branches in the decision tree.
    The first three lines could be represented in plain language as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了决策树中的一些初步分支。前三行可以用简单的语言表示为：
- en: If the checking account balance is unknown or greater than 200 DM, then classify
    as "not likely to default."
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果支票账户余额未知或大于200德国马克，则分类为“ unlikely to default”（不太可能违约）。
- en: Otherwise, if the checking account balance is less than zero DM or between one
    and 200 DM.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，如果支票账户余额小于零德国马克或在1至200德国马克之间。
- en: And the credit history is perfect or very good, then classify as "likely to
    default."
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果信用历史完美或非常好，则分类为“ likely to default”（可能违约）。
- en: The numbers in parentheses indicate the number of examples meeting the criteria
    for that decision, and the number incorrectly classified by the decision. For
    instance, on the first line, `412/50` indicates that of the 412 examples reaching
    the decision, 50 were incorrectly classified as not likely to default. In other
    words, 50 applicants actually defaulted, in spite of the model's prediction to
    the contrary.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的数字表示满足该决策标准的示例数量，以及被该决策错误分类的数量。例如，在第一行，`412/50`表示在达到该决策的412个示例中，50个被错误分类为不太可能违约。换句话说，尽管模型预测相反，实际上有50个申请人违约了。
- en: Tip
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Sometimes a tree results in decisions that make little logical sense. For example,
    why would an applicant whose credit history is very good be likely to default,
    while those whose checking balance is unknown are not likely to default? Contradictory
    rules like this occur sometimes. They might reflect a real pattern in the data,
    or they may be a statistical anomaly. In either case, it is important to investigate
    such strange decisions to see whether the tree's logic makes sense for business
    use.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，决策树会产生一些逻辑上没有意义的决策。例如，为什么信用历史非常好的申请人可能会违约，而支票账户余额未知的申请人不太可能违约？像这样的矛盾规则有时会出现。它们可能反映了数据中的真实模式，或者可能是统计异常。无论是哪种情况，调查这些奇怪的决策，看看树的逻辑是否适用于业务使用，都是很重要的。
- en: 'After the tree, the `summary(credit_model)` output displays a confusion matrix,
    which is a cross-tabulation that indicates the model''s incorrectly classified
    records in the training data:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树之后，`summary(credit_model)`的输出显示了一个混淆矩阵，这是一个交叉表，表示模型在训练数据中错误分类的记录：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The Errors output notes that the model correctly classified all but 133 of the
    900 training instances for an error rate of 14.8 percent. A total of 35 actual
    no values were incorrectly classified as yes (false positives), while 98 yes values
    were misclassified as no (false negatives).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 错误输出指出，模型正确分类了900个训练实例中除了133个实例之外的所有实例，错误率为14.8%。总共有35个实际的"no"被错误分类为"yes"（假阳性），而98个"yes"被错误分类为"no"（假阴性）。
- en: Decision trees are known for having a tendency to overfit the model to the training
    data. For this reason, the error rate reported on training data may be overly
    optimistic, and it is especially important to evaluate decision trees on a test
    dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树因其容易将模型过度拟合训练数据而闻名。因此，报告的训练数据错误率可能过于乐观，特别重要的是要在测试数据集上评估决策树。
- en: Step 4 – evaluating model performance
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四步 – 评估模型性能
- en: 'To apply our decision tree to the test dataset, we use the `predict()` function,
    as shown in the following line of code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的决策树应用于测试数据集，我们使用`predict()`函数，如下面的代码行所示：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This creates a vector of predicted class values, which we can compare to the
    actual class values using the `CrossTable()` function in the `gmodels` package.
    Setting the `prop.c` and `prop.r` parameters to `FALSE` removes the column and
    row percentages from the table. The remaining percentage (`prop.t`) indicates
    the proportion of records in the cell out of the total number of records:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个预测类别值的向量，我们可以使用`gmodels`包中的`CrossTable()`函数将其与实际类别值进行比较。将`prop.c`和`prop.r`参数设置为`FALSE`可以从表格中移除列和行的百分比。剩余的百分比（`prop.t`）表示单元格中记录占总记录数的比例：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following table:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下表格：
- en: '![Step 4 – evaluating model performance](img/B03905_05_12.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![第四步 – 评估模型性能](img/B03905_05_12.jpg)'
- en: Out of the 100 test loan application records, our model correctly predicted
    that 59 did not default and 14 did default, resulting in an accuracy of 73 percent
    and an error rate of 27 percent. This is somewhat worse than its performance on
    the training data, but not unexpected, given that a model's performance is often
    worse on unseen data. Also note that the model only correctly predicted 14 of
    the 33 actual loan defaults in the test data, or 42 percent. Unfortunately, this
    type of error is a potentially very costly mistake, as the bank loses money on
    each default. Let's see if we can improve the result with a bit more effort.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在100个测试贷款申请记录中，我们的模型正确预测了59个未违约和14个违约，准确率为73%，错误率为27%。这比其在训练数据上的表现稍差，但考虑到模型在未见数据上的表现通常较差，这并不意外。还要注意，模型只正确预测了测试数据中33个实际贷款违约中的14个，正确率为42%。不幸的是，这种类型的错误可能是一个非常昂贵的错误，因为银行在每次违约时都会损失资金。让我们看看是否可以通过更多的努力来改进结果。
- en: Step 5 – improving model performance
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五步 – 改进模型性能
- en: Our model's error rate is likely to be too high to deploy it in a real-time
    credit scoring application. In fact, if the model had predicted "no default" for
    every test case, it would have been correct 67 percent of the time—a result not
    much worse than our model's, but requiring much less effort! Predicting loan defaults
    from 900 examples seems to be a challenging problem.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的错误率可能太高，无法在实时信用评分应用中部署。事实上，如果模型对每个测试案例都预测为"无违约"，它的正确率将是67%，这与我们模型的结果差不多，但所需的努力要小得多！从900个样本中预测贷款违约似乎是一个具有挑战性的问题。
- en: Making matters even worse, our model performed especially poorly at identifying
    applicants who do default on their loans. Luckily, there are a couple of simple
    ways to adjust the C5.0 algorithm that may help to improve the performance of
    the model, both overall and for the more costly type of mistakes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，我们的模型在识别那些确实违约的申请人时表现尤其糟糕。幸运的是，有几种简单的方法可以调整C5.0算法，这可能有助于提高模型的整体性能，并改善那些更为昂贵的错误类型。
- en: Boosting the accuracy of decision trees
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提高决策树的准确率
- en: One way the C5.0 algorithm improved upon the C4.5 algorithm was through the
    addition of **adaptive boosting**. This is a process in which many decision trees
    are built and the trees vote on the best class for each example.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0算法通过增加**自适应提升**（adaptive boosting）改进了C4.5算法。这是一个过程，其中构建了多个决策树，并且这些树对每个实例的最佳类别进行投票。
- en: Note
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The idea of boosting is based largely upon the research by Rob Schapire and
    Yoav Freund. For more information, try searching the web for their publications
    or their recent textbook *Boosting: Foundations and Algorithms*. The MIT Press
    (2012).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '提升的理念主要基于Rob Schapire和Yoav Freund的研究。欲了解更多信息，请尝试在网上搜索他们的出版物或他们的近期教材《Boosting:
    Foundations and Algorithms》。MIT出版社（2012年）。'
- en: As boosting can be applied more generally to any machine learning algorithm,
    it is covered in detail later in this book in [Chapter 11](ch11.html "Chapter 11. Improving
    Model Performance"), *Improving Model Performance*. For now, it suffices to say
    that boosting is rooted in the notion that by combining a number of weak performing
    learners, you can create a team that is much stronger than any of the learners
    alone. Each of the models has a unique set of strengths and weaknesses and they
    may be better or worse in solving certain problems. Using a combination of several
    learners with complementary strengths and weaknesses can therefore dramatically
    improve the accuracy of a classifier.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提升可以更广泛地应用于任何机器学习算法，它将在本书后续章节中详细介绍，[第11章](ch11.html "第11章：提高模型性能")，*提高模型性能*。现在，我们只需说提升基于这样一个概念：通过将多个表现较弱的学习者结合起来，可以创建一个比任何单独的学习者都强大的团队。每个模型都有独特的优缺点，它们在解决某些问题时可能表现得更好或更差。因此，使用多个具有互补优缺点的学习者的组合，可以显著提高分类器的准确性。
- en: 'The `C5.0()` function makes it easy to add boosting to our C5.0 decision tree.
    We simply need to add an additional `trials` parameter indicating the number of
    separate decision trees to use in the boosted team. The `trials` parameter sets
    an upper limit; the algorithm will stop adding trees if it recognizes that additional
    trials do not seem to be improving the accuracy. We''ll start with 10 trials,
    a number that has become the de facto standard, as research suggests that this
    reduces error rates on test data by about 25 percent:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`C5.0()`函数使得在我们的C5.0决策树中添加提升变得非常简单。我们只需要添加一个额外的`trials`参数，指示要在提升团队中使用的单独决策树的数量。`trials`参数设置了上限；如果算法识别到额外的试验似乎并没有改善准确性，它将停止添加树。我们将从10次试验开始，这是一个事实上的标准，研究表明这样可以将测试数据的错误率降低大约25%：'
- en: '[PRE17]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'While examining the resulting model, we can see that some additional lines
    have been added, indicating the changes:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查结果模型时，我们可以看到一些额外的线条被添加进来，表明了变化：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Across the 10 iterations, our tree size shrunk. If you would like, you can
    see all 10 trees by typing `summary(credit_boost10)` at the command prompt. It
    also lists the model''s performance on the training data:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这10次迭代中，我们的树的大小缩小了。如果您愿意，可以通过在命令提示符下键入`summary(credit_boost10)`来查看所有10棵树。它还列出了模型在训练数据上的表现：
- en: '[PRE19]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The classifier made 34 mistakes on 900 training examples for an error rate
    of 3.8 percent. This is quite an improvement over the 13.9 percent training error
    rate we noted before adding boosting! However, it remains to be seen whether we
    see a similar improvement on the test data. Let''s take a look:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器在900个训练样本上犯了34个错误，错误率为3.8%。这相比我们在添加提升前注意到的13.9%的训练误差率有了相当大的改进！然而，是否能在测试数据上看到类似的改进仍然有待观察。让我们来看一下：
- en: '[PRE20]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The resulting table is as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表格如下：
- en: '![Boosting the accuracy of decision trees](img/B03905_05_13.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![提升决策树的准确性](img/B03905_05_13.jpg)'
- en: Here, we reduced the total error rate from 27 percent prior to boosting down
    to 18 percent in the boosted model. It does not seem like a large gain, but it
    is in fact larger than the 25 percent reduction we expected. On the other hand,
    the model is still not doing well at predicting defaults, predicting only *20/33
    = 61%* correctly. The lack of an even greater improvement may be a function of
    our relatively small training dataset, or it may just be a very difficult problem
    to solve.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将总错误率从提升前的27%降低到了提升后模型的18%。这看起来并不是一个很大的增益，但实际上它比我们预期的25%的降低要大。另一方面，模型在预测违约方面仍然表现不佳，只有*20/33
    = 61%*的预测是正确的。没有看到更大改进的原因可能是我们相对较小的训练数据集，或者这可能只是一个非常难以解决的问题。
- en: This said, if boosting can be added this easily, why not apply it by default
    to every decision tree? The reason is twofold. First, if building a decision tree
    once takes a great deal of computation time, building many trees may be computationally
    impractical. Secondly, if the training data is very noisy, then boosting might
    not result in an improvement at all. Still, if greater accuracy is needed, it's
    worth giving it a try.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果提升（boosting）可以如此轻松地添加，为什么不默认将其应用于每个决策树呢？原因有二。首先，如果建立一个决策树需要大量的计算时间，那么构建多个树可能在计算上是不可行的。其次，如果训练数据非常嘈杂，那么提升可能根本不会带来改进。不过，如果需要更高的准确度，尝试一下还是值得的。
- en: Making mistakes more costlier than others
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使错误的成本比其他错误更高
- en: Giving a loan out to an applicant who is likely to default can be an expensive
    mistake. One solution to reduce the number of false negatives may be to reject
    a larger number of borderline applicants, under the assumption that the interest
    the bank would earn from a risky loan is far outweighed by the massive loss it
    would incur if the money is not paid back at all.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 向可能违约的申请人发放贷款可能是一个昂贵的错误。减少错误负样本数量的一种解决方案可能是拒绝更多边缘申请人，假设银行从高风险贷款中获得的利息远远不能弥补若贷款完全无法偿还时所遭受的巨额损失。
- en: The C5.0 algorithm allows us to assign a penalty to different types of errors,
    in order to discourage a tree from making more costly mistakes. The penalties
    are designated in a **cost matrix**, which specifies how much costlier each error
    is, relative to any other prediction.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0算法允许我们为不同类型的错误分配惩罚，以避免决策树犯更多代价更高的错误。惩罚值被指定在**成本矩阵**中，矩阵定义了每个错误相对于其他预测的成本。
- en: 'To begin constructing the cost matrix, we need to start by specifying the dimensions.
    Since the predicted and actual values can both take two values, `yes` or `no`,
    we need to describe a 2 x 2 matrix, using a list of two vectors, each with two
    values. At the same time, we''ll also name the matrix dimensions to avoid confusion
    later on:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始构建成本矩阵，我们需要首先指定维度。由于预测值和实际值都可以取“是”或“否”两种值，我们需要描述一个2 x 2矩阵，使用两个向量的列表，每个向量包含两个值。同时，我们还将为矩阵的维度命名，以避免日后混淆：
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Examining the new object shows that our dimensions have been set up correctly:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 检查新对象时，表明我们的维度已正确设置：
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to assign the penalty for the various types of errors by supplying
    four values to fill the matrix. Since R fills a matrix by filling columns one
    by one from top to bottom, we need to supply the values in a specific order:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过提供四个值来为各种类型的错误分配惩罚，以填充矩阵。由于R通过从上到下依次填充列来填充矩阵，我们需要按特定顺序提供这些值：
- en: Predicted no, actual no
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测为否，实际为否
- en: Predicted yes, actual no
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测为是，实际为否
- en: Predicted no, actual yes
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测为否，实际为是
- en: Predicted yes, actual yes
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测为是，实际为是
- en: 'Suppose we believe that a loan default costs the bank four times as much as
    a missed opportunity. Our penalty values could then be defined as:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们认为贷款违约对银行的成本是错失机会的四倍。那么我们的惩罚值可以定义为：
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This creates the following matrix:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建以下矩阵：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As defined by this matrix, there is no cost assigned when the algorithm classifies
    a no or yes correctly, but a false negative has a cost of 4 versus a false positive''s
    cost of 1\. To see how this impacts classification, let''s apply it to our decision
    tree using the `costs` parameter of the `C5.0()` function. We''ll otherwise use
    the same steps as we did earlier:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个矩阵的定义，当算法正确地分类为“否”或“是”时没有任何成本，但错误负样本的成本为4，而错误正样本的成本为1。为了了解这如何影响分类，我们可以将其应用到决策树中，使用`C5.0()`函数的`costs`参数。其他步骤与我们之前做的一样：
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This produces the following confusion matrix:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下混淆矩阵：
- en: '![Making mistakes more costlier than others](img/B03905_05_14.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![将错误的成本设为其他错误的多倍](img/B03905_05_14.jpg)'
- en: 'Compared to our boosted model, this version makes more mistakes overall: 37
    percent error here versus 18 percent in the boosted case. However, the types of
    mistakes are very different. Where the previous models incorrectly classified
    only 42 and 61 percent of defaults correctly, in this model, 79 percent of the
    actual defaults were predicted to be non-defaults. This trade resulting in a reduction
    of false negatives at the expense of increasing false positives may be acceptable
    if our cost estimates were accurate.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们提升后的模型相比，这个版本的错误更多：这里的错误率为37%，而提升模型的错误率为18%。然而，错误的类型却大不相同。之前的模型仅有42%和61%的违约案例被正确分类，而在这个模型中，79%的实际违约被预测为非违约。这种以增加假阳性为代价减少假阴性的权衡，若我们的成本估算准确，可能是可以接受的。
- en: Understanding classification rules
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类规则
- en: Classification rules represent knowledge in the form of logical if-else statements
    that assign a class to unlabeled examples. They are specified in terms of an **antecedent**
    and a **consequent**; these form a hypothesis stating that "if this happens, then
    that happens." A simple rule might state, "if the hard drive is making a clicking
    sound, then it is about to fail." The antecedent comprises certain combinations
    of feature values, while the consequent specifies the class value to assign when
    the rule's conditions are met.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则以逻辑的 if-else 语句形式表示知识，赋予未标记示例一个类别。它们通过**前件**和**后件**来指定；这些形成一个假设，声明“如果发生这个，那么就会发生那个”。一个简单的规则可能是，“如果硬盘发出点击声，那么它即将发生故障。”前件包含某些特征值的组合，而后件则指定当规则条件满足时，应该分配的类别值。
- en: 'Rule learners are often used in a manner similar to decision tree learners.
    Like decision trees, they can be used for applications that generate knowledge
    for future action, such as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习者通常以类似于决策树学习者的方式使用。像决策树一样，它们可以用于生成未来行动的知识应用，如：
- en: Identifying conditions that lead to a hardware failure in mechanical devices
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定导致机械设备硬件故障的条件
- en: Describing the key characteristics of groups of people for customer segmentation
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述群体的关键特征用于客户细分
- en: Finding conditions that precede large drops or increases in the prices of shares
    on the stock market
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找股票市场上股价大幅下跌或上涨之前的条件
- en: On the other hand, rule learners offer some distinct advantages over trees for
    some tasks. Unlike a tree, which must be applied from top-to-bottom through a
    series of decisions, rules are propositions that can be read much like a statement
    of fact. Additionally, for reasons that will be discussed later, the results of
    a rule learner can be more simple, direct, and easier to understand than a decision
    tree built on the same data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，规则学习者在某些任务中相比树形结构提供了一些独特的优势。与必须通过一系列决策从上到下应用的树不同，规则是一种命题，可以像陈述事实一样被读取。此外，出于稍后会讨论的原因，规则学习者的结果可能比基于相同数据构建的决策树更简单、直接且更容易理解。
- en: Tip
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: As you will see later in this chapter, rules can be generated using decision
    trees. So, why bother with a separate group of rule learning algorithms? The reason
    is that decision trees bring a particular set of biases to the task that a rule
    learner avoids by identifying the rules directly.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在本章后面看到的，规则可以通过决策树生成。那么，为什么还要使用一个单独的规则学习算法呢？原因在于，决策树给任务带来了一些特定的偏差，而规则学习者通过直接识别规则来避免这些偏差。
- en: Rule learners are generally applied to problems where the features are primarily
    or entirely nominal. They do well at identifying rare events, even if the rare
    event occurs only for a very specific interaction among feature values.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习者通常应用于特征主要或完全为名义型的数据问题。即使罕见事件仅在特征值之间的某种特定交互作用下发生，它们也能很好地识别这些罕见事件。
- en: Separate and conquer
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分而治之
- en: Classification rule learning algorithms utilize a heuristic known as **separate
    and conquer**. The process involves identifying a rule that covers a subset of
    examples in the training data, and then separating this partition from the remaining
    data. As the rules are added, additional subsets of the data are separated until
    the entire dataset has been covered and no more examples remain.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则学习算法利用一种叫做**分而治之**的启发式方法。这个过程包括识别一个覆盖训练数据子集的规则，然后将这个子集与剩余数据分离。当规则被添加时，数据的其他子集也会被分离，直到整个数据集被覆盖，且没有更多的例子留下。
- en: 'One way to imagine the rule learning process is to think about drilling down
    into the data by creating increasingly specific rules to identify class values.
    Suppose you were tasked with creating rules to identify whether or not an animal
    is a mammal. You could depict the set of all animals as a large space, as shown
    in the following diagram:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 想象规则学习过程的一种方式是通过创建越来越具体的规则来逐步深入数据，以识别类别值。假设你的任务是创建规则来判断一个动物是否是哺乳动物。你可以将所有动物的集合描绘成一个大空间，如下图所示：
- en: '![Separate and conquer](img/B03905_05_15.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![分离并征服](img/B03905_05_15.jpg)'
- en: 'A rule learner begins by using the available features to find homogeneous groups.
    For example, using a feature that indicates whether the species travels via land,
    sea, or air, the first rule might suggest that any land-based animals are mammals:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习者首先利用可用的特征来找到同质的群体。例如，使用一个表示物种是通过陆地、海洋还是空中移动的特征，第一个规则可能建议所有陆地动物都是哺乳动物：
- en: '![Separate and conquer](img/B03905_05_16.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![分离并征服](img/B03905_05_16.jpg)'
- en: 'Do you notice any problems with this rule? If you''re an animal lover, you
    might have realized that frogs are amphibians, not mammals. Therefore, our rule
    needs to be a bit more specific. Let''s drill down further by suggesting that
    mammals must walk on land and have a tail:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到这个规则有什么问题吗？如果你是一个动物爱好者，你可能已经意识到青蛙是两栖动物，而不是哺乳动物。因此，我们的规则需要更具体一点。我们可以进一步深入，假设哺乳动物必须在陆地上行走并且有尾巴：
- en: '![Separate and conquer](img/B03905_05_17.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![分离并征服](img/B03905_05_17.jpg)'
- en: An additional rule can be defined to separate out the bats, the only remaining
    mammal. Thus, this subset can be separated from the other data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可以定义一个额外的规则来分离蝙蝠，它是唯一剩下的哺乳动物。因此，这个子集可以与其他数据分开。
- en: 'An additional rule can be defined to separate out the bats, the only remaining
    mammal. A potential feature distinguishing bats from the other remaining animals
    would be the presence of fur. Using a rule built around this feature, we have
    then correctly identified all the animals:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 可以定义一个额外的规则来区分蝙蝠，它是唯一剩下的哺乳动物。一个可能的特征是蝙蝠与其他剩余动物的区别在于它们有毛发。通过使用基于这个特征的规则，我们就正确地识别了所有的动物：
- en: '![Separate and conquer](img/B03905_05_18.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![分离并征服](img/B03905_05_18.jpg)'
- en: 'At this point, since all of the training instances have been classified, the
    rule learning process would stop. We learned a total of three rules:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点时，由于所有训练实例都已经被分类，规则学习过程将停止。我们总共学到了三个规则：
- en: Animals that walk on land and have tails are mammals
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 会走在陆地上并且有尾巴的动物是哺乳动物。
- en: If the animal does not have fur, it is not a mammal
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物没有毛发，它就不是哺乳动物。
- en: Otherwise, the animal is a mammal
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，动物就是哺乳动物。
- en: The previous example illustrates how rules gradually consume larger and larger
    segments of data to eventually classify all instances.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子说明了规则如何逐渐消耗更大、更大的数据片段，最终对所有实例进行分类。
- en: As the rules seem to cover portions of the data, separate and conquer algorithms
    are also known as **covering algorithms**, and the resulting rules are called
    covering rules. In the next section, we will learn how covering rules are applied
    in practice by examining a simple rule learning algorithm. We will then examine
    a more complex rule learner, and apply both to a real-world problem.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规则似乎覆盖了数据的部分内容，分离并征服算法也被称为**覆盖算法**，而由此产生的规则则被称为覆盖规则。在下一节中，我们将通过研究一个简单的规则学习算法来了解覆盖规则是如何在实践中应用的。然后我们将研究一个更复杂的规则学习者，并将这两者应用于实际问题。
- en: The 1R algorithm
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1R算法
- en: Suppose a television game show has a wheel with ten evenly sized colored slices.
    Three of the segments were colored red, three were blue, and four were white.
    Prior to spinning the wheel, you are asked to choose one of these colors. When
    the wheel stops spinning, if the color shown matches your prediction, you will
    win a large cash prize. What color should you pick?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个电视游戏节目，节目中有一个轮盘，轮盘上有十个大小均等的彩色分区。三个分区是红色的，三个是蓝色的，四个是白色的。在转动轮盘之前，你需要选择其中一种颜色。当轮盘停止时，如果显示的颜色与你的预测匹配，你将赢得一大笔现金奖励。你应该选择哪种颜色？
- en: If you choose white, you are, of course, more likely to win the prize—this is
    the most common color on the wheel. Obviously, this game show is a bit ridiculous,
    but it demonstrates the simplest classifier, **ZeroR**, a rule learner that literally
    learns no rules (hence the name). For every unlabeled example, regardless of the
    values of its features, it predicts the most common class.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择白色，当然更有可能赢得奖品——这是轮盘上最常见的颜色。显然，这个游戏节目有点荒谬，但它展示了最简单的分类器**ZeroR**，一个实际上不学习任何规则的规则学习器（因此得名）。对于每一个未标记的样本，无论其特征值如何，都会预测最常见的类别。
- en: The **1R algorithm** (**One Rule** or **OneR**), improves over ZeroR by selecting
    a single rule. Although this may seem overly simplistic, it tends to perform better
    than you might expect. As demonstrated in empirical studies, the accuracy of this
    algorithm can approach that of much more sophisticated algorithms for many real-world
    tasks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**1R算法**（**单规则**或**OneR**）通过选择一个规则改进了ZeroR。尽管这看起来可能过于简化，但它往往比你想象的表现更好。正如实证研究所示，对于许多实际任务，这个算法的准确率可以接近更复杂算法的表现。'
- en: Note
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For an in-depth look at the surprising performance of 1R, see Holte RC. *Very
    simple classification rules perform well on most commonly used datasets*. Machine
    Learning. 1993; 11:63-91.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对1R算法出乎意料的表现进行深入了解，请参见 Holte RC. *非常简单的分类规则在大多数常用数据集上表现良好*。《机器学习》1993年；11:63-91。
- en: 'The strengths and weaknesses of the 1R algorithm are shown in the following
    table:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 1R算法的优缺点如下表所示：
- en: '| Strengths | Weaknesses |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 缺点 |'
- en: '| --- | --- |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Generates a single, easy-to-understand, human-readable rule of thumb
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个简单易懂、可读性强的经验法则
- en: Often performs surprisingly well
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常表现得出奇的好
- en: Can serve as a benchmark for more complex algorithms
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以作为更复杂算法的基准
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Uses only a single feature
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用一个特征
- en: Probably overly simplistic
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能过于简化
- en: '|'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The way this algorithm works is simple. For each feature, 1R divides the data
    into groups based on similar values of the feature. Then, for each segment, the
    algorithm predicts the majority class. The error rate for the rule based on each
    feature is calculated and the rule with the fewest errors is chosen as the one
    rule.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的工作原理很简单。对于每个特征，1R将数据根据特征的相似值划分成组。然后，对于每个子集，算法预测多数类。计算基于每个特征的规则的错误率，选择错误最少的规则作为最终的单一规则。
- en: 'The following tables show how this would work for the animal data we looked
    at earlier in this section:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了这个算法如何作用于我们在本节中早些时候看到的动物数据：
- en: '![The 1R algorithm](img/B03905_05_19.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![1R算法](img/B03905_05_19.jpg)'
- en: 'For the **Travels By** feature, the dataset was divided into three groups:
    **Air**, **Land**, and **Sea**. Animals in the **Air** and **Sea** groups were
    predicted to be non-mammal, while animals in the **Land** group were predicted
    to be mammals. This resulted in two errors: bats and frogs. The **Has Fur** feature
    divided animals into two groups. Those with fur were predicted to be mammals,
    while those without fur were not predicted to be mammals. Three errors were counted:
    pigs, elephants, and rhinos. As the **Travels By** feature results in fewer errors,
    the 1R algorithm will return the following "one rule" based on **Travels By**:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**通过何种方式移动**特征，数据集被划分为三组：**空中**、**陆地**和**海洋**。**空中**和**海洋**组的动物被预测为非哺乳动物，而**陆地**组的动物则被预测为哺乳动物。这导致了两个错误：蝙蝠和青蛙。**是否有毛发**特征将动物分为两组。有毛发的被预测为哺乳动物，而没有毛发的则被预测为非哺乳动物。统计了三个错误：猪、大象和犀牛。由于**通过何种方式移动**特征导致的错误较少，1R算法将基于**通过何种方式移动**返回以下“单一规则”：
- en: If the animal travels by air, it is not a mammal
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物通过空气移动，它不是哺乳动物
- en: If the animal travels by land, it is a mammal
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物通过陆地移动，它是哺乳动物
- en: If the animal travels by sea, it is not a mammal
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物通过海洋移动，它不是哺乳动物
- en: The algorithm stops here, having found the single most important rule.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在这里停止，已找到最重要的单一规则。
- en: Obviously, this rule learning algorithm may be too basic for some tasks. Would
    you want a medical diagnosis system to consider only a single symptom, or an automated
    driving system to stop or accelerate your car based on only a single factor? For
    these types of tasks, a more sophisticated rule learner might be useful. We'll
    learn about one in the following section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这个规则学习算法对于某些任务可能过于基础。你希望医疗诊断系统只考虑一个症状，还是希望自动驾驶系统仅根据一个因素来决定停车或加速？对于这些类型的任务，可能需要更复杂的规则学习器。我们将在接下来的章节中了解一个。
- en: The RIPPER algorithm
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RIPPER算法
- en: Early rule learning algorithms were plagued by a couple of problems. First,
    they were notorious for being slow, which made them ineffective for the increasing
    number of large datasets. Secondly, they were often prone to being inaccurate
    on noisy data.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的规则学习算法存在一些问题。首先，它们以速度慢而闻名，这使得它们在处理日益增多的大型数据集时效果不佳。其次，它们在噪声数据上通常容易出现不准确的情况。
- en: A first step toward solving these problems was proposed in 1994 by Johannes
    Furnkranz and Gerhard Widmer. Their **Incremental Reduced Error Pruning (IREP)
    algorithm** uses a combination of pre-pruning and post-pruning methods that grow
    very complex rules and prune them before separating the instances from the full
    dataset. Although this strategy helped the performance of rule learners, decision
    trees often still performed better.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的第一步由Johannes Furnkranz和Gerhard Widmer于1994年提出。他们的**增量减少误差修剪（IREP）算法**结合了预修剪和后修剪方法，这些方法使得规则变得非常复杂，然后在从完整数据集中分离实例之前进行修剪。尽管这一策略提高了规则学习者的性能，但决策树通常仍然表现得更好。
- en: Note
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on IREP, see Furnkranz J, Widmer G. *Incremental Reduced
    Error Pruning*. Proceedings of the 11^(th) International Conference on Machine
    Learning. 1994: 70-77.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 关于IREP的更多信息，参见Furnkranz J, Widmer G. *增量减少误差修剪*。1994年《第11届国际机器学习大会论文集》：70-77。
- en: Rule learners took another step forward in 1995 when William W. Cohen introduced
    the **Repeated Incremental Pruning to Produce Error Reduction (RIPPER) algorithm**,
    which improved upon IREP to generate rules that match or exceed the performance
    of decision trees.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习算法在1995年迈出了新的一步，当时William W. Cohen提出了**重复增量修剪以产生误差减少（RIPPER）算法**，该算法在IREP的基础上进行了改进，生成的规则能够匹配或超越决策树的性能。
- en: Note
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more detail on RIPPER, see Cohen WW. *Fast effective rule induction*. Proceedings
    of the 12^(th) International Conference on Machine Learning. 1995:115-123.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RIPPER的更多细节，参见Cohen WW. *快速有效的规则归纳*。1995年《第12届国际机器学习大会论文集》：115-123。
- en: 'As outlined in the following table, the strengths and weaknesses of RIPPER
    are generally comparable to decision trees. The chief benefit is that they may
    result in a slightly more parsimonious model:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表所示，RIPPER的优缺点与决策树基本相当。主要的优点是，它们可能生成一个稍微更简洁的模型：
- en: '| Strengths | Weaknesses |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 劣势 |'
- en: '| --- | --- |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Generates easy-to-understand, human-readable rules
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成易于理解的人类可读规则
- en: Efficient on large and noisy datasets
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型和噪声数据集上效率高
- en: Generally produces a simpler model than a comparable decision tree
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常比可比的决策树生成更简单的模型
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: May result in rules that seem to defy common sense or expert knowledge
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会生成似乎违背常识或专家知识的规则
- en: Not ideal for working with numeric data
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适合处理数值数据
- en: Might not perform as well as more complex models
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不如更复杂的模型表现得好
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Having evolved from several iterations of rule learning algorithms, the RIPPER
    algorithm is a patchwork of efficient heuristics for rule learning. Due to its
    complexity, a discussion of the technical implementation details is beyond the
    scope of this book. However, it can be understood in general terms as a three-step
    process:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: RIPPER算法是从多个规则学习算法的迭代中发展而来的，它是规则学习的高效启发式算法的拼凑。由于其复杂性，技术实现的详细讨论超出了本书的范围。然而，可以将其大致理解为一个三步过程：
- en: Grow
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增长
- en: Prune
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修剪
- en: Optimize
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化
- en: The growing phase uses the separate and conquer technique to greedily add conditions
    to a rule until it perfectly classifies a subset of data or runs out of attributes
    for splitting. Similar to decision trees, the information gain criterion is used
    to identify the next splitting attribute. When increasing a rule's specificity
    no longer reduces entropy, the rule is immediately pruned. Steps one and two are
    repeated until it reaches a stopping criterion, at which point the entire set
    of rules is optimized using a variety of heuristics.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 增长阶段使用分离与征服技术，贪婪地向规则中添加条件，直到它完美分类数据子集或没有更多的属性可用于拆分。与决策树类似，信息增益标准用于确定下一个拆分属性。当增加规则的特异性不再减少熵时，规则会立即被修剪。步骤一和步骤二会反复进行，直到达到停止标准，此时使用各种启发式方法优化整个规则集。
- en: The RIPPER algorithm can create much more complex rules than can the 1R algorithm,
    as in can consider more than one feature. This means that it can create rules
    with multiple antecedents such as "if an animal flies and has fur, then it is
    a mammal." This improves the algorithm's ability to model complex data, but just
    like decision trees, it means that the rules can quickly become more difficult
    to comprehend.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: RIPPER算法能够创建比1R算法更复杂的规则，因为它可以考虑多个特征。这意味着它可以创建具有多个前提的规则，例如“如果动物会飞并且有毛发，那么它是哺乳动物”。这提高了算法处理复杂数据的能力，但就像决策树一样，这也意味着规则可能会迅速变得更难理解。
- en: Note
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The evolution of classification rule learners didn't stop with RIPPER. New rule
    learning algorithms are being proposed rapidly. A survey of literature shows algorithms
    called IREP++, SLIPPER, TRIPPER, among many others.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则学习者的发展并未止步于RIPPER。新的规则学习算法正在快速提出。文献调查显示了诸如IREP++、SLIPPER、TRIPPER等多种算法。
- en: Rules from decision trees
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自决策树的规则
- en: 'Classification rules can also be obtained directly from decision trees. Beginning
    at a leaf node and following the branches back to the root, you will have obtained
    a series of decisions. These can be combined into a single rule. The following
    figure shows how rules could be constructed from the decision tree to predict
    movie success:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则也可以直接从决策树中获得。从一个叶节点开始，沿着分支回到根节点，你将获得一系列决策。这些可以组合成一条规则。下图展示了如何从决策树构建规则来预测电影的成功：
- en: '![Rules from decision trees](img/B03905_05_20.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![决策树中的规则](img/B03905_05_20.jpg)'
- en: 'Following the paths from the root node down to each leaf, the rules would be:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 从根节点到每个叶子节点的路径，规则将是：
- en: If the number of celebrities is low, then the movie will be a **Box Office Bust**.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果明星数量较少，那么电影将会是**票房惨败**。
- en: If the number of celebrities is high and the budget is high, then the movie
    will be a **Mainstream Hit**.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果明星数量较多且预算较高，那么电影将会是**主流热片**。
- en: If the number of celebrities is high and the budget is low, then the movie will
    be a **Critical Success**.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果明星数量较多且预算较低，那么电影将会是**口碑成功**。
- en: For reasons that will be made clear in the following section, the chief downside
    to using a decision tree to generate rules is that the resulting rules are often
    more complex than those learned by a rule learning algorithm. The divide and conquer
    strategy employed by decision trees biases the results differently than that of
    a rule learner. On the other hand, it is sometimes more computationally efficient
    to generate rules from trees.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 由于接下来的部分将会阐明的原因，使用决策树生成规则的主要缺点是，结果的规则通常比规则学习算法学到的规则更复杂。决策树采用的分治策略与规则学习者的偏差不同。另一方面，有时从树中生成规则在计算上更加高效。
- en: Tip
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The `C5.0()` function in the `C50` package will generate a model using classification
    rules if you specify `rules = TRUE` when training the model.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`C5.0()`函数在`C50`包中会生成一个使用分类规则的模型，前提是你在训练模型时指定`rules = TRUE`。'
- en: What makes trees and rules greedy?
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 是什么让树和规则显得贪心？
- en: Decision trees and rule learners are known as **greedy learners** because they
    use data on a first-come, first-served basis. Both the divide and conquer heuristic
    used by decision trees and the separate and conquer heuristic used by rule learners
    attempt to make partitions one at a time, finding the most homogeneous partition
    first, followed by the next best, and so on, until all examples have been classified.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和规则学习者被称为**贪心学习者**，因为它们按先到先得的方式使用数据。决策树使用的分治策略和规则学习者使用的分离策略都试图一次做出一个分割，首先找到最同质的分割，然后是下一个最好的分割，依此类推，直到所有实例都被分类。
- en: The downside to the greedy approach is that greedy algorithms are not guaranteed
    to generate the optimal, most accurate, or smallest number of rules for a particular
    dataset. By taking the low-hanging fruit early, a greedy learner may quickly find
    a single rule that is accurate for one subset of data; however, in doing so, the
    learner may miss the opportunity to develop a more nuanced set of rules with better
    overall accuracy on the entire set of data. However, without using the greedy
    approach to rule learning, it is likely that for all but the smallest of datasets,
    rule learning would be computationally infeasible.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪方法的缺点是，贪婪算法并不保证为特定数据集生成最佳、最准确或最少数量的规则。通过提前采摘低悬果实，贪婪学习者可能会迅速找到一个对某个数据子集准确的单一规则；然而，采取这种做法，学习者可能会错失开发一个更加细致的规则集的机会，而这个规则集在整个数据集上具有更好的整体准确性。然而，如果不使用贪婪方法进行规则学习，那么对于除了最小的数据集以外的所有情况，规则学习将变得计算上不可行。
- en: '![What makes trees and rules greedy?](img/B03905_05_21.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![什么使得树和规则贪婪？](img/B03905_05_21.jpg)'
- en: Though both trees and rules employ greedy learning heuristics, there are subtle
    differences in how they build rules. Perhaps the best way to distinguish them
    is to note that once divide and conquer splits on a feature, the partitions created
    by the split may not be re-conquered, only further subdivided. In this way, a
    tree is permanently limited by its history of past decisions. In contrast, once
    separate and conquer finds a rule, any examples not covered by all of the rule's
    conditions may be re-conquered.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管树和规则都采用贪婪学习启发式，但它们在构建规则的方式上存在微妙的差异。也许区分它们最好的方式是注意到，一旦分治法在某个特征上进行分裂，分裂产生的分区就不能被重新征服，只能进一步细分。通过这种方式，树会永久地受到过去决策历史的限制。相反，一旦分离与征服法找到一个规则，任何不被所有规则条件覆盖的示例都可以被重新征服。
- en: 'To illustrate this contrast, consider the previous case in which we built a
    rule learner to determine whether an animal was a mammal. The rule learner identified
    three rules that perfectly classify the example animals:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一对比，考虑前面的例子，其中我们构建了一个规则学习器来判断一种动物是否是哺乳动物。规则学习器识别了三个规则，能够完美地分类示例动物：
- en: Animals that walk on land and have tails are mammals (bears, cats, dogs, elephants,
    pigs, rabbits, rats, rhinos)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上陆行走且有尾巴的动物是哺乳动物（熊、猫、狗、大象、猪、兔子、老鼠、犀牛）
- en: If the animal does not has fur, it is not a mammal (birds, eels, fish, frogs,
    insects, sharks)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物没有毛发，那么它就不是哺乳动物（鸟类、电鳗、鱼类、青蛙、昆虫、鲨鱼）
- en: Otherwise, the animal is a mammal (bats)
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，该动物是哺乳动物（蝙蝠）
- en: 'In contrast, a decision tree built on the same data might have come up with
    four rules to achieve the same perfect classification:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，基于相同数据构建的决策树可能会提出四个规则来实现相同的完美分类：
- en: If an animal walks on land and has fur, then it is a mammal (bears, cats, dogs,
    elephants, pigs, rabbits, rats, rhinos)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物上陆行走且有毛发，那么它是哺乳动物（熊、猫、狗、大象、猪、兔子、老鼠、犀牛）
- en: If an animal walks on land and does not have fur, then it is not a mammal (frogs)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一种动物上陆行走且没有毛发，那么它就不是哺乳动物（青蛙）
- en: If the animal does not walk on land and has fur, then it is a mammal (bats)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物不上陆行走且有毛发，那么它是哺乳动物（蝙蝠）
- en: If the animal does not walk on land and does not have fur, then it is not a
    mammal (birds, insects, sharks, fish, eels)![What makes trees and rules greedy?](img/B03905_05_22.jpg)
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物不上陆行走且没有毛发，那么它就不是哺乳动物（鸟类、昆虫、鲨鱼、鱼类、电鳗）![什么使得树和规则贪婪？](img/B03905_05_22.jpg)
- en: The different result across these two approaches has to do with what happens
    to the frogs after they are separated by the "walk on land" decision. Where the
    rule learner allows frogs to be re-conquered by the "does not have fur" decision,
    the decision tree cannot modify the existing partitions, and therefore must place
    the frog into its own rule.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法产生不同结果的原因在于青蛙在“上陆行走”决策后发生的变化。规则学习器允许青蛙被“没有毛发”决策重新分类，而决策树无法修改现有的分区，因此必须将青蛙归入自己的规则中。
- en: On one hand, because rule learners can reexamine cases that were considered
    but ultimately not covered as part of prior rules, rule learners often find a
    more parsimonious set of rules than those generated from decision trees. On the
    other hand, this reuse of data means that the computational cost of rule learners
    may be somewhat higher than for decision trees.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，由于规则学习器可以重新审视那些曾被考虑但最终未被先前规则覆盖的案例，规则学习器通常会找到比决策树生成的规则集更加简洁的规则集。另一方面，这种数据的重复使用意味着规则学习器的计算成本可能比决策树略高。
- en: Example – identifying poisonous mushrooms with rule learners
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例——使用规则学习器识别有毒蘑菇
- en: Each year, many people fall ill and sometimes even die from ingesting poisonous
    wild mushrooms. Since many mushrooms are very similar to each other in appearance,
    occasionally even experienced mushroom gatherers are poisoned.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，许多人因食用有毒野生蘑菇而生病，有时甚至死亡。由于许多蘑菇在外观上非常相似，偶尔甚至经验丰富的蘑菇采摘者也会中毒。
- en: Unlike the identification of harmful plants such as a poison oak or poison ivy,
    there are no clear rules such as "leaves of three, let them be" to identify whether
    a wild mushroom is poisonous or edible. Complicating matters, many traditional
    rules, such as "poisonous mushrooms are brightly colored," provide dangerous or
    misleading information. If simple, clear, and consistent rules were available
    to identify poisonous mushrooms, they could save the lives of foragers.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 与识别有毒植物（如毒橡树或常春藤）不同，识别野生蘑菇是否有毒或可食用并没有像“叶子三片，留它们”这样的明确规则。更复杂的是，许多传统规则，如“有毒蘑菇是鲜艳的颜色”，提供了危险或误导性的信息。如果有简单、明确且一致的规则来识别有毒蘑菇，它们可能会拯救采蘑菇者的生命。
- en: Because one of the strengths of rule learning algorithms is the fact that they
    generate easy-to-understand rules, they seem like an appropriate fit for this
    classification task. However, the rules will only be as useful as they are accurate.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 由于规则学习算法的一个优势是它们生成易于理解的规则，因此它们似乎非常适合这项分类任务。然而，这些规则的有效性将取决于它们的准确性。
- en: Step 1 – collecting data
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步——收集数据
- en: To identify rules for distinguishing poisonous mushrooms, we will utilize the
    Mushroom dataset by Jeff Schlimmer of Carnegie Mellon University. The raw dataset
    is available freely at the UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别区分有毒蘑菇的规则，我们将使用卡内基梅隆大学Jeff Schlimmer的蘑菇数据集。原始数据集可以在UCI机器学习库中免费获得（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）。
- en: 'The dataset includes information on 8,124 mushroom samples from 23 species
    of gilled mushrooms listed in *Audubon Society Field Guide to North American Mushrooms*
    (1981). In the Field Guide, each of the mushroom species is identified "definitely
    edible," "definitely poisonous," or "likely poisonous, and not recommended to
    be eaten." For the purposes of this dataset, the latter group was combined with
    the "definitely poisonous" group to make two classes: poisonous and nonpoisonous.
    The data dictionary available on the UCI website describes the 22 features of
    the mushroom samples, including characteristics such as cap shape, cap color,
    odor, gill size and color, stalk shape, and habitat.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含来自《北美蘑菇观鸟指南》（1981年版）中23种带柄蘑菇的8,124个蘑菇样本的信息。在该指南中，每种蘑菇的种类都被标定为“肯定可食用”，“肯定有毒”或“可能有毒，不建议食用”。对于该数据集，后者与“肯定有毒”类别合并，形成了两个类别：有毒和无毒。UCI网站上的数据字典描述了蘑菇样本的22个特征，包括盖形、盖色、气味、鳃的大小和颜色、柄形状以及栖息地等特征。
- en: Tip
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This chapter uses a slightly modified version of the mushroom data. If you plan
    on following along with the example, download the `mushrooms.csv` file from the
    Packt Publishing website and save it in your R working directory.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的是经过略微修改的蘑菇数据。如果你打算跟着例子一起操作，请从Packt Publishing网站下载`mushrooms.csv`文件并将其保存到你的R工作目录中。
- en: Step 2 – exploring and preparing the data
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步——探索并准备数据
- en: 'We begin by using `read.csv()`, to import the data for our analysis. Since
    all the 22 features and the target class are nominal, in this case, we will set
    `stringsAsFactors = TRUE` and take advantage of the automatic factor conversion:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`read.csv()`导入数据进行分析。由于所有22个特征和目标类别都是名义型的，在本例中，我们将设置`stringsAsFactors
    = TRUE`并利用自动因子转换功能：
- en: '[PRE26]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output of the `str(mushrooms)` command notes that the data contain 8,124
    observations of 23 variables as the data dictionary had described. While most
    of the `str()` output is unremarkable, one feature is worth mentioning. Do you
    notice anything peculiar about the `veil_type` variable in the following line?
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`str(mushrooms)`命令的输出显示数据包含8,124个观察值，涵盖23个变量，正如数据字典所描述的那样。虽然`str()`的输出大部分没有特别之处，但有一个特征值得一提。你是否注意到以下行中的`veil_type`变量有什么不同之处？'
- en: '[PRE27]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If you think it is odd that a factor has only one level, you are correct. The
    data dictionary lists two levels for this feature: partial and universal. However,
    all the examples in our data are classified as partial. It is likely that this
    data element was somehow coded incorrectly. In any case, since the veil type does
    not vary across samples, it does not provide any useful information for prediction.
    We will drop this variable from our analysis using the following command:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得一个因子只有一个水平很奇怪，那么你是对的。数据字典列出了该特征的两个水平：部分和普遍。然而，我们数据中的所有示例都被归类为部分。很可能这个数据元素被错误地编码了。无论如何，由于面纱类型在样本间没有变化，它不会为预测提供任何有用信息。我们将通过以下命令从分析中删除该变量：
- en: '[PRE28]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: By assigning `NULL` to the veil type vector, R eliminates the feature from the
    `mushrooms` data frame.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`NULL`赋值给面纱类型向量，R会从`mushrooms`数据框中删除该特征。
- en: 'Before going much further, we should take a quick look at the distribution
    of the mushroom `type` class variable in our dataset:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入分析之前，我们应该快速查看一下数据集中蘑菇`type`类别变量的分布：
- en: '[PRE29]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: About 52 percent of the mushroom samples (*N = 4,208*) are edible, while 48
    percent (*N = 3,916*) are poisonous.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 大约52%的蘑菇样本（*N = 4,208*）是可食用的，而48%（*N = 3,916*）是有毒的。
- en: For the purposes of this experiment, we will consider the 8,214 samples in the
    mushroom data to be an exhaustive set of all the possible wild mushrooms. This
    is an important assumption, because it means that we do not need to hold some
    samples out of the training data for testing purposes. We are not trying to develop
    rules that cover unforeseen types of mushrooms; we are merely trying to find rules
    that accurately depict the complete set of known mushroom types. Therefore, we
    can build and test the model on the same data.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本实验的目的，我们将蘑菇数据集中的8,214个样本视为所有可能野生蘑菇的完整集合。这是一个重要的假设，因为这意味着我们不需要将一些样本从训练数据中排除用于测试。我们不是试图开发涵盖未知蘑菇类型的规则；我们只是试图找到准确描述已知蘑菇类型完整集合的规则。因此，我们可以在相同的数据上构建并测试模型。
- en: Step 3 – training a model on the data
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 – 在数据上训练模型
- en: If we trained a hypothetical ZeroR classifier on this data, what would it predict?
    Since ZeroR ignores all of the features and simply predicts the target's mode,
    in plain language, its rule would state that all the mushrooms are edible. Obviously,
    this is not a very helpful classifier, because it would leave a mushroom gatherer
    sick or dead with nearly half of the mushroom samples bearing the possibility
    of being poisonous. Our rules will need to do much better than this in order to
    provide safe advice that can be published. At the same time, we need simple rules
    that are easy to remember.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在这些数据上训练一个假设的ZeroR分类器，它会预测什么？由于ZeroR忽略所有特征，仅仅预测目标的众数，用简单的话来说，它的规则会说所有蘑菇都是可食用的。显然，这不是一个很有用的分类器，因为它会让采蘑菇的人生病或死亡，因为几乎一半的蘑菇样本有可能是有毒的。我们的规则必须比这个做得好得多，才能提供可以发布的安全建议。同时，我们需要简单易记的规则。
- en: Since simple rules can often be extremely predictive, let's see how a very simple
    rule learner performs on the mushroom data. Toward the end, we will apply the
    1R classifier, which will identify the most predictive single feature of the target
    class and use it to construct a set of rules.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 由于简单规则通常具有极强的预测性，让我们看看一个非常简单的规则学习器在蘑菇数据上的表现。最后，我们将应用1R分类器，它将识别目标类别中最具预测性的单一特征，并使用它来构建一套规则。
- en: 'We will use the 1R implementation in the `RWeka` package called `OneR()`. You
    may recall that we had installed `RWeka` in [Chapter 1](ch01.html "Chapter 1. Introducing
    Machine Learning"), *Introducing Machine Learning*, as a part of the tutorial
    on installing and loading packages. If you haven''t installed the package per
    these instructions, you will need to use the `install.packages("RWeka")` command
    and have Java installed on your system (refer to the installation instructions
    for more details). With these steps complete, load the package by typing `library(RWeka)`:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`RWeka`包中的1R实现，名为`OneR()`。你可能记得我们在[第1章](ch01.html "第1章：机器学习介绍")，*机器学习介绍*中，作为安装和加载包的教程一部分，安装了`RWeka`。如果你没有按照这些说明安装该包，你将需要使用`install.packages("RWeka")`命令，并确保你的系统上安装了Java（参阅安装说明获取更多详情）。完成这些步骤后，输入`library(RWeka)`加载该包：
- en: '![Step 3 – training a model on the data](img/B03905_05_23.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 3 – 在数据上训练模型](img/B03905_05_23.jpg)'
- en: The `OneR()` implementation uses the R formula syntax to specify the model to
    be trained. The formula syntax uses the `~` operator (known as the tilde) to express
    the relationship between a target variable and its predictors. The class variable
    to be learned goes to the left of the tilde, and the predictor features are written
    on the right, separated by `+` operators. If you like to model the relationship
    between the `y` class and predictors `x1` and `x2`, you could write the formula
    as `y ~ x1 + x2`. If you like to include all the variables in the model, the special
    term `.` can be used. For example, `y ~ .` specifies the relationship between
    `y` and all the other features in the dataset.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '`OneR()`实现使用R公式语法来指定要训练的模型。公式语法使用`~`运算符（称为波浪号）来表达目标变量与其预测变量之间的关系。要学习的类别变量放在波浪号的左侧，预测特征写在右侧，用`+`运算符分隔。如果你想建模`y`类与预测变量`x1`和`x2`之间的关系，你可以写成`y
    ~ x1 + x2`。如果你想在模型中包含所有变量，可以使用特殊术语`.`。例如，`y ~ .`指定了`y`与数据集中的所有其他特征之间的关系。'
- en: Tip
  id: totrans-340
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The R formula syntax is used across many R functions and offers some powerful
    features to describe the relationships among predictor variables. We will explore
    some of these features in the later chapters. However, if you're eager for a sneak
    peek, feel free to read the documentation using the `?formula` command.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: R公式语法在许多R函数中被广泛使用，并提供了一些强大的功能来描述预测变量之间的关系。我们将在后面的章节中探索其中的一些特性。如果你急于了解，可以使用`?formula`命令查看文档。
- en: 'Using the `type ~ .` formula, we will allow our first `OneR()` rule learner
    to consider all the possible features in the mushroom data while constructing
    its rules to predict type:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`type ~ .`公式，我们将允许我们的第一个`OneR()`规则学习器在构建预测类型的规则时，考虑蘑菇数据中的所有可能特征：
- en: '[PRE30]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To examine the rules it created, we can type the name of the classifier object,
    in this case, `mushroom_1R`:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查它创建的规则，我们可以输入分类器对象的名称，在这个例子中是`mushroom_1R`：
- en: '[PRE31]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the first line of the output, we see that the odor feature was selected
    for rule generation. The categories of odor, such as almond, anise, and so on,
    specify rules for whether the mushroom is likely to be edible or poisonous. For
    instance, if the mushroom smells fishy, foul, musty, pungent, spicy, or like creosote,
    the mushroom is likely to be poisonous. On the other hand, mushrooms with more
    pleasant smells like almond and anise, and those with no smell at all are predicted
    to be edible. For the purposes of a field guide for mushroom gathering, these
    rules could be summarized in a simple rule of thumb: "if the mushroom smells unappetizing,
    then it is likely to be poisonous."'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出的第一行，我们看到选中了“气味”特征用于规则生成。气味的类别，如杏仁、茴香等，指定了蘑菇是否可能是可食用或有毒的规则。例如，如果蘑菇闻起来有腥味、腐臭味、霉味、刺激味、辣味或柏油味，那么蘑菇很可能是有毒的。另一方面，闻起来更宜人的气味，如杏仁和茴香，或者没有气味的蘑菇，通常被预测为可食用。对于蘑菇采集的野外指南来说，这些规则可以总结为一个简单的经验法则：“如果蘑菇闻起来不舒服，那么它很可能是有毒的。”
- en: Step 4 – evaluating model performance
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: 'The last line of the output notes that the rules correctly predicted the edibility
    of 8,004 of the 8,124 mushroom samples or nearly 99 percent of the mushroom samples.
    We can obtain additional details about the classifier using the `summary()` function,
    as shown in the following example:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后一行指出，规则正确预测了8,124个蘑菇样本中8,004个的可食性，约占99％。我们可以使用`summary()`函数获得分类器的更多细节，如下例所示：
- en: '[PRE32]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The section labeled `Summary` lists a number of different ways to measure the
    performance of our 1R classifier. We will cover many of these statistics later
    on in [Chapter 10](ch10.html "Chapter 10. Evaluating Model Performance"), *Evaluating
    Model Performance*, so we will ignore them for now.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 标记为`Summary`的部分列出了多种不同的方式来衡量我们1R分类器的性能。我们将在[第10章](ch10.html "第10章：评估模型性能")中详细介绍其中许多统计量，*评估模型性能*，因此现在暂时不讨论这些内容。
- en: The section labeled `Confusion Matrix` is similar to those used before. Here,
    we can see where our rules went wrong. The key is displayed on the right, with
    `a = edible` and `b = poisonous`. Table columns indicate the predicted class of
    the mushroom while the table rows separate the 4,208 edible mushrooms from the
    3,916 poisonous mushrooms. Examining the table, we can see that although the 1R
    classifier did not classify any edible mushrooms as poisonous, it did classify
    120 poisonous mushrooms as edible—which makes for an incredibly dangerous mistake!
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 标有`混淆矩阵`的部分与之前使用的相似。在这里，我们可以看到规则出现错误的地方。右侧显示了关键，其中`a = 可食用`，`b = 有毒`。表格的列表示预测的蘑菇类别，行则将4,208个可食用蘑菇与3,916个有毒蘑菇分开。通过检查表格，我们可以看到，尽管1R分类器没有将任何可食用蘑菇错误分类为有毒蘑菇，但它却将120个有毒蘑菇错误分类为可食用的——这可是一个非常危险的错误！
- en: Considering that the learner utilized only a single feature, it did reasonably
    well; if one avoids unappetizing smells when foraging for mushrooms, they will
    almost avoid a trip to the hospital. That said, close does not cut it when lives
    are involved, not to mention the field guide publisher might not be happy about
    the prospect of a lawsuit when its readers fall ill. Let's see if we can add a
    few more rules and develop an even better classifier.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到学习者仅使用了一个特征，模型表现得相当不错；如果在采摘蘑菇时避免不愉快的气味，他们几乎可以避免去医院的风险。尽管如此，当涉及到生命时，“接近”并不足够，更不用说当读者生病时，野外指南的出版商可能会因为面临诉讼而不高兴。让我们看看能否增加一些规则，开发出一个更好的分类器。
- en: Step 5 – improving model performance
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 5 – 改进模型性能
- en: 'For a more sophisticated rule learner, we will use `JRip()`, a Java-based implementation
    of the RIPPER rule learning algorithm. As with the 1R implementation we used previously,
    `JRip()` is included in the `RWeka` package. If you have not done so yet, be sure
    to load the package using the `library(RWeka)` command:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的规则学习器，我们将使用`JRip()`，它是一个基于Java的RIPPER规则学习算法实现。与之前使用的1R实现一样，`JRip()`包含在`RWeka`包中。如果尚未安装，确保使用`library(RWeka)`命令加载该包：
- en: '![Step 5 – improving model performance](img/B03905_05_24.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 5 – 改进模型性能](img/B03905_05_24.jpg)'
- en: As shown in the syntax box, the process of training a `JRip()` model is very
    similar to how we previously trained a `OneR()` model. This is one of the pleasant
    benefits of the functions in the `RWeka` package; the syntax is consistent across
    algorithms, which makes the process of comparing a number of different models
    very simple.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如语法框所示，训练`JRip()`模型的过程与我们之前训练`OneR()`模型的方式非常相似。这是`RWeka`包的一个优点；不同算法的语法一致，使得比较多个不同模型的过程变得非常简单。
- en: 'Let''s train the `JRip()` rule learner as we did with `OneR()`, allowing it
    to choose rules from all the available features:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像训练`OneR()`模型一样训练`JRip()`规则学习器，让它从所有可用的特征中选择规则：
- en: '[PRE33]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To examine the rules, type the name of the classifier:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查规则，输入分类器的名称：
- en: '[PRE34]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `JRip()` classifier learned a total of nine rules from the mushroom data.
    An easy way to read these rules is to think of them as a list of if-else statements,
    similar to programming logic. The first three rules could be expressed as:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '`JRip()`分类器从蘑菇数据中学习了共九条规则。可以将这些规则看作是一系列的if-else语句，类似于编程逻辑。前面三条规则可以表示为：'
- en: If the odor is foul, then the mushroom type is poisonous
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果气味刺鼻，则蘑菇类型为有毒
- en: If the gill size is narrow and the gill color is buff, then the mushroom type
    is poisonous
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果鳃的大小狭窄且鳃的颜色为浅棕色，则蘑菇类型为有毒
- en: If the gill size is narrow and the odor is pungent, then the mushroom type is
    poisonous
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果鳃的大小狭窄且气味刺鼻，则蘑菇类型为有毒
- en: 'Finally, the ninth rule implies that any mushroom sample that was not covered
    by the preceding eight rules is edible. Following the example of our programming
    logic, this can be read as:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第九条规则意味着任何未被前面八条规则涵盖的蘑菇样本都是可食用的。按照我们编程逻辑的示例，可以这样理解：
- en: Else, the mushroom is edible
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，蘑菇是可食用的
- en: The numbers next to each rule indicate the number of instances covered by the
    rule and a count of misclassified instances. Notably, there were no misclassified
    mushroom samples using these nine rules. As a result, the number of instances
    covered by the last rule is exactly equal to the number of edible mushrooms in
    the data (*N = 4,208*).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 每条规则旁边的数字表示该规则涵盖的实例数量以及误分类实例的数量。值得注意的是，使用这九条规则时，没有任何蘑菇样本被误分类。因此，最后一条规则涵盖的实例数量正好等于数据中可食用蘑菇的数量（*N
    = 4,208*）。
- en: The following figure provides a rough illustration of how the rules are applied
    to the mushroom data. If you imagine everything within the oval as all the species
    of mushroom, the rule learner identified features or sets of features, which separate
    homogeneous segments from the larger group. First, the algorithm found a large
    group of poisonous mushrooms uniquely distinguished by their foul odor. Next,
    it found smaller and more specific groups of poisonous mushrooms. By identifying
    covering rules for each of the varieties of poisonous mushrooms, all of the remaining
    mushrooms were found to be edible. Thanks to Mother Nature, each variety of mushrooms
    was unique enough that the classifier was able to achieve 100 percent accuracy.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表大致说明了规则如何应用于蘑菇数据。如果你把椭圆内的所有内容想象为所有种类的蘑菇，规则学习器识别了特征或特征集，将同质的群体从更大的群体中分离出来。首先，算法找到了一大群具有独特恶臭味的有毒蘑菇。接着，它找到了更小、更具体的有毒蘑菇群体。通过识别每种有毒蘑菇的覆盖规则，剩余的所有蘑菇都被确定为可食用。感谢大自然，每种蘑菇的特性足够独特，以至于分类器能够实现100%的准确率。
- en: '![Step 5 – improving model performance](img/B03905_05_25.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 5 – 提升模型性能](img/B03905_05_25.jpg)'
- en: Summary
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered two classification methods that use so-called "greedy"
    algorithms to partition the data according to feature values. Decision trees use
    a divide and conquer strategy to create flowchart-like structures, while rule
    learners separate and conquer data to identify logical if-else rules. Both methods
    produce models that can be interpreted without a statistical background.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了两种分类方法，它们使用所谓的“贪婪”算法，根据特征值将数据进行划分。决策树采用分而治之的策略，创建类似流程图的结构，而规则学习器则通过分离和征服数据，识别逻辑的if-else规则。两种方法生成的模型都可以在没有统计背景的情况下进行解释。
- en: One popular and highly configurable decision tree algorithm is C5.0\. We used
    the C5.0 algorithm to create a tree to predict whether a loan applicant will default.
    Using options for boosting and cost-sensitive errors, we were able to improve
    our accuracy and avoid risky loans that would cost the bank more money.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行且高度可配置的决策树算法是C5.0。我们使用C5.0算法创建了一个决策树来预测贷款申请人是否会违约。通过使用提升和成本敏感错误的选项，我们能够提高准确率，避免那些会使银行蒙受更大损失的高风险贷款。
- en: We also used two rule learners, 1R and RIPPER, to develop rules to identify
    poisonous mushrooms. The 1R algorithm used a single feature to achieve 99 percent
    accuracy in identifying potentially fatal mushroom samples. On the other hand,
    the set of nine rules generated by the more sophisticated RIPPER algorithm correctly
    identified the edibility of each mushroom.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了两种规则学习器，1R和RIPPER，来开发规则以识别有毒蘑菇。1R算法使用一个特征，在识别潜在致命的蘑菇样本时达到了99%的准确率。另一方面，使用更复杂的RIPPER算法生成的九条规则正确地识别了每个蘑菇的可食性。
- en: This chapter merely scratched the surface of how trees and rules can be used.
    In [Chapter 6](ch06.html "Chapter 6. Forecasting Numeric Data – Regression Methods"),
    *Forecasting Numeric Data – Regression Methods*, we will learn techniques known
    as regression trees and model trees, which use decision trees for numeric prediction
    rather than classification. In [Chapter 11](ch11.html "Chapter 11. Improving Model
    Performance"), *Improving Model Performance*, we will discover how the performance
    of decision trees can be improved by grouping them together in a model known as
    a random forest. In [Chapter 8](ch08.html "Chapter 8. Finding Patterns – Market
    Basket Analysis Using Association Rules"), *Finding Patterns – Market Basket Analysis
    Using Association Rules*, we will see how association rules—a relative of classification
    rules—can be used to identify groups of items in transactional data.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅仅触及了树和规则应用的表面。在[第六章](ch06.html "第6章：数值数据预测—回归方法")，*数值数据预测—回归方法*中，我们将学习回归树和模型树等技术，它们使用决策树进行数值预测而非分类。在[第十一章](ch11.html
    "第11章：提升模型性能")，*提升模型性能*中，我们将探讨如何通过将决策树组合成一个叫做随机森林的模型来提升其性能。在[第八章](ch08.html "第8章：寻找模式—使用关联规则的市场篮子分析")，*寻找模式—使用关联规则的市场篮子分析*中，我们将看到如何利用关联规则（分类规则的一种相关形式）来识别交易数据中的项目组。
