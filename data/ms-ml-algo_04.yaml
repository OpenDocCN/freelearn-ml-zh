- en: Bayesian Networks and Hidden Markov Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯网络和隐马尔可夫模型
- en: In this chapter, we're going to introduce the basic concepts of Bayesian models,
    which allow working with several scenarios where it's necessary to consider uncertainty
    as a structural part of the system. The discussion will focus on static (time-invariant)
    and dynamic methods that can be employed where necessary to model time sequences.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍贝叶斯模型的基本概念，这些概念允许在需要将不确定性作为系统结构部分考虑的多个场景中工作。讨论将集中在静态（时间不变）和动态方法，这些方法在必要时可以用来建模时间序列。
- en: 'In particular, the chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，本章涵盖了以下主题：
- en: Bayes' theorem and its applications
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯定理及其应用
- en: Bayesian networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: Sampling from a Bayesian network using direct methods and **Markov chain Monte
    Carlo** (**MCMC**) ones (Gibbs and Metropolis-Hastings samplers)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用直接方法和**马尔可夫链蒙特卡洛**（**MCMC**）（Gibbs和Metropolis-Hastings采样器）从贝叶斯网络中采样
- en: Modeling a Bayesian network with PyMC3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyMC3建模贝叶斯网络
- en: '**Hidden Markov Models** (**HMMs**)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐马尔可夫模型**（**HMMs**）'
- en: Examples with hmmlearn
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用hmmlearn的示例
- en: Conditional probabilities and Bayes' theorem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率和贝叶斯定理
- en: 'If we have a probability space *S* and two events *A* and *B*, the probability
    of *A* given *B* is called **conditional probability**, and it''s defined as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个概率空间 *S* 和两个事件 *A* 和 *B*，事件 *A* 在给定 *B* 的概率被称为**条件概率**，其定义为：
- en: '![](img/352da6fc-79b9-478c-acef-eac9462996be.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/352da6fc-79b9-478c-acef-eac9462996be.png)'
- en: 'As *P(A, B)* = *P(B, A)*, it''s possible to derive **Bayes'' theorem**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *P(A, B)* = *P(B, A)*，可以推导出**贝叶斯定理**：
- en: '![](img/c34fa3ad-5cf8-4d3d-80f1-768034c0d1a3.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c34fa3ad-5cf8-4d3d-80f1-768034c0d1a3.png)'
- en: 'This theorem allows expressing a conditional probability as a function of the
    opposite one and the two marginal probabilities *P(A)* and *P(B)*. This result
    is fundamental to many machine learning problems, because, as we''re going to
    see in this and in the next chapters, normally it''s easier to work with a conditional
    probability in order to get the opposite, but it''s hard to work directly from
    the latter. A common form of this theorem can be expressed as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理允许将条件概率表示为对立概率和两个边缘概率 *P(A)* 和 *P(B)* 的函数。这个结果对于许多机器学习问题来说是基本的，因为我们将在本章和下一章中看到，通常更容易通过条件概率来获取对立概率，但直接从后者开始工作却很困难。这个定理的常见形式可以表示为：
- en: '![](img/d7d71973-5b65-4bbc-8eef-433b4cd5950d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7d71973-5b65-4bbc-8eef-433b4cd5950d.png)'
- en: Let's suppose that we need to estimate the probability of an event *A* given
    some observations *B*, or using the standard notation, **the posterior probability
    of A**; the previous formula expresses this value as proportional to the term
    *P(A)*, which is the marginal probability of *A*, called **prior probability**,
    and the conditional probability of the observations *B* given the event *A*. *P(B|A)*
    is called **likelihood**, and defines how event *A* is likely to determine *B*.
    Therefore, we can summarize the relation as *posterior probability ∝ likelihood ·
    prior probability*. The proportion is not a limitation, because the term *P(B)*
    is always a normalizing constant that can be omitted. Of course, the reader must
    remember to normalize *P(A|B)* so that its terms always sum up to one.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '假设我们需要估计给定一些观察结果 *B* 的事件 *A* 的概率，或者使用标准符号，**A的后验概率**；前面的公式将这个值表示为与项 *P(A)*
    成比例，这是 *A* 的边缘概率，称为**先验概率**，以及观察结果 *B* 在事件 *A* 给定下的条件概率。*P(B|A)* 被称为**似然函数**，它定义了事件
    *A* 如何可能决定 *B*。因此，我们可以总结关系为*后验概率 ∝ 似然函数 · 先验概率*。比例不是一个限制，因为项 *P(B)* 总是一个归一化常数，可以省略。当然，读者必须记住归一化
    *P(A|B)*，使其项总是加起来等于一。 '
- en: 'This is a key concept of Bayesian statistics, where we don''t directly trust
    the prior probability, but we reweight it using the likelihood of some observations.
    As an example, we can think to toss a coin 10 times (event *A*). We know that
    *P(A) = 0.5* if the coin is fair. If we''d like to know what the probability is
    to get 10 heads, we could employ the Binomial distribution obtaining *P(10 heads)
    = 0.5^k*; however, let''s suppose that we don''t know whether the coin is fair
    or not, but we suspect it''s loaded with a prior probability *P(Loaded) = 0.7*
    in favor of tails. We can define a complete prior probability *P(Coin status)*
    using the indicator functions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝叶斯统计的一个关键概念，我们并不直接信任先验概率，而是使用某些观察结果的似然性重新加权。例如，我们可以考虑抛掷一枚硬币 10 次（事件 *A*）。我们知道，如果硬币是公平的，那么
    *P(A) = 0.5*。如果我们想知道得到 10 个正面的概率是多少，我们可以使用二项分布得到 *P(10 heads) = 0.5^k*；然而，假设我们不知道硬币是否公平，但我们怀疑它可能被加载，先验概率
    *P(Loaded) = 0.7* 有利于反面。我们可以使用指示函数定义一个完整的先验概率 *P(Coin status)*：
- en: '![](img/d017c7ab-85b8-4687-8cea-ce1f6222e7c9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d017c7ab-85b8-4687-8cea-ce1f6222e7c9.png)'
- en: Where *P(Fair) = 0.5* and *P(Loaded) = 0.7*, the indicator *I[Coin=Fair]* is
    equal to 1 only if the coin is fair, and 0 otherwise. The same happens with *I[Coin=Loaded]* when
    the coin is loaded. Our goal now is to determine the posterior probability *P(Coin
    status|B[1], B[2], ..., **B[n])* to be able to confirm or to reject our hypothesis.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *P(Fair) = 0.5* 和 *P(Loaded) = 0.7* 的情况下，指示器 *I[Coin=Fair]* 仅在硬币是公平的情况下等于
    1，否则为 0。当硬币是加重的，同样会发生 *I[Coin=Loaded]* 的情况。我们的目标现在是要确定后验概率 *P(Coin status|B[1],
    B[2], ..., **B[n])*，以便能够确认或拒绝我们的假设。
- en: 'Let''s imagine to observe *n = 10* events with *B[1] = Head* and *B[2], ..., B[n] =
    Tail*. We can express the probability using the binomial distribution:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设观察 *n = 10* 个事件，其中 *B[1] = Head* 和 *B[2], ..., *B[n] = Tail*。我们可以使用二项分布来表示概率：
- en: '![](img/845c235e-2666-4685-88fe-65cdb967a8ba.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/845c235e-2666-4685-88fe-65cdb967a8ba.png)'
- en: 'After simplifying the expression, we get:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在简化表达式后，我们得到：
- en: '![](img/48d546d0-7d6d-4bb8-8824-202451d938da.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/48d546d0-7d6d-4bb8-8824-202451d938da.png)'
- en: We still need to normalize by dividing both terms by 0.083 (the sum of the two
    terms), so we get the final posterior probability *P(Coin status|B[1], B[2], ..., **Bn)
    = 0.04I[Fair] + 0.96I[Loaded]*. This result confirms and strengthens our hypothesis.
    The probability of a loaded coin is now about 96%, thanks to the sequence of nine
    tail observations after one head.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要通过将两个项都除以 0.083（两个项的和）来归一化，所以我们得到最终的后验概率 *P(Coin status|B[1], B[2], ...,
    **Bn) = 0.04I[Fair] + 0.96I[Loaded]*。这个结果证实并加强了我们的假设。由于在出现一个正面之后观察到连续九次反面，加重的硬币的概率现在约为
    96%。
- en: This example was presented to show how the data (observations) is plugged into
    the Bayesian framework. If the reader is interested in studying these concepts
    in more detail, in *Introduction to Statistical Decision Theory*, *Pratt J.*,
    *Raiffa H.*, *Schlaifer R.*, *The MIT Press*,it's possible to find many interesting
    examples and explanations; however, before introducing Bayesian networks, it's
    useful to define two other essential concepts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了如何将数据（观察结果）插入贝叶斯框架中。如果读者有兴趣更详细地研究这些概念，可以在 *《统计决策理论导论》*，作者 *Pratt J.*，*Raiffa
    H.*，*Schlaifer R.*，*麻省理工学院出版社* 中找到许多有趣的例子和解释；然而，在介绍贝叶斯网络之前，定义两个其他基本概念是有用的。
- en: 'The first concept is called **conditional independence**, and it can be formalized
    considering two variables *A* and *B*, which are conditioned to a third one, *C*.
    We say that *A* and *B* are conditionally independent given *C* if:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个概念被称为 **条件独立性**，可以通过考虑两个变量 *A* 和 *B* 来形式化，这两个变量依赖于第三个变量 *C*。我们说，在给定 *C* 的条件下，*A*
    和 *B* 是条件独立的，如果：
- en: '![](img/36bb9203-b429-4355-a8dc-85614c716870.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/36bb9203-b429-4355-a8dc-85614c716870.png)'
- en: 'Now, let''s suppose we have an event *A* that is conditioned to a series of
    causes *C[1], C[2], ..., C[n]*; the conditional probability is, therefore, *P(A|C[1],
    C[2], ..., C[n])*. Applying Bayes'' theorem, we get:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设我们有一个事件 *A*，它依赖于一系列原因 *C[1], C[2], ..., C[n]*；因此，条件概率是 *P(A|C[1], C[2],
    ..., C[n])*。应用贝叶斯定理，我们得到：
- en: '![](img/fe21f59c-5b35-43b8-8e36-a87618f446de.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fe21f59c-5b35-43b8-8e36-a87618f446de.png)'
- en: 'If there is conditional independence, the previous expression can be simplified
    and rewritten as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在条件独立性，前面的表达式可以简化并重新写为：
- en: '![](img/981fd6f3-7f95-411f-8c54-81cb045249d8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/981fd6f3-7f95-411f-8c54-81cb045249d8.png)'
- en: This property is fundamental in Naive Bayes classifiers, where we assume that
    the effect produced by a cause does not influence the other causes. For example,
    in a spam detector, we could say that the length of the mail and the presence
    of some particular keywords are independent events, and we only need to compute
    *P(Length|Spam)* and *P(Keywords|Spam)* without considering the joint probability
    *P(Length, Keywords|Spam)*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性在朴素贝叶斯分类器中是基本的，我们假设原因产生的效果不会影响其他原因。例如，在垃圾邮件检测器中，我们可以说邮件的长度和某些特定关键词的存在是独立事件，我们只需要计算
    *P(Length|Spam)* 和 *P(Keywords|Spam)*，而不需要考虑联合概率 *P(Length, Keywords|Spam)*。
- en: 'Another important element is the **chain rule** of probabilities. Let''s suppose
    we have the joint probability *P(X[1], X[2], ..., X[n])*. It can be expressed
    as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要元素是概率的 **链式法则**。假设我们有一个联合概率 *P(X[1], X[2], ..., X[n])*。它可以表示为：
- en: '![](img/107285da-ab3d-4be3-a4b0-d680ff35d2cd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/107285da-ab3d-4be3-a4b0-d680ff35d2cd.png)'
- en: 'Repeating the procedure with the joint probability on the right side, we get:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用右侧联合概率的步骤，我们得到：
- en: '![](img/2fe5a7d2-8909-4da8-9322-2582f45b29a0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fe5a7d2-8909-4da8-9322-2582f45b29a0.png)'
- en: In this way, it's possible to express a full joint probability as the product
    of hierarchical conditional probabilities, until the last term, which is a marginal
    distribution. We are going to use this concept extensively in the next paragraph
    when exploring Bayesian networks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，我们可以将完整的联合概率表示为分层条件概率的乘积，直到最后一个项，它是一个边缘分布。在下一段探索贝叶斯网络时，我们将广泛使用这个概念。
- en: Bayesian networks
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: 'A **Bayesian network** is a probabilistic model represented by a direct acyclic
    graph *G = {V, E}*, where the vertices are random variables *X[i]*, and the edges
    determine a conditional dependence among them. In the following diagram, there''s
    an example of simple Bayesian networks with four variables:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯网络** 是一个由直接无环图 *G = {V, E}* 表示的概率模型，其中顶点是随机变量 *X[i]*，边确定它们之间的条件依赖关系。在下面的图中，有一个包含四个变量的简单贝叶斯网络的例子：'
- en: '![](img/cae2f7b1-4b9f-4cc2-b225-0e12772b4866.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cae2f7b1-4b9f-4cc2-b225-0e12772b4866.png)'
- en: Example of Bayesian network
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络示例
- en: 'The variable *x[4]* is dependent on *x[3]*, which is dependent on *x[1]* and *x[2]*. To
    describe the network, we need the marginal probabilities *P(x[1])* and *P(x[2])*
    and the conditional probabilities *P(x[3]|x[1],x[2])* and *P(x[4]|x[3])*. In fact,
    using the chain rule, we can derive the full joint probability as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 *x[4]* 依赖于 *x[3]*，而 *x[3]* 依赖于 *x[1]* 和 *x[2]*。为了描述这个网络，我们需要边缘概率 *P(x[1])*
    和 *P(x[2])* 以及条件概率 *P(x[3]|x[1],x[2])* 和 *P(x[4]|x[3])*。实际上，使用链式法则，我们可以推导出完整的联合概率如下：
- en: '![](img/c97c045a-48c4-48c0-88f0-bd904ce463c6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c97c045a-48c4-48c0-88f0-bd904ce463c6.png)'
- en: 'The previous expression shows an important concept: as the graph is direct
    and acyclic, each variable is conditionally independent of all other variables
    that are not successors given its predecessors. To formalize this concept, we
    can define the function *Predecessors(x[i])*, which returns the set of nodes that
    influence *x[i]* directly, for example, *Predecessors(x[3]) = {x[1],x[2]}* (we
    are using lowercase letters, but we are considering the random variable, not a
    sample). Using this function, it''s possible to write a general expression for
    the full joint probability of a Bayesian network with *N* nodes:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的表达式展示了一个重要的概念：由于图是直接且无环的，每个变量在给定其前驱的情况下，条件独立于所有其他非后继变量。为了形式化这个概念，我们可以定义函数
    *Predecessors(x[i])*，该函数返回直接影响 *x[i]* 的节点集合，例如，*Predecessors(x[3]) = {x[1],x[2]}*（我们使用小写字母，但考虑的是随机变量，而不是样本）。使用这个函数，我们可以为具有
    *N* 个节点的贝叶斯网络的完整联合概率写出一个通用表达式：
- en: '![](img/303e7bb6-83c4-48be-afea-573888168eef.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/303e7bb6-83c4-48be-afea-573888168eef.png)'
- en: The general procedure to build a Bayesian network should always start with the
    first causes, adding their effects one by one, until the last nodes are inserted
    into the graph. If this rule is not respected, the resulting graph can contain
    useless relations that can increase the complexity of the model. For example,
    if *x[4]* is caused indirectly by both *x[1]* and *x[2]*, therefore adding the
    edges *x[1] → x[4]* and *x[2] → x[4]* could seem a good modeling choice; however,
    we know that the final influence on *x[4]* is determined only by the values of
    *x*[*3*, ]whose probability must be conditioned on *x[1]* and *x[2]*, hence we
    can remove the spurious edges. I suggest reading *Introduction to Statistical
    Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press*to learn many
    best practices that should be employed in this procedure.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 构建贝叶斯网络的一般程序应该始终从第一个原因开始，逐个添加它们的效果，直到最后将节点插入到图中。如果不遵守此规则，生成的图可能包含无用的关系，这会增加模型的复杂性。例如，如果
    *x[4]* 间接由 *x[1]* 和 *x[2]* 导致，因此添加边 *x[1] → x[4]* 和 *x[2] → x[4]* 可能看起来是一个好的建模选择；然而，我们知道对
    *x[4]* 的最终影响仅由 *x*[*3*, ]的值决定，其概率必须根据 *x[1]* 和 *x[2]* 进行条件化，因此我们可以删除虚假的边。我建议阅读
    *《统计决策理论导论》，作者：Pratt J.，Raiffa H.，Schlaifer R.，麻省理工学院出版社*，以了解在此过程中应采用许多最佳实践。
- en: Sampling from a Bayesian network
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从贝叶斯网络中采样
- en: Performing a direct inference on a Bayesian network can be a very complex operation
    when the number of variables and edges is high. For this reason, several sampling
    methods have been proposed. In this paragraph, we are going to show how to determine
    the full joint probability sampling from a network using a direct approach, and
    two MCMC algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯网络上进行直接推理可能是一个非常复杂的操作，当变量和边的数量很高时。因此，已经提出了几种采样方法。在本段中，我们将展示如何使用直接方法从网络中确定全联合概率采样，以及两种
    MCMC 算法。
- en: 'Let''s start considering the previous network and, for simplicity, let''s assume
    to have only *Bernoulli* distributions. *X[1]* and *X[2]* are modeled as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从之前的网络开始考虑，为了简化，我们假设只有 *伯努利* 分布。变量 *X[1]* 和 *X[2]* 被建模为：
- en: '![](img/eb8c8451-da95-42e4-923a-d096f4041bc0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eb8c8451-da95-42e4-923a-d096f4041bc0.png)'
- en: 'The conditional distribution *X[3]* is defined as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 条件分布 *X[3]* 定义为：
- en: '![](img/789d0fe3-5573-429b-a2e3-37a4043b7fed.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/789d0fe3-5573-429b-a2e3-37a4043b7fed.png)'
- en: 'While the conditional distribution *X[4]* is defined as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当条件分布 *X[4]* 定义为：
- en: '![](img/c964debc-3b61-4b6d-9b5a-7d68f7f4a514.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c964debc-3b61-4b6d-9b5a-7d68f7f4a514.png)'
- en: We can now use a direct sampling to estimate the full joint probability *P(x[1],
    x[2], x[3], x[4])* using the chain rule previously introduced.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用直接采样来估计使用之前引入的链式法则的全联合概率 *P(x[1], x[2], x[3], x[4]*)。
- en: Direct sampling
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接采样
- en: 'With **direct sampling**, our goal is to approximate the full joint probability
    through a sequence of samples drawn from each conditional distribution. If we
    assume that the graph is well-structured (without unnecessary edges) and we have
    *N* variables, the algorithm is made up of the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **直接采样**，我们的目标是通过对每个条件分布抽取的样本序列来近似全联合概率。如果我们假设图结构良好（没有不必要的边）并且有 *N* 个变量，算法由以下步骤组成：
- en: Initialize the variable *N[Samples]*.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变量 *N[Samples]*。
- en: Initialize a vector *S* with shape *(N, N[Samples])*.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个形状为 *(N, N[Samples]*) 的向量 *S*。
- en: Initialize a frequency vector *F[Samples]* with shape *(N, N[Samples])*. In
    Python, it's better to employ a dictionary where the key is a combination *(x[1],
    x[2], x[3], ..., x[N])*.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个频率向量 *F[Samples]*，其形状为 *(N, N[Samples]*)。在 Python 中，最好使用一个字典，其中键是组合 *(x[1],
    x[2], x[3], ..., x[N]*)。
- en: For *t=1* to *N*[*Samples*:]
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=1* 到 *N[Samples]*：
- en: 'For *i=1* to *N*:'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Sample from *P(X[i]|Predecessors(X[i]))*
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *P(X[i]|Predecessors(X[i]))* 中采样
- en: Store the sample in *S[i, t]*
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将样本存储在 *S[i, t]*
- en: 'If *F[Samples]* contains the sampled tuple *S[:, t]*:'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *F[Samples]* 包含采样的元组 *S[:, t]*：
- en: '**F[Samples][S[:, t]] += 1**'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**F[Samples][S[:, t]] += 1**'
- en: 'Else:'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则：
- en: '*F[Samples][**S[:, t]] = 1* (both these operations are immediate with Python
    dictionaries)'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*F[Samples][**S[:, t]] = 1*（这两个操作在 Python 字典中都是立即完成的）'
- en: Create a vector *P[Sampled]* with shape *(**N, 1)*.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个形状为 *(**N, 1)* 的向量 *P[Sampled]*。
- en: Set *P[Sampled][i, 0] = F[Samples][**i]/N*.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *P[Sampled][i, 0] = F[Samples][**i]/N* 设置。
- en: 'From a mathematical viewpoint, we are first creating a frequency vector *F[Samples](x[1],
    x[2], x[3], ..., x[N]; N[Samples]**)* and then we approximate the full joint probability
    considering *N[Samples] → ∞*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学观点来看，我们首先创建一个频率向量 *F[Samples](x[1], x[2], x[3], ..., x[N]; N[Samples]**)，然后我们考虑
    *N[Samples] → ∞* 来近似全联合概率：
- en: '![](img/66ba24b0-7188-465c-925c-17d0b4d179e2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66ba24b0-7188-465c-925c-17d0b4d179e2.png)'
- en: Example of direct sampling
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接采样的例子
- en: 'We can now implement this algorithm in Python. Let''s start by defining the
    sample methods using the NumPy function `np.random.binomial(1, p)`, which draws
    a sample from a *Bernoulli* distribution with probability `p`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以用Python实现这个算法。让我们首先定义使用NumPy函数 `np.random.binomial(1, p)` 的样本方法，该函数从概率为
    `p` 的**伯努利分布**中抽取样本：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'At this point, we can implement the main cycle. As the variables are Boolean,
    the total number of probabilities is 16, so we set `Nsamples` to `5000` (smaller
    values are also acceptable):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以实现主循环。由于变量是布尔值，所以概率总数为16，因此我们将`Nsamples`设置为`5000`（较小的值也可以接受）：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When the sampling is complete, it''s possible to extract the full joint probability:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当采样完成时，可以提取完整的联合概率：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also query the model. For example, we could be interested in *P(X[4]=True)*.
    We can do this by looking for all the elements where *X[4]=True*, and summing
    up the relative probabilities:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查询模型。例如，我们可能对 *P(X[4]=True)* 感兴趣。我们可以通过查找所有 *X[4]=True* 的元素，并求和相应的概率来实现这一点：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This value is coherent with the definition of *X[4]*, which is always *p >=
    0.5*. The reader can try to change the values and repeat the simulation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值与 *X[4]* 的定义是一致的，*X[4]* 总是 *p >= 0.5*。读者可以尝试更改值并重复模拟。
- en: A gentle introduction to Markov chains
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫链的温和介绍
- en: In order to discuss the MCMC algorithms, it's necessary to introduce the concept
    of Markov chains. In fact, while the direct sample method draws samples without
    any particular order, the MCMC strategies draw a sequence of samples according
    to a precise transition probability from a sample to the following one.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论MCMC算法，有必要引入马尔可夫链的概念。实际上，虽然直接采样方法无序地抽取样本，但MCMC策略根据从样本到下一个样本的精确转移概率抽取一系列样本。
- en: 'Let''s consider a time-dependent random variable *X(t)*, and let''s assume
    a discrete time sequence **X[1]**, **X[2]**, ..., **X[t]**, **X[t+1]**, ... where
    **X[t]** represents the value assumed at time *t*. In the following diagram, there''s
    a schematic representation of this sequence:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个时间相关的随机变量 *X(t)*，并假设一个离散的时间序列 **X[1]**, **X[2]**, ..., **X[t]**, **X[t+1]**,
    ... 其中 **X[t]** 表示在时间 *t* 时的取值。在以下图中，有这个序列的示意图：
- en: '![](img/69b55183-7574-40b2-9b1e-96e2d4cf5a3e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69b55183-7574-40b2-9b1e-96e2d4cf5a3e.png)'
- en: Structure of a generic Markov chain
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通用马尔可夫链的结构
- en: 'We can suppose to have *N* different states *s[i]* for *i=1..N*, therefore
    it''s possible to consider the probability *P(X[t]=s[i]|X[t-1]=s[j], ..., X[1]=s[p])*.
    *X(t)* is defined as a **first-order Markov process** if:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设有 *N* 个不同的状态 *s[i]* 对于 *i=1..N*，因此可以考虑到概率 *P(X[t]=s[i]|X[t-1]=s[j], ...,
    X[1]=s[p])*。如果 *X(t)* 被定义为**一阶马尔可夫过程**，则：
- en: '![](img/4414956f-285c-4f5d-bf7a-7de1a5f60c52.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4414956f-285c-4f5d-bf7a-7de1a5f60c52.png)'
- en: 'In other words, in a Markov process (from now on, we omit *first-order*, even
    if there are cases when it''s useful to consider more previous states), the probability
    that *X(t)* is in a certain state depends only on the state assumed in the previous
    time instant. Therefore, we can define a **transition probability **for every
    couple *i*, *j*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在马尔可夫过程中（从现在起，我们省略**一阶**，即使在某些情况下考虑更多先前状态是有用的），*X(t)* 处于某个状态的概率只取决于前一时间瞬间所假设的状态。因此，我们可以为每一对
    *i*，*j* 定义一个**转移概率**：
- en: '![](img/a16ad93b-a206-445b-9ffb-42f3407edb16.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a16ad93b-a206-445b-9ffb-42f3407edb16.png)'
- en: 'Considering all the couples *(i, j)*, it''s also possible to build a transition
    probability matrix *T(i, j) = P(i → j)*. The marginal probability that *X[t]=s[i]*
    using a standard notation is defined as:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑所有对 *(i, j)*，也可以构建一个转移概率矩阵 *T(i, j) = P(i → j)*。使用标准符号，*X[t]=s[i]* 的边缘概率定义为：
- en: '![](img/17cc4835-7ef7-4686-a2e4-124c42e4a982.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17cc4835-7ef7-4686-a2e4-124c42e4a982.png)'
- en: 'At this point, it''s easy to prove (**Chapman-Kolmogorov** equation) that:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，很容易证明（**查普曼-科尔莫哥洛夫方程**）：
- en: '![](img/ee16d959-98d0-4b92-9ceb-7a345491283c.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee16d959-98d0-4b92-9ceb-7a345491283c.png)'
- en: 'In the previous expression, in order to compute *π[i](t+1)*, we need to sum
    over all possible previous states, considering the relative transition probability.
    This operation can be rewritten in matrix form, using a vector *π(t)* containing
    all states and the transition probability matrix *T* (the uppercase superscript
    *T* means that the matrix is transposed). The evolution of the chain can be computed
    recursively:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，为了计算 *π[i](t+1)*，我们需要对所有可能的前一个状态进行求和，考虑相对转移概率。这个操作可以用矩阵形式重写，使用包含所有状态的向量
    *π(t)* 和转移概率矩阵 *T*（大写上标 *T* 表示矩阵是转置的）。链的演变可以通过递归计算：
- en: '![](img/d9526323-5850-4106-aa9d-b2bddc9343fa.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d9526323-5850-4106-aa9d-b2bddc9343fa.png)'
- en: 'For our purposes, it''s important to consider Markov chains that are able to
    reach a *stationary distribution* *π[s]*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的来说，考虑能够达到 *stationary distribution* *π[s]* 的马尔可夫链是很重要的：
- en: '![](img/66b29160-1d3f-4c7c-ae04-157fcd2ef634.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66b29160-1d3f-4c7c-ae04-157fcd2ef634.png)'
- en: In other words, the state does not depend on the initial condition *π(1)*, and
    it's no longer able to change. The stationary distribution is unique if the underlying
    Markov process is *ergodic*. This concept means that the process has the same
    properties if averaged over time (which is often impossible), or averaged vertically (freezing
    the time) over the states (which is simpler in the majority of cases).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，状态不依赖于初始条件 *π(1)*，并且它不再能够改变。如果基础马尔可夫过程是 *ergodic* 的，那么稳态分布是唯一的。这个概念意味着，如果平均时间（这通常是不可能的）或垂直平均（冻结时间）在状态上（在大多数情况下这更简单），过程具有相同的属性。
- en: 'The process of ergodicity for Markov chains is assured by two conditions. The
    first is aperiodicityfor all states, which means that it is impossible to find
    a positive number *p* so that the chain returns in the same state sequence after
    a number of instants equal to a multiple of *p*. The second condition is that
    all states must be positive recurrent: this means that, given a random variable *N[instants]**(i)*,
    describing the number of time instants needed to return to the state *s[i]*, *E[N[instants](i)]
    < ∞*; therefore, potentially, all the states can be revisited in a finite time.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链的遍历过程由两个条件保证。第一个条件是所有状态都是非周期的，这意味着不可能找到一个正数 *p*，使得链在经过等于 *p* 的倍数个瞬间后回到相同的状态序列。第二个条件是所有状态都必须是正
    recurrent 的：这意味着，给定一个随机变量 *N[instants]**(i)*，描述了返回到状态 *s[i]* 所需的时间瞬间数，*E[N[instants](i)]
    < ∞*；因此，理论上，所有状态都可以在有限的时间内重新访问。
- en: 'The reason why we need the ergodicity condition, and hence the existence of
    a unique stationary distribution, is that we are considering the sampling processes
    modeled as Markov chains, where the next value is sampled according to the current
    state. The transition from one state to another is done in order to find better
    samples, as we''re going to see in the Metropolis-Hastings sampler, where we can
    also decide to reject a sample and keep the chain in the same state. For this
    reason, we need to be sure that the algorithms converge to the unique stable distribution
    (that approximates the real full joint distribution of our Bayesian network).
    It''s possible to prove that a chain always reaches a stationary distribution
    if:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要遍历性条件，以及因此存在唯一稳态分布的原因，是因为我们正在考虑将采样过程建模为马尔可夫链，其中下一个值是根据当前状态采样的。从一个状态到另一个状态的转换是为了找到更好的样本，正如我们将在
    Metropolis-Hastings 采样器中看到的那样，我们也可以选择拒绝一个样本并保持链处于相同的状态。因此，我们需要确保算法收敛到唯一的稳定分布（该分布近似于我们的贝叶斯网络的真实完整联合分布）。可以证明，如果：
- en: '![](img/ec1e6164-4511-457d-9598-93c25d184f80.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec1e6164-4511-457d-9598-93c25d184f80.png)'
- en: The previous equation is called detailed balance, and implies the reversibility
    of the chain. Intuitively, it means that the probability of finding the chain
    in the state *A* times the probability of a transition to the state *B* is equal
    to the probability of finding the chain in the state *B* times the probability
    of a transition to *A*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程被称为详细平衡，它意味着链的可逆性。直观地说，这意味着链处于状态 *A* 的概率乘以转移到状态 *B* 的概率等于链处于状态 *B* 的概率乘以转移到
    *A* 的概率。
- en: For both methods that we are going to discuss, it's possible to prove that they
    satisfy the previous condition, and therefore their convergence is assured.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们将要讨论的两种方法，可以证明它们满足上述条件，因此它们的收敛性得到保证。
- en: Gibbs sampling
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 吉布斯抽样
- en: 'Let''s suppose that we want to obtain the full joint probability for a Bayesian
    network *P(x[1], x[2], x[3], ..., x[N]**)*; however, the number of variables is
    large and there''s no way to solve this problem easily in a closed form. Moreover,
    imagine that we would like to get some marginal distribution, such as *P(x[2])*,
    but to do so we should integrate the full joint probability, and this task is
    even harder. Gibbs sampling allows approximating of all marginal distributions
    with an iterative process. If we have *N* variables, the algorithm proceeds with
    the following steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要获得贝叶斯网络 *P(x[1], x[2], x[3], ..., x[N]**) 的完整联合概率；然而，变量的数量很大，无法以封闭形式轻松解决这个问题。此外，想象一下，如果我们想得到一些边缘分布，例如 *P(x[2])*，但为了做到这一点，我们需要对整个联合概率进行积分，这项任务甚至更难。吉布斯抽样允许通过迭代过程近似所有边缘分布。如果我们有
    *N* 个变量，算法按照以下步骤进行：
- en: Initialize the variable *N[Iterations]*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变量 *N[迭代次数]*
- en: Initialize a vector *S* with shape *(N, N[Iterations])*
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个形状为 *(N, N[迭代次数])* 的向量 *S*
- en: Randomly initialize *x[1]^((0)), x[2]^((0)), ..., x[N]^((0))* (the superscript
    index is referred to the iteration)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化 *x[1]^((0)), x[2]^((0)), ..., x[N]^((0))* (上标索引指的是迭代次数)
- en: 'For *t=1* to *N[Iterations]*:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=1* 到 *N[迭代次数]*：
- en: Sample *x[1]^((t))* from *p(x[1]|x[2]^((t-1)), x[3]^((t-1)), ..., x[N]^((t-1)))*
    and store it in *S[0, t]*
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *p(x[1]|x[2]^((t-1)), x[3]^((t-1)), ..., x[N]^((t-1)))* 中采样 *x[1]^((t))* 并将其存储在
    *S[0, t]*
- en: Sample *x[2]^((t))* from *p(**x[2]|x[1]^((t)), x[3]^((t-1)), ..., x[N]^((t-1))**)* and
    store it in *S[1, t]*
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *p(**x[2]|x[1]^((t)), x[3]^((t-1)), ..., x[N]^((t-1))**)* 中采样 *x[2]^((t))*
    并将其存储在 *S[1, t]*
- en: Sample *x[3]^((t))* from *p(**x[3]|x[1]^((t)), x[2]^((t)), ..., x[N]^((t-1))**)* and
    store it in *S[2, t]*
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *p(**x[3]|x[1]^((t)), x[2]^((t)), ..., x[N]^((t-1))**)* 中采样 *x[3]^((t))* 并将其存储在
    *S[2, t]*
- en: '...'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '...'
- en: Sample *x[N]^((t))* from *p(**x[N]|x[1]^((t)), x[2]^((t)), ..., x[N-1]^((t))**)* and
    store it in *S[N-1, t]*
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *p(**x[N]|x[1]^((t)), x[2]^((t)), ..., x[N-1]^((t))**)* 中采样 *x[N]^((t))* 并将其存储在
    *S[N-1, t]*
- en: At the end of the iterations, vector *S* will contain *N*[*Iterations* ]samples
    for each distribution. As we need to determine the probabilities, it's necessary
    to proceed like in the direct sampling algorithm, counting the number of single
    occurrences and normalizing dividing by *N[Iterations]*. If the variables are
    continuous, it's possible to consider intervals, counting how many samples are
    contained in each of them.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代结束时，向量 *S* 将包含每个分布的 *N*[*迭代次数*] 个样本。由于我们需要确定概率，因此有必要像直接采样算法那样进行操作，计算单次出现的次数，并通过除以
    *N[迭代次数]* 进行归一化。如果变量是连续的，可以考虑区间，计算每个区间包含的样本数量。
- en: For small networks, this procedure is very similar to direct sampling, except
    that when working with very large networks, the sampling process could become
    slow; however, the algorithm can be simplified after introducing the concept of
    the Markov blanket of *X[i]*, which is the set of random variables that are predecessors,
    successors, and successors' predecessors of *X[i]* (in some books, they use the
    terms *parents* and *children*). In a Bayesian network, a variable *X[i]* is a
    conditional independent of all other variables given its Markov blanket. Therefore,
    if we define the function *MB(X[i])*, which returns the set of variables in the
    blanket, the generic sampling step can be rewritten as *p(x[i]|MB(X[i]))*, and
    there's no more need to consider all the other variables.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小型网络，此过程与直接采样非常相似，但是当处理非常大的网络时，采样过程可能会变得缓慢；然而，在引入 *X[i]* 的马尔可夫层概念之后，算法可以简化。马尔可夫层是
    *X[i]* 的前驱、后继及其后继的前驱的随机变量集合（在某些书中，他们使用术语 *父母* 和 *子女*）。在贝叶斯网络中，变量 *X[i]* 在其马尔可夫层给定条件下与所有其他变量条件独立。因此，如果我们定义函数
    *MB(X[i])*，它返回层中的变量集合，则通用采样步骤可以重写为 *p(x[i]|MB(X[i]))*，并且不再需要考虑所有其他变量。
- en: 'To understand this concept, let''s consider the network shown in the following
    diagram:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个概念，让我们考虑以下图中显示的网络：
- en: '![](img/1f40b1fa-6e12-401a-a769-3bd4b0438600.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f40b1fa-6e12-401a-a769-3bd4b0438600.png)'
- en: Bayesian network for the Gibbs sampling example
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 吉布斯抽样的贝叶斯网络示例
- en: 'The Markov blankets are:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫层包括：
- en: '*MB(X[1])* = *{ X[2], X[3] }*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MB(X[1])* = *{ X[2], X[3] }*'
- en: '*MB(X[2])* = *{ X[1, ]X[3], X[4] }*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MB(X[2])* = *{ X[1, ]X[3], X[4] }*'
- en: '*MB(X[3])* = *{ X[1, ]X[2], X[4], X[5] }*'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MB(X[3])* = *{ X[1, ]X[2], X[4], X[5] }*'
- en: '*MB(X[4])* = *{ X[3] }*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MB(X[4])* = *{ X[3] }*'
- en: '*MB(X[5])* = *{ X[3] }*'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MB(X[5])* = *{ X[3] }*'
- en: '*MB(X[6])* = *{ X[2] }*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MB(X[6])* = *{ X[2] }*'
- en: 'In general, if *N* is very large, the cardinality of *|MB(X[i])| << N*, thus
    simplifying the process (the *vanilla* Gibbs sampling needs *N-1* conditions for
    each variable). We can prove that the Gibbs sampling generates samples from a
    Markov chain that is in detailed balance:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果 *N* 非常大，那么 *|MB(X[i])|* 的基数远小于 *N*，从而简化了过程（*vanilla* 吉布斯采样需要对每个变量有
    *N-1* 个条件）。我们可以证明吉布斯采样生成的样本来自一个处于详细平衡状态的马尔可夫链：
- en: '![](img/30105cca-76a4-4cad-bf32-b2c541feed6f.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/30105cca-76a4-4cad-bf32-b2c541feed6f.png)'
- en: 'Therefore, the procedure converges to the unique stationary distribution. This
    algorithm is quite simple; however, its performance is not excellent, because
    the random walks are not tuned up in order to explore the right regions of the
    state-space, where the probability to find good samples is high. Moreover, the
    trajectory can also return to bad states, slowing down the whole process. An alternative
    (also implemented by PyMC3 for continuous random variables) is the **No-U-Turn**
    algorithm, which we don''t discuss in this book. The reader interested in this
    topic can find a full description in *The* *No-U-Turn Sampler: Adaptively Setting
    Path Lengths in Hamiltonian Monte Carlo*, *Hoffmann M. D.*, *Gelman A.*, *arXiv:1111.4246*.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，该过程收敛到唯一的平稳分布。这个算法相当简单；然而，其性能并不出色，因为随机游走没有被调整以探索状态空间中的正确区域，在这些区域找到好样本的概率很高。此外，轨迹也可能返回到不良状态，从而减慢整个过程。一个替代方案（也被
    PyMC3 用于连续随机变量）是 **No-U-Turn** 算法，我们在这本书中不讨论。对这一主题感兴趣的读者可以在 *The No-U-Turn Sampler:
    Adaptively Setting Path Lengths in Hamiltonian Monte Carlo* 一书中找到完整的描述，作者为 *Hoffmann
    M. D.*，*Gelman A.*，*arXiv:1111.4246*。'
- en: Metropolis-Hastings sampling
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Metropolis-Hastings 采样
- en: We have seen that the full joint probability distribution of a Bayesian network
    *P(x[1], x[2], x[3], ..., x[N]**)* can become intractable when the number of variables
    is large. The problem can become even harder when it's needed to marginalize it
    in order to obtain, for example, *P(x[i])*, because it's necessary to integrate
    a very complex function. The same problem happens when applying the Bayes' theorem
    in simple cases. Let's suppose we have the expression *p(A|B) = K · P(B|A)P(A)*.
    I've expressly inserted the normalizing constant *K*, because if we know it, we
    can immediately obtain the posterior probability; however, finding it normally
    requires integrating *P(B|A)P(A)*, and this operation can be impossible in closed
    form.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，当变量数量很大时，贝叶斯网络的完整联合概率分布 *P(x[1], x[2], x[3], ..., x[N]*) 可能变得难以处理。当需要对其进行边缘化以获得，例如，*P(x[i]*)
    时，问题可能变得更加困难，因为这需要积分一个非常复杂的功能。在简单情况下应用贝叶斯定理时也会出现相同的问题。假设我们有表达式 *p(A|B) = K · P(B|A)P(A)*。我明确地插入了归一化常数
    *K*，因为我们知道它，我们可以立即获得后验概率；然而，通常需要找到它需要积分 *P(B|A)P(A)*，而这个操作可能是无法用封闭形式表示的。
- en: The Metropolis-Hastings algorithm can help us in solving this problem. Let's
    imagine that we need to sample from *P(x[1], x[2], x[3], ..., x[N]**)*, but we
    know this distribution up to a normalizing constant, so *P(x[1], x[2], x[3], ...,
    x[N]) ∝ g(x[1], x[2], x[3], ..., x[N])*. For simplicity, from now on we collapse
    all variables into a single vector, so *P(x) ∝ g(x)*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings 算法可以帮助我们解决这个问题。让我们想象一下，我们需要从 *P(x[1], x[2], x[3], ..., x[N]*)
    中采样，但我们只知道这个分布直到归一化常数，所以 *P(x[1], x[2], x[3], ..., x[N]) ∝ g(x[1], x[2], x[3],
    ..., x[N]*)。为了简化，从现在开始我们将所有变量合并成一个单向量，所以 *P(x) ∝ g(x)*。
- en: Let's take another distribution *q(x'|x^((i-1)))*, which is called **candidate-generating
    distribution**. There are no particular restrictions on this choice, only that
    *q* is easy to sample. In some situations, *q* can be chosen as a function very
    similar to the distribution *p(x)*, which is our target, while in other cases,
    it's possible to use a normal distribution with mean equal to *x^((i-1))*. As
    we're going to see, this function acts as a proposal-generator, but we're not
    obliged to accept all the samples drawn from it therefore, potentially any distribution
    with the same domain of *P(X)* can be employed. When a sample is accepted, the
    Markov chain transitions to the next state, otherwise it remains in the current
    one. This decisional process is based on the idea that the sampler must explore
    the most important state-space regions and discard the ones where the probability
    to find good samples is low.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再取另一个分布*q(x'|x^((i-1)))*，这被称为**候选生成分布**。对这个选择没有特别的限制，只需*q*易于采样即可。在某些情况下，*q*可以选择与目标分布*p(x)*非常相似的功能，而在其他情况下，可以使用均值为*x^((i-1))*的正态分布。正如我们将要看到的，这个函数充当提议生成器，但我们没有义务接受从这个分布中抽取的所有样本，因此，可以采用具有相同*P(X)*域的任何分布。当一个样本被接受时，马尔可夫链过渡到下一个状态，否则它保持在当前状态。这个决策过程基于采样器必须探索最重要的状态空间区域并丢弃那些找到好样本概率低的区域的观点。
- en: 'The algorithm proceeds with the following steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 算法按照以下步骤进行：
- en: Initialize the variable *N[Iterations]*
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变量*N[迭代]*
- en: Initialize *x^((0))* randomly
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化*x^((0))*
- en: 'For *t=1* to *N**[Iterations]*:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*t=1*到*N[迭代]*：
- en: Draw a candidate sample *x'* from *q(x'|x^((i-1))**)*
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*q(x'|x^((i-1)))*中抽取候选样本*x'*
- en: Compute the following value:![](img/e2dfa594-aaac-4af2-a7b0-133f55e1fd30.png)
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算以下值：![](img/e2dfa594-aaac-4af2-a7b0-133f55e1fd30.png)
- en: 'If *α ≥ 1*:'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*α ≥ 1*：
- en: Accept the sample *x^((t)) = x'*
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受样本*x^((t)) = x'*
- en: 'Else if *0 < α < 1*:'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则如果*0 < α < 1*：
- en: Accept the sample *x^((t))** = x'* with probability *α*; or
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以概率*α*接受样本*x^((t)) = x'*；或者
- en: Reject the sample *x'* setting *x^((t)) = x^((t-1))* with probability *1 - **α*
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以概率*1 - α*拒绝样本*x'*，设置*x^((t)) = x^((t-1))*
- en: It's possible to prove (the proof will be omitted, but it's available in *Markov
    Chain Monte Carlo and Gibbs Sampling, **Walsh B., Lecture Notes for EEB 596z*)
    that the transition probability of the Metropolis-Hastings algorithm satisfies
    the detailed balance equation, and therefore the algorithm converges to the true
    posterior distribution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明（证明将省略，但可在*马尔可夫链蒙特卡洛和Gibbs抽样，**Walsh B.，EEB 596z课程讲义**）Metropolis-Hastings算法的转移概率满足详细平衡方程，因此算法收敛到真实的后验分布。
- en: Example of Metropolis-Hastings sampling
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Metropolis-Hastings采样示例
- en: We can implement this algorithm to find the posterior distribution *P(A|B)*
    given the product of *P(B|A)* and *P(A)*, without considering the normalizing
    constant that requires a complex integration.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此算法实现为找到给定*P(B|A)*和*P(A)*的乘积的后验分布*P(A|B)*，而不考虑需要复杂积分的归一化常数。
- en: 'Let''s suppose that:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设：
- en: '![](img/54a4353b-48d1-4357-90cd-01824204939f.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54a4353b-48d1-4357-90cd-01824204939f.png)'
- en: 'Therefore, the resulting *g(x)* is:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，得到的*g(x)*是：
- en: '![](img/c55f8193-4471-4ed9-907c-e98992b1a0f5.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c55f8193-4471-4ed9-907c-e98992b1a0f5.png)'
- en: To solve this problem, we adopt the random walk Metropolis-Hastings, which consists
    of choosing *q ∼ Normal(μ=x^((t-1)))*. This choice allows simplifying the value
    *α*, because the two terms *q(x^((t-1))|x')* and *q(x'|x^((t-1)))* are equal (thanks
    to the symmetry around the vertical axis passing through *x[mean]*) and can be
    canceled out, so *α* becomes the ratio between *g(x')* and *g(x^((t-1))*).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们采用随机游走Metropolis-Hastings方法，该方法包括选择*q ∼ 正态分布(μ=x^((t-1)))*. 这个选择允许简化值*α*，因为两个项*q(x^((t-1))|x')*和*q(x'|x^((t-1)))*是相等的（多亏了通过*x[mean]*的垂直轴的对称性）并且可以相互抵消，所以*α*变成了*g(x')*和*g(x^((t-1)))*的比值。
- en: 'The first thing is defining the functions:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事是定义函数：
- en: '[PRE4]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we can start our sampling process with 100,000 iterations and *x^((0))
    = 1.0*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始我们的采样过程，使用100,000次迭代和*x^((0)) = 1.0*：
- en: '[PRE5]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get a representation of the posterior distribution, we need to create a
    histogram through the NumPy function `np.histogram()`, which accepts an array
    of values and the number of desired intervals (`bins`); in our case, we set `100`
    intervals:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到后验分布的表示，我们需要通过NumPy函数`np.histogram()`创建一个直方图，该函数接受一个值数组以及所需的区间数（`bins`）；在我们的情况下，我们设置了`100`个区间：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The resulting plot of *p(x)* is shown in the following graph:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x)*的结果图示如下：'
- en: '![](img/7d6fb2c3-0a41-45e3-93e8-2ca302dbbf90.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/7d6fb2c3-0a41-45e3-93e8-2ca302dbbf90.png)'
- en: Sampled probability density function
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样概率密度函数
- en: Sampling example using PyMC3
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyMC3的抽样示例
- en: '**PyMC3** is a powerful Python Bayesian framework that relies on Theano to
    perform high-speed computations (see the information box at the end of this paragraph
    for the installation instructions). It implements all the most important continuous
    and discrete distributions, and performs the sampling process mainly using the
    No-U-Turn and Metropolis-Hastings algorithms. For all the details about the API
    (distributions, functions, and plotting utilities), I suggest visiting the documentation
    home page [http://docs.pymc.io/index.html](http://docs.pymc.io/index.html), where
    it''s also possible to find some very intuitive tutorials.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyMC3**是一个强大的Python贝叶斯框架，它依赖于Theano来执行高速计算（有关安装说明，请参阅本段末尾的信息框）。它实现了所有最重要的连续和离散分布，并主要使用No-U-Turn和Metropolis-Hastings算法进行抽样过程。有关API（分布、函数和绘图实用工具）的所有详细信息，我建议访问文档主页[http://docs.pymc.io/index.html](http://docs.pymc.io/index.html)，在那里还可以找到一些非常直观的教程。'
- en: 'The example we want to model and simulate is based on this scenario: a daily
    flight from London to Rome has a scheduled departure time at 12:00 am, and a standard
    flight time of two hours. We need to organize the operations at the destination
    airport, but we don''t want to allocate resources when the plane hasn''t landed
    yet. Therefore, we want to model the process using a Bayesian network and considering
    some common factors that can influence the arrival time. In particular, we know
    that the onboarding process can be longer than expected, as well as the refueling
    one, even if they are carried out in parallel. London air traffic control can
    also impose a delay, and the same can happen when the plane is approaching Rome.
    We also know that the presence of rough weather can cause another delay due to
    a change of route. We can summarize this analysis with the following plot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要模拟和仿真的例子基于以下场景：从伦敦到罗马的每日航班有一个预定起飞时间为午夜12:00，标准飞行时间为两小时。我们需要组织目的地机场的运营，但我们不希望在飞机尚未降落之前分配资源。因此，我们希望使用贝叶斯网络来模拟这个过程，并考虑一些可能影响到达时间的常见因素。特别是，我们知道登机过程可能会比预期更长，以及加油过程，即使它们是并行进行的。伦敦空中交通管制也可以造成延误，当飞机接近罗马时也可能发生同样的情况。我们还知道，恶劣天气的存在可能会因为路线改变而造成另一个延误。我们可以用以下图表总结这个分析：
- en: '![](img/db234ff4-d869-4c51-80bb-7d28a56e0783.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/db234ff4-d869-4c51-80bb-7d28a56e0783.png)'
- en: Bayesian network representing the air traffic control problem
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表示空中交通管制问题的贝叶斯网络
- en: 'Considering our experience, we decide to model the random variables using the
    following distributions:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的经验，我们决定使用以下分布来模拟随机变量：
- en: '*Passenger onboarding ∼ Wald(μ=0.5, λ=0.2)*'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*乘客登机∼Wald(μ=0.5, λ=0.2)*'
- en: '*Refueling ∼ Wald(μ=0.25, λ=0.5)*'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加油∼Wald(μ=0.25, λ=0.5)*'
- en: '*Departure traffic delay ∼ Wald(μ=0.1, λ=0.2)*'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出发交通延误∼Wald(μ=0.1, λ=0.2)*'
- en: '*Arrival traffic delay ∼ Wald(μ=0.1, λ=0.2)*'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*到达交通延误∼Wald(μ=0.1, λ=0.2)*'
- en: '*Departure time = 12 + Departure traffic delay + max(Passenger onboarding,
    Refueling)*'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出发时间 = 12 + 出发交通延误 + max(乘客登机, 加油)*'
- en: '*Rough weather ∼ Bernoulli(p=0.35)*'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恶劣天气∼伯努利(p=0.35)*'
- en: '*Flight time ∼ Exponential(λ=0.5 - (0.1 · Rough weather))* (The output of a
    Bernoulli distribution is *0* or *1* corresponding to False and True)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*飞行时间∼指数(λ=0.5 - (0.1 · 恶劣天气))(伯努利分布的输出为*0*或*1*，分别对应False和True)*'
- en: '*Arrival time = Departure time + Flight time + Arrival traffic delay*'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*到达时间 = 出发时间 + 飞行时间 + 到达交通延误*'
- en: 'The probability density functions are:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 概率密度函数如下：
- en: '![](img/6b14d7f2-e3a1-4226-b874-59a38946bbd5.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/6b14d7f2-e3a1-4226-b874-59a38946bbd5.png)'
- en: '`Departure Time` and `Arrival Time` are functions of random variables, and
    the parameter λ of `Flight Time` is also a function of `Rough Weather`.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`出发时间`和`到达时间`是随机变量的函数，`飞行时间`的参数λ也是`恶劣天气`的函数。'
- en: Even if the model is not very complex, the direct inference is rather inefficient,
    and therefore we want to simulate the process using PyMC3.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型不是很复杂，直接的推理效率也相当低，因此我们希望使用PyMC3来模拟这个过程。
- en: 'The first step is to create a `model` instance:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个`model`实例：
- en: '[PRE7]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From now on, all operations must be performed using the context manager provided
    by the `model` variable. We can now set up all the random variables of our Bayesian
    network:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，所有操作都必须使用由`model`变量提供的上下文管理器来执行。我们现在可以设置贝叶斯网络的所有随机变量：
- en: '[PRE8]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We have imported two namespaces, `pymc3.distributions.continuous` and `pymc3.distributions.discrete`,
    because we are using both kinds of variable. Wald and exponential are continuous
    distributions, while `Bernoulli` is discrete. In the first three rows, we declare
    the variables `passenger_onboarding`, `refueling`, and `departure_traffic_delay`.
    The structure is always the same: we need to specify the class corresponding to
    the desired distribution, passing the name of the variable and all the required
    parameters.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了两个命名空间，`pymc3.distributions.continuous`和`pymc3.distributions.discrete`，因为我们正在使用这两种类型的变量。Wald和指数是连续分布，而`Bernoulli`是离散的。在前三行中，我们声明了变量`passenger_onboarding`、`refueling`和`departure_traffic_delay`。结构始终相同：我们需要指定对应于所需分布的类，传递变量的名称和所有必需的参数。
- en: The `departure_time` variable is declared as `pm.Deterministic`. In PyMC3, this
    means that, once all the random elements have been set, its value becomes completely
    determined. Indeed, if we sample from `departure_traffic_delay`, `passenger_onboarding`,
    and `refueling`, we get a determined value for `departure_time`. In this declaration,
    we've also used the utility function `pmm.switch`, which operates a binary choice
    based on its first parameter (for example, if *A > B*, return *A*, else return
    *B*).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`departure_time`变量被声明为`pm.Deterministic`。在PyMC3中，这意味着一旦所有随机元素都已设置，其值就完全确定。实际上，如果我们从`departure_traffic_delay`、`passenger_onboarding`和`refueling`中采样，我们将得到`departure_time`的确定值。在这个声明中，我们还使用了实用函数`pmm.switch`，它根据其第一个参数进行二进制选择（例如，如果*A
    > B*，则返回*A*，否则返回*B*）。'
- en: The other variables are very similar, except for `flight_time`, which is an
    exponential variable with a parameter *λ*, which is a function of another variable
    (`rough_weather`). As a Bernoulli variable outputs *1* with probability *p* and
    *0* with probability *1 - p*, *λ = 0.4* if there's rough weather, and *0.5* otherwise.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其他变量非常相似，除了`flight_time`，它是一个具有参数*λ*的指数变量，*λ*是另一个变量（`rough_weather`）的函数。作为一个伯努利变量，以概率*p*输出*1*，以概率*1
    - p*输出*0*，当有恶劣天气时，*λ = 0.4*，否则为*0.5*。
- en: 'Once the model has been set up, it''s possible to simulate it through a sampling
    process. PyMC3 picks the best sampler automatically, according to the type of
    variables. As the model is not very complex, we can limit the process to `500`
    samples:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型设置完成，就可以通过采样过程对其进行模拟。PyMC3会根据变量的类型自动选择最佳采样器。由于模型并不复杂，我们可以将过程限制在`500`个样本：
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output can be analyzed using the built-in `pm.traceplot()` function, which
    generates the plots for each of the sample''s variables. The following graph shows
    the detail of one of them:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用内置的`pm.traceplot()`函数分析输出，该函数为每个样本的变量生成图表。以下图表显示了其中一个的详细情况：
- en: '![](img/2a2fae86-4a34-4d33-8fcb-d95901ceb486.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a2fae86-4a34-4d33-8fcb-d95901ceb486.png)'
- en: Distribution and samples for the arrival time random variable
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 到达时间随机变量的分布和样本
- en: 'The right column shown the samples generated for the random variable (in this
    case, the arrival time), while the left column shows the relative frequencies.
    This plot can be useful to have a visual confirmation of our initial ideas; in
    fact, the arrival time has the majority of its mass concentrated in the interval
    14:00 to 16:00 (the numbers are always decimal, so it''s necessary to convert
    the times); however, we should integrate to get the probabilities. Instead, through
    the `pm.summary()` function, PyMC3 provides a statistical summary that can help
    us in making the right decisions. In the following snippet, the output containing
    the summary of a single variable is shown:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 右列显示了为随机变量（在这种情况下，到达时间）生成的样本，而左列显示了相对频率。这个图可以用来验证我们的初步想法；实际上，到达时间的大部分质量集中在14:00到16:00（数字总是十进制，因此需要将时间转换为十进制）；然而，我们应该进行积分以得到概率。相反，通过`pm.summary()`函数，PyMC3提供了一个统计摘要，可以帮助我们做出正确的决策。以下代码片段显示了单个变量的摘要输出：
- en: '[PRE10]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For each variable, it contains mean, standard deviation, Monte Carlo error,
    95% highest posterior density interval, and the posterior quantiles. In our case,
    we know that the plane will land at about 15:10 (`15.174`).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个变量，它包含均值、标准差、蒙特卡洛误差、95%最高后验密度区间和后验分位数。在我们的案例中，我们知道飞机将在大约15:10（`15.174`）着陆。
- en: This is only a very simple example to show the power of Bayesian networks. For
    deep insight, I suggest the book *Introduction to Statistical Decision Theory*,
    *Pratt J.*, *Raiffa H.*, *Schlaifer R.*, *The MIT Press*, where it's possible
    to study different Bayesian applications that are out of the scope of this book.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个非常简单的例子，用以展示贝叶斯网络的强大功能。为了深入了解，我建议阅读《统计决策理论导论》，作者 *Pratt J.*，*Raiffa H.*，*Schlaifer
    R.*，由 *MIT Press* 出版，其中可以研究这本书范围之外的多种贝叶斯应用。
- en: PyMC3 ([http://docs.pymc.io/index.html](http://docs.pymc.io/index.html)) can
    be installed using the `pip install -U pymc3` command. As it requires Theano (which
    is installed automatically), it's also necessary to provide it with a C/C++ compiler.
    I suggest using distributions such as Anaconda ([https://www.anaconda.com/download/](https://www.anaconda.com/download/)),
    which allows installing MinGW through the `conda install -c anaconda mingw` command.
    For any problems, on the website you can find detailed installation instructions.
    For further information on how to configure Theano to work with GPU support (the
    default installation is based on CPU NumPy algorithms), please visit this page: [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC3 ([http://docs.pymc.io/index.html](http://docs.pymc.io/index.html)) 可以使用
    `pip install -U pymc3` 命令安装。由于它需要 Theano（会自动安装），还需要提供一个 C/C++ 编译器。我建议使用 Anaconda
    ([https://www.anaconda.com/download/](https://www.anaconda.com/download/)) 这样的发行版，它允许通过
    `conda install -c anaconda mingw` 命令安装 MinGW。对于任何问题，您可以在网站上找到详细的安装说明。有关如何配置 Theano
    以支持 GPU（默认安装基于 CPU NumPy 算法）的更多信息，请访问此页面：[http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)。
- en: Hidden Markov Models (HMMs)
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏马尔可夫模型 (HMMs)
- en: 'Let''s consider a stochastic process *X(t)* that can assume *N* different states:
    *s[1], s[2], ..., s[N]* with first-order Markov chain dynamics. Let''s also suppose
    that we cannot observe the state of *X(t)*, but we have access to another process
    *O(t)*, connected to *X(t)*, which produces observable outputs (often known as
    **emissions**). The resulting process is called a **Hidden Markov Model** (**HMM**),
    and a generic schema is shown in the following diagram:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个可以假设 *N* 个不同状态的随机过程 *X(t)*：*s[1]*，*s[2]*，...，*s[N]*，具有一阶马尔可夫链动力学。我们还假设我们无法观察到
    *X(t)* 的状态，但我们有权访问另一个与 *X(t)* 相连的过程 *O(t)*，它产生可观察的输出（通常称为 **发射**）。这个结果过程被称为 **隐藏马尔可夫模型**（**HMM**），其通用架构如下图所示：
- en: '![](img/3f054b58-c9c5-4832-b900-76d03dd95007.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3f054b58-c9c5-4832-b900-76d03dd95007.png)'
- en: Structure of a generic Hidden Markov Model
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通用隐藏马尔可夫模型的结构
- en: 'For each hidden state *s[i]*, we need to define a transition probability *P(i →
    j)*, normally represented as a matrix if the variable is discrete. For the Markov
    assumption, we have:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个隐藏状态 *s[i]*，我们需要定义一个转移概率 *P(i → j)*，如果变量是离散的，通常表示为一个矩阵。对于马尔可夫假设，我们有：
- en: '![](img/ae2d037e-d1ac-4fbf-a67a-6fd10b65dcf1.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae2d037e-d1ac-4fbf-a67a-6fd10b65dcf1.png)'
- en: 'Moreover, given a sequence of observations *o[1], o[2], ..., o[M]*, we also
    assume the following assumption about the independence of the **emission probability**:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，给定一个观察序列 *o[1]*，*o[2]*，...，*o[M]*，我们还假设以下关于 **发射概率** 独立性的假设：
- en: '![](img/592ba42f-5436-4409-8576-8f415fd3cb99.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/592ba42f-5436-4409-8576-8f415fd3cb99.png)'
- en: In other words, the probability of the observation *o[i]* (in this case, we
    mean the value at time *i*) is conditioned only by the state of the hidden variable
    at time *i (x[i])*. Conventionally, the first state *x[0]* and the last one *x[Ending]*
    are never emittied, and therefore all the sequences start with the index *1* and
    end with an extra timestep corresponding to the final state.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，观察值 *o[i]*（在这种情况下，我们指的是时间 *i* 时的值）的概率仅由时间 *i* 的隐藏变量状态 *x[i]* 决定。传统上，第一个状态
    *x[0]* 和最后一个状态 *x[Ending]* 从不发射，因此所有序列都从索引 *1* 开始，并以一个对应最终状态的超时间步结束。
- en: HMMs can be employed in all those contexts where it's impossible to measure
    the state of a system (we can only model it as a stochastic variable with a known
    transition probability), but it's possible to access some data connected to it.
    An example can be a complex engine that is made up of a large number of parts.
    We can define some internal states and learn a transition probability matrix (we're
    going to learn how to do that), but we can only receive measures provided by specific
    sensors.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: HMMs可以应用于所有那些无法测量系统状态（我们只能将其建模为具有已知转移概率的随机变量）但可以访问与其相关的数据的情境。一个例子可以是一个由大量部件组成的复杂引擎。我们可以定义一些内部状态并学习一个转移概率矩阵（我们将学习如何做到这一点），但我们只能接收由特定传感器提供的测量值。
- en: Sometimes, even if not extremely realistic, but it's useful to include the Markov
    assumption and the emission probability independence into our model. The latter
    can be justified considering that we can sample all the *peak* emissions corresponding
    to precise states and, as the random process *O(t)* is implicitly dependent on
    *X(t)*, it's not unreasonable to think of it like a *pursuer* of *X(t)*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，即使不是非常现实，但将马尔可夫假设和发射概率独立性包含到我们的模型中是有用的。后者可以这样证明：我们可以采样所有对应于精确状态的*峰值*发射，由于随机过程*O(t)*隐式地依赖于*X(t)*，将其视为*X(t)*的*追逐者*是合情合理的。
- en: The Markov assumption holds for many real-life processes if either they are
    naturally first-order Markov ones, or if the states contain all the history needed
    to justify a transition. In other words, in many cases, if the state is *A*, then
    there's a transit to *B* and finally to *C*. We assume that when in *C*, the system
    moved from a state *(B)* that carries a part of the information provided by *A*.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些过程要么是自然的一阶马尔可夫过程，要么状态包含所有用于证明转移所需的历史，那么马尔可夫假设对许多现实生活中的过程成立。换句话说，在许多情况下，如果状态是*A*，那么会转移到*B*，最终转移到*C*。我们假设当在*C*时，系统从包含*A*提供部分信息的*(B)*状态移动过来。
- en: For example, if we are filling a tank, we can measure the level (the state of
    our system) at time *t*, *t+1*, ... If the water flow is modeled by a random variable
    because we don't have a stabilizer, we can find the probability that the water
    has reached a certain level at time *t*, *p(L[t]=x|L[t-1])*. Of course, it doesn't
    make sense to condition over all the previous states, because if the level is,
    for example, 80 m at time t-1, all the information needed to determine the probability
    of a new level (state) at time *t* is already contained in this state (80 m).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们正在填充一个水箱，我们可以在时间*t*，*t+1*，...测量水平（我们系统的状态）。由于我们没有稳定器，我们将水流建模为随机变量，我们可以找到水在时间*t*达到一定水平*p(L[t]=x|L[t-1])*的概率。当然，对所有先前状态进行条件化是没有意义的，因为如果水平在时间t-1是，例如，80米，那么确定时间*t*时新水平（状态）的概率所需的所有信息已经包含在这个状态（80米）中。
- en: 'At this point, we can start analyzing how to train a hidden Markov model, and
    how to determine the most likely hidden states given a sequence of observations.
    For simplicity, we call *A* the transition probability matrix, and *B* the matrix
    containing all *P(o[i]|x[t])*. The resulting model can be determined by the knowledge
    of those elements: *HMM = { A, B }*.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以开始分析如何训练一个隐马尔可夫模型，以及如何根据一系列观察值确定最可能的隐藏状态。为了简化，我们称*A*为转移概率矩阵，而*B*为包含所有*P(o[i]|x[t])*的矩阵。该模型可以通过这些元素的知识来确定：*HMM
    = { A, B }*。
- en: Forward-backward algorithm
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向-后向算法
- en: The **forward-backward algorithm** is a simple but effective method to find
    the transition probability matrix *T* given a sequence of observations *o[1],
    o[2], ..., o[t]*. The first step is called the *forward phase*, and consists of
    determining the probability of a sequence of observations *P(o[1], o[2], ...,
    o[Sequence Length]|A, B)*. This piece of information can be directly useful if
    we need to know the likelihood of a sequence and it's necessary, together with
    the *backward phase*, to estimate the structure (*A* and *B*) of the underlying
    HMM.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向-后向算法**是一种简单但有效的方法，用于根据一系列观察值*o[1], o[2], ..., o[t]*找到转移概率矩阵*T*。第一步被称为*前向阶段*，包括确定一系列观察值的概率*P(o[1],
    o[2], ..., o[序列长度]|A, B)*。如果我们需要知道序列的似然性，这部分信息可以直接使用，并且与*后向阶段*一起，可以用来估计潜在HMM的结构(*A*和*B*)。'
- en: Both algorithms are based on the concept of dynamic programming, which consists
    of splitting a complex problem into sub-problems that can be easily solved, and
    reusing the solutions to solve more complex steps in a recursive/iterative fashion.
    For further information on this, please refer to *Dynamic Programming and Markov
    Process, Ronald A. Howard, The MIT Press*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种算法都是基于动态规划的概念，即将复杂问题分解为可以轻松解决的子问题，并以递归/迭代的方式重用这些解决方案来解决更复杂的步骤。有关此方面的更多信息，请参阅
    *《动态规划与马尔可夫过程》，作者：Ronald A. Howard，麻省理工学院出版社*。
- en: Forward phase
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向阶段
- en: 'If we call *p[ij]* the transition probability *P(i → j)*, we define a recursive
    procedure considering the following probability:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 *p[ij]* 称为转移概率 *P(i → j)*，我们定义一个考虑以下概率的递归过程：
- en: '![](img/aa938322-d572-4c1e-98fc-f027a2787715.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aa938322-d572-4c1e-98fc-f027a2787715.png)'
- en: 'The variable *f[t]^i* represents the probability that the HMM is in the state
    *i* (at time *t*) after *t* observations (from *1* to *t*). Considering the HMM
    assumptions, we can state that *f[t]*^(*i* )depends on all possible *f[t-1]^j*.
    More precisely, we have:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 *f[t]^i* 表示HMM在 *t* 次观察（从 *1* 到 *t*）后处于状态 *i*（在时间 *t*）的概率。考虑到HMM的假设，我们可以断言
    *f[t]*^(*i* )依赖于所有可能的 *f[t-1]^j*。更精确地说，我们有：
- en: '![](img/e46b6f9b-3b8a-4129-b8fb-c0ceb4d53ece.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e46b6f9b-3b8a-4129-b8fb-c0ceb4d53ece.png)'
- en: With this process, we are considering that the HMM can reach any of the states
    at time *t-1* (with the first *t-1* observations), and transition to the state *i*
    at time *t* with probability *p[ji]*. We need also to consider the emission probability
    for the final state *o[t]* conditioned to each of the possible previous states.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程，我们考虑HMM可以在时间 *t-1*（即前 *t-1* 次观察）达到任何状态，并以概率 *p[ji]* 转移到时间 *t* 的状态 *i*。我们还需要考虑最终状态
    *o[t]* 在每个可能的前一状态下的发射概率。
- en: 'For definition, the initial and ending states are not emitting. It means that
    we can write any sequence of observations as *0, o[1], o[2], ..., o[Sequence Length],
    0*, where the first and the final values are null. The procedure starts with computing
    the forward message at time *1*:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定义，初始状态和结束状态不产生输出。这意味着我们可以将任何观察序列写成 *0, o[1], o[2], ..., o[Sequence Length],
    0*，其中第一个和最后一个值是空的。该过程从计算时间 *1* 的前向消息开始：
- en: '![](img/f825279c-c495-40c8-866c-4a7279c157a2.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f825279c-c495-40c8-866c-4a7279c157a2.png)'
- en: 'The non-emitting ending state must be also considered:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 必须也要考虑非发射的结束状态：
- en: '![](img/c62826ee-9eab-404a-b9b6-e3656e23da6f.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c62826ee-9eab-404a-b9b6-e3656e23da6f.png)'
- en: The expression for the last state *x*[*Ending* ]is interpreted here as the index
    of the ending state in both *A* and *B* matrices. For example, we indicate *p[ij]*
    as *A[i, j]*, meaning the transition probability at a generic time instant from
    the state *x[t] = i* to the state *x[t+1] = j*. In the same way, *p[i][Ending]*
    is represented as *A[i, x[Ending]]*, meaning the transition probability from the
    penultimate state *x[Sequence Length-1] = i* to the ending one*x[Sequence Length]
    = Ending State*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里对最后一个状态 *x*[*Ending* ]的表达式解释为在 *A* 和 *B* 矩阵中结束状态的索引。例如，我们表示 *p[ij]* 为 *A[i,
    j]*，意味着从状态 *x[t] = i* 到状态 *x[t+1] = j* 的一般时间瞬间的转移概率。同样，*p[i][Ending]* 表示为 *A[i, x[Ending]]*，意味着从倒数第二个状态
    *x[Sequence Length-1] = i* 到结束状态 *x[Sequence Length] = Ending State* 的转移概率。
- en: 'The Forward algorithm can, therefore, be summarized in the following steps
    (we assume to have *N* states, hence we need to allocate *N+2* positions, considering
    the initial and the ending states):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前向算法可以总结为以下步骤（我们假设有 *N* 个状态，因此我们需要考虑初始和结束状态，需要分配 *N+2* 个位置）：
- en: Initialization of a *Forward* vector with shape (*N + 2*, *Sequence Length*).
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化形状为 (*N + 2*, *Sequence Length*) 的 *Forward* 向量。
- en: Initialization of *A* (transition probability matrix) with shape (*N, N*). Each
    element is *P(x[i]|x[j]**)*.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化形状为 (*N, N*) 的 *A*（转移概率矩阵）。每个元素是 *P(x[i]|x[j]**)*。
- en: Initialization of *B* with shape (*Sequence Length*, *N*). Each element is *P(o[i]|x[j]**)*.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化形状为 (*Sequence Length*, *N*) 的 *B*。每个元素是 *P(o[i]|x[j]**)*。
- en: 'For *i=1* to *N*:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *Forward[i, 1]* = *A[0, i] · B[1, i]*
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Forward[i, 1]* = *A[0, i] · B[1, i]*
- en: 'For *t=2* to *Sequence Length-1*:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=2* 到 *Sequence Length-1*：
- en: 'For *i=1* to *N*:'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *S = 0*
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = 0*
- en: 'For *j=1* to *N*:'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *j=1* 到 *N*：
- en: Set *S = S + Forward[j, t-1] · A[j, i] · B[t, i]*
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = S + Forward[j, t-1] · A[j, i] · B[t, i]*
- en: Set *Forward[i, t] = S*
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Forward[i, t] = S* 设置为
- en: Set *S = 0*.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = 0*。
- en: 'For *i=1* to *N*:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *S = S + Forward[i, Sequence Length] · A[i, x[Ending]]*
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = S + Forward[i, Sequence Length] · A[i, x[Ending]]* 设置为
- en: Set *Forward[x[Ending], Sequence Length] = S*.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Forward[x[Ending], Sequence Length] = S* 设置为
- en: Now it should be clear that the name **forward** derives from the procedure
    to propagate the information from the previous step to the next one, until the
    ending state, which is not emittied.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很清楚，**正向**这个名字来源于将信息从前一步传播到下一步，直到结束状态（该状态不产生输出）的过程。
- en: Backward phase
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向后阶段
- en: 'During the **backward phase**, we need to compute the probability of a sequence
    starting at time *t+1: o[t+1], o[t+2], ..., o[Sequence Length]*, given that the
    state at time *t* is *i*. Just like we have done before, we define the following
    probability:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **向后阶段**，我们需要计算在时间 *t* 的状态为 *i* 的条件下，从时间 *t+1* 开始的序列的概率：o[t+1], o[t+2], ...,
    o[Sequence Length]。就像我们之前所做的那样，我们定义以下概率：
- en: '![](img/7c024525-9838-4223-a54f-c158fb18e0fd.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c024525-9838-4223-a54f-c158fb18e0fd.png)'
- en: 'The backward algorithm is very similar to the forward one, but in this case,
    we need to move in the opposite direction, assuming we know that the state at
    time *t* is *i*. The first state to consider is the last one *x[Ending]*, which
    is not emitting, like the initial state; therefore we have:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 向后算法与正向算法非常相似，但在这个情况下，我们需要朝相反的方向移动，假设我们知道在时间 *t* 的状态是 *i*。首先考虑的状态是最后一个状态 *x[Ending]*，它不产生输出，就像初始状态一样；因此我们有：
- en: '![](img/9cae7840-24b7-4382-aad0-91eb97efa387.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cae7840-24b7-4382-aad0-91eb97efa387.png)'
- en: 'We terminate the recursion with the initial state:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用初始状态终止递归：
- en: '![](img/d62d9350-f901-4766-a303-43d0ec77f12c.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d62d9350-f901-4766-a303-43d0ec77f12c.png)'
- en: 'The steps are the following ones:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: Initialization of a vector *Backward* with shape *(N + 2, Sequence Length)*.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用形状为 *(N + 2, Sequence Length)* 的向量 *Backward* 进行初始化。
- en: Initialization of *A* (transition probability matrix) with shape *(N, N)*. Each
    element is *P(x[i]|x[j]**)*.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用形状为 *(N, N)* 的 *A*（转移概率矩阵）进行初始化。每个元素是 *P(x[i]|x[j]*)*。
- en: Initialization of *B* with shape *(Sequence Length, N)*. Each element is* P(o[i]|x[j]**)*.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用形状为 *(Sequence Length, N)* 的 *B* 进行初始化。每个元素是 *P(o[i]|x[j]*)*。
- en: 'For *i=1* to *N*:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *Backward[x[Endind], Sequence Length] = A[i, x[Endind]]*
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Backward[x[Endind], Sequence Length] = A[i, x[Endind]]*。
- en: 'For *t=Sequence Length-1* to *1*:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=Sequence Length-1* 到 *1*：
- en: 'For *i=1* to *N*:'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *S = 0*
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = 0*。
- en: For *j=1* to *N*
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *j=1* 到 *N*：
- en: Set *S = S + Backward[j, t+1] · A[j, i] · B[t+1, i]*
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = S + Backward[j, t+1] · A[j, i] · B[t+1, i]*
- en: Set *Backward[i, t] = S*
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Backward[i, t] = S*。
- en: Set *S = 0*.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = 0*。
- en: 'For *i=1* to *N*:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *S = S + Backward[i, 1] · A[0, i] · B[1, i]*
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *S = S + Backward[i, 1] · A[0, i] · B[1, i]*。
- en: Set *Backward[0, 1] = S*.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *Backward[0, 1] = S*。
- en: HMM parameter estimation
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HMM参数估计
- en: 'Now that we have defined both the forward and the backward algorithms, we can
    use them to estimate the structure of the underlying HMM. The procedure is an
    application of the Expectation-Maximization algorithm, which will be discussed
    in the next chapter, [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM
    Algorithm and Applications*, and its goal can be summarized as defining how we
    want to estimate the values of *A* and *B*. If we define *N(i, j)* as the number
    of transitions from the state *i* to the state *j*, and *N(i)* the total number
    of transitions from the state *i*, we can approximate the transition probability
    *P(i → j)* with:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了正向和向后算法，我们可以使用它们来估计潜在HMM的结构。该过程是期望最大化算法的应用，将在下一章（[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，*EM算法及其应用*）中讨论，其目标可以概括为定义我们如何估计
    *A* 和 *B* 的值。如果我们定义 *N(i, j)* 为从状态 *i* 到状态 *j* 的转移次数，以及 *N(i)* 为从状态 *i* 的总转移次数，我们可以用以下方式近似转移概率
    *P(i → j)*：
- en: '![](img/599b50d2-75d5-4c85-bc1f-e36280247077.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/599b50d2-75d5-4c85-bc1f-e36280247077.png)'
- en: 'In the same way, if we define *M(i, p)* the number of times we have observed
    the emission *o[p]* in the state *i*, we can approximate the emission probability *P(o[p]|x[i])*
    with:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们定义 *M(i, p)* 为在状态 *i* 中观察到发射 *o[p]* 的次数，我们可以用以下方式近似发射概率 *P(o[p]|x[i])*：
- en: '![](img/0efa14ed-4579-43c4-8458-49df67a3689a.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0efa14ed-4579-43c4-8458-49df67a3689a.png)'
- en: 'Let''s start with the estimation of the transition probability matrix *A*.
    If we consider the probability that the HMM is in the state *i* at time *t*, and
    in the state *j* at time *t+1* given the observations, we have:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从转移概率矩阵 *A* 的估计开始。如果我们考虑在给定观察结果的情况下，HMM在时间 *t* 处于状态 *i*，在时间 *t+1* 处于状态 *j*
    的概率，我们有：
- en: '![](img/9c0c449d-33da-43c9-b894-c5c211bf45c5.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c0c449d-33da-43c9-b894-c5c211bf45c5.png)'
- en: 'We can compute this probability using the forward and backward algorithms,
    given a sequence of observations *o[1], o[2], ..., o[Sequence Length]*. In fact,
    we can use both the forward message *f[t]^i*, which is the probability that the
    HMM is in the state *i* after *t* observations, and the backward message *b[t+1]^j*,
    which is the probability of a sequence *o[t+1], o[t+1], ..., o[Sequence Length]*
    starting at time *t+1*, given that the HMM is in state *j* at time *t+1*. Of course,
    we need also to include the emission probability and the transition probability
    *p[ij]*, which is what we are estimating. The algorithm, in fact, starts with
    a random hypothesis and iterates until the values of *A* become stable. The estimation *α[ij]*
    at time *t* is equal to:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前向和后向算法来计算这个概率，给定一系列观测值 *o[1], o[2], ..., o[Sequence Length]*。实际上，我们可以使用前向消息
    *f[t]^i*，它是HMM在 *t* 次观测后处于状态 *i* 的概率，以及后向消息 *b[t+1]^j*，它是给定HMM在时间 *t+1* 处于状态 *j*
    时，从时间 *t+1* 开始的序列 *o[t+1], o[t+1], ..., o[Sequence Length]* 的概率。当然，我们还需要包括发射概率和转移概率
    *p[ij]*，这是我们正在估计的。实际上，算法从随机假设开始，迭代直到 *A* 的值变得稳定。时间 *t* 处的估计 *α[ij]* 等于：
- en: '![](img/24edfb52-1fa3-405e-a9a3-768e2ef640c7.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/24edfb52-1fa3-405e-a9a3-768e2ef640c7.png)'
- en: In this context, we are omitting the full proof due to its complexity; however,
    the reader can find it in *A tutorial on hidden Markov models and selected applications
    in speech recognition,* *Rabiner L. R., Proceedings of the IEEE 77.2**.*
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们省略了完整的证明，因为它很复杂；然而，读者可以在 *《隐马尔可夫模型及其在语音识别中的应用教程》，Rabiner L. R.，IEEE
    77.2 会议论文* 中找到它。
- en: 'To compute the emission probabilities, it''s easier to start with the probability
    of being in the state *i* at time *t* given the sequence of observations:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算发射概率，从时间 *t* 处处于状态 *i* 给定观测值序列的概率开始计算更容易：
- en: '![](img/8d28be2d-6fc3-499d-86af-6ee4523a4625.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8d28be2d-6fc3-499d-86af-6ee4523a4625.png)'
- en: 'In this case, the computation is immediate, because we can multiply the forward
    and backward messages computed at the same time *t* and state *i* (remember that
    considering the observations, the backward message is conditioned to *x[t] = i*,
    while the forward message computes the probability of the observations joined
    with *x[t] = i*. Hence, the multiplication is the unnormalized probability of
    being in the state *i* at time *t*). Therefore, we have:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，计算是立即的，因为我们可以在同一时间 *t* 和状态 *i* 计算前向和后向消息（记住，考虑到观测值，后向消息是条件于 *x[t] = i*
    的，而前向消息计算观测值与 *x[t] = i* 联合的概率。因此，乘法是时间 *t* 处处于状态 *i* 的未归一化概率）。因此，我们有：
- en: '![](img/75ac8487-2059-47de-9904-5f7f0e9ed4fc.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/75ac8487-2059-47de-9904-5f7f0e9ed4fc.png)'
- en: 'The proof of how the normalizing constant is obtained can be found in the aforementioned
    paper. We can now plug these expressions to the estimation of *a[ij]* and *b[ip]*:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化常数的证明可以在上述论文中找到。现在我们可以将这些表达式插入到 *a[ij]* 和 *b[ip]* 的估计中：
- en: '![](img/4f1a3d0a-b4f0-4249-b65a-07c68b0a4ccf.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f1a3d0a-b4f0-4249-b65a-07c68b0a4ccf.png)'
- en: In the numerator of the second formula, we adopted the indicator function (it's
    *1* only if the condition is true, *0* otherwise) to limit the sum only where
    those elements are *o[t] = p*. During an iteration *k*, *p[ij]* is the estimated
    value *a[ij]* found in the previous iteration *k-1*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个公式的分子中，我们采用了指示函数（当条件为真时为 *1*，否则为 *0*）来限制仅在那些元素为 *o[t] = p* 的位置求和。在一个迭代 *k*
    中，*p[ij]* 是在先前迭代 *k-1* 中找到的估计值 *a[ij]*。
- en: 'The algorithm is based on the following steps:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 算法基于以下步骤：
- en: Randomly initialize the matrices *A* and *B*
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化矩阵 *A* 和 *B*
- en: Initialize a tolerance variable *Tol* (for example, *Tol = 0.001*)
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个容差变量 *Tol*（例如，*Tol = 0.001*）
- en: 'While *Norm(A^k - A^(k-1)) > Tol* and *Norm(B^k - B^(k-1)**) > Tol* (*k* is
    the iteration index):'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *Norm(A^k - A^(k-1)) > Tol* 和 *Norm(B^k - B^(k-1)**) > Tol* （*k* 是迭代索引）时：
- en: 'For *t=1* to *Sequence Length-1*:'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=1* 到 *Sequence Length-1*：
- en: 'For *i=1* to *N*:'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: 'For *j=1* to *N*:'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *j=1* 到 *N*：
- en: Compute *α^t*[*ij*]
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *α^t*[*ij*]
- en: Compute *β^t[i]*
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *β^t[i]*
- en: Compute the estimations of *a[ij]* and *b[ip]* and store them in *A^k*
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算估计的 *a[ij]* 和 *b[ip]* 并将它们存储在 *A^k*
- en: Alternatively, it's possible to fix the number of iterations, even if the best
    solution is using both a tolerance and a maximum number of iterations, to terminate
    the process when the first condition is met.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以固定迭代次数，尽管最佳解决方案是同时使用容差和最大迭代次数，以便在满足第一个条件时终止过程。
- en: Example of HMM training with hmmlearn
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 hmmlearn 的 HMM 训练示例
- en: For this example, we are going to use hmmlearn, which is a package for HMM computations
    (see the information box at the end of this section for further details). For
    simplicity, let's consider the airport example discussed in the paragraph about
    the Bayesian networks, and let's suppose we have a single hidden variable that
    represents the weather (of course, this is not a real hidden variable!), modeled
    as a multinomial distribution with two components (good and rough).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用hmmlearn，这是一个用于HMM计算的包（有关更多详细信息，请参阅本节末尾的信息框）。为了简单起见，让我们考虑关于贝叶斯网络的段落中讨论的机场示例，并假设我们有一个代表天气（当然，这并不是一个真正的隐藏变量！）的单个隐藏变量，它被建模为具有两个成分（好和差）的多项式分布。
- en: We observe the arrival time of our flight London-Rome (which partially depends
    on the weather conditions), and we want to train an HMM to infer future states
    and compute the posterior probability of hidden states corresponding to a given
    sequence.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察我们的航班伦敦-罗马的到达时间（这部分取决于天气条件），并希望训练一个HMM来推断未来状态并计算对应给定序列的隐藏状态的后验概率。
- en: 'The schema for our example is shown in the following diagram:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例架构如下所示：
- en: '![](img/f88ad311-0701-48b9-9f86-024a2edf0510.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f88ad311-0701-48b9-9f86-024a2edf0510.png)'
- en: HMM for the weather-arrival delay problem
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 用于天气-到达延误问题的HMM
- en: 'Let''s start by defining our observation vector. As we have two states, its
    values will be `0` and `1`. Let''s assume that `0` means **On-time** and `1` means **Delay**:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义我们的观测向量。由于我们有两个状态，其值将是 `0` 和 `1`。让我们假设 `0` 表示**准时**，而 `1` 表示**延误**：
- en: '[PRE11]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have 35 consecutive observations whose values are either `0` or `1`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有35个连续的观测值，其值要么是 `0` 要么是 `1`。
- en: To build the HMM, we are going to use the `MultinomialHMM` class, with `n_components=2`, `n_iter=100`,
    and `random_state=1000` (it's important to always use the same seed to avoid differences
    in the results). The number of iterations is sometimes hard to determine; for
    this reason, hmmlearn provides a utility `ConvergenceMonitor` class which can
    be checked to be sure that the algorithm has successfully converged.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建HMM，我们将使用 `MultinomialHMM` 类，参数为 `n_components=2`、`n_iter=100` 和 `random_state=1000`（始终使用相同的种子以避免结果差异是很重要的）。迭代次数有时很难确定；因此，hmmlearn提供了一个名为
    `ConvergenceMonitor` 的实用工具类，可以检查算法是否已成功收敛。
- en: 'Now we can train our model using the `fit()` method, passing as argument the
    list of observations (the array must be always bidimensional with shape *Sequence
    Length × N[Components]*):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `fit()` 方法来训练我们的模型，将观测列表（数组必须是始终具有形状 *序列长度 × N[组件]* 的二维数组）作为参数传递：
- en: '[PRE12]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The process is very fast, and the monitor (available as instance variable `monitor`)
    has confirmed the convergence. If the model is very big and needs to be retrained,
    it''s also possible to check smaller values of `n_iter`). Once the model is trained,
    we can immediately visualize the transition probability matrix, which is available
    as an instance variable `transmat_`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程非常快，监控器（作为实例变量 `monitor` 可用）已确认收敛。如果模型非常大且需要重新训练，也可以检查 `n_iter` 的较小值）。一旦模型训练完成，我们就可以立即可视化转换概率矩阵，该矩阵作为实例变量
    `transmat_` 可用：
- en: '[PRE13]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can interpret these values as saying that the probability to transition from
    `0` (good weather) to `1` (rough weather) is higher (*p[01]* is close to *1*)
    than the opposite, and it's more likely to remain in state `1` than in state `0`
    (*p[00]* is almost null). We could deduce that the observations have been collected
    during the winter period! After explaining the Viterbi algorithmin the next paragraph,
    we can also check, given some observations, what the most likely hidden state
    sequence is.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些值解释为从 `0`（好天气）转换到 `1`（恶劣天气）的概率比相反的概率更高（*p[01]* 接近 *1*），并且更有可能保持在状态 `1`
    而不是状态 `0`（*p[00]* 几乎为零）。我们可以推断出观测值是在冬季期间收集的！在下一段解释维特比算法之后，我们还可以检查，给定一些观测值，最可能的隐藏状态序列是什么。
- en: hmmlearn ([http://hmmlearn.readthedocs.io/en/latest/index.html](http://hmmlearn.readthedocs.io/en/latest/index.html))
    is a framework originally built to be a part of Scikit-Learn. It supports multinomial
    and Gaussian HMM, and allows training and inferring using the most common algorithms.
    It can be installed using the `pip install hmmlearn` command.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: hmmlearn ([http://hmmlearn.readthedocs.io/en/latest/index.html](http://hmmlearn.readthedocs.io/en/latest/index.html))
    是一个最初被构建为Scikit-Learn一部分的框架。它支持多项式和高斯HMM，并允许使用最常用的算法进行训练和推断。可以使用 `pip install
    hmmlearn` 命令进行安装。
- en: Viterbi algorithm
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维特比算法
- en: 'The **Viterbi algorithm** is one of most common decoding algorithms for HMM.
    Its goal is to find the most likely hidden state sequence corresponding to a series
    of observations. The structure is very similar to the forward algorithm, but instead
    of computing the probability of a sequence of observations joined with the state
    at the last time instant, this algorithm looks for:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '**维特比算法**是HMM中最常见的解码算法之一。其目标是找到与一系列观察相对应的最可能的隐藏状态序列。其结构与前向算法非常相似，但该算法不是计算与最后一个时间点状态联合的观察序列的概率，而是寻找：'
- en: '![](img/1ba7094a-03d5-4453-9755-557392e2bffd.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ba7094a-03d5-4453-9755-557392e2bffd.png)'
- en: 'The variable *v[t]^i* represents that maximum probability of the given observation
    sequence joint with *x[t] = i*, considering all possible hidden state paths (from
    time instant *1* to *t-1*). We can compute *v[t]^i* recursively by evaluating
    all the *v[t-1]^j* multiplied by the corresponding transition probabilities *p[ji]*
    and emission probability *P(o[t]|x[i])*, and always picking the maximum overall possible
    values of *j*:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 变量 *v[t]^i* 表示给定观察序列与 *x[t] = i* 联合的最大概率，考虑到所有可能的隐藏状态路径（从时间点 *1* 到 *t-1*）。我们可以通过评估所有
    *v[t-1]^j* 乘以相应的转移概率 *p[ji]* 和发射概率 *P(o[t]|x[i])* 来递归地计算 *v[t]^i*，并且总是选择 *j* 的所有可能值的最大值：
- en: '![](img/09735a93-2d90-4f2f-b9fe-125a6626c989.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09735a93-2d90-4f2f-b9fe-125a6626c989.png)'
- en: 'The algorithm is based on a backtracking approach, using a backpointer *bp[t]^i*
    whose recursive expression is the same as *v[t]^i*, but with the *argmax* function
    instead of *max*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 算法基于回溯方法，使用一个回溯指针 *bp[t]^i*，其递归表达式与 *v[t]^i* 相同，但用 *argmax* 函数代替 *max*：
- en: '![](img/613be74e-74c1-4af3-b1ce-a250c35c1a21.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/613be74e-74c1-4af3-b1ce-a250c35c1a21.png)'
- en: Therefore, *bp[t]*^(*i* )represents the partial sequence of hidden states *x**[1],
    x[2], ..., x*[*t-1* ]that maximizes* v[t]^i*. During the recursion, we add the
    timesteps one by one, so the previous path could be invalidated by the last observation.
    That's why we need to backtrack the partial result and replace the sequence built
    at time *t* that doesn't maximize *v[t+1]^i* anymore.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*bp[t]*^(*i* )代表最大化 *v[t]^i* 的部分隐藏状态序列 *x**[1], x[2], ..., x*[*t-1* ]。在递归过程中，我们逐个添加时间步，因此之前的路径可能会被最后一个观察所无效化。这就是为什么我们需要回溯部分结果并替换时间
    *t* 时不再最大化 *v[t+1]^i* 的序列。
- en: 'The algorithm is based on the following steps (like in the other cases, the
    initial and ending states are not emitting):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 算法基于以下步骤（与其他情况一样，初始状态和结束状态不产生输出）：
- en: Initialization of a vector *V* with shape *(N + 2, Sequence Length)*.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个形状为 *(N + 2, 序列长度)* 的向量 *V*。
- en: Initialization of a vector *BP* with shape *(N + 2, Sequence Length)*.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个形状为 *(N + 2, 序列长度)* 的向量 *BP*。
- en: Initialization of *A* (transition probability matrix) with shape *(N, N)*. Each
    element is *P(x[i]|x[j]**)*.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *A*（转移概率矩阵）的形状为 *(N, N)*。每个元素是 *P(x[i]|x[j]**)*。
- en: Initialization of *B* with shape *(Sequence Length, N)*. Each element is *P(o[i]|x[j]**)*.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *B*（发射概率矩阵）的形状为 *(序列长度, N)*。每个元素是 *P(o[i]|x[j]**)*。
- en: 'For *i=1* to *N*:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *V[i, 1]* = *A[i, 0] **· B[1, i]*
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *V[i, 1]* = *A[i, 0] **· B[1, i]*
- en: '*BP[i, 1]* = Null (or any other value that cannot be interpreted as a state)'
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*BP[i, 1]* = Null（或任何其他不能解释为状态的值）'
- en: 'For *t=1* to *Sequence Length*:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *t=1* 到 *序列长度*：
- en: 'For *i=1* to *N*:'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N*：
- en: Set *V[i, t] = max[j] V[j, t-1] · A[j, i] · B[t, i]*
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *V[i, t] = max[j] V[j, t-1] · A[j, i] · B[t, i]*
- en: Set *BP[i, t] = argmax[j] V[j, t-1] · A[j, i] · B[t, i]*
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *BP[i, t] = argmax[j] V[j, t-1] · A[j, i] · B[t, i]*
- en: Set *V[x[Endind], Sequence Length] = max[j] V[j, Sequence Length] **· A[j, x[Endind]]*.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *V[x[Endind], 序列长度] = max[j] V[j, 序列长度] **· A[j, x[Endind]]*。
- en: Set *BP**[x[Endind], Sequence Length] = argmax[j] V[j, Sequence Length] **·
    A[j, x[Endind]]*.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *BP**[x[Endind], 序列长度] = argmax[j] V[j, 序列长度] **· A[j, x[Endind]]*。
- en: Reverse *BP*.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反转 *BP*。
- en: The output of the Viterbi algorithm is a tuple with the most likely sequence
    *BP*, and the corresponding probabilities *V*.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 维特比算法的输出是一个元组，包含最可能的序列 *BP* 和相应的概率 *V*。
- en: Finding the most likely hidden state sequence with hmmlearn
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 hmmlearn 寻找最可能的隐藏状态序列
- en: 'At this point, we can continue with the previous example, using our model to
    find the most likely hidden state sequence given a set of possible observations.
    We can use either the `decode()` method or the `predict()` method. The first one
    returns the log probability of the whole sequence and the sequence itself; however,
    they all use the Viterbi algorithm as a default decoder:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以继续使用之前的例子，使用我们的模型来找到给定一组可能观察到的最可能的隐藏状态序列。我们可以使用`decode()`方法或`predict()`方法。第一个返回整个序列的对数概率以及序列本身；然而，它们都默认使用维特比算法作为解码器：
- en: '[PRE14]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The sequence is coherent with the transition probability matrix; in fact, it''s
    more likely the persistence of rough weather (`1`) than the opposite. As a consequence,
    the transition from `1` to X is less likely than the one from `0` to `1`. The
    choice of state is made by selecting the highest probability; however, in some
    cases, the differences are minimal (in our example, it can happen to have *p =
    [0.49, 0.51]*, meaning that there''s a high error chance), so it''s useful to
    check the posterior probabilities for all the states in the sequence:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 序列与转移概率矩阵一致；事实上，持续恶劣天气（`1`）的可能性比相反的可能性更大。因此，从`1`到X的转移比从`0`到`1`的转移可能性更小。状态的选择是通过选择最高概率来进行的；然而，在某些情况下，差异很小（在我们的例子中，可能会出现*p
    = [0.49, 0.51]*，这意味着存在高误差的可能性），因此检查序列中所有状态的后验概率是有用的：
- en: '[PRE15]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In our case, there are a couple of states that have *p ∼ [0.495, 0.505]*, so
    even if the output state is *1* (rough weather), it's also useful to consider
    a moderate probability to observe good weather. In general, if a sequence is coherent
    with the transition probability previously learned (or manually input), those
    cases are not very common. I suggest trying different configurations and observations
    sequences, and to also assess the probabilities for the *strangest* situations
    (like a sequence of zero second). At that point, it's possible to retrain the
    model and recheck the new evidence has been correctly processed.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，有几个状态的概率为*p ∼ [0.495, 0.505]*，因此即使输出状态是*1*（恶劣天气），考虑观察到良好天气的适度概率也是有用的。一般来说，如果一个序列与之前学习（或手动输入）的转移概率一致，那么这些情况并不常见。我建议尝试不同的配置和观察序列，并评估*最奇怪*情况（如零秒序列）的概率。在那个点上，可以重新训练模型并重新检查新证据是否已被正确处理。
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have introduced Bayesian networks, describing their structure
    and relations. We have seen how it's possible to build a network to model a probabilistic
    scenario where some elements can influence the probability of others. We have
    also described how to obtain the full joint probability using the most common
    sampling methods, which allow reducing the computational complexity through an
    approximation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了贝叶斯网络，描述了它们的结构和关系。我们看到了如何构建一个网络来模拟一个概率场景，其中某些元素可以影响其他元素的概率。我们还描述了如何使用最常见的采样方法来获得完整的联合概率，这些方法通过近似可以降低计算复杂度。
- en: The most common sampling methods belong to the family of MCMC algorithms, which
    model the transition probability from a sample to another one as a first-order
    Markov chain. In particular, the Gibbs sampler is based on the assumption that
    it's easier to sample from conditional distribution than work directly with the
    full joint probability. The method is very easy to implement, but it has some
    performance drawbacks that can be avoided by adopting more complex strategies.
    The Metropolis-Hastings sampler, instead, works with a candidate-generating distribution
    and a criterion to accept or reject the samples. Both methods satisfy the detailed
    balance equation, which guarantees the convergence (the underlying Markov chain
    will reach the unique stationary distribution).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的采样方法属于MCMC算法家族，这些算法将从一个样本到另一个样本的转移概率建模为一阶马尔可夫链。特别是，吉布斯采样基于这样的假设：从条件分布中采样比直接处理完整联合概率要容易。这种方法很容易实现，但它有一些性能上的缺点，可以通过采用更复杂的策略来避免。相反，Metropolis-Hastings采样器使用候选生成分布和接受或拒绝样本的标准。两种方法都满足详细平衡方程，这保证了收敛（基础马尔可夫链将达到唯一的平稳分布）。
- en: In the last part of the chapter, we introduced HMMs, which allow modeling time
    sequences based on observations corresponding to a series of hidden states. The
    main concept of such models, in fact, is the presence of unobservable states that
    condition the emission of a particular observation (which is observable). We have
    discussed the main assumptions and how to build, train, and infer from a model.
    In particular, the Forward-Backward algorithm can be employed when it's necessary
    to learn the transition probability matrix and the emission probabilities, while
    the Viterbi algorithm is adopted to find the most likely hidden state sequence
    given a set of consecutive observations.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们介绍了隐马尔可夫模型（HMMs），它允许根据一系列隐藏状态对应的观察值来建模时间序列。实际上，这类模型的主要概念是存在不可观测的状态，这些状态会影响特定观察值的发射（该观察值是可观测的）。我们已经讨论了主要假设以及如何构建、训练和从模型中进行推断。特别是，当需要学习转移概率矩阵和发射概率时，可以使用前向-后向算法；而维特比算法则用于在给定一系列连续观察值的情况下找到最可能的隐藏状态序列。
- en: In the next chapter, [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM
    Algorithm and Applications*, we're going to briefly discuss the Expectation-Maximization
    algorithm, focusing on some important applications based on the **Maximum Likelihood
    Estimation** (**MLE**) approach.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第五章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)，*期望最大化算法及其应用*中，我们将简要讨论期望最大化算法，重点关注基于**最大似然估计（MLE**）方法的一些重要应用。
