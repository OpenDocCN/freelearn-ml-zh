- en: '*Chapter 15*: Model Interoperability, Hardware Optimization, and Integrations'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第15章*：模型互操作性、硬件优化和集成'
- en: In the previous chapter, we discovered how to deploy our machine learning scoring
    either as a batch or real-time scorer, what endpoints are and how we can deploy
    them, and finally, we had a look at how we can monitor our deployed solutions.
    In this chapter, we will dive deeper into additional deployment scenarios for
    ML inferencing, possible other hardware infrastructure we can utilize, and how
    we can integrate our models and endpoints with other Azure services.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们发现了如何将我们的机器学习评分部署为批量或实时评分器，什么是端点以及我们如何部署它们，最后，我们查看了一下如何监控我们部署的解决方案。在本章中，我们将更深入地探讨ML推理的额外部署场景，我们可以利用的其他可能的硬件基础设施，以及我们如何将我们的模型和端点与其他Azure服务集成。
- en: In the first section, we will have a look at how to provide model interoperability
    by converting ML models into a standardized model format and an inference-optimized
    scoring framework. **Open Neural Network Exchange** (**ONNX**) is a standardized
    format to serialize and store ML models and acyclic computational graphs and operations
    efficiently. We will learn what the ONNX framework is, how we can convert ML models
    from popular ML frameworks to ONNX, and how we can score ONNX models on multiple
    platforms using ONNX Runtime.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们将探讨如何通过将ML模型转换为标准化的模型格式和推理优化的评分框架来提供模型互操作性。**开放神经网络交换**（**ONNX**）是一种标准化的格式，用于有效地序列化和存储ML模型以及循环计算图和操作。我们将学习ONNX框架是什么，如何将ML模型从流行的ML框架转换为ONNX，以及如何使用ONNX
    Runtime在多个平台上评分ONNX模型。
- en: Following that, we will take a look at alternative hardware targets, such as
    **field-programmable gate arrays** (**FPGAs**). We will understand how they work
    internally and how they can lead to higher performance and better efficiency compared
    to standard hardware or even GPUs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将探讨替代硬件目标，例如**现场可编程门阵列**（**FPGA**）。我们将了解它们内部是如何工作的，以及它们如何与标准硬件甚至GPU相比实现更高的性能和更好的效率。
- en: Finally, we will have a look at how we can integrate ML models and endpoints
    into other services. We will get a deeper understanding of the process to deploy
    ML to edge devices, and we will integrate one of our previously set up endpoints
    with Power BI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨如何将ML模型和端点集成到其他服务中。我们将更深入地了解将ML部署到边缘设备的过程，并将我们之前设置的一个端点与Power BI集成。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Model interoperability with ONNX
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与ONNX的模型互操作性
- en: Hardware optimization with FPGAs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用FPGA进行硬件优化
- en: Integrating ML models and endpoints with Azure services
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将ML模型和端点与Azure服务集成
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will require access to a Microsoft Power BI account. You
    can get one either through your place of work or by creating a trial account here:
    [https://app.powerbi.com/signupredirect?pbi_source=web](https://app.powerbi.com/signupredirect?pbi_source=web).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要访问一个Microsoft Power BI账户。您可以通过您的工作场所或在此处创建一个试用账户来获取一个账户：[https://app.powerbi.com/signupredirect?pbi_source=web](https://app.powerbi.com/signupredirect?pbi_source=web)。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15](https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码示例都可以在本书的GitHub存储库中找到：[https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15](https://github.com/PacktPublishing/Masthttps://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter15)。
- en: Model interoperability with ONNX
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与ONNX的模型互操作性
- en: In the previous chapter, we learned how to deploy ML models as web services
    for online and batch scoring. However, many real-world use cases require you to
    embed a trained ML model directly into an application without the use of a separate
    scoring service. The target service is likely written in a different language
    than the language used for training the ML model. A common example is that a simple
    model trained in Python using scikit-learn needs to be embedded into a Java application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何将ML模型作为Web服务部署以进行在线和批量评分。然而，许多实际用例要求您将训练好的ML模型直接嵌入到应用程序中，而不使用单独的评分服务。目标服务可能使用与训练ML模型的语言不同的语言编写。一个常见的例子是，使用scikit-learn在Python中训练的简单模型需要嵌入到Java应用程序中。
- en: Model interoperability gives you the flexibility to train your model with your
    language and framework of choice, export it to a common format, and then score
    it in a different language and platform using the shared format. In some cases,
    using a native runtime optimized for scoring on the target environment even achieves
    a better scoring performance than running the original model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型互操作性为您提供了灵活性，您可以使用您选择的语言和框架来训练模型，将其导出为通用格式，然后使用共享格式在不同的语言和平台上对其进行评分。在某些情况下，使用针对目标环境优化的本地运行时甚至可以实现比运行原始模型更好的评分性能。
- en: First, we will take a look at the ONNX initiative, consisting of the specification,
    runtime, and ecosystem, and how it helps to achieve model interoperability across
    a large set of support languages, frameworks, operations, and target platforms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解 ONNX 创举，包括规范、运行时和生态系统，以及它是如何帮助实现跨大量支持语言、框架、操作和目标平台的模型互操作性的。
- en: Then, we will look into converting ML models from popular frameworks to ONNX
    (called ONNX frontends) and executing ONNX models in a native inferencing runtime
    using ONNX Runtime, one of the multiple ONNX backends. Let's delve into it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将探讨如何将机器学习模型从流行的框架转换为 ONNX（称为 ONNX 前端），并使用 ONNX Runtime（ONNX 的多个后端之一）在本地推理运行时中执行
    ONNX 模型。让我们深入了解。
- en: What is model interoperability and how can ONNX help?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型互操作性是什么？ONNX 如何帮助？
- en: As an IT organization grows, so does the amount of tooling, development, and
    deployment platforms and choices. In ML, this problem is even more present as
    there are multiple ML frameworks as well as model serialization formats. Therefore,
    once the organization grows, it becomes a near-impossible challenge to align every
    scientist and engineer on the same tooling, frameworks, and model formats that
    also need to support all your target environments. Does your XGBoost model run
    on iOS? Does your PyTorch model work in Java? Can your scikit-learn model be loaded
    in a browser-based JavaScript application? One way to solve this problem of model
    interoperability is to ensure that trained ML models can be ported to a standardized
    format that can be executed natively across all target platforms. This is exactly
    what ONNX is about.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着信息技术组织的成长，工具、开发和部署平台以及选择也会增加。在机器学习中，这个问题更为突出，因为存在多个机器学习框架以及模型序列化格式。因此，一旦组织规模扩大，就几乎不可能让每个科学家和工程师都同意使用相同的工具、框架和模型格式，这些格式还需要支持所有目标环境。您的
    XGBoost 模型能在 iOS 上运行吗？您的 PyTorch 模型能在 Java 中工作吗？您的 scikit-learn 模型能在基于浏览器的 JavaScript
    应用程序中加载吗？解决模型互操作性问题的一种方法确保训练好的机器学习模型可以被移植到可以在所有目标平台上本地执行的标准化格式。这正是 ONNX 的目的。
- en: 'ONNX is a joint initiative from major IT companies such as Microsoft, Facebook,
    Amazon, ARM, and Intel to facilitate ML model interoperability. It allows organizations
    to choose different languages, frameworks, and environments for ML training, as
    well as different languages, environments, and devices for inferencing. As an
    example, ONNX enables an organization to train deep learning models using PyTorch
    and TensorFlow and traditional ML models using LightGBM and XGBoost, and deploy
    these models to a Java-based web service, an Objective-C-based iOS application,
    and a browser-based JavaScript application. This interoperability is enabled through
    three key ingredients:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 是由微软、Facebook、亚马逊、ARM 和英特尔等主要 IT 公司共同发起的一项联合倡议，旨在促进机器学习模型的互操作性。它允许组织为机器学习训练选择不同的语言、框架和环境，以及为推理选择不同的语言、环境和设备。例如，ONNX
    允许组织使用 PyTorch 和 TensorFlow 训练深度学习模型，使用 LightGBM 和 XGBoost 训练传统机器学习模型，并将这些模型部署到基于
    Java 的 Web 服务、基于 Objective-C 的 iOS 应用程序和基于浏览器的 JavaScript 应用程序。这种互操作性是通过三个关键要素实现的：
- en: '**ONNX specification**: A data format for *efficient serialization and deserialization*
    for model definitions and model weights using **Protocol Buffers** (**Protobuf**).
    To represent a wide range of ML models, the ONNX specification is comprised of
    a definition of an extensible computation graph model, as well as definitions
    of standard data types and built-in operators. With the ONNX specification, many
    ML models consisting of a variety of supported architectures, building blocks,
    operations, and data types can be efficiently represented in a single file, which
    we call the *ONNX model*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ONNX规范**：一种使用**协议缓冲区**（**Protobuf**）进行模型定义和模型权重**高效序列化和反序列化**的数据格式。为了表示广泛的机器学习模型，ONNX规范由可扩展的计算图模型的定义以及标准数据类型和内置算子的定义组成。使用ONNX规范，许多由各种支持的架构、构建块、操作和数据类型组成的机器学习模型可以高效地表示在单个文件中，我们称之为*ONNX模型*。'
- en: '**ONNX Runtime**: An efficient *native inferencing engine* with bindings to
    many higher-level languages, such as C#, Python, JavaScript, Java/Kotlin (Android),
    and Objective-C (iOS). This means that with the ONNX Runtime bindings for one
    of these languages, we can load, score, and even train ONNX models. It also provides
    built-in GPU acceleration using DirectML, TensorRT, **Deep Neural Network Library**
    (**DNNL**), nGraph, CUDA, and the **Microsoft Linear Algebra Subprograms** (**MLAS**)
    library, and weight quantization and graph optimization to run efficiently on
    various compute targets, such as Cloud Compute, Jupyter kernels, mobile phones,
    and web browsers.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ONNX Runtime**: 一个高效的*原生推理引擎*，支持与多种高级语言绑定，例如C#、Python、JavaScript、Java/Kotlin（Android）和Objective-C（iOS）。这意味着，通过这些语言之一的ONNX
    Runtime绑定，我们可以加载、评分，甚至训练ONNX模型。它还提供了内置的GPU加速，使用DirectML、TensorRT、**深度神经网络库**（**DNNL**）、nGraph、CUDA以及**微软线性代数子程序**（**MLAS**）库，并支持权重量化和图优化，以便在各种计算目标上高效运行，例如云计算、Jupyter内核、手机和网页浏览器。'
- en: '**ONNX ecosystem**: A *collection of libraries* that facilitate conversion
    from and to ONNX. ONNX libraries can be broadly categorized into ONNX frontends
    (*to ONNX*) and ONNX backends (*from ONNX*). While *ONNX frontend* libraries help
    to convert arbitrary computations into ONNX models (models following the ONNX
    specification), *ONNX backend* libraries provide support to execute ONNX models
    or to convert ONNX models into a specific framework runtime. ONNX is widely used
    within Microsoft as well as other large companies and, therefore, supports a wide
    range of frameworks and languages. Many popular libraries are officially supported
    frontends, such as traditional ML algorithms, scikit-learn, LightGBM, XGBoost,
    and CatBoost, as well as modern DL frameworks, such as TensorFlow, Keras, PyTorch,
    Caffe 2, and CoreML.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ONNX生态系统**：一组*库集合*，便于在ONNX之间进行转换。ONNX库可以广泛分为ONNX前端（*转换为ONNX*）和ONNX后端（*从ONNX*）。虽然*ONNX前端*库帮助将任意计算转换为ONNX模型（遵循ONNX规范的模型），但*ONNX后端*库提供执行ONNX模型或将其转换为特定框架运行时的支持。ONNX在微软以及其他大型公司中广泛使用，因此支持广泛的框架和语言。许多流行的库是官方支持的前端，例如传统的机器学习算法、scikit-learn、LightGBM、XGBoost和CatBoost，以及现代深度学习框架，如TensorFlow、Keras、PyTorch、Caffe
    2和CoreML。'
- en: ONNX is a great choice for providing model interoperability to allow an organization
    to decouple model training, model serialization, and model inferencing. Let's
    learn about popular ONNX frontends and backends in action in the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX是提供模型互操作性以允许组织解耦模型训练、模型序列化和模型推理的绝佳选择。让我们在下一节中了解ONNX的流行前端和后端在实际中的应用。
- en: Converting models to ONNX format with ONNX frontends
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ONNX前端将模型转换为ONNX格式
- en: 'ONNX frontends are packages, tools, or libraries that can convert existing
    ML models or numeric computations into ONNX models. While popular ML frameworks
    used to implement ONNX export out of the box (similar to the PyTorch `torch.onnx`
    module), most frameworks today support ONNX through a separate conversion library.
    The most popular ONNX frontends at the time of writing are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX前端是包、工具或库，可以将现有的机器学习模型或数值计算转换为ONNX模型。虽然流行的机器学习框架通常自带ONNX导出功能（类似于PyTorch的`torch.onnx`模块），但如今大多数框架都通过单独的转换库支持ONNX。在撰写本文时，最受欢迎的ONNX前端如下：
- en: '`skl2onnx`: Converts scikit-learn models to ONNX'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skl2onnx`：将scikit-learn模型转换为ONNX'
- en: '`tf2onnx`: Converts TensorFlow models to ONNX'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf2onnx`：将TensorFlow模型转换为ONNX'
- en: '`onnxmltools`: Converts XGBoost, LightGBM, CatBoost, H2O, libsvm, and CoreML
    models to ONNX'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onnxmltools`：将XGBoost、LightGBM、CatBoost、H2O、libsvm和CoreML模型转换为ONNX'
- en: '`torch.onnx`: Converts PyTorch models to ONNX'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.onnx`：将PyTorch模型转换为ONNX'
- en: 'Once, the ONNX frontend libraries are installed, the conversion to ONNX specification
    is often simply done by running a single command. Let''s see this in action with
    TensorFlow as an example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了ONNX前端库，将模型转换为ONNX规范通常只需运行一个命令。让我们以TensorFlow为例看看这个操作：
- en: 'First, we will save a Keras model using the TensorFlow `SaveModel` format.
    We can achieve this by calling `model.save()` and providing the path to serialize
    the `SaveModel` model to disk:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用TensorFlow的`SaveModel`格式保存一个Keras模型。我们可以通过调用`model.save()`并提供将`SaveModel`模型序列化到磁盘的路径来实现这一点：
- en: train.py
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: train.py
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can then use the `tf2onnx` library to convert the `SaveModel` model into
    an ONNX model, as shown in the following snippet:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`tf2onnx`库将`SaveModel`模型转换为ONNX模型，如下面的代码片段所示：
- en: convert.sh
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: convert.sh
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we see in the preceding example, all we need is a single command to convert
    TensorFlow models into ONNX models. Once we have an ONNX model, we can use ONNX
    backends to score them, as shown in the following section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，我们只需要一个命令就可以将TensorFlow模型转换为ONNX模型。一旦我们有了ONNX模型，我们可以使用ONNX后端对其进行评分，如下节所述。
- en: Native scoring of ONNX models with ONNX backends
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ONNX后端对ONNX模型进行原生评分
- en: Once a model is exported as an ONNX model, we can load it using an ONNX-compatible
    backend. The reference implementation for the ONNX backend is called **ONNX Runtime**,
    and is a native implementation with bindings in many high-level languages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被导出为ONNX模型，我们可以使用ONNX兼容的后端加载它。ONNX后端的参考实现被称为**ONNX Runtime**，它是一个具有许多高级语言绑定的本地实现。
- en: 'First, we can load, analyze, and check an ONNX model using the `onnx` library,
    as shown in the following example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以使用`onnx`库加载、分析和检查一个ONNX模型，如下面的示例所示：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, if we want to score the model, we need to use the `onnxruntime` backend
    library. First, we need to load the model for an inferencing session; this means
    we can load the optimized model and don''t need to allocate any buffers for storing
    gradients. In the next step, we can score the model by executing `run(output_names,
    input_feed, run_options=None)`. The `output_names` argument refers to the named
    output layer we want to return from the model, whereas `input_feed` represents
    the data we want to pass to the model. The scoring properties, such as the log
    level, can be configured through the `run_options` argument. The following example
    shows how to score the model and return the last layer''s output from an ONNX
    model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想对模型进行评分，我们需要使用`onnxruntime`后端库。首先，我们需要为推理会话加载模型；这意味着我们可以加载优化后的模型，而无需分配任何用于存储梯度的缓冲区。在下一步中，我们可以通过执行`run(output_names,
    input_feed, run_options=None)`来对模型进行评分。`output_names`参数指向我们希望从模型中返回的命名输出层，而`input_feed`表示我们想要传递给模型的数据。评分属性，如日志级别，可以通过`run_options`参数进行配置。以下示例展示了如何对模型进行评分并从ONNX模型中返回最后一层的输出：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code, we load the ONNX model optimized for inferencing, pass
    data to the model's `input` parameter, and return the last layer's output using
    the ONNX Runtime Python API. You can access the layer information, as well as
    names of inputs and outputs, using the helper method, `session.get_modelmeta()`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们加载了针对推理优化的ONNX模型，将数据传递给模型的`input`参数，并使用ONNX Runtime Python API返回最后一层的输出。您可以使用辅助方法`session.get_modelmeta()`访问层信息以及输入和输出的名称。
- en: In this section, we learned about ONNX, how to create an ONNX model from trained
    ML models using ONNX frontends, and how to score an ONNX model using ONNX Runtime,
    the reference implementation for an ONNX backend. While we looked only at the
    Python API of ONNX Runtime, many other high-level bindings are available.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了ONNX，学习了如何使用ONNX前端从训练好的机器学习模型创建ONNX模型，以及如何使用ONNX Runtime（ONNX后端的参考实现）对ONNX模型进行评分。虽然我们只看了ONNX
    Runtime的Python API，但还有许多其他高级绑定可用。
- en: Hardware optimization with FPGAs
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FPGA进行硬件优化
- en: 'In the previous section, we exported a model to ONNX to take advantage of an
    inference-optimized and hardware-accelerated runtime to improve the scoring performance.
    In this section, we will take this approach one step further to deploy on even
    faster inferencing hardware: FPGAs.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们将模型导出为ONNX以利用推理优化和硬件加速的运行时来提高评分性能。在本节中，我们将进一步采取这种方法，在甚至更快的推理硬件上部署：FPGA。
- en: But, before we talk about how to deploy a model to an FPGA, let's first understand
    what an FPGA is and why we would choose one as a target for DL inference instead
    of a GPU.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在我们讨论如何将模型部署到FPGA之前，让我们首先了解FPGA是什么，以及为什么我们会选择它作为深度学习推理的目标而不是GPU。
- en: Understanding FPGAs
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解FPGA
- en: Most people typically come across a specific variety of **integrated circuit**
    (**IC**), called an **application-specific integrated circuit** (**ASIC**). ASICs
    are purpose-built ICs, such as the processor in your laptop, the GPU cores on
    your graphics card, or the microcontroller in your washing machine. These chips
    share the fact that they have a fixed hardware footprint optimized to support
    a specific task. Often, like any general processor, they operate with a specific
    **instruction set**, allowing certain commands to be run. When you program something
    with a higher-level language, such as Java, C++, or Python, the compiler or interpreter
    will translate this high-level code into machine code, which is the set of commands
    the processor understands and is able to run.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人通常遇到一种特定的**集成电路**（**IC**），称为**专用集成电路**（**ASIC**）。ASIC是专门设计的集成电路，例如笔记本电脑中的处理器、显卡上的GPU核心或洗衣机中的微控制器。这些芯片共享的事实是，它们具有固定的硬件足迹，优化以支持特定任务。通常，像任何通用处理器一样，它们使用特定的**指令集**运行，允许运行某些命令。当你用高级语言，如Java、C++或Python编程时，编译器或解释器将这种高级代码转换为机器代码，这是处理器理解并能运行的命令集。
- en: The strength of an ASIC is that the underlying chip architecture can be optimized
    for the specific workload, resulting in the most optimal design for the hardware
    in terms of the area it requires. The weakness of an ASIC is that it is only good
    for performing the specific task it was designed for, and its design is fixed,
    as the underlying hardware cannot be altered.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ASIC的强大之处在于，其底层芯片架构可以根据特定的工作负载进行优化，从而在所需的面积方面实现最优化设计。ASIC的弱点在于，它仅适用于执行其设计时指定的特定任务，并且其设计是固定的，因为底层硬件无法更改。
- en: Even though we can run any task on a standard processor, for something very
    specific, such as the computation and backtracking for thousands of nodes in a
    neural network, they might not be optimal. Therefore, a lot of these calculations
    are now run on a GPU instead, as its chip architecture leans more toward running
    the same calculations in parallel, which leans more toward the ingrained structure
    of a neural network algorithm than a standard CPU would.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以在标准处理器上运行任何任务，但对于非常具体的事情，例如神经网络中数千个节点的计算和回溯，它们可能不是最优的。因此，现在许多这些计算都是在GPU上运行的，因为其芯片架构更倾向于并行运行相同的计算，这更接近神经网络算法的固有结构，而不是标准CPU。
- en: FPGAs are defined by a different concept than their ASIC counterparts. FPGAs
    trade in the most optimal design, especially when it comes to the used area on
    a chip, for the freedom of *re-programmability*. This main feature allows a user
    to purchase an FPGA and then build themselves their own processor, a hardware
    switch, a network router, or anything else, and change the underlying hardware
    design any time they feel like it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA的定义与它们的ASIC对应物不同。FPGA在最优设计方面进行交易，尤其是在芯片上使用的面积方面，以换取*可重编程性*的自由。这一主要特性允许用户购买FPGA，然后自己构建自己的处理器、硬件交换机、网络路由器或其他任何东西，并且可以在任何时候更改底层硬件设计。
- en: As hardware in the end is something physical made up of some form of binary
    logic gates, registers, and wires, this capability of FPGAs might sound like magic.
    Then again, we are using flash drives daily that can store data and can erase
    data again. For example, modern **NAND flash drives** are erased through a process
    called **field electron emission**, which allows a charge to move through a thin
    layer of insulation to *reset* the setting of bits or, to be more precise, blocks
    of bits.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件最终是由某种形式的二进制逻辑门、寄存器和电线组成的物理实体，因此FPGA的这种能力听起来可能像是魔法。然而，我们每天都在使用可以存储和擦除数据的闪存驱动器。例如，现代**NAND闪存驱动器**通过称为**场电子发射**的过程进行擦除，这允许电荷通过一层薄绝缘层来*重置*位或，更准确地说，位块。
- en: 'Remembering this, let''s have a look at the basic building blocks of an FPGA,
    called **logic elements**. *Figure 15.1* shows the general concept of these building
    blocks. Different manufacturers tweak different aspects of these, but the base
    concept remains the same:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，让我们看看FPGA的基本构建块，称为**逻辑元素**。*图15.1*显示了这些构建块的一般概念。不同的制造商调整这些构建块的不同方面，但基本概念保持不变：
- en: '![Figure 15.1 – Structure of a logic element in an FPGA ](img/B17928_15_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图15.1 – FPGA中逻辑元件的结构](img/B17928_15_01.jpg)'
- en: Figure 15.1 – Structure of a logic element in an FPGA
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – FPGA中逻辑元件的结构
- en: 'A logic element is typically made up of the following components:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑元件通常由以下组件组成：
- en: '**Input/output** (**I/O**): Denotes the interconnection with other logical
    elements or with external I/O (think of Ethernet and USB, for example).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入/输出**（**I/O**）：表示与其他逻辑元件或外部I/O（例如，考虑以太网和USB）的互连。'
- en: '**Lookup table** (**LUT**): Holds the main logical function performed in this
    logic element. Any logic in a digital circuit can be broken down to a **Boolean
    function** that maps a certain number of binary inputs to a certain number of
    binary outputs.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查找表**（**LUT**）：存储在此逻辑元件中执行的主要逻辑功能。数字电路中的任何逻辑都可以分解为一个**布尔函数**，该函数将一定数量的二进制输入映射到一定数量的二进制输出。'
- en: '**D-FlipFlop (Register)**: Stores the input value of the current **clock cycle**
    for the next clock cycle, the length of which is the inverse of the **frequency**
    of the running circuit. The idea to store something for the next round is the
    basic principle of all digital hardware and a necessity to be able to do hardware
    pipelining. The maximum processing time between any adjacent registers in the
    circuit defines the maximum frequency the circuit can run at.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**D触发器（寄存器）**：存储当前**时钟周期**的输入值，以便在下一个时钟周期使用，其长度是运行电路**频率**的倒数。存储东西以供下一轮使用是所有数字硬件的基本原则，也是能够进行硬件流水线操作所必需的。电路中任何相邻寄存器之间的最大处理时间定义了电路可以运行的最大频率。'
- en: '**Multiplexer** (**MUX**): Chooses which of its inputs are shown as the output.
    In this case, it either shows the current result from the Boolean function, or
    the one from the previous clock cycle.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多路复用器**（**MUX**）：选择其输入中哪一个被显示为输出。在这种情况下，它要么显示布尔函数的当前结果，要么显示前一个时钟周期的结果。'
- en: Through the LUT, any Boolean function (and through a register, any multi-layered
    hardware logic) can be realized. In addition, the LUT can be erased and reset,
    which enables the reprogrammable nature of FPGAs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查找表，任何布尔函数（以及通过寄存器，任何多层硬件逻辑）都可以实现。此外，查找表可以被擦除和重置，这使得FPGA具有可编程性。
- en: 'The full schematic structure of an FPGA is shown in *Figure 15.2*. Just understand
    that a normal-sized FPGA will have upward of 500,000 logic elements:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: FPGA的完整示意图结构在*图15.2*中显示。只需理解一个正常大小的FPGA将拥有超过500,000个逻辑元件：
- en: '![Figure 15.2 – Schematic structure of an FPGA ](img/B17928_15_02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图15.2 – FPGA的示意图结构](img/B17928_15_02.jpg)'
- en: Figure 15.2 – Schematic structure of an FPGA
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 – FPGA的示意图结构
- en: In addition to logic elements, *Figure 15.2* shows **switch matrices** and **I/O
    blocks**. Switch matrices are the last piece of the puzzle and allow the setting
    and resetting of the required connections among logic elements, and between them
    and the I/O blocks. With their help, it is possible to fully reprogram the circuit
    structure on an FPGA.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了逻辑元件外，*图15.2*还显示了**开关矩阵**和**I/O块**。开关矩阵是最后一部分拼图，允许在逻辑元件之间以及它们与I/O块之间设置和重置所需的连接。借助它们，可以完全重新编程FPGA上的电路结构。
- en: Finally, to facilitate the programming of an FPGA, a so-called **hardware description
    language** (**HDL**) is used. There are two major languages used for hardware
    design (be it for FPGAs or ASICs), **SystemVerilog** and **VHDL**. When you see
    code written in these languages, it might look like a high-level programming language,
    but in reality, you are not programming anything; you are instead *describing*
    the desired hardware architecture. In a sense, you give the machine a picture
    of a circuit in the form of code, and it tries to map this onto the given elements
    on the FPGA. This step is called **synthesis**. After this step, a binary is sent
    to the FPGA that populates the required logic elements with the correct Boolean
    functions and sets all the interconnections accordingly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了便于FPGA的编程，使用了一种所谓的**硬件描述语言**（**HDL**）。用于硬件设计的两种主要语言是**SystemVerilog**和**VHDL**。当你看到用这些语言编写的代码时，它可能看起来像高级编程语言，但实际上，你并没有在编程任何东西；你是在*描述*所需的硬件架构。从某种意义上说，你以代码的形式给机器提供了一张电路的图片，它试图将此映射到FPGA上的给定元素。这一步称为**综合**。在此步骤之后，将一个二进制文件发送到FPGA，该文件用正确的布尔函数填充所需的逻辑元件，并相应地设置所有互连。
- en: Besides this logical structure, you will find a lot of other integrated systems
    in modern FPGAs, combining the strength of ASICs and FPGAs. You might even find
    a processor such as an **ARM Cortex** on the IC itself. The idea is to let anything
    that would be extremely time-consuming to build from scratch on the FPGA fabric
    run on the processor instead while using the FPGA to host your custom hardware
    designs. For example, it would take a lot of time to build the lower layers of
    the Ethernet protocol on an FPGA, as TCP requires a highly sophisticated hardware
    circuit. Therefore, outsourcing this part into a processor can speed up development
    time immensely.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种逻辑结构之外，你还会在现代FPGA中发现许多其他集成系统，结合了ASIC和FPGA的优势。你甚至可能会在IC上找到像**ARM Cortex**这样的处理器。其理念是让任何在FPGA上从头开始构建将非常耗时的事物都在处理器上运行，同时使用FPGA来托管你的定制硬件设计。例如，在FPGA上构建以太网协议的低层将花费很多时间，因为TCP需要一个高度复杂的硬件电路。因此，将这部分外包给处理器可以极大地加快开发时间。
- en: Now that we have a general idea of what an FPGA is and how it works, let's discuss
    why they might be more useful for DL than GPUs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对FPGA是什么以及它是如何工作的有一个大致的了解，让我们讨论为什么它们可能比GPU更适合深度学习。
- en: Comparing GPUs and FPGAs for deep neural networks
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较GPU和FPGA在深度神经网络中的应用
- en: As we discussed in the previous section, the underlying hardware structure of
    a GPU supports deep neural networks for training and inference. The reason for
    this is that they are designed with 3D image rendering in mind and, therefore,
    have a lot of logic on board to facilitate matrix multiplications, a task that
    is extremely time-consuming on CPUs and crucial for DNNs. Through GPUs, the processing
    time can typically be lowered from days to mere hours. The same can be said for
    FPGAs, as we can basically build any specialized circuit we require to optimize
    the speed and power consumption of any tasks we want to perform.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的那样，GPU的底层硬件结构支持深度神经网络的训练和推理。原因在于它们的设计考虑了3D图像渲染，因此，在板上有很多逻辑来促进矩阵乘法，这是一个在CPU上耗时极多的任务，对于DNNs至关重要。通过GPU，处理时间通常可以从几天降低到仅仅几小时。对于FPGA也是如此，因为我们基本上可以构建我们需要的任何专用电路来优化我们想要执行的任务的速度和功耗。
- en: 'Therefore, both are options that are far superior for DNNs than general CPUs.
    But, which one should we choose and why? Let''s now go through a list of aspects
    to consider and how each of these two options fares in both cases:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于DNNs来说，这两种选择都比通用CPU优越得多。但是，我们应该选择哪一个，为什么？现在让我们通过一个考虑方面的列表来探讨，以及这两个选项在这两种情况下各自的表现如何：
- en: '**Complexity to implement**: GPUs typically offer a software-level language
    (for example, CUDA) to disconnect the programmer from the underlying hardware.
    For FPGAs, the programmer must understand the hardware domain and how to design
    for it. Therefore, building the correct circuit for an FPGA is far more complicated
    than just using another library in a high-level programming language. But, there
    is work being done to abstract this layer as much as possible with specialized
    tooling and converters.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施复杂性**：GPU通常提供一种软件级语言（例如，CUDA），以使程序员与底层硬件分离。对于FPGA，程序员必须理解硬件领域以及如何为其设计。因此，为FPGA构建正确的电路比在高级编程语言中使用另一个库要复杂得多。但是，正在通过专门的工具和转换器尽可能多地抽象这一层。'
- en: '**Power consumption**: GPUs produce a lot of heat and require a lot of cooling
    and electricity. This is because of the additional complexity of the hardware
    design in order to facilitate software programmability, in turn supporting the
    base hardware stack of RAM, CPU, and GPU. FPGAs, on the other hand, do not require
    this stack to operate and, therefore, in most cases, have a low to medium power
    output, through which they are 4 to 10 times more power-efficient than GPUs.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功耗**：GPU产生大量热量，需要大量冷却和电力。这是因为为了促进软件可编程性而增加的硬件设计复杂性，从而支持RAM、CPU和GPU的基础硬件堆栈。另一方面，FPGA不需要这个堆栈来运行，因此，在大多数情况下，具有低到中等的功耗，这使得它们比GPU节能4到10倍。'
- en: '**Hardware stack**: GPUs are dependent on the whole memory management of the
    standard hardware stack (CPU cache, RAM, and GPU memory), and require an external
    system to control them. This leads to an inefficient but required hardware design
    for GPUs to facilitate the connection layers to the standard hardware stack, which
    makes it less performant. FPGAs, on the other hand, have all the required elements
    (such as high-speed memory) on board the IC and, therefore, can run completely
    *autonomously* without pulling any data from system memory or any other place.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件堆栈**: GPU依赖于整个标准硬件堆栈的内存管理（CPU缓存、RAM和GPU内存），并需要一个外部系统来控制它们。这导致GPU的硬件设计既低效又必需，以促进连接层到标准硬件堆栈，这使其性能降低。另一方面，FPGA在其IC上拥有所有必需的元素（如高速内存），因此可以完全*自主*运行，无需从系统内存或其他任何地方拉取数据。'
- en: '**Latency and interconnectability**: While GPUs are connected to a standard
    hardware stack and only have a few actual hardware ports at the back of it (HDMI
    and DisplayPort), which are often only outputs, an FPGA can connect to anything.
    This means it can support vastly different input and output standards at the same
    time, making it extremely flexible and adaptable to any given situation. In addition,
    it can process data with very low latency, as no data needs to pass through the
    system memory, CPU, or SW layer, making it far superior for applications such
    as real-time video processing.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟和互连性**: 虽然GPU连接到标准的硬件堆栈，并且在其后面只有几个实际的硬件端口（HDMI和DisplayPort），这些端口通常仅作为输出，但FPGA可以连接到任何设备。这意味着它可以同时支持大量不同的输入和输出标准，使其极其灵活且能够适应任何特定情况。此外，它能够以非常低的延迟处理数据，因为不需要通过系统内存、CPU或SW层传输数据，这使得它在实时视频处理等应用中远优于GPU。'
- en: '**Flexibility**: Even though GPUs have a parallel hardware architecture, you
    might not be able to use it effectively. The specific DNN algorithm must be mapped
    to the underlying hardware, and this might be neither perfect nor even feasible.
    It falls into the same problem class as distributing processes among CPU cores.
    In addition, GPUs are designed to handle 32-bit or 64-bit standard data types.
    If you are using a very specialized data type or a custom one, you might not be
    able to run it on a GPU at all. FPGAs, on the other hand, allow you to define
    whatever data size or data type you want to work with and, on top of that, allow
    even a so-called *partial reconfiguration* during runtime, which it uses to reprogram
    parts of the logic during runtime.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性**: 尽管GPU具有并行硬件架构，但你可能无法有效地使用它。特定的DNN算法必须映射到底层硬件，这可能既不完美也不可行。它属于与在CPU核心之间分配进程相同的问题类别。此外，GPU设计用于处理32位或64位标准数据类型。如果你使用的是非常专业的数据类型或自定义数据类型，你可能根本无法在GPU上运行它。另一方面，FPGA允许你定义你想要工作的任何数据大小或数据类型，并且在此基础上，甚至在运行时允许所谓的*部分重构*，它使用此功能在运行时重新编程逻辑的一部分。'
- en: '**Industry readiness**: In a typical industrial scenario, be it defense, manufacturing,
    smart cities, or any other, the hardware deployed must be compact, must have a
    long lifespan, should have low power consumption, should survive the environment
    it is positioned in (dust, heat, humidity), and in some scenarios, needs to have
    *functional safety*, which means it must follow certain compliance standards and
    protocols. A GPU is a bad choice for any of these circumstances, as it is very
    power-hungry, has a lifespan of 2 to 5 years, requires massive amounts of cooling,
    does not survive hostile environments, and does not have functional safety. FPGAs
    were designed with industrial settings in mind and, therefore, are typically built
    for long life (10 to 30 years) and safety, while having a low footprint on power
    and required space.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业适应性**: 在典型的工业场景中，无论是国防、制造业、智能城市还是其他任何领域，部署的硬件必须紧凑，必须具有较长的使用寿命，应该具有低功耗，应该能够适应其所在的环境（灰尘、高温、湿度），在某些情况下，还需要具备*功能性安全*，这意味着它必须遵循某些合规标准和协议。GPU对于这些情况中的任何一种都不是一个好的选择，因为它非常耗电，使用寿命为2到5年，需要大量的冷却，无法在恶劣环境中生存，并且没有功能性安全。FPGA的设计初衷就是针对工业环境，因此通常是为了长期使用（10到30年）和安全而构建的，同时功耗和所需空间的影响很小。'
- en: '**Costs**: If you''ve ever bought a GPU for your PC, you might have an idea
    of the cost of such an extension card. FPGAs, on the other hand, can be expensive
    but are typically cheaper to obtain for comparable setup requirements.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**: 如果你曾经为你的PC购买过GPU，你可能对这种扩展卡的代价有所了解。另一方面，FPGA可能成本较高，但对于相当的需求配置来说，通常更便宜。'
- en: 'Taking all these points into consideration, FPGAs are technically superior
    in most ways and often cheaper, but have the major problem that they require developers
    to understand hardware design. This problem led to the creation of toolkits helping
    bridge the gap between hardware and ML development, some of which are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些因素，FPGAs在大多数方面在技术上更优越，而且通常更便宜，但它们的主要问题是需要开发者理解硬件设计。这个问题导致了帮助弥合硬件和机器学习开发之间差距的工具包的创建，以下是一些例子：
- en: '**Vitis AI for Xilinx FPGAs**: A development kit for ML inferencing utilizing
    pre-designed **Deep Learning Processor Units** (**DLUs**). More information can
    be found here: [https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html).
    In addition, you can find some information on how to use this with the NP VM series
    in Azure here: [https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure](https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vitis AI for Xilinx FPGAs**：这是一个用于机器学习推理的预设计**深度学习处理器单元（DLUs**）的开发套件。更多信息请参阅[https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html](https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html)。此外，您还可以在此处找到有关如何使用NP
    VM系列在Azure中的一些信息：[https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure](https://github.com/Xilinx/Vitis-AI/tree/master/docs/azure)。'
- en: '**OpenVINO for Intel FPGAs**: A development kit for DL and ML inferencing.
    More information can be found here: [https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html](https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenVINO for Intel FPGAs**：这是一个用于深度学习和机器学习推理的开发套件。更多信息请参阅[https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html](https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/solutions.html)。'
- en: '**Microsoft Project Brainwave**: A development platform for DL and ML inferencing
    for computer vision and NLP. More information can be found here: [https://www.microsoft.com/en-us/research/project/project-brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Project Brainwave**：这是一个用于计算机视觉和NLP的深度学习和机器学习推理的开发平台。更多信息请参阅[https://www.microsoft.com/en-us/research/project/project-brainwave](https://www.microsoft.com/en-us/research/project/project-brainwave)。'
- en: These are just a few options to support the deployment and acceleration of ML
    models through FPGAs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是支持通过FPGA部署和加速机器学习模型的一些选项。
- en: Important Note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: FPGAs are a very exceptional technology, but they require an ample understanding
    of hardware design to be used efficiently and successfully in any project, or
    a very sophisticated toolkit for abstracting the hardware layer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: FPGAs是一种非常卓越的技术，但它们需要充分理解硬件设计才能在任何项目中高效且成功地使用，或者需要一个非常复杂的工具集来抽象硬件层。
- en: Now that we know why we might prefer to take an FPGA for DNNs, let's have a
    brief look at how FPGAs can be utilized in that regard with Azure Machine Learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道了为什么我们可能更愿意选择FPGA用于DNN，那么让我们简要地看看如何使用Azure机器学习来利用FPGA在这方面。
- en: Running DNN inferencing on Intel FPGAs with Azure
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Azure上使用Intel FPGAs运行DNN推理
- en: As discussed in the previous section, building a hardware design for an FPGA
    is not an easy task. You could certainly do this from scratch utilizing one of
    the Azure VMs sporting an FPGA ([https://docs.microsoft.com/en-us/azure/virtual-machines/np-series](https://docs.microsoft.com/en-us/azure/virtual-machines/np-series)),
    or with your own FPGA development kit. Another option is to use the hardware-accelerated
    Python package that is available in the Azure Machine Learning Python SDK. This
    package gives you an abstraction layer through a generic hardware design supporting
    a subset of models and options to use, specifically ones for DNN inferencing.
    Through this, you have access to the **Azure PBS VM family**, which has an Intel
    FPGA attached and is only available through Azure Machine Learning. This machine
    type is deployable in East US, Southeast Asia, West Europe, and West US 2.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，为FPGA构建硬件设计并非易事。您当然可以从头开始，利用Azure VM中配备FPGA的选项（[https://docs.microsoft.com/en-us/azure/virtual-machines/np-series](https://docs.microsoft.com/en-us/azure/virtual-machines/np-series)），或者使用您自己的FPGA开发套件。另一个选择是使用Azure机器学习Python
    SDK中可用的硬件加速Python包。此包通过支持模型子集和选项的通用硬件设计提供抽象层，特别是用于DNN推理的选项。通过这种方式，您可以访问**Azure
    PBS VM系列**，该系列配备有Intel FPGA，并且仅通过Azure机器学习提供。这种机器类型可在东US、东南亚、西欧和西US 2进行部署。
- en: The general approach is very similar to ONNX; you take a trained model and convert
    it to a specific format that can be executed on FPGAs. In this case, your model
    must be either ResNet, DenseNet, VGG, or SSD-VGG, and must be written in TensorFlow
    in order to fit the underlying hardware design. Furthermore, we will use quantized
    16-bit float model weights converted to ONNX models, which will be run on the
    FPGA. For these models, FPGAs give you the best inference performance in the cloud.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通用方法与ONNX非常相似；您将一个训练好的模型转换为可以在FPGA上执行的具体格式。在这种情况下，您的模型必须是ResNet、DenseNet、VGG或SSD-VGG，并且必须使用TensorFlow编写，以便适应底层硬件设计。此外，我们将使用量化16位浮点模型权重转换为ONNX模型，这些模型将在FPGA上运行。对于这些模型，FPGA在云中提供了最佳推理性能。
- en: 'To enable hardware acceleration through FPGAs, we require a few extra steps
    compared to the ONNX example. The following list shows what steps need to be performed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过FPGA实现硬件加速，与ONNX示例相比，我们需要执行一些额外的步骤。以下列表显示了需要执行哪些步骤：
- en: Pick a supported model featurizer.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个受支持的模型特征提取器。
- en: Train the supported model with a custom classifier.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义分类器训练受支持的模型。
- en: Quantize the model featurizer's weights to 16-bit precision.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型特征提取器的权重量化为16位精度。
- en: Convert the model to an ONNX format.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型转换为ONNX格式。
- en: (Optional) Register the model.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (可选) 注册模型。
- en: Create a compute target (preferably Azure Kubernetes Service) with PBS nodes.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个计算目标（最好是Azure Kubernetes服务）带有PBS节点。
- en: Deploy the model.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署模型。
- en: Important Note
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: As the code is cluttered and hard to interpret, we will skip the code examples
    in this section. However, you can find detailed examples of FPGA model training,
    conversion, and deployments on Azure's GitHub repository at [https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models).
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于代码杂乱且难以理解，我们将跳过本节中的代码示例。然而，您可以在Azure的GitHub仓库中找到有关FPGA模型训练、转换和部署的详细示例，链接为[https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/deployment/accelerated-models)。
- en: Let's discuss these steps in some more detail.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这些步骤。
- en: From the DNN layers we discussed in [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, only the feature extractor layers (`azureml.accel.models`
    package ([https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models](https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models)).
    You can attach any classification or regression head (or both) on top using TensorFlow
    or Keras, but they will not be hardware-accelerated, similar to running only certain
    operations on GPUs. The designers opted here to deploy only the most time-consuming
    parts onto the FPGA.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们讨论的[*第10章*](B17928_10_ePub.xhtml#_idTextAnchor165)中，*在Azure上训练深度神经网络*，只有特征提取器层（`azureml.accel.models`包
    ([https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models](https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.models)）。您可以使用TensorFlow或Keras在顶部附加任何分类或回归头（或两者），但它们将不会进行硬件加速，类似于仅在GPU上运行某些操作。设计者在这里选择仅将最耗时的部分部署到FPGA上。
- en: In the next step, you can train the model, consisting of a predefined feature
    extractor and a custom classification head, using your own data and weights, or
    by fine-tuning, for example, provided ImageNet weights. This should happen with
    32-bit precision, as convergence will be faster during training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，您可以使用自己的数据和权重，或者通过微调，例如使用提供的ImageNet权重，来训练一个由预定义的特征提取器和自定义分类头组成的模型。这应该在32位精度下进行，因为训练期间收敛会更快。
- en: Once the training is finished, you need to quantize the weights of the featurizer
    into half-precision floats, using the quantized models provided in the `azureml.accel.models`
    package. This step needs to be done because the designers opted here for a fixed
    data size of 16-bit in order to make the hardware design as generic and reusable
    as possible.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，您需要使用`azureml.accel.models`包中提供的量化模型，将特征提取器的权重量化为半精度浮点数。这一步骤需要执行，因为设计者在这里选择了16位固定数据大小，以便使硬件设计尽可能通用和可重用。
- en: For the next step, you convert the whole model into an ONNX model, using the
    `AccelOnnxConverter` method from the same Azure package. In addition, the `AccelContainerImage`
    class helps you to define `InferenceConfig` for the FPGA-based compute targets.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，你将使用来自同一 Azure 包的 `AccelOnnxConverter` 方法将整个模型转换为 ONNX 模型。此外，`AccelContainerImage`
    类帮助你为基于 FPGA 的计算目标定义 `InferenceConfig`。
- en: Finally, you can register your model using the Azure Machine Learning model
    registry, and you can create an AKS cluster using the `Standard_PB6s` nodes. Once
    the cluster is up and running, you use your `Webservice.deploy_from_image` method
    to deploy the web service.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用 Azure Machine Learning 模型注册表注册你的模型，并可以使用 `Standard_PB6s` 节点创建一个 AKS
    集群。一旦集群启动并运行，你就可以使用你的 `Webservice.deploy_from_image` 方法部署网络服务。
- en: Important Note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You can find a detailed example of the deployment steps in the Azure Machine
    Learning documentation here: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Azure Machine Learning 文档中找到部署步骤的详细示例：[https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service)。
- en: The workflow to deploy a model through Azure Machine Learning to an FPGA-based
    compute target is a bit different from simply deploying ONNX models, as you have
    to consider the limited supported selection of models right from the beginning.
    Another difference is that, while you choose a predefined supported model for
    FPGA deployment, you can only accelerate the feature extractor part of the model.
    This means you have to attach an additional classification or regression head—a
    step that is not immediately obvious. Once you understand this, it will make more
    sense that you only quantize the feature extractor to half-precision floats after
    training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Azure Machine Learning 将模型部署到基于 FPGA 的计算目标的工作流程与简单地部署 ONNX 模型略有不同，因为你从一开始就必须考虑有限支持的模型选择。另一个区别是，当你选择一个预定义的受支持模型进行
    FPGA 部署时，你只能加速模型的特征提取部分。这意味着你必须附加一个额外的分类或回归头——这是一个不太明显的一步。一旦你理解了这一点，你就会觉得在训练后只对特征提取进行半精度浮点量化更有意义。
- en: While this process seems a bit difficult and customized, the performance and
    latency gain, especially when dealing with predictions on image data, is huge.
    But, you should take advantage of this optimization only if you are ready to adapt
    your training processes and pipelines to this specific environment, as shown throughout
    the section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个过程看起来有点困难且定制化，但在处理图像数据的预测时，性能和延迟的增益是巨大的。但是，只有当你准备好将你的训练过程和管道适应到这个特定环境时，你才应该利用这种优化，正如本节中所示。
- en: Now that we have a good understanding of what FPGAs are and how we can utilize
    them through Azure Machine Learning, let's have a look in the next section at
    what other Azure services we can integrate with our models.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地理解了 FPGA 是什么以及我们如何通过 Azure Machine Learning 利用它们，让我们在下一节中看看我们可以与我们的模型集成的其他
    Azure 服务。
- en: Integrating ML models and endpoints with Azure services
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 ML 模型和端点与 Azure 服务集成
- en: Relying on the Azure Machine Learning service either for experimentation, performing
    end-to-end training, or simply registering your trained models and environments
    brings you a ton of value. In [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217),
    *Model Deployment, Endpoints, and Operations*, we covered two main scenarios,
    a real-time scoring web service through automated deployments and batch scoring
    through a deployed pipeline. While these two use cases are quite different in
    requirement and deployment types, they show what is possible once you have a trained
    model and packaged environment stored in Azure Machine Learning. In this section,
    we will discuss how to use and integrate these models or their endpoints in other
    Azure services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 依靠 Azure Machine Learning 服务进行实验、执行端到端训练或简单地注册你的训练模型和环境，都能为你带来巨大的价值。在[*第 14
    章*](B17928_14_ePub.xhtml#_idTextAnchor217)，“模型部署、端点和操作”中，我们介绍了两个主要场景，一个是通过自动化部署的实时评分网络服务，另一个是通过部署的管道进行批量评分。虽然这两个用例在需求和部署类型上相当不同，但它们展示了当你有一个存储在
    Azure Machine Learning 中的训练模型和打包环境时，你可以做到什么。
- en: In many scenarios, abstracting your batch-scoring pipeline from the actual data
    processing pipeline to separate concerns and responsibilities makes a lot of sense.
    However, sometimes your scoring should happen directly during the data processing
    or querying time and in the same system. Once your ML model is registered and
    versioned with Azure Machine Learning, you can pull out a specific version of
    the model anywhere using the Azure ML SDK, either in Python, C#, the command line,
    or any other language that can make a call to a REST service.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多场景中，将您的批量评分流程从实际数据处理流程中抽象出来，以分离关注点和责任，是非常有意义的。然而，有时您的评分应该在数据处理或查询时间直接进行，并在同一系统中进行。一旦您的ML模型在Azure
    Machine Learning中注册并版本化，您就可以使用Azure ML SDK在任何地方提取模型的特定版本，无论是Python、C#、命令行还是任何可以调用REST服务的其他语言。
- en: This makes it possible to pull trained and converted ONNX models from a desktop
    application, either during build time or at runtime. You can load models while
    running a Spark job, for example, on Azure Databricks or Azure Synapse. Through
    that, you can avoid transferring TBs of data to a separate scoring service.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得从桌面应用程序中提取训练和转换后的ONNX模型成为可能，无论是在构建时间还是运行时。例如，您可以在Azure Databricks或Azure Synapse上运行Spark作业时加载模型。通过这种方式，您可以避免将TB级的数据传输到单独的评分服务。
- en: Other services, such as Azure Data Explorer, allow you to call models directly
    from the service through a Python extension ([https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin)).
    Azure Data Explorer is an exciting managed service for storing and querying large
    amounts of telemetry data efficiently. It is used internally at Azure to power
    Azure Log Analytics, Azure Application Insights, and Time Series Insights. It
    has a powerful Python runtime with many popular packages available, and so provides
    the perfect service for performing anomaly detection or time-series analysis based
    on your custom models. In addition, it allows you to access its time-series data
    during ML modeling through a Python extension called **Kqlmagic** ([https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic](https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其他服务，如Azure Data Explorer，允许您通过Python扩展直接从服务中调用模型（[https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin)）。Azure
    Data Explorer是一个用于高效存储和查询大量遥测数据的托管服务。它被Azure内部使用，为Azure Log Analytics、Azure Application
    Insights和Time Series Insights提供动力。它拥有强大的Python运行时，提供了许多流行的包，因此提供了执行基于自定义模型的异常检测或时间序列分析的理想服务。此外，它还允许您通过名为**Kqlmagic**的Python扩展在ML建模期间访问其时间序列数据（[https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic](https://docs.microsoft.com/en-us/azure/data-explorer/kqlmagic)）。
- en: Important Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When using Azure Machine Learning for model deployments, you can take advantage
    of all the Azure ecosystem and can expect to see model or endpoint integration
    with more and more Azure services over time.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Azure Machine Learning进行模型部署时，您可以利用所有Azure生态系统的好处，并可以期待随着时间的推移，模型或端点与越来越多的Azure服务进行集成。
- en: Closing this chapter, we will dive deeper into two other integration options
    in the upcoming sections. We will have a look at deploying ML models through **Azure
    IoT Edge** to a gateway or device in the field, and we will look at how to utilize
    ML endpoints for data augmentation in **Power BI**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将深入探讨即将到来的部分中的两个其他集成选项。我们将查看如何通过**Azure IoT Edge**将ML模型部署到现场网关或设备，以及如何利用ML端点在**Power
    BI**中进行数据增强。
- en: Integrating with Azure IoT Edge
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Azure IoT Edge集成
- en: So far, we have discussed different ways to make our models run on systems in
    the cloud, be it on machines with CPUs, GPUs, or FPGAs, either as a batch-scoring
    process or as a real-time endpoint. Now, let's discuss another interesting deployment
    scenario, deploying real-time scorers to one to up to hundreds of thousands of
    devices in the field. The control of such devices and the processing of gathered
    telemetry and events fall under the topic of the so-called **Internet of Things**
    (**IoT**), which enables us to react in near real time to changes and critical
    problems in any sort of environment.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了不同的方法来让我们的模型在云中的系统上运行，无论是CPU、GPU还是FPGA机器，无论是作为批量评分过程还是作为实时端点。现在，让我们讨论另一个有趣的部署场景，将实时评分器部署到现场的一个到数十万个设备上。对这些设备和收集的遥测数据和事件的控制属于所谓的**物联网**（**IoT**）的范畴，它使我们能够几乎实时地对任何环境中的变化和关键问题做出反应。
- en: In these scenarios, the integration of ML allows us to distribute a model to
    a multitude of systems and devices simultaneously, allowing these so-called **edge
    devices** to execute the model on the local runtime in order to react to the result
    of the ML processing accordingly. This could be a local camera system that performs
    ML-powered image processing to react to intruders and send out alarms or any other
    scenario you might imagine.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些场景中，机器学习的集成使我们能够同时将模型分发到多个系统和设备，使这些所谓的**边缘设备**能够在本地运行时执行模型，以便根据机器处理的结果做出相应反应。这可能是一个执行机器学习图像处理以应对入侵者并发送警报的本地摄像头系统，或者任何你可能想象到的其他场景。
- en: To get a base understanding of how to achieve this utilizing the Azure platform,
    let's first have a look at how IoT scenarios are realized through the help of
    **Azure IoT Hub** and other services, and then discuss how this can be integrated
    with Azure Machine Learning and our trained models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何利用 Azure 平台实现这一目标的基础知识，让我们首先看看 IoT 场景是如何通过**Azure IoT Hub**和其他服务的帮助实现的，然后讨论如何将其与
    Azure 机器学习和我们的训练模型集成。
- en: Understanding IoT solutions on Azure
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Azure 上的 IoT 解决方案
- en: 'The basis for any IoT architecture in Azure is Azure IoT Hub. This serves as
    a cloud gateway to communicate with devices and other gateways in the field and
    offers the ability to control them to a certain extent. On the one hand, it runs
    Azure Event Hubs underneath to be able to handle a huge amount of incoming telemetry
    through a distributed structure, not too different from Apache Kafka. On the other
    hand, it serves as a control instrument serving the following functions:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 上任何 IoT 架构的基础是 Azure IoT Hub。它作为云网关与现场中的设备和其他网关进行通信，并能够在一定程度上控制它们。一方面，它运行
    Azure Event Hubs 以能够通过分布式结构处理大量传入遥测，这与 Apache Kafka 并无太大不同。另一方面，它作为控制工具，提供以下功能：
- en: '**Device cataloging**: The ledger of all devices registered to Azure IoT Hub.
    Any device connected receives its own device name and connection configuration,
    defining how the direct connection between hub and device is secured, which happens
    using either a rotating key or a device certificate.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备目录**：记录所有注册到 Azure IoT Hub 的设备。任何连接的设备都会获得自己的设备名称和连接配置，定义了设备与中心之间的直接连接如何安全，这可以通过旋转密钥或设备证书来实现。'
- en: '**Device provisioning**: A service that allows devices to automatically register
    themselves to IoT Hub to obtain either a connection string with a key or a certificate.
    Useful if more than a handful of devices must be registered.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备配置**：一种允许设备自动注册到 IoT Hub 以获取带有密钥的连接字符串或证书的服务。如果需要注册的设备数量较多，则非常有用。'
- en: '**Device twin**: A configuration file that defines important properties for
    the device, which can be set or requested. In between the stream of telemetry,
    the device is asked to send this file sporadically, updating the state of the
    device in the cloud gateway. Therefore, the device twin always holds the most
    recent state of the device. This functionality is automatically implemented when
    using the **Azure IoT device SDK** on the device.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备孪生**：一个配置文件，定义了设备的重要属性，可以设置或请求。在遥测流之间，设备被要求偶尔发送此文件，更新云网关中设备的状态。因此，设备孪生始终持有设备的最新状态。当在设备上使用**Azure
    IoT 设备 SDK**时，此功能会自动实现。'
- en: '**Command and control**: This is enabled through the **Azure IoT Service SDK**.
    Commands from a console or an external application can be used to either send
    new desired properties to single devices, define configurations for a group of
    devices, or send a predefined command that the device needs to understand and
    implement. This could be a request to restart the device or flash its firmware.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命令与控制**：这是通过**Azure IoT 服务 SDK**实现的。来自控制台或外部应用程序的命令可以用来向单个设备发送新的期望属性，为设备组定义配置，或发送设备需要理解和实施的预定义命令。这可能是一个请求重新启动设备或更新其固件的请求。'
- en: '**Monitoring and diagnostics**: A diagnostic view on any incoming and outgoing
    messaging from and to IoT Hub. It can be used to understand the throughput of
    incoming telemetry, understand any control plane information exchanged, and warn
    if a device is unreachable and malfunctioning.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和诊断**：对来自和发送到 IoT Hub 的任何传入和传出消息的诊断视图。它可以用来了解传入遥测的吞吐量，了解交换的任何控制平面信息，并在设备不可达且出现故障时发出警告。'
- en: 'In addition to this cloud gateway, Azure offers a device runtime on the edge
    called Azure IoT Edge, which can be installed on a device or gateway. It is powered
    by the Moby Docker runtime ([https://mobyproject.org/](https://mobyproject.org/)),
    which allows users to deploy Docker containers to a device in the field. The setup
    of any solution operating in this runtime is defined by a **deployment manifest**
    that is set up for an edge device through a device twin configuration file in
    IoT Hub. This manifest defines the following components:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个云网关之外，Azure还提供了一种边缘设备运行时，称为Azure IoT Edge，它可以安装在设备或网关上。它由Moby Docker运行时([https://mobyproject.org/](https://mobyproject.org/))提供支持，允许用户将Docker容器部署到现场设备。在此运行时中运行的任何解决方案的设置都由一个**部署清单**定义，该清单通过IoT
    Hub中的设备孪生配置文件为边缘设备设置。此清单定义以下组件：
- en: '**IoT Edge agent**: Verifies and instantiates modules, checks their state during
    runtime, and reports back any configuration or runtime problem utilizing the device
    twin configuration file. It is the main module of the runtime and is *required*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IoT Edge代理**：验证和实例化模块，在运行时检查它们的状态，并利用设备孪生配置文件报告任何配置或运行时问题。它是运行时的主要模块，是**必需的**。'
- en: '**IoT Edge hub**: Enables the IoT Edge runtime to mimic IoT Hub for additional
    devices connecting to this local edge device. This enables any form of complex
    hierarchy, while devices can use the same protocol communicating with an IoT Edge
    device as they would with IoT Hub. This module is *required*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IoT Edge网关**：使IoT Edge运行时能够模拟IoT Hub，以便连接到该本地边缘设备的额外设备。这允许任何形式的复杂层次结构，同时设备可以使用与IoT
    Hub相同的协议与IoT Edge设备通信。此模块是**必需的**。'
- en: '**Container modules**: Defines the container images to be copied to the edge
    runtime. This is done by defining a link to the source files stored in Azure Container
    Registry. Besides any user-defined container that can be deployed in this manner,
    there are also a bunch of containerized versions of Azure services that can be
    sent to the runtime. This list includes Blob storage, an Azure Function app, certain
    Cognitive Services, and even a small, optimized version of a SQL server called
    **SQL Edge**.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器模块**：定义要复制到边缘运行时的容器镜像。这是通过定义一个链接到存储在Azure容器注册表中的源文件来完成的。除了任何可以通过这种方式部署的用户定义容器外，还有许多Azure服务的容器化版本可以发送到运行时。这个列表包括Blob存储、Azure
    Function应用、某些认知服务，甚至还有一个称为**SQL Edge**的小型、优化的SQL服务器版本。'
- en: '**Local communication via routing**: Defines the first option to connect modules
    together by setting direct connections between inputs and outputs of the various
    modules defined before.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过路由进行本地通信**：通过设置先前定义的各种模块的输入和输出之间的直接连接，定义将模块连接在一起的第一种选项。'
- en: '**Local communication via an MQTT broker**: Defines the second option to connect
    modules together. Instead of setting direct connections, a broker is used to which
    modules can subscribe. This broker also offers connections to external devices
    that understand how to talk to an MQTT broker.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过MQTT代理进行本地通信**：定义将模块连接在一起的第二种选项。而不是设置直接连接，使用一个代理，模块可以订阅。此代理还提供连接到理解如何与MQTT代理通信的外部设备。'
- en: These are the main components and options to consider when defining the deployment
    manifest.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 定义部署清单时，需要考虑的主要组件和选项。
- en: Important Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The greatest strength that Azure IoT Edge brings to the table is the ability
    to define, manage, and version containers in the cloud, and deploy them to thousands
    of devices. With the help of device configurations, we can group devices and only
    target a certain group for a new test update, thus enabling best practices for
    DevOps in an IoT setting.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Azure IoT Edge带来的最大优势是能够在云中定义、管理和版本控制容器，并将它们部署到成千上万的设备上。借助设备配置，我们可以分组设备，并且只为特定组进行新的测试更新，从而在物联网环境中实现DevOps的最佳实践。
- en: 'Now, let''s briefly have a look at an example. *Figure 15.3* shows a simple
    setup for scoring a containerized ML model on incoming telemetry through Azure
    IoT Edge and its connection with Azure IoT Hub:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要地看一下一个示例。*图15.3*显示了通过Azure IoT Edge对传入遥测数据进行容器化ML模型评分的简单设置及其与Azure IoT
    Hub的连接：
- en: '![Figure 15.3 – Azure IoT Hub connecting to the edge runtime ](img/B17928_15_03.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图15.3 – Azure IoT Hub连接到边缘运行时](img/B17928_15_03.jpg)'
- en: Figure 15.3 – Azure IoT Hub connecting to the edge runtime
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 – Azure IoT Hub连接到边缘运行时
- en: The connections in *Figure 15.3* show the internal routing between containers,
    including actioning that takes place locally, while any insights from the ML scoring
    and any initial telemetry are sent additionally to the cloud for further analysis.
    This is the typical scenario for any ML model operating on the edge.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.3*中的连接显示了容器之间的内部路由，包括本地执行的操作，而来自机器学习评分的任何见解和任何初始遥测数据都额外发送到云端进行进一步分析。这是任何在边缘运行的机器学习模型的典型场景。'
- en: With this knowledge in mind, let's now have a look at how to integrate Azure
    Machine Learning in such an IoT architecture.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解这些知识的基础上，我们现在来看看如何将Azure机器学习集成到这样的物联网架构中。
- en: Integrating Azure Machine Learning
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成Azure机器学习
- en: 'In [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054), *Preparing the Azure
    Machine Learning Workspace*, we learned that every Azure Machine Learning workspace
    comes with its own Azure Container Registry. We can now use this registry to achieve
    our goal. *Figure 15.4* shows an example of an end-to-end solution for ML on the
    edge:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B17928_03_ePub.xhtml#_idTextAnchor054)，“准备Azure机器学习工作区”，我们了解到每个Azure机器学习工作区都自带一个Azure容器注册表。我们现在可以使用这个注册表来实现我们的目标。*图15.4*展示了边缘机器学习端到端解决方案的示例：
- en: '![Figure 15.4 – End-to-end ML on Azure IoT Edge scenario ](img/B17928_15_04.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图15.4 – Azure IoT Edge端到端机器学习场景](img/B17928_15_04.jpg)'
- en: Figure 15.4 – End-to-end ML on Azure IoT Edge scenario
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 – Azure IoT Edge端到端机器学习场景
- en: 'It depicts the following steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 它描述了以下步骤：
- en: Collecting telemetry in a storage account, either through routing single messages
    from IoT Hub or through a batch upload from the Blob storage on the edge to the
    storage account in the cloud
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在存储账户中收集遥测数据，无论是通过从IoT Hub路由单个消息，还是通过从边缘的Blob存储批量上传到云端的存储账户
- en: Training an ML model on the captured data as we learned previously
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在捕获的数据上训练ML模型，正如我们之前所学的
- en: Registering a container including the trained model and dependencies in the
    existing Azure Container Registry of the Azure Machine Learning workspace
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Azure机器学习工作区的现有Azure容器注册表中注册包含训练模型和依赖项的容器
- en: Creating an IoT Edge deployment manifest defining an ML module sourced fromAzure
    Container Registry
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个定义从Azure容器注册表源ML模块的IoT Edge部署清单
- en: Deploying the created configuration through Azure IoT Hub to the edge device
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Azure IoT Hub将创建的配置部署到边缘设备
- en: Through this setup, we are now able to deploy and control an ML model on the
    edge, enabling vast scenarios for running low-latency ML solutions on external
    devices.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个设置，我们现在能够部署和控制边缘的ML模型，从而在外部设备上运行低延迟的ML解决方案。
- en: Important Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If you are interested to try this out, feel free to follow the tutorial for
    setting up an example ML model on Azure IoT Edge, found here: [https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro](https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试一下，请随意遵循设置Azure IoT Edge上的示例机器学习模型的教程，教程链接如下：[https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro](https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-01-intro)。
- en: Finally, if you are interested in further options for ML solutions on the edge,
    have a look at one of the newest additions to the Azure IoT space, called **Azure
    Percept** ([https://azure.microsoft.com/en-us/services/azure-percept/](https://azure.microsoft.com/en-us/services/azure-percept/)).
    It offers a ready-made hardware development kit for video and audio inferencing
    that works together with Azure IoT Hub and Azure Machine Learning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你对边缘机器学习解决方案的更多选项感兴趣，可以查看Azure物联网空间中最新的添加之一，名为**Azure Percept** ([https://azure.microsoft.com/en-us/services/azure-percept/](https://azure.microsoft.com/en-us/services/azure-percept/))。它提供了一个现成的硬件开发套件，用于视频和音频推理，可与Azure
    IoT Hub和Azure机器学习协同工作。
- en: Now that we've had a glimpse into the world of IoT and scenarios for ML on the
    edge, let's have a look at how to utilize real-time ML endpoints with Power BI.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对物联网世界和边缘机器学习场景有了初步了解，让我们来看看如何利用Power BI实时机器学习端点。
- en: Integrating with Power BI
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成Power BI
- en: One of the most interesting integrations from an enterprise perspective is the
    Azure Machine Learning integration with Power BI. It allows us to utilize our
    ML endpoints to apply our models to data columns from the comfort of the built-in
    **Power Query editor**. Think for a second how powerful this concept of rolling
    out ML models to be used by data analysts in their BI tools is.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从企业角度来看，最有趣的集成之一是Azure机器学习与Power BI的集成。它允许我们利用我们的ML端点，在内置的**Power Query编辑器**中应用我们的模型到数据列。想想看，将ML模型推广给数据分析师在他们的BI工具中使用是多么强大的概念。
- en: 'Let''s try this out by utilizing the `sentiment-analysis-pbi` endpoint we created
    in [*Chapter 14*](B17928_14_ePub.xhtml#_idTextAnchor217), *Model Deployment, Endpoints,
    and Operations*, by following these steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过利用在[*第14章*](B17928_14_ePub.xhtml#_idTextAnchor217)“模型部署、端点和操作”中创建的`sentiment-analysis-pbi`端点来尝试一下，按照以下步骤操作：
- en: If you haven't done so already, download the Power BI Desktop application ([https://powerbi.microsoft.com/en-gb/desktop/](https://powerbi.microsoft.com/en-gb/desktop/))
    to your machine, run it, and log in.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请将Power BI桌面应用程序（[https://powerbi.microsoft.com/en-gb/desktop/](https://powerbi.microsoft.com/en-gb/desktop/））下载到您的计算机上，运行它，并登录。
- en: Download the `sentiment_examples.csv` file from the chapter repository, and
    select **Get Data** | **Text/CSV** to load the content of this local file into
    an in-memory dataset in Power BI.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从章节存储库下载`sentiment_examples.csv`文件，然后选择**获取数据** | **文本/CSV**，将此本地文件的内容加载到Power
    BI的内存数据集中。
- en: The Power Query editor will open and will show you an icon of the file with
    the name and size. Right-click on that, and select **Text**.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Power Query编辑器将打开，并显示带有名称和大小的文件图标。右键单击该图标，然后选择**文本**。
- en: 'You should be greeted by a table with one column. Rename the column `Phrases`,
    as shown in *Figure 15.5*:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该会看到一个只有一个列的表格。将列重命名为`Phrases`，如图15.5所示：
- en: '![Figure 15.5 – Sample phrases for sentiment analysis ](img/B17928_15_05.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图15.5 – 情感分析样本短语](img/B17928_15_05.jpg)'
- en: Figure 15.5 – Sample phrases for sentiment analysis
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 – 情感分析样本短语
- en: The editor gives you a lot of possibilities to apply transformations to this
    data. Looking at the menu, you should see a button on the far-right side called
    **Azure Machine Learning**. Click on it.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑器为您提供了许多将转换应用于这些数据的方法。查看菜单，您应该在最右侧看到一个名为**Azure机器学习**的按钮。点击它。
- en: 'If you are logged in correctly, you should see all available endpoints in all
    the Azure Machine Learning workspaces you have access to. Select our previously
    created endpoint, `AzureML.sentiment-analysis-pbi`. In the `Phrases` column. This
    will be the input for our ML endpoint. *Figure 15.6* shows what this should look
    like:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您登录正确，您应该看到您有权访问的所有Azure机器学习工作区中的所有可用端点。选择我们之前创建的端点`AzureML.sentiment-analysis-pbi`。在`Phrases`列中。这将是我们的ML端点的输入。*图15.6*显示了它应该看起来是什么样子：
- en: '![Figure 15.6 – Choosing the right ML endpoint in Power BI ](img/B17928_15_06.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图15.6 – 在Power BI中选择正确的ML端点](img/B17928_15_06.jpg)'
- en: Figure 15.6 – Choosing the right ML endpoint in Power BI
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.6 – 在Power BI中选择正确的ML端点
- en: Click on **OK**. Power BI will now start sending the request to the endpoint.
    Please be aware that you might get a warning in one of the Power BI windows concerning
    data privacy, as we are sending potentially private data to another service. Please
    accept this by selecting the first checkbox, so the action can be performed.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**确定**。Power BI现在将开始向端点发送请求。请注意，您可能会在Power BI窗口中收到有关数据隐私的警告，因为我们正在将可能涉及隐私的数据发送到另一个服务。请通过选择第一个复选框接受此警告，以便执行此操作。
- en: 'As a result, you should now see a new column called `AzureML.sentiment-analysis-pbi`,
    with a lot of fields denoted as `Record`. As our endpoints send more than one
    output, we receive a record. You can now click on each record individually, or
    you can click on the small button showing two arrows next to the column header
    name. This allows you to expand this `Record` column into multiple ones. Select
    all column names and press **OK**. *Figure 15.7* shows the result you should see:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果，您现在应该看到一个名为`AzureML.sentiment-analysis-pbi`的新列，其中包含许多标记为`Record`的字段。由于我们的端点发送了多个输出，我们收到了一个记录。您现在可以单独点击每个记录，或者您可以点击列标题旁边显示两个箭头的按钮。这允许您将此`Record`列展开成多个列。选择所有列名并按**确定**。*图15.7*显示了您应该看到的结果：
- en: '![Figure 15.7 – Power BI sentiment results ](img/B17928_15_07.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图15.7 – Power BI情感分析结果](img/B17928_15_07.jpg)'
- en: Figure 15.7 – Power BI sentiment results
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7 – Power BI情感分析结果
- en: As we can see, the model gives a label for each sentence (`NEGATIVE` or `POSITIVE`)
    and a confidence value score, denoting how sure the ML model is about the label
    given. The results are reasonably accurate, except perhaps for the fourth phrase.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，该模型为每个句子给出一个标签（`NEGATIVE`或`POSITIVE`）和一个置信度值分数，表示ML模型对给定标签的确定性。结果相当准确，也许第四个短语除外。
- en: You can now click **Close & Apply** in the upper left-hand corner, which will
    result in Power BI creating an ML-enhanced dataset, with which you could now build
    visuals in a report and eventually publish a report to the Power BI service in
    the cloud.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以点击左上角的**关闭并应用**，这将导致Power BI创建一个增强型ML数据集，您现在可以构建报告中的可视化，并最终将报告发布到云中的Power
    BI服务。
- en: As you can see for yourself, integrating with Power BI is a quick and easy way
    to empower everyone to utilize your deployed ML endpoints with their business
    data, while not understanding much about the inner workings of the ML services.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如您自己所见，与Power BI集成是一种快速简单的方法，使每个人都能利用他们的业务数据使用您部署的ML端点，同时不必深入了解ML服务的内部工作原理。
- en: Feel free to add some of your own phrases to play around with.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 随意添加一些你自己的短语来尝试一下。
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to convert ML models into a portable and executable
    format with ONNX, what an FPGA is, and how we can deploy a DNN featurizer to an
    FPGA VM through Azure Machine Learning. In addition, we learned how to integrate
    our ML models into various Azure services, such as Azure IoT Edge and Power BI.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用ONNX将ML模型转换为便携和可执行格式，什么是FPGA，以及我们如何通过Azure Machine Learning将DNN特征提取器部署到FPGA
    VM。此外，我们还学习了如何将我们的ML模型集成到各种Azure服务中，例如Azure IoT Edge和Power BI。
- en: This concludes our discussion through the previous two chapters on the various
    options to deploy ML models for batch or real-time inferencing.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们通过前两章对部署ML模型进行批量或实时推理的各种选项的讨论。
- en: In the next chapter, we will bring everything we learned so far together to
    understand and build an end-to-end MLOps pipeline, enabling us to create an enterprise-ready
    and automated environment for any kind of process that requires the addition of
    ML.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把迄今为止所学的一切整合起来，理解和构建一个端到端的MLOps流水线，使我们能够为任何需要添加ML的过程创建一个企业级和自动化的环境。
