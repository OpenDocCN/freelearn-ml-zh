- en: Chapter 2. Classifying with Real-world Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 使用现实世界示例进行分类
- en: 'The topic of this chapter is **classification**. You have probably already
    used this form of machine learning as a consumer, even if you were not aware of
    it. If you have any modern e-mail system, it will likely have the ability to automatically
    detect spam. That is, the system will analyze all incoming e-mails and mark them
    as either spam or not-spam. Often, you, the end user, will be able to manually
    tag e-mails as spam or not, in order to improve its spam detection ability. This
    is a form of machine learning where the system is taking examples of two types
    of messages: spam and ham (the typical term for "non spam e-mails") and using
    these examples to automatically classify incoming e-mails.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主题是**分类**。即使你没有意识到，你可能已经作为消费者使用过这种形式的机器学习。如果你有任何现代的电子邮件系统，它可能具有自动检测垃圾邮件的能力。也就是说，系统将分析所有传入的电子邮件，并将其标记为垃圾邮件或非垃圾邮件。通常，你作为最终用户，可以手动标记电子邮件为垃圾邮件或非垃圾邮件，以提高系统的垃圾邮件检测能力。这是一种机器学习形式，系统通过分析两种类型的消息示例：垃圾邮件和正常邮件（“非垃圾邮件”邮件的典型术语），并使用这些示例自动分类传入的电子邮件。
- en: The general method of classification is to use a set of examples of each class
    to learn rules that can be applied to new examples. This is one of the most important
    machine learning modes and is the topic of this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的一般方法是使用每个类别的一组示例来学习可以应用于新示例的规则。这是机器学习中最重要的模式之一，也是本章的主题。
- en: Working with text such as e-mails requires a specific set of techniques and
    skills, and we discuss those in the next chapter. For the moment, we will work
    with a smaller, easier-to-handle dataset. The example question for this chapter
    is, "Can a machine distinguish between flower species based on images?" We will
    use two datasets where measurements of flower morphology are recorded along with
    the species for several specimens.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 处理如电子邮件这样的文本需要一套特定的技术和技能，我们将在下一章讨论这些内容。目前，我们将使用一个较小、易于处理的数据集。本章的示例问题是，“机器能否根据图像区分花卉物种？”我们将使用两个数据集，其中记录了花卉形态学的测量值以及几个样本的物种信息。
- en: We will explore these small datasets using a few simple algorithms. At first,
    we will write classification code ourselves in order to understand the concepts,
    but we will quickly switch to using scikit-learn whenever possible. The goal is
    to first understand the basic principles of classification and then progress to
    using a state-of-the-art implementation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一些简单的算法来探索这些小数据集。最初，我们将自己编写分类代码，以便理解概念，但我们会在有可能的情况下迅速切换到使用scikit-learn。目标是首先理解分类的基本原理，然后进步到使用最先进的实现。
- en: The Iris dataset
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鸢尾花数据集
- en: The Iris dataset is a classic dataset from the 1930s; it is one of the first
    modern examples of statistical classification.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集是一个经典的1930年代数据集；它是统计分类的第一个现代示例之一。
- en: The dataset is a collection of morphological measurements of several Iris flowers.
    These measurements will enable us to distinguish multiple species of the flowers.
    Today, species are identified by their DNA fingerprints, but in the 1930s, DNA's
    role in genetics had not yet been discovered.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是几种鸢尾花形态学测量的集合。这些测量将使我们能够区分花卉的多个物种。如今，物种是通过DNA指纹来识别的，但在20世纪30年代，DNA在遗传学中的作用尚未被发现。
- en: 'The following four attributes of each plant were measured:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每个植物的四个测量属性：
- en: sepal length
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼长度
- en: sepal width
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花萼宽度
- en: petal length
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度
- en: petal width
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度
- en: In general, we will call the individual numeric measurements we use to describe
    our data **features**. These features can be directly measured or computed from
    intermediate data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将用来描述数据的单个数值测量称为**特征**。这些特征可以直接测量或从中间数据计算得到。
- en: This dataset has four features. Additionally, for each plant, the species was
    recorded. The problem we want to solve is, "Given these examples, if we see a
    new flower out in the field, could we make a good prediction about its species
    from its measurements?"
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有四个特征。此外，每个植物的物种也被记录。我们想要解决的问题是，“给定这些示例，如果我们在田野中看到一朵新花，我们能从它的测量数据中准确预测它的物种吗？”
- en: 'This is the **supervised learning** or **classification** problem: given labeled
    examples, can we design a rule to be later applied to other examples? A more familiar
    example to modern readers who are not botanists is spam filtering, where the user
    can mark e-mails as spam, and systems use these as well as the non-spam e-mails
    to determine whether a new, incoming message is spam or not.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**监督学习**或**分类**问题：给定标记的样本，我们能否设计一个规则，之后可以应用于其他样本？一个现代读者更为熟悉的例子是垃圾邮件过滤，用户可以将电子邮件标记为垃圾邮件，系统则利用这些标记以及非垃圾邮件来判断一封新收到的邮件是否是垃圾邮件。
- en: Later in the book, we will look at problems dealing with text (starting in the
    next chapter). For the moment, the Iris dataset serves our purposes well. It is
    small (150 examples, four features each) and can be easily visualized and manipulated.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书后续章节中，我们将研究与文本相关的问题（从下一章开始）。目前，鸢尾花数据集很好地服务了我们的目的。它很小（150个样本，每个样本四个特征），且可以轻松可视化和操作。
- en: Visualization is a good first step
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化是一个很好的第一步
- en: Datasets, later in the book, will grow to thousands of features. With only four
    in our starting example, we can easily plot all two-dimensional projections on
    a single page. We will build intuitions on this small example, which can then
    be extended to large datasets with many more features. As we saw in the previous
    chapter, visualizations are excellent at the initial exploratory phase of the
    analysis as they allow you to learn the general features of your problem as well
    as catch problems that occurred with data collection early.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在本书后续章节中将扩展到成千上万的特征。在我们从一个包含四个特征的简单示例开始时，我们可以轻松地在单一页面上绘制所有二维投影。我们将在这个小示例上建立直觉，之后可以将其扩展到包含更多特征的大型数据集。正如我们在上一章中所见，数据可视化在分析的初期探索阶段非常有用，它能帮助我们了解问题的总体特征，并及早发现数据收集过程中出现的问题。
- en: 'Each subplot in the following plot shows all points projected into two of the
    dimensions. The outlying group (triangles) are the Iris Setosa plants, while Iris
    Versicolor plants are in the center (circle) and Iris Virginica are plotted with
    *x* marks. We can see that there are two large groups: one is of Iris Setosa and
    another is a mixture of Iris Versicolor and Iris Virginica.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下图中的每个子图展示了所有点投影到两个维度中的情况。外部群体（三角形）是鸢尾花Setosa，而鸢尾花Versicolor位于中心（圆形），鸢尾花Virginica则用*x*标记。我们可以看到，这里有两个大群体：一个是鸢尾花Setosa，另一个是鸢尾花Versicolor和鸢尾花Virginica的混合群体。
- en: '![Visualization is a good first step](img/2772OS_02_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![可视化是一个很好的第一步](img/2772OS_02_01.jpg)'
- en: 'In the following code snippet, we present the code to load the data and generate
    the plot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们展示了加载数据并生成图表的代码：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Building our first classification model
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建我们的第一个分类模型
- en: 'If the goal is to separate the three types of flowers, we can immediately make
    a few suggestions just by looking at the data. For example, petal length seems
    to be able to separate Iris Setosa from the other two flower species on its own.
    We can write a little bit of code to discover where the cut-off is:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标是将三种花卉分开，我们仅通过查看数据就可以立即做出一些建议。例如，花瓣长度似乎可以单独将鸢尾花Setosa与其他两种花卉区分开。我们可以编写一些代码来发现分割点的位置：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Therefore, we can build a simple model: if the petal length is smaller than
    2, then this is an Iris Setosa flower; otherwise it is either Iris Virginica or
    Iris Versicolor. This is our first model and it works very well in that it separates
    Iris Setosa flowers from the other two species without making any mistakes. In
    this case, we did not actually do any machine learning. Instead, we looked at
    the data ourselves, looking for a separation between the classes. Machine learning
    happens when we write code to look for this separation automatically.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以构建一个简单的模型：如果花瓣长度小于2，那么这是一朵鸢尾花Setosa；否则它要么是鸢尾花Virginica，要么是鸢尾花Versicolor。这是我们的第一个模型，它表现得非常好，因为它能够在没有任何错误的情况下将鸢尾花Setosa从其他两种花卉中分开。在这种情况下，我们实际上并没有进行机器学习，而是自己查看了数据，寻找类别之间的分离。机器学习发生在我们编写代码自动寻找这种分离的时刻。
- en: The problem of recognizing Iris Setosa apart from the other two species was
    very easy. However, we cannot immediately see what the best threshold is for distinguishing
    Iris Virginica from Iris Versicolor. We can even see that we will never achieve
    perfect separation with these features. We could, however, look for the best possible
    separation, the separation that makes the fewest mistakes. For this, we will perform
    a little computation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 区分 Iris Setosa 和其他两个物种的问题非常简单。然而，我们不能立即看到区分 Iris Virginica 和 Iris Versicolor
    的最佳阈值是什么。我们甚至可以看到，使用这些特征我们永远无法实现完美的分割。然而，我们可以寻找最好的可能分割，即犯错最少的分割。为此，我们将进行一些计算。
- en: 'We first select only the non-Setosa features and labels:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先仅选择非Setosa的特征和标签：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here we are heavily using NumPy operations on arrays. The `is_setosa` array
    is a Boolean array and we use it to select a subset of the other two arrays, `features`
    and `labels`. Finally, we build a new boolean array, `virginica`, by using an
    equality comparison on labels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们大量使用了 NumPy 对数组的操作。`is_setosa` 数组是一个布尔数组，我们用它来选择其他两个数组 `features` 和 `labels`
    的一个子集。最后，我们通过对标签进行相等比较，构建了一个新的布尔数组 `virginica`。
- en: Now, we run a loop over all possible features and thresholds to see which one
    results in better accuracy. Accuracy is simply the fraction of examples that the
    model classifies correctly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们遍历所有可能的特征和阈值，看看哪个能带来更好的准确率。准确率简单地是模型正确分类的示例的比例。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We need to test two types of thresholds for each feature and value: we test
    a *greater than threshold* and the reverse comparison. This is why we need the
    `rev_acc` variable in the preceding code; it holds the accuracy of reversing the
    comparison.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要测试每个特征和每个值的两种类型的阈值：我们测试一个*大于阈值*和反向比较。这就是为什么我们在前面的代码中需要 `rev_acc` 变量；它保存了反向比较的准确率。
- en: 'The last few lines select the best model. First, we compare the predictions,
    `pred`, with the actual labels, `is_virginica`. The little trick of computing
    the mean of the comparisons gives us the fraction of correct results, the accuracy.
    At the end of the `for` loop, all the possible thresholds for all the possible
    features have been tested, and the variables `best_fi`, `best_t`, and `best_reverse`
    hold our model. This is all the information we need to be able to classify a new,
    unknown object, that is, to assign a class to it. The following code implements
    exactly this method:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几行选择最佳模型。首先，我们将预测值 `pred` 与实际标签 `is_virginica` 进行比较。通过计算比较的均值的小技巧，我们可以得到正确结果的比例，即准确率。在
    `for` 循环的末尾，所有可能的特征的所有可能阈值都已被测试，变量 `best_fi`、`best_t` 和 `best_reverse` 保存了我们的模型。这就是我们所需的所有信息，能够对一个新的、未知的对象进行分类，也就是说，给它分配一个类别。以下代码正是实现了这个方法：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'What does this model look like? If we run the code on the whole data, the model
    that is identified as the best makes decisions by splitting on the petal width.
    One way to gain intuition about how this works is to visualize the **decision
    boundary**. That is, we can see which feature values will result in one decision
    versus the other and exactly where the boundary is. In the following screenshot,
    we see two regions: one is white and the other is shaded in grey. Any datapoint
    that falls on the white region will be classified as Iris Virginica, while any
    point that falls on the shaded side will be classified as Iris Versicolor.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型是什么样子的？如果我们在整个数据上运行代码，识别为最佳的模型通过在花瓣宽度上进行分割来做出决策。理解这一过程的一种方式是可视化**决策边界**。也就是说，我们可以看到哪些特征值会导致一个决策与另一个决策的区别，并准确地看到边界在哪里。在以下截图中，我们看到两个区域：一个是白色的，另一个是灰色阴影的。任何落在白色区域的
    datapoint 将被分类为 Iris Virginica，而任何落在阴影区域的点将被分类为 Iris Versicolor。
- en: '![Building our first classification model](img/2772OS_02_02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![构建我们的第一个分类模型](img/2772OS_02_02.jpg)'
- en: In a threshold model, the decision boundary will always be a line that is parallel
    to one of the axes. The plot in the preceding screenshot shows the decision boundary
    and the two regions where points are classified as either white or grey. It also
    shows (as a dashed line) an alternative threshold, which will achieve exactly
    the same accuracy. Our method chose the first threshold it saw, but that was an
    arbitrary choice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在阈值模型中，决策边界将始终是与其中一个轴平行的直线。前面截图中的图表显示了决策边界和两个区域，其中的点被分类为白色或灰色。它还显示了（作为虚线）一个替代阈值，这个阈值将获得完全相同的准确率。我们的方法选择了它看到的第一个阈值，但这是一个任意选择。
- en: Evaluation – holding out data and cross-validation
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估 – 数据持出和交叉验证
- en: The model discussed in the previous section is a simple model; it achieves 94
    percent accuracy of the whole data. However, this evaluation may be overly optimistic.
    We used the data to define what the threshold will be, and then we used the same
    data to evaluate the model. Of course, the model will perform better than anything
    else we tried on this dataset. The reasoning is circular.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节中讨论的模型是一个简单的模型，它在整个数据集上的准确率达到了94%。然而，这种评估可能过于乐观。我们使用数据来定义阈值，然后用相同的数据来评估模型。当然，模型在这个数据集上会表现得比我们尝试过的其他任何方法都要好。这种推理是循环的。
- en: 'What we really want to do is estimate the ability of the model to generalize
    to new instances. We should measure its performance in instances that the algorithm
    has not seen at training. Therefore, we are going to do a more rigorous evaluation
    and use held-out data. For this, we are going to break up the data into two groups:
    on one group, we''ll train the model, and on the other, we''ll test the one we
    held out of training. The full code, which is an adaptation of the code presented
    earlier, is available on the online support repository. Its output is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想做的是估计模型对新实例的泛化能力。我们应该衡量算法在训练时没有见过的实例上的表现。因此，我们将进行更严格的评估并使用保留数据。为此，我们将把数据分成两组：一组用来训练模型，另一组用来测试我们从训练中保留的数据。完整的代码是对之前展示的代码的改编，可以在在线支持仓库中找到。其输出如下：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result on the training data (which is a subset of the whole data) is apparently
    even better than before. However, what is important to note is that the result
    in the testing data is lower than that of the training error. While this may surprise
    an inexperienced machine learner, it is expected that testing accuracy will be
    lower than the training accuracy. To see why, look back at the plot that showed
    the decision boundary. Consider what would have happened if some of the examples
    close to the boundary were not there or that one of them between the two lines
    was missing. It is easy to imagine that the boundary will then move a little bit
    to the right or to the left so as to put them on the *wrong* side of the border.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上的结果（训练数据是整个数据的一个子集）显然比之前更好。然而，值得注意的是，测试数据上的结果低于训练误差。虽然这可能会让没有经验的机器学习者感到惊讶，但测试精度低于训练精度是可以预期的。要理解为什么，请回顾一下显示决策边界的图表。想象一下，如果有些接近边界的例子不存在，或者两条线之间的某个例子缺失，会发生什么情况。很容易想象，边界会稍微向右或向左移动，从而将它们放置在边界的*错误*一侧。
- en: Tip
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The accuracy on the training data, the **training accuracy**, is almost always
    an overly optimistic estimate of how well your algorithm is doing. We should always
    measure and report the **testing accuracy**, which is the accuracy on a collection
    of examples that were not used for training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上的准确度，即**训练准确度**，几乎总是过于乐观地估计了算法的表现。我们应该始终测量并报告**测试准确度**，即在没有用于训练的例子上计算的准确度。
- en: These concepts will become more and more important as the models become more
    complex. In this example, the difference between the accuracy measured on training
    data and on testing data is not very large. When using a complex model, it is
    possible to get 100 percent accuracy in training and do no better than random
    guessing on testing!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变得越来越复杂，这些概念将变得越来越重要。在这个例子中，训练数据和测试数据上测量的准确度差异并不大。而使用复杂模型时，可能在训练时达到100%的准确度，却在测试时表现不比随机猜测好！
- en: One possible problem with what we did previously, which was to hold out data
    from training, is that we only used half the data for training. Perhaps it would
    have been better to use more training data. On the other hand, if we then leave
    too little data for testing, the error estimation is performed on a very small
    number of examples. Ideally, we would like to use all of the data for training
    and all of the data for testing as well, which is impossible.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前做的一个可能存在的问题是，保留部分数据用于测试，这意味着我们只使用了一半的数据进行训练。也许使用更多的训练数据会更好。另一方面，如果我们留下的数据用于测试太少，错误估计就会基于非常少量的例子来进行。理想情况下，我们希望将所有数据用于训练，并将所有数据用于测试，但这是不可能的。
- en: We can achieve a good approximation of this impossible ideal by a method called
    **cross-validation**. One simple form of cross-validation is *leave-one-out cross-validation*.
    We will take an example out of the training data, learn a model without this example,
    and then test whether the model classifies this example correctly. This process
    is then repeated for all the elements in the dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一种叫做**交叉验证**的方法，较好地接近这一不可能的理想。交叉验证的一种简单形式是*留一交叉验证*。我们将从训练数据中取出一个样本，学习一个不包含该样本的模型，然后测试该模型是否能正确分类该样本。这个过程会对数据集中的所有元素重复进行。
- en: 'The following code implements exactly this type of cross-validation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码正是实现这种类型的交叉验证：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: At the end of this loop, we will have tested a series of models on all the examples
    and have obtained a final average result. When using cross-validation, there is
    no circularity problem because each example was tested on a model which was built
    without taking that datapoint into account. Therefore, the cross-validated estimate
    is a reliable estimate of how well the models would generalize to new data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环结束时，我们将会在所有样本上测试一系列模型，并获得最终的平均结果。在使用交叉验证时，不会出现循环问题，因为每个样本都在没有考虑该数据点的模型上进行测试。因此，交叉验证估计是一个可靠的估计，可以反映模型在新数据上的泛化能力。
- en: The major problem with leave-one-out cross-validation is that we are now forced
    to perform many times more work. In fact, you must learn a whole new model for
    each and every example and this cost will increase as our dataset grows.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 留一交叉验证的主要问题在于，我们现在不得不进行更多的工作。事实上，你必须为每个样本学习一个全新的模型，随着数据集的增大，这个成本也会增加。
- en: We can get most of the benefits of leave-one-out at a fraction of the cost by
    using x-fold cross-validation, where *x* stands for a small number. For example,
    to perform five-fold cross-validation, we break up the data into five groups,
    so-called five folds.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用x折交叉验证，在成本较低的情况下获得大部分的留一交叉验证的好处，其中*x*代表一个较小的数字。例如，为了执行五折交叉验证，我们将数据分成五组，也就是所谓的五折。
- en: 'Then you learn five models: each time you will leave one fold out of the training
    data. The resulting code will be similar to the code given earlier in this section,
    but we leave 20 percent of the data out instead of just one element. We test each
    of these models on the left-out fold and average the results.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你会学习五个模型：每次你都会将其中一个折叠从训练数据中剔除。结果代码将与本节前面给出的代码相似，但我们将把数据中的20％剔除，而不是仅仅剔除一个元素。我们会在剔除的折叠上测试这些模型，并计算结果的平均值。
- en: '![Evaluation – holding out data and cross-validation](img/2772OS_02_03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![评估 – 剔除数据与交叉验证](img/2772OS_02_03.jpg)'
- en: 'The preceding figure illustrates this process for five blocks: the dataset
    is split into five pieces. For each fold, you hold out one of the blocks for testing
    and train on the other four. You can use any number of folds you wish. There is
    a trade-off between computational efficiency (the more folds, the more computation
    is necessary) and accurate results (the more folds, the closer you are to using
    the whole of the data for training). Five folds is often a good compromise. This
    corresponds to training with 80 percent of your data, which should already be
    close to what you will get from using all the data. If you have little data, you
    can even consider using 10 or 20 folds. In the extreme case, if you have as many
    folds as datapoints, you are simply performing leave-one-out cross-validation.
    On the other hand, if computation time is an issue and you have more data, 2 or
    3 folds may be the more appropriate choice.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了这一过程，使用了五个折叠：数据集被分成五个部分。对于每一个折叠，你会保留其中一个块进行测试，其余四个块用于训练。你可以使用任何数量的折叠。折叠数与计算效率（折叠数越多，需要的计算越多）和结果准确性（折叠数越多，训练数据越接近于整个数据集）之间存在权衡。五个折叠通常是一个不错的折中方案。这意味着你用80％的数据进行训练，这已经接近于使用全部数据的效果。如果你的数据很少，甚至可以考虑使用10折或20折。在极端情况下，如果折叠数等于数据点数，你就只是在执行留一交叉验证。另一方面，如果计算时间是个问题，并且你有更多的数据，2折或3折可能是更合适的选择。
- en: When generating the folds, you need to be careful to keep them balanced. For
    example, if all of the examples in one fold come from the same class, then the
    results will not be representative. We will not go into the details of how to
    do this, because the machine learning package scikit-learn will handle them for
    you.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成折叠时，你需要小心保持它们的平衡。例如，如果一个折叠中的所有样本都来自同一类别，那么结果将不具代表性。我们不会详细讲解如何做到这一点，因为机器学习库scikit-learn会为你处理这些问题。
- en: We have now generated several models instead of just one. So, "What final model
    do we return and use for new data?" The simplest solution is now to train a single
    overall model on all your training data. The cross-validation loop gives you an
    estimate of how well this model should generalize.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们生成了多个模型，而不仅仅是一个。那么，“我们应该返回哪个最终模型来处理新数据？”最简单的解决方案是，在所有训练数据上训练一个整体的单一模型。交叉验证循环给出了这个模型应该如何推广的估计。
- en: Tip
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A cross-validation schedule allows you to use all your data to estimate whether
    your methods are doing well. At the end of the cross-validation loop, you can
    then use all your data to train a final model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证安排允许你使用所有数据来估算你的方法是否有效。在交叉验证循环结束时，你可以使用所有数据来训练最终模型。
- en: Although it was not properly recognized when machine learning was starting out
    as a field, nowadays, it is seen as a very bad sign to even discuss the training
    accuracy of a classification system. This is because the results can be very misleading
    and even just presenting them marks you as a newbie in machine learning. We always
    want to measure and compare either the error on a held-out dataset or the error
    estimated using a cross-validation scheme.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在机器学习作为一个领域刚起步时，这一点并没有被充分认识到，但如今，讨论分类系统的训练准确率被视为一种非常糟糕的迹象。因为结果可能会非常具有误导性，甚至仅仅展示这些结果就会让你看起来像是机器学习的新手。我们总是希望衡量并比较保留数据集上的误差或使用交叉验证方案估算的误差。
- en: Building more complex classifiers
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建更复杂的分类器
- en: 'In the previous section, we used a very simple model: a threshold on a single
    feature. Are there other types of systems? Yes, of course! Many others. Throughout
    this book, you will see many other types of models and we''re not even going to
    cover everything that is out there.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了一个非常简单的模型：对单一特征进行阈值判断。是否还有其他类型的系统？当然有！很多其他类型。在本书中，你将看到许多其他类型的模型，我们甚至不会涵盖所有现有的模型。
- en: 'To think of the problem at a higher abstraction level, "What makes up a classification
    model?" We can break it up into three parts:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从更高的抽象层次来思考这个问题，“一个分类模型由什么组成？”我们可以将其分为三部分：
- en: '**The structure of the model**: How exactly will a model make decisions? In
    this case, the decision depended solely on whether a given feature was above or
    below a certain threshold value. This is too simplistic for all but the simplest
    problems.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型的结构**：模型究竟是如何做出决策的？在这种情况下，决策完全依赖于某个特征是否高于或低于某个阈值。除了最简单的问题，这种方法过于简化。'
- en: '**The search procedure**: How do we find the model we need to use? In our case,
    we tried every possible combination of feature and threshold. You can easily imagine
    that as models get more complex and datasets get larger, it rapidly becomes impossible
    to attempt all combinations and we are forced to use approximate solutions. In
    other cases, we need to use advanced optimization methods to find a good solution
    (fortunately, scikit-learn already implements these for you, so using them is
    easy even if the code behind them is very advanced).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索过程**：我们如何找到需要使用的模型？在我们的案例中，我们尝试了每一种特征和阈值的可能组合。你可以很容易地想象，随着模型变得更加复杂，数据集变得更大，尝试所有组合变得几乎不可能，我们不得不使用近似解决方案。在其他情况下，我们需要使用先进的优化方法来找到一个好的解决方案（幸运的是，scikit-learn已经为你实现了这些方法，所以即使它们背后的代码非常先进，使用起来也很简单）。'
- en: '**The gain or loss function**: How do we decide which of the possibilities
    tested should be returned? Rarely do we find the perfect solution, the model that
    never makes any mistakes, so we need to decide which one to use. We used accuracy,
    but sometimes it will be better to optimize so that the model makes fewer errors
    of a specific kind. For example, in spam filtering, it may be worse to delete
    a good e-mail than to erroneously let a bad e-mail through. In that case, we may
    want to choose a model that is conservative in throwing out e-mails rather than
    the one that just makes the fewest mistakes overall. We can discuss these issues
    in terms of gain (which we want to maximize) or loss (which we want to minimize).
    They are equivalent, but sometimes one is more convenient than the other.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增益或损失函数**：我们如何决定应该返回哪些测试过的可能性？我们很少能找到完美的解决方案，即永远不会出错的模型，因此我们需要决定使用哪一个。我们使用了准确率，但有时更好的做法是优化，使得模型在特定类型的错误上减少。比如在垃圾邮件过滤中，删除一封好邮件可能比错误地让一封坏邮件通过更糟糕。在这种情况下，我们可能希望选择一个在丢弃邮件时较为保守的模型，而不是那个只做最少错误的模型。我们可以通过增益（我们希望最大化）或损失（我们希望最小化）来讨论这些问题。它们是等效的，但有时一个比另一个更方便。'
- en: We can play around with these three aspects of classifiers and get different
    systems. A simple threshold is one of the simplest models available in machine
    learning libraries and only works well when the problem is very simple, such as
    with the Iris dataset. In the next section, we will tackle a more difficult classification
    task that requires a more complex structure.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调整分类器的这三个方面来创建不同的系统。简单的阈值是机器学习库中最简单的模型之一，并且仅在问题非常简单时有效，例如在鸢尾花数据集上。在下一节中，我们将处理一个更复杂的分类任务，需要更复杂的结构。
- en: In our case, we optimized the threshold to minimize the number of errors. Alternatively,
    we might have different loss functions. It might be that one type of error is
    much costlier than the other. In a medical setting, false negatives and false
    positives are not equivalent. A **false negative** (when the result of a test
    comes back negative, but that is false) might lead to the patient not receiving
    treatment for a serious disease. A **false positive** (when the test comes back
    positive even though the patient does not actually have that disease) might lead
    to additional tests to confirm or unnecessary treatment (which can still have
    costs, including side effects from the treatment, but are often less serious than
    missing a diagnostic). Therefore, depending on the exact setting, different trade-offs
    can make sense. At one extreme, if the disease is fatal and the treatment is cheap
    with very few negative side-effects, then you want to minimize false negatives
    as much as you can.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们优化了阈值以最小化错误数量。或者，我们可能会有不同的损失函数。某些类型的错误可能比其他错误更昂贵。在医疗环境中，假阴性和假阳性并不等价。**假阴性**（当测试结果为阴性，但实际上是错误的）可能导致患者没有接受严重疾病的治疗。**假阳性**（当测试结果为阳性，而患者实际上并没有这种疾病）可能会导致额外的检查以确认或不必要的治疗（这些治疗仍然可能带来成本，包括治疗的副作用，但通常不如错过诊断那么严重）。因此，根据具体环境，不同的权衡是合理的。在一个极端情况下，如果疾病是致命的，而且治疗便宜且副作用很小，那么你希望尽可能减少假阴性。
- en: Tip
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: What the **gain/cost** function should be is always dependent on the exact problem
    you are working on. When we present a general-purpose algorithm, we often focus
    on minimizing the number of mistakes, achieving the highest accuracy. However,
    if some mistakes are costlier than others, it might be better to accept a lower
    overall accuracy to minimize the overall costs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**增益/成本**函数的选择总是依赖于你所处理的具体问题。当我们提出通用算法时，我们通常关注最小化错误数量，达到最高的准确度。然而，如果某些错误的成本高于其他错误，那么接受较低的整体准确度可能更好，以最小化整体成本。'
- en: A more complex dataset and a more complex classifier
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个更复杂的数据集和一个更复杂的分类器
- en: We will now look at a slightly more complex dataset. This will motivate the
    introduction of a new classification algorithm and a few other ideas.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一个稍微复杂一点的数据集。这将为引入一种新的分类算法和其他一些想法提供动机。
- en: Learning about the Seeds dataset
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解种子数据集
- en: 'We now look at another agricultural dataset, which is still small, but already
    too large to plot exhaustively on a page as we did with Iris. This dataset consists
    of measurements of wheat seeds. There are seven features that are present, which
    are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来看另一个农业数据集，尽管它仍然很小，但已经足够大，不再像鸢尾花数据集那样可以在一页上完全绘制。这个数据集包含了小麦种子的测量数据。数据集中有七个特征，具体如下：
- en: area A
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域 A
- en: perimeter P
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周长 P
- en: compactness C = 4πA/P²
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑度 C = 4πA/P²
- en: length of kernel
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核长度
- en: width of kernel
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核宽度
- en: asymmetry coefficient
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不对称系数
- en: length of kernel groove
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核槽长度
- en: 'There are three classes, corresponding to three wheat varieties: Canadian,
    Koma, and Rosa. As earlier, the goal is to be able to classify the species based
    on these morphological measurements. Unlike the Iris dataset, which was collected
    in the 1930s, this is a very recent dataset and its features were automatically
    computed from digital images.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个类别，分别对应三种小麦品种：加拿大小麦、Koma小麦和Rosa小麦。如前所述，目标是根据这些形态学测量值来分类物种。与1930年代收集的鸢尾花数据集不同，这是一个非常新的数据集，其特征是通过数字图像自动计算得出的。
- en: 'This is how image pattern recognition can be implemented: you can take images,
    in digital form, compute a few relevant features from them, and use a generic
    classification system. In [Chapter 10](ch10.html "Chapter 10. Computer Vision"),
    *Computer Vision*, we will work through the computer vision side of this problem
    and compute features in images. For the moment, we will work with the features
    that are given to us.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何实现图像模式识别的：你可以获取数字形式的图像，从中计算出一些相关特征，并使用一个通用的分类系统。在[第10章](ch10.html "第10章
    计算机视觉")，*计算机视觉*，我们将通过解决这个问题的计算机视觉部分来计算图像中的特征。现在，我们将使用已给出的特征。
- en: Note
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**UCI Machine Learning Dataset Repository**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**UCI机器学习数据集仓库**'
- en: The University of California at Irvine (UCI) maintains an online repository
    of machine learning datasets (at the time of writing, they list 233 datasets).
    Both the Iris and the Seeds dataset used in this chapter were taken from there.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学欧文分校（UCI）维护着一个在线机器学习数据集仓库（在写本文时，他们列出了233个数据集）。本章中使用的Iris数据集和Seeds数据集都来源于此。
- en: The repository is available online at [http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该仓库可以在线访问：[http://archive.ics.uci.edu/ml/](http://archive.ics.uci.edu/ml/)。
- en: Features and feature engineering
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征与特征工程
- en: One interesting aspect of these features is that the compactness feature is
    not actually a new measurement, but a function of the previous two features, area
    and perimeter. It is often very useful to derive new combined features. Trying
    to create new features is generally called **feature engineering**. It is sometimes
    seen as less glamorous than algorithms, but it often matters more for performance
    (a simple algorithm on well-chosen features will perform better than a fancy algorithm
    on not-so-good features).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征的一个有趣方面是，紧凑度特征实际上不是一种新的度量，而是之前两个特征——面积和周长——的函数。推导新的组合特征通常非常有用。尝试创建新特征通常被称为**特征工程**。它有时被认为不如算法引人注目，但它往往对性能影响更大（在精心挑选的特征上应用一个简单的算法，会比在不太好的特征上使用一个复杂的算法表现得更好）。
- en: In this case, the original researchers computed the **compactness**, which is
    a typical feature for shapes. It is also sometimes called **roundness**. This
    feature will have the same value for two kernels, one of which is twice as big
    as the other one, but with the same shape. However, it will have different values
    for kernels that are very round (when the feature is close to one) when compared
    to kernels that are elongated (when the feature is closer to zero).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，原始研究人员计算了**紧凑度**，这是一个典型的形状特征。它有时也被称为**圆度**。对于两个内核，它们的形状相同，但一个是另一个的两倍大，紧凑度特征的值是相同的。然而，对于非常圆的内核（当该特征接近1时），与形状拉长的内核（当该特征接近0时）相比，它将有不同的值。
- en: The goals of a good feature are to simultaneously vary with what matters (the
    desired output) and be invariant with what does not. For example, compactness
    does not vary with size, but varies with the shape. In practice, it might be hard
    to achieve both objectives perfectly, but we want to approximate this ideal.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好特征的目标是既要随着重要因素（期望的输出）变化，又要在不重要的因素上保持不变。例如，紧凑度不随大小变化，但随形状变化。在实践中，可能很难完美地同时达到这两个目标，但我们希望尽可能接近这个理想。
- en: You will need to use background knowledge to design good features. Fortunately,
    for many problem domains, there is already a vast literature of possible features
    and feature-types that you can build upon. For images, all of the previously mentioned
    features are typical and computer vision libraries will compute them for you.
    In text-based problems too, there are standard solutions that you can mix and
    match (we will also see this in the next chapter). When possible, you should use
    your knowledge of the problem to design a specific feature or to select which
    ones from the literature are more applicable to the data at hand.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要使用背景知识来设计良好的特征。幸运的是，对于许多问题领域，已经有大量的文献提供了可用的特征和特征类型，你可以在此基础上进行构建。对于图像，所有之前提到的特征都是典型的，计算机视觉库会为你计算它们。在基于文本的问题中，也有标准的解决方案，你可以将它们混合搭配（我们将在下一章中也会看到）。在可能的情况下，你应该利用你对问题的了解来设计特定的特征，或者选择文献中哪些特征更适用于手头的数据。
- en: Even before you have data, you must decide which data is worthwhile to collect.
    Then, you hand all your features to the machine to evaluate and compute the best
    classifier.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在你还没有数据之前，你也必须决定哪些数据值得收集。然后，你将所有特征交给机器进行评估，并计算出最佳的分类器。
- en: A natural question is whether we can select good features automatically. This
    problem is known as **feature selection**. There are many methods that have been
    proposed for this problem, but in practice very simple ideas work best. For the
    small problems we are currently exploring, it does not make sense to use feature
    selection, but if you had thousands of features, then throwing out most of them
    might make the rest of the process much faster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的问题是，我们是否可以自动选择好的特征。这个问题被称为**特征选择**。已经提出了许多方法来解决这个问题，但实际上非常简单的思路效果最好。对于我们目前探索的小问题，使用特征选择没有意义，但如果你有成千上万的特征，那么去掉大部分特征可能会使后续的处理速度更快。
- en: Nearest neighbor classification
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近邻分类
- en: 'For use with this dataset, we will introduce a new classifier: **the nearest
    neighbor classifier**. The nearest neighbor classifier is very simple. When classifying
    a new element, it looks at the training data for the object that is closest to
    it, its nearest neighbor. Then, it returns its label as the answer. Notice that
    this model performs perfectly on its training data! For each point, its closest
    neighbor is itself, and so its label matches perfectly (unless two examples with
    different labels have exactly the same feature values, which will indicate that
    the features you are using are not very descriptive). Therefore, it is essential
    to test the classification using a cross-validation protocol.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，我们将引入一个新的分类器：**最近邻分类器**。最近邻分类器非常简单。在对一个新元素进行分类时，它会查看训练数据中与其最接近的对象，即最近邻。然后，它会返回该对象的标签作为答案。请注意，这个模型在训练数据上表现完美！对于每一个点，它的最近邻就是它自己，因此它的标签完全匹配（除非两个不同标签的示例具有完全相同的特征值，这将表明你使用的特征不是很具描述性）。因此，使用交叉验证协议来测试分类是至关重要的。
- en: The nearest neighbor method can be generalized to look not at a single neighbor,
    but to multiple ones and take a vote amongst the neighbors. This makes the method
    more robust to outliers or mislabeled data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻方法可以推广到不仅仅看单个邻居，而是看多个邻居，并在这些邻居中进行投票。这使得该方法对异常值或标签错误的数据更加健壮。
- en: Classifying with scikit-learn
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行分类
- en: We have been using handwritten classification code, but Python is a very appropriate
    language for machine learning because of its excellent libraries. In particular,
    scikit-learn has become the standard library for many machine learning tasks,
    including classification. We are going to use its implementation of nearest neighbor
    classification in this section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用手写的分类代码，但 Python 由于其出色的库，是机器学习的非常合适的语言。特别是，scikit-learn 已经成为许多机器学习任务（包括分类）的标准库。在本节中，我们将使用它实现的最近邻分类方法。
- en: 'The scikit-learn classification API is organized around classifier objects.
    These objects have the following two essential methods:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 分类 API 是围绕分类器对象组织的。这些对象有以下两个基本方法：
- en: '`fit(features, labels)`: This is the learning step and fits the parameters
    of the model'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit(features, labels)`：这是学习步骤，拟合模型的参数。'
- en: '`predict(features)`: This method can only be called after fit and returns a
    prediction for one or more inputs'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict(features)`：该方法只有在调用 `fit` 之后才能使用，并且返回一个或多个输入的预测结果。'
- en: 'Here is how we could use its implementation of k-nearest neighbors for our
    data. We start by importing the `KneighborsClassifier` object from the `sklearn.neighbors`
    submodule:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何使用其实现的 k-最近邻方法来处理我们的数据。我们从 `sklearn.neighbors` 子模块中导入 `KneighborsClassifier`
    对象，开始：
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The scikit-learn module is imported as sklearn (sometimes you will also find
    that scikit-learn is referred to using this short name instead of the full name).
    All of the sklearn functionality is in submodules, such as `sklearn.neighbors`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 模块以 sklearn 导入（有时你也会发现 scikit-learn 使用这个简短的名字而不是全名）。所有 sklearn
    的功能都在子模块中，如 `sklearn.neighbors`。
- en: 'We can now instantiate a classifier object. In the constructor, we specify
    the number of neighbors to consider, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实例化一个分类器对象。在构造函数中，我们指定要考虑的邻居数量，如下所示：
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If we do not specify the number of neighbors, it defaults to `5`, which is often
    a good choice for classification.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有指定邻居数量，默认值为 `5`，这是分类中通常很好的选择。
- en: 'We will want to use cross-validation (of course) to look at our data. The scikit-learn
    module also makes this easy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用交叉验证（当然）来查看我们的数据。scikit-learn 模块也使这变得很容易：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using five folds for cross-validation, for this dataset, with this algorithm,
    we obtain 90.5 percent accuracy. As we discussed in the earlier section, the cross-validation
    accuracy is lower than the training accuracy, but this is a more credible estimate
    of the performance of the model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五折交叉验证，对于这个数据集，使用这个算法，我们获得了 90.5% 的准确率。正如我们在前一部分讨论的那样，交叉验证的准确率低于训练准确率，但这是对模型性能更可靠的估计。
- en: Looking at the decision boundaries
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看决策边界
- en: 'We will now examine the decision boundary. In order to plot these on paper,
    we will simplify and look at only two dimensions. Take a look at the following
    plot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将考察决策边界。为了在纸上绘制这些边界，我们将简化问题，只考虑二维情况。请看以下图表：
- en: '![Looking at the decision boundaries](img/2772OS_02_04.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![查看决策边界](img/2772OS_02_04.jpg)'
- en: Canadian examples are shown as diamonds, Koma seeds as circles, and Rosa seeds
    as triangles. Their respective areas are shown as white, black, and grey. You
    might be wondering why the regions are so horizontal, almost weirdly so. The problem
    is that the *x* axis (area) ranges from 10 to 22, while the *y* axis (compactness)
    ranges from 0.75 to 1.0\. This means that a small change in *x* is actually much
    larger than a small change in *y*. So, when we compute the distance between points,
    we are, for the most part, only taking the *x* axis into account. This is also
    a good example of why it is a good idea to visualize our data and look for red
    flags or surprises.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大样本以菱形表示，Koma 种子以圆形表示，Rosa 种子以三角形表示。它们各自的区域分别用白色、黑色和灰色表示。你可能会想，为什么这些区域如此水平，几乎是奇怪的水平。问题在于，*x*
    轴（面积）的范围是从 10 到 22，而 *y* 轴（紧凑度）的范围是从 0.75 到 1.0。也就是说，*x* 轴的微小变化实际上要比 *y* 轴的微小变化大得多。因此，当我们计算点与点之间的距离时，大部分情况下，我们只考虑了
    *x* 轴。这也是为什么将数据可视化并寻找潜在问题或惊讶的一个好例子。
- en: 'If you studied physics (and you remember your lessons), you might have already
    noticed that we had been summing up lengths, areas, and dimensionless quantities,
    mixing up our units (which is something you never want to do in a physical system).
    We need to normalize all of the features to a common scale. There are many solutions
    to this problem; a simple one is to *normalize to z-scores*. The z-score of a
    value is how far away from the mean it is, in units of standard deviation. It
    comes down to this operation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你学过物理（并且记得你的课），你可能已经注意到，我们之前在求和长度、面积和无量纲量时混淆了单位（这是在物理系统中绝对不应该做的事情）。我们需要将所有特征归一化到一个统一的尺度。对此问题有许多解决方法；一个简单的解决方法是
    *标准化为 z 分数*。一个值的 z 分数是它与均值的偏差，单位是标准差。其操作如下：
- en: '![Looking at the decision boundaries](img/2772OS_02_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![查看决策边界](img/2772OS_02_07.jpg)'
- en: In this formula, *f* is the old feature value, *f'* is the normalized feature
    value, *µ* is the mean of the feature, and *σ* is the standard deviation. Both
    *µ* and *σ* are estimated from training data. Independent of what the original
    values were, after z-scoring, a value of zero corresponds to the training mean,
    positive values are above the mean, and negative values are below it.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*f* 是原始特征值，*f'* 是归一化后的特征值，*µ* 是特征的均值，*σ* 是标准差。*µ* 和 *σ* 都是从训练数据中估算出来的。无论原始值是什么，经过
    z 评分后，值为零表示训练均值，正值表示高于均值，负值表示低于均值。
- en: 'The scikit-learn module makes it very easy to use this normalization as a preprocessing
    step. We are going to use a pipeline of transformations: the first element will
    do the transformation and the second element will do the classification. We start
    by importing both the pipeline and the feature scaling classes as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 模块使得将这种归一化作为预处理步骤变得非常简单。我们将使用一个转换流水线：第一个元素将进行转换，第二个元素将进行分类。我们首先按如下方式导入流水线和特征缩放类：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we can combine them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将它们结合起来。
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The Pipeline constructor takes a list of pairs `(str,clf)`. Each pair corresponds
    to a step in the pipeline: the first element is a string naming the step, while
    the second element is the object that performs the transformation. Advanced usage
    of the object uses these names to refer to different steps.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Pipeline 构造函数接受一个由 `(str, clf)` 组成的配对列表。每一对都对应流水线中的一步：第一个元素是命名步骤的字符串，而第二个元素是执行转换的对象。该对象的高级用法使用这些名称来引用不同的步骤。
- en: After normalization, every feature is in the same units (technically, every
    feature is now dimensionless; it has no units) and we can more confidently mix
    dimensions. In fact, if we now run our nearest neighbor classifier, we obtain
    93 percent accuracy, estimated with the same five-fold cross-validation code shown
    previously!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 经过归一化处理后，每个特征都处于相同的单位（从技术上讲，每个特征现在是无量纲的；它没有单位），因此我们可以更自信地混合不同的维度。事实上，如果我们现在运行最近邻分类器，我们可以获得93%的准确率，这个结果是通过之前显示的五折交叉验证代码来估算的！
- en: 'Look at the decision space again in two dimensions:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看二维中的决策空间：
- en: '![Looking at the decision boundaries](img/2772OS_02_05.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![查看决策边界](img/2772OS_02_05.jpg)'
- en: The boundaries are now different and you can see that both dimensions make a
    difference for the outcome. In the full dataset, everything is happening on a
    seven-dimensional space, which is very hard to visualize, but the same principle
    applies; while a few dimensions are dominant in the original data, after normalization,
    they are all given the same importance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在边界发生了变化，你可以看到两个维度对结果都有影响。在完整的数据集中，一切都发生在一个七维空间中，这很难可视化，但相同的原理仍然适用；尽管原始数据中某些维度占主导地位，但经过归一化后，它们都被赋予了相同的重要性。
- en: Binary and multiclass classification
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分类和多分类
- en: The first classifier we used, the threshold classifier, was a simple binary
    classifier. Its result is either one class or the other, as a point is either
    above the threshold value or it is not. The second classifier we used, the nearest
    neighbor classifier, was a natural multiclass classifier, its output can be one
    of the several classes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的第一个分类器是阈值分类器，它是一个简单的二分类器。其结果是一个类别或另一个类别，因为一个点要么在阈值以上，要么不在。我们使用的第二个分类器是最近邻分类器，它是一个自然的多类别分类器，其输出可以是多个类别中的一个。
- en: 'It is often simpler to define a simple binary method than the one that works
    on multiclass problems. However, we can reduce any multiclass problem to a series
    of binary decisions. This is what we did earlier in the Iris dataset, in a haphazard
    way: we observed that it was easy to separate one of the initial classes and focused
    on the other two, reducing the problem to two binary decisions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个简单的二分类方法通常比解决多类别问题的方法更简单。然而，我们可以将任何多类别问题简化为一系列的二元决策。这正是我们在之前的Iris数据集中所做的，以一种无序的方式：我们观察到很容易将其中一个初始类别分开，并专注于另外两个，从而将问题简化为两个二元决策：
- en: Is it an Iris Setosa (yes or no)?
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是Iris Setosa吗（是或不是）？
- en: If not, check whether it is an Iris Virginica (yes or no).
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有，检查它是否是Iris Virginica（是或不是）。
- en: Of course, we want to leave this sort of reasoning to the computer. As usual,
    there are several solutions to this multiclass reduction.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们希望将这种推理交给计算机来处理。像往常一样，针对这种多类别的简化方法有多种解决方案。
- en: The simplest is to use a series of *one versus the rest* classifiers. For each
    possible label ℓ, we build a classifier of the type *is this ℓ or something else?*
    When applying the rule, exactly one of the classifiers will say *yes* and we will
    have our solution. Unfortunately, this does not always happen, so we have to decide
    how to deal with either multiple positive answers or no positive answers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是使用一系列的*一对其他*分类器。对于每个可能的标签ℓ，我们构建一个分类器，类型是*这是ℓ还是其他什么？* 当应用规则时，正好有一个分类器会说*是的*，我们就得到了我们的解决方案。不幸的是，这并不总是发生，所以我们需要决定如何处理多个积极回答或没有积极回答的情况。
- en: '![Binary and multiclass classification](img/2772OS_02_06.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![二分类和多分类](img/2772OS_02_06.jpg)'
- en: Alternatively, we can build a classification tree. Split the possible labels
    into two, and build a classifier that asks, "Should this example go in the left
    or the right bin?" We can perform this splitting recursively until we obtain a
    single label. The preceding diagram depicts the tree of reasoning for the Iris
    dataset. Each diamond is a single binary classifier. It is easy to imagine that
    we could make this tree larger and encompass more decisions. This means that any
    classifier that can be used for binary classification can also be adapted to handle
    any number of classes in a simple way.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以构建一个分类树。将可能的标签分成两组，并构建一个分类器，问：“这个例子应该放入左边的箱子还是右边的箱子？”我们可以递归地进行这种分裂，直到我们得到一个单一的标签。前面的图展示了Iris数据集的推理树。每个菱形代表一个二分类器。可以想象，我们可以将这棵树做得更大，涵盖更多的决策。这意味着，任何可以用于二分类的分类器，都可以简单地调整来处理任意数量的类别。
- en: There are many other possible ways of turning a binary method into a multiclass
    one. There is no single method that is clearly better in all cases. The scikit-learn
    module implements several of these methods in the `sklearn.multiclass` submodule.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多其他方法可以将二元方法转化为多类方法。没有一种方法在所有情况下都明显优于其他方法。scikit-learn 模块在 `sklearn.multiclass`
    子模块中实现了几种这样的方法。
- en: Tip
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Some classifiers are binary systems, while many real-life problems are naturally
    multiclass. Several simple protocols reduce a multiclass problem to a series of
    binary decisions and allow us to apply the binary models to our multiclass problem.
    This means methods that are apparently only for binary data can be applied to
    multiclass data with little extra effort.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一些分类器是二元系统，而许多现实生活中的问题本质上是多类的。几种简单的协议将多类问题简化为一系列二元决策，并允许我们将二元模型应用于多类问题。这意味着看似仅适用于二元数据的方法，可以以极小的额外努力应用于多类数据。
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Classification means generalizing from examples to build a model (that is, a
    rule that can automatically be applied to new, unclassified objects). It is one
    of the fundamental tools in machine learning and we will see many more examples
    of this in the forthcoming chapters.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是从示例中进行泛化，构建模型（即一个可以自动应用于新的、未分类对象的规则）。它是机器学习中的基本工具之一，我们将在接下来的章节中看到更多这样的例子。
- en: In a sense, this was a very theoretical chapter, as we introduced generic concepts
    with simple examples. We went over a few operations with the Iris dataset. This
    is a small dataset. However, it has the advantage that we were able to plot it
    out and see what we were doing in detail. This is something that will be lost
    when we move on to problems with many dimensions and many thousands of examples.
    The intuitions we gained here will all still be valid.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，这一章是非常理论性的，因为我们用简单的例子介绍了通用概念。我们使用鸢尾花数据集进行了几次操作。这个数据集很小。然而，它的优势在于我们能够绘制出它的图像，并详细看到我们所做的事情。这一点在我们转向处理多维度和数千个样本的问题时将丧失。我们在这里获得的直觉依然有效。
- en: You also learned that the training error is a misleading, over-optimistic estimate
    of how well the model does. We must, instead, evaluate it on testing data that
    has not been used for training. In order to not waste too many examples in testing,
    a cross-validation schedule can get us the best of both worlds (at the cost of
    more computation).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学到了训练误差是对模型表现的误导性、过于乐观的估计。我们必须改为在未用于训练的测试数据上评估模型。为了避免在测试中浪费太多样本，交叉验证调度可以让我们兼得两全其美（代价是更多的计算量）。
- en: We also had a look at the problem of feature engineering. Features are not predefined
    for you, but choosing and designing features is an integral part of designing
    a machine learning pipeline. In fact, it is often the area where you can get the
    most improvements in accuracy, as better data beats fancier methods. The chapters
    on text-based classification, music genre recognition, and computer vision will
    provide examples for these specific settings.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到了特征工程的问题。特征并不是预先为你定义好的，选择和设计特征是设计机器学习管道的一个重要部分。事实上，这通常是你能在准确性上获得最多改进的领域，因为更好的数据胜过更复杂的方法。接下来的章节将通过文本分类、音乐流派识别和计算机视觉等具体实例，提供这些特定设置的示例。
- en: The next chapter looks at how to proceed when your data does not have predefined
    classes for classification.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论当你的数据没有预定义分类时，如何进行分类。
