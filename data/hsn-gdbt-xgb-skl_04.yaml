- en: '*Chapter 3*: Bagging with Random Forests'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章*：使用随机森林进行Bagging'
- en: In this chapter, you will gain proficiency in building **random forests**, a
    leading competitor to XGBoost. Like XGBoost, random forests are ensembles of decision
    trees. The difference is that random forests combine trees via **bagging**, while
    XGBoost combines trees via **boosting**. Random forests are a viable alternative
    to XGBoost with advantages and limitations that are highlighted in this chapter.
    Learning about random forests is important because they provide valuable insights
    into the structure of tree-based ensembles (XGBoost), and they allow a deeper
    understanding of boosting in comparison and contrast with their own method of
    bagging.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将掌握构建**随机森林**的技巧，随机森林是与XGBoost竞争的领先方法。与XGBoost一样，随机森林也是决策树的集成体。不同之处在于，随机森林通过**bagging**结合树，而XGBoost则通过**boosting**结合树。随机森林是XGBoost的一个可行替代方案，具有本章中强调的优点和局限性。了解随机森林非常重要，因为它们能为树基集成方法（如XGBoost）提供宝贵的见解，并使你能够更深入地理解boosting与bagging之间的比较和对比。
- en: In this chapter, you will build and evaluate **random forest classifiers** and
    **random forest regressors**, gain mastery of random forest hyperparameters, learn
    about bagging in the machine learning landscape, and explore a case study that
    highlights some random forest limitations that spurred the development of gradient
    boosting (XGBoost).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将构建和评估**随机森林分类器**和**随机森林回归器**，掌握随机森林的超参数，学习机器学习中的bagging技巧，并探索一个案例研究，突出随机森林的局限性，这些局限性促使了梯度提升（XGBoost）的发展。
- en: 'This chapter covers the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主要内容：
- en: Bagging ensembles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging集成方法
- en: Exploring random forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索随机森林
- en: Tuning random forest hyperparameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整随机森林超参数
- en: Pushing random forest boundaries – case study
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动随机森林的边界——案例研究
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is available at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter03)找到
- en: Bagging ensembles
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging集成方法
- en: In this section, you will learn why ensemble methods are usually superior to
    individual machine learning models. Furthermore, you will learn about the technique
    of bagging. Both are essential features of random forests.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解为什么集成方法通常优于单一的机器学习模型。此外，你还将学习bagging技巧。这两者是随机森林的重要特征。
- en: Ensemble methods
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法
- en: In machine learning, an ensemble method is a machine learning model that aggregates
    the predictions of individual models. Since ensemble methods combine the results
    of multiple models, they are less prone to error, and therefore tend to perform
    better.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，集成方法是一种通过聚合单个模型的预测结果来构建的模型。由于集成方法结合了多个模型的结果，它们较不容易出错，因此往往表现更好。
- en: Imagine your goal is to determine whether a house will sell within the first
    month of being on the market. You run several machine learning algorithms and
    find that **logistic regression** gives 80% accuracy, **decision trees** 75% accuracy,
    and **k-nearest neighbors** 77% accuracy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你的目标是确定一栋房子是否会在上市的第一个月内售出。你运行了几个机器学习算法，发现**逻辑回归**的准确率为80%，**决策树**为75%，**k最近邻**为77%。
- en: One option is to use logistic regression, the most accurate model, as your final
    model. A more compelling option is to combine the predictions of each individual
    model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选项是使用逻辑回归，作为最准确的模型，作为你的最终模型。更有说服力的选项是将每个单独模型的预测结果结合起来。
- en: For classifiers, the standard option is to take the majority vote. If at least
    two of three models predict that a house will sell within the first month, the
    prediction is *YES*. Otherwise, it's *NO*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类器，标准选项是采用多数投票。如果至少有三个模型中的两个预测房子会在第一个月内卖掉，那么预测结果是*YES*。否则，预测结果是*NO*。
- en: Overall accuracy is usually higher with ensemble methods. For a prediction to
    be wrong, it's not enough for one model to get it wrong; the majority of classifiers
    must get it wrong.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成方法通常能提高整体准确性。要让预测错误，仅仅一个模型出错是不够的；必须有大多数分类器都预测错误。
- en: Ensemble methods are generally classified into two types. The first type combines
    different machine learning models, such as scikit-learn's `VotingClassifier`,
    as chosen by the user. The second type of ensemble method combines many versions
    of the same model, as is the case with XGBoost and random forests.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法通常分为两类。第一类是将不同的机器学习模型组合在一起，例如用户选择的scikit-learn的`VotingClassifier`。第二类集成方法将同一模型的多个版本组合在一起，就像XGBoost和随机森林一样。
- en: Random forests are among the most popular and widespread of all ensemble methods.
    The individual models of random forests are decision trees, the focus of the previous
    chapter, [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision
    Trees in Depth*. A random forest may consist of hundreds or thousands of decision
    trees whose predictions are combined for the final result.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是所有集成方法中最流行和最广泛使用的。随机森林的单独模型是决策树，正如上一章的重点，[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)，*深入了解决策树*。一个随机森林可能包含数百或数千棵决策树，其预测结果将被合并成最终结果。
- en: Although random forests use majority rules for classifiers, and the average
    of all models for regressors, they also use a special method called bagging, short
    for bootstrap aggregation, to select individual trees.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机森林使用多数规则来分类器，使用所有模型的平均值来回归器，但它们还使用一种叫做袋装（bagging）的方法，袋装是自助聚合（bootstrap aggregation）的缩写，用来选择单独的决策树。
- en: Bootstrap aggregation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自助聚合
- en: '**Bootstrapping** means sampling with replacement.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**自助法（Bootstrapping）**是指带放回的抽样。'
- en: Imagine you have a bag of 20 shaded marbles. You are going to select 10 marbles,
    one at a time. Each time you select a marble, you put it back in the bag. This
    means that it's possible, though extremely unlikely, that you could pick the same
    marble 10 times.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一袋20颗有色大理石。你将依次选择10颗大理石，每次选择后都将其放回袋中。这意味着，虽然极不可能，但你有可能选中同一颗大理石10次。
- en: It's more likely that you will pick some marbles more than once, and some not
    at all.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你更可能多次选中一些大理石，而有些大理石则可能一次也不选中。
- en: 'Here is a visual of the marbles:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是大理石的可视化图：
- en: '![Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia
    Commons, https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg)](img/B15551_03_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 袋装的可视化示意图（改绘自：Siakorn，Wikimedia Commons，https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg）](img/B15551_03_01.jpg)'
- en: 'Figure 3.1 – Visual demonstration of bagging (Redrawn from: Siakorn, Wikimedia
    Commons, [https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg](https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg))'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 袋装的可视化示意图（改绘自：Siakorn，Wikimedia Commons，[https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg](https://commons.wikimedia.org/wiki/File:Ensemble_Bagging.svg)）
- en: As you can see from the preceding diagram, bootstrap samples are achieved by
    sampling with replacement. If the marbles were not replaced, it would be impossible
    to obtain a sample with more black (*blue* in the original diagram) marbles than
    the original bag, as in the far-right box.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的示意图可以看出，自助样本是通过带放回的抽样获得的。如果大理石没有被放回，那么就不可能得到比原始袋中更多黑色（*原图中的蓝色*）大理石的样本，正如最右侧的框所示。
- en: When it comes to random forests, bootstrapping works under the hood. The bootstrapping
    occurs when each decision tree is made. If the decision trees all consisted of
    the same samples, the trees would give similar predictions making the aggregate
    result similar to the individual tree. Instead, with random forests, the trees
    are built using bootstrapping, usually with the same number of samples as in the
    original dataset. Mathematical estimations are that two-thirds of the samples
    for each tree are unique, and one-third include duplicates.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，自助法在幕后发挥作用。自助法发生在每棵决策树建立时。如果所有的决策树都由相同的样本组成，那么这些树会给出类似的预测，最终结果也会与单棵树的预测相似。而在随机森林中，树是通过自助法建立的，通常样本数与原始数据集相同。数学估计表明，每棵树中约三分之二的样本是唯一的，三分之一包含重复样本。
- en: After the bootstrapping phase of the model-build, each decision tree makes its
    own individual predictions. The result is a forest of trees whose predictions
    are aggregated into one final prediction using majority rules for classifiers
    and the average for regressors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型建立的自助法阶段后，每棵决策树会做出自己的单独预测。结果是由多棵树组成的森林，这些树的预测结果会根据分类器的多数规则和回归器的平均值合并成最终预测。
- en: In summary, a random forest aggregates the predictions of bootstrapped decision
    trees. This general ensemble method is known in machine learning as bagging.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，随机森林聚合了自助法生成的决策树的预测。这种通用的集成方法在机器学习中被称为自助法（bagging）。
- en: Exploring random forests
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索随机森林
- en: To get a better sense of how random forests work, let's build one using scikit-learn.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解随机森林的工作原理，让我们使用 scikit-learn 构建一个随机森林。
- en: Random forest classifiers
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: 'Let''s use a random forest classifier to predict whether a user makes more
    or less than USD 50,000 using the census dataset we cleaned and scored in [*Chapter
    1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning Landscape*,
    and revisited in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*. We are going to use `cross_val_score` to ensure that
    our test results generalize well:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个随机森林分类器，使用我们在[**第1章**](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)，*机器学习概览*中清理和评分的普查数据集，预测用户收入是否超过
    50,000 美元，并在[**第2章**](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)，*决策树深入剖析*中重新检查。我们将使用`cross_val_score`确保我们的测试结果具有良好的泛化能力：
- en: 'The following steps build and score a random forest classifier using the census
    dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤使用普查数据集构建并评分一个随机森林分类器：
- en: 'Import `pandas`, `numpy`, `RandomForestClassifier`, and `cross_val_score` before
    silencing warnings:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`RandomForestClassifier`和`cross_val_score`，然后关闭警告：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the dataset `census_cleaned.csv` and split it into `X` (a predictor column)
    and `y` (a target column):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集`census_cleaned.csv`并将其拆分为`X`（预测变量）和`y`（目标变量）：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With our imports and data ready to go, it's time to build a model.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在准备好导入和数据后，现在是时候构建模型了。
- en: Next, we initialize the random forest classifier. In practice, ensemble algorithms
    work just like any other machine learning algorithm. A model is initialized, fit
    to the training data, and scored against the test data.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化随机森林分类器。在实践中，集成算法与其他机器学习算法一样工作。一个模型被初始化、拟合训练数据，并在测试数据上评分。
- en: 'We initialize a random forest by setting the following hyperparameters in advance:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过提前设置以下超参数来初始化随机森林：
- en: a) `random_state=2` to ensure that your results are consistent with ours.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `random_state=2`确保你的结果与我们的结果一致。
- en: b) `n_jobs=-1` to speed up computations by taking advantage of parallel processing.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `n_jobs=-1`通过利用并行处理加速计算。
- en: 'c) `n_estimators=10`, a previous scikit-learn default sufficient to speed up
    computations and avoid ambiguity; new defaults have set `n_estimators=100`. `n_esmitators`
    will be explored in further detail in the next section:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `n_estimators=10`，这是 scikit-learn 的默认值，足以加速计算并避免歧义；新的默认值已设置为`n_estimators=100`。`n_estimators`将在下一节中详细探讨：
- en: '[PRE2]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now we''ll use `cross_val_score`. `Cross_val_score` requires a model, predictor
    columns, and a target column as inputs. Recall that `cross_val_score` splits,
    fits, and scores the data:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用`cross_val_score`。`cross_val_score`需要一个模型、预测列和目标列作为输入。回顾一下，`cross_val_score`会对数据进行拆分、拟合和评分：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Display the results:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果：
- en: '[PRE4]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The default random forest classifier provides a better score for the census
    dataset than the decision tree in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth* (81%), but not quite as good as XGBoost in [*Chapter
    1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022), *Machine Learning Landscape*
    (86%). Why does it perform better than individual decision trees?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的随机森林分类器在[**第2章**](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)，*决策树深入剖析*（81%）的数据集上，比决策树表现更好，但还不如[**第1章**](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)，*机器学习概览*（86%）中的
    XGBoost。为什么它比单一的决策树表现更好？
- en: The improved performance is likely on account of the bagging method described
    in the previous section. With 10 trees in this forest (since `n_estimators=10`),
    each prediction is based on 10 decision trees instead of 1\. The trees are bootstrapped,
    which increases diversity, and aggregated, which reduces variance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提升可能与上一节中描述的自助法（bagging）有关。在这个森林中有 10 棵树（因为`n_estimators=10`），每个预测是基于 10 棵决策树，而不是
    1 棵。树是通过自助法生成的，这增加了多样性，并通过聚合减少了方差。
- en: By default, random forest classifiers select from the square root of the total
    number of features when looking for a split. So, if there are 100 features (columns),
    each decision tree will only consider 10 features when choosing a split. Thus
    two trees with duplicate samples may give very different predictions due to the
    different splits. This is another way that random forests reduce variance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，随机森林分类器在寻找分裂时会从特征总数的平方根中选择特征。因此，如果有100个特征（列），每棵决策树在选择分裂时只会考虑10个特征。因此，两个样本重复的树可能由于分裂的不同而给出完全不同的预测。这是随机森林减少方差的另一种方式。
- en: In addition to classification, random forests also work with regression.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分类，随机森林还可以用于回归。
- en: Random forest regressors
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林回归器
- en: In a random forest regressor, the samples are bootstrapped, as with the random
    forest Classifier, but the max number of features is the total number of features
    instead of the square root. This change is due to experimental results (see [https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林回归器中，样本是通过自助法（bootstrap）抽取的，和随机森林分类器一样，但最大特征数是特征总数，而不是平方根。这个变化是基于实验结果（参见
    [https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf](https://orbi.uliege.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf)）。
- en: Furthermore, the final prediction is made by taking the average of the predictions
    of all the trees, instead of a majority rules vote.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最终的预测是通过对所有决策树的预测结果求平均得出的，而不是通过多数规则投票。
- en: 'To see a random forest regressor in action, complete the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看随机森林回归器的实际应用，请完成以下步骤：
- en: 'Upload the bike rental dataset from [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*, and pull up the first five rows for a refresher:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深度剖析》上传自行车租赁数据集，并提取前五行以供回顾：
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code should result in the following output:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码应生成以下输出：
- en: '![Figure 3.2 – Bike rentals dataset – cleaned](img/B15551_03_02.jpg)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.2 – 自行车租赁数据集 – 已清理](img/B15551_03_02.jpg)'
- en: Figure 3.2 – Bike rentals dataset – cleaned
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.2 – 自行车租赁数据集 – 已清理
- en: 'Split the data into `X` and `y`, the predictive and target columns:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据划分为`X`和`y`，即预测列和目标列：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Import the regressor, then initialize it using the same default hyperparameters,
    `n_estimators=10`, `random_state=2`, and `n_jobs=-1`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入回归器，然后使用相同的默认超参数进行初始化，`n_estimators=10`，`random_state=2`，`n_jobs=-1`：
- en: '[PRE7]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we need to use `cross_val_score`. Place the regressor, `rf`, along with
    predictor and target columns inside `cross_val_score`. Note that the negative
    mean squared error (`''neg_mean_squared_error''`) should be defined as the scoring
    parameter. Select 10 folds (`cv=10`):'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要使用`cross_val_score`。将回归器`rf`与预测器和目标列一起放入`cross_val_score`。请注意，负均方误差（`'neg_mean_squared_error'`）应定义为评分参数。选择10折交叉验证（`cv=10`）：
- en: '[PRE8]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Find and display the **root mean squared error** (**RMSE**):'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找并显示**均方根误差**（**RMSE**）：
- en: '[PRE9]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The random forest performs respectably, though not as well as other models that
    we have seen. We will further examine the bike rentals dataset in the case study
    later in this chapter to see why.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的表现令人满意，尽管不如我们之前看到的其他模型。我们将在本章后面的案例研究中进一步分析自行车租赁数据集，以了解原因。
- en: Next, let's examine random forest hyperparameters in detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细查看随机森林的超参数。
- en: Random forest hyperparameters
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林超参数
- en: The range of random forest hyperparameters is large, unless one already has
    a working knowledge of decision tree hyperparameters, as covered in [*Chapter
    2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision Trees in Depth*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林超参数的范围很大，除非已经具备决策树超参数的工作知识，如在[*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深度剖析》中所讨论的那样。
- en: In this section, we will go over additional random forest hyperparameters before
    grouping the hyperparameters that you have already seen. Many of these hyperparameters
    will be used by XGBoost.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在介绍您已见过的超参数分组之前，讨论一些额外的随机森林超参数。许多超参数将被 XGBoost 使用。
- en: oob_score
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: oob_score
- en: Our first hyperparameter, and perhaps the most intriguing, is `oob_score`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个超参数，可能也是最引人注目的，是`oob_score`。
- en: Random forests select decision trees via bagging, meaning that samples are selected
    with replacement. After all of the samples have been chosen, some samples should
    remain that have not been chosen.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过袋装（bagging）选择决策树，这意味着样本是带替换地选取的。所有样本选择完后，应该会有一些未被选择的样本。
- en: It's possible to hold back these samples as the test set. After the model is
    fit on one tree, the model can immediately be scored against this test set. When
    the hyperparameter is set to `oob_score=True`, this is exactly what happens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将这些样本作为测试集保留。在模型拟合完一棵树后，模型可以立即用这个测试集进行评分。当超参数设置为`oob_score=True`时，正是发生了这种情况。
- en: In other words, `oob_score` provides a shortcut to get a test score. `oob_score`
    may be printed out immediately after the model has been fit.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`oob_score`提供了一种获取测试分数的快捷方式。在模型拟合后，可以立即打印出`oob_score`。
- en: Let's use `oob_score` on the census dataset to see how it works in practice.
    Since we are using `oob_score` to test the model, it's not necessary to split
    the data into a training set and test set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在普查数据集上使用`oob_score`，看看它在实践中的表现。由于我们使用`oob_score`来测试模型，因此不需要将数据拆分为训练集和测试集。
- en: 'The random forest may be initialized as usual with `oob_score=True`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以像往常一样初始化，设置`oob_score=True`：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, `rf` may be fit on the data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，可以在数据上拟合`rf`：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since `oob_score=True`, the score is available after the model has been fit.
    It may be accessed using the model attribute `.oob_score_` as follows (note the
    underscore after `score`):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`oob_score=True`，在模型拟合后可以获得分数。可以通过模型的属性`.oob_score_`来访问分数，如下所示（注意`score`后有下划线）：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The score is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As described previously, `oob_score` is created by scoring samples on individual
    trees excluded during the training phase. When the number of trees in the forest
    is small, as is the case with 10 estimators, there may not be enough test samples
    to maximize accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`oob_score`是通过对训练阶段被排除的个别树上的样本进行评分生成的。当森林中的树木数量较少时（例如使用10个估计器），可能没有足够的测试样本来最大化准确度。
- en: More trees mean more samples, and often greater accuracy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的树意味着更多的样本，通常也意味着更高的准确性。
- en: n_estimators
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n_estimators
- en: Random forests are powerful when there are many trees in the forest. How many
    is enough? Recently, scikit-learn defaults changed from 10 to 100\. While 100
    trees may be enough to cut down on variance and obtain good scores, for larger
    datasets, 500 or more trees may be required.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当森林中有很多树时，随机森林的效果非常强大。那么多少棵树才足够呢？最近，scikit-learn的默认设置已从10改为100。虽然100棵树可能足够减少方差并获得良好的分数，但对于较大的数据集，可能需要500棵或更多的树。
- en: 'Let''s start with `n_estimators=50` to see how `oob_score` changes:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`n_estimators=50`开始，看看`oob_score`是如何变化的：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The score is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A definite improvement. What about 100 trees?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有了明显的提升。那么100棵树呢？
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The score is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The gain is smaller. As `n_estimators` continues to rise, scores will eventually
    level off.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 增益较小。随着`n_estimators`的不断增加，分数最终会趋于平稳。
- en: warm_start
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: warm_start
- en: The `warm_start` hyperparameter is great for determining the number of trees
    in the forest (`n_estimators`). When `warm_start=True`, adding more trees does
    not require starting over from scratch. If you change `n_estimators` from 100
    to 200, it may take twice as long to build the forest with 200 trees. When `warm_start=True`,
    the random forest with 200 trees does not start from scratch, but rather starts
    where the previous model stopped.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`warm_start`超参数非常适合确定森林中树的数量（`n_estimators`）。当`warm_start=True`时，添加更多树木不需要从头开始。如果将`n_estimators`从100改为200，构建200棵树的森林可能需要两倍的时间。使用`warm_start=True`时，200棵树的随机森林不会从头开始，而是从先前模型停止的位置继续。'
- en: '`warm_start` may be used to plot various scores with a range of `n_estimators`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`warm_start`可以用来绘制不同`n_estimators`值下的各种分数。'
- en: 'As an example, the following code takes increments of 50 trees, starting with
    50 and ending at 500, to display a range of scores. This code may take time to
    run as it is building 10 random forests by adding 50 new trees each round! The
    code is broken down in the following steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，以下代码每次增加50棵树，从50开始，到500结束，显示一系列分数。由于每轮都在通过添加50棵新树来构建10个随机森林，这段代码可能需要一些时间才能完成运行！代码按以下步骤分解：
- en: 'Import matplotlib and seaborn, then set the seaborn dark grid with `sns.set()`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入matplotlib和seaborn，然后通过`sns.set()`设置seaborn的暗色网格：
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize an empty list of scores and initialize a random forest classifier
    with 50 estimators, making sure that `warm_start=True` and `oob_score=True`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空的分数列表，并用50个估计器初始化随机森林分类器，确保`warm_start=True`和`oob_score=True`：
- en: '[PRE20]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Fit `rf` to the dataset, then append `oob_score` to the `oob_scores` list:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`rf`拟合到数据集上，然后将`oob_score`添加到`oob_scores`列表中：
- en: '[PRE21]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Prepare a list of estimators that contains the number of trees starting with
    50:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个估计器列表，其中包含从50开始的树的数量：
- en: '[PRE22]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Write a for loop that adds 50 trees each round. For each round, add 50 to `est`,
    append `est` to the `estimators` list, change `n_estimators` with `rf.set_params(n_estimators=est)`,
    fit the random forest on the data, then append the new `oob_score_`:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写一个 for 循环，每轮添加 50 棵树。每一轮，向 `est` 添加 50， 将 `est` 附加到 `estimators` 列表中，使用 `rf.set_params(n_estimators=est)`
    更改 `n_estimators`，在数据上拟合随机森林，然后附加新的 `oob_score_`：
- en: '[PRE23]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For a nice display, show a larger graph, then plot the estimators and `oob_scores`.
    Add the appropriate labels, then save and show the graph:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好的展示，显示一个更大的图表，然后绘制估计器和 `oob_scores`。添加适当的标签，然后保存并显示图表：
- en: '[PRE24]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This generates the following graph:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![Figure 3.3 – Random forest Warm Start – oob_score per number of trees](img/B15551_03_03.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 随机森林热启动 – 每棵树的 oob_score](img/B15551_03_03.jpg)'
- en: Figure 3.3 – Random forest Warm Start – oob_score per number of trees
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 随机森林热启动 – 每个树的 oob_score
- en: As you can see, the number of trees tends to peak at around 300\. It's more
    costly and time-consuming to use more trees than 300, and the gains are minimal
    at best.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，树木的数量在大约 300 时趋于峰值。使用超过 300 棵树成本较高且耗时，而且收益微乎其微。
- en: bootstrap
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bootstrap
- en: Although random forests are traditionally bootstrapped, the `bootstrap` hyperparameter
    may be set to `False`. If `bootstrap=False`, `oob_score` cannot be included since
    `oob_score` is only possible when samples have been left out.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机森林通常是自助法（bootstrap），但 `bootstrap` 超参数可以设置为 `False`。如果 `bootstrap=False`，则无法包含
    `oob_score`，因为 `oob_score` 仅在样本被排除时才可能。
- en: We will not pursue this option, although it makes sense if underfitting occurs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再继续此选项，尽管如果发生欠拟合，这个方法是合理的。
- en: Verbose
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冗长
- en: The `verbose` hyperparameter may be changed to a higher number to display more
    information when building a model. You may try it on your own for experimentation.
    When building large models, `verbose=1` may provide helpful information along
    the way.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`verbose` 超参数可以设置为更高的数字，以在构建模型时显示更多信息。你可以自己尝试实验。当构建大型模型时，`verbose=1` 可以在过程中提供有用的信息。'
- en: Decision Tree hyperparameters
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树超参数
- en: The remaining hyperparameters all come from decision trees. It turns out that
    decision tree hyperparameters are not as significant within random forests since
    random forests cut down on variance by design.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的超参数都来自决策树。事实证明，在随机森林中，决策树的超参数并不那么重要，因为随机森林本身通过设计减少了方差。
- en: Here are decision tree hyperparameters grouped according to category for you
    to review.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是按类别分组的决策树超参数，供您查看。
- en: Depth
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度
- en: 'The hyperparameters that fall under this category are:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 属于此类别的超参数有：
- en: '`max_depth`: Always good to tune. Determines the number of times splits occur.
    Known as the length of the tree. A great way to reduce variance.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：总是需要调整。决定分裂发生的次数，也就是树的长度。是减少方差的一个好方法。'
- en: Splits
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分裂
- en: 'The hyperparameters that fall under this category are:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 属于此类别的超参数有：
- en: '`max_features`: Limits the number of features to choose from when making splits.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：限制在进行分裂时可选择的特征数。'
- en: '`min_samples_split`: Increases the number of samples required for new splits.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：增加进行新分裂所需的样本数。'
- en: '`min_impurity_decrease`: Limits splits to decrease impurity greater than the
    set threshold.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_impurity_decrease`：限制分裂以减少超过设定阈值的杂质。'
- en: Leaves
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 叶子
- en: 'The hyperparameters that fall under this category are:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 属于此类别的超参数有：
- en: '`min_samples_leaf`: Increases the minimum number of samples required for a
    node to be a leaf.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`：增加成为叶子节点所需的最小样本数。'
- en: '`min_weight_fraction_leaf`: The fraction of the total weights required to be
    a leaf.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_weight_fraction_leaf`：成为叶子的所需总权重的比例。'
- en: 'For more information on the preceding hyperparameters, check out the official
    random forest regressor documentation: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于上述超参数的信息，请查阅官方随机森林回归器文档：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
- en: Pushing random forest boundaries – case study
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推动随机森林的边界 – 案例研究
- en: Imagine you work for a bike rental company and your goal is to predict the number
    of bike rentals per day depending upon the weather, the time of day, the time
    of year, and the growth of the company.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你为一家自行车租赁公司工作，目标是根据天气、一天中的时间、季节和公司的成长来预测每天的自行车租赁数量。
- en: Earlier in this chapter, you implemented a random forest regressor with cross-validation
    to obtain an RMSE of 945 bikes. Your goal is to modify the random forest to obtain
    the lowest error score possible.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，您实现了一个带有交叉验证的随机森林回归器，得到了945辆自行车的RMSE。您的目标是修改随机森林以获得尽可能低的误差得分。
- en: Preparing the dataset
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'Earlier in this chapter, you downloaded the dataset `df_bikes` and split it
    into `X_bikes` and `y_bikes`. Now that you are doing some serious testing, you
    decide to split `X_bikes` and `y_bikes` into training sets and test sets as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，您下载了数据集`df_bikes`并将其分割为`X_bikes`和`y_bikes`。现在，您进行一些严肃的测试，决定将`X_bikes`和`y_bikes`拆分为训练集和测试集，如下所示：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: n_estimators
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n_estimators
- en: Start by choosing a reasonable value for `n_estimators`. Recall that `n_estimators`
    can be increased to improve accuracy at the cost of computational resources and
    time.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先选择一个合理的`n_estimators`值。回想一下，`n_estimators`可以增加以提高准确性，但会以计算资源和时间为代价。
- en: 'The following is a graph of RMSE using the `warm_start` method for a variety
    of `n_estimators` using the same general code provided previously under the *warm_start*
    heading:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用`warm_start`方法对多种`n_estimators`进行RMSE图形展示，所使用的代码与之前在*warm_start*部分提供的相同：
- en: '![Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees](img/B15551_03_04.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 随机森林自行车租赁 – 每棵树的RMSE](img/B15551_03_04.jpg)'
- en: Figure 3.4 – Random forest Bike Rentals – RMSE per number of trees
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 随机森林自行车租赁 – 每棵树的RMSE
- en: This graph is very interesting. The random forest provides the best score with
    50 estimators. After 100 estimators, the error gradually starts to go up, a concept
    that will be revisited later.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表非常有趣。随机森林在50个估计器下提供了最佳得分。在100个估计器后，误差开始逐渐增加，这是一个稍后会重新讨论的概念。
- en: For now, it's sensible to use `n_estimators=50` as the starting point.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`n_estimators=50`作为起点是合理的选择。
- en: cross_val_score
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: cross_val_score
- en: With errors ranging from 620 to 690 bike rentals according to the preceding
    graph, it's time to see how the dataset performs with cross-validation using `cross_val_score`.
    Recall that in cross-validation the purpose is to divide the samples into *k*
    different folds, and to use all samples as test sets over the different folds.
    Since all samples are used to test the model, `oob_score` will not work.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的图表，误差范围从620到690辆自行车租赁，现在是时候看看数据集在使用`cross_val_score`进行交叉验证时的表现了。回想一下，在交叉验证中，目的是将样本划分为*k*个不同的折，并在不同的折中使用所有样本作为测试集。由于所有样本都用于测试模型，`oob_score`将无法使用。
- en: 'The following code contains the same steps that you used earlier in the chapter:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码包含了您在本章早期使用的相同步骤：
- en: Initialize the model.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型。
- en: Score the model, using `cross_val_score` with the model, predictor columns,
    target column, scoring, and the number of folds as parameters.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型进行评分，使用`cross_val_score`与模型、预测列、目标列、评分标准和折数作为参数。
- en: Compute the RMSE.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算RMSE。
- en: Display the cross-validation scores and the mean.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示交叉验证得分和平均值。
- en: 'Here is the code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是代码：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This score is better than earlier in the chapter. Notice that the error in the
    last fold is much higher according to the last entry in the RMSE array. This could
    be due to errors within the data or outliers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分比本章之前的得分更好。注意，最后一个折中的误差显著更高，根据RMSE数组中的最后一项。这可能是由于数据中的错误或异常值所致。
- en: Fine-tuning hyperparameters
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调超参数
- en: 'It''s time to create a grid of hyperparameters to fine-tune our model using
    `RandomizedSearchCV`. Here is a function that uses `RandomizedSearchCV` to display
    the RMSEs along with the mean score and best hyperparameters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候创建一个超参数网格，使用`RandomizedSearchCV`来微调我们的模型了。以下是一个使用`RandomizedSearchCV`的函数，用于显示RMSE和平均得分以及最佳超参数：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is a starter''s grid of hyperparameters placed inside the new `randomized_search_reg`
    function to obtain the first results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个初学者的超参数网格，放入新的`randomized_search_reg`函数中以获得初步结果：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This is a major improvement. Let''s see if we can do better by narrowing the
    range:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的改进。让我们看看通过缩小范围是否能够得到更好的结果：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The score has improved yet again.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 得分再次提高了。
- en: 'Now let''s increase the number of runs, and give more options for `max_depth`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们增加运行次数，并为`max_depth`提供更多选项：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The score keeps getting better. At this point, it may be worth narrowing the
    ranges further, based upon the previous results:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 得分持续提升。此时，根据之前的结果，可能值得进一步缩小范围：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The test score has gone back up. Increasing `n_estimators` at this point could
    be a good idea. The more trees in the forest, the more potential there may be
    to realize small gains.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 测试分数已经回升。此时增加`n_estimators`可能是一个好主意。森林中的树木越多，可能带来的小幅提升也越大。
- en: 'We can also increase the number of runs to `20` to look for better hyperparameter
    combinations. Keep in mind that results are based on a randomized search, not
    a full grid search:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将运行次数增加到`20`，以寻找更好的超参数组合。请记住，结果是基于随机搜索，而不是完全的网格搜索：
- en: '[PRE37]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE38]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This matches the best score achieved thus far. We could keep tinkering. It's
    possible with enough experimentation that the test score may drop to under 600
    bikes. But we also seem to be peaking around the low 600 mark.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这与迄今为止取得的最佳得分相匹配。我们可以继续调整。通过足够的实验，测试分数可能会降到低于 600 辆的水平。但我们似乎已经在 600 辆附近达到了瓶颈。
- en: 'Finally, let''s place our best model in `cross_val_score` to see how the result
    compares with the original:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将最好的模型放入`cross_val_score`，看看结果与原始模型相比如何：
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The RMSE goes back up to `817`. The score is much better than `903`, but it's
    considerably worse than `619`. What's going on here?
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 回升至`817`。这个得分比`903`好得多，但比`619`差得多。这是怎么回事？
- en: 'There may be an issue with the last split in `cross_val_score` since its score
    is twice as bad as the others. Let''s see if shuffling the data does the trick.
    Scikit-learn has a shuffle module that may be imported from `sklearn.utils` as
    follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cross_val_score`中可能存在最后一次分割的问题，因为它的得分比其他的差了两倍。让我们看看打乱数据是否能解决这个问题。Scikit-learn
    有一个 shuffle 模块，可以从 `sklearn.utils` 导入，方法如下：
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we can shuffle the data as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按如下方式打乱数据：
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now split the data into a new `X` and `y` and run `RandomForestRegressor` with
    `cross_val_score` again:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将数据分成新的`X`和`y`，再次运行`RandomForestRegressor`并使用`cross_val_score`：
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the shuffled data, there is no issue with the last split, and the score is
    much higher, as expected.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在打乱数据后，最后一次分割没有问题，得分比预期要高得多。
- en: Random forest drawbacks
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林的缺点
- en: At the end of the day, the random forest is limited by its individual trees.
    If all trees make the same mistake, the random forest makes this mistake. There
    are scenarios, as is revealed in this case study before the data was shuffled,
    where random forests are unable to significantly improve upon errors due to challenges
    within the data that individual trees are unable to address.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 到头来，随机森林的性能受限于其单棵树。如果所有树都犯了同样的错误，随机森林也会犯这个错误。正如本案例研究所揭示的，在数据未打乱之前，随机森林无法显著改进错误，原因是数据中的某些问题单棵树无法处理。
- en: An ensemble method capable of improving upon initial shortcomings, an ensemble
    method that will learn from the mistakes of trees in future rounds, could be advantageous.
    Boosting was designed to learn from the mistakes of trees in early rounds. Boosting,
    in particular gradient boosting – the focus of the next chapter – addresses this
    topic.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一种能够改进初始缺陷的集成方法，一种能够在未来回合中从决策树的错误中学习的集成方法，可能会带来优势。Boosting 的设计就是为了从早期回合中树木的错误中学习。Boosting，特别是梯度提升——下一章的重点——讨论了这一主题。
- en: 'In closure, the following graph displays the results of the tuned random forest
    regressor and the default XGBoost regressor when increasing the number of trees
    in the bike rentals dataset if the data is not shuffled:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下图表展示了调优后的随机森林回归器和默认的 XGBoost 回归器，在没有打乱数据的情况下增加树木数量时的表现：
- en: '![](img/B15551_03_05.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15551_03_05.jpg)'
- en: Figure 3.5 – Comparing the XGBoost default model with a tuned random forest
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 比较 XGBoost 默认模型和调优后的随机森林
- en: As you can see, XGBoost does a much better job of learning as the number of
    trees increases. And the XGBoost model has not even been tuned!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，XGBoost 在树木数量增加时的学习表现远远优于其他方法。而且 XGBoost 模型甚至还没有进行调优！
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about the importance of ensemble methods. In particular,
    you learned about bagging, the combination of bootstrapping, sampling with replacement,
    and aggregation, combining many models into one. You built random forest classifiers
    and regressors. You adjusted `n_estimators` with the `warm_start` hyperparameter
    and used `oob_score_` to find errors. Then you modified random forest hyperparameters
    to fine-tune models. Finally, you examined a case study where shuffling the data
    gave excellent results but adding more trees to the random forest did not result
    in any gains with the unshuffled data, as contrasted with XGBoost.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了集成方法的重要性，特别是了解了袋装法（bagging），即自助抽样（bootstrapping）、带放回的采样和聚合的结合，将多个模型合并成一个。你构建了随机森林分类器和回归器。你通过调整`n_estimators`和`warm_start`超参数，并使用`oob_score_`来查找误差。然后，你修改了随机森林的超参数以优化模型。最后，你分析了一个案例研究，其中数据洗牌带来了优秀的结果，而对未洗牌数据增加更多的树并没有带来任何提升，这与XGBoost的结果形成了对比。
- en: In the next chapter, you will learn the fundamentals of boosting, an ensemble
    method that learns from its mistakes to improve upon accuracy as more trees are
    added. You will implement gradient boosting to make predictions, thereby setting
    the stage for Extreme gradient boosting, better known as XGBoost.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习提升方法（boosting）的基本原理，这是一种集成方法，通过从错误中学习，随着更多决策树的加入，提升准确率。你将实现梯度提升（gradient
    boosting）进行预测，为极端梯度提升（Extreme Gradient Boosting，简称XGBoost）奠定基础。
