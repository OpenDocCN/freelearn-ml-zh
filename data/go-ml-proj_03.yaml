- en: Classification - Spam Email Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类 - 垃圾邮件检测
- en: What makes you you? I have dark hair, pale skin, and Asiatic features. I wear
    glasses. My facial structure is vaguely round, with extra subcutaneous fat in
    my cheeks compared to my peers. What I have done is describe the features of my
    face. Each of these features described can be thought of as a point within a probability
    continuum. What is the probability of having dark hair? Among my friends, dark
    hair is a very common feature, and so are glasses (a remarkable statistic is out
    of the 300 people or so I polled on my Facebook page, 281 of them require prescription
    glasses). The epicanthic folds of my eyes are probably less common, as is the
    extra subcutaneous fat in my cheeks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 什么让你成为你？我有一头黑发，皮肤苍白，具有亚洲特征。我戴眼镜。我的面部结构略呈圆形，与同龄人相比，我的脸颊有更多的皮下脂肪。我所做的是描述我的面部特征。这些描述的每个特征都可以被视为概率连续体中的一个点。拥有黑发的概率是多少？在我的朋友们中，黑发是一个非常常见的特征，眼镜也是如此（一个引人注目的统计数据显示，在我Facebook页面上调查的大约300人中，有281人需要处方眼镜）。我的眼睑褶皱可能不太常见，脸颊上的额外皮下脂肪也是如此。
- en: Why am I bringing up my facial features in a chapter about spam classification?
    It's because the principles are the same. If I show you a photo of a human face,
    what is the probability that the photo is of me? We can say that the probability
    that the photo is a photo of my face is a combination of the probability of having
    dark hair, the probability of having pale skin, the probability of having an epicanthic
    fold, and so on, and so forth. From a Naive point of view, we can think of each
    of the features independently contributing to the probability that the photo is
    me—the fact that I have an epicanthic fold in my eyes is independent from the
    fact that my skin is of a yellow pallor. But, of course, with recent advancements
    in genetics, this has been shown to be patently untrue. These features are, in
    real life, correlated with one another. We will explore this in a future chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我在关于垃圾邮件分类的章节中提到我的面部特征？这是因为原理是相同的。如果我给你一张人类面孔的照片，这张照片是我照片的概率是多少？我们可以这样说，照片是我面孔的概率是拥有黑发、拥有苍白皮肤、拥有眼睑褶皱等概率的组合。从朴素的角度来看，我们可以认为每个特征独立地贡献于照片是我面孔的概率——我眼睛上有眼睑褶皱这一事实与我的皮肤是黄色这一事实是独立的。但是，当然，随着遗传学最近的发展，这已经被证明是完全错误的。在现实生活中，这些特征是相互关联的。我们将在未来的章节中探讨这一点。
- en: Despite a real-life dependence of probability, we can still assume the Naive
    position and think of these probabilities as independent contributions to the
    probability that the photo is one of my face.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在现实生活中概率是相互依赖的，我们仍然可以假设朴素立场，并将这些概率视为对照片是我面孔的概率的独立贡献。
- en: In this chapter, we will build a email spam classification system using a Naive
    Bayes algorithm, which can be used beyond email spam classification. Along the
    way, we will explore the very basics of natural language processing, and how probability
    is inherently tied to the very language we use. A probabilistic understanding
    of language will be built up from the ground with the introduction of the **term
    frequency-inverse document frequency** (**TF-IDF**), which will then be translated
    into Bayesian probabilities, which is used to classify the emails.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用朴素贝叶斯算法构建一个电子邮件垃圾邮件分类系统，该系统可以用于电子邮件垃圾邮件分类之外的应用。在这个过程中，我们将探讨自然语言处理的基础知识，以及概率如何与我们所使用的语言本质相连。通过引入**词频-逆文档频率**（**TF-IDF**），我们将从地面开始构建对语言的概率理解，并将其转化为贝叶斯概率，用于对电子邮件进行分类。
- en: The project
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: 'What we want to do is simple: given an email, is it kosher (which we call ham),
    or is it a spam email? We will be using the `LingSpam` database. The emails from
    that database are a little dated—spammers update their techniques and words all
    the time. However, I chose the `LingSpam` corpus for a good reason: it is already
    nicely preprocessed. The original scope of this chapter was to introduce the preprocessing
    of emails; however, the topic of preprocessing options for natural language is
    itself a topic for an entire book, so we will use a dataset that has already been
    preprocessed. This allows us to focus more on the mechanics of a very elegant
    algorithm.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的是很简单：给定一封电子邮件，它是可接受的（我们称之为正常邮件），还是垃圾邮件？我们将使用`LingSpam`数据库。该数据库中的电子邮件有些过时——垃圾邮件发送者不断更新他们的技术和词汇。然而，我选择`LingSpam`语料库有一个很好的原因：它已经被很好地预处理了。本章最初的范围是介绍电子邮件的预处理；然而，自然语言预处理选项本身就是一个可以写成一整本书的主题，所以我们将使用已经预处理过的数据集。这使我们能够更多地关注非常优雅的算法的机制。
- en: Fear not, though, as I will actually walk through the brief basics of preprocessing.
    Be warned, however, that the level of complexity jumps up in a very steep curve,
    so be prepared to be sucked into a black hole of many hours on preprocessing natural
    language. At the end of this chapter, I will also recommend some libraries that
    will be useful for preprocessing.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 不要担心，因为我将实际介绍预处理的基本知识。但是，警告一下，复杂度将以非常陡峭的曲线上升，所以请准备好投入许多小时进行自然语言的预处理。在本章结束时，我还会推荐一些有用的库，这些库将有助于预处理。
- en: Exploratory data analysis
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'Let''s jump into the data. The `LingSpam` corpus comes with four variants of
    the same corpus: `bare`, `lemm`, `lemm_stop`, and `stop`. In each variant, there
    are ten parts and each part contains multiple files. Each file represents an email.
    Files with a `spmsg` prefix in its name are spam, while the rest are ham. An example
    email looks as follows (from the `bare` variant):'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数据。`LingSpam`语料库包含同一语料库的四个变体：`bare`、`lemm`、`lemm_stop`和`stop`。在每个变体中，都有十个部分，每个部分包含多个文件。每个文件代表一封电子邮件。文件名带有`spmsg`前缀的是垃圾邮件，其余的是正常邮件。以下是一个示例电子邮件（来自`bare`变体）：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are some things to note about this particular email:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这封特定的电子邮件，这里有一些需要注意的事项：
- en: This is an email about linguistics—specifically, about the parsing of a natural
    sentence into multiple **noun phrases** (**np**). This is a largely irrelevant
    fact to the project at hand. I do, however, think it's a good idea to go through
    the topics, if only to provide a sanity check on manual occasions.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一封关于语言学的电子邮件——具体来说，是关于将自然句子解析成多个**名词短语**（**np**）。这与当前的项目几乎没有关系。然而，我认为通过这些主题进行探讨是个好主意，至少在手动操作时可以提供一个理智的检查。
- en: There is an email and a person attached to this email—the dataset is not particularly
    anonymized. This has some implications in the future of machine learning, which
    I will explore in the final chapter of this book.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这封电子邮件和一个人相关联——数据集并没有特别进行匿名化。这将对机器学习的未来产生一些影响，我将在本书的最后一章中探讨。
- en: The email is very nicely split into fields (that is, space separated for each
    word).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件被非常巧妙地分割成字段（即每个单词之间用空格分隔）。
- en: The email has a `Subject` line.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件有一个`主题`行。
- en: The first two points are particularly noteworthy. Sometimes, the subject matter
    actually matters in machine learning. In our case, we can build our algorithms
    to be blind—they can be used generically across all emails. But there are times
    where being context-sensitive will bring new heights to your machine-learning
    algorithms. The second thing to note is anonymity. We live in an age where software
    flaws are often the downfall of companies. Doing machine learning on non-anonymous
    datasets are often fraught with biases. We should try to anonymize data as much
    as possible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前两点尤其值得关注。有时，在机器学习中，主题内容实际上很重要。在我们的案例中，我们可以构建我们的算法使其具有盲点——它们可以通用于所有电子邮件。但有时，具有上下文敏感性将使您的机器学习算法达到新的高度。需要注意的是，第二个问题是匿名性。我们生活在一个软件缺陷往往是公司倒闭的时代。在非匿名数据集上进行机器学习往往充满偏见。我们应该尽可能地对数据进行匿名化。
- en: Tokenization
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词元化
- en: When dealing with natural language sentences, the first activity is typically
    to tokenize the sentence. Given a sentence that reads such as `The child was learning
    a new word and was using it excessively. "Shan't!", she cried`. We need to split
    the sentence into the components that make up the sentence. We call each component
    a token, hence the name of the process is **tokenization**. Here's one possible
    tokenization method, in which we do a simple `strings.Split(a, " ")`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理自然语言句子时，通常的第一步活动是对句子进行分词。给定一个如下的句子：“The child was learning a new word and
    was using it excessively. "Shan't!", she cried”。我们需要将句子拆分成构成句子的各个部分。我们将每个部分称为一个标记，因此这个过程被称为**分词**。这里有一个可能的分词方法，我们使用简单的`strings.Split(a,
    " ")`。
- en: 'Here''s a simple program:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的程序：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is the output we will get:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将得到的结果：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now think about this in the context of adding words to a dictionary to learn.
    Let''s say we want to use the same set of English words to form a new sentence:
    `she shan''t be learning excessively.` (Forgive the poor implications in the sentence).
    We add it to our program, and see if it shows up in the dictionary:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑在向字典添加单词以学习的情况下。假设我们想要使用同一组英语单词来形成一个新的句子：“she shan't be learning excessively.”（请原谅句子中的不良含义）。我们将其添加到我们的程序中，看看它是否出现在字典中：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This leads to the following result:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A superior tokenization algorithm would yield a result as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个优秀的分词算法会产生如下结果：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A particular thing to note is that the symbols and punctuation are now tokens.
    Another particular thing to note is `shan''t` is now split into two tokens: `sha` and
    `n''t`. The word `shan''t` is a contraction of *shall* and *not*; therefore, it
    is tokenized into two words. This is a tokenization strategy that is unique to
    English. Another unique point of English is that words are separated by a boundary
    marker—the humble space. In languages where there are no word boundary markers,
    such as Chinese or Japanese, the process of tokenization becomes significantly
    more complicated. Add to that languages such as Vietnamese, where there are markers
    for boundaries of syllables, but not words, and you have a very complicated tokenizer
    at hand.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个特别需要注意的事项是，符号和标点现在被视为标记。另一个需要注意的事项是`shan't`现在被拆分为两个标记：`sha`和`n't`。单词`shan't`是*shall*和*not*的缩写；因此，它被分词成两个单词。这是一种独特的英语分词策略。英语的另一个独特之处在于单词之间由边界标记——谦逊的空格分隔。在像中文或日语这样的没有单词边界标记的语言中，分词过程变得显著更加复杂。再加上像越南语这样的语言，其中存在音节边界标记，但没有单词边界标记，你就有了一个非常复杂的分词器。
- en: The details of a good tokenization algorithm are fairly complicated, and tokenization
    is worthy of a book to itself, so we `shan't` cover it here.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的分词算法的细节相当复杂，分词本身值得一本书来专门讨论，所以我们在这里不涉及它。
- en: The best part about the `LingSpam` corpus is that the tokenization has already
    been done. Some notes such as compound words and contractions are not tokenized
    into different tokens such as the example of `shan't`. They are treated as a single
    word. For the purposes of a spam classifier, this is fine. However, when working
    with different types of NLP projects, the reader might want to consider better
    tokenization strategies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`LingSpam`语料库的最好之处在于分词已经完成。一些注释，如复合词和缩写，没有被分词成不同的标记，例如`shan''t`的例子。它们被视为一个单词。对于垃圾邮件分类器来说，这是可以的。然而，当处理不同类型的NLP项目时，读者可能需要考虑更好的分词策略。'
- en: 'Here is a final note about tokenization strategies: English is not a particularly
    regular language. Despite this, regular expressions are useful for small datasets.
    For this project, you may get away with the following regular expression:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分词策略的最后一项注意：英语不是一个特别规则的语言。尽管如此，正则表达式对于小数据集是有用的。对于这个项目，你可能可以用以下正则表达式来解决问题：
- en: '``const re = `([A-Z])(\.[A-Z])+\.?|\w+(-\w+)*|\$?\d+(\.\d+)?%?|\.\.\.|[][.,;"''?():-_`
    + "`]"``'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '``const re = `([A-Z])(\.[A-Z])+\.?|\w+(-\w+)*|\$?\d+(\.\d+)?%?|\.\.\.|[][.,;"''?():-_`
    + "`]"`'
- en: Normalizing and lemmatizing
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规范化和词形还原
- en: 'In the previous section, I wrote that all the words in the second example,
    `she shan''t be excessively learned`, are already in the dictionary from the first
    sentence. The observant reader might note the word `be` isn''t actually in the
    dictionary. From a linguistics point of view, that isn''t necessarily false. The
    word `be` is the root word of `is`, of which `was` is the past tense. Here, there
    is a notion that instead of just adding the words directly, we should add the
    root word. This is called **lemmatization**. Continuing from the previous example,
    the following are the lemmatized words from the first sentence:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我提到第二个例子中的所有单词，即“她不应该过分学习”，在第一句话的字典中已经存在。细心的读者可能会注意到单词“be”实际上不在字典中。从语言学的角度来看，这并不一定错误。单词“be”是“is”的词根，而“was”是其过去式。在这里，有一个观点认为，我们不应该只是直接添加单词，而应该添加词根。这被称为**词形还原**。继续上一个例子，以下是从第一句话中词形还原的单词：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Again, here I would like to point out some inconsistencies that will be immediately
    obvious to the observant reader. Specifically, the word `excessively` has the
    root word of `excess`. So why was `excessively` listed? Again, the task of lemmatization
    isn't exactly a straightforward lookup of the root word in a dictionary. Often,
    in complex NLP related tasks, the words have to be lemmatized according to the
    context they are in. That's beyond the scope of this chapter because, as before,
    it's a fairly involved topic that could span an entire chapter of a book on NLP
    preprocessing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我想指出一些观察者会立即注意到的不一致之处。特别是，单词“excessively”的词根是“excess”。那么为什么“excessively”被列出？再次，词形还原的任务并不完全是直接在字典中查找词根。在复杂的NLP相关任务中，单词通常需要根据它们所在的上下文进行词形还原。这超出了本章的范围，因为，就像之前一样，这是一个相当复杂的话题，可能需要整章的NLP预处理书籍来阐述。
- en: 'So, let''s go back to the topic of adding a word to a dictionary. Another useful
    thing to do is to normalize the words. In English, this typically means lowercasing
    the text, replacing unicode combination characters and the like. In the Go ecosystem,
    there is an extended standard library package that does just this: `golang.org/x/text/unicode/norm`.
    In particular, if we are going to work on real datasets, I personally prefer a
    NFC normalization schema. A good resource on string normalization is on the Go
    blog post as well: [https://blog.golang.org/normalization](https://blog.golang.org/normalization).
    The content is not specific to Go, and is a good guide to string normalization
    in general.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们回到向字典中添加单词的话题。另一件有用的事情是对单词进行规范化。在英语中，这通常意味着将文本转换为小写，替换Unicode组合字符等。在Go生态系统中有扩展的标准库包可以做到这一点：`golang.org/x/text/unicode/norm`。特别是，如果我们将要处理真实的数据集，我个人更喜欢NFC规范化方案。关于字符串规范化的好资源可以在Go博客文章中找到：[https://blog.golang.org/normalization](https://blog.golang.org/normalization)。内容并不仅限于Go，而且是一般字符串规范化的良好指南。
- en: The `LingSpam` corpus comes with variants that are normalized (by lowercasing
    and NFC) and lemmatized. They can be found in the `lemm` and `lemm_stop` variants
    of the corpus.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`LingSpam`语料库包含经过规范化（通过小写化和NFC）和词形还原的变体。它们可以在语料库的`lemm`和`lemm_stop`变体中找到。'
- en: Stopwords
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词
- en: By reading this, I would assume the reader is familiar with English. And you
    may have noticed that some words are used more often than others. Words such as `the`,
    `there`, `from`, and so on. The task of classifying whether an email is spam or
    ham is inherently statistical in nature. When certain words are used often in
    a document (such as an email), it conveys more weight about what that document
    is about. For example, I received an email today about cats (I am a patron of
    the Cat Protection Society). The word `cat` or `cats` occurred eleven times out
    of the 120 or so words. It would not be difficult to assume that the email is
    about cats.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读这篇文章，我会假设读者熟悉英语。你可能已经注意到，有些单词比其他单词使用得更频繁。例如，“the”、“there”、“from”等。判断一封电子邮件是垃圾邮件还是正常邮件的任务本质上是统计性的。当某些单词在文档（如电子邮件）中使用频率较高时，它更多地传达了该文档的主题。例如，我今天收到了一封关于猫的电子邮件（我是猫保护协会的赞助者）。单词“cat”或“cats”在约120个单词中出现了11次。可以假设这封电子邮件是关于猫的。
- en: However, the word `the` showed up 19 times. If we were to classify the topic
    of the email by a count of words, the email would be classified under the topic
    `the`. Connective words such as these are useful in understanding the specific
    context of the sentences, but for a Naïve statistical analysis, they often add
    nothing more than noise. So, we have to remove them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单词`the`出现了19次。如果我们通过单词计数来分类电子邮件的主题，电子邮件将被分类在主题`the`下。像这样的连接词对于理解句子的特定上下文是有用的，但在朴素统计分析中，它们通常只会增加噪音。因此，我们必须删除它们。
- en: 'Stopwords are often specific to projects, and I''m not a particular fan of
    removing them outright. However, the `LingSpam` corpus has two variants: `stop` and
    `lemm_stop`, which has the stopwords list applied, and the stopwords removed.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词通常针对特定项目，我并不是特别喜欢直接删除它们。然而，`LingSpam`语料库有两个变体：`stop`和`lemm_stop`，其中包含了停用词列表，以及移除了停用词。
- en: Ingesting the data
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: 'Now, without much further ado, let''s write some code to ingest the data. First,
    we need a data structure of a training example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，无需多言，让我们编写一些代码来导入数据。首先，我们需要一个训练示例的数据结构：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The reason for this is so that we can parse our files into a list of `Example`.
    The function is shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是为了将我们的文件解析成`Example`列表。函数如下所示：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, I used `filepath.Glob` to find a list of files that matches the pattern
    within the specific directory, which is hardcoded. It doesn''t have to be hardcoded
    in your actual code, but hardcoding the path makes for simpler demo programs.
    For each of the matching filenames, we parse the file using the `ingestOneFile` function.
    Then we check whether the filename contains `spmsg` as a prefix. If it does, we
    create an `Example` that has `Spam` as its class. Otherwise, it will be marked
    as `Ham`. In the later sections of this chapter, I will walk through the `Class` type
    and the rationale for choosing it. For now, here''s the `ingestOneFile` function.
    Take note of its simplicity:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用了`filepath.Glob`来找到特定目录内匹配模式的文件列表，这是硬编码的。在实际代码中，不一定需要硬编码路径，但硬编码路径可以使演示程序更简单。对于每个匹配的文件名，我们使用`ingestOneFile`函数解析文件。然后我们检查文件名是否以`spmsg`为前缀。如果是，我们创建一个具有`Spam`类的`Example`。否则，它将被标记为`Ham`。在本章的后续部分，我将介绍`Class`类型及其选择理由。现在，这里是`ingestOneFile`函数。请注意其简单性：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Handling errors
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理错误
- en: There is a central thesis in some programming language theories that errors
    in most programs happen at the boundary. While there are many interpretations
    of this thesis (boundaries of what? Some scholars think it's at the boundaries
    of functions; some think it's at the boundaries of computation), what is certainly
    true from experience is that boundaries of I/O are where the most errors happen.
    Hence, we have to be extra careful when dealing with input and output.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些编程语言理论中有一个中心论点，即大多数程序中的错误发生在边界。虽然对这个论点有许多解释（边界的什么？一些学者认为是在函数的边界；一些认为是在计算的边界），但从经验来看，确实是真的，I/O的边界是最容易发生错误的地方。因此，我们在处理输入和输出时必须格外小心。
- en: 'For the purposes of ingesting the files, we define an `errList` type as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了导入文件，我们定义了一个`errList`类型，如下所示：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That way we can continue, even if an error happens while reading a file. The
    error will be bubbled back all the way to the top without causing any panic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以继续进行，即使读取文件时发生错误。错误会一直冒泡到顶部，而不会引起任何恐慌。
- en: The classifier
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器
- en: 'Before we continue to build our classifier, let''s imagine what the main function
    will look as follows. It will look something similar to this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续构建我们的分类器之前，让我们想象一下主函数将如下所示。它看起来将与以下类似：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The use of `Train` and `Predict` as exported methods are useful in guiding
    us on what to build next. From the sketch in the preceding code block, we need
    a `Classifier` type, that has `Train` and `Predict` at the very least. So we''ll
    start by doing that:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Train`和`Predict`作为导出方法是有用的，它可以指导我们接下来要构建什么。从前面的代码块中的草图来看，我们需要一个`Classifier`类型，它至少有`Train`和`Predict`。所以我们将从这里开始：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, now, it becomes a question of how the classifier works.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这变成了一个关于分类器如何工作的问题。
- en: Naive Bayes
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单贝叶斯
- en: 'The classifier is a Naive Bayes classifier. To break it down, Naive in the
    phrase Naive Bayes means that we are assuming that all the input features are
    independent. To understand how the classifier works, an additional component needs
    to be introduced first: the **term frequency**-**inverse frequency** (**TF**-**IF**)
    pair of statistics.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器是一个朴素贝叶斯分类器。为了解释清楚，朴素贝叶斯中的“朴素”意味着我们假设所有输入特征都是独立的。为了理解分类器的工作原理，首先需要引入一个额外的组件：**词频**-**逆文档频率**（**TF**-**IDF**）的统计对。
- en: TF-IDF
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: 'TF-IDF, per its namesake, is comprised of two statistics: **term frequency**
    (**TF**) and **inverse document frequency** (**IDF**).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF，正如其名称所示，由两个统计量组成：**词频**（**TF**）和**逆文档频率**（**IDF**）。
- en: The central thesis to TF is that if a word (called a **term**) occurs many times
    in a document, it means that the document revolves more around that word. It makes
    sense; look at your emails. The keywords typically revolve around a central topic.
    But TF is a lot more simplistic than that. There is no notion of topics. It's
    just a count of how many times a word happens in a document.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TF的中心论点是，如果一个词（称为**术语**）在文档中多次出现，这意味着文档更多地围绕这个词展开。这是有道理的；看看你的电子邮件。关键词通常围绕一个中心主题。但TF比这简单得多。没有主题的概念。它只是计算一个词在文档中出现的次数。
- en: 'IDF, on the other hand, is a statistic that determines how important a term
    is to a document. In the examples we''ve seen, do note that the word `Subject`,
    with a capital `S` occurs once in both types of documents: spam and ham. In broad
    strokes, IDF is calculated by the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，IDF是一种统计量，用于确定一个术语对文档的重要性。在我们看到的例子中，请注意，带有大写`S`的单词`Subject`在两种类型的文档中（垃圾邮件和正常邮件）都只出现了一次。总的来说，IDF的计算方法如下：
- en: '![](img/ef3186a5-8b35-4ae4-9d6c-169b821834dc.png).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/ef3186a5-8b35-4ae4-9d6c-169b821834dc.png).'
- en: The exact formula varies and there are subtleties to each variation, but all
    adhere to the notion of dividing the total number of documents over the frequency
    of the term.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的公式各不相同，每种变化都有细微差别，但所有公式都遵循将文档总数除以术语频率的概念。
- en: 'For the purposes of our project, we will be using the `tf-idf` library from
    `go-nlp`, which is a repository of NLP-related libraries for Go. To install it,
    simply run the following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，我们将使用来自`go-nlp`的`tf-idf`库，这是一个Go语言的NLP相关库的存储库。要安装它，只需运行以下命令：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It is an extremely well, tested library, with 100% test coverage.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个经过极好测试的库，测试覆盖率达到了100%。
- en: When used together, ![](img/45393d95-43fe-4cf0-874e-c4c23c09f1ea.png) represents
    a useful weighting scheme for calculating the importance of a word in a document.
    It may seem simple, but it is very powerful, especially when used in the context
    of probability.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当一起使用时，![](img/45393d95-43fe-4cf0-874e-c4c23c09f1ea.png)代表了一种用于计算文档中词重要性的有用加权方案。它看起来很简单，但非常强大，尤其是在概率的背景下使用时。
- en: Do note that TF-IDF cannot strictly be interpreted as a probability. There are
    some theoretical nastiness that presents itself when strictly interpreting IDF
    as a probability. Hence, in the context of this project, we will be treating TF-IDF
    as a sort of weighting scheme to a probability.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TF-IDF不能严格地解释为概率。当严格将IDF解释为概率时，会出现一些理论上的问题。因此，在本项目的背景下，我们将把TF-IDF视为一种概率的加权方案。
- en: Now we are ready to talk about the basics of the Naive Bayes algorithm. But
    first I'd like to further emphasize certain intuitions of Bayes' theorem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好讨论朴素贝叶斯算法的基本原理。但首先我想进一步强调贝叶斯定理的一些直觉。
- en: Conditional probability
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率
- en: 'We''ll start with the notion of conditional probability. To set a scene, we''ll
    consider several fruit types:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从条件概率的概念开始。为了设定场景，我们将考虑几种水果类型：
- en: Apple
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苹果
- en: Avocado
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鳄梨
- en: Banana
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 香蕉
- en: Pineapple
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 菠萝
- en: Nectarine
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 油桃
- en: Mango
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 芒果
- en: Strawberry
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 草莓
- en: 'For each fruit type, we will have several instances of those fruits—so we could
    have a green Granny Smith and a red Red Delicious in the class of apples. Likewise,
    we could have ripe and unripe fruits—mangoes and bananas could be yellow (ripe)
    or green (unripe), for example. Lastly, we can also classify these fruits by what
    kind of fruit it is—tropical (avocado, banana, pineapple, and mango) versus non-tropical
    fruits:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种水果类型，我们将有这些水果的几个实例——例如，我们可以在苹果类别中有绿色的Granny Smith和红色的Red Delicious。同样，我们也可以有成熟和未成熟的水果——例如，芒果和香蕉可以是黄色的（成熟）或绿色的（未成熟）。最后，我们还可以根据水果的种类对这些水果进行分类——热带水果（鳄梨、香蕉、菠萝和芒果）与非热带水果：
- en: '| **Fruit** | **Can be green** | **Can be yellow** | **Can be red** | **Is
    tropical** |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| **水果** | **可以是绿色的** | **可以是黄色的** | **可以是红色的** | **是热带水果** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Apple** | yes | no | yes | no |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **苹果** | 是 | 否 | 是 | 否 |'
- en: '| **Avocado** | yes | no | no | yes |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| **鳄梨** | 是 | 否 | 否 | 是 |'
- en: '| **Banana** | yes | yes | no | yes |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **香蕉** | 是 | 是 | 否 | 是 |'
- en: '| **Lychee** | yes | no | yes | yes |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **荔枝** | 是 | 否 | 是 | 是 |'
- en: '| **Mango** | yes | yes | no | yes |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **芒果** | 是 | 是 | 否 | 是 |'
- en: '| **Nectarine** | no | yes | yes | no |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **油桃** | 否 | 是 | 是 | 否 |'
- en: '| **Pineapple** | yes | yes | no | yes |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **菠萝** | 是 | 是 | 否 | 是 |'
- en: '| **Strawberry** | yes | no | yes | no |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **草莓** | 是 | 否 | 是 | 否 |'
- en: I would like you to now imagine you're blindfolded and you pick a fruit. I will
    then describe a feature of the fruit, and you would guess the fruit.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我想让你现在想象你被蒙上了眼睛，你随机摘了一个水果。然后我会描述这个水果的一个特征，你将猜测这个水果是什么。
- en: Let's say the fruit you picked has a yellow outside. What are the possible fruits?
    Nectarines, bananas, pineapples, and mangoes come to mind. If you pick one of
    the options you would have a one in four chance of being correct. We call this
    the probability of yellow ![](img/dce6e753-bfdf-40ed-9684-1a0af16d0bad.png). The
    numerator is the number of yeses along the `Can be yellow` column, and the denominator
    is the total number of rows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你摘的水果外面是黄色的。可能的水果有哪些？桃子、香蕉、菠萝和芒果都会出现在你的脑海中。如果你选择其中一个选项，你有四分之一的正确机会。我们称之为黄色的概率！[](img/dce6e753-bfdf-40ed-9684-1a0af16d0bad.png)。分子是“可以是黄色的”这一列中的“是”的数量，分母是总行数。
- en: If I give you another feature about the fruit, you can improve your odds. Let's
    say I tell you that the fruit is tropical. Now you have a one in three chance of
    being right—nectarines has been eliminated from the possible choices.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我给你另一个关于水果的特征，你可以提高你的胜算。比如说，我告诉你这个水果是热带的。现在你有三分之一的正确机会——油桃已经被排除在可能的选择之外了。
- en: 'We can ask this question: If we know a fruit is tropical, what is the probability
    that the fruit is yellow? The answer is 3/5\. From the preceding table, we can
    see that there are five tropical fruits and three of them are yellow. This is
    called a **conditional probability**. We write it in a formula such as this (for
    the more mathematically inclined, this is the Kolmogorov definition of conditional
    probability):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出这样的问题：如果我们知道一个水果是热带水果，那么这个水果是黄色的概率是多少？答案是3/5。从先前的表格中，我们可以看到有五种热带水果，其中三种是黄色的。这被称为**条件概率**。我们用公式来表示它（对于更倾向于数学的人来说，这是柯尔莫哥洛夫条件概率的定义）：
- en: '![](img/e921c7f2-f074-4779-b639-d00531b48477.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e921c7f2-f074-4779-b639-d00531b48477.png)'
- en: 'This is how you read the formula: the probability of *A* given *B* is known,
    and we will need to get the probability of *A AND B* happening at the same time
    and the probability of *B* itself.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这样读取公式：已知A在B的条件下发生的概率，我们需要得到A和B同时发生的概率以及B本身发生的概率。
- en: The conditional probability of a fruit being yellow, given that it's tropical
    is three in five; there are actually a lot of tropical fruits that are yellow—tropical
    conditions allow for greater depositions of carotinoids and vitamin C during the
    growth of the fruit.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在已知水果是热带水果的条件下，水果是黄色的条件概率是五分之三；实际上有很多热带水果是黄色的——热带条件允许在水果生长过程中有更多的类胡萝卜素和维生素C的沉积。
- en: 'Looking at a tabulated result can yield an easier understanding of conditional
    probability. However, it must be noted that the conditional probability *can*
    be calculated. Specifically, to calculate the conditional probability, this is
    the formula:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看表格结果可以更容易地理解条件概率。然而，必须注意的是，条件概率**可以**被计算。具体来说，要计算条件概率，这是公式：
- en: '![](img/5d0ea89d-9e2a-49d1-a27c-95425ba0b4b5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d0ea89d-9e2a-49d1-a27c-95425ba0b4b5.png)'
- en: The probability of a fruit being yellow *and* tropical (![](img/2ca0214e-2b30-4f5f-890d-f3d377d1c099.png) )
    is three in eight; there are three such fruits, out of a total of eight. The probability
    of a fruit being tropical (![](img/d379de41-c16a-4b6b-97de-6fa1633949e1.png))
    is five in eight; there are five topical fruits out of the eight listed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 水果既是黄色又是热带的概率是八分之三；在总共八种水果中，有三种是这样的水果。水果是热带的概率是八分之五；在列出的八种水果中，有五种是热带水果。
- en: 'And now, we are finally ready to figure out how we got to that one in three number.
    The probability of each class of fruits is uniform. If you had to choose randomly,
    you would get it right one in eight of the time. We can rephrase the question
    to this: What is the probability of a fruit being a banana given that it''s yellow
    and tropical?'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于准备好弄清楚我们是如何得到那个三分之一数字的。水果类别的概率是均匀的。如果你随机选择，你会在八分之一的时间里选对。我们可以将问题重新表述为：如果一个水果是黄色的并且是热带的，那么它是香蕉的概率是多少？
- en: 'Let''s rewrite this as a formula:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其重写为一个公式：
- en: '![](img/d03e4a8c-f03d-464a-971a-5b17c15c517d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d03e4a8c-f03d-464a-971a-5b17c15c517d.png)'
- en: 'It is important that we relied on a special trick to perform the analysis of
    the preceding probabilities. Specifically, we acted as though each *yes* represents
    a singular example existing, while a *no* indicates that there are no examples,
    or, in short, this table:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们依赖于一个特殊的技巧来分析前面的概率。具体来说，我们假设每个“是”代表一个存在的单一示例，而“否”表示没有示例，简而言之，这个表格：
- en: '| **Fruit** | **Is Green** | **Is Yellow** | **Is Red** | **Is Tropical** |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **水果** | **是绿色** | **是黄色** | **是红色** | **是热带** |'
- en: '| **Apple** | 1 | 0 | 1 | 0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **苹果** | 1 | 0 | 1 | 0 |'
- en: '| **Avocado** | 1 | 0 | 0 | 1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| **鳄梨** | 1 | 0 | 0 | 1 |'
- en: '| **Banana** | 1 | 1 | 0 | 1 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **香蕉** | 1 | 1 | 0 | 1 |'
- en: '| **Lychee ** | 1 | 0 | 1 | 1 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **荔枝** | 1 | 0 | 1 | 1 |'
- en: '| **Mango** | 1 | 1 | 0 | 1 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **芒果** | 1 | 1 | 0 | 1 |'
- en: '| **Nectarine** | 0 | 1 | 1 | 0 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **桃子** | 0 | 1 | 1 | 0 |'
- en: '| **Pineapple** | 1 | 1 | 0 | 1 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **菠萝** | 1 | 1 | 0 | 1 |'
- en: '| **Strawberry** | 1 | 0 | 1 | 0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **草莓** | 1 | 0 | 1 | 0 |'
- en: This will be important for analysis for the spam detection project. The numbers
    in each would be the number of occurrences within the dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于垃圾邮件检测项目的分析非常重要。每个数字代表在数据集中出现的次数。
- en: Features
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征
- en: 'We''ve seen from the previous examples, that we need features, such as whether
    a fruit can be green, yellow, or red, or whether it''s tropical. We''re now focused
    on the project at hand. What should the features be?:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的例子中，我们已经看到，我们需要特征，比如水果是否可以是绿色、黄色或红色，或者它是否是热带的。我们现在专注于手头的项目。特征应该是什么？：
- en: '| **Class** | **???** | **???** | **???** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **???** | **???** | **???** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Spam** |  |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **垃圾邮件** |  |  |  |'
- en: '| **Ham** |  |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **火腿** |  |  |  |'
- en: What makes up an email? Words make an email. So, it would be appropriate to
    consider the appearance of each word feature. We can take it further, and take
    the intuition that we have developed previously with TF-IDF and instead use the
    frequency of the words among the document types. Instead of counting 1 for the
    existence, we count the total number of times a word exists in the document types.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一封电子邮件由什么组成？单词组成电子邮件。因此，考虑每个单词特征的出现情况是合适的。我们可以更进一步，利用我们之前开发的TF-IDF直觉，而不是简单地计数1表示存在，而是计数单词在文档类型中出现的总次数。
- en: 'The table would look something as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表格看起来可能如下所示：
- en: '| **Class** | **Has XXX** | **Has Site** | **Has Free** | **Has Linguistics**
    | **...** |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **有XXX** | **有站点** | **有免费** | **有语言学** | **...** |'
- en: '| **Spam** | 200 | 189 | 70 | 2 | ... |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **垃圾邮件** | 200 | 189 | 70 | 2 | ... |'
- en: '| **Ham** | 1 | 2 | 55 | 120 | ... |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **火腿** | 1 | 2 | 55 | 120 | ... |'
- en: That also means that there are many features. We can certainly try to enumerate
    all possible calculations. But doing so would be tedious and quite computationally
    intensive. Instead, we can try to be clever about it. Specifically, we will use
    another definition of conditional probability to do the trick to reduce the amount
    of computations that needs to be done.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着有很多特征。我们当然可以尝试列举所有可能的计算。但这样做会很繁琐，而且计算量很大。相反，我们可以尝试做得更聪明一些。具体来说，我们将使用条件概率的另一个定义来巧妙地减少需要进行的计算量。
- en: Bayes' theorem
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: 'A conditional probability formula can also be written as Bayes'' theorem:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率公式也可以写成贝叶斯定理：
- en: '![](img/a0a5ddfe-b14b-4efa-8c8b-e41b9e0c7dc6.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0a5ddfe-b14b-4efa-8c8b-e41b9e0c7dc6.png)'
- en: We call ![](img/163c55c0-c926-4f98-ac7b-3d89861ee3d7.png) the prior probability. ![](img/b453362e-b08e-403e-a7d9-d12feb0a7520.png) is
    called the **likelihood**. These are the things we're interested in, as ![](img/6a6f5b33-fc93-4124-887a-b1284152abf6.png) is
    essentially a constant anyway.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称![](img/163c55c0-c926-4f98-ac7b-3d89861ee3d7.png)为**先验概率**。![](img/b453362e-b08e-403e-a7d9-d12feb0a7520.png)被称为**似然**。这些是我们感兴趣的东西，因为![](img/6a6f5b33-fc93-4124-887a-b1284152abf6.png)本质上是一个常数。
- en: The theory at this point is a little dry. How does this relate to our project?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的理论有点枯燥。这与我们的项目有什么关系？
- en: 'For one, we can rewrite the generic Bayes'' theorem to one that fits our project:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以将通用的贝叶斯定理重写为适合我们项目的形式：
- en: '![](img/3841b295-8be7-4370-9e26-37628bba8f67.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3841b295-8be7-4370-9e26-37628bba8f67.png)'
- en: This formula perfectly encapsulates our project; given a document made up of
    words, what is the probability that it's `Ham` or `Spam`? In the next section,
    I will show you how to translate this formula into a very powerful classifier,
    in fewer than 100 lines of code.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式完美地封装了我们的项目；给定一个由单词组成的文档，它是`Ham`或`Spam`的概率是多少？在下一节中，我将向您展示如何将这个公式翻译成一个功能强大的分类器，代码行数少于100行。
- en: Implementating the classifier
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现分类器
- en: 'In the earlier parts of the chapter, we sketched out a dummy `Classifier` type
    that does nothing. Let''s make it do something now:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们草拟了一个什么也不做的`Classifier`类型。现在让我们让它做些事情：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, there are introductions to a few things. Let''s walk them through one
    by one:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍了一些内容。让我们逐一了解：
- en: We'll start with the `corpus.Corpus` type.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从`corpus.Corpus`类型开始。
- en: This is a type imported from the `corpus package`, which is a subpackage of
    the NLP library for Go, `lingo`.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个从`corpus`包导入的类型，它是Go语言NLP库`lingo`的子包。
- en: To install `lingo`, simply run `go get -u github.com/chewxy/lingo/...`.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要安装`lingo`，只需运行`go get -u github.com/chewxy/lingo/...`.
- en: 'To use the `corpus `package, simply import it like so: `import "github.com/chewxy/lingo/corpus"`.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用`corpus`包，只需像这样导入：`import "github.com/chewxy/lingo/corpus"`.
- en: Bear in mind that in the near future, the package will change to `github.com/go-nlp/lingo`.
    If you are reading this after January 2019, use the new address.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在不久的将来，这个包将更改为`github.com/go-nlp/lingo`。如果你在2019年1月之后阅读这篇文章，请使用新的地址。
- en: 'A `corpus.Corpus` object simply maps from a word to an integer. The reason
    for doing this is twofold:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`corpus.Corpus`对象简单地将单词映射到整数。这样做的原因有两个：'
- en: '**It saves on memory**: A `[]int` uses considerably less memory than `[]string`.
    Once a corpus has been converted to be IDs, the memory for the strings can be
    freed. The purpose of this is to provide an alternative to string interning.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它节省了内存**：`[]int`比`[]string`使用更少的内存。一旦语料库被转换为ID，字符串的内存就可以释放。这样做是为了提供一个字符串池化的替代方案。'
- en: '**String interning is fickle**: String interning is a procedure where for the
    entire program''s memory, only exactly one copy of the string exists. This turns
    out to be harder than expected for most tasks. Integers provide a more stable
    interning procedure.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字符串池化是变幻莫测的**：字符串池化是一种程序，在整个程序的内存中，只有一个字符串的副本。这比预期的要难得多。整数提供了一个更稳定的池化程序。'
- en: Next, we are faced with two fields which are arrays. Specifically, `tfidfs [MAXCLASS]*tfidf.TFIDF` and
    `totals [MAXCLASS]float64`. At this point, it might be a good idea to talk about
    the `Class` type.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们面临两个字段，它们是数组。具体来说，`tfidfs [MAXCLASS]*tfidf.TFIDF`和`totals [MAXCLASS]float64`。在这个时候，讨论`Class`类型可能是个好主意。
- en: Class
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别
- en: 'We were introduced to the `Class` type when we were writing the ingestion code.
    This is the definition of `Class`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们编写摄入代码时，我们接触到了`Class`类型。这是`Class`的定义：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In other words, `Ham` is `0`, `Spam` is `1`, and `MAXCLASS` is `2`. They're
    all constant values and can't be changed at runtime.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`Ham`是`0`，`Spam`是`1`，`MAXCLASS`是`2`。它们都是常量值，不能在运行时更改。
- en: 'It would be prudent to note upfront, that there are limitations to this approach.
    In particular, it means that you have to know before running the program how many
    classes there will be. In our case, we know that there will be at most two classes:
    `Spam` or `Ham`. If we know there is a third class, say `Prosciutto`, for example,
    then we can code it as a value before `MAXCLASS`. There are many reasons for using
    a constant numerical value typed as a `Class`. Two of the primary reasons would
    be correctness and performance.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要谨慎注意的是，这种方法有一些局限性。特别是，这意味着在运行程序之前，你必须知道将有多少个类别。在我们的例子中，我们知道最多会有两个类别：`Spam`或`Ham`。如果我们知道有第三个类别，比如`Prosciutto`，那么我们可以在`MAXCLASS`之前将其编码为一个值。使用常量数值类型的`Class`有许多原因。其中两个主要原因是正确性和性能。
- en: 'Imagine we have a function that takes `Class` as an input:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个以`Class`作为输入的函数：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Someone who uses this function outside this library may pass in `3` as the
    class: `ExportedFn(Class(3))`. We can instantly tell if the value is valid if
    we have a validation function that looks something as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个函数的人可能会传入`3`作为类别：`ExportedFn(Class(3))`。如果我们有一个如下所示的有效性验证函数，我们可以立即判断值是否有效：
- en: '[PRE17]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Granted, this is not as nice as other languages, such as Haskell, where you
    could just do this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不如 Haskell 等其他语言那样优雅，在 Haskell 中你可以直接这样做：
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And let the compiler check for you if that is at the call site, whether the
    value passed in was valid or not. We still want the correctness, so we defer the
    checks to the runtime. `ExportedFn` now reads as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 并让编译器检查在调用点传入的值是否有效。我们仍然想要保证正确性，所以我们将检查推迟到运行时。`ExportedFn` 现在的读取方式如下：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The notion of data types with ranges of valid value is not a revolutionary notion.
    Ada for example, has bounded ranges since the 1990s. And the best part about using
    a constant value as a range with `MAXCLASS` is that we can fake the range checks
    and do them at runtime. In this respect, Go is more or less the same as Python,
    Java, or other unsafe languages. Where this truly shines however, is in performance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型具有有效值范围的观念并不是一个革命性的观念。例如，Ada 自 1990 年代以来就有有界范围。而使用常量值作为 `MAXCLASS` 的范围的好处是，我们可以伪造范围检查并在运行时执行它们。在这方面，Go
    大概与 Python、Java 或其他不安全语言相似。然而，真正闪耀的地方在于性能。
- en: A tip for good software engineering practice is to make your program as knowable
    by the human as possible without sacrificing understanding or neatness. Using
    constant numerical values (or enums) generally allows the human programmer to
    understand the constrains that the value is allowed to have. Having constant string
    values, as we will see in the next section, exposes the programmer to unconstrained
    values. This is where bugs usually happen.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程实践的一个小贴士是，在不牺牲理解或整洁性的情况下，尽可能让程序对人类来说是可知的。使用常量数值（或枚举）通常允许人类程序员理解值可以拥有的约束。在下一节中，我们将看到，使用常量字符串值会让程序员面临无约束的值。这就是通常发生错误的地方。
- en: Note that in the `Classifier` struct, both `tfidfs` and `totals` are arrays.
    Unlike slices, arrays in Go do not require an extra layer of indirection when
    accessing values. This makes things a tiny bit faster. But in order to truly understand
    the tradeoffs of this design, we need to look at alternative designs for `Class` and
    with them the alternative designs of the fields, `tfidfs` and `totals`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 `Classifier` 结构体中，`tfidfs` 和 `totals` 都是数组。与切片不同，在 Go 中访问值时，数组不需要额外的间接层。这使得事情变得稍微快一点。但为了真正理解这种设计的权衡，我们需要查看
    `Class` 的替代设计以及相应的 `tfidfs` 和 `totals` 字段的替代设计。
- en: Alternative class design
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代类设计
- en: 'Here, we imagine an alternative design of `Class`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们想象 `Class` 的一个替代设计：
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With this change, we will have to update the definition of `Classifier`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个变化，我们将不得不更新 `Classifier` 的定义：
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Consider now the steps required to get the totals of class `Ham`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑获取 `Ham` 类总量的步骤：
- en: The string has to be hashed
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 字符串必须进行哈希处理
- en: The hash will be used to look up the bucket where the data for `totals` is stored
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将使用哈希查找存储 `totals` 数据的桶
- en: An indirection is made to the bucket and the data is retrieved and returned
    to the user
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对桶进行间接引用，并检索数据返回给用户
- en: 'Consider now the steps required to get the totals of class `Ham` if the class
    design was the original:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类设计是原始的，现在考虑获取 `Ham` 类总量的步骤：
- en: Since `Ham` is a number, we can directly compute the location of the data for
    retrieval and return to the user.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 `Ham` 是一个数字，我们可以直接计算用于检索和返回给用户的 数据位置。
- en: By using a constant value and a numeric definition of the type `Class`, and
    an array type for `totals`, we are able to skip two steps. This yields very slight
    performance improvements. In this project, they're mostly negligible, until your
    data gets to a certain size.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用一个常量值和类型 `Class` 的数值定义，以及 `totals` 的数组类型，我们能够跳过两个步骤。这带来非常微小的性能提升。在这个项目中，直到数据达到一定规模，这些提升大多是可以忽略不计的。
- en: The aim of this section on the `Class` design is to instill a sense of mechanical
    sympathy. If you understand how the machine works, you can design very fast machine
    learning algorithms.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本节关于 `Class` 设计的目的是培养一种机械同情感。如果你理解机器是如何工作的，你就可以设计非常快的机器学习算法。
- en: All this said and done, there is one assumption that underpins this entire exercise.
    This is a `main` package. If you're designing a package that will be reused on
    different datasets, the tradeoff considerations are significantly different. In
    the context of software engineering, overgeneralizing your package often leads
    to leaky abstractions that are hard to debug. Better to write slightly more concrete
    and specific data structures that are purpose built.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，有一个假设支撑着整个练习。这是一个`main`包。如果你正在设计一个将在不同数据集上重用的包，权衡考虑会有很大不同。在软件工程的背景下，过度泛化你的包往往会导致难以调试的抽象。最好是编写稍微更具体和特定的数据结构，这些结构是专门为特定目的构建的。
- en: Classifier part II
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器第二部分
- en: One of the main considerations is that a Naive Bayes classifier is a very simple
    program, and very difficult to get wrong. The entire program is in fact fewer
    than 100 lines. Let's look at it further.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 主要考虑之一是朴素贝叶斯分类器是一个非常简单的程序，而且很难出错。整个程序实际上少于100行。让我们进一步看看它。
- en: 'We have sketched out so far the method `Train`, which will train the classifier
    on a given set of inputs. Here''s how it looks:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经概述了`Train`方法，该方法将在给定的一组输入上训练分类器。以下是它的样子：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: So here it's very clear that `Train` is an ![](img/8becaf0d-f9e5-4b14-a6a7-dba5530d07d9.png) operation.
    But the function is structured in such a way that it would be trivial to parallelize
    the calls to `c.trainOne`. Within the context of this project, this wasn't necessary
    because the program was able to complete in under a second. However, if you are
    adapting this program for larger and more varied datasets, it may be instructive
    to parallelize the calls. The `Classifier` and `tfidf.TFIDF` structs have mutexes
    in them to allow for these sorts of extensions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里非常清楚，`Train`是一个![](img/8becaf0d-f9e5-4b14-a6a7-dba5530d07d9.png)操作。但是函数的结构使得并行调用`c.trainOne`变得非常简单。在这个项目的背景下，这并不是必要的，因为程序能在不到一秒内完成。然而，如果你正在为更大、更多样化的数据集修改这个程序，并行化调用可能会有所帮助。《Classifier》和`tfidf.TFIDF`结构中包含互斥锁，以允许这类扩展。
- en: 'But what''s more interesting is the `trainOne` example. Looking at it, all
    it seems to do is to add each word to the corpus, get its ID, and then add the
    ID to the `doc` type. `doc`, incidentally, is defined as such:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 但更有趣的是`trainOne`示例。看看它，它似乎只是将每个单词添加到语料库中，获取其ID，然后将ID添加到`doc`类型中。顺便说一句，`doc`被定义为如下：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This definition is done to fit into the interface that `tfidf.TFIDF.Add` accepts.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义是为了适应`tfidf.TFIDF.Add`接口所接受的格式。
- en: Let's look closer at the `trainOne` method. After making the `doc`, the words
    from the example are added to the corpus, while the IDs are then put into the
    `doc`. The `doc` is then added to the `tfidf.TFIDF` of the relevant class.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看`trainOne`方法。在创建`doc`之后，示例中的单词被添加到语料库中，而ID随后被放入`doc`中。然后，`doc`被添加到相关类的`tfidf.TFIDF`中。
- en: At first glance, there isn't much training here; we're just adding to the TF
    statistic.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这里似乎没有太多训练过程；我们只是在增加TF统计量。
- en: The real magic happens in the `Predict` and `Score `methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的魔法发生在`Predict`和`Score`方法中。
- en: '`Score` is defined as such:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`Score`被定义为如下：'
- en: '[PRE24]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Given a tokenized sentence, we want to return the `scores` of each class. The
    idea is so that we can then look through the `scores` and find the class with
    the highest score:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个分词句子，我们希望返回每个类的`scores`。这样我们就可以查看`scores`并找到得分最高的类：
- en: '[PRE25]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `Score` function is worth a deeper look because that's where all the magic
    happens. First, we check the classifier is ready to score. An online machine learning
    system learns as new data comes in. This design means that the classifier cannot
    be used in an online fashion. All the training needs to be done up front. Once
    that training is done, the classifier will be locked, and won't train any further.
    Any new data will have to be part of a different run.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`Score`函数值得深入探讨，因为所有的魔法都发生在这里。首先，我们检查分类器是否准备好评分。在线机器学习系统随着新数据的到来而学习。这种设计意味着分类器不能以在线方式使用。所有训练都需要在前面完成。一旦完成训练，分类器将被锁定，不再进行训练。任何新的数据都必须是不同运行的一部分。'
- en: 'The `Postprocess` method is quite simple. Having recorded all the TF statistics,
    we now want to calculate the relative importance of each term to the documents.
    The `tfidf` package comes with a simple `Log`-based calculation of the IDF, but
    you can use any other IDF calculating function, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`后处理`方法相当简单。记录了所有TF统计信息后，我们现在想要计算每个术语相对于文档的相对重要性。`tfidf`包附带了一个基于`Log`的IDF简单计算，但你也可以使用任何其他IDF计算函数，如下所示：'
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It is important to note that there is an update to the document count of each
    class: `t.Docs = docs` to the sum of all the documents seen. This was because
    as we were adding to the term frequency of each class, the `tfidf.TFIDF` struct
    wouldn''t be aware of documents in other classes.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，每个类别的文档计数都有更新：`t.Docs = docs`到所有已看到的文档的总和。这是因为当我们向每个类别的词频添加时，`tfidf.TFIDF`结构不会意识到其他类别的文档。
- en: The reason we would want to calculate the IDF is to control the values a bit
    more.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要计算IDF的原因是为了更好地控制这些值。
- en: 'Recall that the conditional probability can be written in the Bayes'' theorem
    form:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，条件概率可以写成贝叶斯定理的形式：
- en: '![](img/badb5fed-45e2-4754-9e26-b828ab836786.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/badb5fed-45e2-4754-9e26-b828ab836786.png)'
- en: 'Let''s familiarize ourselves with the formula, once again by restating it in
    English, first by familiarizing ourselves with the terms:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次熟悉一下公式，通过用英语重述它，首先熟悉一下术语：
- en: '![](img/cd6757b2-66c0-4bb3-aef7-7d932918498b.png): This is the **prior probability** of
    a class. If we have a pool of email messages and we randomly pick one out, what
    is the probability that the email is `Ham` or `Spam`? This largely corresponds
    to the dataset that we have. From the exploratory analysis, we know that the ratio
    between `Ham` and `Spam` is around 80:20.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/cd6757b2-66c0-4bb3-aef7-7d932918498b.png): 这是指一个类的**先验概率**。如果我们有一批电子邮件消息，并且随机挑选一个，那么这封电子邮件是`Ham`或`Spam`的概率是多少？这很大程度上对应于我们拥有的数据集。从探索性分析中，我们知道`Ham`和`Spam`的比例大约是80:20。'
- en: '![](img/75331005-fca9-4cb6-aa00-a1f76789cbb0.png): This is the **likelihood** of
    any random document belongs to a class. Because a document is comprised of individual
    words, we simply make a Naïve assumption that these words are independent of one
    another. So we want the probability of ![](img/14e84a1b-e473-4149-bcfa-66b3ec599b4c.png).
    Assuming the words are independent gives us the ability to simply multiply the
    probabilities.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/75331005-fca9-4cb6-aa00-a1f76789cbb0.png): 这是指任何随机文档属于某一类的**可能性**。因为一个文档由单个单词组成，我们简单地做出一个朴素假设，即这些单词之间是相互独立的。因此，我们想要计算![图片](img/14e84a1b-e473-4149-bcfa-66b3ec599b4c.png)的概率。假设单词是独立的，这使我们能够简单地乘以概率。'
- en: 'So, to put it in English:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用英语来说：
- en: The conditional probability of a class being Ham given a document is the result
    of multiplying the prior probability of a document being ham and the likelihood
    that the document is Ham.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文档，该文档属于Ham类的条件概率是文档属于Ham的先验概率与文档是Ham的似然性的乘积。
- en: The observant reader may note that I have elided explanation of ![](img/9001d080-13de-44e8-87e2-29688dc8aca6.png).
    The reason is simple. Consider what the probability of the document is. It's simply
    the multiplication of all the probabilities of a word in the corpus. It doesn't
    in anyway interact with the `Class`. It could well be a constant.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我没有解释![图片](img/9001d080-13de-44e8-87e2-29688dc8aca6.png)的原因很简单。考虑文档的概率。它只是语料库中所有单词概率的乘积。它根本不与`Class`交互。它可能是一个常数。
- en: Furthermore, we run into another problem if we do use probabilities multiplied.
    Multiplying probabilities tend to yield smaller and smaller numbers. Computers
    do not have true rational numbers. `float64` is a neat trick to mask the fundamental
    limitations that a computer has. You will frequently run into edge cases where
    the numbers become too small or too big when working on machine learning problems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们使用乘积的概率，我们还会遇到另一个问题。乘积的概率往往会得到越来越小的数字。计算机没有真正的有理数。`float64`是一种巧妙的方法来掩盖计算机的基本限制。当处理机器学习问题时，你经常会遇到数字变得太小或太大的边缘情况。
- en: 'Fortunately, for this case, we have an elegant solution: We can elect to work
    in the log domain. Instead of considering the likelihood, we would consider the
    log likelihood. Upon taking logs, multiplication becomes addition. This allows
    us to keep it out of sight, and out of mind. For most cases, this project included,
    this is a fine choice. There may be cases where you wish to normalize the probabilities.
    Then, ignoring the denominator wouldn''t work well.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于这个案例，我们有一个优雅的解决方案：我们可以选择在对数域中工作。我们不会考虑似然性，而是考虑对数似然性。在对数运算后，乘法变成了加法。这使得我们可以将其置于视线之外，并从脑海中排除。对于大多数情况，包括本项目，这是一个很好的选择。可能存在你希望归一化概率的情况。那么，忽略分母就不会有效。
- en: 'Let''s look at some code on how to write `priors`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何编写`priors`的代码：
- en: '[PRE27]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The priors are essentially the proportion of `Ham` or `Spam` to the sum of
    all documents. This is fairly simple. To compute the likelihood, let''s look at
    the loop in `Score`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 先验概率基本上是`Ham`或`Spam`与所有文档总和的比例。这相当简单。为了计算似然性，让我们看看`Score`中的循环：
- en: '[PRE28]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We incorporate the likelihood function into the scoring function simply for
    ease of understanding. But the important takeaway of the likelihood function is
    that we''re summing the probabilities of the word given the class. How do you
    calculate ![](img/6a3e0ad0-7fa3-4a97-8b09-49bea7234a2b.png) ? such as the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将似然函数纳入评分函数只是为了便于理解。但似然函数的重要启示是我们正在对给定类别的词的概率进行求和。你是如何计算 ![](img/6a3e0ad0-7fa3-4a97-8b09-49bea7234a2b.png)
    的？例如以下内容：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: First, we check whether the word has been seen. If the word hasn't been seen
    before, then we return a default value `tiny`—a small non-zero value that won't
    cause a division-by-zero error.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查单词是否已被看到。如果单词之前没有见过，那么我们返回一个默认值`tiny`——一个小的非零值，不会引起除以零错误。
- en: The probability of a word occurring in a class is simply its frequency divided
    by the number of words seen by the class. But we want to go a bit further; we
    want to control for frequent words being too important a factor in deciding the
    probability of the class, so we multiply it by the IDF that we had calculated
    earlier. And that's how you'd get the probabilities of the word given a class.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一个词在某个类别中出现的概率简单地是其频率除以该类别看到的单词数。但我们要更进一步；我们想要控制频繁的单词在决定类别概率时不要成为过于重要的因素，所以我们将其乘以我们之前计算出的IDF。这就是如何得到给定类别的词的概率。
- en: After we have the probability, we take the log of it, and then add it to the
    score.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到概率后，我们取其对数，然后将其加到分数上。
- en: Putting it all together
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将它们全部组合起来
- en: 'Now we have all the pieces. Let''s look at how to put it all together:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有部件。让我们看看如何将它们组合在一起：
- en: 'We first `ingest` the dataset and then split the data out into training and
    cross validation sets. The dataset is split into ten parts for a k-fold cross-validation.
    We won''t do that. Instead, we''ll do a single fold cross-validation by holding
    out 30% of the data for cross-validation:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先`ingest`数据集，然后将数据分割成训练集和交叉验证集。数据集被分成十部分以进行k折交叉验证。我们不会这样做。相反，我们将通过保留30%的数据进行交叉验证来执行单折交叉验证：
- en: '[PRE30]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then train the classifier and then check to see whether the classifier can
    predict its own dataset well:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先训练分类器，然后检查分类器是否能够很好地预测其自己的数据集：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After training the classifier, we perform a cross-validation on the data:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练分类器后，我们在数据上执行交叉验证：
- en: '[PRE32]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, I also added an `unseen` and `totalWords` count, as a simple statistic
    to see how well the classifier can generalize when encountering previously unseen
    words.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我还添加了`unseen`和`totalWords`计数，作为一个简单的统计来查看分类器在遇到之前未见过的单词时如何泛化。
- en: 'Additionally, because we know ahead of time that the dataset comprises roughly
    80% `Ham` and 20% `Spam`, we have a baseline to beat. Simply put, we could write
    a classifier that does this:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为我们事先知道数据集大约由80%的`Ham`和20%的`Spam`组成，所以我们有一个要击败的基线。简单来说，我们可以编写一个执行此操作的分类器：
- en: '[PRE33]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Imagine we have such a classifier. Then it would be right 80% of the time!
    For us to know that our classifier is good, it would have to beat a baseline.
    For the purposes of this chapter, we simply print out the statistics and tweak
    accordingly:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个这样的分类器。那么它80%的时候都是正确的！为了知道我们的分类器是好是坏，它必须击败一个基线。为了本章的目的，我们简单地打印出统计数据并相应地进行调整：
- en: '[PRE34]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'So, this is what the final `main` function looks as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的`main`函数看起来如下所示：
- en: '[PRE35]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Running it on `bare`, this is the result I get the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在`bare`上运行它，这是我得到的结果：
- en: '[PRE36]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To see the effects of removing stopwords and lemmatization, we simply switch
    to using the `lemm_stop` dataset, and this is the result I get the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到去除停用词和词形还原的影响，我们只需切换到使用`lemm_stop`数据集，这就是我得到的结果：
- en: '[PRE37]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Either way, the classifier is brutally effective.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 不论哪种方式，这个分类器都非常有效。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I have shown the basics of what a Naive Bayes classifier looks
    like—a classifier written with the fundamental understanding of statistics will
    trump any publicly available library any day.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我展示了朴素贝叶斯分类器的基本形态——一个用对统计学基本理解编写的分类器，在任何时候都会胜过任何公开可用的库。
- en: The classifier itself is fewer than 100 lines of code, but with it comes a great
    deal of power. Being able to perform classification with 98% or greater accuracy
    is no mean feat.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器本身的代码行数不到100行，但它带来了巨大的力量。能够以98%或更高的准确率进行分类可不是什么容易的事情。
- en: 'A note on the 98% figure: This is not state of the art. State of the art is
    in the high 99.xx%. The main reason why there is a race for that final percent
    is because of scale. Imagine you''re Google and you''re running Gmail. A 0.01%
    error means millions of emails being misclassified. That means many unhappy customers.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 关于98%这个数字的说明：这并不是最先进的技术。最先进的技术在99.xx%的高位。之所以有为了那最后1%而进行的竞争，主要是因为规模问题。想象一下，你是谷歌，你在运行Gmail。0.01%的错误意味着数百万封邮件被错误分类。这意味着许多不满意的客户。
- en: For the most part, in machine learning, the case of whether to go for newer
    untested methods really depends on the scale of your problems. In my experience
    from the past 10 years doing machine learning, most companies do not reach that
    scale of data. As such, the humble Naive Bayes classifier would serve very well.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，是否采用新且未经测试的方法在很大程度上取决于你问题的规模。根据我过去10年从事机器学习的经验，大多数公司并没有达到那么大的数据规模。因此，朴素的朴素贝叶斯分类器会非常适用。
- en: 'In the next chapter, we shall look at one of the most vexing issues that humans
    face: time.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨人类面临的最棘手的问题之一：时间。
