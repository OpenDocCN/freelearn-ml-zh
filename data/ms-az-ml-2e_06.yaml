- en: '*Chapter 4*: Ingesting Data and Managing Datasets'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*：导入数据和管理数据集'
- en: In the previous chapter, we set up and explored the Azure Machine Learning workspace,
    performed data experimentation, and scheduled scripts to run on remote compute
    targets in Azure Machine Learning. In this chapter, we will learn how to connect
    datastores and create, explore, access, and track data in Azure Machine Learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们设置了Azure机器学习工作区，进行了数据实验，并安排了在Azure机器学习中远程计算目标上运行的脚本。在本章中，我们将学习如何连接数据存储，创建、探索、访问和跟踪Azure机器学习中的数据。
- en: First, we will take a look at how data is managed in Azure Machine Learning
    by understanding the concepts of **datastores and datasets**. We will see different
    types of datastores and learn best practices for organizing and storing data for
    **machine learning** (**ML**) in Azure.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过了解**数据存储和数据集**的概念来探讨Azure机器学习中数据是如何管理的。我们将查看不同类型的数据存储，并学习在Azure中组织和管理数据以用于**机器学习（ML**）的最佳实践。
- en: Next, we will create an **Azure Blob storage** account and connect it as a datastore
    to Azure Machine Learning. We will cover best practices for ingesting data into
    Azure using popular CLI tools as well as **Azure Data Factory** and **Azure Synapse
    Spark** services.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个**Azure Blob存储**账户，并将其作为数据存储连接到Azure机器学习。我们将介绍使用流行的CLI工具以及**Azure
    Data Factory**和**Azure Synapse Spark**服务将数据导入Azure的最佳实践。
- en: In the following section, we will learn how to create datasets from data in
    Azure, access and explore these datasets, and pass data efficiently to compute
    environments in your Azure Machine Learning workspace. Finally, we will discuss
    how to access Azure Open Datasets to improve your model's performance through
    third-party data sources.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何从Azure中的数据创建数据集，访问和探索这些数据集，并将数据有效地传递到Azure机器学习工作区中的计算环境中。最后，我们将讨论如何通过第三方数据源访问Azure开放数据集，以改善模型性能。
- en: 'The following are the topics that will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Choosing data storage solutions for Azure Machine Learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择Azure机器学习的数据存储解决方案
- en: Creating a datastore and ingesting data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据存储和导入数据
- en: Using datasets in Azure Machine Learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Azure机器学习中使用数据集
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create and manage datastores and datasets:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建和管理数据存储和数据集：
- en: '`azureml-core 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或Azure机器学习中的笔记本环境运行此代码。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter04)。
- en: Choosing data storage solutions for Azure Machine Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择Azure机器学习的数据存储解决方案
- en: When running ML experiments or training scripts on your local development machine,
    you often don't think about managing your datasets. You probably store your training
    data on your local hard drive, external storage device, or file share. In such
    a case, accessing the data for experimentation or training is not a problem, and
    you don't have to worry about the data location, access permissions, maximal throughput,
    parallel access, storage and egress cost, data versioning, and such.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当在本地开发机器上运行ML实验或训练脚本时，您通常不会考虑管理您的数据集。您可能将训练数据存储在本地硬盘、外部存储设备或文件共享中。在这种情况下，访问实验或训练数据不会成为问题，您也不必担心数据位置、访问权限、最大吞吐量、并行访问、存储和出口成本、数据版本等问题。
- en: However, as soon as you start training an ML model on remote compute targets,
    such as a VM in the cloud or within Azure Machine Learning, you must make sure
    that all your executables can access the training data efficiently. This is even
    more relevant if you collaborate with other people who also need to access the
    data in parallel for experimentation, labeling, and training from multiple environments
    and multiple machines. And if you deploy a model that requires access to this
    data as well – for example, looking up labels for categorical results, scoring
    recommendations based on a user's history of ratings, and the like – then this
    environment needs to access the data as well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦你开始在远程计算目标上训练机器学习模型，例如云中的 VM 或 Azure 机器学习内部，你必须确保所有可执行文件都能有效地访问训练数据。如果你与其他人合作，他们也需要从多个环境和多台机器并行访问数据以进行实验、标记和训练，那么这一点尤为重要。如果你部署了一个需要访问这些数据的模型——例如，查找分类结果的标签、根据用户的评分历史进行评分推荐等——那么这个环境也需要访问数据。
- en: In this section, we will learn how to manage data for different use cases in
    Azure. We will first see the abstractions Azure Machine Learning provides to facilitate
    data access for ML experimentation, training, and deployment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 Azure 中管理不同用例的数据。我们将首先了解 Azure 机器学习提供的抽象，以方便数据访问进行机器学习实验、训练和部署。
- en: Organizing data in Azure Machine Learning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Azure 机器学习中组织数据
- en: In Azure Machine Learning, data is managed as **datasets** and data storage
    as **datastores**. This abstraction hides the details of location, data format,
    data transport protocol, and access permissions behind the dataset and datastore
    objects and hence lets Azure Machine Learning users focus on exploring, transforming,
    and managing data without worrying about the underlying storage system.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 机器学习中，数据以**数据集**的形式进行管理，数据存储以**数据存储**的形式进行管理。这种抽象隐藏了数据集和数据存储对象背后的位置、数据格式、数据传输协议和访问权限的细节，因此让
    Azure 机器学习用户能够专注于探索、转换和管理数据，而无需担心底层存储系统。
- en: 'A **datastore** is an abstraction of a physical data storage system that is
    used to link the existing storage system to an Azure Machine Learning workspace.
    In order to connect the existing storage to the workspace – by creating a datastore
    – you need to provide the connection and authentication details of the storage
    system. Once created, the data storage can be accessed by users through the datastore
    object, which will automatically use the provided credentials of the datastore
    definition. This makes it easy to provide access to data storage to your developers,
    data engineers, and scientists who are collaborating in an Azure Machine Learning
    workspace. Currently, the following services can be connected as datastores to
    a workspace:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据存储**是物理数据存储系统的抽象，用于将现有的存储系统连接到 Azure 机器学习工作区。为了通过创建数据存储将现有存储连接到工作区，你需要提供存储系统的连接和身份验证详情。一旦创建，数据存储就可以通过数据存储对象被用户访问，该对象将自动使用数据存储定义中提供的凭据。这使得向在
    Azure 机器学习工作区中协作的开发人员、数据工程师和科学家提供数据存储访问变得容易。目前，以下服务可以作为数据存储连接到工作区：'
- en: Azure Blob containers
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Blob 容器
- en: Azure file share
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 文件共享
- en: Azure Data Lake
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Data Lake
- en: Azure Data Lake Gen2
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Data Lake Gen2
- en: Azure SQL Database
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure SQL 数据库
- en: Azure Database for PostgreSQL
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Database for PostgreSQL
- en: Databricks File System
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks 文件系统
- en: Azure Database for MySQL
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Database for MySQL
- en: While datastores are abstractions of data storage systems, a **dataset** is
    an abstraction of data in general – for example, data in the form of a file on
    a remote server accessible through a public URL or files and tables within a datastore.
    Azure Machine Learning supports two types of abstraction on data formats, namely
    **tabular datasets** and **file datasets**. The former is used to define *tabular*
    data – for example, from comma- or delimiter-separated files, from Parquet and
    JSON files, or from SQL queries – whereas the latter is used to specify *any binary*
    data from files and folders, such as images, audio, and video data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据存储是数据存储系统的抽象，但**数据集**是数据的通用抽象——例如，以远程服务器上的文件形式存在的数据，这些文件可以通过公共URL访问，或者是在数据存储中的文件和表。Azure
    机器学习支持两种数据格式抽象，即**表格数据集**和**文件数据集**。前者用于定义*表格*数据——例如，来自逗号或分隔符分隔的文件、Parquet 和 JSON
    文件，或来自 SQL 查询——而后者用于指定来自文件和文件夹的*任何二进制*数据，例如图像、音频和视频数据。
- en: Tabular datasets can also be defined and used directly from their publicly available
    URL, which is called a `pandas` and `requests`. Both tabular and file datasets
    can be registered in your workspace. We will refer to these datasets as **registered
    datasets**. Registered datasets will show up in your Azure Machine Learning Studio
    under **Datasets**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接从公开可用的 URL 定义和使用表格数据集，这被称为 `pandas` 和 `requests`。表格数据集和文件数据集都可以在您的工作区中注册。我们将把这些数据集称为
    **注册数据集**。注册数据集将在您的 Azure Machine Learning Studio 下的 **数据集** 中显示。
- en: Understanding the default storage accounts of Azure Machine Learning
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Azure 机器学习的默认存储账户
- en: There exists one special datastore in Azure Machine Learning that is used internally
    to store all snapshots, logs, figures, models, and more when executing experiment
    runs. This is called the **default datastore**, is an Azure Blob storage account,
    and is created automatically with Azure Machine Learning when you set up the initial
    workspace. You can select your own Blob storage as the default datastore during
    the workspace creation or connect your storage account and mark it as default
    in Azure Machine Learning Studio.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure Machine Learning 中存在一个特殊的数据存储，用于在执行实验运行时内部存储所有快照、日志、图表、模型等。这被称为 **默认数据存储**，是一个
    Azure Blob 存储账户，并在您设置初始工作区时自动创建。在创建工作区期间，您可以选择自己的 Blob 存储作为默认数据存储，或者在 Azure Machine
    Learning Studio 中连接您的存储账户并将其标记为默认。
- en: '*Figure 4.1* shows you the list of datastores in Azure Machine Learning Studio.
    The default datastore is marked as **Default** and generated automatically when
    setting up an Azure Machine Learning workspace. To go to this view, simply click
    on **Datastores** under the **Manage** category in the left menu in Azure Machine
    Learning Studio. To view existing datasets, click on **Datasets** in the **Assets**
    category:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.1* 展示了 Azure Machine Learning Studio 中的数据存储列表。默认数据存储被标记为 **默认**，并在设置 Azure
    Machine Learning 工作区时自动生成。要访问此视图，只需在 Azure Machine Learning Studio 左侧菜单的 **管理**
    类别下点击 **数据存储**。要查看现有数据集，请在 **资产** 类别下点击 **数据集**：'
- en: '![Figure 4.1 – Default datastore in Azure Machine Learning ](img/B17928_04_01.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – Azure 机器学习中的默认数据存储](img/B17928_04_01.jpg)'
- en: Figure 4.1 – Default datastore in Azure Machine Learning
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – Azure 机器学习中的默认数据存储
- en: 'The default datastore is used by Azure Machine Learning internally to store
    all assets and artifacts when no other datastore is defined. You can access and
    use the default datastore in your workspace identically to your custom datastores
    by creating a datastore reference. The following code snippet shows how to get
    a reference to the default datastore:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有定义其他数据存储时，默认数据存储由 Azure Machine Learning 内部使用来存储所有资产和工件。您可以通过创建数据存储引用，以与自定义数据存储相同的方式访问和使用默认数据存储。以下代码片段显示了如何获取默认数据存储的引用：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The default datastore is used internally by Azure Machine Learning to store
    all assets and artifacts during the ML life cycle. Using the previous code snippet,
    you can access the default datastore to store custom datasets and files.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 默认数据存储由 Azure Machine Learning 内部使用，用于在机器学习生命周期中存储所有资产和工件。使用前面的代码片段，您可以访问默认数据存储以存储自定义数据集和文件。
- en: Once we have accessed the default datastore and connected custom datastores,
    we need to think about a strategy for efficiently storing data for different ML
    use cases. Let's tackle this in the next section.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们访问了默认的数据存储并连接了自定义数据存储，我们就需要考虑一个策略来高效地存储不同机器学习用例的数据。让我们在下一节中解决这个问题。
- en: Exploring options for storing training data in Azure
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索在 Azure 中存储训练数据的选项
- en: Azure supports a myriad of different data storage solutions and technologies
    to store data in the cloud – and as we saw in the previous section, many of these
    are supported datastores in Azure Machine Learning. In this section, we will explore
    some of these services and technologies to understand which ones can be used for
    machine learning use cases.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 支持多种不同的数据存储解决方案和技术，以在云中存储数据——正如我们在上一节中看到的，其中许多都是 Azure Machine Learning
    支持的数据存储。在本节中，我们将探讨一些这些服务和技术，以了解哪些可以用于机器学习用例。
- en: 'Database systems can be broadly categorized by the type of *data* and *data
    access* into the following two categories:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库系统可以根据数据类型和访问数据的类型进行广泛分类，分为以下两类：
- en: '**Relational database management systems** (**RDBMSs**) are often used to store
    normalized transactional data using B-tree-based ordered indices. Typical queries
    filter, group, and aggregate results by joining multiple rows from multiple tables.
    Azure supports different RDBMSs, such as Azure SQL Database, as well as Azure
    Database for PostgreSQL and MySQL.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系数据库管理系统**（**RDBMSs**）通常用于存储使用基于 B 树的有序索引的规范化事务数据。典型的查询通过连接多个表中的多行来过滤、分组和聚合结果。Azure
    支持不同的 RDBMS，例如 Azure SQL 数据库、Azure Database for PostgreSQL 和 MySQL。'
- en: '**NoSQL**: Key-value-based storage systems are often used to store de-normalized
    data with hash-based or ordered indices. Typical queries access a single record
    from a collection distributed based on a partition key. Azure supports different
    NoSQL-based services such as Azure Cosmos DB and Azure Table storage.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NoSQL**：基于键值存储的系统通常用于存储使用基于哈希或有序索引的非规范化数据。典型的查询通过基于分区键分布的集合访问单个记录。Azure 支持不同的基于
    NoSQL 的服务，如 Azure Cosmos DB 和 Azure 表存储。'
- en: As you can see, depending on your use cases, you can use both database technologies
    to store data for machine learning. While RDBMSs are great technologies to store
    training data for machine learning, NoSQL systems are great to store lookup data
    – such as training labels – or ML results such as recommendations, predictions,
    or feature vectors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，根据您的用例，您可以使用这两种数据库技术来存储机器学习数据。虽然 RDBMS 是存储机器学习训练数据的优秀技术，但 NoSQL 系统非常适合存储查找数据，例如训练标签，或机器学习结果，如推荐、预测或特征向量。
- en: Instead of choosing a database service, another popular choice for machine learning
    is to use data storage systems. On disk, most database services persist as data
    pages on **file** or **blob storage systems**. Blob storage systems are a very
    popular choice for storing all kinds of data and assets for machine learning due
    to their scalability, performance, throughput, and cost. Azure Machine Learning
    makes extensive use of blob storage systems, especially for storing all operational
    assets and logs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择数据库服务之外，机器学习的另一种流行选择是使用数据存储系统。在磁盘上，大多数数据库服务以数据页的形式持久化在 **文件** 或 **blob 存储系统**
    上。由于它们的可扩展性、性能、吞吐量和成本，blob 存储系统是存储各种数据和资产以供机器学习使用的非常流行的选择。Azure 机器学习广泛使用 blob
    存储系统，特别是用于存储所有操作资产和日志。
- en: Popular Azure blob storage services are Azure Blob storage and Azure Data Lake
    Storage, which provide great flexibility to implement efficient data storage and
    access solutions through different choices of data formats. While Azure Blob storage
    supports most common blob-based filesystem operations, Azure Data Lake Storage
    implements efficient directory services, which makes it a popular general-purpose
    storage solution for horizontally scalable filesystems. It is a popular choice
    for storing large machine learning training datasets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的 Azure Blob 存储服务包括 Azure Blob 存储和 Azure Data Lake Storage，它们通过不同的数据格式选择提供了极大的灵活性，以实现高效的数据存储和访问解决方案。虽然
    Azure Blob 存储支持大多数基于 blob 的文件系统操作，但 Azure Data Lake Storage 实现了高效的目录服务，这使得它成为横向可扩展文件系统的流行通用存储解决方案。它是存储大型机器学习训练数据集的流行选择。
- en: While tabular data can be stored efficiently in RDBMS systems, similar properties
    can be achieved by choosing the correct data formats and embedded clustered indices
    while storing data on blob storage systems. Choosing the right data format will
    allow your filesystem to efficiently store, read, parse, and aggregate information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然表格数据可以在 RDBMS 系统中有效地存储，但在 blob 存储系统上存储数据时，通过选择正确的数据格式和嵌入的聚类索引也可以实现类似的功能。选择正确的数据格式将允许您的文件系统有效地存储、读取、解析和聚合信息。
- en: Common data format choices can be categorized into textual (CSV, JSON, and more)
    as well as binary formats (images, audio, video, and more). Binary formats for
    storing tabular data are broadly categorized into row-compressed (Protobuf, Avro,
    SequenceFiles, and more) or column-compressed (Parquet, ORC, and more) formats.
    A popular choice is also to compress the whole file using Gzip, Snappy, or other
    compression algorithms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的数据格式选择可以分为文本格式（CSV、JSON 等）以及二进制格式（图像、音频、视频等）。用于存储表格数据的二进制格式通常分为行压缩格式（Protobuf、Avro、SequenceFiles
    等）或列压缩格式（Parquet、ORC 等）。另一种流行的选择是使用 Gzip、Snappy 或其他压缩算法压缩整个文件。
- en: One structure that most data storage systems have in common is a hierarchical
    path or directory structure to organize data blobs. A popular choice for storing
    training data for machine learning is to implement a partitioning strategy for
    your data. This means that data is organized in multiple directories where each
    directory contains all the data for a specific key, also called the partitioning
    key.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据存储系统都共同拥有一种结构，即分层路径或目录结构来组织数据块。对于存储机器学习训练数据的一个流行选择是实施数据分区策略。这意味着数据被组织在多个目录中，每个目录包含特定键的所有数据，也称为分区键。
- en: Cloud providers offer a variety of different storage solutions, which can be
    customized further by choosing different indexing, partitioning, format, and compression
    techniques. A common choice for storing tabular training data for machine learning
    is a column-compressed binary format such as Parquet, partitioned by ingestion
    date, stored on Azure Data Lake Storage, for efficient management operations and
    scalable access.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商提供各种不同的存储解决方案，可以通过选择不同的索引、分区、格式和压缩技术进一步定制。对于存储机器学习表格训练数据的一个常见选择是使用列压缩的二进制格式，如Parquet，按摄取日期分区，存储在Azure数据湖存储上，以实现高效的管理操作和可扩展的访问。
- en: Creating a datastore and ingesting data
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据存储和导入数据
- en: After having a look through the options for storing data in Azure for ML processing,
    we will now create a storage account, which we will use throughout the book for
    our raw data and ML datasets. In addition, we will have a look at how to transfer
    some data into our storage account manually and how to perform this task automatically
    by utilizing integration engines available in Azure.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看Azure中用于ML处理的数据存储选项之后，我们现在将创建一个存储账户，我们将在整个书中使用它来存储原始数据和ML数据集。此外，我们还将探讨如何手动将一些数据传输到我们的存储账户，以及如何通过利用Azure中可用的集成引擎自动执行此任务。
- en: Creating Blob Storage and connecting it with the Azure Machine Learning workspace
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Blob存储并将其与Azure机器学习工作区连接
- en: Let's start by creating a storage account. Any storage account will come with
    a file share, a queue, and table storage for you to utilize in other scenarios.
    In addition to those three, you can either end up with Blob Storage or a Data
    Lake, depending on the settings you provide at creation time. By default, a Blob
    storage account will be created. If we instead want a Data Lake account, we must
    set the `enable-hierarchical-namespace` setting to `True`, as Data Lake offers
    an actual hierarchical folder structure and not a flat namespace.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个存储账户。任何存储账户都将附带一个文件共享、一个队列以及表格存储，以便您在其他场景中利用。除了这三个之外，根据您在创建时提供的设置，您最终可能会得到Blob存储或数据湖。默认情况下，将创建Blob存储账户。如果我们想创建数据湖账户，我们必须将`enable-hierarchical-namespace`设置设置为`True`，因为数据湖提供了一个实际的分层文件夹结构，而不是一个扁平的命名空间。
- en: Creating Blob Storage
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Blob存储
- en: 'Keeping that in mind, let''s create a Blob Storage account:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这一点，让我们创建一个Blob存储账户：
- en: Navigate to a terminal of your choosing, log in to Azure, and check that you
    are working in the correct subscription as we learned in [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054),
    *Preparing the Azure Machine Learning Workspace*.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到您选择的终端，登录Azure，并确认您正在正确的订阅中工作，正如我们在[*第3章*](B17928_03_ePub.xhtml#_idTextAnchor054)，“准备Azure机器学习工作区”中学到的。
- en: 'As we want to create a storage account, let''s have a look at the options and
    required settings for doing so by running the following command:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们要创建一个存储账户，让我们通过运行以下命令来查看创建账户的选项和所需设置：
- en: '[PRE1]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looking through the result, you will see a very long list of possible arguments,
    but the only required ones are `name` and `resource-group`. Still, we should look
    further through this, as a lot of the other settings are still set to certain
    default values, which might be incorrect for our case.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果，您将看到一个非常长的可能参数列表，但唯一必需的是`name`和`resource-group`。尽管如此，我们仍然应该进一步查看，因为许多其他设置仍然设置为某些默认值，这些值可能不适合我们的情况。
- en: Going through the list, you will find a lot of options concerning network or
    security settings. The default for most of them is to at least allow access from
    everywhere. At this moment, we are not too concerned about virtual network integration
    or handling our own managed keys in Azure Key Vault.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看列表时，您会发现许多关于网络或安全设置的选项。大多数选项的默认设置是至少允许从任何地方访问。在这个时候，我们不太关心虚拟网络集成或处理Azure
    Key Vault中的自己的管理密钥。
- en: Besides all these options, there are a few that define the type of storage account
    we set, namely `enable-hierarchical-namespace`, `kind`, `location`, and `sku`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所有这些选项之外，还有一些定义了我们设置的存储账户类型的选项，即`enable-hierarchical-namespace`、`kind`、`location`和`sku`。
- en: We already discussed the first option and as the default is `False`, we can
    ignore it.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了第一个选项，并且默认值为`False`，因此我们可以忽略它。
- en: Looking at `kind`, you see a list of storage types. You might think we need
    to choose `BlobStorage`, but unfortunately, that is a legacy setting left there
    for any storage account still running on the first version, V1\. The default (`StorageV2`)
    is the best option for our scenario.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`kind`，您会看到一个存储类型的列表。您可能会认为我们需要选择`BlobStorage`，但不幸的是，这是一个遗留设置，留在了仍在运行第一版（V1）的任何存储账户中。对于我们的场景，默认的（`StorageV2`）是最佳选项。
- en: Looking at `location`, we see that we apparently can set a default location
    for all deployments, therefore it is not flagged as required. As we did not do
    that so far, we will just provide it when deploying the storage account.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`location`，我们看到我们可以为所有部署设置一个默认位置，因此它没有被标记为必需。由于我们迄今为止还没有这样做，我们将在部署存储账户时提供它。
- en: 'Finally, looking at `sku`, we see a combined setting of an option concerning
    the type of disk technology used (`Standard`/`Premium`), where `Standard` denotes
    HDD storage and `Premium` denotes SSD storage, and an option defining the data
    redundancy scheme (LRS/ZRS/GRS/RAGRS/GZRS). If you want to learn more about the
    redundancy options, follow this link: [https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy](https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy).
    As both increase costs, feel free to either stick with the default (`Standard_RAGRS`)
    or go with local redundancy (`Standard_LRS`).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查看`sku`，我们看到这是一个关于所使用的磁盘技术类型（`Standard`/`Premium`）的选项组合，其中`Standard`表示HDD存储，`Premium`表示SSD存储，以及一个定义数据冗余方案（LRS/ZRS/GRS/RAGRS/GZRS）的选项。如果您想了解更多关于冗余选项的信息，请点击此链接：[https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy](https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy)。由于两者都会增加成本，您可以保留默认值（`Standard_RAGRS`）或选择本地冗余（`Standard_LRS`）。
- en: 'Let''s create our storage account. Please be aware that the name you choose
    must be globally unique, therefore you cannot choose the one you will read in
    the following command:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建我们的存储账户。请注意，您选择的名称必须是全局唯一的，因此您不能选择在以下命令中将要读取的名称：
- en: '[PRE2]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output this creates will show you the detailed settings for the created
    storage account.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令生成的输出将显示创建的存储账户的详细设置。
- en: 'As a final step, let''s create a container in our new blob storage. For that,
    run the following command with the appropriate account name:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为最后一步，让我们在我们的新Blob存储中创建一个容器。为此，请使用适当的账户名称运行以下命令：
- en: '[PRE3]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result will show `True` at the end, but will give you some warnings beforehand,
    something like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将在最后显示`True`，但在之前会给出一些警告，类似于以下内容：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The command worked because it automatically pulled the account key of the storage
    account through our session. Normally, to access a storage account, we either
    need an AD identity, a key to access the whole account (`account-key`), or a shared-access
    key (`sas-token`) to access only a specific subset of folders or containers. We
    will come back to this when connecting from the ML workspace.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 命令成功执行，因为它通过我们的会话自动拉取了存储账户的密钥。通常，要访问存储账户，我们需要一个AD身份、访问整个账户的密钥（`account-key`）或共享访问密钥（`sas-token`）以访问特定子目录或容器。当从ML工作区连接时，我们将会回到这一点。
- en: 'To check the result, run this command:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查结果，请运行以下命令：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have our storage, let's connect it to our Azure Machine Learning
    workspace.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了存储账户，让我们将其连接到我们的Azure机器学习工作区。
- en: Creating a datastore in Azure Machine Learning
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Azure机器学习中创建数据存储
- en: In order to not bother with the storage account itself anymore when working
    with our ML scripts, we will now create a permanent connection to a container
    in a storage account and define it as one of our datastores in the Azure Machine
    Learning workspace.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在处理我们的ML脚本时不再烦恼存储账户本身，我们现在将创建一个永久连接到存储账户中的容器，并将其定义为Azure机器学习工作区中的一个数据存储。
- en: 'The following steps will guide you through this process:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导您完成此过程：
- en: 'First, let''s understand what is required to create a datastore by running
    the following command:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们通过运行以下命令来了解创建数据存储所需的内容：
- en: '[PRE6]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Looking through the output,, we understand that the name of the resource group,
    the name of the ML workspace, and a YAML file is needed. We have two of those
    three things. Therefore, let's understand what the YAML file has to look like.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看输出，我们了解到需要资源组的名称、ML工作区的名称和一个YAML文件。我们有两个这样的东西。因此，让我们了解YAML文件应该是什么样子。
- en: Navigate to [https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob),
    where you will find the required schema of our file and some examples. Going through
    the examples, you will see that they mainly differ concerning the way to authenticate
    to the storage account. The most secure of them is limited access via a SAS token
    and therefore we will pick that route.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航至[https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob](https://docs.microsoft.com/en-us/azure/machine-learning/reference-yaml-datastore-blob)，在那里你可以找到我们文件的所需架构和一些示例。通过查看示例，你会发现它们主要在认证存储账户的方式上有所不同。其中最安全的是通过SAS令牌进行限制访问，因此我们将选择这条路径。
- en: 'Please either download the `blobdatastore.yml` file from the files for [*Chapter
    4*](B17928_04_ePub.xhtml#_idTextAnchor071), *Ingesting Data and Managing Datasets,*
    from the GitHub repository or create a file with the same name and the following
    content:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请从GitHub仓库中下载[*第4章*](B17928_04_ePub.xhtml#_idTextAnchor071)，“数据摄取和管理数据集”的`blobdatastore.yml`文件，或者创建一个具有相同名称和以下内容的文件：
- en: '[PRE7]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Please enter the appropriate account name for your case. The only thing missing
    now is the SAS token, which we need to create for our `mlfiles` container.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请输入适合你情况的适当账户名称。现在唯一缺少的是SAS令牌，我们需要为我们的`mlfiles`容器创建它。
- en: 'Run the following command to create a SAS token for our container:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令为我们的容器创建一个SAS令牌：
- en: '[PRE8]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This command generates a SAS token with an expiration date of 01/01/2023 and
    permissions to `mlfiles` container. Choose an expiration date that is far enough
    in the future for you to work with this book. In normal circumstances, you would
    choose a much shorter expiration date and rotate this key accordingly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令生成一个SAS令牌，有效期为2023年1月1日，并具有对`mlfiles`容器的权限。选择一个足够远的未来日期，以便你可以使用这本书。在正常情况下，你会选择一个更短的过期日期，并相应地旋转此密钥。
- en: 'The result should be in this kind of format:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是这种格式：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Take this result (without quotations) and enter it in the `sas_token` field
    in the YAML file.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 将此结果（不带引号）输入YAML文件中的`sas_token`字段。
- en: 'Navigate to the directory the YAML file is in so that we can finally create
    the datastore in the Azure Machine Learning workspace by running the following
    command:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到YAML文件所在的目录，这样我们就可以通过运行以下命令在Azure Machine Learning工作区中最终创建数据存储：
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result should look like the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该看起来像以下这样：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With these steps, we have registered a datastore connected to our blob storage
    using a SAS token.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些步骤，我们已经使用SAS令牌注册了一个连接到我们的blob存储的数据存储。
- en: Important Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You can follow the same steps when connecting to a Data Lake Storage, but be
    aware that to access a data lake, you will need to create a **service p****rincipal**.
    A detailed description of this can be found here: [https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当连接到数据湖存储时，你可以遵循相同的步骤，但请注意，要访问数据湖，你需要创建一个**服务主体**。有关此内容的详细描述，请参阅此处：[https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal](https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal)。
- en: As discussed before, we could have created a blob storage by navigating to the
    wizard in the Azure portal, creating a SAS token for the container there, and
    entering it in the datastore creation wizard in Azure Machine Learning Studio.
    We used the Azure CLI so that you can get comfortable with this, as this is required
    to automate such steps in the future, especially when we talk about infrastructure-as-code
    and DevOps environments.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以在Azure门户中的向导中创建一个blob存储，为该容器创建一个SAS令牌，并在Azure Machine Learning Studio的数据存储创建向导中输入它。我们使用了Azure
    CLI，这样你可以熟悉这个过程，因为这在将来自动化此类步骤时是必需的，尤其是在我们谈论基础设施即代码和DevOps环境时。
- en: 'In any case, feel free to navigate to the **Datastores** tab in Azure Machine
    Learning Studio. *Figure 4.2* shows our newly created workspace:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，请随意导航到Azure Machine Learning Studio中的**数据存储**选项卡。*图4.2*显示了我们的新创建的工作区：
- en: '![Figure 4.2 – Created datastore ](img/B17928_04_02.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 创建的数据存储](img/B17928_04_02.jpg)'
- en: Figure 4.2 – Created datastore
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 创建的数据存储
- en: Keep this tab open, so we can verify later via the `mlfiles` container, which
    we will start doing in the following section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 保持此标签页打开，以便我们可以在下一节中通过 `mlfiles` 容器进行验证。
- en: Ingesting data into Azure
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据导入 Azure
- en: We created an Azure Blob storage account and learned how to organize and format
    files and tabular data for common ML use cases. However, one often-neglected step
    is how to efficiently ingest data into these datastores, or into Azure in general.
    There are different solutions for different datasets and use cases, from ad hoc,
    automated, parallelized solutions, and more. In this section, we will have a look
    at methods to upload and transform data either in a manual or an automated fashion
    to a relational database (SQL, MySQL, or PostgreSQL) or a storage account in Azure.
    Finally, we will upload a dataset file to the previously created blob storage.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个 Azure Blob 存储帐户，并学习了如何为常见的机器学习用例组织和格式化文件和表格数据。然而，一个常被忽视的步骤是如何将这些数据存储或
    Azure 中的数据有效地导入。针对不同的数据集和用例有不同的解决方案，从临时、自动化、并行化解决方案等。在本节中，我们将探讨将数据手动或自动上传到关系数据库（SQL、MySQL
    或 PostgreSQL）或 Azure 中的存储帐户的方法。最后，我们将上传一个数据集文件到之前创建的 blob 存储。
- en: Understanding tooling for the manual ingestion of data
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解手动导入数据的相关工具
- en: If you work with a small number of datasets and files and you do not need to
    transfer data from other existing sources, a manual upload of data is the go-to
    option.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您处理少量数据集和文件，并且不需要从其他现有源传输数据，手动上传数据是首选选项。
- en: 'The following list shows possible options to bring data into your datastores
    or directly into your ML pipelines:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了将数据带入您的数据存储或直接带入您的机器学习管道的可能选项：
- en: '**Azure Storage Explorer**: Storage Explorer is an interactive application
    that allows you to upload data to and control datastores, such as storage accounts
    and managed disks. This is the easiest tool to use for managing storage accounts
    and can be found here: [https://azure.microsoft.com/en-us/features/storage-explorer/#overview](https://azure.microsoft.com/en-us/features/storage-explorer/#overview).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure 存储资源管理器**：存储资源管理器是一个交互式应用程序，允许您上传数据到并控制数据存储，例如存储帐户和管理磁盘。这是管理存储帐户最容易使用的工具，您可以在以下位置找到它：[https://azure.microsoft.com/en-us/features/storage-explorer/#overview](https://azure.microsoft.com/en-us/features/storage-explorer/#overview)。'
- en: '**Azure CLI**: As we saw before, we basically can do anything with the CLI,
    including creating and uploading blobs to a storage account. You can find the
    appropriate commands to upload blobs in the storage extension described here:
    [https://docs.microsoft.com/en-us/cli/azure/storage/blob](https://docs.microsoft.com/en-us/cli/azure/storage/blob).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure CLI**：正如我们之前看到的，我们基本上可以使用 CLI 做任何事情，包括在存储帐户中创建和上传 blob。您可以在以下存储扩展中找到上传
    blob 的适当命令：[https://docs.microsoft.com/en-us/cli/azure/storage/blob](https://docs.microsoft.com/en-us/cli/azure/storage/blob)。'
- en: '**AzCopy**: This is another command-line tool specifically designed to copy
    blobs or files to a storage account. Whether you use Azure CLI packages or AzCopy
    comes down to personal preference, as there are no clear performance differences
    between these two options. You can find the download link and the description
    here: [https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AzCopy**：这是另一个专门设计用于将 blob 或文件复制到存储帐户的命令行工具。您使用 Azure CLI 软件包还是 AzCopy 取决于个人喜好，因为这两种选项之间没有明显的性能差异。您可以在以下位置找到下载链接和描述：[https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10)。'
- en: '**The Azure portal**: For any service, you will always find a web interface
    directly in the Azure portal to upload or change data. If you navigate to a storage
    account, you can use the inbuilt storage browser to upload blobs and files directly
    through the web interface. The same is true for any of the database technologies.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure 门户**：对于任何服务，您都会在 Azure 门户中直接找到一个用于上传或更改数据的网络界面。如果您导航到存储帐户，您可以使用内置的存储浏览器通过网络界面直接上传
    blob 和文件。对于任何数据库技术也是如此。'
- en: '**RDBMS management tooling**: You can use any typical management tool to configure,
    create, and change tables and schemas in a relational database. For a SQL database
    and Synapse, this would be **SQL Server Management Studio** ([https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15));
    for PostgreSQL, this would be **pgAdmin** ([https://www.pgadmin.org/](https://www.pgadmin.org/));
    and for MySQL, this would be **MySQL Workbench** ([https://docs.microsoft.com/en-us/azure/mysql/connect-workbench](https://docs.microsoft.com/en-us/azure/mysql/connect-workbench)).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RDBMS管理工具**：您可以使用任何典型的管理工具来配置、创建和更改关系数据库中的表和模式。对于SQL数据库和Synapse，这将使用**SQL
    Server Management Studio** ([https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15))；对于PostgreSQL，这将使用**pgAdmin**
    ([https://www.pgadmin.org/](https://www.pgadmin.org/))；对于MySQL，这将使用**MySQL Workbench**
    ([https://docs.microsoft.com/en-us/azure/mysql/connect-workbench](https://docs.microsoft.com/en-us/azure/mysql/connect-workbench))。'
- en: '**Azure Data Studio**: Data Studio allows you to connect to any Microsoft SQL
    database, to Synapse, to a PostgreSQL database in Azure, and to Azure Data Explorer.
    It is a multiplatform tool very similar to the typical management tooling mentioned
    in the last point but in one platform. You can download this tool here: [https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Data Studio**：Data Studio允许您连接到任何Microsoft SQL数据库、Synapse、Azure中的PostgreSQL数据库以及Azure数据探索器。这是一个多平台工具，与最后一点中提到的典型管理工具非常相似，但仅在一个平台上。您可以从这里下载此工具：[https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15)。'
- en: '**Azure Machine Learning designer (Import Data)**: If you do not want to use
    an Azure Machine Learning datastore, you can use the **Import Data** component
    in the Machine Learning designer to add data ad hoc to your pipelines. This is
    not the cleanest way to operate, but an option nonetheless. You can find all information
    about this method here: [https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Machine Learning designer（导入数据）**：如果您不想使用Azure机器学习数据存储，您可以使用机器学习设计器中的**导入数据**组件将数据临时添加到您的管道中。这不是最干净的操作方式，但仍然是一个选择。您可以在以下位置找到有关此方法的所有信息：[https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/import-data)。'
- en: Before we test out some of these options, let's have a look at the options to
    create automated data flows and transform data in Azure.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们测试这些选项之前，让我们看看在Azure中创建自动化数据流和转换数据的选择。
- en: Understanding tooling for automated ingestion and transformation of data
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自动化数据摄取和转换的工具
- en: Copying data manually is completely fine for small tests and probably even for
    most of the tasks we will perform in this book, but in a real-world scenario,
    we will need to not only integrate with a lot of different sources but will also
    need a process that does not include a person manually moving data from A to B.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小型测试来说，手动复制数据是完全可行的，甚至可能是我们将在本书中执行的大多数任务，但在现实世界的场景中，我们不仅需要与许多不同的源进行集成，还需要一个不涉及人员手动将数据从A移动到B的过程。
- en: Therefore, we will now have a look at services that allow us to transform and
    move data in an automated fashion and that integrate very well with pipelines
    and MLOps in Azure Machine Learning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在将查看允许我们以自动化方式转换和移动数据的服务，并且这些服务与Azure机器学习中的管道和MLOps集成得非常好。
- en: Azure Data Factory
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Azure Data Factory
- en: Azure Data Factory is the enterprise-ready solution for moving and transforming
    data in Azure. It offers the ability to connect to hundreds of different sources
    and to create pipelines to transform the integrated data, calling multiple other
    services in Azure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Data Factory是Azure中用于移动和转换数据的企业级解决方案。它提供了连接到数百个不同源的能力，并能够创建管道以转换集成数据，同时调用Azure中的多个其他服务。
- en: 'Run the following command to create a data factory:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以创建数据工厂：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Please be aware that the name, once again, has to be globally unique. In addition,
    before deployment, the CLI will ask you to install the `datafactory` extension.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，名称必须再次是全球唯一的。此外，在部署之前，CLI将要求您安装`datafactory`扩展。
- en: 'Once you are done, navigate to the resource in the Azure portal, and on the
    **Overview** tab, click on **Open Azure Data Factory Studio**, which will lead
    you to the workbench for your data factory instance. You should see a view as
    shown in *Figure 4.3*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，导航到 Azure 门户中的资源，在 **概览** 选项卡上点击 **打开 Azure 数据工厂工作室**，这将带您到您的数据工厂实例的工作台。您应该看到一个如
    *图 4.3* 所示的视图：
- en: '![Figure 4.3 – Data Factory resource view ](img/B17928_04_03.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 数据工厂资源视图](img/B17928_04_03.jpg)'
- en: Figure 4.3 – Data Factory resource view
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 数据工厂资源视图
- en: 'From this view, you can create pipelines, datasets, data flows, and power queries.
    Let''s briefly discuss what they are:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从此视图，您可以创建管道、数据集、数据流和 Power Query。让我们简要讨论一下它们是什么：
- en: '**Pipelines**: Pipelines are the main star of Azure Data Factory. You can create
    complex pipelines calling multiple services to pull data from a source, transform
    it, and store it in a sink.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：管道是 Azure 数据工厂的主要明星。您可以通过创建复杂的管道，调用多个服务从源拉取数据，对其进行转换，并将其存储在目标中。'
- en: '**Datasets**: Datasets are used in a pipeline as a source or a sink. Therefore,
    before building a pipeline, you can define a connection to specific data in a
    datastore that you want to read from or write to in the end.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：数据集在管道中用作源或目标。因此，在构建管道之前，您可以定义一个连接到您最终想要从中读取或写入特定数据的数据存储的连接。'
- en: '**Data flows**: Data flows allows you to do the actual processing or transformation
    of data within Data Factory itself, instead of calling a different service to
    do the heavy lifting.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据流**：数据流允许您在数据工厂内部本身进行实际的数据处理或转换，而不是调用不同的服务来完成繁重的工作。'
- en: '**Power Query**: Power Query allows you to do data exploration with DAX inside
    Data Factory, which is typically only possible with Power BI or Excel otherwise.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Power Query**：Power Query 允许您在数据工厂内部使用 DAX 进行数据探索，这在其他情况下通常只有使用 Power BI
    或 Excel 才能实现。'
- en: 'If you click on the three dots next to **Pipeline**, you can create a new one,
    which will result in the following view shown in *Figure 4.4*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您点击 **Pipeline** 旁边的三个点，可以创建一个新的，这将导致如 *图 4.4* 所示的以下视图：
- en: '![Figure 4.4 – Creating a Data Factory pipeline ](img/B17928_04_04.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 创建数据工厂管道](img/B17928_04_04.jpg)'
- en: Figure 4.4 – Creating a Data Factory pipeline
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 创建数据工厂管道
- en: Having a look through the possible activities, you will find a way to copy data
    (**Copy Data**) from A to B, to execute a script in Azure Functions (**Azure Function**),
    to call a stored procedure in a SQL database (**Stored Procedure**), to execute
    a notebook in Databricks (**Notebook**), and to execute an ML pipeline (**Machine
    Learning Execute Pipeline**), among other things. With these activities and the
    control tools you will find in **General** and **Iteration & conditionals**, you
    can build very complex data pipelines to move and transform your data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 查看可能的活动，您会发现一种从 A 到 B 复制数据（**Copy Data**）、在 Azure Functions 中执行脚本（**Azure Function**）、在
    SQL 数据库中调用存储过程（**Stored Procedure**）、在 Databricks 中执行笔记本（**Notebook**）以及执行 ML
    管道（**Machine Learning Execute Pipeline**）等方法。通过这些活动和您在 **通用** 和 **迭代与条件** 中找到的控制工具，您可以构建非常复杂的数据管道来移动和转换您的数据。
- en: As you might have noticed, Azure Synapse is missing from the list of activities.
    The reason for that is that Synapse has its own version of Data Factory integrated
    into the platform. Therefore, if you are using a SQL pool or a Spark pool in Synapse,
    you can use the integration tool of Synapse instead, which will give you access
    to running a notebook in the Synapse Spark pool or a stored procedure on the SQL
    pool.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，Azure Synapse 在活动列表中缺失。原因是 Synapse 在平台中集成了自己的数据工厂版本。因此，如果您在 Synapse
    中使用 SQL 池或 Spark 池，您可以使用 Synapse 的集成工具，这将为您提供在 Synapse Spark 池中运行笔记本或在 SQL 池上调用存储过程的访问权限。
- en: 'If you are looking for an in-depth overview of Azure Data Factory, have a look
    at Catherine Wilhelmsen''s *Beginners Guide to Azure Data Factory*: [https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/](https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找 Azure 数据工厂的深入概述，请查看 Catherine Wilhelmsen 的 *Azure 数据工厂入门指南*：[https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/](https://www.cathrinewilhelmsen.net/series/beginners-guide-azure-data-factory/)。
- en: 'Now, what we need to understand is that there are two ways to integrate this
    Data Factory pipeline into Azure Machine Learning:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要理解的是，有两种方法可以将此数据工厂管道集成到 Azure Machine Learning 中：
- en: '**Read results from a storage account**: We can just run the transformation
    pipeline in Data Factory, transforming our data, and then store the result in
    a storage account. We then access the data as we learned via an ML datastore.
    In this scenario, any pipeline we have in Azure Machine Learning is disconnected
    from the transformation pipelines in Data Factory, which might not be the optimal
    way for MLOps.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从存储帐户读取结果**：我们可以在数据工厂中运行转换管道，转换我们的数据，然后将结果存储在存储帐户中。然后，我们可以通过我们学习的方式访问数据，即通过一个
    ML 数据存储。在这种情况下，我们在 Azure 机器学习中的任何管道都与数据工厂中的转换管道断开连接，这可能不是 MLOps 的最佳方式。'
- en: '**Invoke Azure Machine Learning from Data Factory**: We can create a transformation
    pipeline and invoke the actual Azure Machine Learning pipeline as part of the
    Data Factory pipeline. This is the preferred way if we are starting to build an
    end-to-end MLOps workflow.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从数据工厂调用 Azure 机器学习**：我们可以创建一个转换管道，并将实际的 Azure 机器学习管道作为数据工厂管道的一部分来调用。如果我们开始构建端到端的
    MLOps 工作流程，这是首选方式。'
- en: 'For further information on this, have a read through the following article:
    [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这方面的更多信息，请阅读以下文章：[https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-data-ingest-adf)。
- en: Azure Synapse Spark pools
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Azure Synapse Spark 池
- en: As we discussed in [*Chapter 2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing
    the Right Machine Learning Service in Azure*, Azure Databricks and Azure Synapse
    give you the option to run Spark jobs in a Spark pool. Apache Spark can help you
    transform and preprocess extremely large datasets by utilizing the distributive
    nature of the node pool underneath. Therefore, this tool can be helpful to take
    apart and filter out datasets before starting the actual machine learning process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[*第 2 章*](B17928_02_ePub.xhtml#_idTextAnchor034)中讨论的，*在 Azure 中选择合适的机器学习服务*，Azure
    Databricks 和 Azure Synapse 提供了在 Spark 池中运行 Spark 作业的选项。Apache Spark 可以通过利用节点池的分布式特性来帮助您转换和预处理极其庞大的数据集。因此，这个工具在开始实际的机器学习过程之前，可以帮助我们分解和过滤数据集。
- en: We have seen that we can run notebooks from either Azure Data Factory or from
    the integration engine in Azure Synapse and therefore already have access to these
    services. On top of that, we have the option to add a Synapse Spark pool as a
    so-called **linked service** in the Azure Machine Learning workspace (see the
    **Linked Services** tab in Azure Machine Learning Studio). Doing this step gives
    us the opportunity to access not only the ML compute targets but also the Spark
    pool as a target for computations via the Azure Machine Learning SDK.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们可以从 Azure Data Factory 或 Azure Synapse 的集成引擎中运行笔记本，因此已经可以访问这些服务。除此之外，我们还有选项将
    Synapse Spark 池作为所谓的**链接服务**添加到 Azure 机器学习工作区中（请参阅 Azure 机器学习工作室中的**链接服务**选项卡）。执行此步骤后，我们不仅可以访问
    ML 计算目标，还可以通过 Azure 机器学习 SDK 将 Spark 池作为计算目标。
- en: 'You can create this link either via Azure Machine Learning Studio or via the
    Azure Machine Learning Python SDK, both of which are described in the following
    article: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过 Azure 机器学习工作室或 Azure 机器学习 Python SDK 创建此链接，这两者都在以下文章中进行了描述：[https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-link-synapse-ml-workspaces)。
- en: Through this direct integration, we can run transformation steps in our ML pipelines
    through a Spark cluster and therefore get another good option for building a clean
    end-to-end MLOps workflow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种直接集成，我们可以在我们的 ML 管道中通过 Spark 集群运行转换步骤，因此又得到了构建干净端到端 MLOps 工作流程的另一个好选择。
- en: Copying data to Blob storage
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据复制到 Blob 存储
- en: Now, that we have a good understanding of most of the options to move and transform
    data, let's upload a dataset to our storage account.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经对大多数移动和转换数据的选项有了很好的理解，让我们将数据集上传到我们的存储帐户。
- en: In [*Chapter 5*](B17928_05_ePub.xhtml#_idTextAnchor085), *Performing Data Analysis
    and Visualization*, we will start analyzing and preprocessing data. To prepare
    for this, let's upload the dataset we will work with in that chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 5 章*](B17928_05_ePub.xhtml#_idTextAnchor085)中，*执行数据分析与可视化*，我们将开始分析和预处理数据。为此，让我们上传本章中将使用的数据集。
- en: 'We will work with the **Melbourne Housing dataset**, created by Anthony Pino,
    which you can find here: [https://www.kaggle.com/anthonypino/melbourne-housing-market](https://www.kaggle.com/anthonypino/melbourne-housing-market).
    The reason to work with this dataset is the domain it covers, as everyone understands
    housing, and the reasonable cleanliness of the data. If you continue your journey
    through working with data, you will find out that there are a lot of datasets
    out there, but only a few that are clean and educational.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用由Anthony Pino创建的**墨尔本住房数据集**，您可以在以下链接找到：[https://www.kaggle.com/anthonypino/melbourne-housing-market](https://www.kaggle.com/anthonypino/melbourne-housing-market)。选择这个数据集的原因是它覆盖的领域，因为每个人都理解住房，以及数据的合理清洁度。如果你继续通过处理数据来推进你的旅程，你会发现有很多数据集，但只有少数是干净且具有教育意义的。
- en: In addition, to make our lives a bit easier when analyzing the dataset in the
    next chapter, we will actually work with a subset of this dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使我们在下一章分析数据集时生活更加便利，我们将实际上只使用这个数据集的一个子集。
- en: 'Follow the next steps so that we can make this file available in our `mldemoblob`
    datastore:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作，以便我们可以将此文件放入我们的`mldemoblob`数据存储中：
- en: Download the `melb_data.csv` file from [https://www.kaggle.com/dansbecker/melbourne-housing-snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot)
    and store it in a suitable folder on your device.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.kaggle.com/dansbecker/melbourne-housing-snapshot](https://www.kaggle.com/dansbecker/melbourne-housing-snapshot)下载`melb_data.csv`文件，并将其存储在你的设备上的一个合适的文件夹中。
- en: 'Navigate to that folder and run the following command in the CLI, replacing
    the storage account name with your own:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到该文件夹，并在CLI中运行以下命令，将存储账户名称替换为您自己的名称：
- en: '[PRE13]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To verify this, let''s have a look at another option to move this file. Install
    Azure Storage Explorer and log in to your Azure account in that application. Navigate
    to your storage account and open the `mlfiles` container. It should show you a
    view as seen in *Figure 4.5*:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证这一点，让我们看看另一种移动此文件的方法。安装Azure存储资源管理器，并在该应用程序中登录到您的Azure账户。导航到您的存储账户并打开`mlfiles`容器。它应该显示如图4.5所示的视图：
- en: '![Figure 4.5 – Azure Storage Explorer ](img/B17928_04_05.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – Azure存储资源管理器](img/B17928_04_05.jpg)'
- en: Figure 4.5 – Azure Storage Explorer
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – Azure存储资源管理器
- en: As you can see, our file is where it is supposed to be. We could have also just
    dragged and dropped the file directly here, creating a blob file automatically.
    From here on out, feel free to use what feels more comfortable to you.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的文件就在它应该的位置。我们也可以直接将文件拖放到这里，自动创建一个blob文件。从现在开始，请随意使用您觉得更舒适的方法。
- en: To finish this up, have a look at the application itself. For example, if you
    right-click on the container, you can choose an option called **Get Shared Access
    Signature**, which opens a wizard allowing you to create a SAS token directly
    here, instead of as we did via the command line.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完成这个步骤，请查看应用程序本身。例如，如果你在容器上右键点击，你可以选择一个名为**获取共享访问签名**的选项，这将打开一个向导，允许你直接在这里创建SAS令牌，而不是像我们通过命令行那样做。
- en: With the previous steps, we made our raw dataset file available in our storage
    account and therefore in our ML datastore. In the next section, we will have a
    look at how to create an Azure Machine Learning dataset from these raw files and
    what features they offer to support us in our ongoing ML journey.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过之前的步骤，我们已经将原始数据集文件放入我们的存储账户中，因此也放入了我们的机器学习数据存储中。在下一节中，我们将探讨如何从这些原始文件创建Azure机器学习数据集，以及它们提供了哪些功能来支持我们当前的机器学习之旅。
- en: Using datasets in Azure Machine Learning
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Azure机器学习中使用数据集
- en: In the previous sections of this chapter, we discussed how to get data into
    the cloud, store the data in a datastore, and connect the data via a **datastore
    and d****ataset** to an Azure Machine Learning workspace. We did all this effort
    of managing the data and data access centrally in order to use the data across
    all compute environments, either for experimentation, training, or inferencing.
    In this section, we will focus on how to create, explore, and access these datasets
    during training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前几节中，我们讨论了如何将数据放入云端，将数据存储在数据存储中，以及如何通过**数据存储和数据集**将数据连接到Azure机器学习工作区。我们集中管理数据和数据访问，以便在所有计算环境中使用数据，无论是用于实验、训练还是推理。在本节中，我们将重点介绍如何在训练期间创建、探索和访问这些数据集。
- en: Once the data is managed as datasets, we can track the data that was used for
    each experimentation or training run in Azure Machine Learning. This will give
    us visibility of the data used for a specific training run and for the trained
    model – an essential step in creating reproducible end-to-end machine learning
    workflows.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被管理为数据集，我们就可以在Azure Machine Learning中跟踪每个实验或训练运行所使用的数据。这将使我们能够了解特定训练运行和训练模型所使用的数据，这是创建可重复端到端机器学习工作流程的关键步骤。
- en: Another benefit of organizing your data into datasets is that you can easily
    pass a managed dataset to your experimentation or training scripts via **direct
    access**, **download**, or **mount**. The direct access method is useful for publicly
    available data sources, the *download* method is convenient for small datasets,
    and the *mount* method is useful for large datasets. In Azure Machine Learning
    training clusters, this is completely transparent, and the data will be provided
    automatically. However, we can use the same technique to access the data in any
    other Python environment, by simply having access to the dataset object.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的数据组织到数据集中还有另一个好处，那就是您可以通过**直接访问**、**下载**或**挂载**轻松地将管理数据集传递到您的实验或训练脚本中。直接访问方法适用于公开可用的数据源，**下载**方法适用于小型数据集，**挂载**方法适用于大型数据集。在Azure
    Machine Learning训练集群中，这完全透明，数据将自动提供。然而，我们可以使用相同的技巧通过访问数据集对象来访问任何其他Python环境中的数据。
- en: In the last part of this section, we will explore Azure Open Datasets – a collection
    of curated Azure Machine Learning datasets you can consume directly from within
    your Azure Machine Learning workspace.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后部分，我们将探讨Azure Open Datasets——一组您可以直接从Azure Machine Learning工作区中消费的精选Azure
    Machine Learning数据集。
- en: Creating new datasets
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建新的数据集
- en: 'There are multiple ways to create new datasets, but most of them differentiate
    between tabular and file datasets. You need to use different constructors based
    on the type of dataset you want to create:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新的数据集有多种方法，但大多数方法会区分表格数据和文件数据集。您需要根据您想要创建的数据集类型使用不同的构造函数：
- en: '`Dataset.Tabular.from_*` for tabular datasets'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset.Tabular.from_*` 用于表格数据集'
- en: '`Dataset.File.from_*` for file-based datasets (for example, image, audio, and
    more)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset.File.from_*` 用于基于文件的数据集（例如，图像、音频等）'
- en: For tabular datasets, we also differentiate between the data being accessed
    from the original location through a public URL – called a *direct dataset* –
    or stored on either the default or a custom *datastore*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表格数据集，我们也会区分从原始位置通过公开URL访问的数据——称为**直接数据集**——或存储在默认或自定义**数据存储**中。
- en: A `Dataset` object can be accessed or passed around in the current environment
    through its object reference. However, a dataset can also be registered (and versioned),
    and hence accessed through the dataset name (and version) – this is called a *registered
    dataset*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`对象可以通过其对象引用在当前环境中访问或传递。然而，数据集也可以注册（并版本化），因此可以通过数据集名称（和版本）访问——这被称为**注册数据集**。'
- en: 'Let''s see a simple example of a direct dataset, which is defined as a tabular
    dataset, and a publicly available URL containing a delimiter-separated file with
    the data:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的直接数据集示例，它被定义为表格数据集，并包含一个公开可用的URL，该URL包含包含数据的分隔符文件：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see in the code, we can create a *direct dataset* by passing the
    URL to a publicly accessible delimiter-separated file. When passing this dataset
    internally, every consumer will attempt to fetch the dataset from its URL.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在代码中所见，我们可以通过传递公开可访问的分隔符文件URL来创建一个**直接数据集**。当内部传递此数据集时，每个消费者都会尝试从其URL获取数据集。
- en: '![Figure 4.6 – Direct dataset ](img/B17928_04_06.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 直接数据集](img/B17928_04_06.jpg)'
- en: Figure 4.6 – Direct dataset
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 直接数据集
- en: 'Once we have a reference to a datastore, we can access data within it. In the
    following example, we create a file dataset from files stored in a directory of
    the `mldata` datastore:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个数据存储的引用，我们就可以访问其中的数据。在以下示例中，我们从`mldata`数据存储的目录中创建一个文件数据集：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see in the example, we can register data from within the datastore
    as datasets. In this example, we defined all files in a folder as a file dataset,
    but we could also define a delimiter-separated file in Blob storage as a tabular
    dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如示例所示，我们可以将数据存储中的数据注册为数据集。在这个例子中，我们将文件夹中的所有文件定义为文件数据集，但我们也可以将Blob存储中的分隔符文件定义为表格数据集。
- en: '![Figure 4.7 – File dataset ](img/B17928_04_07.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – 文件数据集](img/B17928_04_07.jpg)'
- en: Figure 4.7 – File dataset
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 文件数据集
- en: 'In the next step, we register this dataset in the workspace using the following
    code snippet to create a *registered dataset*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们使用以下代码片段在工作空间中注册此数据集以创建一个 *已注册的数据集*：
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The previous code will register the direct dataset in your workspace and return
    a registered dataset. Registered datasets are listed in Azure Machine Learning
    Studio, and can be accessed via the dataset name instead of the `Dataset` Python
    object.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码将直接数据集注册到你的工作空间，并返回一个已注册的数据集。已注册的数据集列在 Azure Machine Learning Studio 中，可以通过数据集名称而不是
    `Dataset` Python 对象访问。
- en: The `create_new_version` argument controls whether we want to create a new version
    of an existing dataset. Once a new dataset version is created, the dataset can
    be accessed through the dataset name – which will implicitly access the latest
    version – or through its name and a specific version. Dataset versions are useful
    to manage different iterations of the dataset within your workspace.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_new_version` 参数控制我们是否想要创建现有数据集的新版本。一旦创建了一个新的数据集版本，就可以通过数据集名称访问数据集 -
    这将隐式访问最新版本 - 或者通过其名称和特定版本。数据集版本对于管理工作空间内数据集的不同迭代非常有用。'
- en: Exploring data in datasets
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索数据集中的数据
- en: 'There are multiple options to explore registered datasets in Azure Machine
    Learning. For tabular datasets, the most convenient way is to load and analyze
    a dataset programmatically in an Azure Machine Learning workspace. To do so, you
    can simply reference a dataset by its name and version as shown in the following
    snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure Machine Learning 中探索已注册数据集有多种选择。对于表格数据集，最方便的方法是在 Azure Machine Learning
    工作空间中以编程方式加载和分析数据集。为此，你可以简单地通过其名称和版本引用数据集，如下面的代码片段所示：
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once you have a reference to the dataset, you can convert a dataset reference
    to an actual in-memory **pandas** DataFrame or a lazy-loaded **Spark** or **Dask**
    DataFrame. To do so, you can call one of the following methods:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了数据集的引用，你可以将数据集引用转换为实际的内存中 **pandas** DataFrame 或懒加载的 **Spark** 或 **Dask**
    DataFrame。为此，你可以调用以下方法之一：
- en: '`to_pandas_dataframe()` to create an in-memory pandas DataFrame'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_pandas_dataframe()` 用于创建内存中的 pandas DataFrame'
- en: '`to_spark_dataframe()` to create a lazily loaded Spark DataFrame'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_spark_dataframe()` 用于创建一个懒加载的 Spark DataFrame'
- en: '`to_dask_dataframe()` to create a lazily loaded Dask DataFrame'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_dask_dataframe()` 用于创建懒加载的 Dask DataFrame'
- en: 'Let''s see the three commands in action, starting with the in-memory pandas
    DataFrame. The following code snippet will load all the data into a pandas DataFrame
    and then return the first five rows of the DataFrame:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看三个命令的实际操作，从内存中的 pandas DataFrame 开始。以下代码片段将所有数据加载到 pandas DataFrame 中，然后返回
    DataFrame 的前五行：
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After loading the DataFrame, you can run your favorite pandas methods to explore
    the datasets. For example, good commands to get started are `info()` to see columns
    and datatypes and `describe()` to see statistics of the numerical values of the
    DataFrame.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载 DataFrame 之后，你可以运行你喜欢的 pandas 方法来探索数据集。例如，开始时使用 `info()` 命令查看列和数据类型，以及使用
    `describe()` 命令查看 DataFrame 中数值的统计信息。
- en: Lazy datasets are datasets that only load some data to memory when explicitly
    needed, for example, when a result of a computation is required. Non-lazy datasets
    load all the data into memory and hence are limited by the available memory.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 懒加载数据集是仅在需要时才将部分数据加载到内存中的数据集，例如，当需要计算结果时。非懒加载数据集将所有数据加载到内存中，因此受可用内存的限制。
- en: 'If you are more familiar with PySpark, you can also transform a dataset into
    a Spark DataFrame with the following code snippet. In contrast to the previous
    example, this code won''t actually load all data into memory but only fetches
    the data required for executing the `show()` command – this makes it a great choice
    for analyzing large datasets:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更熟悉 PySpark，你也可以使用以下代码片段将数据集转换为 Spark DataFrame。与上一个示例相比，此代码实际上不会将所有数据加载到内存中，而只会获取执行
    `show()` 命令所需的数据 - 这使其成为分析大型数据集的绝佳选择：
- en: '[PRE19]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Another alternative is to return a Dask DataFrame of the dataset. Dask is a
    Python library for parallel computing that supports lazy datasets with a pandas-
    and NumPy-like API. Hence you can run the following code to return the first five
    rows of the DataFrame lazily:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是返回数据集的 Dask DataFrame。Dask 是一个支持懒加载数据集的 Python 库，具有类似 pandas 和 NumPy 的
    API。因此，你可以运行以下代码以懒加载方式返回 DataFrame 的前五行：
- en: '[PRE20]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once you have programmatic access to the data in your favorite numeric or statistical
    libraries, you can slice and dice your dataset as much as needed. While programmatic
    access is great for reproducibility and customization, users often just want to
    understand how the data is structured and see a few example records. Azure Machine
    Learning also offers the possibility to explore the dataset in the Data Studio
    UI.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获得了访问你喜欢的数值或统计库中数据的编程权限，你就可以根据需要对你的数据集进行切片和切块。虽然编程访问对于可重复性和定制化来说很棒，但用户通常只想了解数据是如何结构的，并查看一些示例记录。Azure
    Machine Learning 也提供了在 Data Studio UI 中探索数据集的可能性。
- en: 'To get to this view, go to **Datasets**, select a dataset, and click on the
    **Explore** tab. The first page shows you a preview of your data, including the
    first *n* rows as well as some basic information about the data – such as the
    number of rows and columns. The following screenshot shows an example:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入此视图，请转到 **数据集**，选择一个数据集，然后点击 **探索** 选项卡。第一页显示了你的数据预览，包括前 *n* 行以及一些关于数据的基本信息——例如行数和列数。以下截图是一个示例：
- en: '![Figure 4.8 – Dataset with data preview ](img/B17928_04_08.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 带有数据预览的数据集](img/B17928_04_08.jpg)'
- en: Figure 4.8 – Dataset with data preview
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 带有数据预览的数据集
- en: 'If you click on the second tab, you can generate and view a data profile. This
    profile is similar to calling `describe()` on the pandas DataFrame – a statistical
    analysis of each column in the dataset, but with support for categorical data
    and some more useful information. As you can see in *Figure 4.9*, it also shows
    a figure with the data distribution for each column:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击第二个选项卡，你可以生成并查看数据概要。此概要类似于在 pandas DataFrame 上调用 `describe()`——对数据集中每一列的统计分析，但支持分类数据和一些更有用的信息。如图
    4.9 所示，它还显示了每个列的数据分布图：
- en: '![Figure 4.9 – Dataset with data profile ](img/B17928_04_09.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 带有数据概要的数据集](img/B17928_04_09.jpg)'
- en: Figure 4.9 – Dataset with data profile
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 带有数据概要的数据集
- en: As you can see in the previous figure, this is a very useful summary of the
    dataset. The insights from this view are important for everyone working with this
    dataset.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，这是数据集的一个非常有用的总结。从这个视图中获得的见解对于所有使用此数据集的人来说都很重要。
- en: In this section, we saw multiple ways to access and analyze data stored in Azure
    Machine Learning datasets – programmatically via Python and your favorite numerical
    libraries or via the UI.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了多种访问和分析 Azure Machine Learning 数据集中存储的数据的方法——通过 Python 和你喜欢的数值库进行编程访问，或者通过
    UI。
- en: Tracking datasets in Azure Machine Learning
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Machine Learning 中的数据集跟踪
- en: End-to-end tracking of all assets that go into your final production model is
    essential for reproducibility and interpretability but also auditing and tracking.
    A machine learning model is a function that minimizes a loss function by iterating
    and sampling experiments from your training data. Therefore, the training data
    itself should be treated as being a part of the model itself, and hence should
    be managed, versioned, and tracked through the end-to-end machine learning process.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对最终生产模型中所有资产的端到端跟踪对于可重复性、可解释性、审计和跟踪都是必不可少的。机器学习模型是一个通过迭代和从你的训练数据中采样实验来最小化损失函数的函数。因此，训练数据本身应被视为模型的一部分，因此应通过端到端的机器学习过程进行管理、版本控制和跟踪。
- en: 'We want to take advantage of datasets to add data tracking to our experiments.
    A good way to understand the differences between data tracking capabilities is
    to look at two examples: first, loading a CSV dataset from a URL, and then loading
    the same data from the same URL but through a dataset abstraction in Azure Machine
    Learning. However, we don''t only want to load the data, but also pass it from
    the authoring script to the training script as an argument.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想利用数据集来为我们的实验添加数据跟踪。了解数据跟踪能力差异的一个好方法是通过两个示例：首先，从 URL 加载 CSV 数据集，然后通过 Azure
    Machine Learning 中的数据集抽象从同一 URL 加载数据。然而，我们不仅想加载数据，还想将其从编写脚本传递到训练脚本作为参数。
- en: We will first use `pandas` to load a CSV file directly from the URL and pass
    it to the training script as a URL. In the next step, we will enhance this method
    by using a direct dataset instead, allowing us to conveniently pass the dataset
    configuration to the training script and track the dataset for the experiment
    run in Azure Machine Learning.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用 `pandas` 直接从 URL 加载 CSV 文件，并将其作为 URL 传递给训练脚本。在下一步中，我们将通过使用直接的数据集来增强此方法，这样我们就可以方便地将数据集配置传递给训练脚本，并跟踪
    Azure Machine Learning 中实验运行的数据集。
- en: Passing external data as a URL
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将外部数据作为 URL 传递
- en: 'We start our example using data that is available as a CSV file from a remote
    URL, a common way to distribute public datasets. In the first example without
    Azure Machine Learning dataset tracking, we will use the `pandas` library to fetch
    and parse the CSV file:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以从远程 URL 可用的 CSV 文件数据开始我们的示例，这是分发公共数据集的常见方式。在第一个没有 Azure Machine Learning
    数据集跟踪的示例中，我们将使用 `pandas` 库来获取和解析 CSV 文件：
- en: 'Let''s get started with the first code snippet using pandas'' `read_csv()`
    method as an example to fetch data via a public URL from a remote server. However,
    this is just an example – you could replace it with any other method to fetch
    data from a remote location:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从使用 pandas 的 `read_csv()` 方法作为示例的第一个代码片段开始，通过公共 URL 从远程服务器获取数据。然而，这只是一个示例——你可以用任何其他方法从远程位置获取数据：
- en: '[PRE21]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our goal is to pass the data from the authoring script to the training script,
    so it can be tracked and updated easily in the future. To achieve this, we can''t
    send the DataFrame directly, but have to pass the URL to the CSV file and use
    the same method to fetch the data in the training script. Let''s write a small
    training script whose only job is to parse the command-line arguments and fetch
    the data from the URL:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目的是将数据从编写脚本传递到训练脚本，以便将来可以轻松跟踪和更新。为了实现这一点，我们不能直接传递 DataFrame，而必须传递 CSV 文件的
    URL 并在训练脚本中使用相同的方法获取数据。让我们编写一个小型训练脚本，其唯一任务是解析命令行参数并从 URL 获取数据：
- en: '**code/access_data_from_path.py**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**code/access_data_from_path.py**'
- en: '[PRE22]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we see in the preceding code, we pass the data path from the command-line
    `--input` argument and then load the data from the location using pandas' `read_csv()`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们通过命令行的 `--input` 参数传递数据路径，然后使用 pandas 的 `read_csv()` 从位置加载数据。
- en: 'Next, we create a `ScriptRunConfig` constructor to submit an experiment run
    to Azure Machine Learning that executes the training script from *step 1*. For
    now, we are not performing any training but only want to understand what data
    is passed between the authoring and execution runtime:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 `ScriptRunConfig` 构造函数，将实验运行提交给 Azure Machine Learning，以执行从 *步骤 1*
    开始的训练脚本。现在，我们不做任何训练，只想了解编写和执行运行时之间传递的数据：
- en: '**Access_data_from_path.ipynb**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**Access_data_from_path.ipynb**'
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s execute the run configuration to run the experiment and track the run
    details in Azure Machine Learning. Once the experiment run has finished, we navigate
    to Azure Machine Learning and check the details of this run. As we can see in
    *Figure 4.10*, Azure Machine Learning will track the `script` argument as expected
    but cannot associate the argument to a dataset:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们执行运行配置以运行实验并跟踪 Azure Machine Learning 中的运行详情。一旦实验运行完成，我们导航到 Azure Machine
    Learning 并检查这次运行的详细信息。正如我们在 *图 4.10* 中可以看到的，Azure Machine Learning 将按预期跟踪 `script`
    参数，但不能将参数关联到数据集：
- en: '![Figure 4.10 – Run details of the experiment ](img/B17928_04_10.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 实验运行详情](img/B17928_04_10.jpg)'
- en: Figure 4.10 – Run details of the experiment
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 实验运行详情
- en: 'Let''s summarize the downsides of this approach:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下这种方法的缺点：
- en: We can't pass the pandas DataFrame or a DataFrame identifier to the training
    script; we have to pass the data through the URL to its location. If the file
    path changes, we have to update the argument for the training script.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不能将 pandas DataFrame 或 DataFrame 标识符传递给训练脚本；我们必须通过 URL 将数据传递到其位置。如果文件路径发生变化，我们必须更新训练脚本的参数。
- en: The training script doesn't know that the input path refers to the input data
    for the training script, it's simply a string argument to the training script.
    While we can track the argument in Azure Machine Learning, we can't automatically
    track the data.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练脚本不知道输入路径指的是训练脚本的输入数据，它只是训练脚本的一个字符串参数。虽然我们可以在 Azure Machine Learning 中跟踪参数，但我们不能自动跟踪数据。
- en: Passing external data as a direct dataset
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将外部数据作为直接数据集传递
- en: 'As promised, we will now enhance the previous example using a dataset in Azure
    Machine Learning. This will allow us to pass the dataset as a named configuration
    – abstracting the URL and access to the physical location of the data. It also
    automatically enables dataset tracking for the experiment:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺，我们现在将使用 Azure Machine Learning 中的数据集增强先前的示例。这将允许我们将数据集作为命名配置传递——抽象 URL 和数据的物理位置访问。它还自动为实验启用数据集跟踪：
- en: 'We start in the authoring script, and load the data from the path – only this
    time, using Azure Machine Learning''s `TabularDataset`, created through the `from_delimited_files()`
    factory method:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从编写脚本开始，从路径加载数据 - 只不过这次，我们使用 Azure Machine Learning 的 `TabularDataset`，通过
    `from_delimited_files()` 工厂方法创建：
- en: '[PRE24]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will output the same set of rows as the previous example in pandas – so
    there is almost no difference other than using a different method to create the
    DataFrame. However, now that we have created a *direct dataset*, we can easily
    pass the dataset to the training script as a named dataset configuration – which
    will use the dataset ID under the hood.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 pandas 中输出与上一个示例相同的行集 - 所以除了使用不同的方法创建 DataFrame 之外，几乎没有区别。然而，现在我们已经创建了一个
    *直接数据集*，我们可以轻松地将数据集传递给训练脚本作为命名数据集配置 - 这将在底层使用数据集 ID。
- en: 'Like the pandas example, we write a simplified training script that will access
    the dataset and print the first few records by parsing the input dataset from
    the command-line arguments. In the training script, we can use the `Dataset.get_by_id()`
    method to fetch the dataset by its ID from a workspace:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与 pandas 示例类似，我们编写了一个简化的训练脚本，该脚本将访问数据集并通过解析命令行参数中的输入数据集来打印前几条记录。在训练脚本中，我们可以使用
    `Dataset.get_by_id()` 方法通过其 ID 从工作区获取数据集：
- en: '**code/access_data_from_dataset.py**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**code/access_data_from_dataset.py**'
- en: '[PRE25]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see in the preceding code, we modified the previous code slightly
    and added code to retrieve the current run context, experiment, and the workspace.
    This lets us access the direct dataset from the workspace by passing the dataset
    ID to the `Dataset.get_by_id()` method.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们稍微修改了之前的代码，并添加了代码来检索当前的运行上下文、实验和工作区。这使得我们可以通过将数据集 ID 传递给 `Dataset.get_by_id()`
    方法从工作区访问直接数据集。
- en: Next, we write a run configuration to submit the preceding code as an experiment
    to Azure Machine Learning. First, we need to convert the dataset into a command-line
    argument and pass it to the training script so it can be automatically retrieved
    in the execution runtime. We can achieve this by using the `as_named_input(name)`
    method on the dataset instance, which will convert the dataset into a named `DatasetConsumptionConfig`
    argument that allows the dataset to be passed to other environments.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个运行配置，将前面的代码作为实验提交给 Azure Machine Learning。首先，我们需要将数据集转换为命令行参数，并将其传递给训练脚本，以便在执行运行时自动检索。我们可以通过在数据集实例上使用
    `as_named_input(name)` 方法来实现这一点，这将数据集转换为命名的 `DatasetConsumptionConfig` 参数，允许数据集传递到其他环境。
- en: In this case, the dataset will be passed in direct mode and provided as the
    `name` environment variable in the runtime environment or as the dataset ID in
    the command-line arguments. The dataset will also get tracked in Azure Machine
    Learning as an input argument to the training script.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据集将以直接模式传递，并在运行时环境中作为 `name` 环境变量提供，或在命令行参数中作为数据集 ID。数据集也将作为训练脚本的输入参数在
    Azure Machine Learning 中进行跟踪。
- en: 'However, as we saw in the previous code snippet, we use the `Dataset.get_by_id()`
    method to retrieve the dataset in the training script from the dataset ID. We
    don''t need to manually create or access the dataset ID, because the `DatasetConsumptionConfig`
    argument will be automatically expanded into the dataset ID when the training
    script is called by Azure Machine Learning with a direct dataset:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如前述代码片段所示，我们在训练脚本中使用 `Dataset.get_by_id()` 方法从数据集 ID 获取数据集。我们不需要手动创建或访问数据集
    ID，因为当 Azure Machine Learning 使用直接数据集调用训练脚本时，`DatasetConsumptionConfig` 参数将自动展开为数据集
    ID。
- en: '**Access_data_from_dataset.ipynb**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**Access_data_from_dataset.ipynb**'
- en: '[PRE26]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we can see in the preceding code, the dataset is converted to a configuration
    that can simply be passed to the training script through the `as_named_input(name)`
    method. If we submit the experiment and check the logs of the run, we can see
    that Azure Machine Learning passed the dataset ID to the training script:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，数据集被转换为可以通过 `as_named_input(name)` 方法简单传递给训练脚本的配置。如果我们提交实验并检查运行日志，我们可以看到
    Azure Machine Learning 将数据集 ID 传递给了训练脚本：
- en: 70_driver_log.txt
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 70_driver_log.txt
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The run details for this experiment are shown in *Figure 4.11*. If you look
    at the input arguments, you can see that we passed the `DatasetConsumptionConfig`
    object to the script, which was then converted automatically to the dataset ID.
    Not only is the input argument passed without any information about the location
    of the underlying data, but the input dataset is also recognized as an input to
    the training data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 该实验的运行详情如图 *图 4.11* 所示。如果您查看输入参数，您可以看到我们传递了 `DatasetConsumptionConfig` 对象给脚本，该对象随后自动转换为数据集
    ID。不仅输入参数传递时没有关于底层数据位置的任何信息，输入数据集也被识别为训练数据的输入：
- en: '![Figure 4.11 – Run details of the experiment ](img/B17928_04_11.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 实验运行详情](img/B17928_04_11.jpg)'
- en: Figure 4.11 – Run details of the experiment
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 实验运行详情
- en: 'By passing a dataset to a training script, Azure Machine Learning automatically
    tracks the dataset with the experiment run. As you can see in *Figure 4.11*, the
    dataset ID is a link to the tracked dataset. When clicking on the dataset ID in
    Azure Machine Learning, it will open a page showing details about the tracked
    dataset, such as description, URL, size, and type of dataset, as shown in *Figure
    4.12*. Like registered datasets, you can also explore the raw data and look at
    dataset column statistics – called the profile – or see any registered models
    derived from this data. Tracked datasets can easily be registered – and hence
    versioned and managed – by clicking on the **Register** action or from code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据集传递给训练脚本，Azure Machine Learning 会自动跟踪实验运行中的数据集。如图 *图 4.11* 所示，数据集 ID 是追踪数据集的链接。当在
    Azure Machine Learning 中点击数据集 ID 时，它将打开一个页面，显示追踪数据集的详细信息，例如描述、URL、大小和数据集类型，如图
    *图 4.12* 所示。与已注册的数据集一样，您也可以探索原始数据，查看数据集列统计信息——称为概要——或查看从这些数据派生的任何已注册模型。通过点击 **注册**
    操作或从代码中，可以轻松地将追踪数据集注册——从而进行版本控制和管理工作：
- en: '![Figure 4.12 – Direct dataset tracked in Azure Machine Learning ](img/B17928_04_12.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – Azure Machine Learning 中追踪的直接数据集](img/B17928_04_12.jpg)'
- en: Figure 4.12 – Direct dataset tracked in Azure Machine Learning
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – Azure Machine Learning 中追踪的直接数据集
- en: As we saw in this section, there are important benefits to passing the input
    data to your training script as a dataset argument. This will automatically track
    the dataset in your workspace and connect the dataset with the experimentation
    run.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节所示，将输入数据作为数据集参数传递给训练脚本有许多重要好处。这将自动跟踪您工作区中的数据集，并将数据集与实验运行连接起来。
- en: In the code snippets of this section, we passed the data as a *direct dataset*,
    which means that the training script has to fetch the data again from the external
    URL. This is not always optimal, especially when dealing with large amounts of
    data or when data should be managed in Azure Machine Learning. In the next section,
    we will explore different ways to pass data to the training script.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的代码片段中，我们以 *直接数据集* 的形式传递了数据，这意味着训练脚本必须再次从外部 URL 获取数据。这并不总是最优的，尤其是在处理大量数据或数据应在
    Azure Machine Learning 中管理时。在下一节中，我们将探讨将数据传递给训练脚本的不同方法。
- en: Accessing data during training
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练过程中的数据访问
- en: In the previous section, we implicitly passed the URL of the original dataset
    to the training script. While this is a practical and fast solution for small
    public datasets, it's often not the preferred approach for private or larger datasets.
    Imagine your data is stored on a SQL server, Blob storage, or file share instead,
    and password protected. Imagine your dataset contains many gigabytes of files.
    In this section, we will see techniques that work well for both cases.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们隐式地将原始数据集的 URL 传递给训练脚本。虽然这对于小型公共数据集来说是一个实用且快速的方法，但对于私有或更大的数据集来说，通常不是首选的方法。想象一下，您的数据存储在
    SQL 服务器、Blob 存储或文件共享上，并且受密码保护。想象一下，您的数据集包含许多千兆字节的文件。在本节中，我们将看到适用于这两种情况的技术。
- en: While external public data reachable through a URL is created and passed as
    a *direct dataset*, all other datasets can be accessed either as a **download**
    or as a **mount**. For big data datasets, Azure Machine Learning also provides
    an option to mount a dataset as a **Hadoop Distributed File System** (**HDFS**).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过 URL 可达的外部公共数据以 *直接数据集* 的形式创建并传递时，所有其他数据集都可以通过 **下载** 或 **挂载** 的方式访问。对于大数据数据集，Azure
    Machine Learning 还提供了一个将数据集挂载为 **Hadoop 分布式文件系统**（**HDFS**）的选项。
- en: 'In this section, we will see authoring scripts that will pass datasets both
    as a download and as a mount. Let''s first create a reference in the authoring
    script to the `cifar10` dataset, which we registered in the previous section.
    The following snippet retrieves a dataset by name from the Azure Machine Learning
    workspace:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到创作脚本，这些脚本将以下载和挂载的方式传递数据集。让我们首先在创作脚本中创建对 `cifar10` 数据集的引用，该数据集我们在上一节中注册过。以下代码片段从
    Azure Machine Learning 工作区通过名称检索数据集：
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, we want to pass the dataset to the training script so that we can access
    the training data from the script. The benefit of using datasets is not only tracking
    but the fact that we can simply choose the appropriate data consumption configuration
    that is appropriate for each dataset. It will also help us to separate the training
    script from the training data, making it easy to pass new, updated, or enriched
    data to the same training script without needing to update the training script.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望将数据集传递给训练脚本，以便我们可以从脚本中访问训练数据。使用数据集的好处不仅在于跟踪，还在于我们可以简单地选择适合每个数据集的适当数据消费配置。它还将帮助我们分离训练脚本和训练数据，使得将新数据、更新数据或增强数据传递给相同的训练脚本变得容易，而无需更新训练脚本。
- en: Independently of the consumption method, the training script can always load
    the data from a directory path where it will be either downloaded or mounted.
    Under the hood, Azure Machine Learning inspects the command-line arguments of
    `ScriptRunConfig`, detects the dataset reference, delivers the data to the compute
    environment, and replaces the argument with the path of the dataset in the local
    filesystem.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是哪种消费方式，训练脚本都可以始终从目录路径加载数据，该路径是数据下载或挂载的位置。在底层，Azure Machine Learning 会检查 `ScriptRunConfig`
    的命令行参数，检测数据集引用，将数据传递到计算环境，并用本地文件系统中数据集的路径替换参数。
- en: 'Azure Machine Learning uses parameter expansion to replace the dataset reference
    with the path to the actual data on disk. To make this more obvious, we will write
    a single training file that will simply list all training files that were passed
    to it. The following code snippet implements this training script:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning 使用参数扩展将数据集引用替换为磁盘上实际数据的路径。为了使这一点更加明显，我们将编写一个单独的训练文件，该文件将简单地列出传递给它的所有训练文件。以下代码片段实现了这个训练脚本：
- en: code/access_dataset.py
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: code/access_dataset.py
- en: '[PRE29]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the previous script, we define a single `--input` argument that we will use
    to pass the training data. Then we will output this argument and list all files
    from the directory. We will use this script to pass data with different mounting
    techniques and will see that the data will always be available in the folder.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的脚本中，我们定义了一个单一的 `--input` 参数，我们将使用它来传递训练数据。然后我们将输出此参数并列出目录中的所有文件。我们将使用此脚本通过不同的挂载技术传递数据，并将看到数据始终可用在文件夹中。
- en: Having the dataset reference and a simple training script, we can now look at
    a different `ScriptRunConfig` to pass the `cifar10` dataset using the different
    data consumption configurations. While the code is downloaded or mounted by Azure
    Machine Learning before the training script is invoked, we will also explore what
    happens under the hood – so we can apply the same technique to load the training
    data outside of Azure Machine Learning-managed compute environments.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有数据集引用和简单的训练脚本之后，我们现在可以查看不同的 `ScriptRunConfig`，使用不同的数据消费配置传递 `cifar10` 数据集。虽然代码在调用训练脚本之前由
    Azure Machine Learning 下载或挂载，但我们将探索底层发生了什么——这样我们就可以将相同的技巧应用于在 Azure Machine Learning
    管理的计算环境之外加载训练数据。
- en: Accessing data as a download
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以下载方式访问数据
- en: 'We will first look at downloading the data to the training instance. To do
    so, we will first create a `ScriptRunConfig` constructor in the authoring environment
    where we pass the data to `as_download()`. We will schedule a code snippet that
    will access and output the files passed to the script:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看将数据下载到训练实例的过程。为此，我们将在创作环境中创建一个 `ScriptRunConfig` 构造函数，并将数据传递给 `as_download()`。我们将安排一个代码片段，该片段将访问并输出传递给脚本的文件：
- en: Access_dataset_as_download.ipynb
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Access_dataset_as_download.ipynb
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Azure will interpolate the dataset passed by the `input` parameter and replace
    it with the location of the dataset on disk. The data will be automatically downloaded
    to the training environment if the dataset is passed with the `Dataset.as_download()`
    method.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 会将 `input` 参数传递的数据集进行插值，并用磁盘上数据集的位置替换它。如果数据集是通过 `Dataset.as_download()`
    方法传递的，数据将自动下载到训练环境中。
- en: 'If you run this script configuration, the `access_dataset.py` script will output
    the temporary location of the dataset, which was automatically downloaded to disk.
    You can replicate the exact same process in your authoring environment that Azure
    Machine Learning does under the hood. To do so, you can simply call the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行此脚本配置，`access_dataset.py`脚本将输出数据集的临时位置，该数据集已自动下载到磁盘。你可以在你的创作环境中复制Azure
    Machine Learning在内部所执行的确切过程。为此，你可以简单地调用以下代码：
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Passing data as a download is convenient for small datasets or when using a
    large number of consumers that require a high throughput on the data. However,
    if you are dealing with large datasets, you can also pass them as a *mount* instead.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下载方式传递数据对于小型数据集或使用大量消费者且对数据有高吞吐量需求的情况来说很方便。然而，如果你处理的是大型数据集，你也可以将它们作为*挂载*传递。
- en: Accessing data as a mount
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以挂载方式访问数据
- en: 'In this example, we will mount the data on the training environment. To do
    so, we will again create a `ScriptRunConfig` constructor in the authoring environment
    and this time we invoke the `as_mount()`. We will schedule a code snippet that
    will access and output the files passed to the script:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将数据挂载到训练环境中。为此，我们将在创作环境中再次创建一个`ScriptRunConfig`构造函数，这次我们将调用`as_mount()`。我们将安排一个代码片段，该片段将访问并输出传递给脚本的文件：
- en: Access_dataset_as_mount.ipynb
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Access_dataset_as_mount.ipynb
- en: '[PRE32]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, the preceding example is very similar to the previous example
    where data was downloaded to disk. In fact, we are reusing the exact same scheduled
    script, `access_dataset.py`, which will output the location of the data on disk.
    However, in this example, the data is not downloaded to this location but mounted
    to the file path.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，前面的示例与之前将数据下载到磁盘的示例非常相似。事实上，我们正在重用完全相同的计划脚本`access_dataset.py`，该脚本将输出数据在磁盘上的位置。然而，在这个示例中，数据并没有下载到这个位置，而是挂载到文件路径。
- en: 'Azure Machine Learning will interpolate the dataset passed through the input
    argument with the mounted path on disk. Similar to the previous example, you can
    replicate what happens under the hood in Azure Machine Learning and mount the
    data from within your authoring environment:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning会将通过输入参数传递的数据集与磁盘上的挂载路径进行插值。类似于上一个示例，你可以在Azure Machine
    Learning内部复制所发生的事情，并在你的创作环境中挂载数据：
- en: '[PRE33]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can see in the previous snippet, the dataset is mounted and released
    using the mount context''s `start` and `stop` methods. You can also simplify the
    code snippet using Python''s `with` statement to automatically mount and unmount
    the data as shown in the following snippet:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从前面的代码片段中看到的，数据集是通过挂载上下文的`start`和`stop`方法挂载和释放的。你还可以使用Python的`with`语句简化代码片段，自动挂载和卸载数据，如下面的代码片段所示：
- en: '[PRE34]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Hence, depending on the use case, we have different options to pass a dataset
    reference to a scheduled script. Independent of the data transport, Azure Machine
    Learning will implement the correct method under the hood and interpolate the
    input arguments so that the training script doesn't need to know how a dataset
    was configured. For the executed script, the data is simply made available through
    a path in the filesystem.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据用例的不同，我们有不同的选项将数据集引用传递给计划中的脚本。独立于数据传输，Azure Machine Learning将在内部实现正确的方法，并插值输入参数，这样训练脚本就不需要知道数据集是如何配置的。对于执行的脚本，数据通过文件系统中的路径简单地提供。
- en: Using external datasets with open datasets
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用外部数据集与公开数据集
- en: One of the most effective methods to improve the prediction performance of any
    ML model is to add additional information to your training data. A common way
    to achieve this is by joining external datasets to the training data. A good indication
    to join external data is the availability of popular joining keys in your dataset,
    such as dates, locations, countries, and more.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 提高任何机器学习模型预测性能的最有效方法之一是向你的训练数据中添加额外的信息。实现这一目标的一种常见方式是将外部数据集与训练数据合并。一个很好的迹象是，在你的数据集中存在流行的合并键，例如日期、位置、国家等。
- en: When you work with transactional data that contains dates, you can easily join
    external data to create additional features for the training dataset and hence
    improve prediction performance. Common derived features for dates are weekdays,
    weekends, time to or since weekends, holidays, time to or since holidays, sports
    events, concerts, and more. When dealing with country information, you can often
    join additional country-specific data, such as population data, economic data,
    sociological data, health data, labor data, and more. When dealing with geolocation,
    you can join distance to points of interest, weather data, traffic data, and more.
    Each of these additional datasets gives you additional insights and hence can
    boost your model's performance significantly.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 当您处理包含日期的交易数据时，您可以轻松地将外部数据连接起来，为训练数据集创建额外的特征，从而提高预测性能。常见的日期衍生特征包括工作日、周末、到或自周末的时间、假日、到或自假日的时间、体育赛事、音乐会等。当处理国家信息时，您通常可以连接额外的特定国家数据，例如人口数据、经济数据、社会学数据、健康数据、劳动数据等。当处理地理位置时，您可以连接到兴趣点的距离、天气数据、交通数据等。每个额外的数据集都为您提供了额外的见解，因此可以显著提升模型性能。
- en: Open Datasets is a service that provides access to curated datasets for the
    transportation, health and genomics, labor and economics, population, and safety,
    categories and common datasets that you can use to boost your model's performance.
    Let's look into three examples.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 开放数据集是一个提供访问精选数据集的服务，这些数据集可用于交通、健康和基因组学、劳动和经济、人口以及安全等类别和常用数据集，您可以使用这些数据集来提升模型性能。让我们看看三个例子。
- en: Important Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before using a specific dataset for a commercial service, please make sure that
    your application is covered by the license. If in doubt, reach out to Microsoft
    via [aod@microsoft.com](mailto:aod@microsoft.com).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用特定数据集为商业服务之前，请确保您的应用程序受许可证覆盖。如有疑问，请联系微软的[aod@microsoft.com](mailto:aod@microsoft.com)。
- en: 'In the first example, we will investigate the dataset for *worldwide public
    holidays*. The data covers holidays in almost 40 countries or regions from 1970
    to 2099\. It is curated from Wikipedia and the `holidays` Python package. You
    can import them into your environment and access these holidays using the `opendatasets`
    library as shown in the following example:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，我们将研究*全球公共假日*的数据集。数据涵盖了1970年至2099年近40个国家和地区或地区的假日。这些数据来自维基百科和`holidays`
    Python包。您可以将它们导入到您的环境中，并使用以下示例中的`opendatasets`库访问这些假日：
- en: '[PRE35]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we see in the code, we can access the dataset from the `azureml-opendatasets`
    package and use it as an Azure Machine Learning dataset. This means we can return
    the pandas or Spark DataFrame for further processing.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在代码中看到的，我们可以从`azureml-opendatasets`包中访问数据集，并将其用作Azure Machine Learning数据集。这意味着我们可以返回pandas或Spark
    DataFrame以进行进一步处理。
- en: 'Another popular dataset is the *US population* by county for the years 2000
    and 2010\. It is broken down by gender and race and sourced from the United States
    Census Bureau:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的数据集是按县划分的*2000年和2010年美国人口*。它按性别和种族细分，并来源于美国人口普查局：
- en: '[PRE36]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Another example open dataset is the *Current Employment Statistics* of the
    United States, published by the US **Bureau of Labor Statistics** (**BLS**). It
    contains estimates of employment, hours, and earnings of workers on payrolls in
    the US:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个公开数据集的例子是美国**劳工统计局**（**BLS**）发布的*当前就业统计*。它包含了美国工资单上工人的就业、小时数和收入估计：
- en: '[PRE37]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you saw in this section, Azure Open Datasets gives you a convenient option
    to access curated datasets in the form of Azure Machine Learning datasets right
    from within your Azure Machine Learning workspace. While the number of available
    datasets is still manageable, you can expect the number of available datasets
    to grow over time.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本节中看到的，Azure Open Datasets为您提供了一个方便的选项，可以直接在您的Azure Machine Learning工作区中访问以Azure
    Machine Learning数据集形式提供的精选数据集。虽然可用的数据集数量仍然可控，但您可以预期可用的数据集数量会随着时间的推移而增长。
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to manage data in Azure Machine Learning using
    datastores and datasets. We saw how to configure the default datastore that is
    responsible for storing all assets, logs, models, and more in Azure Machine Learning,
    as well as other services that can be used as datastores for different types of
    data.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用数据存储和数据集在Azure Machine Learning中管理数据。我们看到了如何配置负责在Azure Machine
    Learning中存储所有资产、日志、模型等默认数据存储，以及可用于不同类型数据的其他服务。
- en: After creating an Azure Blob storage account and configuring it as a datastore
    in Azure Machine Learning, we saw different tools to ingest data into Azure, such
    as Azure Storage Explorer, Azure CLI, and AzCopy, as well as services optimized
    for data ingestion and transformation, Azure Data Factory and Azure Synapse Spark.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 Azure Blob 存储账户并将其配置为 Azure Machine Learning 中的数据存储后，我们看到了将数据导入 Azure 的不同工具，例如
    Azure Storage Explorer、Azure CLI 和 AzCopy，以及针对数据导入和转换优化的服务，如 Azure Data Factory
    和 Azure Synapse Spark。
- en: In the subsequent section, we got our hands on datasets. We created file and
    tabular datasets and learned about direct and registered datasets. Datasets can
    be passed as a download or a mount to executed scripts, which will automatically
    track datasets in Azure Machine Learning.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的部分，我们实际操作了数据集。我们创建了文件和表格数据集，并了解了直接和注册数据集。数据集可以作为下载或挂载传递给执行脚本，这将自动跟踪 Azure
    Machine Learning 中的数据集。
- en: Finally, we learned how to improve predication performance by joining third-party
    datasets from Azure Open Datasets to our machine learning process. In the next
    chapter, we will learn how to explore data by performing data analysis and visualization.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何通过将 Azure Open Datasets 的第三方数据集加入我们的机器学习流程中来提高预测性能。在下一章中，我们将学习如何通过执行数据分析与可视化来探索数据。
