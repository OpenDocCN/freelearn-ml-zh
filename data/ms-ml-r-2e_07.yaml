- en: Neural Networks and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习
- en: '"Forget artificial intelligence - in the brave new world of big data, it''s
    artificial idiocy we should be looking out for."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “忘记人工智能吧 - 在大数据的勇敢新世界中，我们应该警惕的是人工愚蠢。”
- en: '- Tom Chatfield'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 托马斯·查特菲尔德'
- en: I recall that at some meeting circa mid-2012, I was part of a group discussing
    the results of some analysis or other, when one of the people around the table
    sounded off with a hint of exasperation mixed with a tinge of fright, *this* *isn't*
    *one* *of* *those* *neural* *networks,* *is* *it?* I knew of his past run-ins
    with and deep-seated anxiety about neural networks, so I assuaged his fears making
    some sarcastic comment that neural networks have basically gone the way of the
    dinosaur. No one disagreed! Several months later, I was gobsmacked when I attended
    a local meeting where the discussion focused on, of all things, neural networks
    and this mysterious deep learning. Machine learning pioneers such as Ng, Hinton,
    Salakhutdinov, and Bengio have revived neural networks and improved their performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得在2012年中期左右的一次会议上，我参与了一个小组讨论某些分析的结果，当时桌子旁的一位人士带着一丝恼怒和恐惧的语气说，“这”**不是**“那种”**神经****网络**，“是吗？”我知道他过去与神经网络有过冲突，并且对此有深深的焦虑，所以我用一些讽刺的话安慰他，说神经网络基本上已经走向了恐龙的道路。没有人反对！几个月后，当我参加一个当地会议时，我被震惊了，因为讨论的重点竟然是神经网络和神秘的深度学习。像Ng、Hinton、Salakhutdinov和Bengio这样的机器学习先驱们使神经网络复兴，并提高了它们的性能。
- en: Much media hype revolves around these methods with high-tech companies such
    as Facebook, Google, and Netflix investing tens, if not hundreds, of millions
    of dollars. The methods have yielded promising results in voice recognition, image
    recognition, and automation. If self-driving cars ever stop running off the road
    and into each other, it will certainly be from the methods discussed here.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的媒体炒作围绕着这些方法，高科技公司如Facebook、Google和Netflix投入了数亿美元，甚至可能更多。这些方法在语音识别、图像识别和自动化方面取得了有希望的结果。如果自动驾驶汽车不再偏离道路相互碰撞，那肯定是因为这里讨论的方法。
- en: In this chapter, we will discuss how the methods work, their benefits, and inherent
    drawbacks so that you can become conversationally competent about them. We will
    work through a practical business application of a neural network. Finally, we
    will apply the deep learning methodology in a cloud-based application.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论这些方法的工作原理、它们的优点和固有的缺点，以便你能够就它们进行交谈。我们将通过一个神经网络的实际商业应用来操作。最后，我们将在一个基于云的应用中应用深度学习方法。
- en: Introduction to neural networks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Neural network is a fairly broad term that covers a number of related methods,
    but in our case, we will focus on a **feed forward** network that trains with
    **backpropagation**. I'm not going to waste our time discussing how the machine
    learning methodology is similar or dissimilar to how a biological brain works.
    We only need to start with a working definition of what a neural network is. I
    think the Wikipedia entry is a good start.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一个相当宽泛的术语，涵盖了多种相关方法，但就我们而言，我们将专注于一种**前馈**网络，它通过**反向传播**进行训练。我不会浪费我们的时间讨论机器学习方法论与生物大脑工作方式的相似之处或不同之处。我们只需要从神经网络是什么的工作定义开始。我认为维基百科的条目是一个很好的起点。
- en: In machine learning and cognitive science, **Artificial neural networks** (**ANNs**)
    are a family of statistical learning models inspired by biological neural networks
    (the central nervous systems of animals, in particular, the brain) and are used
    to estimate or approximate functions that can depend on a large number of inputs
    and are generally unknown. [https://en.wikipedia.org/wiki/Artificial_neural_network](https://en.wikipedia.org/wiki/Artificial_neural_network)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和认知科学中，**人工神经网络**（**ANNs**）是一系列受生物神经网络（特别是动物的中央神经系统，尤其是大脑）启发的统计学习模型，用于估计或近似依赖于大量输入且通常未知的功能。[https://zh.wikipedia.org/wiki/人工神经网络](https://zh.wikipedia.org/wiki/人工神经网络)
- en: The motivation or benefit of ANNs is that they allow the modeling of highly
    complex relationships between inputs/features and response variable(s), especially
    if the relationships are highly nonlinear. No underlying assumptions are required
    to create and evaluate the model, and it can be used with qualitative and quantitative
    responses. If this is the yin, then the yang is the common criticism that the
    results are black box, which means that there is no equation with the coefficients
    to examine and share with the business partners. In fact, the results are almost
    not interpretable. The other criticisms revolve around how results can differ
    by just changing the initial random inputs and that training ANNs is computationally
    expensive and time-consuming.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的动机或好处是，它们允许对输入/特征和响应变量之间的复杂关系进行建模，尤其是如果这些关系高度非线性。创建和评估模型不需要任何潜在假设，并且它可以用于定性和定量响应。如果这是阴，那么阳就是常见的批评，即结果是黑盒，这意味着没有方程式可以检查和与商业伙伴分享。事实上，结果几乎不可解释。其他批评围绕着结果如何仅通过改变初始随机输入而有所不同，以及训练神经网络计算量大且耗时。
- en: The mathematics behind ANNs is not trivial by any measure. However, it is crucial
    to at least get a working understanding of what is happening. A good way to intuitively
    develop this understanding is to start a diagram of a simplistic neural network.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的数学背景不是微不足道的。然而，至少要了解正在发生的事情是至关重要的。一种直观地发展这种理解的好方法是开始一个简单神经网络的图表。
- en: 'In this simple network, the inputs or covariates consist of two nodes or neurons.
    The neuron labeled **1** represents a constant or more appropriately, the intercept.
    **X1** represents a quantitative variable. The **W**''s represent the weights
    that are multiplied by the input node values. These values become **Input Nodes**
    to **Hidden Node**. You can have multiple hidden nodes, but the principal of what
    happens in just this one is the same. In the hidden node, **H1**, the *weight
    * value* computations are summed. As the intercept is notated as **1**, then that
    input value is simply the weight, **W1**. Now the magic happens. The summed value
    is then transformed with the **Activation** function, turning the input signal
    to an output signal. In this example, as it is the only **Hidden Node**, it is
    multiplied by **W3** and becomes the estimate of **Y**, our response. This is
    the feed-forward portion of the algorithm:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的网络中，输入或协变量由两个节点或神经元组成。标记为**1**的神经元代表常数，或者更合适地说，是截距。**X1**代表一个定量变量。**W**代表乘以输入节点值的权重。这些值成为**输入节点**到**隐藏节点**的输入。你可以有多个隐藏节点，但这里只发生的事情的基本原则是相同的。在隐藏节点**H1**中，*权重*值计算被求和。由于截距表示为**1**，那么这个输入值仅仅是权重，**W1**。现在发生魔法。求和的值随后通过**激活**函数进行转换，将输入信号转换为输出信号。在这个例子中，因为它只有一个**隐藏节点**，所以它乘以**W3**，成为**Y**的估计，我们的响应。这是算法的前馈部分：
- en: '![](img/image_07_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_001.jpg)'
- en: 'But wait, there''s more! To complete the cycle or epoch, as it is known, backpropagation
    happens and trains the model based on what was learned. To initiate the backpropagation,
    an error is determined based on a loss function such as **Sum of Squared Error**
    or **Cross-Entropy**, among others. As the weights, **W1** and **W2**, were set
    to some initial random values between *[-1, 1]*, the initial error may be high.
    Working backward, the weights are changed to minimize the error in the loss function.
    The following diagram portrays the backpropagation portion:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等，还有更多！正如所知，为了完成循环或时代，会发生反向传播，并根据所学内容训练模型。为了启动反向传播，会根据损失函数（如**均方误差**或**交叉熵**等）确定一个错误。由于权重**W1**和**W2**被设置为*[-1,
    1]*之间的某些初始随机值，初始错误可能很高。反向工作，权重被改变以最小化损失函数中的错误。以下图表描绘了反向传播部分：
- en: '![](img/image_07_002.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_002.jpg)'
- en: This completes one epoch. This process continues, using gradient descent (discussed
    in [Chapter 5](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml), *More Classification
    Techniques - K-Nearest Neighbors and Support Vector Machines*) until the algorithm
    converges to the minimum error or prespecified number of epochs. If we assume
    that our activation function is simply linear, in this example, we would end up
    with *Y = W3(W1(1) + W2(X1))*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了一个时代。这个过程会继续进行，使用梯度下降（在第5章中讨论，*更多分类技术 - K最近邻和支持向量机*），直到算法收敛到最小误差或预指定的时代数。如果我们假设我们的激活函数仅仅是线性的，在这个例子中，我们最终会得到*Y
    = W3(W1(1) + W2(X1))*。
- en: The networks can get complicated if you add numerous input neurons, multiple
    neurons in a hidden node, and even multiple hidden nodes. It is important to note
    that the output from a neuron is connected to all the subsequent neurons and has
    weights assigned to all these connections. This greatly increases the model complexity.
    Adding hidden nodes and increasing the number of neurons in the hidden nodes has
    not improved the performance of ANNs as we had hoped. Thus, the development of
    deep learning occurs, which in part relaxes the requirement of all these neuron
    connections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果添加了大量的输入神经元、隐藏节点中的多个神经元，甚至多个隐藏节点，网络可能会变得复杂。需要注意的是，从神经元输出的连接到所有后续神经元的连接都分配了权重。这大大增加了模型复杂性。增加隐藏节点和在隐藏节点中增加神经元数量并没有像我们希望的那样提高人工神经网络的性能。因此，深度学习的发展发生了，这在一定程度上放宽了所有这些神经元连接的要求。
- en: There are a number of activation functions that one can use/try, including a
    simple linear function, or for a classification problem, the `sigmoid` function,
    which is a special case of the logistic function ([Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml),
    *Logistic Regression and Discriminant Analysis*). Other common activation functions
    are `Rectifier`, `Maxout`, and **hyperbolic tangent** (**tanh**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多激活函数可以使用/尝试，包括一个简单的线性函数，或者对于分类问题，使用`sigmoid`函数，它是逻辑函数的特殊情况（[第3章](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml)，*逻辑回归和判别分析*）。其他常见的激活函数包括`Rectifier`、`Maxout`和**双曲正切**（**tanh**）。
- en: 'We can plot a `sigmoid` function in R, first creating an `R` function in order
    to calculate the `sigmoid` function values:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在R中绘制`sigmoid`函数，首先创建一个`R`函数来计算`sigmoid`函数的值：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, it is a simple matter to plot the function over a range of values, say
    `-5` to `5`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在一系列值范围内绘制函数就变得简单了，比如从`-5`到`5`：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_07_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_01.png)'
- en: 'The `tanh` function (hyperbolic tangent) is a rescaling of the logistic `sigmoid`
    with the output between **-1** and **1**. The `tanh` function relates to `sigmoid`
    as follows, where **x** is the `sigmoid` function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`tanh`函数（双曲正切）是对逻辑`sigmoid`的缩放，输出值在**-1**和**1**之间。`tanh`函数与`sigmoid`的关系如下，其中**x**是`sigmoid`函数：'
- en: '*tanh(x)* *=* *2* *** *sigmoid(2x)* *-* *1*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*tanh(x)* *=* *2* *** *sigmoid(2x)* *-* *1*'
- en: 'Let''s plot the `tanh` and `sigmoid` functions for comparison purposes. Let''s
    also use `ggplot`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，让我们绘制`tanh`和`sigmoid`函数。同时，我们也使用`ggplot`：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_07_02-1.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_02-1.png)'
- en: So why use the `tanh` function versus `sigmoid`? It seems there are many opinions
    on the subject; is `tanh` popular in neural networks? In short, assuming you have
    scaled data with mean 0 and variance 1, the `tanh` function permits weights that
    are on average close to zero (zero-centered). This helps in avoiding bias and
    improves convergence. Think about the implications of always having positive weights
    from an output neuron to an input neuron as in a `sigmoid` function activation.
    During backpropagation, the weights will become either all positive or all negative
    between layers. This may cause performance issues. Also, since the gradient at
    the tails of a `sigmoid` (0 and 1) are almost zero, during backpropagation it
    can happen that almost no signal will flow between neurons of different layers.
     A full discussion of the issue is available, LeCun (1998). Keep in mind it is
    not a foregone conclusion that `tanh` is always better.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么使用`tanh`函数而不是`sigmoid`函数呢？关于这个问题似乎有很多意见；`tanh`在神经网络中是否流行？简而言之，假设你有一个均值为0，方差为1的缩放数据，`tanh`函数允许权重平均接近零（零中心化）。这有助于避免偏差并提高收敛性。想想在`sigmoid`函数激活中，从输出神经元到输入神经元始终有正权重的含义。在反向传播过程中，权重将在层之间变为全部为正或全部为负。这可能会导致性能问题。此外，由于`sigmoid`在尾部（0和1）的梯度几乎为零，在反向传播过程中，可能会发生几乎没有任何信号在不同层神经元之间流动的情况。关于这个问题的全面讨论，参见LeCun（1998）。记住，`tanh`总是更好的结论并不是必然的。
- en: This all sounds fascinating, but the ANN almost went the way of disco as it
    just did not perform as well as advertised, especially when trying to use deep
    networks with many hidden layers and neurons. It seems that a slow yet gradual
    revival came about with the seminal paper by Hinton and Salakhutdinov (2006) in
    the reformulated and, dare I say, rebranded neural network, deep learning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都听起来非常吸引人，但人工神经网络（ANN）几乎步入了迪斯科的行列，因为它并没有像宣传的那样表现出色，尤其是在尝试使用具有许多隐藏层和神经元的深度网络时。似乎随着Hinton和Salakhutdinov（2006年）发表的开创性论文，神经网络经过重新定义，甚至可以说是重新命名，深度学习才逐渐复兴。
- en: Deep learning, a not-so-deep overview
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习，一个不太深入的概述
- en: 'So, what is this deep learning that is grabbing our attention and headlines?
    Let''s turn to Wikipedia again for a working definition: *Deep learning is a branch
    of machine learning based on a set of algorithms that attempt to model high-level
    abstractions in data by using model architectures, with complex structures or
    otherwise, composed of multiple nonlinear transformations*. That sounds as if
    a lawyer wrote it. The characteristics of deep learning are that it is based on
    ANNs where the machine learning techniques, primarily unsupervised learning, are
    used to create new features from the input variables. We will dig into some unsupervised
    learning techniques in the next couple of chapters, but one can think of it as
    finding structure in data where no response variable is available. A simple way
    to think of it is the **Periodic Table of Elements**, which is a classic case
    of finding structure where no response is specified. Pull up this table online
    and you will see that it is organized based on atomic structure, with metals on
    one side and non-metals on the other. It was created based on latent classification/structure.
    This identification of latent structure/hierarchy is what separates deep learning
    from your run-of-the-mill ANN. Deep learning sort of addresses the question whether
    there is an algorithm that better represents the outcome than just the raw inputs.
    In other words, can our model learn to classify pictures other than with just
    the raw pixels as the only input? This can be of great help in a situation where
    you have a small set of labeled responses but a vast amount of unlabeled input
    data. You could train your deep learning model using unsupervised learning and
    then apply this in a supervised fashion to the labeled data, iterating back and
    forth.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种吸引我们注意力和头条的深度学习究竟是什么呢？让我们再次查阅维基百科，以获得一个工作定义：*深度学习是机器学习的一个分支，它基于一系列算法，通过使用模型架构，以复杂结构或其它方式，由多个非线性变换组成，试图在数据中模拟高级抽象*。这听起来就像是一位律师写的。深度学习的特点在于它基于人工神经网络（ANN），其中机器学习技术，主要是无监督学习，用于从输入变量中创建新特征。我们将在接下来的几章中深入探讨一些无监督学习技术，但可以将其视为在无响应变量存在的情况下寻找数据结构。简单来说，就是**元素周期表**，这是一个在未指定响应的情况下寻找结构的经典案例。在网上查找这张表，你会看到它是根据原子结构组织的，金属在一侧，非金属在另一侧。它是基于潜在分类/结构创建的。这种对潜在结构/层次结构的识别是深度学习与普通ANN的区别所在。深度学习某种程度上解决了是否有一个算法比仅仅使用原始输入更好地表示结果的问题。换句话说，我们的模型能否学会仅用原始像素作为唯一输入来对图片进行分类？这在只有少量标记响应但大量未标记输入数据的情况下非常有帮助。你可以使用无监督学习来训练你的深度学习模型，然后将这种监督方式应用于标记数据，来回迭代。
- en: Identification of these latent structures is not trivial mathematically, but
    one example is the concept of regularization that we looked at in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml),
    *Advanced Feature Selection in Linear Models*. In deep learning, one can penalize
    weights with regularization methods such as *L1* (penalize non-zero weights),
    *L2* (penalize large weights), and dropout (randomly ignore certain inputs and
    zero their weight out). In standard ANNs, none of these regularization methods
    takes place.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些潜在结构的识别在数学上并不简单，但一个例子是我们在[第4章](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml)，*线性模型中的高级特征选择*中探讨的正则化概念。在深度学习中，可以使用正则化方法（如*L1*（惩罚非零权重）、*L2*（惩罚大权重）和dropout（随机忽略某些输入并将其权重置零））来惩罚权重。在标准ANN中，这些正则化方法都没有发生。
- en: 'Another way is to reduce the dimensionality of the data. One such method is
    the `autoencoder`. This is a neural network where the inputs are transformed into
    a set of reduced dimension weights. In the following diagram, notice that **Feature
    A** is not connected to one of the hidden nodes:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是降低数据的维度。其中一种方法是`自动编码器`。这是一个神经网络，其中输入被转换为一组降低维度的权重。在下面的图中，请注意**特征A**没有连接到隐藏节点之一：
- en: '![](img/image_07_03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_03.png)'
- en: This can be applied recursively and learning can take place over many hidden
    layers. What you have seen happening in this case is that the network is developing
    features of features as they are stacked on each other. Deep learning will learn
    the weights between two layers in sequence first and then only use backpropagation
    in order to fine-tune these weights. Other feature selection methods include **Restricted
    Boltzmann Machine** and **Sparse Coding Model**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以递归地应用，学习可以在多个隐藏层上发生。在这种情况下，你所看到的是网络在层层叠加时发展出特征的特征。深度学习首先按顺序学习两层之间的权重，然后仅使用反向传播来微调这些权重。其他特征选择方法包括**受限玻尔兹曼机**和**稀疏编码模型**。
- en: The details are beyond our scope, and many resources are available to learn
    about the specifics. Here are a couple of starting points:[ ](http://www.cs.toronto.edu/~hinton/)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些细节超出了我们的范围，有很多资源可以学习具体细节。以下是一些起点：[ ](http://www.cs.toronto.edu/~hinton/)
- en: '[http://www.cs.toronto.edu/~hinton/](http://www.cs.toronto.edu/~hinton/)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.cs.toronto.edu/~hinton/](http://www.cs.toronto.edu/~hinton/)'
- en: '[http://deeplearning.net/](http://deeplearning.net/)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://deeplearning.net/](http://deeplearning.net/)'
- en: Deep learning has performed well on many classification problems, including
    winning a Kaggle contest or two. It still suffers from the problems of ANNs, especially
    the black box problem. Try explaining to the uninformed what is happening inside
    a neural network. However, it is appropriate for problems where an explanation
    of How is not a problem and the important question is What. After all, do we really
    care why an autonomous car avoided running into a pedestrian, or do we care about
    the fact that it did not? Additionally, the Python community has a bit of a head
    start on the R community in deep learning usage and packages. As we will see in
    the practical exercise, the gap is closing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多分类问题上表现良好，包括赢得Kaggle竞赛或两次。它仍然受到人工神经网络的问题，特别是黑盒问题的困扰。试着向不知情的人解释神经网络内部正在发生什么。然而，对于解释“如何”不是问题，而重要问题是“什么”的问题，它是合适的。毕竟，我们真的关心自动驾驶汽车为什么避免撞到行人，还是我们关心它没有撞到的事实？此外，Python社区在深度学习使用和包方面比R社区有先发优势。正如我们将在实际练习中看到的，差距正在缩小。
- en: 'While deep learning is an exciting undertaking, be aware that to achieve the
    full benefit of its capabilities, you will need a high degree of computational
    power along with taking the time to train the best model by fine-tuning the hyperparameters.
    Here is a list of some that you will need to consider:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度学习是一项令人兴奋的任务，但请注意，为了充分发挥其功能，你需要高度的计算能力，并花时间通过微调超参数来训练最佳模型。以下是一些你需要考虑的因素：
- en: An activation function
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Size and number of the hidden layers
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的尺寸和数量
- en: Dimensionality reduction, that is, Restricted Boltzmann versus autoencoder
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低，即受限玻尔兹曼机与自动编码器
- en: The number of epochs
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代次数
- en: The gradient descent learning rate
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降学习率
- en: The loss function
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: Regularization
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: Deep learning resources and advanced methods
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习资源和高级方法
- en: 'One of the more interesting visual tools you can use for both learning and
    explaining is the interactive widget provided by TensorFlow^(TM): [http://playground.tensorflow.org/](http://playground.tensorflow.org/).
    This tool allows you to explore, or **tinker**, as the site calls it, the various
    parameters and how they impact on the response, be it a classification problem
    or a regression problem. I could spend, well I have spent hours tinkering with
    it.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用作学习和解释的更有趣的视觉工具之一是TensorFlow^(TM)提供的交互式小部件：[http://playground.tensorflow.org/](http://playground.tensorflow.org/)。这个工具允许你探索，或者像网站所说的那样，**摆弄**各种参数以及它们如何影响响应，无论是分类问题还是回归问题。我可以花很多小时摆弄它。
- en: 'Here is an interesting task: create your own experimental design and see how
    the various parameters affect your prediction.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的任务：创建你自己的实验设计，看看各种参数如何影响你的预测。
- en: 'At this point, it seems that the two fastest growing deep learning open-source
    tools are TensorFlow^(TM) and MXNet. I still prefer working with the package we
    will see, `h2o`, but it is important to understand and learn the latest techniques.
    You can access TensorFlow^(TM )with R, but it requires you to install python first.
    This series of tutorials will walk you through how to get it up and running:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，似乎增长最快的两个深度学习开源工具是TensorFlow^(TM)和MXNet。我仍然更喜欢使用我们将要看到的包`h2o`，但了解和学习最新技术是很重要的。你可以使用R访问TensorFlow^(TM)，但需要先安装Python。本系列教程将指导你如何启动和运行：
- en: '[https://rstudio.github.io/tensorflow/](https://rstudio.github.io/tensorflow/).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://rstudio.github.io/tensorflow/](https://rstudio.github.io/tensorflow/).'
- en: 'MXNet does not require the installation of Python and is relatively easy to
    install and make operational. It also offers a number of pretrained models that
    allow you to start making predictions quickly. Several R tutorials are available:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet不需要安装Python，相对容易安装和运行。它还提供了一些预训练模型，允许你快速开始预测。有几个R教程可用：
- en: '[http://mxnet.io/](http://mxnet.io/).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://mxnet.io/](http://mxnet.io/).'
- en: I now want to take the time to enumerate some of the variations of deep neural
    networks along with the learning tasks where they have performed well.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在想花时间列举一些深度神经网络的变体以及它们在哪些学习任务中表现良好。
- en: '**Convolutional neural networks** (**CNN**) make the assumption that the inputs
    are images and create features from slices or small portions of the data, which
    are combined to create a feature map. Think of these small slices as filters or
    probably more appropriately, kernels that the network learns during training.
    The activation function for CNN is a **Rectified** **Linear** **Unit** (**ReLU**).
    It is simply *f(x) = max(0, x)*, where *x* is the input to the neuron. CNNs perform
    well on image classification, object detection, and even sentence classification.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）假设输入是图像，并从数据切片或小部分中创建特征，这些特征被组合起来创建特征图。将这些小切片想象成网络在训练过程中学习的过滤器或更恰当地说，是核。CNN的激活函数是**修正线性单元**（**ReLU**）。它简单表示为
    *f(x) = max(0, x)*，其中 *x* 是神经元的输入。CNN在图像分类、目标检测甚至句子分类方面表现良好。'
- en: '**Recurrent neural networks** (**RNN**) are created to make use of sequential
    information. In traditional neural networks, the inputs and outputs are independent
    of each other. With RNN, the output is dependent on the computations of previous
    layers, permitting information to persist across layers. So, take an output from
    a neuron (y); it is calculated not only on its input (t) but on all previous layers
    (t-1, t-n...). It is effective at handwriting and speech detection.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）是为了利用序列信息而创建的。在传统的神经网络中，输入和输出是相互独立的。在RNN中，输出依赖于先前层的计算，允许信息在层之间持续。所以，从一个神经元（y）的输出；它不仅基于其输入（t），还基于所有先前层（t-1,
    t-n...）。它在手写和语音检测方面非常有效。'
- en: '**Long Short-Term Memory** (**LSTM**) is a special case of RNN. The problem
    with RNN is that it does not perform well on data with long signals. Thus LSTMs
    were created to capture complex patterns in data. RNNs combine information during
    training from previous steps in the same way, regardless of the fact that information
    in one step is more or less valuable than other steps. LSTMs seek to overcome
    this limitation by deciding what to remember at each step during training. This
    multiplication of a weight matrix by the data vector is referred to as a gate,
    which acts as an information filter. A neuron in LSTM will have two inputs and
    two outputs. The input from prior outputs and the memory vector passed from the
    previous gate. Then, it produces the output values and output memory as inputs
    to the next layer. LSTMs have the limitation of requiring a healthy dose of training
    data and are computationally intensive. LSTMs have performed well on speech recognition
    problems.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）是循环神经网络（RNN）的一个特例。RNN的问题在于它对具有长信号的数据表现不佳。因此，LSTMs被创建出来以捕捉数据中的复杂模式。RNN在训练过程中以相同的方式结合信息，而不管信息在一个步骤中相对于其他步骤是更有价值还是更无价值。LSTMs试图通过在训练的每个步骤中决定要记住什么来克服这一限制。这种权重矩阵与数据向量的乘积被称为门，它充当信息过滤器。LSTM中的神经元将有两个输入和两个输出。来自先前输出的输入和从先前门传递的内存向量。然后，它产生输出值和输出内存作为下一层的输入。LSTMs的局限性在于需要大量的训练数据，并且计算密集。LSTMs在语音识别问题上表现良好。'
- en: I recommend you work with the tutorials on MXNet to help you understand how
    to develop these models for your own use.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你使用MXNet的教程来帮助你理解如何开发这些模型以供自己使用。
- en: With that, let's move on to some practical applications.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，让我们继续探讨一些实际应用。
- en: Business understanding
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: It was a calm, clear night on the 20th of April, 1998\. I was a student pilot
    in a Hughes 500D helicopter on a cross-country flight from the St. Paul, MN downtown
    airport back home to good old Grand Forks, ND. The flight was my final requirement
    prior to taking the test to achieve a helicopter instrument rating. My log book
    shows that we were 35 **Distance Measuring Equipment** (**DME**) or 35 nautical
    miles from the VOR on Airway Victor 2\. This put us somewhere south/southeast
    of St. Cloud, MN, cruising along at what I recall was 4,500 feet above sea level
    at approximately 120 knots. Then, it happened...BOOOOM! It is not hyperbole to
    say that it was a thunderous explosion, followed by a hurricane blast of wind
    to the face.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年4月20日那天晚上，天气晴朗，天空清澈。我是一名学生飞行员，驾驶着一架休斯500D直升机，从明尼苏达州圣保罗市中心的机场进行了一次长途飞行，返回到古老的格兰
    Forks，北达科他州。这次飞行是我获得直升机仪表等级测试前的最后要求。我的飞行日志显示，我们距离VOR在Victor 2航线上35英里或35海里。这使我们位于明尼苏达州圣克劳德市以南/东南方向，以我记忆中的4,500英尺的海拔高度，大约以120节的速度飞行。然后，发生了...BOOOOM！说这是一声震耳欲聋的爆炸声，随后是一阵飓风般的风吹到脸上，这并不夸张。
- en: 'It all started when my flight instructor asked a mundane question about our
    planned instrument approach into Alexandria, MN. We swapped control of the aircraft
    and I bent over to consult the instrument approach plate on my kneeboard. As I
    snapped on the red lens flashlight, the explosion happened. Given my face-down
    orientation, the sound, and ensuing blast of wind, several thoughts crossed my
    mind: the helicopter is falling apart, I''m plunging to my death, and the Space
    Shuttle Challenger explosion as an HD quality movie going off in my head. In the
    1.359 seconds that it took us to stop screaming, we realized that the Plexiglas
    windscreen in front of me was essentially gone, but everything else was good to
    go. After slowing the craft, a cursory inspection revealed that the cockpit was
    covered in blood, guts, and feathers. We had done the improbable by hitting a
    Mallard duck over Central Minnesota and in the process, destroyed the windscreen.
    Had I not been looking at my kneeboard, I would have been covered in pate. We
    simply declared an emergency and canceled our flight plan with Minneapolis Center
    and, like the Memphis Belle, limped our way into Alexandria to await rescue from
    our compatriots at the University of North Dakota (home of the Fighting Sioux).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这一切都始于我的飞行教练问了一个关于我们计划进入明尼苏达州亚历山大的仪表进近的平凡问题。我们交换了飞机的控制权，我弯下腰查看膝上板上仪表进近图。当我戴上红色镜头手电筒时，爆炸发生了。由于我是脸朝下的姿势，声音和随之而来的风力，几个想法闪过我的脑海：直升机正在解体，我正在坠落死亡，我的脑海中浮现出像高清电影一样的挑战者号航天飞机爆炸场景。在1.359秒的尖叫停止后，我们意识到我面前的聚碳酸酯风挡几乎已经没有了，但其他一切正常。在减速飞机后，粗略检查发现驾驶舱覆盖着血、内脏和羽毛。我们在明尼苏达州中部击中了一只绿头鸭，在这个过程中，我们摧毁了风挡。如果我没有在看我的膝上板，我就会被羽毛覆盖。我们只是宣布紧急情况，取消了与明尼苏达中心机场的飞行计划，就像孟菲斯美女号一样，一瘸一拐地进入亚历山大便于等待来自北达科他大学（战斗苏族之家）的同胞们的救援。
- en: So what? Well, I wanted to point out how much of a NASA fan and astronaut I
    am. In a terrifying moment, where for a split second I thought that I was checking
    out, my mind drifted to the Space Shuttle. Most males my age wanted to shake the
    hands of George Brett or Wayne Gretzky. I wanted to, and in fact did, shake the
    hands of Buzz Aldrin. (he was after all on the North Dakota faculty at the time.)
    Thus, when I found the `shuttle` dataset in the `MASS` package, I had to include
    it in this tome. By the way, if you ever get the chance to see the Space Shuttle
    Atlantis display at Kennedy Space Center, do not miss it.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那又如何呢？我想指出我是多么的喜欢NASA和宇航员。在一个令人恐惧的时刻，我短暂地以为我将要结束生命，我的思绪飘向了航天飞机。我们这个年龄的大部分男性都想和乔治·布雷特或韦恩·格雷茨基握手。我也想，事实上我也确实和巴兹·奥尔德林握了手。（毕竟，当时他在北达科他大学任教。）因此，当我发现`shuttle`数据集在`MASS`包中时，我不得不将其包含在这本书中。顺便说一句，如果你有机会去肯尼迪航天中心看看航天飞机亚特兰蒂斯号的展览，千万不要错过。
- en: 'For this problem, we will try and develop a neural network to answer the question
    of whether or not the shuttle should use the autolanding system. The default decision
    is to let the crew land the craft. However, the autoland capability may be required
    for situations of crew incapacitation or adverse effects of gravity upon re-entry
    after extended orbital operations. This data is based on computer simulations,
    not actual flights. In reality, the autoland system went through some trials and
    tribulations and, for the most part, the shuttle astronauts were in charge during
    the landing process. Here are a couple of links for further background information:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，我们将尝试开发一个神经网络来回答是否应该使用自动着陆系统。默认决策是让机组人员着陆飞船。然而，在机组人员受伤或长时间轨道操作后重新进入时重力产生不利影响的情况下，可能需要自动着陆能力。这些数据基于计算机模拟，而不是实际飞行。实际上，自动着陆系统经历了一些考验和磨难，大多数情况下，航天飞机宇航员在着陆过程中负责。以下是一些提供更多背景信息的链接：
- en: '[http://www.spaceref.com/news/viewsr.html?pid=10518](http://www.spaceref.com/news/viewsr.html?pid=10518)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.spaceref.com/news/viewsr.html?pid=10518](http://www.spaceref.com/news/viewsr.html?pid=10518)'
- en: '[https://waynehale.wordpress.com/2011/03/11/breaking-through/](https://waynehale.wordpress.com/2011/03/11/breaking-through/)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://waynehale.wordpress.com/2011/03/11/breaking-through/](https://waynehale.wordpress.com/2011/03/11/breaking-through/)'
- en: Data understanding and preparation
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'To start, we will load these four packages. The data is in the `MASS` package:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载这四个包。数据位于`MASS`包中：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `neuralnet` package will be used for the building of the model and `caret`
    for the data preparation. The `vcd` package will assist us in data visualization.
    Let''s load the data and examine its structure:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用`neuralnet`包来构建模型，使用`caret`进行数据准备。`vcd`包将帮助我们进行数据可视化。让我们加载数据并检查其结构：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The data consists of `256` observations and `7 variables`. Notice that all
    of the variables are categorical and the response is `use` with two levels, `auto`
    and `noauto`. The covariates are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据由`256`个观测值和`7`个变量组成。请注意，所有变量都是分类变量，响应变量为`use`，有两个水平，即`auto`和`noauto`。协变量如下：
- en: '`stability`: This is stable positioning or not (`stab`/`xstab`)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stability`：这是稳定的定位或不是（`stab`/`xstab`）'
- en: '`error`: This is the size of the error (`MM` / `SS` / `LX`)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`error`：这是错误的尺寸（`MM` / `SS` / `LX`）'
- en: '`sign`: This is the sign of the error, positive or negative (`pp`/`nn`)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sign`：这是错误的符号，正或负（`pp`/`nn`）'
- en: '`wind`: This is the `wind` sign (`head` / `tail`)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wind`：这是风力的符号（`head` / `tail`）'
- en: '`magn`: This is the `wind` strength (`Light` / `Medium` / `Strong` / `Out of
    Range`)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`magn`：这是风力强度（`Light` / `Medium` / `Strong` / `Out of Range`）'
- en: '`vis`: This is the visibility (`yes` / `no`)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vis`：这是可见性（`yes` / `no`）'
- en: 'We will build a number of tables to explore the data, starting with the response/outcome:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建多个表格来探索数据，从响应/结果开始：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Almost 57 per cent of the time, the decision is to use the autolander. There
    are a number of possibilities to build tables for categorical data. The `table()`
    function is perfectly adequate to compare one with another, but if you add a third,
    it can turn into a mess to look at. The `vcd` package offers a number of table
    and plotting functions. One is `structable()`. This function will take a formula
    (*column1 + column2 ~ column3*), where *column3* becomes the rows in the table:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎57%的时间，决策是使用自动着陆器。对于分类数据，有许多构建表格的可能性。`table()`函数完全足够用于比较，但如果添加第三个，看起来可能会变得混乱。`vcd`包提供了一系列表格和绘图函数。其中一个是`structable()`。这个函数将接受一个公式（*column1
    + column2 ~ column3*），其中*column3*将成为表格的行：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we can see that in the cases of a headwind that was `Light` in magnitude,
    `auto` occurred `19` times and `noauto`, `13` times. The `vcd` package offers
    the `mosaic()` function to plot the table created by `structable()` and provide
    the **p-value** for a chi-squared test:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到在风力`Light`的情况下，`auto`出现了`19`次，而`noauto`出现了`13`次。`vcd`包提供了`mosaic()`函数来绘制由`structable()`创建的表格，并提供卡方检验的**p值**：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_07_04.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_07_04.png)'
- en: 'The plot tiles correspond to the proportional size of their respective cells
    in the table, created by recursive splits. You can also see that the **p-value**
    is not significant, so the variables are independent, which means that knowing
    the levels of wind and/or **magn** does not help us predict the use of the autolander.
    You do not need to include a `structable()` object in order to create the plot
    as it will accept a formula just as well:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图表标题对应于它们在表格中各自单元格的成比例大小，这是通过递归拆分创建的。您还可以看到，**p值**并不显著，因此变量是独立的，这意味着了解风速和/或**magn**的水平并不能帮助我们预测自动着陆器的使用。您不需要包含一个
    `structable()` 对象来创建图表，因为它同样可以接受公式：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_07_05.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_07_05.png)'
- en: Note that the shading of the table has changed, reflecting the rejection of
    the null hypothesis and dependence in the variables. The plot first takes and
    splits the visibility. The result is that if the visibility is **no**, then the
    autolander is used. The next split is horizontal by **error**. If **error** is
    **SS** or **MM** when **vis** is **no**, then the autolander might be recommended,
    otherwise it is not. A p-value is not necessary as the gray shading indicates
    significance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，表格的阴影已经改变，反映了零假设被拒绝和变量之间的依赖性。图表首先获取并拆分可见性。结果是，如果可见性为**无**，则使用自动着陆器。下一个拆分是按**错误**水平进行的。如果**错误**为**SS**或**MM**且**vis**为**无**，则可能建议使用自动着陆器，否则则不。不需要
    p值，因为灰色阴影表示显著性。
- en: 'One can also examine proportional tables with the `prop.table()` function as
    a wrapper around `table()`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `prop.table()` 函数作为 `table()` 的包装来检查成比例的表格：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In case we forget, the chi-squared tests are quite simple:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们忘记了，卡方检验相当简单：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Preparing the data for a neural network is very important as all the covariates
    and responses need to be numeric. In our case, all of the input features are categorical.
    However, the `caret` package allows us to quickly create dummy variables as our
    input features:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 准备神经网络的数据非常重要，因为所有协变量和响应都需要是数值的。在我们的案例中，所有输入特征都是分类的。然而，`caret` 包允许我们快速创建虚拟变量作为我们的输入特征：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To put this into a data frame, we need to predict the `dummies` object to an
    existing data, either the same or different, in `as.data.frame()`. Of course,
    the same data is needed here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这些放入数据框中，我们需要将 `dummies` 对象预测到现有的数据中，无论是相同还是不同的，在 `as.data.frame()` 中。当然，这里也需要相同的数据：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We now have an input feature space of ten variables. Stability is now either
    `0` for `stab` or `1` for `xstab`. The base error is `LX`, and three variables
    represent the other categories.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个包含十个变量的输入特征空间。稳定性现在是 `stab` 的 `0` 或 `xstab` 的 `1`。基本错误是 `LX`，其余三个变量代表其他类别。
- en: 'The response can be created using the `ifelse()` function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `ifelse()` 函数创建响应：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `caret` package also provides us with the functionality to create the `train`
    and `test` sets. The idea is to index each observation as `train` or `test` and
    then split the data accordingly. Let''s do this with a 70/30 `train` to `test`
    split, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret` 包还提供了创建 `train` 和 `test` 集的功能。想法是将每个观察值索引为 `train` 或 `test`，然后相应地拆分数据。让我们以
    70/30 的 `train` 到 `test` 比例进行拆分，如下所示：'
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The values in `trainIndex` provide us with the row number; in our case, 70
    per cent of the total row numbers in `shuttle.2`. It is now a simple case of creating
    the `train`/`test` datasets:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainIndex` 中的值为我们提供了行号；在我们的案例中，是 `shuttle.2` 中总行数的 70%。现在只需要创建 `train`/`test`
    数据集：'
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Nicely done! We are now ready to begin building the neural networks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们现在可以开始构建神经网络了。
- en: Modeling and evaluation
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: 'As mentioned, the package that we will use is `neuralnet`. The function in
    `neuralnet` will call for the use of a formula as we used elsewhere, such as *y~x1+x2+x3+x4*,
    *data = df*. In the past, we used *y~,* to specify all the other variables in
    the data as inputs. However, `neuralnet` does not accommodate this at the time
    of writing. The way around this limitation is to use the `as.formula()` function.
    After first creating an object of the variable names, we will use this as an input
    in order to paste the variables properly on the right side of the equation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用的包是 `neuralnet`。`neuralnet` 中的函数将调用使用公式，就像我们在其他地方使用的那样，例如 *y~x1+x2+x3+x4*，*data
    = df*。在过去，我们使用 *y~* 来指定数据中的所有其他变量作为输入。然而，`neuralnet` 在撰写本文时并不支持这一点。绕过这种限制的方法是使用
    `as.formula()` 函数。首先创建一个变量名对象，然后我们将使用它作为输入，以便正确地将变量粘贴在等式的右侧：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Keep this function in mind for your own use as it may come in quite handy.
    In the `neuralnet` package, the function that we will use is appropriately named
    `neuralnet()`. Other than the formula, there are four other critical arguments
    that we will need to examine:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住这个函数供您自己使用，因为它可能非常有用。在 `neuralnet` 包中，我们将使用的函数名为 `neuralnet()`。除了公式外，还有四个其他关键参数我们需要检查：
- en: '`hidden`: This is the number of hidden neurons in each layer, which can be
    up to three layers; the default is 1'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden`: 这是每层的隐藏神经元数量，最多可达三层；默认为 1'
- en: '`act.fct`: This is the activation function with the default logistic and `tanh`
    available'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`act.fct`: 这是激活函数，默认为逻辑函数和 `tanh` 函数'
- en: '`err.fct`: This is the function used to calculate the error with the default
    `sse`; as we are dealing with binary outcomes, we will use `ce` for cross-entropy'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`err.fct`: 这是计算误差的函数，默认为 `sse`；由于我们处理的是二元结果，我们将使用 `ce` 进行交叉熵'
- en: '`linear.output`: This is a logical argument on whether or not to ignore `act.fct`
    with the default TRUE, so for our data, this will need to be `FALSE`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linear.output`: 这是一个关于是否忽略默认为 TRUE 的 `act.fct` 的逻辑论证，因此对于我们的数据，这需要设置为 `FALSE`'
- en: 'You can also specify the algorithm. The default is resilient with backpropagation
    and we will use it along with the default of one hidden neuron:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以指定算法。默认为具有反向传播的弹性算法，我们将使用它，并使用默认的一个隐藏神经元：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here are the overall results:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是总体结果：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see that the error is extremely low at `0.0099`. The number of steps
    required for the algorithm to reach the threshold, which is when the absolute
    partial derivatives of the error function become smaller than this error (default
    = 0.1). The highest weight of the first neuron is `vis.yes.to.1layhid1` at 6.21.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，错误率极低，为 `0.0099`。算法达到阈值所需的步数，即误差函数的绝对偏导数小于此误差（默认 = 0.1）时，第一个神经元的最高权重为
    `vis.yes.to.1layhid1`，为 6.21。
- en: 'You can also look at what are known as generalized weights. According to the
    authors of the `neuralnet` package, the generalized weight is defined as the contribution
    of the *i*th covariate to the log-odds:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看所谓的广义权重。根据 `neuralnet` 包的作者，广义权重定义为第 *i* 个协变量对对数优势的贡献：
- en: '*The generalized weight expresses the effect of each covariate x[i] and thus
    has an analogous interpretation as the ith regression parameter in regression
    models. However, the generalized weight depends on all other covariates* (Gunther
    and Fritsch, 2010).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*广义权重表示每个协变量 x[i] 的影响，因此在回归模型中与第 i 个回归参数有类似的解释。然而，广义权重依赖于所有其他协变量*（Gunther 和
    Fritsch，2010）。'
- en: The weights can be called and examined. I've abbreviated the output to the first
    four variables and six observations only. Note that if you sum each row, you will
    get the same number, which means that the weights are equal for each covariate
    combination. Please note that your results might be slightly different because
    of random weight initialization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调用并检查权重。我已经将输出简化为前四个变量和六个观测值。请注意，如果对每一行求和，您将得到相同的数字，这意味着权重对于每个协变量组合是相等的。请注意，由于随机权重初始化，您的结果可能会有所不同。
- en: 'The results are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To visualize the neural network, simply use the `plot()` function:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化神经网络，只需使用 `plot()` 函数：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the output of the preceding command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_07_06-1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_07_06-1.png)'
- en: 'This plot shows the weights of the variables and intercepts. You can also examine
    the generalized weights in a plot. Let''s look at `vis.yes` versus `wind.tail`,
    which has a low overall synaptic weight. Notice how `vis.yes` is skewed and `wind.tail`
    has an even distribution of weights, implying little predictive power:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示了变量权重和截距。您还可以在图中检查广义权重。让我们看看 `vis.yes` 与 `wind.tail` 的对比，后者具有较低的总体突触权重。注意
    `vis.yes` 是偏斜的，而 `wind.tail` 权重的分布均匀，这意味着预测能力很小：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following is the output of the preceding commands:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_07_07.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_07_07.png)'
- en: 'We now want to see how well the model performs. This is done with the `compute()`
    function and specifying the fit model and covariates. This syntax will be the
    same for the predictions on the `test` and `train` sets. Once computed, a list
    of the predictions is created with `$net.result`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在想看看模型的表现如何。这是通过 `compute()` 函数和指定拟合模型和协变量来完成的。此语法将用于对 `test` 和 `train` 集合的预测。一旦计算，就会创建一个包含
    `$net.result` 的预测列表：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'These results are in probabilities, so let''s turn them into `0` or `1` and
    follow this up with a confusion matrix:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是以概率表示的，所以让我们将它们转换为`0`或`1`，并随后使用混淆矩阵：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lo and behold, the neural network model has achieved 100 per cent accuracy.
    We will now hold our breath and see how it does on the `test` set:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 看看吧，神经网络模型已经达到了100%的准确率。我们现在屏住呼吸，看看它在`test`集中的表现如何：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Only one false positive in the `test` set. If you wanted to identify which
    one this was, use the `which()` function to single it out, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在`test`集中只有一个假阳性。如果您想识别这是哪一个，请使用`which()`函数将其单独列出，如下所示：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It is row `62` in the `test` set and observation `203` in the full dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 它是`test`集中的第`62`行，在完整数据集中是第`203`个观测值。
- en: I'll leave it to you to see if you can build a neural network that achieves
    100% accuracy!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我将留给您去尝试构建一个达到100%准确率的神经网络！
- en: An example of deep learning
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的一个示例
- en: Shifting gears away from the Space Shuttle, let's work through a practical example
    of deep learning, using the `h2o` package. We will do this on data I've modified
    from the UCI Machine Learning Repository. The original data and its description
    is available at [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/).
     What I've done is, take the smaller dataset `bank.csv`, scale the numeric variables
    to mean 0 and variance of 1, create dummies for the character variables/sparse
    numerics, and eliminate near zero variance varaibles.  The data is available on
    github [https://github.com/datameister66/data/](https://github.com/datameister66/data/)
    named also `bank_DL.csv`. In this section, we will focus on how to load the data
    in the H20 platform and run the deep learning code to build a classifier to predict
    whether a customer will respond to a marketing campaign.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 转移到航天飞机的话题，让我们通过使用`h2o`包来处理一个深度学习的实际示例。我们将使用我从UCI机器学习仓库修改过的数据来完成这项工作。原始数据和其描述可在[https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/)找到。我所做的是，从较小的数据集`bank.csv`中提取数据，将数值变量缩放到均值为0和方差为1，为字符变量/稀疏数值创建虚拟变量，并消除接近零方差变量。数据可在github
    [https://github.com/datameister66/data/](https://github.com/datameister66/data/)上找到，也命名为`bank_DL.csv`。在本节中，我们将关注如何在H20平台上加载数据并运行深度学习代码来构建一个分类器，以预测客户是否会响应营销活动。
- en: H2O background
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H2O背景
- en: H2O is an open source predictive analytics platform with prebuilt algorithms,
    such as k-nearest neighbor, gradient boosted machines, and deep learning. You
    can upload data to the platform via Hadoop, AWS, Spark, SQL, noSQL, or your hard
    drive. The great thing about it is that you can utilize the machine learning algorithms
    in R and, at a much greater scale, on your local machine. If you are interested
    in learning more, you can visit the site: [http://h2o.ai/product/](http://h2o.ai/product/).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: H2O是一个开源的预测分析平台，具有预构建的算法，例如k近邻、梯度提升机和深度学习。您可以通过Hadoop、AWS、Spark、SQL、noSQL或您的硬盘驱动器将数据上传到该平台。它的好处是您可以在R中利用机器学习算法，并在本地机器上以更大的规模使用。如果您想了解更多信息，可以访问网站：[http://h2o.ai/product/](http://h2o.ai/product/).
- en: 'The process of installing H2O on R is a little different. I put the code here
    that gave me the latest update (as of February 25, 2017). You can use it to reinstall
    the latest version or pull it off of the website: [http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/](http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/).
    The following is the code to install the latest version:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在R上安装H2O的过程略有不同。我在这里放上了给我最新更新（截至2017年2月25日）的代码。您可以使用它重新安装最新版本或从网站上获取：[http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/](http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/).
    下面是安装最新版本的代码：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Data upload to H2O
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据上传到H2O
- en: 'Let''s assume you have the  `bank_DL.csv` file saved in your working directory.
    Remember, `getwd()` will provide you with the path to it. So, let''s load the
    library and create an object with the file path to the data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经将`bank_DL.csv`文件保存在工作目录中。记住，`getwd()`会为您提供路径。因此，让我们加载库并创建一个包含数据文件路径的对象：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now connect to H2O and start an instance on the cluster. Specifying
    `nthreads = -1` requests our instance use all CPUs on the cluster:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以连接到H2O并在集群上启动一个实例。指定`nthreads = -1`请求我们的实例使用集群上的所有CPU：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The H2O function, `h2o.uploadFile()`, allows you to upload/import your file
    to the H2O cloud. The following functions are also available for uploads:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: H2O函数`h2o.uploadFile()`允许你将文件上传/导入到H2O云。以下函数也适用于上传：
- en: '`h2o.importFolder`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o.importFolder`'
- en: '`h2o.importURL`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o.importURL`'
- en: '`h2o.importHDFS`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o.importHDFS`'
- en: 'It is quite simple to upload the file and a per cent indicator tracks the status:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上传文件非常简单，一个百分比指示器跟踪状态：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The data is now in `H2OFrame`, which you can verify with `class()`, as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在在`H2OFrame`中，你可以通过`class()`来验证，如下所示：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Many of the R commands in H2O may produce a different output than what you
    are used to seeing. For instance, look at the structure of our data (abbreviated
    output):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: H2O中的许多R命令可能产生的输出与您习惯看到的不同。例如，看看我们数据的结构（输出摘要）：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We see that it consists of 4,521 observations (nrow) and 64 columns (ncol).
    By the way, the `head()` and `summary()` functions work exactly the same as in
    regular R. Before splitting the datasets, let''s have a look at our response distribution.
    It is the column named **y**:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到它由4,521个观测值（nrow）和64列（ncol）组成。顺便说一句，`head()`和`summary()`函数在常规R中的工作方式完全相同。在分割数据集之前，让我们看看我们的响应分布。它是名为**y**的列：
- en: '[PRE32]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We see that 521 of the bank's customers responded yes to the offer and 4,000
    did not. This response is a bit unbalanced. Techniques that can be used to handle
    unbalanced response labels are discussed in the chapter on multi-class learning.
    In this exercise, let's see how deep learning will perform with this lack of label
    balance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，银行中有521位客户对优惠表示了肯定，而4,000位没有。这种响应是不平衡的。在多类学习章节中讨论了可以用来处理不平衡响应标签的技术。在这个练习中，让我们看看深度学习在没有标签平衡的情况下会表现如何。
- en: Create train and test datasets
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集
- en: 'You can use H2O''s functionality to partition the data into train and test
    sets. The first thing to do is create a vector of random and uniform numbers for
    the full data:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用H2O的功能将数据分割成训练和测试集。首先要做的是为全部数据创建一个随机和均匀的数字向量：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can then build your partitioned data and assign it with a desired `key`
    name, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以构建你的分区数据，并使用所需的`key`名称分配它，如下所示：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With these created, it is probably a good idea that we have a balanced response
    variable between the `train` and `test` sets. To do this, you can use the `h2o.table()`
    function and, in our case, it would be column 64:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了这些之后，我们可能需要确保`train`和`test`集之间有一个平衡的响应变量。为此，你可以使用`h2o.table()`函数，在我们的例子中，将是第64列：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This appears all well and good, so let''s begin the modeling process:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很好，所以让我们开始建模过程：
- en: Modeling
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模
- en: 'As we will see, the deep learning function has quite a few arguments and parameters
    that you can tune. The thing that I like about the package is the ability to keep
    it as simple as possible and let the defaults do their thing. If you want to see
    all the possibilities along with the defaults, see help or run the following command:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将要看到的，深度学习函数有很多参数和选项可以调整。我喜欢这个包的地方在于它尽可能地保持简单，并让默认设置发挥作用。如果你想查看所有可能的选项以及默认设置，请查看帮助或运行以下命令：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Documentation on all the arguments and tuning parameters is available online
    at [http://h2o.ai/docs/master/model/deep-learning/](http://h2o.ai/docs/master/model/deep-learning/).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参数和调整参数的文档可在[http://h2o.ai/docs/master/model/deep-learning/](http://h2o.ai/docs/master/model/deep-learning/)在线获取。
- en: As on a side note, you can run a demo for the various machine learning methods
    by just running `demo("method")`. For instance, you can go through the deep learning
    demo with `demo(h2o.deeplearning)`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，你可以通过运行`demo("method")`来运行各种机器学习方法的演示。例如，你可以通过`demo(h2o.deeplearning)`来浏览深度学习演示。
- en: 'Our next goal is to tune the hyper-parameters using a random search. It takes
    less time than a full grid search. We will look at `tanh`, with and without dropout,
    three different hidden layer/neuron combinations, two different dropout ratios,
    and two different learning rates:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个目标是使用随机搜索调整超参数。它比完整的网格搜索花费的时间少。我们将查看带有和不带有dropout的`tanh`，三种不同的隐藏层/神经元组合，两种不同的dropout比率，以及两种不同的学习率：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You now help specify the random search criteria in a list. Since we want a
    random search we will specify `RandomDiscrete`. A full grid search would require
    `Cartesian`. It is recommended to specify one or more early stopping criterion
    for a random search such as `max_runtime_secs`, `max_models`. We also specify
    here that it will stop when the top five models are withing 1% error of each other:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在帮助指定一个列表中的随机搜索标准。由于我们想要随机搜索，我们将指定`RandomDiscrete`。完整的网格搜索将需要`Cartesian`。建议为随机搜索指定一个或多个早期停止标准，如`max_runtime_secs`、`max_models`。我们还指定在这里，当前五名模型的误差彼此相差1%时停止：
- en: '[PRE38]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, this is where the magic should happen using the `h2o.grid()` function.
    We tell it, we want to use the deep learning algorithm, our test data, any validation
    data (we will use the test set), our input features, and response variable:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是使用`h2o.grid()`函数应该发生魔法的地方。我们告诉它，我们想使用深度学习算法，我们的测试数据，任何验证数据（我们将使用测试集），我们的输入特征，以及响应变量：
- en: '[PRE39]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: An indicator bar tracks the progress, and with this dataset, it should take
    less than a few seconds.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一个指示条跟踪进度，并且使用这个数据集，它应该只需要几秒钟。
- en: 'We now examine the results of the top five models:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在检查前五名模型的结果：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So the winning model is `#57` with activation of `TanhWithDropout`, three hidden
    layers with 30 neurons each, dropout ratio of 0.05, and learning rate of 0.25,
    which had an AUC of almost 0.864\.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，获胜模型是`#57`，激活函数为`TanhWithDropout`，三个隐藏层，每个层有30个神经元，dropout比率为0.05，学习率为0.25，其AUC接近0.864。
- en: 'We now have a look at our error rates in the validation/test data with a confusion
    matrix:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在查看我们的验证/测试数据中的错误率，使用混淆矩阵：
- en: '[PRE41]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Even though we only have 11% error, we had high errors for the `yes` label with
    high rates of false positives and false negatives. It possibly indicates that
    class imbalance may be an issue. We also have just started the hyper-parameter
    tuning process, so much work could be done to improve the outcome. I'll leave
    that task to you!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们只有11%的错误率，但我们对于`yes`标签有较高的错误率，包括假阳性和假阴性率。这可能表明类别不平衡可能是一个问题。我们刚刚开始超参数调整过程，所以还有很多工作可以改进结果。我将把这个任务留给你！
- en: 'Now let''s examine how to build a model using cross-validation. Notice how
    the hyper-parameters are included in the function `h2o.deeplearning()` with the
    exception of learning rate, which is specified as adaptive. I also included the
    functionality to up-sample the minority class to achieve balanced labels during
    training. On another note, the folds are a stratified sample based on the response
    variable:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用交叉验证构建模型。注意超参数包含在`h2o.deeplearning()`函数中，除了学习率，它被指定为自适应。我还包括了在训练期间通过上采样少数类以实现平衡标签的功能。另一方面，折是按照响应变量进行分层抽样的：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If you call the object `dlmodel`, you will receive rather lengthy output. In
    this instance, let''s examine the performance on the holdout folds:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你调用对象`dlmodel`，你会收到相当长的输出。在这个例子中，让我们检查保留集的模型性能：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Given these results, I think more tuning is in order for the hyper-parameters,
    particularly with the hidden layers/neurons. Examining out of sample performance
    is a little different, but is quite comprehensive, utilizing the `h2o.performance()`
    function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些结果，我认为需要对超参数进行更多调整，尤其是对隐藏层/神经元。检查样本外性能略有不同，但相当全面，使用了`h2o.performance()`函数：
- en: '[PRE44]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The overall error increased, but we have lower false positive and false negative
    rates. As before, additional tuning is required.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 总体错误率上升，但我们有更低的假阳性和假阴性率。和之前一样，需要额外的调整。
- en: 'Finally, the variable importance can be produced. This is calculated based
    on the so-called `Gedeon` Method. Keep in mind that these results can be misleading.
    In the table, we can see the order of the variable importance, but this importance
    is subject to the sampling variation, and if you change the seed value, the order
    of the variable importance could change quite a bit. These are the top five variables
    by importance:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以生成变量重要性。这是基于所谓的`Gedeon`方法计算的。请注意，这些结果可能是误导性的。在表中，我们可以看到变量重要性的顺序，但这个重要性是受采样变异影响的，如果你改变种子值，变量重要性的顺序可能会发生很大变化。以下是按重要性排序的前五个变量：
- en: '[PRE45]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: With this, we have completed the introduction to deep learning in R using the
    capabilities of the `H2O` package. It is simple to use while offering plenty of
    flexibility to tune the hyperparameters and create deep neural networks. Enjoy!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方法，我们已经完成了使用`H2O`包在R中介绍深度学习的内容。它使用简单，同时提供了大量的灵活性来调整超参数和创建深度神经网络。享受吧！
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, the goal was to get you up and running in the exciting world
    of neural networks and deep learning. We examined how the methods work, their
    benefits, and their inherent drawbacks with applications to two different datasets.
    These techniques work well where complex, nonlinear relationships exist in the
    data. However, they are highly complex, potentially require a ton of hyper-parameter
    tuning, are the quintessential black boxes, and are difficult to interpret. We
    don't know why the self-driving car made a right on red, we just know that it
    did so properly. I hope you will apply these methods by themselves or supplement
    other methods in an ensemble modeling fashion. Good luck and good hunting! We
    will now shift gears to unsupervised learning, starting with clustering.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，目标是让您在神经网络和深度学习的激动人心的世界中开始运行。我们探讨了这些方法的工作原理、它们的优点以及它们固有的缺点，并应用到了两个不同的数据集上。这些技术在数据中存在复杂、非线性关系的地方效果很好。然而，它们非常复杂，可能需要大量的超参数调整，是典型的黑盒子，且难以解释。我们不知道自动驾驶汽车为什么在红灯时右转，我们只知道它转对了。希望您能单独应用这些方法，或者以集成建模的方式补充其他方法。祝您好运，祝您狩猎愉快！我们现在将转换到无监督学习，从聚类开始。
