- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Protecting User Privacy with Differential Privacy
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用差分隐私保护用户隐私
- en: With the growing prevalence of machine learning, some concerns have been raised
    about how it could potentially be a risk to user privacy. Prior research has shown
    that even carefully anonymized datasets can be analyzed by attackers and de-anonymized
    using pattern analysis or background knowledge. The core idea that privacy is
    based upon is a user’s right to control the collection, storage, and use of their
    data. Additionally, privacy regulations mandate that no sensitive information
    about a user should be leaked, and they also restrict what user information can
    be used for machine learning tasks such as ad targeting or fraud detection. This
    has led to concerns about user data being used for machine learning, and privacy
    is a crucial topic every data scientist needs to know about.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的日益普及，人们对其可能对用户隐私构成的风险提出了担忧。先前的研究表明，即使是经过仔细匿名化的数据集也可能被攻击者分析，并使用模式分析或背景知识进行去匿名化。隐私建立的核心思想是用户有权控制其数据的收集、存储和使用。此外，隐私法规规定，不应泄露任何关于用户的敏感信息，并且也限制了可用于机器学习任务（如广告定位或欺诈检测）的用户信息。这导致了人们对用户数据用于机器学习的担忧，隐私是每个数据科学家都需要了解的关键话题。
- en: This chapter covers differential privacy, a technique used to perform data analysis
    while maintaining user privacy at the same time. Differential privacy aims to
    add noise to data and query results, such that the query accuracy is retained
    but no user data is leaked. This can help with simple analytical tasks as well
    as machine learning. We will start by understanding the fundamentals of privacy
    and what it means for users, and for you as engineers, scientists, and developers.
    We will then also work our way through privacy by design, and the legal implications
    of violating privacy regulations. Finally, we will implement differential privacy
    in machine learning and deep learning models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了差分隐私，这是一种在保持用户隐私的同时进行数据分析的技术。差分隐私旨在向数据和查询结果添加噪声，以保持查询的准确性，同时不泄露任何用户数据。这可以帮助进行简单的分析任务以及机器学习。我们将首先了解隐私的基础以及它对用户、工程师、科学家和开发者的意义。然后，我们还将探讨隐私设计以及违反隐私法规的法律影响。最后，我们将实现机器学习和深度学习模型中的差分隐私。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: The basics of privacy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私基础
- en: Differential privacy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分隐私
- en: Differentially private machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分隐私机器学习
- en: Differentially private deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差分隐私深度学习
- en: By the end of this chapter, you will have a better understanding of why privacy
    is important and how it can be incorporated into machine learning systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将更好地理解隐私的重要性以及如何将其纳入机器学习系统。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files for this chapter on GitHub at https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%2010.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，网址为https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%2010。
- en: The basics of privacy
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私基础
- en: Privacy is the ability of an individual or a group of individuals to control
    their personal information and to be able to decide when, how, and to whom that
    information is shared. It involves the right to be free from unwanted or unwarranted
    intrusion into their personal life and the right to maintain the confidentiality
    of personal data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私是指个人或一组个人控制其个人信息的能力，并能够决定何时、如何以及与谁分享这些信息。它涉及免受不受欢迎或不正当的侵扰个人生活的权利，以及维护个人数据机密性的权利。
- en: Privacy is an important aspect of individual autonomy, and it is essential for
    maintaining personal freedom, dignity, and trust in personal relationships. It
    can be protected by various means, such as legal safeguards, technological measures,
    and social norms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私是个体自主性的重要方面，对于维护个人自由、尊严以及个人关系中的信任至关重要。它可以通过各种手段得到保护，例如法律保障、技术措施和社会规范。
- en: With the increasing use of technology in our daily lives, privacy has become
    an increasingly important concern, particularly in relation to the collection,
    use, and sharing of personal data by organizations and governments. As a result,
    there has been growing interest in developing effective policies and regulations
    to protect individual privacy. In this section, we will cover the fundamental
    concepts of privacy, associated legal measures, and the implications to machine
    learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术在日常生活中的日益普及，隐私问题变得越来越重要，尤其是在组织和国家机关收集、使用和共享个人数据方面。因此，人们越来越关注制定有效的政策和法规来保护个人隐私。在本节中，我们将介绍隐私的基本概念、相关法律措施以及其对机器学习的影响。
- en: Core elements of data privacy
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据隐私的核心要素
- en: 'The fundamental principle behind data privacy is that users should be able
    to answer and have control over the following questions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私背后的基本原则是用户应该能够回答并控制以下问题：
- en: What data about me is being collected?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在收集关于我的哪些数据？
- en: What will that data be used for?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 那些数据将用于什么目的？
- en: Who will have access to the data?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将有权访问数据？
- en: How will the data be protected?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据将如何得到保护？
- en: In this section, we will explore these concepts in detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细探讨这些概念。
- en: Data collection
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: '**Data collection** refers to the process of gathering personal information
    or data from individuals. This data can include any information that can identify
    an individual, such as name, address, phone number, email, date of birth, social
    security number, and so on. Organizations that collect personal data must ensure
    that the data is collected only for specific, legitimate purposes and that individuals
    are made aware of what data is being collected and why. In the case of fraud detection
    or other security applications, data collection may seem overly intrusive (such
    as private messages being collected for detecting abuse, or computer processes
    for detecting malware). Additionally, data collection must comply with applicable
    laws and regulations, and organizations must obtain explicit consent from individuals
    before collecting their data.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据收集**指的是从个人收集个人信息或数据的过程。这些数据可以包括任何可以识别个人的信息，例如姓名、地址、电话号码、电子邮件、出生日期、社会保险号等。收集个人数据的组织必须确保数据只用于具体、合法的目的，并且个人应了解正在收集哪些数据以及为什么收集。在欺诈检测或其他安全应用的情况下，数据收集可能显得过于侵入（例如收集私人消息以检测滥用，或计算机进程以检测恶意软件）。此外，数据收集必须遵守适用的法律和法规，并且组织在收集个人数据之前必须获得个人的明确同意。'
- en: Data use
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据使用
- en: '**Data use** refers to how the collected data is used by organizations or individuals.
    Organizations must ensure that they use personal data only for the specific, legitimate
    purposes for which it was collected and that they do not use the data for any
    other purposes without the individual’s explicit consent. Additionally, organizations
    must ensure that they do not use personal data in a way that discriminates against
    individuals, such as denying them services or opportunities based on their personal
    characteristics. Data use also includes using the data for machine learning models
    as training – some users may not want their data to be used for training or analysis.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据使用**指的是组织或个人如何使用收集到的数据。组织必须确保他们只将个人数据用于收集该数据的具体、合法目的，并且未经个人明确同意，不得将数据用于其他任何目的。此外，组织必须确保他们不以歧视个人的方式使用个人数据，例如根据个人的特征拒绝他们服务或机会。数据使用还包括将数据用于机器学习模型的训练——一些用户可能不希望他们的数据被用于训练或分析。'
- en: Data access
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据访问
- en: '**Data access** refers to the control that individuals have over their personal
    data. Individuals have the right to know what data is being collected about them,
    who is collecting it, and why it is being collected. They also have the right
    to access their own personal data and correct any inaccuracies. Additionally,
    individuals have the right to know who their data is being shared with and for
    what purposes. This also includes data sharing with other organizations, applications,
    and services (for example, a shopping website selling your search history with
    a marketing company). Personal data should only be shared with the individual’s
    explicit consent and should only be shared for specific, legitimate purposes.
    Organizations must ensure that they have appropriate security measures in place
    to protect personal data from unauthorized access, disclosure, alteration, or
    destruction.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据访问**指的是个人对其个人数据的控制。个人有权了解有关他们的数据正在被收集什么，谁在收集它，以及为什么收集它。他们也有权访问自己的个人数据并纠正任何不准确之处。此外，个人还有权了解他们的数据与谁共享以及用于什么目的。这还包括与其他组织、应用程序和服务的共享数据（例如，一个购物网站将您的搜索历史与营销公司共享）。个人数据仅应在个人明确同意的情况下共享，并且仅应共享于特定、合法的目的。组织必须确保他们有适当的安保措施来保护个人数据免受未经授权的访问、披露、更改或破坏。'
- en: Data protection
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据保护
- en: '**Data protection** refers to the measures taken to protect personal data from
    unauthorized access, disclosure, alteration, or destruction. This includes technical,
    physical, and administrative measures to ensure the security and confidentiality
    of personal data. Organizations must ensure that they have appropriate security
    measures in place to protect personal data, such as encryption, access controls,
    and firewalls. Additionally, organizations must ensure that they have policies
    and procedures in place to detect and respond to security incidents or breaches.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据保护**指的是为防止个人数据被未经授权的访问、披露、更改或破坏而采取的措施。这包括技术、物理和行政措施，以确保个人数据的安全和保密性。组织必须确保他们有适当的安保措施来保护个人数据，例如加密、访问控制和防火墙。此外，组织必须确保他们有政策和程序来检测和应对安全事件或泄露。'
- en: Privacy and the GDPR
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私与GDPR
- en: While privacy has to do with user consent on data, it is not a purely ethical
    or moral concern – there are legal requirements and regulations that organizations
    must comply with. The **GDPR** stands for the **General Data Protection Regulation**.
    It is a comprehensive data protection law that came into effect on May 25, 2018,
    in the **European** **Union** (**EU**).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然隐私与用户对数据的同意有关，但它不仅仅是纯粹伦理或道德问题——组织必须遵守法律要求和法规。**GDPR**代表**通用数据保护条例**。这是一项全面的数据保护法律，于2018年5月25日在**欧洲联盟**（**EU**）生效。
- en: The GDPR regulates the processing of personal data of individuals within the
    EU, as well as the export of personal data outside the EU. It gives individuals
    more control over their personal data and requires organizations to be transparent
    about how they collect, use, and store personal data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GDPR规范了欧盟内个人的个人数据处理，以及个人数据向欧盟外部的出口。它赋予个人对其个人数据的更多控制权，并要求组织对其如何收集、使用和存储个人数据保持透明。
- en: 'The GDPR sets out several key principles, including the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GDPR规定了几个关键原则，包括以下内容：
- en: Lawfulness, fairness, and transparency
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合法性、公平性和透明度
- en: Purpose limitation
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目的限定
- en: Data minimization
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据最小化
- en: Accuracy
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确性
- en: Storage limitation
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储限制
- en: Integrity and confidentiality (security)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整性和保密性（安全性）
- en: Accountability
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 责任制
- en: Under the GDPR, individuals have the right to access their personal data, correct
    inaccurate data, have their data erased in certain circumstances, and object to
    the processing of their data. Organizations that fail to comply with the GDPR
    can face significant fines and other sanctions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 根据《通用数据保护条例》（GDPR），个人有权访问他们的个人数据，纠正不准确的数据，在特定情况下删除他们的数据，并反对处理他们的数据。未能遵守GDPR的组织可能面临重大罚款和其他制裁。
- en: 'For example, in January 2019, the CNIL (the French data protection authority)
    fined Google €50 million for GDPR violations related to the company’s ad personalization
    practices. The CNIL found that Google had violated the GDPR in two key ways:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，2019年1月，法国数据保护机构CNIL（法国数据保护机构）因谷歌公司广告个性化实践违反GDPR而对其处以5000万欧元罚款。CNIL发现谷歌在两个方面违反了GDPR：
- en: '**Lack of transparency**: The CNIL found that Google had not provided users
    with clear and easily accessible information about how their personal data was
    being used for ad personalization. The information was spread across several different
    documents, making it difficult for users to understand.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏透明度**：法国国家信息与自由委员会发现，谷歌没有向用户提供关于其个人数据如何用于广告个性化清晰且易于访问的信息。这些信息散布在几份不同的文件中，使得用户难以理解。'
- en: '**Lack of valid consent**: The CNIL found that Google had not obtained valid
    consent from users for ad personalization. The consent was not sufficiently informed,
    as users were not clearly told what specific data was being collected and how
    it was being used.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏有效同意**：法国国家信息与自由委员会发现，谷歌没有从用户那里获得关于广告个性化的有效同意。同意并不充分知情，因为用户没有被告知具体收集哪些数据以及如何使用这些数据。'
- en: The CNIL’s investigation was initiated following two complaints filed by privacy
    advocacy groups, **None Of Your Business** (**NOYB**) and La Quadrature du Net,
    on May 25, 2018, the same day that the GDPR came into effect.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 法国国家信息与自由委员会的调查是在2018年5月25日，即GDPR生效的同一天，由隐私倡导组织**None Of Your Business**（NOYB）和La
    Quadrature du Net提交的两项投诉后开始的。
- en: In addition to the fine, the CNIL ordered Google to make changes to its ad personalization
    practices, including making it easier for users to access and understand information
    about how their data is being used and obtaining valid consent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了罚款外，法国国家信息与自由委员会（CNIL）还命令谷歌修改其广告个性化实践，包括让用户更容易访问和理解他们数据被使用的信息，以及获取有效同意。
- en: The Google fine was significant, as it was the largest GDPR fine at the time
    and demonstrated that regulators were willing to take enforcement action against
    large tech companies for GDPR violations. The fine also underscored the importance
    of transparency and valid consent in data processing under the GDPR.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的罚款数额巨大，因为这是当时最大的GDPR罚款，表明监管机构愿意对大型科技公司采取执法行动以违反GDPR。罚款还强调了在GDPR下数据处理中透明度和有效同意的重要性。
- en: The GDPR has had a significant impact on how organizations handle personal data,
    not only in the EU but also worldwide, as many companies have had to update their
    policies and practices to comply with the regulation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GDPR对组织处理个人数据的方式产生了重大影响，不仅限于欧盟，而且全球范围内，因为许多公司不得不更新其政策和实践以符合该法规。
- en: Privacy by design
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私设计
- en: '**Privacy by design** is an approach to privacy protection that aims to embed
    privacy and data protection into the design and architecture of systems, products,
    and services from the outset. The concept was first introduced by the Information
    and Privacy Commissioner of Ontario, Canada, in the 1990s, and has since been
    adopted as a best practice by privacy regulators and organizations worldwide.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐私设计**是一种隐私保护方法，旨在从一开始就将隐私和数据保护嵌入到系统、产品和服务的架构中。这一概念最初由加拿大安大略省的信息和隐私专员在20世纪90年代提出，此后已被全球隐私监管机构和组织采纳为最佳实践。'
- en: The privacy-by-design approach involves proactively identifying and addressing
    privacy risks, rather than trying to retrofit privacy protections after a system
    or product has been developed. It requires organizations to consider privacy implications
    at every stage of the design process, from the initial planning and conceptualization
    phase through to implementation and ongoing operation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私设计方法涉及主动识别和解决隐私风险，而不是在系统或产品开发后再尝试添加隐私保护。它要求组织在设计过程的每个阶段考虑隐私影响，从最初的规划和概念化阶段到实施和持续运营。
- en: 'As data scientists and machine learning engineers, if you are designing any
    system at scale, understanding privacy concerns is important. You should follow
    the principles of privacy by design while developing any system. There are five
    key principles that define privacy by design: proactivity, privacy as default,
    privacy embedded into the design, full functionality, and end-to-end security.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家和机器学习工程师，如果您在设计任何大规模系统，理解隐私问题很重要。您应该在开发任何系统时遵循隐私设计原则。有五个关键原则定义了隐私设计：主动性、默认隐私、隐私嵌入到设计中、完整功能和端到端安全。
- en: Proactive not reactive
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 积极而非被动
- en: The principle of being proactive not reactive means that organizations should
    anticipate potential privacy risks and take steps to mitigate them before they
    become a problem. This involves conducting **privacy impact assessments** (**PIAs**)
    to identify and address potential privacy issues at the outset of a project. By
    taking a proactive approach to privacy, organizations can reduce the likelihood
    of privacy breaches, protect individual rights, and build trust with their customers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 积极而非被动原则意味着组织应预测潜在的隐私风险，并在它们成为问题之前采取措施减轻这些风险。这包括在项目初期进行**隐私影响评估**（**PIAs**）以识别和解决潜在的隐私问题。通过采取积极的隐私方法，组织可以降低隐私泄露的可能性，保护个人权利，并与客户建立信任。
- en: Privacy as the default setting
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私作为默认设置
- en: The principle of privacy as the default setting means that individuals should
    not have to take any action to protect their privacy. This means that privacy
    protection should be built into systems, products, and services by default and
    that individuals should not be required to opt out of sharing their data. By making
    privacy the default setting, individuals are empowered to make informed decisions
    about their personal information, without having to navigate complex privacy settings
    or policies.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私作为默认设置原则意味着个人不应需要采取任何行动来保护他们的隐私。这意味着隐私保护应默认集成到系统和产品中，个人不应被要求选择退出数据共享。通过将隐私设置为默认设置，个人可以有权就其个人信息做出明智的决定，而无需导航复杂的隐私设置或政策。
- en: Privacy embedded into the design
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私嵌入到设计中
- en: The principle of embedding privacy into the design means that privacy should
    be a core consideration in the development of systems, products, and services
    from the outset. This involves incorporating privacy features and controls into
    the design of these products and services, such as anonymization, encryption,
    and data minimization. By building privacy into the design of products and services,
    organizations can help ensure that privacy is protected by default, rather than
    as an afterthought.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将隐私嵌入到设计中的原则意味着隐私应从系统、产品和服务的开发之初就是一个核心考虑因素。这包括将这些产品和服务的设计中纳入隐私功能和控制，如匿名化、加密和数据最小化。通过将隐私构建到产品和服务的设计中，组织可以帮助确保隐私默认得到保护，而不是作为事后考虑。
- en: Full functionality
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完全功能
- en: The principle of full functionality means that privacy protections should not
    come at the expense of functionality or usability. This means that privacy protections
    should be integrated into systems and products without compromising their performance
    or functionality. By adopting a positive-sum approach to privacy, organizations
    can build trust with their customers and demonstrate that they take privacy seriously.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 完全功能原则意味着隐私保护不应以牺牲功能或可用性为代价。这意味着隐私保护应集成到系统和产品中，而不会损害其性能或功能。通过采用积极的隐私方法，组织可以与客户建立信任，并表明他们认真对待隐私。
- en: End-to-end security
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 端到端安全
- en: The principle of end-to-end security means that comprehensive security measures
    should be implemented throughout the entire life cycle of a product or service,
    from development to disposal. This involves implementing a range of security measures,
    such as access controls, encryption, and monitoring, to protect against unauthorized
    access, use, and disclosure of personal information. By taking a comprehensive
    approach to security, organizations can help ensure that personal information
    is protected at every stage of the life cycle and build trust with their customers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端安全原则意味着应在整个产品或服务的生命周期内实施全面的安全措施，从开发到废弃。这包括实施一系列安全措施，如访问控制、加密和监控，以防止未经授权的访问、使用和披露个人信息。通过采取全面的安全方法，组织可以帮助确保个人信息在生命周期的每个阶段都得到保护，并与客户建立信任。
- en: Privacy and machine learning
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私与机器学习
- en: 'Why did we spend all this time discussing the concepts behind privacy, the
    elements of data privacy, and the GDPR? In security areas (such as fraud, abuse,
    and misinformation), significant types and amounts of user data are collected.
    Some of it might be deemed obtrusive, such as the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么花费这么多时间讨论隐私背后的概念、数据隐私的要素以及GDPR？在安全领域（如欺诈、滥用和虚假信息），会收集大量和多种类型用户数据。其中一些可能被认为是侵扰性的，例如以下内容：
- en: Browsers collecting users’ mouse movements and click patterns to detect click
    fraud and bots
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览器收集用户的鼠标移动和点击模式以检测点击欺诈和机器人
- en: Security software collecting information on system processes to detect the presence
    of malware
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全软件收集系统进程信息以检测恶意软件的存在
- en: Social media companies extracting information from private messages and images
    to detect child pornography
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体公司从私信和图片中提取信息以检测儿童色情
- en: For data scientists in the security domain, the ultimate goal is to provide
    maximum user security by building a system with the highest precision and recall.
    However, at the same time, it is important to understand the limitations you will
    face in data collection and use while designing your systems. For example, if
    you are building a system for fraud detection, you may not be able to use cookie
    data in France. Additionally, the GDPR will apply if the data you are collecting
    is from European users.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于安全领域的数据科学家来说，最终目标是构建一个具有最高精确度和召回率的系统，以提供最大的用户安全性。然而，在设计系统时，了解您在数据收集和使用过程中将面临哪些限制同样重要。例如，如果您正在构建一个用于欺诈检测的系统，您可能无法在法国使用cookie数据。此外，如果收集的数据来自欧洲用户，GDPR将适用。
- en: Depending on the jurisdiction, you may not be able to collect certain data,
    or even if it is collected, you may not be able to use it for machine learning
    models. These factors must be taken into consideration as you design your systems
    and algorithms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据司法管辖权，您可能无法收集某些数据，或者即使收集到了，也可能无法将其用于机器学习模型。在设计系统和算法时，必须考虑这些因素。
- en: Furthermore, we know that machine learning is based on identifying trends and
    patterns from data. Privacy considerations and regulations will severely limit
    your ability to collect data, extract features, and train models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们知道机器学习是基于从数据中识别趋势和模式。隐私考虑和法规将严重限制您收集数据、提取特征和训练模型的能力。
- en: Now that you have been introduced to the fundamentals of privacy, we will look
    at differential privacy, which is considered to be state-of-the-art in privacy
    and is used by many tech giants.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了隐私的基本知识，我们将探讨差分隐私，它被认为是隐私领域的最先进技术，并被许多科技巨头所采用。
- en: Differential privacy
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差分隐私
- en: In this section, we will cover the basics of differential privacy, including
    the mathematical definition and a real-world example.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍差分隐私的基本知识，包括数学定义和现实世界示例。
- en: What is differential privacy?
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是差分隐私？
- en: '**Differential privacy** (**DP**) is a framework for preserving the privacy
    of individuals in a dataset when it is used for statistical analysis or machine
    learning. The goal of DP is to ensure that the output of a computation on a dataset
    does not reveal sensitive information about any individual in the dataset. This
    is accomplished by adding controlled noise to the computation in order to mask
    the contribution of any individual data point.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**差分隐私**（**DP**）是在对数据集进行统计分析或机器学习时保护数据集中个人隐私的框架。DP的目标是确保对数据集进行计算的结果不会泄露数据集中任何个人的敏感信息。这是通过向计算中添加受控噪声来实现的，以掩盖任何单个数据点的贡献。'
- en: DP provides a mathematically rigorous definition of privacy protection by quantifying
    the amount of information that an attacker can learn about an individual by observing
    the output of a computation. Specifically, DP requires that the probability of
    observing a particular output from a computation is roughly the same whether a
    particular individual is included in the dataset or not.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私（DP）通过量化攻击者通过观察计算输出所能了解的关于个人的信息量，提供了一个对隐私保护的数学严格定义。具体来说，DP要求从计算中观察到特定输出的概率，无论特定个人是否包含在数据集中，大致相同。
- en: 'Formally speaking, let D and D ′  be two datasets that differ by, at most,
    one element, and let f be a function that takes a dataset as input and produces
    an output in some range, *R*. Then, the f function satisfies ε-differential privacy
    for any two datasets D and D ′ :'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，设D和D′是两个最多只相差一个元素的数据集，设f是一个以数据集为输入并在某个范围R中产生输出的函数。那么，f函数对于任何两个数据集D和D′都满足ε-差分隐私：
- en: Pr[f(D) ∈ S] ≤ e ε . Pr[f(D ′ ) ∈ S]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Pr[f(D) ∈ S] ≤ e ε . Pr[f(D′) ∈ S]
- en: where S is any subset of *R* and *δ* is a small positive number that accounts
    for the probability of events that have low probability. In other words, the probability
    of the output of the f function on dataset D falling within a set S should be
    very similar to the probability of the output of the f function on dataset D ′ 
    falling within the same set S, up to a multiplicative factor of exp(ε). The smaller
    the value of ε, the stronger the privacy protection, but also the less accurate
    the results. The *δ* parameter is typically set to a very small value, such as
    10-9, to ensure that the overall privacy guarantee is strong.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 S 是 *R* 的任何子集，*δ* 是一个很小的正数，用于考虑低概率事件发生的概率。换句话说，f 函数在数据集 D 上的输出落在集合 S 内的概率应该与
    f 函数在数据集 D' 上的输出落在同一集合 S 内的概率非常相似，直到一个乘以 exp(ε) 的因子。ε 的值越小，隐私保护越强，但结果也越不准确。*δ*
    参数通常设置为一个非常小的值，例如 10^-9，以确保整体隐私保证是强大的。
- en: The key idea behind DP is to add random noise to the computation in a way that
    preserves the statistical properties of the data while obscuring the contribution
    of any individual. The amount of noise added is controlled by a parameter called
    the **privacy budget**, which determines the maximum amount of privacy loss that
    can occur during the computation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私背后的关键思想是以一种保留数据统计属性的同时掩盖任何个人贡献的方式向计算中添加随机噪声。添加的噪声量由一个称为**隐私预算**的参数控制，该参数决定了计算过程中可能发生的最大隐私损失量。
- en: 'There are several mechanisms for achieving differential privacy, including
    the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实现差分隐私有几种机制，包括以下几种：
- en: '**Laplace mechanism**: The Laplace mechanism adds random noise to the output
    of a computation based on the sensitivity of the computation. The amount of noise
    added is proportional to the sensitivity of the computation and inversely proportional
    to the privacy budget.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拉普拉斯机制**：拉普拉斯机制根据计算的敏感性向计算输出添加随机噪声。添加的噪声量与计算的敏感性成正比，与隐私预算成反比。'
- en: '**Exponential mechanism**: The Exponential mechanism is used to select an output
    from a set of possible outputs in a way that minimizes the amount of information
    revealed about any individual. This mechanism selects the output with the highest
    utility score, where the utility score is a measure of how well the output satisfies
    the desired properties of the computation.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数机制**：指数机制用于从一组可能的输出中选择一个输出，以最小化关于任何个人的信息泄露量。此机制选择具有最高效用分数的输出，其中效用分数是衡量输出满足计算所需属性程度的一个指标。'
- en: '**Randomized response**: Randomized response is a mechanism used to obtain
    accurate estimates of binary data while preserving privacy. The mechanism involves
    flipping the value of the data point with a certain probability, which is determined
    by the privacy budget.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机响应**：随机响应是一种用于在保护隐私的同时获得二元数据准确估计的机制。该机制涉及以一定概率翻转数据点的值，该概率由隐私预算决定。'
- en: DP has become increasingly important in recent years due to the widespread use
    of data in machine learning and statistical analysis. DP can be used to train
    machine learning models on sensitive data while ensuring that the privacy of individuals
    in the dataset is preserved. It is also used in other applications, such as census
    data and medical research.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据在机器学习和统计分析中的广泛应用，差分隐私在近年来变得越来越重要。差分隐私可用于在敏感数据上训练机器学习模型，同时确保数据集中个人的隐私得到保护。它还用于其他应用，如人口普查数据和医学研究。
- en: Differential privacy – a real-world example
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分隐私 – 一个现实世界的例子
- en: The concept of differential privacy can be clarified in detail using a practical
    example. Suppose a credit card company has a dataset containing information about
    the transaction amounts and times of its customers, and they want to identify
    potential cases of fraud. However, the credit card company is concerned about
    preserving the privacy of its customers and wants to ensure that the analysis
    cannot be used to identify the transaction amounts or times of any individual
    customer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用一个实际例子详细阐明差分隐私的概念。假设一家信用卡公司有一个包含其客户交易金额和时间信息的数据库，他们想识别潜在的欺诈案例。然而，信用卡公司担心保护其客户的隐私，并希望确保分析不能用于识别任何个别客户的交易金额或时间。
- en: To accomplish this, the credit card company can use differential privacy to
    add noise to the analysis. Specifically, they can add random noise to the computed
    statistics in a way that preserves the overall statistical properties of the data
    but makes it difficult to determine the transaction amounts or times of any individual
    customer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，信用卡公司可以使用差分隐私来向分析中添加噪声。具体来说，他们可以在保持数据整体统计特性的同时，向计算出的统计数据添加随机噪声，使得确定任何单个客户的交易金额或时间变得困难。
- en: For example, the credit card company could use the Laplace mechanism to add
    noise to the computed statistics. Let’s say the credit card company wants to compute
    the total transaction amount for a specific time period, and the sensitivity of
    the computation is *1*, meaning that changing the amount of one transaction can
    change the computed total by, at most, 1 dollar. The credit card company wants
    to achieve a privacy budget of *epsilon = 1*, meaning that the probability of
    observing a particular output from the computation should be roughly the same
    whether a particular customer is included in the dataset or not.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，信用卡公司可以使用拉普拉斯机制向计算出的统计数据添加噪声。假设信用卡公司想要计算特定时间段的交易总额，计算的敏感性为*1*，这意味着改变一笔交易的金额最多可以改变计算出的总额1美元。信用卡公司希望实现隐私预算*epsilon
    = 1*，这意味着观察特定计算输出的概率应该在大约相同，无论特定客户是否包含在数据集中。
- en: Using the Laplace mechanism with these parameters, the credit card company can
    add noise drawn from a Laplace distribution with a scale parameter of *1/epsilon
    = 1*. This will add random noise to the computed total in a way that preserves
    the overall statistical properties of the data but makes it difficult to determine
    the transaction amounts or times of any individual customer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数的拉普拉斯机制，信用卡公司可以向计算出的总额添加来自拉普拉斯分布的噪声，其尺度参数为*1/epsilon = 1*。这将以保持数据整体统计特性的方式向计算出的总额添加随机噪声，但使得确定任何单个客户的交易金额或时间变得困难。
- en: For example, the computed total transaction amount might be $10,000, but with
    the added noise, it might be reported as $10,100\. This ensures that the analysis
    cannot be used to identify the transaction amounts or times of any individual
    customer with high confidence, while still providing useful information about
    the overall transaction amounts for the specific time period.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，计算出的总交易金额可能是$10,000，但添加噪声后，可能会报告为$10,100。这确保了分析不能被用来以高置信度识别任何单个客户的交易金额或时间，同时仍然提供了关于特定时间段的总体交易金额的有用信息。
- en: However, suppose the credit card company wants to achieve a higher level of
    privacy protection and sets the privacy budget to *epsilon = 10* instead of *epsilon
    = 1*. This means that the added noise will be larger and the analysis will be
    more private, but it will also be less accurate. For example, the computed total
    transaction amount might be reported as $15,000 with *epsilon = 10*, which is
    further from the true value of $10,000.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设信用卡公司想要实现更高的隐私保护水平，并将隐私预算设置为*epsilon = 10*而不是*epsilon = 1*。这意味着添加的噪声将更大，分析将更加私密，但也将更不准确。例如，当*epsilon
    = 10*时，计算出的总交易金额可能会报告为$15,000，这比真实值$10,000更远。
- en: In summary, differential privacy can be used in the context of fraud detection
    to protect the privacy of individuals in a dataset while still allowing useful
    statistical analysis to be performed. However, the choice of privacy budget (epsilon)
    is important and should be balanced with the level of privacy protection and the
    desired accuracy of the analysis.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，差分隐私可以在欺诈检测的背景下使用，以保护数据集中个人的隐私，同时仍然允许进行有用的统计分析。然而，隐私预算（epsilon）的选择很重要，应该与隐私保护水平和分析所需的准确性相平衡。
- en: Benefits of differential privacy
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分隐私的好处
- en: Why use differential privacy at all? What benefits will it provide to users?
    And what benefits, if any, will it provide to engineers, researchers, and scientists?
    There are several key benefits that differential privacy provides, including strong
    user privacy guarantees, flexibility in analysis, balance between privacy and
    utility, robustness, and transparency. These are more important in the cybersecurity
    domain than others, as we discussed in [*Chapter 1*](B19327_01.xhtml#_idTextAnchor013),
    *On Cybersecurity and* *Machine Learning*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用差分隐私？它将为用户带来哪些好处？它将为工程师、研究人员和科学家带来哪些好处（如果有）？差分隐私提供了几个关键好处，包括强大的用户隐私保障、分析的灵活性、隐私与效用的平衡、弹性以及透明度。这些在网络安全领域比其他领域更为重要，正如我们在[*第一章*](B19327_01.xhtml#_idTextAnchor013)中讨论的，*关于网络安全与机器学习*。
- en: User privacy guarantees
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用户隐私保障
- en: Differential privacy provides a rigorous mathematical definition of privacy
    protection that offers strong guarantees of privacy. It ensures that an individual’s
    personal data cannot be distinguished from the data of others in the dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私提供了一个严格的数学定义的隐私保护，提供了强大的隐私保障。它确保个人的个人数据在数据集中不能与其他人的数据区分开来。
- en: In a cybersecurity context, differential privacy can be used to protect the
    privacy of user data in security logs. For example, let’s say a security analyst
    is examining a log of user login attempts. Differential privacy can be used to
    protect the privacy of individual users by adding random noise to the log data
    so that it is impossible to determine whether a specific user attempted to log
    in.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全环境中，差分隐私可以用来保护安全日志中用户数据的隐私。例如，假设一个安全分析师正在检查用户登录尝试的日志。差分隐私可以通过向日志数据添加随机噪声来保护个别用户的隐私，使得无法确定是否特定用户尝试登录。
- en: Flexibility
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 灵活性
- en: Differential privacy can be applied to a wide range of data analysis techniques,
    including queries, machine learning algorithms, and statistical models. In cybersecurity,
    it can be applied to a variety of security-related data analysis techniques. For
    example, it can be used to protect the privacy of user data in intrusion detection
    systems. It can also be applied to the algorithms used by these systems to detect
    anomalous network activity and to ensure that the privacy of individual users
    is protected.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私可以应用于广泛的数据分析技术，包括查询、机器学习算法和统计模型。在网络安全中，它可以应用于各种与安全相关的数据分析技术。例如，它可以用来保护入侵检测系统中用户数据的隐私。它还可以应用于这些系统用于检测异常网络活动并确保保护个别用户隐私的算法。
- en: Privacy-utility trade-off
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私与效用权衡
- en: Differential privacy provides a way to balance privacy and utility so that accurate
    statistical analysis can be performed on a dataset while minimizing the risk of
    exposing sensitive information. It can be used to protect the privacy of sensitive
    data in cybersecurity applications while still allowing useful insights to be
    obtained. For example, it can be used to protect the privacy of user data in threat
    intelligence-sharing systems. It can also be used to protect the privacy of individual
    users while still allowing organizations to share information about threats and
    vulnerabilities.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私提供了一种平衡隐私和效用以进行准确统计分析的方法，同时最大限度地降低泄露敏感信息的风险。它可以在网络安全应用中用来保护敏感数据的隐私，同时仍然允许获得有用的见解。例如，它可以用来保护威胁情报共享系统中用户数据的隐私。它还可以用来保护个别用户的隐私，同时允许组织分享有关威胁和漏洞的信息。
- en: Robustness
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弹性
- en: Differential privacy is robust to various types of attacks, including statistical
    attacks and inference attacks. Differential privacy is designed to protect against
    a wide range of attacks, including statistical attacks and inference attacks.
    For example, in a cybersecurity context, differential privacy can be used to protect
    the privacy of user data in forensic investigations. It can also be used to ensure
    that sensitive data cannot be inferred from forensic evidence, even if an attacker
    has access to a large amount of other data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私对各种类型的攻击具有鲁棒性，包括统计攻击和推理攻击。差分隐私旨在抵御广泛的攻击，包括统计攻击和推理攻击。例如，在网络安全环境中，差分隐私可以用来保护调查中用户数据的隐私。它还可以用来确保即使攻击者访问了大量其他数据，也无法从调查证据中推断出敏感数据。
- en: Transparency
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 透明度
- en: Differential privacy provides a way to quantify the amount of privacy protection
    provided by a particular technique, which allows individuals and organizations
    to make informed decisions about the level of privacy they need for their data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 差分隐私提供了一种量化特定技术提供的隐私保护程度的方法，这使得个人和组织能够就他们数据所需的隐私保护水平做出明智的决定。
- en: It provides a way to measure the effectiveness of privacy protection techniques,
    which can be useful in making decisions about data protection in cybersecurity.
    For example, it can be used to protect the privacy of user data in threat modeling.
    It can also be used to help organizations understand the level of privacy protection
    they need to protect against various types of threats and to measure the effectiveness
    of their existing privacy protection measures.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了一种衡量隐私保护技术有效性的方法，这在做出关于网络安全中数据保护的决定时可能很有用。例如，它可以用于保护威胁建模中用户数据的隐私。它还可以帮助组织了解他们需要保护以抵御各种类型威胁的隐私保护水平，并衡量他们现有隐私保护措施的有效性。
- en: So far, we have looked at privacy and then differential privacy. Now, let us
    see how it can be practically applied in the context of machine learning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了隐私和差分隐私。现在，让我们看看它在机器学习环境中的实际应用。
- en: Differentially private machine learning
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差分隐私机器学习
- en: In this section, we will look at how a fraud detection model can incorporate
    differential privacy. We will first look at the library we use to implement differential
    privacy, followed by how a credit card fraud detection machine learning model
    can be made differentially private.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何将差分隐私集成到欺诈检测模型中。我们将首先查看我们用于实现差分隐私的库，然后是信用卡欺诈检测机器学习模型如何变得差分隐私。
- en: IBM Diffprivlib
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM Diffprivlib
- en: '`Diffprivlib` is an open source Python library that provides a range of differential
    privacy tools and algorithms for data analysis. The library is designed to help
    data scientists and developers apply differential privacy techniques to their
    data in a simple and efficient way.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`Diffprivlib` 是一个开源的 Python 库，提供了一系列差分隐私工具和算法，用于数据分析。该库旨在帮助数据科学家和开发者以简单高效的方式将差分隐私技术应用于他们的数据。'
- en: One of the key features of `Diffprivlib` is its extensive range of differentially
    private mechanisms. These include mechanisms for adding noise to data, such as
    the Gaussian, Laplace, and Exponential mechanisms, as well as more advanced mechanisms,
    such as the hierarchical and subsample mechanisms. The library also includes tools
    for calculating differential privacy parameters, such as sensitivity and privacy
    budget (epsilon), and for evaluating the privacy of a given dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`Diffprivlib` 的一个关键特性是其广泛的差分隐私机制。这些包括向数据添加噪声的机制，如高斯、拉普拉斯和对数机制，以及更高级的机制，如分层和子样本机制。该库还包括用于计算差分隐私参数的工具，如敏感性（sensitivity）和隐私预算（epsilon），以及评估给定数据集隐私性的工具。'
- en: Another important feature of `Diffprivlib` is its ease of use. The library provides
    a simple and intuitive API that allows users to apply differential privacy to
    their data with just a few lines of code. The API is designed to be compatible
    with `scikit-learn`, a popular machine learning library for Python, which makes
    it easy to incorporate differential privacy into existing data analysis workflows.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Diffprivlib` 的另一个重要特性是它的易用性。该库提供了一个简单直观的 API，使用户只需几行代码即可将差分隐私应用于他们的数据。该 API
    设计为与 `scikit-learn` 兼容，这是一个流行的 Python 机器学习库，这使得将差分隐私集成到现有的数据分析工作流程中变得容易。'
- en: In addition to its core functionality, `Diffprivlib` includes a number of advanced
    features and tools that can be used to improve the accuracy and efficiency of
    differential privacy applications. For example, the library includes tools for
    generating synthetic datasets that are differentially private, which can be used
    to test and validate differential privacy mechanisms. It also includes tools for
    differential private machine learning, which can be used to build models that
    are both accurate and privacy-preserving.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其核心功能外，`Diffprivlib` 包含了一系列高级功能和工具，可用于提高差分隐私应用程序的准确性和效率。例如，该库包括用于生成差分隐私合成数据集的工具，这些数据集可用于测试和验证差分隐私机制。它还包括用于差分隐私机器学习的工具，可用于构建既准确又保护隐私的模型。
- en: Overall, `Diffprivlib` provides a powerful set of tools for data privacy that
    can be used in a wide range of applications, from healthcare and finance to social
    media and online advertising. Its extensive range of differentially private mechanisms,
    ease of use, and advanced features make it a valuable resource for anyone looking
    to improve the privacy and security of their data analysis workflows.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，`Diffprivlib`提供了一套强大的数据隐私工具，可用于各种应用，从医疗保健和金融到社交媒体和在线广告。其广泛的不同隐私机制、易用性和高级功能使其成为任何希望提高其数据分析工作流程隐私和安全性的宝贵资源。
- en: In the following sections, we will use `Diffprivlib` to train and evaluate differentially
    private machine learning models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将使用`Diffprivlib`来训练和评估不同的差分隐私机器学习模型。
- en: Credit card fraud detection with differential privacy
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有差分隐私的信用卡欺诈检测
- en: As we know, differential privacy is a framework for preserving the privacy of
    individuals while allowing statistical analysis of their data. Many applications
    today are powered by analysis through machine learning, and hence, the application
    of DP in machine learning has been a field of growing interest and importance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，差分隐私是一个框架，用于在允许对个人数据进行统计分析的同时保护个人的隐私。今天许多应用都是通过机器学习分析来实现的，因此，DP在机器学习中的应用已经成为一个日益增长的兴趣和重要领域。
- en: 'To apply differential privacy to a machine learning technique, we will perform
    the following steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要将差分隐私应用于机器学习技术，我们将执行以下步骤：
- en: '**Define the privacy budget**: The first step is to define the privacy budget,
    which determines the level of privacy protection that will be provided. The privacy
    budget is typically expressed as ε, which is a small positive number. The smaller
    the value of ε, the stronger the privacy protection, but also the less accurate
    the results.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义隐私预算**：第一步是定义隐私预算，它决定了将提供的隐私保护水平。隐私预算通常表示为ε，它是一个小的正数。ε的值越小，隐私保护越强，但结果也越不准确。'
- en: '**Add noise to the data**: To ensure differential privacy, noise is added to
    the data before logistic regression is performed. Specifically, random noise is
    added to each data point, so that the noise cancels out when the data is aggregated.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**向数据添加噪声**：为了确保差分隐私，在执行逻辑回归之前向数据添加噪声。具体来说，对每个数据点添加随机噪声，以便在数据聚合时噪声相互抵消。'
- en: '**Train the model**: Once the data has been randomized, a machine learning
    model is trained on the randomized data. This model will be less accurate than
    a model trained on the original data, but it will still be useful for making predictions.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：一旦数据被随机化，机器学习模型将在随机化数据上训练。这个模型的准确性将低于在原始数据上训练的模型，但它仍然可以用于预测。'
- en: '**Evaluate the model**: Once the model has been trained, it can be used to
    make predictions on new data. The accuracy of the model will depend on the value
    of ε that was chosen, as well as the size and complexity of the dataset.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：一旦模型训练完成，就可以用它对新数据进行预测。模型的准确性将取决于所选择的ε值，以及数据集的大小和复杂性。'
- en: 'In the following sections, we will look at how this can be applied in practice
    to two popular classification models: logistic regression and random forests.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将探讨如何将这些方法应用于实践中的两种流行的分类模型：逻辑回归和随机森林。
- en: Differentially private logistic regression
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 差分隐私逻辑回归
- en: 'First, we will import the required libraries:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所需的库：
- en: '[PRE0]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a simulation, we will be using the credit card fraud detection dataset from
    Kaggle. You can use any dataset of your choice. We split the data into training
    and test sets, with 2% reserved for testing:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为模拟，我们将使用Kaggle的信用卡欺诈检测数据集。你可以使用任何你选择的数据集。我们将数据分为训练集和测试集，其中2%保留用于测试：
- en: '[PRE1]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To print the columns, you can simply run the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要打印列，你可以简单地运行以下命令：
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And you should see the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下内容：
- en: "![Figure 1\uFEFF0.1 – Dataset columns](img/B19327_10_01.jpg)"
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 数据集列](img/B19327_10_01.jpg)'
- en: Figure 10.1 – Dataset columns
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 数据集列
- en: 'We want to use columns `V1` through `V28` and `Amount` as features, and `Class`
    as the label. We then want to split the data into training and test sets:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用列`V1`至`V28`和`Amount`作为特征，`Class`作为标签。然后我们希望将数据分为训练集和测试集：
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we train a logistic regression model to predict the class of the data.
    Note that this is the vanilla logistic regression model from scikit-learn without
    any differential privacy involved:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练一个逻辑回归模型来预测数据的类别。请注意，这是一个来自scikit-learn的普通逻辑回归模型，其中不涉及任何差分隐私：
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we evaluate the performance of this model on the test set:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们评估这个模型在测试集上的性能：
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This should print something like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打印出类似的内容：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Great! We have nearly 99.9% accuracy on the test set.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们在测试集上几乎达到了99.9%的准确率。
- en: 'Now, we fit a differentially private logistic regression model on the same
    data. Here, we set the value of the `epsilon` parameter to `1`. You can set this
    to any value you want, as long as it is not zero (an epsilon of zero indicates
    no differential privacy, and the model will be equivalent to the vanilla one):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在相同的数据上拟合一个差分隐私逻辑回归模型。在这里，我们将`epsilon`参数的值设置为`1`。你可以将其设置为任何你想要的值，只要它不是零（零epsilon表示没有差分隐私，模型将与普通模型等效）：
- en: '[PRE7]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, evaluate it on the test set as we did with the previous model:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像之前模型那样在测试集上评估它：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see an output like this:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似这样的输出：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Wow – that’s a huge drop! The accuracy on the test set dropped from 99.9% to
    about 64%. This is the utility cost associated with increased privacy.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 哇——这是一个巨大的下降！测试集上的准确率从99.9%下降到大约64%。这是增加隐私所伴随的效用成本。
- en: Differentially private random forest
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 差分隐私随机森林
- en: 'As a fun experiment, let us try the same with a random forest. The code remains
    almost the same, except both classifiers are switched to random forests. Here’s
    the code snippet:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项有趣的实验，让我们尝试用随机森林来做同样的实验。代码几乎相同，只是将两个分类器都切换为随机森林。以下是代码片段：
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives the following output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE11]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Interestingly, the drop in accuracy in random forests is much less pronounced
    and is less than 1%. Therefore, random forests would be a better classifier to
    use in this scenario if both increased privacy and utility are to be achieved.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，随机森林中的准确率下降并不明显，并且小于1%。因此，如果既要增加隐私又要提高效用，随机森林将是这个场景中更好的分类器。
- en: Examining the effect of ϵ
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查ϵ的影响
- en: 'Now, we will examine how the accuracy of the classifier on the test set varies
    as we change the value of `epsilon`. For multiple values of `epsilon` from `0`
    to `5`, we will train a differentially private classifier and compute the accuracy
    on the test set:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将检查随着`epsilon`值的改变，分类器在测试集上的准确率如何变化。对于从`0`到`5`的多个`epsilon`值，我们将训练一个差分隐私分类器，并在测试集上计算准确率：
- en: '[PRE12]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After this block is run, we can plot the scores against the corresponding `epsilon`
    values:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此块之后，我们可以将分数与相应的`epsilon`值相对应：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This should show you a plot like this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示一个像这样的图表：
- en: "![Figure 1\uFEFF0.2 – Accuracy variation with epsilon for logistic regression](img/B19327_10_02.jpg)"
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 逻辑回归的epsilon值与准确率的变化](img/B19327_10_02.jpg)'
- en: Figure 10.2 – Accuracy variation with epsilon for logistic regression
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 逻辑回归的epsilon值与准确率的变化
- en: 'How about the same evaluation for a random forest? Just replace the model instantiated
    with a random forest instead of logistic regression. Here is the complete code
    snippet:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机森林，同样的评估结果如何？只需将实例化的模型替换为随机森林而不是逻辑回归。以下是完整的代码片段：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Plotting this gives you the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制这个图表给你以下结果：
- en: "![Figure 1\uFEFF0.3 – Accuracy variation with epsilon for random forest](img/B19327_10_03.jpg)"
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 随机森林的epsilon值与准确率的变化](img/B19327_10_03.jpg)'
- en: Figure 10.3 – Accuracy variation with epsilon for random forest
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 随机森林的epsilon值与准确率的变化
- en: The graph *appears* volatile – but note that the accuracy is always between
    99.8% and 99.83%. This means that higher values of `epsilon` do not cause a meaningful
    difference in accuracy. This model is better suited for differential privacy than
    the logistic regression model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图表*看起来*波动很大——但请注意，准确率始终在99.8%和99.83%之间。这意味着更高的`epsilon`值不会在准确率上造成有意义的变化。这个模型比逻辑回归模型更适合差分隐私。
- en: Differentially private deep learning
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 差分隐私深度学习
- en: In the sections so far, we covered how differential privacy can be implemented
    in standard machine learning classifiers. In this section, we will cover how it
    can be implemented for neural networks.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了如何在标准的机器学习分类器中实现差分隐私。在本节中，我们将介绍如何将其应用于神经网络。
- en: DP-SGD algorithm
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DP-SGD算法
- en: '**Differentially private stochastic gradient descent** (**DP-SGD**) is a technique
    used in machine learning to train models on sensitive or private data without
    revealing the data itself. The technique is based on the concept of differential
    privacy, which guarantees that an algorithm’s output remains largely unchanged,
    even if an individual’s data is added or removed.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**差分隐私随机梯度下降**（**DP-SGD**）是一种机器学习技术，用于在不泄露数据本身的情况下在敏感或私有数据上训练模型。该技术基于差分隐私的概念，该概念保证即使添加或删除个别数据，算法的输出也基本保持不变。'
- en: DP-SGD is a variation of the **stochastic gradient descent** (**SGD**) algorithm,
    which is commonly used for training deep neural networks. In SGD, the algorithm
    updates the model parameters by computing the gradient of the loss function on
    a small randomly selected subset (or “batch”) of the training data. This is done
    iteratively until the algorithm converges with a minimum of the loss function.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: DP-SGD 是 **随机梯度下降**（**SGD**）算法的一种变体，通常用于训练深度神经网络。在 SGD 中，算法通过计算训练数据的小随机子集（或“批次”）上的损失函数的梯度来更新模型参数。这个过程迭代进行，直到算法收敛到损失函数的最小值。
- en: In DP-SGD, the SGD algorithm is modified to incorporate a privacy mechanism.
    Specifically, a small amount of random noise is added to the gradients at each
    iteration, which makes it difficult for an adversary to infer individual data
    points from the output of the algorithm.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DP-SGD 中，SGD 算法被修改以引入隐私机制。具体来说，在每一迭代中都会向梯度添加一小部分随机噪声，这使得攻击者难以从算法的输出中推断出单个数据点。
- en: The amount of noise added to the gradients is controlled by a parameter called
    the privacy budget ε, which determines the maximum amount of information that
    can be leaked about an individual data point. A smaller value of ε corresponds
    to a stronger privacy guarantee but also reduces the accuracy of the model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到梯度中的噪声量由一个称为隐私预算 ε 的参数控制，该参数决定了关于单个数据点可能泄露的最大信息量。ε 的值越小，隐私保证就越强，但也会降低模型的准确性。
- en: The amount of noise added to the gradients is calculated using a technique called
    the Laplace mechanism. The Laplace mechanism adds random noise sampled from the
    Laplace distribution, which has a probability density function proportional to
    *exp(-|x|/b)*, where *b* is the scale parameter. The larger the value of *b*,
    the smaller the amount of noise added.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到梯度中的噪声量是通过一种称为 Laplace 机制的技术来计算的。Laplace 机制添加从 Laplace 分布中采样的随机噪声，其概率密度函数与
    *exp(-|x|/b)* 成正比，其中 *b* 是尺度参数。*b* 的值越大，添加的噪声量就越小。
- en: To ensure that the privacy budget ε is not exceeded over the course of the training
    process, a technique called **moment accountant** is used. Moment accountant estimates
    the cumulative privacy loss over multiple iterations of the algorithm and ensures
    that the privacy budget is not exceeded.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保在整个训练过程中隐私预算 ε 不会超过，使用了一种称为 **矩估计器**的技术。矩估计器估计算法多次迭代的累积隐私损失，并确保隐私预算不会超过。
- en: DP-SGD differs from a standard SGD only in the gradient calculation step. First,
    the gradient is calculated for a batch as the partial derivative of the loss with
    respect to the parameter. Then, the gradients are clipped so that they remain
    within a fixed window. Finally, random noise is added to the gradients to form
    the final gradients. This final gradient is used in the parameter update step
    in gradient descent.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: DP-SGD 与标准 SGD 的区别仅在于梯度计算步骤。首先，计算批次的梯度作为损失函数相对于参数的偏导数。然后，对梯度进行裁剪，以确保它们保持在固定窗口内。最后，向梯度添加随机噪声以形成最终的梯度。这个最终梯度用于梯度下降中的参数更新步骤。
- en: In summary, DP-SGD is a variant of SGD that incorporates a privacy mechanism
    by adding random noise to the gradients at each iteration. The privacy level is
    controlled by a parameter called the privacy budget ε, which determines the amount
    of noise added to the gradients. The Laplace mechanism is used to add the noise,
    and the moment accountant technique is used to ensure that the privacy budget
    is not exceeded.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DP-SGD 是 SGD 的一个变体，通过在每一迭代中向梯度添加随机噪声来引入隐私机制。隐私级别由一个称为隐私预算 ε 的参数控制，该参数决定了添加到梯度中的噪声量。Laplace
    机制用于添加噪声，而矩估计技术用于确保隐私预算不会超过。
- en: 'DP-SGD has several advantages over traditional SGD algorithms:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DP-SGD 相比于传统的 SGD 算法具有几个优点：
- en: '**Privacy-preserving**: The primary advantage of DP-SGD is that it preserves
    the privacy of individual data points. This is particularly important when dealing
    with sensitive or confidential data, such as medical records or financial data.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私保护**：DP-SGD的主要优势在于它保护了单个数据点的隐私。当处理敏感或机密数据，如医疗记录或财务数据时，这一点尤为重要。'
- en: '**Robustness to re-identification attacks**: DP-SGD provides robustness to
    re-identification attacks, which attempt to match the output of the algorithm
    to individual data points. By adding random noise to the gradients, DP-SGD makes
    it difficult for an attacker to distinguish between individual data points.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对重新识别攻击的鲁棒性**：DP-SGD对重新识别攻击具有鲁棒性，这些攻击试图将算法的输出与单个数据点相匹配。通过向梯度添加随机噪声，DP-SGD使得攻击者难以区分单个数据点。'
- en: '**Improved fairness**: DP-SGD can also improve the fairness of machine learning
    models by ensuring that the model does not rely too heavily on any individual
    data point. This can help prevent biases in the model and ensure that it performs
    well across different demographic groups.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高公平性**：DP-SGD还可以通过确保模型不过度依赖任何单个数据点来提高机器学习模型的公平性。这有助于防止模型中的偏差，并确保模型在不同人口群体中表现良好。'
- en: '**Scalability**: DP-SGD can scale to large datasets and complex models. By
    using SGD, DP-SGD can train models on large datasets by processing small batches
    of data at a time. This allows for efficient use of computing resources.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：DP-SGD可以扩展到大型数据集和复杂模型。通过使用随机梯度下降（SGD），DP-SGD可以通过一次处理小批量数据来在大数据集上训练模型。这允许高效地使用计算资源。'
- en: '**Accuracy trade-off**: Finally, DP-SGD offers a trade-off between accuracy
    and privacy. By adjusting the privacy budget ε, the user can control the level
    of privacy protection while still achieving a reasonable level of accuracy. This
    makes DP-SGD a flexible and adaptable tool for machine learning applications.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确度与隐私权衡**：最后，差分隐私随机梯度下降（DP-SGD）在准确度和隐私之间提供了一个权衡。通过调整隐私预算ε，用户可以在保持合理准确度的同时控制隐私保护的水平。这使得DP-SGD成为机器学习应用的灵活和适应性工具。'
- en: Implementation
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现方法
- en: 'We will begin, as usual, by implementing the necessary libraries. Apart from
    the usual processing and deep learning libraries, we will be using a new one,
    known as `tensorflow-privacy`. This library provides tools for adding differential
    privacy to TensorFlow models, including an implementation of the TensorFlow privacy
    algorithm for training deep learning models with differential privacy. The library
    also includes tools for measuring the privacy properties of a model, such as its
    `epsilon` value, which quantifies the level of privacy protection provided by
    the differential privacy mechanism:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像往常一样，首先实现必要的库。除了常用的处理和深度学习库之外，我们还将使用一个新的库，称为`tensorflow-privacy`。这个库提供了为TensorFlow模型添加差分隐私的工具，包括用于使用差分隐私训练深度学习模型的TensorFlow隐私算法的实现。该库还包括测量模型隐私属性的工具，如其`epsilon`值，该值量化了差分隐私机制提供的隐私保护水平：
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We will now write a function that will load and preprocess our MNIST data. The
    MNIST dataset is a large collection of handwritten digits that is commonly used
    as a benchmark dataset for testing machine learning algorithms, particularly those
    related to image recognition and computer vision. The dataset consists of 60,000
    training images and 10,000 testing images, with each image being a grayscale 28x28
    pixel image of a handwritten digit (0-9).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将编写一个函数来加载和预处理我们的MNIST数据。MNIST数据集是一个包含大量手写数字的大型集合，通常用作测试机器学习算法的基准数据集，特别是与图像识别和计算机视觉相关的算法。该数据集包括60,000个训练图像和10,000个测试图像，每个图像都是一个28x28像素的手写数字（0-9）的灰度图像。
- en: 'Our function will first load the training and test sets from this data. The
    data is then scaled to 1/255th its value, followed by reshaping into the image
    dimensions. The labels, which are integers from `0` to `9`, are converted into
    one-hot vectors:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将首先从这个数据中加载训练和测试集。然后，数据被缩放到其值的1/255，随后调整为图像尺寸。标签，即从`0`到`9`的整数，被转换为one-hot向量：
- en: '[PRE16]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, we will define a function that creates our classification model. In this
    case, we will be using CNNs. We have seen and used CNNs in earlier chapters; however,
    we will provide a brief recap here.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个创建分类模型的函数。在这种情况下，我们将使用卷积神经网络（CNNs）。我们在前面的章节中已经看到并使用过CNNs；然而，我们在这里将简要回顾一下。
- en: A CNN is a type of neural network that is specifically designed for image recognition
    and computer vision tasks. CNNs are highly effective at processing and analyzing
    images due to their ability to detect local patterns and features within an image.
    At a high level, a CNN consists of a series of layers, including convolutional
    layers, pooling layers, and fully connected layers. In the convolutional layers,
    the network learns to detect local features and patterns in the input image by
    applying a set of filters to the image. The pooling layers then downsample the
    feature maps obtained from the convolutional layers to reduce the size of the
    input and make the network more computationally efficient. Finally, the fully
    connected layers process the output of the convolutional and pooling layers to
    generate a prediction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）是一种专门为图像识别和计算机视觉任务设计的神经网络。CNNs因其能够检测图像中的局部模式和特征而非常有效地处理和分析图像。从高层次来看，CNN由一系列层组成，包括卷积层、池化层和全连接层。在卷积层中，网络通过将一组滤波器应用于图像来学习检测输入图像中的局部特征和模式。然后，池化层将卷积层获得的特征图下采样，以减少输入的大小并使网络更高效。最后，全连接层处理卷积层和池化层的输出以生成预测。
- en: The key innovation of CNNs is the use of convolutional layers, which allow the
    network to learn spatially invariant features from the input image. This is achieved
    by sharing weights across different parts of the image, which allows the network
    to detect the same pattern regardless of its position within the image. CNNs have
    achieved state-of-the-art performance in a wide range of computer vision tasks,
    including image classification, object detection, and semantic segmentation. They
    have been used in many real-world applications, such as self-driving cars, medical
    image analysis, and facial recognition systems.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的关键创新是卷积层的使用，这使得网络能够从输入图像中学习空间不变特征。这是通过在不同图像部分之间共享权重来实现的，这使得网络能够检测到无论其在图像中的位置如何都相同的模式。CNN在广泛的计算机视觉任务中实现了最先进的性能，包括图像分类、目标检测和语义分割。它们已被用于许多实际应用，如自动驾驶汽车、医学图像分析和面部识别系统。
- en: 'Our model creation function initializes an empty list and adds layers to it
    one by one to build up the CNN structure:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型创建函数初始化一个空列表，并逐个添加层来构建CNN结构：
- en: '[PRE17]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'All of the core functions needed have been defined. Now, we use the data loader
    we implemented earlier to load the training and test data and labels:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的所有核心函数都已定义。现在，我们使用我们之前实现的加载数据和标签的数据加载器来加载训练数据和测试数据：
- en: '[PRE18]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we will set some hyperparameters that will be used by the model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将设置一些模型将使用的超参数：
- en: '`NUM_EPOCHS`: This defines the number of epochs (one epoch is a full pass over
    the training data) that the model will undergo while training.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NUM_EPOCHS`：这定义了模型在训练过程中将经历的epoch数量（一个epoch是对训练数据的完整遍历）。'
- en: '`BATCH_SIZE`: This defines the number of data instances that will be processed
    in one batch. Processing here involves running the data through the network, calculating
    the predicted labels, the loss, and the gradients, and then updating the weights
    by gradient descent.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BATCH_SIZE`：这定义了在一次批次中将被处理的数据实例数量。处理涉及将数据通过网络，计算预测标签、损失和梯度，然后通过梯度下降更新权重。'
- en: '`MICRO_BATCHES`: The dataset is divided into smaller units called microbatches,
    with each microbatch containing a single training example by default. This allows
    us to clip gradients for each individual example, which reduces the negative impact
    of clipping on the gradient signal and maximizes the model’s utility. However,
    increasing the size of microbatches can decrease computational overhead, but it
    involves clipping the average gradient across multiple examples. It’s important
    to note that the total number of training examples consumed in a batch remains
    constant, regardless of the microbatch size. To ensure proper division, the number
    of microbatches should evenly divide the batch size.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MICRO_BATCHES`：数据集被划分为更小的单元，称为微批次，默认情况下每个微批次包含一个训练示例。这允许我们对每个单独的示例剪裁梯度，从而减少剪裁对梯度信号的不利影响并最大化模型的效用。然而，增加微批次的大小可以减少计算开销，但这涉及到跨多个示例剪裁平均梯度。需要注意的是，无论微批次的大小如何，批次中消耗的训练示例总数保持不变。为了确保适当的划分，微批次的数量应该能够均匀地除以批次大小。'
- en: '`L2_NORM_CLIP`: This refers to the maximum L2-norm that is allowed for the
    gradient of the loss function with respect to the model parameters. During training,
    the gradient computed on a minibatch of data is clipped to ensure that its L2-norm
    does not exceed the `L2_NORM_CLIP` value. This clipping operation is an essential
    step in the DP-SGD algorithm because it helps to bind the sensitivity of the gradient
    with respect to the input data. A higher value can lead to better accuracy but
    may decrease privacy guarantees, while a lower value can provide stronger privacy
    guarantees but may result in slower convergence and lower accuracy.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`L2_NORM_CLIP`：这指的是相对于模型参数的损失函数梯度的最大 L2-范数。在训练过程中，对数据的小批量计算的梯度被剪裁以确保其 L2-范数不超过
    `L2_NORM_CLIP` 值。这种剪裁操作是 DP-SGD 算法中的一个重要步骤，因为它有助于将梯度相对于输入数据的敏感性绑定起来。更高的值可能导致更好的准确度，但可能会降低隐私保证，而较低的值可以提供更强的隐私保证，但可能会导致收敛速度变慢和准确度降低。'
- en: '`NOISE_MULTIPLIER`: This controls the amount of noise that is added to the
    gradient updates during training to provide privacy guarantees. In DP-SGD, each
    gradient update is perturbed by a random noise vector to mask the contribution
    of individual training examples to the gradient. A higher value increases the
    amount of noise that is added to the gradient, which in turn provides stronger
    privacy guarantees but can decrease the accuracy of the model.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NOISE_MULTIPLIER`：这控制了在训练过程中添加到梯度更新中的噪声量，以提供隐私保证。在 DP-SGD 中，每个梯度更新都通过一个随机噪声向量进行扰动，以掩盖单个训练示例对梯度的贡献。更高的值会增加添加到梯度中的噪声量，这反过来又提供了更强的隐私保证，但可能会降低模型的准确度。'
- en: '`LEARN_RATE`: This is the learning rate, and as seen in earlier chapters, controls
    the degree to which gradients are updated.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LEARN_RATE`：这是学习率，正如前几章所见，它控制了梯度更新的程度。'
- en: 'Note that the following values we set for these hyperparameters have been derived
    through experimentation. There is no sure way of knowing what the best parameters
    are. In fact, you are encouraged to experiment with different values and examine
    how they affect the privacy and accuracy guarantees of the model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们为这些超参数设置的以下值是通过实验得出的。没有确定最佳参数的方法。实际上，我们鼓励您尝试不同的值，并检查它们如何影响模型的隐私和准确度保证：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will initialize the model using the function we defined earlier, and print
    out a summary to verify the structure:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前定义的函数初始化模型，并打印出摘要以验证结构：
- en: '[PRE20]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will show you something like this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示如下内容：
- en: "![Figure 1\uFEFF0.4 – Model structure](img/B19327_10_04.jpg)"
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.0.4 – 模型结构](img/B19327_10_04.jpg)'
- en: Figure 10.4 – Model structure
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 模型结构
- en: Now, we will define the loss and optimizer used for training. While the loss
    is categorical cross-entropy (as expected for a multi-class classification problem),
    we will not use the standard Adam optimizer here but will use a specialized optimizer
    for differential privacy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义用于训练的损失函数和优化器。虽然损失函数是分类交叉熵（对于多类分类问题这是预期的），但在这里我们不会使用标准的 Adam 优化器，而是将使用一个专门用于差分隐私的优化器。
- en: '`DPKerasSGDOptimizer` is a class in the TensorFlow `Privacy` library that provides
    an implementation of the SGD optimizer with differential privacy guarantees. It
    uses the DP-SGD algorithm, which adds random noise to the gradients computed during
    each step of the SGD optimization process. The amount of noise added is controlled
    by two parameters: the noise multiplier and the clipping norm. The noise multiplier
    determines the amount of noise added to the gradients, while the clipping norm
    limits the magnitude of the gradients to prevent large updates:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`DPKerasSGDOptimizer` 是 TensorFlow `Privacy` 库中的一个类，它提供了一个带有差分隐私保证的 SGD 优化器的实现。它使用
    DP-SGD 算法，该算法在 SGD 优化过程的每一步中向梯度添加随机噪声。添加的噪声量由两个参数控制：噪声乘数和剪裁范数。噪声乘数决定了添加到梯度中的噪声量，而剪裁范数限制了梯度的幅度以防止大的更新：'
- en: '[PRE21]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we will build the model and start the training process:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将构建模型并开始训练过程：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This should show you the training loop as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该显示以下训练循环：
- en: "![Figure 1\uFEFF0.5 – Training loop](img/B19327_10_05.jpg)"
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.0.5 – 训练循环](img/B19327_10_05.jpg)'
- en: Figure 10.5 – Training loop
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 训练循环
- en: 'The model is now trained. The `compute_dp_sgd_privacy` function is useful for
    analyzing the privacy properties of a differentially private machine learning
    model trained using the DP-SGD algorithm. By computing the privacy budget, we
    can ensure that the model satisfies a desired level of privacy protection and
    can adjust the parameters of the algorithm accordingly. The function uses the
    moment accountant method to estimate the privacy budget of the DP-SGD algorithm.
    This method calculates an upper bound on the privacy budget by analyzing the moments
    of the privacy loss distribution:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在已训练。`compute_dp_sgd_privacy`函数用于分析使用DP-SGD算法训练的差分隐私机器学习模型的隐私属性。通过计算隐私预算，我们可以确保模型满足所需的隐私保护水平，并相应地调整算法的参数。该函数使用矩估计方法来估计DP-SGD算法的隐私预算。这种方法通过分析隐私损失分布的矩来计算隐私预算的上限：
- en: '[PRE23]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'And this should show you the following privacy measurements:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会展示以下隐私度量：
- en: '[PRE24]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Differential privacy in practice
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差分隐私的实际应用
- en: Understanding the importance and utility of differential privacy, technology
    giants have started implementing it in their products. Two popular examples are
    Apple and Microsoft.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 理解差分隐私的重要性和实用性，技术巨头已经开始在其产品中实施它。两个流行的例子是苹果和微软。
- en: Apple routinely collects users’ typing history and behavior locally – this helps
    power features such as autocorrect and automatic completion of messages. However,
    it also invites the risk of collecting personal and sensitive information. Users
    may talk about medical issues, financial details, or other information that they
    want to protect, and hence using it directly would be a privacy violation. Differential
    privacy comes to the rescue here. Apple implements **local differential privacy**,
    which guarantees that it is difficult to determine whether a certain user contributed
    to the computation of an aggregate feature by adding noise to the data before
    it is shared with Apple for computation and processing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果通常会本地收集用户的打字历史和行为记录——这有助于支持诸如自动纠错和消息自动完成等功能。然而，这也带来了收集个人和敏感信息的风险。用户可能会谈论医疗问题、财务细节或其他他们希望保护的信息，因此直接使用这些信息将构成隐私侵犯。差分隐私在这里发挥了救星的作用。苹果实现了**本地差分隐私**，这保证了在将数据共享给苹果进行计算和处理之前，通过向数据添加噪声，很难确定某个用户是否通过贡献计算聚合特征而参与了计算。
- en: Another tech giant that has been a forerunner in differential privacy is Microsoft.
    The Windows operating system needs to collect telemetry in order to understand
    usage patterns, diagnose faults, and detect malicious software. Microsoft applies
    differential privacy to the features it collects by adding noise before they are
    aggregated and sent to Microsoft. Microsoft Office has a *Suggested Replies* feature,
    which enables auto-completion and response suggestions in Outlook and Word. As
    there might be sensitive data in the emails/documents the model is trained on,
    Microsoft uses differential privacy in order to ensure that the model doesn’t
    learn from or leak any such information.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 另一家在差分隐私领域一直处于领先地位的技术巨头是微软。Windows操作系统需要收集遥测数据以了解使用模式、诊断故障和检测恶意软件。微软在将收集到的数据聚合并发送给微软之前，通过添加噪声来应用差分隐私。Microsoft
    Office有一个*建议回复*功能，该功能可以在Outlook和Word中实现自动完成和回复建议。由于模型训练所依赖的电子邮件/文档中可能包含敏感数据，因此微软使用差分隐私来确保模型不会从或泄露任何此类信息。
- en: These algorithms often take longer to train and often require tuning for accuracy,
    but according to Microsoft, this effort can be worth it due to the more rigorous
    privacy guarantees that differential privacy enables.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法通常需要更长的时间来训练，并且通常需要调整以提高准确性，但根据微软的说法，由于差分隐私提供的更严格的隐私保证，这种努力是值得的。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, user privacy has grown as a field of importance. Users are
    to have full control over their data, including its collection, storage, and use.
    This can be a hindrance to machine learning, especially in the cybersecurity domain,
    where increased privacy causing a decreased utility can lead to fraud, network
    attacks, data theft, or abuse.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，用户隐私已成为一个重要的研究领域。用户应完全控制自己的数据，包括其收集、存储和使用。这可能会阻碍机器学习，尤其是在网络安全领域，隐私的增加可能导致效用降低，进而可能导致欺诈、网络攻击、数据盗窃或滥用。
- en: This chapter first covered the fundamental aspects of privacy – what it entails,
    why it is important, the legal requirements surrounding it, and how it can be
    incorporated into practice through the privacy-by-design framework. We then covered
    differential privacy, a statistical technique to add noise to data so that analysis
    can be performed while maintaining user privacy. Finally, we looked at how differential
    privacy can be applied to machine learning in the domain of credit card fraud
    detection, as well as deep learning models.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍了隐私的基本方面——它包含的内容、为什么它很重要、与之相关的法律要求，以及如何通过隐私设计框架将其融入实践。然后，我们介绍了差分隐私，这是一种向数据添加噪声的统计技术，以便在保持用户隐私的同时进行数据分析。最后，我们探讨了如何将差分隐私应用于信用卡欺诈检测领域的机器学习以及深度学习模型。
- en: This completes our journey into building machine learning solutions for cybersecurity!
    Now, it is time to introspect and develop more skills in the domain. The next
    chapter, which contains a series of interview-related questions and additional
    blueprints, will help you do just that.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们构建用于网络安全机器学习解决方案的旅程的结束！现在，是时候进行反思并在这个领域发展更多技能了。下一章，其中包含一系列与面试相关的问题和额外的蓝图，将帮助你实现这一点。
