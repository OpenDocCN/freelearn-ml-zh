- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*'
- en: Ensemble Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成建模
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of the chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Explain the concepts of bias and variance and how they lead to underfitting
    and overfitting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释偏差和方差的概念，以及它们如何导致欠拟合和过拟合
- en: Explain the concepts behind bootstrapping
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释自助法（bootstrapping）背后的概念
- en: Implement a bagging classifier using decision trees
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树实现一个袋装分类器（bagging classifier）
- en: Implement adaptive boosting and gradient boosting models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自适应增强（adaptive boosting）和梯度增强（gradient boosting）模型
- en: Implement a stacked ensemble using a number of classifiers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个分类器实现堆叠集成（stacked ensemble）
- en: This chapter covers bias and variance, and underfitting and overfitting, and
    then introduces ensemble modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了偏差与方差，欠拟合与过拟合的内容，然后介绍集成建模。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapters, we discussed the two types of supervised learning
    problems: regression and classification. We looked at a number of algorithms for
    each type and delved into how those algorithms worked.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了两种监督学习问题：回归和分类。我们研究了每种类型的若干算法，并深入探讨了这些算法的工作原理。
- en: But there are times when these algorithms, no matter how complex they are, just
    don't seem to perform well on the data that we have. There could be a variety
    of causes and reasons – perhaps the data is not good enough, perhaps there really
    is no trend where we are trying to find one, or perhaps the model itself is too
    complex.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时这些算法，无论多么复杂，都似乎无法在我们拥有的数据上表现得很好。可能有多种原因：也许数据本身不够好，也许我们试图找出的趋势根本不存在，或者可能是模型本身太复杂。
- en: Wait. What? How can a model being *too complex* be a problem? Oh, but it can!
    If a model is too complex and there isn't enough data, the model could fit so
    well to the data that it learns even the noise and outliers, which is never what
    we want.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。什么？模型“过于复杂”怎么会是一个问题？哦，当然可以！如果模型过于复杂，而且数据量不足，模型可能会与数据拟合得过于精确，甚至学习到噪声和异常值，这正是我们所不希望发生的。
- en: Oftentimes, where a single complex algorithm can give us a result that is way
    off, aggregating the results from a group of models can give us a result that's
    closer to the actual truth. This is because there is a high likelihood that the
    errors from all the individual models would cancel out when we take them all into
    account when making a prediction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，当单个复杂算法给出的结果差异很大时，通过聚合一组模型的结果，我们可以得到更接近实际真相的结果。这是因为所有单个模型的误差有很大可能性会在我们做预测时互相抵消。
- en: This approach to grouping multiple algorithms to give an aggregated prediction
    is what **ensemble modeling** is based on. The ultimate goal of an ensemble method
    is to combine several underperforming **base estimators** (that is, individual
    algorithms) in such a way that the overall performance of the system improves
    and the **ensemble** of algorithms results in a model that is more robust and
    can generalize well compared to an individual algorithm.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将多个算法组合在一起以进行聚合预测的方法就是**集成建模**的基础。集成方法的最终目标是将若干表现不佳的**基本估计器**（即各个独立算法）以某种方式组合起来，从而提高系统的整体性能，使得**集成**的算法结果能够生成一个比单一算法更强大、能更好地泛化的模型。
- en: In this chapter, we'll discuss how building an ensemble model can help us build
    a robust system that makes accurate predictions without increasing variance. We
    will start by talking about some reasons a model may not perform well, and then
    move on to discussing the concepts of bias and variance, as well as overfitting
    and underfitting. We will introduce ensemble modeling as a solution for these
    performance issues and discuss different ensemble methods that could be used to
    overcome different types of problems when it comes to underperforming models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何构建一个集成模型来帮助我们建立一个强健的系统，使其能够做出准确的预测，而不会增加方差。我们将从讨论模型表现不佳的一些原因开始，然后转到偏差和方差的概念，以及过拟合和欠拟合。我们将介绍集成建模作为解决这些性能问题的方法，并讨论不同的集成方法，这些方法可以用于解决与表现不佳的模型相关的不同类型问题。
- en: The chapter will discuss three types of ensemble methods. Namely, bagging, boosting,
    and stacking. Each of these will be discussed right from the basic theory to discussions
    on which use cases each type deals with well and which use cases each type might
    not be a good fit for. This chapter will also walk you through a number of exercises
    to implement the models using the scikit-learn library in Python.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论三种集成方法：装袋（bagging）、提升（boosting）和堆叠（stacking）。每种方法将从基本理论讨论到各种使用案例的讨论，以及每种方法可能不适合的使用案例。本章还将通过多个练习步骤引导您使用Python中的scikit-learn库来实现这些模型。
- en: 'Exercise 43: Importing Modules and Preparing the Dataset'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习43：导入模块并准备数据集
- en: 'In this exercise, we''ll import all the modules we will need for this chapter
    and get our dataset in shape for the exercises to come:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将导入本章所需的所有模块，并准备好我们的数据集以进行接下来的练习：
- en: 'Import all the modules required to manipulate the data and evaluate the model:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的模块来操作数据和评估模型：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset that we will use in this chapter is the Titanic dataset, which
    was introduced in the previous chapters as well. Read the dataset and print the
    first five rows:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章中将使用的数据集是泰坦尼克号数据集，此数据集在之前的章节中也有介绍。读取数据集并打印前五行：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.1: The first five rows](img/C12622_05_01.jpg)'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图5.1：前五行](img/C12622_05_01.jpg)'
- en: 'Figure 5.1: The first five rows'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.1：前五行
- en: In order to make the dataset ready for use, we will add a `preprocess` function,
    which will preprocess the dataset to get it into a format that is ingestible by
    the scikit-learn library.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使数据集准备好使用，我们将添加一个`preprocess`函数，该函数将预处理数据集以使其符合scikit-learn库可接受的格式。
- en: This chapter assumes that the dataset has already been preprocessed and is ready
    for use, but we will add a `preprocess` function, which will preprocess the dataset
    to get it into a format that is ingestible by the `Scikit-learn` library.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本章假设数据集已经经过预处理并准备好使用，但我们将添加一个`preprocess`函数，该函数将预处理数据集以使其符合`Scikit-learn`库可接受的格式。
- en: First, we create a `fix_age` function to preprocess the `age` function and get
    an integer value. If the age is null, the function returns a value of *-1* to
    differentiate it from available values, and if the value is a fraction less than
    *1*, multiply the age value by *100*. We then apply this function to the `age`
    column.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`fix_age`函数来预处理`age`函数并获得整数值。如果年龄为空，函数将返回*-1*以区分可用值，并且如果值小于*1*的分数，则将年龄值乘以*100*。然后，我们将此函数应用于`age`列。
- en: 'Then, we convert the `Sex` column into a binary variable with *1* for female
    and *0* for male values, and subsequently create dummy binary columns for the
    `Embarked` column using pandas'' `get_dummies` function. Following this, we combine
    the DataFrame containing the dummy columns with the remaining numerical columns
    to create the final DataFrame, which is returned by the function:'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们将`Sex`列转换为二元变量，女性为*1*，男性为*0*，随后使用pandas的`get_dummies`函数为`Embarked`列创建虚拟二元列。之后，我们将包含虚拟列的DataFrame与其余数值列组合，以创建最终DataFrame，并由该函数返回。
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Split the dataset into training and validation sets.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和验证集。
- en: We split the dataset into two parts – one on which we will train the models
    during the exercises (`train`), and another on which we will make predictions
    to evaluate the performance of each of those models (`val`). We will use the function
    we wrote in the previous step to preprocess the training and validation datasets
    separately.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将数据集分为两部分 - 一部分用于练习中训练模型（`train`），另一部分用于进行预测以评估每个模型的性能（`val`）。我们将使用前一步中编写的函数分别预处理训练和验证数据集。
- en: 'Here, the `Survived` binary variable is the target variable that determines
    whether or not the individual in each row survived the sinking of the Titanic,
    so we create `y_train` and `y_val` as the dependent variable columns from both
    the splits:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`Survived`二元变量是目标变量，确定每行中个体是否幸存于泰坦尼克号的沉没，因此我们从这两个拆分中的依赖变量列创建`y_train`和`y_val`：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's begin.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Overfitting and Underfitting
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: Let's say we fit a supervised learning algorithm to our data and subsequently
    use the model to perform a prediction on a hold-out validation set. The performance
    of this model will be considered to be good based on how well it generalizes,
    that is, the predictions it makes for data points in an independent validation
    dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将一个监督学习算法拟合到数据上，并随后使用该模型对一个独立的验证集进行预测。基于该模型如何进行泛化，也就是它对验证数据集中的数据点做出的预测，我们将认为该模型表现良好。
- en: Sometimes we find that the model is not able to make accurate predictions and
    gives poor performance on the validation data. This poor performance can be the
    result of a model that is too simple to model the data appropriately, or a model
    that is too complex to generalize to the validation dataset. In the former case,
    the model has a **high bias** and results in **underfitting**, while in the latter
    case, the model has a **high variance** and results in **overfitting**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们发现模型无法做出准确的预测，并且在验证数据集上的表现较差。这种较差的表现可能是由于模型过于简单，无法适当地拟合数据，或者模型过于复杂，无法对验证数据集进行有效的泛化。在前一种情况下，模型具有**高偏差**，导致**欠拟合**，而在后一种情况下，模型具有**高方差**，导致**过拟合**。
- en: '**Bias**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**'
- en: The bias in the prediction of a machine learning model represents the difference
    between the predicted values and the true values. A model is said to have a high
    bias if the average predicted values are far off from the true values and is conversely
    said to have a low bias if the average predicted values are close to the true
    values.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型预测中的偏差表示预测值与真实值之间的差异。如果平均预测值与真实值相差较大，则模型被认为具有高偏差；反之，如果平均预测值接近真实值，则模型被认为具有低偏差。
- en: A high bias indicates that the model cannot capture the complexity in the data
    and is unable to identify the relevant relationships between the inputs and outputs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 高偏差表示模型无法捕捉数据中的复杂性，并且无法识别输入和输出之间的相关关系。
- en: '**Variance**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**方差**'
- en: The variance in prediction of a machine learning model represents how scattered
    the predicted values are compared to the true values. A model is said to have
    high variance if the predictions are scattered and unstable and is conversely
    said to have low variance if the predictions are consistent and not very scattered.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型预测中的方差表示预测值与真实值之间的分散程度。如果预测值分散且不稳定，则模型被认为具有高方差；反之，如果预测值一致且不太分散，则模型被认为具有低方差。
- en: 'A high variance indicates the model''s inability to generalize and make accurate
    predictions on data points previously unseen by the model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 高方差表示模型无法对模型以前未见过的数据点进行泛化和做出准确预测：
- en: '![Figure 5.2: Visual representation of data points having high and low bias
    and variance](img/C12622_05_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2：数据点具有高偏差和高方差的可视化表示](img/C12622_05_02.jpg)'
- en: 'Figure 5.2: Visual representation of data points having high and low bias and
    variance'
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.2：数据点具有高偏差和高方差的可视化表示
- en: Underfitting
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合
- en: Let's say that we fit a simple model on the training dataset, one with a low
    model complexity, such as a simple linear model. We have fit a function that's
    able to represent the relationship between the X and Y data points in the training
    data to some extent, but we see that the training error is still high.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在训练数据集上拟合了一个简单的模型，一个具有低模型复杂度的模型，例如一个简单的线性模型。我们拟合了一个能够在一定程度上表示训练数据中 X 和 Y
    数据点之间关系的函数，但我们发现训练误差仍然很高。
- en: '![Figure 5.3: Underfitting versus an ideal fit in regression](img/C12622_05_03.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3：回归中的欠拟合与理想拟合](img/C12622_05_03.jpg)'
- en: 'Figure 5.3: Underfitting versus an ideal fit in regression'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.3：回归中的欠拟合与理想拟合
- en: For example, look at the two regression plots shown in *Figure 5.3*; while the
    first plot shows a model that fits a straight line to the data, the second plot
    shows a model that attempts to fit a relatively more complex polynomial to the
    data, one that seems to represent the mapping between X and Y quite well.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看*图 5.3*中的两个回归图；第一个图显示了一个将直线拟合到数据的模型，第二个图显示了一个尝试将相对复杂的多项式拟合到数据的模型，后者似乎很好地表示了
    X 和 Y 之间的映射关系。
- en: We can say that the first model demonstrates underfitting, since it shows the
    characteristics of a high bias and low variance; that is, while it is unable to
    capture the complexity in the mapping between the inputs and outputs, it is consistent
    in its predictions. This model will have a high prediction error on both the training
    data and validation data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，第一个模型展示了欠拟合，因为它表现出了高偏差和低方差的特征；也就是说，虽然它无法捕捉输入与输出之间映射的复杂性，但它在预测中保持一致。这个模型在训练数据和验证数据上都会有较高的预测误差。
- en: Overfitting
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'Let''s say that we trained a highly complex model that is able to make predictions
    on the training dataset almost perfectly. We have managed to fit a function to
    represent the relationship between the X and Y data points in the training data
    such that prediction error on the training data is extremely low:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们训练了一个高度复杂的模型，几乎可以完美地对训练数据集进行预测。我们已经设法拟合了一个函数来表示训练数据中X和Y数据点之间的关系，使得训练数据上的预测误差极低：
- en: '![Figure 5.4: An ideal fit versus overfitting in regression](img/C12622_05_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4：回归中的理想拟合与过拟合](img/C12622_05_04.jpg)'
- en: 'Figure 5.4: An ideal fit versus overfitting in regression'
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：回归中的理想拟合与过拟合
- en: Looking at the two plots in *Figure 5.4*, we can see that the second plot shows
    a model that attempts to fit a highly complex function to the data points, compared
    to the plot on the left, which represents the ideal fit for the given data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 5.4*中的两个图中，我们可以看到，第二个图显示了一个试图对数据点拟合高度复杂函数的模型，而左侧的图代表了给定数据的理想拟合。
- en: 'It is evident that, when we try to use the first model to predict the Y values
    for X data points that did not appear in the training set, we will see that the
    predictions are way off from the corresponding true values. This is a case of
    overfitting: the phenomenon where the model fits the data *too well* so that it
    is unable to generalize to new data points, since the model learns even the random
    noise and outliers in the training data.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，当我们尝试使用第一个模型预测在训练集未出现的X数据点的Y值时，我们会发现预测结果与相应的真实值相差甚远。这就是过拟合的表现：模型对数据拟合得*过于精确*，以至于无法对新的数据点进行泛化，因为模型学习了训练数据中的随机噪声和离群值。
- en: 'This model shows the characteristics of high variance and low bias: while the
    average predicted values would be close to the true values, they would be quite
    scattered compared to the true values.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型展示了高方差和低偏差的特征：虽然平均预测值会接近真实值，但与真实值相比，预测值会相对分散。
- en: Overcoming the Problem of Underfitting and Overfitting
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 克服欠拟合和过拟合的问题
- en: From the previous sections, we can see that, as we move from an overly simplistic
    to an overly complex model, we go from having an underfitting model with high
    bias and low variance to an overfitting model with a low bias and high variance.
    The goal of any supervised machine learning algorithm is to achieve low bias and
    low variance and find that sweet spot between underfitting and overfitting. This
    will help the algorithm generalize well from the training data to validation data
    points as well, resulting in good prediction performance on data the model has
    never seen.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中我们可以看到，当我们从一个过于简单的模型过渡到过于复杂的模型时，我们从一个具有高偏差和低方差的欠拟合模型，过渡到一个具有低偏差和高方差的过拟合模型。任何监督学习算法的目标是实现低偏差和低方差，并找到欠拟合和过拟合之间的平衡点。这将有助于算法从训练数据到验证数据点的良好泛化，从而在模型从未见过的数据上也能表现出良好的预测性能。
- en: The best way to improve performance when the model underfits the data is to
    increase the model complexity so as to identify the relevant relationships in
    the data. This can be done by adding new features, or by creating an ensemble
    of high-bias models. However, in this case, adding more data to train on would
    not help, as the constraining factor is model complexity and more data will not
    help to reduce the model's bias.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型对数据欠拟合时，改进性能的最佳方法是增加模型的复杂性，以便识别数据中的相关关系。这可以通过添加新特征或创建高偏差模型的集成来实现。然而，在这种情况下，添加更多的数据进行训练并没有帮助，因为限制因素是模型复杂度，更多的数据不会帮助减少模型的偏差。
- en: 'Overfitting is, however, more difficult to tackle. Here are some common techniques
    used to overcome the problem posed by overfitting:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，过拟合问题更难解决。以下是一些常见的应对过拟合问题的技术：
- en: '**To get more data**: A highly complex model can easily overfit to a small
    dataset but not be able to as easily on a larger dataset.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取更多数据**：一个高度复杂的模型很容易在小数据集上过拟合，但在大数据集上则不容易出现过拟合。'
- en: '**Dimensionality Reduction**: Reducing the number of features can help make
    the model less complex.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：减少特征数量有助于让模型变得不那么复杂。'
- en: '**Regularization**: A new term is added to the cost function to adjust the
    coefficients (especially the high-degree coefficients in linear regression) toward
    a low value.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：向代价函数中添加一个新项，以调整系数（特别是线性回归中的高阶系数）使其趋向于较低值。'
- en: '**Ensemble modeling**: Aggregating the predictions of several overfitting models
    can effectively eliminate high variance in prediction and perform better than
    individual models that overfit to the training data.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成建模**：聚合多个过拟合模型的预测结果可以有效地消除预测中的高方差，并且比单个过拟合训练数据的模型表现得更好。'
- en: 'We will talk in more detail about the nuances and considerations involved in
    the first three in *Chapter 6*, *Model Evaluation*; this chapter will focus on
    different ensemble modeling techniques. Some of the common types of ensembles
    are:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第六章*《模型评估》中更详细地讨论前三种方法的细微差别和考虑因素；本章将重点介绍不同的集成建模技术。一些常见的集成方法包括：
- en: '**Bagging**: A shorter term for **bootstrap aggregation**, this technique is
    also used to decrease the model''s variance and avoid overfitting. It involves
    taking a subset of features and data points at a time, training a model on each
    subset, and subsequently aggregating the results from all the models into a final
    prediction.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagging**：即**引导聚合**的简称，这种技术也用于减少模型的方差并避免过拟合。其过程是一次性选择一部分特征和数据点，对每个子集训练一个模型，随后将所有模型的结果汇聚成最终的预测。'
- en: '**Boosting**: This technique is used to reduce bias rather than to reduce variance,
    and involves incrementally training new models that focus on the misclassified
    data points in the previous model.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boosting**：这种技术用于减少偏差，而不是减少方差，它通过逐步训练新的模型，聚焦于之前模型中的错误分类数据点。'
- en: '**Stacking**: The aim of this technique is to increase the predictive power
    of the classifier, as it involves training multiple models and then using a combiner
    algorithm to make the final prediction by using the predictions from all these
    models additional inputs.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stacking**：这种技术的目的是提高分类器的预测能力，它涉及训练多个模型，然后使用组合算法根据所有这些模型的预测结果作为额外输入来做出最终预测。'
- en: Let's start with bagging, and then move on to boosting and stacking.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Bagging开始，然后转向Boosting和Stacking。
- en: Bagging
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: The term bagging is derived from a technique called bootstrap aggregation. In
    order to implement a successful predictive model, it's important to know in what
    situation we could benefit from using bootstrapping methods to build ensemble
    models. In this section, we'll talk about a way to use bootstrap methods to create
    an ensemble model that minimizes variance and look at how we can build an ensemble
    of decision trees, that is, the Random Forest algorithm. But what is bootstrapping
    and how does it help us build robust ensemble models?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: “Bagging”一词来源于一种名为引导聚合（bootstrap aggregation）的方法。为了实现成功的预测模型，了解在何种情况下我们可以从使用引导法（bootstrapping）构建集成模型中受益是非常重要的。在本节中，我们将讨论如何利用引导方法创建一个最小化方差的集成模型，并探讨如何构建一个决策树集成模型，也就是随机森林算法。那么，什么是引导法，它如何帮助我们构建强健的集成模型呢？
- en: Bootstrapping
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引导法
- en: 'The bootstrap method refers to random sampling with replacement, that is, drawing
    multiple samples (each known as a resample) from the dataset consisting of randomly
    chosen data points, where there can be an overlap in the data points contained
    in each resample and each data point has an equal probability of being selected
    from the overall dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 引导法是指带有放回的随机抽样，即从由随机选择的数据点组成的数据集中抽取多个样本（每个样本称为重抽样），其中每个重抽样可能包含重复的数据点，每个数据点都有相同的概率从整个数据集中被选中：
- en: '![Figure 5.5: Randomly choosing data points](img/C12622_05_05.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5：随机选择数据点](img/C12622_05_05.jpg)'
- en: 'Figure 5.5: Randomly choosing data points'
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：随机选择数据点
- en: From the previous diagram, we can see that each of the five bootstrapped samples
    taken from the primary dataset is different and has different characteristics.
    As such, training models on each of these resamples would result in different
    predictions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示中，我们可以看到，从主数据集中抽取的五个引导样本各不相同，且具有不同的特征。因此，在每个重抽样上训练模型将会得到不同的预测结果。
- en: 'The following are the advantages of bootstrapping:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是自举的优势：
- en: Each resample can contain different characteristics from that of the entire
    dataset, allowing us a different perspective of how the data behaves.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个重新采样可以包含与整个数据集不同的特征，这使我们能够从不同的视角了解数据的行为。
- en: Algorithms that utilize bootstrapping can be more robust and handle unseen data
    better, especially on smaller datasets that have a tendency to cause overfitting.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用自举法的算法能够更加健壮，并且更好地处理未见过的数据，特别是在容易导致过拟合的较小数据集上。
- en: The bootstrap method can test the stability of a prediction by testing models
    using datasets with different variations and characteristics, resulting in a model
    that is more robust.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自举法可以通过使用具有不同变化和特征的数据集来测试预测的稳定性，从而得到更加健壮的模型。
- en: Bootstrap Aggregation
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自举聚合
- en: Now that we are aware of what bootstrapping is, what exactly does a bagging
    ensemble do? It is essentially an ensemble model that generates multiple versions
    of a predictor on each resample and uses these to get an aggregated predictor.
    The aggregation step gives us a *meta prediction*, which involves taking an average
    over the models when predicting a continuous numerical value for regression problems,
    and also involves taking a vote when predicting a class for classification problems.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了什么是自举，那么装袋集成究竟是什么？它本质上是一个集成模型，它在每个重新采样上生成多个预测器的版本，并使用这些版本来获取聚合的预测器。在回归问题中，聚合步骤通过模型的平均值来进行*元预测*，在分类问题中则通过投票来进行预测类别。
- en: 'The following diagram gives us a visual representation of how a bagging estimator
    is built from the bootstrap sampling shown in *Figure 5.5*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了如何从自举抽样构建装袋估计器，具体见*图 5.5*：
- en: '![Figure 5.6: Bagging estimator built from bootstrap sampling](img/C12622_05_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6：从自举抽样构建的装袋估计器](img/C12622_05_06.jpg)'
- en: 'Figure 5.6: Bagging estimator built from bootstrap sampling'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：从自举抽样构建的装袋估计器
- en: Since each model is essentially independent of the others, all the base models
    can be trained in parallel, considerably speeding up the training process and
    allowing us to take advantage of the computational power we have on our hands
    today.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型基本上是独立的，所有基础模型可以并行训练，这显著加快了训练过程，并允许我们利用当今手头的计算能力。
- en: 'Bagging essentially helps reduce the variance of the entire ensemble by introducing
    randomization into its construction procedure and is usually used with a base
    predictor that has a tendency to overfit the training data. The primary point
    of consideration here would be the stability (or lack thereof) of the training
    dataset: Bagging can improve accuracy in cases where a slight perturbation in
    the data could result in a significant change in the trained model.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋通过在其构建过程中引入随机化来帮助减少整个集成的方差，并且通常与具有过度拟合训练数据倾向的基础预测器一起使用。在这里需要考虑的主要点是训练数据集的稳定性（或缺乏稳定性）：在数据中轻微的扰动可能导致训练模型显著变化的情况下，装袋可以提高准确性。
- en: scikit-learn uses `BaggingClassifier` and `BaggingRegressor` to implement generic
    bagging ensembles for classification and regression tasks respectively. The primary
    inputs to these are the base estimators to use on each resample, along with the
    number of estimators to use (that is, the number of resamples).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 使用 `BaggingClassifier` 和 `BaggingRegressor` 来实现用于分类和回归任务的通用装袋集成。这些的主要输入是在每次重新采样上使用的基础估计器，以及要使用的估计器数量（即重新采样的数量）。
- en: 'Exercise 44: Using the Bagging Classifier'
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 44：使用装袋分类器
- en: 'In this exercise, we will use scikit-learn''s `BaggingClassifier` as our ensemble
    with `DecisionTreeClassifier` as the base estimator. We know that decision trees
    are prone to overfitting, and so will have a high variance and low bias, both
    being important characteristics for the base estimators to be used in bagging
    ensembles:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 scikit-learn 的 `BaggingClassifier` 作为我们的集成，使用 `DecisionTreeClassifier`
    作为基础估计器。我们知道决策树容易过拟合，因此在装袋集成中使用的基础估计器应具有高方差和低偏差，这两者都是重要的特征。
- en: 'Import the base and ensemble classifiers:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入基础和集成分类器：
- en: '[PRE4]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Specify the hyperparameters and initialize the model.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: Here, we will first specify the hyperparameters of the base estimator, for which
    we are using the decision tree classifier with the entropy or information gain
    as the splitting criterion. We will not specify any limits on the depth of the
    tree or size/number of leaves to each tree to grow fully. Following this, we will
    define the hyperparameters for the bagging classifier and pass the base estimator
    object to the classifier as a hyperparameter.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将首先指定基础估计器的超参数，使用决策树分类器，并以熵或信息增益作为划分标准。我们不会对树的深度或每棵树的叶节点大小/数量设置任何限制，以便树能够完全生长。接下来，我们将定义袋装分类器的超参数，并将基础估计器对象作为超参数传递给分类器。
- en: 'We will take 50 base estimators for our example, which will run in parallel
    and utilize all the processes available in the machine (which is done by specifying
    `n_jobs=-1`). Additionally, we will specify `max_samples` as 0.5, indicating that
    the number of datapoints in the bootstrap should be half that in the total dataset.
    We will also set a random state (to any arbitrary value, which will stay constant
    throughout) to maintain the reproducibility of the results:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将选择 50 个基础估计器，这些估计器将并行运行并利用机器上所有可用的处理器（通过指定`n_jobs=-1`来实现）。此外，我们将指定`max_samples`为
    0.5，表示自助法样本数量应为总数据集的一半。我们还将设置一个随机状态（为任意值，且在整个过程中保持不变），以确保结果的可复现性：
- en: '[PRE5]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Fit the bagging classifier model to the training data and calculate the prediction
    accuracy.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合袋装分类器模型到训练数据并计算预测准确性。
- en: 'Let''s fit the bagging classifier and find the meta predictions for both the
    training and validation set. Following this, let''s find the prediction accuracy
    on the training and validation datasets:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们拟合袋装分类器，并找出训练集和验证集的元预测。接下来，找出训练集和验证集数据集的预测准确性：
- en: '[PRE6]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.7: Prediction accuracy of the bagging classifier](img/C12622_05_07.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.7：袋装分类器的预测准确性](img/C12622_05_07.jpg)'
- en: 'Figure 5.7: Prediction accuracy of the bagging classifier'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：袋装分类器的预测准确性
- en: Fit the decision tree model to the training data to compare prediction accuracy.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合决策树模型到训练数据以比较预测准确性。
- en: 'Let''s also fit the decision tree (from the object we initialized in *step
    two*) so that we will be able to compare the prediction accuracies of the ensemble
    with that of the base predictor:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将拟合决策树（使用在*第二步*中初始化的对象），以便能够将集成模型的预测准确性与基础预测器进行比较：
- en: '[PRE7]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.8: Prediction accuracy of the decision tree](img/C12622_05_08.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8：决策树的预测准确性](img/C12622_05_08.jpg)'
- en: 'Figure 5.8: Prediction accuracy of the decision tree'
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：决策树的预测准确性
- en: Here, we can see that, although the decision tree has a much higher training
    accuracy than the bagging classifier, its accuracy on the validation dataset is
    lower, a clear signal that the decision tree is overfitting to the training data.
    The bagging ensemble, on the other hand, reduces the overall variance and results
    in a much more accurate prediction.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，尽管决策树的训练准确度远高于袋装分类器，但在验证集上的准确度较低，明显表明决策树对训练数据发生了过拟合。另一方面，袋装集成方法减少了整体方差，从而得到更为准确的预测。
- en: Random Forest
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: An issue that is commonly faced with decision trees is that the split on each
    node is performed using a **greedy** algorithm that minimizes the entropy of the
    leaf nodes. Keeping this in mind, the base estimator decision trees in a bagging
    classifier can still be similar in terms of the features they split on, and so
    can have predictions that are quite similar. However, bagging is only useful in
    reducing the variance in predictions if the predictions from the base models are
    not correlated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树常见的问题是每个节点的划分是使用**贪婪**算法进行的，该算法通过最小化叶节点的熵来进行划分。考虑到这一点，袋装分类器中的基础估计器决策树在划分特征上可能仍然相似，因此其预测结果也可能非常相似。然而，只有当基础模型的预测结果不相关时，袋装方法才有助于减少预测的方差。
- en: The Random Forest algorithm attempts to overcome this problem by not only bootstrapping
    the data points in the overall training dataset, but also bootstrapping the features
    available for each tree to split on. This ensures that when the greedy algorithm
    is searching for the *best* feature to split on, the overall *best* feature may
    not always be available in the bootstrapped features for the base estimator and
    so would not be chosen – resulting in base trees that have different structures.
    This simple tweak lets the best estimators be trained in such a way that the predictions
    from each tree in the forest have a lower probability of being correlated to the
    predictions from other trees.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法通过不仅对整体训练数据集中的数据点进行引导抽样，还对每棵树的分裂特征进行引导抽样，从而尝试克服这个问题。这确保了当贪心算法在搜索*最佳*特征进行分裂时，整体*最佳*特征可能并不总是在引导抽样的特征中可用，因此不会被选择——从而导致基础树具有不同的结构。这个简单的调整使得最佳估计器能够以这样的方式进行训练：即森林中每棵树的预测结果与其他树的预测结果相关的概率更低。
- en: Each base estimator in the Random Forest has a random sample of data points
    as well as a random sample of features. And since the ensemble is made up of decision
    trees, the algorithm is called a Random Forest.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林中的每个基础估计器都有一个随机的数据点样本和一个随机的特征样本。由于集成是由决策树构成的，因此该算法被称为随机森林。
- en: 'Exercise 45: Building the Ensemble Model Using Random Forest'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习45：使用随机森林构建集成模型
- en: The two primary parameters that Random Forest takes is the fraction of features
    and the fraction of data points to bootstrap to train each base decision tree
    on.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的两个主要参数是特征的比例和训练每个基础决策树的引导数据点的比例。
- en: 'In this exercise, we will use scikit-learn''s `RandomForestClassifier` to build
    the ensemble model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用scikit-learn的`RandomForestClassifier`来构建集成模型：
- en: 'Import the ensemble classifier:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入集成分类器：
- en: '[PRE8]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Specify the hyperparameters and initialize the model.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: 'Here, we will use entropy as the splitting criterion for the decision trees
    in a forest comprising 100 trees. As before, we will not specify any limits on
    the depth of the trees or size/number of leaves. Unlike the bagging classifier,
    which took `max_samples` as an input during initialization, the Random Forest
    algorithm takes in only `max_features`, indicating the number (or fraction) of
    features in the bootstrap sample. We will specify the value for this as 0.5, so
    that only three out of six features are considered for each tree:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将使用熵作为决策树分裂标准，森林中包含100棵树。与之前一样，我们不会对树的深度或叶子节点的大小/数量设置任何限制。与袋装分类器不同，袋装分类器在初始化时需要输入`max_samples`，而随机森林算法只接受`max_features`，表示引导样本中的特征数（或比例）。我们将把此值设置为0.5，这样每棵树只考虑六个特征中的三个：
- en: '[PRE9]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fit the Random Forest classifier model to the training data and calculate the
    prediction accuracy.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将随机森林分类器模型拟合到训练数据并计算预测准确度。
- en: 'Let''s fit the Random Forest model and find the meta predictions for both the
    training and validation set. Following this, let''s find the prediction accuracy
    on the training and validation datasets:'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们拟合随机森林模型，并找到训练集和验证集的元预测。接下来，我们计算训练集和验证数据集上的预测准确度：
- en: '[PRE10]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.9: Accuracy on training and validation using Random Forest](img/C12622_05_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9：使用随机森林的训练和验证准确度](img/C12622_05_09.jpg)'
- en: 'Figure 5.9: Accuracy on training and validation using Random Forest'
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.9：使用随机森林的训练和验证准确度
- en: If we compare the prediction accuracies of Random Forest on our dataset to that
    of the bagging classifier, we can see that the accuracy on the validation set
    is pretty much the same, although the latter has higher accuracy on the training
    dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将随机森林在我们的数据集上的预测准确度与袋装分类器的预测准确度进行比较，我们可以看到，尽管后者在训练数据集上的准确度更高，但在验证集上的准确度几乎相同。
- en: Boosting
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升法
- en: The second ensemble technique we'll be looking at is **boosting**, which involves
    incrementally training new models that focus on the misclassified data points
    in the previous model and utilizes weighted averages to turn weak models (underfitting
    models having high bias) into stronger models. Unlike bagging, where each base
    estimator could be trained independently of the others, the training of each base
    estimator in a boosted algorithm depends on the previous one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第二种集成技术是**boosting**，它涉及逐步训练新模型，这些模型专注于先前模型中被错分的数据点，并利用加权平均将弱模型（具有高偏差的欠拟合模型）转变为更强的模型。与bagging不同，其中每个基本估计器可以独立训练，boosted算法中每个基本估计器的训练依赖于前一个估计器。
- en: Although boosting also uses the concept of bootstrapping, it's done differently
    from bagging, since each sample of data is weighted, implying that some bootstrapped
    samples can be used for training more often than other samples. When training
    each model, the algorithm keeps track of which features are most useful and which
    data samples have the most prediction error; these are given higher weightage
    and are considered to require more iterations to properly train the model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管boosting也使用了自举法的概念，但与bagging不同，由于每个数据样本都有权重，这意味着某些自举样本可能被更频繁地用于训练。在训练每个模型时，算法跟踪哪些特征最有用，哪些数据样本具有最大的预测误差；这些样本被赋予更高的权重，并被认为需要更多次迭代来正确训练模型。
- en: When predicting the output, the boosting ensemble takes a weighted average of
    the predictions from each base estimator, giving a higher weight to the ones that
    had lower errors during the training stage. This means that, for the data points
    that are misclassified by the model in an iteration, the weights for those data
    points are increased so that the next model is more likely to classify it correctly.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测输出时，boosting集成从每个基本估计器的预测中取加权平均值，对训练阶段中误差较小的模型给予较高的权重。这意味着对于在迭代中由模型错分的数据点，增加这些数据点的权重，以便下一个模型更有可能正确分类它们。
- en: 'As was the case with bagging, the results from all the boosting base estimators
    are aggregated to produce a meta prediction. However, unlike bagging, the accuracy
    of a boosted ensemble increases significantly with the number of base estimators
    in the boosted ensemble:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与bagging类似，所有boosting基本估计器的结果被聚合以产生元预测。然而，与bagging不同的是，boosted集成的准确性随着集成中基本估计器的数量显著增加而增加：
- en: '![Figure 5.10: A boosted ensemble](img/C12622_05_10.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10: 一个boosted集成](img/C12622_05_10.jpg)'
- en: 'Figure 5.10: A boosted ensemble'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.10: 一个boosted集成'
- en: In the diagram, we can see that, after each iteration, the misclassified points
    have increased weights (represented by larger icons) so that the next base estimator
    that is trained is able to focus on those points. The final predictor has aggregated
    the decision boundaries from each of its base estimators.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们可以看到，在每次迭代后，错分的点具有增加的权重（由较大的图标表示），以便下一个被训练的基本估计器能够专注于这些点。最终预测器已经整合了每个基本估计器的决策边界。
- en: Adaptive Boosting
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应Boosting
- en: 'Let''s talk about a boosting technique called **adaptive boosting**, which
    is best used to boost the performance of decision stumps for binary classification
    problems. Decision stumps are essentially decision trees with a maximum depth
    of one (only one split is made on a single feature), and, as such, are weak learners.
    The primary principle that adaptive boosting works on is the same: to improve
    the areas where the base estimator fails to turn an ensemble of weak learners
    to a strong learner.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈一种称为**自适应boosting**的boosting技术，它最适合提升决策桩在二元分类问题中的性能。决策桩本质上是深度为一的决策树（只对一个特征进行一次分割），因此是弱学习器。自适应boosting的主要原理与之前相同：通过改进基本估计器在失败区域上的表现，将一组弱学习器转化为强学习器。
- en: To start with, the first base estimator takes a bootstrap of data points from
    the main training set and fits a decision stump to classify the sampled points,
    after which the trained decision tree stump is fit to the complete training data.
    For the samples that are misclassified, the weights are increased so that there
    is a higher probability of these data points being selected in the bootstrap for
    the next base estimator. A decision stump is again trained on the new bootstrap
    to classify the data points in the sample. Subsequently, the mini ensemble comprising
    the two base estimators is used to classify the data points in the entire training
    set. The misclassified data points from the second round are given a higher weight
    to improve their probability of selection, and so on until the ensemble reaches
    the limit on the number of base estimators it should contain.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，第一个基学习器从主训练集中抽取一个数据点的自助样本（bootstrap），并拟合一个决策树桩来对样本点进行分类，之后将训练好的决策树桩拟合到完整的训练数据上。对于那些被误分类的样本，权重会增加，从而增加这些数据点在下一个基学习器的自助样本中被选中的概率。随后，在新的自助样本上再次训练一个决策树桩，对数据点进行分类。接下来，包含两个基学习器的小型集成模型被用来对整个训练集中的数据点进行分类。在第二轮中被误分类的数据点会获得更高的权重，以提高它们被选中的概率，直到集成模型达到所需的基学习器数量为止。
- en: One drawback of adaptive boosting is that the algorithm is easily influenced
    by noisy data points and outliers since it tries to fit every point perfectly.
    As such, it is prone to overfitting if the number of estimators is very high.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应增强（adaptive boosting）的一个缺点是，算法容易受到噪声数据点和异常值的影响，因为它试图完美拟合每一个数据点。因此，当基学习器的数量非常高时，算法容易出现过拟合。
- en: 'Exercise 46: Adaptive Boosting'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 46：自适应增强
- en: 'In this exercise, we''ll use scikit-learn''s implementation of adaptive boosting
    for classification, `AdaBoostClassifier`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 scikit-learn 实现的自适应增强分类算法 `AdaBoostClassifier`：
- en: 'Import the classifier:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入分类器：
- en: '[PRE11]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Specify the hyperparameters and initialize the model.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: 'Here, we will first specify the hyperparameters of the base estimator, for
    which we are using the decision tree classifier with a maximum depth of one, that
    is, a decision stump. Following this, we will define the hyperparameters for the
    AdaBoost classifier and pass the base estimator object to the classifier as a
    hyperparameter:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先指定基学习器的超参数，使用的分类器是最大深度为 1 的决策树分类器，即决策树桩。接下来，我们将定义 AdaBoost 分类器的超参数，并将基学习器对象作为超参数传递给分类器：
- en: '[PRE12]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Fit the model to the training data.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到训练数据。
- en: 'Let''s fit the AdaBoost model and find the meta predictions for both the training
    and validation set. Following this, let''s find the prediction accuracy on the
    training and validation datasets:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们拟合 AdaBoost 模型，并找到训练集和验证集的元预测。接下来，计算训练集和验证集上的预测准确率：
- en: '[PRE13]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.11: Accuracy of training and validation data using adaptive boosting](img/C12622_05_11.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.11：使用自适应增强的训练数据和验证数据的准确性](img/C12622_05_11.jpg)'
- en: 'Figure 5.11: Accuracy of training and validation data using adaptive boosting'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.11：使用自适应增强的训练数据和验证数据的准确性
- en: Calculate the prediction accuracy of the model on the training and validation
    data for a varying number of base estimators.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算不同基学习器数量下，模型在训练数据和验证数据上的预测准确率。
- en: 'Earlier, we claimed that the accuracy tends to increase with an increasing
    number of base estimators, but also that the model has a tendency to overfit if
    too many base estimators are used. Let''s calculate the prediction accuracies
    so that we can find the point where the model begins to overfit the training data:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前我们提到过，随着基学习器数量的增加，准确性通常会提高，但如果使用过多的基学习器，模型也容易出现过拟合。让我们计算预测准确率，以便找出模型开始过拟合训练数据的点：
- en: '[PRE14]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Plot a line graph to visualize the trend of the prediction accuracies on both
    the training and validation datasets:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一条折线图，直观展示训练集和验证集上的预测准确率趋势：
- en: '[PRE15]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.12: The trend of the prediction accuracies](img/C12622_05_12.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12：预测准确率的趋势](img/C12622_05_12.jpg)'
- en: 'Figure 5.12: The trend of the prediction accuracies'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.12：预测准确率的趋势
- en: As was mentioned earlier, we can see that the training accuracy almost consistently
    increases as the number of decision tree stumps increases from 10 to 200\. However,
    the validation accuracy fluctuates between 0.84 and 0.86 and begins to drop as
    the number of decision stumps goes higher. This happens because the AdaBoost algorithm
    is trying fit the noisy data points and outliers as well.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们可以看到，当决策树桩的数量从 10 增加到 200 时，训练准确度几乎一直在增加。然而，验证准确度在 0.84 到 0.86 之间波动，并且随着决策树桩数量的增加开始下降。这是因为
    AdaBoost 算法试图拟合噪声数据点和离群值。
- en: Gradient Boosting
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting is an extension to the boosting method that visualizes boosting
    as an optimization problem. A loss function is defined that is representative
    of the error residuals (the difference between the predicted and true values),
    and the gradient descent algorithm is used to optimize the loss function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是对提升方法的扩展，它将提升过程视为一个优化问题。定义了一个损失函数，代表误差残差（预测值与真实值之间的差异），并使用梯度下降算法来优化损失函数。
- en: In the first step, a base estimator (which would be a weak learner) is added
    and trained on the entire training dataset. The loss associated with the prediction
    is calculated, and, in order to reduce the error residuals, the loss function
    is updated to add more base estimators for the data points where the existing
    estimators are performing poorly. Subsequently, the algorithm iteratively adds
    new base estimators and computes the loss to allow the optimization algorithm
    to update the model and minimize the residuals themselves.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，添加一个基估计器（这将是一个弱学习器），并在整个训练数据集上进行训练。计算预测所带来的损失，并且为了减少误差残差，更新损失函数，为那些现有估计器表现不佳的数据点添加更多的基估计器。接着，算法迭代地添加新的基估计器并计算损失，以便优化算法更新模型，最小化残差。
- en: In the case of adaptive boosting, decision stumps were used as the weak learners
    for the base estimators. However, for gradient boosting methods, larger trees
    can be used, but the weak learners should still be constrained by providing a
    limit to the maximum number of layers, nodes, splits, or leaf nodes. This ensures
    that the base estimators are still weak learners, but they can be constructed
    in a greedy manner.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在自适应提升的情况下，决策树桩被用作基估计器的弱学习器。然而，对于梯度提升方法，可以使用更大的树，但仍应通过限制最大层数、节点数、分裂数或叶节点数来约束弱学习器。这确保了基估计器仍然是弱学习器，但它们可以以贪婪的方式构建。
- en: From *Chapter 3*, *Regression Analysis*, we know that the gradient descent algorithm
    can be used to minimize a set of parameters, such as the coefficients in a regression
    equation. When building an ensemble, however, we have decision trees instead of
    parameters that need to be optimized. After calculating the loss at each step,
    the gradient descent algorithm then has to modify the parameters of the new tree
    that's to be added to the ensemble in such a way that reduces the loss. This approach
    is more commonly known as **functional gradient descent.**
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从*第 3 章*，*回归分析*中我们知道，梯度下降算法可以用来最小化一组参数，比如回归方程中的系数。然而，在构建集成时，我们使用的是决策树而不是需要优化的参数。每一步计算损失后，梯度下降算法必须修改将要加入集成的新树的参数，以减少损失。这种方法更常被称为**功能梯度下降**。
- en: 'Exercise 47: GradientBoostingClassifier'
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 47：GradientBoostingClassifier
- en: The two primary parameters that Random Forest takes is the fraction of features
    and the fraction of data points to bootstrap to train each base decision tree
    on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的两个主要参数是特征的比例和用于训练每棵基决策树的自助法数据点的比例。
- en: 'In this exercise, we will use scikit-learn''s `GradientBoostingClassifier`
    to build the boosting ensemble model:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用 scikit-learn 的`GradientBoostingClassifier`来构建提升集成模型：
- en: 'Import the ensemble classifier:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入集成分类器：
- en: '[PRE16]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Specify the hyperparameters and initialize the model.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: 'Here, we will use 100 decision trees as the base estimator, with each tree
    having a maximum depth of three and a minimum of five samples in each of its leaves.
    Although we are not using decision stumps, as in the previous example, the tree
    is still small and would be considered a weak learner:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 100 棵决策树作为基估计器，每棵树的最大深度为 3，每个叶节点的最小样本数为 5。虽然我们没有像前面的例子那样使用决策树桩，但树仍然很小，并且可以被视为一个弱学习器：
- en: '[PRE17]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Fit the gradient boosting model to the training data and calculate the prediction
    accuracy.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合梯度提升模型到训练数据并计算预测准确性。
- en: 'Let''s fit the ensemble model and find the meta predictions for both the training
    and validation set. Following this, we will find the prediction accuracy on the
    training and validation datasets:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们拟合集成模型，并找到训练集和验证集的元预测结果。接下来，我们将找到训练集和验证集上的预测准确性：
- en: '[PRE18]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.13: Prediction accuracy on the training and validation datasets](img/C12622_05_13.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13：训练集和验证集上的预测准确性](img/C12622_05_13.jpg)'
- en: 'Figure 5.13: Prediction accuracy on the training and validation datasets'
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.13：训练集和验证集上的预测准确性
- en: We can see that the gradient boosting ensemble has greater accuracy on both
    the training and validation datasets compared to those for the adaptive boosting
    ensemble.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与自适应提升集成模型相比，梯度提升集成模型在训练集和验证集上的准确性都更高。
- en: Stacking
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠
- en: Stacking, or stacked generalization (also called meta ensembling), is a model
    ensembling technique that involves combining information from multiple predictive
    models and using them as features to generate a new model. The stacked model will
    most likely outperform each of the individual models due the smoothing effect
    it adds, as well as due to its ability to "choose" the base model that performs
    best in certain scenarios. Keeping this in mind, stacking is usually most effective
    when each of the base models is significantly different from each other.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠（Stacking）或堆叠泛化（也叫元集成）是一种模型集成技术，涉及将多个预测模型的信息结合起来，并将其作为特征生成一个新模型。由于堆叠模型通过平滑效应以及能够“选择”在某些场景下表现最好的基础模型，它通常会优于每个单独的模型。考虑到这一点，当每个基础模型之间有显著差异时，堆叠通常是最有效的。
- en: Stacking uses the predictions of the base models as additional features when
    training the final model – these are known as **meta features**. The stacked model
    essentially acts as a classifier that determines where each model is performing
    well and where it is performing poorly.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠使用基础模型的预测作为训练最终模型的附加特征——这些被称为**元特征**。堆叠模型本质上充当一个分类器，决定每个模型在哪些地方表现良好，在哪些地方表现较差。
- en: However, you cannot simply train the base models on the full training data,
    generate predictions on the full validation dataset, and then output these for
    second-level training. This runs the risk of your base model predictions already
    having "seen" the test set and therefore overfitting when feeding these predictions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你不能简单地在整个训练数据上训练基础模型，在整个验证数据集上生成预测结果，然后将这些结果用于二级训练。这会导致基础模型的预测结果已经“看过”测试集，因此在使用这些预测结果时可能会发生过拟合。
- en: 'It is important to note that the value of the meta features for each row cannot
    be predicted using a model that contained that row in the training data, as we
    then run the risk of overfitting since the base predictions would have already
    "seen" the target variable for that row. The common practice is to divide the
    training data into *k* subsets so that, when finding the meta features for each
    of those subsets, we only train the model on the remaining data. Doing this also
    avoids the problem of overfitting the data the model has already "seen":'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，对于每一行的元特征，其值不能使用包含该行训练数据的模型来预测，因为这样会导致过拟合的风险，因为基础预测已经“看过”该行的目标变量。常见的做法是将训练数据分成*k*个子集，这样，在为每个子集找到元特征时，我们只会在剩余数据上训练模型。这样做还能避免模型已经“看到”的数据过拟合的问题：
- en: '![Figure 5.14: A stacking ensemble](img/C12622_05_14.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14：一个堆叠集成模型](img/C12622_05_14.jpg)'
- en: 'Figure 5.14: A stacking ensemble'
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.14：一个堆叠集成模型
- en: 'The preceding diagram shows how this is done: we divide the training data into
    *k* folds and find the predictions from the base models on each fold by training
    the model on the remaining *k-1* folds. So, once we have the meta predictions
    for each of the folds, we can use those meta predictions along with the original
    features to train the stacked model.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了如何实现这一过程：我们将训练数据分成*k*个折叠，并通过在剩余的*k-1*个折叠上训练模型，找到每个折叠上基础模型的预测结果。因此，一旦我们得到每个折叠的元预测结果，就可以将这些元预测结果与原始特征一起用于训练堆叠模型。
- en: 'Exercise 48: Building a Stacked Model'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 48：构建堆叠模型
- en: In this exercise, we will use a support vector machine (scikit-learn's `LinearSVC`)
    and k-nearest neighbors (scikit-learn's `KNeighborsClassifier`) as the base predictors,
    and the stacked model will be a logistic regression classifier.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在此练习中，我们将使用支持向量机（scikit-learn 的 `LinearSVC`）和 k 近邻（scikit-learn 的 `KNeighborsClassifier`）作为基础预测器，堆叠模型将是逻辑回归分类器。
- en: 'Import the base models and the model used for stacking:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入基础模型和用于堆叠的模型：
- en: '[PRE19]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Create a new training set with additional columns for predictions from base
    predictors.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的训练集，其中包含来自基础预测器的额外列。
- en: 'We need to create two new columns for predicted values from each model to be
    used as features for the ensemble model in both the test and train set. Since
    NumPy arrays are immutable, we will create a new array that will have the same
    number of rows as the training dataset, and two columns more than those in the
    training dataset. Once the dataset is created, let''s print it to see what it
    looks like:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要为每个模型的预测值创建两个新列，这些列将作为集成模型在测试和训练集中的特征使用。由于NumPy数组是不可变的，我们将创建一个新数组，其行数与训练数据集相同，列数比训练数据集多两列。创建数据集后，让我们打印出来看看它的样子：
- en: '[PRE20]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.15: The new columns for the predicted values](img/C12622_05_15.jpg)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.15：预测值的新列](img/C12622_05_15.jpg)'
- en: 'Figure 5.15: The new columns for the predicted values'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.15：预测值的新列
- en: As we can see, there are two extra columns filled with *-1* values at the end
    of each row.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，每行末尾有两列填充有 *-1* 值。
- en: Train base models using the k-fold strategy.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k折策略训练基础模型。
- en: Let's take *k=5*. For each of the five folds, train on the other four folds
    and predict on the fifth fold. These predictions should then be added into the
    placeholder columns for base predictions in the new NumPy array.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们取 *k=5*。对于这五个折叠，使用其他四个折叠进行训练，并在第五个折叠上进行预测。然后，将这些预测添加到新的NumPy数组中用于基础预测的占位列中。
- en: First, we initialize the `KFold` object with the value of `k` and a random state
    to maintain reproducibility. The `kf.split()` function takes the dataset to split
    as an input and returns an iterator, each element in the iterator corresponding
    to the list of indices in the training and validation folds respectively. These
    index values in each loop over the iterator can be used to subdivide the training
    data for training and prediction for each row.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们用值为 `k` 和一个随机状态初始化 `KFold` 对象，以保持可重现性。`kf.split()` 函数将数据集作为输入进行分割，并返回一个迭代器，迭代器中的每个元素分别对应于训练和验证折叠中的索引列表。每次循环迭代器时，可以使用这些索引值将训练数据细分为每行的训练和预测。
- en: 'Once the data is adequately divided, we train the two base predictors on four-fifths
    of the data and predict the values on the remaining one-fifth of the rows. These
    predictions are then inserted into the two placeholder columns we initialized
    with `-1` in *step 2*:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦数据适当地分割，我们就会在四分之四的数据上训练这两个基础预测器，并在剩余四分之一的行上预测值。然后，将这些预测值插入到在 *步骤 2* 中用 `-1`
    初始化的两个占位列中：
- en: '[PRE21]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Create a new validation set with additional columns for predictions from base
    predictors.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的验证集，其中包含来自基础预测器的额外预测列。
- en: 'As we did in *step 2*, we will add two placeholder columns for the base model
    predictions in the validation dataset as well:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就像我们在 *步骤 2* 中所做的那样，我们也会在验证数据集中添加两个基础模型预测的占位列：
- en: '[PRE22]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.16: Additional columns for predictions from base predictors](img/C12622_05_16.jpg)'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.16：来自基础预测器的额外预测列](img/C12622_05_16.jpg)'
- en: 'Figure 5.16: Additional columns for predictions from base predictors'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.16：来自基础预测器的额外预测列
- en: Fit base models on the complete training set to get meta features for the validation
    set.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完整的训练集上拟合基础模型，以获取验证集的元特征。
- en: 'Next, we will train the two base predictors on the complete training dataset
    to get the meta prediction values for the validation dataset. This is similar
    to what we did for each fold in *step 3*:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将在完整的训练数据集上训练这两个基础预测器，以获取验证数据集的元预测值。这类似于我们在 *步骤 3* 中对每个折叠所做的操作：
- en: '[PRE23]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Train the stacked model and use the final predictions to calculate accuracy.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练堆叠模型并使用最终预测计算准确性。
- en: 'The last step is to train the logistic regression model on all the columns
    of the training dataset plus the meta predictions from the base estimators. We
    use the model to find the prediction accuracies for both the training and validation
    datasets:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后一步是使用训练数据集的所有列以及基模型的元预测结果来训练逻辑回归模型。我们使用该模型计算训练集和验证集的预测准确性：
- en: '[PRE24]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.17: Accuracy using a stacked classifier](img/C12622_05_17.jpg)'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.17：使用堆叠分类器的准确率](img/C12622_05_17.jpg)'
- en: 'Figure 5.17: Accuracy using a stacked classifier'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.17：使用堆叠分类器的准确率
- en: Compare the accuracy with that of base models.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较准确度与基模型的准确度。
- en: 'To get a sense of the performance boost from stacking, we calculate the accuracies
    of the base predictors on the training and validation datasets and compare it
    to that of the stacked model:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了了解堆叠方法带来的性能提升，我们计算基预测器在训练集和验证集上的准确率，并将其与堆叠模型的准确率进行比较：
- en: '[PRE25]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.18: Accuracy of training and validation data using SVM and K-NN](img/C12622_05_18.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18：使用SVM和K-NN的训练和验证数据准确度](img/C12622_05_18.jpg)'
- en: 'Figure 5.18: Accuracy of training and validation data using SVM and K-NN'
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：使用SVM和K-NN的训练和验证数据准确度
- en: As we can see, not only does the stacked model give us a validation accuracy
    that is significantly higher than either of the base predictors, but it also has
    the highest accuracy, of nearly 89%, of all the ensemble models discussed in this
    chapter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，堆叠模型不仅使得验证准确度显著高于任何一个基预测器，而且它的准确度接近89%，是本章讨论的所有集成模型中最高的。
- en: 'Activity 14: Stacking with Standalone and Ensemble Algorithms'
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 14：使用独立和集成算法进行堆叠
- en: 'In this activity, we''ll use the *Kaggle House Prices: Advanced Regression
    Techniques database* (available at [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)
    or on GitHub at [https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)),
    that we did EDA on in *Chapter 2*, *Exploratory Data Analysis and Visualization*.
    This dataset is aimed toward solving a regression problem (that is, the target
    variable takes on a range of continuous values). In this activity, we will use
    decision trees, K-nearest neighbors, Random Forest, and gradient boosting algorithms
    to train individual regressors on the data. Then, we will build a stacked linear
    regression model that uses all these algorithms and compare the performance of
    each. We will use the **mean absolute error** (**MAE**) as the evaluation metric
    for this activity.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用*Kaggle房价：高级回归技术数据库*（可在[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)上获取，或在GitHub上访问[https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python](https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-Python)），该数据集我们在*第2章*《探索性数据分析与可视化》中做过EDA。这份数据集旨在解决回归问题（即目标变量为连续值的范围）。在本活动中，我们将使用决策树、K-最近邻、随机森林和梯度提升算法在数据上训练个体回归器。然后，我们将构建一个堆叠线性回归模型，使用所有这些算法并比较每个模型的性能。我们将使用**平均绝对误差**（**MAE**）作为本活动的评估指标。
- en: 'The steps to be performed are as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Import the relevant libraries.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关库。
- en: Read the data.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据。
- en: Preprocess the dataset to remove null values and one-hot encode categorical
    variables to prepare the data for modeling.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集进行预处理，去除空值，并对分类变量进行独热编码，为建模准备数据。
- en: Divide the dataset into train and validation DataFrames.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和验证集。
- en: Initialize dictionaries in which to store the train and validation MAE values.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化字典以存储训练和验证的MAE值。
- en: 'Train a decision tree model with the following hyperparameters and save the
    scores:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下超参数训练决策树模型并保存得分：
- en: '[PRE26]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Train a k-nearest neighbors model with the following hyperparameters and save
    the scores:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下超参数训练k-最近邻模型并保存得分：
- en: '[PRE27]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Train a Random Forest model with the following hyperparameters and save the
    scores:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下超参数训练随机森林模型并保存得分：
- en: '[PRE28]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Train a gradient boosting model with the following hyperparameters and save
    the scores:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下超参数训练梯度提升模型并保存得分：
- en: '[PRE29]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Prepare the training and validation datasets, with the four meta estimators
    having the same hyperparameters that were used in the previous steps.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练集和验证集，其中四个元估计器具有与前面步骤中使用的相同的超参数。
- en: Train a linear regression model as the stacked model.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个线性回归模型作为堆叠模型。
- en: Visualize the train and validation errors for each individual model and the
    stacked model.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化每个独立模型和堆叠模型的训练误差和验证误差。
- en: Note
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 364.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第364页找到。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started off with a discussion on overfitting and underfitting
    and how these can affect the performance of a model on unseen data. The chapter
    looked at ensemble modeling as a solution for these and went on to discuss different
    ensemble methods that could be used, and how they could decrease the overall bias
    or variance encountered when making predictions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从讨论过拟合和欠拟合以及它们如何影响模型在未见数据上的表现开始。接着，本章探讨了集成建模作为解决这些问题的方法，并继续讨论了可以使用的不同集成方法，以及它们如何减少在进行预测时遇到的总体偏差或方差。
- en: We first discussed bagging algorithms and introduced the concept of bootstrapping.
    Then, we looked at Random Forest as a classic example of a Bagged ensemble and
    solved exercises that involved building a bagging classifier and Random Forest
    classifier on the previously seen Titanic dataset.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论了袋装算法并介绍了自助抽样的概念。然后，我们看了随机森林作为袋装集成的经典例子，并完成了在之前的泰坦尼克数据集上构建袋装分类器和随机森林分类器的练习。
- en: We then moved on to discussing boosting algorithms, how they successfully reduce
    bias in the system, and gained an understanding of how to implement adaptive boosting
    and gradient boosting. The last ensemble method we discussed was stacking, which,
    as we saw from the exercise, gave us the best accuracy score of all the ensemble
    methods we implemented.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续讨论了提升算法，如何成功减少系统中的偏差，并理解了如何实现自适应提升和梯度提升。我们讨论的最后一种集成方法是堆叠，正如我们从练习中看到的那样，它给出了我们实现的所有集成方法中最好的准确率。
- en: Although building an ensemble model is a great way to decrease bias and variance,
    and they generally outperform any single model by itself, they themselves come
    with their own problems and use cases. While bagging is great when trying to avoid
    overfitting, boosting can reduce both bias and variance, though it may still have
    a tendency to overfit. Stacking, on the other hand, is a good choice for when
    one model performs well on a portion of the data while another model performs
    better on another portion of the data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管构建集成模型是减少偏差和方差的好方法，而且它们通常比单一模型表现得更好，但它们本身也有自己的问题和使用场景。虽然袋装（bagging）在避免过拟合时非常有效，但提升（boosting）可以减少偏差和方差，尽管它仍然可能有过拟合的倾向。而堆叠（stacking）则是当一个模型在某部分数据上表现良好，而另一个模型在另一部分数据上表现更好时的好选择。
- en: In the next chapter, we will explore more ways to overcome the problems of overfitting
    and underfitting in detail by looking at validation techniques, that is, ways
    to judge our model's performance, and how to use different metrics as indicators
    to build the best possible model for our use case.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将通过探讨验证技术来详细研究克服过拟合和欠拟合问题的方法，也就是评估模型性能的方法，以及如何使用不同的指标作为参考，构建最适合我们用例的模型。
