- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Mining the 20 Newsgroups Dataset with Text Analysis Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文本分析技术挖掘20个新闻组数据集
- en: In previous chapters, we went through a bunch of fundamental machine learning
    concepts and supervised learning algorithms. Starting from this chapter, as the
    second step of our learning journey, we will be covering in detail several important
    unsupervised learning algorithms and techniques related to text analysis. To make
    our journey more interesting, we will start with a **Natural Language Processing**
    (**NLP**) problem—exploring the 20 newsgroups data. You will gain hands-on experience
    and learn how to work with text data, especially how to convert words and phrases
    into machine-readable values and how to clean up words with little meaning. We
    will also visualize text data by mapping it into a two-dimensional space in an
    unsupervised learning manner.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们学习了一些基本的机器学习概念和监督学习算法。从这一章开始，作为我们学习旅程的第二步，我们将详细讲解与文本分析相关的几种重要无监督学习算法和技术。为了让我们的旅程更加有趣，我们将从一个**自然语言处理**（**NLP**）问题开始——探索20个新闻组数据。你将获得实践经验，学习如何处理文本数据，特别是如何将单词和短语转换为计算机可读的数值，并如何清理意义不大的词语。我们还将通过将文本数据映射到二维空间，以无监督学习的方式进行可视化。
- en: 'We will go into detail on each of the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论以下每个主题：
- en: How computers understand language – NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机如何理解语言——自然语言处理（NLP）
- en: Touring popular NLP libraries and picking up NLP basics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解流行的NLP库并学习NLP基础
- en: Getting the newsgroups data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取新闻组数据
- en: Exploring the newsgroups data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索新闻组数据
- en: Thinking about features for text data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思考文本数据的特征
- en: Visualizing the newsgroups data with t-SNE
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用t-SNE可视化新闻组数据
- en: Representing words with dense vectors – word embedding
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用密集向量表示词语——词嵌入
- en: How computers understand language – NLP
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机如何理解语言——自然语言处理（NLP）
- en: 'In *Chapter 1*, *Getting Started with Machine Learning and Python*, I mentioned
    that machine learning-driven programs or computers are good at discovering event
    patterns by processing and working with data. When the data is well structured
    or well defined, such as in a Microsoft Excel spreadsheet table or a relational
    database table, it is intuitively obvious why machine learning is better at dealing
    with it than humans. Computers read such data the same way as humans—for example,
    `revenue: 5,000,000` as the revenue being 5 million, and `age: 30` as the age
    being 30; then computers crunch assorted data and generate insights in a faster
    way than humans. However, when the data is unstructured, such as words with which
    humans communicate, news articles, or someone’s speech in another language, it
    seems that computers cannot understand words as well as humans do (yet). While
    computers have made significant progress in understanding words and natural language,
    they still fall short of human-level understanding in many aspects.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '在*第一章*，*机器学习与Python入门*中，我提到过，机器学习驱动的程序或计算机擅长通过处理和利用数据发现事件模式。当数据结构良好或定义明确时，例如在Microsoft
    Excel电子表格表格或关系数据库表格中，为什么机器学习比人类更擅长处理这些数据是直观显而易见的。计算机读取这些数据的方式与人类相同——例如，`revenue:
    5,000,000` 表示收入为500万，`age: 30` 表示年龄为30；然后计算机会处理各种数据，并以比人类更快的方式生成洞察。然而，当数据是非结构化的，例如人类沟通使用的词语、新闻文章，或某人用另一种语言的演讲时，似乎计算机无法像人类一样理解这些词语（尽管如此）。尽管计算机在理解词语和自然语言方面取得了显著进展，但在许多方面仍无法达到人类级别的理解。'
- en: What is NLP?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是NLP？
- en: 'There is a lot of information in the world about words, raw text, or, broadly
    speaking, **natural language**. This refers to any language that humans use to
    communicate with each other. Natural language can take various forms, including,
    but not limited to, the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于词语、原始文本或广义上的**自然语言**，世界上有大量的信息。自然语言指的是人类用来相互沟通的任何语言。自然语言可以有多种形式，包括但不限于以下内容：
- en: Text, such as a web page, SMS, emails, and menus
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本，例如网页、短信、电子邮件和菜单
- en: Audio, such as speech and commands to Siri
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频，如语音和Siri的命令
- en: Signs and gestures
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手势与符号
- en: Many other forms, such as songs, sheet music, and Morse code
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多其他形式，如歌曲、乐谱和摩尔斯电码
- en: The list is endless, and we are all surrounded by natural language all of the
    time (that’s right, right now as you are reading this book). Given the importance
    of this type of unstructured data (natural language data), we must have methods
    to get computers to understand and reason with natural language and to extract
    data from it. Programs equipped with NLP techniques can already do a lot in certain
    areas, which already seems magical!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表是无尽的，我们随时都被自然语言包围着（没错，现在你在读这本书时也是如此）。考虑到这种非结构化数据（自然语言数据）的重要性，我们必须找到方法让计算机理解和推理自然语言，并从中提取数据。配备了NLP技术的程序在某些领域已经可以做很多事情，这已经显得有些神奇了！
- en: 'NLP is a significant subfield of machine learning that deals with the interactions
    between machines (computers) and human (natural) languages. The data for NLP tasks
    can be in different forms, for example, text from social media posts, web pages,
    or even medical prescriptions, or audio from voice mails, commands to control
    systems, or even a favorite song or movie. Nowadays, NLP is broadly involved in
    our daily lives: we cannot live without machine translation, weather forecast
    scripts are automatically generated, we find voice search convenient, we get the
    answer to a question (such as “What is the population of Canada?”) quickly thanks
    to intelligent question-answering systems, speech-to-text technology helps people
    with special needs, and so on.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是机器学习的一个重要子领域，处理的是机器（计算机）与人类（自然）语言之间的交互。NLP任务的数据可以以不同的形式存在，例如社交媒体帖子、网页，甚至医疗处方中的文本，或者语音邮件、控制系统命令，甚至是你最喜欢的歌曲或电影中的音频。如今，NLP已经广泛应用到我们的日常生活中：我们无法离开机器翻译，天气预报脚本被自动生成，我们觉得语音搜索很方便，我们可以迅速获得问题的答案（例如“加拿大的人口是多少？”），得益于智能问答系统，语音转文字技术帮助有特殊需求的人，等等。
- en: Generative AI and its applications like ChatGPT are pushing the boundaries of
    NLP even further. Imagine a world where you can have a conversation with a virtual
    assistant that can not only answer your questions in a comprehensive way but also
    generate different creative text formats, like poems, code, scripts, musical pieces,
    emails, letters, and so on. By analyzing massive amounts of text data, it can
    learn the underlying patterns and structures of language, allowing it to generate
    human-quality text content. For instance, you could ask ChatGPT to write a funny
    birthday poem for your friend, craft a compelling marketing email for your business,
    or even brainstorm ideas for a new blog post.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能及其应用，如ChatGPT，正在进一步推动NLP的边界。想象一下这样一个世界，你可以与一个虚拟助手对话，它不仅可以全面回答你的问题，还可以生成各种创意文本格式，如诗歌、代码、剧本、音乐作品、电子邮件、信件等等。通过分析大量的文本数据，它可以学习语言的基本模式和结构，使其能够生成接近人类质量的文本内容。例如，你可以让ChatGPT为你的朋友写一首幽默的生日诗，为你的商业制作一封引人注目的营销邮件，甚至为一个新的博客文章头脑风暴一些创意点子。
- en: The history of NLP
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的历史
- en: If machines are able to understand language like humans do, we consider them
    intelligent. In 1950, the famous mathematician Alan Turing proposed in an article,
    *Computing Machinery and Intelligence*, a test as a criterion of machine intelligence.
    It’s now called the **Turing test** ([https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/)),
    and its goal is to examine whether a computer is able to adequately understand
    languages so as to fool humans into thinking that the machine is another human.
    It is probably no surprise to you that no computer has passed the Turing test
    yet, but the 1950s is considered to be when the history of NLP started.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器能够像人类一样理解语言，我们就认为它们是智能的。1950年，著名数学家艾伦·图灵在一篇文章《计算机机械与智能》中提出了一个测试作为机器智能的标准。现在这个测试被称为**图灵测试**（[https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/)），其目的是检验计算机是否能够充分理解语言，从而使人类误以为机器是另一个人。可能你并不惊讶，因为至今没有计算机通过图灵测试，但1950年代被认为是自然语言处理历史的起点。
- en: Understanding language might be difficult, but would it be easier to automatically
    translate texts from one language to another? On my first ever programming course,
    the lab booklet had the algorithm for coarse-grained machine translation. This
    type of translation involved looking up words in dictionaries and generating text
    in a new language. A more practically feasible approach would be to gather texts
    that are already translated by humans and train a computer program on these texts.
    In 1954, in the Georgetown–IBM experiment ([https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment)),
    scientists claimed that machine translation would be solved in three to five years.
    Unfortunately, a machine translation system that can beat human expert translators
    does not exist yet. But machine translation has been greatly evolving since the
    introduction of deep learning and has seen incredible achievements in certain
    areas, for example, social media (Facebook open sourced a neural machine translation
    system, [https://ai.facebook.com/tools/translate/](https://ai.facebook.com/tools/translate/)),
    real-time conversation (Microsoft Translator, SwiftKey Keyboard, and Google Pixel
    Buds), and image-based translation, such as Google Translate.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理解语言可能很困难，但自动将文本从一种语言翻译成另一种语言会更容易吗？在我第一次参加编程课程时，实验手册中有粗粒度机器翻译的算法。这种翻译方式涉及查阅字典中的单词，并生成新语言的文本。一种更为实际可行的方法是收集已经由人类翻译的文本，并用这些文本训练计算机程序。在1954年，乔治城–IBM实验（[https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment](https://en.wikipedia.org/wiki/Georgetown%E2%80%93IBM_experiment)）中，科学家们声称机器翻译将在三到五年内解决。不幸的是，能够超过人类专家翻译的机器翻译系统至今尚未出现。但自从引入深度学习以来，机器翻译已大大发展，并在某些领域取得了令人难以置信的成就，例如社交媒体（Facebook开源了一个神经机器翻译系统，[https://ai.facebook.com/tools/translate/](https://ai.facebook.com/tools/translate/)），实时对话（微软翻译、SwiftKey键盘和Google
    Pixel Buds），以及基于图像的翻译，如Google翻译。
- en: Conversational agents, or chatbots, are another hot topic in NLP. The fact that
    computers are able to have a conversation with us has reshaped the way businesses
    are run. In 2016, Microsoft’s AI chatbot, Tay ([https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/)),
    was unleashed to mimic a teenage girl and converse with users on Twitter (now
    X) in real time. She learned how to speak from all the things users posted and
    commented on Twitter. However, she was overwhelmed by tweets from trolls and automatically
    learned their bad behaviors and started to output inappropriate things on her
    feeds. She ended up being terminated within 24 hours. Generative AI models like
    ChatGPT are another area of active research, pushing the boundaries of what’s
    possible. They can be helpful for creative text formats or specific tasks, but
    achieving true human-level understanding in conversation remains an ongoing pursuit.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 会话代理或聊天机器人是自然语言处理中的另一个热门话题。计算机能够与我们对话，重新定义了商业运作的方式。在2016年，微软的AI聊天机器人Tay（[https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/)）被推出，模拟一个年轻女孩并实时与Twitter（现为X）上的用户进行对话。她通过用户发布和评论的内容学习如何说话。然而，她被恶意用户的推文淹没，自动学会了他们的不良行为，并开始在她的动态中输出不当的内容。最终，她在24小时内被终止了。像ChatGPT这样的生成式AI模型是另一个活跃的研究领域，推动了可能性的边界。它们在创造性文本格式或特定任务中可能有所帮助，但实现真正的人类水平的对话理解仍然是一个持续的追求。
- en: NLP applications
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理应用
- en: There are also several text analysis tasks that attempt to organize knowledge
    and concepts in such a way that they become easier for computer programs to manipulate.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些文本分析任务，试图以某种方式组织知识和概念，使它们更容易被计算机程序操作。
- en: The way we organize and represent concepts is called **ontology**. An ontology
    defines concepts and relationships between concepts. For instance, we can have
    a so-called triple, such as (`"python"`, `"language"`, `"is-a"`) representing
    the relationship between two concepts, such as *Python is a language*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们组织和表示概念的方式被称为**本体论**。本体论定义了概念及其之间的关系。例如，我们可以有一个所谓的三元组，如（`"python"`，`"language"`，`"is-a"`），表示两个概念之间的关系，例如*Python是一种语言*。
- en: An important use case for NLP at a much lower level, compared to the previous
    cases, is **part-of-speech** (**PoS**) **tagging**. A PoS is a grammatical word
    category such as a noun or verb. PoS tagging tries to determine the appropriate
    tag for each word in a sentence or a larger document.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述案例相比，NLP 的一个重要用例是**词性标注**（**PoS tagging**），这在很大程度上是一个较低级别的应用。词性是一个词类，如名词或动词。词性标注试图确定句子或更大文档中每个词的适当标签。
- en: 'The following table gives examples of English PoSs:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格给出了英语 PoS 的示例：
- en: '| **Part of speech** | **Examples** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **词性** | **示例** |'
- en: '| Noun | David, machine |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 名词 | 大卫, 机器 |'
- en: '| Pronoun | They, her |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 代词 | 他们, 她的 |'
- en: '| Adjective | Awesome, amazing |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 形容词 | 很棒, 了不起 |'
- en: '| Verb | Read, write |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 动词 | 阅读, 写作 |'
- en: '| Adverb | Very, quite |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 副词 | 非常, 相当 |'
- en: '| Preposition | Out, at |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 介词 | 出, 在 |'
- en: '| Conjunction | And, but |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 连词 | 和, 但是 |'
- en: '| Interjection | Phew, oops |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 感叹词 | 哎呀, 哎哟 |'
- en: '| Article | A, the |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 冠词 | 一, 这 |'
- en: 'Table 7.1: PoS examples'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7.1: PoS 示例'
- en: There are a variety of real-world NLP applications involving supervised learning,
    such as PoS tagging, mentioned earlier, and **sentiment analysis**. A typical
    example is identifying news sentiment, which could be positive or negative in
    the binary case, or positive, neutral, or negative in multiclass classification.
    News sentiment analysis provides a significant signal to trading in the stock
    market.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种涉及监督学习的现实世界 NLP 应用，例如之前提到的 PoS 标签，以及**情感分析**。一个典型的例子是识别新闻情感，这可能在二进制情况下是积极或消极的，或者在多类分类中是积极的、中性的或消极的。新闻情感分析为股市交易提供了重要信号。
- en: Another example we can easily think of is news topic classification, where classes
    may or may not be mutually exclusive. In the newsgroup example that we just discussed,
    classes are mutually exclusive (despite slight overlapping), such as technology,
    sports, and religion. It is, however, good to realize that a news article can
    be occasionally assigned multiple categories (multi-label classification). For
    example, an article about the Olympic Games may be labeled sports and politics
    if there is unexpected political involvement.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以轻易想到的例子是新闻主题分类，其中类别可能是互斥的，也可能不是。在刚才讨论的新闻组例子中，类别是互斥的（尽管稍有重叠），如技术、体育和宗教。然而，需要意识到新闻文章偶尔可能被分配多个类别（多标签分类）。例如，如果涉及到意外的政治参与，关于奥运会的文章可能被标记为体育和政治。
- en: 'Finally, an interesting application that is perhaps unexpected is **Named Entity
    Recognition** (**NER**). Named entities are phrases of definitive categories,
    such as names of persons, companies, geographic locations, dates and times, quantities,
    and monetary values. NER is an important subtask of information extraction to
    seek and identify such entities. For example, we can conduct NER on the following
    sentence: `SpaceX[Organization]`, a `California[Location]`-based company founded
    by a famous tech entrepreneur `Elon Musk[Person]`, announced that it would manufacture
    the next-generation, `9[Quantity]`-meter-diameter launch vehicle and spaceship
    for the first orbital flight in `2020[Date]`.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个也许意想不到但很有趣的应用是**命名实体识别**（**NER**）。命名实体是明确类别的短语，如人名、公司名、地理位置、日期和时间、数量和货币值。NER
    是信息提取的重要子任务，旨在寻找和识别这些实体。例如，我们可以对以下句子进行NER：`SpaceX[组织]`，一家由著名技术企业家`Elon Musk[人物]`创立，总部位于`加利福尼亚[地点]`的公司，宣布将为首次轨道飞行制造下一代直径为`9[数量]`米的发射车和飞船。
- en: 'Other key NLP applications include:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其他关键的 NLP 应用包括：
- en: '**Language translation**: NLP powers machine translation systems, enabling
    automatic translation of text or speech from one language to another. Platforms
    like Google Translate and Microsoft Translator utilize NLP to provide real-time
    translation services.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：NLP 推动机器翻译系统，实现从一种语言到另一种语言的自动翻译。像 Google Translate 和 Microsoft Translator
    这样的平台利用 NLP 提供实时翻译服务。'
- en: '**Speech recognition**: NLP is essential in speech recognition systems, converting
    spoken language into written text. Virtual assistants like Siri, Alexa, and Google
    Assistant rely on NLP to understand user commands and respond appropriately.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：NLP 在语音识别系统中至关重要，将口语转换为书面文本。像 Siri、Alexa 和 Google Assistant 这样的虚拟助手依赖于
    NLP 来理解用户命令并适当地回应。'
- en: '**Text summarization**: NLP can automatically generate concise summaries of
    lengthy texts, providing a quick overview of the content. Text summarization is
    useful for information retrieval and content curation.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要**：NLP可以自动生成简明扼要的长文本摘要，提供内容的快速概览。文本摘要对于信息检索和内容策划非常有用。'
- en: '**Language generation**: NLP models, such as **Generative Pre-trained Transformers**
    (**GPTs**), can generate human-like text, including creative writing, poetry,
    and dialogue generation.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言生成**：NLP模型，如**生成预训练变换器**（**GPTs**），可以生成类人文本，包括创意写作、诗歌和对话生成。'
- en: '**Information retrieval**: NLP assists in information retrieval from large
    volumes of unstructured data, such as web pages, documents, and news articles.
    Search engines use NLP techniques to understand user queries and retrieve relevant
    results.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息检索**：NLP帮助从大量的非结构化数据中检索信息，如网页、文档和新闻文章。搜索引擎使用NLP技术来理解用户查询并检索相关结果。'
- en: '**Chatbots, question answering, and virtual assistants**: NLP powers chatbots
    and virtual assistants to provide interactive and conversational experiences.
    These systems can answer queries, assist with tasks, and guide users through various
    processes.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天机器人、问答系统和虚拟助手**：NLP为聊天机器人和虚拟助手提供支持，提供交互式和对话式的体验。这些系统可以回答查询、协助任务，并引导用户完成各种流程。'
- en: In the next chapter, we will discuss how unsupervised learning, including clustering
    and topic modeling, is applied to text data. We will begin by covering NLP basics
    in the upcoming sections of this chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何将无监督学习（包括聚类和主题建模）应用于文本数据。我们将从本章的后续部分开始，介绍NLP的基础知识。
- en: Touring popular NLP libraries and picking up NLP basics
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参观流行的NLP库并学习NLP基础
- en: Now that we have covered a short list of real-world applications of NLP, we
    will be touring the essential stack of Python NLP libraries. These packages handle
    a wide range of NLP tasks, as mentioned previously, including sentiment analysis,
    text classification, and NER.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些NLP的实际应用，我们将参观Python NLP库的核心工具包。这些包处理广泛的NLP任务，如情感分析、文本分类和NER。
- en: Installing famous NLP libraries
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装著名的NLP库
- en: 'The most famous NLP libraries in Python include the **Natural Language Toolkit**
    (**NLTK**), **spaCy**, **Gensim**, and **TextBlob.** The scikit-learn library
    also has impressive NLP-related features. Let’s take a look at them in more detail:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Python中最著名的NLP库包括**自然语言工具包**（**NLTK**）、**spaCy**、**Gensim**和**TextBlob**。scikit-learn库也具有令人印象深刻的与NLP相关的功能。让我们更详细地了解它们：
- en: '**NLTK**: This library ([http://www.nltk.org/](http://www.nltk.org/)) was originally
    developed for educational purposes and is now widely used in industry as well.
    It is said that you can’t talk about NLP without mentioning NLTK. It is one of
    the most famous and leading platforms for building Python-based NLP applications.
    You can install it simply by running the following command line in the terminal:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLTK**：这个库（[http://www.nltk.org/](http://www.nltk.org/)）最初是为教育目的开发的，现在也被广泛应用于工业界。据说，如果不提到NLTK，就不能谈论NLP。它是构建基于Python的NLP应用程序最著名和领先的平台之一。你可以通过在终端中运行以下命令来简单安装它：'
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you’re using `conda`, execute the following command line:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用`conda`，执行以下命令：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**spaCy**: This library (`https://spacy.io/`) is a more powerful toolkit in
    the industry than NLTK. This is mainly for two reasons: first, `spaCy` is written
    in Cython, which is much more memory-optimized (now you can see where the `Cy`
    in `spaCy` comes from) and excels in NLP tasks; second, `spaCy` uses state-of-the-art
    algorithms for core NLP problems, such as **convolutional neural network** (**CNN**)
    models for tagging and NER. However, it could seem advanced for beginners. In
    case you’re interested, here are the installation instructions.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spaCy**：这个库（`https://spacy.io/`）在行业中比NLTK更强大。主要有两个原因：首先，`spaCy`是用Cython编写的，内存优化效果更好（现在你可以理解`spaCy`中的`Cy`是什么意思），并且在NLP任务中表现优异；其次，`spaCy`使用最先进的算法解决核心NLP问题，例如用于标注和命名实体识别（NER）的**卷积神经网络**（**CNN**）模型。然而，对于初学者来说，它可能显得较为复杂。如果你有兴趣，下面是安装说明。'
- en: 'Run the following command line in the terminal:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行以下命令：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For `conda`, execute the following command line:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`conda`，执行以下命令：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Gensim**: This library ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)),
    developed by Radim Rehurek, has been gaining popularity over recent years. It
    was initially designed in 2008 to generate a list of similar articles given an
    article, hence the name of this library (`generate similar`—> `Gensim`). It was
    later drastically improved by Radim Rehurek in terms of its efficiency and scalability.
    Again, you can easily install it via `pip` by running the following command line:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gensim**：这个库 ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/))
    由 Radim Rehurek 开发，近年来逐渐受到欢迎。它最初在 2008 年设计，用于给定一篇文章后生成相似文章的列表，因此得名（`generate similar`
    --> `Gensim`）。后来，Radim Rehurek 在效率和可扩展性方面对其进行了大幅改进。再次提醒，你可以通过运行以下命令行轻松安装它：'
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the case of `conda`, you can execute the following command line in the terminal:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用 `conda`，你可以在终端中执行以下命令行：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should make sure that the dependencies, NumPy and SciPy, are already installed
    before Gensim.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 Gensim 之前，你需要确保已经安装了依赖项 NumPy 和 SciPy。
- en: '**TextBlob**: This library ([https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/))
    is a relatively new one built on top of NLTK. It simplifies NLP and text analysis
    with easy-to-use built-in functions and methods, as well as wrappers around common
    tasks. We can install `TextBlob` by running the following command line in the
    terminal:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TextBlob**：这个库 ([https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/))
    是一个相对较新的库，建立在 NLTK 之上。它通过易于使用的内置函数和方法，以及对常见任务的包装，简化了自然语言处理（NLP）和文本分析。我们可以通过在终端运行以下命令行安装
    `TextBlob`：'
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or, for conda:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，使用 conda：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`TextBlob` has some useful features that are not available in NLTK (currently),
    such as spell checking and correction, language detection, and translation.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextBlob` 有一些在 NLTK 中（目前）没有的有用功能，比如拼写检查与纠正、语言检测和翻译。'
- en: Corpora
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语料库
- en: 'NLTK comes with over 100 collections of large and well-structured text datasets,
    which are called **corpora** in NLP. Here are some of the main corpora that NLTK
    provides:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 提供了 100 多个大型且结构良好的文本数据集，称为 NLP 中的 **语料库**。以下是 NLTK 提供的一些主要语料库：
- en: '**Gutenberg Corpus**: A collection of literary works from Project Gutenberg,
    containing thousands of books in various languages.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gutenberg Corpus**：来自 Gutenberg 项目的文学作品集合，包含多种语言的数千本书籍。'
- en: '**Reuters Corpus**: A collection of news articles from the Reuters newswire
    service, widely used for text classification and topic modeling tasks.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reuters Corpus**：来自路透社新闻服务的新闻文章集合，广泛用于文本分类和主题建模任务。'
- en: '**Web and Chat Text**: A collection of web text and chat conversations, providing
    a glimpse into informal language and internet slang.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Web and Chat Text**：一个包含网络文本和聊天对话的集合，展示了非正式语言和互联网俚语。'
- en: '**Movie Reviews Corpus**: A collection of movie reviews, often used for sentiment
    analysis and text classification tasks.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Movie Reviews Corpus**：一个包含电影评论的语料库，常用于情感分析和文本分类任务。'
- en: '**Treebank Corpus**: A collection of parsed and tagged sentences from the Penn
    Treebank, used for training and evaluating syntactic parsers.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Treebank Corpus**：来自 Penn Treebank 的解析和标注句子集合，用于训练和评估句法分析器。'
- en: '**WordNet**: A lexical database of English words, containing synsets (groups
    of synonymous words) and hypernyms (is-a relationships).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WordNet**：一个英语词汇数据库，包含同义词集（同义词的组合）和上位词（“是”关系）。'
- en: Corpora can be used as dictionaries for checking word occurrences and as training
    pools for model learning and validating. Some more useful and interesting corpora
    include the Web Text corpus, Twitter (X) samples, the Shakespeare corpus, Sentiment
    Polarity, the Names corpus (this contains lists of popular names, which we will
    be exploring very shortly), WordNet, and the Reuters benchmark corpus. The full
    list can be found at [http://www.nltk.org/nltk_data](http://www.nltk.org/nltk_data).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库可以用作检查单词出现频率的词典，也可以作为模型学习和验证的训练池。一些更有用且有趣的语料库包括 Web Text 语料库、Twitter（X）样本、莎士比亚语料库、情感极性语料库、名字语料库（该语料库包含流行名字的列表，我们很快就会探索这些），WordNet
    和 Reuters 基准语料库。完整列表可以在 [http://www.nltk.org/nltk_data](http://www.nltk.org/nltk_data)
    查找到。
- en: 'Before using any of these corpus resources, we need to first download them
    by running the following code in the Python interpreter:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何这些语料库资源之前，我们需要先通过在 Python 解释器中运行以下代码下载它们：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A new window will pop up and ask you which collections (the **Collections**
    tab in the following screenshot) or corpus (the **Corpora** tab in the following
    screenshot) to download, and where to keep the data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新窗口会弹出，询问你要下载哪些集合（下图中的**Collections**标签）或语料库（下图中的**Corpora**标签），以及数据存储的位置：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21047_07_01.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21047_07_01.png)'
- en: 'Figure 7.1: Collections tab in the NLTK installation'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：NLTK安装中的“Collections”标签
- en: 'Installing the whole popular package is the quickest solution since it contains
    all the important corpora needed for your current study and future research. Installing
    a particular corpus, as shown in the following screenshot, is also fine:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 安装整个流行的包是最快的解决方案，因为它包含了当前学习和未来研究所需的所有重要语料库。安装特定的语料库，如下图所示，也是可以的：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21047_07_02.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21047_07_02.png)'
- en: 'Figure 7.2: Corpora tab in the NLTK installation'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：NLTK安装中的“Corpora”标签
- en: Once the package or corpus you want to explore is installed, you can take a
    look at the **Names** corpus (make sure the names corpus is installed for this
    example).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你想要探索的包或语料库安装完成，你可以查看**Names**语料库（确保这个语料库已经安装以便进行本例的操作）。
- en: 'First, import the names corpus:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入名称语料库：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can check out the first `10` names in the list:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看列表中的前`10`个名称：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are, in total, `7944` names, as shown in the following output derived
    by executing the following command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有`7944`个名称，如下所示，执行以下命令后得到的输出：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Other corpora are also fun to explore.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其他语料库也很有趣，值得探索。
- en: Besides the easy-to-use and abundant corpora pool, more importantly, NLTK is
    also good at many NLP and text analysis tasks, including tokenization, PoS tagging,
    NER, word stemming, and lemmatization. We’ll look at these tasks next.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了易于使用且丰富的语料库池，更重要的是，NLTK还擅长许多NLP和文本分析任务，包括标记化、词性标注、命名实体识别（NER）、词干提取和词形还原。接下来我们将讨论这些任务。
- en: Tokenization
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化
- en: Given a text sequence, **tokenization** is the task of breaking it into fragments,
    which can be words, characters, or sentences. Certain characters are usually removed,
    such as punctuation marks, digits, and emoticons. The remaining fragments are
    the so-called **tokens** used for further processing.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本序列，**标记化**是将其拆分成碎片的任务，这些碎片可以是单词、字符或句子。通常会去除一些字符，如标点符号、数字和表情符号。剩下的碎片就是所谓的**标记**，用于进一步处理。
- en: 'Tokens composed of one word are also called **unigrams** in computational linguistics;
    **bigrams** are composed of two consecutive words; **trigrams** of three consecutive
    words; and **n-grams** of *n* consecutive words. Here is an example of tokenization:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由一个词组成的标记在计算语言学中也称为**单词单元**（unigrams）；由两个连续词组成的标记称为**二元组**（bigrams）；由三个连续词组成的标记称为**三元组**（trigrams）；而**n-grams**是由*n*个连续词组成的。这里是一个标记化的例子：
- en: '![A picture containing text, font, line, number  Description automatically
    generated](img/B21047_07_03.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, font, line, number  Description automatically
    generated](img/B21047_07_03.png)'
- en: 'Figure 7.3: Tokenization example'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：标记化示例
- en: 'We can implement word-based tokenization using the `word_tokenize` function
    in NLTK. We will use the input text `''''''I am reading a book.`, and on the next
    line, `It is Python Machine Learning By Example,`, then `4th edition.''''''`,
    as an example, as shown in the following commands:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NLTK中的`word_tokenize`函数实现基于单词的标记化。我们将以输入文本`'''I am reading a book.`, 然后是`It
    is Python Machine Learning By Example,`, 接着是`4th edition.'''`作为示例，如下命令所示：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Word tokens are obtained.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 得到了单词标记。
- en: The `word_tokenize` function keeps punctuation marks and digits, and only discards
    whitespaces and newlines.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_tokenize`函数会保留标点符号和数字，只丢弃空格和换行符。'
- en: 'You might think word tokenization is simply splitting a sentence by space and
    punctuation. Here’s an interesting example showing that tokenization is more complex
    than you think:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为词语标记化仅仅是通过空格和标点符号分割句子。这里有一个有趣的例子，表明标记化比你想象的要复杂：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The tokenizer accurately recognizes the words `'U.K.'` and `'U.S.A'` as tokens
    instead of `'U'` and `'.'` followed by `'K'`, for example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，分词器能准确地将`'U.K.'`和`'U.S.A'`识别为标记，而不是将`'U'`和`'.'`跟在`'K'`后面。
- en: 'spaCy also has an outstanding tokenization feature. It uses an accurately trained
    model that is constantly updated. To install it, we can run the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy还具有出色的分词功能。它使用一个精确训练的模型，该模型会不断更新。要安装它，我们可以运行以下命令：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we load the `en_core_web_sm` model (if you have not downloaded the model,
    you can run `python -m spacy download en_core_web_sm` to do so) and parse the
    sentence using this model:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们加载`en_core_web_sm`模型（如果您尚未下载该模型，可以运行`python -m spacy download en_core_web_sm`来下载），并使用此模型解析句子：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also segment text based on sentences. For example, in the same input
    text, using the `sent_tokenize` function from NLTK, we have the following commands:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以根据句子对文本进行分段。例如，在相同的输入文本中，使用NLTK的`sent_tokenize`函数，得到以下命令：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Two sentence-based tokens are returned, as there are two sentences in the input
    text.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 返回两个基于句子的标记，因为输入文本中有两个句子。
- en: PoS tagging
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PoS标注
- en: We can apply an off-the-shelf tagger from NLTK or combine multiple taggers to
    customize the tagging process. It is easy to directly use the built-in tagging
    function, `pos_tag`, as in `pos_tag(input_tokens)`, for instance, but behind the
    scenes, it is actually a prediction from a pre-built supervised learning model.
    The model is trained based on a large corpus composed of words that are correctly
    tagged.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用NLTK的现成标注器，或者组合多个标注器来定制标注过程。直接使用内置的标注功能`pos_tag`也很容易，例如在`pos_tag(input_tokens)`中使用，但幕后实际上是通过一个预先构建的监督学习模型进行预测。该模型是基于一个由正确标注的单词构成的大型语料库训练的。
- en: 'Reusing an earlier example, we can perform PoS tagging as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 重新使用之前的示例，我们可以如下进行PoS标注：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The PoS tag following each token is returned. We can check the meaning of a
    tag using the `help` function. Looking up `PRP` and `VBP`, for example, gives
    us the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记后会返回其PoS标签。我们可以使用`help`函数查看标签的含义。例如，查找`PRP`和`VBP`会得到以下输出：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In spaCy, getting a PoS tag is also easy. The `token` object parsed from an
    input sentence has an attribute called `pos_`, which is the tag we are looking
    for. Let’s print `pos_` for each token, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy中，获取PoS标签也很简单。`token`对象从输入句子解析后具有一个名为`pos_`的属性，它就是我们要查找的标签。让我们为每个标记打印`pos_`，如下所示：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have just played around with PoS tagging with NLP packages. What about NER?
    Let’s see in the next section.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚尝试了使用NLP包进行PoS标注。那么NER呢？让我们在下一部分看看。
- en: NER
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）
- en: Given a text sequence, the NER task is to locate and identify words or phrases
    that are of definitive categories, such as names of persons, companies, locations,
    and dates. Let’s take a peep at an example of using spaCy for NER.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文本序列，NER任务是定位并识别具有确定类别的单词或短语，例如人名、公司名、地点名和日期。我们来看一个使用spaCy进行NER的示例。
- en: 'First, tokenize an input sentence, `The book written by Hayden Liu in 2024
    was sold at $30 in America`, as usual, as shown in the following command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样对输入句子`The book written by Hayden Liu in 2024 was sold at $30 in America`进行分词，如下所示：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The resultant `token` object contains an attribute called `ents`, which are
    the named entities. We can extract the tagging for each recognized named entity
    as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果`token`对象包含一个名为`ents`的属性，它表示命名实体。我们可以按如下方式提取每个识别出的命名实体的标签：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We can see from the results that `Hayden Liu` is `PERSON`, `2024` is `DATE`,
    `30` is `MONEY`, and `America` is `GPE` (country). Please refer to [https://spacy.io/api/annotation#section-named-entities](https://spacy.io/api/annotation#section-named-entities)
    for a full list of named entity tags.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中我们可以看到，`Hayden Liu`是`PERSON`，`2024`是`DATE`，`30`是`MONEY`，`America`是`GPE`（国家）。完整的命名实体标签列表请参考[https://spacy.io/api/annotation#section-named-entities](https://spacy.io/api/annotation#section-named-entities)。
- en: Stemming and lemmatization
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: Word **stemming** is a process of reverting an inflected or derived word to
    its root form. For instance, *machine* is the stem of *machines*, and *learning*
    and *learned* are generated from *learn* as their stem.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 单词**词干提取**是将屈折或派生词还原为其词根形式的过程。例如，*machine*是*machines*的词干，而*learning*和*learned*是从*learn*派生出的词干。
- en: 'The word **lemmatization** is a cautious version of stemming. It considers
    the PoS of a word when conducting stemming. Also, it traces back to the lemma
    (base or canonical form) of the word. We will discuss these two text preprocessing
    techniques, stemming and lemmatization, in further detail shortly. For now, let’s
    take a quick look at how they’re implemented respectively in NLTK by performing
    the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**词形还原（lemmatization）**是词干提取（stemming）的谨慎版本。在进行词干提取时，它会考虑单词的词性（PoS）。同时，它会追溯到单词的词元（基本或标准形式）。我们将在稍后详细讨论这两种文本预处理技术——词干提取和词形还原。现在，让我们快速看看它们在
    NLTK 中的实现步骤：'
- en: 'Import `porter` as one of the three built-in stemming algorithms (`LancasterStemmer`
    and `SnowballStemmer` are the other two) and initialize the stemmer as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`porter`作为三种内置词干提取算法之一（另外两种分别是`LancasterStemmer`和`SnowballStemmer`），并按如下方式初始化词干提取器：
- en: '[PRE22]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Stem `machines` and `learning`, as shown in the following code:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下代码所示，对`machines`和`learning`进行词干提取：
- en: '[PRE23]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Stemming sometimes involves the chopping of letters if necessary, as you can
    see in `machin` in the preceding command output.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取有时会在必要时截断字母，就像在前面的命令输出中看到的`machin`。
- en: 'Now, import a lemmatization algorithm based on the built-in WordNet corpus
    and initialize a lemmatizer:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，导入一个基于内置 WordNet 语料库的词形还原算法，并初始化词形还原器：
- en: '[PRE24]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similar to stemming, we lemmatize `machines` and `learning`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与词干提取类似，我们对`machines`和`learning`进行词形还原：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Why is `learning` unchanged? The algorithm defaults to finding the lemma for
    nouns unless you specify otherwise. If you want to treat `learning` as a verb,
    you can specify it in `lemmatizer.lemmatize('learning', nltk.corpus.wordnet.VERB)`,
    which will return `learn`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么`learning`没有变化？算法默认会为名词找到词元，除非你特别指定。如果你希望将`learning`视为动词，可以在`lemmatizer.lemmatize('learning',
    nltk.corpus.wordnet.VERB)`中指定，这将返回`learn`。
- en: Semantics and topic modeling
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义和主题建模
- en: Gensim is famous for its powerful semantic and topic-modeling algorithms. Topic
    modeling is a typical text-mining task of discovering the hidden semantic structures
    in a document. A semantic structure in plain English is the distribution of word
    occurrences. It is obviously an unsupervised learning task. What we need to do
    is to feed in plain text and let the model figure out the abstract topics. For
    example, we can use topic modeling to group product reviews on an e-commerce site
    based on the common themes expressed in the reviews. We will study topic modeling
    in detail in *Chapter 8*, *Discovering Underlying Topics in the Newsgroups Dataset
    with Clustering and Topic Modeling*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 以其强大的语义和主题建模算法而闻名。主题建模是一个典型的文本挖掘任务，旨在发现文档中隐藏的语义结构。用简单的英语来说，语义结构就是单词出现的分布。这显然是一个无监督学习任务。我们需要做的是输入纯文本，让模型找出抽象的主题。例如，我们可以使用主题建模根据评论中表达的共同主题将电商网站上的产品评论进行分组。我们将在*第8章*《通过聚类和主题建模发现新闻组数据集中的潜在主题》中详细学习主题建模。
- en: 'In addition to robust semantic modeling methods, gensim also provides the following
    functionalities:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了强大的语义建模方法，gensim 还提供以下功能：
- en: '**Word embedding**: Also known as **word vectorization**, this is an innovative
    way to represent words while preserving words’ co-occurrence features. Later in
    this chapter, we will delve into a comprehensive exploration of word embedding.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入（Word embedding）**：也称为**词向量化**，这是一种创新的单词表示方式，能够保留单词的共现特征。在本章后面，我们将深入探讨词嵌入。'
- en: '**Similarity querying**: This functionality retrieves objects that are similar
    to the given query object. It’s a feature built on top of word embedding.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似度查询**：此功能检索与给定查询对象相似的对象。这是基于词嵌入技术构建的功能。'
- en: '**Distributed computing**: This functionality makes it possible to efficiently
    learn from millions of documents.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式计算**：此功能使得从数百万文档中高效学习成为可能。'
- en: Last but not least, as mentioned in the first chapter, *Getting Started with
    Machine Learning and Python*, scikit-learn is the main package we have used throughout
    this entire book. Luckily, it provides all the text processing features we need,
    such as tokenization, along with comprehensive machine learning functionalities.
    Plus, it comes with a built-in loader for the 20 newsgroups dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，正如第一章《机器学习与 Python 入门》中所提到的，scikit-learn 是本书中我们贯穿始终使用的主要包。幸运的是，它提供了我们所需的所有文本处理功能，例如分词，并且具备全面的机器学习功能。此外，它还内置了一个加载器，用于加载20个新闻组数据集。
- en: Now that the tools are available and properly installed, what about the data?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在工具已经准备好并正确安装，接下来是数据呢？
- en: Getting the newsgroups data
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取新闻组数据
- en: The project in this chapter is about the 20 newsgroups dataset. It’s composed
    of text taken from newsgroup articles, as its name implies. It was originally
    collected by Ken Lang and now has been widely used for experiments in text applications
    of machine learning techniques, specifically NLP techniques.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的项目是关于20个新闻组数据集。顾名思义，它由从新闻组文章中提取的文本组成。最初由Ken Lang收集，现在广泛用于机器学习技术，特别是自然语言处理（NLP）技术的文本应用实验。
- en: The data contains approximately 20,000 documents across 20 online newsgroups.
    A newsgroup is a place on the internet where people can ask and answer questions
    about a certain topic. The data is already cleaned to a certain degree and already
    split into training and testing sets. The cutoff point is at a certain date.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含大约20,000篇文档，分布在20个在线新闻组中。新闻组是互联网上一个人们可以就某一主题提问和回答问题的地方。数据已经经过一定程度的清理，并且已被划分为训练集和测试集。切分点是在某个特定日期。
- en: 'The original data comes from [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/),
    with 20 different topics listed, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据来源于[http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)，列出了20个不同的主题，具体如下：
- en: '`comp.graphics`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comp.graphics`'
- en: '`comp.os.ms-windows.misc`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comp.os.ms-windows.misc`'
- en: '`comp.sys.ibm.pc.hardware`'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comp.sys.ibm.pc.hardware`'
- en: '`comp.sys.mac.hardware`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comp.sys.mac.hardware`'
- en: '`comp.windows.x`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`comp.windows.x`'
- en: '`rec.autos`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rec.autos`'
- en: '`rec.motorcycles`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rec.motorcycles`'
- en: '`rec.sport.baseball`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rec.sport.baseball`'
- en: '`rec.sport.hockey`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rec.sport.hockey`'
- en: '`sci.crypt`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sci.crypt`'
- en: '`sci.electronics`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sci.electronics`'
- en: '`sci.med`'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sci.med`'
- en: '`sci.space`'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sci.space`'
- en: '`misc.forsale`'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`misc.forsale`'
- en: '`talk.politics.misc`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`talk.politics.misc`'
- en: '`talk.politics.guns`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`talk.politics.guns`'
- en: '`talk.politics.mideast`'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`talk.politics.mideast`'
- en: '`talk.religion.misc`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`talk.religion.misc`'
- en: '`alt.atheism`'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alt.atheism`'
- en: '`soc.religion.christian`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soc.religion.christian`'
- en: All of the documents in the dataset are in English. And we can easily deduce
    the topics from the newsgroups’ names.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的所有文档都是英文的。我们可以很容易地从新闻组的名称推断出主题。
- en: The dataset is labeled and each document is composed of text data and a group
    label. This also makes it a perfect fit for supervised learning, such as text
    classification. At the end of the chapter, feel free to practice classification
    on this dataset using what you’ve learned so far in this book.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已经标注，每篇文档由文本数据和一个组标签组成。这也使得它非常适合用于监督学习，如文本分类。在本章结束时，您可以根据到目前为止在本书中学到的内容，尝试使用该数据集进行分类练习。
- en: Some of the newsgroups are closely related or even overlapping – for instance,
    those five computer groups (`comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`,
    `comp.sys.mac.hardware`, and `comp.windows.x`). Some are not closely related to
    each other, such as Christian (`soc.religion.christian`) and baseball (`rec.sport.baseball`).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一些新闻组之间关系密切，甚至有重叠——例如，那五个计算机相关的组（`comp.graphics`、`comp.os.ms-windows.misc`、`comp.sys.ibm.pc.hardware`、`comp.sys.mac.hardware`和`comp.windows.x`）。一些新闻组之间的关系则不太密切，比如基督教组（`soc.religion.christian`）和棒球组（`rec.sport.baseball`）。
- en: Hence, it’s a perfect use case for unsupervised learning such as clustering,
    with which we can see whether similar topics are grouped together and unrelated
    ones are far apart. Moreover, we can even discover abstract topics beyond the
    original 20 labels using topic modeling techniques.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是无监督学习（如聚类）的一个完美用例，借此我们可以看到相似的主题是否被归类在一起，而不相关的主题是否被分开。此外，我们甚至可以使用主题建模技术发现超越原始20个标签的抽象主题。
- en: For now, let’s focus on exploring and analyzing the text data. We will get started
    with acquiring the data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们专注于探索和分析文本数据。我们将从获取数据开始。
- en: It is possible to download the dataset manually from the original website or
    many other online repositories. However, there are also many versions of the dataset—some
    are cleaned in a certain way and some are in raw form. To avoid confusion, it
    is best to use a consistent acquisition method. The scikit-learn library provides
    a utility function that loads the dataset. Once the dataset is downloaded, it’s
    automatically cached. We don’t need to download the same dataset twice.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以手动从原始网站或其他许多在线仓库下载数据集。然而，也有很多不同版本的数据集——有些已经以某种方式进行了清理，有些则是原始数据。为了避免混淆，最好使用一致的数据获取方法。scikit-learn库提供了一个实用函数来加载数据集。数据集一旦下载，将自动缓存。我们无需重复下载同一数据集。
- en: In most cases, caching the dataset, especially for a relatively small one, is
    considered a good practice. Other Python libraries also provide data download
    utilities, but not all of them implement automatic caching. This is another reason
    why we love scikit-learn.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，缓存数据集，特别是对于相对较小的数据集，是一种良好的实践。其他 Python 库也提供数据下载工具，但并非所有库都实现了自动缓存。这是我们喜爱
    scikit-learn 的另一个原因。
- en: 'As always, we first import the loader function for the 20 newsgroups data,
    as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先导入20个新闻组数据的加载器函数，如下所示：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we download the dataset with all the default parameters, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们下载包含所有默认参数的数据集，如下所示：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can also specify one or more certain topic groups and particular sections
    (training, testing, or both) and just load such a subset of data in the program.
    The full list of parameters and options for the loader function is summarized
    in the following table:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定一个或多个特定的主题组以及特定的部分（训练集、测试集或两者），并仅加载该数据的子集。加载器函数的所有参数和选项总结在下表中：
- en: '| **Parameter** | **Default value** | **Example values** | **Description**
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **默认值** | **示例值** | **描述** |'
- en: '| `subset` | `''train''` | `''train'',''test'',''all''` | The dataset to load:
    the training set, the testing set, or both. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `subset` | `''train''` | `''train'',''test'',''all''` | 要加载的数据集：训练集、测试集或两者。
    |'
- en: '| `data_home` | `~/scikit_learn_data` | `~/myfolder` | Directory where the
    files are stored and cached. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `data_home` | `~/scikit_learn_data` | `~/myfolder` | 存储和缓存文件的目录。 |'
- en: '| `categories` | `None` | `[''sci.space",alt.atheism'']` | List of newsgroups
    to load. If `None`, all newsgroups will be loaded. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `categories` | `None` | `[''sci.space'', ''alt.atheism'']` | 要加载的新闻组列表。如果是
    `None`，则加载所有新闻组。 |'
- en: '| `shuffle` | `True` | `True, False` | Boolean indicating whether to shuffle
    the data. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `shuffle` | `True` | `True, False` | 布尔值，指示是否洗牌数据。 |'
- en: '| `random_state` | `42` | `7, 43` | Random seed integer used to shuffle the
    data. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| `random_state` | `42` | `7, 43` | 用于洗牌数据的随机种子整数。 |'
- en: '| `remove` | `0` | `(''headers'',''footers'',''quotes'')` | Tuple indicating
    the part(s) among “header, footer, and quote” of each newsgroup post to omit.
    Nothing is removed by default. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| `remove` | `0` | `(''headers'',''footers'',''quotes'')` | 元组，指示要省略每个新闻组帖子中的“头部、尾部和引用”部分。默认情况下不删除任何内容。
    |'
- en: '| `download_if_missing` | `True` | `True, False` | Boolean indicating whether
    to download the data if it is not found locally. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| `download_if_missing` | `True` | `True, False` | 布尔值，指示如果在本地找不到数据是否下载数据。
    |'
- en: 'Table 7.2: List of parameters of the fetch_20newsgroups() function'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.2：fetch_20newsgroups() 函数的参数列表
- en: Remember that `random_state` is useful for the purpose of reproducibility. You
    are able to get the same dataset every time you run the script. Otherwise, working
    on datasets shuffled under different orders might bring in unnecessary variations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`random_state` 对于可重复性很有用。每次运行脚本时，你都能获得相同的数据集。否则，在不同的顺序下处理洗牌数据集可能会带来不必要的变化。
- en: In this section, we loaded the newsgroups data. Let’s explore it next.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们加载了新闻组数据。接下来，让我们来探索它。
- en: Exploring the newsgroups data
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索新闻组数据
- en: 'After we download the 20 newsgroups dataset by whatever means we prefer, the
    `data` object of `groups` is cached in memory. The `data` object is in the form
    of a key-value dictionary. Its keys are as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通过任何方式下载20个新闻组数据集后，`groups` 的 `data` 对象会被缓存到内存中。`data` 对象以键值字典的形式存在。它的键如下：
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `target_names` key gives the 20 newsgroups names:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`target_names` 键提供了20个新闻组的名称：'
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `target` key corresponds to a newsgroup, but is encoded as an integer:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`target` 键对应一个新闻组，但它被编码为一个整数：'
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, what are the distinct values for these integers? We can use the `unique`
    function from NumPy to figure it out:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些整数的不同值是什么呢？我们可以使用 NumPy 的 `unique` 函数来弄清楚：
- en: '[PRE31]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: They range from `0` to `19`, representing the 1st, 2nd, 3rd, …, and 20th newsgroup
    topics in `groups['target_names']`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的范围是从 `0` 到 `19`，表示 `groups['target_names']` 中的第 1、2、3、…、20 个新闻组主题。
- en: In the context of multiple topics or categories, it is important to know what
    the distribution of topics is. A balanced class distribution is the easiest to
    deal with because there are no under-represented or over-represented categories.
    However, frequently, we have a skewed distribution with one or more categories
    dominating.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个主题或类别的上下文中，了解主题的分布非常重要。平衡的类别分布最容易处理，因为没有类别被低估或高估。然而，通常我们会遇到偏斜的分布，一个或多个类别占主导地位。
- en: 'We will use the `seaborn` package ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    to compute the histogram of categories and plot it utilizing the `matplotlib`
    package ([https://matplotlib.org/](https://matplotlib.org/)). We can install both
    packages via `pip` as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`seaborn`包（[https://seaborn.pydata.org/](https://seaborn.pydata.org/)）来计算类别的直方图，并利用`matplotlib`包（[https://matplotlib.org/](https://matplotlib.org/)）绘制它。我们可以通过`pip`来安装这两个包，如下所示：
- en: '[PRE32]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the case of `conda`, you can execute the following command line:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`conda`，你可以执行以下命令行：
- en: '[PRE33]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Remember to install `matplotlib` before `seaborn` as `matplotlib` is one of
    the dependencies of the `seaborn` package.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在安装`seaborn`之前先安装`matplotlib`，因为`matplotlib`是`seaborn`包的一个依赖项。
- en: 'Now, let’s display the distribution of the classes, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们显示类别的分布，如下所示：
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Refer to the following screenshot for the result:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下截图以查看结果：
- en: '![A graph of a number  Description automatically generated](img/B21047_07_04.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![A graph of a number  Description automatically generated](img/B21047_07_04.png)'
- en: 'Figure 7.4: Distribution of newsgroup classes'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：新闻组类别分布
- en: As you can see, the distribution is approximately uniform so that’s one less
    thing to worry about.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，分布大致均匀，所以这是我们不必担心的一点。
- en: It’s good to visualize data to get a general idea of how the data is structured,
    what possible issues may arise, and whether there are any irregularities that
    we have to take care of.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据是很好的方法，可以帮助我们大致了解数据的结构，可能出现的问题，以及是否有任何我们需要处理的不规律现象。
- en: 'Other keys are quite self-explanatory: `data` contains all newsgroup documents
    and `filenames` stores the path where each document is located in your filesystem.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其他键非常直观：`data`包含所有新闻组文档，`filenames`存储每个文档在你的文件系统中的路径。
- en: 'Now, let’s have a look at the first document and its topic number and name
    by executing the following command:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过执行以下命令来查看第一篇文档及其主题编号和名称：
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If `random_state` isn’t fixed (`42` by default), you may get different results
    running the preceding scripts.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`random_state`没有固定（默认值为`42`），你可能会在运行前面的脚本时得到不同的结果。
- en: As you can see, the first document is from the `rec.autos` newsgroup, which
    was assigned the number `7`. Reading this post, we can easily figure out that
    it’s about cars. The word `car` actually occurs a number of times in the document.
    Words such as `bumper` also seem very car-oriented. However, words such as `doors`
    may not necessarily be car-related, as they may also be associated with home improvement
    or another topic.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，第一篇文档来自`rec.autos`新闻组，且被分配了编号`7`。阅读这篇文章，我们可以轻松判断它是关于汽车的。实际上，`car`这个词在文档中出现了好几次。诸如`bumper`之类的词也显得非常与汽车相关。然而，像`doors`这样的词不一定与汽车相关，因为它们也可能与家庭装修或其他话题相关。
- en: As a side note, it makes sense to not distinguish between `doors` and `door`,
    or the same word with different capitalization, such as `Doors`. There are some
    rare cases where capitalization does matter – for instance, if we’re trying to
    find out whether a document is about the band called `The Doors` or the more common
    concept, `the doors` (made of wood or another material).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，区分`doors`和`door`，或者同一个词的不同大小写（如`Doors`）并没有太大意义。只有在一些少见的情况下，大小写才是重要的—例如，如果我们试图了解一篇文档是否是关于名为`The
    Doors`的乐队，还是关于常见的`the doors`（木门或其他材料的门）这个概念。
- en: Thinking about features for text data
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考文本数据的特征
- en: From the preceding analysis, we can safely conclude that if we want to figure
    out whether a document was from the `rec.autos` newsgroup, the presence or absence
    of words such as `car`, `doors`, and `bumper` can be very useful features. The
    presence or not of a word is a Boolean variable, and we can also look at the count
    of certain words. For instance, `car` occurs multiple times in the document. Maybe
    the more times such a word is found in a text, the more likely it is that the
    document has something to do with cars.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的分析中，我们可以得出结论，如果我们想要判断一篇文档是否来自`rec.autos`新闻组，诸如`car`、`doors`和`bumper`等词的出现与否可以是非常有用的特征。一个词的出现与否是一个布尔变量，我们还可以查看某些词的计数。例如，`car`在文档中出现多次。也许一个词在文本中出现的次数越多，文档与汽车相关的可能性就越大。
- en: Counting the occurrence of each word token
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算每个词项的出现次数
- en: It seems that we are only interested in the occurrence of certain words, their
    count, or a related measure, and not in the order of the words. We can therefore
    view a text as a collection of words. This is called the **Bag of Words** (**BoW**)
    model. This is a very basic model but it works pretty well in practice. We can
    optionally define a more complex model that takes into account the order of words
    and PoS tags. However, such a model is going to be more computationally expensive
    and more difficult to program. In reality, the basic BoW model, in most cases,
    suffices. We can give it a shot and see whether the BoW model makes sense.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们只关心某些词语的出现次数，或者相关的度量，而不关心词语的顺序。因此，我们可以将文本视为一组词。这被称为**词袋模型**（**BoW**）。这是一个非常基础的模型，但在实践中效果相当好。我们还可以选择定义一个更复杂的模型，考虑词语的顺序和词性标记（PoS标签）。然而，这种模型在计算上会更加昂贵，并且编程上更为复杂。实际上，基础的BoW模型在大多数情况下已经足够了。我们可以试试看，看看BoW模型是否合理。
- en: 'We begin by converting documents into a matrix where each row represents each
    newsgroup document and each column represents a word token, or specifically, a
    unigram to begin with. The value of each element in the matrix is the number of
    times the word (column) occurs in the document (row). We are utilizing the `CountVectorizer`
    class from scikit-learn to do the work:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将文档转换为矩阵，每行表示每个新闻组文档，每列表示一个词元，或者具体来说，首先是单字词n-gram。矩阵中每个元素的值表示该词（列）在文档（行）中出现的次数。我们使用的是来自scikit-learn的`CountVectorizer`类来完成这项工作：
- en: '[PRE36]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The important parameters and options for the count conversion function are
    summarized in the following table:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 计数转换函数的重要参数和选项总结在下表中：
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| **构造函数参数** | **默认值** | **示例值** | **描述** |'
- en: '| `ngram_range` | `(1,1)` | `(1,2), (2,2)` | Lower and upper bound of the n-grams
    to be extracted in the input text, for example `(1,1)` means unigram, `(1,2)`
    means unigram and bigram. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `ngram_range` | `(1,1)` | `(1,2), (2,2)` | 输入文本中要提取的n-gram的上下限，例如`(1,1)`表示单词级n-gram，`(1,2)`表示单词级和双词级n-gram。
    |'
- en: '| `stop_words` | `None` | `''english'' or list [''a'',''the'', ''of''] or None`
    | Which stop word list to use: can be `''english''` referring to the built-in
    list, or a customized input list. If `None`, no words will be removed. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `stop_words` | `None` | `''english'' 或列表 [''a'',''the'', ''of''] 或 None`
    | 要使用的停用词列表：可以是 `''english''`，即内建列表，或者是自定义的输入列表。如果是`None`，则不删除任何词语。 |'
- en: '| `lowercase` | `True` | `True, False` | Whether or not to convert all characters
    to lowercase. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `lowercase` | `True` | `True, False` | 是否将所有字符转换为小写字母。 |'
- en: '| `max_features` | `None` | `None, 200, 500` | The number of top (most frequent)
    tokens to consider, or all tokens if `None`. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `max_features` | `None` | `None, 200, 500` | 要考虑的前（最常见的）词元数，如果是`None`，则考虑所有词元。
    |'
- en: '| `binary` | `False` | `True, False` | If true, all non-zero counts become
    1s. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `binary` | `False` | `True, False` | 如果为true，所有非零计数变为1。 |'
- en: 'Table 7.3: List of parameters of the CountVectorizer() function'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3：CountVectorizer()函数的参数列表
- en: 'We first initialize the count vectorizer with the `500` top features (500 most
    frequent tokens):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先用`500`个最常见的特征初始化计数向量化器（500个最频繁的词元）：
- en: '[PRE37]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Use it to fit on the raw text data as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，将其拟合到原始文本数据中：
- en: '[PRE38]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now the count vectorizer captures the top 500 features and generates a token
    count matrix out of the original text input:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，计数向量化器捕获前500个特征，并从原始文本输入中生成一个词元计数矩阵：
- en: '[PRE39]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The resulting count matrix is a sparse matrix where each row only stores non-zero
    elements (hence, only 798,221 elements instead of `11314 * 500 = 5,657,000`).
    For example, the first document is converted into a sparse vector composed of
    `53` non-zero elements.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 结果计数矩阵是一个稀疏矩阵，每行仅存储非零元素（因此，只有798,221个元素，而不是`11314 * 500 = 5,657,000`）。例如，第一篇文档被转换为由`53`个非零元素组成的稀疏向量。
- en: 'If you are interested in seeing the whole matrix, feel free to run the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣查看整个矩阵，随时可以运行以下代码：
- en: '[PRE40]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you just want the first row, run the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想查看第一行，可以运行以下代码：
- en: '[PRE41]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s take a look at the following output derived from the preceding command:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下从前面的命令得出的以下输出：
- en: '![A number pattern with numbers  Description automatically generated](img/B21047_07_05.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![A number pattern with numbers  Description automatically generated](img/B21047_07_05.png)'
- en: 'Figure 7.5: Output of count vectorization'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：计数向量化输出
- en: 'So, what are those 500 top features? They can be found in the following output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这500个最重要的特征是什么呢？它们可以在以下输出中找到：
- en: '[PRE42]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Our first trial doesn’t look perfect. Obviously, the most popular tokens are
    numbers, or letters with numbers such as `a86`, which do not convey important
    information. Moreover, there are many words that have no actual meaning, such
    as `you`, `the`, `them`, and `then`. Also, some words contain identical information,
    for example, `tell` and `told`, `use` and `used`, and `time` and `times`. Let’s
    tackle these issues.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一次尝试看起来并不完美。显然，最常见的标记是数字，或者像`a86`这样的字母和数字的组合，这些并不传递重要信息。此外，还有许多没有实际意义的词汇，如`you`、`the`、`them`和`then`。此外，一些词汇包含相同的信息，例如`tell`和`told`、`use`和`used`、`time`和`times`。让我们来解决这些问题。
- en: Text preprocessing
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本预处理
- en: 'We begin by retaining letter-only words so that numbers such as `00` and `000`
    and combinations of letters and numbers such as `b8f` will be removed. The filter
    function is defined as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先保留只有字母的单词，这样像`00`和`000`以及字母和数字的组合如`b8f`将被移除。过滤函数定义如下：
- en: '[PRE43]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This will generate a cleaned version of the newsgroups data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个清理过的新闻组数据版本。
- en: Dropping stop words
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去除停用词
- en: We didn’t talk about `stop_words` as an important parameter in `CountVectorizer`.
    **Stop words** are those common words that provide little value in helping to
    differentiate documents. In general, stop words add noise to the BoW model and
    can be removed.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有提到`stop_words`作为`CountVectorizer`中的一个重要参数。**停用词**是那些对区分文档帮助不大的常见词汇。通常，停用词会给词袋模型增加噪音，因此可以去除。
- en: 'There’s no universal list of stop words. Hence, depending on the tools or packages
    you are using, you will remove different sets of stop words. Take scikit-learn
    as an example—you can check the list that follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 没有通用的停用词列表。因此，根据你使用的工具或软件包，你会移除不同的停用词集合。以scikit-learn为例——你可以查看如下的列表：
- en: '[PRE44]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To drop stop words from the newsgroups data, we simply just need to specify
    the `stop_words` parameter:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 要从新闻组数据中去除停用词，我们只需指定`stop_words`参数：
- en: '[PRE45]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Besides stop words, you may notice that names are included in the top features,
    such as `andrew`. We can filter names with the `Names` corpus from NLTK we just
    worked with.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 除了停用词之外，你可能会注意到一些名字出现在特征中，比如`andrew`。我们可以使用刚刚处理过的NLTK中的`Names`语料库来过滤名字。
- en: Reducing inflectional and derivational forms of words
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少词汇的屈折形式和派生形式
- en: As mentioned earlier, we have two basic strategies to deal with words from the
    same root—stemming and lemmatization. Stemming is a quicker approach that involves,
    if necessary, chopping off letters; for example, *words* becomes *word* after
    stemming. The result of stemming doesn’t have to be a valid word. For instance,
    *trying* and *try* become *tri*. Lemmatizing, on the other hand, is slower but
    more accurate. It performs a dictionary lookup and guarantees to return a valid
    word. Recall that we implemented both stemming and lemmatization using NLTK previously.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们有两种基本策略来处理来自相同词根的单词——词干提取（stemming）和词形还原（lemmatization）。词干提取是一种更快捷的方法，必要时通过切割字母来实现；例如，*words*
    在词干提取后变成 *word*。词干提取的结果不一定是一个有效的单词。例如，*trying* 和 *try* 会变成 *tri*。而词形还原则较慢但更准确。它通过字典查找，保证返回一个有效的单词。回想一下，我们之前使用NLTK实现了词干提取和词形还原。
- en: 'Putting all of these (preprocessing, dropping stop words, lemmatizing, and
    count vectorizing) together, we obtain the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些（预处理、去除停用词、词形还原和计数向量化）结合起来，我们得到如下结果：
- en: '[PRE46]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now the features are much more meaningful:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些特征更加有意义：
- en: '[PRE47]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We have just converted text from each raw newsgroup document into a sparse vector
    of size `500`. For a vector from a document, each element represents the number
    of times a word token occurs in this document. Also, these 500 word tokens are
    selected based on their overall occurrences after text preprocessing, the removal
    of stop words, and lemmatization. Now, you may ask questions such as, “Is such
    an occurrence vector representative enough, or does such an occurrence vector
    convey enough information that can be used to differentiate the document from
    documents on other topics?” You will see the answer in the next section.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚将每个原始新闻组文档的文本转换为一个大小为`500`的稀疏向量。对于每个文档的向量，每个元素表示该单词标记在该文档中出现的次数。此外，这500个单词标记是基于文本预处理、去除停用词和词形还原后的整体出现次数来选择的。现在，你可能会问，“这样的出现向量是否足够具有代表性，或者这样的出现向量是否能传递足够的信息，帮助我们区分该文档与其他主题的文档？”你将在下一节中找到答案。
- en: Visualizing the newsgroups data with t-SNE
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用t-SNE可视化新闻组数据
- en: We can answer these questions easily by visualizing those representation vectors.
    If we can see that the document vectors from the same topic form a cluster, we
    did a good job mapping the documents into vectors. But how? They are of 500 dimensions,
    while we can visualize data of, **at most**, three dimensions. We can resort to
    t-SNE for dimensionality reduction.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过可视化那些表示向量来轻松回答这些问题。如果我们能看到来自同一主题的文档向量聚成一簇，那说明我们在将文档映射为向量时做得很好。那么，怎么做到的呢？它们有500维，而我们只能可视化最多三维的数据。我们可以借助t-SNE来进行降维。
- en: What is dimensionality reduction?
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是降维？
- en: '**Dimensionality reduction** is an important machine learning technique that
    reduces the number of features and, at the same time, retains as much information
    as possible. It is usually performed by obtaining a set of new principal features.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**是一种重要的机器学习技术，它减少特征的数量，同时尽可能保留更多信息。通常通过获得一组新的主特征来执行降维。'
- en: As mentioned before, it is difficult to visualize data of high dimensions. Given
    a three-dimensional plot, we sometimes don’t find it straightforward to observe
    any findings, not to mention 10, 100, or 1,000 dimensions. Moreover, some of the
    features in high-dimensional data may be correlated and, as a result, bring in
    redundancy. This is why we need dimensionality reduction.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，处理高维数据时，很难进行可视化。给定一个三维图，我们有时并不容易直接观察出任何发现，更别提10维、100维或1,000维了。此外，高维数据中的某些特征可能是相关的，从而带来冗余。这就是为什么我们需要降维的原因。
- en: Dimensionality reduction is not simply taking out a pair of two features from
    the original feature space. It is transforming the original feature space into
    a new space of fewer dimensions. The data transformation can be linear, such as
    the famous one, **Principal Component Analysis** (**PCA**), which maps the data
    in a higher dimensional space to a lower dimensional space where the variance
    of the data is maximized, which we will talk about in *Chapter 9*, *Recognizing
    Faces with Support Vector Machine*, or nonlinear, such as neural networks and
    t-SNE, which is coming up shortly. **Non-negative Matrix Factorization** (**NMF**)
    is another powerful algorithm, which we will study in detail in *Chapter 8*, *Discovering
    Underlying Topics in the Newsgroups Dataset with Clustering and Topic Modeling*.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 降维不仅仅是从原始特征空间中去除一对特征。它是将原始特征空间转化为一个维度较低的新空间。数据转换可以是线性的，比如著名的**主成分分析**（**PCA**），它将高维空间中的数据映射到一个低维空间，在这个空间中数据的方差被最大化，我们将在*第9章*《用支持向量机识别面孔》中讨论这一点；也可以是非线性的，比如神经网络和即将介绍的t-SNE。**非负矩阵分解**（**NMF**）是另一种强大的算法，我们将在*第8章*《用聚类和主题建模发现新闻组数据集中的潜在主题》中详细学习。
- en: At the end of the day, most dimensionality reduction algorithms are in the family
    of **unsupervised learning** as the target or label information (if available)
    is not used in data transformation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，大多数降维算法都属于**无监督学习**范畴，因为在数据转换中并没有使用目标或标签信息（如果有的话）。
- en: t-SNE for dimensionality reduction
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: t-SNE降维
- en: '**t-SNE** stands for **t-distributed Stochastic Neighbor Embedding**. It is
    a popular nonlinear dimensionality reduction technique developed by Laurens van
    der Maaten and Geoffrey Hinton ([https://www.cs.toronto.edu/~hinton/absps/tsne.pdf](https://www.cs.toronto.edu/~hinton/absps/tsne.pdf)).
    t-SNE has been widely used for data visualization in various domains, including
    computer vision, NLP, bioinformatics, and computational genomics.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-SNE**代表**t-分布随机邻域嵌入**。它是一种流行的非线性降维技术，由Laurens van der Maaten和Geoffrey Hinton开发（[https://www.cs.toronto.edu/~hinton/absps/tsne.pdf](https://www.cs.toronto.edu/~hinton/absps/tsne.pdf)）。t-SNE在各个领域的数据可视化中得到了广泛应用，包括计算机视觉、自然语言处理、生物信息学和计算基因组学。'
- en: As its name implies, t-SNE embeds high-dimensional data into a low-dimensional
    (usually two-dimensional or three-dimensional) space while preserving the local
    structure and pairwise similarities of the data as much as possible. It first
    models a probability distribution over neighbors around data points by assigning
    a high probability to similar data points and an extremely small probability to
    dissimilar ones. Note that similarity and neighbor distances are measured by Euclidean
    distance or other metrics. Then, t-SNE constructs a projection onto a low-dimensional
    space where the divergence between the input distribution and output distribution
    is minimized. The original high-dimensional space is modeled as a Gaussian distribution,
    while the output low-dimensional space is modeled as a t-distribution.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，t-SNE将高维数据嵌入低维（通常是二维或三维）空间，同时尽可能保持数据的局部结构和成对相似性。它首先通过为相似数据点分配较高的概率，为不相似的数据点分配极小的概率，来对数据点周围的邻居建模概率分布。注意，相似度和邻居距离是通过欧几里得距离或其他度量来衡量的。然后，t-SNE构建一个投影到低维空间，其中输入分布和输出分布之间的散度被最小化。原始的高维空间被建模为高斯分布，而输出的低维空间则被建模为t分布。
- en: 'We’ll herein implement t-SNE using the `TSNE` class from scikit-learn:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此使用来自scikit-learn的`TSNE`类实现t-SNE：
- en: '[PRE48]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now, let’s use t-SNE to verify our count vector representation.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用t-SNE来验证我们的计数向量表示。
- en: We pick three distinct topics, `talk.religion.misc`, `comp.graphics`, and `sci.space`,
    and visualize document vectors from these three topics.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了三个不同的主题，`talk.religion.misc`、`comp.graphics`和`sci.space`，并可视化这三个主题的文档向量。
- en: 'First, just load documents of these three labels, as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载这三个标签的文档，如下所示：
- en: '[PRE49]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We go through the same process and generate a count matrix, `data_cleaned_count_3`,
    with 500 features from the input, `groups_3`. You can refer to the steps in previous
    sections as you just need to repeat the same code.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过相同的过程，使用输入`groups_3`生成了一个包含500个特征的计数矩阵`data_cleaned_count_3`。你可以参考前面章节中的步骤，只需重复相同的代码。
- en: 'Next, we apply t-SNE to reduce the 500-dimensional matrix to a two-dimensional
    matrix:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应用t-SNE将500维矩阵降到二维矩阵：
- en: '[PRE50]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The parameters we specify in the `TSNE` object are as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`TSNE`对象中指定的参数如下：
- en: '`n_components`: The output dimension'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_components`：输出维度'
- en: '`perplexity`: The number of nearest data points considered neighbors in the
    algorithm with a typical value of between 5 and 50'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perplexity`：算法中考虑为邻居的最近数据点的数量，通常值在5到50之间'
- en: '`random_state`: The random seed for program reproducibility'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`：程序可重复性的随机种子'
- en: '`learning_rate`: The factor affecting the process of finding the optimal mapping
    space with a typical value of between 10 and 1,000'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：影响寻找最佳映射空间过程的因素，通常值在10到1,000之间'
- en: Note that the `TSNE` object only takes in a dense matrix, hence we convert the
    sparse matrix, `data_cleaned_count_3`, into a dense one using `toarray()`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`TSNE`对象只接受稠密矩阵，因此我们使用`toarray()`将稀疏矩阵`data_cleaned_count_3`转换为稠密矩阵。
- en: 'We just successfully reduced the input dimension from 500 to 2\. Finally, we
    can easily visualize it in a two-dimensional scatter plot where the *x* axis is
    the first dimension, the *y* axis is the second dimension, and the color, `c`,
    is based on the topic label of each original document:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚成功地将输入维度从500降到了2。最后，我们可以轻松地在二维散点图中可视化，其中 *x* 轴是第一维度，*y* 轴是第二维度，颜色 `c` 基于每个原始文档的主题标签：
- en: '[PRE51]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下截图以查看最终结果：
- en: '![A colorful dots on a white background  Description automatically generated](img/B21047_07_06.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![白色背景上的五彩斑斓的点  描述自动生成](img/B21047_07_06.png)'
- en: 'Figure 7.6: Applying t-SNE to data from three different topics'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：将t-SNE应用于来自三个不同主题的数据
- en: Data points from the three topics are in different colors – green, purple, and
    yellow. We can observe three clear clusters. Data points from the same topic are
    close to each other, while those from different topics are far away. Obviously,
    count vectors are great representations of original text data as they preserve
    the distinction between three different topics.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 来自三个主题的数据点呈现不同的颜色——绿色、紫色和黄色。我们可以观察到三个明显的聚类。同一主题的数据点彼此接近，而不同主题的数据点则相距较远。显然，计数向量是原始文本数据的良好表示，因为它们保持了三个不同主题之间的区分。
- en: You can also play around with the parameters and see whether you can obtain
    a nicer plot where the three clusters are better separated.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试调整参数，看看是否能获得一个更好的图形，其中三个聚类被更好地分开。
- en: 'Count vectorization does well in keeping document disparity. How about maintaining
    similarity? We can also check that using documents from overlapping topics, such
    as these five topics—`comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`,
    `comp.sys.mac.hardware`, and `comp.windows.x`:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 计数向量化在保持文档差异性方面表现良好。那么，如何保持相似性呢？我们也可以使用来自重叠主题的文档进行检验，譬如这五个主题——`comp.graphics`、`comp.os.ms-windows.misc`、`comp.sys.ibm.pc.hardware`、`comp.sys.mac.hardware`
    和 `comp.windows.x`：
- en: '[PRE52]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Similar processes (including text clean-up, count vectorization, and t-SNE)
    are repeated and the resulting plot is displayed as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 相似的过程（包括文本清理、计数向量化和t-SNE）会重复进行，最终得到的图形如下所示：
- en: '![A colorful dots on a white background  Description automatically generated](img/B21047_07_07.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![白色背景上的彩色点  描述自动生成](img/B21047_07_07.png)'
- en: 'Figure 7.7: Applying t-SNE to data from five similar topics'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：对来自五个相似主题的数据应用 t-SNE
- en: 'Data points from those five computer-related topics are all over the place,
    which means they are contextually similar. To conclude, count vectors are simple
    yet great representations of original text data as they are also good at preserving
    similarity among related topics. The question now arises: can we improve upon
    word (term) count representations? Let’s progress to the next section, where we
    will explore dense vector representations.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 来自这五个与计算机相关的主题的数据点分布杂乱无章，这意味着它们在语境上是相似的。总结来说，计数向量是原始文本数据的简单而又出色的表示方法，因为它们在保持相关主题之间的相似性方面表现优异。现在问题出现了：我们能否改进单词（术语）计数表示？接下来让我们进入下一部分，探索稠密向量表示。
- en: Representing words with dense vectors – word embedding
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用稠密向量表示单词——单词嵌入
- en: Word count representation results in a high-dimensional, sparse vector where
    each element represents the frequency of a specific word. Recall that we only
    looked at the `500` most frequent words previously to avoid this issue. Otherwise,
    we would have to represent each document with a vector of more than 1 million
    dimensions (depending on the size of the vocabulary). Also, word count representation
    lacks the ability to capture the semantics or context of words. It only considers
    the frequency of words in a document or corpus. On the contrary, **word embedding**
    represents words in a **dense** (**continuous**) vector space.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 单词计数表示会产生一个高维稀疏向量，其中每个元素代表特定单词的频率。回想一下，为了避免这个问题，我们之前只查看了`500`个最常见的单词。否则，我们必须用一个超过100万维的向量表示每个文档（取决于词汇表的大小）。此外，单词计数表示缺乏捕捉单词语义或上下文的能力。它只考虑单词在文档或语料库中的频率。相反，**单词嵌入**将单词表示在**稠密**（**连续**）的向量空间中。
- en: Building embedding models using shallow neural networks
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用浅层神经网络构建嵌入模型
- en: Word embedding maps each word to a dense vector of fixed dimensions. Its dimensionality
    is a lot lower than the size of the vocabulary and is usually several hundred
    only. For example, the word *machine* can be represented as a vector `[1.4, 2.1,
    10.3, 0.2, 6.81]`.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入将每个单词映射到一个固定维度的稠密向量。它的维度远低于词汇表的大小，通常只有几百维。例如，单词*machine*可以表示为向量`[1.4, 2.1,
    10.3, 0.2, 6.81]`。
- en: So, how can we embed a word into a vector? One solution is **word2vec** (see
    *Efficient Estimation of Word Representations in Vector Space*, by Tomas Mikolov,
    Kai Chen, Greg Corrado, and Jeff Dean, [https://arxiv.org/pdf/1301.3781](https://arxiv.org/pdf/1301.3781));
    this trains a **shallow neural network** to predict a word given the other words
    around it, which is called **Continuous Bag of Words** (**CBOW**), or to predict
    the other words around a word, which is called the **skip-gram** approach. The
    **weights** (**coefficients**) of the trained neural network are the embedding
    vectors for the corresponding words. Let’s look at a concrete example.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将一个单词嵌入到向量中呢？一种解决方案是**word2vec**（见 *Efficient Estimation of Word Representations
    in Vector Space*，Tomas Mikolov、Kai Chen、Greg Corrado 和 Jeff Dean， [https://arxiv.org/pdf/1301.3781](https://arxiv.org/pdf/1301.3781)）；该方法训练一个**浅层神经网络**，根据周围的其他单词预测一个单词，这叫做**连续词袋模型**（**CBOW**），或者根据一个单词预测它周围的其他单词，这叫做**跳字模型**（skip-gram）。训练好的神经网络的**权重**（**系数**）就是相应单词的嵌入向量。让我们来看一个具体的例子。
- en: 'Given the sentence *I love reading python machine learning by example* in a
    corpus and `5` as the size of the **word window**, we can have the following training
    sets for the CBOW neural network:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 给定语料库中的句子*我喜欢通过实例学习 Python 机器学习*，以及`5`作为**词窗口**的大小，我们可以得到以下CBOW神经网络的训练集：
- en: '| **Input of neural network** | **Output of neural network** |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| **神经网络输入** | **神经网络输出** |'
- en: '| `(I, love, python, machine)` | `(reading)` |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| `(I, love, python, machine)` | `(reading)` |'
- en: '| `(love, reading, machine, learning)` | `(python)` |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| `(love, reading, machine, learning)` | `(python)` |'
- en: '| `(reading, python, learning, by)` | `(machine)` |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| `(reading, python, learning, by)` | `(machine)` |'
- en: '| `(python, machine, by, example)` | `(learning)` |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| `(python, machine, by, example)` | `(learning)` |'
- en: 'Table 7.4: Input and output of the neural network for CBOW'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.4：CBOW神经网络的输入和输出
- en: During training, the inputs and outputs of the neural network are one-hot encoding
    vectors, where values are either 1 for present words or 0 for absent words. And
    we can have millions of training samples constructed from a corpus, sentence by
    sentence. After the network is trained, the weights that connect the input layer
    and hidden layer embed individual input words.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，神经网络的输入和输出是独热编码向量，值为1表示词语存在，值为0表示词语不存在。我们可以从语料库中逐句构建数百万个训练样本。在网络训练完成后，连接输入层和隐藏层的权重会嵌入每个输入词。
- en: 'A skip-gram-based neural network embeds words in a similar way. But its input
    and output are an inverse version of CBOW. Given the same sentence, *I love reading
    python machine learning by example*, and `5` as the size of the word window, we
    can have the following training sets for the skip-gram neural network:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 基于跳字模型的神经网络以类似的方式将词嵌入。但它的输入和输出是CBOW的反向版本。给定相同的句子，*我喜欢通过实例学习 Python 机器学习*，以及`5`作为词窗口的大小，我们可以得到以下跳字模型神经网络的训练集：
- en: '| **Input of neural network** | **Output of neural network** |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| **神经网络输入** | **神经网络输出** |'
- en: '| `(reading)` | `(i)` |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| `(reading)` | `(i)` |'
- en: '| `(reading)` | `(love)` |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| `(reading)` | `(love)` |'
- en: '| `(reading)` | `(python)` |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| `(reading)` | `(python)` |'
- en: '| `(reading)` | `(machine)` |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| `(reading)` | `(machine)` |'
- en: '| `(python)` | `(love)` |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| `(python)` | `(love)` |'
- en: '| `(python)` | `(reading)` |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| `(python)` | `(reading)` |'
- en: '| `(python)` | `(machine)` |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| `(python)` | `(machine)` |'
- en: '| `(python)` | `(learning)` |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| `(python)` | `(learning)` |'
- en: '| `(machine)` | `(reading)` |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| `(machine)` | `(reading)` |'
- en: '| `(machine)` | `(python)` |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| `(machine)` | `(python)` |'
- en: '| `(machine)` | `(learning)` |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| `(machine)` | `(learning)` |'
- en: '| `(machine)` | `(by)` |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| `(machine)` | `(by)` |'
- en: '| `(learning)` | `(python)` |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| `(learning)` | `(python)` |'
- en: '| `(learning)` | `(machine)` |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| `(learning)` | `(machine)` |'
- en: '| `(learning)` | `(by)` |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| `(learning)` | `(by)` |'
- en: '| `(learning)` | `(example)` |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| `(learning)` | `(example)` |'
- en: 'Table 7.5: Input and output of the neural network for skip-gram'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.5：跳字模型神经网络的输入和输出
- en: The embedding vectors are of real values, where each dimension encodes an aspect
    of meaning for the words in the vocabulary. This helps preserve the semantic information
    of the words, as opposed to discarding it, as in the dummy one-hot encoding approach
    using the word count approach. An interesting phenomenon is that vectors from
    semantically similar words are proximate to each other in geometric space. For
    example, both the words *clustering* and *grouping* refer to unsupervised clustering
    in the context of machine learning, hence their embedding vectors are close together.
    Word embedding is able to capture the meanings of words and their **context**.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量是实数值，每个维度编码词汇中单词的某个意义方面。这有助于保持单词的语义信息，而不是像使用词频方法的独热编码那样丢失它。一个有趣的现象是，语义相似的单词的向量在几何空间中相互接近。例如，*聚类*（clustering）和*分组*（grouping）都指机器学习中的无监督聚类，因此它们的嵌入向量非常接近。词嵌入能够捕捉单词及其**上下文**的意义。
- en: '**Best practice**'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Visualizing word embeddings can be a helpful tool to explore patterns, identify
    relationships between words, and assess the effectiveness of your embedding model.
    Here are some best practices for visualizing word embeddings:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化词嵌入是一个有用的工具，可以帮助探索模式、识别单词之间的关系，并评估嵌入模型的效果。以下是一些可视化词嵌入的最佳实践：
- en: '**Dimensionality reduction**: Word embeddings typically have high-dimensional
    vectors. To visualize them, reduce their dimensionality. We can use techniques
    like PCA or t-SNE to project the high-dimensional data into 2D or 3D space while
    preserving distances between data points.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：词嵌入通常具有高维向量。为了可视化它们，我们需要降低它们的维度。可以使用PCA或t-SNE等技术将高维数据投影到二维或三维空间，同时保持数据点之间的距离。'
- en: '**Clustering**: Cluster similar word embeddings together to identify groups
    of words with similar meanings or contexts.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：将相似的单词嵌入聚类在一起，以识别具有相似意义或上下文的单词群体。'
- en: Utilizing pre-trained embedding models
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用预训练的嵌入模型
- en: 'Training a word embedding neural network can be time-consuming and computationally
    expensive. Fortunately, many organizations and research institutions (such as
    Google, Meta AI Research, OpenAI, Stanford NLP Group, and Hugging Face) have developed
    pre-trained word embedding models based on different kinds of corpora and made
    them readily available for developers and researchers to use in various NLP tasks.
    We can simply use these **pre-trained** models to map words to vectors. Some popular
    pre-trained word embedding models are as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个单词嵌入神经网络可能既费时又消耗计算资源。幸运的是，许多组织和研究机构（如Google、Meta AI Research、OpenAI、斯坦福NLP小组和Hugging
    Face）已经基于不同种类的语料库开发了预训练的单词嵌入模型，并使其可以供开发者和研究人员在各种NLP任务中使用。我们可以直接使用这些**预训练**模型将单词映射到向量。以下是一些流行的预训练单词嵌入模型：
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_07_08.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_07_08.png)'
- en: 'Figure 7.8: Configurations of popular pre-trained word embedding models'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：流行的预训练单词嵌入模型配置
- en: Once we have embedding vectors for individual words, we can represent a document
    sample by averaging all of the vectors of words present in this document.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为单个单词获取了嵌入向量，就可以通过对文档中所有单词的向量进行平均来表示一个文档样本。
- en: '**Best practice**'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Once you have the word embeddings for all words in the document, aggregate
    them into a single vector representation for the entire document. Common aggregation
    techniques include averaging and summation. More sophisticated methods include
    the following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了文档中所有单词的嵌入向量，就将它们聚合成一个单一的向量表示整个文档。常见的聚合方法包括平均和求和。更复杂的方法包括以下几种：
- en: Weighted average, where the weights are based on word importance, such as TF-IDF
    score
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权平均，其中权重基于单词重要性，如TF-IDF分数
- en: Max/min pooling, where the maximum or minimum value for each dimension across
    all word embeddings is taken
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大/最小池化，即对所有单词嵌入的每个维度取最大值或最小值
- en: The resulting vectors of document samples are then consumed by downstream predictive
    tasks, such as classification, similarity ranking in search engines, and clustering.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文档样本向量将被下游预测任务所使用，如分类、搜索引擎中的相似性排名和聚类。
- en: Now let’s play around with `gensim`, a popular NLP package with powerful word
    embedding modules.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来玩一下`gensim`，一个流行的自然语言处理（NLP）包，拥有强大的单词嵌入模块。
- en: 'First, we import the package and load a pre-trained model, `glove-twitter-25`,
    as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入包并加载一个预训练模型，`glove-twitter-25`，如下所示：
- en: '[PRE53]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: You will see the process bar if you run this line of code. The `glove-twitter-25`
    model is one of the smallest ones so the download will not take very long.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这行代码，你会看到进度条。`glove-twitter-25`模型是最小的模型之一，所以下载不会花费太长时间。
- en: 'We can obtain the embedding vector for a word (`computer`, for example), as
    follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式获取一个单词（例如`computer`）的嵌入向量：
- en: '[PRE54]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The result is a 25-dimension float vector, as expected.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个25维的浮动向量，正如预期的那样。
- en: 'We can also get the top 10 words that are most contextually relevant to `computer`
    using the `most_similar` method, as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`most_similar`方法获取与`computer`最相关的前10个单词，如下所示：
- en: '[PRE55]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The result looks promising.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来很有前景。
- en: 'Finally, we demonstrate how to generate embedding vectors for a document with
    a simple example, as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过一个简单的示例展示如何生成文档的嵌入向量，如下所示：
- en: '[PRE56]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The resulting vector is the **average** of embedding vectors of eight input
    words.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的向量是八个输入单词嵌入向量的**平均值**。
- en: In traditional NLP applications, such as text classification and information
    retrieval tasks, where word frequency plays a significant role, word count representation
    is still an outstanding solution. In more complicated areas requiring understanding
    and semantic relationships between words, such as text summarization, machine
    translation, and question answering, word embedding is used extensively and extracts
    far better features than the traditional approach.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的NLP应用中，如文本分类和信息检索任务中，单词频率起着重要作用，单词计数表示仍然是一个优秀的解决方案。在更复杂的领域中，如文本摘要、机器翻译和问答系统，需要理解和单词间的语义关系，单词嵌入得到了广泛应用，并且比传统方法提取了更优的特征。
- en: Summary
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the fundamental concepts of NLP as an important
    subfield in machine learning, including tokenization, stemming and lemmatization,
    and PoS tagging. We also explored three powerful NLP packages and worked on some
    common tasks using NLTK and spaCy. Then we continued with the main project, exploring
    the 20 newsgroups data. We began by extracting features with tokenization techniques
    and went through text preprocessing, stop word removal, and lemmatization. We
    then performed dimensionality reduction and visualization with t-SNE and proved
    that count vectorization is a good representation of text data. We proceeded with
    a more modern representation technique, word embedding, and illustrated how to
    utilize a pre-trained embedding model.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了自然语言处理（NLP）作为机器学习一个重要子领域的基本概念，包括分词、词干提取与词形还原以及词性标注。我们还探索了三个强大的NLP包，并使用NLTK和spaCy完成了一些常见任务。随后，我们继续了主要项目，探索了20个新闻组数据。我们首先使用分词技术提取特征，经过文本预处理、停用词去除和词形还原。接着，我们进行了降维并用t-SNE进行可视化，证明了计数向量化是文本数据的良好表示方法。接下来，我们采用了更现代的表示技术——词嵌入，并展示了如何利用预训练的嵌入模型。
- en: We had some fun mining the 20 newsgroups data using dimensionality reduction
    as an unsupervised approach. Moving forward, in the next chapter, we’ll be continuing
    our unsupervised learning journey, specifically looking at topic modeling and
    clustering.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用降维作为无监督方法挖掘20个新闻组数据时玩得很开心。接下来，在下一章中，我们将继续我们的无监督学习之旅，专注于主题建模和聚类。
- en: Exercises
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Do you think all of the top 500 word tokens contain valuable information? If
    not, can you impose another list of stop words?
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你认为排名前500的单词标记都包含有价值的信息吗？如果不是，你能否添加另一份停用词表？
- en: Can you use stemming instead of lemmatization to process the 20 newsgroups data?
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能否使用词干提取（stemming）而不是词形还原（lemmatization）来处理20个新闻组数据？
- en: Can you increase `max_features` in `CountVectorizer` from `500` to `5000` and
    see how the t-SNE visualization will be affected?
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能将`CountVectorizer`中的`max_features`从`500`增加到`5000`，看看t-SNE可视化会受到怎样的影响吗？
- en: Experiment with representing the data for the three topics discussed in the
    chapter using the `word2vec-google-news-300` model in Gensim and visualize them
    with t-SNE. Assess whether the visualization appears more improved compared to
    the result shown in *Figure 7.6* using word count representation.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用Gensim中的`word2vec-google-news-300`模型表示本章讨论的三个主题的数据，并用t-SNE进行可视化。评估与使用词频表示法在*图7.6*中展示的结果相比，是否有所改善。
- en: Join our book’s Discord space
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1878468721786989681.png)'
