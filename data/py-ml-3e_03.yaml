- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: A Tour of Machine Learning Classifiers Using scikit-learn
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn的机器学习分类器之旅
- en: In this chapter, we will take a tour of a selection of popular and powerful
    machine learning algorithms that are commonly used in academia as well as in industry.
    While learning about the differences between several supervised learning algorithms
    for classification, we will also develop an appreciation of their individual strengths
    and weaknesses. In addition, we will take our first steps with the scikit-learn
    library, which offers a user-friendly and consistent interface for using those
    algorithms efficiently and productively.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一些常用且强大的机器学习算法，这些算法在学术界和工业界都有广泛应用。在学习几种用于分类的监督学习算法的区别时，我们还将深入了解它们各自的优缺点。此外，我们还将迈出使用scikit-learn库的第一步，scikit-learn提供了一个用户友好且一致的接口，用于高效和富有成效地使用这些算法。
- en: 'The topics that will be covered throughout this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖的主题如下：
- en: An introduction to robust and popular algorithms for classification, such as
    logistic regression, support vector machines, and decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍一些稳健且流行的分类算法，如逻辑回归、支持向量机和决策树
- en: Examples and explanations using the scikit-learn machine learning library, which
    provides a wide variety of machine learning algorithms via a user-friendly Python
    API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn机器学习库的示例和解释，scikit-learn提供了通过用户友好的Python API访问各种机器学习算法的功能
- en: Discussions about the strengths and weaknesses of classifiers with linear and
    nonlinear decision boundaries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论具有线性和非线性决策边界的分类器的优缺点
- en: Choosing a classification algorithm
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择分类算法
- en: 'Choosing an appropriate classification algorithm for a particular problem task
    requires practice and experience; each algorithm has its own quirks and is based
    on certain assumptions. To restate the **no free lunch theorem** by David H. Wolpert,
    no single classifier works best across all possible scenarios (*The Lack of A
    Priori Distinctions Between Learning Algorithms*, *Wolpert, David H*, *Neural
    Computation 8.7* (1996): 1341-1390). In practice, it is always recommended that
    you compare the performance of at least a handful of different learning algorithms
    to select the best model for the particular problem; these may differ in the number
    of features or examples, the amount of noise in a dataset, and whether the classes
    are linearly separable or not.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为特定问题选择合适的分类算法需要实践和经验；每种算法都有其独特之处，并且基于某些假设。重新表述大卫·沃尔珀特（David H. Wolpert）提出的**无免费午餐定理**，没有一种分类器在所有可能的情境中表现最佳（《学习算法之间缺乏先验区分》，*沃尔珀特，大卫·H*，*神经计算
    8.7*（1996年）：1341-1390）。实际上，建议始终比较至少几种不同学习算法的性能，以选择最适合特定问题的模型；这些算法可能在特征或样本数量、数据集中的噪声量以及类别是否线性可分等方面有所不同。
- en: 'Eventually, the performance of a classifier—computational performance as well
    as predictive power—depends heavily on the underlying data that is available for
    learning. The five main steps that are involved in training a supervised machine
    learning algorithm can be summarized as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，分类器的性能——包括计算性能和预测能力——在很大程度上依赖于可用于学习的基础数据。训练一个监督机器学习算法所涉及的五个主要步骤可以总结如下：
- en: Selecting features and collecting labeled training examples.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择特征并收集标注的训练样本。
- en: Choosing a performance metric.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择性能度量标准。
- en: Choosing a classifier and optimization algorithm.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择分类器和优化算法。
- en: Evaluating the performance of the model.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型的性能。
- en: Tuning the algorithm.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整算法。
- en: Since the approach of this book is to build machine learning knowledge step
    by step, we will mainly focus on the main concepts of the different algorithms
    in this chapter and revisit topics such as feature selection and preprocessing,
    performance metrics, and hyperparameter tuning for more detailed discussions later
    in the book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书的教学方法是一步步构建机器学习知识，我们将在本章中主要关注不同算法的基本概念，并将在本书后续章节中重新探讨特征选择与预处理、性能度量和超参数调整等主题，进行更为详细的讨论。
- en: First steps with scikit-learn – training a perceptron
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn的第一步——训练感知机
- en: In *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*,
    you learned about two related learning algorithms for classification, the **perceptron**
    rule and **Adaline**, which we implemented in Python and NumPy by ourselves. Now
    we will take a look at the scikit-learn API, which, as mentioned, combines a user-friendly
    and consistent interface with a highly optimized implementation of several classification
    algorithms. The scikit-learn library offers not only a large variety of learning
    algorithms, but also many convenient functions to preprocess data and to fine-tune
    and evaluate our models. We will discuss this in more detail, together with the
    underlying concepts, in *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*,
    and *Chapter 5*, *Compressing Data via Dimensionality Reduction*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第二章*，*训练简单的机器学习分类算法*中，你学习了两种相关的分类学习算法：**感知机**规则和**Adaline**，我们通过 Python 和
    NumPy 实现了这两种算法。现在我们将看一下 scikit-learn API，正如前面提到的，它结合了一个用户友好且一致的接口，以及几种分类算法的高度优化实现。scikit-learn
    库不仅提供了大量的学习算法，还提供了许多便捷的函数，用于预处理数据以及微调和评估我们的模型。我们将在*第四章*，*构建良好的训练数据集 – 数据预处理*和*第五章*，*通过降维压缩数据*中进一步讨论这些内容，并讲解其背后的基本概念。
- en: To get started with the scikit-learn library, we will train a perceptron model
    similar to the one that we implemented in *Chapter 2*. For simplicity, we will
    use the already familiar **Iris dataset** throughout the following sections. Conveniently,
    the Iris dataset is already available via scikit-learn, since it is a simple yet
    popular dataset that is frequently used for testing and experimenting with algorithms.
    Similar to the previous chapter, we will only use two features from the Iris dataset
    for visualization purposes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始使用 scikit-learn 库，我们将训练一个类似于*第二章*中实现的感知机模型。为简便起见，接下来的部分我们将继续使用已经熟悉的**鸢尾花数据集**。方便的是，鸢尾花数据集已经可以通过
    scikit-learn 获取，因为它是一个简单但广泛使用的数据集，经常用于算法测试和实验。与前一章类似，我们将在本章中仅使用鸢尾花数据集的两个特征来进行可视化展示。
- en: 'We will assign the petal length and petal width of the 150 flower examples
    to the feature matrix, `X`, and the corresponding class labels of the flower species
    to the vector array, `y`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把 150 个花卉样本的花瓣长度和花瓣宽度分配给特征矩阵 `X`，并将对应的花卉物种的类别标签分配给向量数组 `y`：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `np.unique(y)` function returned the three unique class labels stored in
    `iris.target`, and as we can see, the Iris flower class names, `Iris-setosa`,
    `Iris-versicolor`, and `Iris-virginica`, are already stored as integers (here:
    `0`, `1`, `2`). Although many scikit-learn functions and class methods also work
    with class labels in string format, using integer labels is a recommended approach
    to avoid technical glitches and improve computational performance due to a smaller
    memory footprint; furthermore, encoding class labels as integers is a common convention
    among most machine learning libraries.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.unique(y)` 函数返回了存储在 `iris.target` 中的三个唯一类别标签，正如我们所见，鸢尾花的类别名称 `Iris-setosa`、`Iris-versicolor`
    和 `Iris-virginica` 已经以整数形式存储（此处为：`0`、`1`、`2`）。尽管许多 scikit-learn 函数和类方法也可以处理字符串格式的类别标签，但使用整数标签是一种推荐的方法，能避免技术故障并提高计算性能，因为它占用更少的内存；此外，将类别标签编码为整数是大多数机器学习库的常见约定。'
- en: 'To evaluate how well a trained model performs on unseen data, we will further
    split the dataset into separate training and test datasets. In *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we will discuss
    the best practices around model evaluation in more detail:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估训练好的模型在未见过的数据上的表现，我们将进一步将数据集拆分为单独的训练集和测试集。在*第六章*，*模型评估与超参数调整的最佳实践*中，我们将更详细地讨论有关模型评估的最佳实践：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the `train_test_split` function from scikit-learn's `model_selection`
    module, we randomly split the `X` and `y` arrays into 30 percent test data (45
    examples) and 70 percent training data (105 examples).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `train_test_split` 函数来自 scikit-learn 的 `model_selection` 模块，我们将 `X` 和 `y`
    数组随机分割为 30% 的测试数据（45 个样本）和 70% 的训练数据（105 个样本）。
- en: Note that the `train_test_split` function already shuffles the training datasets
    internally before splitting; otherwise, all examples from class `0` and class
    `1` would have ended up in the training datasets, and the test dataset would consist
    of 45 examples from class `2`. Via the `random_state` parameter, we provided a
    fixed random seed (`random_state=1`) for the internal pseudo-random number generator
    that is used for shuffling the datasets prior to splitting. Using such a fixed
    `random_state` ensures that our results are reproducible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`train_test_split` 函数在拆分数据之前已经在内部对训练数据集进行了洗牌；否则，所有来自类别 `0` 和类别 `1` 的示例将会被分配到训练数据集中，而测试数据集将包含来自类别
    `2` 的 45 个示例。通过 `random_state` 参数，我们为内部用于数据集洗牌的伪随机数生成器提供了一个固定的随机种子（`random_state=1`）。使用这样的固定
    `random_state` 确保了我们的结果是可重复的。
- en: 'Lastly, we took advantage of the built-in support for stratification via `stratify=y`.
    In this context, stratification means that the `train_test_split` method returns
    training and test subsets that have the same proportions of class labels as the
    input dataset. We can use NumPy''s `bincount` function, which counts the number
    of occurrences of each value in an array, to verify that this is indeed the case:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们利用了 `stratify=y` 提供的内建分层支持。在此上下文中，分层意味着 `train_test_split` 方法返回的训练集和测试集子集与输入数据集具有相同的类别标签比例。我们可以使用
    NumPy 的 `bincount` 函数，它用于计算数组中每个值的出现次数，以验证这一点是否成立：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Many machine learning and optimization algorithms also require feature scaling
    for optimal performance, as we saw in the **gradient descent** example in *Chapter
    2*, *Training Simple Machine Learning Algorithms for Classification*. Here, we
    will standardize the features using the `StandardScaler` class from scikit-learn''s
    `preprocessing` module:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*第2章*，*训练简单机器学习算法进行分类*中的**梯度下降**示例所看到的，许多机器学习和优化算法也需要特征缩放才能获得最佳性能。在这里，我们将使用
    scikit-learn 的 `preprocessing` 模块中的 `StandardScaler` 类对特征进行标准化：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using the preceding code, we loaded the `StandardScaler` class from the `preprocessing`
    module and initialized a new `StandardScaler` object that we assigned to the `sc`
    variable. Using the `fit` method, `StandardScaler` estimated the parameters, ![](img/B13208_03_003.png)
    (sample mean) and ![](img/B13208_03_004.png) (standard deviation), for each feature
    dimension from the training data. By calling the `transform` method, we then standardized
    the training data using those estimated parameters, ![](img/B13208_03_003.png)
    and ![](img/B13208_03_004.png). Note that we used the same scaling parameters
    to standardize the test dataset so that both the values in the training and test
    dataset are comparable to each other.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们从 `preprocessing` 模块加载了 `StandardScaler` 类，并初始化了一个新的 `StandardScaler`
    对象，将其分配给 `sc` 变量。通过调用 `fit` 方法，`StandardScaler` 从训练数据中估算了每个特征维度的参数，![](img/B13208_03_003.png)（样本均值）和
    ![](img/B13208_03_004.png)（标准差）。接着，我们通过调用 `transform` 方法，利用这些估算的参数对训练数据进行了标准化。请注意，我们使用相同的缩放参数对测试数据集进行了标准化，这样训练数据集和测试数据集中的数值可以互相比较。
- en: 'Having standardized the training data, we can now train a perceptron model.
    Most algorithms in scikit-learn already support multiclass classification by default
    via the **one-vs.-rest** (**OvR**) method, which allows us to feed the three flower
    classes to the perceptron all at once. The code is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化了训练数据后，我们现在可以训练一个感知机模型。scikit-learn 中的大多数算法默认通过**一对多**（**OvR**）方法支持多类别分类，这使得我们可以一次性将三个花卉类别输入到感知机模型中。代码如下：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The scikit-learn interface will remind you of our perceptron implementation
    in *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*.
    After loading the `Perceptron` class from the `linear_model` module, we initialized
    a new `Perceptron` object and trained the model via the `fit` method. Here, the
    model parameter, `eta0`, is equivalent to the learning rate, `eta`, that we used
    in our own perceptron implementation, and the `n_iter` parameter defines the number
    of epochs (passes over the training dataset).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 接口让你想起我们在*第2章*，*训练简单机器学习算法进行分类*中实现的感知机。在从 `linear_model` 模块加载 `Perceptron`
    类之后，我们初始化了一个新的 `Perceptron` 对象，并通过 `fit` 方法训练了该模型。在这里，模型参数 `eta0` 相当于我们在自己实现的感知机中的学习率
    `eta`，而 `n_iter` 参数定义了迭代次数（对训练数据集的遍历次数）。
- en: As you will remember from *Chapter 2*, finding an appropriate learning rate
    requires some experimentation. If the learning rate is too large, the algorithm
    will overshoot the global cost minimum. If the learning rate is too small, the
    algorithm will require more epochs until convergence, which can make the learning
    slow—especially for large datasets. Also, we used the `random_state` parameter
    to ensure the reproducibility of the initial shuffling of the training dataset
    after each epoch.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*第2章*中所记得的，找到合适的学习率需要进行一些实验。如果学习率过大，算法会超过全局代价最小值。如果学习率太小，算法会需要更多的epochs直到收敛，这会使得学习变慢
    —— 特别是对于大数据集。此外，我们使用了`random_state`参数来确保每个epoch后对训练数据集进行的初始洗牌是可重现的。
- en: 'Having trained a model in scikit-learn, we can make predictions via the `predict`
    method, just like in our own perceptron implementation in *Chapter 2*. The code
    is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中训练完模型后，我们可以通过`predict`方法进行预测，就像在*第2章*中我们自己的感知器实现中一样。代码如下：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Executing the code, we can see that the perceptron misclassifies 1 out of the
    45 flower examples. Thus, the misclassification error on the test dataset is approximately
    0.022 or 2.2 percent (![](img/B13208_03_005.png)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们可以看到感知器在45个花的示例中误分类了1个。因此，测试数据集上的误分类错误大约为0.022或2.2% (![](img/B13208_03_005.png))。
- en: '**Classification error versus accuracy**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类错误与准确率**'
- en: 'Instead of the misclassification error, many machine learning practitioners
    report the classification accuracy of a model, which is simply calculated as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习实践者报告模型的分类准确率而不是误分类错误，这简单地计算如下：
- en: 1–*error* = 0.978 or 97.8 percent
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 1 - *错误* = 0.978 或 97.8%
- en: Whether we use the classification error or accuracy is merely a matter of preference.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类错误还是准确率仅仅是个人偏好的问题。
- en: 'Note that scikit-learn also implements a large variety of different performance
    metrics that are available via the `metrics` module. For example, we can calculate
    the classification accuracy of the perceptron on the test dataset as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，scikit-learn还实现了许多不同的性能度量，通过`metrics`模块可用。例如，我们可以计算感知器在测试数据集上的分类准确率如下：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, `y_test` are the true class labels and `y_pred` are the class labels
    that we predicted previously. Alternatively, each classifier in scikit-learn has
    a `score` method, which computes a classifier''s prediction accuracy by combining
    the `predict` call with `accuracy_score`, as shown here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`y_test`是真实的类标签，`y_pred`是我们之前预测的类标签。另外，scikit-learn中每个分类器都有一个`score`方法，通过将`predict`调用与`accuracy_score`结合起来计算分类器的预测准确率，如下所示：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Overfitting**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**'
- en: Note that we will evaluate the performance of our models based on the test dataset
    in this chapter. In *Chapter 6*, *Learning Best Practices for Model Evaluation
    and Hyperparameter Tuning*, you will learn about useful techniques, including
    graphical analysis, such as learning curves, to detect and prevent **overfitting**.
    Overfitting, which we will return to later in this chapter, means that the model
    captures the patterns in the training data well but fails to generalize well to
    unseen data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在本章中，我们将基于测试数据集评估模型的性能。在*第6章*，*学习模型评估和超参数调优的最佳实践*，你将学习到有用的技术，包括图形分析，如学习曲线，来检测和预防**过拟合**。过拟合，我们稍后将在本章返回讨论，意味着模型很好地捕捉了训练数据的模式，但对未见过的数据泛化能力不佳。
- en: 'Finally, we can use our `plot_decision_regions` function from *Chapter 2*,
    *Training Simple Machine Learning Algorithms for Classification*, to plot the
    **decision regions** of our newly trained perceptron model and visualize how well
    it separates the different flower examples. However, let''s add a small modification
    to highlight the data instances from the test dataset via small circles:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用我们从*第2章*，*简单机器学习算法的分类训练*中的`plot_decision_regions`函数来绘制我们新训练的感知器模型的**决策区域**，并可视化它如何将不同的花示例分离。然而，让我们通过小圆圈来突出显示来自测试数据集的数据实例：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the slight modification that we made to the `plot_decision_regions` function,
    we can now specify the indices of the examples that we want to mark on the resulting
    plots. The code is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍微修改了`plot_decision_regions`函数，现在我们可以指定要在生成的图中标记的示例的索引。代码如下：
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we can see in the resulting plot, the three flower classes cannot be perfectly
    separated by a linear decision boundary:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在生成的图中看到的，这三类花不能完美地通过线性决策边界分开：
- en: '![](img/B13208_03_01.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_01.png)'
- en: Remember from our discussion in *Chapter 2*, *Training Simple Machine Learning
    Algorithms for Classification*, that the perceptron algorithm never converges
    on datasets that aren't perfectly linearly separable, which is why the use of
    the perceptron algorithm is typically not recommended in practice. In the following
    sections, we will look at more powerful linear classifiers that converge to a
    cost minimum even if the classes are not perfectly linearly separable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 记住在*第二章*《训练简单的机器学习分类算法》中提到的内容，感知器算法在数据集不是完全线性可分时永远不会收敛，这也是为什么在实际应用中通常不推荐使用感知器算法的原因。在接下来的章节中，我们将探讨一些更强大的线性分类器，即使类不是完全线性可分，它们也能收敛到一个最小的代价。
- en: '**Additional perceptron settings**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**附加的感知器设置**'
- en: The `Perceptron`, as well as other scikit-learn functions and classes, often
    has additional parameters that we omit for clarity. You can read more about those
    parameters using the `help` function in Python (for instance, `help(Perceptron)`)
    or by going through the excellent scikit-learn online documentation at [http://scikit-learn.org/stable/](http://scikit-learn.org/stable/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`感知器`（Perceptron）以及其他 scikit-learn 函数和类，通常有一些额外的参数，为了简洁起见我们在这里省略了这些参数。你可以通过
    Python 中的 `help` 函数（例如，`help(Perceptron)`）或通过浏览出色的 scikit-learn 在线文档，了解更多关于这些参数的内容：[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)。'
- en: Modeling class probabilities via logistic regression
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过逻辑回归建模类概率
- en: Although the perceptron rule offers a nice and easy-going introduction to machine
    learning algorithms for classification, its biggest disadvantage is that it never
    converges if the classes are not perfectly linearly separable. The classification
    task in the previous section would be an example of such a scenario. The reason
    for this is that the weights are continuously being updated, since there is always
    at least one misclassified training example present in each epoch. Of course,
    you can change the learning rate and increase the number of epochs, but be warned
    that the perceptron will never converge on this dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器规则为机器学习分类算法提供了一个简单且易于理解的入门，但其最大缺点是，如果类不是完全线性可分的，它永远不会收敛。上一节中的分类任务就是一个这样的例子。其原因在于，权重会持续更新，因为每个周期内总会至少存在一个被错误分类的训练样本。当然，你可以调整学习率并增加周期数，但请注意，在此数据集上感知器永远不会收敛。
- en: 'To make better use of our time, we will now take a look at another simple,
    yet more powerful, algorithm for linear and binary classification problems: **logistic
    regression**. Note that, in spite of its name, logistic regression is a model
    for classification, not regression.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地利用时间，我们现在来看另一种简单但更强大的线性和二分类问题算法：**逻辑回归**。需要注意的是，尽管它的名字里有“回归”二字，逻辑回归实际上是一种分类模型，而非回归模型。
- en: Logistic regression and conditional probabilities
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归与条件概率
- en: Logistic regression is a classification model that is very easy to implement
    and performs very well on linearly separable classes. It is one of the most widely
    used algorithms for classification in industry. Similar to the perceptron and
    Adaline, the logistic regression model in this chapter is also a linear model
    for binary classification.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种分类模型，容易实现，并且在类是线性可分的情况下表现非常好。它是工业界中应用最广泛的分类算法之一。与感知器和 Adaline 类似，本章中的逻辑回归模型也是一种用于二分类的线性模型。
- en: '**Logistic regression for multiple classes**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**多类逻辑回归**'
- en: Note that logistic regression can be readily generalized to multiclass settings,
    which is known as multinomial logistic regression or softmax regression. A more
    detailed coverage of multinomial logistic regression is outside the scope of this
    book, but the interested reader can find more information in my lecture notes
    at [https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L05_gradient-descent_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L05_gradient-descent_slides.pdf)
    or [http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/](http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，逻辑回归可以很容易地推广到多类设置，这被称为多项式逻辑回归或软最大回归（softmax regression）。关于多项式逻辑回归的详细内容超出了本书的范围，但感兴趣的读者可以在我的讲义中找到更多信息：[https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L05_gradient-descent_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L05_gradient-descent_slides.pdf)
    或 [http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/](http://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/)。
- en: Another way to use logistic regression in multiclass settings is via the OvR
    technique, which we discussed previously.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类设置中使用逻辑回归的另一种方式是通过OvR技术，这也是我们之前讨论过的。
- en: 'To explain the idea behind logistic regression as a probabilistic model for
    binary classification, let''s first introduce the **odds**: the odds in favor
    of a particular event. The odds can be written as ![](img/B13208_03_006.png) where
    *p* stands for the probability of the positive event. The term "positive event"
    does not necessarily mean "good," but refers to the event that we want to predict,
    for example, the probability that a patient has a certain disease; we can think
    of the positive event as class label *y* = 1\. We can then further define the
    **logit** function, which is simply the logarithm of the odds (log-odds):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将逻辑回归作为二元分类的概率模型来解释，我们首先引入**赔率**：某个特定事件的赔率。赔率可以写成![](img/B13208_03_006.png)，其中*p*代表正事件的概率。这里的“正事件”不一定意味着“好”，而是指我们想要预测的事件，例如，某个患者患有某种疾病的概率；我们可以将正事件视为类别标签*y*
    = 1。接着，我们可以进一步定义**logit**函数，它仅仅是赔率（log-odds）的对数：
- en: '![](img/B13208_03_007.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_007.png)'
- en: 'Note that *log* refers to the natural logarithm, as it is the common convention
    in computer science. The *logit* function takes input values in the range 0 to
    1 and transforms them to values over the entire real-number range, which we can
    use to express a linear relationship between feature values and the log-odds:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*log*指的是自然对数，因为在计算机科学中这是常见的约定。*logit*函数接受0到1范围内的输入值，并将其转换为整个实数范围内的值，我们可以用它来表达特征值与log-odds之间的线性关系：
- en: '![](img/B13208_03_008.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_008.png)'
- en: Here, ![](img/B13208_03_009.png) is the conditional probability that a particular
    example belongs to class 1 given its features, *x*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_03_009.png)是给定特征*x*的条件概率，即某个特定示例属于类别1的概率。
- en: Now, we are actually interested in predicting the probability that a certain
    example belongs to a particular class, which is the inverse form of the logit
    function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实际上关心的是预测某个示例属于某个特定类别的概率，这就是logit函数的逆函数形式。
- en: 'It is also called the **logistic sigmoid function**, which is sometimes simply
    abbreviated to **sigmoid function** due to its characteristic S-shape:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它也被称为**逻辑sigmoid函数**，有时由于其S形特征，简写为**sigmoid函数**：
- en: '![](img/B13208_03_010.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_010.png)'
- en: 'Here, *z* is the net input, the linear combination of weights, and the inputs
    (that is, the features associated with the training examples):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*z*是净输入，即权重和输入的线性组合（即与训练示例相关的特征）：
- en: '![](img/B13208_03_011.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_011.png)'
- en: Note that similar to the convention we used in *Chapter 2*, *Training Simple
    Machine Learning Algorithms for Classification*, ![](img/B13208_03_012.png) refers
    to the bias unit and is an additional input value that we provide to ![](img/B13208_03_013.png),
    which is set equal to 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，类似于我们在*第2章*中使用的约定，*训练简单机器学习算法进行分类*，![](img/B13208_03_012.png)表示偏置单元，是我们提供给![](img/B13208_03_013.png)的附加输入值，且其值设置为1。
- en: 'Now, let''s simply plot the sigmoid function for some values in the range –7
    to 7 to see how it looks:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制sigmoid函数在-7到7范围内的一些值，看看它的形态：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As a result of executing the previous code example, we should now see the S-shaped
    (sigmoidal) curve:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码示例后，我们现在应该能看到S形（sigmoidal）曲线：
- en: '![](img/B13208_03_02.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_02.png)'
- en: We can see that ![](img/B13208_03_014.png) approaches 1 if *z* goes toward infinity
    (![](img/B13208_03_015.png)) since ![](img/B13208_03_016.png) becomes very small
    for large values of *z*. Similarly, ![](img/B13208_03_017.png) goes toward 0 for
    ![](img/B13208_03_018.png) as a result of an increasingly large denominator. Thus,
    we can conclude that this sigmoid function takes real-number values as input and
    transforms them into values in the range [0, 1] with an intercept at ![](img/B13208_03_019.png).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果*z*趋向于无穷大（![](img/B13208_03_015.png)），则![](img/B13208_03_014.png)趋近于1，因为当*z*的值很大时，![](img/B13208_03_016.png)变得非常小。类似地，![](img/B13208_03_017.png)会趋向于0，原因是分母越来越大。由此，我们可以得出结论，这个sigmoid函数接收实数值作为输入，并将其转换为[0,
    1]范围内的值，截距为![](img/B13208_03_019.png)。
- en: To build some understanding of the logistic regression model, we can relate
    it to *Chapter 2*. In Adaline, we used the identity function, ![](img/B13208_03_020.png),
    as the activation function. In logistic regression, this activation function simply
    becomes the sigmoid function that we defined earlier.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解逻辑回归模型，我们可以将其与 *第 2 章* 关联。在 Adaline 中，我们使用恒等函数 ![](img/B13208_03_020.png)
    作为激活函数。在逻辑回归中，这个激活函数则变成了我们之前定义的 Sigmoid 函数。
- en: 'The difference between Adaline and logistic regression is illustrated in the
    following figure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline 和逻辑回归之间的差异如下面的图示所示：
- en: '![](img/B13208_03_03.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_03.png)'
- en: 'The output of the sigmoid function is then interpreted as the probability of
    a particular example belonging to class 1, ![](img/B13208_03_021.png), given its
    features, *x*, parameterized by the weights, *w*. For example, if we compute ![](img/B13208_03_022.png)
    for a particular flower example, it means that the chance that this example is
    an `Iris-versicolor` flower is 80 percent. Therefore, the probability that this
    flower is an `Iris-setosa` flower can be calculated as ![](img/B13208_03_023.png)
    or 20 percent. The predicted probability can then simply be converted into a binary
    outcome via a threshold function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数的输出被解释为给定特征 *x* 和由权重 *w* 参数化的情况下，一个特定样本属于类别 1 的概率，如 ![](img/B13208_03_021.png)。例如，如果我们为某个特定的花朵样本计算
    ![](img/B13208_03_022.png)，这意味着该样本是 `Iris-versicolor` 花朵的概率是 80%。因此，这朵花是 `Iris-setosa`
    花朵的概率可以通过 ![](img/B13208_03_023.png) 计算，结果是 20%。接下来，预测的概率可以通过阈值函数简化为二元结果：
- en: '![](img/B13208_03_024.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_024.png)'
- en: 'If we look at the preceding plot of the sigmoid function, this is equivalent
    to the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看之前的 Sigmoid 函数图像，这就相当于以下内容：
- en: '![](img/B13208_03_025.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_025.png)'
- en: In fact, there are many applications where we are not only interested in the
    predicted class labels, but where the estimation of the class-membership probability
    is particularly useful (the output of the sigmoid function prior to applying the
    threshold function). Logistic regression is used in weather forecasting, for example,
    not only to predict whether it will rain on a particular day but also to report
    the chance of rain. Similarly, logistic regression can be used to predict the
    chance that a patient has a particular disease given certain symptoms, which is
    why logistic regression enjoys great popularity in the field of medicine.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在许多应用中，我们不仅仅关心预测的类别标签，还特别关心类别成员概率的估算（即在应用阈值函数之前的 Sigmoid 函数输出）。例如，逻辑回归被广泛应用于天气预报，不仅用来预测某天是否会下雨，还用来报告降雨的概率。类似地，逻辑回归还可以用来预测给定某些症状时患者患有特定疾病的几率，这也是逻辑回归在医学领域广受欢迎的原因。
- en: Learning the weights of the logistic cost function
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习逻辑回归代价函数的权重
- en: 'You have learned how we can use the logistic regression model to predict probabilities
    and class labels; now, let''s briefly talk about how we fit the parameters of
    the model, for instance the weights, *w*. In the previous chapter, we defined
    the sum-squared-error cost function as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了如何使用逻辑回归模型来预测概率和类别标签；现在，让我们简要讨论如何拟合模型的参数，例如权重 *w*。在上一章中，我们将平方和误差代价函数定义为如下形式：
- en: '![](img/B13208_03_026.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_026.png)'
- en: 'We minimized this function in order to learn the weights, *w*, for our Adaline
    classification model. To explain how we can derive the cost function for logistic
    regression, let''s first define the likelihood, *L*, that we want to maximize
    when we build a logistic regression model, assuming that the individual examples
    in our dataset are independent of one another. The formula is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最小化这个函数来学习 Adaline 分类模型的权重 *w*。为了说明我们如何推导逻辑回归的代价函数，首先让我们定义我们希望最大化的似然 *L*，假设数据集中的每个样本是相互独立的。公式如下：
- en: '![](img/B13208_03_027.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_027.png)'
- en: 'In practice, it is easier to maximize the (natural) log of this equation, which
    is called the **log-likelihood** function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，最大化该方程的（自然）对数会更容易，这个过程被称为 **对数似然** 函数：
- en: '![](img/B13208_03_028.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_028.png)'
- en: Firstly, applying the log function reduces the potential for numerical underflow,
    which can occur if the likelihoods are very small. Secondly, we can convert the
    product of factors into a summation of factors, which makes it easier to obtain
    the derivative of this function via the addition trick, as you may remember from
    calculus.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，应用对数函数减少了数值下溢的可能性，若似然值非常小时可能会发生这种情况。其次，我们可以将因子的乘积转换为因子的求和，这使得通过加法技巧（你可能还记得微积分中的这一技巧）更容易得到该函数的导数。
- en: 'Now, we could use an optimization algorithm such as gradient ascent to maximize
    this log-likelihood function. Alternatively, let''s rewrite the log-likelihood
    as a cost function, *J*, that can be minimized using gradient descent as in *Chapter
    2*, *Training Simple Machine Learning Algorithms for Classification*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用一种优化算法，例如梯度上升法，来最大化这个对数似然函数。或者，我们可以将对数似然函数重写为一个成本函数 *J*，并通过梯度下降法最小化，正如在*第2章*《训练简单的机器学习分类算法》中所示：
- en: '![](img/B13208_03_029.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_029.png)'
- en: 'To get a better grasp of this cost function, let''s take a look at the cost
    that we calculate for one single training example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个成本函数，让我们看一下为单个训练样本计算的成本：
- en: '![](img/B13208_03_030.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_030.png)'
- en: 'Looking at the equation, we can see that the first term becomes zero if *y*
    = 0, and the second term becomes zero if *y* = 1:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 看这个方程，我们可以看到，当 *y* = 0 时，第一个项为零；当 *y* = 1 时，第二个项为零：
- en: '![](img/B13208_03_031.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_031.png)'
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The resulting plot shows the sigmoid activation on the *x*-axis in the range
    0 to 1 (the inputs to the sigmoid function were *z* values in the range –10 to
    10) and the associated logistic cost on the *y*-axis:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示了在 *x* 轴上从 0 到 1 范围内的 sigmoid 激活（sigmoid 函数的输入值是 *z*，范围为 -10 到 10），以及在
    *y* 轴上相应的逻辑回归成本：
- en: '![](img/B13208_03_04.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_04.png)'
- en: We can see that the cost approaches 0 (continuous line) if we correctly predict
    that an example belongs to class 1\. Similarly, we can see on the *y*-axis that
    the cost also approaches 0 if we correctly predict *y* = 0 (dashed line). However,
    if the prediction is wrong, the cost goes toward infinity. The main point is that
    we penalize wrong predictions with an increasingly larger cost.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果我们正确预测一个示例属于类别 1，成本会接近 0（连续线）。同样，我们也可以在 *y* 轴上看到，如果我们正确预测 *y* = 0，成本也会接近
    0（虚线）。然而，如果预测错误，成本将趋向无穷大。关键点是，我们会用越来越大的成本惩罚错误的预测。
- en: Converting an Adaline implementation into an algorithm for logistic regression
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Adaline 实现转换为逻辑回归算法
- en: 'If we were to implement logistic regression ourselves, we could simply substitute
    the cost function, *J*, in our Adaline implementation from *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*, with the new cost function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们自己实现逻辑回归，可以简单地将成本函数 *J* 从*第2章*《训练简单的机器学习分类算法》中的 Adaline 实现中替换为新的成本函数：
- en: '![](img/B13208_03_033.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_033.png)'
- en: 'We use this to compute the cost of classifying all training examples per epoch.
    Also, we need to swap the linear activation function with the sigmoid activation
    and change the threshold function to return class labels 0 and 1, instead of –1
    and 1\. If we make those changes to the Adaline code, we will end up with a working
    logistic regression implementation, as shown here:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用它来计算每个训练样本在每个周期的分类成本。此外，我们需要将线性激活函数替换为 sigmoid 激活，并将阈值函数更改为返回类别标签 0 和 1，而不是
    -1 和 1。如果我们对 Adaline 代码进行这些更改，我们将得到一个可工作的逻辑回归实现，如下所示：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When we fit a logistic regression model, we have to keep in mind that it only
    works for binary classification tasks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拟合逻辑回归模型时，必须记住它仅适用于二分类任务。
- en: 'So, let''s consider only `Iris-setosa` and `Iris-versicolor` flowers (classes
    `0` and `1`) and check that our implementation of logistic regression works:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们只考虑 `Iris-setosa` 和 `Iris-versicolor` 花（类别 `0` 和 `1`），并检查我们实现的逻辑回归是否有效：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting decision region plot looks as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的决策区域图如下所示：
- en: '![](img/B13208_03_05.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_05.png)'
- en: '**The gradient descent learning algorithm for logistic regression**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归的梯度下降学习算法**'
- en: Using calculus, we can show that the weight update in logistic regression via
    gradient descent is equal to the equation that we used in Adaline in *Chapter
    2*, *Training Simple Machine Learning Algorithms for Classification*. However,
    please note that the following derivation of the gradient descent learning rule
    is intended for readers who are interested in the mathematical concepts behind
    the gradient descent learning rule for logistic regression. It is not essential
    for following the rest of this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过微积分，我们可以证明，通过梯度下降在逻辑回归中进行的权重更新等同于我们在*第2章*《*训练简单的机器学习算法进行分类*》中使用的Adaline方程。然而，请注意，下面的梯度下降学习规则的推导是为那些对逻辑回归梯度下降学习规则背后的数学概念感兴趣的读者准备的。对于本章余下内容的理解，并非必须。
- en: 'Let''s start by calculating the partial derivative of the log-likelihood function
    with respect to the *j*th weight:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从计算对数似然函数相对于*j*权重的偏导数开始：
- en: '![](img/B13208_03_034.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_034.png)'
- en: 'Before we continue, let''s also calculate the partial derivative of the sigmoid
    function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，先来计算一下sigmoid函数的偏导数：
- en: '![](img/B13208_03_035.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_035.png)'
- en: 'Now, we can resubstitute ![](img/B13208_03_036.png) in our first equation to
    obtain the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在我们的第一个方程中重新代入![](img/B13208_03_036.png)，得到如下结果：
- en: '![](img/B13208_03_037.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_037.png)'
- en: 'Remember that the goal is to find the weights that maximize the log-likelihood
    so that we perform the update for each weight as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，目标是找到使对数似然最大化的权重，从而对每个权重执行如下更新：
- en: '![](img/B13208_03_038.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_038.png)'
- en: 'Since we update all weights simultaneously, we can write the general update
    rule as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们同时更新所有权重，因此我们可以将一般的更新规则写成如下形式：
- en: '![](img/B13208_03_039.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_039.png)'
- en: 'We define ![](img/B13208_03_040.png) as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将![](img/B13208_03_040.png)定义如下：
- en: '![](img/B13208_03_041.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_041.png)'
- en: 'Since maximizing the log-likelihood is equal to minimizing the cost function,
    *J*, that we defined earlier, we can write the gradient descent update rule as
    follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最大化对数似然等同于最小化之前定义的代价函数*J*，我们可以将梯度下降更新规则写成如下形式：
- en: '![](img/B13208_03_042.png)![](img/B13208_03_043.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_042.png)![](img/B13208_03_043.png)'
- en: This is equal to the gradient descent rule for Adaline in *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这与*第2章*《*训练简单的机器学习算法进行分类*》中Adaline的梯度下降规则相等。
- en: Training a logistic regression model with scikit-learn
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练一个逻辑回归模型
- en: We just went through useful coding and math exercises in the previous subsection,
    which helped to illustrate the conceptual differences between Adaline and logistic
    regression. Now, let's learn how to use scikit-learn's more optimized implementation
    of logistic regression, which also supports multiclass settings off the shelf.
    Note that in recent versions of scikit-learn, the technique used for multiclass
    classification, multinomial, or OvR, is chosen automatically. In the following
    code example, we will use the `sklearn.linear_model.LogisticRegression` class
    as well as the familiar `fit` method to train the model on all three classes in
    the standardized flower training dataset. Also, we set `multi_class='ovr'` for
    illustration purposes. As an exercise for the reader, you may want to compare
    the results with `multi_class='multinomial'`. Note that the `multinomial` setting
    is usually recommended in practice for mutually exclusive classes, such as those
    found in the Iris dataset. Here, "mutually exclusive" means that each training
    example can only belong to a single class (in contrast to multilabel classification,
    where a training example can be a member of multiple classes).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一小节中进行了一些有用的编码和数学练习，帮助说明了Adaline与逻辑回归的概念性差异。现在，让我们学习如何使用scikit-learn中更优化的逻辑回归实现，该实现也原生支持多类设置。需要注意的是，在最近版本的scikit-learn中，用于多类分类的技术（多项式或OvR）会自动选择。在接下来的代码示例中，我们将使用`sklearn.linear_model.LogisticRegression`类和熟悉的`fit`方法，使用标准化的花卉训练数据集训练模型，同时设置`multi_class='ovr'`作为示例。作为读者的练习，您可能会想比较一下使用`multi_class='multinomial'`的结果。请注意，`multinomial`设置通常在实际应用中推荐用于互斥类，比如鸢尾花数据集中的类。这里，"互斥"意味着每个训练样本只能属于一个类（与多标签分类不同，后者一个训练样本可以属于多个类）。
- en: 'Now, let''s have a look at the code example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看代码示例：
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After fitting the model on the training data, we plotted the decision regions,
    training examples, and test examples, as shown in the following plot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上拟合模型后，我们绘制了决策区域、训练样本和测试样本，如下图所示：
- en: '![](img/B13208_03_06.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_06.png)'
- en: Note that there exist many different optimization algorithms for solving optimization
    problems. For minimizing convex loss functions, such as the logistic regression
    loss, it is recommended to use more advanced approaches than regular stochastic
    gradient descent (SGD). In fact, scikit-learn implements a whole range of such
    optimization algorithms, which can be specified via the solver parameter, namely,
    `'newton-cg'`, `'lbfgs'`, `'liblinear'`, `'sag'`, and `'saga'`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，解决优化问题的方法有很多种。对于最小化凸损失函数（如逻辑回归损失），建议使用比常规随机梯度下降（SGD）更先进的方法。事实上，scikit-learn实现了一整套此类优化算法，可以通过solver参数指定，分别是`'newton-cg'`、`'lbfgs'`、`'liblinear'`、`'sag'`和`'saga'`。
- en: While the logistic regression loss is convex, most optimization algorithms should
    converge to the global loss minimum with ease. However, there are certain advantages
    of using one algorithm over the other. For instance, in the current version (v
    0.21), scikit-learn uses `'liblinear'` as a default, which cannot handle the multinomial
    loss and is limited to the OvR scheme for multi-class classification. However,
    in future versions of scikit-learn (that is, v 0.22), the default solver will
    be changed to `'lbfgs'`, which stands for the **limited-memory Broyden–Fletcher–Goldfarb–Shanno**
    (**BFGS**) algorithm ([https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS))
    and is more flexible in this regard. To adopt this new default choice, we will
    specify `solver='lbfgs'` explicitly when using logistic regression throughout
    this book.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然逻辑回归损失是凸的，但大多数优化算法都应该轻松收敛到全局最小损失。然而，某些算法相比其他算法有一定的优势。例如，在当前版本（v 0.21）中，scikit-learn默认使用`'liblinear'`，它无法处理多项式损失，并且在多类分类中仅限于OvR方案。然而，在scikit-learn的未来版本（即v
    0.22）中，默认的求解器将更改为`'lbfgs'`，即**有限内存Broyden-Fletcher-Goldfarb-Shanno**（**BFGS**）算法（[https://en.wikipedia.org/wiki/Limited-memory_BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)），并且在这方面更加灵活。为了采用这一新的默认选择，在本书中使用逻辑回归时我们将明确指定`solver='lbfgs'`。
- en: Looking at the preceding code that we used to train the `LogisticRegression`
    model, you might now be wondering, "What is this mysterious parameter C?" We will
    discuss this parameter in the next subsection, where we will introduce the concepts
    of overfitting and regularization. However, before we move on to those topics,
    let's finish our discussion of class-membership probabilities.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下我们用于训练`LogisticRegression`模型的前面的代码，你可能会想，“这个神秘的参数C到底是什么？”我们将在下一小节中讨论这个参数，并介绍过拟合和正则化的概念。不过，在我们继续讨论这些主题之前，让我们先完成关于类别归属概率的讨论。
- en: 'The probability that training examples belong to a certain class can be computed
    using the `predict_proba` method. For example, we can predict the probabilities
    of the first three examples in the test dataset as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`predict_proba`方法计算训练样本属于某一类的概率。例如，我们可以预测测试数据集中前三个样本的概率，如下所示：
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This code snippet returns the following array:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码返回如下数组：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first row corresponds to the class-membership probabilities of the first
    flower, the second row corresponds to the class-membership probabilities of the
    second flower, and so forth. Notice that the columns all sum up to one, as expected.
    (You can confirm this by executing `lr.predict_proba(X_test_std[:3, :]).sum(axis=1)`.)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行对应于第一个花朵的类别归属概率，第二行对应于第二个花朵的类别归属概率，以此类推。请注意，各列的和都为1，正如预期的那样。（你可以通过执行`lr.predict_proba(X_test_std[:3,
    :]).sum(axis=1)`来确认这一点。）
- en: 'The highest value in the first row is approximately 0.85, which means that
    the first example belongs to class three (`Iris-virginica`) with a predicted probability
    of 85 percent. So, as you may have already noticed, we can get the predicted class
    labels by identifying the largest column in each row, for example, using NumPy''s
    `argmax` function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行中的最大值约为0.85，这意味着第一个样本属于类别三（`Iris-virginica`）的预测概率为85%。所以，正如你可能已经注意到的那样，我们可以通过识别每一行中最大的一列来获得预测的类别标签，例如，使用NumPy的`argmax`函数：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The returned class indices are shown here (they correspond to `Iris-virginica`,
    `Iris-setosa`, and `Iris-setosa`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的类别索引如下所示（它们对应于`Iris-virginica`、`Iris-setosa`和`Iris-setosa`）：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code example, we computed the conditional probabilities and
    converted these into class labels manually by using NumPy''s `argmax` function.
    In practice, the more convenient way of obtaining class labels when using scikit-learn
    is to call the `predict` method directly:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们计算了条件概率并通过使用NumPy的`argmax`函数手动将其转换为类别标签。在实际操作中，使用scikit-learn时，更便捷的获取类别标签的方式是直接调用`predict`方法：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Lastly, a word of caution if you want to predict the class label of a single
    flower example: scikit-learn expects a two-dimensional array as data input; thus,
    we have to convert a single row slice into such a format first. One way to convert
    a single row entry into a two-dimensional data array is to use NumPy''s `reshape`
    method to add a new dimension, as demonstrated here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你想预测单一花卉示例的类别标签，需要注意：scikit-learn期望输入数据为二维数组；因此，我们必须先将单行切片转换成这种格式。将单行数据转换为二维数据数组的一种方式是使用NumPy的`reshape`方法添加一个新维度，如下所示：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Tackling overfitting via regularization
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过正则化来应对过拟合
- en: Overfitting is a common problem in machine learning, where a model performs
    well on training data but does not generalize well to unseen data (test data).
    If a model suffers from overfitting, we also say that the model has a high variance,
    which can be caused by having too many parameters, leading to a model that is
    too complex given the underlying data. Similarly, our model can also suffer from
    **underfitting** (high bias), which means that our model is not complex enough
    to capture the pattern in the training data well and therefore also suffers from
    low performance on unseen data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是机器学习中的一个常见问题，指的是模型在训练数据上表现良好，但在未见过的数据（测试数据）上泛化能力较差。如果模型出现过拟合，我们也可以说模型具有较高的方差，这可能是由参数过多造成的，从而导致模型过于复杂，不符合数据的真实结构。同样地，我们的模型也可能会遭遇**欠拟合**（高偏差），这意味着模型不足够复杂，无法很好地捕捉训练数据中的模式，因此在未见过的数据上表现不佳。
- en: 'Although we have only encountered linear models for classification so far,
    the problems of overfitting and underfitting can be best illustrated by comparing
    a linear decision boundary to more complex, nonlinear decision boundaries, as
    shown in the following figure:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然到目前为止我们只遇到了用于分类的线性模型，但过拟合和欠拟合问题可以通过将线性决策边界与更复杂的非线性决策边界进行比较来最清楚地说明，如下图所示：
- en: '![](img/B13208_03_07.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_07.png)'
- en: '**The bias-variance tradeoff**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差-方差权衡**'
- en: Often, researchers use the terms "bias" and "variance" or "bias-variance tradeoff"
    to describe the performance of a model—that is, you may stumble upon talks, books,
    or articles where people say that a model has a "high variance" or "high bias."
    So, what does that mean? In general, we might say that "high variance" is proportional
    to overfitting and "high bias" is proportional to underfitting.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，研究人员使用“偏差”和“方差”或“偏差-方差权衡”这两个术语来描述模型的表现——也就是说，你可能会在讲座、书籍或文章中看到人们说某个模型具有“高方差”或“高偏差”。那么，这是什么意思呢？一般来说，我们可以说“高方差”与过拟合成正比，“高偏差”与欠拟合成正比。
- en: In the context of machine learning models, **variance** measures the consistency
    (or variability) of the model prediction for classifying a particular example
    if we retrain the model multiple times, for example, on different subsets of the
    training dataset. We can say that the model is sensitive to the randomness in
    the training data. In contrast, **bias** measures how far off the predictions
    are from the correct values in general if we rebuild the model multiple times
    on different training datasets; bias is the measure of the systematic error that
    is not due to randomness.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型的背景下，**方差**衡量的是如果我们多次重新训练模型，例如，在不同的训练数据子集上进行训练时，模型预测某个特定示例的一致性（或变异性）。我们可以说，模型对训练数据中的随机性比较敏感。相对地，**偏差**衡量的是如果我们在不同的训练数据集上多次重建模型时，预测值与正确值之间的偏差；偏差是衡量由系统性误差引起的偏差，而这种误差与随机性无关。
- en: 'If you are interested in the technical specification and derivation of the
    "bias" and "variance" terms, I''ve written about it in my lecture notes here:
    [https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对“偏差”和“方差”术语的技术规格及推导感兴趣，我在我的讲义中有写过相关内容，详见：[https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/08_eval-intro_notes.pdf)。
- en: One way of finding a good bias-variance tradeoff is to tune the complexity of
    the model via regularization. Regularization is a very useful method for handling
    collinearity (high correlation among features), filtering out noise from data,
    and eventually preventing overfitting.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找良好的偏差-方差平衡的一种方法是通过正则化来调整模型的复杂性。正则化是处理共线性（特征之间的高相关性）、过滤数据噪声并最终防止过拟合的非常有用的方法。
- en: 'The concept behind regularization is to introduce additional information (bias)
    to penalize extreme parameter (weight) values. The most common form of regularization
    is so-called **L2 regularization** (sometimes also called L2 shrinkage or weight
    decay), which can be written as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化背后的概念是引入额外的信息（偏置），以惩罚极端的参数（权重）值。最常见的正则化形式是所谓的 **L2正则化**（有时也称为L2收缩或权重衰减），可以写作如下：
- en: '![](img/B13208_03_044.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_044.png)'
- en: Here, ![](img/B13208_03_045.png) is the so-called **regularization parameter**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_03_045.png) 就是所谓的 **正则化参数**。
- en: '**Regularization and feature normalization**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化和特征归一化**'
- en: Regularization is another reason why feature scaling such as standardization
    is important. For regularization to work properly, we need to ensure that all
    our features are on comparable scales.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是特征缩放（如标准化）重要性的另一个原因。为了使正则化正常工作，我们需要确保所有特征都在可比的尺度上。
- en: 'The cost function for logistic regression can be regularized by adding a simple
    regularization term, which will shrink the weights during model training:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的代价函数可以通过添加一个简单的正则化项来进行正则化，这将在模型训练过程中收缩权重：
- en: '![](img/B13208_03_046.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_046.png)'
- en: Via the regularization parameter, ![](img/B13208_03_047.png), we can then control
    how well we fit the training data, while keeping the weights small. By increasing
    the value of ![](img/B13208_03_048.png), we increase the regularization strength.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过正则化参数！[](img/B13208_03_047.png)，我们可以控制拟合训练数据的程度，同时保持权重较小。通过增加！[](img/B13208_03_048.png)的值，我们增加了正则化强度。
- en: 'The parameter, `C`, that is implemented for the `LogisticRegression` class
    in scikit-learn comes from a convention in support vector machines, which will
    be the topic of the next section. The term `C` is directly related to the regularization
    parameter, ![](img/B13208_03_049.png), which is its inverse. Consequently, decreasing
    the value of the inverse regularization parameter, `C`, means that we are increasing
    the regularization strength, which we can visualize by plotting the L2 regularization
    path for the two weight coefficients:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`LogisticRegression`类中实现的参数`C`来自支持向量机中的一种约定，这将是下一节的主题。术语`C`与正则化参数！[](img/B13208_03_049.png)直接相关，它们是倒数。因此，减小逆正则化参数`C`的值意味着我们增加了正则化强度，我们可以通过绘制两个权重系数的L2正则化路径来可视化这一点：
- en: '[PRE22]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'By executing the preceding code, we fitted 10 logistic regression models with
    different values for the inverse-regularization parameter, `C`. For the purposes
    of illustration, we only collected the weight coefficients of class `1` (here,
    the second class in the dataset: `Iris-versicolor`) versus all classifiers—remember
    that we are using the OvR technique for multiclass classification.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行前面的代码，我们拟合了10个逻辑回归模型，每个模型使用不同的逆正则化参数`C`值。为了方便说明，我们只收集了类别`1`（在数据集中是第二类：`Iris-versicolor`）与所有分类器的权重系数——请记住，我们使用的是一对多（OvR）技术进行多分类。
- en: 'As we can see in the resulting plot, the weight coefficients shrink if we decrease
    parameter `C`, that is, if we increase the regularization strength:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果图中可以看出，当我们减小参数`C`时，权重系数会收缩，也就是说，当我们增加正则化强度时：
- en: '![](img/B13208_03_08.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_08.png)'
- en: '**An additional resource on logistic regression**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归的额外资源**'
- en: 'Since an in-depth coverage of the individual classification algorithms exceeds
    the scope of this book, *Logistic Regression: From Introductory to Advanced Concepts
    and Applications*, *Dr. Scott Menard*, *Sage Publications*, *2009*, is recommended
    to readers who want to learn more about logistic regression.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '由于对各个分类算法的深入讨论超出了本书的范围，*Logistic Regression: From Introductory to Advanced
    Concepts and Applications*，*Dr. Scott Menard*，*Sage Publications*，*2009*，推荐给那些想要深入了解逻辑回归的读者。'
- en: Maximum margin classification with support vector machines
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机的最大间隔分类
- en: 'Another powerful and widely used learning algorithm is the **support vector
    machine** (**SVM**), which can be considered an extension of the perceptron. Using
    the perceptron algorithm, we minimized misclassification errors. However, in SVMs
    our optimization objective is to maximize the margin. The margin is defined as
    the distance between the separating hyperplane (decision boundary) and the training
    examples that are closest to this hyperplane, which are the so-called **support
    vectors**. This is illustrated in the following figure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个强大且广泛使用的学习算法是**支持向量机**（**SVM**），它可以看作是感知机的扩展。通过使用感知机算法，我们最小化了误分类错误。然而，在SVM中，我们的优化目标是最大化间隔。间隔定义为分隔超平面（决策边界）与离该超平面最近的训练示例之间的距离，这些训练示例被称为**支持向量**。这在下图中得到了说明：
- en: '![](img/B13208_03_09.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_09.png)'
- en: Maximum margin intuition
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大间隔直觉
- en: 'The rationale behind having decision boundaries with large margins is that
    they tend to have a lower generalization error, whereas models with small margins
    are more prone to overfitting. To get an idea of the margin maximization, let''s
    take a closer look at those positive and negative hyperplanes that are parallel
    to the decision boundary, which can be expressed as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 设置具有大间隔的决策边界的原理是，它们通常具有较低的泛化误差，而具有小间隔的模型更容易出现过拟合。为了更好地理解间隔最大化的概念，我们来看一下那些与决策边界平行的正负超平面，它们可以表示为以下形式：
- en: '![](img/B13208_03_050.png) (1)![](img/B13208_03_051.png) (2)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_050.png)（1）![](img/B13208_03_051.png)（2）'
- en: 'If we subtract those two linear equations (1) and (2) from each other, we get:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这两个线性方程（1）和（2）相互减去，我们得到：
- en: '![](img/B13208_03_052.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_052.png)'
- en: 'We can normalize this equation by the length of the vector *w*, which is defined
    as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过向量*w*的长度来对这个方程进行归一化，定义如下：
- en: '![](img/B13208_03_053.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_053.png)'
- en: 'So, we arrive at the following equation:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到如下方程：
- en: '![](img/B13208_03_054.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_054.png)'
- en: The left side of the preceding equation can then be interpreted as the distance
    between the positive and negative hyperplane, which is the so-called **margin**
    that we want to maximize.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 前面方程的左侧可以解释为正负超平面之间的距离，这就是我们希望最大化的所谓**间隔**。
- en: 'Now, the objective function of the SVM becomes the maximization of this margin
    by maximizing ![](img/B13208_03_055.png) under the constraint that the examples
    are classified correctly, which can be written as:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，SVM的目标函数变成了通过最大化！[](img/B13208_03_055.png)来实现间隔最大化，同时约束条件是正确分类所有示例，可以写作：
- en: '![](img/B13208_03_056.png)![](img/B13208_03_057.png)![](img/B13208_03_058.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_056.png)![](img/B13208_03_057.png)![](img/B13208_03_058.png)'
- en: Here, *N* is the number of examples in our dataset.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N*是我们数据集中示例的数量。
- en: 'These two equations basically say that all negative-class examples should fall
    on one side of the negative hyperplane, whereas all the positive-class examples
    should fall behind the positive hyperplane, which can also be written more compactly
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个方程基本上表示，所有负类示例应位于负超平面的一侧，而所有正类示例应位于正超平面的另一侧，这也可以更紧凑地写成如下形式：
- en: '![](img/B13208_03_059.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_059.png)'
- en: 'In practice, though, it is easier to minimize the reciprocal term, ![](img/B13208_03_060.png),
    which can be solved by quadratic programming. However, a detailed discussion about
    quadratic programming is beyond the scope of this book. You can learn more about
    SVMs in *The Nature of Statistical Learning Theory*, *Springer Science*+*Business
    Media*, *Vladimir Vapnik*, 2000, or read Chris J.C. Burges'' excellent explanation
    in *A Tutorial on Support Vector Machines for Pattern Recognition* (*Data Mining
    and Knowledge Discovery*, *2(2)*: 121-167, *1998*).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上，更容易最小化倒数项！[](img/B13208_03_060.png)，它可以通过二次规划求解。然而，关于二次规划的详细讨论超出了本书的范围。您可以在*The
    Nature of Statistical Learning Theory*（*Springer Science*+*Business Media*，*Vladimir
    Vapnik*，2000年）中深入了解SVM，或者阅读Chris J.C. Burges在*《A Tutorial on Support Vector Machines
    for Pattern Recognition》*（*Data Mining and Knowledge Discovery*，*2(2)*：121-167，*1998年*）中的精彩讲解。
- en: Dealing with a nonlinearly separable case using slack variables
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用松弛变量处理非线性可分情况
- en: Although we don't want to dive much deeper into the more involved mathematical
    concepts behind the maximum-margin classification, let's briefly mention the slack
    variable, ![](img/B13208_03_061.png), which was introduced by Vladimir Vapnik
    in 1995 and led to the so-called **soft-margin classification**. The motivation
    for introducing the slack variable was that the linear constraints need to be
    relaxed for nonlinearly separable data to allow the convergence of the optimization
    in the presence of misclassifications, under appropriate cost penalization.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不想深入探讨最大间隔分类背后更复杂的数学概念，但简要提一下由 Vladimir Vapnik 在 1995 年引入的松弛变量 ![](img/B13208_03_061.png)，它促成了所谓的
    **软间隔分类**。引入松弛变量的动机是，线性约束需要被放宽，以便对非线性可分数据进行优化，从而在误分类的情况下仍能让优化过程收敛，并进行适当的成本惩罚。
- en: 'The positive-valued slack variable is simply added to the linear constraints:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正值松弛变量简单地被添加到线性约束中：
- en: '![](img/B13208_03_062.png)![](img/B13208_03_063.png)![](img/B13208_03_064.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_062.png)![](img/B13208_03_063.png)![](img/B13208_03_064.png)'
- en: 'Here, *N* is the number of examples in our dataset. So, the new objective to
    be minimized (subject to the constraints) becomes:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N* 是我们数据集中样本的数量。所以，新的目标函数（在约束条件下最小化）变成了：
- en: '![](img/B13208_03_065.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_065.png)'
- en: 'Via the variable, `C`, we can then control the penalty for misclassification.
    Large values of `C` correspond to large error penalties, whereas we are less strict
    about misclassification errors if we choose smaller values for `C`. We can then
    use the `C` parameter to control the width of the margin and therefore tune the
    bias-variance tradeoff, as illustrated in the following figure:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过变量 `C`，我们可以控制误分类的惩罚。较大的 `C` 值对应较大的误差惩罚，而选择较小的 `C` 值时，我们对误分类的错误惩罚要求就会降低。然后我们可以利用
    `C` 参数来控制间隔的宽度，从而调节偏差-方差的权衡，具体如下图所示：
- en: '![](img/B13208_03_10.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_10.png)'
- en: This concept is related to regularization, which we discussed in the previous
    section in the context of regularized regression, where decreasing the value of
    `C` increases the bias and lowers the variance of the model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念与正则化相关，我们在上一节中讨论过正则化回归，其中减小 `C` 的值可以增加模型的偏差并降低方差。
- en: 'Now that we have learned the basic concepts behind a linear SVM, let''s train
    an SVM model to classify the different flowers in our Iris dataset:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了线性 SVM 背后的基本概念，接下来让我们训练一个 SVM 模型来分类 Iris 数据集中的不同花卉：
- en: '[PRE23]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The three decision regions of the SVM, visualized after training the classifier
    on the Iris dataset by executing the preceding code example, are shown in the
    following plot:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练分类器时，使用 Iris 数据集执行上述代码示例后，SVM 的三个决策区域如以下图所示：
- en: '![](img/B13208_03_11.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_11.png)'
- en: '**Logistic regression versus SVMs**'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归与支持向量机（SVM）**'
- en: In practical classification tasks, linear logistic regression and linear SVMs
    often yield very similar results. Logistic regression tries to maximize the conditional
    likelihoods of the training data, which makes it more prone to outliers than SVMs,
    which mostly care about the points that are closest to the decision boundary (support
    vectors). On the other hand, logistic regression has the advantage that it is
    a simpler model and can be implemented more easily. Furthermore, logistic regression
    models can be easily updated, which is attractive when working with streaming
    data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的分类任务中，线性逻辑回归和线性 SVM 通常会产生非常相似的结果。逻辑回归试图最大化训练数据的条件似然，这使得它比 SVM 更容易受到异常值的影响，而
    SVM 则更关注距离决策边界最近的点（支持向量）。另一方面，逻辑回归的优势在于它是一个更简单的模型，且实现起来更容易。此外，逻辑回归模型可以很容易地更新，这对于处理流数据时尤其具有吸引力。
- en: Alternative implementations in scikit-learn
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn 中的替代实现
- en: The scikit-learn library's `LogisticRegression` class, which we used in the
    previous sections, makes use of the LIBLINEAR library, which is a highly optimized
    C/C++ library developed at the National Taiwan University ([http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 库中的 `LogisticRegression` 类，我们在前面的章节中使用过，利用了 LIBLINEAR 库，这是一款由台湾大学开发的高度优化的
    C/C++ 库 ([http://www.csie.ntu.edu.tw/~cjlin/liblinear/](http://www.csie.ntu.edu.tw/~cjlin/liblinear/))。
- en: Similarly, the `SVC` class that we used to train an SVM makes use of LIBSVM,
    which is an equivalent C/C++ library specialized for SVMs ([http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们用来训练 SVM 的 `SVC` 类利用了 LIBSVM，这是一个专门为 SVM 提供支持的 C/C++ 库（[http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)）。
- en: 'The advantage of using LIBLINEAR and LIBSVM over native Python implementations
    is that they allow the extremely quick training of large amounts of linear classifiers.
    However, sometimes our datasets are too large to fit into computer memory. Thus,
    scikit-learn also offers alternative implementations via the `SGDClassifier` class,
    which also supports online learning via the `partial_fit` method. The concept
    behind the `SGDClassifier` class is similar to the stochastic gradient algorithm
    that we implemented in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*, for Adaline. We could initialize the SGD version of the perceptron,
    logistic regression, and an SVM with default parameters, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LIBLINEAR 和 LIBSVM 的优势在于，它们能极其快速地训练大量线性分类器。而有时我们的数据集太大，无法完全加载到计算机内存中。因此，scikit-learn
    还通过 `SGDClassifier` 类提供了替代实现，该类也支持通过 `partial_fit` 方法进行在线学习。`SGDClassifier` 类的概念类似于我们在*第二章*《训练简单的机器学习分类算法》中为
    Adaline 实现的随机梯度算法。我们可以使用默认参数初始化感知机、逻辑回归和 SVM 的 SGD 版本，如下所示：
- en: '[PRE24]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Solving nonlinear problems using a kernel SVM
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核 SVM 解决非线性问题
- en: Another reason why SVMs enjoy high popularity among machine learning practitioners
    is that they can be easily **kernelized** to solve nonlinear classification problems.
    Before we discuss the main concept behind the so-called **kernel SVM**, the most
    common variant of SVMs, let's first create a synthetic dataset to see what such
    a nonlinear classification problem may look like.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 在机器学习实践者中广受欢迎的另一个原因是它们可以很容易地**核化**，用于解决非线性分类问题。在我们讨论所谓的**核 SVM**这一最常见的 SVM
    变体的主要概念之前，让我们首先创建一个合成数据集，看看这种非线性分类问题是什么样的。
- en: Kernel methods for linearly inseparable data
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对线性不可分数据的核方法
- en: 'Using the following code, we will create a simple dataset that has the form
    of an XOR gate using the `logical_or` function from NumPy, where 100 examples
    will be assigned the class label `1`, and 100 examples will be assigned the class
    label `-1`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们将创建一个简单的数据集，该数据集通过 NumPy 的 `logical_or` 函数生成，形态类似于 XOR 门，其中 100 个示例被分配标签
    `1`，另 100 个示例被分配标签 `-1`：
- en: '[PRE25]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After executing the code, we will have an XOR dataset with random noise, as
    shown in the following plot:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们将得到一个带有随机噪声的 XOR 数据集，如下图所示：
- en: '![](img/B13208_03_12.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_12.png)'
- en: Obviously, we would not be able to separate the examples from the positive and
    negative class very well using a linear hyperplane as a decision boundary via
    the linear logistic regression or linear SVM model that we discussed in earlier
    sections.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们无法使用线性超平面作为决策边界很好地分离正负类别的示例，使用我们在前面章节中讨论的线性逻辑回归或线性支持向量机（SVM）模型。
- en: 'The basic idea behind **kernel methods** to deal with such linearly inseparable
    data is to create nonlinear combinations of the original features to project them
    onto a higher-dimensional space via a mapping function, ![](img/B13208_03_066.png),
    where the data becomes linearly separable. As shown in the following plot, we
    can transform a two-dimensional dataset into a new three-dimensional feature space,
    where the classes become separable via the following projection:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**核方法**处理这类线性不可分数据的基本思想是，利用原始特征的非线性组合，通过映射函数将数据投影到更高维空间，![](img/B13208_03_066.png)，使得数据变得线性可分。如以下图所示，我们可以将二维数据集转换为新的三维特征空间，在这个空间中，类别可以通过以下投影分离：'
- en: '![](img/B13208_03_067.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_067.png)'
- en: 'This allows us to separate the two classes shown in the plot via a linear hyperplane
    that becomes a nonlinear decision boundary if we project it back onto the original
    feature space:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们能够通过一个线性超平面来分离图中显示的两个类别，如果我们将其投影回原始特征空间，它会变成一个非线性的决策边界：
- en: '![](img/B13208_03_13.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_13.png)'
- en: Using the kernel trick to find separating hyperplanes in a high-dimensional
    space
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用核技巧在高维空间中寻找分隔超平面
- en: To solve a nonlinear problem using an SVM, we would transform the training data
    into a higher-dimensional feature space via a mapping function, ![](img/B13208_03_068.png),
    and train a linear SVM model to classify the data in this new feature space. Then,
    we could use the same mapping function, ![](img/B13208_03_069.png), to transform
    new, unseen data to classify it using the linear SVM model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决一个非线性问题，我们可以通过映射函数![](img/B13208_03_068.png)将训练数据转换到更高维的特征空间，并训练一个线性SVM模型，在这个新的特征空间中对数据进行分类。然后，我们可以使用相同的映射函数![](img/B13208_03_069.png)将新的、未见过的数据转换，并使用线性SVM模型对其进行分类。
- en: However, one problem with this mapping approach is that the construction of
    the new features is computationally very expensive, especially if we are dealing
    with high-dimensional data. This is where the so-called **kernel trick** comes
    into play.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种映射方法的一个问题是，构建新特征的计算开销非常大，特别是当我们处理高维数据时。这就是所谓的**核技巧**发挥作用的地方。
- en: 'Although we did not go into much detail about how to solve the quadratic programming
    task to train an SVM, in practice, we just need to replace the dot product ![](img/B13208_03_070.png)
    by ![](img/B13208_03_071.png). In order to save the expensive step of calculating
    this dot product between two points explicitly, we define a so-called **kernel
    function**:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们没有深入讨论如何解决二次规划任务来训练SVM，但在实际操作中，我们只需要将点积![](img/B13208_03_070.png)替换为![](img/B13208_03_071.png)。为了节省显式计算两点之间点积的昂贵步骤，我们定义了一个所谓的**核函数**：
- en: '![](img/B13208_03_072.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_072.png)'
- en: 'One of the most widely used kernels is the **radial basis function** (**RBF**)
    kernel, which can simply be called the **Gaussian kernel**:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的核函数之一是**径向基函数**（**RBF**）核，也可以简单地称为**高斯核**：
- en: '![](img/B13208_03_073.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_073.png)'
- en: 'This is often simplified to:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常简化为：
- en: '![](img/B13208_03_074.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_074.png)'
- en: Here, ![](img/B13208_03_075.png) is a free parameter to be optimized.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_03_075.png)是一个需要优化的自由参数。
- en: Roughly speaking, the term "kernel" can be interpreted as a **similarity function**
    between a pair of examples. The minus sign inverts the distance measure into a
    similarity score, and, due to the exponential term, the resulting similarity score
    will fall into a range between 1 (for exactly similar examples) and 0 (for very
    dissimilar examples).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略来说，“核函数”这一术语可以解释为一对示例之间的**相似性函数**。负号将距离度量反转为相似性分数，而由于指数项的存在，最终的相似性分数将落在1（完全相似的示例）和0（非常不相似的示例）之间。
- en: 'Now that we have covered the big picture behind the kernel trick, let''s see
    if we can train a kernel SVM that is able to draw a nonlinear decision boundary
    that separates the XOR data well. Here, we simply use the `SVC` class from scikit-learn
    that we imported earlier and replace the `kernel=''linear''` parameter with `kernel=''rbf''`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了核技巧背后的大致原理，接下来让我们看看能否训练一个能够画出非线性决策边界的核SVM，从而很好地分离XOR数据。这里，我们只需使用之前导入的scikit-learn中的`SVC`类，并将`kernel='linear'`参数替换为`kernel='rbf'`：
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we can see in the resulting plot, the kernel SVM separates the XOR data
    relatively well:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在结果图中看到的，核SVM相对较好地分离了XOR数据：
- en: '![](img/B13208_03_14.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_14.png)'
- en: 'The ![](img/B13208_03_076.png) parameter, which we set to `gamma=0.1`, can
    be understood as a cut-off parameter for the Gaussian sphere. If we increase the
    value for ![](img/B13208_03_077.png), we increase the influence or reach of the
    training examples, which leads to a tighter and bumpier decision boundary. To
    get a better understanding of ![](img/B13208_03_078.png), let''s apply an RBF
    kernel SVM to our Iris flower dataset:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B13208_03_076.png)参数，我们将其设置为`gamma=0.1`，可以理解为高斯球的截断参数。如果我们增加![](img/B13208_03_077.png)的值，我们会增加训练样本的影响力或覆盖范围，从而导致更紧密且更崎岖的决策边界。为了更好地理解![](img/B13208_03_078.png)，我们来对鸢尾花数据集应用RBF核SVM：'
- en: '[PRE27]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since we chose a relatively small value for ![](img/B13208_03_079.png), the
    resulting decision boundary of the RBF kernel SVM model will be relatively soft,
    as shown in the following plot:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择了相对较小的![](img/B13208_03_079.png)值，RBF核SVM模型的决策边界将相对平缓，如下图所示：
- en: '![](img/B13208_03_15.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_15.png)'
- en: 'Now, let''s increase the value of ![](img/B13208_03_080.png) and observe the
    effect on the decision boundary:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们增加![](img/B13208_03_080.png)的值，并观察其对决策边界的影响：
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the resulting plot, we can now see that the decision boundary around the
    classes `0` and `1` is much tighter using a relatively large value of ![](img/B13208_03_080.png):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的图中，我们现在可以看到，在使用相对较大值的条件下，类`0`和`1`周围的决策边界要紧密得多：
- en: '![](img/B13208_03_16.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_16.png)'
- en: Although the model fits the training dataset very well, such a classifier will
    likely have a high generalization error on unseen data. This illustrates that
    the ![](img/B13208_03_082.png) parameter also plays an important role in controlling
    overfitting or variance when the algorithm is too sensitive to fluctuations in
    the training dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型非常适合训练数据集，这样的分类器在未见数据上可能会有很高的泛化误差。这说明当算法对训练数据集中的波动过于敏感时，**参数**也在控制过拟合或方差方面起着重要作用。
- en: Decision tree learning
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树学习
- en: '**Decision tree** classifiers are attractive models if we care about interpretability.
    As the name "decision tree" suggests, we can think of this model as breaking down
    our data by making a decision based on asking a series of questions.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**分类器是具有吸引力的模型，如果我们关心其可解释性的话。正如名称“决策树”所示，我们可以将这个模型视为通过询问一系列问题来将我们的数据进行分解的模型。'
- en: 'Let''s consider the following example in which we use a decision tree to decide
    upon an activity on a particular day:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下示例，在这个示例中，我们使用决策树来决定特定日期的活动：
- en: '![](img/B13208_03_17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_17.png)'
- en: 'Based on the features in our training dataset, the decision tree model learns
    a series of questions to infer the class labels of the examples. Although the
    preceding figure illustrates the concept of a decision tree based on categorical
    variables, the same concept applies if our features are real numbers, like in
    the Iris dataset. For example, we could simply define a cut-off value along the
    **sepal width** feature axis and ask a binary question: "Is the sepal width ≥
    2.8 cm?"'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们训练数据集中的特征，决策树模型学习一系列问题来推断示例的类标签。尽管前面的图示了基于分类变量的决策树概念，但是如果我们的特征是实数，比如在鸢尾花数据集中，同样的概念也适用。例如，我们可以简单地沿着**花萼宽度**特征轴定义一个截断值，并提出一个二元问题：“花萼宽度是否≥
    2.8 厘米？”
- en: Using the decision algorithm, we start at the tree root and split the data on
    the feature that results in the largest **information gain** (**IG**), which will
    be explained in more detail in the following section. In an iterative process,
    we can then repeat this splitting procedure at each child node until the leaves
    are pure. This means that the training examples at each node all belong to the
    same class. In practice, this can result in a very deep tree with many nodes,
    which can easily lead to overfitting. Thus, we typically want to **prune** the
    tree by setting a limit for the maximal depth of the tree.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策算法，我们从树根开始，并在导致最大**信息增益**（**IG**）的特征上分割数据，这将在以下部分详细解释。在迭代过程中，我们可以在每个子节点上重复这个分割过程，直到叶子节点变得纯净。这意味着每个节点上的训练示例都属于同一类。在实践中，这可能导致一个非常深的树，具有许多节点，这很容易导致过拟合。因此，我们通常希望通过设置树的最大深度限制来**修剪**树。
- en: Maximizing IG – getting the most bang for your buck
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大化信息增益 - 获取最大的回报
- en: 'In order to split the nodes at the most informative features, we need to define
    an objective function that we want to optimize via the tree learning algorithm.
    Here, our objective function is to maximize the IG at each split, which we define
    as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在最具信息特征处分割节点，我们需要定义一个通过树学习算法优化的目标函数。在这里，我们的目标函数是在每个分割点最大化信息增益（IG），定义如下：
- en: '![](img/B13208_03_083.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_083.png)'
- en: 'Here, *f* is the feature to perform the split; ![](img/B13208_03_084.png) and
    ![](img/B13208_03_085.png) are the dataset of the parent and *j*th child node;
    *I* is our **impurity** measure; ![](img/B13208_03_086.png) is the total number
    of training examples at the parent node; and ![](img/B13208_03_087.png) is the
    number of examples in the *j*th child node. As we can see, the information gain
    is simply the difference between the impurity of the parent node and the sum of
    the child node impurities—the lower the impurities of the child nodes, the larger
    the information gain. However, for simplicity and to reduce the combinatorial
    search space, most libraries (including scikit-learn) implement binary decision
    trees. This means that each parent node is split into two child nodes, ![](img/B13208_03_088.png)
    and ![](img/B13208_03_089.png):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 是执行分割的特征；![](img/B13208_03_084.png) 和 ![](img/B13208_03_085.png) 是父节点和第
    *j* 个子节点的数据集；*I* 是我们的**不纯度**度量；![](img/B13208_03_086.png) 是父节点的训练示例总数；![](img/B13208_03_087.png)
    是第 *j* 个子节点的示例数。正如我们所看到的，信息增益简单地是父节点的不纯度与子节点不纯度之和的差异——子节点的不纯度越低，信息增益越大。然而，为了简化和减少组合搜索空间，大多数库（包括
    scikit-learn）实现了二叉决策树。这意味着每个父节点被分割成两个子节点，![](img/B13208_03_088.png) 和 ![](img/B13208_03_089.png)：
- en: '![](img/B13208_03_090.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_090.png)'
- en: 'The three impurity measures or splitting criteria that are commonly used in
    binary decision trees are **Gini impurity** (![](img/B13208_03_091.png)), **entropy**
    (![](img/B13208_03_092.png)), and the **classification error** (![](img/B13208_03_093.png)).
    Let''s start with the definition of entropy for all **non-empty** classes (![](img/B13208_03_094.png)):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在二叉决策树中常用的三个不纯度度量或分割标准是**基尼不纯度**（![](img/B13208_03_091.png)）、**熵**（![](img/B13208_03_092.png)）和**分类错误率**（![](img/B13208_03_093.png)）。让我们从所有**非空**类别（![](img/B13208_03_094.png)）的熵定义开始：
- en: '![](img/B13208_03_095.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_095.png)'
- en: Here, ![](img/B13208_03_096.png) is the proportion of the examples that belong
    to class *i* for a particular node, *t*. The entropy is therefore 0 if all examples
    at a node belong to the same class, and the entropy is maximal if we have a uniform
    class distribution. For example, in a binary class setting, the entropy is 0 if
    ![](img/B13208_03_097.png) or ![](img/B13208_03_098.png). If the classes are distributed
    uniformly with ![](img/B13208_03_099.png) and ![](img/B13208_03_100.png), the
    entropy is 1\. Therefore, we can say that the entropy criterion attempts to maximize
    the mutual information in the tree.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_03_096.png) 是属于特定节点 *t* 的类别 *i* 的示例比例。因此，如果一个节点上的所有示例都属于同一类，则熵为
    0，如果我们具有均匀的类分布，则熵最大。例如，在二元类设置中，如果 ![](img/B13208_03_097.png) 或 ![](img/B13208_03_098.png)，则熵为
    0。如果类别均匀分布为 ![](img/B13208_03_099.png) 和 ![](img/B13208_03_100.png)，则熵为 1。因此，我们可以说熵标准试图在树中最大化互信息。
- en: 'The Gini impurity can be understood as a criterion to minimize the probability
    of misclassification:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度可以理解为最小化误分类的概率：
- en: '![](img/B13208_03_101.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_101.png)'
- en: 'Similar to entropy, the Gini impurity is maximal if the classes are perfectly
    mixed, for example, in a binary class setting (*c* = 2):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于熵，基尼不纯度在类别完全混合时最大，例如，在二元类设置中（*c* = 2）：
- en: '![](img/B13208_03_102.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_102.png)'
- en: However, in practice, both Gini impurity and entropy typically yield very similar
    results, and it is often not worth spending much time on evaluating trees using
    different impurity criteria rather than experimenting with different pruning cut-offs.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，基尼不纯度和熵通常产生非常相似的结果，通常不值得花费太多时间评估使用不同不纯度标准的树，而不是尝试不同的修剪截止值。
- en: 'Another impurity measure is the classification error:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不纯度度量是分类错误率：
- en: '![](img/B13208_03_103.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_103.png)'
- en: 'This is a useful criterion for pruning but not recommended for growing a decision
    tree, since it is less sensitive to changes in the class probabilities of the
    nodes. We can illustrate this by looking at the two possible splitting scenarios
    shown in the following figure:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这是修剪决策树的一个有用标准，但不建议用于生长决策树，因为它对节点类别概率的变化不太敏感。我们可以通过查看以下图示的两种可能的分割场景来说明这一点：
- en: '![](img/B13208_03_18.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_18.png)'
- en: 'We start with a dataset, ![](img/B13208_03_104.png), at the parent node, which
    consists of 40 examples from class 1 and 40 examples from class 2 that we split
    into two datasets, ![](img/B13208_03_105.png) and ![](img/B13208_03_106.png).
    The information gain using the classification error as a splitting criterion would
    be the same (![](img/B13208_03_107.png)) in both scenarios, A and B:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个数据集开始，![](img/B13208_03_104.png)，这是父节点处的 40 个类别 1 的示例和 40 个类别 2 的示例，我们将其拆分为两个数据集，![](img/B13208_03_105.png)
    和 ![](img/B13208_03_106.png)。使用分类误差作为分割标准时，信息增益在场景 A 和场景 B 中是相同的（![](img/B13208_03_107.png)）：
- en: '![](img/B13208_03_108.png)![](img/B13208_03_109.png)![](img/B13208_03_110.png)![](img/B13208_03_111.png)![](img/B13208_03_112.png)![](img/B13208_03_113.png)![](img/B13208_03_114.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_108.png)![](img/B13208_03_109.png)![](img/B13208_03_110.png)![](img/B13208_03_111.png)![](img/B13208_03_112.png)![](img/B13208_03_113.png)![](img/B13208_03_114.png)'
- en: 'However, the Gini impurity would favor the split in scenario B (![](img/B13208_03_115.png))
    over scenario A (![](img/B13208_03_116.png)), which is indeed purer:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基尼不纯度会偏向场景 B（![](img/B13208_03_115.png)）中的分割，而不是场景 A（![](img/B13208_03_116.png)），因为
    B 的不纯度确实更低。
- en: '![](img/B13208_03_117.png)![](img/B13208_03_118.png)![](img/B13208_03_119.png)![](img/B13208_03_120.png)![](img/B13208_03_121.png)![](img/B13208_03_122.png)![](img/B13208_03_123.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_117.png)![](img/B13208_03_118.png)![](img/B13208_03_119.png)![](img/B13208_03_120.png)![](img/B13208_03_121.png)![](img/B13208_03_122.png)![](img/B13208_03_123.png)'
- en: 'Similarly, the entropy criterion would also favor scenario B (![](img/B13208_03_124.png))
    over scenario A (![](img/B13208_03_125.png)):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，熵准则也会偏向场景 B（![](img/B13208_03_124.png)）而非场景 A（![](img/B13208_03_125.png)）：
- en: '![](img/B13208_03_126.png)![](img/B13208_03_127.png)![](img/B13208_03_128.png)![](img/B13208_03_129.png)![](img/B13208_03_130.png)![](img/B13208_03_131.png)![](img/B13208_03_132.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_126.png)![](img/B13208_03_127.png)![](img/B13208_03_128.png)![](img/B13208_03_129.png)![](img/B13208_03_130.png)![](img/B13208_03_131.png)![](img/B13208_03_132.png)'
- en: 'For a more visual comparison of the three different impurity criteria that
    we discussed previously, let''s plot the impurity indices for the probability
    range [0, 1] for class 1\. Note that we will also add a scaled version of the
    entropy (entropy / 2) to observe that the Gini impurity is an intermediate measure
    between entropy and the classification error. The code is as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地比较我们之前讨论的三种不同的不纯度准则，让我们绘制类别 1 的概率范围 [0, 1] 对应的不纯度指数。请注意，我们还将添加一个缩放版本的熵（熵
    / 2），以观察基尼不纯度是熵和分类误差之间的中介度量。代码如下：
- en: '[PRE29]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The plot produced by the preceding code example is as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码示例生成的图如下：
- en: '![](img/B13208_03_19.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_19.png)'
- en: Building a decision tree
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树
- en: 'Decision trees can build complex decision boundaries by dividing the feature
    space into rectangles. However, we have to be careful since the deeper the decision
    tree, the more complex the decision boundary becomes, which can easily result
    in overfitting. Using scikit-learn, we will now train a decision tree with a maximum
    depth of 4, using Gini impurity as a criterion for impurity. Although feature
    scaling may be desired for visualization purposes, note that feature scaling is
    not a requirement for decision tree algorithms. The code is as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以通过将特征空间划分为矩形来构建复杂的决策边界。然而，我们必须小心，因为决策树越深，决策边界就越复杂，这很容易导致过拟合。使用 scikit-learn，我们将训练一个最大深度为
    4 的决策树，使用基尼不纯度作为不纯度标准。虽然为了可视化目的可能需要特征缩放，但请注意，特征缩放并不是决策树算法的要求。代码如下：
- en: '[PRE30]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After executing the code example, we get the typical axis-parallel decision
    boundaries of the decision tree:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码示例后，我们得到了决策树的典型轴平行决策边界：
- en: '![](img/B13208_03_20.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_20.png)'
- en: 'A nice feature in scikit-learn is that it allows us to readily visualize the
    decision tree model after training via the following code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的一个很好的功能是，它允许我们在训练后通过以下代码直观地可视化决策树模型：
- en: '[PRE31]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/B13208_03_21.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_21.png)'
- en: 'However, nicer visualizations can be obtained by using the Graphviz program
    as a backend for plotting scikit-learn decision trees. This program is freely
    available from [http://www.graphviz.org](http://www.graphviz.org) and is supported
    by Linux, Windows, and macOS. In addition to Graphviz, we will use a Python library
    called PyDotPlus, which has capabilities similar to Graphviz and allows us to
    convert `.dot` data files into a decision tree image file. After you have installed
    Graphviz (by following the instructions on [http://www.graphviz.org/download](http://www.graphviz.org/download)),
    you can install PyDotPlus directly via the pip installer, for example, by executing
    the following command in your command-line terminal:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过使用 Graphviz 程序作为绘制 scikit-learn 决策树的后端，可以获得更漂亮的可视化效果。这个程序可以从[http://www.graphviz.org](http://www.graphviz.org)免费下载，并且支持
    Linux、Windows 和 macOS。除了 Graphviz，我们还将使用一个名为 PyDotPlus 的 Python 库，它具有类似 Graphviz
    的功能，并允许我们将 `.dot` 数据文件转换为决策树图像文件。在你安装了 Graphviz（按照[http://www.graphviz.org/download](http://www.graphviz.org/download)上的说明进行安装）后，你可以通过
    pip 安装 PyDotPlus，例如在命令行终端中执行以下命令：
- en: '[PRE32]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Installing PyDotPlus prerequisites**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PyDotPlus 先决条件**'
- en: 'Note that on some systems, you may have to install the PyDotPlus prerequisites
    manually by executing the following commands:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在某些系统上，你可能需要手动安装 PyDotPlus 的先决条件，可以通过执行以下命令来安装：
- en: '[PRE33]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following code will create an image of our decision tree in PNG format
    in our local directory:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将在本地目录中创建我们决策树的 PNG 格式图像：
- en: '[PRE34]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'By using the `out_file=None` setting, we directly assigned the DOT data to
    a `dot_data` variable, instead of writing an intermediate `tree.dot` file to disk.
    The arguments for `filled`, `rounded`, `class_names`, and `feature_names` are
    optional, but make the resulting image file visually more appealing by adding
    color, rounding the box edges, showing the name of the majority class label at
    each node, and displaying the feature name in each splitting criterion. These
    settings resulted in the following decision tree image:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `out_file=None` 设置，我们直接将 DOT 数据分配给 `dot_data` 变量，而不是将中间的 `tree.dot` 文件写入磁盘。`filled`、`rounded`、`class_names`
    和 `feature_names` 的参数是可选的，但通过添加颜色、圆角框边缘、在每个节点显示主要类标签的名称以及在每个分裂标准中显示特征名称，使得最终的图像文件在视觉上更具吸引力。这些设置产生了以下的决策树图像：
- en: '![](img/B13208_03_22.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_22.png)'
- en: Looking at the decision tree figure, we can now nicely trace back the splits
    that the decision tree determined from our training dataset. We started with 105
    examples at the root and split them into two child nodes with 35 and 70 examples,
    using the petal width cut-off ≤ 0.75 cm. After the first split, we can see that
    the left child node is already pure and only contains examples from the `Iris-setosa`
    class (Gini impurity = 0). The further splits on the right are then used to separate
    the examples from the `Iris-versicolor` and `Iris-virginica` class.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 看着决策树图，我们现在可以清晰地追踪决策树从我们的训练数据集确定的分裂。我们从根节点的 105 个示例开始，并使用花瓣宽度 ≤ 0.75 cm 将它们分成了两个子节点，分别包含
    35 个和 70 个示例。在第一次分裂后，我们可以看到左子节点已经纯净，且仅包含 `Iris-setosa` 类的示例（基尼不纯度 = 0）。接下来的分裂则用于将
    `Iris-versicolor` 和 `Iris-virginica` 类的示例分开。
- en: Looking at this tree, and the decision region plot of the tree, we can see that
    the decision tree does a very good job of separating the flower classes. Unfortunately,
    scikit-learn currently does not implement functionality to manually post-prune
    a decision tree. However, we could go back to our previous code example, change
    the `max_depth` of our decision tree to `3`, and compare it to our current model,
    but we leave this as an exercise for the interested reader.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 从这棵树以及树的决策区域图中，我们可以看到决策树在分离花卉类别方面做得非常好。不幸的是，当前的 scikit-learn 并没有实现手动后剪枝决策树的功能。不过，我们可以回到之前的代码示例，将决策树的`max_depth`改为`3`，并将其与当前模型进行比较，但我们将这个留给有兴趣的读者作为练习。
- en: Combining multiple decision trees via random forests
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过随机森林结合多个决策树
- en: 'Ensemble methods have gained huge popularity in applications of machine learning
    during the last decade due to their good classification performance and robustness
    toward overfitting. While we are going to cover different ensemble methods, including
    **bagging** and **boosting**, later in *Chapter 7*, *Combining Different Models
    for Ensemble Learning*, let''s discuss the decision tree-based **random forest**
    algorithm, which is known for its good scalability and ease of use. A random forest
    can be considered as an **ensemble** of decision trees. The idea behind a random
    forest is to average multiple (deep) decision trees that individually suffer from
    high variance to build a more robust model that has a better generalization performance
    and is less susceptible to overfitting. The random forest algorithm can be summarized
    in four simple steps:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 由于集成方法在过去十年里因其出色的分类性能和对过拟合的鲁棒性，已经在机器学习应用中获得了巨大的普及。虽然我们将在*第7章*《集成学习中不同模型的结合》中详细介绍不同的集成方法，包括**袋装法**和**提升法**，但在此我们先讨论基于决策树的**随机森林**算法，它因良好的可扩展性和易用性而著称。随机森林可以看作是决策树的**集成**。随机森林的基本思想是通过平均多个（深度）决策树的结果来降低每棵树的高方差，从而构建出一个更为稳健的模型，具有更好的泛化性能，并且更不容易过拟合。随机森林算法可以用四个简单步骤来总结：
- en: Draw a random **bootstrap** sample of size *n* (randomly choose *n* examples
    from the training dataset with replacement).
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中随机抽取一个大小为*n*的**自助法**样本（带放回地随机选择*n*个样本）。
- en: 'Grow a decision tree from the bootstrap sample. At each node:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从自助法样本中生成一棵决策树。在每个节点：
- en: Randomly select *d* features without replacement.
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择*d*个特征进行无放回抽样。
- en: Split the node using the feature that provides the best split according to the
    objective function, for instance, maximizing the information gain.
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用根据目标函数提供最佳切分的特征来分割节点，例如，最大化信息增益。
- en: Repeat the steps 1-2 *k* times.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行步骤 1-2，共*k*次。
- en: Aggregate the prediction by each tree to assign the class label by **majority
    vote**. Majority voting will be discussed in more detail in *Chapter 7*, *Combining
    Different Models for Ensemble Learning*.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过每棵树的预测结果进行汇总，采用**多数投票**的方式来确定类别标签。**多数投票**将在*第7章*《集成学习中不同模型的结合》中进行更详细的讨论。
- en: 'We should note one slight modification in step 2 when we are training the individual
    decision trees: instead of evaluating all features to determine the best split
    at each node, we only consider a random subset of those.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练单独的决策树时，我们应注意第2步的一个小修改：不是评估所有特征来确定每个节点的最佳切分，而是只考虑这些特征的一个随机子集。
- en: '**Sampling with and without replacement**'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**有放回和无放回抽样**'
- en: In case you are not familiar with the terms sampling "with" and "without" replacement,
    let's walk through a simple thought experiment. let's assume that we are playing
    a lottery game where we randomly draw numbers from an urn. We start with an urn
    that holds five unique numbers, 0, 1, 2, 3, and 4, and we draw exactly one number
    each turn. In the first round, the chance of drawing a particular number from
    the urn would be 1/5\. Now, in sampling without replacement, we do not put the
    number back into the urn after each turn. Consequently, the probability of drawing
    a particular number from the set of remaining numbers in the next round depends
    on the previous round. For example, if we have a remaining set of numbers 0, 1,
    2, and 4, the chance of drawing number 0 would become 1/4 in the next turn.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉“有放回”和“无放回”抽样的术语，让我们通过一个简单的思维实验来讲解。假设我们正在玩一个彩票游戏，在这个游戏中，我们从一个抽签盒中随机抽取数字。我们从一个包含五个唯一数字的抽签盒开始，数字分别是
    0、1、2、3 和 4，并且每次抽取一个数字。在第一轮中，从抽签盒中抽取特定数字的概率为 1/5。现在，在无放回抽样中，我们每次抽取后不会将数字放回抽签盒中。因此，在下一轮中，从剩余数字集合中抽取特定数字的概率会受到前一轮的影响。例如，如果剩余的数字集合是
    0、1、2 和 4，那么下一轮抽取数字 0 的概率将变为 1/4。
- en: 'However, in random sampling with replacement, we always return the drawn number
    to the urn so that the probability of drawing a particular number at each turn
    does not change; we can draw the same number more than once. In other words, in
    sampling *with* replacement, the samples (numbers) are independent and have a
    covariance of zero. For example, the results from five rounds of drawing random
    numbers could look like this:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在有放回的随机抽样中，我们每次抽取后都会将数字放回抽签盒中，因此每次抽取特定数字的概率不会发生变化；同一个数字可能会被多次抽取。换句话说，在有放回抽样中，样本（数字）是独立的，并且它们的协方差为零。例如，五轮抽取随机数字的结果可能如下所示：
- en: 'Random sampling without replacement: 2, 1, 3, 4, 0'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无放回的随机采样：2, 1, 3, 4, 0
- en: 'Random sampling with replacement: 1, 3, 3, 4, 1'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有放回的随机采样：1, 3, 3, 4, 1
- en: Although random forests don't offer the same level of interpretability as decision
    trees, a big advantage of random forests is that we don't have to worry so much
    about choosing good hyperparameter values. We typically don't need to prune the
    random forest since the ensemble model is quite robust to noise from the individual
    decision trees. The only parameter that we really need to care about in practice
    is the number of trees, *k*, (step 3) that we choose for the random forest. Typically,
    the larger the number of trees, the better the performance of the random forest
    classifier at the expense of an increased computational cost.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机森林不像决策树那样提供相同级别的可解释性，但随机森林的一个大优势是我们不必太过担心选择合适的超参数值。通常，我们不需要修剪随机森林，因为集成模型对单个决策树的噪声相当鲁棒。在实践中，我们真正需要关心的唯一参数是我们为随机森林选择的树的数量，*k*（步骤3）。通常，树的数量越大，随机森林分类器的性能越好，但代价是增加了计算成本。
- en: Although it is less common in practice, other hyperparameters of the random
    forest classifier that can be optimized—using techniques that we will discuss
    in *Chapter 6*, *Learning Best Practices for Model Evaluation and Hyperparameter
    Tuning*—are the size, *n*, of the bootstrap sample (step 1), and the number of
    features, *d*, that are randomly chosen for each split (step 2.a), respectively.
    Via the sample size, *n*, of the bootstrap sample, we control the bias-variance
    tradeoff of the random forest.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在实际操作中不太常见，但可以优化的随机森林分类器的其他超参数——使用我们将在*第6章*《*模型评估与超参数调优的最佳实践学习*》中讨论的技巧——分别是自助采样的大小，*n*（步骤1），以及为每个分裂随机选择的特征数量，*d*（步骤2.a）。通过自助采样的大小，*n*，我们可以控制随机森林的偏差-方差权衡。
- en: Decreasing the size of the bootstrap sample increases the diversity among the
    individual trees, since the probability that a particular training example is
    included in the bootstrap sample is lower. Thus, shrinking the size of the bootstrap
    samples may increase the *randomness* of the random forest, and it can help to
    reduce the effect of overfitting. However, smaller bootstrap samples typically
    result in a lower overall performance of the random forest, and a small gap between
    training and test performance, but a low test performance overall. Conversely,
    increasing the size of the bootstrap sample may increase the degree of overfitting.
    Because the bootstrap samples, and consequently the individual decision trees,
    become more similar to each other, they learn to fit the original training dataset
    more closely.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 减小自助采样的大小会增加个体树之间的多样性，因为某个特定训练样本被包含在自助采样中的概率较低。因此，缩小自助采样的大小可能会增加随机森林的*随机性*，并有助于减少过拟合的影响。然而，更小的自助采样通常会导致随机森林的整体性能较低，并且训练和测试性能之间差距较小，但测试性能整体较差。相反，增大自助采样的大小可能会增加过拟合的程度。由于自助采样，从而个体决策树变得彼此更相似，它们学会更紧密地拟合原始训练数据集。
- en: In most implementations, including the `RandomForestClassifier` implementation
    in scikit-learn, the size of the bootstrap sample is chosen to be equal to the
    number of training examples in the original training dataset, which usually provides
    a good bias-variance tradeoff. For the number of features, *d*, at each split,
    we want to choose a value that is smaller than the total number of features in
    the training dataset. A reasonable default that is used in scikit-learn and other
    implementations is ![](img/B13208_03_133.png), where *m* is the number of features
    in the training dataset.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实现中，包括scikit-learn中的`RandomForestClassifier`实现，自助采样的大小被选择为等于原始训练数据集中训练样本的数量，这通常提供一个良好的偏差-方差权衡。对于每个分裂时的特征数量，*d*，我们希望选择一个比训练数据集中总特征数小的值。在scikit-learn和其他实现中使用的合理默认值是![](img/B13208_03_133.png)，其中*m*是训练数据集中的特征数量。
- en: 'Conveniently, we don''t have to construct the random forest classifier from
    individual decision trees by ourselves because there is already an implementation
    in scikit-learn that we can use:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，我们不需要自己从单独的决策树构建随机森林分类器，因为scikit-learn中已经有现成的实现可以使用：
- en: '[PRE35]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After executing the preceding code, we should see the decision regions formed
    by the ensemble of trees in the random forest, as shown in the following plot:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后，我们应该会看到由随机森林中树的集成所形成的决策区域，如下图所示：
- en: '![](img/B13208_03_23.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_23.png)'
- en: Using the preceding code, we trained a random forest from 25 decision trees
    via the `n_estimators` parameter and used the Gini impurity measure as a criterion
    to split the nodes. Although we are growing a very small random forest from a
    very small training dataset, we used the `n_jobs` parameter for demonstration
    purposes, which allows us to parallelize the model training using multiple cores
    of our computer (here, two cores).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们通过`n_estimators`参数从25棵决策树训练了一个随机森林，并使用Gini不纯度作为标准来划分节点。尽管我们从一个非常小的训练数据集生长了一个非常小的随机森林，但为了演示的目的，我们使用了`n_jobs`参数，这允许我们使用计算机的多个核心（这里是两个核心）来并行化模型训练。
- en: K-nearest neighbors – a lazy learning algorithm
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K近邻 – 一种惰性学习算法
- en: The last supervised learning algorithm that we want to discuss in this chapter
    is the **k-nearest neighbor** (**KNN**) classifier, which is particularly interesting
    because it is fundamentally different from the learning algorithms that we have
    discussed so far.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们想讨论的最后一个监督学习算法是**k近邻**（**KNN**）分类器，它尤其有趣，因为它在本质上与我们迄今为止讨论的学习算法有所不同。
- en: KNN is a typical example of a **lazy learner**. It is called "lazy" not because
    of its apparent simplicity, but because it doesn't learn a discriminative function
    from the training data but memorizes the training dataset instead.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是**惰性学习者**的典型例子。它之所以被称为“惰性”，并不是因为它显得简单，而是因为它并不从训练数据中学习一个区分性函数，而是记住了训练数据集。
- en: '**Parametric versus nonparametric models**'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数模型与非参数模型**'
- en: Machine learning algorithms can be grouped into **parametric** and **nonparametric**
    models. Using parametric models, we estimate parameters from the training dataset
    to learn a function that can classify new data points without requiring the original
    training dataset anymore. Typical examples of parametric models are the perceptron,
    logistic regression, and the linear SVM. In contrast, nonparametric models can't
    be characterized by a fixed set of parameters, and the number of parameters grows
    with the training data. Two examples of nonparametric models that we have seen
    so far are the decision tree classifier/random forest and the kernel SVM.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以分为**参数模型**和**非参数模型**。使用参数模型时，我们通过训练数据集来估计参数，从而学习一个能够分类新数据点的函数，且不再需要原始的训练数据集。参数模型的典型例子有感知机、逻辑回归和线性支持向量机（SVM）。与之相对，非参数模型无法通过一组固定的参数来描述，且参数的数量会随着训练数据的增加而增长。到目前为止，我们所见的两个非参数模型的例子是决策树分类器/随机森林和核SVM。
- en: KNN belongs to a subcategory of nonparametric models that is described as **instance-based
    learning**. Models based on instance-based learning are characterized by memorizing
    the training dataset, and lazy learning is a special case of instance-based learning
    that is associated with no (zero) cost during the learning process.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: KNN属于非参数模型中的一个子类，称为**基于实例的学习**。基于实例的学习模型的特点是记住训练数据集，而惰性学习则是基于实例学习的一个特殊情况，它在学习过程中没有（零）成本。
- en: 'The KNN algorithm itself is fairly straightforward and can be summarized by
    the following steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: KNN算法本身相对简单，可以通过以下步骤总结：
- en: Choose the number of *k* and a distance metric.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择* k *的值和一个距离度量。
- en: Find the *k*-nearest neighbors of the data record that we want to classify.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到我们想要分类的数据记录的* k *个最近邻。
- en: Assign the class label by majority vote.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多数投票分配类别标签。
- en: The following figure illustrates how a new data point (**?**) is assigned the
    triangle class label based on majority voting among its five nearest neighbors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了如何通过多数投票的方式，根据其五个最近邻的类别，将一个新的数据点（**?**）分配到三角形类别标签。
- en: '![](img/B13208_03_24.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_24.png)'
- en: Based on the chosen distance metric, the KNN algorithm finds the *k* examples
    in the training dataset that are closest (most similar) to the point that we want
    to classify. The class label of the data point is then determined by a majority
    vote among its *k* nearest neighbors.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 基于所选的距离度量，KNN算法会找到训练数据集中与我们要分类的点最接近（最相似）的* k *个样本。然后，数据点的类别标签是通过它的* k *个最近邻的多数投票来确定的。
- en: 'The main advantage of such a memory-based approach is that the classifier immediately
    adapts as we collect new training data. However, the downside is that the computational
    complexity for classifying new examples grows linearly with the number of examples
    in the training dataset in the worst-case scenario—unless the dataset has very
    few dimensions (features) and the algorithm has been implemented using efficient
    data structures such as k-d trees (*An Algorithm for Finding Best Matches in Logarithmic
    Expected Time*, *J. H. Friedman*, *J. L. Bentley*, and *R.A. Finkel*, *ACM transactions
    on mathematical software* (*TOMS*), *3(3): 209–226*, *1977*). Furthermore, we
    can''t discard training examples since no training step is involved. Thus, storage
    space can become a challenge if we are working with large datasets.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '这种基于记忆的方法的主要优点是，当我们收集到新的训练数据时，分类器能够立即适应。然而，缺点是，在最坏的情况下，分类新示例的计算复杂度随着训练数据集中示例数量的增加而线性增长——除非数据集的维度（特征）非常少，且算法已使用高效的数据结构如k-d树（*An
    Algorithm for Finding Best Matches in Logarithmic Expected Time*，*J. H. Friedman*，*J.
    L. Bentley*，和*R.A. Finkel*，*ACM transactions on mathematical software* (*TOMS*)，*3(3):
    209–226*，*1977*）来实现。此外，由于不涉及训练步骤，我们无法丢弃训练示例。因此，如果我们处理的是大数据集，存储空间可能会成为一个挑战。'
- en: 'By executing the following code, we will now implement a KNN model in scikit-learn
    using a Euclidean distance metric:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下代码，我们将使用欧几里得距离度量在scikit-learn中实现KNN模型：
- en: '[PRE36]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'By specifying five neighbors in the KNN model for this dataset, we obtain a
    relatively smooth decision boundary, as shown in the following plot:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集的KNN模型中，我们通过指定五个邻居，获得了一个相对平滑的决策边界，如下图所示：
- en: '![](img/B13208_03_25.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_25.png)'
- en: '**Resolving ties**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决平局**'
- en: In the case of a tie, the scikit-learn implementation of the KNN algorithm will
    prefer the neighbors with a closer distance to the data record to be classified.
    If the neighbors have similar distances, the algorithm will choose the class label
    that comes first in the training dataset.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在平局情况下，scikit-learn实现的KNN算法会优先选择距离数据记录较近的邻居进行分类。如果邻居之间的距离相似，算法将选择训练数据集中出现的第一个类别标签。
- en: 'The *right* choice of *k* is crucial to finding a good balance between overfitting
    and underfitting. We also have to make sure that we choose a distance metric that
    is appropriate for the features in the dataset. Often, a simple Euclidean distance
    measure is used for real-value examples, for example, the flowers in our Iris
    dataset, which have features measured in centimeters. However, if we are using
    a Euclidean distance measure, it is also important to standardize the data so
    that each feature contributes equally to the distance. The `minkowski` distance
    that we used in the previous code is just a generalization of the Euclidean and
    Manhattan distance, which can be written as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 选择*正确的* *k*值对于找到过拟合和欠拟合之间的良好平衡至关重要。我们还必须确保选择适合数据集特征的距离度量。通常，对于实值示例（例如，我们的Iris数据集中的花卉，它们的特征是以厘米为单位测量的），会使用简单的欧几里得距离度量。然而，如果我们使用欧几里得距离度量，那么标准化数据也非常重要，这样每个特征对距离的贡献才能平等。我们在之前的代码中使用的`minkowski`距离就是欧几里得和曼哈顿距离的一个推广，可以写作如下：
- en: '![](img/B13208_03_134.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_03_134.png)'
- en: It becomes the Euclidean distance if we set the parameter `p=2` or the Manhattan
    distance at `p=1`. Many other distance metrics are available in scikit-learn and
    can be provided to the `metric` parameter. They are listed at [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将参数`p=2`，则变为欧几里得距离，`p=1`时则是曼哈顿距离。scikit-learn中还有许多其他的距离度量，可以提供给`metric`参数使用。它们的详细列表可以在[http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)查看。
- en: '**The curse of dimensionality**'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '**维度灾难**'
- en: It is important to mention that KNN is very susceptible to overfitting due to
    the **curse of dimensionality**. The curse of dimensionality describes the phenomenon
    where the feature space becomes increasingly sparse for an increasing number of
    dimensions of a fixed-size training dataset. We can think of even the closest
    neighbors as being too far away in a high-dimensional space to give a good estimate.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，KNN非常容易受到过拟合的影响，这是由于**维度灾难**所致。维度灾难描述的是在固定大小的训练数据集维度数增加时，特征空间变得越来越稀疏的现象。我们甚至可以认为在高维空间中，最接近的邻居也会因距离过远而无法提供好的估计。
- en: We discussed the concept of regularization in the section about logistic regression
    as one way to avoid overfitting. However, in models where regularization is not
    applicable, such as decision trees and KNN, we can use feature selection and dimensionality
    reduction techniques to help us to avoid the curse of dimensionality. This will
    be discussed in more detail in the next chapter.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在关于逻辑回归的章节中讨论了正则化的概念，作为避免过拟合的一种方法。然而，在正则化无法应用的模型中，如决策树和KNN，我们可以使用特征选择和降维技术来帮助我们避免维度灾难。这个内容将在下一章中详细讨论。
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about many different machine learning algorithms
    that are used to tackle linear and nonlinear problems. You have seen that decision
    trees are particularly attractive if we care about interpretability. Logistic
    regression is not only a useful model for online learning via SGD, but also allows
    us to predict the probability of a particular event.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了许多不同的机器学习算法，这些算法用于解决线性和非线性问题。你已经看到，决策树特别适合我们关注可解释性的情况。逻辑回归不仅是通过SGD进行在线学习的有用模型，而且还允许我们预测某个特定事件的概率。
- en: Although SVMs are powerful linear models that can be extended to nonlinear problems
    via the kernel trick, they have many parameters that have to be tuned in order
    to make good predictions. In contrast, ensemble methods, such as random forests,
    don't require much parameter tuning and don't overfit as easily as decision trees,
    which makes them attractive models for many practical problem domains. The KNN
    classifier offers an alternative approach to classification via lazy learning
    that allows us to make predictions without any model training, but with a more
    computationally expensive prediction step.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SVM是强大的线性模型，可以通过核技巧扩展到非线性问题，但它们有许多参数需要调优才能做出良好的预测。相比之下，集成方法如随机森林不需要太多的参数调优，且不像决策树那样容易过拟合，这使得它们在许多实际问题领域中成为有吸引力的模型。KNN分类器提供了一种通过惰性学习进行分类的替代方法，允许我们在没有任何模型训练的情况下进行预测，但预测步骤更具计算开销。
- en: However, even more important than the choice of an appropriate learning algorithm
    is the available data in our training dataset. No algorithm will be able to make
    good predictions without informative and discriminatory features.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，比选择合适的学习算法更重要的是我们训练数据集中的可用数据。没有具有信息性和区分性的特征，任何算法都无法做出良好的预测。
- en: In the next chapter, we will discuss important topics regarding the preprocessing
    of data, feature selection, and dimensionality reduction, which means that we
    will need to build powerful machine learning models. Later, in *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we will see how
    we can evaluate and compare the performance of our models and learn useful tricks
    to fine-tune the different algorithms.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论数据预处理、特征选择和降维的相关重要话题，这意味着我们将需要构建强大的机器学习模型。稍后，在*第6章*，*模型评估与超参数调优的最佳实践*中，我们将了解如何评估和比较模型的性能，并学习一些有用的技巧来微调不同的算法。
