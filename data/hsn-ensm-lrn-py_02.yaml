- en: A Machine Learning Refresher
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习复习
- en: Machine learning is a sub field of **artificial intelligence** (**AI**) focused
    on the aim of developing algorithms and techniques that enable computers to learn
    from massive amounts of data. Given the increasing rate at which data is produced,
    machine learning has played a critical role in solving difficult problems in recent
    years. This success was the main driving force behind the funding and development
    of many great machine learning libraries that make use of data in order to build
    predictive models. Furthermore, businesses have started to realize the potential
    of machine learning, driving the demand for data scientists and machine learning
    engineers to new heights, in order to design better-performing predictive models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是**人工智能**（**AI**）的一个子领域，致力于开发使计算机能够从海量数据中学习的算法和技术。随着数据产生速度的不断增加，机器学习在近年来在解决复杂问题中发挥了关键作用。这一成功是许多优秀机器学习库的资金支持和发展的主要推动力，这些库利用数据来构建预测模型。此外，企业也开始意识到机器学习的潜力，推动了对数据科学家和机器学习工程师的需求飙升，以设计性能更优的预测模型。
- en: This chapter serves as a refresher on the main concepts and terminology, as
    well as an introduction to the frameworks that will be used throughout the book,
    in order to approach ensemble learning with a solid foundation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在复习主要概念和术语，并介绍将在本书中使用的框架，以便以扎实的基础接触集成学习。
- en: 'The main topics covered in this chapter are the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章覆盖的主要内容如下：
- en: The various machine learning problems and datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种机器学习问题和数据集
- en: How to evaluate the performance of a predictive model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估预测模型的性能
- en: Machine learning algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: Python environment setup and the required libraries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 环境设置及所需库
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解 Python 语法和约定。最后，熟悉 NumPy 库将极大帮助读者理解一些自定义算法的实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter01)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/30u8sv8](http://bit.ly/30u8sv8).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，观看代码实例：[http://bit.ly/30u8sv8](http://bit.ly/30u8sv8)。
- en: Learning from data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: Data is the raw ingredient of machine learning. Processing data can produce
    information; for example, measuring the height of a portion of a school's students
    (data) and calculating their average (processing) can give us an idea of the whole
    school's height (information). If we process the data further, for example, by
    grouping males and females and calculating two averages – one for each group,
    we will gain more information, as we will have an idea about the average height
    of the school's males and females. Machine learning strives to produce the most
    information possible from any given data. In this example, we produced a very
    basic predictive model. By calculating the two averages, we can predict the average
    height of any student just by knowing whether the student is male or female.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是机器学习的原材料。处理数据可以生成信息；例如，测量一部分学生的身高（数据），并计算他们的平均值（处理），可以帮助我们了解整个学校的身高（信息）。如果我们进一步处理数据，例如，将男性和女性分组并计算每个组的平均值，我们将获得更多信息，因为我们将能够知道学校男性和女性的平均身高。机器学习旨在从任何给定的数据中提取尽可能多的信息。在这个例子中，我们生成了一个非常基础的预测模型。通过计算两个平均值，我们只需知道学生是男性还是女性，就能预测任何学生的平均身高。
- en: The set of data that a machine learning algorithm is tasked with processing
    is called the problem's dataset. In our example, the dataset consists of height
    measurements (in centimeters) and the child's sex (male/female). In machine learning,
    input variables are called features and output variables are called targets. In
    this dataset, the features of our predictive model consist solely of the students'
    sex, while our target is the students' height in centimeters. The predictive model
    that is produced and maps features to targets will be referred to as simply the model from
    now on, unless otherwise specified. Each data point is called an instance. In
    this problem, each student is an instance of the dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法处理的数据集合被称为问题的数据集。在我们的例子中，数据集包括身高测量（单位为厘米）和孩子的性别（男性/女性）。在机器学习中，输入变量称为特征，输出变量称为目标。在这个数据集中，我们预测模型的特征仅由学生的性别组成，而我们的目标是学生的身高（以厘米为单位）。从现在开始，除非另有说明，否则生成的预测模型将简单地称为**模型**。每个数据点称为一个实例。在这个问题中，每个学生都是数据集的一个实例。
- en: When the target is a continuous variable (a number), it presents a regression
    problem, as the aim is to regress the target on the features. When the target
    is a set of categories, it presents a classification problem, as we try to assign
    each instance to a category or class.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标是一个连续变量（即数字）时，问题是回归问题，因为目标是根据特征进行回归。当目标是一个类别集时，问题是分类问题，因为我们试图将每个实例分配到一个类别或类中。
- en: Note that, in classification problems, the target class can be represented by
    a number; this does not mean that it is a regression problem. The most useful
    way to determine whether it is a regression problem is to think about whether
    the instances can be ordered by their targets. In our example, the target is height,
    so we can order the students from tallest to shortest, as 100 cm is less than
    110 cm. As a counter example, if the target was their favorite color, we could
    represent each color by a number, but we could not order them. Even if we represented
    red as one and blue as two, we could not say that red is "before" or "less than"
    blue. Thus, this counter example is a classification problem.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，目标类可以用一个数字表示；这并不意味着它是回归问题。判断是否为回归问题的最有用的方法是思考是否可以通过目标对实例进行排序。在我们的例子中，目标是身高，因此我们可以将学生按身高从高到低排序，因为100厘米小于110厘米。举个反例，如果目标是他们最喜欢的颜色，我们可以用数字表示每种颜色，但无法对其进行排序。即使我们把红色表示为一，蓝色表示为二，我们也不能说红色是“在前”或“比”蓝色小。因此，这个反例是一个分类问题。
- en: Popular machine learning datasets
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行的机器学习数据集
- en: Machine learning relies on data in order to produce high-performing models.
    Without data, it's not even possible to create models. In this section, we'll
    present some popular machine learning datasets, which we will utilize throughout
    this book.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习依赖于数据来生成高性能模型。没有数据，甚至无法创建模型。在本节中，我们将介绍一些流行的机器学习数据集，我们将在本书中使用这些数据集。
- en: Diabetes
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 糖尿病
- en: The diabetes dataset concerns 442 individual diabetes patients and the progression
    of the disease one year after a baseline measurement. The dataset consists of
    10 features, which are the patient's age, sex, **body mass index** (**bmi**),
    average **blood pressure** (**bp**), and six measurements of their blood serum.
    The dataset target is the progression of the disease one year after the baseline
    measurement. This is a regression dataset, as the target is a number.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病数据集涉及442名糖尿病患者及其在基准测量后一年内病情的进展。数据集包括10个特征，包括患者的年龄、性别、**体重指数**（**bmi**）、平均**血压**（**bp**）以及六个血清测量值。数据集的目标是基准测量后一年内病情的进展。这是一个回归数据集，因为目标是一个数字。
- en: 'In this book, the dataset features are mean-centered and scaled such that the
    dataset sum of squares for each feature equals one. The following table depicts
    a sample of the diabetes dataset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，数据集的**特征**已经进行了均值中心化和缩放处理，使得每个特征的数据集平方和等于1。下表展示了糖尿病数据集的一个样本：
- en: '| **age** | **sex** | **bmi** | **bp** | **s1** | **s2** | **s3** | **s4**
    | **s5** | **s6** | **target** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **性别** | **bmi** | **血压** | **s1** | **s2** | **s3** | **s4** |
    **s5** | **s6** | **目标** |'
- en: '| 0.04 | 0.05 | 0.06 | 0.02 | -0.04 | -0.03 | -0.04 | 0.00 | 0.02 | -0.02 |
    151 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 0.04 | 0.05 | 0.06 | 0.02 | -0.04 | -0.03 | -0.04 | 0.00 | 0.02 | -0.02 |
    151 |'
- en: '| 0.00 | -0.04 | -0.05 | -0.03 | -0.01 | -0.02 | 0.07 | -0.04 | -0.07 | -0.09
    | 75 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 0.00 | -0.04 | -0.05 | -0.03 | -0.01 | -0.02 | 0.07 | -0.04 | -0.07 | -0.09
    | 75 |'
- en: '| 0.09 | 0.05 | 0.04 | -0.01 | -0.05 | -0.03 | -0.03 | 0.00 | 0.00 | -0.03
    | 141 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 0.09 | 0.05 | 0.04 | -0.01 | -0.05 | -0.03 | -0.03 | 0.00 | 0.00 | -0.03
    | 141 |'
- en: '| -0.09 | -0.04 | -0.01 | -0.04 | 0.01 | 0.02 | -0.04 | 0.03 | 0.02 | -0.01
    | 206 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| -0.09 | -0.04 | -0.01 | -0.04 | 0.01 | 0.02 | -0.04 | 0.03 | 0.02 | -0.01
    | 206 |'
- en: Breast cancer
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 乳腺癌
- en: The breast cancer dataset concerns 569 biopsies of malignant and benign tumors.
    The dataset provides 30 features extracted from images of fine-needle aspiration
    biopsies that describe cell nuclei. The images provide information about the shape,
    size, and texture of each cell nucleus. Furthermore, for each characteristic,
    three distinct values are provided. The mean, the standard error, and the worst
    or largest value. This ensures that, for each image, the cell population is adequately
    described.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集涉及 569 例恶性和良性肿瘤的活检。该数据集提供了从细针穿刺活检图像中提取的 30 个特征，这些特征描述了细胞核的形状、大小和纹理。此外，对于每个特征，还提供了三个不同的值：均值、标准误差以及最差或最大值。这确保了每个图像中的细胞群体得到充分描述。
- en: 'The dataset target concerns the diagnosis, that is, whether a tumor is malignant
    or benign. Thus, this is a classification dataset. The available features are
    listed as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的目标是诊断，即肿瘤是恶性还是良性。因此，这是一个分类数据集。可用的特征如下所列：
- en: Mean radius
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均半径
- en: Mean texture
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均纹理
- en: Mean perimeter
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均周长
- en: Mean area
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均面积
- en: Mean smoothness
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均平滑度
- en: Mean compactness
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均紧密度
- en: Mean concavity
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均凹度
- en: Mean concave points
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均凹点
- en: Mean symmetry
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均对称性
- en: Mean fractal dimension
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均分形维度
- en: Radius error
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半径误差
- en: Texture error
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纹理误差
- en: Perimeter error
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周长误差
- en: Area error
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积误差
- en: Smoothness error
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑度误差
- en: Compactness error
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧密度误差
- en: Concavity error
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹度误差
- en: Concave points error
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹点误差
- en: Symmetry error
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称性误差
- en: Fractal dimension error
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分形维度误差
- en: Worst radius
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差半径
- en: Worst texture
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差纹理
- en: Worst perimeter
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差周长
- en: Worst area
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差面积
- en: Worst smoothness
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差平滑度
- en: Worst compactness
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差紧密度
- en: Worst concavity
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差凹度
- en: Worst concave points
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差凹点
- en: Worst symmetry
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差对称性
- en: Worst fractal dimension
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差分形维度
- en: Handwritten digits
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手写数字
- en: 'The MNIST handwritten digit dataset is one of the most famous image recognition
    datasets. It consists of square images, 8 x 8 pixels, each containing a single
    handwritten digit. Thus, the dataset features are an 8 by 8 matrix, containing
    each pixel''s color in grayscale. The target consists of 10 classes, one for each
    digit from 0 to 9\. This is a classification dataset. The following figure is
    a sample from the handwritten digit dataset:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 手写数字数据集是最著名的图像识别数据集之一。它由 8 x 8 像素的方形图像组成，每个图像包含一个手写数字。因此，数据集的特征是一个 8 x
    8 的矩阵，包含每个像素的灰度色值。目标包括 10 个类别，分别对应数字 0 到 9。这个数据集是一个分类数据集。下图是手写数字数据集中的一个样本：
- en: '![](img/67c53ed8-f82c-4835-9794-93a597511b7a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67c53ed8-f82c-4835-9794-93a597511b7a.png)'
- en: Sample of the handwritten digit dataset
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字数据集样本
- en: Supervised and unsupervised learning
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习与无监督学习
- en: Machine learning can be divided into many subcategories; two broad categories
    are supervised and unsupervised learning. These categories contain some of the
    most popular and widely used machine learning methods. In this section, we present
    them, as well as some toy example uses of supervised and unsupervised learning.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以分为多个子类别；其中两个大类是有监督学习和无监督学习。这些类别包含了许多流行且广泛应用的机器学习方法。在本节中，我们将介绍这些方法，并给出一些有监督学习和无监督学习的小例子。
- en: Supervised learning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: In examples such as those in the previous section, the data consisted of some
    features and a target; no matter whether the target was quantitative (regression)
    or categorical (classification). Under these circumstances, we call the dataset
    a labeled dataset. When we try to produce a model from a labeled dataset in order
    to make predictions about unseen or future data (for example, to diagnose a new
    tumor case), we make use of supervised learning. In simple cases, supervised learning
    models can be visualized as a line. This line's purpose is to either separate
    the data based on the target (in classification) or to closely follow the data
    (in regression).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的例子中，数据包含了一些特征和一个目标；无论目标是定量的（回归）还是分类的（分类）。在这种情况下，我们将数据集称为标注数据集。当我们尝试从标注数据集中生成一个模型，以便对看不见的或未来的数据做出预测（例如，诊断新的肿瘤病例）时，我们使用有监督学习。在简单的情况下，有监督学习模型可以被视为一条线。这条线的目的是根据目标（分类）将数据分开，或者紧密跟随数据（回归）。
- en: 'The following figure illustrates a simple regression example. Here, *y* is
    the target and *x* is the dataset feature. Our model consists of the simple equation
    *y*=2*x*-5\. As is evident, the line closely follows the data. In order to estimate
    the *y* value of a new unseen point, we calculate its value using the preceding
    formula. The following figure shows a simple regression with *y*=2*x*-5 as the
    predictive model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个简单的回归示例。在这里，*y*是目标，*x*是数据集特征。我们的模型由简单的方程*y*=2*x*-5组成。显然，直线紧密地跟随数据。为了估算一个新的、未见过的点的*y*值，我们使用上述公式计算其值。下图显示了一个使用*y*=2*x*-5作为预测模型的简单回归：
- en: '![](img/b1680c76-c8ad-4c1f-92f7-7091d33a1870.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1680c76-c8ad-4c1f-92f7-7091d33a1870.png)'
- en: Simple regression with y=2x-5 as the predictive model
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预测模型y=2x-5的简单回归
- en: 'In the following figure, a simple classification problem is depicted. Here,
    the dataset features are *x* and *y*, while the target is the instance color.
    Again, the dotted line is *y*=2*x*-5, but this time we test whether the point
    is above or below the line. If the point''s *y* value is lower than expected (smaller),
    then we expect it to be orange. If it is higher (greater), we expect it to be
    blue. The following figure is a simple classification with *y*=2*x*-5 as the boundary:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，展示了一个简单的分类问题。在这里，数据集特征是*x*和*y*，目标是实例的颜色。再次，虚线是*y*=2*x*-5，但这次我们测试点是位于线的上方还是下方。如果该点的*y*值低于预期（较小），我们期望它是橙色的。如果它较高（较大），我们期望它是蓝色的。下图是使用*y*=2*x*-5作为边界的简单分类：
- en: '![](img/210bb1bf-db70-4be2-ba9c-a344b16a1bf7.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/210bb1bf-db70-4be2-ba9c-a344b16a1bf7.png)'
- en: Simple classification with y=2x-5 as boundary
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用y=2x-5作为边界的简单分类
- en: Unsupervised learning
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In both regression and classification, we have a clear understanding of how
    the data is structured or how it behaves. Our goal is to simply model that structure
    or behavior. In some cases, we do not know how the data is structured. In those
    cases, we can utilize unsupervised learning in order to discover the structure,
    and thus information, within the data. The simplest form of unsupervised learning
    is clustering. As the name implies, clustering techniques attempt to group (or
    cluster) data instances. Thus, instances that belong to the same cluster share
    many similarities in their features, while they are dissimilar to instances that
    belong in separate clusters. A simple example with three clusters is depicted
    in the following figure. Here, the dataset features are *x* and *y*, while there
    is no target.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归和分类中，我们清楚地理解数据的结构或行为。我们的目标只是对该结构或行为进行建模。在某些情况下，我们不知道数据的结构。在这些情况下，我们可以利用无监督学习来发现数据中的结构，从而获得信息。无监督学习的最简单形式是聚类。顾名思义，聚类技术尝试将数据实例分组（或聚类）。因此，属于同一聚类的实例在特征上有很多相似之处，而与属于不同聚类的实例则有很大差异。下图展示了一个具有三个聚类的简单例子。在这里，数据集特征是*x*和*y*，没有目标。
- en: 'The clustering algorithm discovered three distinct groups, centered around
    the points (0, 0), (1, 1), and (2, 2):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法发现了三个不同的组，分别以点(0, 0)、(1, 1)和(2, 2)为中心：
- en: '![](img/c0ab1932-b0fd-4959-b37a-583d2257a464.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab1932-b0fd-4959-b37a-583d2257a464.png)'
- en: Clustering with three distinct groups
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 具有三个不同组的聚类
- en: Dimensionality reduction
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Another form of unsupervised learning is dimensionality reduction. The number
    of features present in a dataset equals the dataset's dimensions. Often, many
    features can be correlated, noisy, or simply not provide much information. Nonetheless,
    the cost of storing and processing data is correlated with a dataset's dimensionality.
    Thus, by reducing the dataset's dimensions, we can help the algorithms to better
    model the data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种无监督学习的形式是降维。数据集中存在的特征数量等于数据集的维度。通常，许多特征可能是相关的、有噪声的，或者根本没有提供太多信息。然而，存储和处理数据的成本与数据集的维度是相关的。因此，通过减少数据集的维度，我们可以帮助算法更好地建模数据。
- en: Another use of dimensionality reduction is for the visualization of high-dimensional
    datasets. For example, using the t-distributed Stochastic Neighbor Embedding (t-SNE)
    algorithm, we can reduce the breast cancer dataset to two dimensions or components.
    Although it is not easy to visualize 30 dimensions, it is quite easy to visualize
    two.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的另一个用途是高维数据集的可视化。例如，使用t-分布随机邻居嵌入（t-SNE）算法，我们可以将乳腺癌数据集降至两个维度或组件。尽管可视化30个维度并不容易，但可视化两个维度却相当简单。
- en: 'Furthermore, we can visually test whether the information contained within
    the dataset can be utilized to separate the dataset''s classes or not. The next
    figure depicts the two components on the *y* and *x* axis, while the color represents
    the instance''s class. Although we cannot plot all of the dimensions, by plotting
    the two components, we can conclude that a degree of separability between the
    classes exists:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以通过可视化测试数据集中包含的信息是否能够用来区分数据集的各个类别。下图展示了*y*轴和*x*轴上的两个组件，而颜色则代表实例的类别。尽管我们无法绘制所有维度，但通过绘制这两个组件，我们可以得出类之间存在一定可分性的结论：
- en: '![](img/a007fc1c-de3b-4a3f-9f7f-afeea4c6546e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a007fc1c-de3b-4a3f-9f7f-afeea4c6546e.png)'
- en: Using t-SNE to reduce the dimensionality of the breast cancer dataset
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用t-SNE降维乳腺癌数据集
- en: Performance measures
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: Machine learning is a highly quantitative field. Although we can gauge the performance
    of a model by plotting how it separates classes and how closely it follows data,
    more quantitative performance measures are needed in order to evaluate models.
    In this section, we present cost functions and metrics. Both of them are used
    in order to assess a model's performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个高度定量的领域。虽然我们可以通过绘制模型如何区分类别以及它如何紧密地跟随数据来衡量模型的性能，但为了评估模型的表现，还需要更多的定量性能指标。在本节中，我们将介绍代价函数和评估指标。它们都用于评估模型的表现。
- en: Cost functions
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代价函数
- en: A machine learning model's objective is to model our dataset. In order to assess
    each model's performance, we define an objective function. These functions usually
    express a cost, or how far from perfect a model is. These cost functions usually
    utilize a loss function to assess how well the model performed on each individual
    dataset instance.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的目标是对我们的数据集进行建模。为了评估每个模型的表现，我们定义了目标函数。这些函数通常表示一个代价，或者说是模型距离完美的程度。这些代价函数通常使用损失函数来评估模型在每个单独数据实例上的表现。
- en: Some of the most widely used cost functions are described in the following sections,
    assuming that the dataset has *n* instances, the target's true value for instance *i* is *t[i]* and
    the model's output is *y[i .]*
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节描述了一些最常用的代价函数，假设数据集有*n*个实例，实例*i*的目标真实值为*t[i]*，而模型的输出为*y[i]*。
- en: Mean absolute error
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均绝对误差
- en: '**Mean absolute error** (**MAE**) or L1 loss is the mean absolute distance
    between the target''s real values and the model''s outputs. It is calculated as
    follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）或L1损失是目标真实值与模型输出之间的均绝对距离。它的计算公式如下：'
- en: '![](img/e08a7ef6-713b-408c-af9f-ae0a55486bc5.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e08a7ef6-713b-408c-af9f-ae0a55486bc5.png)'
- en: Mean squared error
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: '**Mean squared error** (**MSE**) or L2 loss is the mean squared distance between
    the target''s real values and the model''s output. It is calculated as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）或L2损失是目标真实值与模型输出之间的均方距离。它的计算公式如下：'
- en: '![](img/3233ee2b-4852-4c38-baeb-1c1482ac59b7.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3233ee2b-4852-4c38-baeb-1c1482ac59b7.png)'
- en: Cross entropy loss
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵损失
- en: 'Cross entropy loss is used in models that output probabilities between 0 and
    1, usually to express the probability that an instance is a member of a specific
    class. As the output probability diverges from the actual label, the loss increases.
    For a simple case where the dataset consists of two classes, it is calculated
    as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失用于输出介于0和1之间的概率的模型，通常用来表示一个实例属于特定类别的概率。当输出概率偏离实际标签时，损失值会增加。在一个简单的例子中，假设数据集由两个类别组成，计算方法如下：
- en: '![](img/ca72266c-12e1-4d41-828d-049d20b0a341.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca72266c-12e1-4d41-828d-049d20b0a341.png)'
- en: Metrics
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标
- en: Cost functions are useful when we try to numerically optimize our models. But
    as humans, we need metrics that are useful and intuitive to understand and report.
    As such, there are a number of metrics available that give insight into a model's
    performance. The most common metrics are presented in the following sections.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代价函数在我们尝试通过数值优化模型时非常有用。但作为人类，我们需要一些既有用又直观的指标来理解和报告。因此，有许多可以提供模型性能洞察的指标。以下几节将介绍最常用的指标。
- en: Classification accuracy
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类准确率
- en: 'The simplest and easiest to grasp of all, classification accuracy refers to
    the percentage of correct predictions. In order to calculate accuracy, we divide
    the number of correct predictions by the total number of instances:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所有指标中最简单且最容易掌握的分类准确率，指的是正确预测的百分比。为了计算准确率，我们将正确预测的数量除以总实例数：
- en: '![](img/be71cf1e-4842-4b5c-888f-7226840eb631.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be71cf1e-4842-4b5c-888f-7226840eb631.png)'
- en: In order for accuracy to hold any substantial value, the dataset must contain
    an equal number of instances belonging to each class. If the dataset is unbalanced,
    accuracy will be affected. For example, if a dataset consists of 90% class A and
    10% class B, a model that predicts each instance as class A will have 90% accuracy,
    although it will hold zero predictive power.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使准确率具有实质性的意义，数据集必须包含相等数量的属于每个类别的实例。如果数据集不平衡，准确率将受到影响。例如，如果一个数据集由90%的A类和10%的B类组成，那么一个将每个实例预测为A类的模型将有90%的准确率，尽管它的预测能力为零。
- en: Confusion matrix
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'In order to tackle the preceding problem, it is possible to utilize a confusion
    matrix. Confusion matrices present the number of instances correctly or incorrectly
    predicted as each possible class. In a dataset with only two classes (Yes and
    No), a confusion matrix has the following form:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前面的问题，可以使用混淆矩阵。混淆矩阵呈现正确或错误预测为每个可能类别的实例数量。在只有两个类别（“是”和“否”）的数据集中，混淆矩阵具有以下形式：
- en: '| **n = 200** | **Predicted: Yes** | **Predicted: No** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **n = 200** | **预测: 是** | **预测: 否** |'
- en: '| **Target: Yes** | 80 | 70 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **目标: 是** | 80 | 70 |'
- en: '| **Target: No** | 20 | 30 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **目标: 否** | 20 | 30 |'
- en: 'There are four cells, each corresponding to one of the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有四个单元格，每个对应以下之一：
- en: '**True Positives** (**TP**): When the target belongs to the Yes class and the
    model predicted Yes'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例** (**TP**): 当目标属于“是”类别且模型预测为“是”'
- en: '**True Negatives** (**TN**): When the target belongs to the No class and the
    model predicted No'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例** (**TN**): 当目标属于“否”类别且模型预测为“否”'
- en: '**False Positives** (**FP**): When the target belongs to the No class and the
    model predicted Yes'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例** (**FP**): 当目标属于“否”类别且模型预测为“是”'
- en: '**False Negatives** (**FN**): When the target belongs to the Yes class and
    the model predicted No'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例** (**FN**): 当目标属于“是”类别且模型预测为“否”'
- en: 'Confusion matrices provide information about the balance of the true and predicted
    classes. In order to calculate the accuracy from a confusion matrix, we divide
    the sum of TP and TN by the total number of instances:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵提供有关真实类别和预测类别平衡的信息。为了从混淆矩阵中计算准确率，我们将TP和TN的总和除以实例的总数：
- en: '![](img/56ea3bcb-3940-47ff-bae2-6699fa045bf1.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ea3bcb-3940-47ff-bae2-6699fa045bf1.png)'
- en: Sensitivity, specificity, and area under the curve
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灵敏度、特异性和曲线下面积
- en: 'Area under the curve (AUC) concerns binary classification datasets, and it
    depicts the probability that the model will rank any given instance correctly.
    In order to define it, we must first define sensitivity and specificity:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线下面积（AUC）关注二分类数据集，它描述了模型正确排名任何给定实例的概率。为了定义AUC，我们首先必须定义灵敏度和特异性：
- en: '**Sensitivity** (**True Positive Rate**): Sensitivity is the percentage of
    positive instances correctly predicted as positive, relative to all positive instances.
    It is calculated as follows:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵敏度** (**真正例率**): 灵敏度是指相对于所有正例，正确预测为正例的正例比例。其计算方法如下：'
- en: '![](img/44038c3e-a8c4-466f-8c94-001702317038.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44038c3e-a8c4-466f-8c94-001702317038.png)'
- en: '**Specificity** (**False Positive Rate**): Specificity is the percentage of
    negative instances incorrectly predicted as positive, relative to all negative
    instances. It is calculated as follows:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性** (**假正例率**): 特异性是指相对于所有负例，错误预测为正例的负例比例。其计算方法如下：'
- en: '![](img/f1bd2469-3805-4932-86c7-e075947d82ce.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1bd2469-3805-4932-86c7-e075947d82ce.png)'
- en: 'By iteratively computing (1-specificity) and sensitivity at specific intervals
    (for example, in 0.05 increments), we can see how the model behaves. The intervals
    concern the model''s output probability for each instance; for example, we first
    compute them for all instances with an estimated probability of belonging to the
    Yes class of less than 0.05\. Then, we re-compute for all instances with an estimated
    probability of less than 0.1 and so on. The result is depicted here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在特定的间隔（例如，每0.05增加一次）计算（1-特异性）和灵敏度，我们可以观察模型的表现。间隔与模型对每个实例的输出概率相关；例如，首先我们计算所有估计属于“是”类别的概率小于0.05的实例。然后，重新计算所有估计概率小于0.1的实例，以此类推。结果如下所示：
- en: '![](img/289e210c-25ec-47c6-a2be-a45556c5018e.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/289e210c-25ec-47c6-a2be-a45556c5018e.png)'
- en: Receiver operator characteristic curve
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线
- en: 'The straight line represents an equal probability of ranking an instance correctly
    or incorrectly: a random model. The orange line (ROC curve) depicts the model''s
    probability. If the ROC curve is below the straight line, it means that the model
    performs worse than a random, uninformed model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 直线表示正确或错误地对实例进行排名的概率相等：一个随机模型。橙色线（ROC曲线）描绘了模型的概率。如果ROC曲线位于直线下方，意味着模型的表现比随机的、没有信息的模型差。
- en: Precision, recall, and the F1 score
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确度、召回率和F1分数
- en: 'Precision gauges how a model behaves by quantifying the percentage of instances
    correctly classified as a specific class, relative to all instances predicted
    as the same class. It is calculated as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度衡量模型的表现，通过量化正确分类为特定类别的实例的百分比，相对于所有预测为同一类别的实例。计算公式如下：
- en: '![](img/996f726a-e333-4ba7-b981-460abc3ff77a.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/996f726a-e333-4ba7-b981-460abc3ff77a.png)'
- en: 'Recall is another name for sensitivity. The harmonic mean of precision and
    recall is called the F1 score and is calculated as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率也叫做敏感度。精确度和召回率的调和平均数称为F1分数，计算公式如下：
- en: '![](img/89a6f1fc-1249-429a-802a-e77de45ad214.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89a6f1fc-1249-429a-802a-e77de45ad214.png)'
- en: The reason to use the harmonic mean instead of a simple average is that the
    harmonic mean is greatly affected by imbalances between the two values (precision
    and recall). Thus, if either precision or recall is significantly smaller than
    the other, the F1 score will reflect this imbalance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用调和平均数而不是简单平均数的原因是，调和平均数受两者（精确度和召回率）之间不平衡的影响很大。因此，如果精确度或召回率显著小于另一个，F1分数将反映这种不平衡。
- en: Evaluating models
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: Although there are various metrics that indicate a model's performance, it is
    important to carefully set the testing environment. One of the most important
    things is to split the dataset into two parts. One part of the dataset will be
    utilized by the algorithm in order to generate a model; the second part will be
    utilized to assess the model. These are usually called the train and test set.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有各种各样的指标来表明模型的表现，但仔细设置测试环境是非常重要的。最重要的事情之一是将数据集分成两部分。数据集的一部分将由算法用于生成模型；另一部分将用于评估模型。这通常被称为训练集和测试集。
- en: The train set is available to the algorithm to generate and optimize a model,
    using any cost function. After the algorithm is finished, the produced model is
    tested on the test set, in order to assess its predictive ability on unseen data.
    While the algorithm may produce a model that performs well on the train set (in-sample
    performance), it may not be able to generalize and perform as well on the test
    set (out-of-sample performance). This can be attributed to many factors – covered
    in the next chapter. Some of the problems that arise can be tackled with the use
    of ensembles. Nonetheless, if the algorithm is presented with low-quality data,
    there is little that can be done to improve out-of-sample performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集可以被算法用来生成和优化模型，使用任何代价函数。算法完成后，生成的模型将在测试集上进行测试，以评估其对未见数据的预测能力。虽然算法可能在训练集上生成表现良好的模型（样本内表现），但它可能无法泛化并在测试集上表现同样好（样本外表现）。这可以归因于许多因素——将在下一章中讨论。一些出现的问题可以通过使用集成方法来解决。然而，如果算法面对的是低质量的数据，几乎无法改进样本外的表现。
- en: In order to obtain a fair estimate, we sometimes iteratively split different
    parts of a dataset into fixed-size train and test sets, say, 90% train and 10%
    test, until we have tested the whole dataset. This is called K-fold cross validation.
    In the case of a 90% to 10% split, it is called 10-fold cross validation, because
    we need to perform it 10 times in order to get an estimate for the whole dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一个公正的估计，我们有时会将数据集的不同部分迭代地分成固定大小的训练集和测试集，比如90%的训练集和10%的测试集，直到对整个数据集进行了测试。这被称为K折交叉验证。在90%与10%的分割情况下，它被称为10折交叉验证，因为我们需要进行10次以便获得整个数据集的估计。
- en: Machine learning algorithms
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: There are a number of machine learning algorithms, for both supervised and unsupervised
    learning. In this book, we will cover some of the most popular algorithms that
    can be utilized within ensembles. In this chapter, we will go over the key concepts
    behind each algorithm, the basic algorithms, and the libraries that implement
    them in Python.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机器学习算法，适用于监督学习和无监督学习。在本书中，我们将介绍一些最受欢迎的算法，这些算法可以在集成方法中使用。本章将介绍每个算法背后的关键概念、基本算法以及实现这些算法的Python库。
- en: Python packages
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python包
- en: 'In order to leverage the power of any programming language, libraries are essential.
    They provide a convenient and tested implementation of many algorithms. In this
    book, we will be using Python 3.6 along with the following libraries: NumPy, for
    its excellent implementation of numerical operators and matrices; Pandas, for
    its convenient data manipulation methods; Matplotlib, to visualize our data; scikit-learn,
    for its excellent implementations of various machine learning algorithms, and
    Keras to build neural networks, utilizing its Pythonic, intuitive interface. Keras
    is an interface for other frameworks, such as TensorFlow, PyTorch, and Theano.
    The specific versions of each library used in this book are listed as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分发挥任何编程语言的强大功能，库是必不可少的。它们提供了许多算法的便捷且经过测试的实现。在本书中，我们将使用Python 3.6，并结合以下库：NumPy，因其出色的数值运算符和矩阵实现；Pandas，因其便捷的数据操作方法；Matplotlib，用于可视化我们的数据；scikit-learn，因其出色的各种机器学习算法实现；Keras，用于构建神经网络，利用其Pythonic、直观的接口。Keras是其他框架的接口，如TensorFlow、PyTorch和Theano。本书中使用的每个库的具体版本如下：
- en: numpy==1.15.1
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: numpy==1.15.1
- en: pandas==0.23.4
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas==0.23.4
- en: scikit-learn==0.19.1
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn==0.19.1
- en: matplotlib==2.2.2
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib==2.2.2
- en: Keras==2.2.4
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras==2.2.4
- en: Supervised learning algorithms
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习算法
- en: The most common class of machine learning algorithm is supervised learning algorithms.
    These concern problems where data has a known structure. This means that each
    data point has a specific value related to it that we wish to model or predict.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的机器学习算法类别是监督学习算法。这类算法涉及数据具有已知结构的问题。这意味着每个数据点都有一个与之相关的特定值，我们希望对其进行建模或预测。
- en: Regression
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: Regression is one of the simplest machine learning algorithms. The **Ordinary
    Least Squares** (**OLS**) regression of the form *y=ax+b *attempts to optimize
    the *a* and *b* parameters in order to fit the data. It uses MSE as its cost function.
    As the name implies, it is able to solve regression problems.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是最简单的机器学习算法之一。**普通最小二乘法**（**OLS**）回归形式为 *y=ax+b*，旨在优化 *a* 和 *b* 参数，以便拟合数据。它使用均方误差（MSE）作为其代价函数。顾名思义，它能够解决回归问题。
- en: 'We can use the scikit-learn implementation of OLS to try and model the diabetes
    dataset (the dataset is provided with the library):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用scikit-learn实现的OLS来尝试建模糖尿病数据集（该数据集随库一起提供）：
- en: '[PRE0]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first section deals with importing libraries and loading data. We use the
    `LinearRegression` implementation that exists in the `linear_model` package:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分处理导入库和加载数据。我们使用 `linear_model` 包中的 `LinearRegression` 实现：
- en: '[PRE1]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The second section splits the data into a train and a test set. For this example,
    we used the first 400 instances as the train set and the other 42 as the test
    set:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分将数据划分为训练集和测试集。在这个示例中，我们使用前400个实例作为训练集，另外42个作为测试集：
- en: '[PRE2]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next section instantiates a linear regression object with `ols = LinearRegression()`.
    It then optimizes the parameters, or fits the model with our training instances,
    using `ols.fit(train_x, train_y)`. Finally, by using the `metrics` package, we
    calculate the MSE and *R²* of our model, using the test data in Section 4:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分通过 `ols = LinearRegression()` 创建一个线性回归对象。然后，它通过使用 `ols.fit(train_x, train_y)`
    在训练实例上优化参数，或者说拟合模型。最后，通过使用 `metrics` 包，我们使用第4节中的测试数据计算模型的MSE和 *R²*：
- en: '[PRE3]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The code''s output is the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出如下：
- en: '[PRE4]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Another form of regression, logistic regression, attempts to model the probability
    that an instance belongs to one of two classes. Again, it attempts to optimize
    the *a* and *b* parameters in order to model *p=1/(1+e^(-(ax+b)))* . Once again,
    using scikit-learn and the breast cancer dataset, we can create and evaluate a
    simple logistic regression. The following code sections are similar to the preceding
    ones, but this time we''ll use classification accuracy and a confusion matrix
    rather than *R²* as a metric:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种回归形式，逻辑回归，试图建模一个实例属于两个类之一的概率。同样，它试图优化 *a* 和 *b* 参数，以便建模 *p=1/(1+e^(-(ax+b)))*。同样，使用scikit-learn和乳腺癌数据集，我们可以创建并评估一个简单的逻辑回归。以下代码部分与前面的类似，不过这次我们将使用分类准确率和混淆矩阵，而不是
    *R²* 作为度量：
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The test classification accuracy achieved with this model is 95%, which is
    quite good. Furthermore, the confusion matrix that follows here indicates that
    the model does not try to take advantage of class imbalances. Later in this book,
    we will learn how to further increase the classification accuracy with the use
    of ensemble methods. The following table shows the logit model confusion matrix:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型实现的测试分类准确率为 95%，表现相当不错。此外，下面的混淆矩阵表明该模型并没有试图利用类别不平衡的问题。在本书的后续章节中，我们将学习如何通过集成方法进一步提高分类准确率。以下表格展示了逻辑回归模型的混淆矩阵：
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **n = 169** | **预测：恶性** | **预测：良性** |'
- en: '| **Target: Malignant** | 38 | 1 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **目标：恶性** | 38 | 1 |'
- en: '| **Target: Benign** | 8 | 122 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **目标：良性** | 8 | 122 |'
- en: Support vector machines
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: 'Support vector machines or SVMs use a subset of training data, specifically
    data points near the edge of each class, in order to define a separating hyperplane
    (in two dimensions, a line). These edge cases are called support vectors. The
    goal of an SVM is to find the hyperplane that maximizes the margin (distance)
    between the support vectors (depicted in the following figure). In order to classify
    nonlinear separable classes, SVMs use the kernel trick to map data in a higher
    dimensional space, where it can become linearly separable:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）使用训练数据的一个子集，特别是每个类别边缘附近的数据点，用以定义一个分隔超平面（在二维中为一条直线）。这些边缘数据点称为支持向量。SVM
    的目标是找到一个最大化支持向量之间间距（距离）的超平面（如下图所示）。为了分类非线性可分的类别，SVM 使用核技巧将数据映射到更高维的空间，在该空间中数据可以变得线性可分：
- en: '![](img/d9689d3f-38d0-478f-abdb-313232de0725.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9689d3f-38d0-478f-abdb-313232de0725.png)'
- en: SVM margins and support vectors
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 边界和支持向量
- en: If you want to learn more about the kernel trick, this is a good starting point: [https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于核技巧的内容，这是一个很好的起点：[https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick)。
- en: 'In scikit-learn, an SVM is implemented under `sklearn.svm`, both for regression
    with `sklearn.svm.SVR` and classification with `sklearn.svm.SVC`. Once again,
    we''ll test the algorithm''s potential using scikit-learn and the code utilized
    in the regression examples. Using an SVM with a linear kernel on the breast cancer
    dataset results in 95% accuracy and the following confusion matrix:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，SVM 被实现为 `sklearn.svm`，支持回归（使用 `sklearn.svm.SVR`）和分类（使用 `sklearn.svm.SVC`）。我们将再次使用
    scikit-learn 测试算法的潜力，并使用回归示例中的代码。使用带有线性核的 SVM 对乳腺癌数据集进行测试，结果为 95% 的准确率，混淆矩阵如下：
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **n = 169** | **预测：恶性** | **预测：良性** |'
- en: '| **Target: Malignant** | 39 | 0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **目标：恶性** | 39 | 0 |'
- en: '| **Target: Benign** | 9 | 121 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| **目标：良性** | 9 | 121 |'
- en: On the diabetes dataset, by fine-tuning the *C* parameter to 1,000 during the `(svr
    = SVR(kernel='linear', C=1e3))` object instantiation, we are able to achieve an R2
    of 0.71 and an MSE of 1622.36, marginally better than the logit model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在糖尿病数据集上，通过在 `(svr = SVR(kernel='linear', C=1e3))` 对象实例化过程中将 *C* 参数调整为 1000，我们能够实现一个
    R2 值为 0.71 和 MSE 值为 1622.36，略微优于逻辑回归模型。
- en: Neural networks
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Neural networks, inspired by the way biological brains are connected, consist
    of many neurons, or computational modules, organized in layers. Data is provided
    at the input layer and predictions are produced at the output layer. All intermediate
    layers are called hidden layers. Neurons that belong to the same layer are not
    connected to each other, only to neurons that belong in other layers. Each neuron
    can have multiple inputs, where each input is multiplied by a specific weight
    and the sum of multiplied inputs is passed to an activation function that defines
    the neuron''s output. Common activation functions include the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，灵感来自于生物大脑的连接方式，由许多神经元或计算模块组成，这些模块按层组织。数据从输入层提供，预测结果由输出层产生。所有中间层称为隐藏层。属于同一层的神经元之间没有直接连接，只与属于其他层的神经元连接。每个神经元可以有多个输入，每个输入都与特定权重相乘，乘积的和被传递到激活函数，激活函数决定神经元的输出。常见的激活函数包括以下几种：
- en: '| **Sigmoid** | **Tanh** | **ReLU** | **Linear** |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **Sigmoid** | **Tanh** | **ReLU** | **线性** |'
- en: '|     ![](img/15d85d5f-1a56-4f98-a267-9bec857704fa.png) |        ![](img/3b36b9f5-f2db-4490-b19a-977905bd2c45.png)
    |        ![](img/263f0f9f-6ebc-4155-a347-2f46288986d6.png) |           ![](img/1c60ae33-f144-43ff-890a-0739f7e46fc1.png)
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|     ![](img/15d85d5f-1a56-4f98-a267-9bec857704fa.png) |        ![](img/3b36b9f5-f2db-4490-b19a-977905bd2c45.png)
    |        ![](img/263f0f9f-6ebc-4155-a347-2f46288986d6.png) |           ![](img/1c60ae33-f144-43ff-890a-0739f7e46fc1.png)
    |'
- en: The network's goal is to optimize each neuron's weights, such that the cost
    function is minimized. Neural networks can be either used for regression, where
    the output layer consists of a single neuron, or classification, where it consists
    of many neurons, usually equal to the number of classes. There are a number of
    optimizing algorithms or optimizers available for neural networks. The most common
    is stochastic gradient descent or SGD. The main idea is that the weights are updated
    based on the direction and magnitude (first derivative) of the error's gradient,
    multiplied by a factor called the learning rate.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的目标是优化每个神经元的权重，以使成本函数最小化。神经网络可以用于回归问题，其中输出层由一个神经元组成，或者用于分类问题，其中输出层由多个神经元组成，通常与类别数相等。神经网络有多种优化算法或优化器可供选择，最常见的是随机梯度下降（**SGD**）。其基本思想是，根据误差梯度的方向和大小（即一阶导数），乘以一个称为学习率的因子来更新权重。
- en: Variations and extensions have been proposed that take into account the second
    derivative, adapt the learning rate, or use the momentum of previous weight changes
    to update the weights.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出了多种变体和扩展，考虑了二阶导数，调整了学习率，或利用前一个权重变化的动量来更新权重。
- en: Although the concept of neural networks has existed for a long time, recently
    their popularity has greatly increased with the advent of deep learning. Modern
    architectures consist of convolutional layers, where each layer's weights consist
    of matrices, and the output is calculated by sliding the weight matrix onto the
    input. Another type of layers, max pooling layers, calculates the output as the
    maximum input element again by sliding a fixed-size window onto the input. Recurrent
    layers retain information about their previous
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络的概念已经存在很长时间，但随着深度学习的出现，它们的受欢迎程度最近大大增加。现代架构由卷积层组成，每个层的权重是矩阵，输出通过将权重矩阵滑动到输入上来计算。另一种类型的层是最大池化层，它通过将固定大小的窗口滑动到输入上来计算输出，输出为最大输入元素。递归层则保留了关于前一个状态的信息。
- en: states. Finally, fully connected layers are traditional neurons, as described
    previously.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，全连接层是传统神经元，如前所述。
- en: 'Scikit-learn implements traditional neural networks, under the `sklearn.neural_network`
    package. Once again, using the preceding examples, we''ll try to model the diabetes
    and breast cancer datasets. On the diabetes dataset, we''ll use `MLPRegressor`
    with **Stochastic Gradient Descent** (**SGD**) as the optimizer, with `mlpr =
    MLPRegressor(solver=''sgd'')`. Without any further fine-tuning, we achieve an
    R² of 0.64 and an MSE of 1977\. On the breast cancer dataset, using the **Limited-memory
    Broyden–Fletcher–Goldfarb–Shanno** (**LBFGS**) optimizer, with `mlpc = MLPClassifier(solver=''lbfgs'')`,
    we get a classification accuracy of 93% and a competent confusion matrix. The
    following table shows the neural network confusion matrix for the breast cancer
    dataset:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 实现了传统的神经网络，位于 `sklearn.neural_network` 包下。再次使用之前的示例，我们将尝试对糖尿病和乳腺癌数据集进行建模。在糖尿病数据集中，我们将使用
    `MLPRegressor`，并选择**随机梯度下降**（**SGD**）作为优化器，代码为 `mlpr = MLPRegressor(solver='sgd')`。在没有进一步微调的情况下，我们达到了
    0.64 的 R² 和 1977 的均方误差（MSE）。在乳腺癌数据集上，我们使用**有限记忆 Broyden–Fletcher–Goldfarb–Shanno**（**LBFGS**）优化器，代码为
    `mlpc = MLPClassifier(solver='lbfgs')`，我们得到了 93% 的分类准确率，并得到了一个合格的混淆矩阵。下表展示了乳腺癌数据集的神经网络混淆矩阵：
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **n = 169** | **预测：恶性** | **预测：良性** |'
- en: '| **Target: Malignant** | 35 | 4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| **目标：恶性** | 35 | 4 |'
- en: '| **Target: Benign** | 8 | 122 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **目标：良性** | 8 | 122 |'
- en: 'A very important note on neural networks: the initial weights of a network
    are randomly initialized. Thus, the same code can perform differently if it is
    executed several times. In order to ensure non-random (non-stochastic) execution,
    the initial random state of the network must be fixed. The two scikit-learn classes
    implement this feature through the `random_state` parameter in the object constructor.
    In order to set the random state to a specific seed value, the constructor must
    be called as follows: `mlpc = MLPClassifier(solver=''lbfgs'', random_state=12418)`.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络的一个非常重要的说明：网络的初始权重是随机初始化的。因此，如果多次执行相同的代码，结果可能会不同。为了确保非随机（非随机性）执行，必须固定网络的初始随机状态。两个
    scikit-learn 类通过对象构造器中的 `random_state` 参数实现了这一功能。为了将随机状态设置为特定的种子值，构造器应按以下方式调用：`mlpc
    = MLPClassifier(solver='lbfgs', random_state=12418)`。
- en: Decision trees
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are less of a black box than other machine learning algorithms.
    They can easily explain how they produce a prediction, which is called **interpretability**.
    The main concept is that they produce rules by splitting the training set using
    the provided features. By iteratively splitting the data, a tree form is produced,
    thus this is where their name derives from. Let's consider a dataset where the
    instances are individual persons deciding on their vacations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树相比其他机器学习算法，更不具有黑箱性质。它们可以轻松解释如何产生预测，这被称为**可解释性**。其主要概念是通过使用提供的特征分割训练集来生成规则。通过迭代地分割数据，形成了一棵树，因此它们的名字由此而来。我们考虑一个数据集，其中的实例是每个个体在决定度假地点时的选择。
- en: 'The dataset features consist of the person''s age and available money, while
    the target is their preferred destination, one of either **Summer Camp**, **Lake**,
    or **Bahamas**. A possible decision tree model is depicted in the following figure:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的特征包括个人的年龄和可用资金，而目标是他们偏好的度假地点，可能的选择为 **夏令营**、**湖泊** 或 **巴哈马**。以下是一个可能的决策树模型：
- en: '![](img/f80c05fa-627e-4833-b9ae-d3ae34c8aa1e.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f80c05fa-627e-4833-b9ae-d3ae34c8aa1e.png)'
- en: Decision tree model for the vacation destination problem
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 度假目的地问题的决策树模型
- en: As is evident, the model can explain how it produces any predictions. The way
    that the model itself is built is by trying to select the feature and threshold
    that maximize the information produced. Roughly, this means that the model will
    try to iteratively split the dataset in a way that separates the greatest number
    of remaining instances.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 正如可以看到的，模型能够解释它是如何产生任何预测的。模型本身的构建方式是通过选择能够最大化信息量的特征和阈值。大致而言，这意味着模型会尝试通过迭代的方式分割数据集，从而最大程度地分离剩余的实例。
- en: Although intuitive to understand, decision trees can produce unreasonable models,
    with the extreme being the generation of so many rules that, eventually, each
    rule combination leads to a single instance. In order to avoid such models, we
    can restrict the model by requiring that it does not exceed a specific depth (maximum
    number of consecutive rules), or that each node has at least a minimum number
    of instances before it can be further split.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树直观易懂，但它们也可能产生不合理的模型，极端情况下会生成过多的规则，最终每个规则组合都会指向一个单独的实例。为了避免此类模型，我们可以通过要求模型的深度不超过特定的最大值（即连续规则的最大数量），或者要求每个节点在进一步分裂之前，至少包含一定数量的实例来限制模型。
- en: 'In scikit-learn, decision trees are implemented under the `sklearn.tree` package,
    with `DecisionTreeClassifier` and `DecisionTreeRegressor`. In our examples, using
    `DecisionTreeRegressor` with `dtr = DecisionTreeRegressor(max_depth=2)`, we achieve
    an R² of 0.52 and an MSE of 2655\. On the breast cancer dataset, using `dtc =
    DecisionTreeClassifier(max_depth=2)`, we achieve 89% accuracy and the following
    confusion matrix:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，决策树是通过 `sklearn.tree` 包实现的，包含 `DecisionTreeClassifier` 和 `DecisionTreeRegressor`。在我们的示例中，使用
    `DecisionTreeRegressor` 和 `dtr = DecisionTreeRegressor(max_depth=2)`，我们得到了 R²
    为 0.52，均方误差（MSE）为 2655。在乳腺癌数据集上，使用 `dtc = DecisionTreeClassifier(max_depth=2)`，我们得到了
    89% 的准确率，并且得到了以下的混淆矩阵：
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| **n = 169** | **预测: 恶性** | **预测: 良性** |'
- en: '| **Target: Malignant** | 37 | 2 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| **目标: 恶性** | 37 | 2 |'
- en: '| **Target: Benign** | 17 | 113 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| **目标: 良性** | 17 | 113 |'
- en: 'Although not the best-performing algorithm so far, we can clearly see how each
    individual was classified, by exporting the tree to the `graphviz` format with
    `export_graphviz(dtc, feature_names=bc.feature_names, class_names=bc.target_names,
    impurity=False)`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个算法迄今为止表现得不是最好，但我们可以清晰地看到每个个体是如何被分类的，通过将树导出为`graphviz`格式，使用`export_graphviz(dtc,
    feature_names=bc.feature_names, class_names=bc.target_names, impurity=False)`：
- en: '![](img/9da5bb06-3901-4bb1-9903-c3e921d77f90.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9da5bb06-3901-4bb1-9903-c3e921d77f90.png)'
- en: The decision tree generated for the breast cancer dataset
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为乳腺癌数据集生成的决策树
- en: K-Nearest Neighbors
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻
- en: '**k-Nearest Neighbors** (**k-NN**) is a relatively simple machine learning
    algorithm. Each instance is classified by comparing it to its K-nearest examples
    as the majority class. In regression, the average value of neighbors is used.
    Scikit-learn''s implementation lies within the `sklearn.neighbors` package of
    the library. As it is the naming convention of the library, `KNeighborsClassifier`
    implements the classification and `KNeighborsRegressor` implements the regression
    version of the algorithm. Using them in our examples, the regressor generates
    an R² of 0.58 with an MSE of 2342, while the classifier achieves 93% accuracy.
    The following table shows the k-NN confusion matrix for the breast cancer dataset:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-最近邻**（**k-NN**）是一个相对简单的机器学习算法。每个实例通过与其K个最近的样本进行比较来分类，采用多数类作为分类结果。在回归中，使用邻居的平均值。Scikit-learn的实现位于库的`sklearn.neighbors`包中。根据库的命名规范，`KNeighborsClassifier`实现了分类版本，`KNeighborsRegressor`实现了回归版本。通过在我们的示例中使用它们，回归器生成的R²为0.58，均方误差（MSE）为2342，而分类器达到了93%的准确率。下表显示了乳腺癌数据集的k-NN混淆矩阵：'
- en: '| **n = 169** | **Predicted: Malignant** | **Predicted: Benign** |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **n = 169** | **预测：恶性** | **预测：良性** |'
- en: '| **Target: Malignant** | 37 | 2 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| **目标：恶性** | 37 | 2 |'
- en: '| **Target: Benign** | 9 | 121 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **目标：良性** | 9 | 121 |'
- en: K-means
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means
- en: 'K-means is a clustering algorithm that presents similarities to k-NN. A number
    of cluster centers are produced, and each instance is assigned to its nearest
    cluster. After all instances are assigned to a cluster, the centroid of the cluster
    becomes the new center, until the algorithm converges to a stable solution. In
    scikit-learn, this algorithm is implemented in `sklearn.cluster.KMeans`. We can
    try to cluster the first two features of the breast cancer dataset: the mean radius
    and the texture of the FNA imaging.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是一种聚类算法，与k-NN有相似之处。它生成多个聚类中心，并将每个实例分配给其最近的聚类。所有实例分配到聚类后，聚类的中心点变成新的中心，直到算法收敛到稳定的解。在scikit-learn中，该算法通过`sklearn.cluster.KMeans`实现。我们可以尝试对乳腺癌数据集的前两个特征进行聚类：平均半径和FNA成像的纹理。
- en: 'First, we load the required data and libraries, while retaining only the first
    two features of the dataset:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载所需的数据和库，同时只保留数据集的前两个特征：
- en: '[PRE6]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we fit the cluster on the data. Note that we don''t have to split the
    data into train and test sets:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将聚类拟合到数据上。注意，我们不需要将数据拆分为训练集和测试集：
- en: '[PRE7]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following that, we create a two-dimensional mesh and cluster every point, in
    order to plot the cluster areas and boundaries:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个二维网格并对每个点进行聚类，以便绘制聚类区域和边界：
- en: '[PRE8]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we plot the actual data, color-mapped to its respective clusters:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制实际数据，并将其颜色映射到相应的聚类：
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result is a two-dimensional image with color-coded boundaries of each cluster,
    as well as the instances:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个二维图像，显示了每个聚类的彩色边界，以及各个实例：
- en: '![](img/5590354e-8202-4dda-9414-be4d73bfa4ec.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5590354e-8202-4dda-9414-be4d73bfa4ec.png)'
- en: K-means clustering of the first two features of the breast cancer dataset
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集前两个特征的K-means聚类
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we presented the basic datasets, algorithms, and metrics that
    we will use throughout the book. We talked about regression and classification
    problems, where datasets have not only features but also targets. We called these
    labeled datasets. We also talked about unsupervised learning, in the form of clustering
    and dimensionality reduction. We introduced cost functions and model metrics that
    we will use to evaluate the models that we generate. Furthermore, we presented
    the basic learning algorithms and Python libraries that we will utilize in the
    majority of our examples.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了将在整本书中使用的基本数据集、算法和指标。我们讨论了回归和分类问题，其中数据集不仅包含特征，还包含目标。我们称这些为标注数据集。我们还讨论了无监督学习，包括聚类和降维。我们介绍了成本函数和模型指标，这些将用于评估我们生成的模型。此外，我们介绍了将在大多数示例中使用的基本学习算法和Python库。
- en: 'In the next chapter, we will introduce the concepts of bias and variance, as
    well as the concept of ensemble learning. Some key points to remember are as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍偏差和方差的概念，以及集成学习的概念。以下是一些关键点：
- en: We try to solve a regression problem when the target variable is a continuous
    number and its values have a meaning in terms of magnitude, such as speed, cost,
    blood pressure, and so on. Classification problems can have their targets coded
    as numbers, but we cannot treat them as such. There is no meaning in trying to
    sort colors or foods based on the number they are assigned during a problem's
    encoding.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当目标变量是一个连续数值且其值具有某种大小意义时，例如速度、成本、血压等，我们尝试解决回归问题。分类问题的目标可能会以数字编码，但我们不能将其视为数值来处理。在问题编码时，尝试根据所分配的数字对颜色或食物进行排序是没有意义的。
- en: Cost functions are a way to quantify how far away a predictive model is from
    modelling data perfectly. Metricsprovide information that is easier for humans
    to understand and report.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数是一种量化预测模型与完美建模数据之间差距的方法。指标提供的信息更容易让人理解和报告。
- en: All of the algorithms presented in this chapter have implementations for both
    classification and regression problems in scikit-learn. Some are better suited
    to particular tasks, at least without tuning their hyper parameters. Decision
    trees produce models that are easily interpreted by humans.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章中介绍的所有算法在scikit-learn中都有分类和回归问题的实现。有些算法更适合某些特定任务，至少在不调整其超参数的情况下，决策树生成的模型易于人类解释。
