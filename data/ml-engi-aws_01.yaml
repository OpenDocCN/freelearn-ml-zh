- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to ML Engineering on AWS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 上的机器学习工程简介
- en: Most of us started our **machine learning** (**ML**) journey by training our
    first ML model using a sample dataset on our laptops or home computers. Things
    are somewhat straightforward until we need to work with much larger datasets and
    run our ML experiments in the cloud. It also becomes more challenging once we
    need to deploy our trained models to production-level inference endpoints or web
    servers. There are a lot of things to consider when designing and building ML
    systems and these are just some of the challenges data scientists and ML engineers
    face when working on real-life requirements. That said, we must use the right
    platform, along with the right set of tools, when performing ML experiments and
    deployments in the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大多数人都是通过在笔记本电脑或家用电脑上使用样本数据集训练第一个机器学习模型（**机器学习**，**ML**）来开始我们的机器学习之旅的。事情在需要处理大量数据集并在云中运行我们的机器学习实验之前相对简单。一旦我们需要将我们的训练模型部署到生产级别的推理端点或网络服务器，这也会变得更加具有挑战性。在设计构建机器学习系统时有很多事情要考虑，这些只是数据科学家和机器学习工程师在处理现实需求时面临的一些挑战。话虽如此，我们在云中进行机器学习实验和部署时，必须使用正确的平台，以及正确的一套工具。
- en: At this point, you might be wondering why we should even use a cloud platform
    when running our workloads. *Can’t we build this platform ourselves*? Perhaps
    you might be thinking that building and operating your own data center is a relatively
    easy task. In the past, different teams and companies have tried setting up infrastructure
    within their data centers and on-premise hardware. Over time, these companies
    started migrating their workloads to the cloud as they realized how hard and expensive
    it was to manage and operate data centers. A good example of this would be the
    *Netflix* team, which migrated their resources to the **AWS** cloud. Migrating
    to the cloud allowed them to scale better and allowed them to have a significant
    increase in service availability.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道为什么我们甚至需要使用云平台来运行我们的工作负载。*难道我们不能自己构建这个平台吗*？也许你可能会想，建立和运营自己的数据中心相对容易。在过去，不同的团队和公司试图在他们的数据中心和本地硬件上设置基础设施。随着时间的推移，这些公司开始将他们的工作负载迁移到云中，因为他们意识到管理和运营数据中心是多么困难和昂贵。一个很好的例子是
    *Netflix* 团队，他们将资源迁移到了 **AWS** 云。迁移到云使他们能够更好地扩展，并显著提高了服务可用性。
- en: The **Amazon Web Services** (**AWS**) platform provides a lot of services and
    capabilities that can be used by professionals and companies around the world
    to manage different types of workloads in the cloud. These past couple of years,
    AWS has announced and released a significant number of services, capabilities,
    and features that can be used for production-level ML experiments and deployments
    as well. This is due to the increase in ML workloads being migrated to the cloud
    globally. As we go through each of the chapters in this book, we will have a better
    understanding of how different services are used to solve the challenges when
    productionizing ML models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚马逊网络服务**（**AWS**）平台为全球的专业人士和企业提供了大量的服务和功能，可以用来在云中管理不同类型的工作负载。在过去几年中，AWS
    宣布并发布了许多服务和功能，这些服务和功能可以用于生产级别的机器学习实验和部署。这是由于全球机器学习工作负载迁移到云中的增加。随着我们阅读本书中的每一章，我们将更好地了解如何使用不同的服务来解决在生产中部署机器学习模型时的挑战。'
- en: 'The following diagram shows the hands-on journey for this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了本章的动手实践之旅：
- en: '![Figure 1.1 – Hands-on journey for this chapter ](img/B18638_01_001.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 本章的动手实践之旅](img/B18638_01_001.jpg)'
- en: Figure 1.1 – Hands-on journey for this chapter
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 本章的动手实践之旅
- en: In this introductory chapter, we will focus on getting our feet wet by trying
    out different options when building an ML model on AWS. As shown in the preceding
    diagram, we will use a variety of **AutoML** services and solutions to build ML
    models that can help us predict if a hotel booking will be cancelled or not based
    on the information available. We will start by setting up a **Cloud9** environment,
    which will help us run our code through an **integrated development environment**
    (**IDE**) in our browser. In this environment, we will generate a realistic synthetic
    dataset using a **deep learning** model called the **Conditional Generative Adversarial
    Network**. We will upload this dataset to **Amazon S3** using the **AWS CLI**.
    Inside the Cloud9 environment, we will also install **AutoGluon** and run an **AutoML**
    experiment to train and generate multiple models using the synthetic dataset.
    Finally, we will use **SageMaker Canvas** and **SageMaker Autopilot** to run AutoML
    experiments using the uploaded dataset in S3\. If you are wondering what these
    fancy terms are, keep reading as we demystify each of these in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的入门部分，我们将通过在 AWS 上构建机器学习模型时尝试不同的选项来“湿脚”。如图所示，我们将使用各种 **AutoML** 服务和解决方案来构建可以帮助我们根据可用信息预测酒店预订是否会取消的机器学习模型。我们将首先设置一个
    **Cloud9** 环境，这将帮助我们通过浏览器中的 **集成开发环境**（**IDE**）运行我们的代码。在这个环境中，我们将使用名为 **Conditional
    Generative Adversarial Network** 的 **深度学习** 模型生成一个真实的合成数据集。我们将使用 **AWS CLI** 将此数据集上传到
    **Amazon S3**。在 Cloud9 环境中，我们还将安装 **AutoGluon** 并运行一个 **AutoML** 实验，使用合成数据集训练和生成多个模型。最后，我们将使用
    **SageMaker Canvas** 和 **SageMaker Autopilot** 在 S3 上运行使用上传数据集的 AutoML 实验。如果你想知道这些术语是什么意思，请继续阅读，我们将在本章中揭示每个术语的奥秘。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is expected from ML engineers?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习工程师的期望是什么？
- en: How ML engineers can get the most out of AWS
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何让机器学习工程师充分利用 AWS
- en: Essential prerequisites
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必要的先决条件
- en: Preparing the dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集
- en: AutoML with AutoGluon
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AutoGluon 进行 AutoML
- en: Getting started with SageMaker and SageMaker Canvas
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 SageMaker 和 SageMaker Canvas
- en: No-code machine learning with SageMaker Canvas
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Canvas 进行无代码机器学习
- en: AutoML with SageMaker Autopilot
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Autopilot 进行 AutoML
- en: In addition to getting our feet wet using key ML services, libraries, and tools
    to perform AutoML experiments, this introductory chapter will help us gain a better
    understanding of several ML and ML engineering concepts that will be relevant
    to the succeeding chapters of this book. With this in mind, let’s get started!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用关键机器学习服务、库和工具进行 AutoML 实验“湿脚”之外，本章入门部分还将帮助我们更好地理解几个与本书后续章节相关的机器学习和机器学习工程概念。考虑到这一点，让我们开始吧！
- en: Technical requirements
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before we start, we must have an AWS account. If you do not have an AWS account
    yet, simply create an account here: [https://aws.amazon.com/free/](https://aws.amazon.com/free/).
    You may proceed with the next steps once the account is ready.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们必须有一个 AWS 账户。如果您还没有 AWS 账户，请在此处创建一个账户：[https://aws.amazon.com/free/](https://aws.amazon.com/free/)。一旦账户准备好，您就可以进行下一步了。
- en: 'The Jupyter notebooks, source code, and other files for each chapter are available
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每一章的 Jupyter 笔记本、源代码和其他文件都可以在这个书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).
- en: What is expected from ML engineers?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工程师的期望是什么？
- en: ML engineering involves using ML and **software engineering** concepts and techniques
    to design, build, and manage production-level ML systems, along with pipelines.
    In a team working to build ML-powered applications, **ML engineers** are generally
    expected to build and operate the ML infrastructure that’s used to train and deploy
    models. In some cases, data scientists may also need to work on infrastructure-related
    requirements, especially if there is no clear delineation between the roles and
    responsibilities of ML engineers and data scientists in an organization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程涉及使用机器学习和 **软件工程** 概念和技术来设计、构建和管理生产级机器学习系统，以及管道。在一个致力于构建机器学习应用程序的团队中，**机器学习工程师**
    通常被期望构建和运营用于训练和部署模型的机器学习基础设施。在某些情况下，数据科学家可能还需要处理与基础设施相关的要求，特别是在组织内部机器学习工程师和数据科学家的角色和责任没有明确划分的情况下。
- en: There are several things an ML engineer should consider when designing and building
    ML systems and platforms. These would include the *quality* of the deployed ML
    model, along with the *security*, *scalability*, *evolvability*, *stability*,
    and *overall cost* of the ML infrastructure used. In this book, we will discuss
    the different strategies and best practices to achieve the different objectives
    of an ML engineer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当设计和构建机器学习系统和平台时，机器学习工程师应该考虑几个因素。这包括部署的机器学习模型的**质量**，以及所使用的机器学习基础设施的**安全性**、**可扩展性**、**可进化性**、**稳定性**和**总体成本**。在这本书中，我们将讨论实现机器学习工程师不同目标的策略和最佳实践。
- en: ML engineers should also be capable of designing and building automated ML workflows
    using a variety of solutions. Deployed models degrade over time and **model retraining**
    becomes essential in ensuring the quality of deployed ML models. Having automated
    ML pipelines in place helps enable automated model retraining and deployment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程师还应该能够使用各种解决方案设计和构建自动化的机器学习工作流程。部署的模型会随着时间的推移而退化，**模型重新训练**对于确保部署的机器学习模型的质量变得至关重要。拥有自动化的机器学习管道有助于实现自动化的模型重新训练和部署。
- en: Important note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If you are excited to learn more about how to build custom ML pipelines on
    AWS, then you should check out the last section of this book: *Designing and building
    end-to-end MLOps pipelines*. You should find several chapters dedicated to deploying
    complex ML pipelines on AWS!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你热衷于学习如何在AWS上构建定制的机器学习管道，那么你应该查看这本书的最后一部分：*设计和构建端到端MLOps管道*。你应该会找到几个章节专门讨论如何在AWS上部署复杂的机器学习管道！
- en: How ML engineers can get the most out of AWS
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工程师如何充分利用AWS
- en: 'There are many services and capabilities in the AWS platform that an ML engineer
    can choose from. Professionals who are already familiar with using virtual machines
    can easily spin up **EC2** instances and run ML experiments using deep learning
    frameworks inside these virtual private servers. Services such as **AWS Glue**,
    **Amazon EMR**, and **AWS Athena** can be utilized by ML engineers and data engineers
    for different data management and processing needs. Once the ML models need to
    be deployed into dedicated inference endpoints, a variety of options become available:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: AWS平台中有许多服务和功能，机器学习工程师可以选择。已经熟悉使用虚拟机的专业人士可以轻松启动**EC2**实例，并在这些虚拟私有服务器内部使用深度学习框架运行机器学习实验。例如，**AWS
    Glue**、**Amazon EMR**和**AWS Athena**等服务可以被机器学习工程师和数据工程师用于不同的数据管理和处理需求。一旦机器学习模型需要部署到专用的推理端点，就会有各种选择：
- en: '![Figure 1.2 – AWS machine learning stack ](img/B18638_01_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – AWS机器学习堆栈](img/B18638_01_002.jpg)'
- en: Figure 1.2 – AWS machine learning stack
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – AWS机器学习堆栈
- en: As shown in the preceding diagram, data scientists, developers, and ML engineers
    can make use of multiple services and capabilities from the **AWS machine learning
    stack**. The services grouped under **AI services** can easily be used by developers
    with minimal ML experience. To use the services listed here, all we need would
    be some experience working with data, along with the software development skills
    required to use SDKs and APIs. If we want to quickly build ML-powered applications
    with features such as language translation, text-to-speech, and product recommendation,
    then we can easily do that using the services under the AI Services bucket. In
    the middle, we have **ML services** and their capabilities, which help solve the
    more custom ML requirements of data scientists and ML engineers. To use the services
    and capabilities listed here, a solid understanding of the ML process is needed.
    The last layer, **ML frameworks and infrastructure**, offers the highest level
    of flexibility and customizability as this includes the ML infrastructure and
    framework support needed by more advanced use cases.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，数据科学家、开发人员和机器学习工程师可以利用**AWS机器学习堆栈**中的多个服务和功能。在**AI服务**下分组的服务可以很容易地被具有最少机器学习经验的开发者使用。要使用这里列出的服务，我们只需要一些与数据工作的经验，以及使用SDK和API所需的软件开发技能。如果我们想快速构建具有语言翻译、文本到语音和产品推荐等功能的机器学习应用程序，那么我们可以很容易地使用AI服务桶下的服务。中间部分是**机器学习服务**及其功能，这些功能有助于解决数据科学家和机器学习工程师更定制的机器学习需求。要使用这里列出的服务和功能，需要对机器学习过程有一个扎实的理解。最外层是**机器学习框架和基础设施**，它提供了最高级别的灵活性和可定制性，因为它包括更高级用例所需的机器学习基础设施和框架支持。
- en: So, how can ML engineers make the most out of the AWS machine learning stack?
    The ability of ML engineers to design, build, and manage ML systems improves as
    they become more familiar with the services, capabilities, and tools available
    in the AWS platform. They may start with AI services to quickly build AI-powered
    applications on AWS. Over time, these ML engineers will make use of the different
    services, capabilities, and infrastructure from the lower two layers as they become
    more comfortable dealing with intermediate ML engineering requirements.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，机器学习工程师如何充分利用AWS机器学习堆栈呢？随着他们对AWS平台中可用的服务、功能和工具越来越熟悉，机器学习工程师设计、构建和管理机器学习系统的能力也会提高。他们可能从AI服务开始，快速在AWS上构建AI应用程序。随着时间的推移，这些机器学习工程师将利用底层两个不同层的服务、功能和基础设施，因为他们越来越擅长处理中级机器学习工程需求。
- en: Essential prerequisites
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 必要的先决条件
- en: 'In this section, we will prepare the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将准备以下内容：
- en: The Cloud9 environment
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud9环境
- en: The S3 bucket
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3存储桶
- en: The synthetic dataset, which will be generated using a deep learning model
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将使用深度学习模型生成的合成数据集
- en: Let’s get started.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Creating the Cloud9 environment
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Cloud9环境
- en: One of the more convenient options when performing ML experiments inside a virtual
    private server is to use the **AWS Cloud9** service. AWS Cloud9 allows developers,
    data scientists, and ML engineers to manage and run code within a development
    environment using a browser. The code is stored and executed inside an EC2 instance,
    which provides an environment similar to what most developers have.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟私有服务器内部执行机器学习实验时，更方便的选项之一是使用**AWS Cloud9**服务。AWS Cloud9允许开发人员、数据科学家和机器学习工程师使用浏览器在开发环境中管理和运行代码。代码存储和执行在EC2实例内部，这提供了一个与大多数开发人员相似的环境。
- en: Important note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is recommended to use an **Identity and Access Management** (**IAM**) user
    with limited permissions instead of the root account when running the examples
    in this book. We will discuss this along with other security best practices in
    detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*, Security, Governance,
    and Compliance Strategies*. If you are just starting to use AWS, you may proceed
    with using the root account in the meantime.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在运行本书中的示例时使用具有有限权限的**身份和访问管理（IAM**）用户，而不是root账户。我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)*，安全、治理和合规策略*中详细讨论这一点，以及其他安全最佳实践。如果你刚开始使用AWS，你可以暂时使用root账户。
- en: 'Follow these steps to create a Cloud9 environment where we will generate the
    synthetic dataset and run the **AutoGluon AutoML** experiment:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建一个Cloud9环境，我们将在此生成合成数据集并运行**AutoGluon AutoML**实验：
- en: 'Type `cloud9` in the search bar. Select **Cloud9** from the list of results:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索栏中输入`cloud9`。从结果列表中选择**Cloud9**：
- en: '![Figure 1.3 – Navigating to the Cloud9 console ](img/B18638_01_003.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 导航到Cloud9控制台](img/B18638_01_003.jpg)'
- en: Figure 1.3 – Navigating to the Cloud9 console
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 导航到Cloud9控制台
- en: Here, we can see that the region is currently set to `us-west-2`). Make sure
    that you change this to where you want the resources to be created.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到区域当前设置为`us-west-2`)。请确保您将其更改为您希望创建资源的位置。
- en: Next, click **Create environment**.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击**创建环境**。
- en: Under the `mle-on-aws`) and click **Next step**.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`mle-on-aws`)下点击**下一步**。
- en: 'Under **Environment type**, choose **Create a new EC2 instance for environment
    (direct access)**. Select **m5.large** for **Instance type** and then **Ubuntu
    Server (18.04 LTS)** for **Platform**:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**环境类型**下，选择**为环境创建新的EC2实例（直接访问）**。对于**实例类型**选择**m5.large**，然后对于**平台**选择**Ubuntu
    Server (18.04 LTS)**：
- en: '![Figure 1.4 – Configuring the Cloud9 environment settings ](img/B18638_01_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 配置Cloud9环境设置](img/B18638_01_004.jpg)'
- en: Figure 1.4 – Configuring the Cloud9 environment settings
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 配置Cloud9环境设置
- en: Here, we can see that there are other options for the instance type. In the
    meantime, we will stick with **m5.large** as it should be enough to run the hands-on
    solutions in this chapter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到实例类型还有其他选项。同时，我们将坚持使用**m5.large**，因为它应该足以运行本章的手动解决方案。
- en: For the **Cost-saving setting** option, choose **After four hours** from the
    list of drop-down options. This means that the server where the Cloud9 environment
    is running will automatically shut down after 4 hours of inactivity.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**节省成本设置**选项，从下拉列表中选择**四小时后**。这意味着运行Cloud9环境的服务器将在4小时不活动后自动关闭。
- en: Under `vpc-abcdefg (default)`. For the `subnet-abcdefg | Default in us-west-2a`.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`vpc-abcdefg (默认)`下。对于`subnet-abcdefg | Default in us-west-2a`。
- en: Important note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is recommended that you use the default VPC since the networking configuration
    is simple. This will help you avoid issues, especially if you’re just getting
    started with VPCs. If you encounter any VPC-related issues when launching a Cloud9
    instance, you may need to check if the selected subnet has been configured with
    internet access via the route table configuration in the VPC console. You may
    retry launching the instance using another subnet or by using a new VPC altogether.
    If you are planning on creating a new VPC, navigate to [https://go.aws/3sRSigt](https://go.aws/3sRSigt)
    and create a **VPC with a Single Public Subnet**. If none of these options work,
    you may try launching the Cloud9 instance in another region. We’ll discuss **Virtual
    Private Cloud** (**VPC**) networks in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*,
    Security, Governance, and Compliance Strategies*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 建议您使用默认的VPC，因为网络配置很简单。这将帮助您避免问题，尤其是如果您刚开始使用VPC。如果在启动Cloud9实例时遇到任何与VPC相关的问题，您可能需要检查所选子网是否已在VPC控制台的路由表配置中配置了互联网访问。您可以使用另一个子网或完全使用新的VPC重试启动实例。如果您计划创建新的VPC，请导航至[https://go.aws/3sRSigt](https://go.aws/3sRSigt)并创建一个**具有单个公共子网的VPC**。如果这些选项都不起作用，您可能需要在另一个区域启动Cloud9实例。我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)*，安全、治理和合规策略*中详细讨论**虚拟专用网络**（**VPC**）网络。
- en: Click **Next Step**.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**。
- en: 'On the review page, click **Create environment**. This should redirect you
    to the Cloud9 environment, which should take a minute or so to load. The Cloud9
    **IDE** is shown in the following screenshot. This is where we can write our code
    and run the scripts and commands needed to work on some of the hands-on solutions
    in this book:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在审查页面上，点击**创建环境**。这应该会将您重定向到Cloud9环境，该环境可能需要一分钟或更长时间来加载。Cloud9 **IDE**如以下截图所示。这是我们编写代码和运行脚本以及执行工作于本书中一些动手解决方案所需的命令的地方：
- en: '![Figure 1.5 – AWS Cloud9 interface ](img/B18638_01_005.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – AWS Cloud9界面](img/B18638_01_005.jpg)'
- en: Figure 1.5 – AWS Cloud9 interface
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – AWS Cloud9界面
- en: Using this IDE is fairly straightforward as it looks very similar to code editors
    such as **Visual Studio Code** and **Sublime Text**. As shown in the preceding
    screenshot, we can find the **menu bar** at the top (**A**). The **file tree**
    can be found on the left-hand side (**B**). The **editor** covers a major portion
    of the screen in the middle (**C**). Lastly, we can find the **terminal** at the
    bottom (**D**).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个IDE相当简单，因为它看起来非常类似于代码编辑器，如**Visual Studio Code**和**Sublime Text**。如图所示，我们可以在顶部找到**菜单栏**（**A**）。**文件树**可以在左侧找到（**B**）。**编辑器**占据了屏幕中间的大部分区域（**C**）。最后，我们可以在底部找到**终端**（**D**）。
- en: Important note
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If this is your first time using AWS Cloud9, here is a 4-minute introduction
    video from AWS to help you get started: [https://www.youtube.com/watch?v=JDHZOGMMkj8](https://www.youtube.com/watch?v=JDHZOGMMkj8).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用AWS Cloud9，这里有一个AWS提供的4分钟入门视频，可以帮助您开始：[https://www.youtube.com/watch?v=JDHZOGMMkj8](https://www.youtube.com/watch?v=JDHZOGMMkj8)。
- en: Now that we have our Cloud9 environment ready, it is time we configure it with
    a larger storage space.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了Cloud9环境，是时候配置更大的存储空间了。
- en: Increasing Cloud9’s storage
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加Cloud9的存储空间
- en: When a Cloud9 instance is created, the attached volume only starts with 10GB
    of disk space. Given that we will be installing different libraries and frameworks
    while running ML experiments in this instance, we will need more than 10GB of
    disk space. We will resize the volume programmatically using the `boto3` library.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建Cloud9实例时，附加的卷最初只有10GB的磁盘空间。鉴于我们将在该实例中运行ML实验并安装不同的库和框架，我们需要超过10GB的磁盘空间。我们将使用`boto3`库以编程方式调整卷大小。
- en: Important note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If this is your first time using the `boto3` library, it is the **AWS SDK for
    Python**, which gives us a way to programmatically manage the different AWS resources
    in our AWS accounts. It is a service-level SDK that helps us list, create, update,
    and delete AWS resources such as EC2 instances, S3 buckets, and EBS volumes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用`boto3`库，它是**AWS SDK for Python**，它为我们提供了一种以编程方式管理我们AWS账户中不同AWS资源的方法。这是一个服务级别的SDK，它帮助我们列出、创建、更新和删除AWS资源，如EC2实例、S3存储桶和EBS卷。
- en: 'Follow these steps to download and run some scripts to increase the volume
    disk space from 10GB to 120GB:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤下载并运行一些脚本，将卷磁盘空间从10GB增加到120GB：
- en: 'In the terminal of our Cloud9 environment (right after the `$` sign at the
    bottom of the screen), run the following bash command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的Cloud9环境终端中（屏幕底部的`$`符号之后），运行以下bash命令：
- en: '[PRE0]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will download the script file located at [https://bit.ly/3ea96tW](https://bit.ly/3ea96tW).
    Here, we are simply using a URL shortener, which would map the shortened link
    to [https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载位于[https://bit.ly/3ea96tW](https://bit.ly/3ea96tW)的脚本文件。在这里，我们只是使用了一个URL缩短器，它将缩短的链接映射到[https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter01/resize_and_reboot.py)。
- en: Important note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that we are using the big `O` flag instead of a small `o` or a zero (`0`)
    when using the `wget` command.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在使用`wget`命令时使用的是大写`O`标志，而不是小写`o`或零（`0`）。
- en: 'What’s inside the file we just downloaded? Let’s quickly inspect the file before
    we run the script. Double-click the `resize_and_reboot.py` file in the file tree
    (located on the left-hand side of the screen) to open the Python script file in
    the editor pane. As shown in the following screenshot, the `resize_and_reboot.py`
    script has three major sections. The first block of code focuses on importing
    the prerequisites needed to run the script. The second block of code focuses on
    resizing the volume of a selected EC2 instance using the `boto3` library. It makes
    use of the `describe_volumes()` method to get the volume ID of the current instance,
    and then makes use of the `modify_volume()` method to update the volume size to
    120GB. The last section involves a single line of code that simply reboots the
    EC2 instance. This line of code uses the `os.system()` method to run the `sudo
    reboot` shell command:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们刚刚下载的文件里有什么？在我们运行脚本之前，让我们快速检查一下文件。双击文件树（位于屏幕左侧）中的`resize_and_reboot.py`文件，在编辑器窗格中打开Python脚本文件。如图所示，`resize_and_reboot.py`脚本有三个主要部分。第一段代码专注于导入运行脚本所需的先决条件。第二段代码专注于使用`boto3`库调整所选EC2实例的卷大小。它使用`describe_volumes()`方法获取当前实例的卷ID，然后使用`modify_volume()`方法将卷大小更新为120GB。最后一部分涉及一行代码，该代码简单地重启EC2实例。此行代码使用`os.system()`方法运行`sudo
    reboot`shell命令：
- en: '![Figure 1.6 – The resize_and_reboot.py script file ](img/B18638_01_006.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – `resize_and_reboot.py`脚本文件](img/B18638_01_006.jpg)'
- en: Figure 1.6 – The resize_and_reboot.py script file
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – `resize_and_reboot.py`脚本文件
- en: 'You can find the `resize_and_reboot.py` script file in this book’s GitHub repository:
    [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py).
    Note that for this script to work, the `EC2_INSTANCE_ID` environment variable
    must be set to select the correct target instance. We’ll set this environment
    variable a few steps from now before we run the `resize_and_reboot.py` script.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的GitHub仓库中找到`resize_and_reboot.py`脚本文件：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/resize_and_reboot.py)。请注意，为了使此脚本正常工作，必须将`EC2_INSTANCE_ID`环境变量设置为选择正确的目标实例。我们将在运行`resize_and_reboot.py`脚本之前设置此环境变量。
- en: 'Next, run the following command in the terminal:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在终端中运行以下命令：
- en: '[PRE1]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will upgrade the version of `boto3` using `pip`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用`pip`升级`boto3`的版本。
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If this is your first time using `pip`, it is the package installer for Python.
    It makes it convenient to install different packages and libraries using the command
    line.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用`pip`，它是Python的包安装程序。它使得使用命令行安装不同的包和库变得方便。
- en: You may use `python3 -m pip show boto3` to check the version you are using.
    This book assumes that you are using version `1.20.26` or later.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`python3 -m pip show boto3`来检查您正在使用的版本。本书假设您正在使用`1.20.26`或更高版本。
- en: 'The remaining statements focus on getting the Cloud9 environment’s `instance_id`
    from the instance metadata service and storing this value in the `EC2_INSTANCE_ID`
    variable. Let’s run the following in the terminal:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的语句专注于从实例元数据服务获取Cloud9环境的`instance_id`并将其存储在`EC2_INSTANCE_ID`变量中。让我们在终端中运行以下命令：
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This should give us an EC2 instance ID with a format similar to `i-01234567890abcdef`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给我们一个格式类似于`i-01234567890abcdef`的EC2实例ID。
- en: 'Now that we have the `EC2_INSTANCE_ID` environment variable set with the appropriate
    value, we can run the following command:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将`EC2_INSTANCE_ID`环境变量设置为适当的值，我们可以运行以下命令：
- en: '[PRE5]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will run the Python script we downloaded earlier using the `wget` command.
    After performing the volume resize operation using `boto3`, the script will reboot
    the instance. You should see a **Reconnecting…** notification at the top of the
    page while the Cloud9 environment’s EC2 instance is being restarted.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用`wget`命令运行我们之前下载的Python脚本。在用`boto3`执行卷大小调整操作后，脚本将重启实例。当Cloud9环境的EC2实例正在重启时，您应该在页面顶部看到**正在重新连接…**的通知。
- en: Important note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Feel free to run the `lsblk` command after the instance has been restarted.
    This should help you verify that the volume of the Cloud9 environment instance
    has been resized to 120GB.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 实例重启后，您可以自由运行`lsblk`命令。这应该有助于您验证Cloud9环境实例的卷已调整为120GB。
- en: Now that we have successfully resized the volume to 120GB, we should be able
    to work on the next set of solutions without having to worry about disk space
    issues inside our Cloud9 environment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功将卷大小调整为120GB，我们应该能够在不担心Cloud9环境内部磁盘空间问题的前提下，继续处理下一组解决方案。
- en: Installing the Python prerequisites
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Python必备组件
- en: 'Follow these steps to install and update several Python packages inside the
    Cloud9 environment:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤在Cloud9环境中安装和更新几个Python包：
- en: 'In the terminal of our Cloud9 environment (right after the `$` sign at the
    bottom of the screen), run the following commands to update `pip`, `setuptools`,
    and `wheel`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的Cloud9环境终端（屏幕底部的`$`符号之后），运行以下命令以更新`pip`、`setuptools`和`wheel`：
- en: '[PRE6]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Upgrading these versions will help us make sure that the other installation
    steps work smoothly. This book assumes that you are using the following versions
    or later: `pip` – `21.3.1`, `setuptools` – `59.6.0`, and `wheel` – `0.37.1`.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 升级这些版本将帮助我们确保其他安装步骤能够顺利执行。本书假设您正在使用以下版本或更高版本：`pip` – `21.3.1`、`setuptools` –
    `59.6.0`和`wheel` – `0.37.1`。
- en: Important note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To check the versions, you may use the `python3 -m pip show <package>` command
    in the terminal. Simply replace `<package>` with the name of the package. An example
    of this would be `python3 -m pip show wheel`. If you want to install a specific
    version of a package, you may use `python3 -m pip install -U <package>==<version>`.
    For example, if you want to install `wheel` version `0.37.1`, you can run `python3
    -m pip install -U wheel==0.37.1`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查版本，您可以在终端中使用`python3 -m pip show <package>`命令。只需将`<package>`替换为包的名称。例如，可以使用`python3
    -m pip show wheel`。如果您想安装特定版本的包，可以使用`python3 -m pip install -U <package>==<version>`。例如，如果您想安装`wheel`版本`0.37.1`，可以运行`python3
    -m pip install -U wheel==0.37.1`。
- en: 'Next, install `ipython` by running the following command. **IPython** provides
    a lot of handy utilities that help professionals use Python interactively. We
    will see how easy it is to use IPython later in the *Performing your first AutoGluon
    AutoML experiment* section:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过运行以下命令安装`ipython`。**IPython**提供许多实用的工具，帮助专业人士交互式地使用Python。我们将在*执行第一个AutoGluon
    AutoML实验*部分中看到使用IPython有多简单：
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This book assumes that you are using `ipython` – `7.16.2` or later.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您正在使用`ipython` – `7.16.2`或更高版本。
- en: 'Now, let’s install `ctgan`. CTGAN allows us to utilize **Generative Adversarial
    Network** (**GAN**) deep learning models to generate synthetic datasets. We will
    discuss this shortly in the *Generating a synthetic dataset using a deep learning
    model* section, after we have installed the Python prerequisites:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们安装`ctgan`。CTGAN允许我们利用**生成对抗网络**（**GAN**）深度学习模型来生成合成数据集。我们将在安装Python必备组件后，在*使用深度学习模型生成合成数据集*部分中简要讨论这一点：
- en: '[PRE9]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This book assumes that you are using `ctgan` – `0.5.0`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您正在使用`ctgan` – `0.5.0`。
- en: Important note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'This step may take around 5 to 10 minutes to complete. While waiting, let’s
    talk about what CTGAN is. **CTGAN** is an open source library that uses deep learning
    to learn about the properties of an existing dataset and generates a new dataset
    with columns, values, and properties similar to the original dataset. For more
    information, feel free to check its GitHub page here: [https://github.com/sdv-dev/CTGAN](https://github.com/sdv-dev/CTGAN).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要大约5到10分钟才能完成。在等待期间，让我们谈谈CTGAN是什么。**CTGAN**是一个开源库，它使用深度学习来了解现有数据集的特性，并生成一个具有与原始数据集相似列、值和特性的新数据集。有关更多信息，请随时查看其GitHub页面：[https://github.com/sdv-dev/CTGAN](https://github.com/sdv-dev/CTGAN)。
- en: 'Finally, install `pandas_profiling` by running the following command. This
    allows us to easily generate a profile report for our dataset, which will help
    us with our **exploratory data analysis** (**EDA**) work. We will see this in
    action in the *Exploratory data analysis* section, after we have generated the
    synthetic dataset:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过运行以下命令安装`pandas_profiling`。这使我们能够轻松地为我们的数据集生成一个概要报告，这将有助于我们进行**探索性数据分析**（**EDA**）工作。我们将在生成合成数据集后，在*探索性数据分析*部分看到这一功能：
- en: '[PRE10]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This book assumes that you are using `pandas_profiling` – `3.1.0` or later.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您正在使用`pandas_profiling` – `3.1.0`或更高版本。
- en: Now that we have finished installing the Python prerequisites, we can start
    generating a realistic synthetic dataset using a deep learning model!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了Python必备条件的安装，我们可以开始使用深度学习模型生成逼真的合成数据集了！
- en: Preparing the dataset
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: In this chapter, we will build multiple ML models that will *predict whether
    a hotel booking will be cancelled or not based on the information available*.
    Hotel cancellations cause a lot of issues for hotel owners and managers, so trying
    to predict which reservations will be cancelled is a good use of our ML skills.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建多个机器学习模型，这些模型将**根据可用信息预测酒店预订是否会取消**。酒店取消给酒店业主和管理者带来了很多问题，因此尝试预测哪些预订会被取消是运用我们的机器学习技能的好方法。
- en: Before we start with our ML experiments, we will need a dataset that can be
    used when training our ML models. We will generate a realistic synthetic dataset
    similar to the *Hotel booking demands* dataset from *Nuno Antonio*, *Ana de Almeida*,
    and *Luis Nunes*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始进行机器学习实验之前，我们需要一个可以用于训练我们的机器学习模型的数据库。我们将生成一个类似于*Nuno Antonio*、*Ana de Almeida*和*Luis
    Nunes*的*酒店预订需求*数据集的逼真合成数据集。
- en: 'The synthetic dataset will have a total of 21 columns. Here are some of the
    columns:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据集将包含总共21列。以下是一些列的示例：
- en: '`is_cancelled`: Indicates whether the hotel booking was cancelled or not'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_cancelled`: 指示酒店预订是否被取消'
- en: '`lead_time`: [*arrival date*] – [*booking date*]'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lead_time`: [*到达日期*] – [*预订日期*]'
- en: '`adr`: Average daily rate'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adr`: 平均每日房价'
- en: '`adults`: Number of adults'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adults`: 成人数量'
- en: '`days_in_waiting_list`: Number of days a booking stayed on the waiting list
    before getting confirmed'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`days_in_waiting_list`: 预订在确认前在等待名单上停留的天数'
- en: '`assigned_room_type`: The type of room that was assigned'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`assigned_room_type`: 分配的房间类型'
- en: '`total_of_special_requests`: The total number of special requests made by the
    customer'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`total_of_special_requests`: 客户提出的特殊请求总数'
- en: We will not discuss each of the fields in detail, but this should help us understand
    what data is available for us to use. For more information, you can find the original
    version of this dataset at [https://www.kaggle.com/jessemostipak/hotel-booking-demand](https://www.kaggle.com/jessemostipak/hotel-booking-demand)
    and [https://www.sciencedirect.com/science/article/pii/S2352340918315191](https://www.sciencedirect.com/science/article/pii/S2352340918315191).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讨论每个字段，但这应该有助于我们了解我们可以使用哪些数据。更多信息，您可以在[https://www.kaggle.com/jessemostipak/hotel-booking-demand](https://www.kaggle.com/jessemostipak/hotel-booking-demand)和[https://www.sciencedirect.com/science/article/pii/S2352340918315191](https://www.sciencedirect.com/science/article/pii/S2352340918315191)找到该数据集的原始版本。
- en: Generating a synthetic dataset using a deep learning model
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度学习模型生成合成数据集
- en: One of the cool applications of ML would be having a **deep learning** model
    “absorb” the properties of an existing dataset and generate a new dataset with
    a similar set of fields and properties. We will use a pre-trained **Generative
    Adversarial Network** (**GAN**) model to generate the synthetic dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个酷应用就是让一个**深度学习**模型“吸收”现有数据集的特性，并生成一个具有相似字段和特性的新数据集。我们将使用预训练的**生成对抗网络**（**GAN**）模型来生成合成数据集。
- en: Important note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Generative modeling** involves learning patterns from the values of an input
    dataset, which are then used to generate a new dataset with a similar set of values.
    GANs are popular when it comes to generative modeling. For example, research papers
    have focused on how GANs can be used to generate “deepfakes,” where realistic
    images of humans are generated from a source dataset.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成建模**涉及从输入数据集的值中学习模式，然后使用这些模式生成一个具有相似值的新数据集。GANs在生成建模中很受欢迎。例如，研究论文已经关注GANs如何被用来生成“deepfakes”，即从源数据集中生成逼真的人类图像。'
- en: 'Generating and using a synthetic dataset has a lot of benefits, including the
    following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 生成和使用合成数据集有很多好处，包括以下内容：
- en: The ability to generate a much larger dataset than the original dataset that
    was used to train the model
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够生成比用于训练模型的原始数据集更大的数据集的能力
- en: The ability to anonymize any sensitive information in the original dataset
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够匿名化原始数据集中任何敏感信息的能力
- en: Being able to have a cleaner version of the dataset after data generation
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据生成后能够拥有一个更干净的数据集版本
- en: 'That said, let’s start generating the synthetic dataset by running the following
    commands in the terminal of our Cloud9 environment (right after the `$` sign at
    the bottom of the screen):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们通过在 Cloud9 环境的终端中运行以下命令来开始生成合成数据集（屏幕底部的 `$` 符号之后）：
- en: 'Continuing from where we left off in the *Installing the Python prerequisites*
    section, run the following command to create an empty directory named `tmp` in
    the current working directory:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *安装 Python 先决条件* 部分的我们停止的地方继续，运行以下命令以在当前工作目录中创建一个名为 `tmp` 的空目录：
- en: '[PRE11]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that this is different from the `/tmp` directory.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与 `/tmp` 目录不同。
- en: 'Next, let’s download the `utils.py` file using the `wget` command:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 `wget` 命令下载 `utils.py` 文件：
- en: '[PRE12]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `utils.py` file contains the `block()` function, which will help us read
    and troubleshoot the logs generated by our scripts.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`utils.py` 文件包含 `block()` 函数，这将帮助我们阅读和调试脚本生成的日志。'
- en: 'Run the following command to download the pre-built GAN model into the Cloud9
    environment:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令将预构建的 GAN 模型下载到 Cloud9 环境：
- en: '[PRE13]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we have a serialized pickle file that contains the properties of the deep
    learning model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个包含深度学习模型属性的序列化 pickle 文件。
- en: Important note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are a variety of ways to save and load ML models. One of the options would
    be to use the **Pickle** module to serialize a Python object and store it in a
    file. This file can later be loaded and deserialized back to a Python object with
    a similar set of properties.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 存储和加载 ML 模型有多种方式。其中一种选择是使用 **Pickle** 模块将 Python 对象序列化并存储在文件中。该文件可以稍后加载并反序列化为具有类似属性集的
    Python 对象。
- en: 'Create an empty `data_generator.py` script file using the `touch` command:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `touch` 命令创建一个空的 `data_generator.py` 脚本文件：
- en: '[PRE14]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before proceeding, make sure that the `data_generator.py`, `hotel_bookings.gan.pkl`,
    and `utils.py` files are in the same directory so that the synthetic data generator
    script works.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保 `data_generator.py`、`hotel_bookings.gan.pkl` 和 `utils.py` 文件位于同一目录中，以便合成数据生成器脚本能够正常工作。
- en: Double-click the `data_generator.py` file in the file tree (located on the left-hand
    side of the Cloud9 environment) to open the empty Python script file in the editor
    pane.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击文件树中的 `data_generator.py` 文件（位于 Cloud9 环境的左侧）以在编辑器面板中打开空的 Python 脚本文件。
- en: 'Add the following lines of code to import the prerequisites needed to run the
    script:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下代码行添加到脚本中，以导入运行脚本所需的先决条件：
- en: '[PRE15]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, let’s add the following lines of code to load the pre-trained GAN model:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们添加以下代码行以加载预训练的 GAN 模型：
- en: '[PRE18]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Run the following command in the terminal (right after the `$` sign at the
    bottom of the screen) to test if our initial blocks of code in the script are
    working as intended:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中运行以下命令（屏幕底部的 `$` 符号之后）以测试脚本中的初始代码块是否按预期工作：
- en: '[PRE22]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This should give us a set of logs similar to what is shown in the following
    screenshot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们一组类似于以下截图所示的日志：
- en: '![Figure 1.7 – GAN model successfully loaded by the script ](img/B18638_01_007.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 脚本成功加载的 GAN 模型](img/B18638_01_007.jpg)'
- en: Figure 1.7 – GAN model successfully loaded by the script
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 脚本成功加载的 GAN 模型
- en: Here, we can see that the pre-trained GAN model was loaded successfully using
    the `CTGANSynthesizer.load()` method. Here, we can also see what `block` (from
    the `utils.py` file we downloaded earlier) does to improve the readability of
    our logs. It simply helps mark the start and end of the execution of a block of
    code so that we can easily debug our scripts.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到使用 `CTGANSynthesizer.load()` 方法成功加载了预训练的 GAN 模型。我们还可以看到 `block`（来自我们之前下载的
    `utils.py` 文件）是如何提高我们日志的可读性的。它只是帮助标记代码块执行的开始和结束，这样我们就可以轻松调试脚本。
- en: 'Let’s go back to the editor pane (where we are editing `data_generator.py`)
    and add the following lines of code:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们回到编辑器面板（我们在这里编辑 `data_generator.py`）并添加以下代码行：
- en: '[PRE23]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When we run the script later, these lines of code will generate `10000` records
    and store them inside the `synthetic_data` variable.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们稍后运行脚本时，这些代码行将生成 `10000` 条记录并将它们存储在 `synthetic_data` 变量中。
- en: 'Next, let’s add the following block of code, which will save the generated
    data to a CSV file inside the `tmp` directory:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们添加以下代码块，它将生成的数据保存到 `tmp` 目录内的 CSV 文件中：
- en: '[PRE26]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, let’s add the following lines of code to complete the script:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们添加以下几行代码来完成脚本：
- en: '[PRE33]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This block of code will analyze the synthetic dataset and generate a profile
    report to help us analyze the properties of our dataset.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将分析合成数据集并生成一个配置报告，以帮助我们分析数据集的特性。
- en: Important note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You can find a copy of the `data_generator.py` file here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到 `data_generator.py` 文件的副本：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/data_generator.py)。
- en: 'With everything ready, let’s run the following command in the terminal (right
    after the `$` sign at the bottom of the screen):'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切准备就绪后，让我们在终端中运行以下命令（屏幕底部的 `$` 符号右侧）：
- en: '[PRE37]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'It should take about a minute or so for the script to finish. Running the script
    should give us a set of logs similar to what is shown in the following screenshot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本完成应该需要大约一分钟的时间。运行脚本应该会给我们一组类似于以下截图所示的日志：
- en: '![Figure 1.8 – Logs generated by data_generator.py ](img/B18638_01_008.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 由 data_generator.py 生成的日志](img/B18638_01_008.jpg)'
- en: Figure 1.8 – Logs generated by data_generator.py
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 由 data_generator.py 生成的日志
- en: As we can see, running the `data_generator.py` script generates multiple blocks
    of logs, which should make it easy for us to read and debug what’s happening while
    the script is running. In addition to loading the CTGAN model, the script will
    generate the synthetic dataset using the deep learning model (`tmp` directory
    (`tmp/bookings.all.csv`) (`pandas_profiling` (**C**).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，运行 `data_generator.py` 脚本会生成多个日志块，这应该使我们能够轻松阅读和调试脚本运行时的操作。除了加载 CTGAN 模型外，脚本还将使用深度学习模型（`tmp`
    目录中的 `tmp/bookings.all.csv`）生成合成数据集（`pandas_profiling`（**C**）。
- en: Wasn’t that easy? Before proceeding to the next section, feel free to use the
    file tree (located on the left-hand side of the Cloud9 environment) to check the
    generated files stored in the `tmp` directory.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不是很容易吗？在进入下一节之前，请随意使用文件树（位于屏幕底部的 `$` 符号左侧的 Cloud9 环境中）来检查存储在 `tmp` 目录中的生成的文件。
- en: Exploratory data analysis
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: At this point, we should have a synthetic dataset with `10000` rows. You might
    be wondering what our data looks like. Does our dataset contain invalid values?
    Do we have to worry about missing records? We must have a good understanding of
    our dataset since we may need to clean and process the data first before we do
    any model training work. EDA is a key step when analyzing datasets before they
    can be used to train ML models. There are different ways to analyze datasets and
    generate reports — using `pandas_profiling` is one of the faster ways to perform
    EDA.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该有一个包含 `10000` 行的合成数据集。你可能想知道我们的数据看起来像什么。我们的数据集是否包含无效值？我们是否需要担心缺失的记录？在我们进行任何模型训练工作之前，我们必须对我们的数据集有一个很好的理解，因为我们可能需要先清理和处理数据。EDA
    是在分析数据集以用于训练机器学习模型之前的关键步骤。有不同方式来分析数据集并生成报告——使用 `pandas_profiling` 是执行 EDA 的更快方式之一。
- en: 'That said, let’s check the report that was generated by the `pandas_profiling`
    Python library. Right-click on `tmp/profile-report.xhtml` in the file tree (located
    on the left-hand side of the Cloud9 environment) and then select **Preview** from
    the list of options. We should find a report similar to the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们检查由 `pandas_profiling` Python 库生成的报告。在文件树（位于 Cloud9 环境左侧）中右键单击 `tmp/profile-report.xhtml`，然后从选项列表中选择
    **预览**。我们应该找到一个类似于以下报告：
- en: '![Figure 1.9 – Generated report ](img/B18638_01_009.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 生成的报告](img/B18638_01_009.jpg)'
- en: Figure 1.9 – Generated report
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 生成的报告
- en: 'The report has multiple sections: **Overview**, **Variables**, **Interactions**,
    **Correlations** **Missing Values**, and **Sample**. In the **Overview** section,
    we can find a quick summary of the dataset statistics and the variable types.
    This includes the number of variables, number of records (observations), number
    of missing cells, number of duplicate rows, and other relevant statistics. In
    the **Variables** section, we can find the statistics and the distribution of
    values for each variable (column) in the dataset. In the **Interactions** and
    **Correlations** sections, we can see different patterns and observations regarding
    the potential relationship of the variables in the dataset. In the **Missing values**
    section, we can see if there are columns with missing values that we need to take
    care of. Finally, in the **Sample** section, we can see the first 10 and last
    10 rows of the dataset.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 报告包含多个部分：**概述**、**变量**、**交互**、**相关性**、**缺失值**和**样本**。在**概述**部分，我们可以找到数据集统计信息和变量类型的快速总结。这包括变量的数量、记录（观测）的数量、缺失单元格的数量、重复行的数量以及其他相关统计信息。在**变量**部分，我们可以找到数据集中每个变量（列）的统计信息和值分布。在**交互**和**相关性**部分，我们可以看到关于数据集中变量潜在关系的不同模式和观察。在**缺失值**部分，我们可以看到是否有需要我们注意的缺失值的列。最后，在**样本**部分，我们可以看到数据集的前10行和最后10行。
- en: Feel free to read through the report before proceeding to the next section.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，请随意阅读报告。
- en: Train-test split
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练-测试集划分
- en: 'Now that we have finished performing EDA, what do we do next? Assuming that
    our data is clean and ready for model training, do we just use all of the 10,000
    records that were generated to train and build our ML model? Before we train our
    binary classifier model, we must split our dataset into training and test sets:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了EDA（探索性数据分析），接下来我们做什么？假设我们的数据是干净的，并且已经准备好进行模型训练，我们是否直接使用生成的10,000条记录来训练和构建我们的机器学习模型？在我们训练二元分类器模型之前，我们必须将我们的数据集划分为训练集和测试集：
- en: '![Figure 1.10 – Train-test split ](img/B18638_01_010.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 训练-测试集划分](img/B18638_01_010.jpg)'
- en: Figure 1.10 – Train-test split
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 训练-测试集划分
- en: As we can see, the **training set** is used to build the model and update its
    parameters during the training phase. The **test set** is then used to evaluate
    the final version of the model on data it has not seen before. What’s not shown
    here is the **validation set**, which is used to evaluate a model to fine-tune
    the **hyperparameters** during the model training phase. In practice, the ratio
    when dividing the dataset into training, validation, and test sets is generally
    around **60:20:20**, where the training set gets the majority of the records.
    In this chapter, we will no longer need to divide the training set further into
    smaller training and validation sets since the AutoML tools and services will
    automatically do this for us.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**训练集**用于在训练阶段构建模型并更新其参数。然后，**测试集**用于评估模型在之前未见过的数据上的最终版本。这里没有展示的是**验证集**，它用于在模型训练阶段评估模型以微调**超参数**。在实践中，将数据集划分为训练集、验证集和测试集的比例通常在**60:20:20**左右，其中训练集获得大部分记录。在本章中，我们不再需要将训练集进一步划分为更小的训练集和验证集，因为AutoML工具和服务将自动为我们完成这项工作。
- en: Important note
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before proceeding with the hands-on solutions in this section, we must have
    an idea of what hyperparameters and parameters are. `y = m * x`, where `m` is
    a parameter, `x` is a single predictor variable, and `y` is the target variable.
    For example, if we are testing the relationship between cancellations (`y`) and
    income (`x`), then `m` is the parameter that defines this relationship. If `m`
    is positive, cancellations go up as income goes up. If it is negative, cancellations
    lessen as income increases. On the other hand, **hyperparameters** are configurable
    values that are tweaked before the model is trained. These variables affect how
    our chosen ML models “model” the relationship. Each ML model has its own set of
    hyperparameters, depending on the algorithm used. These concepts will make more
    sense once we have looked at a few more examples in [*Chapter 2*](B18638_02.xhtml#_idTextAnchor041),
    *Deep Learning AMIs*, and [*Chapter 3*](B18638_03.xhtml#_idTextAnchor060), *Deep
    Learning Containers*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本节的手动解决方案之前，我们必须了解超参数和参数的概念。`y = m * x`，其中`m`是参数，`x`是单个预测变量，`y`是目标变量。例如，如果我们正在测试取消（`y`）与收入（`x`）之间的关系，那么`m`是定义这种关系的参数。如果`m`为正，随着收入的增加，取消也会增加。如果它是负的，随着收入的增加，取消会减少。另一方面，**超参数**是在模型训练之前可配置的值，这些值会影响我们选择的机器学习模型如何“建模”关系。每个机器学习模型都有自己的超参数集，这取决于使用的算法。一旦我们查看更多示例，这些概念就会更加清晰，例如在[*第2章*](B18638_02.xhtml#_idTextAnchor041)
    *AWS上的机器学习工程*和[*第3章*](B18638_03.xhtml#_idTextAnchor060) *深度学习容器*。
- en: 'Now, let’s create a script that will help us perform the train-test split:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个脚本，帮助我们执行训练-测试分割：
- en: 'In the terminal of our Cloud9 environment (right after the `$` sign at the
    bottom of the screen), run the following command to create an empty file called
    `train_test_split.py`:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的Cloud9环境的终端中（屏幕底部的`$`符号之后），运行以下命令以创建一个名为`train_test_split.py`的空文件：
- en: '[PRE38]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Using the file tree (located on the left-hand side of the Cloud9 environment),
    double-click the `train_test_split.py` file to open the file in the editor pane.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文件树（位于Cloud9环境的左侧），双击`train_test_split.py`文件以在编辑器面板中打开文件。
- en: 'In the editor pane, add the following lines of code to import the prerequisites
    to run the script:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器面板中，添加以下代码行以导入运行脚本所需的先决条件：
- en: '[PRE39]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Add the following block of code, which will read the contents of a CSV file
    and store it inside a `DataFrame`:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下代码块，它将读取CSV文件的内容并将其存储在`DataFrame`中：
- en: '[PRE42]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, let’s use the `train_test_split()` function from scikit-learn to divide
    the dataset we have generated into a training set and a test set:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用scikit-learn中的`train_test_split()`函数将我们生成的数据集划分为训练集和测试集：
- en: '[PRE44]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Lastly, add the following lines of code to save the training and test sets
    into their respective CSV files inside the `tmp` directory:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，添加以下代码行以将训练集和测试集分别保存到`tmp`目录中的相应CSV文件中：
- en: '[PRE52]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Important note
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You can find a copy of the `train_test_split.py` file here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到`train_test_split.py`文件的副本：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter01/train_test_split.py)。
- en: 'Now that we have completed our script file, let’s run the following command
    in the terminal (right after the `$` sign at the bottom of the screen):'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了脚本文件，让我们在终端中运行以下命令（屏幕底部的`$`符号之后）：
- en: '[PRE57]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This should generate a set of logs similar to what is shown in the following
    screenshot:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成一组类似于以下截图所示的日志：
- en: '![Figure 1.11 – Train-test split logs   ](img/B18638_01_011.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11 – 训练-测试分割日志   ](img/B18638_01_011.jpg)'
- en: Figure 1.11 – Train-test split logs
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 – 训练-测试分割日志
- en: Here, we can see that our training dataset contains 7,000 records, while the
    test set contains 3,000 records.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的训练数据集包含7,000条记录，而测试集包含3,000条记录。
- en: With this, we can upload our dataset to **Amazon S3**.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以将我们的数据集上传到**Amazon S3**。
- en: Uploading the dataset to Amazon S3
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集上传到Amazon S3
- en: Amazon S3 is the object storage service for AWS and is where we can store different
    types of files, such as dataset CSV files and output artifacts. When using the
    different services of AWS, it is important to note that these services sometimes
    require the input data and files to be stored in an S3 bucket first or in a resource
    created using another service.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3 是 AWS 的对象存储服务，我们可以在其中存储不同类型的文件，例如数据集 CSV 文件和输出工件。在使用 AWS 的不同服务时，需要注意的是，这些服务有时要求输入数据和文件首先存储在
    S3 存储桶中，或者存储在另一个服务创建的资源中。
- en: 'Uploading the dataset to S3 should be easy. Continuing where we left off in
    the *Train-test split* section, we will run the following commands in the terminal:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集上传到 S3 应该很简单。继续我们在*训练-测试分割*部分留下的内容，我们将在终端运行以下命令：
- en: 'Run the following commands in the terminal. Here, we are going to create a
    new S3 bucket that will contain the data we will be using in this chapter. Make
    sure that you replace the value of `<INSERT BUCKET NAME HERE>` with a bucket name
    that is globally unique across all AWS users:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端运行以下命令。在这里，我们将创建一个新的 S3 存储桶，它将包含我们在本章中使用的数据。请确保将 `<INSERT BUCKET NAME HERE>`
    的值替换为一个在所有 AWS 用户中全局唯一的存储桶名称：
- en: '[PRE58]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: For more information on S3 bucket naming rules, feel free to check out [https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 S3 存储桶命名规则的更多信息，请随时查看[https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml)。
- en: 'Now that the S3 bucket has been created, let’s upload the training and test
    datasets using the **AWS CLI**:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在S3存储桶已经创建，让我们使用**AWS CLI**上传训练和测试数据集：
- en: '[PRE60]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Now that everything is ready, we can proceed with the exciting part! It’s about
    time we perform multiple **AutoML** experiments using a variety of solutions and
    services.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，我们可以进行激动人心的部分了！是时候我们使用各种解决方案和服务执行多个**AutoML**实验了。
- en: AutoML with AutoGluon
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AutoGluon 进行 AutoML
- en: Previously, we discussed what **hyperparameters** are. When training and tuning
    ML models, it is important for us to know that the performance of an ML model
    depends on the algorithm, the training data, and the hyperparameter configuration
    that’s used when training the model. Other input configuration parameters may
    also affect the performance of the model, but we’ll focus on these three for now.
    Instead of training a single model, teams build multiple models using a variety
    of hyperparameter configurations. Changes and tweaks in the hyperparameter configuration
    affect the performance of a model – some lead to better performance, while others
    lead to worse performance. It takes time to try out all possible combinations
    of hyperparameter configurations, especially if the model tuning process is not
    automated.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了什么是**超参数**。在训练和调整机器学习模型时，了解机器学习模型的性能取决于算法、训练数据以及训练模型时使用的超参数配置对我们来说非常重要。其他输入配置参数也可能影响模型的性能，但我们现在将专注于这三个方面。团队不是训练单个模型，而是使用各种超参数配置构建多个模型。超参数配置的变化和调整会影响模型的性能——一些会导致更好的性能，而另一些则会导致更差的表现。尝试所有可能的超参数配置组合需要时间，尤其是如果模型调整过程没有自动化的话。
- en: These past couple of years, several libraries, frameworks, and services have
    allowed teams to make the most out of **automated machine learning** (**AutoML**)
    to automate different parts of the ML process. Initially, AutoML tools focused
    on automating the **hyperparameter optimization** (**HPO**) processes to obtain
    the optimal combination of hyperparameter values. Instead of spending hours (or
    even days) manually trying different combinations of hyperparameters when running
    training jobs, we’ll just need to configure, run, and wait for this automated
    program to help us find the optimal set of hyperparameter values. For years, several
    tools and libraries that focus on automated hyperparameter optimization were available
    for ML practitioners for use. After a while, other aspects and processes of the
    ML workflow were automated and included in the AutoML pipeline.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，几个库、框架和服务使团队能够充分利用 **自动化机器学习**（**AutoML**）来自动化 ML 流程的不同部分。最初，AutoML 工具专注于自动化
    **超参数优化**（**HPO**）过程以获得最佳的超参数值组合。在运行训练作业时，我们不需要花费数小时（甚至数天）手动尝试不同的超参数组合，我们只需配置、运行并等待这个自动化程序帮助我们找到最佳的超参数值组合。多年来，几个专注于自动化超参数优化的工具和库可供
    ML 实践者使用。过了一段时间，ML 工作流程的其他方面和过程也被自动化并包含在 AutoML 流程中。
- en: 'There are several tools and services available for AutoML and one of the most
    popular options is **AutoGluon**. With **AutoGluon**, we can train multiple models
    using different algorithms and evaluate them with just a few lines of code:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AutoML，有几种工具和服务可用，其中最受欢迎的选项之一是 **AutoGluon**。使用 **AutoGluon**，我们可以使用不同的算法训练多个模型，并通过几行代码来评估它们：
- en: '![Figure 1.12 – AutoGluon leaderboard – models trained using a variety of algorithms
    ](img/B18638_01_012.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.12 – AutoGluon 排行榜 – 使用各种算法训练的模型](img/B18638_01_012.jpg)'
- en: Figure 1.12 – AutoGluon leaderboard – models trained using a variety of algorithms
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – AutoGluon 排行榜 – 使用各种算法训练的模型
- en: Similar to what is shown in the preceding screenshot, we can also compare the
    generated models using a leaderboard. In this chapter, we’ll use AutoGluon with
    a tabular dataset. However, it is important to note that AutoGluon also supports
    performing AutoML tasks for text and image data.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面截图所示类似，我们还可以使用排行榜比较生成的模型。在本章中，我们将使用 AutoGluon 与表格数据集。然而，重要的是要注意，AutoGluon
    也支持为文本和图像数据执行 AutoML 任务。
- en: Setting up and installing AutoGluon
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置和安装 AutoGluon
- en: 'Before using AutoGluon, we need to install it. It should take a minute or so
    to complete the installation process:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 AutoGluon 之前，我们需要安装它。安装过程可能需要一分钟或更长时间：
- en: 'Run the following commands in the terminal to install and update the prerequisites
    before we install AutoGluon:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装 AutoGluon 之前，请在终端中运行以下命令来安装和更新先决条件：
- en: '[PRE65]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This book assumes that you are using the following versions or later: `mxnet`
    – `1.9.0`, `numpy` – `1.19.5`, and `cython` – `0.29.26`.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您正在使用以下版本或更高版本：`mxnet` – `1.9.0`，`numpy` – `1.19.5`，和 `cython` – `0.29.26`。
- en: 'Next, run the following command to install `autogluon`:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下命令来安装 `autogluon`：
- en: '[PRE69]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This book assumes that you are using `autogluon` version `0.3.1` or later.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设您正在使用 `autogluon` 版本 `0.3.1` 或更高版本。
- en: Important note
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This step may take around 5 to 10 minutes to complete. Feel free to grab a cup
    of coffee or tea!
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要大约 5 到 10 分钟才能完成。您可以随意拿一杯咖啡或茶！
- en: With AutoGluon installed in our Cloud9 environment, let’s proceed with our first
    AutoGluon AutoML experiment.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Cloud9 环境中安装了 AutoGluon 后，让我们开始我们的第一个 AutoGluon AutoML 实验。
- en: Performing your first AutoGluon AutoML experiment
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行您的第一个 AutoGluon AutoML 实验
- en: 'If you have used `fit()` and `predict()`. Follow these steps:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经使用了 `fit()` 和 `predict()`，请按照以下步骤操作：
- en: 'To start, run the following command in the terminal:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，请在终端中运行以下命令：
- en: '[PRE70]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This will open the **IPython** **Read-Eval-Print-Loop** (**REPL**)/interactive
    shell. We will use this similar to how we use the **Python shell**.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开 **IPython** **读取-评估-打印循环**（**REPL**）/交互式外壳。我们将像使用 **Python shell** 一样使用它。
- en: 'Inside the console, type in (or copy) the following block of code. Make sure
    that you press *Enter* after typing the closing parenthesis:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台中，键入（或复制）以下代码块。确保在键入关闭括号后按 *Enter*：
- en: '[PRE71]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, let’s load the synthetic data stored in the `bookings.train.csv` and `bookings.test.csv`
    files into the `train_data` and `test_data` variables, respectively, by running
    the following statements:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过运行以下语句将存储在 `bookings.train.csv` 和 `bookings.test.csv` 文件中的合成数据分别加载到
    `train_data` 和 `test_data` 变量中：
- en: '[PRE75]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Since the parent class of AutoGluon, `TabularDataset`, is a pandas DataFrame,
    we can use different methods on `train_data` and `test_data` such as `head()`,
    `describe()`, `memory_usage()`, and more.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AutoGluon的父类`TabularDataset`是一个pandas DataFrame，我们可以在`train_data`和`test_data`上使用不同的方法，如`head()`、`describe()`、`memory_usage()`等。
- en: 'Next, run the following lines of code:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下代码行：
- en: '[PRE79]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Here, we specify `is_cancelled` as the target variable of the AutoML task and
    the `tmp` directory as the location where the generated models will be stored.
    This block of code will use the training data we have provided to train multiple
    models using different algorithms. AutoGluon will automatically detect that we
    are dealing with a binary classification problem and generate multiple binary
    classifier models using a variety of ML algorithms.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`is_cancelled`指定为AutoML任务的目标变量，并将`tmp`目录指定为生成的模型将被存储的位置。此代码块将使用我们提供的训练数据，使用不同的算法训练多个模型。AutoGluon将自动检测我们正在处理的是一个二元分类问题，并使用多种机器学习算法生成多个二元分类器模型。
- en: Important note
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Inside the `tmp/models` directory, we should find `CatBoost`, `ExtraTreesEntr`,
    and `ExtraTreesGini`, along with other directories corresponding to the algorithms
    used in the AutoML task. Each of these directories contains a `model.pkl` file
    that contains the serialized model. Why do we have multiple models? Behind the
    scenes, AutoGluon runs a significant number of training experiments using a variety
    of algorithms, along with different combinations of hyperparameter values, to
    produce the “best” model. The “best” model is selected using a certain evaluation
    metric that helps identify which model performs better than the rest. For example,
    if the evaluation metric that’s used is *accuracy*, then a model with an accuracy
    score of 90% (which gets 9 correct answers every 10 tries) is “better” than a
    model with an accuracy score of 80% (which gets 8 correct answers every 10 tries).
    That said, once the models have been generated and evaluated, AutoGluon simply
    chooses the model with the highest evaluation metric value (for example, *accuracy*)
    and tags it as the “best model.”
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在`tmp/models`目录内，我们应该找到`CatBoost`、`ExtraTreesEntr`和`ExtraTreesGini`，以及其他与AutoML任务中使用的算法对应的目录。这些目录中的每一个都包含一个`model.pkl`文件，其中包含序列化的模型。为什么我们有多个模型？在幕后，AutoGluon运行了大量的训练实验，使用各种算法以及不同超参数值的组合，以产生“最佳”模型。使用某种评估指标来选择“最佳”模型，该指标有助于识别哪个模型比其他模型表现更好。例如，如果使用的评估指标是*准确率*，那么准确率为90%（每10次尝试中有9次正确）的模型比准确率为80%（每10次尝试中有8次正确）的模型“更好”。话虽如此，一旦模型生成并评估完毕，AutoGluon将简单地选择评估指标值（例如，*准确率*）最高的模型，并将其标记为“最佳模型”。
- en: 'Now that we have our “best model” ready, what do we do next? The next step
    is for us to evaluate the “best model” using the test dataset. That said, let’s
    prepare the test dataset for inference by removing the target label:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了“最佳模型”，接下来我们该做什么？下一步是我们使用测试数据集来评估“最佳模型”。换句话说，让我们准备测试数据集以进行推理，通过移除目标标签：
- en: '[PRE83]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'With everything ready, let’s use the `predict()` method to predict the `is_cancelled`
    column value of the test dataset provided as the payload:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切准备就绪后，让我们使用`predict()`方法来预测提供的测试数据集的`is_cancelled`列值：
- en: '[PRE85]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now that we have the actual *y* values (`y_test`) and the predicted *y* values
    (`y_pred`), let’s quickly check the performance of the trained model by using
    the `evaluate_predictions()` method:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了实际的*y*值（`y_test`）和预测的*y*值（`y_pred`），让我们快速检查训练模型的性能，通过使用`evaluate_predictions()`方法：
- en: '[PRE86]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The previous block of code should yield performance metric values similar to
    the following:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码块应该产生类似于以下性能指标值的输出：
- en: '[PRE91]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: In this step, we compare the actual values with the predicted values for the
    target column using a variety of formulas that compare how close these values
    are to each other. Here, the goal of the trained models is to make “the least
    number of mistakes” as possible over unseen data. Better models generally have
    better scores for performance metrics such as **accuracy**, **Matthews correlation
    coefficient** (**MCC**), and **F1-score**. We won’t go into the details of how
    model performance metrics work here. Feel free to check out [https://bit.ly/3zn2crv](https://bit.ly/3zn2crv)
    for more information.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们使用各种公式比较实际值与目标列预测值之间的接近程度。在这里，训练模型的目的是在未见数据上尽可能少犯错误。更好的模型通常在性能指标如**准确率**、**马修斯相关系数**（**MCC**）和**F1分数**上得分更高。我们不会在这里详细介绍模型性能指标的工作原理。如有需要，请自由查阅[https://bit.ly/3zn2crv](https://bit.ly/3zn2crv)获取更多信息。
- en: 'Now that we are done with our quick experiment, let’s exit the **IPython**
    shell:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了快速实验，让我们退出**IPython** shell：
- en: '[PRE92]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: There’s more we can do using AutoGluon but this should help us appreciate how
    easy it is to use AutoGluon for AutoML experiments. There are other methods we
    can use, such as `leaderboard()`, `get_model_best()`, and `feature_importance()`,
    so feel free to check out [https://auto.gluon.ai/stable/index.xhtml](https://auto.gluon.ai/stable/index.xhtml)
    for more information.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AutoGluon我们还可以做更多的事情，但这应该有助于我们欣赏使用AutoGluon进行AutoML实验的简便性。我们还可以使用其他方法，例如`leaderboard()`、`get_model_best()`和`feature_importance()`，如有需要，请自由查阅[https://auto.gluon.ai/stable/index.xhtml](https://auto.gluon.ai/stable/index.xhtml)获取更多信息。
- en: Getting started with SageMaker and SageMaker Studio
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用SageMaker和SageMaker Studio
- en: When performing ML and ML engineering on AWS, professionals should consider
    using one or more of the capabilities and features of **Amazon SageMaker**. If
    this is your first time learning about SageMaker, it is a fully managed ML service
    that helps significantly speed up the process of preparing, training, evaluating,
    and deploying ML models.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上执行机器学习和机器学习工程时，专业人士应考虑使用**Amazon SageMaker**的一个或多个功能和特性。如果你是第一次了解SageMaker，它是一个全托管的机器学习服务，有助于显著加快准备、训练、评估和部署机器学习模型的过程。
- en: If you are wondering what these capabilities are, check out some of the capabilities
    tagged under **ML SERVICES** in *Figure 1.2* from the *How ML engineers can get
    the most out of AWS* section. We will tackle several capabilities of SageMaker
    as we go through the different chapters of this book. In the meantime, we will
    start with SageMaker Studio as we will need to set it up first before we work
    on the SageMaker Canvas and SageMaker Autopilot examples.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道这些功能是什么，请查看*《如何利用AWS的机器学习工程师》*章节中*图1.2*下标记为**ML服务**的一些功能。随着我们阅读本书的不同章节，我们将探讨SageMaker的几个功能。在此期间，我们将从SageMaker
    Studio开始，因为在我们处理SageMaker Canvas和SageMaker Autopilot示例之前，我们需要先设置它。
- en: Onboarding with SageMaker Studio
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Studio进行入门
- en: '**SageMaker Studio** provides a feature-rich IDE for ML practitioners. One
    of the great things about SageMaker Studio is its tight integration with the other
    capabilities of SageMaker, which allows us to manage different SageMaker resources
    by just using the interface.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker Studio**为机器学习从业者提供了一个功能丰富的IDE。SageMaker Studio的其中一个优点是它与SageMaker的其他功能紧密集成，这使我们能够仅通过界面来管理不同的SageMaker资源。'
- en: 'For us to have a good idea of what it looks like and how it works, let’s proceed
    with setting up and configuring SageMaker Studio:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们对它的外观和工作方式有一个良好的了解，让我们继续设置和配置SageMaker Studio：
- en: In the search bar of the AWS console, type `sagemaker studio`. Select **SageMaker
    Studio** under **Features**.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台的搜索栏中，输入`sagemaker studio`。在**功能**下选择**SageMaker Studio**。
- en: 'Choose **Standard setup**, as shown in the following screenshot:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**标准设置**，如图下截图所示：
- en: '![Figure 1.13 – Setup SageMaker Domain ](img/B18638_01_013.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图1.13 – 设置SageMaker域](img/B18638_01_013.jpg)'
- en: Figure 1.13 – Setup SageMaker Domain
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13 – 设置SageMaker域
- en: As we can see, **Standard setup** should give us more configuration options
    to tweak over **Quick setup**. Before clicking the **Configure** button, make
    sure that you are using the same region where the S3 bucket and training and test
    datasets are located.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**标准设置**应该会给我们提供比**快速设置**更多的配置选项来调整。在点击**配置**按钮之前，请确保您使用的是S3存储桶和训练及测试数据集所在的相同区域。
- en: Under **Authentication**, select **AWS Identity and Access Management (IAM)**.
    For the default execution role under **Permission**, choose **Create a new role**.
    Choose **Any S3 bucket**. Then, click **Create role**.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**身份验证**下，选择**AWS身份和访问管理(IAM)**。在**权限**下的默认执行角色中，选择**创建新角色**。选择**任何S3存储桶**。然后，点击**创建角色**。
- en: 'Under `us-west-2a`), similar to what is shown in the following screenshot:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`us-west-2a`下)，类似于以下截图所示：
- en: '![Figure 1.14 – Network and Storage Section ](img/B18638_01_014.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![图1.14 – 网络和存储部分](img/B18638_01_014.jpg)'
- en: Figure 1.14 – Network and Storage Section
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14 – 网络和存储部分
- en: Here, we have also configured the SageMaker Domain to use the default SageMaker
    internet access by selecting **Public Internet Only**. Under **Encryption key**,
    we leave this unchanged by choosing **No Custom Encryption**. Review the configuration
    and then click **Next**.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还已将SageMaker域配置为使用默认的SageMaker互联网访问，通过选择**仅公共互联网**。在**加密密钥**下，我们选择**无自定义加密**以保持不变。审查配置后，然后点击**下一步**。
- en: Important note
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that for production environments, the security configuration specified
    in the last few steps needs to be reviewed and upgraded further. In the meantime,
    this should do the trick since we’re dealing with a sample dataset. We will discuss
    how to secure environments in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*,
    Security, Governance, and Compliance Strategies*.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于生产环境，最后几步中指定的安全配置需要审查和进一步升级。同时，由于我们处理的是样本数据集，这应该足够了。我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)*，安全、治理和合规策略*中详细讨论如何保护环境。
- en: Under **Studio settings**, leave everything as-is and click **Next**.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**工作室设置**下，保持一切不变并点击**下一步**。
- en: Similarly, under **General settings** | **RStudio Workbench**, click **Submit**.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，在**常规设置** | **RStudio工作台**下，点击**提交**。
- en: Once you have completed these steps, you should see the **Preparing SageMaker
    Domain** loading message. This step should take around 3 to 5 minutes to complete.
    Once complete, you should see a notification stating **The SageMaker Domain is
    ready**.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你应该会看到**准备SageMaker域**的加载信息。这一步大约需要3到5分钟才能完成。完成后，你应该会看到一个通知，表明**SageMaker域已就绪**。
- en: Adding a user to an existing SageMaker Domain
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向现有SageMaker域添加用户
- en: 'Now that our **SageMaker Domain** is ready, let’s create a user. Creating a
    user is straightforward. So, let’s begin:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了**SageMaker域**，让我们创建一个用户。创建用户很简单，所以让我们开始：
- en: On the **SageMaker Domain/Control Panel** page, click **Add user**.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**SageMaker域/控制面板**页面，点击**添加用户**。
- en: Specify the name of the user under **Name**. Under **Default execution role**,
    select the execution role that you created in the previous step. Click **Next**.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**名称**下指定用户名。在**默认执行角色**下，选择你在上一步中创建的执行角色。点击**下一步**。
- en: Under **Studio settings** | **SageMaker Projects and JumpStart**, click **Next.**
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**工作室设置** | **SageMaker项目和JumpStart**下，点击**下一步**。
- en: Under **RStudio settings** | **Rstudio Workbench**, click **Submit.**
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**RStudio设置** | **Rstudio工作台**下，点击**提交**。
- en: This should do the trick for now. In [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187)*,
    Security, Governance, and Compliance Strategies*, we will review how we can improve
    the configuration here to improve the security of our environment.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 目前这应该就足够了。在[*第9章*](B18638_09.xhtml#_idTextAnchor187)*，安全、治理和合规策略*中，我们将审查如何改进这里的配置以提高我们环境的安全性。
- en: No-code machine learning with SageMaker Canvas
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Canvas进行无代码机器学习
- en: Before we proceed with using the more comprehensive set of SageMaker capabilities
    to perform ML experiments and deployments, let’s start by building a model using
    **SageMaker Canvas**. One of the great things about SageMaker Canvas is that no
    coding work is needed to build models and use them to perform predictions. Of
    course, **SageMaker Autopilot** would have a more powerful and flexible set of
    features, but SageMaker Canvas should help business analysts, data scientists,
    and junior ML engineers understand the ML process and get started building models
    right away.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用更全面的SageMaker功能进行ML实验和部署之前，让我们首先使用**SageMaker Canvas**构建一个模型。SageMaker Canvas的一个优点是构建模型和使用它们进行预测不需要编写代码。当然，**SageMaker
    Autopilot**将提供更强大和灵活的功能集，但SageMaker Canvas应该有助于业务分析师、数据科学家和初级ML工程师理解ML流程并立即开始构建模型。
- en: 'Since our dataset has already been uploaded to the S3 bucket, we can start
    building and training our first SageMaker Canvas model:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集已经上传到S3存储桶，我们可以开始构建和训练我们的第一个SageMaker Canvas模型：
- en: 'On the **SageMaker Domain/Control Panel** page, locate the row of the user
    we just created and click **Launch app**. Choose **Canvas** from the list of options
    available in the drop-down menu, as shown in the following screenshot:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**SageMaker域/控制面板**页面，找到我们刚刚创建的用户所在的行，然后点击**启动应用**。从下拉菜单中选择**Canvas**，如图下所示：
- en: '![Figure 1.15 – Launching SageMaker Canvas ](img/B18638_01_015.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图1.15 – 启动SageMaker Canvas](img/B18638_01_015.jpg)'
- en: Figure 1.15 – Launching SageMaker Canvas
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15 – 启动SageMaker Canvas
- en: As we can see, we can launch SageMaker Canvas from the **SageMaker Domain/Control
    Panel** page. We can launch SageMaker Studio here as well, which we’ll do later
    in this chapter.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们可以从**SageMaker域/控制面板**页面启动SageMaker Canvas。我们也可以在这里启动SageMaker Studio，我们将在本章的后面进行操作。
- en: 'Click **New model**:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**新建模型**：
- en: '![Figure 1.16 – The SageMaker Canvas Models page ](img/B18638_01_016.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![图1.16 – SageMaker Canvas模型页面](img/B18638_01_016.jpg)'
- en: Figure 1.16 – The SageMaker Canvas Models page
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16 – SageMaker Canvas模型页面
- en: Here, we have the SageMaker Canvas **Models** page, which should list the models
    we have trained. Since we have not trained anything yet, we should see the **You
    haven’t created any models yet** message.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有SageMaker Canvas的**模型**页面，它应该列出我们已训练的模型。由于我们还没有训练任何东西，我们应该看到**您尚未创建任何模型**的消息。
- en: In the `first-model`) and click **Create**.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`first-model`)中点击**创建**。
- en: When you see the **Getting Started** guide window, click **Skip intro**.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你看到**入门**指南窗口时，点击**跳过介绍**。
- en: Click `booking.train.csv` and `booking.test.csv` files inside the `Amazon S3/<S3
    BUCKET>/datasets/bookings` folder of the S3 bucket.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在S3存储桶的`Amazon S3/<S3 BUCKET>/datasets/bookings`文件夹中点击`booking.train.csv`和`booking.test.csv`文件。
- en: '![Figure 1.17 – Choose files to import ](img/B18638_01_017.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图1.17 – 选择要导入的文件](img/B18638_01_017.jpg)'
- en: Figure 1.17 – Choose files to import
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17 – 选择要导入的文件
- en: Select the necessary CSV files, as shown in the preceding screenshot, and click
    **Import data**.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 选择必要的CSV文件，如图中所示，然后点击**导入数据**。
- en: Important note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that you may have a hard time locating the S3 bucket we created in the
    *Uploading the dataset to S3* section if you have a significant number of S3 buckets
    in your account. Feel free to use the search box (with the **Search Amazon S3**
    placeholder) located on the right-hand side, just above the table that lists the
    different S3 buckets and resources.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你在你的账户中有大量的S3存储桶，你可能会在“将数据集上传到S3”部分难以找到我们创建的S3存储桶。请随意使用位于右侧、表格上方带有**搜索Amazon
    S3**占位符的搜索框。
- en: Once the files have been imported, click the radio button of the row that contains
    `bookings.train.csv`. Click **Select dataset**.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件导入完成后，点击包含`bookings.train.csv`的行对应的单选按钮。点击**选择数据集**。
- en: In the `is_cancelled` from the list of drop-down options for the **Target column**
    field.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**目标列**字段的下拉选项列表中选择`is_cancelled`。
- en: 'Next, click **Preview model** (under the **Quick build** button), as highlighted
    in the following screenshot:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击**预览模型**（位于**快速构建**按钮下方），如图下所示：
- en: '![Figure 1.18 – The Build tab ](img/B18638_01_018.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![图1.18 – 构建选项卡](img/B18638_01_018.jpg)'
- en: Figure 1.18 – The Build tab
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18 – 构建选项卡
- en: After a few minutes, we should get an estimated accuracy of around 70%. Note
    that you might get a different set of numbers in this step.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，我们应该得到大约70%的估计准确率。请注意，在这个步骤中，你可能会得到不同的数字。
- en: Click **Quick build** and wait for the model to be ready.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**快速构建**并等待模型准备就绪。
- en: Important note
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This step may take up to 15 minutes to complete. While waiting, let’s quickly
    discuss the difference between **Quick build** and **Standard build**. Quick build
    uses fewer records for training and generally lasts around 2 to 15 minutes, while
    Standard build lasts much longer – generally around 2 to 4 hours. It is important
    to note that models that are trained using Quick build can’t be shared with other
    data scientists or ML engineers in SageMaker Studio. On the other hand, models
    trained using Standard build can be shared after the build has been completed.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要15分钟才能完成。在等待期间，让我们快速讨论**快速构建**和**标准构建**之间的区别。快速构建使用较少的记录进行训练，通常持续2到15分钟，而标准构建则持续更长的时间——通常大约2到4小时。重要的是要注意，使用快速构建训练的模型不能在SageMaker
    Studio与其他数据科学家或ML工程师共享。另一方面，使用标准构建训练的模型在构建完成后可以共享。
- en: 'Once the results are available, you may open the **Scoring** tab by clicking
    the tab highlighted in the following screenshot:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦结果可用，您可以通过点击以下截图中的高亮选项卡来打开**评分**选项卡：
- en: '![Figure 1.19 – The Analyze tab ](img/B18638_01_019.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图1.19 – 分析选项卡](img/B18638_01_019.jpg)'
- en: Figure 1.19 – The Analyze tab
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.19 – 分析选项卡
- en: We should see a quick chart showing the number of records that were used to
    analyze the model, along with the number of correct versus incorrect predictions
    the model has made.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个快速图表，显示用于分析模型的记录数，以及模型做出的正确与错误预测的数量。
- en: Important note
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: At this point, we have built an ML model that we can use to predict whether
    a booking will be cancelled or not. Since the accuracy score in this example is
    only around 70%, we’re expecting the model to get about 7 correct answers every
    10 tries. In [*Chapter 11*](B18638_11.xhtml#_idTextAnchor231)*, Machine Learning
    Pipelines with SageMaker Pipelines*, we will train an improved version of this
    model with an accuracy score of around 88%.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了一个可以用来预测预订是否会取消的ML模型。由于在这个例子中准确率分数只有大约70%，我们预计模型在10次尝试中大约会有7次正确答案。在[*第11章*](B18638_11.xhtml#_idTextAnchor231)*，使用SageMaker
    Pipelines的机器学习管道*中，我们将训练这个模型的改进版本，其准确率分数约为88%。
- en: Once we are done checking the different numbers and charts in the **Analyze**
    tab, we can proceed by clicking the **Predict** button.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们检查完**分析**选项卡中的不同数字和图表，我们可以通过点击**预测**按钮继续。
- en: Click `bookings.test.csv` and click **Generate predictions**.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`bookings.test.csv`并点击**生成预测**。
- en: 'Once the **Status** column value is set to **Ready**, hover over the **Status**
    column of the row, click the 3 dots (which will appear after hovering over the
    row), and then select **Preview** from the list of options:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦**状态**列的值设置为**就绪**，将鼠标悬停在行中的**状态**列上，点击3个点（在悬停行后出现），然后从选项列表中选择**预览**：
- en: '![Figure 1.20 – Batch prediction results ](img/B18638_01_020.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![图1.20 – 批量预测结果](img/B18638_01_020.jpg)'
- en: Figure 1.20 – Batch prediction results
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20 – 批量预测结果
- en: We should see a table of values, similar to what is shown in the preceding screenshot.
    In the first column, we should have the predicted values for the `is_cancelled`
    field for each of the rows of our test dataset. In the second column, we should
    find the probability of the prediction being correct.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个值表，类似于前面截图中所显示的。在第一列，我们应该有我们测试数据集每一行的`is_cancelled`字段的预测值。在第二列，我们应该找到预测正确的概率。
- en: Important note
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that we can also perform a single prediction by using the interface provided
    after clicking **Single prediction** under **Predict target values**.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还可以通过点击**预测目标值**下的**单次预测**来使用提供的界面进行单次预测。
- en: Finally, let’s log out of our session. Click the **Account** icon in the left
    sidebar and select the **Log out** option.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们登出我们的会话。点击左侧侧边栏中的**账户**图标，然后选择**登出**选项。
- en: Important note
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure that you always log out of the current session after using SageMaker
    Canvas to avoid any unexpected charges. For more information, go to [https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您在使用SageMaker Canvas后始终登出当前会话，以避免任何意外费用。有关更多信息，请参阅[https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-log-out.xhtml)。
- en: Wasn’t that easy? Now that we have a good idea of how to use SageMaker Canvas,
    let’s run an AutoML experiment using SageMaker Autopilot.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不是很容易吗？现在我们已经对如何使用SageMaker Canvas有了很好的了解，让我们使用SageMaker Autopilot运行一个AutoML实验。
- en: AutoML with SageMaker Autopilot
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Autopilot的AutoML
- en: '**SageMaker Autopilot** allows ML practitioners to build high-quality ML models
    without having to write a single line of code. Of course, it is possible to programmatically
    configure, run, and manage SageMaker Autopilot experiments using the **SageMaker
    Python SDK**, but we will focus on using the SageMaker Studio interface to run
    the AutoML experiment. Before jumping into configuring our first Autopilot experiment,
    let’s see what happens behind the scenes:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker Autopilot**允许机器学习从业者构建高质量的机器学习模型，而无需编写一行代码。当然，可以使用SageMaker Python
    SDK编程配置、运行和管理SageMaker Autopilot实验，但我们将专注于使用SageMaker Studio界面运行AutoML实验。在我们配置第一个Autopilot实验之前，让我们看看幕后发生了什么：'
- en: '![Figure 1.21 – AutoML with SageMaker Autopilot   ](img/B18638_01_021.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图1.21 – 使用SageMaker Autopilot的AutoML](img/B18638_01_021.jpg)'
- en: Figure 1.21 – AutoML with SageMaker Autopilot
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.21 – 使用SageMaker Autopilot的AutoML
- en: 'In the preceding diagram, we can see the different steps that are performed
    by SageMaker Autopilot when we run the AutoML experiment. It starts with the **data
    pre-processing** step and proceeds with the **generation of candidate models**
    (pipeline and algorithm pair) step. Then, it continues to perform the **feature
    engineering** and **model tuning** steps, which would yield multiple trained models
    from different model families, hyperparameter values, and model performance metric
    values. The generated model with the best performance metric values is tagged
    as the “best model” by the Autopilot job. Next, two reports are generated: the
    **explainability report** and the **insights report**. Finally, the model is deployed
    to an inference endpoint.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到SageMaker Autopilot在运行AutoML实验时执行的不同步骤。它从**数据预处理**步骤开始，然后进行**候选模型生成**（管道和算法对）步骤。接着，它继续执行**特征工程**和**模型调优**步骤，这将从不同的模型家族、超参数值和模型性能指标值中产生多个训练模型。具有最佳性能指标值的模型被Autopilot作业标记为“最佳模型”。接下来，生成两个报告：**可解释性报告**和**洞察报告**。最后，模型被部署到推理端点。
- en: 'Let’s dive a bit deeper into what is happening in each step:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解一下每个步骤中发生的情况：
- en: '**Data pre-processing**: Data is cleaned automatically and missing values are
    automatically imputed.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理**：数据会自动清理，缺失值会自动填充。'
- en: '**Candidate definition generation**: Multiple “candidate definitions” (composed
    of a data processing job and a training job) are generated, all of which will
    be used on the dataset.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**候选定义生成**：生成多个“候选定义”（由数据处理作业和训练作业组成），所有这些都将用于数据集上。'
- en: '**Feature engineering**: Here, data transformations are applied to perform
    automated feature engineering.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：在这里，应用数据转换以执行自动特征工程。'
- en: '**Model tuning**: The **Automatic Model Tuning** (hyperparameter tuning) capability
    of SageMaker is used to generate multiple models using a variety of hyperparameter
    configuration values to find the “best model.”'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型调优**：SageMaker的**自动模型调优**（超参数调优）功能用于使用各种超参数配置值生成多个模型，以找到“最佳模型”。'
- en: '**Explainability report generation**: The model explainability report, which
    makes use of SHAP values to help explain the behavior of the generated model,
    is generated using tools provided by **SageMaker Clarify** (another capability
    of SageMaker focused on AI **fairness** and **explainability**). We’ll dive a
    bit deeper into this topic later in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性报告生成**：使用SageMaker Clarify（SageMaker关注AI **公平性**和**可解释性**的另一个功能）提供的工具生成的模型可解释性报告，利用SHAP值帮助解释生成的模型的行为。我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)“安全、治理和合规策略”中稍后更深入地探讨这个话题。'
- en: '**Insights report generation**: The insights report, which includes data insights
    such as scalar metrics, which help us understand our dataset better, is generated.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洞察报告生成**：洞察报告包括数据洞察，如标量指标，这些指标有助于我们更好地理解我们的数据集，该报告被生成。'
- en: '**Model deployment**: The best model is deployed to a dedicated inference endpoint.
    Here, the value of the objective metric is used to determine which is the best
    model out of all the models trained during the model tuning step.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署**：最佳模型被部署到专门的推理端点。在这里，使用目标指标值来确定在模型调优步骤中训练的所有模型中哪个是最佳模型。'
- en: Important note
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you are wondering if AutoML solutions would fully “replace” data scientists,
    then a quick answer to your question would be “no” or “not anytime soon.” There
    are specific areas of the ML process that require domain knowledge to be available
    to data scientists. AutoML solutions help provide a good starting point that data
    scientists and ML practitioners can build on top of. For example, white box AutoML
    solutions such as SageMaker Autopilot can generate scripts and notebooks that
    can be modified by data scientists and ML practitioners to produce custom and
    complex data processing, experiment, and deployment flows and pipelines.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道AutoML解决方案是否会完全“取代”数据科学家，那么对您问题的快速回答将是“不会”或“不会很快”。ML流程的某些特定领域需要数据科学家具备领域知识。AutoML解决方案有助于提供良好的起点，数据科学家和ML从业者可以在其基础上构建。例如，白盒AutoML解决方案如SageMaker
    Autopilot可以生成脚本和笔记本，数据科学家和ML从业者可以对其进行修改，以产生定制和复杂的数据处理、实验和部署流程和管道。
- en: 'Now that we have a better idea of what happens during an Autopilot experiment,
    let’s run our first Autopilot experiment:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Autopilot实验期间发生的事情有了更好的了解，让我们运行我们的第一个Autopilot实验：
- en: 'On the **Control Panel** page, click the **Launch app** drop-down menu and
    choose **Studio** from the list of drop-down options, as shown in the following
    screenshot:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**控制面板**页面，点击**启动应用**下拉菜单，从下拉选项列表中选择**Studio**，如图以下截图所示：
- en: '![Figure 1.22 – Opening SageMaker Studio ](img/B18638_01_022.jpg)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![图1.22 – 打开SageMaker Studio](img/B18638_01_022.jpg)'
- en: Figure 1.22 – Opening SageMaker Studio
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.22 – 打开SageMaker Studio
- en: Note that it may take around 5 minutes for **SageMaker Studio** to load if this
    is your first time opening it.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果这是您第一次打开**SageMaker Studio**，它可能需要大约5分钟才能加载。
- en: Important note
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: AWS releases updates and upgrades for SageMaker Studio regularly. To ensure
    that you are using the latest version, make sure that you shut down and update
    SageMaker Studio and Studio Apps. For more information, go to [https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: AWS定期发布SageMaker Studio的更新和升级。为确保您使用的是最新版本，请确保您关闭并更新SageMaker Studio和Studio
    Apps。更多信息，请访问[https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update.xhtml)。
- en: 'Open the **File** menu and click **Experiment** under the **New** submenu:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**文件**菜单，在**新建**子菜单下点击**实验**：
- en: '![Figure 1.23 – Using the File menu to create a new experiment   ](img/B18638_01_023.jpg)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![图1.23 – 使用文件菜单创建新实验](img/B18638_01_023.jpg)'
- en: Figure 1.23 – Using the File menu to create a new experiment
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.23 – 使用文件菜单创建新实验
- en: Here, we have multiple options under the **New** submenu. We will explore the
    other options throughout this book.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**新建**子菜单下有多个选项。我们将在本书的其余部分探索其他选项。
- en: 'In the next set of steps, we will configure the Autopilot experiment, similar
    to what is shown in the following screenshot:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将配置Autopilot实验，类似于以下截图所示：
- en: '![Figure 1.24 – Configuring the Autopilot experiment ](img/B18638_01_024.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![图1.24 – 配置Autopilot实验](img/B18638_01_024.jpg)'
- en: Figure 1.24 – Configuring the Autopilot experiment
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.24 – 配置Autopilot实验
- en: Here, we can see the different configuration options that are available before
    running the Autopilot experiment. Note that the actual Autopilot experiment settings
    form only has a single column instead of two.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到在运行Autopilot实验之前可用的不同配置选项。请注意，实际的Autopilot实验设置表单只有一列，而不是两列。
- en: Specify the `first-automl-job`).
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`first-automl-job`）。
- en: Under `bookings.train.csv` we uploaded earlier by clicking **Browse**.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们之前通过点击**浏览**上传的`bookings.train.csv`文件下。
- en: 'In the **Target** drop-down menu, choose **is_cancelled**. Click **Next: Training
    method**.'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**目标**下拉菜单中，选择**is_cancelled**。点击**下一步：训练方法**。
- en: 'Leave everything else as is, and then click **Next**: **Deployment and advanced
    settings**.'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他一切保持不变，然后点击**下一步**：**部署和高级设置**。
- en: Make sure that the **Auto deploy**? configuration is set to Yes.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保将**自动部署**？配置设置为是。
- en: Important note
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You may opt to set the **Auto deploy** configuration to **No** instead so that
    an inference endpoint will not be created by the Autopilot job. If you have set
    this to **Yes** make sure that you delete the inference endpoint if you are not
    using it.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择将**自动部署**配置设置为**否**，这样Autopilot作业就不会创建推理端点。如果您将其设置为**是**，请确保您删除了不使用的推理端点。
- en: 'Under **Advanced Settings** (**optional**) **> Runtime**, set **Max Candidates**
    to **20** (or alternatively, setting both **Max trial runtime Minutes** and **Max
    job runtime Minutes** to **20**). Click **Next: Review and create**.'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**高级设置**（可选）**> 运行时**下，将**最大候选数**设置为**20**（或者也可以将**最大试验运行时间分钟**和**最大作业运行时间分钟**都设置为**20**）。点击**下一步：审查和创建**。
- en: Important note
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Setting the value for `20` means that Autopilot will train and consider only
    20 candidate models for this Autopilot job. Of course, we can set this to a higher
    number, which would increase the chance of finding a candidate with a higher evaluation
    metric score (for example, a model that performs better). However, this would
    mean that it would take longer for Autopilot to run since we’ll be running more
    training jobs. Since we are just trying out this capability, we should be fine
    setting `20` in the meantime.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 将值设置为`20`意味着Autopilot将为这个自动驾驶作业训练和考虑仅20个候选模型。当然，我们可以将其设置为更高的数字，这将增加找到具有更高评估指标分数的候选模型（例如，表现更好的模型）的机会。然而，这意味着Autopilot的运行时间会更长，因为我们将会运行更多的训练作业。由于我们只是尝试这个功能，所以我们暂时将`20`设置好应该没问题。
- en: 'Review all the configuration parameters we have set in the previous steps and
    click **Create experiment**. When asked if you want to auto-deploy the best model,
    click **Confirm**. Once the AutoML job has started, we should see a loading screen
    similar to the following:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查我们在之前步骤中设置的配置参数，并点击**创建实验**。当询问是否要自动部署最佳模型时，点击**确认**。一旦AutoML作业开始，我们应该看到一个类似于以下内容的加载屏幕：
- en: '![Figure 1.25 – Waiting for the AutoML job to complete ](img/B18638_01_025.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![图1.25 – 等待AutoML作业完成](img/B18638_01_025.jpg)'
- en: Figure 1.25 – Waiting for the AutoML job to complete
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.25 – 等待AutoML作业完成
- en: 'Here, we can see that the Autopilot job involves the following steps:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到自动驾驶作业涉及以下步骤：
- en: '**Pre-processing**'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**'
- en: '**Candidate Definitions Generated**'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成的候选定义**'
- en: '**Feature Engineering**'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征工程**'
- en: '**Model Tuning**'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型调优**'
- en: '**Explainability Report Generated**'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成的可解释性报告**'
- en: '**Insights Report Generated**'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成的洞察报告**'
- en: '**Deploying Model**'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署模型**'
- en: If we have set the **Auto deploy** configuration to **Yes,** the best model
    is deployed automatically into an inference endpoint that will run 24/7\.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将**自动部署**配置设置为**是**，则最佳模型将自动部署到24/7运行的推理端点。
- en: Important note
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This step may take around 30 minutes to 1 hour to complete. Feel free to get
    a cup of coffee or tea!
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要大约30分钟到1小时才能完成。请随意喝杯咖啡或茶！
- en: 'After about an hour, we should see a list of trials, along with several models
    that have been generated by multiple training jobs, as shown in the following
    screenshot:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 大约一个小时后，我们应该会看到一个试验列表，以及由多个训练作业生成的几个模型，如下面的截图所示：
- en: '![Figure 1.26 – Autopilot job results ](img/B18638_01_026.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![图1.26 – 自动驾驶作业结果](img/B18638_01_026.jpg)'
- en: Figure 1.26 – Autopilot job results
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.26 – 自动驾驶作业结果
- en: 'We should also see two buttons on the top right-hand side of the page: **Open
    candidate generation notebook** and **Open data exploration notebook**. Since
    these two notebooks are generated early in the process, we may see the buttons
    appear about 10 to 15 minutes after the experiment started.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该在页面右上角看到两个按钮：**打开候选生成笔记本**和**打开数据探索笔记本**。由于这两个笔记本是在过程早期生成的，我们可能会在实验开始后大约10到15分钟看到这些按钮出现。
- en: 'Click the **Open candidate generation notebook** and **Open data exploration
    notebook** buttons to open the notebooks that were generated by SageMaker Autopilot:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**打开候选生成笔记本**和**打开数据探索笔记本**按钮，打开由SageMaker Autopilot生成的笔记本：
- en: '![Figure 1.27 – The Data Exploration Report (left) and the Candidate Definition
    Notebook (right) ](img/B18638_01_027.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![图1.27 – 数据探索报告（左）和候选定义笔记本（右）](img/B18638_01_027.jpg)'
- en: Figure 1.27 – The Data Exploration Report (left) and the Candidate Definition
    Notebook (right)
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.27 – 数据探索报告（左）和候选定义笔记本（右）
- en: Here, we can see the **Data Exploration Report** on the left-hand side and the
    **Candidate Definition Notebook** on the right. The **Data Exploration Report**
    helps data scientists and ML engineers identify issues in the given dataset. It
    contains a column analysis report that shows the percentage of missing values,
    along with some count statistics and descriptive statistics. On the other hand,
    the **Candidate Definition Notebook** contains the suggested ML algorithm, along
    with the prescribed hyperparameter ranges. In addition to these, it contains the
    recommended pre-processing steps before the training step starts.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到左侧的**数据探索报告**和右侧的**候选定义笔记本**。**数据探索报告**帮助数据科学家和机器学习工程师识别给定数据集中的问题。它包含一个列分析报告，显示了缺失值的百分比，以及一些计数统计和描述性统计。另一方面，**候选定义笔记本**包含了建议的机器学习算法，以及规定的超参数范围。除此之外，它还包含了在训练步骤开始之前推荐的预处理步骤。
- en: The great thing about these generated notebooks is that we can modify certain
    sections of these notebooks as needed. This makes SageMaker Autopilot easy for
    beginners to use while still allowing intermediate users to customize certain
    parts of the AutoML process.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 这些生成的笔记本的伟大之处在于，我们可以根据需要修改这些笔记本的某些部分。这使得SageMaker Autopilot对初学者来说易于使用，同时仍然允许中级用户自定义AutoML过程的某些部分。
- en: Important note
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you want to know more about SageMaker Autopilot, including the output artifacts
    generated by the AutoML experiment, check out [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132),
    *SageMaker Training and Debugging Solutions*, of the book *Machine Learning with
    Amazon SageMaker Cookbook*. You should find several recipes there that focus on
    programmatically running and managing an Autopilot experiment using the **SageMaker
    Python SDK**.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于SageMaker Autopilot的信息，包括AutoML实验生成的输出工件，请查看《使用Amazon SageMaker CookBook进行机器学习》一书的[*第6章*](B18638_06.xhtml#_idTextAnchor132)，*SageMaker训练和调试解决方案*。您应该在那里找到几个侧重于使用**SageMaker
    Python SDK**编程运行和管理Autopilot实验的食谱。
- en: 'Navigate back to the tab containing the results of the Autopilot job. Right-click
    on the row with the **Best Model** tag and choose **Open in model details** from
    the options in the context menu. This should open a page similar to what is shown
    in the following screenshot:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回包含Autopilot作业结果的标签页。在带有**最佳模型**标签的行上右键单击，并在上下文菜单中选择**在模型详情中打开**。这应该会打开一个类似于以下截图所示的页面：
- en: '![Figure 1.28 – The model details page ](img/B18638_01_028.jpg)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![图1.28 – 模型详情页面](img/B18638_01_028.jpg)'
- en: Figure 1.28 – The model details page
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.28 – 模型详情页面
- en: Here, we can see that **reserved_room_type, lead_time, and adr** are the most
    important features that affect the chance of a hotel booking getting canceled.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到**reserved_room_type、lead_time和adr**是影响酒店预订取消机会的最重要特征。
- en: Note
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Note that you may get a different set of results from what we have in this section.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您可能得到的结果与我们本节中的结果不同。
- en: 'We should see the following information on the model details page as well:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该在模型详情页面上看到以下信息：
- en: Problem type
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题类型
- en: Algorithm used
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用的算法
- en: Location of the input and output artifacts
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和输出工件的位置
- en: Model metric values
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型指标值
- en: Hyperparameter values used to train the model
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型使用的超参数值
- en: Important note
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure that you delete the inference endpoint(s) created after running the
    SageMaker Autopilot experiment. To find the running inference endpoints, simply
    navigate to [https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints)
    and manually delete the unused resources. Note that the link provided assumes
    that the inference endpoint has been created in the **Oregon** (**us-west-2**)
    region. We will skip performing sample predictions using the inference endpoint
    for now. We will cover this, along with deployment strategies, in [*Chapter 7*](B18638_07.xhtml#_idTextAnchor151),
    *SageMaker Deployment Solutions*.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您删除了运行SageMaker Autopilot实验后创建的推理端点（s）。要找到正在运行的推理端点，只需导航到[https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints](https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-west-2#/endpoints)，并手动删除未使用的资源。请注意，提供的链接假设推理端点已在**俄勒冈州**（**us-west-2**）区域创建。我们现在将跳过使用推理端点进行样本预测的操作。我们将在此之后，包括部署策略，在第[*第7章*](B18638_07.xhtml#_idTextAnchor151)，*SageMaker部署解决方案*中介绍。
- en: At this point, we should have a good grasp of how to use several AutoML solutions
    such as **AutoGluon**, **SageMaker Canvas**, and **SageMaker Autopilot**. As we
    saw in the hands-on solutions of this section, we have a significant number of
    options when using SageMaker Autopilot to influence the process of finding the
    best model. If we are more comfortable with a simpler UI with fewer options, then
    we may use SageMaker Canvas instead. If we are more comfortable developing and
    engineering ML solutions through code, then we can consider using AutoGluon as
    well.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该已经很好地掌握了如何使用几个 AutoML 解决方案，例如 **AutoGluon**、**SageMaker Canvas** 和
    **SageMaker Autopilot**。正如我们在本节的动手解决方案中所看到的，当我们使用 SageMaker Autopilot 来影响最佳模型寻找过程时，我们有相当多的选择。如果我们更习惯于一个选项更少的简单
    UI，那么我们可能会选择使用 SageMaker Canvas。如果我们更习惯于通过代码开发和工程 ML 解决方案，那么我们也可以考虑使用 AutoGluon。
- en: Summary
  id: totrans-477
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we got our feet wet by performing multiple AutoML experiments
    using a variety of services, capabilities, and tools on AWS. This included using
    AutoGluon within a Cloud9 environment and SageMaker Canvas and SageMaker Autopilot
    to run AutoML experiments. The solutions presented in this chapter helped us have
    a better understanding of the fundamental ML and ML engineering concepts as well.
    We were able to see some of the steps in the ML process in action, such as EDA,
    train-test split, model training, evaluation, and prediction.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过在 AWS 上使用各种服务、功能和工具进行了多次 AutoML 实验，开始了我们的学习之旅。这包括在 Cloud9 环境中使用 AutoGluon，以及使用
    SageMaker Canvas 和 SageMaker Autopilot 来运行 AutoML 实验。本章中提出的解决方案帮助我们更好地理解了基本的 ML
    和 ML 工程概念。我们能够看到 ML 过程中的一些步骤是如何实际操作的，例如 EDA、训练-测试分割、模型训练、评估和预测。
- en: In the next chapter, we will focus on how the **AWS Deep Learning AMIs** help
    speed up the ML experimentation process. We will also take a closer look at how
    AWS pricing works for EC2 instances so that we are better equipped when managing
    the overall cost of running ML workloads in the cloud.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将关注 **AWS 深度学习 AMI** 如何帮助加快 ML 实验过程。我们还将更深入地了解 AWS 如何为 EC2 实例定价，以便我们在管理云中运行
    ML 工作负载的整体成本时能更好地装备自己。
- en: Further reading
  id: totrans-480
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information regarding the topics that were covered in this chapter,
    check out the following resources:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章涵盖主题的更多信息，请参阅以下资源：
- en: '*AutoGluon: AutoML for Text, Image, and Tabular Data* ([https://auto.gluon.ai/stable/index.xhtml](https://auto.gluon.ai/stable/index.xhtml))'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《AutoGluon：文本、图像和表格数据的 AutoML》([https://auto.gluon.ai/stable/index.xhtml](https://auto.gluon.ai/stable/index.xhtml))
- en: '*Automate model development with Amazon SageMaker Autopilot* (https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.xhtml)'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《使用 Amazon SageMaker Autopilot 自动化模型开发》([https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.xhtml))
- en: '*SageMaker Canvas Pricing* (https://aws.amazon.com/sagemaker/canvas/pricing/)'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《SageMaker Canvas 定价》([https://aws.amazon.com/sagemaker/canvas/pricing/](https://aws.amazon.com/sagemaker/canvas/pricing/))
- en: '*Machine Learning with Amazon SageMaker Cookbook*, by Joshua Arvin Lat ([https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/](https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/))'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《使用 Amazon SageMaker 的机器学习食谱》，作者：Joshua Arvin Lat ([https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/](https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030/))
