- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Open-Source ML Platforms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源机器学习平台
- en: In the previous chapter, we covered how Kubernetes can be used as the foundational
    infrastructure for running ML tasks, such as running model training jobs or building
    data science environments such as **Jupyter Notebook** servers. However, to perform
    these tasks at scale and more efficiently for large organizations, you will need
    to build ML platforms with the capabilities to support the full data science lifecycle.
    These capabilities include scalable data science environments, model training
    services, model registries, and model deployment capabilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何使用Kubernetes作为运行机器学习任务的基础设施，例如运行模型训练作业或构建数据科学环境，如**Jupyter Notebook**服务器。然而，为了在大规模组织中进行这些任务并提高效率，您需要构建具有支持完整数据科学生命周期能力的机器学习平台。这些能力包括可扩展的数据科学环境、模型训练服务、模型注册和模型部署能力。
- en: In this chapter, we will discuss the core components of an ML platform and explore
    additional open-source technologies that can be used for building ML platforms.
    We will begin with technologies designed for building a data science environment
    capable of supporting a large number of users for experimentation. Subsequently,
    we will delve into various technologies for model training, model registries,
    model deployment, and ML pipeline automation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论机器学习平台的核心理念，并探讨可用于构建机器学习平台的额外开源技术。我们将从为支持大量用户进行实验而设计的数据科学环境技术开始。随后，我们将深入研究各种模型训练、模型注册、模型部署和机器学习管道自动化的技术。
- en: 'In a nutshell, the following topics are covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，以下主题被涵盖：
- en: Core components of an ML platform
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习平台的核心理念
- en: Open-source technologies for building ML platforms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建机器学习平台的开源技术
- en: Core components of an ML platform
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习平台的核心理念
- en: An ML platform is a complex system encompassing multiple environments for running
    distinct tasks and orchestrating complex workflow processes. Furthermore, an ML
    platform needs to cater to a multitude of roles, including data scientists, ML
    engineers, infrastructure engineers, operations teams, and security and compliance
    stakeholders. To construct an ML platform, several components come into play.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习平台是一个复杂的系统，包括运行不同任务的环境和编排复杂工作流程过程。此外，机器学习平台需要满足多种角色，包括数据科学家、机器学习工程师、基础设施工程师、运维团队以及安全和合规利益相关者。要构建机器学习平台，需要几个组件共同作用。
- en: 'These components include:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件包括：
- en: '**Data science environment**: The data science environment provides data analysis
    and ML tools, such as Jupyter notebooks, data sources and storage, code repositories,
    and ML frameworks. Data scientists and ML engineers use the data science environment
    to perform data analysis, run data science experiments, and build and tune models.
    The data science environment also provides collaboration capabilities, allowing
    data scientists to share and collaborate on code, data, experiments, and models.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学环境**：数据科学环境提供数据分析工具和机器学习工具，如Jupyter笔记本、数据源和存储、代码仓库和机器学习框架。数据科学家和机器学习工程师使用数据科学环境进行数据分析、运行数据科学实验以及构建和调整模型。数据科学环境还提供协作功能，允许数据科学家共享和协作代码、数据、实验和模型。'
- en: '**Model training environment**: The model training environment provides a separate
    infrastructure tailored to meet specific model training requirements. While data
    scientists and ML engineers can execute small-scale model training tasks directly
    within their local Jupyter environment, they need a separate dedicated infrastructure
    for large-scale model training. By utilizing a dedicated training infrastructure,
    organizations can exercise greater control over model training process management
    and model lineage management processes.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练环境**：模型训练环境提供专门的基础设施，以满足特定的模型训练需求。虽然数据科学家和机器学习工程师可以直接在其本地Jupyter环境中执行小规模的模型训练任务，但他们需要为大规模模型训练提供单独的专用基础设施。通过利用专用训练基础设施，组织可以更好地控制模型训练过程管理和模型血缘管理流程。'
- en: '**Model registry**: Trained models need to be tracked and managed within a
    model registry. The model registry serves as a centralized repository for inventorying
    and managing models, ensuring effective lineage management, version control, model
    discovery, and comprehensive lifecycle management. This becomes particularly significant
    when dealing with a large number of models. Data scientists can register models
    directly in the registry as they perform experiments in their data science environment.
    Additionally, models can be registered as part of automated ML model pipeline
    executions, enabling streamlined and automated integration of models into the
    registry.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册**：训练好的模型需要在模型注册表中进行跟踪和管理。模型注册表作为集中式存储库，用于库存和管理模型，确保有效的血缘管理、版本控制、模型发现和全面的生命周期管理。当处理大量模型时，这一点尤为重要。数据科学家可以直接在他们的数据科学环境中进行实验时在注册表中注册模型。此外，模型可以作为自动化机器学习模型管道执行的一部分进行注册，从而实现模型到注册表的流畅和自动化集成。'
- en: '**Model serving environment**: To serve predictions from trained ML models
    to client applications, it is necessary to host the models within a dedicated
    model serving infrastructure that operates behind an API endpoint in real time.
    This infrastructure should also provide support for batch transform capabilities,
    allowing predictions to be processed in large batches. Several types of model
    serving frameworks are available to fulfill these requirements.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型服务环境**：为了将训练好的机器学习模型的预测结果服务于客户端应用程序，需要在实时操作的后端API端点内托管模型。该基础设施还应提供对批量转换功能的支持，允许在大批量中处理预测。有几种类型的模型服务框架可供满足这些要求。'
- en: '**ML pipeline development:** To effectively manage the various ML components
    and stages in the lifecycle, it is crucial to incorporate capabilities that enable
    pipeline development to orchestrate ML training and prediction workflows. These
    pipelines play an important role in coordinating different stages, such as data
    preparation, model training, and evaluation.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习管道开发**：为了有效地管理生命周期中各种机器学习组件和阶段，关键是要纳入能够使管道开发编排机器学习训练和预测工作流程的能力。这些管道在协调不同阶段，如数据准备、模型训练和评估等方面发挥着重要作用。'
- en: '**Model monitoring**: Robust model monitoring is crucial for maintaining the
    high performance of ML models in production. Continuous monitoring tracks metrics
    like prediction accuracy, data drift, latency, errors, and anomalies over time.
    Monitoring enables platform operators to detect production model degradation before
    it impacts users. When monitored metrics cross defined thresholds, alerts trigger
    investigative workflows and mitigation where needed. Effective monitoring also
    provides performance dashboards and visibility into all deployed models. This
    facilitates continuous improvement of models and allows replacing underperforming
    models proactively.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型监控**：强大的模型监控对于保持生产中机器学习模型的高性能至关重要。持续的监控跟踪预测准确性、数据漂移、延迟、错误和异常等指标。监控使平台操作员能够在影响用户之前检测到生产模型退化。当监控指标超过定义的阈值时，会触发调查工作流程和必要的缓解措施。有效的监控还提供性能仪表板和所有部署模型的可见性。这促进了模型的持续改进，并允许主动替换表现不佳的模型。'
- en: '**ML feature management**: Managing features is a key capability in the ML
    lifecycle. Feature management entails the ongoing curation, monitoring, and sharing
    of ML features to accelerate model development. This includes tools for discovery,
    lineage tracking, and governance of feature data. Centralized feature stores democratize
    access to high-quality features by teams across an organization. They provide
    a single source of truth, eliminating duplication of feature engineering efforts.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习特征管理**：在机器学习生命周期中，管理特征是一项关键能力。特征管理包括对机器学习特征的持续维护、监控和共享，以加速模型开发。这包括用于发现、血缘跟踪和特征数据治理的工具。集中式特征存储通过为组织内的团队提供高质量特征，实现了访问的民主化。它们提供了一个单一的真实来源，消除了特征工程努力的重复。'
- en: '**Continuous integration** (**CI**)/**continuous deployment** (**CD**) **and
    workflow automation**: Finally, to streamline the data processing, model training,
    and model deployment processes on an ML platform, it is crucial to establish CI/CD
    practices, along with workflow automation capabilities. These practices and tools
    significantly contribute to increasing the velocity, consistency, reproducibility,
    and observability of ML deployments.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续集成**（**CI**）/**持续部署**（**CD**）和**工作流程自动化**：最后，为了在机器学习平台上简化数据处理、模型训练和模型部署流程，建立CI/CD实践以及工作流程自动化能力至关重要。这些实践和工具显著提高了机器学习部署的速度、一致性、可重复性和可观察性。'
- en: In addition to these core components, there are several other platform architecture
    factors to consider when building an end-to-end ML platform. These factors include
    security and authentication, version control and reproducibility, and data management
    and governance. By integrating these additional architectural factors into the
    ML platform, organizations can enhance security, gain visibility into system operations,
    and enforce governance policies. In the following sections, we will explore various
    open-source technologies that can be used to build an end-to-end ML platform.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些核心组件之外，在构建端到端机器学习平台时，还需要考虑几个其他平台架构因素。这些因素包括安全性和身份验证、版本控制和可重复性，以及数据管理和治理。通过将这些额外的架构因素整合到机器学习平台中，组织可以增强安全性，对系统操作有更清晰的了解，并执行治理政策。在接下来的章节中，我们将探讨可用于构建端到端机器学习平台的多种开源技术。
- en: Open-source technologies for building ML platforms
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于构建机器学习平台的开源技术
- en: Managing ML tasks individually by deploying standalone ML containers in a Kubernetes
    cluster can become challenging when dealing with a large number of users and workloads.
    To address this complexity and enable efficient scaling, many open-source technologies
    have emerged as viable solutions. These technologies, including Kubeflow, MLflow,
    Seldon Core, GitHub, Feast, and Airflow, provide comprehensive support for building
    data science environments, model training services, model inference services,
    and ML workflow automation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在Kubernetes集群中部署独立的机器学习容器来单独管理机器学习任务，在处理大量用户和工作负载时可能会变得具有挑战性。为了解决这种复杂性并实现高效扩展，许多开源技术已成为可行的解决方案。这些技术包括Kubeflow、MLflow、Seldon
    Core、GitHub、Feast和Airflow，它们为构建数据科学环境、模型训练服务、模型推理服务和机器学习工作流程自动化提供了全面的支持。
- en: Before delving into the technical details, let’s first explore why numerous
    organizations opt for open-source technologies to construct their ML platforms.
    For many, the appeal lies in the ability to tailor the platform to specific organizational
    needs and workflows, with open standards and interoperable components preventing
    vendor lock-in and allowing the flexibility to adopt new technologies over time.
    Leveraging popular open-source ML projects also taps into a rich talent pool,
    as many practitioners are already proficient with these technologies. Additionally,
    open-source allows complete control over the platform roadmap to internal teams,
    reducing dependence on a vendor’s priorities. When executed efficiently, an open-source
    stack can lead to cost savings for organizations, as there are no licensing costs
    associated with the software.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨技术细节之前，让我们首先探讨为什么许多组织选择开源技术来构建他们的机器学习平台。对于许多人来说，吸引力在于能够根据特定的组织需求和流程定制平台，开放标准和可互操作组件防止了供应商锁定，并允许随着时间的推移采用新技术。利用流行的开源机器学习项目还可以利用丰富的人才库，因为许多从业者已经熟练掌握了这些技术。此外，开源允许内部团队完全控制平台路线图，减少了对供应商优先级的依赖。当高效执行时，开源堆栈可以为组织节省成本，因为与软件相关的没有许可费用。
- en: Building an ML platform using open-source technology comes with notable advantages.
    However, it’s also crucial to consider the potential drawbacks. Challenges may
    arise from integration complexities, a lack of comprehensive support, security
    vulnerabilities, and potential limitations in features compared to commercial
    solutions. Additionally, the resource-intensive nature of maintaining an open-source
    platform, coupled with a potential learning curve for the team, could impact efficiency
    and total cost of ownership. Concerns about documentation quality, the absence
    of standardization, and the responsibility for updates and maintenance further
    underscore the need for careful consideration. You must weigh these factors against
    the benefits, taking into account their specific requirements, resources, and
    expertise before opting for an open-source approach.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源技术构建机器学习平台具有显著优势。然而，考虑潜在的缺点也同样重要。可能出现的挑战包括集成复杂性、缺乏全面支持、安全漏洞以及与商业解决方案相比可能的功能限制。此外，维护开源平台的资源密集型特性，以及团队可能面临的学习曲线，可能会影响效率和总拥有成本。对文档质量、缺乏标准化以及更新和维护的责任的担忧，进一步强调了仔细考虑的必要性。在决定采用开源方法之前，您必须权衡这些因素，并考虑其具体要求、资源和专业知识。
- en: With these considerations in mind, let’s explore the design of core ML platform
    components using open-source technologies.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，让我们探讨使用开源技术设计核心机器学习平台组件。
- en: Implementing a data science environment
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施数据科学环境
- en: Kubeflow is an open-source ML platform built on top of Kubernetes. It offers
    a set of tools and frameworks specifically designed to simplify the deployment,
    orchestration, and management of ML workloads. Kubeflow provides features like
    Jupyter notebooks for interactive data exploration and experimentation, distributed
    training capabilities, and model serving infrastructure.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个基于Kubernetes的开源机器学习平台。它提供了一套专门设计的工具和框架，旨在简化机器学习工作的部署、编排和管理。Kubeflow提供了诸如Jupyter笔记本用于交互式数据探索和实验、分布式训练能力和模型服务基础设施等功能。
- en: 'Core capabilities of Kubeflow include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow的核心功能包括：
- en: A central UI dashboard
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个中央UI仪表板
- en: A Jupyter Notebook server for code authoring and model building
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于代码编写和模型构建的Jupyter Notebook服务器
- en: A Kubeflow pipeline for ML pipeline orchestration
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于机器学习流程编排的Kubeflow管道
- en: '**KFServing** for model serving'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KFServing**用于模型服务'
- en: Training operators for model training support
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于模型训练支持的训练操作员
- en: The following figure illustrates how Kubeflow can provide the various components
    needed for a data science environment. Specifically, we will delve into its support
    for Jupyter Notebook servers as it is the main building block for a data science
    environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了Kubeflow如何提供数据科学环境所需的各种组件。具体来说，我们将深入探讨其对Jupyter Notebook服务器的支持，因为它是数据科学环境的主要构建块。
- en: '![Figure 7.1 – A Kubeflow-based data science environment ](img/B20836_07_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 基于Kubeflow的数据科学环境](img/B20836_07_01.png)'
- en: 'Figure 7.1: A Kubeflow-based data science environment'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：基于Kubeflow的数据科学环境
- en: 'Kubeflow provides a multi-tenant Jupyter Notebook server environment with built-in
    authentication and authorization support. Let’s discuss each of these core components
    in detail:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow提供了一个具有内置身份验证和授权支持的多个租户Jupyter Notebook服务器环境。让我们详细讨论这些核心组件：
- en: '**Jupyter Notebook**: As a data scientist, you can take advantage of the Kubeflow
    Jupyter Notebook server, which offers a platform to author and run your **Python**
    code to explore data and build models inside the Jupyter notebook. With Kubeflow,
    you can spawn multiple notebook servers, each server associated with a single
    Kubernetes namespace that corresponds to a team, project, or individual user.
    Each notebook server runs a container inside a **Kubernetes** **Pod**. By default,
    a Kubeflow notebook server provides a list of notebook container images hosted
    in public container image repositories to choose from. Alternatively, you can
    create custom notebook container images to tailor to your specific requirements.
    To ensure standards and consistency, Kubeflow administrators can provide a list
    of standard images for users to use. When creating a notebook server, you select
    the namespace to run the notebook server in. Additionally, you specify the **Universal
    Resource Identifier** (**URI**) of the container image for the notebook server.
    You also have the flexibility to specify the resource requirements, such as the
    number of CPUs/GPUs and memory size.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**：作为数据科学家，您可以利用Kubeflow Jupyter Notebook服务器，该服务器提供了一个平台，用于在Jupyter笔记本中编写和运行**Python**代码以探索数据和构建模型。使用Kubeflow，您可以启动多个笔记本服务器，每个服务器都与一个单独的Kubernetes命名空间相关联，该命名空间对应于一个团队、项目或个人用户。每个笔记本服务器在**Kubernetes**
    **Pod**内部运行一个容器。默认情况下，Kubeflow笔记本服务器提供了一系列托管在公共容器镜像仓库中的笔记本容器镜像供您选择。或者，您也可以创建定制的笔记本容器镜像以满足您的特定需求。为了确保标准和一致性，Kubeflow管理员可以为用户提供标准镜像列表。在创建笔记本服务器时，您选择运行笔记本服务器的命名空间。此外，您还需要指定笔记本服务器的容器镜像的**统一资源标识符**（**URI**）。您还可以灵活地指定资源需求，例如CPU/GPU的数量和内存大小。'
- en: '**Authentication and authorization**: You access the notebook server through
    the Kubeflow UI dashboard, which provides an authentication service through the
    Dex **OpenID Connect** (**OIDC**) provider. Dex is an identity service that uses
    OIDC to provide authentication for other applications. Dex can federate with other
    authentication services such as the **Active Directory** service. Each notebook
    is associated with a default Kubernetes service account (`default-editor`) that
    can be used for entitlement purposes (such as granting the notebook permission
    to access various resources in the Kubernetes cluster). Kubeflow uses Istio **role-based
    access control** (**RBAC**) to control in-cluster traffic. The following **YAML**
    file grants the `default-editor` service account (which is associated with the
    Kubeflow notebook) access to the Kubeflow pipeline service by attaching the `ml-pipeline-services`
    service role to it:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份验证和授权**：您通过Kubeflow UI仪表板访问笔记本服务器，该仪表板通过Dex **OpenID Connect**（**OIDC**）提供者提供身份验证服务。Dex是一个使用OIDC为其他应用程序提供身份验证的标识服务。Dex可以与其他身份验证服务（如**Active
    Directory**服务）进行联合。每个笔记本都与一个默认的Kubernetes服务帐户（`default-editor`）相关联，可用于授权目的（例如，授予笔记本访问Kubernetes集群中各种资源的权限）。Kubeflow使用Istio
    **基于角色的访问控制**（**RBAC**）来控制集群内的流量。以下**YAML**文件通过将`ml-pipeline-services`服务角色附加到它，授予与Kubeflow笔记本关联的`default-editor`服务帐户访问Kubeflow管道服务的权限：'
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Multi-tenancy**: Kubeflow offers the capability for multiple users to access
    a shared Kubeflow environment while ensuring resource isolation. This is achieved
    by creating individual namespaces for each user and leveraging Kubernetes RBAC
    and Istio RBAC to manage access control for these namespaces and their associated
    resources. For collaborative work within teams, the owner of a namespace has the
    ability to grant access to other users directly from the Kubeflow dashboard UI.
    Using the `Manage Contributor` function, the namespace owner can specify which
    users are granted access to the namespace and its resources.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多租户**：Kubeflow提供了多用户访问共享Kubeflow环境的能力，同时确保资源隔离。这是通过为每个用户创建单独的命名空间并利用Kubernetes
    RBAC和Istio RBAC来管理这些命名空间及其相关资源的访问控制来实现的。对于团队内部的协作工作，命名空间的所有者可以直接从Kubeflow仪表板UI授予其他用户的访问权限。使用`管理贡献者`功能，命名空间所有者可以指定哪些用户被授予访问命名空间及其资源的权限。'
- en: In addition to the preceding core components, Kubeflow provides a mechanism
    for onboarding users to access different Kubeflow resources. To onboard a new
    Kubeflow user, you create a new user profile, which automatically generates a
    new namespace for the profile.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述核心组件之外，Kubeflow还提供了一种机制，用于将用户引入以访问不同的Kubeflow资源。要引入新的Kubeflow用户，您需要创建一个新的用户配置文件，这将自动为该配置文件生成一个新的命名空间。
- en: 'The following YAML file, once applied using `kubectl`, creates a new user profile
    called `test-user` with an email of `test-user@kubeflow.org`, and it also creates
    a new namespace called `test-user`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下YAML文件，一旦使用`kubectl`应用，就会创建一个名为`test-user`的新用户配置文件，其电子邮件地址为`test-user@kubeflow.org`，同时还会创建一个名为`test-user`的新命名空间：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can run the `kubectl get profiles` and `kubectl get namespaces` commands
    to verify that the profile and namespaces have been created.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`kubectl get profiles`和`kubectl get namespaces`命令来验证配置文件和命名空间是否已创建。
- en: After a user is created and added to the Kubeflow Dex authentication service,
    the new user can log in to the Kubeflow dashboard and access the Kubeflow resources
    (such as a Jupyter Notebook server) under the newly created namespace.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 用户创建并添加到Kubeflow Dex身份验证服务后，新用户可以登录到Kubeflow仪表板，并在新创建的命名空间下访问Kubeflow资源（如Jupyter
    Notebook服务器）。
- en: Utilizing Kubeflow for a data science environment poses several key challenges
    that one must be aware of before committing to its implementation. Installing
    Kubeflow on top of Kubernetes, whether on-premises or in the cloud on platforms
    like AWS, can be complicated, often requiring substantial configuration and debugging.
    The many moving parts make installation non-trivial. Kubeflow consists of many
    loosely coupled components, each with its own version. Orchestrating these diverse
    components across different versions to work together seamlessly as an integrated
    platform can present difficulties. There is a lack of documentation on Kubeflow.
    Kubeflow documentation often points to older component versions. The out-of-date
    documentation makes adoption more difficult as new Kubeflow users battle mismatches
    between docs and platform versions. Despite these limitations, Kubeflow is a highly
    recommended technology for building ML platforms due to its support for end-to-end
    pipelines, rich support for different ML frameworks, and portability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学环境中使用Kubeflow会带来一些关键挑战，这些挑战在决定实施之前必须了解。在Kubernetes之上安装Kubeflow，无论是在本地还是在AWS等云平台，都可能很复杂，通常需要大量的配置和调试。众多的组件使得安装变得非同寻常。Kubeflow由许多松散耦合的组件组成，每个组件都有自己的版本。在不同版本之间协调这些多样化的组件，以无缝地作为一个集成平台工作，可能会遇到困难。Kubeflow的文档不足。Kubeflow文档经常指向较旧的组件版本。过时的文档使得新用户在文档和平台版本之间出现不匹配时，采用变得更加困难。尽管存在这些限制，但由于其对端到端管道的支持、对各种ML框架的丰富支持以及可移植性，Kubeflow仍然是构建ML平台高度推荐的技术。
- en: With that, we have reviewed how Kubeflow can be used to provide a multi-tenant
    Jupyter Notebook environment for experimentation and model building. Next, let’s
    see how to build a model training environment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经了解了如何使用Kubeflow提供多租户Jupyter Notebook环境进行实验和模型构建。接下来，让我们看看如何构建模型训练环境。
- en: Building a model training environment
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型训练环境
- en: As discussed earlier, within an ML platform, it is common to provide a dedicated
    model training service and infrastructure to support large-scale and automated
    model training in an ML pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在ML平台中，通常提供专门的模型训练服务和基础设施，以支持在ML管道中进行大规模和自动化的模型训练。
- en: This dedicated training service should be easily accessible from different components
    within the platform, such as the experimentation environment (such as a Jupyter
    notebook) as well as the ML automation pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个专门的培训服务应该可以从平台内的不同组件轻松访问，例如实验环境（如Jupyter笔记本）以及ML自动化管道。
- en: 'In a Kubernetes-based environment, there are two main approaches for model
    training you can choose from depending on your training needs:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于Kubernetes的环境中，根据您的训练需求，您可以选择两种主要的模型训练方法：
- en: Model training using **Kubernetes** **Jobs**
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Kubernetes** **作业**进行模型训练
- en: Model training using **Kubeflow** **training operators**
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Kubeflow** **训练操作员**进行模型训练
- en: 'Let’s take a closer look at each one of these approaches in detail:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一详细地审视这些方法：
- en: '**Model training using Kubernetes Jobs**: As we discussed in *Chapter 6*, *Kubernetes
    Container Orchestration Infrastructure Management*, a Kubernetes Job creates one
    or more containers and runs them through to completion. This pattern is well suited
    for running certain types of ML model training jobs, as an ML job runs a training
    loop to completion and does not run forever. For example, you can package a container
    with a Python training script and all the dependencies that train a model and
    use the Kubernetes Job to load the container and kick off the training script.
    When the script completes and exits, the Kubernetes Job also ends. The following
    sample YAML file kicks off a model training job if submitted with the `kubectl
    apply` command:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Kubernetes Jobs进行模型训练**：正如我们在*第6章*，*Kubernetes容器编排基础设施管理*中讨论的那样，Kubernetes
    Job创建一个或多个容器并将它们运行到完成。这种模式非常适合运行某些类型的机器学习模型训练作业，因为机器学习作业运行一个训练循环直到完成，而不是无限期运行。例如，您可以将一个包含Python训练脚本和所有训练模型所需的依赖项的容器打包，并使用Kubernetes
    Job来加载容器并启动训练脚本。当脚本完成并退出时，Kubernetes Job也会结束。以下示例YAML文件，如果使用`kubectl apply`命令提交，将启动一个模型训练作业：'
- en: '[PRE2]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To query the status of the job and see the detailed training logs, you can run
    the `kubectl get jobs` command and the `kubectl logs <pod name>` command, respectively.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要查询作业状态并查看详细的训练日志，您可以分别运行`kubectl get jobs`命令和`kubectl logs <pod name>`命令。
- en: '**Model training using Kubeflow training operators**: A Kubernetes Job can
    launch a model training container and run a training script inside the container
    to completion. Since the controller for a Kubernetes Job does not have application-specific
    knowledge about the training job, it can only handle generic Pod deployment and
    management for the running jobs, such as running the container in a Pod, monitoring
    the Pod, and handling generic Pod failure. However, some model training jobs,
    such as distributed training jobs in a cluster, require the special deployment,
    monitoring, and maintenance of stateful communications among various Pods. This
    is where the Kubernetes training operator pattern can be applied.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用Kubeflow训练操作符进行模型训练**：Kubernetes Job可以启动一个模型训练容器并在容器内运行训练脚本直到完成。由于Kubernetes
    Job的控制器没有关于训练作业的应用特定知识，它只能处理运行作业的通用Pod部署和管理，例如在Pod中运行容器、监控Pod和处理通用Pod故障。然而，某些模型训练作业，如集群中的分布式训练作业，需要特殊部署、监控和维护各个Pod之间状态化通信。这就是Kubernetes训练操作符模式可以应用的地方。'
- en: 'Kubeflow offers a list of pre-built training operators (such as the **TensorFlow**,
    **PyTorch**, and **XGBoost** operators) for complex model training jobs. Each
    Kubeflow training operator has a **custom resource** (**CR**) (for example, `TFJob
    CR` for TensorFlow jobs) that defines the training job’s specific configurations,
    such as the type of Pod in the training job (for example, `master`, `worker`,
    or `parameter server`), or runs policies on how to clean up resources and for
    how long a job should run. The controller for the CR is responsible for configuring
    the training environment, monitoring the training job’s specific status, and maintaining
    the desired training job’s specific state. For instance, the controller can set
    environment variables to make the training cluster specifications (for example,
    types of Pods and indices) available to the training code running inside the containers.
    Additionally, the controller can inspect the exit code of a training process and
    fail the training job if the exit code indicates a permanent failure. The following
    YAML file sample template represents a specification for running training jobs
    using the TensorFlow operator (`tf-operator`):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow提供了一系列预构建的训练操作符（例如**TensorFlow**、**PyTorch**和**XGBoost**操作符）用于复杂的模型训练作业。每个Kubeflow训练操作符都有一个**自定义资源**（**CR**）（例如，TensorFlow作业的`TFJob
    CR`），它定义了训练作业的特定配置，例如训练作业中的Pod类型（例如，`master`、`worker`或`parameter server`），或者运行策略以清理资源以及作业应运行多长时间。CR的控制器负责配置训练环境、监控训练作业的特定状态以及维护期望的训练作业的特定状态。例如，控制器可以设置环境变量，使训练集群规范（例如，Pod类型和索引）对容器内运行的训练代码可用。此外，控制器还可以检查训练过程的退出代码，如果退出代码指示永久性故障，则失败训练作业。以下YAML文件示例模板代表使用TensorFlow操作符（`tf-operator`）运行训练作业的规范：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example template, the specification will create one copy of the parameter
    servers (which aggregate model parameters across different containers) and two
    copies of the workers (which run model training loops and communicate with the
    parameter servers). The operator will process the `TFJob` object according to
    the specification, keep the `TFJob` object stored in the system with the actual
    running services and Pods, and replace the actual state with the desired state.
    You can submit the training job using `kubectl apply -f <TFJob specs template>`
    and can get the status of the `TFJob` with the `kubectl get tfjob` command.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例模板中，规范将创建一个参数服务器副本（它聚合不同容器中的模型参数）和两个工作副本（它们运行模型训练循环并与参数服务器通信）。操作符将根据规范处理`TFJob`对象，将实际运行的服务和Pods中存储的`TFJob`对象保持与系统中的规范一致，并用期望状态替换实际状态。您可以使用`kubectl
    apply -f <TFJob specs template>`提交训练作业，并使用`kubectl get tfjob`命令获取`TFJob`的状态。
- en: As a data scientist, you can submit Kubernetes training jobs or Kubeflow training
    jobs using the `kubectl` utility, or from your Jupyter Notebook environment using
    the **Python SDK**. For example, the `TFJob` object has a Python SDK called `kubernet.tfjob`,
    and Kubernetes has a client SDK called `kubernetes.client` for interacting with
    the Kubernetes and Kubeflow environments from your Python code. You can also invoke
    training jobs using the `Kubeflow Pipeline` component, which we will cover later,
    in the *Kubeflow pipeline* section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，您可以使用`kubectl`实用工具提交Kubernetes训练作业或Kubeflow训练作业，或者从您的Jupyter Notebook环境中使用**Python
    SDK**。例如，`TFJob`对象有一个名为`kubernet.tfjob`的Python SDK，Kubernetes有一个名为`kubernetes.client`的客户端SDK，用于从Python代码与Kubernetes和Kubeflow环境交互。您还可以使用我们将在后面的*Kubeflow管道*部分中介绍的`Kubeflow
    Pipeline`组件调用训练作业。
- en: Using Kubernetes Jobs for ML training requires the installation and configuration
    of the necessary ML software components for training different models using different
    frameworks. You will also need to build logging and monitoring capabilities to
    monitor the training progress.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Kubernetes作业进行机器学习训练需要安装和配置必要的机器学习软件组件，以便使用不同的框架训练不同的模型。您还需要构建日志和监控能力以监控训练进度。
- en: The adoption of Kubeflow training operators also presents its own set of challenges.
    Several operators, including the MPI training operator, are still in the maturing
    phase and are not yet suitable for production adoption. While operators provide
    certain logs and metrics, obtaining a comprehensive view of extensive training
    runs across Pods remains challenging and necessitates the integration of multiple
    dashboards. The existence of separate operators for various ML frameworks with
    fragmented capabilities and statuses complicates achieving a unified experience.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 采用Kubeflow训练操作符也带来了一系列挑战。包括MPI训练操作符在内的几个操作符仍处于成熟阶段，还不适合用于生产环境。虽然操作符提供了一些日志和指标，但获取跨Pods的广泛训练运行的全面视图仍然具有挑战性，需要集成多个仪表板。为各种具有碎片化功能和状态的机器学习框架提供单独的操作符，使得实现统一体验变得更加复杂。
- en: The learning curve for running training operators can be high, as it involves
    understanding many components, such as the development of training YAML files,
    distributed training job configuration, and training job monitoring.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 运行训练操作符的学习曲线可能很高，因为它涉及到理解许多组件，例如训练YAML文件的开发、分布式训练作业配置和训练作业监控。
- en: Registering models with a model registry
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型注册表注册模型
- en: A **model registry** is an important component in model management and governance,
    and it is a key link between the model training stage and the model deployment
    stage, as models need to be properly stored in a managed repository for a governed
    model deployment. There are several open-source options for implementing a model
    registry in an ML platform. In this section, we will explore MLflow Model Registry
    for model management.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型注册表**是模型管理和治理的重要组件，它是模型训练阶段与模型部署阶段之间的关键链接，因为模型需要被适当地存储在受管理的存储库中，以便进行受控的模型部署。在机器学习平台中实现模型注册表有几种开源选项。在本节中，我们将探讨用于模型管理的MLflow模型注册表。'
- en: 'MLflow Model Registry is one of the leading model registry solutions with strong
    support in model artifacts lifecycle management, versioning support, and a range
    of deployment targets like Docker, Amazon SageMaker, and Azure ML. It is designed
    for managing all stages of the ML lifecycle, including experiment management,
    model management, reproducibility, and model deployment. It has the following
    four main components:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow模型注册表是领先的模型注册解决方案之一，在模型工件生命周期管理、版本支持以及Docker、Amazon SageMaker和Azure ML等部署目标方面具有强大的支持。它旨在管理机器学习生命周期的所有阶段，包括实验管理、模型管理、可复现性和模型部署。它具有以下四个主要组件：
- en: '**Experiment tracking**: During model development, data scientists run many
    training jobs as experiments using different datasets, algorithms, and configurations
    to find the best working model. Tracking the inputs and outputs of these experiments
    is critical to ensure efficient progress. Experiment tracking logs the parameters,
    code versions, metrics, and artifacts when running your ML code and for later
    visualizing of the results.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验跟踪：** 在模型开发过程中，数据科学家会使用不同的数据集、算法和配置运行多个训练作业作为实验，以找到最佳的工作模型。跟踪这些实验的输入和输出对于确保高效进展至关重要。实验跟踪会在运行您的机器学习代码时记录参数、代码版本、指标和工件，以便稍后可视化结果。'
- en: '**ML projects**: Projects package data science code in a format to reproduce
    runs on any platform.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习项目：** 项目将数据科学代码打包成可以在任何平台上复现运行的格式。'
- en: '**Models:** Models provide a standard unit for packaging and deploying ML models.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型：** 模型为打包和部署机器学习模型提供了一个标准单元。'
- en: '**Model Registry:** Model Registry stores, annotates, discovers, and manages
    models in a central repository.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册表：** 模型注册表在中央存储库中存储、注释、发现和管理模型。'
- en: The Model Registry component of MLflow provides a central model repository for
    saved models. It captures model details such as model lineage, model version,
    annotation, and description, and also captures model stage transitions from staging
    to production (so the status of the model state is clearly described).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow模型注册表组件提供了一个中央模型存储库，用于保存模型。它捕获模型细节，如模型血缘、模型版本、注释和描述，并捕获模型从预发布到生产的阶段转换（因此模型状态的状态被清楚地描述）。
- en: To use MLflow Model Registry in a team environment, you need to set up an MLflow
    tracking server with a database as a backend and storage for the model artifacts.
    MLflow provides a UI and an API to interact with its core functionality, including
    Model Registry. Once the model is registered in Model Registry, you can add, modify,
    update, transition, or delete the model through the UI or the API.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要在团队环境中使用MLflow模型注册表，您需要设置一个带有数据库作为后端和模型工件存储的MLflow跟踪服务器。MLflow提供UI和API与其核心功能进行交互，包括模型注册表。一旦模型在模型注册表中注册，您就可以通过UI或API添加、修改、更新、过渡或删除模型。
- en: 'The following figure shows an architecture setup for an MLflow tracking server
    and its associated Model Registry:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了MLflow跟踪服务器及其相关模型注册表的架构设置：
- en: '![Figure 7.2 – The MLflow tracking server and model registry ](img/B20836_07_02.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – MLflow跟踪服务器和模型注册表](img/B20836_07_02.png)'
- en: 'Figure 7.2: The MLflow tracking server and Model Registry'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：MLflow跟踪服务器和模型注册表
- en: MLflow supports basic HTTP authentication to enable access control over experiments
    and registered models. The MLflow tracking server also supports basic authentication.
    However, these security capabilities might not be sufficient for enterprise requirements
    such as user group management and integration with third-party authentication
    providers. Organizations often need to implement separate security and authentication
    controls to manage access to resources.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow支持基本HTTP身份验证，以实现对实验和已注册模型的访问控制。MLflow跟踪服务器也支持基本身份验证。然而，这些安全功能可能不足以满足企业需求，例如用户组管理和与第三方身份验证提供商的集成。组织通常需要实施单独的安全和身份验证控制来管理对资源的访问。
- en: Serving models using model serving services
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型服务来提供模型
- en: Once a model has been trained and saved, utilizing it to generate predictions
    is a matter of loading the saved model into an ML package and invoking the appropriate
    model prediction function provided by the package. However, for large-scale and
    complex model serving requirements, you will need to consider implementing a dedicated
    model serving infrastructure to meet those needs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练并保存，利用它来生成预测只需将保存的模型加载到机器学习包中并调用该包提供的适当模型预测函数即可。然而，对于大规模和复杂的模型服务需求，您需要考虑实施专门的服务器模型服务基础设施以满足这些需求。
- en: In the subsequent sections, we will explore a variety of open-source model serving
    frameworks that can assist in addressing such needs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨各种开源模型服务框架，这些框架可以帮助解决这些需求。
- en: The Gunicorn and Flask inference engine
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gunicorn 和 Flask 推理引擎
- en: '**Gunicorn** and **Flask** are often used for building custom model serving
    web frameworks. The following figure shows a typical architecture that uses Flask,
    Gunicorn, and Nginx as the building blocks for a model serving service.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**Gunicorn** 和 **Flask** 常用于构建自定义模型服务 Web 框架。以下图显示了使用 Flask、Gunicorn 和 Nginx
    作为模型服务服务的构建块的一个典型架构。'
- en: '![Figure 7.3 – A model serving architecture using Flask and Gunicorn ](img/B20836_07_03.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 使用 Flask 和 Gunicorn 的模型服务架构](img/B20836_07_03.png)'
- en: 'Figure 7.3: A model serving architecture using Flask and Gunicorn'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：使用 Flask 和 Gunicorn 的模型服务架构
- en: Flask is a Python-based micro web framework for building web apps quickly. It
    is lightweight and has almost no dependencies on external libraries. With Flask,
    you can define different invocation routes and associate handler functions to
    handle different web calls (such as health check calls and model invocation calls).
    To handle model prediction requests, the Flask app would load the model into memory
    and call the `predict` function on the model to generate the prediction. Flask
    comes with a built-in web server, but it does not scale well as it can only support
    one request at a time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Flask 是一个基于 Python 的微型网络框架，用于快速构建 Web 应用。它轻量级，几乎不依赖于外部库。使用 Flask，您可以定义不同的调用路由并将处理函数关联到不同的
    Web 调用（例如健康检查调用和模型调用）。为了处理模型预测请求，Flask 应用将模型加载到内存中，并在模型上调用 `predict` 函数以生成预测。Flask
    内置了网络服务器，但它扩展性不佳，因为它一次只能支持一个请求。
- en: This is where Gunicorn can help address the scalability gap. Gunicorn is a web
    server for hosting web apps, including Flask apps. It can handle multiple requests
    in parallel and distribute the traffic to the hosted web apps efficiently. When
    it receives a web request, it will invoke the hosted Flask app to handle the request,
    such as invoking the function to generate the model prediction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 Gunicorn 可以帮助解决可扩展性差距的地方。Gunicorn 是一个用于托管 Web 应用的网络服务器，包括 Flask 应用。它可以并行处理多个请求并有效地将流量分配给托管
    Web 应用。当它收到一个 Web 请求时，它将调用托管的 Flask 应用来处理请求，例如调用函数以生成模型预测。
- en: In addition to serving prediction requests as web requests, an enterprise inference
    engine also needs to handle secure web traffic (such as SSL/TLS traffic), as well
    as load balancing when there are multiple web servers. This is where Nginx can
    play an important role. Nginx can serve as a load balancer for multiple web servers
    and can handle termination for SSL/TLS traffic more efficiently, so web servers
    do not have to handle it.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了作为 Web 请求提供预测请求之外，企业推理引擎还需要处理安全的 Web 流量（例如 SSL/TLS 流量），以及当有多个 Web 服务器时的负载均衡。这正是
    Nginx 可以发挥重要作用的地方。Nginx 可以作为多个 Web 服务器的负载均衡器，并且可以更有效地处理 SSL/TLS 流量的终止，这样 Web 服务器就不必处理它。
- en: A Flask/Gunicorn-based model serving architecture can be a good option for hosting
    simple model serving patterns. But for more complicated patterns such as serving
    different versions of models, A/B testing (showing two variants of a model to
    different user groups and comparing their responses), or large model serving,
    this architecture will have limitations. The Flask/Gunicorn architecture pattern
    also requires custom code (such as the Flask app) to work, as it does not provide
    built-in support for the different ML models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Flask/Gunicorn 的模型服务架构可以是托管简单模型服务模式的好选择。但对于更复杂的模式，例如托管不同版本的模型、A/B 测试（向不同的用户群体展示模型的两个变体并比较它们的响应）或大型模型服务，这种架构将有限制。Flask/Gunicorn
    架构模式还需要自定义代码（如 Flask 应用）才能工作，因为它不提供对不同机器学习模型的内置支持。
- en: Next, let’s explore some purpose-built model serving frameworks and see how
    they are different from the custom Flask-based inference engine.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索一些专门设计的模型服务框架，并看看它们与基于 Flask 的自定义推理引擎有何不同。
- en: The TensorFlow Serving framework
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow Serving 框架
- en: '**TensorFlow Serving** is a production-grade, open-source model serving framework,
    and provides out-of-the-box support for serving TensorFlow models behind a RESTFul
    endpoint. It manages the model lifecycle for model serving and provides access
    to versioned and multiple models behind a single endpoint. There is also built-in
    support for canary deployments. A **canary deployment** allows you to deploy a
    model to support a subset of traffic. In addition to the real-time inference support,
    there is also a batch scheduler feature that can batch multiple prediction requests
    and perform a single joint execution. With TensorFlow Serving, there is no need
    to write custom code to serve the model.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Serving** 是一个生产级、开源的模型服务框架，并提供了在 RESTFul 端点后服务 TensorFlow 模型的即用型支持。它管理模型服务的生命周期，并为单个端点后提供版本化和多个模型提供访问。此外，还内置了对金丝雀部署的支持。**金丝雀部署**
    允许您将模型部署以支持部分流量。除了实时推理支持外，还有一个批量调度功能，可以批量处理多个预测请求并执行单个联合操作。使用 TensorFlow Serving，无需编写自定义代码来服务模型。'
- en: 'The following figure shows the architecture of TensorFlow Serving:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 TensorFlow Serving 的架构：
- en: '![Figure 7.4 – TensorFlow Serving architecture ](img/B20836_07_04.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – TensorFlow Serving 架构](img/B20836_07_04.png)'
- en: 'Figure 7.4: TensorFlow Serving architecture'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：TensorFlow Serving 架构
- en: 'Let’s discuss each of the architecture components in more detail:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论每个架构组件：
- en: The *API handler* provides APIs for TensorFlow Serving. It comes with a built-in,
    lightweight HTTP server to serve RESTful-based API requests. It also supports
    **gRPC** (a **remote procedure call** protocol) traffic. gRPC is a more efficient
    and fast networking protocol; however, it is more complicated to use than the
    REST protocol. TensorFlow Serving has a concept called a *servable*, which refers
    to the actual objects that handle a task, such as model inferences or lookup tables.
    For example, a trained model is represented as a *servable*, and it can contain
    one or more algorithms and lookup tables or embedding tables. The API handler
    uses the servable to fulfill client requests.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*API 处理器* 为 TensorFlow Serving 提供接口。它内置了一个轻量级的 HTTP 服务器，用于服务基于 REST 的 API 请求。它还支持
    **gRPC**（一种 **远程过程调用** 协议）流量。gRPC 是一种更高效、更快的网络协议；然而，它比 REST 协议更复杂。TensorFlow Serving
    有一个称为 *可服务对象* 的概念，它指的是处理任务的实际对象，例如模型推理或查找表。例如，训练好的模型表示为一个 *可服务对象*，它可以包含一个或多个算法和查找表或嵌入表。API
    处理器使用可服务对象来满足客户端请求。'
- en: The *model manager* manages the lifecycle of servables, including loading the
    servables, serving the servables, and unloading the servables. When a servable
    is needed to perform a task, the model manager provides the client with a handler
    to access the servable instances. The model manager can manage multiple versions
    of a servable, allowing gradual rollout of different versions of a model.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型管理器* 负责管理可服务对象的生命周期，包括加载可服务对象、提供可服务对象服务以及卸载可服务对象。当需要可服务对象来执行任务时，模型管理器向客户端提供一个处理程序以访问可服务对象实例。模型管理器可以管理多个版本的可服务对象，允许逐步推出不同版本的模型。'
- en: The *model loader* is responsible for loading models from different sources,
    such as **Amazon** **S3**. When a new model is loaded, the model loader notifies
    the model manager about the availability of the new model, and the model manager
    will decide what the next step should be, such as unloading the previous version
    and loading the new version.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型加载器* 负责从不同来源加载模型，例如 **Amazon** **S3**。当加载新模型时，模型加载器会通知模型管理器新模型的可用性，模型管理器将决定下一步应该做什么，例如卸载旧版本并加载新版本。'
- en: TensorFlow Serving can be extended to support non-TensorFlow models. For example,
    models trained in other frameworks can be converted to the **ONNX** format and
    served using TensorFlow Serving. ONNX is a common format for representing models
    to support interoperability across different ML frameworks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving 可以扩展以支持非 TensorFlow 模型。例如，在其他框架中训练的模型可以被转换为 **ONNX** 格式，并使用
    TensorFlow Serving 提供服务。ONNX 是一种用于表示模型以支持不同机器学习框架之间互操作性的通用格式。
- en: The TorchServe serving framework
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: The TorchServe serving framework
- en: '**TorchServe** is an open-source framework for serving trained **PyTorch**
    models. Similar to TensorFlow Serving, TorchServe provides a REST API for serving
    models with its built-in web server. With core features such as multi-model serving,
    model versioning, server-side request batching, and built-in monitoring, TorchServe
    can serve production workloads at scale. There is also no need to write custom
    code to host PyTorch models with TorchServe. In addition, TorchServe comes with
    a built-in web server for hosting the model.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**TorchServe**是一个用于部署训练好的**PyTorch**模型的开源框架。类似于TensorFlow Serving，TorchServe通过其内置的Web服务器提供REST
    API来部署模型。它具有多模型部署、模型版本控制、服务器端请求批处理和内置监控等核心功能，能够大规模地部署生产工作负载。使用TorchServe部署PyTorch模型无需编写自定义代码。此外，TorchServe还内置了Web服务器来托管模型。'
- en: 'The following figure illustrates the architecture components of the TorchServe
    framework:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了TorchServe框架的架构组件：
- en: '![Figure 7.5 – TorchServe architecture ](img/B20836_07_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – TorchServe架构](img/B20836_07_05.png)'
- en: 'Figure 7.5: TorchServe architecture'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：TorchServe架构
- en: The *inference API* is responsible for handling prediction requests from client
    applications using loaded PyTorch models. It supports the REST protocol and provides
    a prediction API, as well as other supporting APIs such as health check and model
    explanation APIs. The inference API can handle prediction requests for multiple
    models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理API**负责处理来自客户端应用程序的预测请求，这些请求使用加载的PyTorch模型。它支持REST协议，并提供预测API以及其他支持API，如健康检查和模型解释API。推理API可以处理多个模型的预测请求。'
- en: The model artifacts are packaged into a single archive file and stored in a
    model store within the TorchServe environment. You use a **command-line interface**
    (**CLI**) command called `torch-mode-archive` to package the model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 模型工件被打包成一个单独的归档文件，并存储在TorchServe环境中的模型存储中。您可以使用名为`torch-mode-archive`的**命令行界面**（CLI）命令来打包模型。
- en: The TorchServe backend loads the archived models from the model store into different
    worker processes. These worker processes interact with the inference API to process
    requests and send back responses.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe后端将从模型存储中加载归档模型到不同的工作进程中。这些工作进程与推理API交互，处理请求并发送响应。
- en: The management API is responsible for handling management tasks such as registering
    and unregistering PyTorch models, checking the model status, and scaling the worker
    process. The management API is normally used by system administrators.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 管理API负责处理管理任务，如注册和注销PyTorch模型、检查模型状态以及扩展工作进程。管理API通常由系统管理员使用。
- en: TorchServe also provides built-in support for logging and metrics. The logging
    component logs both access logs and processing logs. The TorchServe metrics collect
    a list of system metrics, such as CPU/GPU utilization and custom model metrics.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe还提供了内置的日志记录和指标支持。日志组件记录访问日志和处理日志。TorchServe指标收集系统指标列表，例如CPU/GPU利用率以及自定义模型指标。
- en: KFServing framework
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KFServing框架
- en: 'TensorFlow Serving and TorchServe are standalone model serving frameworks for
    a specific deep learning framework. In contrast, **KFServing** is a general-purpose,
    multi-framework, model serving framework that supports different ML models. KFServing
    uses standalone model serving frameworks such as TensorFlow Serving and TorchServe
    as the backend model servers. It is part of the Kubeflow project and provides
    pluggable architecture for different model formats:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving和TorchServe是特定深度学习框架的独立模型部署框架。相比之下，**KFServing**是一个通用、多框架、支持不同机器学习模型的模型部署框架。KFServing使用如TensorFlow
    Serving和TorchServe这样的独立模型部署框架作为后端模型服务器。它是Kubeflow项目的一部分，并为不同的模型格式提供了可插拔的架构：
- en: '![Figure 7.6 – KFServing components ](img/B20836_07_06.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – KFServing组件](img/B20836_07_06.png)'
- en: 'Figure 7.6: KFServing components'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：KFServing组件
- en: 'As a general-purpose, multi-framework model serving solution, KFServing provides
    several out-of-the-box model servers (also known as predictors) for different
    model types, including TensorFlow, **PyTorch** **XGBoost**, **scikit-learn**,
    and ONNX. With KFServing, you can serve models using both REST and gRPC protocols.
    To deploy a supported model type, you simply need to define a YAML specification
    that points to the model artifact in a data store. Furthermore, you can build
    your own custom containers to serve models in KFServing. The container needs to
    provide a model serving implementation as well as a web server. The following
    code shows a sample YAML specification to deploy a `tensorflow` model using KFServing:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种通用、多框架的模型服务解决方案，KFServing 为不同类型的模型提供了多个开箱即用的模型服务器（也称为预测器），包括 TensorFlow、**PyTorch**、**XGBoost**、**scikit-learn**
    和 ONNX。使用 KFServing，您可以使用 REST 和 gRPC 协议来提供服务。要部署支持的模型类型，您只需定义一个指向数据存储中模型实体的 YAML
    规范即可。此外，您还可以构建自己的自定义容器，在 KFServing 中提供服务。该容器需要提供模型服务实现以及一个网络服务器。以下代码展示了使用 KFServing
    部署 `tensorflow` 模型的示例 YAML 规范：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: KFServing has a transformer component that allows the custom processing of the
    input payload before it is sent to the predictors, and also allows transforming
    the response from the predictor before it is sent back to the calling client.
    Sometimes, you need to provide an explanation for the model prediction, such as
    which features have a stronger influence on the prediction, which we will cover
    in more detail in a later chapter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: KFServing 具有一个转换组件，允许在将输入有效载荷发送到预测器之前对其进行自定义处理，并在将响应发送回调用客户端之前对预测器的响应进行转换。有时，您需要为模型预测提供解释，例如哪些特征对预测有更强的影响，我们将在后面的章节中更详细地介绍。
- en: KFServing is designed for production deployment and provides a range of production
    deployment capabilities. Its auto-scaling feature allows the model server to scale
    up/down based on the amount of request traffic. With KFServing, you can deploy
    both the default model serving endpoint and the canary endpoint, split the traffic
    between the two, and specify model revisions behind the endpoint. For operational
    support, KFServing also has built-in functionality for monitoring (for example,
    monitoring request data and request latency).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: KFServing 设计用于生产部署，并提供了一系列生产部署功能。其自动扩展功能允许模型服务器根据请求流量量进行扩展/缩减。使用 KFServing，您可以部署默认模型服务端点和金丝雀端点，在两者之间分配流量，并指定端点后面的模型修订版。对于运营支持，KFServing
    还内置了监控功能（例如，监控请求数据和请求延迟）。
- en: Seldon Core
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Seldon Core
- en: '**Seldon Core** is another multi-framework model serving framework for deploying
    models on Kubernetes. Compared to KFServing, Seldon Core provides richer model
    serving features, for example, model serving inference graphs for use cases such
    as A/B testing and model ensembles. The following figure shows the core components
    of the Seldon Core framework:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Seldon Core** 是另一个多框架模型服务框架，用于在 Kubernetes 上部署模型。与 KFServing 相比，Seldon Core
    提供了更丰富的模型服务功能，例如用于 A/B 测试和模型集成的模型服务推理图。以下图显示了 Seldon Core 框架的核心组件：'
- en: '![Figure 7.7 – The Seldon Core model serving framework architecture ](img/B20836_07_07.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – Seldon Core 模型服务框架架构](img/B20836_07_07.png)'
- en: 'Figure 7.7: The Seldon Core model serving framework architecture'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：Seldon Core 模型服务框架架构
- en: Seldon Core provides packaged model servers for some of the common ML libraries,
    including the `SKLearn` server for scikit-learn models, the XGBoost server for
    XGBoost models, TensorFlow Serving for TensorFlow models, and MLflow server-based
    model serving. You can also build your own custom serving container for specific
    model serving needs and host it using Seldon Core.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon Core 为一些常见的机器学习库提供了打包的模型服务器，包括用于 scikit-learn 模型的 `SKLearn` 服务器、用于 XGBoost
    模型的 XGBoost 服务器、用于 TensorFlow 模型的 TensorFlow Serving 以及基于 MLflow 的模型服务。您还可以为特定的模型服务需求构建自己的自定义服务容器，并使用
    Seldon Core 进行托管。
- en: 'The following template shows how to deploy a model using the SKLearn server
    using Seldon Core. You simply need to change the `modelUri` path to point to a
    saved model on a cloud object storage provider such as **Google** **Cloud Storage**,
    **Amazon S3 storage**, or **Azure** **Blob storage**. To test with an example,
    you can change the following `modelUri` value to an example provided by Seldon
    Core – `gs://seldon-models/sklearn/iris`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模板展示了如何使用Seldon Core通过SKLearn服务器部署模型。您只需更改`modelUri`路径，使其指向云对象存储提供商上保存的模型，例如**Google
    Cloud Storage**、**Amazon S3存储**或**Azure Blob存储**。为了测试示例，您可以将以下`modelUri`值更改为Seldon
    Core提供的示例 - `gs://seldon-models/sklearn/iris`：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Seldon Core also supports an advanced workflow, known as an inference graph,
    for serving models. The *inference graph* feature allows you to have a graph with
    different models and other components in a single inference pipeline. An inference
    graph can consist of several components:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Seldon Core还支持一种高级工作流程，称为推理图，用于服务模型。**推理图**功能允许您在单个推理管道中拥有不同模型和其他组件的图。一个推理图可以由几个组件组成：
- en: One or more ML models for the different prediction tasks
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于不同预测任务的一个或多个ML模型
- en: Traffic routing management for different usage patterns, such as traffic splitting
    to different models for A/B testing
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同使用模式（如A/B测试中对不同模型的流量分割）的流量路由管理
- en: A component for combining results from multiple models, such as a model ensemble
    component
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于组合多个模型结果的组件，例如模型集成组件
- en: Components for transforming the input requests (such as performing feature engineering)
    or output responses (for example, returning an array format as a **JSON** format)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于转换输入请求（例如执行特征工程）或输出响应（例如，以**JSON**格式返回数组格式）的组件
- en: 'To build inference graph specifications in YAML, you need the following key
    components in the `seldondeployment` YAML file:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要在YAML中构建推理图规范，您需要在`seldondeployment` YAML文件中包含以下关键组件：
- en: A list of predictors, with each predictor having its own `componentSpecs` section
    that specifies details such as container images
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预测因子列表，每个预测因子都有自己的`componentSpecs`部分，指定容器镜像等详细信息
- en: A graph that describes how the components are linked together for each `componentSpecs`
    section
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述每个`componentSpecs`部分中组件如何相互连接的图
- en: 'The following sample template shows the inference graph for a custom canary
    deployment to split the traffic into two different versions of a model – one with
    75% of the traffic and another one with 25% of the traffic:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例模板展示了自定义金丝雀部署的推理图，将流量分割为模型的两个不同版本 - 一个有75%的流量，另一个有25%的流量：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once a deployment manifest is applied, the Seldon Core operator is responsible
    for creating all the resources needed to serve an ML model. Specifically, the
    operator will create resources defined in the manifest, add orchestrators to the
    Pods to manage the orchestration of the inference graph, and configure the traffic
    using ingress gateways such as Istio.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用部署清单，Seldon Core操作员负责创建用于服务ML模型所需的所有资源。具体来说，操作员将创建清单中定义的资源，向Pod添加编排器以管理推理图的编排，并使用如Istio等入口网关配置流量。
- en: Triton Inference Server
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Triton推理服务器
- en: 'Triton Inference Server is open-source software designed to streamline the
    process of AI inferencing. It offers a versatile solution for deploying AI models
    from various deep learning and ML frameworks, including TensorRT, TensorFlow,
    PyTorch, ONNX, OpenVINO, Python, and more. Triton is compatible with a wide range
    of devices, supporting inference across cloud environments, data centers, edge
    devices, and embedded systems. Compared to Seldon Core, Triton Inference Server
    is more focused on performance. It is designed to be highly scalable and efficient,
    making it a good choice for high-traffic applications. The following figure depicts
    the core components of Triton Inference Server:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Triton推理服务器是一种开源软件，旨在简化AI推理的过程。它为从各种深度学习和ML框架（包括TensorRT、TensorFlow、PyTorch、ONNX、OpenVINO、Python等）部署AI模型提供了一种灵活的解决方案。Triton与广泛的设备兼容，支持在云环境、数据中心、边缘设备和嵌入式系统中进行推理。与Seldon
    Core相比，Triton推理服务器更注重性能。它被设计为高度可扩展和高效，是高流量应用的理想选择。以下图展示了Triton推理服务器的核心组件：
- en: '![A picture containing text, screenshot, diagram, number  Description automatically
    generated](img/B20836_07_08.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、数字的图片，自动生成描述](img/B20836_07_08.png)'
- en: 'Figure 7.8: Triton Inference Server architecture'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：Triton推理服务器架构
- en: The Triton Inference Server architecture encompasses several components that
    work together to enable efficient and scalable inferencing. At its core is the
    backend, which represents specific deep learning or ML frameworks supported by
    Triton. Each backend handles the loading and execution of models trained with
    its corresponding framework. Triton Inference Server acts as the central hub,
    receiving and managing inference requests. It communicates with clients, such
    as web applications or services, and orchestrates the inferencing process. The
    model repository serves as the storage location for trained models. It contains
    serialized versions of models compatible with the supported backends. When requested
    by clients, the server accesses and loads the models into memory for inferencing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Triton推理服务器架构包括多个组件，它们协同工作以实现高效和可扩展的推理。其核心是后端，代表Triton支持的特定深度学习或机器学习框架。每个后端处理使用其相应框架训练的模型的加载和执行。Triton推理服务器作为中央枢纽，接收和管理推理请求。它与客户端（如Web应用程序或服务）通信，并协调推理过程。模型仓库作为训练模型的存储位置。它包含与支持的各个后端兼容的模型序列化版本。当客户端请求时，服务器会访问并加载模型到内存中进行推理。
- en: Triton supports multiple inference protocols, including HTTP/REST and gRPC,
    allowing clients to communicate with the server and make inference requests. Clients
    can specify input data and desired output formats using these protocols. To monitor
    and optimize performance, Triton provides metrics and monitoring capabilities.
    These metrics include GPU utilization, server throughput, latency, and other relevant
    statistics. Monitoring these metrics helps administrators optimize resource utilization
    and identify potential bottlenecks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Triton支持多种推理协议，包括HTTP/REST和gRPC，允许客户端与服务器通信并发出推理请求。客户端可以使用这些协议指定输入数据和期望的输出格式。为了监控和优化性能，Triton提供了指标和监控功能。这些指标包括GPU利用率、服务器吞吐量、延迟以及其他相关统计数据。监控这些指标有助于管理员优化资源利用并识别潜在的瓶颈。
- en: Triton also offers dynamic batching capability. This feature allows for efficient
    processing of multiple inference requests by grouping them together into batches.
    This batching mechanism optimizes resource utilization and improves overall inferencing
    performance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Triton还提供动态批处理功能。此功能允许通过将多个推理请求分组到一起成批处理来高效处理。这种批处理机制优化了资源利用并提高了整体推理性能。
- en: Overall, the architecture of Triton Inference Server is designed to facilitate
    the efficient deployment and execution of AI models across diverse frameworks
    and hardware platforms. It offers flexibility, scalability, and extensibility,
    enabling organizations to leverage their preferred frameworks while ensuring high-performance
    inferencing capabilities.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Triton推理服务器架构旨在促进AI模型在多种框架和硬件平台上的高效部署和执行。它提供灵活性、可扩展性和可扩展性，使组织能够利用其首选框架，同时确保高性能的推理能力。
- en: Monitoring models in production
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控生产中的模型
- en: Model performance can deteriorate over time due to various factors such as changing
    data patterns, shifts in user behavior, or unforeseen scenarios. To ensure the
    ongoing effectiveness of deployed ML models, continuous monitoring of their performance
    and behavior in production is essential.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种因素，如数据模式的变化、用户行为的转变或不可预见的情况，模型性能可能会随时间恶化。为确保部署的机器学习模型持续有效，对它们在生产中的性能和行为进行持续监控至关重要。
- en: 'Model monitoring involves actively tracking and analyzing the performance of
    deployed ML models. This process includes collecting data on different metrics
    and indicators, comparing them to predefined thresholds or baselines, and identifying
    anomalies or deviations from expected behavior. Two critical aspects of model
    monitoring are data drift and model drift:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控涉及积极跟踪和分析已部署的机器学习模型的性能。这个过程包括收集不同指标和指标的数据，将它们与预定义的阈值或基线进行比较，并识别异常或与预期行为不符的情况。模型监控的两个关键方面是数据漂移和模型漂移：
- en: '**Data drift**: Data drift refers to the scenarios where the statistical properties
    of incoming data change over time. This can create a disconnect between the data
    used to train the model and the data it encounters in the production environment.
    Data drift significantly impacts the performance and reliability of ML models,
    as they may struggle to adapt to new and evolving patterns in the data.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移**：数据漂移指的是随时间变化的数据的统计属性发生变化的情况。这可能导致用于训练模型的数据与生产环境中遇到的数据之间产生脱节。数据漂移对机器学习模型的表现和可靠性有重大影响，因为它们可能难以适应数据中的新和演变模式。'
- en: '**Model drift**: Model drift refers to the degradation of an ML model’s performance
    over time due to changes in underlying patterns or relationships in the data.
    When the assumptions made during model training no longer hold true in the production
    environment, model drift occurs. It can lead to decreased accuracy, increased
    errors, and suboptimal decision-making.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型漂移**：模型漂移指的是由于数据中潜在模式或关系的变化而导致机器学习模型性能随时间退化的情况。当模型训练期间做出的假设在生产环境中不再成立时，就会发生模型漂移。它可能导致准确性下降、错误增加和决策不佳。'
- en: To support model monitoring efforts, there are several open-source and commercial
    products available in the market. These tools provide capabilities for monitoring
    model performance, detecting data drift, identifying model drift, and generating
    insights to help organizations take necessary corrective actions. Some popular
    examples include Evidently AI, Arize AI, Seldon Core, Fiddler, and Author AI.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持模型监控工作，市场上提供了多种开源和商业产品。这些工具提供了监控模型性能、检测数据漂移、识别模型漂移以及生成洞察力以帮助组织采取必要纠正措施的能力。一些流行的例子包括Evidently
    AI、Arize AI、Seldon Core、Fiddler和Author AI。
- en: Managing ML features
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理机器学习特征
- en: As organizations increasingly adopt ML solutions, they recognize the need to
    standardize and share commonly used data and code throughout the ML lifecycle.
    One crucial element that organizations seek to manage centrally is ML features,
    which are commonly used data attributes that serve as inputs to ML models. To
    enable standardization and reuse of these features, organizations often turn to
    a feature store.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织越来越多地采用机器学习解决方案，它们认识到在整个机器学习生命周期中标准化和共享常用数据和代码的必要性。组织寻求集中管理的关键要素之一是机器学习特征，这些特征是常用数据属性，作为机器学习模型的输入。为了实现这些特征的标准化和重用，组织通常转向特征存储。
- en: A feature store acts as a centralized repository for storing and managing ML
    features. It provides a dedicated platform for organizing, validating, and sharing
    features across different ML projects and teams within an organization. By consolidating
    features in a single location, the feature store promotes consistency and facilitates
    collaboration among data scientists and ML practitioners.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储充当存储和管理机器学习特征的集中式仓库。它提供了一个专门的平台，用于组织、验证和在不同机器学习项目和团队之间共享特征。通过在单一位置整合特征，特征存储促进了一致性并促进了数据科学家和机器学习实践者之间的协作。
- en: The concept of a feature store has gained significant attention in the ML community
    due to its numerous benefits. Firstly, it enhances productivity by eliminating
    the need to recreate and engineer features for each ML project. Instead, data
    scientists can readily access precomputed and validated features from the store,
    saving time and effort. Additionally, a feature store improves model performance
    by ensuring the consistency and quality of features used in ML models. By centralizing
    feature management, organizations can enforce data governance practices, perform
    feature validation, and monitor feature quality, leading to more reliable and
    accurate ML models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其众多益处，特征存储的概念在机器学习社区中受到了广泛关注。首先，它通过消除为每个机器学习项目重新创建和构建特征的需求来提高生产力。相反，数据科学家可以轻松地从存储中访问预先计算和验证的特征，节省时间和精力。此外，特征存储通过确保用于机器学习模型中特征的一致性和质量来提高模型性能。通过集中管理特征，组织可以实施数据治理实践，执行特征验证，并监控特征质量，从而实现更可靠和准确的机器学习模型。
- en: Several open-source feature store frameworks are available in the market, such
    as Feast and Hopsworks Feature Store, offering organizations flexible options
    for managing their ML features. Let’s take a closer look at Feast as an example
    to get a deep understanding of how a feature store works.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上可用的开源特征存储框架包括Feast和Hopsworks Feature Store，为组织提供了灵活的选项来管理他们的机器学习特征。让我们以Feast为例，深入了解特征存储是如何工作的。
- en: Feast is an open-source feature store that enables organizations to manage,
    discover, and serve features for ML applications. Developed by Tecton, Feast is
    designed to handle large-scale, real-time feature data. It supports feature ingestion
    from various sources, including batch pipelines and streaming systems like Apache
    Kafka. Feast integrates well with popular ML frameworks such as TensorFlow and
    PyTorch, allowing seamless integration into ML workflows. With features like feature
    versioning, data validation, and online and offline serving capabilities, Feast
    provides a comprehensive solution for feature management.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Feast是一个开源的特征存储库，它使组织能够管理、发现并为机器学习应用提供服务。由Tecton开发，Feast旨在处理大规模、实时特征数据。它支持从各种来源获取特征，包括批处理管道和Apache
    Kafka等流系统。Feast与TensorFlow和PyTorch等流行的机器学习框架良好集成，允许无缝集成到机器学习工作流程中。凭借特征版本控制、数据验证和在线离线服务功能，Feast为特征管理提供了一个全面的解决方案。
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B20836_07_09.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、字体描述的图片，自动生成](img/B20836_07_09.png)'
- en: 'Figure 7.9: Feast feature store'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：Feast特征存储
- en: At the core of the Feast architecture is the online and offline feature storage,
    which serves as the centralized storage for feature data. The feature repository
    stores feature data in a distributed storage system, allowing for scalable and
    efficient storage and retrieval of feature values.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Feast架构的核心是线上线下特征存储，它作为特征数据的集中存储。特征存储库将特征数据存储在分布式存储系统中，允许进行可扩展和高效的存储和检索特征值。
- en: Feast employs a decoupled architecture, where the ingestion of feature data
    and the serving of features are separated. The data ingestion component is responsible
    for extracting feature data from various sources, such as data warehouses, databases,
    and streaming platforms. It then transforms and loads the feature data into the
    feature storage, ensuring data quality and consistency.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Feast采用了解耦架构，其中特征数据的摄取和特征的服务是分离的。数据摄取组件负责从各种来源提取特征数据，例如数据仓库、数据库和流平台。然后它将特征数据转换并加载到特征存储中，确保数据质量和一致性。
- en: The feature serving component is responsible for providing low-latency access
    to feature data for ML models during training and inference. The feature serving
    component also supports online and offline serving modes, allowing for real-time
    and batch feature serving.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 特征服务组件负责在训练和推理期间为机器学习模型提供低延迟的特征数据访问。特征服务组件还支持在线和离线服务模式，允许实时和批量特征服务。
- en: To enable efficient data discovery, Feast employs a feature registry. The feature
    registry allows for the fast lookup and retrieval of feature values based on different
    feature combinations and time ranges.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高效的数据发现，Feast采用了一个特征注册表。特征注册表允许根据不同的特征组合和时间范围快速查找和检索特征值。
- en: Feast also integrates with popular ML frameworks, such as TensorFlow and PyTorch,
    through its SDKs and client libraries. These integrations enable seamless integration
    of Feast into ML pipelines and workflows, making it easy for data scientists and
    ML engineers to access and utilize feature data in their models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Feast还通过其SDK和客户端库与流行的机器学习框架，如TensorFlow和PyTorch集成。这些集成使得Feast能够无缝集成到机器学习管道和工作流程中，使数据科学家和机器学习工程师能够轻松访问和利用模型中的特征数据。
- en: Overall, the Feast feature store architecture provides a robust and scalable
    solution for managing and serving ML features. By centralizing feature data management,
    Feast enables organizations to enhance productivity, improve model performance,
    and promote collaboration in ML development.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Feast特征存储架构为管理和提供机器学习特征提供了一个强大且可扩展的解决方案。通过集中管理特征数据，Feast使组织能够提高生产力、改善模型性能并促进机器学习开发中的协作。
- en: Automating ML pipeline workflows
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化机器学习管道工作流程
- en: To automate the core ML platform components we have discussed so far, we need
    to build pipelines that can orchestrate different steps using these components.
    Automation brings efficiency, productivity, and consistency while enabling reproducibility
    and minimizing human errors. There are several open-source technologies available
    to automate ML workflows, with Apache Airflow and Kubeflow Pipelines being prominent
    examples.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动化我们迄今为止讨论的核心机器学习平台组件，我们需要构建能够使用这些组件编排不同步骤的管道。自动化提高了效率、生产力和一致性，同时实现了可重复性和最小化了人为错误。有几种开源技术可用于自动化机器学习工作流程，Apache
    Airflow和Kubeflow Pipelines是其中的突出例子。
- en: Apache Airflow
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Airflow
- en: '**Apache** **Airflow** is an open-source software package for programmatically
    authoring, scheduling, and monitoring multi-step workflows. It is a general-purpose
    workflow orchestration tool that can be leveraged to define workflows for a wide
    range of tasks, including ML tasks. First, let’s explore some core Airflow concepts:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Airflow** 是一个开源软件包，用于以编程方式创建、调度和监控多步骤工作流程。它是一个通用的工作流程编排工具，可以用于定义各种任务的流程，包括机器学习任务。首先，让我们探索一些核心的
    Airflow 概念：'
- en: '**Directed Acyclic Graph** (**DAG**): A DAG defines independent tasks that
    are executed independently in a pipeline. The sequences of the execution can be
    visualized like a graph.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有向无环图（DAG**）：DAG 定义了独立执行的任务，它们在管道中独立执行。执行顺序可以像图形一样可视化。'
- en: '**Tasks**: Tasks are basic units of execution in Airflow. Tasks have dependencies
    between them during executions.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：任务是在 Airflow 中执行的基本单元。在执行过程中，任务之间存在依赖关系。'
- en: '**Operators**: Operators are DAG components that describe a single task in
    the pipeline. An operator implements the task execution logic. Airflow provides
    a list of operators for common tasks, such as a Python operator for running Python
    code, or an Amazon S3 operator to interact with the S3 service. Tasks are created
    when operators are instantiated.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作符**：操作符是 DAG 组件，用于描述管道中的单个任务。操作符实现了任务执行逻辑。Airflow 提供了一系列操作符，用于常见任务，例如运行
    Python 代码的 Python 操作符，或与 S3 服务交互的 Amazon S3 操作符。当实例化操作符时，会创建任务。'
- en: '**Scheduling**: A DAG can run on demand or on a predetermined schedule.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度**：DAG 可以按需运行或按预定的时间表运行。'
- en: '**Sensors**: Sensors are a special type of operator that are designed to wait
    for something to occur. They can then help trigger a downstream task to happen.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器**：传感器是一种特殊类型的操作符，旨在等待某个事件发生。然后，它们可以帮助触发下游任务的发生。'
- en: 'Airflow can run on a single machine or in a cluster. Additionally, it can be
    deployed on the Kubernetes infrastructure. The following figure shows a multi-node
    Airflow deployment:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 可以在单台机器或集群上运行。此外，它还可以部署在 Kubernetes 基础设施上。以下图显示了多节点 Airflow 部署：
- en: '![Figure 7.8 – Apache Airflow architecture ](img/B20836_07_10.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – Apache Airflow 架构](img/B20836_07_10.png)'
- en: 'Figure 7.10: Apache Airflow architecture'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：Apache Airflow 架构
- en: The *master node* mainly runs the *web server* and *scheduler*. The scheduler
    is responsible for scheduling the execution of the DAGs. It sends tasks to a queue,
    and the worker nodes retrieve the tasks from the queue and run them. The metadata
    store is used to store the metadata of the Airflow cluster and processes, such
    as task instance details or user data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**主节点**主要运行 **Web 服务器** 和 **调度器**。调度器负责调度 DAG 的执行。它将任务发送到队列，工作节点从队列中检索任务并运行它们。元数据存储用于存储
    Airflow 集群和进程的元数据，例如任务实例详情或用户数据。'
- en: 'You can author the Airflow DAGs using Python. The following sample code shows
    how to author a basic Airflow DAG in Python with two Bash operators in a sequence:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Python 编写 Airflow DAG。以下示例代码展示了如何使用 Python 编写一个基本的 Airflow DAG，其中包含两个按顺序排列的
    Bash 操作符：
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Airflow can connect to many different sources and has built-in operators for
    many external services, such as **Amazon** **EMR** and **Amazon** **SageMaker**.
    It has been widely adopted by many organizations to run large-scale workflow orchestration
    jobs in production, such as coordinating ETL jobs and ML data processing jobs.
    AWS even has a managed Airflow offering to help reduce the operational overhead
    of running Airflow infrastructure.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 可以连接到许多不同的来源，并为许多外部服务内置了操作符，例如 **Amazon EMR** 和 **Amazon SageMaker**。它已被许多组织广泛采用，用于在生产环境中运行大规模工作流程编排作业，例如协调
    ETL 作业和机器学习数据处理作业。AWS 甚至提供了一种管理的 Airflow 服务，以帮助减少运行 Airflow 基础设施的操作开销。
- en: Airflow also comes with some limitations. Airflow does not offer a UI designer
    for DAG development, which can be a challenge for users without Python programming
    skills to design workflows. The lack of versioning control with DAG pipelines
    also poses some challenges with managing and understanding the evolving variations
    of pipelines. Operating Airflow on Kubernetes can be complex, which is why many
    organizations opt for managed offerings. Despite these limitations, however, Airflow
    has emerged as a highly popular workflow orchestration tool due to its enterprise-ready
    capability, strong community support, and rich ecosystem.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 也存在一些限制。Airflow 不提供 DAG 开发的 UI 设计器，这对于没有 Python 编程技能的用户来说可能是一个挑战，因为他们需要设计工作流。与
    DAG 管道缺乏版本控制也带来了管理和理解管道演变变体的一些挑战。在 Kubernetes 上运行 Airflow 可能很复杂，这也是为什么许多组织选择托管服务的原因。然而，尽管存在这些限制，但由于其企业级能力、强大的社区支持和丰富的生态系统，Airflow
    仍然成为了一个高度流行的流程编排工具。
- en: Kubeflow Pipelines
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines
- en: '**Kubeflow Pipelines** is a Kubeflow component, and it is purpose-built for
    authoring and orchestrating end-to-end ML workflows on Kubernetes. First, let’s
    review some core concepts of Kubeflow Pipelines:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubeflow Pipelines** 是一个 Kubeflow 组件，它专为在 Kubernetes 上编写和编排端到端机器学习工作流而设计。首先，让我们回顾一下
    Kubeflow Pipelines 的核心概念：'
- en: '**Pipeline**: A pipeline describes an ML workflow, all the components in the
    workflow, and how the components are related to each other in the pipeline.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：管道描述了一个机器学习工作流，工作流中的所有组件，以及组件在管道中的相互关系。'
- en: '**Pipeline components**: A pipeline component performs a task in the pipeline.
    An example of a pipeline component could be a data processing component or a model
    training component.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道组件**：管道组件在管道中执行任务。管道组件的一个例子可以是数据处理组件或模型训练组件。'
- en: '**Experiment**: An experiment organizes different trial runs (model training)
    for an ML project so you can easily inspect and compare the different runs and
    their results.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验**：实验组织了机器学习项目的不同试验运行（模型训练），以便您可以轻松检查和比较不同的运行及其结果。'
- en: '**Step**: The execution of one component in a pipeline is called a step.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤**：管道中一个组件的执行称为步骤。'
- en: '**Run trigger**: You use a run trigger to kick off the execution of a pipeline.
    A run trigger can be a periodic trigger (for example, to run every 2 hours), or
    a scheduled trigger (for example, run at a specific date and time).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行触发器**：您可以使用运行触发器启动管道的执行。运行触发器可以是周期性触发器（例如，每 2 小时运行一次），也可以是计划触发器（例如，在特定日期和时间运行）。'
- en: '**Output artifacts**: Output artifacts are the outputs from the pipeline components.
    Examples of output artifacts could be model training metrics or visualizations
    of datasets.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出工件**：输出工件是管道组件的输出。输出工件的例子可以是模型训练指标或数据集的可视化。'
- en: 'Kubeflow Pipelines is installed as part of the Kubeflow installation. It comes
    with its own UI, which is part of the overall Kubeflow dashboard UI. The Pipelines
    service manages the pipelines and their run status and stores them in a metadata
    database. There is an orchestration and workflow controller that manages the actual
    execution of the pipelines and the components. The following figure illustrates
    the core architecture components in a Kubeflow pipeline:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 作为 Kubeflow 安装的一部分被安装。它自带一个 UI，这是整体 Kubeflow 仪表板 UI 的一部分。Pipelines
    服务管理管道及其运行状态，并将它们存储在元数据数据库中。有一个编排和工作流控制器来管理管道的实际执行和组件。以下图展示了 Kubeflow 管道中的核心架构组件：
- en: '![Figure 7.9 – Kubeflow Pipelines architecture ](img/B20836_07_11.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – Kubeflow Pipelines 架构](img/B20836_07_11.png)'
- en: 'Figure 7.11: Kubeflow Pipelines architecture'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：Kubeflow Pipelines 架构
- en: 'You author the pipeline using the Pipeline SDK in Python. To create and run
    a pipeline, follow these steps:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Python 中的 Pipeline SDK 编写管道。要创建和运行管道，请按照以下步骤操作：
- en: Create a pipeline definition using the Kubeflow SDK. The pipeline definition
    specifies a list of components and how they are joined together in a graph.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Kubeflow SDK 创建管道定义。管道定义指定了一组组件以及它们如何在图中连接在一起。
- en: Compile the definition into a static YAML specification to be executed by the
    Kubeflow Pipelines service.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将定义编译成由 Kubeflow Pipelines 服务执行的静态 YAML 规范。
- en: Register the specification with the Kubeflow Pipelines service and call the
    pipeline to run from the static definition.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将规范注册到 Kubeflow Pipelines 服务中，并从静态定义中调用管道以运行。
- en: The Kubeflow Pipelines service calls the API server to create resources to run
    the pipeline.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 服务调用 API 服务器以创建运行管道的资源。
- en: Orchestration controllers execute various containers to complete the pipeline
    run.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编排控制器执行各种容器以完成管道运行。
- en: It is important to note that running pipelines using Kubeflow Pipelines requires
    a high degree of competency with Kubernetes, which could be challenging for people
    without deep Kubernetes skills. Building the workflow in Python can be a complex
    task that involves writing a Dockerfile or YAML file for each component, and Python
    scripts for the workflow and execution. Kubeflow Pipelines mainly works within
    the Kubeflow environment, with very limited integration with external tools and
    services. It also lacks native pipeline versioning capability. Despite these challenges,
    Kubeflow Pipelines is still widely adopted due to its support for end-to-end ML
    management, workflow visualization, portability, and reproducibility.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，使用 Kubeflow Pipelines 运行管道需要具备高度的 Kubernetes 熟练度，这对于没有深厚 Kubernetes 技能的人来说可能具有挑战性。在
    Python 中构建工作流程可能是一项复杂的任务，涉及为每个组件编写 Dockerfile 或 YAML 文件，以及为工作流程和执行编写 Python 脚本。Kubeflow
    Pipelines 主要在 Kubeflow 环境中工作，与外部工具和服务的集成非常有限。它还缺少本地的管道版本控制功能。尽管存在这些挑战，但由于其对端到端机器学习管理的支持、工作流程可视化、可移植性和可重复性，Kubeflow
    Pipelines 仍然被广泛采用。
- en: Now that we have explored various open-source tools for building ML platforms,
    let’s delve into end-to-end architecture using these open-source frameworks and
    components.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了用于构建 ML 平台的各种开源工具，让我们深入了解使用这些开源框架和组件的端到端架构。
- en: Designing an end-to-end ML platform
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计端到端ML平台
- en: 'After discussing several open-source technologies individually, let’s now delve
    into their integration and see how these components come together. The architecture
    patterns and technology stack selection may vary based on specific needs and requirements.
    The following diagram presents the conceptual building blocks of an ML platform
    architecture:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在单独讨论了几种开源技术之后，现在让我们深入了解它们的集成，看看这些组件是如何结合在一起的。架构模式和技术栈的选择可能根据具体需求和需求而有所不同。以下图表展示了
    ML 平台架构的概念性构建块：
- en: '![A picture containing text, screenshot, diagram, design  Description automatically
    generated](img/B20836_07_12.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、设计的图片，自动生成描述](img/B20836_07_12.png)'
- en: 'Figure 7.12: ML platform architecture'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：ML 平台架构
- en: Next, let’s delve into different strategies to implement this architecture concept
    with different combinations of open-source technologies.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解不同的策略，以不同的开源技术组合来实现这种架构概念。
- en: ML platform-based strategy
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于ML平台的策略
- en: When designing an ML platform using open-source technologies, one effective
    strategy is to utilize an ML platform framework as a base platform and then integrate
    additional open-source components to address specific requirements. One such ML
    platform framework is Kubeflow, which provides a robust foundation with its built-in
    building blocks for an ML platform. By leveraging Kubeflow, you can benefit from
    its core components while extending the platform’s capabilities through the integration
    of complementary open-source tools.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用开源技术设计 ML 平台时，一种有效的策略是利用 ML 平台框架作为基础平台，然后集成额外的开源组件以满足特定需求。这样的 ML 平台框架之一是
    Kubeflow，它提供了一套强大的基础构建块，用于构建 ML 平台。通过利用 Kubeflow，您可以从其核心组件中受益，并通过集成互补的开源工具来扩展平台的功能。
- en: 'This strategy allows for flexibility and customization by seamlessly integrating
    a range of open-source ML components into the platform. You would choose this
    approach if the base ML platform framework meets most of your requirements, or
    if you can work within the limitations of the framework. The following table outlines
    key ML platform components and their corresponding open-source frameworks and
    tools:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略通过无缝集成一系列开源 ML 组件到平台中，提供了灵活性和定制化。如果您的基础 ML 平台框架满足大部分需求，或者您可以在框架的限制内工作，您会选择这种方法。以下表格概述了关键
    ML 平台组件及其相应的开源框架和工具：
- en: '| **ML Platform Component** | **Open-Source Framework** |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **ML 平台组件** | **开源框架** |'
- en: '| Code repository | GitHub |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 代码仓库 | GitHub |'
- en: '| Experimentation and model development | Kubeflow Jupyter Notebook |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 实验和模型开发 | Kubeflow Jupyter Notebook |'
- en: '| Experiment tracking | MLflow experiment tracker |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 实验跟踪 | MLflow 实验跟踪器 |'
- en: '| Feature store | Feast feature store |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 特征存储 | Feast 特征存储 |'
- en: '| Data annotation | Computer Vision Annotation Tool (CVAT) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 数据标注 | 计算机视觉标注工具 (CVAT) |'
- en: '| Training | Kubeflow training operators |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | Kubeflow 训练操作符 |'
- en: '| Data and model testing | Deepchecks |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 数据和模型测试 | Deepchecks |'
- en: '| Model repository | MLflow Model Repository |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 模型仓库 | MLflow 模型仓库 |'
- en: '| ML pipeline development | Kubeflow Pipelines |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习管道开发 | Kubeflow 管道 |'
- en: '| Model inference | Kubeflow KFServing (Seldon Core, TFServing, Triton) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 模型推理 | Kubeflow KFServing（Seldon Core、TFServing、Triton）|'
- en: '| Docker image repository | Docker Hub |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Docker 镜像仓库 | Docker Hub |'
- en: '| CI/CD | GitHub Actions |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| CI/CD | GitHub Actions |'
- en: '| Drift monitoring | Deepchecks |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 漂移监控 | Deepchecks |'
- en: 'Table 7.1: ML platform components and their corresponding open-source frameworks'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1：机器学习平台组件及其对应的开源框架
- en: 'By incorporating these frameworks and tools into the architectural conceptual
    diagram, we can visualize the resulting diagram as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些框架和工具纳入架构概念图，我们可以将结果图可视化如下：
- en: '![A picture containing text, screenshot, diagram, design  Description automatically
    generated](img/B20836_07_13.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、设计的图片，自动生成描述](img/B20836_07_13.png)'
- en: 'Figure 7.13: Kubeflow-based ML platform'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：基于 Kubeflow 的机器学习平台
- en: Using this architecture, data scientists utilize Kubeflow Jupyter Notebook for
    conducting experiments and building models. Experiment runs and relevant details,
    such as data statistics, hyperparameters, and model metrics, are tracked and saved
    in the MLflow experiment tracking component.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此架构，数据科学家利用 Kubeflow Jupyter Notebook 进行实验和构建模型。实验运行和相关细节，如数据统计、超参数和模型指标，被跟踪并保存在
    MLflow 实验跟踪组件中。
- en: Common ML features are stored in the Feast feature store. When there is a need
    for data annotation, data annotators can employ open-source data annotation tools
    like the **Computer Vision Annotation Tool** (**CVAT**) and Label Studio to label
    data for model training. Data scientists can utilize features from the feature
    store and the labeled dataset as part of their experimentation and model building.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的机器学习功能存储在 Feast 特征存储中。当需要数据标注时，数据标注员可以使用如 **计算机视觉标注工具**（**CVAT**）和 Label
    Studio 等开源数据标注工具来标注数据以供模型训练。数据科学家可以利用特征存储和标注数据集作为他们实验和模型构建的一部分。
- en: GitHub serves as the code repository for data scientists. They save all source
    code, including training scripts, algorithm code, and data transformation scripts,
    in the code repository. Model training and inference Docker images are stored
    in Docker Hub. A Docker image build process can be deployed to create new Docker
    images for training and inference purposes.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 是数据科学家的代码仓库。他们将所有源代码，包括训练脚本、算法代码和数据转换脚本，保存在代码仓库中。模型训练和推理 Docker 镜像存储在
    Docker Hub。可以将 Docker 镜像构建过程部署以创建用于训练和推理的新 Docker 镜像。
- en: For formal model training, the training script is pulled from the GitHub repository,
    and the training Docker image is pulled from Docker Hub into the Kubeflow training
    operator to initiate the model training, along with the training dataset. Deepchecks
    can be utilized to perform data validation and model performance checks. Once
    the model is trained, the model artifacts, along with any metadata such as model
    metrics and evaluation graphs, are stored in MLflow Model Registry.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正式的模型训练，训练脚本从 GitHub 仓库拉取，训练 Docker 镜像从 Docker Hub 拉取到 Kubeflow 训练操作符以启动模型训练，包括训练数据集。可以使用
    Deepchecks 进行数据验证和模型性能检查。一旦模型训练完成，模型工件以及任何元数据，如模型指标和评估图表，都存储在 MLflow 模型注册表中。
- en: When it is time to deploy the model, models are fetched from MLflow Model Registry
    and loaded into KFServing for model inference, along with the model inference
    Docker image and inference script. KFServing offers the flexibility to choose
    different inference servers, including Seldon Core, TFServing, and Triton.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当是时候部署模型时，模型从 MLflow 模型注册表中检索并加载到 KFServing 以进行模型推理，包括模型推理 Docker 镜像和推理脚本。KFServing
    提供了选择不同推理服务器（包括 Seldon Core、TFServing 和 Triton）的灵活性。
- en: Prediction logs can be sent to the model monitoring component for detecting
    data drift and model drift. Open-source software tools like Evidently AI can be
    employed for data drift and model drift detection.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 预测日志可以发送到模型监控组件以检测数据漂移和模型漂移。可以使用如 Evidently AI 等开源软件工具进行数据漂移和模型漂移检测。
- en: To orchestrate various tasks such as data processing, feature engineering, model
    training, and model validation, Kubeflow Pipelines can be developed. For **CI/CD**
    (**Continuous Integration/Continuous Deployment**), GitHub Actions can be used
    as a triggering mechanism to initiate different pipelines.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编排各种任务，如数据处理、特征工程、模型训练和模型验证，可以开发Kubeflow Pipelines。对于**CI/CD**（**持续集成/持续部署**），可以使用GitHub
    Actions作为触发机制来启动不同的管道。
- en: Overall, this approach allows you to combine the benefits of a base ML platform
    framework with the flexibility provided by a wide range of open-source components.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这种方法允许您结合基础机器学习平台框架的优势和广泛开源组件提供的灵活性。
- en: ML component-based strategy
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于机器学习组件的策略
- en: An alternative approach is to build the ML platform using individual components
    rather than relying on a base ML platform framework. This strategy offers the
    advantage of selecting the best-in-class components for each aspect of the platform,
    allowing organizations to adhere to their existing open-source standards for core
    components like pipeline development or notebook IDEs. The following architectural
    pattern illustrates this approach.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用单个组件而不是依赖基础机器学习平台框架来构建机器学习平台。这种策略的优势在于为平台每个方面选择最佳组件，使组织能够遵守其现有的开源标准，如管道开发或笔记本IDE的核心组件。以下架构模式说明了这种方法。
- en: '![A picture containing text, screenshot, diagram, design  Description automatically
    generated](img/B20836_07_14.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、设计的图片，自动生成描述](img/B20836_07_14.png)'
- en: 'Figure 7.14: Component-based architecture'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：基于组件的架构
- en: In this architecture pattern, alternative technologies and tools are utilized
    for pipeline development, notebook environments, and training infrastructure management.
    Additionally, Kubernetes serves as the infrastructure management framework. This
    approach allows organizations to leverage specific technologies and tools that
    align with their unique requirements and preferences.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构模式中，利用替代技术和工具进行管道开发、笔记本环境和训练基础设施管理。此外，Kubernetes作为基础设施管理框架。这种方法允许组织利用与其独特需求和偏好相一致的具体技术和工具。
- en: One notable aspect is the use of Airflow as the standard orchestration and pipeline
    tool across various technical disciplines, including ML and data management. Airflow’s
    widespread adoption within organizations enables it to serve as a unifying pipeline
    management tool, facilitating seamless integration between different components
    and workflows.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的方面是使用Airflow作为跨多个技术学科的标准编排和管道工具，包括机器学习和数据管理。Airflow在组织中的广泛应用使其成为统一的管道管理工具，促进不同组件和工作流程之间的无缝集成。
- en: Moreover, this architecture pattern emphasizes the building of custom training
    and inference infrastructure on top of Kubernetes. By leveraging Kubernetes, organizations
    gain the flexibility to create customized training and inference environments
    tailored to their specific needs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种架构模式强调在Kubernetes之上构建定制的训练和推理基础设施。通过利用Kubernetes，组织可以获得创建定制化训练和推理环境的灵活性，以满足其特定需求。
- en: In addition to the availability of free open-source tools that meet ML platform
    requirements, it is important to consider the integration of commercial components
    into the open-source architecture. These commercial offerings can enhance specific
    aspects of the ML platform and provide additional capabilities.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提供满足机器学习平台要求的免费开源工具外，考虑将商业组件集成到开源架构中也很重要。这些商业产品可以增强机器学习平台的具体方面，并提供额外的功能。
- en: For instance, when deploying this architecture pattern on AWS, it is advisable
    to explore the use of Amazon Elastic Container Registry (ECR) as the Docker image
    repository. Amazon ECR provides a managed and secure solution for storing and
    managing container images, integrating seamlessly with other AWS services.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当在AWS上部署此架构模式时，建议探索使用Amazon Elastic Container Registry (ECR)作为Docker镜像仓库。Amazon
    ECR提供了一个托管和安全的解决方案，用于存储和管理容器镜像，与AWS其他服务无缝集成。
- en: When it comes to monitoring, there are commercial products like Fiddler and
    Author AI that can offer advanced features and insights. These tools can enhance
    the monitoring capabilities of the ML platform, providing in-depth analysis, model
    explainability, and visualization of model behavior and performance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控方面，有像Fiddler和Author AI这样的商业产品可以提供高级功能和见解。这些工具可以增强机器学习平台的监控能力，提供深入分析、模型可解释性和模型行为及性能的可视化。
- en: Overall, the advantages of this architecture pattern include the ability to
    choose alternative technologies and tools for different aspects of the ML platform,
    leveraging Airflow as a unifying pipeline management tool, and building custom
    training and inference infrastructure on Kubernetes. These choices enable organizations
    to create a tailored and optimized ML platform that aligns precisely with their
    requirements and allows for highly customized training and inference processes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这种架构模式的优点包括能够为机器学习平台的不同方面选择替代技术和工具，利用Airflow作为统一的管道管理工具，以及在Kubernetes上构建定制的训练和推理基础设施。这些选择使组织能够创建一个量身定制且优化的机器学习平台，与他们的需求精确匹配，并允许进行高度定制的训练和推理过程。
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you have gained an understanding of the core architecture components
    of a typical ML platform and their capabilities. We have explored various open-source
    technologies such as Kubeflow, MLflow, TensorFlow Serving, Seldon Core, Triton
    Inference Server, Apache Airflow, and Kubeflow Pipelines. Additionally, we have
    discussed different strategies for approaching the design of an ML platform using
    open-source frameworks and tools.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经了解了典型机器学习平台的核心架构组件及其功能。我们探讨了各种开源技术，如Kubeflow、MLflow、TensorFlow Serving、Seldon
    Core、Triton推理服务器、Apache Airflow和Kubeflow流水线。此外，我们还讨论了使用开源框架和工具设计机器学习平台的多种策略。
- en: While these open-source technologies offer powerful features for building sophisticated
    ML platforms, it is important to acknowledge that constructing and maintaining
    such environments requires substantial engineering effort and expertise, especially
    when dealing with large-scale ML platforms.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些开源技术为构建复杂的机器学习平台提供了强大的功能，但重要的是要认识到构建和维护这样的环境需要大量的工程努力和专业知识，尤其是在处理大规模机器学习平台时。
- en: In the next chapter, we will delve into fully managed, purpose-built ML solutions
    that are specifically designed to facilitate the development and operation of
    ML environments. These managed solutions aim to simplify the complexities of building
    and managing ML platforms, providing preconfigured and scalable infrastructure,
    as well as additional features tailored for ML workflows.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨专门为促进机器学习环境开发和运营而设计的完全托管机器学习解决方案。这些托管解决方案旨在简化构建和管理机器学习平台的复杂性，提供预配置和可扩展的基础设施，以及针对机器学习工作流程的额外功能。
- en: Leave a review!
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢这本书吗？通过留下亚马逊评论来帮助像您这样的读者。扫描下面的二维码，获取您选择的免费电子书。
- en: '![](img/Review_Copy.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/Review_Copy.png)'
- en: '**Limited Offer*'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**限时优惠**'
