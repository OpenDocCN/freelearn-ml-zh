- en: Chapter 4. Probabilistic Learning – Classification Using Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：概率学习——使用朴素贝叶斯进行分类
- en: When a meteorologist provides a weather forecast, precipitation is typically
    described with terms such as "70 percent chance of rain." Such forecasts are known
    as probability of precipitation reports. Have you ever considered how they are
    calculated? It is a puzzling question, because in reality, either it will rain
    or not.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当气象学家提供天气预报时，降水通常用诸如“70%可能下雨”这样的术语来描述。这类预报被称为降水概率报告。你有没有想过它们是如何计算出来的？这是一个令人困惑的问题，因为实际上，要么下雨，要么不下雨。
- en: Weather estimates are based on probabilistic methods or those concerned with
    describing uncertainty. They use data on past events to extrapolate future events.
    In the case of weather, the chance of rain describes the proportion of prior days
    to similar measurable atmospheric conditions in which precipitation occurred.
    A 70 percent chance of rain implies that in 7 out of the 10 past cases with similar
    conditions, precipitation occurred somewhere in the area.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 天气预估基于概率方法或那些描述不确定性的方法。它们使用过去事件的数据来推测未来事件。以天气为例，下雨的机会描述了过去在类似的可测量大气条件下降水发生的天数比例。70%的降雨概率意味着，在过去10个类似条件的案例中，有7个地方发生了降水。
- en: 'This chapter covers the Naive Bayes algorithm, which uses probabilities in
    much the same way as a weather forecast. While studying this method, you will
    learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了朴素贝叶斯算法，它通过概率与天气预报的方式类似。学习该方法时，你将了解到：
- en: Basic principles of probability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率的基本原理
- en: The specialized methods and data structures needed to analyze text data with
    R
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R分析文本数据所需的专业方法和数据结构
- en: How to employ Naive Bayes to build an SMS junk message filter
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何运用朴素贝叶斯构建短信垃圾信息过滤器
- en: If you've taken a statistics class before, some of the material in this chapter
    may be a review. Even so, it may be helpful to refresh your knowledge on probability,
    as these principles are the basis of how Naive Bayes got such a strange name.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前上过统计学课，本章中的部分内容可能是复习内容。尽管如此，重新温习一下概率知识可能还是有帮助的，因为这些原理是朴素贝叶斯得名的基础。
- en: Understanding Naive Bayes
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯
- en: The basic statistical ideas necessary to understand the Naive Bayes algorithm
    have existed for centuries. The technique descended from the work of the 18^(th)
    century mathematician Thomas Bayes, who developed foundational principles to describe
    the probability of events, and how probabilities should be revised in the light
    of additional information. These principles formed the foundation for what are
    now known as **Bayesian methods**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 理解朴素贝叶斯算法所需的基本统计学理念已经存在了几个世纪。这一技术源于18世纪数学家托马斯·贝叶斯的工作，他发展了描述事件概率的基础原理，并阐述了在获得额外信息的情况下，如何修正概率。这些原理构成了如今被称为**贝叶斯方法**的基础。
- en: We will cover these methods in greater detail later on. But, for now, it suffices
    to say that a probability is a number between 0 and 1 (that is, between 0 percent
    and 100 percent), which captures the chance that an event will occur in the light
    of the available evidence. The lower the probability, the less likely the event
    is to occur. A probability of 0 indicates that the event will definitely not occur,
    while a probability of 1 indicates that the event will occur with 100 percent
    certainty.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面更详细地讲解这些方法。但目前，只需说概率是一个介于0和1之间的数字（即0%到100%之间），它表示根据现有证据，某个事件发生的概率。概率越低，事件发生的可能性就越小。概率为0表示事件肯定不会发生，而概率为1表示事件将以100%的确定性发生。
- en: 'Classifiers based on Bayesian methods utilize training data to calculate an
    observed probability of each outcome based on the evidence provided by feature
    values. When the classifier is later applied to unlabeled data, it uses the observed
    probabilities to predict the most likely class for the new features. It''s a simple
    idea, but it results in a method that often has results on par with more sophisticated
    algorithms. In fact, Bayesian classifiers have been used for:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯方法的分类器利用训练数据计算每个结果的观察概率，这些结果基于特征值提供的证据。当分类器应用于未标记的数据时，它使用观察到的概率来预测新特征的最可能类别。这是一个简单的理念，但它产生了一种通常能与更复杂算法相媲美的效果。实际上，贝叶斯分类器已被用于：
- en: Text classification, such as junk e-mail (spam) filtering
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，如垃圾邮件（垃圾邮件）过滤
- en: Intrusion or anomaly detection in computer networks
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机网络中的入侵或异常检测
- en: Diagnosing medical conditions given a set of observed symptoms
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一组观察到的症状诊断医疗状况
- en: Typically, Bayesian classifiers are best applied to problems in which the information
    from numerous attributes should be considered simultaneously in order to estimate
    the overall probability of an outcome. While many machine learning algorithms
    ignore features that have weak effects, Bayesian methods utilize all the available
    evidence to subtly change the predictions. If large number of features have relatively
    minor effects, taken together, their combined impact could be quite large.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，贝叶斯分类器最好应用于需要同时考虑多个属性信息的情境，以估算结果的总体概率。尽管许多机器学习算法会忽略效果较弱的特征，贝叶斯方法利用所有可用的证据来微妙地改变预测。如果大量特征的效果相对较小，结合起来，它们的综合影响可能会非常大。
- en: Basic concepts of Bayesian methods
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯方法的基本概念
- en: Before jumping into the Naive Bayes algorithm, it's worth spending some time
    defining the concepts that are used across Bayesian methods. Summarized in a single
    sentence, Bayesian probability theory is rooted in the idea that the estimated
    likelihood of an **event**, or a potential outcome, should be based on the evidence
    at hand across multiple **trials**, or opportunities for the event to occur.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究朴素贝叶斯算法之前，值得花些时间定义贝叶斯方法中使用的概念。用一句话总结，贝叶斯概率论的核心思想是，**事件**或潜在结果的估计可能性应该基于当前证据，并且通过多个**试验**或事件发生的机会来加以判断。
- en: 'The following table illustrates events and trials for several real-world outcomes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了几个现实世界结果的事件和试验：
- en: '| Event | Trial |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 事件 | 试验 |'
- en: '| --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Heads result | Coin flip |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 正面结果 | 投币实验 |'
- en: '| Rainy weather | A single day |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 雨天 | 单一天 |'
- en: '| Message is spam | Incoming e-mail message |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 消息是垃圾邮件 | 收到的电子邮件消息 |'
- en: '| Candidate becomes president | Presidential election |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 候选人当选总统 | 总统选举 |'
- en: '| Win the lottery | Lottery ticket |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 中头奖 | 彩票 |'
- en: Bayesian methods provide insights into how the probability of these events can
    be estimated from the observed data. To see how, we'll need to formalize our understanding
    of probability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法提供了有关如何从观察到的数据估算这些事件概率的洞见。为了了解这一点，我们需要正式化我们的概率理解。
- en: Understanding probability
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解概率
- en: The probability of an event is estimated from the observed data by dividing
    the number of trials in which the event occurred by the total number of trials.
    For instance, if it rained 3 out of 10 days with similar conditions as today,
    the probability of rain today can be estimated as *3 / 10 = 0.30* or 30 percent.
    Similarly, if 10 out of 50 prior email messages were spam, then the probability
    of any incoming message being spam can be estimated as *10 / 50 = 0.20* or 20
    percent.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 事件的概率是通过将事件发生的试验次数除以总的试验次数来估算的。例如，如果在与今天类似的条件下，10天中有3天下雨，那么今天下雨的概率可以估算为*3 /
    10 = 0.30*，即30%。类似地，如果50封先前的电子邮件中有10封是垃圾邮件，那么任何收到的电子邮件是垃圾邮件的概率可以估算为*10 / 50 =
    0.20*，即20%。
- en: To denote these probabilities, we use notation in the form *P(A)*, which signifies
    the probability of event *A*. For example, *P(rain) = 0.30* and *P(spam) = 0.20*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示这些概率，我们使用类似*P(A)*的符号，表示事件*A*的概率。例如，*P(rain) = 0.30* 和 *P(spam) = 0.20*。
- en: The probability of all the possible outcomes of a trial must always sum to 1,
    because a trial always results in some outcome happening. Thus, if the trial has
    two outcomes that cannot occur simultaneously, such as rainy versus sunny or spam
    versus ham (nonspam), then knowing the probability of either outcome reveals the
    probability of the other. For example, given the value *P(spam) = 0.20*, we can
    calculate *P(ham) = 1 – 0.20 = 0.80*. This concludes that spam and ham are **mutually
    exclusive and exhaustive** events, which implies that they cannot occur at the
    same time and are the only possible outcomes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一次试验的所有可能结果的概率之和必须始终为1，因为一次试验总会产生某种结果。因此，如果试验有两个无法同时发生的结果，比如雨天与晴天，或者垃圾邮件与非垃圾邮件（ham），那么知道其中一个结果的概率就能推断出另一个结果的概率。例如，给定*P(spam)
    = 0.20*，我们可以计算出*P(ham) = 1 – 0.20 = 0.80*。这表明垃圾邮件和非垃圾邮件是**互斥且穷尽的**事件，意味着它们不能同时发生，并且是唯一可能的结果。
- en: Because an event cannot simultaneously happen and not happen, an event is always
    mutually exclusive and exhaustive with its **complement**, or the event comprising
    of the outcomes in which the event of interest does not happen. The complement
    of event *A* is typically denoted *A^c* or *A'*. Additionally, the shorthand notation
    *P(¬A)* can used to denote the probability of event *A* not occurring, as in *P(¬spam)
    = 0.80*. This notation is equivalent to *P(A^c)*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为一个事件不能同时发生又不发生，所以一个事件与其**补集**始终是互斥的且穷尽的，补集是指事件不发生时所有可能结果的集合。事件*A*的补集通常表示为*A^c*或*A'*。此外，简写符号*P(¬A)*可以用来表示事件*A*不发生的概率，如*P(¬spam)
    = 0.80*。该符号等同于*P(A^c)*。
- en: 'To illustrate events and their complements, it is often helpful to imagine
    a two-dimensional space that is partitioned into probabilities for each event.
    In the following diagram, the rectangle represents the possible outcomes for an
    e-mail message. The circle represents the 20 percent probability that the message
    is spam. The remaining 80 percent represents the complement *P(¬spam)* or the
    messages that are not spam:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明事件及其补集，通常有助于想象一个被划分为各个事件概率的二维空间。在下图中，矩形表示电子邮件可能的结果。圆圈表示邮件是垃圾邮件的20%的概率。其余的80%表示补集*P(¬spam)*，即那些不是垃圾邮件的消息：
- en: '![Understanding probability](img/B03905_04_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![理解概率](img/B03905_04_01.jpg)'
- en: Understanding joint probability
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解联合概率
- en: 'Often, we are interested in monitoring several nonmutually exclusive events
    for the same trial. If certain events occur with the event of interest, we may
    be able to use them to make predictions. Consider, for instance, a second event
    based on the outcome that an e-mail message contains the word Viagra. In most
    cases, this word is likely to appear only in a spam message; its presence in an
    incoming e-mail is therefore a very strong piece of evidence that the message
    is spam. The preceding diagram, updated for this second event, might appear as
    shown in the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们对监控多个非互斥事件感兴趣，尤其是在同一次试验中。如果某些事件与感兴趣的事件同时发生，我们可能能够利用这些事件进行预测。以电子邮件中包含“伟哥”一词为例，在大多数情况下，这个词只出现在垃圾邮件中；因此它出现在一封邮件中，是垃圾邮件的强烈证据。更新后的图示，加入了第二个事件，可能如下所示：
- en: '![Understanding joint probability](img/B03905_04_02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![理解联合概率](img/B03905_04_02.jpg)'
- en: Notice in the diagram that the Viagra circle does not completely fill the spam
    circle, nor is it completely contained by the spam circle. This implies that not
    all spam messages contain the word Viagra and not every e-mail with the word Viagra
    is spam.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在图示中可以看到，伟哥圆圈并没有完全填满垃圾邮件圆圈，也没有完全被垃圾邮件圆圈包含。这意味着并不是所有的垃圾邮件都包含“伟哥”这个词，而包含“伟哥”一词的邮件也并不一定是垃圾邮件。
- en: 'To zoom in for a closer look at the overlap between the spam and Viagra circles,
    we''ll employ a visualization known as a **Venn diagram**. First used in the late
    19th century by John Venn, the diagram uses circles to illustrate the overlap
    between sets of items. In most Venn diagrams, the size of the circles and the
    degree of the overlap is not meaningful. Instead, it is used as a reminder to
    allocate probability to all possible combinations of events:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地查看垃圾邮件和伟哥圆圈之间的重叠部分，我们将使用一种叫做**维恩图**的可视化方法。维恩图最早由约翰·维恩在19世纪末期使用，利用圆圈来展示各个集合之间的重叠。在大多数维恩图中，圆圈的大小和重叠程度并没有实际意义。它更多地用于提醒我们给所有可能的事件组合分配概率：
- en: '![Understanding joint probability](img/B03905_04_03.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![理解联合概率](img/B03905_04_03.jpg)'
- en: We know that 20 percent of all messages were spam (the left circle) and 5 percent
    of all messages contained the word Viagra (the right circle). We would like to
    quantify the degree of overlap between these two proportions. In other words,
    we hope to estimate the probability that both *P(spam)* and *P(Viagra)* occur,
    which can be written as *P(spam ∩ Viagra)*. The upside down 'U' symbol signifies
    the **intersection** of the two events; the notation *A ∩ B* refers to the event
    in which both *A* and *B* occur.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，20%的邮件是垃圾邮件（左侧圆圈），5%的邮件包含“伟哥”一词（右侧圆圈）。我们希望量化这两个比例之间的重叠程度。换句话说，我们希望估算“垃圾邮件”和“伟哥”同时发生的概率，可以表示为*P(spam
    ∩ Viagra)*。倒“U”符号表示两个事件的**交集**；符号*A ∩ B*表示事件*A*和*B*同时发生的事件。
- en: Calculating *P(spam ∩ Viagra)* depends on the **joint probability** of the two
    events or how the probability of one event is related to the probability of the
    other. If the two events are totally unrelated, they are called **independent
    events**. This is not to say that independent events cannot occur at the same
    time; event independence simply implies that knowing the outcome of one event
    does not provide any information about the outcome of the other. For instance,
    the outcome of a heads result on a coin flip is independent from whether the weather
    is rainy or sunny on any given day.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*P(spam ∩ Viagra)*取决于两个事件的**联合概率**，或者说一个事件的概率与另一个事件的概率之间的关系。如果两个事件完全无关，则称为**独立事件**。这并不意味着独立事件不能同时发生；事件独立性仅意味着知道一个事件的结果不会提供关于另一个事件结果的任何信息。例如，抛硬币的正面结果与某天的天气是否晴天或下雨是独立的。
- en: If all events were independent, it would be impossible to predict one event
    by observing another. In other words, **dependent events** are the basis of predictive
    modeling. Just as the presence of clouds is predictive of a rainy day, the appearance
    of the word Viagra is predictive of a spam e-mail.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有事件都是独立的，就不可能通过观察一个事件来预测另一个事件。换句话说，**依赖事件**是预测建模的基础。就像云的存在可以预测下雨天一样，Viagra这个词的出现可以预测垃圾邮件。
- en: '![Understanding joint probability](img/B03905_04_04.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![理解联合概率](img/B03905_04_04.jpg)'
- en: Calculating the probability of dependent events is a bit more complex than for
    independent events. If *P(spam)* and *P(Viagra)* were independent, we could easily
    calculate *P(spam ∩ Viagra)*, the probability of both events happening at the
    same time. Because 20 percent of all the messages are spam, and 5 percent of all
    the e-mails contain the word Viagra, we could assume that 1 percent of all messages
    are spam with the term Viagra. This is because *0.05 * 0.20 = 0.01*. More generally,
    for independent events *A* and *B*, the probability of both happening can be expressed
    as *P(A ∩ B) = P(A) * P(B)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算依赖事件的概率比计算独立事件的概率更为复杂。如果*P(spam)*和*P(Viagra)*是独立的，我们可以轻松计算*P(spam ∩ Viagra)*，即两个事件同时发生的概率。因为所有消息中有20%是垃圾邮件，而5%的电子邮件包含Viagra这个词，我们可以假设1%的消息是含有Viagra的垃圾邮件。这是因为*0.05
    * 0.20 = 0.01*。更一般地，对于独立事件*A*和*B*，两个事件同时发生的概率可以表示为*P(A ∩ B) = P(A) * P(B)*。
- en: This said, we know that *P(spam)* and *P(Viagra)* are likely to be highly dependent,
    which means that this calculation is incorrect. To obtain a reasonable estimate,
    we need to use a more careful formulation of the relationship between these two
    events, which is based on advanced Bayesian methods.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们知道*P(spam)*和*P(Viagra)*很可能是高度相关的，这意味着这个计算是不正确的。为了获得一个合理的估计，我们需要使用一种更精确的公式来描述这两个事件之间的关系，该公式基于先进的贝叶斯方法。
- en: Computing conditional probability with Bayes' theorem
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理计算条件概率
- en: 'The relationships between dependent events can be described using **Bayes''
    theorem**, as shown in the following formula. This formulation provides a way
    of thinking about how to revise an estimate of the probability of one event in
    light of the evidence provided by another event:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖事件之间的关系可以使用**贝叶斯定理**来描述，如下所示的公式。这种公式提供了一种思路，帮助我们根据另一个事件提供的证据来修正对一个事件概率的估计：
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_05.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯定理计算条件概率](img/B03905_04_05.jpg)'
- en: The notation *P(A|B)* is read as the probability of event *A*, given that event
    *B* occurred. This is known as **conditional probability**, since the probability
    of *A* is dependent (that is, conditional) on what happened with event *B*. Bayes'
    theorem tells us that our estimate of *P(A|B)* should be based on *P(A ∩ B)*,
    a measure of how often *A* and *B* are observed to occur together, and *P(B)*,
    a measure of how often *B* is observed to occur in general.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表示为*P(A|B)*的符号表示在事件*B*发生的情况下，事件*A*发生的概率。这就是**条件概率**，因为*A*的概率依赖于事件*B*的发生（即，条件概率）。贝叶斯定理告诉我们，我们对*P(A|B)*的估计应该基于*P(A
    ∩ B)*，即*A*和*B*共同发生的频率，以及*P(B)*，即*B*发生的频率。
- en: Bayes' theorem states that the best estimate of *P(A|B)* is the proportion of
    trials in which *A* occurred with *B* out of all the trials in which *B* occurred.
    In plain language, this tells us that if we know event *B* occurred, the probability
    of event *A* is higher the more often that *A* and *B* occur together each time
    *B* is observed. In a way, this adjusts *P(A ∩ B)* for the probability of *B*
    occurring; if *B* is extremely rare, *P(B)* and *P(A ∩ B)* will always be small;
    however, if *A* and *B* almost always happen together, *P(A|B)* will be high regardless
    of the probability of *B*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理指出，*P(A|B)*的最佳估计是在所有*B*发生的试验中，*A*与*B*同时发生的比例。简单来说，这告诉我们，如果我们知道事件*B*发生了，那么事件*A*发生的概率会随着每次观察到*A*和*B*同时发生的次数而增加。在某种程度上，这调整了*P(A
    ∩ B)*对*B*发生的概率；如果*B*非常罕见，*P(B)*和*P(A ∩ B)*将始终很小；然而，如果*A*和*B*几乎总是一起发生，无论*B*的概率如何，*P(A|B)*都会很高。
- en: 'By definition, *P(A ∩ B) = P(A|B) * P(B)*, a fact that can be easily derived
    by applying a bit of algebra to the previous formula. Rearranging this formula
    once more with the knowledge that *P(A ∩ B) = P(B ∩ A)* results in the conclusion
    that *P(A ∩ B) = P(B|A) * P(A)*, which we can then use in the following formulation
    of Bayes'' theorem:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，*P(A ∩ B) = P(A|B) * P(B)*，这个事实可以通过对先前公式进行一点代数运算来轻松推导出来。再次重新排列这个公式，利用*P(A
    ∩ B) = P(B ∩ A)*的知识，得出*P(A ∩ B) = P(B|A) * P(A)*的结论，然后我们可以在贝叶斯定理的以下公式中使用它：
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_06.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯定理计算条件概率](img/B03905_04_06.jpg)'
- en: In fact, this is the traditional way in which Bayes' theorem has been specified,
    for reasons that will become clear as we apply it to machine learning. First,
    to better understand how Bayes' theorem works in practice, let's revisit our hypothetical
    spam filter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是贝叶斯定理被规定的传统方式，这样做的原因将在我们将其应用于机器学习时变得清晰。首先，为了更好地理解贝叶斯定理在实践中的工作方式，让我们重新审视我们的假设垃圾邮件过滤器。
- en: Without knowledge of an incoming message's content, the best estimate of its
    spam status would be *P(spam)*, the probability that any prior message was spam,
    which we calculated previously to be 20 percent. This estimate is known as the
    **prior probability**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 没有关于即将到来的消息内容的知识，其垃圾邮件状态的最佳估计将是*P(spam)*，即任何先前消息是垃圾邮件的概率，我们先前计算为20%。这个估计被称为**先验概率**。
- en: Suppose that you obtained additional evidence by looking more carefully at the
    set of previously received messages to examine the frequency that the term Viagra
    appeared. The probability that the word Viagra was used in previous spam messages,
    or *P(Viagra|spam)*, is called the **likelihood**. The probability that Viagra
    appeared in any message at all, or *P(Viagra)*, is known as the **marginal likelihood**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你通过仔细查看先前接收到的消息集合，检查“伟哥”一词出现的频率，获得了额外的证据。在先前的垃圾邮件中使用“伟哥”一词的概率，或*P(Viagra|spam)*，被称为**似然性**。任何消息中出现“伟哥”的概率，或*P(Viagra)*，被称为**边际似然性**。
- en: 'By applying Bayes'' theorem to this evidence, we can compute a **posterior
    probability** that measures how likely the message is to be spam. If the posterior
    probability is greater than 50 percent, the message is more likely to be spam
    than ham and it should perhaps be filtered. The following formula shows how Bayes''
    theorem is applied to the evidence provided by the previous e-mail messages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将贝叶斯定理应用于这些证据，我们可以计算一个衡量消息可能是垃圾邮件的**后验概率**。如果后验概率大于50%，那么消息更可能是垃圾邮件而不是正常邮件，可能应该被过滤。以下公式显示了如何将贝叶斯定理应用于先前电子邮件消息提供的证据：
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_07.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯定理计算条件概率](img/B03905_04_07.jpg)'
- en: 'To calculate these components of Bayes'' theorem, it helps to construct a **frequency
    table** (shown on the left in the following diagram) that records the number of
    times Viagra appeared in spam and ham messages. Just like a two-way cross-tabulation,
    one dimension of the table indicates levels of the class variable (spam or ham),
    while the other dimension indicates levels for features (Viagra: yes or no). The
    cells then indicate the number of instances having the particular combination
    of class value and feature value. The frequency table can then be used to construct
    a **likelihood table**, as shown on right in the following diagram. The rows of
    the likelihood table indicate the conditional probabilities for Viagra (yes/no),
    given that an e-mail was either spam or ham:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '为了计算贝叶斯定理的这些组成部分，有助于构建一个**频率表**（如下图左侧所示），记录了“Viagra”出现在垃圾邮件和正常邮件中的次数。就像一个双向交叉表一样，表的一个维度表示类变量的级别（垃圾邮件或正常邮件），而另一个维度表示特征的级别（Viagra:
    是或否）。然后单元格指示具有特定类值和特征值组合的实例数。然后可以使用频率表构建一个**似然表**，如下图右侧所示。似然表的行表示给定邮件是垃圾邮件或正常邮件时“Viagra”（是/否）的条件概率：'
- en: '![Computing conditional probability with Bayes'' theorem](img/B03905_04_08.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯定理计算条件概率](img/B03905_04_08.jpg)'
- en: The likelihood table reveals that *P(Viagra=Yes|spam) = 4/20 = 0.20*, indicating
    that the probability is 20 percent that a message contains the term Viagra, given
    that the message is spam. Additionally, since *P(A ∩ B) = P(B|A) * P(A)*, we can
    calculate *P(spam ∩ Viagra)* as *P(Viagra|spam) * P(spam) = (4/20) * (20/100)
    = 0.04*. The same result can be found in the frequency table, which notes that
    4 out of the 100 messages were spam with the term Viagra. Either way, this is
    four times greater than the previous estimate of 0.01 we calculated as *P(A ∩
    B) = P(A) * P(B)* under the false assumption of independence. This, of course,
    illustrates the importance of Bayes' theorem while calculating joint probability.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 似然表表明*P(Viagra=Yes|spam) = 4/20 = 0.20*，这意味着在垃圾邮件中包含词汇“Viagra”的概率为20%。此外，由于*P(A
    ∩ B) = P(B|A) * P(A)*，我们可以计算*P(spam ∩ Viagra)*为*P(Viagra|spam) * P(spam) = (4/20)
    * (20/100) = 0.04*。在频率表中也可以找到相同的结果，其中指出100封邮件中有4封是带有词汇“Viagra”的垃圾邮件。无论哪种方式，这都比我们在错误假设独立性下计算的0.01的先前估计要大四倍。这当然说明了在计算联合概率时贝叶斯定理的重要性。
- en: To compute the posterior probability, *P(spam|Viagra)*, we simply take *P(Viagra|spam)
    * P(spam) / P(Viagra)* or *(4/20) * (20/100) / (5/100) = 0.80*. Therefore, the
    probability is 80 percent that a message is spam, given that it contains the word
    Viagra. In light of this result, any message containing this term should probably
    be filtered.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算后验概率*P(spam|Viagra)*，我们只需取*P(Viagra|spam) * P(spam) / P(Viagra)*或*(4/20)
    * (20/100) / (5/100) = 0.80*。因此，如果一封邮件包含词汇“Viagra”，那么该邮件是垃圾邮件的概率为80%。根据这个结果，任何包含这个词汇的邮件可能应该被过滤。
- en: This is very much how commercial spam filters work, although they consider a
    much larger number of words simultaneously while computing the frequency and likelihood
    tables. In the next section, we'll see how this concept is put to use when additional
    features are involved.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是商业垃圾邮件过滤器的工作原理，尽管它们在计算频率和似然表时同时考虑了更多的单词。在下一节中，我们将看到当涉及到其他特征时，这个概念是如何被应用的。
- en: The Naive Bayes algorithm
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法
- en: 'The **Naive** **Bayes** algorithm describes a simple method to apply Bayes''
    theorem to classification problems. Although it is not the only machine learning
    method that utilizes Bayesian methods, it is the most common one. This is particularly
    true for text classification, where it has become the de facto standard. The strengths
    and weaknesses of this algorithm are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**算法描述了一种简单的方法，将贝叶斯定理应用于分类问题。虽然它不是唯一利用贝叶斯方法的机器学习方法，但它是最常见的方法。这在文本分类中尤为明显，已成为事实上的标准。该算法的优势和劣势如下：'
- en: '| Strengths | Weaknesses |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 劣势 |'
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Simple, fast, and very effective
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单、快速且非常有效
- en: Does well with noisy and missing data
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理嘈杂和缺失数据方面表现良好
- en: Requires relatively few examples for training, but also works well with very
    large numbers of examples
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要相对较少的训练示例，但也适用于非常大量的示例
- en: Easy to obtain the estimated probability for a prediction
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 易于获得预测的估计概率
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Relies on an often-faulty assumption of equally important and independent features
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于一个经常错误的假设，即特征同等重要且独立
- en: Not ideal for datasets with many numeric features
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有大量数值特征的数据集不理想
- en: Estimated probabilities are less reliable than the predicted classes
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估算的概率不如预测的类别可靠
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The Naive Bayes algorithm is named as such because it makes some "naive" assumptions
    about the data. In particular, Naive Bayes assumes that all of the features in
    the dataset are equally important and independent. These assumptions are rarely
    true in most real-world applications.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法之所以得名，是因为它对数据做出了一些“天真的”假设。特别地，朴素贝叶斯假设数据集中的所有特征都是同等重要且相互独立的。这些假设在大多数实际应用中几乎都不成立。
- en: For example, if you were attempting to identify spam by monitoring e-mail messages,
    it is almost certainly true that some features will be more important than others.
    For example, the e-mail sender may be a more important indicator of spam than
    the message text. Additionally, the words in the message body are not independent
    from one another, since the appearance of some words is a very good indication
    that other words are also likely to appear. A message with the word Viagra will
    probably also contain the words prescription or drugs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你试图通过监控电子邮件来识别垃圾邮件，那么几乎可以肯定，某些特征比其他特征更为重要。例如，电子邮件发送者可能是识别垃圾邮件的更重要指标，而不仅仅是邮件正文的内容。此外，邮件正文中的词语并不是相互独立的，因为某些词语的出现通常预示着其他词语也可能出现。一封包含“Viagra”这个词的邮件，可能也会包含“prescription”或“drugs”等词语。
- en: However, in most cases when these assumptions are violated, Naive Bayes still
    performs fairly well. This is true even in extreme circumstances where strong
    dependencies are found among the features. Due to the algorithm's versatility
    and accuracy across many types of conditions, Naive Bayes is often a strong first
    candidate for classification learning tasks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大多数情况下，即使这些假设被违反，朴素贝叶斯仍然表现得相当好。即使在特征之间存在强相关性的极端情况下也是如此。由于该算法在多种条件下的灵活性和准确性，朴素贝叶斯通常是分类学习任务中的强有力的首选候选算法。
- en: Note
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The exact reason why Naive Bayes works well in spite of its faulty assumptions
    has been the subject of much speculation. One explanation is that it is not important
    to obtain a precise estimate of probability, so long as the predictions are accurate.
    For instance, if a spam filter correctly identifies spam, does it matter whether
    it was 51 percent or 99 percent confident in its prediction? For one discussion
    of this topic, refer to: Domingos P, Pazzani M. On the optimality of the simple
    Bayesian classifier under zero-one loss. *Machine Learning*. 1997; 29:103-130.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管朴素贝叶斯存在一些错误的假设，但它为何依然能够表现良好，一直是许多学者推测的对象。一个解释是，获取精确的概率估计并不重要，只要预测结果是准确的。例如，如果垃圾邮件过滤器能够正确识别垃圾邮件，是否在预测中它有51%的信心还是99%的信心并不重要？关于这个话题的一个讨论，参考：Domingos
    P, Pazzani M. 在零一损失下，简单贝叶斯分类器的最优性。*机器学习*，1997；29：103-130。
- en: Classification with Naive Bayes
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯进行分类
- en: 'Let''s extend our spam filter by adding a few additional terms to be monitored
    in addition to the term Viagra: Money, Groceries, and Unsubscribe. The Naive Bayes
    learner is trained by constructing a likelihood table for the appearance of these
    four words (labeled *W[1]*, *W[2]*, *W[3]*, and *W[4]*), as shown in the following
    diagram for 100 e-mails:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在监测术语“Viagra”之外，添加一些额外的术语来扩展我们的垃圾邮件过滤器：Money、Groceries和Unsubscribe。通过为这四个单词（标记为*W[1]*，*W[2]*，*W[3]*，*W[4]*）的出现构建似然性表来训练朴素贝叶斯学习器，如下图所示，基于100封电子邮件：
- en: '![Classification with Naive Bayes](img/B03905_04_09.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/B03905_04_09.jpg)'
- en: As new messages are received, we need to calculate the posterior probability
    to determine whether they are more likely to be spam or ham, given the likelihood
    of the words found in the message text. For example, suppose that a message contains
    the terms Viagra and Unsubscribe, but does not contain either Money or Groceries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新邮件的到来，我们需要计算后验概率，以确定根据邮件文本中单词的出现情况，邮件更可能是垃圾邮件还是正常邮件。例如，假设一封邮件包含“Viagra”和“Unsubscribe”这两个词，但不包含“Money”或“Groceries”。
- en: 'Using Bayes'' theorem, we can define the problem as shown in the following
    formula. It captures the probability that a message is spam, given that *Viagra
    = Yes*, *Money = No*, *Groceries = No*, and *Unsubscribe = Yes*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理，我们可以像以下公式所示那样定义问题。它捕获了在给定*Viagra = Yes*，*Money = No*，*Groceries = No*，*Unsubscribe
    = Yes*的情况下，邮件是垃圾邮件的概率：
- en: '![Classification with Naive Bayes](img/B03905_04_10.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯进行分类](img/B03905_04_10.jpg)'
- en: For a number of reasons, this formula is computationally difficult to solve.
    As additional features are added, tremendous amounts of memory are needed to store
    probabilities for all of the possible intersecting events; imagine the complexity
    of a Venn diagram for the events for four words, let alone for hundreds or more.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 出于多种原因，这个公式在计算上是困难的。随着附加特征的增加，需要巨大的内存来存储所有可能交集事件的概率；想象一下，如果四个单词的事件形成一个维恩图的复杂性，更不用说成百上千个单词了。
- en: 'The work becomes much easier if we can exploit the fact that Naive Bayes assumes
    independence among events. Specifically, it assumes **class-conditional independence**,
    which means that events are independent so long as they are conditioned on the
    same class value. Assuming conditional independence allows us to simplify the
    formula using the probability rule for independent events, which states that *P(A
    ∩ B) = P(A) * P(B)*. Because the denominator does not depend on the class (spam
    or ham), it is treated as a constant value and can be ignored for the time being.
    This means that the conditional probability of spam can be expressed as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够利用朴素贝叶斯假设事件之间独立性的事实，工作将变得更轻松。具体来说，它假设**类条件独立性**，意味着只要事件在相同的类别值条件下，它们是独立的。假设条件独立性使我们能够使用独立事件的概率规则简化公式，概率规则是*P(A
    ∩ B) = P(A) * P(B)*。因为分母与类别（垃圾邮件或正常邮件）无关，它被视为常数值，可以暂时忽略。这意味着垃圾邮件的条件概率可以表示为：
- en: '![Classification with Naive Bayes](img/B03905_04_11.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/B03905_04_11.jpg)'
- en: 'And the probability that the message is ham can be expressed as:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 并且消息是“ham”的概率可以表示为：
- en: '![Classification with Naive Bayes](img/B03905_04_12.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![使用朴素贝叶斯分类](img/B03905_04_12.jpg)'
- en: Note that the equals symbol has been replaced by the proportional-to symbol
    (similar to a sideways, open-ended '8') to indicate the fact that the denominator
    has been omitted.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，等号被替换为与符号（类似于一个横向的开放“8”），以表示分母被省略了。
- en: 'Using the values in the likelihood table, we can start filling numbers in these
    equations. The overall likelihood of spam is then:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用似然表中的数值，我们可以开始在这些方程中填写数字。垃圾邮件的总体似然度为：
- en: (4/20) * (10/20) * (20/20) * (12/20) * (20/100) = 0.012
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (4/20) * (10/20) * (20/20) * (12/20) * (20/100) = 0.012
- en: 'While the likelihood of ham is:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 而“ham”的似然度为：
- en: (1/80) * (66/80) * (71/80) * (23/80) * (80/100) = 0.002
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (1/80) * (66/80) * (71/80) * (23/80) * (80/100) = 0.002
- en: Because *0.012/0.002 = 6*, we can say that this message is six times more likely
    to be spam than ham. However, to convert these numbers into probabilities, we
    need to perform one last step to reintroduce the denominator that had been excluded.
    Essentially, we must rescale the likelihood of each outcome by dividing it by
    the total likelihood across all possible outcomes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 因为*0.012/0.002 = 6*，我们可以说这条信息是垃圾邮件的可能性是“ham”的六倍。然而，为了将这些数字转换为概率，我们需要执行最后一步，重新引入之前被排除的分母。实质上，我们必须通过将其除以所有可能结果的总似然度，来重新调整每个结果的似然度。
- en: 'In this way, the probability of spam is equal to the likelihood that the message
    is spam divided by the likelihood that the message is either spam or ham:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，垃圾邮件的概率等于该消息是垃圾邮件的似然度除以该消息是垃圾邮件或正常邮件的似然度：
- en: 0.012/(0.012 + 0.002) = 0.857
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 0.012/(0.012 + 0.002) = 0.857
- en: 'Similarly, the probability of ham is equal to the likelihood that the message
    is ham divided by the likelihood that the message is either spam or ham:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，正常邮件的概率等于该消息是正常邮件的似然度除以该消息是垃圾邮件或正常邮件的似然度：
- en: 0.002/(0.012 + 0.002) = 0.143
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 0.002/(0.012 + 0.002) = 0.143
- en: Given the pattern of words found in this message, we expect that the message
    is spam with 85.7 percent probability and ham with 14.3 percent probability. Because
    these are mutually exclusive and exhaustive events, the probabilities sum to 1.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这条信息中找到的单词模式，我们可以预计该信息是垃圾邮件的概率为85.7%，是“ham”的概率为14.3%。因为这两者是互斥且穷尽的事件，概率加和为1。
- en: 'The Naive Bayes classification algorithm we used in the preceding example can
    be summarized by the following formula. The probability of level *L* for class
    *C*, given the evidence provided by features *F[1]* through *F[n]*, is equal to
    the product of the probabilities of each piece of evidence conditioned on the
    class level, the prior probability of the class level, and a scaling factor *1/Z*,
    which converts the likelihood values into probabilities:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的示例中使用的朴素贝叶斯分类算法可以通过以下公式总结。给定特征*F[1]*到*F[n]*所提供的证据，类*C*的*L*级别的概率等于每个证据在类级别条件下的概率与类级别的先验概率以及一个缩放因子*1/Z*的乘积，后者将似然值转化为概率：
- en: '![Classification with Naive Bayes](img/B03905_04_13.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类](img/B03905_04_13.jpg)'
- en: Although this equation seems intimidating, as the prior example illustrated,
    the series of steps is fairly straightforward. Begin by building a frequency table,
    use this to build a likelihood table, and multiply the conditional probabilities
    according to the Naive Bayes' rule. Finally, divide by the total likelihood to
    transform each class likelihood into a probability. After attempting this calculation
    a few times by hand, it will become second nature.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个公式看起来令人畏惧，但正如前面的例子所示，步骤非常简单。首先构建一个频率表，使用这个表构建一个似然表，然后按照朴素贝叶斯规则相乘条件概率。最后，通过总似然性进行除法，将每个类的似然性转化为概率。经过几次手动计算后，这将变成第二天性。
- en: The Laplace estimator
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拉普拉斯估计量
- en: 'Before we employ Naive Bayes in more complex problems, there are some nuances
    to consider. Suppose that we received another message, this time containing all
    four terms: Viagra, Groceries, Money, and Unsubscribe. Using the Naive Bayes algorithm
    as before, we can compute the likelihood of spam as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将朴素贝叶斯应用于更复杂的问题之前，有一些细节需要考虑。假设我们收到了另一条消息，这次包含了所有四个词：Viagra、Groceries、Money和Unsubscribe。像之前一样使用朴素贝叶斯算法，我们可以计算垃圾邮件的似然性为：
- en: (4/20) * (10/20) * (0/20) * (12/20) * (20/100) = 0
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (4/20) * (10/20) * (0/20) * (12/20) * (20/100) = 0
- en: 'The likelihood of ham is:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正常邮件的似然性为：
- en: (1/80) * (14/80) * (8/80) * (23/80) * (80/100) = 0.00005
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (1/80) * (14/80) * (8/80) * (23/80) * (80/100) = 0.00005
- en: 'Therefore, the probability of spam is:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，垃圾邮件的概率为：
- en: 0/(0 + 0.00005) = 0
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 0/(0 + 0.00005) = 0
- en: 'The probability of ham is:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正常邮件的概率为：
- en: 0.00005/(0 + 0\. 0.00005) = 1
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 0.00005/(0 + 0.00005) = 1
- en: These results suggest that the message is spam with 0 percent probability and
    ham with 100 percent probability. Does this prediction make sense? Probably not.
    The message contains several words usually associated with spam, including Viagra,
    which is rarely used in legitimate messages. It is therefore very likely that
    the message has been incorrectly classified.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，该消息的垃圾邮件概率为0%，而正常邮件的概率为100%。这个预测合理吗？可能不合理。该消息包含多个通常与垃圾邮件相关的词汇，包括Viagra，这是合法邮件中很少使用的词汇。因此，很可能该邮件被错误地分类了。
- en: This problem might arise if an event never occurs for one or more levels of
    the class. For instance, the term Groceries had never previously appeared in a
    spam message. Consequently, *P(spam|groceries) = 0%*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个事件在一个或多个类别级别上从未发生，可能会出现这个问题。例如，术语Groceries以前从未出现在垃圾邮件中。因此，*P(spam|groceries)
    = 0%*。
- en: Because probabilities in the Naive Bayes formula are multiplied in a chain,
    this 0 percent value causes the posterior probability of spam to be zero, giving
    the word Groceries the ability to effectively nullify and overrule all of the
    other evidence. Even if the e-mail was otherwise overwhelmingly expected to be
    spam, the absence of the word Groceries in spam will always veto the other evidence
    and result in the probability of spam being zero.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯公式中的概率是按链式相乘的，这个0%的值导致垃圾邮件的后验概率为零，从而使得“Groceries”这个词能够有效地否定并推翻所有其他证据。即使这封邮件在其他方面明显应该是垃圾邮件，垃圾邮件中缺少“Groceries”一词也总是会否决其他证据，导致垃圾邮件的概率为零。
- en: A solution to this problem involves using something called the **Laplace estimator**,
    which is named after the French mathematician Pierre-Simon Laplace. The Laplace
    estimator essentially adds a small number to each of the counts in the frequency
    table, which ensures that each feature has a nonzero probability of occurring
    with each class. Typically, the Laplace estimator is set to 1, which ensures that
    each class-feature combination is found in the data at least once.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法之一是使用被称为**拉普拉斯估计器**的工具，该工具以法国数学家皮埃尔-西蒙·拉普拉斯的名字命名。拉普拉斯估计器本质上是向频率表中的每个计数值添加一个小的数字，确保每个特征在每个类别中都有非零的发生概率。通常，拉普拉斯估计器设置为1，这确保每个类别-特征组合至少在数据中出现一次。
- en: Tip
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The Laplace estimator can be set to any value and does not necessarily even
    have to be the same for each of the features. If you were a devoted Bayesian,
    you could use a Laplace estimator to reflect a presumed prior probability of how
    the feature relates to the class. In practice, given a large enough training dataset,
    this step is unnecessary and the value of 1 is almost always used.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯估计器可以设置为任何值，并不一定对每个特征都相同。如果你是一个坚定的贝叶斯信徒，你可以使用拉普拉斯估计器来反映特征与类别之间的假定先验概率。在实践中，给定一个足够大的训练数据集，这一步通常是不必要的，值通常设置为1。
- en: 'Let''s see how this affects our prediction for this message. Using a Laplace
    value of 1, we add one to each numerator in the likelihood function. The total
    number of 1 values must also be added to each conditional probability denominator.
    The likelihood of spam is therefore:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这如何影响我们对这条信息的预测。使用拉普拉斯值为1时，我们将1加到似然函数中的每个分子。每个条件概率的分母也必须加上1的总数。因此，垃圾邮件的似然性是：
- en: (5/24) * (11/24) * (1/24) * (13/24) * (20/100) = 0.0004
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (5/24) * (11/24) * (1/24) * (13/24) * (20/100) = 0.0004
- en: 'The likelihood of ham is:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 火腿的似然性是：
- en: (2/84) * (15/84) * (9/84) * (24/84) * (80/100) = 0.0001
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (2/84) * (15/84) * (9/84) * (24/84) * (80/100) = 0.0001
- en: This means that the probability of spam is 80 percent, and the probability of
    ham is 20 percent, which is a more plausible result than the one obtained when
    the term Groceries alone determined the result.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着垃圾邮件的概率是80%，火腿的概率是20%，这个结果比仅凭“杂货”一项就决定结果的情况更为合理。
- en: Using numeric features with Naive Bayes
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯中使用数值特征
- en: Because Naive Bayes uses frequency tables to learn the data, each feature must
    be categorical in order to create the combinations of class and feature values
    comprising of the matrix. Since numeric features do not have categories of values,
    the preceding algorithm does not work directly with numeric data. There are, however,
    ways that this can be addressed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯使用频率表来学习数据，因此每个特征必须是类别型的，以便创建包含类别和特征值组合的矩阵。由于数值特征没有类别值，前述算法不能直接应用于数值数据。然而，有一些方法可以解决这个问题。
- en: One easy and effective solution is to **discretize** numeric features, which
    simply means that the numbers are put into categories known as **bins**. For this
    reason, discretization is also sometimes called **binning**. This method is ideal
    when there are large amounts of training data, a common condition while working
    with Naive Bayes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单有效的解决方案是**离散化**数值特征，这意味着将数字放入被称为**箱**的类别中。因此，离散化有时也被称为**分箱**。当训练数据量非常大时，这种方法非常理想，而在使用朴素贝叶斯时，通常会遇到这种情况。
- en: There are several different ways to discretize a numeric feature. Perhaps the
    most common is to explore the data for natural categories or **cut points** in
    the distribution of data. For example, suppose that you added a feature to the
    spam dataset that recorded the time of night or day the e-mail was sent, from
    0 to 24 hours past midnight.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种不同的方法可以对数值特征进行离散化。最常见的一种方法是通过探索数据中自然的类别或**切点**来实现。例如，假设你在垃圾邮件数据集中添加了一个特征，记录电子邮件发送的时间，时间从午夜后的0到24小时不等。
- en: 'Depicted using a histogram, the time data might look something like the following
    diagram. In the early hours of the morning, the message frequency is low. The
    activity picks up during business hours and tapers off in the evening. This seems
    to create four natural bins of activity, as partitioned by the dashed lines indicating
    places where the numeric data are divided into levels of a new nominal feature,
    which could then be used with Naive Bayes:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直方图表示，时间数据可能类似于以下图示。在清晨时段，消息频率较低。活动在工作时间期间增多，并在晚上逐渐减少。这似乎形成了四个自然的活动区间，由虚线分隔，虚线表示数据在这些位置被分割为新类别特征的不同水平，这些特征随后可以与朴素贝叶斯算法一起使用：
- en: '![Using numeric features with Naive Bayes](img/B03905_04_14.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![使用数值特征与朴素贝叶斯算法](img/B03905_04_14.jpg)'
- en: Keep in mind that the choice of four bins was somewhat arbitrary based on the
    natural distribution of data and a hunch about how the proportion of spam might
    change throughout the day. We might expect that spammers operate in the late hours
    of the night or they may operate during the day, when people are likely to check
    their e-mail. This said, to capture these trends, we could have just as easily
    used three bins or twelve.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，选择四个区间是基于数据的自然分布和对垃圾邮件在一天内比例变化的直觉判断，这一选择有些任意。我们可能会预期垃圾邮件发送者会在深夜时分活跃，或者在白天活跃，当人们更可能查看电子邮件时。也就是说，为了捕捉这些趋势，我们也可以使用三个区间或十二个区间。
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If there are no obvious cut points, one option will be to discretize the feature
    using quantiles. You could divide the data into three bins with tertiles, four
    bins with quartiles, or five bins with quintiles.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有明显的切分点，一个选择是使用分位数对特征进行离散化。你可以通过三分位将数据分为三个区间，四分位分为四个区间，或者五分位分为五个区间。
- en: One thing to keep in mind is that discretizing a numeric feature always results
    in a reduction of information as the feature's original granularity is reduced
    to a smaller number of categories. It is important to strike a balance here. Too
    few bins can result in important trends being obscured. Too many bins can result
    in small counts in the Naive Bayes frequency table, which can increase the algorithm's
    sensitivity to noisy data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，离散化数值特征总是会导致信息的丧失，因为特征的原始粒度被减少为更少的类别数量。在这里找到平衡点非常重要。区间过少可能会导致重要的趋势被忽视，而区间过多则可能导致朴素贝叶斯频率表中的计数过小，从而增加算法对噪声数据的敏感性。
- en: Example – filtering mobile phone spam with the Naive Bayes algorithm
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用朴素贝叶斯算法过滤手机垃圾短信
- en: As the worldwide use of mobile phones has grown, a new avenue for electronic
    junk mail has opened for disreputable marketers. These advertisers utilize Short
    Message Service (SMS) text messages to target potential consumers with unwanted
    advertising known as SMS spam. This type of spam is particularly troublesome because,
    unlike e-mail spam, many cellular phone users pay a fee per SMS received. Developing
    a classification algorithm that could filter SMS spam would provide a useful tool
    for cellular phone providers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随着全球手机使用量的增加，一个新的电子垃圾邮件渠道为不道德的营销商打开了大门。这些广告商利用短信（SMS）文本信息，通过SMS垃圾短信向潜在消费者推送不必要的广告。此类垃圾短信特别麻烦，因为与电子邮件垃圾邮件不同，许多手机用户每接收到一条短信都需要支付费用。开发一种能够过滤SMS垃圾短信的分类算法，将为手机服务提供商提供一个有用的工具。
- en: Since Naive Bayes has been used successfully for e-mail spam filtering, it seems
    likely that it could also be applied to SMS spam. However, relative to e-mail
    spam, SMS spam poses additional challenges for automated filters. SMS messages
    are often limited to 160 characters, reducing the amount of text that can be used
    to identify whether a message is junk. The limit, combined with small mobile phone
    keyboards, has led many to adopt a form of SMS shorthand lingo, which further
    blurs the line between legitimate messages and spam. Let's see how a simple Naive
    Bayes classifier handles these challenges.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯算法在电子邮件垃圾邮件过滤中取得了成功，因此它也可能适用于SMS垃圾短信。然而，与电子邮件垃圾邮件相比，SMS垃圾短信为自动过滤器带来了额外的挑战。SMS消息通常限制为160个字符，减少了可以用来判断消息是否为垃圾信息的文本量。这一限制，再加上手机小巧的键盘，促使许多人采用了一种短信简写语言，这进一步模糊了合法消息和垃圾信息之间的界限。让我们看看一个简单的朴素贝叶斯分类器如何应对这些挑战。
- en: Step 1 – collecting data
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 收集数据
- en: To develop the Naive Bayes classifier, we will use data adapted from the SMS
    Spam Collection at [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发朴素贝叶斯分类器，我们将使用从SMS垃圾信息数据集中改编的数据，数据来源：[http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)。
- en: Note
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'To read more about how the SMS Spam Collection was developed, refer to: Gómez
    JM, Almeida TA, Yamakami A. On the validity of a new SMS spam collection. *Proceedings
    of the 11^(th) IEEE International Conference on Machine Learning and Applications*.
    2012.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 若想了解更多关于如何开发SMS垃圾信息数据集的内容，请参考：Gómez JM, Almeida TA, Yamakami A. 《新SMS垃圾信息数据集的有效性》。《第11届IEEE国际机器学习与应用大会论文集》，2012年。
- en: 'This dataset includes the text of SMS messages along with a label indicating
    whether the message is unwanted. Junk messages are labeled spam, while legitimate
    messages are labeled ham. Some examples of spam and ham are shown in the following
    table:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包括短信文本及其标签，标签指示该短信是否为垃圾信息。垃圾信息标记为spam，正常信息标记为ham。以下表格展示了一些垃圾信息和正常信息的示例：
- en: '| Sample SMS ham | Sample SMS spam |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 示例短信正常信息 | 示例短信垃圾信息 |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Better. Made up for Friday and stuffed myself like a pig yesterday. Now I feel
    bleh. But, at least, its not writhing pain kind of bleh.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好。为了弥补星期五的空缺，我昨天吃得像个猪一样。现在感觉有点不舒服。不过，至少不是什么剧烈的痛苦那种不舒服。
- en: If he started searching, he will get job in few days. He has great potential
    and talent.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果他开始找工作，他很快就能找到工作。他有很大的潜力和才能。
- en: I got another job! The one at the hospital, doing data analysis or something,
    starts on Monday! Not sure when my thesis will finish.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我找到另一份工作了！那份在医院的工作，做数据分析之类的，星期一开始！不确定我的论文什么时候完成。
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Congratulations ur awarded 500 of CD vouchers or 125 gift guaranteed & Free
    entry 2 100 wkly draw txt MUSIC to 87066.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恭喜你获得500元CD优惠券或125元礼品，保证免费参加100次每周抽奖，发送MUSIC到87066。
- en: December only! Had your mobile 11mths+? You are entitled to update to the latest
    colour camera mobile for Free! Call The Mobile Update Co FREE on 08002986906.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅限12月！你的手机已使用超过11个月了吗？你有权免费更新为最新的彩色相机手机！拨打08002986906免费联系移动更新公司。
- en: Valentines Day Special! Win over £1000 in our quiz and take your partner on
    the trip of a lifetime! Send GO to 83600 now. 150 p/msg rcvd.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情人节特别活动！在我们的问答比赛中赢取超过1000英镑，并带上你的伴侣一起去体验一生一次的旅行！现在发送GO到83600。每条信息收费150便士。
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Looking at the preceding messages, did you notice any distinguishing characteristics
    of spam? One notable characteristic is that two of the three spam messages use
    the word "free," yet the word does not appear in any of the ham messages. On the
    other hand, two of the ham messages cite specific days of the week, as compared
    to zero in spam messages.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 查看之前的消息时，你是否注意到垃圾信息的一些显著特点？一个显著特点是三条垃圾信息中有两条使用了“free”这个词，而在任何一条正常信息中都没有出现这个词。另一方面，两条正常信息引用了具体的星期几，而垃圾信息中没有一条这样做。
- en: Our Naive Bayes classifier will take advantage of such patterns in the word
    frequency to determine whether the SMS messages seem to better fit the profile
    of spam or ham. While it's not inconceivable that the word "free" would appear
    outside of a spam SMS, a legitimate message is likely to provide additional words
    explaining the context. For instance, a ham message might state "are you free
    on Sunday?" Whereas, a spam message might use the phrase "free ringtones." The
    classifier will compute the probability of spam and ham, given the evidence provided
    by all the words in the message.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的朴素贝叶斯分类器将利用单词频率中的这些模式来判断短信是否更符合垃圾信息或正常信息的特征。虽然不能排除“free”这个词出现在正常短信中的可能性，但合法信息通常会提供更多解释上下文的单词。例如，正常信息可能会写道：“你星期天有空吗？”而垃圾信息可能会使用“免费铃声”这个词组。分类器将根据消息中的所有单词所提供的证据计算垃圾信息和正常信息的概率。
- en: Step 2 – exploring and preparing the data
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2——探索与准备数据
- en: The first step towards constructing our classifier involves processing the raw
    data for analysis. Text data are challenging to prepare, because it is necessary
    to transform the words and sentences into a form that a computer can understand.
    We will transform our data into a representation known as **bag-of-words**, which
    ignores word order and simply provides a variable indicating whether the word
    appears at all.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分类器的第一步是处理原始数据以供分析。文本数据的准备非常具有挑战性，因为需要将单词和句子转化为计算机能够理解的形式。我们将把数据转化为一种称为**词袋模型**的表示方法，该方法忽略单词顺序，仅提供一个变量，表示该单词是否出现过。
- en: Tip
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The data used here has been modified slightly from the original in order to
    make it easier to work with in R. If you plan on following along with the example,
    download the `sms_spam.csv` file from the Packt website and save it in your R
    working directory.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的数据已从原始数据中稍作修改，以便在 R 中更容易处理。如果你打算跟随本示例进行操作，请从 Packt 网站下载 `sms_spam.csv`
    文件，并将其保存在你的 R 工作目录中。
- en: 'We''ll begin by importing the CSV data and saving it in a data frame:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入 CSV 数据并将其保存在数据框中：
- en: '[PRE0]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the `str()` function, we see that the `sms_raw` data frame includes 5,559
    total SMS messages with two features: `type` and `text`. The SMS type has been
    coded as either `ham` or `spam`. The `text` element stores the full raw SMS text.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `str()` 函数，我们看到 `sms_raw` 数据框包含了 5,559 条短信，总共有两个特征：`type` 和 `text`。短信类型被编码为
    `ham` 或 `spam`。`text` 元素存储了完整的原始短信文本。
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `type` element is currently a character vector. Since this is a categorical
    variable, it would be better to convert it into a factor, as shown in the following
    code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`type` 元素目前是一个字符向量。由于这是一个分类变量，最好将其转换为因子，如以下代码所示：'
- en: '[PRE2]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Examining this with the `str()` and `table()` functions, we see that `type`
    has now been appropriately recoded as a factor. Additionally, we see that 747
    (about 13 percent) of SMS messages in our data were labeled as spam, while the
    others were labeled as ham:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `str()` 和 `table()` 函数检查，我们看到 `type` 现在已经被正确地重新编码为因子。此外，我们看到在我们的数据中，747 条（大约
    13%）短信被标记为垃圾短信，而其他的则被标记为正常短信：
- en: '[PRE3]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For now, we will leave the message text alone. As you will learn in the next
    section, processing the raw SMS messages will require the use of a new set of
    powerful tools designed specifically to process text data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将暂时不处理短信文本。正如你将在下一节中学到的那样，处理原始短信消息将需要使用一套专门设计用于处理文本数据的新工具。
- en: Data preparation – cleaning and standardizing text data
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备——清洗和标准化文本数据
- en: SMS messages are strings of text composed of words, spaces, numbers, and punctuation.
    Handling this type of complex data takes a lot of thought and effort. One needs
    to consider how to remove numbers and punctuation; handle uninteresting words
    such as *and*, *but*, and *or*; and how to break apart sentences into individual
    words. Thankfully, this functionality has been provided by the members of the
    R community in a text mining package titled `tm`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 短信消息是由单词、空格、数字和标点符号组成的文本字符串。处理这种复杂数据需要大量的思考和努力。需要考虑如何去除数字和标点符号；处理如 *and*、*but*
    和 *or* 等无关紧要的词；以及如何将句子拆解成单独的词。幸运的是，这一功能已经由R社区的成员通过一个名为`tm`的文本挖掘包提供。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `tm` package was originally created by Ingo Feinerer as a dissertation
    project at the Vienna University of Economics and Business. To learn more, see:
    Feinerer I, Hornik K, Meyer D. Text Mining Infrastructure in R. *Journal of Statistical
    Software*. 2008; 25:1-54.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm` 包最初是由 Ingo Feinerer 在维也纳经济与商业大学作为论文项目创建的。欲了解更多信息，请参见：Feinerer I, Hornik
    K, Meyer D. 《R中的文本挖掘基础设施》.*统计软件杂志*。2008；25:1-54。'
- en: The `tm` package can be installed via the `install.packages("tm")` command and
    loaded with the `library(tm)` command. Even if you already have it installed,
    it may be worth re-running the install process to ensure that your version is
    up-to-date, as the `tm` package is still being actively developed. This occasionally
    results in changes to its functionality.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `install.packages("tm")` 命令安装 `tm` 包，并通过 `library(tm)` 命令加载它。即使你已经安装了该包，也值得重新运行安装过程，以确保你的版本是最新的，因为
    `tm` 包仍在积极开发中。这有时会导致其功能发生变化。
- en: Tip
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This chapter was written and tested using tm version 0.6-2, which was current
    as of July 2015\. If you see differences in the output or if the code does not
    work, you may be using a different version. The Packt Publishing support page
    for this book will post solutions for future `tm` packages if significant changes
    are noted.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是使用 tm 版本 0.6-2 编写和测试的，该版本在 2015 年 7 月时是最新的。如果你发现输出不同或代码无法运行，可能是你使用的是不同的版本。此书的
    Packt Publishing 支持页面会发布未来 `tm` 包的解决方案，若发现有显著变化。
- en: The first step in processing text data involves creating a **corpus**, which
    is a collection of text documents. The documents can be short or long, from individual
    news articles, pages in a book or on the web, or entire books. In our case, the
    corpus will be a collection of SMS messages.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据的第一步是创建一个 **语料库**，即一组文本文件。文件可以是短的或长的，可能是单个新闻文章、书籍中的页面或整个书籍。对于我们而言，语料库将是一组短信。
- en: 'In order to create a corpus, we''ll use the `VCorpus()` function in the `tm`
    package, which refers to a volatile corpus—volatile as it is stored in memory
    as opposed to being stored on disk (the `PCorpus()` function can be used to access
    a permanent corpus stored in a database). This function requires us to specify
    the source of documents for the corpus, which could be from a computer''s filesystem,
    a database, the Web, or elsewhere. Since we already loaded the SMS message text
    into R, we''ll use the `VectorSource()` reader function to create a source object
    from the existing `sms_raw$text` vector, which can then be supplied to `VCorpus()`
    as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个语料库，我们将使用`tm`包中的`VCorpus()`函数，它指的是一个易失性语料库——因为它存储在内存中，而不是存储在磁盘上（`PCorpus()`函数可以用来访问存储在数据库中的永久语料库）。这个函数要求我们指定语料库的文档来源，这些文档可以来自计算机的文件系统、数据库、网络或其他地方。由于我们已经将短信文本加载到R中，我们将使用`VectorSource()`读取器函数从现有的`sms_raw$text`向量创建一个源对象，然后可以将其传递给`VCorpus()`，如下所示：
- en: '[PRE4]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resulting corpus object is saved with the name `sms_corpus`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结果语料库对象被保存为`sms_corpus`。
- en: Tip
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: By specifying an optional `readerControl` parameter, the `VCorpus()` function
    offers functionality to import text from sources such as PDFs and Microsoft Word
    files. To learn more, examine the *Data Import* section in the `tm` package vignette
    using the `vignette("tm")` command.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定可选的`readerControl`参数，`VCorpus()`函数提供了从PDF和Microsoft Word文件等来源导入文本的功能。要了解更多，请使用`vignette("tm")`命令查看`tm`包手册中的*数据导入*部分。
- en: 'By printing the corpus, we see that it contains documents for each of the 5,559
    SMS messages in the training data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印语料库，我们可以看到它包含了训练数据中5,559条短信的文档：
- en: '[PRE5]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Because the `tm` corpus is essentially a complex list, we can use list operations
    to select documents in the corpus. To receive a summary of specific messages,
    we can use the `inspect()` function with list operators. For example, the following
    command will view a summary of the first and second SMS messages in the corpus:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`tm`语料库本质上是一个复杂的列表，所以我们可以使用列表操作来选择语料库中的文档。为了接收特定信息的摘要，我们可以使用带有列表操作符的`inspect()`函数。例如，以下命令将查看语料库中第一和第二条短信的摘要：
- en: '[PRE6]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To view the actual message text, the `as.character()` function must be applied
    to the desired messages. To view one message, use the `as.character()` function
    on a single list element, noting that the double-bracket notation is required:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看实际的消息文本，必须对所需的消息应用`as.character()`函数。要查看一条消息，可以对单个列表元素使用`as.character()`函数，注意需要使用双括号表示法：
- en: '[PRE7]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To view multiple documents, we''ll need to use `as.character()` on several
    items in the `sms_corpus` object. To do so, we''ll use the `lapply()` function,
    which is a part of a family of R functions that applies a procedure to each element
    of an R data structure. These functions, which include `apply()` and `sapply()`
    among others, are one of the key idioms of the R language. Experienced R coders
    use these much like the way `for` or `while` loops are used in other programming
    languages, as they result in more readable (and sometimes more efficient) code.
    The `lapply()` command to apply `as.character()` to a subset of corpus elements
    is as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看多个文档，我们需要对`sms_corpus`对象中的多个项使用`as.character()`。为此，我们将使用`lapply()`函数，它是R函数家族的一部分，能够对R数据结构的每个元素应用一个过程。包括`apply()`和`sapply()`等在内的这些函数，是R语言的关键用法之一。经验丰富的R程序员使用这些函数，就像其他编程语言中使用`for`或`while`循环一样，因为它们使代码更加易读（有时也更高效）。将`as.character()`应用于语料库元素子集的`lapply()`命令如下所示：
- en: '[PRE8]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As noted earlier, the corpus contains the raw text of 5,559 text messages. In
    order to perform our analysis, we need to divide these messages into individual
    words. But first, we need to clean the text, in order to standardize the words,
    by removing punctuation and other characters that clutter the result. For example,
    we would like the strings *Hello*!, *HELLO*, and *hello* to be counted as instances
    of the same word.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，语料库包含了5,559条短信的原始文本。为了进行分析，我们需要将这些消息分解为单独的单词。但首先，我们需要清理文本，以标准化单词，通过去除标点符号和其他杂乱的字符来简化结果。例如，我们希望字符串*Hello*！，*HELLO*，和*hello*被计为同一个单词的实例。
- en: The `tm_map()` function provides a method to apply a transformation (also known
    as mapping) to a `tm` corpus. We will use this function to clean up our corpus
    using a series of transformations and save the result in a new object called `corpus_clean`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm_map()`函数提供了一种将转换（也称为映射）应用到`tm`语料库的方法。我们将使用此函数通过一系列转换清理语料库，并将结果保存在一个名为`corpus_clean`的新对象中。'
- en: 'Our first order of business will be to standardize the messages to use only
    lowercase characters. To this end, R provides a `tolower()` function that returns
    a lowercase version of text strings. In order to apply this function to the corpus,
    we need to use the `tm` wrapper function `content_transformer()` to treat `tolower()`
    as a transformation function that can be used to access the corpus. The full command
    is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是将消息标准化，只使用小写字母。为此，R提供了一个`tolower()`函数，它返回文本字符串的小写版本。为了将这个函数应用到语料库中，我们需要使用`tm`包装函数`content_transformer()`将`tolower()`视为一种转换函数，可以用于访问语料库。完整的命令如下：
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To check whether the command worked as advertised, let''s inspect the first
    message in the original corpus and compare it to the same in the transformed corpus:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查命令是否如预期那样工作，我们来检查原始语料库中的第一条消息，并将其与转换后的语料库中的相同消息进行对比：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As expected, uppercase letters have been replaced by lowercase versions of the
    same.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，大写字母已被其对应的小写字母替换。
- en: Tip
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The `content_transformer()` function can be used to apply more sophisticated
    text processing and cleanup processes, such as `grep` pattern matching and replacement.
    Simply write a custom function and wrap it before applying via `tm_map()` as done
    earlier.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`content_transformer()`函数可以用于应用更复杂的文本处理和清理过程，如`grep`模式匹配和替换。只需编写自定义函数并在应用之前通过`tm_map()`进行包装，就像之前做的那样。'
- en: 'Let''s continue our cleanup by removing numbers from the SMS messages. Although
    some numbers may provide useful information, the majority would likely be unique
    to individual senders and thus will not provide useful patterns across all messages.
    With this in mind, we''ll strip all the numbers from the corpus as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续清理工作，移除SMS消息中的数字。尽管某些数字可能提供有用信息，但大多数数字可能是特定于发送者的，因此无法在所有消息中提供有用的模式。考虑到这一点，我们将按照以下方式从语料库中删除所有数字：
- en: '[PRE11]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Tip
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Note that the preceding code did not use the `content_transformer()` function.
    This is because `removeNumbers()` is built into `tm` along with several other
    mapping functions that do not need to be wrapped. To see the other built-in transformations,
    simply type `getTransformations()`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的代码没有使用`content_transformer()`函数。这是因为`removeNumbers()`是`tm`内置的，并且还有其他一些不需要包装的映射函数。要查看其他内置转换，只需输入`getTransformations()`。
- en: Our next task is to remove filler words such as *to*, *and*, *but*, and *or*
    from our SMS messages. These terms are known as **stop words** and are typically
    removed prior to text mining. This is due to the fact that although they appear
    very frequently, they do not provide much useful information for machine learning.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个任务是从SMS消息中删除填充词，如*to*、*and*、*but*和*or*。这些词被称为**停用词**，通常在文本挖掘之前被移除。这是因为，尽管它们出现频繁，但对机器学习来说并未提供太多有用信息。
- en: Rather than define a list of stop words ourselves, we'll use the `stopwords()`
    function provided by the `tm` package. This function allows us to access various
    sets of stop words, across several languages. By default, common English language
    stop words are used. To see the default list, type `stopwords()` at the command
    line. To see the other languages and options available, type `?stopwords` for
    the documentation page.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算自己定义停用词列表，而是使用`tm`包提供的`stopwords()`函数。该函数允许我们访问多个语言的停用词集合。默认情况下，使用的是常见的英语停用词。要查看默认列表，请在命令行输入`stopwords()`。要查看其他语言和可用选项，请输入`?stopwords`以访问文档页面。
- en: Tip
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Even within a single language, there is no single definitive list of stop words.
    For example, the default English list in `tm` includes about 174 words while another
    option includes 571 words. You can even specify your own list of stop words if
    you prefer. Regardless of the list you choose, keep in mind the goal of this transformation,
    which is to eliminate all useless data while keeping as much useful information
    as possible.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在同一种语言中，也没有一个统一的停用词列表。例如，`tm`中的默认英文停用词列表包含约174个单词，而另一个选项包含571个单词。如果需要，您甚至可以指定自己的停用词列表。无论选择哪个列表，都请记住这次转换的目标，即在尽可能保留有用信息的同时，消除所有无用数据。
- en: 'The stop words alone are not a useful transformation. What we need is a way
    to remove any words that appear in the stop words list. The solution lies in the
    `removeWords()` function, which is a transformation included with the `tm` package.
    As we have done before, we''ll use the `tm_map()` function to apply this mapping
    to the data, providing the `stopwords()` function as a parameter to indicate exactly
    the words we would like to remove. The full command is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词本身并不是一个有用的转换。我们需要的是一种方法，能够移除停用词列表中出现的任何单词。解决方案在于`removeWords()`函数，这是`tm`包中包含的一个转换功能。正如我们之前所做的那样，我们将使用`tm_map()`函数将此映射应用于数据，并提供`stopwords()`函数作为参数，明确表示我们想要移除的单词。完整命令如下：
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since `stopwords()` simply returns a vector of stop words, had we chosen so,
    we could have replaced it with our own vector of words to be removed. In this
    way, we could expand or reduce the list of stop words to our liking or remove
    a completely different set of words entirely.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`stopwords()`仅仅返回一个停用词向量，如果我们愿意，也可以用我们自己的词向量来替代它。通过这种方式，我们可以根据自己的需要扩展或缩减停用词列表，甚至完全移除一个不同的单词集合。
- en: 'Continuing with our cleanup process, we can also eliminate any punctuation
    from the text messages using the built-in `removePunctuation()` transformation:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的清理过程，我们还可以使用内置的`removePunctuation()`转换来消除文本中的标点符号：
- en: '[PRE13]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `removePunctuation()` transformation strips punctuation characters from
    the text blindly, which can lead to unintended consequences. For example, consider
    what happens when it is applied as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`removePunctuation()`转换会盲目地去除文本中的标点符号，这可能会导致一些意外的后果。例如，考虑当它如下应用时会发生什么：'
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As shown, the lack of blank space after the ellipses has caused the words *hello*
    and *world* to be joined as a single word. While this is not a substantial problem
    for our analysis, it is worth noting for the future.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，省略号后没有空格，导致单词*hello*和*world*被连接为一个单词。虽然这对我们的分析没有实质性影响，但还是值得注意。
- en: Tip
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'To work around the default behavior of `removePunctuation()`, simply create
    a custom function that replaces rather than removes punctuation characters:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规避`removePunctuation()`的默认行为，只需创建一个自定义函数，用来替换而不是移除标点符号字符：
- en: '[PRE15]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Essentially, this uses R's `gsub()` function to substitute any punctuation characters
    in `x` with a blank space. The `replacePunctuation()` function can then be used
    with `tm_map()` as with other transformations.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这使用了R的`gsub()`函数，将`x`中的任何标点符号字符替换为空格。然后，可以像其他转换一样，使用`replacePunctuation()`函数与`tm_map()`一起使用。
- en: Another common standardization for text data involves reducing words to their
    root form in a process called **stemming**. The stemming process takes words like
    *learned*, *learning*, and *learns*, and strips the suffix in order to transform
    them into the base form, *learn*. This allows machine learning algorithms to treat
    the related terms as a single concept rather than attempting to learn a pattern
    for each variant.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的文本数据标准化方法是通过一个叫做**词干提取**的过程，将单词简化为其词根形式。词干提取过程将像*learned*、*learning*和*learns*这样的单词去除后缀，转化为基础形式*learn*。这使得机器学习算法能够将相关术语视为单一概念，而不是尝试为每个变体学习一个模式。
- en: The `tm` package provides stemming functionality via integration with the `SnowballC`
    package. At the time of this writing, `SnowballC` was not installed by default
    with `tm`. Do so with `install.packages("SnowballC")` if it is not installed already.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm`包通过与`SnowballC`包的集成提供词干提取功能。在撰写本文时，`SnowballC`并未默认安装在`tm`中。如果尚未安装，可以使用`install.packages("SnowballC")`进行安装。'
- en: Note
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `SnowballC` package is maintained by Milan Bouchet-Valat and provides an
    R interface to the C-based `libstemmer` library, which is based on M.F. Porter's
    "Snowball" word stemming algorithm, a widely used open source stemming method.
    For more detail, see [http://snowball.tartarus.org](http://snowball.tartarus.org).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`SnowballC`包由Milan Bouchet-Valat维护，并提供了一个R接口，连接到基于C的`libstemmer`库，后者基于M.F.
    Porter的“Snowball”词干提取算法，这是一个广泛使用的开源词干提取方法。更多细节，见[http://snowball.tartarus.org](http://snowball.tartarus.org)。'
- en: 'The `SnowballC` package provides a `wordStem()` function, which for a character
    vector, returns the same vector of terms in its root form. For example, the function
    correctly stems the variants of the word *learn*, as described previously:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`SnowballC`包提供了一个`wordStem()`函数，对于一个字符向量，它返回该向量的词根形式。例如，该函数能够正确地提取*learn*一词的不同变体，如前所述：'
- en: '[PRE16]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In order to apply the `wordStem()` function to an entire corpus of text documents,
    the `tm` package includes a `stemDocument()` transformation. We apply this to
    our corpus with the `tm_map()` function exactly as done earlier:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将`wordStem()`函数应用于整个文本文档语料库，`tm`包提供了一个`stemDocument()`转换。我们通过`tm_map()`函数将其应用于我们的语料库，方式与之前相同：
- en: '[PRE17]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Tip
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you receive an error message while applying the `stemDocument()` transformation,
    please confirm that you have the `SnowballC` package installed. If after installing
    the package you still encounter the message that `all scheduled cores encountered
    errors`, you can also try forcing the `tm_map()` command to a single core, by
    adding an additional parameter to specify `mc.cores=1`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在应用`stemDocument()`转换时遇到错误消息，请确认你已安装`SnowballC`包。如果在安装包后仍然遇到“所有计划的核心都遇到错误”的消息，你还可以尝试强制将`tm_map()`命令限制为单一核心，通过添加一个额外参数`mc.cores=1`来指定。
- en: 'After removing numbers, stop words, and punctuation as well as performing stemming,
    the text messages are left with the blank spaces that previously separated the
    now-missing pieces. The final step in our text cleanup process is to remove additional
    whitespace, using the built-in `stripWhitespace()` transformation:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在移除数字、停用词和标点符号并进行词干提取之后，文本消息留下了原先分隔这些已消失部分的空格。我们文本清理过程的最后一步是去除额外的空白字符，使用内置的`stripWhitespace()`转换：
- en: '[PRE18]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following table shows the first three messages in the SMS corpus before
    and after the cleaning process. The messages have been limited to the most interesting
    words, and punctuation and capitalization have been removed:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了SMS语料库中前3条消息在清理过程前后的样子。消息已经缩减为最有趣的单词，标点符号和大小写已被移除：
- en: '| SMS messages before cleaning | SMS messages after cleaning |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 清理前的SMS消息 | 清理后的SMS消息 |'
- en: '| --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE20]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Data preparation – splitting text documents into words
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 将文本文件拆分为单词
- en: Now that the data are processed to our liking, the final step is to split the
    messages into individual components through a process called **tokenization**.
    A token is a single element of a text string; in this case, the tokens are words.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经按照我们的需求处理完成，最后一步是通过一个叫做**分词（tokenization）**的过程将消息拆分成单独的组件。一个词元（token）是文本字符串的一个单独元素；在这里，词元指的是单词。
- en: As you might assume, the `tm` package provides functionality to tokenize the
    SMS message corpus. The `DocumentTermMatrix()` function will take a corpus and
    create a data structure called a **Document Term Matrix** (**DTM**) in which rows
    indicate documents (SMS messages) and columns indicate terms (words).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，`tm`包提供了将SMS消息语料库进行分词的功能。`DocumentTermMatrix()`函数将处理一个语料库并创建一个叫做**文档-词矩阵（Document
    Term Matrix，DTM）**的数据结构，其中行表示文档（SMS消息），列表示术语（单词）。
- en: Tip
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The `tm` package also provides a data structure for a **Term Document Matrix**
    (**TDM**), which is simply a transposed DTM in which the rows are terms and the
    columns are documents. Why the need for both? Sometimes, it is more convenient
    to work with one or the other. For example, if the number of documents is small,
    while the word list is large, it may make sense to use a TDM because it is generally
    easier to display many rows than to display many columns. This said, the two are
    often interchangeable.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm`包还提供了一种叫做**词-文档矩阵（Term Document Matrix，TDM）**的数据结构，它仅仅是DTM的转置，其中行是词语，列是文档。为什么需要两者？有时，使用其中一个更方便。例如，如果文档的数量较少，而词汇表很大，使用TDM可能更合适，因为显示许多行通常比显示许多列更为容易。话虽如此，二者常常是可以互换的。'
- en: 'Each cell in the matrix stores a number indicating a count of the times the
    word represented by the column appears in the document represented by the row.
    The following illustration depicts only a small portion of the DTM for the SMS
    corpus, as the complete matrix has 5,559 rows and over 7,000 columns:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每个单元格存储一个数字，表示由列所代表的单词在由行所代表的文档中出现的次数。以下插图仅展示了SMS语料库DTM的一个小部分，完整的矩阵有5,559行和超过7,000列：
- en: '![Data preparation – splitting text documents into words](img/B03905_04_15.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![数据准备 – 将文本文件拆分为单词](img/B03905_04_15.jpg)'
- en: The fact that each cell in the table is zero implies that none of the words
    listed on the top of the columns appear in any of the first five messages in the
    corpus. This highlights the reason why this data structure is called a **sparse
    matrix**; the vast majority of the cells in the matrix are filled with zeros.
    Stated in real-world terms, although each message must contain at least one word,
    the probability of any one word appearing in a given message is small.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表中每个单元格为零，意味着列顶部列出的单词在语料库中的前五条消息中都没有出现。这突显了为什么这种数据结构被称为**稀疏矩阵**的原因；矩阵中的绝大多数单元格都填充的是零。用现实世界的术语来说，尽管每条消息必须至少包含一个单词，但某个单词出现在给定消息中的概率是很小的。
- en: 'Creating a DTM sparse matrix, given a `tm` corpus, involves a single command:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个`tm`语料库，创建一个DTM稀疏矩阵只需要一条命令：
- en: '[PRE21]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This will create an `sms_dtm` object that contains the tokenized corpus using
    the default settings, which apply minimal processing. The default settings are
    appropriate because we have already prepared the corpus manually.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含使用默认设置进行标记化语料库的` sms_dtm`对象，默认设置应用最小处理。默认设置是合适的，因为我们已经手动准备了语料库。
- en: 'On the other hand, if we hadn''t performed the preprocessing, we could do so
    here by providing a list of `control` parameter options to override the defaults.
    For example, to create a DTM directly from the raw, unprocessed SMS corpus, we
    can use the following command:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们没有执行预处理操作，可以在此通过提供` control`参数选项来覆盖默认设置。比如，直接从原始未处理的SMS语料库创建DTM，我们可以使用以下命令：
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This applies the same preprocessing steps to the SMS corpus in the same order
    as done earlier. However, comparing `sms_dtm` to `sms_dtm2`, we see a slight difference
    in the number of terms in the matrix:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这对SMS语料库应用相同的预处理步骤，顺序与之前相同。然而，将` sms_dtm`与` sms_dtm2`进行比较时，我们可以看到矩阵中术语的数量有轻微差异：
- en: '[PRE23]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The reason for this discrepancy has to do with a minor difference in the ordering
    of the preprocessing steps. The `DocumentTermMatrix()` function applies its cleanup
    functions to the text strings only after they have been split apart into words.
    Thus, it uses a slightly different stop words removal function. Consequently,
    some words split differently than when they are cleaned before tokenization.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异的原因与预处理步骤的顺序微小差异有关。` DocumentTermMatrix()`函数仅在文本字符串被拆分成单词后才会应用其清理功能。因此，它使用略有不同的停用词移除函数。因此，一些单词的拆分方式与它们在标记化之前清理时有所不同。
- en: Tip
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'To force the two prior document term matrices to be identical, we can override
    the default stop words function with our own that uses the original replacement
    function. Simply replace `stopwords = TRUE` with the following:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制将两个先前的文档术语矩阵保持一致，我们可以用自己的函数覆盖默认的停用词功能，该函数使用原始的替换功能。只需将`stopwords = TRUE`替换为以下内容：
- en: '[PRE24]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The differences between these two cases illustrate an important principle of
    cleaning text data: the order of operations matters. With this in mind, it is
    very important to think through how early steps in the process are going to affect
    later ones. The order presented here will work in many cases, but when the process
    is tailored more carefully to specific datasets and use cases, it may require
    rethinking. For example, if there are certain terms you hope to exclude from the
    matrix, consider whether you should search for them before or after stemming.
    Also, consider how the removal of punctuation—and whether the punctuation is eliminated
    or replaced by blank space—affects these steps.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况的差异说明了清理文本数据时一个重要的原则：操作的顺序很重要。考虑到这一点，仔细思考处理过程中的早期步骤如何影响后续步骤是非常重要的。这里提供的顺序在许多情况下有效，但当该过程针对特定数据集和使用案例进行更加精细的定制时，可能需要重新思考。例如，如果有某些术语是你希望从矩阵中排除的，考虑是否应该在词干提取之前或之后搜索它们。此外，还要考虑如何去除标点符号——以及标点符号是被消除还是被空格替代——对这些步骤的影响。
- en: Data preparation – creating training and test datasets
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备——创建训练集和测试集
- en: With our data prepared for analysis, we now need to split the data into training
    and test datasets, so that once our spam classifier is built, it can be evaluated
    on data it has not previously seen. But even though we need to keep the classifier
    blinded as to the contents of the test dataset, it is important that the split
    occurs after the data have been cleaned and processed; we need exactly the same
    preparation steps to occur on both the training and test datasets.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备好进行分析后，我们现在需要将数据分成训练集和测试集，以便在构建完垃圾邮件分类器后，可以在未曾见过的数据上进行评估。但即便如此，我们需要确保在数据清洗和处理之后再进行拆分；我们需要在训练集和测试集上进行相同的准备步骤。
- en: 'We''ll divide the data into two portions: 75 percent for training and 25 percent
    for testing. Since the SMS messages are sorted in a random order, we can simply
    take the first 4,169 for training and leave the remaining 1,390 for testing. Thankfully,
    the DTM object acts very much like a data frame and can be split using the standard
    `[row, col]` operations. As our DTM stores SMS messages as rows and words as columns,
    we must request a specific range of rows and all columns for each:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把数据分成两部分：75%用于训练，25%用于测试。由于短信是随机排序的，我们可以简单地取前4,169条用于训练，将剩下的1,390条用于测试。幸运的是，DTM对象非常像数据框，可以使用标准的`[row,
    col]`操作进行拆分。由于我们的DTM将短信存储为行，单词存储为列，我们必须为每一部分请求特定的行范围以及所有列：
- en: '[PRE25]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For convenience later on, it is also helpful to save a pair of vectors with
    labels for each of the rows in the training and testing matrices. These labels
    are not stored in the DTM, so we would need to pull them from the original `sms_raw`
    data frame:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以后的方便，也有必要保存一对包含训练和测试矩阵中每一行标签的向量。这些标签并未存储在DTM中，所以我们需要从原始的`sms_raw`数据框中提取它们：
- en: '[PRE26]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To confirm that the subsets are representative of the complete set of SMS data,
    let''s compare the proportion of spam in the training and test data frames:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认这些子集代表完整的短信数据集，让我们比较训练数据和测试数据框中垃圾邮件的比例：
- en: '[PRE27]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Both the training data and test data contain about 13 percent spam. This suggests
    that the spam messages were divided evenly between the two datasets.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和测试数据中大约有13%的垃圾邮件。这表明垃圾邮件在两个数据集之间被均匀地划分。
- en: Visualizing text data – word clouds
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化文本数据——词云
- en: A **word cloud** is a way to visually depict the frequency at which words appear
    in text data. The cloud is composed of words scattered somewhat randomly around
    the figure. Words appearing more often in the text are shown in a larger font,
    while less common terms are shown in smaller fonts. This type of figures grew
    in popularity recently, since it provides a way to observe trending topics on
    social media websites.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**词云**是一种通过可视化文本数据中单词出现频率的方式。词云由随机分布的单词组成，文本中出现频率更高的单词会显示为较大的字体，而较少出现的词则显示为较小的字体。这种类型的图形最近越来越流行，因为它提供了一种观察社交媒体网站上热门话题的方式。'
- en: The `wordcloud` package provides a simple R function to create this type of
    diagrams. We'll use it to visualize the types of words in SMS messages, as comparing
    the clouds for spam and ham will help us gauge whether our Naive Bayes spam filter
    is likely to be successful. If you haven't already done so, install and load the
    package by typing `install.packages("wordcloud")` and `library(wordcloud)` at
    the R command line.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`wordcloud`包提供了一个简单的R函数来创建这种类型的图表。我们将使用它来可视化短信中的词汇类型，因为比较垃圾邮件和正常邮件的词云将帮助我们评估我们的朴素贝叶斯垃圾邮件过滤器是否可能成功。如果你还没有安装这个包，可以通过在R命令行输入`install.packages("wordcloud")`和`library(wordcloud)`来安装并加载它。'
- en: Note
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: The `wordcloud` package was written by Ian Fellows. For more information on
    this package, visit his blog at [http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`wordcloud`包是由Ian Fellows编写的。有关此包的更多信息，请访问他的博客 [http://blog.fellstat.com/?cat=11](http://blog.fellstat.com/?cat=11)。'
- en: 'A word cloud can be created directly from a `tm` corpus object using the syntax:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接从`tm`语料库对象创建词云，使用以下语法：
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This will create a word cloud from our prepared SMS corpus. Since we specified
    `random.order = FALSE`, the cloud will be arranged in a nonrandom order with higher
    frequency words placed closer to the center. If we do not specify `random.order`,
    the cloud would be arranged randomly by default. The `min.freq` parameter specifies
    the number of times a word must appear in the corpus before it will be displayed
    in the cloud. Since a frequency of 50 is about 1 percent of the corpus, this means
    that a word must be found in at least 1 percent of the SMS messages to be included
    in the cloud.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从我们准备好的短信语料库中创建一个词云。由于我们指定了`random.order = FALSE`，词云将按非随机顺序排列，频率较高的单词会被放置在中心附近。如果我们没有指定`random.order`，词云将默认随机排列。`min.freq`参数指定了一个单词在语料库中必须出现的次数，只有达到这个次数，单词才会出现在词云中。由于频率为50大约是语料库的1%，这意味着一个单词必须至少在1%的短信中出现，才能被包含在词云中。
- en: Tip
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: You might get a warning message noting that R was unable to fit all of the words
    in the figure. If so, try increasing `min.freq` to reduce the number of words
    in the cloud. It might also help to use the `scale` parameter to reduce the font
    size.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会收到一个警告消息，指出R无法将所有单词都显示在图中。如果是这样，尝试增加`min.freq`以减少词云中的单词数量。使用`scale`参数来缩小字体大小也可能有所帮助。
- en: 'The resulting word cloud should appear similar to the following figure:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的词云应类似于下图所示：
- en: '![Visualizing text data – word clouds](img/B03905_04_16.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![可视化文本数据 – 词云](img/B03905_04_16.jpg)'
- en: A perhaps more interesting visualization involves comparing the clouds for SMS
    spam and ham. Since we did not construct separate corpora for spam and ham, this
    is an appropriate time to note a very helpful feature of the `wordcloud()` function.
    Given a vector of raw text strings, it will automatically apply common text preparation
    processes before displaying the cloud.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能更有趣的可视化是比较短信垃圾信息（spam）和正常信息（ham）的词云。由于我们没有为垃圾信息和正常信息构建单独的语料库，这是一个非常适合说明`wordcloud()`函数的有用功能的时机。给定一个原始文本字符串的向量，它会在显示词云之前自动应用常见的文本预处理过程。
- en: 'Let''s use R''s `subset()` function to take a subset of the `sms_raw` data
    by the SMS `type`. First, we''ll create a subset where the message `type` is `spam`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用R的`subset()`函数通过短信`type`来获取` sms_raw`数据的一个子集。首先，我们将创建一个`type`为`spam`的子集：
- en: '[PRE29]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we''ll do the same thing for the `ham` subset:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对`ham`子集做相同的操作：
- en: '[PRE30]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Tip
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Be careful to note the double equals sign. Like many programming languages,
    R uses `==` to test equality. If you accidently use a single equals sign, you'll
    end up with a subset much larger than you expected!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 请小心注意双等号。像许多编程语言一样，R使用`==`来测试相等性。如果你不小心使用了单个等号，你将得到一个比预期更大的子集！
- en: 'We now have two data frames, `spam` and `ham`, each with a `text` feature containing
    the raw text strings for SMSes. Creating word clouds is as simple as before. This
    time, we''ll use the `max.words` parameter to look at the 40 most common words
    in each of the two sets. The scale parameter allows us to adjust the maximum and
    minimum font size for words in the cloud. Feel free to adjust these parameters
    as you see fit. This is illustrated in the following commands:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个数据框，`spam`和`ham`，每个框中都有一个`text`特征，包含短信的原始文本字符串。创建词云与之前一样简单。这一次，我们将使用`max.words`参数来查看两个集合中最常见的40个单词。`scale`参数允许我们调整词云中单词的最大和最小字体大小。根据需要自由调整这些参数。以下命令示范了这一过程：
- en: '[PRE31]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The resulting word clouds are shown in the following diagram:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的词云显示在下图中：
- en: '![Visualizing text data – word clouds](img/B03905_04_17.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![可视化文本数据 – 词云](img/B03905_04_17.jpg)'
- en: Do you have a hunch about which one is the spam cloud and which represents ham?
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜到哪个是垃圾信息词云，哪个是正常信息词云吗？
- en: Tip
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Because of the randomization process, each word cloud may look slightly different.
    Running the `wordcloud()` function several times allows you to choose the cloud
    that is the most visually appealing for presentation purposes.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机化过程，每个词云可能看起来略有不同。运行`wordcloud()`函数几次可以让你选择一个在展示中最具视觉吸引力的词云。
- en: As you probably guessed, the spam cloud is on the left. Spam messages include
    words such as *urgent*, *free*, *mobile*, *claim*, and *stop*; these terms do
    not appear in the ham cloud at all. Instead, ham messages use words such as *can*,
    *sorry*, *need*, and *time*. These stark differences suggest that our Naive Bayes
    model will have some strong key words to differentiate between the classes.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜测的，垃圾短信云在左边。垃圾短信中包含如*urgent*（紧急）、*free*（免费）、*mobile*（手机）、*claim*（索赔）和*stop*（停止）等词汇；这些词汇在正常短信云中完全没有出现。相反，正常短信使用如*can*（可以）、*sorry*（抱歉）、*need*（需要）和*time*（时间）等词汇。这些显著的差异表明，我们的朴素贝叶斯模型将拥有一些强有力的关键词来区分这两类信息。
- en: Data preparation – creating indicator features for frequent words
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备——为频繁出现的词汇创建指示特征
- en: The final step in the data preparation process is to transform the sparse matrix
    into a data structure that can be used to train a Naive Bayes classifier. Currently,
    the sparse matrix includes over 6,500 features; this is a feature for every word
    that appears in at least one SMS message. It's unlikely that all of these are
    useful for classification. To reduce the number of features, we will eliminate
    any word that appear in less than five SMS messages, or in less than about 0.1
    percent of the records in the training data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备过程的最后一步是将稀疏矩阵转换为一个可以用来训练朴素贝叶斯分类器的数据结构。目前，稀疏矩阵包含超过6,500个特征；每个特征对应一个至少在一条短信中出现的词汇。这些特征不一定都是有用的，因此为了减少特征的数量，我们将剔除任何在少于五条短信中出现的词汇，或者出现在训练数据中不到0.1%的记录中的词汇。
- en: 'Finding frequent words requires use of the `findFreqTerms()` function in the
    `tm` package. This function takes a DTM and returns a character vector containing
    the words that appear for at least the specified number of times. For instance,
    the following command will display the words appearing at least five times in
    the `sms_dtm_train` matrix:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 查找频繁词汇需要使用`findFreqTerms()`函数，该函数位于`tm`包中。该函数接受一个文档词项矩阵（DTM）并返回一个字符向量，包含在至少指定次数中出现的词汇。例如，以下命令将显示在`sms_dtm_train`矩阵中至少出现五次的词汇：
- en: '[PRE32]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The result of the function is a character vector, so let''s save our frequent
    words for later on:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的结果是一个字符向量，因此让我们将频繁出现的词保存以供后续使用：
- en: '[PRE33]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A peek into the contents of the vector shows us that there are 1,136 terms
    appearing in at least five SMS messages:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 向量内容的预览显示，我们发现有1,136个术语在至少五条短信中出现：
- en: '[PRE34]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We now need to filter our DTM to include only the terms appearing in a specified
    vector. As done earlier, we''ll use the data frame style `[row, col]` operations
    to request specific portions of the DTM, noting that the columns are named after
    the words the DTM contains. We can take advantage of this to limit the DTM to
    specific words. Since we want all the rows, but only the columns representing
    the words in the `sms_freq_words` vector, our commands are:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要过滤我们的DTM，只保留在指定向量中出现的术语。与之前一样，我们将使用数据框风格的`[row, col]`操作来请求DTM的特定部分，并注意到列名是DTM所包含词汇的名称。我们可以利用这一点将DTM限制为特定的词汇。由于我们希望保留所有行，但仅保留表示`sms_freq_words`向量中词汇的列，因此我们的命令是：
- en: '[PRE35]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The training and test datasets now include 1,136 features, which correspond
    to words appearing in at least five messages.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集现在包含1,136个特征，这些特征对应于在至少五条短信中出现的词汇。
- en: The Naive Bayes classifier is typically trained on data with categorical features.
    This poses a problem, since the cells in the sparse matrix are numeric and measure
    the number of times a word appears in a message. We need to change this to a categorical
    variable that simply indicates yes or no depending on whether the word appears
    at all.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器通常在具有类别特征的数据上进行训练。这就带来了一个问题，因为稀疏矩阵中的单元格是数字型的，表示一个词在消息中出现的次数。我们需要将其转换为一个类别变量，简单地根据该词是否出现来指示“是”或“否”。
- en: 'The following defines a `convert_counts()` function to convert counts to `Yes`/`No`
    strings:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定义了一个`convert_counts()`函数，将计数转换为`Yes`/`No`字符串：
- en: '[PRE36]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: By now, some of the pieces of the preceding function should look familiar. The
    first line defines the function. The `ifelse(x > 0, "Yes", "No")` statement transforms
    the values in `x`, so that if the value is greater than `0`, then it will be replaced
    by `"Yes"`, otherwise it will be replaced by a `"No"` string. Lastly, the newly
    transformed `x` vector is returned.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，前面函数的一些部分应该看起来很熟悉。第一行定义了函数。`ifelse(x > 0, "Yes", "No")`语句将转换`x`中的值，使得如果值大于0，它将被替换为`"Yes"`，否则将被替换为`"No"`字符串。最后，返回新转换的`x`向量。
- en: We now need to apply `convert_counts()` to each of the columns in our sparse
    matrix. You may be able to guess the R function to do exactly this. The function
    is simply called `apply()` and is used much like `lapply()` was used previously.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要对稀疏矩阵中的每一列应用`convert_counts()`。你或许能猜到完成这个操作的R函数是什么。这个函数就是`apply()`，它的使用方式与之前的`lapply()`类似。
- en: 'The `apply()` function allows a function to be used on each of the rows or
    columns in a matrix. It uses a `MARGIN` parameter to specify either rows or columns.
    Here, we''ll use `MARGIN = 2`, since we''re interested in the columns (`MARGIN
    = 1` is used for rows). The commands to convert the training and test matrices
    are as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply()`函数允许在矩阵的每一行或每一列上使用一个函数。它使用一个`MARGIN`参数来指定行或列。这里我们将使用`MARGIN = 2`，因为我们关心的是列（`MARGIN
    = 1`用于行）。用于转换训练矩阵和测试矩阵的命令如下：'
- en: '[PRE37]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The result will be two character type matrixes, each with cells indicating `"Yes"`
    or `"No"` for whether the word represented by the column appears at any point
    in the message represented by the row.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是两个字符类型的矩阵，每个单元格表示列所代表的单词是否出现在行所代表的消息中，显示为“是”或“否”。
- en: Step 3 – training a model on the data
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 – 在数据上训练模型
- en: Now that we have transformed the raw SMS messages into a format that can be
    represented by a statistical model, it is time to apply the Naive Bayes algorithm.
    The algorithm will use the presence or absence of words to estimate the probability
    that a given SMS message is spam.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将原始的短信消息转化为可以通过统计模型表示的格式，是时候应用朴素贝叶斯算法了。该算法将利用单词的出现与否来估算给定短信是垃圾邮件的概率。
- en: The Naive Bayes implementation we will employ is in the `e1071` package. This
    package was developed in the statistics department of the Vienna University of
    Technology (TU Wien), and includes a variety of functions for machine learning.
    If you have not done so already, be sure to install and load the package using
    the `install.packages("e1071")` and `library(e1071)` commands before continuing.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的朴素贝叶斯实现位于`e1071`包中。这个包是由维也纳工业大学（TU Wien）统计学系开发的，包含了多种机器学习函数。如果你还没有安装该包，请确保在继续之前使用`install.packages("e1071")`和`library(e1071)`命令安装并加载该包。
- en: Tip
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Many machine learning approaches are implemented in more than one R package,
    and Naive Bayes is no exception. One other option is `NaiveBayes()` in the `klaR`
    package, which is nearly identical to the one in the `e1071` package. Feel free
    to use whichever option you prefer.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习方法在多个R包中都有实现，朴素贝叶斯也不例外。另一个选择是`klaR`包中的`NaiveBayes()`函数，它与`e1071`包中的函数几乎相同。你可以随意选择你喜欢的选项。
- en: 'Unlike the k-NN algorithm we used for classification in the previous chapter,
    a Naive Bayes learner is trained and used for classification in separate stages.
    Still, as shown in the following table, these steps are is fairly straightforward:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章中使用的k-NN算法不同，朴素贝叶斯学习器是分阶段进行训练和分类的。尽管如此，正如下表所示，这些步骤相对简单：
- en: '![Step 3 – training a model on the data](img/B03905_04_18.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 3 – 在数据上训练模型](img/B03905_04_18.jpg)'
- en: 'To build our model on the `sms_train` matrix, we''ll use the following command:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在`sms_train`矩阵上构建我们的模型，我们将使用以下命令：
- en: '[PRE38]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `sms_classifier` object now contains a `naiveBayes` classifier object that
    can be used to make predictions.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`sms_classifier`对象现在包含一个`naiveBayes`分类器对象，可以用来进行预测。'
- en: Step 4 – evaluating model performance
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 4 – 评估模型性能
- en: To evaluate the SMS classifier, we need to test its predictions on unseen messages
    in the test data. Recall that the unseen message features are stored in a matrix
    named `sms_test`, while the class labels (spam or ham) are stored in a vector
    named `sms_test_labels`. The classifier that we trained has been named `sms_classifier`.
    We will use this classifier to generate predictions and then compare the predicted
    values to the true values.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估短信分类器，我们需要在测试数据中的未见过的消息上测试其预测结果。回想一下，未见消息的特征存储在一个名为`sms_test`的矩阵中，而类标签（垃圾邮件或正常邮件）存储在名为`sms_test_labels`的向量中。我们训练过的分类器被命名为`sms_classifier`。我们将使用该分类器来生成预测，然后将预测值与真实值进行比较。
- en: 'The `predict()` function is used to make the predictions. We will store these
    in a vector named `sms_test_pred`. We will simply supply the function with the
    names of our classifier and test dataset, as shown:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`函数用于进行预测。我们将把这些预测存储在一个名为`sms_test_pred`的向量中。我们只需向函数提供分类器和测试数据集的名称，如下所示：'
- en: '[PRE39]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To compare the predictions to the true values, we''ll use the `CrossTable()`
    function in the `gmodels` package, which we used previously. This time, we''ll
    add some additional parameters to eliminate unnecessary cell proportions and use
    the `dnn` parameter (dimension names) to relabel the rows and columns, as shown
    in the following code:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将预测与真实值进行比较，我们将使用`gmodels`包中的`CrossTable()`函数，这是我们之前使用过的。这次，我们将添加一些附加参数以消除不必要的单元格比例，并使用`dnn`参数（维度名称）重新标记行和列，如下代码所示：
- en: '[PRE40]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This produces the following table:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下表格：
- en: '![Step 4 – evaluating model performance](img/B03905_04_19.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 4 – 评估模型性能](img/B03905_04_19.jpg)'
- en: Looking at the table, we can see that a total of only *6 + 30 = 36* of the 1,390
    SMS messages were incorrectly classified (2.6 percent). Among the errors were
    6 out of 1,207 ham messages that were misidentified as spam, and 30 of the 183
    spam messages were incorrectly labeled as ham. Considering the little effort we
    put into the project, this level of performance seems quite impressive. This case
    study exemplifies the reason why Naive Bayes is the standard for text classification;
    directly out of the box, it performs surprisingly well.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 看着表格，我们可以看到总共有*6 + 30 = 36*条短信被错误分类（占2.6％）。这些错误包括1,207条正常短信中有6条被误分类为垃圾邮件，以及183条垃圾邮件中有30条被错误标记为正常邮件。考虑到我们在项目中投入的精力不多，这个性能水平似乎相当令人印象深刻。这个案例研究展示了朴素贝叶斯成为文本分类标准的原因；直接使用时，它的表现出乎意料地好。
- en: On the other hand, the six legitimate messages that were incorrectly classified
    as spam could cause significant problems for the deployment of our filtering algorithm,
    because the filter could cause a person to miss an important text message. We
    should investigate to see whether we can slightly tweak the model to achieve better
    performance.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，那些被错误分类为垃圾邮件的六条合法信息可能会对我们的过滤算法部署造成重大问题，因为过滤器可能会导致某个人错过一条重要的短信。我们应该调查一下是否可以稍微调整模型以提高性能。
- en: Step 5 – improving model performance
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 5 – 改进模型性能
- en: You may have noticed that we didn't set a value for the Laplace estimator while
    training our model. This allows words that appeared in zero spam or zero ham messages
    to have an indisputable say in the classification process. Just because the word
    "ringtone" only appeared in the spam messages in the training data, it does not
    mean that every message with this word should be classified as spam.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在训练模型时，我们没有为拉普拉斯估计器设置值。这使得在零垃圾邮件或零正常邮件中出现的词汇在分类过程中拥有不可争辩的发言权。仅仅因为“铃声”这个词只出现在训练数据的垃圾邮件中，并不意味着包含该词的每条信息都应该被分类为垃圾邮件。
- en: 'We''ll build a Naive Bayes model as done earlier, but this time set `laplace
    = 1`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像之前一样构建一个朴素贝叶斯模型，但这次设置`laplace = 1`：
- en: '[PRE41]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we''ll make predictions:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行预测：
- en: '[PRE42]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we''ll compare the predicted classes to the actual classifications
    using a cross tabulation:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过交叉表来比较预测类别和实际分类：
- en: '[PRE43]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This results in the following table:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下表格：
- en: '![Step 5 – improving model performance](img/B03905_04_20.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![步骤 5 – 改进模型性能](img/B03905_04_20.jpg)'
- en: Adding the Laplace estimator reduced the number of false positives (ham messages
    erroneously classified as spam) from six to five and the number of false negatives
    from 30 to 28\. Although this seems like a small change, it's substantial considering
    that the model's accuracy was already quite impressive. We'd need to be careful
    before tweaking the model too much in order to maintain the balance between being
    overly aggressive and overly passive while filtering spam. Users would prefer
    that a small number of spam messages slip through the filter than an alternative
    in which ham messages are filtered too aggressively.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 添加拉普拉斯估计器将误报（正常邮件错误地分类为垃圾邮件）的数量从六个减少到了五个，将漏报的数量从30个减少到了28个。虽然这看起来是一个小变化，但考虑到模型的准确性已经相当令人印象深刻，这个变化还是相当显著的。在过度调整模型之前，我们需要小心，以确保在过滤垃圾邮件时保持过于激进和过于被动之间的平衡。用户更愿意接受少数垃圾邮件漏过过滤器，而不是正常邮件被过度过滤的情况。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about classification using Naive Bayes. This algorithm
    constructs tables of probabilities that are used to estimate the likelihood that
    new examples belong to various classes. The probabilities are calculated using
    a formula known as Bayes' theorem, which specifies how dependent events are related.
    Although Bayes' theorem can be computationally expensive, a simplified version
    that makes so-called "naive" assumptions about the independence of features is
    capable of handling extremely large datasets.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了使用朴素贝叶斯进行分类。这种算法构建了概率表，用于估算新样本属于不同类别的可能性。这些概率通过一个被称为贝叶斯定理的公式来计算，该定理说明了事件之间的依赖关系。尽管贝叶斯定理计算开销较大，但其简化版在做出所谓的“朴素”假设，即特征独立性假设的基础上，能够处理极其庞大的数据集。
- en: The Naive Bayes classifier is often used for text classification. To illustrate
    its effectiveness, we employed Naive Bayes on a classification task involving
    spam SMS messages. Preparing the text data for analysis required the use of specialized
    R packages for text processing and visualization. Ultimately, the model was able
    to classify over 97 percent of all the SMS messages correctly as spam or ham.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器通常用于文本分类。为了说明其有效性，我们将朴素贝叶斯应用于一个涉及垃圾短信分类的任务。在进行文本数据分析时，我们使用了专门的R包进行文本处理和可视化。最终，该模型能够正确分类超过97%的短信，准确地识别出垃圾短信和正常短信。
- en: In the next chapter, we will examine two more machine learning methods. Each
    performs classification by partitioning data into groups of similar values.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍两种机器学习方法。每种方法通过将数据分割成相似值的组来执行分类。
