- en: Chapter 3. Working with Spark and MLlib
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用Spark和MLlib
- en: Now that we are powered with the knowledge of where and how statistics and machine
    learning fits in the global data-driven enterprise architecture, let's stop at
    the specific implementations in Spark and MLlib, a machine learning library on
    top of Spark. Spark is a relatively new member of the big data ecosystem that
    is optimized for memory usage as opposed to disk. The data can be still spilled
    to disk as necessary, but Spark does the spill only if instructed to do so explicitly,
    or if the active dataset does not fit into the memory. Spark stores lineage information
    to recompute the active dataset if a node goes down or the information is erased
    from memory for some other reason. This is in contrast to the traditional MapReduce
    approach, where the data is persisted to the disk after each map or reduce task.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了统计和机器学习在全球数据驱动企业架构中的位置和如何应用，让我们专注于Spark和MLlib的具体实现，MLlib是Spark之上的机器学习库。Spark是大数据生态系统中的相对较新成员，它优化了内存使用而不是磁盘。数据仍然可以在必要时溢出到磁盘，但Spark只有在被明确指令或活动数据集不适合内存时才会执行溢出。Spark存储
    lineage 信息，以便在节点故障或由于其他原因信息从内存中删除时重新计算活动数据集。这与传统的MapReduce方法形成对比，在每次map或reduce任务之后，数据都会持久化到磁盘。
- en: Spark is particularly suited for iterative or statistical machine learning algorithms
    over a distributed set of nodes and can scale out of core. The only limitation
    is the total memory and disk space available across all Spark nodes and the network
    speed. I will cover the basics of Spark architecture and implementation in this
    chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spark特别适合在分布式节点集上执行迭代或统计机器学习算法，并且可以扩展到核心之外。唯一的限制是所有Spark节点可用的总内存和磁盘空间以及网络速度。我将在本章中介绍Spark架构和实现的基础知识。
- en: One can direct Spark to execute data pipelines either on a single node or across
    a set of nodes with a simple change in the configuration parameters. Of course,
    this flexibility comes at a cost of slightly heavier framework and longer setup
    times, but the framework is very parallelizable and as most of modern laptops
    are already multithreaded and sufficiently powerful, this usually does not present
    a big issue.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过简单地更改配置参数来指导Spark在单个节点或一组节点上执行数据管道。当然，这种灵活性是以稍微重一些的框架和更长的设置时间为代价的，但框架非常易于并行化，并且由于大多数现代笔记本电脑已经是多线程且足够强大，这通常不会成为一个大问题。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing and configuring Spark if you haven't done so yet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请安装和配置Spark
- en: Learning the basics of Spark architecture and why it is inherently tied to the
    Scala language
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Spark架构的基础以及为什么它与Scala语言天生紧密相连
- en: Learning why Spark is the next technology after sequential implementations and
    Hadoop MapReduce
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习为什么Spark是继顺序实现和Hadoop MapReduce之后的下一代技术
- en: Learning about Spark components
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Spark组件
- en: Looking at the simple implementation of word count in Scala and Spark
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Scala和Spark中单词计数的简单实现
- en: Looking at the streaming word count implementation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看流式单词计数实现
- en: Seeing how to create Spark DataFrames from either a distributed file or a distributed
    database
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看如何从分布式文件或分布式数据库创建Spark DataFrames
- en: Learning about Spark performance tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Spark性能调优
- en: Setting up Spark
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Spark
- en: 'If you haven''t done so yet, you can download the pre-build Spark package from
    [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    The latest release at the time of writing is **1.6.1**:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，您可以从[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)下载预构建的Spark包。写作时的最新版本是**1.6.1**：
- en: '![Setting up Spark](img/B04935_03_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![设置Spark](img/B04935_03_01.jpg)'
- en: Figure 03-1\. The download site at http://spark.apache.org with recommended
    selections for this chapter
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图03-1\. http://spark.apache.org上的下载站点，以及本章推荐的选择
- en: 'Alternatively, you can build the Spark by downloading the full source distribution
    from [https://github.com/apache/spark](https://github.com/apache/spark):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过从[https://github.com/apache/spark](https://github.com/apache/spark)下载完整的源代码分布来构建Spark：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The command will download the necessary dependencies and create the `spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`
    file in the Spark directory; the version is 2.0.0, as it is the next release version
    at the time of writing. In general, you do not want to build from trunk unless
    you are interested in the latest features. If you want a released version, you
    can checkout the corresponding tag. Full list of available versions is available
    via the `git branch –r` command. The `spark*.tgz` file is all you need to run
    Spark on any machine that has Java JRE.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将下载必要的依赖项，并在Spark目录中创建`spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`文件；版本号为2.0.0，因为它是在撰写本文时的下一个发布版本。通常情况下，除非你对最新功能感兴趣，否则你不想从主干分支构建。如果你想获取发布版本，可以检出相应的标签。可通过`git
    branch –r`命令查看可用的完整版本列表。`spark*.tgz`文件是你在任何安装了Java JRE的机器上运行Spark所需的所有内容。
- en: The distribution comes with the `docs/building-spark.md` document that describes
    other options for building Spark and their descriptions, including incremental
    Scala compiler, zinc. Full Scala 2.11 support is in the works for the next Spark
    2.0.0 release.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 伴随`docs/building-spark.md`文档的分布，该文档描述了构建Spark的其他选项及其描述，包括增量Scala编译器zinc。下一个Spark
    2.0.0版本的发布正在努力实现对Scala 2.11的全支持。
- en: Understanding Spark architecture
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark架构
- en: A parallel execution involves splitting the workload into subtasks that are
    executed in different threads or on different nodes. Let's see how Spark does
    this and how it manages execution and communication between the subtasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行涉及将工作负载拆分为在不同线程或不同节点上执行的子任务。让我们看看Spark是如何做到这一点的，以及它是如何管理子任务之间的执行和通信的。
- en: Task scheduling
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务调度
- en: 'Spark workload splitting is determined by the number of partitions for **Resilient
    Distributed Dataset** (**RDD**), the basic abstraction in Spark, and the pipeline
    structure. An RDD represents an immutable, partitioned collection of elements
    that can be operated on in parallel. While the specifics might depend on the mode
    in which Spark runs, the following diagram captures the Spark task/resource scheduling:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Spark工作负载拆分由Spark的基本抽象**弹性分布式数据集**（**RDD**）的分区数量以及管道结构决定。RDD表示一个不可变、分区的元素集合，可以在并行操作上执行。虽然具体细节可能取决于Spark运行的模式，以下图表捕捉了Spark任务/资源调度的过程：
- en: '![Task scheduling](img/B04935_03_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![任务调度](img/B04935_03_02.jpg)'
- en: Figure 03-2\. A generic Spark task scheduling diagram. While not shown explicitly
    in the figure, Spark Context opens an HTTP UI, usually on port 4040 (the concurrent
    contexts will open 4041, 4042, and so on), which is present during a task execution.
    Spark Master UI is usually 8080 (although it is changed to 18080 in CDH) and Worker
    UI is usually 7078\. Each node can run multiple executors, and each executor can
    run multiple tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3-2。一个通用的Spark任务调度图。虽然图示中没有明确显示，但Spark Context会打开一个HTTP UI，通常在端口4040（并发上下文将打开4041、4042等），在任务执行期间存在。Spark
    Master UI通常为8080（尽管在CDH中已更改为18080），Worker UI通常为7078。每个节点可以运行多个执行器，每个执行器可以运行多个任务。
- en: Tip
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You will find that Spark, as well as Hadoop, has a lot of parameters. Some of
    them are specified as environment variables (refer to the `$SPARK_HOME/conf/spark-env.sh`
    file), and yet some can be given as a command-line parameter. Moreover, some files
    with pre-defined names can contain parameters that will change the Spark behavior,
    such as `core-site.xml`. This might be confusing, and I will cover as much as
    possible in this and the following chapters. If you are working with **Hadoop
    Distributed File System** (**HDFS**), then the `core-site.xml` and `hdfs-site.xml`
    files will contain the pointer and specifications for the HDFS master. The requirement
    for picking this file is that it has to be on `CLASSPATH` Java process, which,
    again, may be set by either specifying `HADOOP_CONF_DIR` or `SPARK_CLASSPATH`
    environment variables. As is usual with open source, you need to grep the code
    sometimes to understand how various parameters work, so having a copy of the source
    tree on your laptop is a good idea.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现Spark，以及Hadoop，有很多参数。其中一些作为环境变量指定（参考`$SPARK_HOME/conf/spark-env.sh`文件），还有一些可以作为命令行参数提供。此外，一些具有预定义名称的文件可以包含将改变Spark行为的参数，例如`core-site.xml`。这可能会让人困惑，我将在本章和下一章尽可能多地涵盖这些内容。如果您正在使用**Hadoop分布式文件系统**（**HDFS**），那么`core-site.xml`和`hdfs-site.xml`文件将包含HDFS主节点的指针和规范。选择此文件的要求是它必须位于`CLASSPATH`
    Java进程中，这可以通过指定`HADOOP_CONF_DIR`或`SPARK_CLASSPATH`环境变量来设置。与开源项目一样，有时您需要grep代码来理解各种参数的工作方式，因此，在您的笔记本电脑上保留源代码树副本是个好主意。
- en: Each node in the cluster can run one or more executors, and each executor can
    schedule a sequence of tasks to perform the Spark operations. Spark driver is
    responsible for scheduling the execution and works with the cluster scheduler,
    such as Mesos or YARN to schedule the available resources. Spark driver usually
    runs on the client machine, but in the latest release, it can also run in the
    cluster under the cluster manager. YARN and Mesos have the ability to dynamically
    manage the number of executors that run concurrently on each node, provided the
    resource constraints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的每个节点可以运行一个或多个执行器，每个执行器可以调度一系列任务以执行Spark操作。Spark驱动程序负责调度执行，并与集群调度器（如Mesos或YARN）协同工作以调度可用资源。Spark驱动程序通常在客户端机器上运行，但在最新版本中，它也可以在集群管理器下运行。YARN和Mesos具有动态管理每个节点上并发运行的执行器数量的能力，前提是资源约束允许。
- en: 'In the Standalone mode, **Spark Master** does the work of the cluster scheduler—it
    might be less efficient in allocating resources, but it''s better than nothing
    in the absence of preconfigured Mesos or YARN. Spark standard distribution contains
    shell scripts to start Spark in Standalone mode in the `sbin` directory. Spark
    Master and driver communicate directly with one or several Spark workers that
    run on individual nodes. Once the master is running, you can start Spark shell
    with the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下，**Spark Master** 执行集群调度器的工作——在资源分配方面可能不太高效，但在没有预配置的Mesos或YARN的情况下，它总比没有好。Spark标准发行版在`sbin`目录中包含用于在独立模式下启动Spark的shell脚本。Spark
    Master和驱动程序直接与运行在各个节点上的一个或多个Spark工作节点通信。一旦Master启动，您可以使用以下命令启动Spark shell：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that you can always run Spark in local mode, which means that all tasks
    will be executed in a single JVM, by specifying `--master local[2]`, where `2`
    is the number of threads that have to be at least `2`. In fact, we will be using
    the local mode very often in this book for running small examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您始终可以在本地模式下运行Spark，这意味着所有任务都将在一个单独的JVM中执行，通过指定`--master local[2]`，其中`2`是必须至少为`2`的线程数。实际上，我们将在本书中非常频繁地使用本地模式来运行小型示例。
- en: 'Spark shell is an application from the Spark point of view. Once you start
    a Spark application, you will see it under **Running Applications** in the Spark
    Master UI (or in the corresponding cluster manager), which can redirect you to
    the Spark application HTTP UI at port 4040, where one can see the subtask execution
    timeline and other important properties such as environment setting, classpath,
    parameters passed to the JVM, and information on resource usage (refer to *Figure
    3-3*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Spark shell是从Spark的角度来看的一个应用程序。一旦您启动一个Spark应用程序，您将在Spark Master UI（或相应的集群管理器）中的**运行中的应用程序**下看到它，它可以将您重定向到Spark应用程序HTTP
    UI（在4040端口），在那里可以看到子任务执行时间线和其他重要属性，如环境设置、类路径、传递给JVM的参数以及资源使用信息（参见图3-3）：
- en: '![Task scheduling](img/B04935_03_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![任务调度](img/B04935_03_03.jpg)'
- en: Figure 03-3\. Spark Driver UI in Standalone mode with time decomposition
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-3\. 独立模式 Spark 驱动程序 UI 中的时间分解
- en: 'As we saw, with Spark, one can easily switch between local and cluster mode
    by providing the `--master` command-line option, setting a `MASTER` environment
    variable, or modifying `spark-defaults.conf`, which should be on the classpath
    during the execution, or even set explicitly using the `setters` method on the
    `SparkConf` object directly in Scala, which will be covered later:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用 Spark，可以通过提供 `--master` 命令行选项、设置 `MASTER` 环境变量或修改 `spark-defaults.conf`（应在执行期间位于类路径上）来轻松地在本地和集群模式之间切换，或者甚至可以直接在
    Scala 中使用 `SparkConf` 对象的 `setters` 方法显式设置，这将在后面介绍：
- en: '| Cluster Manager | MASTER env variable | Comments |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 集群管理器 | MASTER 环境变量 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Local (single node, multiple threads) | `local[n]` | *n* is the number of
    threads to use, should be greater than or equal to *2*. If you want Spark to communicate
    with other Hadoop tools such as Hive, you still need to point it to the cluster
    by either setting the `HADOOP_CONF_DIR` environment variable or copying the Hadoop
    `*-site.xml` configuration files into the `conf` subdirectory. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 本地（单节点，多线程） | `local[n]` | *n* 是要使用的线程数，应大于或等于 *2*。如果您想 Spark 与其他 Hadoop
    工具（如 Hive）通信，您仍然需要通过设置 `HADOOP_CONF_DIR` 环境变量或将 Hadoop `*-site.xml` 配置文件复制到 `conf`
    子目录中来将其指向集群。 |'
- en: '| Standalone (Daemons running on the nodes) | `spark:// master-address>:7077`
    | This has a set of start/stop scripts in the `$SPARK_HOME/sbin` directory. This
    also supports the HA mode. More details can be found at [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html).
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 独立（在节点上运行的守护进程） | `spark:// master-address>:7077` | 此版本在 `$SPARK_HOME/sbin`
    目录中有一组启动/停止脚本。这也支持高可用性模式。更多详细信息请参阅 [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html)。
    |'
- en: '| Mesos | `mesos://host:5050` or `mesos://zk://host:2181`(multimaster) | Here,
    you need to set `MESOS_NATIVE_JAVA_LIBRARY=<path to libmesos.so>` and `SPARK_EXECUTOR_URI=<URL
    of spark-1.5.0.tar.gz>`. The default is fine-grained mode, where each Spark task
    runs as a separate Mesos task. Alternatively, the user can specify the coarse-grained
    mode, where the Mesos tasks persists for the duration of the application. The
    advantage is lower total start-up costs. This can use dynamic allocation (refer
    to the following URL) in coarse-grained mode. More details can be found at [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html).
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Mesos | `mesos://host:5050` 或 `mesos://zk://host:2181`(多主) | 在这里，您需要设置 `MESOS_NATIVE_JAVA_LIBRARY=<libmesos.so
    路径>` 和 `SPARK_EXECUTOR_URI=<spark-1.5.0.tar.gz URL>`。默认为细粒度模式，其中每个 Spark 任务作为一个单独的
    Mesos 任务运行。用户还可以指定粗粒度模式，其中 Mesos 任务在应用程序运行期间持续存在。优点是总启动成本较低。在粗粒度模式下可以使用动态分配（请参阅以下
    URL）。更多详细信息请参阅 [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html)。
    |'
- en: '| YARN | `yarn` | Spark driver can run either in the cluster or on the client
    node, which is managed by the `--deploy-mode` parameter (cluster or client, shell
    can only run in the client mode). Set `HADOOP_CONF_DIR` or `YARN_CONF_DIR` to
    point to the YARN config files. Use the `--num-executors` flag or `spark.executor.instances`
    property to set a fixed number of executors (default).Set `spark.dynamicAllocation.enabled`
    to `true` to dynamically create/kill executors depending on the application demand.
    More details are available at [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html).
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| YARN | `yarn` | Spark 驱动程序可以在集群中运行，也可以在客户端节点上运行，客户端节点由 `--deploy-mode` 参数（集群或客户端，shell
    只能在客户端模式下运行）管理。将 `HADOOP_CONF_DIR` 或 `YARN_CONF_DIR` 设置为指向 YARN 配置文件的路径。使用 `--num-executors`
    标志或 `spark.executor.instances` 属性设置固定数量的执行器（默认）。将 `spark.dynamicAllocation.enabled`
    设置为 `true` 以根据应用程序需求动态创建/销毁执行器。更多详细信息请参阅 [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html)。
    |'
- en: 'The most common ports are 8080, the master UI, and 4040, the application UI.
    Other Spark ports are summarized in the following table:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的端口是 8080（主 UI）和 4040（应用程序 UI）。其他 Spark 端口总结在下表中：
- en: '| Standalone ports |   |   |   |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 独立端口 |   |   |   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| From | To | Default Port | Purpose | Configuration Setting |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 从 | 到 | 默认端口 | 目的 | 配置设置 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Browser | Standalone Master | 8080 | Web UI | `spark.master.ui.port /SPARK_MASTER_WEBUI_PORT`
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 独立主节点 | 8080 | Web UI | `spark.master.ui.port /SPARK_MASTER_WEBUI_PORT`
    |'
- en: '| Browser | Standalone worker | 8081 | Web UI | `spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT`
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 独立工作节点 | 8081 | Web UI | `spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT`
    |'
- en: '| Driver / Standalone worker | Standalone Master | 7077 | Submit job to cluster
    / Join cluster | `SPARK_MASTER_PORT` |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 驱动器/独立工作节点 | 独立主节点 | 7077 | 提交作业到集群 / 加入集群 | `SPARK_MASTER_PORT` |'
- en: '| Standalone master | Standalone worker | (random) | Schedule executors | `SPARK_WORKER_PORT`
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 独立主节点 | 独立工作节点 | (随机) | 调度执行器 | `SPARK_WORKER_PORT` |'
- en: '| Executor / Standalone master | Driver | (random) | Connect to application
    / Notify executor state changes | `spark.driver.port` |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 执行器/独立主节点 | 驱动器 | (随机) | 连接到应用程序 / 通知执行器状态变化 | `spark.driver.port` |'
- en: '| **Other ports** |   |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **其他端口** |   |'
- en: '| **From** | **To** | **Default Port** | **Purpose** | **Configuration Setting**
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **从** | **到** | **默认端口** | **用途** | **配置设置** |'
- en: '| Browser | Application | 4040 | Web UI | `spark.ui.port` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 应用程序 | 4040 | Web UI | `spark.ui.port` |'
- en: '| Browser | History server | 18080 | Web UI | `spark.history.ui.port` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 历史服务器 | 18080 | Web UI | `spark.history.ui.port` |'
- en: '| Driver | Executor | (random) | Schedule tasks | `spark.executor.port` |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 驱动器 | 执行器 | (随机) | 调度任务 | `spark.executor.port` |'
- en: '| Executor | Driver | (random) | File server for files and jars | `spark.fileserver.port`
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 执行器 | 驱动器 | (随机) | 文件服务器（用于文件和 JAR 包） | `spark.fileserver.port` |'
- en: '| Executor | Driver | (random) | HTTP broadcast | `spark.broadcast.port` |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 执行器 | 驱动器 | (随机) | HTTP 广播 | `spark.broadcast.port` |'
- en: Also, some of the documentation is available with the source distribution in
    the `docs` subdirectory, but may be out of date.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些文档作为源代码的一部分，在 `docs` 子目录中提供，但可能已过时。
- en: Spark components
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 组件
- en: 'Since the emergence of Spark, multiple applications that benefit from Spark''s
    ability to cache RDDs have been written: Shark, Spork (Pig on Spark), graph libraries
    (GraphX, GraphFrames), streaming, MLlib, and so on; some of these will be covered
    here and in later chapters.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Spark 诞生以来，已经编写了多个利用 Spark 缓存 RDD 能力的应用程序：Shark、Spork（Spark 上的 Pig）、图库（GraphX、GraphFrames）、流处理、MLlib
    等；其中一些将在本章和后续章节中介绍。
- en: 'In this section, I will cover major architecture components to collect, store,
    and analyze the data in Spark. While I will cover a more complete data life cycle
    architecture in [Chapter 2](ch02.xhtml "Chapter 2. Data Pipelines and Modeling"),
    *Data Pipelines and Modeling*, here are Spark-specific components:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍 Spark 中用于收集、存储和分析数据的主要架构组件。虽然我将在第 2 章（“数据管道和建模”）中介绍更完整的数据生命周期架构，但以下是
    Spark 特定的组件：
- en: '![Spark components](img/B04935_03_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![Spark组件](img/B04935_03_04.jpg)'
- en: Figure 03-4\. Spark architecture and components.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-4\. Spark 架构和组件。
- en: MQTT, ZeroMQ, Flume, and Kafka
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MQTT、ZeroMQ、Flume 和 Kafka
- en: 'All of these are different ways to reliably move data from one place to another
    without loss and duplication. They usually implement a publish-subscribe model,
    where multiple writers and readers can write and read from the same queues with
    different guarantees. Flume stands out as a first distributed log and event management
    implementation, but it is slowly replaced by Kafka, a fully functional publish-subscribe
    distributed message queue optionally persistent across a distributed set of nodes
    developed at LinkedIn. We covered Flume and Kafka briefly in the previous chapter.
    Flume configuration is file-based and is traditionally used to deliver messages
    from a Flume source to one or several Flume sinks. One of the popular sources
    is `netcat`—listening on raw data over a port. For example, the following configuration
    describes an agent receiving data and then writing them to HDFS every 30 seconds
    (default):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是在不丢失和重复的情况下可靠地将数据从一个地方移动到另一个地方的不同方式。它们通常实现发布-订阅模型，其中多个编写者和读者可以从不同的队列中写入和读取，并具有不同的保证。Flume
    作为第一个分布式日志和事件管理实现脱颖而出，但它正逐渐被 Kafka 所取代，Kafka 是一个功能齐全的发布-订阅分布式消息队列，可选地跨分布式节点集持久化，由
    LinkedIn 开发。我们在上一章中简要介绍了 Flume 和 Kafka。Flume 配置基于文件，传统上用于将消息从 Flume 源发送到 Flume
    沉积器。其中一种流行的源是 `netcat`——监听端口的原始数据。例如，以下配置描述了一个代理每 30 秒（默认）接收数据并将其写入 HDFS 的情况：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This file is included as part of the code provided with this book in the `chapter03/conf`
    directory. Let''s download and start Flume agent (check the MD5 sum with one provided
    at [http://flume.apache.org/download.html](http://flume.apache.org/download.html)):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件作为本书提供的代码的一部分包含在 `chapter03/conf` 目录中。让我们下载并启动 Flume 代理（使用提供的 MD5 校验和检查[http://flume.apache.org/download.html](http://flume.apache.org/download.html)）：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, in a separate window, you can type a `netcat` command to send text to
    the Flume agent:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在单独的窗口中，你可以输入一个 `netcat` 命令将文本发送到 Flume 代理：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Flume agent will first create a `*.tmp` file and then rename it to a file
    without extension (the file extension can be used to filter out files being written
    to):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 代理将首先创建一个 `*.tmp` 文件，然后将其重命名为没有扩展名的文件（文件扩展名可以用来过滤正在写入的文件）：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, each row is a Unix time in milliseconds and data received. In this case,
    we put the data into HDFS, from where they can be analyzed by a Spark/Scala program,
    we can exclude the files being written to by the `*.tmp` filename pattern. However,
    if you are really interested in up-to-the-last-minute values, Spark, as well as
    some other platforms, supports streaming, which I will cover in a few sections.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每一行都是一个以毫秒为单位的 Unix 时间和接收到的数据。在这种情况下，我们将数据放入 HDFS，然后 Spark/Scala 程序可以对其进行分析，我们可以排除以
    `*.tmp` 文件名模式写入的文件。然而，如果你真的对最后一分钟的数据值感兴趣，Spark 以及一些其他平台支持流式处理，我将在接下来的几节中介绍。
- en: HDFS, Cassandra, S3, and Tachyon
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HDFS、Cassandra、S3 和 Tachyon
- en: 'HDFS, Cassandra, S3, and Tachyon are the different ways to get the data into
    persistent storage and compute nodes as necessary with different guarantees. HDFS
    is a distributed storage implemented as a part of Hadoop, which serves as the
    backend for many products in the Hadoop ecosystem. HDFS divides each file into
    blocks, which are 128 MB in size by default, and stores each block on at least
    three nodes. Although HDFS is reliable and supports HA, a general complain about
    HDFS storage is that it is slow, particularly for machine learning purposes. Cassandra
    is a general-purpose key/value storage that also stores multiple copies of a row
    and can be configured to support different levels of consistency to optimize read
    or write speeds. The advantage that Cassandra over HDFS model is that it does
    not have a central master node; the reads and writes are completed based on the
    consensus algorithm. This, however, may sometimes reflect on the Cassandra stability.
    S3 is the Amazon storage: The data is stored off-cluster, which affects the I/O
    speed. Finally, the recently developed Tachyon claims to utilize node''s memory
    to optimize access to working sets across the nodes.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS、Cassandra、S3 和 Tachyon 是将数据存入持久存储和计算节点的方式，根据不同的保证进行必要的操作。HDFS 是作为 Hadoop
    的一部分实现的分布式存储，为 Hadoop 生态系统中的许多产品提供后端服务。HDFS 将每个文件划分为块，默认大小为 128 MB，并将每个块存储在至少三个节点上。尽管
    HDFS 是可靠的，并支持高可用性，但关于 HDFS 存储的一般抱怨是它速度较慢，尤其是在机器学习方面。Cassandra 是一种通用键/值存储，也存储行的多个副本，并可以配置为支持不同的一致性级别以优化读写速度。Cassandra
    相比于 HDFS 模型的优势在于它没有中央主节点；读写操作基于共识算法完成。然而，这有时可能会反映在 Cassandra 的稳定性上。S3 是亚马逊存储：数据存储在集群之外，这影响了
    I/O 速度。最后，最近开发的 Tachyon 声称利用节点的内存来优化节点间工作集的访问。
- en: Additionally, new backends are being constantly developed, for example, Kudu
    from Cloudera ([http://getkudu.io/kudu.pdf](http://getkudu.io/kudu.pdf)) and **Ignite
    File System** (**IGFS**) from GridGain ([http://apacheignite.gridgain.org/v1.0/docs/igfs)](http://apacheignite.gridgain.org/v1.0/docs/igfs).
    Both are open source and Apache-licensed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，新的后端正在不断开发中，例如 Cloudera 的 Kudu ([http://getkudu.io/kudu.pdf](http://getkudu.io/kudu.pdf))
    和 GridGain 的 **Ignite 文件系统** (**IGFS**) ([http://apacheignite.gridgain.org/v1.0/docs/igfs)](http://apacheignite.gridgain.org/v1.0/docs/igfs))。两者都是开源的，并拥有
    Apache 许可证。
- en: Mesos, YARN, and Standalone
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mesos、YARN 和 Standalone
- en: 'As we mentioned before, Spark can run under different cluster resource schedulers.
    These are various implementations to schedule Spark''s containers and tasks on
    the cluster. The schedulers can be viewed as cluster kernels, performing functions
    similar to the operating system kernel: resource allocation, scheduling, I/O optimization,
    application services, and UI.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Spark 可以在不同的集群资源调度器下运行。这些是用于在集群上调度 Spark 容器和任务的多种实现。调度器可以被视为集群内核，执行类似于操作系统内核的功能：资源分配、调度、I/O
    优化、应用程序服务和用户界面。
- en: Mesos is one of the original cluster managers and is built using the same principles
    as the Linux kernel, only at a different level of abstraction. A Mesos slave runs
    on every machine and provides API's for resource management and scheduling across
    entire datacenter and cloud environments. Mesos is written in C++.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos 是最早的集群管理器之一，它使用与 Linux 内核相同的原理构建，只是在不同的抽象级别上。Mesos 从节点在每个机器上运行，并提供跨整个数据中心和云环境的资源管理和调度
    API。Mesos 使用 C++ 编写。
- en: YARN is a more recent cluster manager developed by Yahoo. Each node in YARN
    runs a **Node Manager**, which communicates with the **Resource Manager** which
    may run on a separate node. The resource manager schedules the task to satisfy
    memory and CPU constraints. The Spark driver itself can run either in the cluster,
    which is called the cluster mode for YARN. Otherwise, in the client mode, only
    Spark executors run in the cluster and the driver that schedules Spark pipelines
    runs on the same machine that runs Spark shell or submit program. The Spark executors
    will talk to the local host over a random open port in this case. YARN is written
    in Java with the consequences of unpredictable GC pauses, which might make latency's
    long tail fatter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是由Yahoo开发的一个较新的集群管理器。YARN中的每个节点运行一个**Node Manager**，它与可能运行在单独节点上的**资源管理器**进行通信。资源管理器调度任务以满足内存和CPU限制。Spark驱动器本身可以在集群中运行，这被称为YARN的集群模式。否则，在客户端模式下，只有Spark执行器在集群中运行，而调度Spark管道的驱动器运行在运行Spark
    shell或提交程序的同一台机器上。在这种情况下，Spark执行器将通过一个随机开放的端口与本地主机通信。YARN是用Java编写的，其后果是GC暂停不可预测，这可能会使延迟的长尾更宽。
- en: Finally, if none of these resource schedulers are available, the standalone
    deployment mode starts a `org.apache.spark.deploy.worker.Worker` process on each
    node that communicates with the Spark Master process run as `org.apache.spark.deploy.master.Master`.
    The worker process is completely managed by the master and can run multiple executors
    and tasks (refer to *Figure 3-2*).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果这些资源调度器都不可用，独立部署模式将在每个节点上启动一个`org.apache.spark.deploy.worker.Worker`进程，该进程与作为`org.apache.spark.deploy.master.Master`运行的Spark
    Master进程进行通信。工作进程完全由主进程管理，可以运行多个执行器和任务（参见图3-2）。
- en: In practical implementations, it is advised to track the program parallelism
    and required resources through driver's UI and adjust the parallelism and available
    memory, increasing the parallelism if necessary. In the following section, we
    will start looking at how Scala and Scala in Spark address different problems.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中，建议通过驱动器的UI跟踪程序的并行度和所需资源，并调整并行度和可用内存，如果需要的话增加并行度。在下一节中，我们将开始探讨Scala和Spark中的Scala如何解决不同的问题。
- en: Applications
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序
- en: Let's consider a few practical examples and libraries in Spark/Scala starting
    with a very traditional problem of word counting.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些Spark/Scala中的实际例子和库，从一个非常传统的单词计数问题开始。
- en: Word count
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词计数
- en: Most modern machine learning algorithms require multiple passes over data. If
    the data fits in the memory of a single machine, the data is readily available
    and this does not present a performance bottleneck. However, if the data becomes
    too large to fit into RAM, one has a choice of either dumping pieces of the data
    on disk (or database), which is about 100 times slower, but has a much larger
    capacity, or splitting the dataset between multiple machines across the network
    and transferring the results. While there are still ongoing debates, for most
    practical systems, analysis shows that storing the data over a set of network
    connected nodes has a slight advantage over repeatedly storing and reading it
    from hard disks on a single node, particularly if we can split the workload effectively
    between multiple CPUs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代机器学习算法需要对数据进行多次遍历。如果数据可以适应单台机器的内存，数据就可以随时可用，这不会成为性能瓶颈。然而，如果数据变得太大而无法适应RAM，可以选择将数据的一部分（或数据库）写入磁盘，这大约慢100倍，但容量要大得多，或者在网络中的多台机器之间分割数据集并传输结果。尽管仍有持续的争论，但对于大多数实际系统，分析表明，在一系列网络连接的节点上存储数据，与在单个节点上反复从硬盘存储和读取数据相比，略有优势，尤其是如果我们能够有效地在多个CPU之间分配工作负载。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: An average disk has bandwidth of about 100 MB/sec and transfers with a few mms
    latency, depending on the rotation speed and caching. This is about 100 times
    slower than reading the data from memory, depending on the data size and caching
    implementation again. Modern data bus can transfer data at over 10 GB/sec. While
    the network speed still lags behind the direct memory access, particularly with
    standard TCP/IP kernel networking layer overhead, specialized hardware can reach
    tens of GB/sec and if run in parallel, it can be potentially as fast as reading
    from the memory. In practice, the network-transfer speeds are somewhere between
    1 to 10 GB/sec, but still faster than the disk in most practical systems. Thus,
    we can potentially fit the data into combined memory of all the cluster nodes
    and perform iterative machine learning algorithms across a system of them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 平均磁盘的带宽约为100 MB/sec，传输延迟为几毫秒，这取决于旋转速度和缓存。这比从内存中读取数据慢大约100倍，具体取决于数据大小和缓存实现。现代数据总线可以以超过10
    GB/sec的速度传输数据。虽然网络速度仍然落后于直接内存访问，尤其是在标准TCP/IP内核网络层开销的情况下，但专用硬件可以达到数十GB/sec，如果并行运行，其速度可能接近从内存中读取。实际上，网络传输速度在1到10
    GB/sec之间，但在大多数实际系统中仍然比磁盘快。因此，我们有可能将数据放入所有集群节点的组合内存中，并在它们组成的系统中执行迭代机器学习算法。
- en: 'One problem with memory, however, is that it is does not persist across node
    failures and reboots. A popular big data framework, Hadoop, made possible with
    the help of the original Dean/Ghemawat paper (Jeff Dean and Sanjay Ghemawat, *MapReduce:
    Simplified Data Processing on Large Clusters*, OSDI, 2004.), is using exactly
    the disk layer persistence to guarantee fault tolerance and store intermediate
    results. A Hadoop MapReduce program would first run a `map` function on each row
    of a dataset, emitting one or more key/value pairs. These key/value pairs then
    would be sorted, grouped, and aggregated by key so that the records with the same
    key would end up being processed together on the same reducer, which might be
    running on same or another node. The reducer applies a `reduce` function that
    traverses all the values that were emitted for the same key and aggregates them
    accordingly. The persistence of intermediate results would guarantee that if a
    reducer fails for one or another reason, the partial computations can be discarded
    and the reduce computation can be restarted from the checkpoint-saved results.
    Many simple ETL-like applications traverse the dataset only once with very little
    information preserved as state from one record to another.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，内存的一个问题是它不能在节点故障和重启后持续存在。一个流行的大数据框架Hadoop，在原始的Dean/Ghemawat论文（Jeff Dean和Sanjay
    Ghemawat, *MapReduce: Simplified Data Processing on Large Clusters*, OSDI, 2004）的帮助下成为可能，正是使用磁盘层持久性来保证容错性和存储中间结果。一个Hadoop
    MapReduce程序首先会在数据集的每一行上运行一个`map`函数，输出一个或多个键值对。这些键值对随后会被排序、分组并按键聚合，以便具有相同键的记录最终会在同一个reducer上一起处理，这个reducer可能运行在同一个或另一个节点上。reducer应用一个`reduce`函数，遍历为相同键发出的所有值，并相应地聚合它们。中间结果的持久性将保证如果reducer由于一个或另一个原因失败，可以丢弃部分计算，并从检查点保存的结果重新启动reduce计算。许多简单的ETL类似的应用只遍历数据集一次，并且从一条记录到另一条记录只保留很少的信息作为状态。'
- en: 'For example, one of the traditional applications of MapReduce is word count.
    The program needs to count the number of occurrences of each word in a document
    consisting of lines of text. In Scala, the word count is readily expressed as
    an application of the `foldLeft` method on a sorted list of words:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MapReduce的一个传统应用是词频统计。程序需要计算由文本行组成的文档中每个单词出现的次数。在Scala中，词频统计可以很容易地表示为对排序单词列表应用`foldLeft`方法：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If I run this program, the output will be a list of (word, count) tuples. The
    program splits the lines into words, sorts the words, and then matches each word
    with the latest entry in the list of (word, count) tuples. The same computation
    in MapReduce would be expressed as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我运行这个程序，输出将是一个包含(word, count)元组的列表。程序将行分割成单词，对单词进行排序，然后将每个单词与(word, count)元组列表中的最新条目进行匹配。在MapReduce中，同样的计算可以表示如下：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we need to process each line of the text by splitting the line into words
    and generation `(word, 1)` pairs. This task is easily parallelized. Then, to parallelize
    the global count, we need to split the counting part by assigning a task to do
    the count for a subset of words. In Hadoop, we compute the hash of the word and
    divide the work based on the value of the hash.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过将行分割成单词并生成 `(word, 1)` 对来处理文本的每一行。这个任务很容易并行化。然后，为了并行化全局计数，我们需要通过为单词子集分配一个执行计数的任务来分割计数部分。在
    Hadoop 中，我们计算单词的哈希值并根据哈希值来划分工作。
- en: Once the map task finds all the entries for a given hash, it can send the key/value
    pairs to the reducer, the sending part is usually called shuffle in MapReduce
    vernacular. A reducer waits until it receives all the key/value pairs from all
    the mappers, combines the values—a partial combine can also happen on the mapper,
    if possible—and computes the overall aggregate, which in this case is just sum.
    A single reducer will see all the values for a given word.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 map 任务找到了给定哈希的所有条目，它就可以将键值对发送到 reducer，发送部分通常在 MapReduce 术语中称为 shuffle。reducer
    等待从所有 mapper 收到所有键值对，合并值——如果可能的话，在 mapper 上也可以进行部分合并——并计算整体聚合，在这种情况下就是求和。单个 reducer
    将看到给定单词的所有值。
- en: 'Let''s look at the log output of the word count operation in Spark (Spark is
    very verbose by default, you can manage the verbosity level by modifying the `conf/log4j.properties`
    file by replacing `INFO` with `ERROR` or `FATAL`):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Spark 中单词计数操作的日志输出（Spark 默认非常详细，您可以通过修改 `conf/log4j.properties` 文件来管理详细程度，将
    `INFO` 替换为 `ERROR` 或 `FATAL`）：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At this stage, the only thing that happened is metadata manipulations, Spark
    has not touched the data itself. Spark estimates that the size of the dataset
    and the number of partitions. By default, this is the number of HDFS blocks, but
    we can specify the minimum number of partitions explicitly with the `minPartitions`
    parameter:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，唯一发生的事情是对元数据的操作，Spark 本身还没有触及数据。Spark 估计数据集的大小和分区数。默认情况下，这是 HDFS 块的数量，但我们可以通过
    `minPartitions` 参数显式指定最小分区数：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We just defined another RDD derived from the original `linesRdd`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚定义了另一个由原始 `linesRdd` 衍生出来的 RDD：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Word count over 2 GB of text data—40,291 lines and 353,087 words—took under
    a second to read, split, and group by words.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对超过 2 GB 的文本数据进行单词计数——40,291 行和 353,087 个单词——读取、分割和按单词分组所需时间不到一秒。
- en: 'With extended logging, you could see the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩展日志记录，您可以看到以下内容：
- en: Spark opens a few ports to communicate with the executors and users
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 打开一些端口以与执行器和用户进行通信
- en: Spark UI runs on port 4040 on `http://localhost:4040`
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark UI 在 `http://localhost:4040` 的 4040 端口上运行
- en: You can read the file either from local or distributed storage (HDFS, Cassandra,
    and S3)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以从本地或分布式存储（HDFS、Cassandra 和 S3）读取文件
- en: Spark will connect to Hive if Spark is built with Hive support
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Spark 是与 Hive 支持一起构建的，它将连接到 Hive
- en: Spark uses lazy evaluation and executes the pipeline only when necessary or
    when output is required
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 使用惰性求值，仅在必要时或需要输出时才执行管道
- en: Spark uses internal scheduler to split the job into tasks, optimize the execution,
    and execute the tasks
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 使用内部调度器将作业拆分为任务，优化执行，并执行任务
- en: The results are stored into RDDs, which can either be saved or brought into
    RAM of the node executing the shell with collect method
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果存储到 RDDs 中，这些 RDDs 可以被保存或通过 collect 方法被带到执行 shell 的节点 RAM 中
- en: The art of parallel performance tuning is to split the workload between different
    nodes or threads so that the overhead is relatively small and the workload is
    balanced.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 平行性能调优的艺术在于在不同节点或线程之间分配工作量，以便开销相对较小且工作量平衡。
- en: Streaming word count
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式单词计数
- en: Spark supports listening on incoming streams, partitioning it, and computing
    aggregates close to real-time. Currently supported sources are Kafka, Flume, HDFS/S3,
    Kinesis, Twitter, as well as the traditional MQs such as ZeroMQ and MQTT. In Spark,
    streaming is implemented as micro-batches. Internally, Spark divides input data
    into micro-batches, usually from subseconds to minutes in size and performs RDD
    aggregation operations on these micro-batches.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持监听传入的流，对其进行分区，并接近实时地计算聚合。目前支持的资源包括 Kafka、Flume、HDFS/S3、Kinesis、Twitter，以及传统的消息队列如
    ZeroMQ 和 MQTT。在 Spark 中，流式处理被实现为微批处理。内部，Spark 将输入数据划分为微批处理，通常大小从几秒到几分钟，并对这些微批处理执行
    RDD 聚合操作。
- en: 'For example, let''s extend the Flume example that we covered earlier. We''ll
    need to modify the Flume configuration file to create a Spark polling sink. Instead
    of HDFS, replace the sink section:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们扩展我们之前讨论的Flume示例。我们需要修改Flume配置文件以创建一个Spark轮询接收器。将接收器部分替换为HDFS：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, instead of writing to HDFS, Flume will wait for Spark to poll for data:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Flume不再直接写入HDFS，而是等待Spark轮询数据：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To run the program, start the Flume agent in one window:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行程序，在一个窗口中启动Flume代理：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then run the `FlumeWordCount` object in another:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在另一个窗口中运行`FlumeWordCount`对象：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, any text typed to the `netcat` connection will be split into words and
    counted every two seconds for a six second sliding window:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，任何输入到`netcat`连接中的文本将被分成单词，并且每两秒计数一次，滑动窗口为六秒：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Spark/Scala allows to seamlessly switch between the streaming sources. For
    example, the same program for Kafka publish/subscribe topic model looks similar
    to the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Spark/Scala允许在流源之间无缝切换。例如，Kafka发布/订阅主题模型的相同程序看起来类似于以下内容：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To start the Kafka broker, first download the latest binary distribution and
    start ZooKeeper. ZooKeeper is a distributed-services coordinator and is required
    by Kafka even in a single-node deployment:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动Kafka代理，首先下载最新的二进制发行版并启动ZooKeeper。ZooKeeper是一个分布式服务协调器，即使在单节点部署中，Kafka也需要它：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In another window, start the Kafka server:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个窗口中，启动Kafka服务器：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Run the `KafkaWordCount` object:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`KafkaWordCount`对象：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, publishing the stream of words into the Kafka topic will produce the window
    counts:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将单词流发布到Kafka主题将产生窗口计数：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you see, the programs output every two seconds. Spark streaming is sometimes
    called **micro-batch processing**. Streaming has many other applications (and
    frameworks), but this is too big of a topic to be entirely considered here and
    needs to be covered separately. I'll cover some ML on streams of data in [Chapter
    5](ch05.xhtml "Chapter 5. Regression and Classification"), *Regression and Classification*.
    Now, let's get back to more traditional SQL-like interfaces.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，程序每两秒输出一次。Spark流有时被称为**微批处理**。流处理有许多其他应用（和框架），但这是一个太大的话题，不能完全在这里讨论，需要单独介绍。我将在[第5章](ch05.xhtml
    "第5章。回归和分类") *回归和分类* 中介绍一些数据流上的机器学习。现在，让我们回到更传统的类似SQL的接口。
- en: Spark SQL and DataFrame
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL和DataFrame
- en: DataFrame was a relatively recent addition to Spark, introduced in version 1.3,
    allowing one to use the standard SQL language for data analysis. We already used
    some SQL commands in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory Data Analysis"),
    *Exploratory Data Analysis* for the exploratory data analysis. SQL is really great
    for simple exploratory analysis and data aggregations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame是Spark相对较新的功能，自1.3版本引入，允许用户使用标准SQL语言进行数据分析。我们已经在[第1章](ch01.xhtml "第1章。探索性数据分析")
    *探索性数据分析* 中使用了一些SQL命令进行探索性数据分析。SQL对于简单的探索性分析和数据聚合来说非常好用。
- en: According to the latest poll results, about 70% of Spark users use DataFrame.
    Although DataFrame recently became the most popular framework for working with
    tabular data, it is a relatively heavyweight object. The pipelines that use DataFrames
    may execute much slower than the ones that are based on Scala's vector or LabeledPoint,
    which will be discussed in the next chapter. The evidence from different developers
    is that the response times can be driven to tens or hundreds of milliseconds depending
    on the query, from submillisecond on simpler objects.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最新的民意调查结果，大约70%的Spark用户使用DataFrame。尽管DataFrame最近已成为处理表格数据最流行的框架，但它是一个相对较重的对象。使用DataFrame的管道可能比基于Scala的向量或LabeledPoint的管道执行得慢得多，这将在下一章中讨论。不同开发者的证据表明，响应时间可以根据查询从数十毫秒到数百毫秒不等，对于更简单的对象甚至可以低于毫秒。
- en: 'Spark implements its own shell for SQL, which can be invoked in addition to
    the standard Scala REPL shell: `./bin/spark-sql` can be used to access the existing
    Hive/Impala or relational DB tables:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Spark实现了自己的SQL shell，除了标准的Scala REPL shell外还可以调用：`./bin/spark-sql`可以用来访问现有的Hive/Impala或关系型数据库表：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In standard Spark''s REPL, the same query can be performed by running the following
    command:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的Spark REPL中，可以通过运行以下命令执行相同的查询：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ML libraries
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习库
- en: Spark, particularly with memory-based storage systems, claims to substantially
    improve the speed of data access within and between nodes. ML seems to be a natural
    fit, as many algorithms require multiple passes over the data, or repartitioning.
    MLlib is the open source library of choice, although private companies are catching,
    up with their own proprietary implementations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Spark，尤其是与基于内存的存储系统结合使用，声称显著提高了节点内和节点间的数据访问速度。机器学习似乎是一个自然的选择，因为许多算法需要多次遍历数据或重新分区。MLlib是首选的开源库，尽管私有公司正在赶上，并推出了自己的专有实现。
- en: 'As I will chow in [Chapter 5](ch05.xhtml "Chapter 5. Regression and Classification"),
    *Regression and Classification*, most of the standard machine learning algorithms
    can be expressed as an optimization problem. For example, classical linear regression
    minimizes the sum of squares of *y* distance between the regression line and the
    actual value of *y*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我将在[第5章](ch05.xhtml "第5章。回归与分类")中详细说明的，*回归与分类*，大多数标准机器学习算法都可以表示为一个优化问题。例如，经典线性回归最小化回归线与实际值*y*之间的平方和距离：
- en: '![ML libraries](img/B04935_03_01F.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习库](img/B04935_03_01F.jpg)'
- en: 'Here, ![ML libraries](img/B04935_03_02F.jpg) are the predicted values according
    to the linear expression:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![机器学习库](img/B04935_03_02F.jpg)是根据线性表达式预测的值：
- en: '![ML libraries](img/B04935_03_03F.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习库](img/B04935_03_03F.jpg)'
- en: '*A* is commonly called the slope, and *B* the intercept. In a more generalized
    formulation, a linear optimization problem is to minimize an additive function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* 通常被称为斜率，而 *B* 被称为截距。在更一般的公式中，线性优化问题是最小化一个加性函数：'
- en: '![ML libraries](img/B04935_03_04F.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习库](img/B04935_03_04F.jpg)'
- en: 'Here, ![ML libraries](img/B04935_03_05F.jpg) is a loss function and ![ML libraries](img/B04935_03_06F.jpg)
    is a regularization function. The regularization function is an increasing function
    of model complexity, for example, the number of parameters (or a natural logarithm
    thereof). Most common loss functions are given in the following table:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![机器学习库](img/B04935_03_05F.jpg)是一个损失函数，![机器学习库](img/B04935_03_06F.jpg)是一个正则化函数。正则化函数是模型复杂度的递增函数，例如参数的数量（或其自然对数）。以下表格给出了最常见的损失函数：
- en: '|   | Loss function L | Gradient |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|   | 损失函数 L | 梯度 |'
- en: '| --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Linear | ![ML libraries](img/B04935_03_07F.jpg) | ![ML libraries](img/B04935_03_08F.jpg)
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | ![机器学习库](img/B04935_03_07F.jpg) | ![机器学习库](img/B04935_03_08F.jpg) |'
- en: '| Logistic | ![ML libraries](img/B04935_03_09F.jpg) | ![ML libraries](img/B04935_03_10F.jpg)
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | ![机器学习库](img/B04935_03_09F.jpg) | ![机器学习库](img/B04935_03_10F.jpg)
    |'
- en: '| Hinge | ![ML libraries](img/B04935_03_11F.jpg) | ![ML libraries](img/B04935_03_13F.jpg)
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 拉伸 | ![机器学习库](img/B04935_03_11F.jpg) | ![机器学习库](img/B04935_03_13F.jpg) |'
- en: 'The purpose of the regularizer is to penalize more complex models to avoid
    overfitting and improve generalization error: more MLlib currently supports the
    following regularizers:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器的目的是惩罚更复杂的模型，以避免过拟合并提高泛化误差：目前MLlib支持以下正则化器：
- en: '|   | Regularizer R | Gradient |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|   | 正则化器 R | 梯度 |'
- en: '| --- | --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| L2 | ![ML libraries](img/B04935_03_14F.jpg) | ![ML libraries](img/B04935_03_15F.jpg)
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| L2 | ![机器学习库](img/B04935_03_14F.jpg) | ![机器学习库](img/B04935_03_15F.jpg) |'
- en: '| L1 | ![ML libraries](img/B04935_03_16F.jpg) | ![ML libraries](img/B04935_03_17F.jpg)
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| L1 | ![机器学习库](img/B04935_03_16F.jpg) | ![机器学习库](img/B04935_03_17F.jpg) |'
- en: '| Elastic net | ![ML libraries](img/B04935_03_18F.jpg) | ![ML libraries](img/B04935_03_19F.jpg)
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 弹性网络 | ![机器学习库](img/B04935_03_18F.jpg) | ![机器学习库](img/B04935_03_19F.jpg)
    |'
- en: Here, *sign(w)* is the vector of the signs of all entries of *w*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*sign(w)* 是所有 *w* 条目的符号向量。
- en: 'Currently, MLlib includes implementation of the following algorithms:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，MLlib包括以下算法的实现：
- en: 'Basic statistics:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本统计：
- en: Summary statistics
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率统计
- en: Correlations
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关系数
- en: Stratified sampling
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层抽样
- en: Hypothesis testing
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设检验
- en: Streaming significance testing
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式显著性检验
- en: Random data generation
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机数据生成
- en: 'Classification and regression:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类与回归：
- en: Linear models (SVMs, logistic regression, and linear regression)
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型（支持向量机、逻辑回归和线性回归）
- en: Naive Bayes
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Decision trees
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Ensembles of trees (Random Forests and Gradient-Boosted Trees)
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的集成（随机森林和梯度提升树）
- en: Isotonic regression
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等距回归
- en: 'Collaborative filtering:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤：
- en: '**Alternating least squares** (**ALS**)'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘法**（**ALS**）'
- en: 'Clustering:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类：
- en: k-means
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: k均值
- en: Gaussian mixture
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类**（**PIC**）'
- en: '**Latent Dirichlet allocation** (**LDA**)'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）'
- en: Bisecting k-means
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分k均值
- en: Streaming k-means
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式 k-means
- en: 'Dimensionality reduction:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度约简：
- en: '**Singular Value Decomposition** (**SVD**)'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）'
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: Feature extraction and transformation
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取和转换
- en: 'Frequent pattern mining:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁模式挖掘：
- en: 'FP-growth     Association rules'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP-growth 关联规则
- en: PrefixSpan
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrefixSpan
- en: 'Optimization:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化：
- en: '**Stochastic Gradient Descent** (**SGD**)'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）'
- en: '**Limited-Memory BFGS** (**L-BFGS**)'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限内存 BFGS**（**L-BFGS**）'
- en: I will go over some of the algorithms in [Chapter 5](ch05.xhtml "Chapter 5. Regression
    and Classification"), *Regression and Classification*. More complex non-structured
    machine learning methods will be considered in [Chapter 6](ch06.xhtml "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在第 5 章 [回归和分类](ch05.xhtml "第 5 章。回归和分类") 中介绍一些算法。更复杂的非结构化机器学习方法将在第 6 章 [处理非结构化数据](ch06.xhtml
    "第 6 章。处理非结构化数据") 中考虑。
- en: SparkR
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkR
- en: R is an implementation of popular S programming language created by John Chambers
    while working at Bell Labs. R is currently supported by the **R Foundation for
    Statistical Computing**. R's popularity has increased in recent years according
    to polls. SparkR provides a lightweight frontend to use Apache Spark from R. Starting
    with Spark 1.6.0, SparkR provides a distributed DataFrame implementation that
    supports operations such as selection, filtering, aggregation, and so on, which
    is similar to R DataFrames, dplyr, but on very large datasets. SparkR also supports
    distributed machine learning using MLlib.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: R 是由约翰·查默斯在贝尔实验室工作时创建的一种流行的 S 编程语言的实现。R 目前由 **R 统计计算基金会** 支持。根据调查，R 的普及率近年来有所增加。SparkR
    为从 R 使用 Apache Spark 提供了一个轻量级的前端。从 Spark 1.6.0 开始，SparkR 提供了一个支持选择、过滤、聚合等操作的分布式
    DataFrame 实现，这与 R DataFrames、dplyr 类似，但适用于非常大的数据集。SparkR 还支持使用 MLlib 进行分布式机器学习。
- en: SparkR required R version 3 or higher, and can be invoked via the `./bin/sparkR`
    shell. I will cover SparkR in [Chapter 8](ch08.xhtml "Chapter 8. Integrating Scala
    with R and Python"), *Integrating Scala with R and Python*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 需要 R 版本 3 或更高版本，可以通过 `./bin/sparkR` shell 调用。我将在第 8 章 [集成 Scala 与 R 和
    Python](ch08.xhtml "第 8 章。集成 Scala 与 R 和 Python") 中介绍 SparkR，*集成 Scala 与 R 和 Python*。
- en: Graph algorithms – GraphX and GraphFrames
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图算法 – GraphX 和 GraphFrames
- en: Graph algorithms are one of the hardest to correctly distribute between nodes,
    unless the graph itself is naturally partitioned, that is, it can be represented
    by a set of disconnected subgraphs. Since the social networking analysis on a
    multi-million node scale became popular due to companies such as Facebook, Google,
    and LinkedIn, researches have been coming up with new approaches to formalize
    the graph representations, algorithms, and types of questions asked.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图算法是节点间正确分配最困难的算法之一，除非图本身是自然划分的，即它可以表示为一组不连接的子图。由于 Facebook、Google 和 LinkedIn
    等公司使得在数百万节点规模上的社交网络分析变得流行，研究人员一直在提出新的方法来形式化图表示、算法和提出的问题类型。
- en: 'GraphX is a modern framework for graph computations described in a 2013 paper
    (*GraphX: A Resilient Distributed Graph System on Spark* by Reynold Xin, Joseph
    Gonzalez, Michael Franklin, and Ion Stoica, GRADES (SIGMOD workshop), 2013). It
    has graph-parallel frameworks such as Pregel, and PowerGraph as predecessors.
    The graph is represented by two RDDs: one for vertices and another one for edges.
    Once the RDDs are joined, GraphX supports either Pregel-like API or MapReduce-like
    API, where the map function is applied to the node''s neighbors and reduce is
    the aggregation step on top of the map results.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 'GraphX 是一个现代的图计算框架，由 2013 年的一篇论文描述（Reynold Xin、Joseph Gonzalez、Michael Franklin
    和 Ion Stoica 的 *GraphX: A Resilient Distributed Graph System on Spark*，GRADES
    (SIGMOD workshop)，2013）。它有图并行框架如 Pregel 和 PowerGraph 作为前身。图由两个 RDD 表示：一个用于顶点，另一个用于边。一旦
    RDD 被连接，GraphX 支持类似 Pregel 的 API 或类似 MapReduce 的 API，其中 map 函数应用于节点的邻居，reduce
    是在 map 结果之上的聚合步骤。'
- en: 'At the time of writing, GraphX includes the implementation for the following
    graph algorithms:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，GraphX 包含以下图算法的实现：
- en: PageRank
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PageRank
- en: Connected components
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连通分量
- en: Triangle counting
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三角计数
- en: Label propagation
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签传播
- en: SVD++ (collaborative filtering)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD++（协同过滤）
- en: Strongly connected components
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强连通分量
- en: 'As GraphX is an open source library, changes to the list are expected. GraphFrames
    is a new implementation from Databricks that fully supports the following three
    languages: Scala, Java, and Python, and is build on top of DataFrames. I''ll discuss
    specific implementations in [Chapter 7](ch07.xhtml "Chapter 7. Working with Graph
    Algorithms"), *Working with Graph Algorithms*.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GraphX 是一个开源库，预期列表会有所变化。GraphFrames 是 Databricks 的新实现，完全支持以下三种语言：Scala、Java
    和 Python，并且建立在 DataFrames 之上。我将在 [第 7 章](ch07.xhtml "第 7 章。使用图算法") *使用图算法* 中讨论具体的实现。
- en: Spark performance tuning
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 性能调优
- en: 'While efficient execution of the data pipeline is prerogative of the task scheduler,
    which is part of the Spark driver, sometimes Spark needs hints. Spark scheduling
    is primarily driven by the two parameters: CPU and memory. Other resources, such
    as disk and network I/O, of course, play an important part in Spark performance
    as well, but neither Spark, Mesos or YARN can currently do anything to actively
    manage them.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据管道的高效执行是任务调度器的特权，它是 Spark 驱动程序的一部分，但有时 Spark 需要提示。Spark 调度主要受两个参数驱动：CPU
    和内存。当然，其他资源，如磁盘和网络 I/O，在 Spark 性能中也扮演着重要角色，但 Spark、Mesos 或 YARN 目前都无法积极管理它们。
- en: The first parameter to watch is the number of RDD partitions, which can be specified
    explicitly when reading the RDD from a file. Spark usually errs on the side of
    too many partitions as it provides more parallelism, and it does work in many
    cases as the task setup/teardown times are relatively small. However, one might
    experiment with decreasing the number of partitions, especially if one does aggregations.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要关注的是 RDD 分区数，当从文件中读取 RDD 时可以显式指定。Spark 通常倾向于过多的分区，因为它提供了更多的并行性，并且在许多情况下确实有效，因为任务设置/拆除时间相对较小。然而，人们可能会尝试减少分区数，尤其是在进行聚合操作时。
- en: The default number of partitions per RDD and the level of parallelism is determined
    by the `spark.default.parallelism` parameter, defined in the `$SPARK_HOME/conf/spark-defaults.conf`
    configuration file. The number of partitions for a specific RDD can also be explicitly
    changed by the `coalesce()` or `repartition()` methods.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 RDD 的分区数和并行级别由 `$SPARK_HOME/conf/spark-defaults.conf` 配置文件中定义的 `spark.default.parallelism`
    参数确定。也可以通过 `coalesce()` 或 `repartition()` 方法显式地更改特定 RDD 的分区数。
- en: The total number of cores and available memory is often the reason for deadlocks
    as the tasks cannot proceed further. One can specify the number of cores for each
    executor with the `--executor-cores` flag when invoking spark-submit, spark-shell,
    or PySpark from the command line. Alternatively, one can set the corresponding
    parameters in the `spark-defaults.conf` file discussed earlier. If the number
    of cores is set too high, the scheduler will not be able to allocate resources
    on the nodes and will deadlock.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 核心总数和可用内存经常是死锁的原因，因为任务无法进一步执行。在通过命令行调用 spark-submit、spark-shell 或 PySpark 时，可以使用
    `--executor-cores` 标志指定每个执行器的核心数。或者，也可以在前面讨论过的 `spark-defaults.conf` 文件中设置相应的参数。如果核心数设置得太高，调度器将无法在节点上分配资源，从而导致死锁。
- en: In a similar way, `--executor-memory` (or the `spark.executor.memory` property)
    specifies the requested heap size for all the tasks (the default is 1g). If the
    executor memory is specified too high, again, the scheduler may be deadlocked
    or will be able to schedule only a limited number of executors on a node.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，`--executor-memory`（或 `spark.executor.memory` 属性）指定了所有任务请求的堆大小（默认为 1g）。如果执行器内存设置得太高，同样，调度器可能会死锁，或者只能在节点上调度有限数量的执行器。
- en: 'The implicit assumption in Standalone mode when counting the number of cores
    and memory is that Spark is the only running application—which may or may not
    be true. When running under Mesos or YARN, it is important to configure the cluster
    scheduler that it has the resources available to schedule the executors requested
    by the Spark Driver. The relevant YARN properties are: `yarn.nodemanager.resource.cpu-vcores`
    and `yarn.nodemanager.resource.memory-mb`. YARN may round the requested memory
    up a little. YARN''s `yarn.scheduler.minimum-allocation-mb` and `yarn.scheduler.increment-allocation-mb`
    properties control the minimum and increment request values respectively.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式（Standalone mode）下，在计算核心数和内存时隐含的假设是Spark是唯一运行的应用程序——这可能是也可能不是真的。当在Mesos或YARN下运行时，配置集群调度器以使其有资源来调度Spark
    Driver请求的执行器非常重要。相关的YARN属性是：`yarn.nodemanager.resource.cpu-vcores`和`yarn.nodemanager.resource.memory-mb`。YARN可能会将请求的内存向上取整一点。YARN的`yarn.scheduler.minimum-allocation-mb`和`yarn.scheduler.increment-allocation-mb`属性分别控制最小和增量请求值。
- en: JVMs can also use some memory off heap, for example, for interned strings and
    direct byte buffers. The value of the `spark.yarn.executor.memoryOverhead` property
    is added to the executor memory to determine the full memory request to YARN for
    each executor. It defaults to max (*384, .07 * spark.executor.memory*).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: JVMs也可以使用一些堆外内存，例如，用于内部字符串和直接字节缓冲区。`spark.yarn.executor.memoryOverhead`属性的值被添加到执行器内存中，以确定每个执行器对YARN的完整内存请求。默认值为最大值（*384,
    .07 * spark.executor.memory*）。
- en: Since Spark can internally transfer the data between executors and client node,
    efficient serialization is very important. I will consider different serialization
    frameworks in [Chapter 6](ch06.xhtml "Chapter 6. Working with Unstructured Data"),
    *Working with Unstructured Data*, but Spark uses Kryo serialization by default,
    which requires the classes to be registered explicitly in a static method. If
    you see a serialization error in your code, it is likely because the corresponding
    class has not been registered or Kryo does not support it, as it happens with
    too nested and complex data types. In general, it is recommended to avoid complex
    objects to be passed between the executors unless the object serialization can
    be done very efficiently.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark可以在执行器和客户端节点之间内部传输数据，因此高效的序列化非常重要。我将在[第6章](ch06.xhtml "第6章。处理非结构化数据")*处理非结构化数据*中考虑不同的序列化框架，但Spark默认使用Kryo序列化，这要求类必须显式地在静态方法中注册。如果你在代码中看到序列化错误，很可能是因为相应的类尚未注册或Kryo不支持它，就像在过于嵌套和复杂的数据类型中发生的那样。一般来说，建议避免在执行器之间传递复杂对象，除非对象序列化可以非常高效地进行。
- en: 'Driver has similar parameters: `spark.driver.cores`, `spark.driver.memory`,
    and `spark.driver.maxResultSize`. The latter one sets the limit for the results
    collected from all the executors with the `collect` method. It is important to
    protect the driver process from out-of-memory exceptions. The other way to avoid
    out-of-memory exceptions and consequent problems are to either modify the pipeline
    to return aggregated or filtered results or use the `take` method instead.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动器有类似的参数：`spark.driver.cores`、`spark.driver.memory`和`spark.driver.maxResultSize`。后者设置了使用`collect`方法收集的所有执行器的结果限制。保护驱动器进程免受内存不足异常非常重要。避免内存不足异常及其后续问题的另一种方法是修改管道以返回聚合或过滤后的结果，或者使用`take`方法代替。
- en: Running Hadoop HDFS
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Hadoop HDFS
- en: A distributed processing framework wouldn't be complete without distributed
    storage. One of them is HDFS. Even if Spark is run on local mode, it can still
    use a distributed file system at the backend. Like Spark breaks computations into
    subtasks, HDFS breaks a file into blocks and stores them across a set of machines.
    For HA, HDFS stores multiple copies of each block, the number of copies is called
    replication level, three by default (refer to *Figure 3-5*).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分布式处理框架如果没有分布式存储就不会完整。其中之一是HDFS。即使Spark在本地模式（local mode）下运行，它仍然可以使用后端分布式文件系统。就像Spark将计算分解为子任务一样，HDFS将文件分解为块并在多台机器上存储它们。对于高可用性（HA），HDFS为每个块存储多个副本，副本的数量称为副本级别，默认为三个（参见图3-5）。
- en: '**NameNode** is managing the HDFS storage by remembering the block locations
    and other metadata such as owner, file permissions, and block size, which are
    file-specific. **Secondary Namenode** is a slight misnomer: its function is to
    merge the metadata modifications, edits, into fsimage, or a file that serves as
    a metadata database. The merge is required, as it is more practical to write modifications
    of fsimage to a separate file instead of applying each modification to the disk
    image of the fsimage directly (in addition to applying the corresponding changes
    in memory). Secondary **Namenode** cannot serve as a second copy of the **Namenode**.
    A **Balancer** is run to move the blocks to maintain approximately equal disk
    usage across the servers—the initial block assignment to the nodes is supposed
    to be random, if enough space is available and the client is not run within the
    cluster. Finally, the **Client** communicates with the **Namenode** to get the
    metadata and block locations, but after that, either reads or writes the data
    directly to the node, where a copy of the block resides. The client is the only
    component that can be run outside the HDFS cluster, but it needs network connectivity
    with all the nodes in the cluster.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**NameNode** 通过记住块位置和其他元数据（如所有者、文件权限和块大小，这些是文件特定的）来管理 HDFS 存储。**Secondary Namenode**
    是一个轻微的错误名称：其功能是将元数据修改、编辑合并到 fsimage 中，或者是一个充当元数据库的文件。合并是必要的，因为将 fsimage 的修改写入单独的文件比直接将每个修改应用到
    fsimage 的磁盘映像更实际（除了在内存中应用相应的更改）。Secondary **Namenode** 不能作为 **Namenode** 的第二个副本。运行
    **Balancer** 以将块移动到服务器，以保持服务器之间大约相等的磁盘使用率——如果空间足够且客户端不在集群内运行，节点上的初始块分配应该是随机的。最后，**Client**
    与 **Namenode** 通信以获取元数据和块位置，但之后，要么直接读取或写入数据到节点，其中存储着块的副本。客户端是唯一可以在 HDFS 集群外运行的组件，但它需要与集群中所有节点建立网络连接。'
- en: 'If any of the node dies or disconnects from the network, the **Namenode** notices
    the change, as it constantly maintains the contact with the nodes via heartbeats.
    If the node does not reconnect to the **Namenode** within 10 minutes (by default),
    the **Namenode** will start replicating the blocks in order to achieve the required
    replication level for the blocks that were lost on the node. A separate block
    scanner thread in the **Namenode** will scan the blocks for possible bit rot—each
    block maintains a checksum—and will delete corrupted and orphaned blocks:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何节点死亡或从网络断开连接，**Namenode** 会注意到变化，因为它通过心跳不断与节点保持联系。如果节点在 10 分钟内（默认值）没有重新连接到
    **Namenode**，**Namenode** 将开始复制块，以实现节点上丢失的块所需的复制级别。**Namenode** 中的单独块扫描线程将扫描块以查找可能的位错——每个块维护一个校验和——并将损坏的和孤立的块删除：
- en: '![Running Hadoop HDFS](img/B04935_03_05.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![运行 Hadoop HDFS](img/B04935_03_05.jpg)'
- en: Figure 03-5\. This is the HDFS architecture. Each block is stored in three separate
    locations (the replication level).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-5\. 这是 HDFS 架构。每个块存储在三个不同的位置（复制级别）。
- en: 'To start HDFS on your machine (with replication level 1), download a Hadoop
    distribution, for example, from [http://hadoop.apache.org](http://hadoop.apache.org):'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在您的机器上启动 HDFS（复制级别为 1），下载一个 Hadoop 发行版，例如，从 [http://hadoop.apache.org](http://hadoop.apache.org)：
- en: '[PRE23]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To get the minimal HDFS configuration, modify the `core-site.xml` and `hdfs-site.xml`
    files, as follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取最小化的 HDFS 配置，修改 `core-site.xml` 和 `hdfs-site.xml` 文件，如下所示：
- en: '[PRE24]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will put the Hadoop HDFS metadata and data directories under the `/tmp/hadoop-$USER`
    directories. To make this more permanent, we can add the `dfs.namenode.name.dir`,
    `dfs.namenode.edits.dir`, and `dfs.datanode.data.dir` parameters, but we will
    leave these out for now. For a more customized distribution, one can download
    a Cloudera version from [http://archive.cloudera.com/cdh](http://archive.cloudera.com/cdh).
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将把 Hadoop HDFS 元数据和数据目录放在 `/tmp/hadoop-$USER` 目录下。为了使其更加持久，我们可以添加 `dfs.namenode.name.dir`、`dfs.namenode.edits.dir`
    和 `dfs.datanode.data.dir` 参数，但现在我们先不添加这些参数。对于更定制的发行版，可以从 [http://archive.cloudera.com/cdh](http://archive.cloudera.com/cdh)
    下载 Cloudera 版本。
- en: 'First, we need to write an empty metadata:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要编写一个空的元数据：
- en: '[PRE25]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then start the `namenode`, `secondarynamenode`, and `datanode` Java processes
    (I usually open three different command-line windows to see the logs, but in a
    production environment, these are usually daemonized):'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后启动 `namenode`、`secondarynamenode` 和 `datanode` Java 进程（我通常打开三个不同的命令行窗口来查看日志，但在生产环境中，这些通常被作为守护进程运行）：
- en: '[PRE26]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We are now ready to create the first HDFS file:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好创建第一个 HDFS 文件：
- en: '[PRE27]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Of course, in this particular case, the actual file is stored only on one node,
    which is the same node we run `datanode` on (localhost). In my case, it is the
    following:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，在这种情况下，实际文件仅存储在一个节点上，这个节点就是我们运行 `datanode` 的节点（localhost）。在我的情况下，如下所示：
- en: '[PRE28]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The Namenode UI can be found at `http://localhost:50070` and displays a host
    of information, including the HDFS usage and the list of DataNodes, the slaves
    of the HDFS Master node as follows:![Running Hadoop HDFS](img/B04935_03_06.jpg)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Namenode UI 可以在 `http://localhost:50070` 找到，并显示大量信息，包括 HDFS 使用情况和 DataNodes
    列表，HDFS 主节点的奴隶节点如下所示：![运行 Hadoop HDFS](img/B04935_03_06.jpg)
- en: Figure 03-6\. A snapshot of HDFS NameNode UI.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 03-6\. HDFS NameNode UI 的快照。
- en: The preceding figure shows HDFS Namenode HTTP UI in a single node deployment
    (usually, `http://<namenode-address>:50070`). The **Utilities** | **Browse the
    file system** tab allows you to browse and download the files from HDFS. Nodes
    can be added by starting DataNodes on a different node and pointing to the Namenode
    with the `fs.defaultFS=<namenode-address>:8020` parameter. The Secondary Namenode
    HTTP UI is usually at `http:<secondarynamenode-address>:50090`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了单节点部署中的 HDFS Namenode HTTP UI（通常为 `http://<namenode-address>:50070`）。**实用工具**
    | **浏览文件系统** 选项卡允许您浏览和下载 HDFS 中的文件。可以通过在另一个节点上启动 DataNodes 并使用 `fs.defaultFS=<namenode-address>:8020`
    参数指向 Namenode 来添加节点。Secondary Namenode HTTP UI 通常位于 `http:<secondarynamenode-address>:50090`。
- en: Scala/Spark by default will use the local file system. However, if the `core-site/xml`
    file is on the classpath or placed in the `$SPARK_HOME/conf` directory, Spark
    will use HDFS as the default.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Scala/Spark 默认将使用本地文件系统。然而，如果 `core-site/xml` 文件位于类路径中或放置在 `$SPARK_HOME/conf`
    目录下，Spark 将使用 HDFS 作为默认。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I covered the Spark/Hadoop and their relationship with Scala
    and functional programming at a very high level. I considered a classic word count
    example and it's implementation in Scala and Spark. I also provided high-level
    components of Spark ecosystem with specific examples of word count and streaming.
    I now have all the components to start looking at the specific implementation
    of classic machine learning algorithms in Scala/Spark. In the next chapter, I
    will start by covering supervised and unsupervised learning—a traditional division
    of learning algorithms for structured data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我非常高级地介绍了 Spark/Hadoop 以及它们与 Scala 和函数式编程的关系。我考虑了一个经典的词频统计示例及其在 Scala 和
    Spark 中的实现。我还提供了 Spark 生态系统的高级组件，包括词频统计和流处理的特定示例。我现在有了所有组件来开始查看 Scala/Spark 中经典机器学习算法的具体实现。在下一章中，我将从监督学习和无监督学习开始，这是结构化数据学习算法的传统划分。
