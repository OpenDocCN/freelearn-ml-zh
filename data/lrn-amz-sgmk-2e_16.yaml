- en: 'Chapter 12: Automating Machine Learning Workflows'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章：自动化机器学习工作流
- en: In the previous chapter, you learned how to deploy machine learning models in
    different configurations, using both the `boto3` SDK. We used their APIs in **Jupyter**
    **Notebooks** – the preferred way to experiment and iterate quickly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何使用 `boto3` SDK 在不同配置中部署机器学习模型。我们在 **Jupyter** **Notebooks** 中使用了它们的
    API——这也是快速实验和迭代的首选方式。
- en: However, running notebooks for production tasks is not a good idea. Even if
    your code has been carefully tested, what about monitoring, logging, creating
    other AWS resources, handling errors, rolling back, and so on? Doing all of this
    right would require a lot of extra work and code, opening the possibility for
    more bugs. A more industrial approach is required.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将笔记本用于生产任务并不是一个好主意。即使你的代码经过了仔细测试，监控、日志记录、创建其他 AWS 资源、处理错误、回滚等问题怎么办？做到这一切正确将需要大量额外的工作和代码，这可能导致更多的错误。需要一种更工业化的方法。
- en: In this chapter, you'll first learn how to provision SageMaker resources with
    **AWS** **CloudFormation** and **AWS** **Cloud Development Kit** (**CDK**) – two
    AWS services purposely built to bring repeatability, predictability, and robustness.
    You'll see how you can preview changes before applying them, in order to avoid
    uncontrolled and potentially destructive operations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将首先学习如何通过**AWS** **CloudFormation** 和 **AWS** **Cloud Development Kit**（**CDK**）配置
    SageMaker 资源——这两个 AWS 服务专为实现可重复性、可预测性和可靠性而构建。你将看到如何在应用更改之前预览它们，以避免不可控和潜在的破坏性操作。
- en: Then, you'll learn how to automate end-to-end machine learning workflows with
    two other services – **AWS** **Step Functions** and **Amazon** **SageMaker Pipelines**.
    You'll see how to build workflows with simple APIs, and how to visualize results
    in **SageMaker Studio**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将学习如何使用另外两个服务——**AWS** **Step Functions** 和 **Amazon** **SageMaker Pipelines**——来自动化端到端的机器学习工作流。你将看到如何使用简单的
    API 构建工作流，并如何在 **SageMaker Studio** 中可视化结果。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Automating with AWS CloudFormation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS CloudFormation 自动化
- en: Automating with AWS CDK
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS CDK 自动化
- en: Building end-to-end workflows with AWS Step Functions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Step Functions 构建端到端工作流
- en: Building end-to-end workflows with Amazon SageMaker Pipelines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Pipelines 构建端到端工作流
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS free tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要一个 AWS 账户来运行本章中包含的示例。如果你还没有账户，请访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建一个。你还应该熟悉 AWS 免费套餐 ([https://aws.amazon.com/free/](https://aws.amazon.com/free/))，该套餐让你在某些使用限制内免费使用许多
    AWS 服务。
- en: You will need to install and configure the **AWS** **Command Line Interface**
    (**CLI**) for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置 **AWS** **Command Line Interface**（**CLI**）([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))。
- en: You will need a working `pandas`, `numpy`, and more).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个可用的 `pandas`、`numpy` 等。
- en: Code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中包含的代码示例可以在 GitHub 上找到，地址是 [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装一个
    Git 客户端来访问这些代码（[https://git-scm.com/](https://git-scm.com/)）。
- en: Automating with AWS CloudFormation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS CloudFormation 自动化
- en: AWS CloudFormation has long been the preferred way to automate infrastructure
    builds and operations on AWS ([https://aws.amazon.com/cloudformation](https://aws.amazon.com/cloudformation)).
    You could certainly write a book on the topic, but we'll stick to the basics in
    this section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AWS CloudFormation 长期以来一直是自动化 AWS 上基础设施构建和操作的首选方式 ([https://aws.amazon.com/cloudformation](https://aws.amazon.com/cloudformation))。虽然你完全可以写一本关于这个话题的书，但我们在本节中将仅介绍基础知识。
- en: The first step in using CloudFormation is to write a template – that is, a **JSON**
    or **YAML** text file describing the **resources** that you want to build, such
    as an **EC2** instance or an **S3** bucket. Resources are available for almost
    all AWS services, and SageMaker is no exception. If we look at [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html),
    we see that we can create SageMaker Studio applications, deploy endpoints, and
    more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CloudFormation 的第一步是编写一个模板——即一个描述你想要构建的**资源**（如 **EC2** 实例或 **S3** 存储桶）的**JSON**
    或 **YAML** 文本文件。几乎所有 AWS 服务的资源都可以使用，SageMaker 也不例外。如果我们查看 [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/AWS_SageMaker.html)，我们可以看到我们可以创建
    SageMaker Studio 应用程序，部署端点等等。
- en: A template can (and should) include parameters and outputs. The former help
    make templates as generic as possible. The latter provide information that can
    be used by downstream applications, such as endpoint URLs or bucket names.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模板可以（并且应该）包含参数和输出。前者有助于使模板尽可能通用，后者提供可以被下游应用程序使用的信息，如端点 URL 或存储桶名称。
- en: Once you've written your template file, you pass it to CloudFormation to create
    a **stack** – that is, a collection of AWS resources. CloudFormation will parse
    the template and create all resources automatically. Dependencies are also managed
    automatically, and resources will be created in the correct order. If a stack
    can't be created correctly, CloudFormation will roll it back, deleting resources
    that have been built so far.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你编写好了模板文件，就将它传递给 CloudFormation 来创建一个**堆栈**——也就是一组 AWS 资源。CloudFormation 会解析模板并自动创建所有资源。依赖关系也会自动管理，资源会按正确的顺序创建。如果堆栈无法正确创建，CloudFormation
    会回滚操作，删除已经创建的资源。
- en: A stack can be updated by applying a newer template revision. CloudFormation
    will analyze changes, and will create, delete, update, or replace resources accordingly.
    Thanks to **change sets**, you can verify changes before they are performed, and
    then decide whether to proceed or not.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用更新后的模板修订版，可以更新堆栈。CloudFormation 会分析更改，并相应地创建、删除、更新或替换资源。得益于**更改集**，你可以在执行之前验证更改，然后决定是否继续进行。
- en: Of course, a stack can be deleted, and CloudFormation will automatically tear
    down all its resources, which is a great way to clean up your builds without leaving
    any cruft behind.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，堆栈也可以被删除，CloudFormation 会自动销毁所有其资源，这是清理构建并避免留下垃圾文件的好方法。
- en: Let's run a first example, where we deploy a model to a real-time endpoint.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个示例，部署一个模型到实时端点。
- en: Writing a template
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写模板
- en: 'This stack will be equivalent to calling the `boto3` API we studied in [*Chapter
    11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine Learning
    Models*: `create_model()`, `create_endpoint_configuration()`, and `create_endpoint()`.
    Accordingly, we''ll define three CloudFormation resources (a model, an endpoint
    configuration, and an endpoint) and their parameters:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个堆栈等同于我们在 [*第11章*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237)《*部署机器学习模型*》中学习的调用
    `boto3` API：`create_model()`、`create_endpoint_configuration()` 和 `create_endpoint()`。因此，我们将定义三个
    CloudFormation 资源（一个模型，一个端点配置和一个端点）及其参数：
- en: 'Creating a new YAML file named `endpoint-one-model.yml`, we first define the
    input parameters for the stack in the `Parameters` section. Each parameter has
    a name, a description, and a type. Optionally, we can provide default values:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `endpoint-one-model.yml` 的新 YAML 文件，我们首先在 `Parameters` 部分定义堆栈的输入参数。每个参数都有名称、描述和类型。我们还可以选择提供默认值：
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the `Resources` section, we define a model resource, using the `Ref` built-in
    function to reference the appropriate input parameters:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Resources` 部分，我们定义一个模型资源，使用 `Ref` 内置函数来引用适当的输入参数：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then define an endpoint configuration resource. We use the `GetAtt` built-in
    function to get the name of the model resource. Of course, this requires that
    the model resource already exists, and CloudFormation will make sure that resources
    are created in the right order:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个端点配置资源。我们使用 `GetAtt` 内置函数来获取模型资源的名称。当然，这要求模型资源已经存在，CloudFormation 会确保资源按正确的顺序创建：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we define an endpoint resource. Likewise, we use `GetAtt` to find
    the name of the endpoint configuration:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们定义一个端点资源。同样，我们使用 `GetAtt` 来获取端点配置的名称：
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the `Outputs` section, we return the CloudFormation identifier of the endpoint,
    as well as its name:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Outputs` 部分，我们返回端点的 CloudFormation 标识符以及其名称：
- en: '[PRE4]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that the template is complete (`endpoint-one-model.yml`), we can create
    a stack.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模板已经完成（`endpoint-one-model.yml`），我们可以创建堆栈。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please make sure that your IAM role has permission to invoke CloudFormation
    APIs. If not, please add the `AWSCloudFormationFullAccess` managed policy to the
    role.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您的 IAM 角色有权限调用 CloudFormation API。如果没有，请将 `AWSCloudFormationFullAccess` 管理策略添加到该角色中。
- en: Deploying a model to a real-time endpoint
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型部署到实时端点
- en: 'Let''s use the `boto3` API to create a stack deploying a **TensorFlow** model.
    We''ll reuse a model trained with **Keras** on **Fashion MNIST**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `boto3` API 创建一个堆栈，部署一个 **TensorFlow** 模型。我们将重用一个用 **Keras** 在 **Fashion
    MNIST** 上训练的模型：
- en: Note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As our template is completely region-independent, you can use any region that
    you want. Just make sure that you have trained a model there, and that you're
    using the appropriate container image.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模板完全不依赖于区域，您可以选择任何您想要的区域。只需确保您已经在该区域训练了模型，并且正在使用适当的容器镜像。
- en: 'We''ll need `boto3` clients for SageMaker and CloudFormation:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要 `boto3` 客户端来访问 SageMaker 和 CloudFormation：
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We describe the training job to find the location of its artifact, and its
    execution role:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们描述了训练任务以查找其工件的位置，以及其执行角色：
- en: '[PRE6]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We set the container to use for deployment. In some cases, this is unnecessary,
    as the same container is used for training and deployment. For **TensorFlow**
    and other frameworks, SageMaker uses two different containers. You can find more
    information at [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md):'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了用于部署的容器。在某些情况下，这是不必要的，因为相同的容器既用于训练也用于部署。对于 **TensorFlow** 和其他框架，SageMaker
    使用两个不同的容器。您可以在 [https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    上找到更多信息：
- en: '[PRE7]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we read our template, create a new stack, and pass the required parameters:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取模板，创建一个新堆栈，并传递所需的参数：
- en: '[PRE8]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Jumping to the CloudFormation console, we see that the stack is being created,
    as shown in the following screenshot. Notice that resources are created in the
    right order: model, endpoint configuration, and endpoint:![Figure 12.1 – Viewing
    stack creation'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳转到 CloudFormation 控制台，我们看到堆栈正在创建，如下图所示。请注意，资源是按正确的顺序创建的：模型、端点配置和端点：![图 12.1
    – 查看堆栈创建
- en: '](img/B17705_12_1.jpg)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_1.jpg)'
- en: Figure 12.1 – Viewing stack creation
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.1 – 查看堆栈创建
- en: 'As we would expect, we also see the endpoint in SageMaker Studio, as shown
    in the following screenshot:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，我们也可以在 SageMaker Studio 中看到端点，如下图所示：
- en: '![Figure 12.2 – Viewing endpoint creation'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.2 – 查看端点创建'
- en: '](img/B17705_12_2.jpg)'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_2.jpg)'
- en: Figure 12.2 – Viewing endpoint creation
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.2 – 查看端点创建
- en: 'Once the stack creation is complete, we can use its output to find the name
    of the endpoint:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦堆栈创建完成，我们可以使用其输出查找端点的名称：
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This prints out the stack status and the endpoint name autogenerated by CloudFormation:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出堆栈状态以及 CloudFormation 自动生成的端点名称：
- en: '**CREATE_COMPLETE**'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**CREATE_COMPLETE**'
- en: '**Endpoint-MTaOIs4Vexpt**'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Endpoint-MTaOIs4Vexpt**'
- en: 'We can test the endpoint as usual. Then, we can delete the stack and its resources:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以像往常一样测试端点。然后，我们可以删除堆栈及其资源：
- en: '[PRE10]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: However, let's not delete the stack right away. Instead, we're going to update
    it using a change set.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们暂时不要删除堆栈。相反，我们将使用更改集来更新它。
- en: Modifying a stack with a change set
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用更改集修改堆栈
- en: 'Here, we''re going to update the number of instances backing the endpoint:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将更新支撑端点的实例数量：
- en: 'We create a new change set using the same template and parameters, except `InstanceCount`,
    which we set to `2`:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用相同的模板和参数创建一个新的更改集，唯一不同的是 `InstanceCount`，我们将其设置为 `2`：
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We see details on the change set in the CloudFormation console, as shown in
    the next screenshot. We could also use the `describe_change_set()` API:![Figure
    12.3 – Viewing a change set
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在 CloudFormation 控制台看到更改集的详细信息，如下图所示。我们也可以使用 `describe_change_set()` API：![图
    12.3 – 查看更改集
- en: '](img/B17705_12_3.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_3.jpg)'
- en: Figure 12.3 – Viewing a change set
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.3 – 查看更改集
- en: This tells us that the endpoint configuration and the endpoint need to be modified,
    and possibly replaced. As we already know from [*Chapter 11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237),
    *Deploying Machine Learning Models*, a new endpoint will be created and applied
    in a non-disruptive fashion to the existing endpoint.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这告诉我们端点配置和端点需要被修改，甚至可能被替换。正如我们从[*第 11 章*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237)《部署机器学习模型》所知，一个新的端点将会创建，并以非破坏性方式应用于现有的端点。
- en: Note
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: When working with CloudFormation, it's critical that you understand the **replacement
    policy** for your resources. Details are available in the documentation for each
    resource type.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用 CloudFormation 时，了解资源的 **替换策略**至关重要。每种资源类型的文档中都有详细信息。
- en: By clicking on the `execute_change_set()` API. As expected, the endpoint is
    immediately updated, as shown in the following screenshot:![Figure 12.4 – Updating
    the endpoint
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 `execute_change_set()` API。正如预期的那样，端点立即更新，如下图所示：![图 12.4 – 更新端点
- en: '](img/B17705_12_4.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_4.jpg)'
- en: Figure 12.4 – Updating the endpoint
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.4 – 更新端点
- en: Once the update is complete, we see the sequence of events in the CloudFormation
    console, as shown in the next screenshot. A new endpoint configuration has been
    created and applied to the endpoint. The previous endpoint configuration has been
    deleted:![Figure 12.5 – Updating the stack
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新完成后，我们可以在 CloudFormation 控制台中看到事件序列，如下图所示。已创建并应用了新的端点配置，之前的端点配置已被删除：![图 12.5
    – 更新堆栈
- en: '](img/B17705_12_5.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_5.jpg)'
- en: Figure 12.5 – Updating the stack
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.5 – 更新堆栈
- en: 'We can check that the endpoint is now backed by two instances:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以检查端点现在由两个实例支持：
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This prints out the number of instances backing the Production Variant:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会打印出支持生产变种的实例数量：
- en: '[PRE13]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let's keep working with change sets and add a second production variant to the
    endpoint.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续处理变更集，并向端点添加第二个生产变种。
- en: Adding a second production variant to the endpoint
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向端点添加第二个生产变种
- en: 'Our initial template only defined a single production variant. We''ll update
    it and add another one (`endpoint-two-models.yml`):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始模板只定义了一个生产变种。我们将更新它并添加另一个变种（`endpoint-two-models.yml`）：
- en: 'In the `Parameters` section, we add entries for a second model:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Parameters` 部分，我们为第二个模型添加了条目：
- en: '[PRE14]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We do the same in the `Resources` section:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 `Resources` 部分做同样的事情：
- en: '[PRE15]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Moving back to our notebook, we get information on another training job. We
    then create a change set, reading the updated template and passing all required
    parameters:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回我们的笔记本，我们获取到另一个训练任务的信息。然后我们创建一个变更集，读取更新后的模板并传递所有必需的参数：
- en: '[PRE16]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Looking at the CloudFormation console, we see the changes caused by the change
    set. Create a new model and modify the endpoint configuration and the endpoint:![Figure
    12.6 – Viewing the change set
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 CloudFormation 控制台，我们可以看到由变更集引起的变化。创建一个新模型并修改端点配置和端点：![图 12.6 – 查看变更集
- en: '](img/B17705_12_6.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_6.jpg)'
- en: Figure 12.6 – Viewing the change set
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.6 – 查看变更集
- en: 'We execute the change set. Once it''s complete, we see that the endpoint now
    supports two production variants. Note that the instance count is back to its
    initial value, as we defined it as `1` in the updated template:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们执行变更集。完成后，我们看到端点现在支持两个生产变种。注意，实例数量恢复到初始值，因为我们在更新的模板中将其定义为 `1`：
- en: '![Figure 12.7 – Viewing production variants'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.7 – 查看生产变种'
- en: '](img/B17705_12_7.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_7.jpg)'
- en: Figure 12.7 – Viewing production variants
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 – 查看生产变种
- en: The new production variant has a weight of `0`, so it won't be used for prediction.
    Let's see how we can gradually introduce it using **canary deployment**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 新的生产变种的权重为 `0`，因此不会用于预测。让我们看看如何使用 **金丝雀部署**逐步引入它。
- en: Implementing canary deployment
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现金丝雀部署
- en: Canary deployment is a popular technique for gradual application deployment
    ([https://martinfowler.com/bliki/CanaryRelease.html](https://martinfowler.com/bliki/CanaryRelease.html)),
    and it can also be used for machine learning models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署是一种流行的渐进式应用部署技术（[https://martinfowler.com/bliki/CanaryRelease.html](https://martinfowler.com/bliki/CanaryRelease.html)），也可用于机器学习模型。
- en: 'Simply put, we''ll use a series of stack updates to gradually increase the
    weight of the second production variant in 10% increments, until it completely
    replaces the first production variant. We''ll also create a CloudWatch alarm monitoring
    the latency of the second production variant – if the alarm is triggered, the
    change set will be rolled back:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们将通过一系列堆栈更新，逐步增加第二个生产变体的权重，每次增加10%，直到它完全替代第一个生产变体。我们还将创建一个CloudWatch警报，监控第二个生产变体的延迟——如果警报被触发，变更集将被回滚：
- en: 'We create a CloudWatch alarm monitoring the 60-second average latency of the
    second production variant. We set the threshold at 500 milliseconds:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个CloudWatch警报，监控第二个生产变体的60秒平均延迟。我们将阈值设置为500毫秒：
- en: '[PRE17]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We find the ARN of the alarm:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们找到警报的ARN：
- en: '[PRE18]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we loop over weights and update the stack. Change sets are unnecessary
    here, as we know exactly what''s going to happen from a resource perspective.
    We set our CloudWatch alarm as a **rollback trigger**, giving it five minutes
    to go off after each update before moving on to the next:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们循环遍历权重并更新堆栈。在这里不需要更改集，因为我们清楚地知道从资源的角度会发生什么。我们将CloudWatch警报设置为**回滚触发器**，每次更新后给它五分钟的时间触发，然后再进行下一步：
- en: '[PRE19]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That's all it takes. Pretty cool, don't you think?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些。很酷，不是吗？
- en: This cell will run for a couple of hours, so don't stop it. In another notebook,
    the next step is to start sending some traffic to the endpoint. For the sake of
    brevity, I won't include the code, which is identical to the one we used in [*Chapter
    7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending Machine Learning
    Services with Built-in Frameworks*. You'll find the notebook in the GitHub repository
    for this book (`Chapter12/cloudformation/Predict Fashion MNIST images.ipynb`).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单元格会运行几个小时，所以不要停止它。在另一个笔记本中，下一步是开始向端点发送一些流量。为了简洁起见，我不会包括代码，它与我们在[*第7章*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130)《使用内置框架扩展机器学习服务》中使用的代码相同。你可以在本书的GitHub仓库中找到该笔记本（`Chapter12/cloudformation/Predict
    Fashion MNIST images.ipynb`）。
- en: Now, all we have to do is sit back, have a cup of tea, and enjoy the fact that
    our model is being deployed safely and automatically. As endpoint updates are
    seamless, client applications won't notice a thing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要坐下来，喝杯茶，享受模型正在安全、自动地部署的事实。由于端点更新是无缝的，客户端应用程序不会察觉任何变化。
- en: 'After a couple of hours, deployment is complete. The next screenshot shows
    invocations for both variants over time. As we can see, traffic was gradually
    shifted from the first variant to the second one:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 几个小时后，部署完成。下一张截图显示了两种变体随时间的调用情况。正如我们所见，流量逐渐从第一个变体转移到第二个变体：
- en: '![Figure 12.8 – Monitoring canary deployment'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.8 – 监控金丝雀部署'
- en: '](img/B17705_12_8.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_8.jpg)'
- en: Figure 12.8 – Monitoring canary deployment
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 监控金丝雀部署
- en: 'Latency stayed well under our 500-millisecond limit, and the alarm wasn''t
    triggered, as shown in the next screenshot:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟保持在我们500毫秒的限制之下，警报没有被触发，正如下一张截图所示：
- en: '![Figure 12.9 – Viewing the CloudWatch alarm'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.9 – 查看CloudWatch警报'
- en: '](img/B17705_12_9.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_9.jpg)'
- en: Figure 12.9 – Viewing the CloudWatch alarm
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – 查看CloudWatch警报
- en: This example can serve as a starting point for your own deployments. For example,
    you could add an alarm monitoring `4xx` or `5xx` HTTP errors. You could also monitor
    a business metric directly impacted by prediction latency and accuracy, such as
    click-through rate, conversion rate, and so on. A useful thing to add would be
    an alarm notification (email, SMS, or even a Lambda function) in order to trigger
    downstream actions, should model deployment fail. The possibilities are endless!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例可以作为你自己部署的起点。例如，你可以添加一个监控`4xx`或`5xx` HTTP错误的警报。你还可以直接监控受预测延迟和准确性影响的业务指标，如点击率、转化率等。一个有用的补充是添加警报通知（电子邮件、短信，甚至Lambda函数），以便在模型部署失败时触发下游操作。可能性是无限的！
- en: When you're done, *don't forget to delete the stack*, either in the CloudFormation
    console or with the `delete_stack()` API. This will automatically clean up all
    AWS resources created by the stack.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，*别忘了删除堆栈*，可以在CloudFormation控制台中删除，也可以使用`delete_stack()`API。这样会自动清理堆栈创建的所有AWS资源。
- en: '**Blue-green deployment** is another popular technique. Let''s see how we can
    implement it on SageMaker.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**蓝绿部署**是另一种流行的技术。让我们看看如何在SageMaker上实现它。'
- en: Implementing blue-green deployment
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现蓝绿部署
- en: 'Blue-green deployment requires two production environments ([https://martinfowler.com/bliki/BlueGreenDeployment.html](https://martinfowler.com/bliki/BlueGreenDeployment.html)):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署需要两个生产环境（[https://martinfowler.com/bliki/BlueGreenDeployment.html](https://martinfowler.com/bliki/BlueGreenDeployment.html)）：
- en: The live production environment (`blue`) running version `n`
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行版本 `n` 的实时生产环境（`blue`）
- en: A copy of this environment (`green`) running version `n+1`
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行版本 `n+1` 的这个环境的副本（`green`）
- en: Let's look at two possible scenarios, which we could implement using the same
    APIs we've used for canary deployment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看两个可能的场景，这些场景可以通过我们为金丝雀发布所使用的相同 API 来实现。
- en: Implementing blue-green deployment with a single endpoint
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用单一端点实现蓝绿部署
- en: 'Starting from an existing endpoint running the current version of the model,
    we would carry out the following steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个运行当前版本模型的现有端点开始，我们将执行以下步骤：
- en: 'Create a new endpoint configuration with two production variants: one for the
    current model and one for the new model. Initial weights would be set to `1` and
    `0` respectively.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含两个生产版本的新端点配置：一个用于当前模型，一个用于新模型。初始权重分别设置为 `1` 和 `0`。
- en: Apply it to the endpoint.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其应用到端点。
- en: Run tests on the new production variant, selecting it explicitly with the `TargetVariant`
    parameter in `invoke_endpoint()`.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新生产版本上运行测试，使用 `invoke_endpoint()` 中的 `TargetVariant` 参数显式选择它。
- en: When tests are satisfactory, update weights to `0` and `1`. This will seamlessly
    switch traffic to the new model. If anything goes wrong, revert the weights to
    `1` and `0`.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当测试通过后，更新权重为 `0` 和 `1`。这将无缝地将流量切换到新模型。如果出现问题，可以将权重还原为 `1` 和 `0`。
- en: When the deployment is complete, update the endpoint to delete the first production
    variant.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，更新端点以删除第一个生产版本。
- en: This is a simple and robust solution. However, updating an endpoint takes several
    minutes, making the whole process not as quick as one may want. Let's see how
    we can fix this problem by using two endpoints.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单而可靠的解决方案。然而，更新端点需要几分钟时间，这使得整个过程不如预期那样快速。让我们看看如何通过使用两个端点来解决这个问题。
- en: Implementing blue-green deployment with two endpoints
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用两个端点实现蓝绿部署
- en: 'Starting from an existing endpoint running the current version of the model,
    we would implement the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个运行当前版本模型的现有端点开始，我们将实施以下步骤：
- en: Create a second endpoint running the new version of the model.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个运行新版本模型的第二个端点。
- en: Run tests on this new endpoint.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个新端点上运行测试。
- en: When the tests are satisfactory, switch all traffic to the new endpoint. This
    could be achieved in different ways; for example, updating a parameter in your
    business application, or updating a private DNS entry. If anything goes wrong,
    revert to the previous setting.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当测试通过后，将所有流量切换到新端点。可以通过不同方式实现此操作；例如，更新业务应用中的参数，或更新私有 DNS 条目。如果出现问题，可以恢复到先前的设置。
- en: When the deployment is complete, delete the old endpoint.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，删除旧的端点。
- en: This setup is a little more complex, but it lets you switch instantly from one
    model version to the next, both for deployments and rollbacks.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置稍微复杂一些，但它让你能够即时从一个模型版本切换到下一个，无论是部署还是回滚。
- en: CloudFormation is a fantastic tool for automation, and any time spent learning
    it will pay dividends. Yet some AWS users prefer writing code to writing templates,
    which is why we introduced the CDK.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: CloudFormation 是一个出色的自动化工具，任何学习它的时间都会得到回报。然而，一些 AWS 用户更喜欢编写代码而非编写模板，这就是我们引入
    CDK 的原因。
- en: Automating with AWS CDK
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS CDK 自动化
- en: AWS CDK is a multi-language SDK that lets you write code to define AWS infrastructure
    ([https://github.com/aws/aws-cdk](https://github.com/aws/aws-cdk)). Using the
    CDK CLI, you can then provision this infrastructure, using CloudFormation under
    the hood.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: AWS CDK 是一个多语言的 SDK，让你编写代码来定义 AWS 基础设施（[https://github.com/aws/aws-cdk](https://github.com/aws/aws-cdk)）。使用
    CDK CLI，你可以在幕后使用 CloudFormation 来配置这些基础设施。
- en: Installing the CDK
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 CDK
- en: The CDK is natively implemented with `npm` tool is installed on your machine
    ([https://www.npmjs.com/get-npm](https://www.npmjs.com/get-npm)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: CDK 是原生实现的，`npm` 工具需要安装在你的机器上（[https://www.npmjs.com/get-npm](https://www.npmjs.com/get-npm)）。
- en: 'Installing the CDK is then as simple as this:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 CDK 过程就这么简单：
- en: '[PRE20]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let's create a CDK application and deploy an endpoint.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个 CDK 应用并部署一个端点。
- en: Creating a CDK application
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个 CDK 应用
- en: 'We''ll deploy the same model that we deployed with CloudFormation. I''ll use
    Python, and you could also use **JavaScript**, **TypeScript**, **Java**, and .**NET**.
    API documentation is available at [https://docs.aws.amazon.com/cdk/api/latest/python/](https://docs.aws.amazon.com/cdk/api/latest/python/):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将部署与CloudFormation一起部署的相同模型。我将使用Python，你也可以使用**JavaScript**、**TypeScript**、**Java**和**.NET**。API文档可以在[https://docs.aws.amazon.com/cdk/api/latest/python/](https://docs.aws.amazon.com/cdk/api/latest/python/)找到：
- en: 'First, we create a Python application named `endpoint`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个名为`endpoint`的Python应用程序：
- en: '[PRE21]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This automatically creates a virtual environment, which we need to activate:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这会自动创建一个虚拟环境，我们需要激活它：
- en: '[PRE22]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This also creates a default `app.py` file for our CDK code, a `cdk.json` file
    for application configuration, and a `requirements.txt` file to install dependencies.
    Instead, we''ll use the files present in the GitHub repository:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这还会为我们的CDK代码创建一个默认的`app.py`文件，一个用于应用配置的`cdk.json`文件，以及一个用于安装依赖的`requirements.txt`文件。相反，我们将使用GitHub仓库中现有的文件：
- en: 'In the `requirements.txt` file, we install the CDK package for S3 and SageMaker.
    Each service requires a different package. For example, we would add `aws_cdk.aws_s3`
    for S3:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`requirements.txt`文件中，我们安装CDK的S3和SageMaker包。每个服务需要不同的包。例如，我们需要为S3添加`aws_cdk.aws_s3`：
- en: '[PRE23]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We then install requirements as usual:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们像往常一样安装依赖：
- en: '[PRE24]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the `cdk.json` file, we store the application context. Namely, key-value
    pairs that can be read by the application for configuration ([https://docs.aws.amazon.com/cdk/latest/guide/context.html](https://docs.aws.amazon.com/cdk/latest/guide/context.html)):'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`cdk.json`文件中，我们存储应用程序的上下文。也就是可以被应用程序读取的键值对，用于配置([https://docs.aws.amazon.com/cdk/latest/guide/context.html](https://docs.aws.amazon.com/cdk/latest/guide/context.html))：
- en: '[PRE25]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is the preferred way to pass values to your application. You should manage
    this file with version control in order to keep track of how stacks were built.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是将值传递给应用程序的首选方法。你应该使用版本控制来管理此文件，以便追踪堆栈的构建过程。
- en: 'We can view the context of our application with the `cdk context` command:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`cdk context`命令查看我们应用程序的上下文：
- en: '![Figure 12.10 – Viewing CDK context'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.10 – 查看CDK上下文'
- en: '](img/B17705_12_10.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_10.jpg)'
- en: Figure 12.10 – Viewing CDK context
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 – 查看CDK上下文
- en: Now, we need to write the actual application.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要编写实际的应用程序。
- en: Writing a CDK application
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写CDK应用程序
- en: 'All code goes in the `app.py` file, which we implement in the following steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都放在`app.py`文件中，我们将在接下来的步骤中实现：
- en: 'We import the required packages:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入所需的包：
- en: '[PRE26]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We extend the `core.Stack` class to create our own stack:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们扩展`core.Stack`类来创建我们自己的堆栈：
- en: '[PRE27]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We add a `CfnModel` object, reading the appropriate context values:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加一个`CfnModel`对象，读取适当的上下文值：
- en: '[PRE28]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We add a `CfnEndpointConfig` object, using the built-in `get_att()` function
    to associate it to the model. This creates a dependency that CloudFormation will
    use to build resources in the right order:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加一个`CfnEndpointConfig`对象，使用内置的`get_att()`函数将其与模型关联。这会创建一个依赖关系，CloudFormation将用来按正确的顺序构建资源：
- en: '[PRE29]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We add a `CfnEndpoint` object, using the built-in `get_att()` function to associate
    it to the endpoint configuration:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加一个`CfnEndpoint`对象，使用内置的`get_att()`函数将其与端点配置关联起来：
- en: '[PRE30]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we instantiate the application:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们实例化应用程序：
- en: '[PRE31]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Our code is complete!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码完成了！
- en: Deploying a CDK application
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署CDK应用程序
- en: 'We can now deploy the endpoint:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以部署端点：
- en: 'We can list the available stacks:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以列出可用的堆栈：
- en: '[PRE32]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can also see the actual CloudFormation template. It should be extremely
    similar to the template we wrote in the previous section:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以看到实际的CloudFormation模板。它应该与我们在前一节中编写的模板非常相似：
- en: '[PRE33]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Deploying the stack is equally simple, as shown in the next screenshot:![Figure
    12.11 – Deploying an endpoint
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署堆栈同样简单，如下图所示：![图12.11 – 部署端点
- en: '](img/B17705_12_11.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_11.jpg)'
- en: Figure 12.11 – Deploying an endpoint
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.11 – 部署端点
- en: Looking at CloudFormation, we see that the stack is created using a change set.
    A few minutes later, the endpoint is in service.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看CloudFormation，我们看到堆栈是使用变更集创建的。几分钟后，端点已投入使用。
- en: Editing `app.py`, we set the initial instance count to `2`. We then ask CDK
    to deploy the stack, but without executing the change set, as shown in the next
    screenshot:![Figure 12.12 – Creating a change set
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑`app.py`时，我们将初始实例数设置为`2`。然后我们要求CDK部署堆栈，但不执行变更集，如下图所示：![图12.12 – 创建变更集
- en: '](img/B17705_12_12.jpg)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_12.jpg)'
- en: Figure 12.12 – Creating a change set
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.12 – 创建变更集
- en: If we're happy with the change set, we can execute it in the CloudFormation
    console, or run the previous command again without `--no-execute`. As expected,
    and as shown in the next screenshot, the endpoint is updated:![Figure 12.13 –
    Updating the endpoint
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们对变更集感到满意，可以在 CloudFormation 控制台中执行它，或者再次运行之前的命令，去掉 `--no-execute`。如预期所示，且如下图所示，端点已更新：![图
    12.13 – 更新端点
- en: '](img/B17705_12_13.jpg)'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_13.jpg)'
- en: Figure 12.13 – Updating the endpoint
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.13 – 更新端点
- en: 'When we''re done, we can destroy the stack:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们完成时，可以销毁堆栈：
- en: '[PRE34]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the CDK is an interesting alternative to writing templates directly,
    while still benefiting from the rigor and the robustness of CloudFormation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，CDK 是一个有趣的替代方案，能够直接编写模板，同时仍然受益于 CloudFormation 的严格性和健壮性。
- en: One thing we haven't done yet is to automate an end-to-end workflow, from training
    to deployment. Let's do this with AWS Step Functions.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未完成的一件事是实现一个从训练到部署的端到端自动化工作流。我们将通过 AWS Step Functions 来完成这个任务。
- en: Building end-to-end workflows with AWS Step Functions
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS Step Functions 构建端到端工作流
- en: '**AWS Step Functions** let you define and run workflows based on **state machines**
    ([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/)).
    A state machine is a combination of steps, which can be sequential, parallel,
    or conditional. Each step receives an input from its predecessor, performs an
    operation, and passes the output to its successor. Step Functions are integrated
    with many AWS services, such as Amazon SageMaker, **AWS** **Lambda**, container
    services, **Amazon** **DynamoDB**, **Amazon** **EMR**, **AWS** **Glue**, and more.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS Step Functions** 让您基于 **状态机** 定义和运行工作流 ([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/))。状态机是步骤的组合，这些步骤可以是顺序的、并行的或条件的。每个步骤接收来自前一个步骤的输入，执行操作，然后将输出传递给下一个步骤。Step
    Functions 与许多 AWS 服务集成，如 Amazon SageMaker、**AWS** **Lambda**、容器服务、**Amazon** **DynamoDB**、**Amazon**
    **EMR**、**AWS** **Glue** 等。'
- en: State machines can be defined using JSON and the **Amazon States Language**,
    and you can visualize them in the service console. State machine execution is
    fully managed, so you don't need to provision any infrastructure to run.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 状态机可以使用 JSON 和 **Amazon States Language** 定义，您可以在服务控制台中可视化它们。状态机执行是完全托管的，因此您无需为其运行提供任何基础设施。
- en: When it comes to SageMaker, Step Functions has a dedicated Python SDK, oddly
    named the **Data Science SDK** ([https://github.com/aws/aws-step-functions-data-science-sdk-python](https://github.com/aws/aws-step-functions-data-science-sdk-python)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SageMaker，Step Functions 提供了一个专门的 Python SDK，奇怪地被命名为 **Data Science SDK**
    ([https://github.com/aws/aws-step-functions-data-science-sdk-python](https://github.com/aws/aws-step-functions-data-science-sdk-python))。
- en: Let's run an example where we automate training and deployment for a **scikit-learn**
    model trained on the **Boston Housing** dataset.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个示例，自动化训练和部署一个基于 **Boston Housing** 数据集训练的 **scikit-learn** 模型。
- en: Setting up permissions
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置权限
- en: First, please make sure that the IAM role for your user or for your notebook
    instance has permission to invoke Step Functions APIs. If not, please add the
    `AWSStepFunctionsFullAccess` managed policy to the role.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保您的用户或笔记本实例的 IAM 角色具有调用 Step Functions API 的权限。如果没有，请将 `AWSStepFunctionsFullAccess`
    管理策略添加到该角色中。
- en: 'Then, we need to create a service role for Step Functions, allowing it to invoke
    AWS APIs on our behalf:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要为 Step Functions 创建一个服务角色，允许它代表我们调用 AWS API：
- en: Starting from the IAM console ([https://console.aws.amazon.com/iam/home#/roles](https://console.aws.amazon.com/iam/home#/roles)),
    we click on **Create role**.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 IAM 控制台开始 ([https://console.aws.amazon.com/iam/home#/roles](https://console.aws.amazon.com/iam/home#/roles))，我们点击
    **创建角色**。
- en: We select **AWS service** and **Step Functions**.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择 **AWS 服务** 和 **Step Functions**。
- en: We click through the next screens until we can enter the role name. Let's call
    it `StepFunctionsWorkflowExecutionRole`, and click on **Create role**.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们点击接下来的页面，直到可以输入角色名称。我们将其命名为 `StepFunctionsWorkflowExecutionRole`，然后点击 **创建角色**。
- en: Selecting this role, we click on its **Permission** tab, then on **Add inline
    policy**.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择该角色后，我们点击它的 **权限** 标签页，再点击 **添加内联策略**。
- en: Selecting the JSON tab, we replace the empty policy with the content of the
    `Chapter12/step_functions/service-role-policy.json` file, and we click on **Review
    policy**.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 JSON 标签，我们用 `Chapter12/step_functions/service-role-policy.json` 文件的内容替换空的策略，然后点击
    **查看策略**。
- en: We name the policy `StepFunctionsWorkflowExecutionPolicy` and click on **Create
    policy**.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将策略命名为 `StepFunctionsWorkflowExecutionPolicy`，然后点击 **创建策略**。
- en: We write down the ARN on the role, and we close the IAM console.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们记下角色的 ARN，然后关闭 IAM 控制台。
- en: The setup is now complete. Now, let's create a workflow.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 设置现在已经完成。现在，让我们创建一个工作流。
- en: Implementing our first workflow
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现我们的第一个工作流
- en: 'In this workflow, we''ll go through the following step sequence: train the
    model, create it, use it for a batch transform, create an endpoint configuration,
    and deploy the model to an endpoint:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个工作流中，我们将按以下步骤顺序进行：训练模型、创建模型、使用模型进行批量转换、创建端点配置，并将模型部署到端点：
- en: 'We upload the training set to S3, as well as a test set where we removed the
    target attribute. We''ll use the latter for a batch transform:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将训练集上传到S3，以及一个去除目标属性的测试集。我们将使用后者进行批量转换：
- en: '[PRE35]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We configure our estimator as usual:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像往常一样配置估算器：
- en: '[PRE36]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We also define the transformer that we''ll use for batch transform:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还定义了用于批量转换的transformer：
- en: '[PRE37]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We import the Step Functions objects required by the workflow. You can find
    the API documentation at [https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/):'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入工作流所需的Step Functions对象。您可以在[https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/)找到API文档：
- en: '[PRE38]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We define the input of the workflow. We''ll pass it a training job name, a
    model name, and an endpoint name:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义工作流的输入。我们将传递给它一个训练任务名称、一个模型名称和一个端点名称：
- en: '[PRE39]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first step of the workflow is the training step. We pass it the estimator,
    the location of the dataset in S3, and a training job name:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作流的第一步是训练步骤。我们将其传递估算器、数据集在S3中的位置以及一个训练任务名称：
- en: '[PRE40]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The next step is the model creation step. We pass it the location of the model
    trained in the previous step, and a model name:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是模型创建步骤。我们将其传递已在前一步中训练的模型位置和模型名称：
- en: '[PRE41]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The next step is running a batch transform on the test dataset. We pass the
    `transformer` object, the location of the test dataset in S3, and its content
    type:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是在测试数据集上运行批量转换。我们传递`transformer`对象、测试数据集在S3中的位置及其内容类型：
- en: '[PRE42]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The next step is creating the endpoint configuration:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建端点配置：
- en: '[PRE43]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The last step is creating the endpoint:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是创建端点：
- en: '[PRE44]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now that all steps have been defined, we chain them in sequential order:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，所有步骤都已经定义，我们按顺序将它们链起来：
- en: '[PRE45]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We now build our workflow, using the workflow definition and the input definition:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在构建我们的工作流，使用工作流定义和输入定义：
- en: '[PRE46]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can visualize the state machine, an easy way to check that we built it as
    expected, as shown in the next screenshot:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以可视化状态机，这是检查我们是否按预期构建它的一个简便方法，如下图所示：
- en: '[PRE47]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Figure 12.14 – Viewing the state machine'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.14 – 查看状态机'
- en: '](img/B17705_12_14.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_14.jpg)'
- en: Figure 12.14 – Viewing the state machine
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.14 – 查看状态机
- en: '­­We create the workflow:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建工作流：
- en: '[PRE48]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: It's visible in the Step Functions console, as shown in the following screenshot.
    We can see both its graphical representation and its JSON definition based on
    the Amazon States Language. We could edit the workflow as well if needed:![Figure
    12.15 – Viewing the state machine in the console
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以在Step Functions控制台中查看，如下图所示。我们可以看到它的图形表示以及基于Amazon States Language的JSON定义。如果需要，我们也可以编辑工作流：![图12.15
    – 在控制台中查看状态机
- en: '](img/B17705_12_15.jpg)'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_15.jpg)'
- en: Figure 12.15 – Viewing the state machine in the console
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.15 – 在控制台中查看状态机
- en: 'We run the workflow:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行工作流：
- en: '[PRE49]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We can track its progress with `render_progress()` and the `list_events()` API.
    We can also see it in the console, as shown in the next screenshot. Note that
    we also see the input and output of each step, which is a great way to troubleshoot
    problems:![Figure 12.16 – Running the state machine
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过`render_progress()`和`list_events()`API跟踪其进度。我们也可以在控制台中看到它，如下图所示。请注意，我们还可以看到每个步骤的输入和输出，这对故障排除问题非常有帮助：![图12.16
    – 运行状态机
- en: '](img/B17705_12_16.jpg)'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_16.jpg)'
- en: Figure 12.16 – Running the state machine
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.16 – 运行状态机
- en: When the workflow is complete, you can test the endpoint as usual. *Don't forget
    to delete it in the SageMaker console when you're done*.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当工作流完成时，您可以像往常一样测试端点。*完成后别忘了在SageMaker控制台删除它*。
- en: This example shows how simple it is to build a SageMaker workflow with this
    SDK. Still, we could improve it by running batch transform and endpoint creation
    in parallel.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了使用这个SDK构建SageMaker工作流是多么简单。不过，我们可以通过让批量转换和端点创建并行运行来进一步改进它。
- en: Adding parallel execution to a workflow
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向工作流添加并行执行
- en: 'The next screenshot shows the workflow we''re going to build. The steps themselves
    are exactly the same. We''re only going to modify the way they''re chained:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下一张截图展示了我们将要构建的工作流。这些步骤本身完全相同，我们只是修改了它们的连接方式：
- en: '![Figure 12.17 – Viewing the parallel state machine'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.17 – 查看并行状态机'
- en: '](img/B17705_12_17.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_17.jpg)'
- en: Figure 12.17 – Viewing the parallel state machine
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.17 – 查看并行状态机
- en: 'We will get started using the following steps:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤开始：
- en: 'Our workflow has two branches – one for batch transform and one for the endpoint:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的工作流有两个分支—一个用于批量转换，另一个用于端点：
- en: '[PRE50]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We create a `Parallel` step in order to allow parallel execution of these two
    branches:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个 `Parallel` 步骤，以便允许这两个分支并行执行：
- en: '[PRE51]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We put everything together:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将所有内容整合在一起：
- en: '[PRE52]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: That's it! We can now create and run this workflow just like in the previous
    example.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在我们可以像在之前的示例中那样创建并运行这个工作流。
- en: Looking at the Step Functions console, we see that the workflow does run the
    two branches in parallel. There is a minor problem, however. The endpoint creation
    step is shown as complete, although the endpoint is still being created. You can
    see in the SageMaker console that the endpoint is listed as `Creating`. This could
    cause a problem if a client application tried to invoke the endpoint right after
    the workflow completes.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Step Functions 控制台，我们可以看到工作流确实并行执行了两个分支。不过，存在一个小问题。端点创建步骤显示为已完成，尽管端点仍在创建中。你可以在
    SageMaker 控制台中看到该端点的状态为 `Creating`。如果客户端应用程序在工作流完成后立即尝试调用该端点，可能会出现问题。
- en: Let's improve this by adding an extra step, waiting for the endpoint to be in
    service. We can easily do this with a Lambda function, allowing us to run our
    own code anywhere in a workflow.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过增加一个额外的步骤来改进这个流程，等待端点进入服务状态。我们可以通过 Lambda 函数轻松实现这一点，从而让我们在工作流中的任何位置运行自己的代码。
- en: Adding a Lambda function to a workflow
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Lambda 函数添加到工作流
- en: If you've never looked at **AWS** **Lambda** ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda)),
    you're missing out! Lambda is at the core of serverless architectures, where you
    can write and deploy short functions running on fully managed infrastructure.
    These functions can be triggered by all sorts of AWS events, and they can also
    be invoked on demand.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从未了解过 **AWS** **Lambda** ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda))，那你就错过了！Lambda
    是无服务器架构的核心，在这种架构下，你可以编写并部署在完全托管基础设施上运行的短小函数。这些函数可以由各种 AWS 事件触发，也可以按需调用。
- en: Setting up permissions
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置权限
- en: 'Creating a Lambda function is simple. The only prerequisite is to create an
    `DescribeEndpoint` API, as well as permission to create a log in CloudWatch. Let''s
    use the `boto3` API for this. You can find more information at [https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html](https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Lambda 函数很简单。唯一的前提是创建一个 `DescribeEndpoint` API，并且有权限在 CloudWatch 中创建日志。我们将使用
    `boto3` API 来实现这一点。你可以在 [https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html](https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html)
    查找更多信息：
- en: 'We first define a **trust policy** for the role, allowing it to be assumed
    by the Lambda service:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先为角色定义一个 **信任策略**，允许 Lambda 服务假设该角色：
- en: '[PRE53]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We create a role and attach the trust policy to it:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个角色并附加信任策略：
- en: '[PRE54]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We define a policy listing the APIs that are allowed:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个列出允许 API 的策略：
- en: '[PRE55]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We create the policy and add it to the role:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建策略并将其添加到角色中：
- en: '[PRE56]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The IAM setup is now complete.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: IAM 设置现已完成。
- en: Writing a Lambda function
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写 Lambda 函数
- en: 'We can now write a short Lambda function. It receives a JSON event as input,
    which stores the ARN of the endpoint being created by the `EndpointStep` step.
    It simply extracts the endpoint name from the ARN, creates a `boto3` waiter, and
    waits until the endpoint is in service. The following screenshot shows the code
    in the Lambda console:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写一个简短的 Lambda 函数。它接收一个 JSON 事件作为输入，事件中存储了 `EndpointStep` 步骤所创建的端点的 ARN。它简单地从
    ARN 中提取端点名称，创建一个 `boto3` 等待器，并等待直到端点服务就绪。下面的截图展示了 Lambda 控制台中的代码：
- en: '![Figure 12.18 – Our Lambda function'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.18 – 我们的 Lambda 函数'
- en: '](img/B17705_12_18.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_18.jpg)'
- en: Figure 12.18 – Our Lambda function
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.18 – 我们的 Lambda 函数
- en: 'Let''s deploy this function:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署这个函数：
- en: 'We create a deployment package for the Lambda function and upload it to S3:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为 Lambda 函数创建一个部署包并将其上传到 S3：
- en: '[PRE57]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We create the function with a timeout of 15 minutes, the longest possible runtime
    for a Lambda function. Endpoints are typically deployed in less than 10 minutes,
    so this should be more than enough:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个超时为 15 分钟的函数，这是 Lambda 函数的最大运行时间。端点通常在不到 10 分钟的时间内部署完成，因此这个时间应该足够：
- en: '[PRE58]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now that the Lambda function has been created, we can easily add it to the
    existing workflow. We define a `LambdaStep` and add it to the endpoint branch.
    Its payload is the endpoint ARN, extracted from the output of the `EndpointStep`:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在 Lambda 函数已经创建完成，我们可以轻松地将其添加到现有的工作流中。我们定义一个 `LambdaStep` 并将其添加到端点分支。它的有效负载是从
    `EndpointStep` 输出中提取的端点 ARN：
- en: '[PRE59]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Running the workflow again, we see in the following screenshot that this new
    step receives the endpoint ARN as input and waits for the endpoint to be in service:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行工作流，我们在下面的截图中看到，新的步骤接收端点 ARN 作为输入，并等待端点处于服务状态：
- en: '![Figure 12.19 – Running the state machine with Lambda'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.19 – 使用 Lambda 运行状态机'
- en: '](img/B17705_12_19.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_19.jpg)'
- en: Figure 12.19 – Running the state machine with Lambda
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.19 – 使用 Lambda 运行状态机
- en: There are many other ways you can use Lambda functions with SageMaker. You can
    extract training metrics, predict test sets on an endpoint, and more. The possibilities
    are endless.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过多种方式将 Lambda 函数与 SageMaker 一起使用。您可以提取训练指标、在端点上预测测试集等。可能性无穷无尽。
- en: Now, let's automate end-to-end workflows with Amazon SageMaker Pipelines.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Amazon SageMaker Pipelines 自动化端到端工作流。
- en: Building end-to-end workflows with Amazon SageMaker Pipelines
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Pipelines 构建端到端工作流
- en: '**Amazon SageMaker Pipelines** lets us create and run end-to-end machine learning
    **workflows** based on SageMaker steps for training, tuning, batch transform,
    and processing scripts, using SageMaker APIs SDK that are very similar to the
    ones we used in Step Functions.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon SageMaker Pipelines** 允许我们基于 SageMaker 步骤（用于训练、调优、批处理转换和处理脚本）创建和运行端到端的机器学习
    **工作流**，使用的 SageMaker API SDK 与我们在 Step Functions 中使用的非常相似。'
- en: 'Compared to Step Functions, SageMaker Pipelines adds the following features:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Step Functions 相比，SageMaker Pipelines 增加了以下功能：
- en: The ability to write, run, visualize and manage your workflows directly in SageMaker
    Studio, without having to jump to the AWS console.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在 SageMaker Studio 中直接编写、运行、可视化和管理工作流，而无需跳转到 AWS 控制台。
- en: A **model registry**, which makes it easier to manage model versions, deploy
    only approved versions, and track **lineage**.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册表**，使得管理模型版本、仅部署经过批准的版本以及跟踪 **血统** 更加容易。'
- en: '**MLOps templates** – a collection of CloudFormation templates published via
    **AWS Service Catalog** that help you automate the deployment of your models.
    Built-in templates are provided, and you can add your own. You (or your Ops team)
    can learn more at [https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html).'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLOps 模板** – 通过 **AWS 服务目录** 发布的一组 CloudFormation 模板，帮助您自动化模型的部署。提供了内置模板，您还可以添加自己的模板。您（或您的运维团队）可以在
    [https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html)
    上了解更多信息。'
- en: Note
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: One thing that SageMaker Pipelines lacks is integration with other AWS services.
    At the time of writing, SageMaker Pipelines only supports **SQS**, whereas Step
    Functions supports many compute and big data services. With SageMaker Pipelines,
    the assumption is either that your training data has already been processed, or
    that you'll process it with SageMaker Processing steps.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SageMaker Pipelines 的一个缺点是缺少与其他 AWS 服务的集成。在撰写本文时，SageMaker Pipelines 仅支持 **SQS**，而
    Step Functions 支持许多计算和大数据服务。在 SageMaker Pipelines 中，假设您的训练数据已经处理好，或者您将使用 SageMaker
    Processing 步骤来处理数据。
- en: 'Now that we know what SageMaker Pipelines is, let''s run a complete example
    based on the Amazon Reviews dataset and the BlazingText algorithm we used in [*Chapter
    6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training Natural Language
    Processing Models*, and [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Advanced Training Techniques*, putting together many of the services we learned
    about so far. Our pipeline will contain the following steps:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了 SageMaker Pipelines，接下来让我们基于 Amazon Reviews 数据集和我们在 [*第 6 章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)
    *训练自然语言处理模型* 和 [*第 10 章*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206) *高级训练技术*
    中使用的 BlazingText 算法，运行一个完整的示例，结合到目前为止学到的许多服务。我们的工作流将包含以下步骤：
- en: A processing step, where we prepare the dataset with **SageMaker Processing**.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理步骤，我们使用 **SageMaker Processing** 准备数据集。
- en: An ingestion step, where we load the processed data set in **SageMaker Feature
    Store**.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据摄取步骤，在此步骤中我们将处理过的数据集加载到 **SageMaker Feature Store** 中。
- en: A dataset building step, where we use **Amazon Athena** to query the offline
    store and save a dataset to S3.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数据集构建步骤，在此步骤中我们使用 **Amazon Athena** 查询离线存储并将数据集保存到 S3。
- en: A training step, where we train a BlazingText model on the dataset.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个训练步骤，在此步骤中我们在数据集上训练 BlazingText 模型。
- en: A model creation step, where we save the trained model as a SageMaker model.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型创建步骤，在此步骤中我们将训练好的模型保存为 SageMaker 模型。
- en: A model registration step, where we add the model to the SageMaker Pipelines
    model registry.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型注册步骤，在此步骤中我们将模型添加到 SageMaker Pipelines 模型注册表中。
- en: In real life, you should not initially worry about automation. You should first
    experiment with Jupyter Notebooks and iterate on all these steps. Then, as the
    project matures, you should start automating each step, eventually assembling
    them as a pipeline.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，你一开始不需要过于担心自动化问题。你应该首先使用 Jupyter Notebooks 进行实验，并迭代这些步骤。然后，随着项目的成熟，你可以开始自动化每个步骤，最终将它们组装成一个管道。
- en: My recommendation is to first automate each processing step, with individual
    SageMaker Processing jobs. Not only will this come in handy in the development
    phase, but it will also create a simple and step-by-step path to full automation.
    Indeed, once steps run fine with SageMaker Processing, it takes little effort
    to combine them with SageMaker Pipelines. In fact, you can use the exact same
    Python script. You'll only have to write code with the Pipelines SDK. As you'll
    see in a minute, it's very similar to the Processing SDK.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是首先自动化每个处理步骤，使用独立的 SageMaker Processing 任务。这不仅在开发阶段非常有用，而且会为完全自动化创建一个简单且逐步的路径。事实上，一旦步骤在
    SageMaker Processing 上顺利运行，将它们与 SageMaker Pipelines 结合起来几乎不需要额外的努力。实际上，你可以使用完全相同的
    Python 脚本。你只需要用 Pipelines SDK 编写代码。正如你将看到的，它与 Processing SDK 非常相似。
- en: This is the approach I've followed with the following example. In the GitHub
    repository, you'll find SageMaker Processing notebooks for the data processing,
    ingestion, and dataset building steps, as well as another notebook for the end-to-end
    workflow. Here, we'll focus on the latter. Let's get started!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在以下示例中采用的方法。在 GitHub 仓库中，你会找到用于数据处理、数据摄取和数据集构建步骤的 SageMaker Processing 笔记本，以及另一本用于端到端工作流的笔记本。在这里，我们将重点关注后者。让我们开始吧！
- en: Defining workflow parameters
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义工作流参数
- en: Just like CloudFormation templates, you can (and should) define parameters in
    your workflows. This makes them easier to reuse in other projects. Parameters
    can be strings, integers, and floats, with an optional default value.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 CloudFormation 模板一样，你可以（并且应该）在工作流中定义参数。这样可以更方便地在其他项目中重用它们。参数可以是字符串、整数和浮点数，并可以设置一个可选的默认值。
- en: 'We create parameters for the AWS region and for the instances we''d like to
    use for processing and training:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为 AWS 区域以及我们希望用于处理和训练的实例创建参数：
- en: '[PRE60]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We also create parameters for the location of input data, the model name, and
    the model status to set in the model registry (more on this later).
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还为输入数据的位置、模型名称以及在模型注册表中设置的模型状态创建了参数（稍后会详细介绍）。
- en: '[PRE61]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now, let's define the data processing step.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义数据处理步骤。
- en: Processing the dataset with SageMaker Processing
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Processing 处理数据集
- en: We reuse the processing script we wrote in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)
    (`preprocessing.py`).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重用了在 [*第6章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108) 中编写的处理脚本（`preprocessing.py`）。
- en: 'We create a `SKLearnProcessor` object with the parameters we just defined:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个 `SKLearnProcessor` 对象，并使用我们刚才定义的参数：
- en: '[PRE62]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We then define the data processing step. Remember that it creates two outputs:
    one in BlazingText format, and one for ingestion in SageMaker Feature Store. As
    mentioned earlier, the SageMaker Pipelines syntax is extremely close to the SageMaker
    Processing syntax (inputs, outputs, and arguments).'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义数据处理步骤。请记住，它会创建两个输出：一个是 BlazingText 格式，另一个是用于摄取到 SageMaker Feature Store
    的格式。如前所述，SageMaker Pipelines 语法与 SageMaker Processing 语法非常相似（输入、输出和参数）。
- en: '[PRE63]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now, let's define the ingestion step.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义数据摄取步骤。
- en: Ingesting the dataset in SageMaker Feature Store with SageMaker Processing
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker Processing 将数据集摄取到 SageMaker Feature Store 中
- en: We reuse the processing script we wrote in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206)
    (`ingesting.py`).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重用了在 [*第10章*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206) 中编写的处理脚本（`ingesting.py`）。
- en: 'We first define a name for the feature group:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先为特征组定义一个名称：
- en: '[PRE64]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We then define a processing step, setting the data input to the output of the
    first processing job. To illustrate step chaining, we define an output pointing
    to a file saved by the script, which contains the name of the feature group:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了一个处理步骤，将数据输入设置为第一个处理作业的输出。为了演示步骤链式处理，我们定义了一个输出，指向由脚本保存的文件，该文件包含特征组的名称：
- en: '[PRE65]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Now, let's take care of the dataset building step.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们处理数据集构建步骤。
- en: Building a dataset with Amazon Athena and SageMaker Processing
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon Athena 和 SageMaker 处理构建数据集
- en: We reuse the processing script we wrote in [*Chapter 10*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206)
    (`querying.py`).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重用了在 [*第 10 章*](B17705_10_Final_JM_ePub.xhtml#_idTextAnchor206) 中编写的处理脚本 (`querying.py`)。
- en: 'We set the input to the output of the ingestion step, in order to retrieve
    the name of the feature group. We also define two outputs for the training and
    validation datasets:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入设置为摄取步骤的输出，以便检索特征组的名称。我们还为训练集和验证集数据集定义了两个输出：
- en: '[PRE66]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now, let's move on to the training step.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行训练步骤。
- en: Training a model
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'No surprises here:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 没有意外：
- en: 'We define an `Estimator` module for this job:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为这个任务定义了一个 `Estimator` 模块：
- en: '[PRE67]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We then define the training step, passing the training and validation datasets
    as inputs:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接着定义训练步骤，将训练集和验证集数据集作为输入：
- en: '[PRE68]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Now, let's take care of the model creation and model registration steps (the
    last ones in the pipeline).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们处理模型创建和模型注册步骤（管道中的最后几个步骤）。
- en: Creating and registering a model in SageMaker Pipelines
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker Pipelines 中创建并注册模型
- en: Once the model has been trained, we need to create it as a SageMaker model and
    register it in the model registry.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们需要将其创建为 SageMaker 模型并在模型注册表中注册它。
- en: 'We create the model, passing the location of the training container and of
    the model artifact:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建模型，传递训练容器和模型工件的位置：
- en: '[PRE69]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We then register the model in the model registry, passing the list of allowed
    instance types for deployment, as well as the approval status. We associate it
    to a model package group that will hold this model, as well as further versions
    we train in the future:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将模型注册到模型注册表中，传递允许的实例类型列表以供部署，以及审批状态。我们将其关联到一个模型包组，该组将保存此模型以及我们未来训练的更多版本：
- en: '[PRE70]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: All the steps are now defined, so let's assemble them in a pipeline.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 所有步骤现在都已定义，让我们将它们组合成一个管道。
- en: Creating a pipeline
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建管道
- en: 'We simply put together all the steps and their parameters. Then, we create
    the pipeline (or update it if it existed previously):'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将所有步骤和它们的参数组合在一起。然后，我们创建管道（如果之前已存在，则更新它）：
- en: '[PRE71]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We're all set. Let's run our pipeline!
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪。让我们运行管道吧！
- en: Running a pipeline
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行管道
- en: 'It takes a single line of code to fire up a pipeline execution:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 只需一行代码即可启动管道执行：
- en: 'We assign values to the data location and model name parameters (the other
    ones have default values):'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为数据位置和模型名称参数分配值（其他参数使用默认值）：
- en: '[PRE72]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In SageMaker Studio, we go **SageMaker resources** / **Pipelines**, and we see
    the pipeline executing, as shown in the next screenshot:![Figure 12.20 – Executing
    a pipeline
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 SageMaker Studio 中，我们进入 **SageMaker 资源** / **管道**，并看到管道正在执行，如下图所示：![图 12.20
    – 执行管道
- en: '](img/B17705_12_20.jpg)'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_20.jpg)'
- en: Figure 12.20 – Executing a pipeline
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.20 – 执行管道
- en: 'After an hour and a half, the pipeline is complete, as shown in the next screenshot:'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个半小时后，管道完成，如下图所示：
- en: '![Figure 12.21 – Visualizing a pipeline'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.21 – 可视化管道'
- en: '](img/B17705_12_21.jpg)'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_12_21.jpg)'
- en: Figure 12.21 – Visualizing a pipeline
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.21 – 可视化管道
- en: 'Finally, for each step of the pipeline, we can see the lineage of all artifacts:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，对于管道中的每个步骤，我们都可以看到所有工件的血缘关系：
- en: '[PRE73]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'For example, the output for the training step is shown in the next image. We
    see exactly which datasets and which container were used to train the model:'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，训练步骤的输出在下图中展示。我们可以清楚地看到使用了哪些数据集和容器来训练模型：
- en: '![Figure 12.22 – Viewing the lineage for the training step'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.22 – 查看训练步骤的血缘关系'
- en: '](img/B17705_12_22.jpg)'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_22.jpg)'
- en: Figure 12.22 – Viewing the lineage for the training step
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.22 – 查看训练步骤的血缘关系
- en: Let's see how we can deploy this model.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何部署这个模型。
- en: Deploying a model from the model registry
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从模型注册表部署模型
- en: 'Going to **SageMaker resources** / **Model registry**, we also see that the
    model has been registered in the model registry, as shown in the next screenshot.
    If we train further versions of the model, they will also appear here:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 **SageMaker 资源** / **模型注册表**，我们还可以看到该模型已经在模型注册表中注册，如下图所示。如果我们训练了更多版本的模型，它们也会出现在这里：
- en: '![Figure 12.23 – Viewing a model in the model registry'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.23 – 在模型注册表中查看模型'
- en: '](img/B17705_12_23.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_12_23.jpg)'
- en: Figure 12.23 – Viewing a model in the model registry
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.23 – 在模型注册表中查看模型
- en: As its status is `Pending`, it can't be deployed for now. We need to change
    it to `Approved` in order to allow deployment. This is a safe way to guarantee
    that only good models are deployed, once all appropriate tests have been performed.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它的状态是 `Pending`，因此暂时无法部署。我们需要将其更改为 `Approved`，以便允许部署。这是一种安全的方式，可以确保只有经过所有适当测试的优质模型被部署。
- en: We right-click on the model and select `Approved`. We also note the model ARN,
    which is visible in the **Settings** tab.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 我们右键点击模型并选择 `Approved`。我们还注意到模型 ARN，它在 **设置** 标签中是可见的。
- en: 'Now, we can deploy and test the model:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以部署并测试模型：
- en: 'Back in our Jupyter Notebook, we create a `ModelPackage` object pointing at
    the model version we''d like to deploy:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的 Jupyter Notebook 中，我们创建一个指向我们想要部署的模型版本的 `ModelPackage` 对象：
- en: '[PRE74]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We call `deploy()` as usual:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像往常一样调用 `deploy()`：
- en: '[PRE75]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We create a `Predictor` and send a test sample for prediction:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个 `Predictor` 并发送一个测试样本进行预测：
- en: '[PRE76]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This prints out the probabilities for all three classes:'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会打印出所有三个类别的概率：
- en: '[PRE77]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Once we're done, we can delete the endpoint.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以删除终端节点。
- en: Note
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: For a full clean-up, you should also delete the pipeline, the feature store,
    and the model package group. You'll find a clean-up notebook in the GitHub repository.
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了完全清理，你还应该删除管道、特征存储和模型包组。你可以在 GitHub 仓库中找到清理的笔记本。
- en: As you can see, SageMaker Pipelines provides you with robust and powerful tools
    to build, run, and track end-to-end machine learning workflows. These tools are
    nicely integrated in SageMaker Studio, which should help you to be more productive
    and get high-quality models in production quicker
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，SageMaker Pipelines 为你提供了强大且高效的工具，帮助你构建、运行和跟踪端到端的机器学习工作流。这些工具在 SageMaker
    Studio 中得到了很好的集成，应该能帮助你提高生产力，并更快地将高质量模型投入生产。
- en: Summary
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you first learned how to deploy and update endpoints with AWS
    CloudFormation. You also saw how it can be used to implement canary deployment
    and blue-green deployment.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你首先学习了如何使用 AWS CloudFormation 部署和更新终端节点。你还看到了它如何用于实现金丝雀部署和蓝绿部署。
- en: Then, you learned about the AWS CDK, an SDK specifically built to easily generate
    and deploy CloudFormation templates using a variety of programming languages.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你了解了 AWS CDK，这是一个专门为使用多种编程语言轻松生成和部署 CloudFormation 模板而构建的 SDK。
- en: Finally, you built complete end-to-end machine learning workflows with AWS Step
    Functions and Amazon SageMaker Pipelines.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你通过 AWS Step Functions 和 Amazon SageMaker Pipelines 构建了完整的端到端机器学习工作流。
- en: In the next and final chapter, you'll learn about additional SageMaker capabilities
    that help you optimize the cost and performance of predictions.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章也是最后一章中，你将了解更多关于 SageMaker 的功能，帮助你优化预测的成本和性能。
