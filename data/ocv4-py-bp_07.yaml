- en: Learning to Recognize Traffic Signs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习识别交通标志
- en: We have previously studied how to describe objects by means of key points and
    features, and how to find the correspondence points in two different images of
    the same physical object. However, our previous approaches were rather limited
    when it came to recognizing objects in real-world settings and assigning them
    to conceptual categories. For example, in [Chapter 2](0bab4fd2-6330-47d2-a2b2-339e8f879ed7.xhtml), *Hand
    Gesture Recognition Using a Kinect Depth Sensor*, the required object in the image
    was a hand, and it had to be nicely placed in the center of the screen. Wouldn't
    it be nice if we could remove these restrictions?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前研究了如何通过关键点和特征来描述对象，以及如何在两个不同图像中找到同一物理对象的对应点。然而，我们之前的方法在识别现实世界中的对象并将它们分配到概念类别方面相当有限。例如，在[第2章](0bab4fd2-6330-47d2-a2b2-339e8f879ed7.xhtml)，*使用Kinect深度传感器进行手势识别*中，图像中所需的对象是一只手，并且它必须放置在屏幕中央。如果我们可以去除这些限制会更好吗？
- en: 'The goal of this chapter is to train a **multiclass** **classifier** to recognize
    traffic signs. In this chapter, we will cover the following concepts:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是训练一个**多类****分类器**来识别交通标志。在本章中，我们将涵盖以下概念：
- en: Planning the app
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划应用
- en: Briefing on supervised learning concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习概念的概述
- en: Understanding the **German Traffic Sign Recognition** **Benchmark** (**GTSRB**)
    dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解**德国交通标志识别**基准数据集（**GTSRB**）
- en: Learning about dataset feature extraction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解数据集特征提取
- en: Learning about **support vector machines** (**SVMs**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解**支持向量机**（**SVMs**）
- en: Putting it all together
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: Improving results with neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络提高结果
- en: In this chapter, you will learn how to apply machine learning models to real-world
    problems. You will learn how to use already available datasets for training models.
    You will also learn how to use SVMs for multiclass classification and how to train,
    test, and improve machine learning algorithms provided with OpenCV to achieve
    real-world tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何将机器学习模型应用于现实世界的问题。你将学习如何使用现有的数据集来训练模型。你还将学习如何使用SVMs进行多类分类，以及如何使用OpenCV提供的机器学习算法进行训练、测试和改进，以实现现实世界任务。
- en: We will train an SVM to recognize all sorts of traffic signs. Although SVMs
    are binary classifiers (that is, they can be used to learn, at most, two categories—positives
    and negatives, animals and non-animals, and so on), they can be extended to be
    used in multiclass classification. In order to achieve good classification performance,
    we will explore a number of color spaces, as well as the **Histogram of Oriented
    Gradients** (**HOG**) feature. The end result will be a classifier that can distinguish more
    than 40 different signs from the dataset, with very high accuracy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个SVM来识别各种交通标志。尽管SVMs是二元分类器（也就是说，它们最多可以学习两个类别——正面和负面，动物和非动物等），但它们可以被扩展用于多类分类。为了实现良好的分类性能，我们将探索多个颜色空间，以及**方向梯度直方图**（**HOG**）特征。最终结果将是一个能够从数据集中区分40多种不同标志的分类器，具有非常高的准确性。
- en: Learning the basics of machine learning will be very useful for the future when
    you would like to make your vision-related applications even smarter. This chapter
    will teach you the basics of machine learning, on which the following chapters
    will build.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 学习机器学习的基础对于未来当你想要使你的视觉相关应用更加智能时将非常有用。本章将教你机器学习的基础知识，后续章节将在此基础上展开。
- en: Getting started
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习
- en: The GTSRB dataset can be freely obtained from [http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) (see
    the *Dataset attribution* section for attribution details).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GTSRB数据集可以从[http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset)（见*数据集归属*部分以获取归属详情）免费获取。
- en: You can find the code that we present in this chapter at our GitHub repository: [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们的GitHub仓库中找到本章中展示的代码：[https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter7)。
- en: Planning the app
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划应用
- en: 'To arrive at such a multiclass classifier (that can differentiate between more
    than 40 different signs from the dataset), we need to perform the following steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到这样一个多类分类器（可以区分数据集中超过40个不同的标志），我们需要执行以下步骤：
- en: '**Preprocess the dataset**: We need a way to load our dataset, extract the
    regions of interest, and split the data into appropriate training and test sets.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理数据集**：我们需要一种方法来加载我们的数据集，提取感兴趣的区域，并将数据分为适当的训练集和测试集。'
- en: '**Extract features**: Chances are that raw pixel values are not the most informative
    representation of the data. We need a way to extract meaningful features from
    the data, such as features based on different color spaces and HOG.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提取特征**：原始像素值可能不是数据最有信息量的表示。我们需要一种方法从数据中提取有意义的特征，例如基于不同颜色空间和HOG的特征。'
- en: '**Train the classifier**: We will train the multiclass classifier on the training
    data using a* one-versus-all* strategy.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练分类器**：我们将使用一种*一对多*策略在训练数据上训练多类分类器。'
- en: '**Score the classifier**: We will evaluate the quality of the trained ensemble
    classifier by calculating different performance metrics, such as **accuracy**, **precision**,
    and **recall**.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估分类器**：我们将通过计算不同的性能指标来评估训练的集成分类器的质量，例如**准确率**、**精确度**和**召回率**。'
- en: We will discuss all these steps in detail in the upcoming sections.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中详细讨论所有这些步骤。
- en: 'The final app will parse a dataset, train the ensemble classifier, assess its
    classification performance, and visualize the result. This will require the following
    components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的应用程序将解析数据集，训练集成分类器，评估其分类性能，并可视化结果。这需要以下组件：
- en: '`main`: The main function routine (in `chapter7.py`) is required for starting
    the application.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main`：主函数例程（在`chapter7.py`中）是启动应用程序所必需的。'
- en: '`datasets.gtsrb`: This is a script for parsing the GTSRB dataset. This script
    contains the following functions:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets.gtsrb`：这是一个解析GTSRB数据集的脚本。此脚本包含以下函数：'
- en: '`load_data`: This function is used to load the GTSRB dataset, extract a feature
    of choice, and split the data into training and test sets.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_data`：此函数用于加载GTSRB数据集，提取所需特征，并将数据分为训练集和测试集。'
- en: '`*_featurize`, `hog_featurize`: These functions are passed to `load_data` for
    extracting a feature of choice from the dataset. Example functions are as follows:'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*_featurize`，`hog_featurize`：这些函数被传递给`load_data`以从数据集中提取所需特征。以下是一些示例函数：'
- en: '`gray_featurize`: This is a function that creates features based on grayscale
    pixel values.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gray_featurize`：这是一个基于灰度像素值创建特征的函数。'
- en: '`surf_featurize`: This is a function that creates features based on **Speeded-Up-Robust
    Features** (**SURF**).'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`surf_featurize`：这是一个基于**加速鲁棒特征**（**SURF**）创建特征的函数。'
- en: The classification performance will be judged based on accuracy, precision,
    and recall. The following sections will explain all of these terms in detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分类性能将基于准确率、精确率和召回率进行判断。以下章节将详细解释所有这些术语。
- en: Briefing on supervised learning concepts
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习概念的概述
- en: An important subfield of machine learning is **supervised learning**. In supervised
    learning, we try to learn from a set of labeled data—that is, every data sample
    has a desired target value or true output value. These target values could correspond
    to the continuous output of a function (such as `y` in `y = sin(x)`), or to more
    abstract and discrete categories (such as *cat* or *dog*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个重要子领域是**监督学习**。在监督学习中，我们试图从一组标记数据中学习——也就是说，每个数据样本都有一个期望的目标值或真实输出值。这些目标值可能对应于函数的连续输出（例如`y
    = sin(x)`中的`y`），或者对应于更抽象和离散的类别（例如*猫*或*狗*）。
- en: A supervised learning algorithm uses the already labeled training data, analyzes
    it, and produces a mapping inferred function from features to a label, which can
    be used for mapping new examples. Ideally, the inferred algorithm will generalize
    well and give correct target values for new data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法使用已经标记的训练数据，对其进行分析，并从特征到标签产生一个推断函数，该函数可以用于映射新的示例。理想情况下，推断算法将很好地泛化，并为新数据给出正确的目标值。
- en: 'We divide supervised learning tasks into two categories:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将监督学习任务分为两类：
- en: If we are dealing with continuous output (for example, the probability of rain),
    the process is called **regression**.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们处理的是连续输出（例如，降雨的概率），这个过程被称为**回归**。
- en: If we are dealing with discrete output (for example, species of an animal),
    the process is called **classification**.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们处理的是离散输出（例如，动物的物种），这个过程被称为**分类**。
- en: In this chapter, we focus on the classification problem of labeling images of
    the GTSRB dataset, and we will use an algorithm called SVM to infer a mapping
    function between images and their labels.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于对GTSRB数据集图像进行标签的分类问题，我们将使用一种称为SVM的算法来推断图像与其标签之间的映射函数。
- en: Let's first understand how machine learning gives *machines* the ability to *learn** like
    humans*. Here is a hint—we train them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解机器学习是如何赋予**机器**像人类一样**学习**的能力的。这里有一个提示——我们训练它们。
- en: The training procedure
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练过程
- en: As an example, we may want to learn what cats and dogs look like. To make this
    a supervised learning task, first, we have to put it as a question that has either
    a categorical answer or a real-valued answer.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能想要学习猫和狗的外观。为了使这个任务成为监督学习任务，首先，我们必须将其作为一个具有分类答案或实值答案的问题来提出。
- en: 'Here are some example questions:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些示例问题：
- en: Which animal is shown in the given picture?
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定的图片中展示了哪种动物？
- en: Is there a cat in the picture?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图片中有没有猫？
- en: Is there a dog in the picture?
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图片中有没有狗？
- en: After that, we have to gather an example picture with its corresponding correct
    answer—**training data**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们必须收集一个与其对应正确答案的示例图片——**训练数据**。
- en: Then, we have to pick a learning algorithm (**learner**) and start tweaking
    its parameters in some way (**learning algorithm**) so that the learner can tell
    the correct answers when presented with a datum from training data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须选择一个学习算法（**学习者**）并开始以某种方式调整其参数（**学习算法**），以便当学习者面对训练数据中的数据时，可以给出正确的答案。
- en: We repeat this process until we are satisfied with the learner's performance
    or **score** (which could be **accuracy**, **precision**, or some other **cost
    function**) on the training data. If we are not satisfied, we change the parameters
    of the learner in order to improve the score over time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程，直到我们对学习者的性能或**分数**（可能是**准确率**、**精确率**或某些其他**成本函数**）满意为止。如果我们不满意，我们将改变学习者的参数，以随着时间的推移提高分数。
- en: 'This procedure is outlined in the following screenshot:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在以下截图中有概述：
- en: '![](img/58a3b6a3-86fc-4fb9-a486-bed5734c4562.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/58a3b6a3-86fc-4fb9-a486-bed5734c4562.png)'
- en: From the previous screenshot, Training data is represented by a set of Features.
    For real-life classification tasks, these features are rarely the raw pixel values
    of an image, since these tend not to represent the data well. Often, the process
    of finding the features that best describe the data is an essential part of the
    entire learning task (also referred to as **feature selection** or **feature engineering**).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的截图可以看出，训练数据由一组特征表示。对于现实生活中的分类任务，这些特征很少是图像的原始像素值，因为这些往往不能很好地代表数据。通常，寻找最能描述数据的特征的过程是整个学习任务（也称为**特征选择**或**特征工程**）的一个基本部分。
- en: That is why it is always a good idea to deeply study the statistics and appearances
    of the training set that you are working with before even thinking about setting
    up a classifier.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在考虑设置分类器之前，深入研究你正在处理的训练集的统计和外观总是一个好主意。
- en: As you are probably aware, there is an entire zoo of learners, cost functions,
    and learning algorithms out there. These make up the core of the learning procedure.
    The learner (for example, a linear classifier or SVM) defines how input features
    are converted into a score function (for example, mean-squared error), whereas
    the Learning algorithm (for example, gradient descent) defines how the parameters
    of the learner are changed over time.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，有一个完整的学习者、成本函数和学习算法的动物园。这些构成了学习过程的核心。学习者（例如，线性分类器或SVM）定义了如何将输入特征转换为评分函数（例如，均方误差），而学习算法（例如，梯度下降）定义了学习者的参数如何随时间变化。
- en: The training procedure in a classification task can also be thought of as finding
    an appropriate **decision boundary**, which is a line that best partitions the
    training set into two subsets, one for each class. For example, consider training
    samples with only two features (x and y values) and a corresponding class label
    (positive (**+**), or negative (**–**)).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类任务中的训练过程也可以被视为寻找一个合适的**决策边界**，这是一个将训练集最好地分成两个子集的线，每个类别一个。例如，考虑只有两个特征（x和y值）以及相应的类别标签（正类（**+**），或负类（**–**））的训练样本。
- en: 'At the beginning of the training procedure, the classifier tries to draw a
    line to separate all positives from all negatives. As the training progresses,
    the classifier sees more and more data samples. These are used to update the decision
    boundary, as illustrated in the following screenshot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程的开始，分类器试图画一条线来区分所有正样本和所有负样本。随着训练的进行，分类器看到了越来越多的数据样本。这些样本被用来更新决策边界，如下面的截图所示：
- en: '![](img/88288338-4a20-42f3-a19b-8dfd07d3edfb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88288338-4a20-42f3-a19b-8dfd07d3edfb.png)'
- en: Compared to this simple illustration, an SVM tries to find the optimal decision
    boundary in a high-dimensional space, so the decision boundary can be more complex
    than a straight line.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个简单的插图相比，SVM试图在高维空间中找到最优的决策边界，因此决策边界可能比直线更复杂。
- en: We now move on to understand the testing procedure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在继续了解测试过程。
- en: The testing procedure
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试过程
- en: In order for a trained classifier to be of any practical value, we need to know
    how it performs when applied to a data sample (also called **generalization**)
    that has never been seen before. To stick to our example shown earlier, we want
    to know which class the classifier predicts when we present it with a previously
    unseen picture of a cat or a dog.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使训练好的分类器具有任何实际价值，我们需要知道它在应用于从未见过的数据样本（也称为**泛化**）时的表现。为了坚持我们之前展示的例子，我们想知道当我们向它展示一只猫或狗的以前未见过的图片时，分类器预测的是哪个类别。
- en: 'More generally speaking, we want to know which class the ? sign, in the following
    screenshot, corresponds to, based on the decision boundary we learned during the
    training phase:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，我们想知道在以下截图中的问号符号对应哪个类别，基于我们在训练阶段学习到的决策边界：
- en: '![](img/7d225b9d-2003-4164-9df6-cc599c2d1bc6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7d225b9d-2003-4164-9df6-cc599c2d1bc6.png)'
- en: From the preceding screenshot, you can see why this is a tricky problem. If
    the location of the question mark (?) were more to the left, we would be certain
    that the corresponding class label is +.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图，你可以看到这是一个棘手的问题。如果问号的位置更偏向左边，我们就可以确定相应的类别标签是+。
- en: 'However, in this case, there are several ways to draw the decision boundary
    such that all the + signs are to the left of it and all the – signs are to the
    right of it, as illustrated in the following screenshot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，有几种方式可以绘制决策边界，使得所有的加号都在它的左边，所有的减号都在它的右边，如下面的截图所示：
- en: '![](img/6274c099-983b-4f31-b9b0-bbdbb196f24d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6274c099-983b-4f31-b9b0-bbdbb196f24d.png)'
- en: The label of **?** thus depends on the exact decision boundary that was derived
    during training. If the **?** sign in the preceding screenshot is actually a **–**
    sign, then only one decision boundary (the leftmost) would get the correct answer.
    A common problem is that training can result in a decision boundary that works
    *too well* on the training set (also known as **overfitting**) but also makes
    a lot of mistakes when applied to unseen data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问号的标签取决于在训练期间推导出的确切决策边界。如果前面截图中的问号实际上是减号，那么只有一个决策边界（最左边的）会得到正确的答案。一个常见的问题是训练可能导致一个在训练集上工作得太好的决策边界（也称为**过拟合**），但在应用于未见数据时犯了很多错误。
- en: In that case, it is likely that the learner imprinted details that are specific
    to the training set on the decision boundary, instead of revealing general properties
    about the data that might also be true for unseen data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在那种情况下，学习者很可能会在决策边界上印刻了特定于训练集的细节，而不是揭示关于数据的一般属性，这些属性也可能适用于未见过的数据。
- en: A common technique for reducing the effect of overfitting is called **regularization**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 减少过拟合影响的一种常见技术被称为**正则化**。
- en: 'Long story short: the problem always comes back to finding the boundary that
    best splits not only the training set but also the test set. That is why the most
    important metric for a classifier is its generalization performance (that is,
    how well it classifies data not seen in the training phase).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之：问题总是回到找到最佳分割边界，这个边界不仅分割了训练集，也分割了测试集。这就是为什么分类器最重要的指标是其泛化性能（即它在训练阶段未见过的数据上的分类效果）。
- en: In order to apply our classifier to traffic-sign recognition, we need a suitable
    dataset. A good choice might be the GTSRB dataset. Let us learn about it next.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的分类器应用于交通标志识别，我们需要一个合适的数据集。一个好的选择可能是GTSRB数据集。让我们接下来了解一下它。
- en: Understanding the GTSRB dataset
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解GTSRB数据集
- en: The GTSRB dataset contains more than 50,000 images of traffic signs belonging
    to 43 classes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GTSRB数据集包含超过50,000张属于43个类别的交通标志图片。
- en: This dataset was used by professionals in a classification challenge during
    the **International Joint Conference on Neural Networks** (**IJCNN**) in 2011\.
    The GTSRB dataset is perfect for our purposes because it is large, organized,
    open source, and annotated.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集在2011年**国际神经网络联合会议**（**IJCNN**）期间被专业人士用于分类挑战。GTSRB数据集非常适合我们的目的，因为它规模大、组织有序、开源且已标注。
- en: Although the actual traffic sign is not necessarily a square or is in the center
    of each image, the dataset comes with an annotation file that specifies the bounding
    boxes for each sign.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实际的交通标志不一定是一个正方形，也不一定位于每个图像的中心，但数据集附带了一个标注文件，指定了每个标志的边界框。
- en: A good idea before doing any sort of machine learning is usually to get a feel
    of the dataset, its qualities, and its challenges. Some good ideas include manually
    going through the data and understanding what are some characteristics of it,
    reading a data description—if it's available on the page—to understand which models
    might work best, and so on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何类型的机器学习之前，通常一个好的想法是了解数据集、其质量和其挑战。一些好的想法包括手动浏览数据，了解其一些特征，阅读数据描述（如果页面上有）以了解哪些模型可能最适合，等等。
- en: 'Here, we present a snippet from `data/gtsrb.py` that loads and then plots a
    random-15 sample of the training dataset, and does that `100` times, so you can
    paginate through the data:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了`data/gtsrb.py`中的一个片段，该片段加载并绘制了训练数据集的随机15个样本，并重复100次，这样您就可以浏览数据：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Another good strategy would be to plot 15 samples from each of the 43 classes
    and see how images change for the given class. The following screenshot shows
    some examples of this dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不错的策略是绘制每个43个类别中的15个样本，看看图像如何随给定类别变化。以下截图显示了该数据集的一些示例：
- en: '![](img/f166f8b5-f216-4a84-ac1d-dcff00181016.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f166f8b5-f216-4a84-ac1d-dcff00181016.png)'
- en: Even from this small data sample, it is immediately clear that this is a challenging
    dataset for any sort of classifier. The appearance of the signs changes drastically
    based on viewing angle (orientation), viewing distance (blurriness), and lighting
    conditions (shadows and bright spots).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 即使从这个小的数据样本来看，也立即清楚这是一个对任何类型的分类器都具有挑战性的数据集。标志的外观会根据观察角度（方向）、观察距离（模糊度）和光照条件（阴影和亮点）发生剧烈变化。
- en: For some of these signs—such as the second sign of the third row—it is difficult,
    even for humans (at least for me), to tell the correct class label right away.
    It's a good thing we are aspiring experts in machine learning!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其中一些标志——例如第三行的第二个标志——即使是人类（至少对我来说），也很难立即说出正确的类别标签。我们作为机器学习的追求者真是件好事！
- en: Let's now learn to parse the dataset in order to convert to a format suitable
    for the SVM to use for training.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在学习如何解析数据集，以便将其转换为适合SVM用于训练的格式。
- en: Parsing the dataset
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析数据集
- en: The GTSRB dataset has 21 files that we can download. We choose to work with
    the raw data to make it more educational and download the official training data—**Images
    and annotations** (`GTSRB_Final_Training_Images.zip`) for training, and the official
    training dataset that was used at the **IJCNN** **2011 competition**—**Images
    and annotations** (`GTSRB-Training_fixed.zip`) for scoring.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: GTSRB数据集包含21个文件，我们可以下载。我们选择使用原始数据以使其更具教育意义，并下载官方训练数据——**图像和标注** (`GTSRB_Final_Training_Images.zip`)
    用于训练，以及用于**IJCNN 2011比赛**的官方训练数据集——**图像和标注** (`GTSRB-Training_fixed.zip`) 用于评分。
- en: 'The following screenshot shows the files from the dataset:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了数据集的文件：
- en: '![](img/4fa8121b-2ea6-41e7-8b5a-14ffa222433b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4fa8121b-2ea6-41e7-8b5a-14ffa222433b.png)'
- en: We chose to download the train and test data separately instead of constructing
    our own train/test data from one of the datasets because, after exploring the
    data, there are usually 30 images of the same sign from different distances that
    look very much alike. Putting these 30 images in different datasets will skew
    the problem and lead to great results, even though our model might not generalize
    well.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择分别下载训练数据和测试数据，而不是从其中一个数据集中构建自己的训练/测试数据，因为在探索数据后，通常会有30张来自不同距离的相同标志的图像看起来非常相似。将这些30张图像放入不同的数据集中将扭曲问题，并导致结果极好，尽管我们的模型可能无法很好地泛化。
- en: 'The following code is a function that downloads the data from the **University
    of Copenhagen Data Archive**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个从**哥本哈根大学数据档案**下载数据的函数：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The previous code takes a filename (you can see the files and their names from
    the previous screenshot) and checks if the file already exists or not (and checks
    whether the `md5sum` matches or not, if provided). This saves a lot of bandwidth
    and time by not having to download the files again and again. Then, it downloads
    the file and stores it in the same directory as the file that contains the code.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码接受一个文件名（您可以从之前的屏幕截图中看到文件及其名称），并检查该文件是否已存在（如果提供了`md5sum`，则检查是否匹配），这样可以节省大量带宽和时间，无需反复下载文件。然后，它下载文件并将其存储在包含代码的同一目录中。
- en: The annotation format can be viewed at [http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 标注格式可以在[http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Annotationformat)查看。
- en: 'After we have downloaded the file, we write a function that unzips and extracts
    the data using the annotation format provided with the data, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，我们编写一个函数，使用与数据一起提供的标注格式解压缩并提取数据，如下所示：
- en: 'First, we open the downloaded `.zip` file (this could be either the training
    or test data), and we iterate over all the files and only open `.csv` files, which
    contain the target information of each image in the corresponding class. This
    is shown in the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们打开下载的`.zip`文件（这可能是指训练数据或测试数据），我们遍历所有文件，只打开包含对应类别中每个图像目标信息的`.csv`文件。这在上面的代码中显示如下：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we check if the label of the image is in the `labels` array that we are
    interested in. Then, we create a `csv.reader` that we will use to iterate over
    the `.csv` file contents, as follows:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们检查图像的标签是否在我们感兴趣的`labels`数组中。然后，我们创建一个`csv.reader`，我们将使用它来遍历`.csv`文件内容，如下所示：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Every line of the file contains the annotation for one data sample. So, we
    extract the image path, read the data, and convert it to a NumPy array. Usually,
    the object in these samples is not perfectly cut out but is embedded in its surroundings.
    We cut the image using the boundary-box information provided in the archive, using
    a `.csv` file for each of the labels. In the following code, we add the sign to
    `data` and add the label to `targets`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件的每一行都包含一个数据样本的标注。因此，我们提取图像路径，读取数据，并将其转换为NumPy数组。通常，这些样本中的对象并不是完美切割的，而是嵌入在其周围环境中。我们使用存档中提供的边界框信息来切割图像，每个标签使用一个`.csv`文件。在下面的代码中，我们将符号添加到`data`中，并将标签添加到`targets`中：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Often, it is desirable to perform some form of feature extraction, because raw
    image data is rarely the best description of the data. We will defer this job
    to another function, which we will discuss in detail later.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，执行某种形式的特征提取是可取的，因为原始图像数据很少是数据的最佳描述。我们将把这个任务推迟到另一个函数，我们将在稍后详细讨论。
- en: 'As pointed out in the previous subsection, it is imperative to separate the
    samples that we use to train our classifier from the samples that we use to test
    it. For this, the following code snippet shows us that we have two different functions
    that download training and testing data and load them into memory:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一小节所述，将我们用于训练分类器的样本与用于测试的样本分开至关重要。为此，以下代码片段显示我们有两个不同的函数，用于下载训练数据和测试数据并将它们加载到内存中：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we know how to convert images into NumPy matrices, we can go on to
    more interesting parts, namely, we can feed the data to the SVM and train it to
    make predictions. So, let's move on to the next section, which covers feature
    extraction.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何将图像转换为NumPy矩阵，我们可以继续到更有趣的部分，即我们可以将数据输入到SVM中并对其进行训练以进行预测。所以，让我们继续到下一节，该节涵盖了特征提取。
- en: Learning about dataset feature extraction
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习数据集特征提取
- en: Chances are that raw pixel values are not the most informative way to represent
    the data, as we have already realized in [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml),
    *Finding Objects via Feature Matching and Perspective Transforms*. Instead, we
    need to derive a measurable property of the data that is more informative for
    classification.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，原始像素值不是表示数据的最佳方式，正如我们在[第3章](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml)，*通过特征匹配和透视变换寻找对象*中已经意识到的，我们需要从数据中推导出一个可测量的属性，这个属性对分类更有信息量。
- en: However, often, it is not clear which features would perform best. Instead,
    it is often necessary to experiment with different features that the practitioner
    finds appropriate. After all, the choice of features might strongly depend on
    the specific dataset to be analyzed or the specific classification task to be
    performed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常不清楚哪些特征会表现最好。相反，通常需要尝试不同的特征，这些特征是实践者认为合适的。毕竟，特征的选择可能强烈依赖于要分析的特定数据集或要执行的特定分类任务。
- en: For example, if you have to distinguish between a stop sign and a warning sign,
    then the most distinctive feature might be the shape of the sign or the color
    scheme. However, if you have to distinguish between two warning signs, then color
    and shape will not help you at all, and you will be required to come up with more
    sophisticated features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你必须区分停车标志和警告标志，那么最显著的特征可能是标志的形状或颜色方案。然而，如果你必须区分两个警告标志，那么颜色和形状将完全帮不上忙，你需要想出更复杂一些的特征。
- en: 'In order to demonstrate how the choice of features affects classification performance,
    we will focus on the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示特征选择如何影响分类性能，我们将关注以下内容：
- en: '**A few simple color transformations** (such as grayscale; **red, green, blue**
    (**RGB**); and **hue, saturation, value** (**HSV**): Classification based on grayscale
    images will give us some baseline performance for the classifier. RGB might give
    us slightly better performance because of the distinct color schemes of some traffic
    signs.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一些简单的颜色变换**（例如灰度；**红色、绿色、蓝色**（**RGB**）；以及**色调、饱和度、亮度**（**HSV**））：基于灰度图像的分类将为我们提供分类器的基准性能。RGB可能会因为某些交通标志独特的颜色方案而提供略好的性能。'
- en: Even better performance is expected from HSV. This is because it represents
    colors even more robustly than RGB. Traffic signs tend to have very bright, saturated
    colors that (ideally) are quite distinct from their surroundings.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 预期HSV会有更好的性能。这是因为它比RGB更稳健地表示颜色。交通标志通常具有非常明亮、饱和的颜色，这些颜色（理想情况下）与周围环境非常不同。
- en: '**SURF**: This should appear very familiar to you by now. We have previously
    recognized SURF as an efficient and robust method of extracting meaningful features
    from an image. So, can''t we use this technique to our advantage in a classification
    task?'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SURF**：到现在为止，这应该对你来说非常熟悉了。我们之前已经将SURF识别为从图像中提取有意义特征的一种高效且稳健的方法。那么，我们能否利用这种技术在分类任务中占得先机？'
- en: '**HOG**: This is by far the most advanced feature descriptor to be considered
    in this chapter. The technique counts occurrences of gradient orientations along
    a dense grid laid out on the image and is well suited for use with SVMs.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HOG**：这是本章要考虑的最先进的特征描述符。该技术沿着图像上密集排列的网格计算梯度方向的出现次数，非常适合与SVMs一起使用。'
- en: Feature extraction is performed by functions in the `data/process.py` file,
    from which we will call different functions to construct and compare different
    features.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取是通过 `data/process.py` 文件中的函数完成的，我们将调用不同的函数来构建和比较不同的特征。
- en: 'Here is a nice blueprint, which—if you follow it—will enable you to easily
    write your own featurization functions and use with our code, and compare if your
    `your_featurize` function will yield better results:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很好的蓝图，如果你遵循它，将能够轻松地编写自己的特征化函数，并使用我们的代码，比较你的 `your_featurize` 函数是否能产生更好的结果：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `*_featurize` functions take a list of images and return a matrix (as a
    2D `np.ndarray`), where each row is a new sample and each column represents a
    feature.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`_featurize` 函数接收一个图像列表并返回一个矩阵（作为二维 `np.ndarray`），其中每一行代表一个新的样本，每一列代表一个特征。'
- en: For most of the following features, we will be using the (already suitable)
    default arguments in OpenCV. However, these values are not set in stone, and,
    even in real-world classification tasks, it is often necessary to search across
    the range of possible values for both features extracting and feature learning
    parameters in a process called **hyperparameter exploration**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下大多数特征，我们将使用OpenCV中（已经合适的）默认参数。然而，这些值并不是一成不变的，即使在现实世界的分类任务中，也经常需要在一个称为**超参数探索**的过程中，在特征提取和特征学习参数的可能值范围内进行搜索。
- en: Now that we know what we are doing, let's take a look at some featurization
    functions that we have come up with that build on top of concepts from previous
    sections and also add some new concepts as well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了我们在做什么，让我们看看一些基于前几节概念并添加了一些新概念的特性化函数。
- en: Understanding common preprocessing
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解常见的预处理
- en: Before we look at what we have come up with, let's take our time to look at
    the two most common forms of preprocessing that are almost always applied to any
    data before machine learning tasks—namely, **mean subtraction** and **normalization**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看我们得到的结果之前，让我们花时间看看在机器学习任务之前几乎总是应用于任何数据的两种最常见的前处理形式——即，**均值减法**和**归一化**。
- en: Mean subtraction is the most common form of preprocessing (sometimes also referred
    to as **zero centering** or de-meaning), where the mean value of every feature
    dimension is calculated across all samples in a dataset. This feature-wise average
    is then subtracted from every sample in the dataset. You can think of this process
    as centering the *cloud* of data on the origin.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 均值减法是最常见的预处理形式（有时也称为**零中心化**或去均值），其中计算数据集中所有样本的每个特征维度的平均值。然后将这个特征维度的平均值从数据集中的每个样本中减去。你可以将这个过程想象为将数据的**云**中心化在原点。
- en: Normalization refers to the scaling of data dimensions so that they are of roughly
    the same scale. This can be achieved by either dividing each dimension by its
    standard deviation (once it has been zero-centered) or scaling each dimension
    to lie in the range of [-1, 1].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是指对数据维度进行缩放，使它们大致具有相同的尺度。这可以通过将每个维度除以其标准差（一旦它已经被零中心化）或缩放每个维度使其位于[-1, 1]的范围内来实现。
- en: It makes sense to apply this step only if you have reason to believe that different
    input features have different scales or units. In the case of images, the relative
    scales of pixels are already approximately equal (and in the range of [0, 255]),
    so it is not strictly necessary to perform this additional preprocessing step.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在你有理由相信不同的输入特征有不同的尺度或单位时，才适用这一步骤。在图像的情况下，像素的相对尺度已经大致相等（并且在[0, 255]的范围内），因此执行这个额外的预处理步骤并不是严格必要的。
- en: Armed with these two concepts, let's take a look at our feature extractors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这两个概念，让我们来看看我们的特征提取器。
- en: Learning about grayscale features
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解灰度特征
- en: The easiest feature to extract is probably the grayscale value of each pixel.
    Usually, grayscale values are not very indicative of the data they describe, but
    we will include them here for illustrative purposes (that is, to achieve baseline
    performance).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最容易提取的特征可能是每个像素的灰度值。通常，灰度值并不非常能说明它们所描述的数据，但在这里我们将包括它们以供说明之用（即，为了达到基线性能）。
- en: 'For each image in the input set, we are going to perform the following steps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入集中的每个图像，我们将执行以下步骤：
- en: 'Resize all images to have the same (usually smaller) size. We use `scale_size=(32,
    32)` to make sure we don''t make the images too small. At the same time, we want
    our data to be small enough to work on our personal computer. We can do this with
    the following code:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有图像调整到相同的大小（通常是更小的尺寸）。我们使用`scale_size=(32, 32)`来确保我们不会使图像太小。同时，我们希望我们的数据足够小，以便在我们的个人电脑上处理。我们可以通过以下代码来实现：
- en: '[PRE7]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Convert the image to grayscale (values are still in 0-255 range), like this:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度（值仍在0-255范围内），如下所示：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Convert each image to have the pixel value in (0, 1) and flatten, so instead
    of a matrix of `(32, 32)` size for each image, we have a vector of size `1024`,
    as follows:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个图像转换为具有（0, 1）范围内的像素值并展平，因此对于每个图像，我们有一个大小为`1024`的向量，如下所示：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Subtract the average pixel value of the flattened vector, like this:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从展平向量的平均像素值中减去，如下所示：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We use the returned matrix as our training data for the machine learning algorithm.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用返回的矩阵作为机器学习算法的训练数据。
- en: Now, let's take a look at another example—*what would happen if we used information
    in the colors as well?*
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一个例子——*如果我们也使用颜色中的信息会怎样？*
- en: Understanding color spaces
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解色彩空间
- en: Alternatively, you might find that colors contain some information that raw
    grayscale values cannot capture. Traffic signs often have a distinct color scheme,
    and it might be indicative of the information it is trying to convey (that is,
    red for stop signs and forbidden actions; green for informational signs; and so
    on). We could opt to use the RGB images as input, but, in our case, we do not
    have to do anything since the dataset is already RGB.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可能发现颜色包含一些原始灰度值无法捕捉的信息。交通标志通常有独特的色彩方案，这可能表明它试图传达的信息（例如，红色表示停车标志和禁止行为；绿色表示信息标志；等等）。我们可以选择使用RGB图像作为输入，但在我们的情况下，我们不必做任何事情，因为数据集已经是RGB的。
- en: However, even RGB might not be informative enough. For example, a stop sign
    in broad daylight might appear very bright and clear, but its colors might appear
    much less vibrant on a rainy or foggy day. A better choice might be the HSV color
    space, which represents colors using hue, saturation, and value (or brightness).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使是RGB可能也不够有信息量。例如，在晴朗的白天，一个停车标志可能非常明亮和清晰，但在雨天或雾天，其颜色可能看起来要暗淡得多。更好的选择可能是HSV颜色空间，它使用色调、饱和度和亮度（或亮度）来表示颜色。
- en: The most telling feature of traffic signs in this color space might be the hue
    (a more perceptually relevant description of color or chromaticity), provides
    an improved ability to distinguish between the color scheme of different sign
    types. Saturation and value could be equally important, however, as traffic signs
    tend to use relatively bright and saturated colors that do not typically appear
    in natural scenes (that is, their surroundings).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个颜色空间中，交通标志的最显著特征可能是色调（对颜色或色相的更感知相关的描述），它提供了区分不同标志类型颜色方案的能力。然而，饱和度和亮度可能同样重要，因为交通标志倾向于使用相对明亮和饱和的颜色，这些颜色在自然场景中通常不会出现（即，它们的周围）。
- en: 'In OpenCV, the HSV color space is only a single call to `cv2.cvtColor` away,
    as shown in the following code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，将图像转换为HSV颜色空间只需要一个`cv2.cvtColor`调用，如下面的代码所示：
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, to summarize, featurization is almost the same as for grayscale features.
    For each image, we carry out the following four steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结一下，特征化几乎与灰度特征相同。对于每张图像，我们执行以下四个步骤：
- en: Resize all images to have the same (usually smaller) size.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有图像调整到相同（通常是较小的）大小。
- en: Convert the image to HSV (values in the 0-255 range).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为HSV（值在0-255范围内）。
- en: Convert each image to have the pixel value in (0, 1), and flatten it.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个图像转换为具有（0，1）范围内的像素值，并将其展平。
- en: Subtract the average pixel value of the flattened vector.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从展平向量的平均像素值中减去。
- en: Now, let's try to look at a more complex example of a feature extractor that
    uses SURF.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试看一个使用SURF的更复杂的特征提取器的例子。
- en: Using SURF descriptor
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SURF描述符
- en: But wait a minute! In [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml),
    *Finding Objects via Feature Matching and Perspective Transforms*, you learned
    that the SURF descriptor is one of the best and most robust ways to describe images
    independent of scale or rotations. Can we use this technique to our advantage
    in a classification task?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等！在[第3章](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml)“通过特征匹配和透视变换查找对象”中，你了解到SURF描述符是描述图像独立于尺度或旋转的最佳和最鲁棒的方法之一。我们能否利用这项技术在分类任务中占得优势？
- en: Glad you asked! To make this work, we need to adjust SURF so that it returns
    a fixed number of features per image. By default, the SURF descriptor is only
    applied to a small list of *interesting* key points in the image, the number of
    which might differ on an image-by-image basis. This is unsuitable for our current
    purposes because we want to find a fixed number of feature values per data sample.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴你问了！为了使这起作用，我们需要调整SURF，使其为每张图像返回固定数量的特征。默认情况下，SURF描述符仅应用于图像中的一小部分*有趣*的关键点，这些关键点的数量可能因图像而异。这对于我们的当前目的来说是不合适的，因为我们想要在每个数据样本中找到固定数量的特征值。
- en: 'Instead, we need to apply SURF to a fixed dense grid laid out over the image,
    for which we create a key points array containing all pixels, as illustrated in
    the following code block:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要将SURF应用于图像上铺设的固定密集网格，为此我们创建了一个包含所有像素的关键点数组，如下面的代码块所示：
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, it is possible to obtain SURF descriptors for each point on the grid
    and append that data sample to our feature matrix. We initialize SURF with a `hessianThreshold`
    value of `400`, as we did before, like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以为网格上的每个点获得SURF描述符，并将该数据样本附加到我们的特征矩阵中。我们像之前一样，使用`hessianThreshold`值为`400`初始化SURF，如下所示：
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The key points and descriptors can then be obtained via the following code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下代码可以获得关键点和描述符：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Because `surf.compute` has two output arguments, `kp_des` will actually be a
    concatenation of both key points and descriptors. The second element in the `kp_des`
    array is the descriptor that we care about.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`surf.compute`有两个输出参数，所以`kp_des`实际上将是关键点和描述符的连接。`kp_des`数组中的第二个元素是我们关心的描述符。
- en: 'We select the first `num_surf_features` from each data sample and return it
    as a feature for the image, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从每个数据样本中选择前`num_surf_features`个，并将其作为图像的特征返回，如下所示：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, let's take a look at a new concept that is very popular in the community—HOG.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一个在社区中非常流行的概念——HOG。
- en: Mapping HOG descriptor
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射HOG描述符
- en: The last feature descriptor to consider is the HOG. HOG features have previously
    been shown to work exceptionally well in combination with SVMs, especially when
    applied to tasks such as pedestrian recognition.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的最后一种特征描述符是HOG。之前的研究表明，HOG特征与SVMs结合使用时效果非常好，尤其是在应用于行人识别等任务时。
- en: The essential idea behind HOG features is that the local shapes and appearance
    of objects within an image can be described by the distribution of edge directions.
    The image is divided into small connected regions, within which a histogram of
    gradient directions (or edge directions) is compiled.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: HOG特征背后的基本思想是，图像中对象的局部形状和外观可以通过边缘方向的分布来描述。图像被分成小的连通区域，在这些区域内，编译了梯度方向（或边缘方向）的直方图。
- en: 'The following screenshot shows such a histogram from a region in a picture.
    Angles are not directional; that''s why the range is (-180, 180):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了图片中的一个区域的直方图。角度不是方向性的；这就是为什么范围是（-180，180）：
- en: '![](img/169da355-a9f3-4b2c-a7cf-551e233b1e8b.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/169da355-a9f3-4b2c-a7cf-551e233b1e8b.png)'
- en: As you can see, it has a lot of edge directions in the horizontal direction
    (angles around +180 and -180 degrees), so this seems like a good feature, especially
    when we are working with arrows and lines.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它在水平方向上有许多边缘方向（+180度和-180度左右的角），因此这似乎是一个很好的特征，尤其是在我们处理箭头和线条时。
- en: Then, the descriptor is assembled by concatenating the different histograms.
    For improved performance, the local histograms can be contrast normalized, which
    results in better invariance to changes in illumination and shadowing. You can
    see why this sort of preprocessing might be just the perfect fit for recognizing
    traffic signs under different viewing angles and lighting conditions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过连接不同的直方图来组装描述符。为了提高性能，局部直方图可以进行对比度归一化，这有助于提高对光照和阴影变化的鲁棒性。您可以看到为什么这种预处理可能非常适合在不同视角和光照条件下识别交通标志。
- en: 'The HOG descriptor is fairly accessible in OpenCV by means of `cv2.HOGDescriptor`,
    which takes the detection window size (32 x 32), the block size (16 x 16), the
    cell size (8 x 8), and the cell stride (8 x 8) as input arguments. For each of
    these cells, the HOG descriptor then calculates a HOG using nine bins, like this:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`cv2.HOGDescriptor`在OpenCV中可以方便地访问HOG描述符，它接受检测窗口大小（32 x 32）、块大小（16 x 16）、单元格大小（8
    x 8）和单元格步长（8 x 8）作为输入参数。对于这些单元格中的每一个，HOG描述符然后使用九个桶计算HOG，如下所示：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Applying the HOG descriptor to every data sample is then as easy as calling
    `hog.compute`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将HOG描述符应用于每个数据样本就像调用`hog.compute`一样简单。
- en: After we have extracted all the features we want, we return a flattened list
    for each of the images.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了我们想要的全部特征之后，我们为每张图像返回一个扁平化的列表。
- en: Now, we are finally ready to train the classifier on the preprocessed dataset.
    So, let's move on to the SVM.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于准备好在预处理后的数据集上训练分类器了。所以，让我们继续到SVM。
- en: Learning about SVMs
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习SVMs
- en: An SVM is a learner for binary classification (and regression) that tries to
    separate examples from the two different class labels with a decision boundary
    that maximizes the margin between the two classes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是一种用于二元分类（和回归）的学习器，它试图通过最大化两个类别之间的间隔来分离来自两个不同类别标签的示例。
- en: 'Let''s return to our example of positive and negative data samples, each of
    which has exactly two features (x and y) and two possible decision boundaries,
    as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到正负数据样本的例子，每个样本恰好有两个特征（x和y）和两个可能的决策边界，如下所示：
- en: '![](img/451b4435-7bdd-4dd9-b9b5-105450095b86.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/451b4435-7bdd-4dd9-b9b5-105450095b86.png)'
- en: Both of these decision boundaries get the job done. They partition all the samples
    of positives and negatives with zero misclassifications. However, one of them
    seems intuitively better. How can we quantify *better* and thus learn the *best* parameter
    settings?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个决策边界都能完成任务。它们将所有正负样本分割开来，没有错误分类。然而，其中一个看起来直观上更好。我们如何量化“更好”，从而学习“最佳”参数设置？
- en: This is where SVMs come into the picture. SVMs are also called **maximal margin
    classifiers** because they can be used to do exactly that—define the decision
    boundary so as to make those two clouds of + and - as far apart as possible; that
    is, as far apart from the decision boundary as possible.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是SVMs发挥作用的地方。SVMs也被称为**最大间隔分类器**，因为它们可以用来做到这一点——定义决策边界，使得两个云团（+和-）尽可能远；也就是说，尽可能远离决策边界。
- en: 'For the preceding example, an SVM would find two parallel lines that pass through
    the data points on the class margins (the *dashed lines* in the following screenshot),
    and then make the line (that passes through the center of the margins) the decision
    boundary (the *bold black line* in the following screenshot):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面的例子，SVM会在类别边缘（以下截图中的虚线）上的数据点找到两条平行线，然后将通过边缘中心的线作为决策边界（以下截图中的粗黑线）：
- en: '![](img/1e826ff4-c743-4082-8bb1-73cebd9ae244.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e826ff4-c743-4082-8bb1-73cebd9ae244.png)'
- en: It turns out that to find the maximal margin, it is only important to consider
    the data points that lie on the class margins. These points are sometimes also
    called **support vectors**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，为了找到最大间隔，只需要考虑位于类别边缘的数据点。这些点有时也被称为**支持向量**。
- en: In addition to performing linear classification (that is, when the decision
    boundary is a straight line), SVMs can also perform a non-linear classification
    using what is called the **kernel trick**, implicitly mapping their input to high-dimensional
    feature spaces.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 除了执行线性分类（即决策边界是直线的情况）之外，SVM还可以使用所谓的**核技巧**执行非线性分类，隐式地将它们的输入映射到高维特征空间。
- en: Now, let's take a look at how we can turn this binary classifier into a multiclass
    classifier that is more appropriate for the 43-class classification problem we
    are trying to tackle.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何将这个二元分类器转换成一个更适合我们试图解决的43个类别分类问题的多类分类器。
- en: Using SVMs for multiclass classification
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM进行多类分类
- en: Whereas some classification algorithms, such as neural networks, naturally lend
    themselves to using more than two classes, SVMs are binary classifiers by nature.
    They can, however, be turned into multiclass classifiers.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与一些分类算法（如神经网络）自然适用于使用多个类别不同，SVM本质上是二元分类器。然而，它们可以被转换成多类分类器。
- en: 'Here, we will consider two different strategies:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将考虑两种不同的策略：
- en: '**One-versus-all**: The *one-versus-all* strategy involves training a single
    classifier per class, with the samples of that class as positive samples and all
    other samples as negatives.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：`一对多`策略涉及为每个类别训练一个单独的分类器，该类别的样本作为正样本，所有其他样本作为负样本。'
- en: For the `k` classes, this strategy thus requires the training of `k` number
    of different SVMs. During testing, all classifiers can express a *+1* vote by
    predicting that an unseen sample belongs to their class.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`k`个类别，这种策略因此需要训练`k`个不同的SVM。在测试期间，所有分类器可以通过预测一个未见样本属于其类别来表示一个`+1`的投票。
- en: In the end, an unseen sample is classified by the ensemble as the class with
    the most votes. Usually, this strategy is used in combination with confidence
    scores instead of predicted labels so that, in the end, the class with the highest
    confidence score can be picked.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个未见样本被集成分类器归类为获得最多投票的类别。通常，这种策略会与置信度分数结合使用，而不是预测标签，这样最终可以选取置信度分数最高的类别。
- en: '**One-versus-one**: The *one-versus-one* strategy involves training a single
    classifier per class pair, with the samples of the first class as positive samples
    and the samples of the second class as negative samples. For the `k` classes,
    this strategy requires the training of `k*(k-1)/2` classifiers.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一**：`一对一`策略涉及为每个类别对训练一个单独的分类器，第一个类别的样本作为正样本，第二个类别的样本作为负样本。对于`k`个类别，这种策略需要训练`k*(k-1)/2`个分类器。'
- en: However, the classifiers have to solve a significantly easier task, so there
    is a trade-off when considering which strategy to use. During testing, all classifiers
    can express a *+1* vote for either the first or the second class. In the end,
    an unseen sample is classified by the ensemble as the class with the most votes.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分类器必须解决一个显著更简单的问题，因此在考虑使用哪种策略时存在权衡。在测试期间，所有分类器可以为第一个或第二个类别表达一个`+1`的投票。最后，一个未见样本被集成分类器归类为获得最多投票的类别。
- en: Usually, you would not have to write your own classification algorithms unless
    you really wanted to dive deep into the algorithms and squeeze the last bit of
    performance out of your model. And luckily, OpenCV already comes with a good machine
    learning toolkit that we will use in this chapter. OpenCV uses a one-versus-all
    approach, and we will focus on that approach.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，除非你真的想深入研究算法并从你的模型中榨取最后一丝性能，否则你不需要编写自己的分类算法。幸运的是，OpenCV已经内置了一个良好的机器学习工具包，我们将在本章中使用。OpenCV使用一对多方法，我们将重点关注这种方法。
- en: Now, let's get our hands dirty, and see how we can code this up with OpenCV
    and get some real results.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们动手实践，看看我们如何使用 OpenCV 编写代码并获取一些实际结果。
- en: Training the SVM
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 SVM
- en: 'We are going to write the training method in a separate function; it''s a good
    practice if we later wanted to change our training method. First, we define the
    signature of our function, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把训练方法写入一个单独的函数中；如果我们以后想更改我们的训练方法，这是一个好的实践。首先，我们定义函数的签名，如下所示：
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Thus, we want a function that takes two arguments—`training_features` and `training_labels`—and
    the correct answers corresponding to each feature. Thus, the first argument will
    be a matrix in the form of a two-dimensional NumPy array, and the second argument
    will be a one-dimensional NumPy array.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们想要一个函数，它接受两个参数——`training_features` 和 `training_labels`——以及与每个特征对应的正确答案。因此，第一个参数将是一个二维
    NumPy 数组的矩阵形式，第二个参数将是一个一维 NumPy 数组。
- en: Then, the function will return an object that should have a `predict` method,
    which takes new unseen data and labels it. So, let's get started and see how we
    could train an SVM with OpenCV.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，函数将返回一个对象，该对象应该有一个 `predict` 方法，该方法接受新的未见数据并将其标记。所以，让我们开始，看看我们如何使用 OpenCV
    训练 SVM。
- en: 'We name our function `train_one_vs_all_SVM`, and do the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的函数命名为 `train_one_vs_all_SVM`，并执行以下操作：
- en: 'Instantiate an SVM class instance using `cv2.ml.SVM_create`, which creates
    a multiclass SVM using the one-versus-all strategy, as follows:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cv2.ml.SVM_create` 实例化 SVM 类，它使用一对一策略创建一个多类 SVM，如下所示：
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Set the hyperparameters of the learner. These are called **hyperparameters**
    because these parameters are out of the control of the learner (versus parameters
    that the learner changes during the learning process). This can be done with the
    following code:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习器的超参数。这些被称为 **超参数**，因为这些参数超出了学习器的控制范围（与学习器在学习过程中更改的参数相对）。可以使用以下代码完成：
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Call the `train` method on the SVM instance, and OpenCV takes care of training
    (this takes a couple of minutes on a regular laptop computer with the GTSRB dataset),
    as follows:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 SVM 实例上调用 `train` 方法，OpenCV 会负责训练（使用 GTSRB 数据集，在普通笔记本电脑上这可能需要几分钟），如下所示：
- en: '[PRE20]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: OpenCV will take care of the rest. What happens under the hood is that the SVM
    training uses **Lagrange multipliers** to optimize some constraints that lead
    to the maximum margin decision boundary.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 会处理其余部分。在底层，SVM 训练使用 **拉格朗日乘数**来优化一些导致最大边缘决策边界的约束。
- en: The optimization process is usually performed until some termination criteria
    are met, which can be specified via the SVM's optional arguments.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 优化过程通常是在满足某些终止条件时进行的，这些条件可以通过 SVM 的可选参数指定。
- en: Now that we have looked at training the SVM, let's look at testing it.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 SVM 的训练过程，让我们来看看如何测试它。
- en: Testing the SVM
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试 SVM
- en: There are many ways to evaluate a classifier, but most often, we are simply
    interested in the accuracy metric—that is, how many data samples from the test
    set were classified correctly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 评估分类器有许多方法，但最常见的是，我们通常只对准确率指标感兴趣——也就是说，测试集中有多少数据样本被正确分类。
- en: 'In order to arrive at this metric, we need to get the prediction results out
    of the SVM—and again, OpenCV has us covered, by providing the `predict` method
    that takes a matrix of features and returns an array of predicted labels. We thus
    need to proceed as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到这个指标，我们需要从 SVM 中获取预测结果——同样，OpenCV 为我们提供了 `predict` 方法，该方法接受一个特征矩阵并返回一个预测标签数组。因此，我们需要按照以下步骤进行：
- en: 'So, we have to first featurize our testing data:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们首先需要对我们的测试数据进行特征化：
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, we feed the featurized data to the classifier and get the predicted labels,
    like this:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将特征化后的数据输入到分类器中，并获取预测标签，如下所示：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After that, we can try to see how many of the labels the classifier got correctly,
    by running the following code:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们可以尝试运行以下代码来查看分类器正确标记了多少个标签：
- en: '[PRE23]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we are ready to calculate the desired performance metrics, as described
    in detail in later sections. For the purpose of this chapter, we choose to calculate accuracy,
    precision, and recall.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备计算所期望的性能指标，这些指标将在后面的章节中详细描述。为了本章的目的，我们选择计算准确率、精确率和召回率。
- en: The `scikit-learn` machine learning package (which can be found at [http://scikit-learn.org](http://scikit-learn.org))
    supports the three metrics—which are accuracy, precision, and recall (as well
    as others)—straight out of the box, and also comes with a variety of other useful
    tools. For educational purposes (and to minimize software dependencies), we will
    derive the three metrics ourselves.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 机器学习包（可在 [http://scikit-learn.org](http://scikit-learn.org)
    找到）直接支持三个指标——准确率、精确率和召回率（以及其他指标），并且还附带了许多其他有用的工具。出于教育目的（以及最小化软件依赖），我们将自己推导这三个指标。'
- en: Accuracy
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: 'The most straightforward metric to calculate is probably accuracy. This metric
    simply counts the number of test samples that have been predicted correctly, and
    returns the number as a fraction of the total number of test samples, as shown
    in the following code block:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 计算最直接的指标可能是准确率。这个指标简单地计算预测正确的测试样本数量，并以总测试样本数的分数形式返回，如下面的代码块所示：
- en: '[PRE24]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The previous code shows that we have extracted `y_predicted` by calling `model.predict(x_test)`.
    This was quite simple, but, again, to make things reusable, we put this inside
    a function that takes `predicted` and `true` labels. And now, we will go on to
    implement slightly more complicated metrics that are useful to measure classifier
    performance.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码显示，我们通过调用 `model.predict(x_test)` 提取了 `y_predicted`。这很简单，但为了使代码可重用，我们将它放在一个接受
    `predicted` 和 `true` 标签的函数中。现在，我们将继续实现一些更复杂的、有助于衡量分类器性能的指标。
- en: Confusion matrix
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix is a 2D matrix of size equal to `(num_classes, num_classes)`,
    where the rows correspond to the predicted class labels, and the columns correspond
    to the actual class labels. Then, the `[r,c]` matrix element contains the number
    of samples that were predicted to have label `r`, but in reality, have label `c`.
    Having access to a confusion matrix will allow us to calculate precision and recall.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个大小为 `(num_classes, num_classes)` 的二维矩阵，其中行对应于预测的类别标签，列对应于实际的类别标签。然后，`[r,c]`
    矩阵元素包含预测为标签 `r` 但实际上具有标签 `c` 的样本数量。通过访问混淆矩阵，我们可以计算精确率和召回率。
- en: 'Now, let''s implement a very simple way to calculate the confusion matrix.
    Similar to accuracy, we create a function with the same arguments, so it''s easy
    to reuse, by following the next steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一种非常简单的方式来计算混淆矩阵。类似于准确率，我们创建一个具有相同参数的函数，这样就可以通过以下步骤轻松重用：
- en: 'Assuming our labels are non-negative integers, we can figure out `num_classes`
    by taking the highest integer and adding `1` to account for zero, as follows:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们的标签是非负整数，我们可以通过取最高整数并加 `1` 来确定 `num_classes`，以考虑零，如下所示：
- en: '[PRE25]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we instantiate an empty matrix, where we will fill the counts, like this:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个空的矩阵，我们将在这里填充计数，如下所示：
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we iterate over all data, and, for each datum, we take predicted value
    `r` and actual value `c`, and we increment the appropriate value in the matrix.
    There are much faster ways to achieve this, but nothing is simpler than counting
    everything one by one. We do this with the following code:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们遍历所有数据，对于每个数据点，我们取预测值 `r` 和实际值 `c`，然后在矩阵中增加相应的值。虽然有许多更快的方法来实现这一点，但没有什么比逐个计数更简单了。我们使用以下代码来完成这项工作：
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After we have accounted for all the data in the training set, we can return
    our confusion matrix, as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们处理完训练集中的所有数据后，我们可以返回我们的混淆矩阵，如下所示：
- en: '[PRE28]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is our confusion matrix for the GTSRB dataset test data:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们的 GTSRB 数据集测试数据的混淆矩阵：
- en: '![](img/d1b2e14e-a3dc-4457-91b2-dad492d830fc.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1b2e14e-a3dc-4457-91b2-dad492d830fc.png)'
- en: As you can see, most of the values are in the diagonal. This means that at first
    glance, our classifier is doing pretty well.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，大多数值都在对角线上。这意味着乍一看，我们的分类器表现相当不错。
- en: 'It is also easy to calculate the accuracy from the confusion matrix as well.
    We just take the number of elements in the diagonal, and divide by the number
    of elements overall, like this:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从混淆矩阵中计算准确率也很容易。我们只需取对角线上的元素数量，然后除以总元素数量，如下所示：
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that we have a different number of elements in each of the classes. Each
    class contributes to accuracy differently, and our next metric will focus on per-class
    performance.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个类别中的元素数量都不同。每个类别对准确率的贡献不同，我们的下一个指标将专注于每个类别的性能。
- en: Precision
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确率
- en: Precision in binary classification is a useful metric for measuring the fraction
    of retrieved instances that are relevant (also called the **positive predictive
    value**). In a classification task, the number of **true positives** is defined
    as the number of items correctly labeled as belonging to the positive class.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类中，精度是一个有用的指标，用于衡量检索到的实例中有多少是相关的（也称为**阳性预测值**）。在分类任务中，**真阳性**的数量被定义为正确标记为属于正类别的项目数量。
- en: Precision is defined as the number of true positives divided by the total number
    of positives. In other words, out of all the pictures in the test set that a classifier
    thinks to contain a cat, precision is the fraction of pictures that actually do
    contain a cat.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 精度被定义为真阳性数量除以总阳性数量。换句话说，在测试集中，一个分类器认为包含猫的所有图片中，精度是实际包含猫的图片的比例。
- en: Note that here, we have a positive label; thus, precision is a per-class value.
    We usually talk about the precision of one class or the precision of cats, and
    so on.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，我们有一个正标签；因此，精度是每个类别的值。我们通常谈论一个类别的精度，或者猫的精度等等。
- en: 'The total number of positives can also be calculated as the sum of **true positives**
    and **false positives**, the latter being the number of samples incorrectly labeled
    as belonging to a particular class. This is where the confusion matrix comes in
    handy because it will allow us to quickly calculate the number of false positives
    and true positives by following the next steps:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 正确样本的总数也可以通过**真阳性**和**假阳性**的总和来计算，后者是指被错误标记为属于特定类别的样本数量。这就是混淆矩阵派上用场的地方，因为它将允许我们通过以下步骤快速计算出假阳性和真阳性的数量：
- en: 'So, in this case, we have to change our function arguments, and add the positive
    class label, like this:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，我们必须更改我们的函数参数，并添加正类标签，如下所示：
- en: '[PRE30]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s use our confusion matrix, and calculate the number of true positives,
    which will be the element at `[positive_label, positive_label]`, as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用我们的混淆矩阵，并计算真阳性的数量，这将是在`[positive_label, positive_label]`位置的元素，如下所示：
- en: '[PRE31]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s calculate the number of true and false positives, which will be
    the sum of all elements on the `positive_label` row since the row indicates the
    predicted class label, as follows:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们计算真阳性和假阳性的数量，这将等于`positive_label`行上所有元素的总和，因为该行表示预测的类别标签，如下所示：
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And finally, return the ratio of true positives and all positives, like this:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，返回真阳性与所有正性的比率，如下所示：
- en: '[PRE33]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Based on different classes, we get very different values of precision. Here
    is a histogram of the precision scores for all 43 classes:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同的类别，我们得到非常不同的精度值。以下是所有43个类别的精度分数直方图：
- en: '![](img/9988c2b9-ae74-42c6-8586-54db08e2b2c2.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9988c2b9-ae74-42c6-8586-54db08e2b2c2.png)'
- en: 'The class with lower precision is 30, which means that a lot of other signs
    are mistaken to be the sign shown in the following screenshot:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 精度较低的类别是30，这意味着很多其他标志被错误地认为是以下截图中的标志：
- en: '![](img/d3c3138f-b7f6-4796-97a5-82469fb203be.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3c3138f-b7f6-4796-97a5-82469fb203be.png)'
- en: In this case, it's alright if we are extra cautious while driving on the icy
    road, but it's possible that we missed something important. So, let's look at
    recall values for different classes.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们在结冰的道路上驾驶时格外小心是可以的，但可能我们错过了某些重要的事情。因此，让我们看看不同类别的召回值。
- en: Recall
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 召回
- en: Recall is similar to precision in the sense that it measures the fraction of
    relevant instances that are retrieved (as opposed to the fraction of retrieved
    instances that are relevant). Thus, it will tell us the probability that we will
    not notice it for a given positive class (given sign).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 召回与精度相似，因为它衡量的是检索到的相关实例的比例（而不是检索到的实例中有多少是相关的）。因此，它将告诉我们对于给定的正类（给定的标志），我们不会注意到它的概率。
- en: In a classification task, the number of false negatives is the number of items
    that are not labeled as belonging to the positive class but should have been labeled.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类任务中，**假阴性**的数量是指那些没有被标记为属于正类别的项目，但实际上应该被标记的项目数量。
- en: Recall is the number of true positives divided by the sum of true positives
    and false negatives. In other words, out of all the pictures of cats in the world,
    recall is the fraction of pictures that have been correctly identified as pictures
    of cats.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 召回是真阳性数量除以真阳性和假阴性总数。换句话说，在世界上所有猫的图片中，召回是正确识别为猫的图片的比例。
- en: 'Here is how to calculate recall of a given positive label using true and predicted
    labels:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何使用真实标签和预测标签来计算给定正标签的召回率的：
- en: 'Again, we have the same signature as for the precision, and we retrieve true
    positives the same way, as follows:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们与精度有相同的签名，并且以相同的方式检索真实正例，如下所示：
- en: '[PRE34]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, notice that the sum of true positives and false negatives is the total
    number of points in the given data class.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意，真实正例和假负例的总和是给定数据类中的点总数。
- en: 'Thus, we just have to count the number of elements in that class, which means
    we sum the `positive_label` column of the confusion matrix, as follows:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们只需计算该类别的元素数量，这意味着我们求混淆矩阵中`positive_label`列的和，如下所示：
- en: '[PRE35]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we return the ratio as for the precision function, like this:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们像精度函数一样返回比率，如下所示：
- en: '[PRE36]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s look at the distribution of recall values for all 43 classes of
    traffic signs, shown in the following screenshot:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看以下截图所示的所有43个交通标志类别的召回值分布：
- en: '![](img/15d10f43-3ea6-4eda-86c9-15cd242e0a64.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/15d10f43-3ea6-4eda-86c9-15cd242e0a64.png)'
- en: 'The recall values are a lot more spread out, with class 21 having a value of
    0.66\. Let''s check out which class has a value of 21:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 召回值分布得更广，第21个类别的值为0.66。让我们检查哪个类别的值为21：
- en: '![](img/f779bebe-02c8-40d0-9a43-9dcb761b09d0.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f779bebe-02c8-40d0-9a43-9dcb761b09d0.png)'
- en: Now, this is not as harmful as driving on a road covered with snowflakes/ice,
    but it's very important not to miss dangerous curves ahead on the road. Missing
    this sign could have bad consequences.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这并不像在覆盖着雪花/冰的道路上驾驶那样有害，但非常重要，不要错过路上的危险弯道。错过这个标志可能会产生不良后果。
- en: The next section will demonstrate the `main()` function routine needed to run
    our app.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将演示运行我们的应用程序所需的`main()`函数例程。
- en: Putting it all together
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: 'To run our app, we will need to execute the main function routine (in `chapter6.py`).
    This loads the data, trains the classifier, evaluates its performance, and visualizes
    the result:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行我们的应用程序，我们需要执行主函数例程（在`chapter6.py`中）。这加载数据，训练分类器，评估其性能，并可视化结果：
- en: 'First, we need to import all the relevant modules and set up the `main` function,
    as follows:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入所有相关模块并设置`main`函数，如下所示：
- en: '[PRE37]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, the goal is to compare classification performance across feature extraction
    methods. This includes running the task using a list of different feature extraction
    approaches. So, we first load the data, and repeat the process for each of the
    featurizing functions, as follows:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，目标是比较不同特征提取方法的分类性能。这包括使用不同特征提取方法运行任务。因此，我们首先加载数据，并重复对每个特征化函数进行过程，如下所示：
- en: '[PRE38]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'For each of the `featurize` functions, we perform the following steps:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个`featurize`函数，我们执行以下步骤：
- en: '`Featurize` the data, so we have a matrix of features, like this:'
  id: totrans-284
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Featurize`数据，以便我们有一个特征矩阵，如下所示：'
- en: '[PRE39]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Train a model using our `train_one_vs_all_SVM` method, as follows:'
  id: totrans-286
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的`train_one_vs_all_SVM`方法训练模型，如下所示：
- en: '[PRE40]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Predict test labels for the training data, by featurizing the test data and
    passing to the `predict` method (we have to featurize test data separately to
    make sure we don''t have information leakage), as follows:'
  id: totrans-288
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对测试数据进行特征化并将结果传递给`predict`方法（我们必须单独对测试数据进行特征化以确保没有信息泄露），为训练数据预测测试标签，如下所示：
- en: '[PRE41]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We score the predicted labels against the true labels, using the `accuracy`
    function, and store the score in a dictionary, to plot after we have results for
    all the `featurize` functions, like this:'
  id: totrans-290
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`accuracy`函数对预测标签和真实标签进行评分，并将分数存储在字典中，以便在所有`featurize`函数的结果出来后进行绘图，如下所示：
- en: '[PRE42]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, it''s time to plot the results, and for this, we choose the `bar` plot
    functionality of `matplotlib`. We also make sure to scale the bar plot accordingly
    to visually understand the scale of difference. Since the accuracy is a number
    between `0` and `1`, we limit the *y* axis to `[0, 1]`, as follows:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候绘制结果了，为此，我们选择了`matplotlib`的`bar`图功能。我们还确保相应地缩放条形图，以便直观地理解差异的规模。由于准确度是一个介于`0`和`1`之间的数字，我们将`y`轴限制在`[0,
    1]`，如下所示：
- en: '[PRE43]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We add some nice formatting to the plot by rotating labels on the horizontal
    axis, adding a `grid` and a `title` to the plot, like this:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过在水平轴上旋转标签、添加`grid`和`title`来为绘图添加一些漂亮的格式化，如下所示：
- en: '[PRE44]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'And after the last line of `plt.show()` has executed, the plot shown in the
    following screenshot pops up in a separate window:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且在执行`plt.show()`的最后一行之后，以下截图所示的绘图在单独的窗口中弹出：
- en: '![](img/fdf5a433-8f27-4d8b-9100-683d862e9fc3.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fdf5a433-8f27-4d8b-9100-683d862e9fc3.png)'
- en: So, we see that `hog_featurize` is a winner on this dataset, but we are far
    from having perfect results—slightly above 95%. To understand how good a result
    it's possible to get, you could do a quick Google search, and you will come across
    a lot of papers achieving 99%+ accuracy. So, even though we are not getting cutting-edge
    results, we did pretty well with an off-the-shelf classifier and an easy `featurize`
    function.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到`hog_featurize`在这个数据集上是一个赢家，但我们离完美的结果还远着呢——略高于95%。要了解可能得到多好的结果，你可以快速进行一次谷歌搜索，你会找到很多实现99%+精度的论文。所以，尽管我们没有得到最前沿的结果，但我们使用现成的分类器和简单的`featurize`函数做得相当不错。
- en: Another interesting fact is that even though we thought that traffic signs having
    bright colors should lead to hsv_featurize (it being more important than grayscale
    features), it turns out that's not the case.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的事实是，尽管我们认为具有鲜艳颜色的交通标志应该使用hsv_featurize（它比灰度特征更重要），但事实并非如此。
- en: So, a good takeaway is that you should experiment with your data to develop
    better intuition about which features work for your data and which don't.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个很好的经验法则是你应该对你的数据进行实验，以发展更好的直觉，了解哪些特征对你的数据有效，哪些无效。
- en: Speaking of experimentation, let's use a neural network to increase the efficiency
    of our obtained results.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 说到实验，让我们用一个神经网络来提高我们获得的结果的效率。
- en: Improving results with neural networks
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络提高结果
- en: Let's do a quick teaser of how good we could get if we were to use some fancy
    **deep neural networks** (**DNNs**), and give you a sneak peek of what is to come
    in the future chapters of this book.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速展示一下，如果我们使用一些花哨的**深度神经网络**（**DNNs**），我们可能会达到多好的水平，并给你一个关于本书未来章节内容的预览。
- en: If we use the following "*not quite so deep"* neural network, which takes about
    2 minutes to train on my laptop (where it takes 1* minute* to train the SVM),
    we get an accuracy of around 0.964!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用以下“不太深”的神经网络，在我的笔记本电脑上训练大约需要2分钟（而SVM的训练只需要1分钟），我们得到的准确率大约为0.964！
- en: 'Here is a snippet of the training method (you should be able to plug it into
    the preceding code, and play with some parameters to see if you could do it later):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是训练方法的一个片段（你应该能够将其插入到前面的代码中，并调整一些参数以查看你能否在以后做到）：
- en: '[PRE45]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The code uses the high-level Keras API of TensorFlow (we will see more of this
    in the upcoming chapters) and creates a neural network with the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用了TensorFlow的高级Keras API（我们将在接下来的章节中看到更多），并创建了一个具有以下结构的神经网络：
- en: '**Convolutional layer** with max pooling that is followed by a dropout—which
    is there only during the training.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**带有最大池化，后面跟着一个dropout——它只在训练期间存在。'
- en: '**Hidden Dense layer** that is followed by a dropout—which is there only during
    the training.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏密集层**后面跟着一个dropout——它只在训练期间存在。'
- en: '**Final Dense layer** that spits out the final result; it should identify which
    class (among the 43 classes) the input data belongs to.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终密集层**输出最终结果；它应该识别输入数据属于哪个类别（在43个类别中）。'
- en: Note that we only have one convolutional layer, which is very similar to HOG
    featurize. If we were to add more convolutional layers, the performance would
    improve quite a lot, but let's leave that for the next chapters to explore.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们只有一个卷积层，这与HOG特征化非常相似。如果我们增加更多的卷积层，性能会显著提高，但让我们把这一点留到下一章去探索。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we trained a multiclass classifier to recognize traffic signs
    from the GTSRB database. We discussed the basics of supervised learning, explored
    the intricacies of feature extraction, and sneaked a peek into DNNs.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们训练了一个多类分类器来识别GTSRB数据库中的交通标志。我们讨论了监督学习的基础，探讨了特征提取的复杂性，并简要介绍了深度神经网络（DNNs）。
- en: Using the approach we took in this chapter, you should be able to formulate
    real-life problems as machine learning models, use your Python skills to download
    a sample labeled dataset from the internet, write your featurizing functions that
    convert images to feature vectors, and use OpenCV for training off-the-shelf machine
    learning models that help you solve your real-life problems.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章中采用的方法，你应该能够将现实生活中的问题表述为机器学习模型，使用你的Python技能从互联网上下载一个标记的样本数据集，编写将图像转换为特征向量的特征化函数，并使用OpenCV来训练现成的机器学习模型，帮助你解决现实生活中的问题。
- en: Notably, we left out some details along the way, such as attempting to fine-tune
    the hyperparameters of the learning algorithm (as they were out of the scope of
    this book). We only looked at accuracy scores and didn't do much feature engineering
    by trying to combine all sets of different features.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们在过程中省略了一些细节，例如尝试微调学习算法的超参数（因为它们超出了本书的范围）。我们只关注准确率分数，并没有通过尝试结合所有不同特征集进行很多特征工程。
- en: With this functional setup and a good understanding of the underlying methodology,
    you can now classify the entire GTSRB dataset to get accuracies higher than 0.97!
    How about 0.99? It is definitely worth taking a look at their website, where you
    will find classification results for a variety of classifiers. Maybe your own
    approach will soon be added to the list.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个功能设置和对其底层方法论的充分理解下，你现在可以分类整个GTSRB数据集，以获得高于0.97的准确率！0.99呢？这绝对值得查看他们的网站，在那里你可以找到各种分类器的分类结果。也许你的方法很快就会被添加到列表中。
- en: In the next chapter, we will move even deeper into the field of machine learning.
    Specifically, we will focus on recognizing emotional expressions in human faces
    using **convolutional neural networks** (**CNNs**). This time, we will combine
    the classifier with a framework for object detection, which will allow us to find
    a human face in an image, and then focus on identifying the emotional expression
    contained in that face.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨机器学习的领域。具体来说，我们将专注于使用**卷积神经网络**（**CNNs**）来识别人类面部表情。这一次，我们将结合分类器与一个目标检测框架，这将使我们能够在图像中找到人脸，然后专注于识别该人脸中包含的情感表情。
- en: Dataset attribution
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集归属
- en: J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, The German Traffic Sign
    Recognition Benchmark—A multiclass classification competition, in *Proceedings
    of the IEEE International Joint Conference on Neural Networks*, 2011, pages 1453–1460.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, The German Traffic Sign
    Recognition Benchmark—A multiclass classification competition, in *Proceedings
    of the IEEE International Joint Conference on Neural Networks*, 2011, pages 1453–1460.
