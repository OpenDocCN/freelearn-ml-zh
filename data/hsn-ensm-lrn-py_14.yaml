- en: Classifying Fraudulent Transactions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类欺诈交易
- en: In this chapter, we will attempt to classify fraudulent transactions in a dataset
    concerning credit card transactions from European card holders that occurred during
    September 2013\. The main problem in this dataset is the extremely small number
    of fraudulent transactions, compared to the dataset's size. These types of datasets
    are called unbalanced, as there are unequal percentages of each label. We will
    try to create ensembles that can classify our particular dataset, which contains
    a small number of fraudulent transactions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试对2013年9月期间发生的欧洲信用卡持有者的信用卡交易数据集进行欺诈交易分类。该数据集的主要问题是，欺诈交易的数量非常少，相比数据集的规模几乎可以忽略不计。这类数据集被称为不平衡数据集，因为每个标签的百分比不相等。我们将尝试创建能够分类我们特定数据集的集成方法，该数据集包含极少数的欺诈交易。
- en: 'In this chapter we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Getting familiar with the dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉数据集
- en: Exploratory analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性分析
- en: Voting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票法
- en: Stacking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠
- en: Bagging
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助法
- en: Boosting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升法
- en: Using random forests
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林
- en: Comparative analysis of ensembles
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法的比较分析
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备机器学习技术和算法的基础知识。此外，还需要了解Python的约定和语法。最后，熟悉NumPy库将有助于读者理解一些自定义算法的实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter09)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter09)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2ShwarF](http://bit.ly/2ShwarF)[.](http://bit.ly/2ShwarF)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下视频，了解代码的实际应用：[http://bit.ly/2ShwarF](http://bit.ly/2ShwarF)[.](http://bit.ly/2ShwarF)
- en: Getting familiar with the dataset
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熟悉数据集
- en: The dataset was originally utilized in the PhD thesis of Andrea Dal Pozzolo,
    [Adaptive Machine learning for credit card fraud detection](http://di.ulb.ac.be/map/adalpozz/pdf/Dalpozzolo2015PhD.pdf)
    ULB MLG, and has since been released by its authors for public use ([www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata](http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata)).
    The dataset contains more than 284,000 instances, but only 492 instances of fraud
    (almost 0.17%).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集最初在Andrea Dal Pozzolo的博士论文《[用于信用卡欺诈检测的自适应机器学习](http://di.ulb.ac.be/map/adalpozz/pdf/Dalpozzolo2015PhD.pdf)》中使用，现已由其作者公开发布供公众使用（[www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata](http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata)）。该数据集包含超过284,000个实例，但其中只有492个欺诈实例（几乎为0.17%）。
- en: 'Its target class value is 0 if the transaction was not a fraud, and 1 if it
    was. The dataset''s features are a number of principal components, as the dataset
    has been transformed using **Principle Components Analysis** (**PCA**), in order
    to retain the confidentiality of the data. The dataset''s features are comprised
    of 28 PCA components, as well as the transaction’s amount and the time elapsed
    from the first transaction in the dataset. Descriptive statistics about the dataset
    are provided as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目标类别值为0时表示交易不是欺诈，1时表示交易是欺诈。该数据集的特征是一些主成分，因为数据集已通过**主成分分析**（**PCA**）进行转化，以保留数据的机密性。数据集的特征包含28个PCA组件，以及交易金额和从数据集中的第一次交易到当前交易的时间。以下是数据集的描述性统计：
- en: '| **Feature** | **Time** | **V1** | **V2** | **V3** | **V4** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **时间** | **V1** | **V2** | **V3** | **V4** |'
- en: '| **count** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
- en: '| **mean** | 94,813.86 | 1.17E-15 | 3.42E-16 | -1.37E-15 | 2.09E-15 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 94,813.86 | 1.17E-15 | 3.42E-16 | -1.37E-15 | 2.09E-15 |'
- en: '| **std** | 47,488.15 | 1.96 | 1.65 | 1.52 | 1.42 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 47,488.15 | 1.96 | 1.65 | 1.52 | 1.42 |'
- en: '| **min** | 0.00 | -56.41 | -72.72 | -48.33 | -5.68 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | 0.00 | -56.41 | -72.72 | -48.33 | -5.68 |'
- en: '| **max** | 172,792.00 | 2.45 | 22.06 | 9.38 | 16.88 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 172,792.00 | 2.45 | 22.06 | 9.38 | 16.88 |'
- en: '| **Feature** | **V5** | **V6** | **V7** | **V8** | **V9** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **V5** | **V6** | **V7** | **V8** | **V9** |'
- en: '| **count** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
- en: '| **mean** | 9.60E-16 | 1.49E-15 | -5.56E-16 | 1.18E-16 | -2.41E-15 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 9.60E-16 | 1.49E-15 | -5.56E-16 | 1.18E-16 | -2.41E-15 |'
- en: '| **std** | 1.38 | 1.33 | 1.24 | 1.19 | 1.10 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 1.38 | 1.33 | 1.24 | 1.19 | 1.10 |'
- en: '| **min** | -113.74 | -26.16 | -43.56 | -73.22 | -13.43 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | -113.74 | -26.16 | -43.56 | -73.22 | -13.43 |'
- en: '| **max** | 34.80 | 73.30 | 120.59 | 20.01 | 15.59 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 34.80 | 73.30 | 120.59 | 20.01 | 15.59 |'
- en: '| **Feature** | **V10** | **V11** | **V12** | **V13** | **V14** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **V10** | **V11** | **V12** | **V13** | **V14** |'
- en: '| **count** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
- en: '| **mean** | 2.24E-15 | 1.67E-15 | -1.25E-15 | 8.18E-16 | 1.21E-15 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 2.24E-15 | 1.67E-15 | -1.25E-15 | 8.18E-16 | 1.21E-15 |'
- en: '| **std** | 1.09 | 1.02 | 1.00 | 1.00 | 0.96 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 1.09 | 1.02 | 1.00 | 1.00 | 0.96 |'
- en: '| **min** | -24.59 | -4.80 | -18.68 | -5.79 | -19.21 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | -24.59 | -4.80 | -18.68 | -5.79 | -19.21 |'
- en: '| **max** | 23.75 | 12.02 | 7.85 | 7.13 | 10.53 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 23.75 | 12.02 | 7.85 | 7.13 | 10.53 |'
- en: '| **Feature** | **V15** | **V16** | **V17** | **V18** | **V19** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **V15** | **V16** | **V17** | **V18** | **V19** |'
- en: '| **count** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
- en: '| **mean** | 4.91E-15 | 1.44E-15 | -3.80E-16 | 9.57E-16 | 1.04E-15 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 4.91E-15 | 1.44E-15 | -3.80E-16 | 9.57E-16 | 1.04E-15 |'
- en: '| **std** | 0.92 | 0.88 | 0.85 | 0.84 | 0.81 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 0.92 | 0.88 | 0.85 | 0.84 | 0.81 |'
- en: '| **min** | -4.50 | -14.13 | -25.16 | -9.50 | -7.21 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | -4.50 | -14.13 | -25.16 | -9.50 | -7.21 |'
- en: '| **max** | 8.88 | 17.32 | 9.25 | 5.04 | 5.59 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 8.88 | 17.32 | 9.25 | 5.04 | 5.59 |'
- en: '| **Feature** | **V20** | **V21** | **V22** | **V23** | **V24** |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **V20** | **V21** | **V22** | **V23** | **V24** |'
- en: '| **count** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
- en: '| **mean** | 6.41E-16 | 1.66E-16 | -3.44E-16 | 2.58E-16 | 4.47E-15 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 6.41E-16 | 1.66E-16 | -3.44E-16 | 2.58E-16 | 4.47E-15 |'
- en: '| **std** | 0.77 | 0.73 | 0.73 | 0.62 | 0.61 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 0.77 | 0.73 | 0.73 | 0.62 | 0.61 |'
- en: '| **min** | -54.50 | -34.83 | -10.93 | -44.81 | -2.84 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | -54.50 | -34.83 | -10.93 | -44.81 | -2.84 |'
- en: '| **max** | 39.42 | 27.20 | 10.50 | 22.53 | 4.58 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 39.42 | 27.20 | 10.50 | 22.53 | 4.58 |'
- en: '| **Feature** | **V25** | **V26** | **V27** | **V28** | **Amount** |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **V25** | **V26** | **V27** | **V28** | **金额** |'
- en: '| **count** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **计数** | 284,807 | 284,807 | 284,807 | 284,807 | 284,807 |'
- en: '| **mean** | 5.34E-16 | 1.69E-15 | -3.67E-16 | -1.22E-16 | 88.34962 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **均值** | 5.34E-16 | 1.69E-15 | -3.67E-16 | -1.22E-16 | 88.34962 |'
- en: '| **std** | 0.52 | 0.48 | 0.40 | 0.33 | 250.12 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 0.52 | 0.48 | 0.40 | 0.33 | 250.12 |'
- en: '| **min** | -10.30 | -2.60 | -22.57 | -15.43 | 0.00 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **最小值** | -10.30 | -2.60 | -22.57 | -15.43 | 0.00 |'
- en: '| **max** | 7.52 | 3.52 | 31.61 | 33.85 | 25,691.16 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **最大值** | 7.52 | 3.52 | 31.61 | 33.85 | 25,691.16 |'
- en: Descriptive statistics of the credit card transaction dataset
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 信用卡交易数据集的描述性统计
- en: Exploratory analysis
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性分析
- en: 'One important characteristic of the dataset is that there are no missing values,
    as it is indicated by the count statistic. All features have the same number of
    values. Another important aspect is that most features are normalized. This is
    due to the PCA applied to the data. PCA normalizes the data before decomposing
    it into principal components. The only two features not normalized are the **Time**
    and **Amount** features. The following histogram for each feature is depicted:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的一个重要特点是没有缺失值，这一点可以从计数统计中看出。所有特征都有相同数量的值。另一个重要方面是大多数特征已经进行了归一化处理。这是因为数据应用了PCA（主成分分析）。PCA在将数据分解为主成分之前会先进行归一化处理。唯一两个没有进行归一化的特征是**时间**和**金额**。以下是每个特征的直方图：
- en: '![](img/d2650536-5a54-41da-a4de-3ece14168987.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2650536-5a54-41da-a4de-3ece14168987.png)'
- en: Histograms for the dataset's features
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集特征的直方图
- en: 'It is interesting to examine more closely the **Time** and **Amount** of each
    transaction. In the **Time** histogram, we notice a sudden drop in transaction
    frequency between 75,000 and 125,000 seconds after the first transaction (around
    13 hours). This is probably due to daily time cycles (for example, during the
    night, when most stores are closed). The histogram for each transaction''s amount
    is provided as follows in the logarithmic scale. It is evident that most transactions
    concern small amounts, with the average being almost €88.00:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更加细致地观察每笔交易的**时间**和**金额**，我们发现，在第一次交易后的75,000秒到125,000秒之间，交易频率出现了突然下降（大约13小时）。这很可能是由于日常时间周期（例如，夜间大多数商店关闭）。每笔交易金额的直方图如下所示，采用对数尺度。可以明显看出，大部分交易金额较小，平均值接近88.00欧元：
- en: '![](img/4a7a51a9-e8e5-4a22-99a5-91c40bea7ca9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a7a51a9-e8e5-4a22-99a5-91c40bea7ca9.png)'
- en: Histogram for amount, logarithmic scale for *y*-axis
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 金额直方图，对数尺度的*y*-轴
- en: 'In order to avoid problems with uneven distribution of weights between features,
    we will standardize the features **Amount** and **Time**. Algorithms that employ
    distance metrics for example (such as K-Nearest Neighbors), can under perform
    when features are not scaled correctly. The standardized features'' histograms
    are provided as follows. Note that standardization transforms the variables in
    order to have a mean value close to 0 and standard deviation of 1:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免特征之间的权重分布不均问题，我们将对**金额**和**时间**这两个特征进行标准化。比如使用距离度量的算法（如K最近邻算法）在特征未正确缩放时，性能可能会下降。以下是标准化特征的直方图。请注意，标准化将变量转换为均值接近0，标准差为1：
- en: '![](img/eb30b820-bc32-49e7-bc9b-1d80402ae12c.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb30b820-bc32-49e7-bc9b-1d80402ae12c.png)'
- en: Standardized amount histogram
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化金额直方图
- en: 'The following plot depicts the histogram for standardized time. We can see
    that it does not affect the drop in transactions during the night time:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了标准化时间的直方图。我们可以看到，它并未影响夜间交易量的下降：
- en: '![](img/0be32d14-cf53-4fd0-9cd8-b49d1b786ece.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0be32d14-cf53-4fd0-9cd8-b49d1b786ece.png)'
- en: Standardized time histogram
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化时间直方图
- en: Evaluation methods
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估方法
- en: As our dataset is highly skewed (that is, it has a high degree of class imbalance),
    we cannot utilize accuracy in order to evaluate our models. This is due to the
    fact that by classifying all instances as non-frauds, we can achieve an accuracy
    of 99.82%. Certainly, this number does not represent an acceptable performance,
    as we are unable to detect any fraudulent transactions. Thus, in order to evaluate
    our models, we will use recall (the percentage of frauds we detected) and F1 score,
    a weighted average between recall and precision (a measure of how many of the
    transactions predicted as fraudulent were indeed fraudulent).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集高度倾斜（即具有较高的类别不平衡），我们不能仅仅通过准确率来评估模型的表现。因为如果将所有实例都分类为非欺诈行为，我们的准确率可以达到99.82%。显然，这个数字并不代表一个可接受的表现，因为我们根本无法检测到任何欺诈交易。因此，为了评估我们的模型，我们将使用召回率（即我们检测到的欺诈行为的百分比）和F1得分，后者是召回率和精确度的加权平均值（精确度衡量的是预测为欺诈的交易中，实际为欺诈的比例）。
- en: Voting
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票
- en: In this section, we will try to classify the dataset by using voting ensembles.
    For our initial ensemble, we will utilize a Naive Bayes classifier, a logistic
    regression, and a decision tree. This will be implemented in two parts, first
    by testing each base learner itself and then combining the base learners into
    an ensemble.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将尝试通过使用投票集成方法来对数据集进行分类。对于我们的初步集成方法，我们将利用朴素贝叶斯分类器、逻辑回归和决策树。这个过程将分为两部分，首先测试每个基础学习器本身，然后将这些基础学习器组合成一个集成模型。
- en: Testing the base learners
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试基础学习器
- en: 'To test the base learners, we will benchmark the base learners by themselves,
    which will help us gauge how well they perform on their own. In order to do so,
    first, we load the libraries and dataset and then split the data with 70% in the
    train set and 30% in the test set. We use `pandas` in order to easily import the
    CSV. Our goal is to train and evaluate each individual base learner before we
    train and evaluate the ensemble as a whole:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试基础学习器，我们将单独对基础学习器进行基准测试，这将帮助我们评估它们单独表现的好坏。为此，首先加载库和数据集，然后将数据划分为70%的训练集和30%的测试集。我们使用`pandas`来轻松导入CSV文件。我们的目标是在训练和评估整个集成模型之前，先训练和评估每个单独的基础学习器：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After loading the libraries and data, we train each classifier and print the
    required metrics from the `sklearn.metrics` package. F1 score is implemented by
    the `f1_score` function and recall is implemented by the `recall_score` function.
    The decision tree is restricted to a maximum depth of three (`max_depth=3`), in
    order to avoid overfitting:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载库和数据后，我们训练每个分类器，并打印出来自`sklearn.metrics`包的必要指标。F1得分通过`f1_score`函数实现，召回率通过`recall_score`函数实现。为了避免过拟合，决策树的最大深度被限制为三（`max_depth=3`）：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The results are depicted in the following table. As is evident, the decision
    tree outperforms the other three learners. Naive Bayes has a higher recall score,
    but its F1 score is considerably worse, compared to the decision tree:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在以下表格中有所展示。显然，决策树的表现优于其他三个学习器。朴素贝叶斯的召回率较高，但其F1得分相较于决策树要差得多：
- en: '| **Learner** | **Metric** | **Value** |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **学习器** | **指标** | **值** |'
- en: '| **Decision Tree** | F1 | 0.770 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **决策树** | F1 | 0.770 |'
- en: '|  | Recall | 0.713 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.713 |'
- en: '| **Naive Bayes** | F1 | 0.107 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **朴素贝叶斯** | F1 | 0.107 |'
- en: '|  | Recall | 0.824 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.824 |'
- en: '| **Logistic Regression** | F1 | 0.751 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| **逻辑回归** | F1 | 0.751 |'
- en: '|  | Recall | 0.632 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.632 |'
- en: 'We can also experiment with the number of features present in the dataset.
    By plotting their correlation to the target, we can filter out features that present
    low correlation to the target. This table depicts each feature''s correlation
    to the target:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以实验数据集中包含的特征数量。通过绘制它们与目标的相关性，我们可以过滤掉那些与目标相关性较低的特征。此表格展示了每个特征与目标的相关性：
- en: '![](img/a82e49d0-da26-4a48-90ad-b643051f7851.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a82e49d0-da26-4a48-90ad-b643051f7851.png)'
- en: Correlation between each variable and the target
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变量与目标之间的相关性
- en: By filtering any feature with a lower absolute value than 0.1, we hope that
    the base learners will be able to better detect the fraudulent transactions, as
    the dataset's noise will be reduced.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过过滤掉任何绝对值小于0.1的特征，我们希望基本学习器能够更好地检测欺诈交易，因为数据集的噪声将会减少。
- en: In order to test our theory, we repeat the experiment, but remove any columns
    from the DataFrame where the absolute correlation is lower than 0.1, as indicated
    by `fs = list(correlations[(abs(correlations)>threshold)].index.values)`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的理论，我们重复实验，但删除DataFrame中任何绝对相关性低于0.1的列，正如`fs = list(correlations[(abs(correlations)>threshold)].index.values)`所示。
- en: 'Here, `fs` holds all column names with a correlation greater than the indicated
    threshold:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`fs`包含了所有与指定阈值相关性大于的列名：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Again, we present the results in the following table. As we can see, the decision
    tree has increased its F1 score, while reducing its recall. Naive Bayes has improved
    on both metrics, while the logistic regression model has become considerably worse:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们展示了以下表格中的结果。正如我们所看到的，决策树提高了其F1得分，同时降低了召回率。朴素贝叶斯在两个指标上都有所提升，而逻辑回归模型的表现大幅下降：
- en: '| **Learner** | **Metric** | **Value** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **学习器** | **指标** | **值** |'
- en: '| **Decision Tree** | F1 | 0.785 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **决策树** | F1 | 0.785 |'
- en: '|  | Recall | 0.699 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.699 |'
- en: '| **Naive Bayes** | F1 | 0.208 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **朴素贝叶斯** | F1 | 0.208 |'
- en: '|  | Recall | 0.846 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.846 |'
- en: '| **Logistic Regression** | F1 | 0.735 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **逻辑回归** | F1 | 0.735 |'
- en: '|  | Recall | 0.610 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.610 |'
- en: Performance metrics for the three base learners for the filtered dataset
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤数据集上三个基本学习器的性能指标
- en: Optimizing the decision tree
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化决策树
- en: We can try to optimize the tree's depth in order to maximize F1 or recall. In
    order to do so, we will experiment with depths in the range of *[3, 11]* on the
    train set.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试优化树的深度，以最大化F1或召回率。为此，我们将在训练集上尝试深度范围为* [3, 11] *的不同深度。
- en: 'The following graph depicts the F1 score and recall for the various maximum
    depths, both for the original and filtered datasets:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了不同最大深度下的F1得分和召回率，包括原始数据集和过滤后的数据集：
- en: '![](img/eacb8254-b19c-4a12-be0a-cc9e23b99e35.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eacb8254-b19c-4a12-be0a-cc9e23b99e35.png)'
- en: Test metrics for various tree depths
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不同树深度的测试指标
- en: Here, we observe that for a maximum depth of 5, F1 and recall are optimized
    for the filtered dataset. Furthermore, recall is optimized for the original dataset
    as well. We will continue with a maximum depth of 5 as trying to further optimize
    the metrics can lead to overfitting, especially since the number of instances
    relevant to the metrics is extremely small. Furthermore, with a maximum depth
    of 5, there is an improvement both in F1, as well as in recall, when the filtered
    dataset is used.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们观察到，对于最大深度为5的情况，F1和召回率在过滤后的数据集上得到了优化。此外，召回率在原始数据集上也得到了优化。我们将继续使用最大深度为5，因为进一步优化这些指标可能会导致过拟合，尤其是在与这些指标相关的实例数量极其少的情况下。此外，使用最大深度为5时，在使用过滤后的数据集时，F1和召回率都有所提高。
- en: Creating the ensemble
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集成模型
- en: 'We can now proceed and create the ensemble. Again, we will first evaluate the
    ensemble on the original dataset, and then proceed to test it on the filtered
    dataset. The code is similar to the previous example. First, we load the libraries
    and data, and create train and test splits as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续创建集成模型。再次，我们将首先在原始数据集上评估集成模型，然后在过滤后的数据集上进行测试。代码与之前的示例相似。首先，我们加载库和数据，并按以下方式创建训练集和测试集的划分：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After loading the required libraries and data, we create our ensemble, and
    then train and evaluate it. Finally, we repeat the experiment as follows with
    reduced features by filtering out features with low correlations to the target
    variable:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载所需的库和数据后，我们创建集成模型，然后对其进行训练和评估。最后，我们按照以下方式通过过滤掉与目标变量相关性较低的特征来减少特征，从而重复实验：
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following table summarizes the results. For the original dataset, voting
    provides a model with a better combination of F1 and recall, compared to any single
    classifier.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了结果。对于原始数据集，投票模型提供了比任何单一分类器更好的 F1 和召回率的组合。
- en: 'Still, the decision tree with a maximum depth of 5 slightly outperforms it
    in F1 score, while Naive Bayes is able to recall a greater percentage of fraudulent
    transactions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最大深度为五的决策树在 F1 分数上稍微超越了它，而朴素贝叶斯能够回忆起更多的欺诈交易：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.822 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.822 |'
- en: '|  | Recall | 0.779 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.779 |'
- en: '| **Filtered** | F1 | 0.828 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.828 |'
- en: '|  | Recall | 0.794 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.794 |'
- en: Voting results for both datasets
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对两个数据集的投票结果
- en: We can try to further diversify our ensemble by also including two additional
    Decision Trees, with maximum depth of three and eight, respectively. This boosts
    the ensemble’s performance to the following numbers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加两个额外的决策树，分别具有最大深度为三和八，进一步多样化我们的集成模型。这将集成模型的性能提升至以下数值。
- en: 'Although the performance remains the same for the filtered dataset, the ensemble
    is able to perform better in the original dataset. Especially for the F1 metric,
    it is able to outperform all other dataset/model combinations:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在过滤数据集上的性能保持不变，但该集成模型在原始数据集上的表现有所提升。特别是在 F1 指标上，它能够超越所有其他数据集/模型组合：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.829 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.829 |'
- en: '|  | Recall | 0.787 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.787 |'
- en: '| **Filtered** | F1 | 0.828 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.828 |'
- en: '|  | Recall | 0.794 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.794 |'
- en: Voting results for both datasets with two additional decision trees
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对两个数据集使用额外两个决策树的投票结果
- en: Stacking
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠
- en: We can also try to stack the base learners, instead of using Voting. First,
    we will try to stack a single decision tree with depth five, a Naive Bayes classifier,
    and a logistic regression. As a meta-learner, we will use a logistic regression.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以尝试将基本学习器堆叠，而不是使用投票。首先，我们将尝试堆叠一个深度为五的决策树，一个朴素贝叶斯分类器和一个逻辑回归模型。作为元学习器，我们将使用逻辑回归。
- en: 'The following code is responsible for loading the required libraries and data,
    training, and evaluating the ensemble on the original and filtered datasets. We
    first load the required libraries and data, while creating train and test splits:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码负责加载所需的库和数据、训练和评估原始数据集和过滤数据集上的集成模型。我们首先加载所需的库和数据，并创建训练集和测试集分割：
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After creating our train and test splits, we train and evaluate our ensemble
    on the original dataset, as well as a reduced-features dataset as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建训练集和测试集分割后，我们在原始数据集以及减少特征的数据集上训练并评估集成模型，如下所示：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As it is seen in the following resultant table, the ensemble achieves a slightly
    better F1 score on the original dataset, but worse recall score, compared to the
    voting ensemble with the same base learners:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表所示，该集成模型在原始数据集上取得了略高的 F1 分数，但召回率较差，相比之下，投票集成模型使用相同的基本学习器表现较好：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.823 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.823 |'
- en: '|  | Recall | 0.750 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.750 |'
- en: '| **Filtered** | F1 | 0.828 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.828 |'
- en: '|  | Recall | 0.794 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.794 |'
- en: Stacking ensemble performance with three base learners
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三个基本学习器的堆叠集成模型表现
- en: We can further experiment with different base learners. By adding two decision
    trees with maximum depths of three and eight, respectively (same with the second
    Voting setup), observe how stacking exhibits the same behavior. It outperforms
    on the F1 score and underperforms on the recall score for the original dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步尝试不同的基本学习器。通过添加两个分别具有最大深度为三和八的决策树（与第二次投票配置相同），观察堆叠模型表现出相同的行为。在原始数据集上，堆叠模型在
    F1 分数上超越了其他模型，但在召回率上表现较差。
- en: 'On the filtered dataset, the performance remains on par with Voting. Finally,
    we experiment with second level of base learners, consisting of a Decision Tree
    with depth two and a linear support vector machine, which performs worse than
    the five base learners'' setup:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤数据集上，性能与投票模型持平。最后，我们尝试第二层次的基本学习器，由一个深度为二的决策树和一个线性支持向量机组成，其表现不如五个基本学习器的配置：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.844 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.844 |'
- en: '|  | Recall | 0.757 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.757 |'
- en: '| **Filtered** | F1 | 0.828 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.828 |'
- en: '|  | Recall | 0.794 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.794 |'
- en: Performance with five base learners
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五个基本学习器的性能
- en: The following table depicts the results for the stacking ensemble with an additional
    level of base learners. As it is evident, it performs worse than the original
    ensemble.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了堆叠集成的结果，增加了一个基础学习器层次。显然，它的表现不如原始集成。
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.827 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.827 |'
- en: '|  | Recall | 0.757 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.757 |'
- en: '| **Filtered** | F1 | 0.827 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **过滤** | F1 | 0.827 |'
- en: '|  | Recall | 0.772 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.772 |'
- en: Performance with five base learners on level 0 and two on level 1
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 五个基础学习器在第0层，两个基础学习器在第1层的表现
- en: Bagging
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 袋装法
- en: In this section, we will classify the dataset using bagging. As we have previously
    shown, decision trees with maximum depth of five are optimal thus, we will use
    these trees for our bagging example.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用袋装法对数据集进行分类。正如我们之前所示，最大深度为五的决策树是最优的，因此我们将使用这些树来进行袋装法示例。
- en: 'We would like to optimize the ensemble''s size. We will generate validation
    curves for the original train set by testing sizes in the range of *[5, 30]*.
    The actual curves are depicted here in the following graph:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望优化集成的大小。我们将通过在*【5，30】*范围内测试不同大小来生成原始训练集的验证曲线。实际的曲线如下图所示：
- en: '![](img/ac802720-c0d2-49ed-88ed-6190c2a50a75.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac802720-c0d2-49ed-88ed-6190c2a50a75.png)'
- en: Validation curves for the original train set, for various ensemble sizes
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 原始训练集的验证曲线，针对不同集成大小
- en: We observe that variance is minimized for an ensemble size of 10, thus we will
    utilize ensembles of size 10.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，集成大小为10时方差最小，因此我们将使用大小为10的集成。
- en: 'The following code loads the data and libraries (*Section 1*), splits the data
    into train and test sets, and fits and evaluates the ensemble on the original
    dataset (*Section 2*) and the reduced-features dataset (*Section 3*):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载数据和库（*第1部分*），将数据拆分为训练集和测试集，并在原始数据集（*第2部分*）和减少特征的数据集（*第3部分*）上拟合并评估集成：
- en: '[PRE7]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After creating our train and test splits, we train and evaluate our ensemble
    on the original dataset, as well as a reduced-features dataset as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了训练集和测试集划分后，我们在原始数据集和减少特征的数据集上训练并评估我们的集成，如下所示：
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using bagging ensembles with trees of a maximum depth of 5 and 10 trees per
    ensemble, we are able to achieve the following F1 and recall scores. It outperforms
    both stacking and voting in both datasets on all metrics, with one exception.
    The F1 score for the original dataset is slightly worse than stacking (0.843 compared
    to 0.844):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最大深度为5且每个集成有10棵树的袋装法集成，我们能够在以下F1和召回率得分中取得较好成绩。在所有度量上，它在两个数据集上均优于堆叠法和投票法，唯一的例外是，原始数据集的F1分数略逊于堆叠法（0.843对比0.844）：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.843 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.843 |'
- en: '|  | Recall | 0.787 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.787 |'
- en: '| **Filtered** | F1 | 0.831 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **过滤** | F1 | 0.831 |'
- en: '|  | Recall | 0.794 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.794 |'
- en: Bagging performance for the original and filtered datasets
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集和过滤数据集的袋装性能
- en: Although we have concluded that a maximum depth of 5 is optimal for a single
    decision tree, it does restrict the diversity of each tree. By increasing the
    maximum depth to 8, we are able to achieve an F1 score of 0.864 and a recall score
    of 0.816 on the filtered dataset, the best performance up to now.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已得出最大深度为5对于单一决策树是最优的结论，但这确实限制了每棵树的多样性。通过将最大深度增加到8，我们能够在过滤数据集上获得0.864的F1分数和0.816的召回率，这也是迄今为止的最佳表现。
- en: 'Nonetheless, performance on the original dataset suffers, confirming that the
    features that we removed were, indeed, noise, as the trees are now able to model
    in-sample noise, and thus, their out-of-sample performance suffers:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，原始数据集上的性能有所下降，这确认了我们移除的特征确实是噪声，因为现在决策树能够拟合样本内的噪声，因此它们的样本外表现下降：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.840 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.840 |'
- en: '|  | Recall | 0.772 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.772 |'
- en: '| **Filtered** | F1 | 0.864 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **过滤** | F1 | 0.864 |'
- en: '|  | Recall | 0.816 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.816 |'
- en: Boosting
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升法
- en: As we move on, we will start to utilize generative methods. The first generative
    method we will experiment with is boosting. We will first try to classify the
    datasets using AdaBoost. As AdaBoost resamples the dataset based on misclassifications,
    we expect that it will be able to handle our imbalanced dataset relatively well.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始使用生成方法。我们将实验的第一个生成方法是提升法。我们将首先尝试使用AdaBoost对数据集进行分类。由于AdaBoost根据误分类重新采样数据集，因此我们预期它能够相对较好地处理我们不平衡的数据集。
- en: 'First, we must decide on the ensemble''s size. We generate validation curves
    for a number of ensemble sizes depicted as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须决定集成的大小。我们生成了多个集成大小的验证曲线，具体如下所示：
- en: '![](img/4657f347-9b54-4486-a151-250f44e7548e.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4657f347-9b54-4486-a151-250f44e7548e.png)'
- en: Validation curves of various ensemble sizes for AdaBoost
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的不同集成大小验证曲线
- en: As we can observe, 70 base learners provide the best trade-off between bias
    and variance. As such, we will proceed with ensembles of size 70.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，70个基学习器提供了偏差与方差之间的最佳权衡。因此，我们将继续使用70个基学习器的集成。
- en: 'The following code implements the training and evaluation for AdaBoost:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了AdaBoost的训练和评估：
- en: '[PRE9]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then train and evaluate our ensemble, using 70 estimators and a learning
    rate of 1.0:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用70个基学习器和学习率1.0来训练和评估我们的集成方法：
- en: '[PRE10]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We reduce the number of features, by selecting only features with high correlation
    with respect to the target. Finally, we repeat the procedure of training and evaluating
    the ensemble:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过选择与目标高度相关的特征来减少特征数量。最后，我们重复训练和评估集成的方法：
- en: '[PRE11]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The results are depicted in the following table. As it is evident, it does
    not perform as well as our previous models:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下表所示。显而易见，它的表现不如我们之前的模型：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.778 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **原始数据** | F1 | 0.778 |'
- en: '|  | Recall | 0.721 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.721 |'
- en: '| **Filtered** | F1 | 0.794 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后数据** | F1 | 0.794 |'
- en: '|  | Recall | 0.721 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.721 |'
- en: Performance of AdaBoost
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的表现
- en: 'We can try to increase the learning rate to 1.3, which seems to improve overall
    performance. If we further increase it to 1.4, we notice a drop in performance.
    If we increase the number of base learners to 80, we notice an increase in performance
    for the filtered dataset, while the original dataset seems to trade recall for
    F1 performance:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试将学习率增加到1.3，这似乎能提高整体表现。如果我们再将其增加到1.4，则会发现性能下降。如果我们将基学习器的数量增加到80，过滤后的数据集性能有所提升，而原始数据集则似乎在召回率和F1表现之间做出了权衡：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.788 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **原始数据** | F1 | 0.788 |'
- en: '|  | Recall | 0.765 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.765 |'
- en: '| **Filtered** | F1 | 0.815 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后数据** | F1 | 0.815 |'
- en: '|  | Recall | 0.743 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.743 |'
- en: Performance of AdaBoost, learning_rate=1.3
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的表现，学习率=1.3
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| Original | F1 | 0.800 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 原始数据 | F1 | 0.800 |'
- en: '|  | Recall | 0.765 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.765 |'
- en: '| Filtered | F1 | 0.800 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 过滤后数据 | F1 | 0.800 |'
- en: '|  | Recall | 0.735 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.735 |'
- en: Performance of AdaBoost, learning_rate=1.4
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的表现，学习率=1.4
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.805 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **原始数据** | F1 | 0.805 |'
- en: '|  | Recall | 0.757 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.757 |'
- en: '| **Filtered** | F1 | 0.805 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后数据** | F1 | 0.805 |'
- en: '|  | Recall | 0.743 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.743 |'
- en: Performance of AdaBoost, learning_rate=1.4, ensemble_size=80
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的表现，学习率=1.4，集成大小=80
- en: 'We can, in fact, observe a Pareto front of F1 and Recall, which is directly
    linked to the learning rate and number of base learners present in the ensemble.
    The front is depicted in the following graph:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们可以观察到一个F1和召回率的帕累托前沿，它直接与学习率和基学习器数量相关。这个前沿如下图所示：
- en: '![](img/b4c0bdd0-25cf-454d-8b46-f74471c4c2ab.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4c0bdd0-25cf-454d-8b46-f74471c4c2ab.png)'
- en: Pareto front of F1 and Recall for AdaBoost
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost的F1和召回率的帕累托前沿
- en: XGBoost
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost
- en: 'We will also try to classify the dataset using XGBoost. As XGBoost uses trees
    of a maximum depth of three, we expect that it will outperform AdaBoost without
    any fine-tuning. Indeed, XGBoost is able to achieve better performance in both
    datasets and for all metrics (as shown in the following table), compared to most
    previous ensembles:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将尝试使用XGBoost对数据集进行分类。由于XGBoost的树最大深度为三，我们预期它会在没有任何微调的情况下超越AdaBoost。的确，XGBoost在两个数据集上，以及所有指标方面（如下表所示），都能表现得比大多数先前的集成方法更好：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.846 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| **原始数据** | F1 | 0.846 |'
- en: '|  | Recall | 0.787 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.787 |'
- en: '| **Filtered** | F1 | 0.849 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后数据** | F1 | 0.849 |'
- en: '|  | Recall | 0.809 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.809 |'
- en: XGBoost out-of-the-box performance
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost的开箱即用表现
- en: 'By increasing the maximum depth of each tree to five, the ensemble is able
    to perform even better, yielding the following results:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每棵树的最大深度增加到五，集成方法的表现得到了进一步提升，结果如下：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.862 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| **原始数据** | F1 | 0.862 |'
- en: '|  | Recall | 0.801 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.801 |'
- en: '| **Filtered** | F1 | 0.862 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.862 |'
- en: '|  | Recall | 0.824 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.824 |'
- en: Performance with max_depth=5
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最大深度为5时的性能
- en: Using random forests
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林
- en: 'Finally, we will employ a random forest ensemble. Once again, using validation
    curves, we will determine the optimal ensemble size. From the following graph,
    we conclude that 50 trees provide the least possible variance in our model, thus
    we proceed with ensemble size 50:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用随机森林集成方法。再次通过验证曲线，我们确定最佳的集成大小。从下图可以看出，50棵树提供了模型的最小方差，因此我们选择集成大小为50：
- en: '![](img/0a1e5ed3-be7c-4e14-a232-a87f9dc3aea7.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a1e5ed3-be7c-4e14-a232-a87f9dc3aea7.png)'
- en: Validation curves for random forest
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的验证曲线
- en: 'We provide the training and validation code as follows, as well as the achieved
    performance for both datasets. The following code is responsible for loading the
    required libraries and data, and training and evaluating the ensemble on the original
    and filtered datasets. We first load the required libraries and data, while creating
    train and test splits:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供以下训练和验证代码，并给出两个数据集的性能表现。以下代码负责加载所需的库和数据，并在原始数据集和过滤后的数据集上训练和评估集成模型。我们首先加载所需的库和数据，同时创建训练集和测试集：
- en: '[PRE12]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We then train and evaluate the ensemble, both on the original dataset, as well
    as on the filtered dataset:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在原始数据集和过滤后的数据集上训练和评估集成模型：
- en: '[PRE13]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.845 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.845 |'
- en: '|  | Recall | 0.743 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.743 |'
- en: '| **Filtered** | F1 | 0.867 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.867 |'
- en: '|  | Recall | 0.794 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.794 |'
- en: Random forest performance
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林性能
- en: 'As our dataset is highly skewed, we can speculate that changing the criterion
    for a tree’s split to entropy would benefit our model. Indeed, by specifying `criterion=''entropy''`
    in the constructor (`ensemble = RandomForestClassifier(n_jobs=4)`), we are able
    to increase the performance on the original dataset to an **F1** score of **0.859**
    and a **Recall** score of **0.786**, two of the highest scores for the original
    dataset:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集高度偏斜，我们可以推测，改变树的分割标准为熵会对我们的模型有帮助。事实上，通过在构造函数中指定`criterion='entropy'`（`ensemble
    = RandomForestClassifier(n_jobs=4)`），我们能够将原始数据集的性能提高到**F1**得分为**0.859**和**召回率**得分为**0.786**，这是原始数据集的两个最高得分：
- en: '| **Dataset** | **Metric** | **Value** |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **指标** | **值** |'
- en: '| **Original** | F1 | 0.859 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | F1 | 0.859 |'
- en: '|  | Recall | 0.787 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.787 |'
- en: '| **Filtered** | F1 | 0.856 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **过滤后** | F1 | 0.856 |'
- en: '|  | Recall | 0.787 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率 | 0.787 |'
- en: Performance with entropy as the splitting criterion
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用熵作为分割标准的性能
- en: Comparative analysis of ensembles
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法的对比分析
- en: 'As we experimented with a reduced feature dataset, where we removed features
    without a strong correlation to the target variable, we would like to provide
    the final scores for the best parameters of each method. In the following graph,
    the results are depicted, sorted in ascending order. Bagging seems to be the most
    robust method when applied to the filtered dataset. XGBoost is the second best
    alternative, providing decent F1 and Recall scores when applied to the filtered
    dataset as well:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用了一个减少特征的数据集，其中去除了与目标变量相关性较弱的特征，我们希望提供每种方法最佳参数下的最终得分。在以下图表中，结果按升序排列。Bagging在应用于过滤后的数据集时似乎是最稳健的方法。XGBoost是第二好的选择，在应用于过滤后的数据集时也能提供不错的F1和召回率得分：
- en: '![](img/a127e35e-debc-4407-80ba-5b68e485ecc9.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a127e35e-debc-4407-80ba-5b68e485ecc9.png)'
- en: F1 scores
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: F1得分
- en: 'Recall scores, depicted in the following plot, show the clear advantage XGBoost
    has on this metric over the other methods, as it is able to outperform all others
    for both the original and filtered datasets:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率得分，如下图所示，清楚地显示了XGBoost在该指标上相较于其他方法的明显优势，因为它能够在原始数据集和过滤后的数据集上都超过其他方法：
- en: '![](img/0df8e22b-871a-4cb9-a2de-fbfc5cb1b38c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0df8e22b-871a-4cb9-a2de-fbfc5cb1b38c.png)'
- en: Recall scores
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率得分
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the possibility of detecting fraudulent transactions
    using various ensemble learning methods. While some performed better than others,
    due to the dataset's nature, it is difficult to produce good results without resampling
    the dataset in some way (either over-sampling or under-sampling).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用各种集成学习方法检测欺诈交易的可能性。虽然一些方法表现优于其他方法，但由于数据集的特点，在某种程度上对数据集进行重采样（过采样或欠采样）是很难得到好结果的。
- en: We were able to show how to use each ensemble learning method and how to explore
    the possibility of fine-tuning its respective parameters in order to achieve better
    performance. In the next chapter, we will try to leverage ensemble learning techniques
    in order to predict Bitcoin prices.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何使用每种集成学习方法，以及如何探索调整其各自参数的可能性，以实现更好的性能。在下一章，我们将尝试利用集成学习技术来预测比特币价格。
