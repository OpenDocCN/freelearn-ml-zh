- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Building an Example ML Microservice
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建示例 ML 微服务
- en: This chapter will be all about bringing together some of what we have learned
    in the book so far with a realistic example. This will be based on one of the
    scenarios introduced in *Chapter 1*, *Introduction to ML Engineering*, where we
    were required to build a forecasting service for store item sales. We will discuss
    the scenario in a bit of detail and outline the key decisions that have to be
    made to make a solution a reality, before showing how we can employ the processes,
    tools, and techniques we have learned through out this book to solve key parts
    of the problem from an ML engineering perspective. By the end of this chapter,
    you should come away with a clear view of how to build your own ML microservices
    for solving a variety of business problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将主要介绍如何将我们在书中学到的知识结合到一个现实示例中。这将是基于在 *第一章*，*机器学习工程简介* 中介绍的场景之一，其中我们被要求为商店商品销售构建预测服务。我们将详细讨论该场景，并概述为了使解决方案成为现实必须做出的关键决策，然后展示我们如何通过本书中学到的过程、工具和技术从机器学习工程的角度解决问题的关键部分。到本章结束时，您应该对如何构建自己的
    ML 微服务以解决各种商业问题有一个清晰的了解。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the forecasting problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解预测问题
- en: Designing our forecasting service
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计我们的预测服务
- en: Selecting the tools
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择工具
- en: Training at scale
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩规模训练
- en: Serving the models with FastAPI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FastAPI 提供模型服务
- en: Containerizing and deploying to Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化并部署到 Kubernetes
- en: Each topic will provide an opportunity for us to walk through the different
    decisions we have to make as engineers working on a complex ML delivery. This
    will provide us with a handy reference when we go out and do this in the real
    world!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主题都将为我们提供一个机会，让我们回顾作为在复杂机器学习交付中工作的工程师所必须做出的不同决策。这将为我们提供在现实世界中执行此类操作时的便捷参考！
- en: With that, let’s get started and build a forecasting microservice!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始构建一个预测微服务吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code examples in this chapter will be simpler to follow if you have the
    following installed and running on your machine:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在您的机器上安装并运行以下内容，本章中的代码示例将更容易理解：
- en: Postman or another API development tool
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Postman 或其他 API 开发工具
- en: A local Kubernetes cluster manager like minikube or kind
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地 Kubernetes 集群管理器，如 minikube 或 kind
- en: The Kubernetes CLI tool, `kubectl`
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes CLI 工具，`kubectl`
- en: 'There are several different `conda` environment `.yml` files contained in the
    `Chapter08` folder in the book’s GitHub repo for the technical examples, as there
    are a few different sub-components. These are:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍 GitHub 仓库中 `Chapter08` 文件夹包含几个不同的技术示例的 `conda` 环境配置 `.yml` 文件，因为有几个不同的子组件。这些是：
- en: '`mlewp-chapter08-train`: This specifies the environment for running the training
    scripts.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlewp-chapter08-train`: 这指定了运行训练脚本的运行环境。'
- en: '`mlewp-chapter08-serve`: This specifies the environment for the local FastAPI
    web service build.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlewp-chapter08-serve`: 这指定了本地 FastAPI 网络服务的环境规范。'
- en: '`mlewp-chapter08-register`: This gives the environment specification for running
    the MLflow tracking server.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlewp-chapter08-register`: 这提供了运行 MLflow 跟踪服务器的环境规范。'
- en: 'In each case, create the Conda environment, as usual, with:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，像往常一样创建 Conda 环境：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Kubernetes examples in this chapter also require some configuration of
    the cluster and the services we will deploy; these are given in the `Chapter08/forecast`
    folder under different `.yml` files. If you are using kind, you can create a cluster
    with a simple configuration by running:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的 Kubernetes 示例还需要对集群和我们将部署的服务进行一些配置；这些配置在 `Chapter08/forecast` 文件夹下的不同 `.yml`
    文件中给出。如果您使用 kind，可以通过运行以下简单配置来创建一个集群：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or you can use one of the configuration `.yaml` files provided in the repository:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用存储库中提供的其中一个配置 `.yaml` 文件：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Minikube does not provide an option to read in a cluster configuration `.yaml`
    like kind, so instead, you should simply run:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube 不提供像 kind 那样读取集群配置 `.yaml` 选项，因此，你应该简单地运行：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: to deploy your local cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 部署您的本地集群。
- en: Understanding the forecasting problem
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解预测问题
- en: 'In *Chapter 1*, *Introduction to ML Engineering*, we considered the example
    of an ML team that has been tasked with providing forecasts of items at the level
    of individual stores in a retail business. The fictional business users had the
    following requirements:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章“ML工程简介”中，我们考虑了一个ML团队，该团队被分配提供零售业务中单个商店层面的商品预测。虚构的业务用户有以下要求：
- en: The forecasts should be rendered and accessible via a web-based dashboard.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测结果应通过基于Web的仪表板进行展示和访问。
- en: The user should be able to request updated forecasts if necessary.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户在必要时应能够请求更新预测。
- en: The forecasts should be carried out at the level of individual stores.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测应在单个商店层面进行。
- en: Users will be interested in their own regions/stores in any one session and
    not be concerned with global trends.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户在任何一次会话中都会对其自己的区域/商店感兴趣，而不会关注全球趋势。
- en: The number of requests for updated forecasts in any one session will be small.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何一次会话中请求更新预测的数量将很少。
- en: 'Given these requirements, we can work with the business to create the following
    user stories, which we can put into a tool such as Jira, as explained in *Chapter
    2*, *The Machine Learning Development Process*. Some examples of user stories
    covering these requirements would be the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些要求，我们可以与业务团队合作创建以下用户故事，我们可以将这些故事放入像Jira这样的工具中，如第2章“机器学习开发过程”中所述。满足这些要求的一些用户故事示例如下：
- en: '**User Story 1**: As a local logistics planner, I want to log in to a dashboard
    in the morning at 09:00 and be able to see forecasts of item demand at the store
    level for the next few days so that I can understand transport demand ahead of
    time.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事 1**：作为一名本地物流规划师，我希望在早上09:00登录仪表板，并能够看到未来几天商店层面的商品需求预测，以便我能够提前了解运输需求。'
- en: '**User Story 2**: As a local logistics planner, I want to be able to request
    an update of my forecast if I see it is out of date. I want the new forecast to
    be returned in under 5 minutes so that I can make decisions on transport demand
    effectively.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事 2**：作为一名本地物流规划师，我希望能够在看到预测信息过时的情况下请求更新。我希望新的预测结果能在5分钟内返回，以便我能够有效地做出运输需求决策。'
- en: '**User Story 3**: As a local logistics planner, I want to be able to filter
    for forecasts for specific stores so that I can understand what stores are driving
    demand and use this in decision-making.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事 3**：作为一名本地物流规划师，我希望能够筛选特定商店的预测信息，以便我能够了解哪些商店在推动需求，并在决策中使用这些信息。'
- en: These user stories are very important for the development of the solution as
    a whole. As we are focused on the ML engineering aspects of the problem, we can
    now dive into what these mean for building the solution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些用户故事对于整个解决方案的开发非常重要。由于我们专注于问题的ML工程方面，我们现在可以深入探讨这些对构建解决方案意味着什么。
- en: For example, the desire to *be able to see forecasts of item demand at the store
    level* can be translated quite nicely into a few technical requirements for the
    ML part of the solution. This tells us that the target variable will be the number
    of items required on a particular day. It tells us that our ML model or models
    need to be able to work at the store level, so either we have one model per store
    or the concept of the store can be taken in as some sort of feature.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，希望“能够看到商店层面的商品需求预测”的愿望可以很好地转化为解决方案ML部分的几个技术要求。这告诉我们目标变量将是特定一天所需商品的数量。这告诉我们我们的ML模型或模型需要能够在商店层面工作，因此我们可能需要为每个商店有一个模型，或者将商店的概念作为某种特征来考虑。
- en: Similarly, the requirement that the user wants to *be able to request an update
    of my forecast if I see it is out of date ... I want the new forecast to be retrieved
    in under five minutes* places a clear latency requirement on training. We cannot
    build something that takes days to retrain, so this may suggest that one model
    built across all of the data may not be the best solution.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，用户希望“能够在看到预测信息过时时请求更新我的预测……我希望新的预测能在五分钟内检索到”的要求对训练的延迟提出了明确的要求。我们不能构建需要几天时间才能重新训练的东西，这可能意味着在整个数据上构建一个模型可能不是最佳解决方案。
- en: Finally, the request *I want to be able to filter for forecasts for specific
    stores* again supports the notion that whatever we build must utilize some sort
    of store identifier in the data but not necessarily as a feature for the algorithm.
    So, we may want to start thinking of application logic that will take a request
    for the forecast for a specific store, identified by this store ID, then the ML
    model and forecast are retrieved only for that store via some kind of lookup or
    retrieval that uses this ID in a filter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请求*I want to be able to filter for forecasts for specific stores*再次支持了这样的观点，即无论我们构建什么，都必须在数据中利用某种类型的存储标识符，但不必一定作为算法的特征。因此，我们可能需要开始考虑应用逻辑，该逻辑将接受对特定店铺的预测请求，该店铺通过此存储ID识别，然后仅通过某种类型的查找或检索使用此ID进行筛选来检索该店铺的ML模型和预测。
- en: 'Walking through this process, we can see how just a few lines of requirements
    have allowed us to start fleshing out how we will tackle the problem in practice.
    Some of these thoughts and others could be consolidated upon a little brainstorming
    among our team for the project in a table like that of *Table 8.1*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个过程，我们可以看到仅仅几行需求是如何使我们开始具体化我们在实践中如何解决问题的。这些想法和其他想法可以通过我们团队在项目中进行一些头脑风暴，并像*表8.1*那样的表格进行整合：
- en: '| **User Story** | **Details** | **Technical Requirements** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **用户故事** | **细节** | **技术需求** |'
- en: '| 1 | As a local logistics planner, I want to log in to a dashboard in the
    morning at 09:00 and be able to see forecasts of item demand at the store level
    for the next few days so that I can understand transport demand ahead of time.
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 作为本地物流规划师，我希望在早上09:00登录仪表板，并能够看到未来几天在店铺层面的项目需求预测，以便我能够提前了解运输需求。 |'
- en: Target variable = item demand.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标变量 = 项目需求。
- en: Forecast horizon – 1-7 days.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测范围 – 1-7天。
- en: API access for a dashboard or other visualization solution.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于仪表板或其他可视化解决方案的API访问。
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2 | As a local logistics planner, I want to be able to request an update
    of my forecast if I see it is out of date. I want the new forecast to be returned
    in under 5 minutes so that I can make decisions on transport demand effectively.
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 作为本地物流规划师，我希望能够在看到预测过时的情况下请求更新我的预测。我希望新的预测在5分钟内返回，以便我能够有效地做出关于运输需求的决策。
    |'
- en: Lightweight retraining.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻量级重新训练。
- en: Model per store.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个店铺的模型。
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3 | As a local logistics planner, I want to be able to filter for forecasts
    for specific stores so that I can understand what stores are driving demand and
    use this in decision-making. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 作为本地物流规划师，我希望能够筛选特定店铺的预测，以便我能够了解哪些店铺在推动需求，并在决策中使用这一点。 |'
- en: Model per store.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个店铺的模型。
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 8.1: Translating user stories to technical requirements.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1：将用户故事转换为技术需求。
- en: Now we will build on our understanding of the problem by starting to pull together
    a design for the ML piece of the solution.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过开始为解决方案的ML部分设计一个设计来加深我们对问题的理解。
- en: Designing our forecasting service
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计我们的预测服务
- en: The requirements in the *Understanding the forecasting problem* section are
    the definitions of the targets we need to hit, but they are not the method for
    getting there. Drawing on our understanding of design and architecture from *Chapter
    5*, *Deployment Patterns and Tools*, we can start building out our design.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*理解预测问题*部分的要求是我们需要达到的目标定义，但它们并不是达到目标的方法。借鉴我们从*第5章*，*部署模式和工具*中关于设计和架构的理解，我们可以开始构建我们的设计。
- en: First, we should confirm what kind of design we should be working on. Since
    we need dynamic requests, it makes sense that we follow the microservice architecture
    discussed in *Chapter 5*, *Deployment Patterns and Tools*. This will allow us
    to build a service that has the sole focus of retrieving the right model from
    our model store and performing the requested inference. The prediction service
    should therefore have interfaces available between the dashboard and the model
    store.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该确认我们应该工作在哪种设计上。由于我们需要动态请求，遵循在第5章*部署模式和工具*中讨论的微服务架构是有意义的。这将使我们能够构建一个专注于从我们的模型存储中检索正确模型并执行请求推理的服务。因此，预测服务应该在仪表板和模型存储之间提供接口。
- en: Furthermore, since a user may want to work with a few different store combinations
    in any one session and maybe switch back and forth between the forecasts of these,
    we should provide a mechanism for doing so that is performant.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于用户可能希望在任何一次会话中与几个不同的存储组合一起工作，并且可能在这些预测之间来回切换，我们应该提供一个高效执行此操作的机制。
- en: It is also clear from the scenario that we can quite easily have a very high
    volume of requests for predictions but a lower request for model updates. This
    means that separating out training and prediction will make sense and that we
    can follow the train-persist process outlined in *Chapter 3*, *From Model to Model
    Factory*. This will mean that prediction will not be dependent on a full training
    run every time and that retrieval of models for prediction is relatively fast.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从场景中也可以清楚地看出，我们可以非常容易地有大量的预测请求，但模型更新的请求较少。这意味着将训练和预测分开是有意义的，我们可以遵循第3章中概述的“从模型到模型工厂”的train-persist过程。这意味着预测不会每次都依赖于完整的训练运行，并且检索用于预测的模型相对较快。
- en: What we have also gathered from the requirements is that our training system
    doesn’t necessarily need to be triggered by drift monitoring in this case, but
    by dynamic requests made by the user. This adds a bit of complexity as it means
    that our solution should not retrain for every request coming in but be able to
    determine whether retraining is worth it for a given request or whether the model
    is already up to date. For example, if four users log on and are looking at the
    same region/store/item combination and all request a retrain, it is pretty clear
    that we do not need to retrain our model four times! Instead, what should happen
    is that the training system registers a request, performs a retrain, and then
    safely ignores the other requests.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从要求中我们还了解到，在这种情况下，我们的训练系统不一定需要由漂移监控触发，而是由用户发出的动态请求触发。这增加了一点点复杂性，因为它意味着我们的解决方案不应该对每个请求都进行重新训练，而应该能够确定重新训练对于给定的请求是否有价值，或者模型是否已经是最新的。例如，如果有四个用户登录并查看相同的区域/商店/商品组合，并且所有用户都请求重新训练，那么很明显我们不需要四次重新训练我们的模型！相反，应该发生的情况是，训练系统记录一个请求，执行重新训练，然后安全地忽略其他请求。
- en: There are several ways to serve ML models, as we have discussed several times
    throughout this book. One very powerful and flexible way is to wrap the models,
    or the model serving logic, into a standalone service that is limited to only
    performing tasks required for the serving of the ML inference. This is the serving
    pattern that we will consider in this chapter and it is the classic “microservice”
    architecture, where different pieces of functionality are broken down into their
    own distinct and separated services. This builds resiliency and extensibility
    into your software systems so it is a great pattern to become comfortable with.
    This is also particularly amenable to the development of ML systems, as these
    have to consist of training, inference, and monitoring services, as outlined in
    *Chapter 3*, *From Model to Model Factory*. This chapter will walk through how
    to serve an ML model using a microservice architecture, using a few different
    approaches with various pros and cons. You will then be able to adapt and build
    on these examples in your own future projects.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在这本书中多次讨论的那样，有几种方式可以提供机器学习模型。一种非常强大且灵活的方式是将模型或模型提供逻辑封装到一个独立的服务中，该服务仅限于执行机器学习推理所需的任务。这是我们将在本章中考虑的提供模式，它是经典的“微服务”架构，其中不同的功能部分被分解成它们自己的独立和分离的服务。这为软件系统增加了弹性和可扩展性，因此这是一个很好的模式，需要变得熟悉。这也特别适合机器学习系统的开发，因为这些系统必须由训练、推理和监控服务组成，如第3章中概述的“从模型到模型工厂”。本章将介绍如何使用微服务架构提供机器学习模型，使用几种不同的方法，各有优缺点。然后你将能够根据这些示例调整和构建你自己的未来项目。
- en: 'We can bring these design points together into a high-level design diagram,
    for example, in *Figure 8.1*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些设计点整合到一个高级设计图中，例如，在*图8.1*中：
- en: '![](img/B19525_08_01.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_08_01.png)'
- en: 'Figure 8.1: High-level design for the forecasting microservice.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：预测微服务的高级设计。
- en: The next section will focus on taking these high-level design considerations
    to a lower level of detail as we perform some tool selection ahead of development.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将专注于在开发前进行一些工具选择时，将这些高级设计考虑因素细化到更低的细节水平。
- en: Selecting the tools
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具选择
- en: Now that we have a high-level design in mind and we have written down some clear
    technical requirements, we can begin to select the toolset we will use to implement
    our solution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个高级设计在心中，并且我们已经写下了一些明确的技术要求，我们可以开始选择我们将用于实现解决方案的工具集。
- en: One of the most important considerations on this front will be what framework
    we use for modeling our data and building our forecasting functionality. Given
    that the problem is a time-series modeling problem with a need for fast retraining
    and prediction, we can consider the pros and cons of a few options that may fit
    the bill before proceeding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方面最重要的考虑因素之一将是我们将使用什么框架来建模我们的数据并构建我们的预测功能。鉴于问题是一个需要快速重新训练和预测的时间序列建模问题，我们可以在继续之前考虑一些可能适合的选择的优缺点。
- en: 'The results of this exercise are shown in *Table 8.2*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的结果显示在表8.2中：
- en: '| **Tool/Framework** | **Pros** | **Cons** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **工具/框架** | **优点** | **缺点** |'
- en: '| Scikit-learn |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Scikit-learn |'
- en: Already understood by almost all data scientists.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有数据科学家都已经理解。
- en: Very easy-to-use syntax.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法非常易于使用。
- en: Lots of great community support.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社区支持非常丰富。
- en: Good feature engineering and pipelining support.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的特征工程和管道支持。
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: No native time-series modeling capabilities (but the popular `sktime` package
    does have these).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有原生的时间序列建模能力（但流行的`sktime`包确实有这些）。
- en: Will require more feature engineering to apply models to time-series data.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将需要更多的特征工程来将模型应用于时间序列数据。
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Prophet |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Prophet |'
- en: Purely focused on forecasting.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专注于预测。
- en: Has inbuilt hyperparameter optimization capabilities.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有内置的超参数优化功能。
- en: Provides a lot of functionality out of the box.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开箱即提供大量功能。
- en: Often gives accurate results on a wide variety of problems.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在广泛的问题上通常给出准确的结果。
- en: Provides confidence intervals out of the box.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开箱即提供置信区间。
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Not as commonly used as scikit-learn (but still relatively popular).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不像scikit-learn那样常用（但仍然相对流行）。
- en: Underlying methods are quite sophisticated – may lead to black box usage by
    data scientists.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础方法相当复杂——可能会导致数据科学家使用黑盒。
- en: Not inherently scalable.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本身不具有可扩展性。
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Spark ML |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Spark ML |'
- en: Natively scalable to large volumes.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地可扩展到大量数据。
- en: Good feature engineering and pipelining support.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的特征工程和管道支持。
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: No native time-series modeling capabilities.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有原生的时间序列建模能力。
- en: Algorithm options are relatively limited.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法选项相对有限。
- en: Can be harder to debug.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调试可能更困难。
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 8.2: The considered pros and cons of some different ML toolkits for solving
    this forecasting problem.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2：考虑的一些不同机器学习工具包解决此预测问题的优缺点。
- en: Based on the information in *Table 8.2*, it looks like the **Prophet** library
    would be a good choice and offer a nice balance between predictive power, the
    desired time-series capabilities, and experience among the developers and scientists
    on the team.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表8.2中的信息，看起来**Prophet**库是一个不错的选择，并在预测能力、所需的时间序列能力和团队中的开发人员和科学家的经验之间提供了一个良好的平衡。
- en: 'The data scientists could then use this information to build a proof-of-concept,
    with code much like that shown in *Chapter 1*, *Introduction to ML Engineering*,
    in the *Example 2: Forecasting API* section, which applies Prophet to a standard
    retail dataset.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可以使用这些信息构建一个概念验证，代码类似于*第一章*，*机器学习工程简介*中的*示例2：预测API*部分，该部分将Prophet应用于标准零售数据集。
- en: This covers the ML package we will use for modeling, but what about the other
    components? We need to build something that allows the frontend application to
    request actions be taken by the backend, so it is a good idea to consider some
    kind of web application framework. We also need to consider what happens when
    this backend application is hit by many requests, so it makes sense to build it
    with scale in mind. Another consideration is that we are tasked with training
    not one but several models in this use case, one for each retail store, and so
    we should try and parallelize the training as much as possible. The last pieces
    of the puzzle are going to be the use of a model management tool and the need
    for an orchestration layer in order to trigger training and monitoring jobs on
    a schedule or dynamically.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了我们将用于建模的机器学习包，但其他组件怎么办？我们需要构建一个允许前端应用程序请求后端执行操作的东西，因此考虑某种类型的Web应用程序框架是个好主意。我们还需要考虑当后端应用程序受到大量请求时会发生什么，因此有意识地构建它以考虑可扩展性是有意义的。另一个考虑因素是我们在这个用例中不仅要训练一个模型，而是要训练多个模型，每个零售店一个，因此我们应该尽可能并行化训练。最后一块拼图将是使用模型管理工具和需要编排层来按计划或动态触发训练和监控作业。
- en: 'Putting all of this together, we can make some design decisions about the lower-level
    tooling required on top of using the Prophet library. We can summarize these in
    the following list:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些内容综合起来，我们可以在使用 Prophet 库的基础上做出一些关于底层工具的设计决策。以下是一个总结列表：
- en: '**Prophet**: We met the Prophet forecasting library in *Chapter 1*, *Introduction
    to ML Engineering*. Here we will provide a deeper dive into that library and how
    it works before developing a training pipeline to create the types of forecasting
    models we saw for that retail use case in the first chapter.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Prophet**：我们在**第1章**，**机器学习工程简介**中遇到了 Prophet 预测库。在这里，我们将深入了解该库及其工作原理，然后再开发一个训练流程来创建我们在第一章中为该零售用例看到的预测模型类型。'
- en: '**Kubernetes**: As discussed in *Chapter 6*, *Scaling Up*, this is a platform
    for orchestrating multiple containers across compute clusters and allows you to
    build highly scalable ML model-serving solutions. We will use this to host the
    main application.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：如**第6章**，**扩展**中讨论的，这是一个在计算集群中编排多个容器的平台，允许你构建高度可扩展的机器学习模型服务解决方案。我们将使用它来托管主要应用程序。'
- en: '**Ray Train**: We already met Ray in *Chapter 6*, *Scaling Up*. Here we will
    use Ray Train to train many different Prophet forecasting models in parallel,
    and we will also allow these jobs to be triggered upon a request to the main web
    service handling the incoming requests.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Ray Train**：我们在**第6章**，**扩展**中已经遇到了 Ray。在这里，我们将使用 Ray Train 并行训练许多不同的 Prophet
    预测模型，并允许这些作业在向处理传入请求的主要网络服务发出请求时触发。'
- en: '**MLflow**: We met MLflow in *Chapter 3*, *From Model to Model Factory*, and
    this will be used as our model registry.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MLflow**：我们在**第3章**，**从模型到模型工厂**中遇到了 MLflow，它将作为我们的模型注册库。'
- en: '**FastAPI**: For Python, the go-to backend web frameworks are typically Django,
    Flask, and FastAPI. We will use FastAPI to create the main backend routing application
    that will serve the forecasts and interact with the other components of the solution.
    FastAPI is a web framework designed to be simple to use and for building highly
    performant web applications and is currently being used by some high-profile organizations,
    including Uber, Microsoft, and Netflix (according to the FastAPI homepage).'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**FastAPI**：对于 Python，典型的后端网络框架通常是 Django、Flask 和 FastAPI。我们将使用 FastAPI 创建主要后端路由应用程序，该应用程序将提供预测并与其他解决方案组件交互。FastAPI
    是一个设计用于简单使用和构建高性能网络应用的 Web 框架，目前被一些知名组织使用，包括 Uber、Microsoft 和 Netflix（根据 FastAPI
    主页信息）。'
- en: There has been some recent discussion around the potential for memory leaks
    when using FastAPI, especially for longer-running services. This means that ensuring
    you have enough RAM on the machines running your FastAPI endpoints can be very
    important. In many cases, this does not seem to be a critical issue, but it is
    an active topic being discussed within the FastAPI community. For more on this,
    please see [https://github.com/tiangolo/fastapi/discussions/9082](https://github.com/tiangolo/fastapi/discussions/9082).
    Other frameworks, such as **Litestar**, [https://litestar.dev/](https://litestar.dev/),
    do not seem to have the same issue, so feel free to play around with different
    web frameworks for the serving layer in the following example and in your projects.
    FastAPI is still a very useful framework with lots of benefits so we will proceed
    with it in this chapter; it is just important to bear this point in mind.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于使用 FastAPI 时可能出现的内存泄漏问题有一些讨论，尤其是对于长时间运行的服务。这意味着确保运行 FastAPI 端点的机器有足够的 RAM
    非常重要。在许多情况下，这似乎不是一个关键问题，但在 FastAPI 社区中是一个活跃的讨论话题。更多关于这个话题的信息，请参阅[https://github.com/tiangolo/fastapi/discussions/9082](https://github.com/tiangolo/fastapi/discussions/9082)。其他框架，如**Litestar**[https://litestar.dev/](https://litestar.dev/)，似乎没有相同的问题，所以你可以自由地尝试不同的网络框架来构建以下示例和你的项目中的服务层。FastAPI
    仍然是一个非常有用的框架，具有许多优点，所以我们将在本章中继续使用它；只是要记住这个要点。
- en: In this chapter, we are going to focus on the components of this system that
    are relevant to serving the models at scale, as the scheduled train and retrain
    aspects will be covered in *Chapter 9*, *Building an Extract, Transform, Machine
    Learning Use Case*. The components we focus on can be thought to consist of our
    “serving layer,” although I will show you how to use Ray to train several forecasting
    models in parallel.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注与大规模服务模型相关的系统组件，因为计划训练和重新训练方面将在**第9章**，**构建提取、转换、机器学习用例**中介绍。我们关注的组件可以被认为是我们的“服务层”，尽管我会向你展示如何使用
    Ray 并行训练多个预测模型。
- en: Now that we have made some tooling choices, let’s get building our ML microservice!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经做出了一些工具选择，让我们开始构建我们的机器学习微服务吧！
- en: Training at scale
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规模化训练
- en: When we introduced Ray in *Chapter 6*, *Scaling Up*, we mentioned use cases
    where the data or processing time requirements were such that using a very scalable
    parallel computing framework made sense. What was not made explicit is that sometimes
    these requirements come from the fact that we actually want to train *many models*,
    not just one model on a large amount of data or one model more quickly. This is
    what we will do here.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第6章“扩展”中介绍Ray时，我们提到了一些用例，其中数据或处理时间需求如此之大，以至于使用一个非常可扩展的并行计算框架是有意义的。没有明确指出的是，有时这些需求来自我们实际上想要训练**许多模型**的事实，而不仅仅是大量数据上的一个模型或更快地训练一个模型。这正是我们将在这里做的事情。
- en: The retail forecasting example we described in *Chapter 1*, *Introduction to
    ML Engineering* uses a data set with several different retail stores in it. Rather
    than creating one model that could have a store number or identifier as a feature,
    a better strategy would perhaps be to train a forecasting model for each individual
    store. This is likely to give better accuracy as the features of the data at the
    store level which may give some predictive power will not be averaged out by the
    model looking at a combination of all the stores together. This is therefore the
    approach we will take, and this is where we can use Ray’s parallelism to train
    multiple forecasting models simultaneously.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第1章“机器学习工程简介”中描述的零售预测示例使用了一个包含多个不同零售店的数据集。与其创建一个可能包含店铺编号或标识符作为特征的模型，也许更好的策略是为每个单独的店铺训练一个预测模型。这可能会提供更好的准确性，因为店铺级别的数据特征可能具有一些预测能力，不会被查看所有店铺组合的模型所平均。因此，我们将采取这种方法，这也是我们可以使用Ray的并行性同时训练多个预测模型的地方。
- en: 'To use **Ray** to do this, we need to take the training code we had in *Chapter
    1*, and adapt it slightly. First, we can bring together the functions we had for
    pre-processing the data and for training the forecasting models. Doing this means
    that we are creating one serial process that we can then distribute to run on
    the shard of the data corresponding to each store. The original functions for
    preprocessing and training the models were:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用**Ray**来完成这个任务，我们需要将我们在第1章中提到的训练代码稍作修改。首先，我们可以将用于预处理数据和训练预测模型的函数组合在一起。这样做意味着我们正在创建一个可以分发给运行在每个存储对应的数据分片上的串行进程。原始的预处理和训练模型函数如下：
- en: '[PRE4]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can now combine these into a single function that will take a `pandas` DataFrame,
    preprocess that data, train a Prophet forecasting model and then return predictions
    on the test set, the training dataset, the test dataset and the size of the training
    set, here labelled by the `train_index` value. Since we wish to distribute the
    application of this function, we need to use the `@ray.remote` decorator that
    we introduced in *Chapter 6*, *Scaling Up*. We pass in the `num_returns=4` argument
    to the decorator to let Ray know that this function will return four values in
    a tuple.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这些合并成一个单独的函数，该函数将接受一个`pandas` DataFrame，预处理这些数据，训练一个Prophet预测模型，然后返回测试集、训练数据集、测试数据集和训练集大小的预测，这里用`train_index`值标记。由于我们希望分发此函数的应用，我们需要使用我们在第6章“扩展”中介绍的`@ray.remote`装饰器。我们将`num_returns=4`参数传递给装饰器，让Ray知道这个函数将以元组的形式返回四个值。
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have our remote function we just need to apply it. First, we assume
    that the dataset has been read into a `pandas` DataFrame in the same manner as
    in *Chapter 1*, *Introduction to ML Engineering*. The assumption here is that
    the dataset is small enough to fit in memory and doesn’t require computationally
    intense transformations. This has the advantage of allowing us to use `pandas`
    relatively smart data ingestion logic, which allows for various formatting of
    the header row for example, as well as apply any filtering or transformation logic
    we want to before distribution using that now familiar `pandas` syntax. If the
    dataset was larger or the transformations more intense, we could have used the
    `ray.data.read_csv()` method from the Ray API to read the data in as a Ray Dataset.
    This reads the data into an Arrow data format, which has its own data manipulation
    syntax.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了远程函数，我们只需要应用它。首先，我们假设数据集已经以与*第1章*，*机器学习工程介绍*中相同的方式读入到一个`pandas` DataFrame中。这里的假设是数据集足够小，可以放入内存，并且不需要计算密集型的转换。这有一个优点，就是允许我们使用`pandas`相对智能的数据摄入逻辑，例如，可以对标题行进行各种格式化，以及在我们现在熟悉的`pandas`语法中使用之前，应用任何我们想要的过滤或转换逻辑。如果数据集更大或转换更密集，我们就可以使用Ray
    API中的`ray.data.read_csv()`方法来读取数据作为Ray Dataset。这会将数据读入到Arrow数据格式中，它有自己的数据操作语法。
- en: Now, we are ready to apply our distributed training and testing. First, we can
    retrieve all of the store identifiers from the dataset, as we are going to train
    a model for each one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好应用我们的分布式训练和测试。首先，我们可以从数据集中检索所有存储标识符，因为我们将为每一个训练一个模型。
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Before we do anything else we will initialize the Ray cluster using the `ray.init()`
    command we met in *Chapter 6*, *Scaling Up*. This avoids performing the intitialization
    when we first call the remote function, meaning we can get accurate timings of
    the actual processing if we perform any benchmarking. To aid performance, we can
    also use `ray.put()` to store the pandas DataFrame in the Ray object store. This
    stops us replicating this dataset every time we run a task. Putting an object
    in the store returns an id, which you can then use for function arguments just
    like the original object.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们做任何事情之前，我们将使用我们在*第6章*，*扩展规模*中遇到的`ray.init()`命令初始化Ray集群。这避免了在我们第一次调用远程函数时执行初始化，这意味着如果我们进行基准测试，我们可以获得实际处理的准确时间。为了提高性能，我们还可以使用`ray.put()`将pandas
    DataFrame存储在Ray对象存储中。这阻止了每次运行任务时都复制此数据集。将对象放入存储返回一个id，然后你可以像原始对象一样将其用作函数参数。
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we need to submit our Ray tasks to the cluster. Whenever you do this, a
    Ray object reference is returned that will allow you to retrieve the data for
    the process when we use `ray.get` to collect the results. The syntax I’ve used
    here may look a bit complicated, but we can break it down piece by piece. The
    core Python function `map`, just applies the list operation to all of the elements
    of the result of the `zip` syntax. The `zip(*iterable)` pattern allows us to unzip
    all of the elements in the list comprehension, so that we can have a list of prediction
    object references, training data object references, test data object references
    and finally the training index object references. Note the use of `df_id` for
    referencing the stored dataframe in the object store.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将我们的Ray任务提交到集群。每次你这样做时，都会返回一个Ray对象引用，这将允许我们在使用`ray.get`收集结果时检索该进程的数据。我在这里使用的语法可能看起来有点复杂，但我们可以一点一点地分解它。核心Python函数`map`只是将列表操作应用于`zip`语法的输出结果的所有元素。`zip(*iterable)`模式允许我们将列表推导式中的所有元素解包，这样我们就可以有一个包含预测对象引用、训练数据对象引用、测试数据对象引用以及最终的训练索引对象引用的列表。注意使用`df_id`来引用对象存储中的存储数据框。
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We then need to get the actual results of these tasks, which we can do by using
    `ray.get()` as discussed.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要获取这些任务的实际结果，这可以通过使用前面讨论的`ray.get()`来实现。
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can then access the values of these for each model with `ray_results['predictions'][<index>]`
    and so on.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用`ray_results['predictions'][<index>]`等来访问每个模型的这些值。
- en: In the Github repository, the file `Chapter08/train/train_forecasters_ray.py`
    runs this syntax and an example for loop for training the Prophet models one by
    one in serial fashion for comparison. Using the `time` library for the measurements
    and running the experiment on my Macbook with four CPUs being utilized by the
    Ray cluster, I was able to train 1,115 Prophet models in just under 40 seconds
    using Ray, compared to around 3 minutes 50 seconds using the serial code. That’s
    an almost six-fold speedup, without doing much optimization!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Github 仓库中，文件 `Chapter08/train/train_forecasters_ray.py` 运行此语法并示例循环，逐个以串行方式训练
    Prophet 模型以进行比较。使用 `time` 库进行测量，并在我的 Macbook 上运行实验，Ray 集群利用了四个 CPU，我仅用不到 40 秒就能用
    Ray 训练 1,115 个 Prophet 模型，而使用串行代码则需要大约 3 分 50 秒。这几乎提高了六倍的速度，而且几乎没有进行多少优化！
- en: We did not cover the saving of the models and metadata into MLFlow, which you
    can do using the syntax we discussed in depth in *Chapter 3*, *From Model to Model
    Factory*. To avoid lots of communication overhead, it may be best to store the
    metadata temporarily as the result of the training process, like we have done
    in the dictionary storing the predictions and then write everything to MLFlow
    at the end. This means you do not slow down the Ray processes with communications
    to the MLFlow server. Note also that we could have optimized this parallel processing
    even further by using the Ray Dataset API discussed and altering the transformation
    logic to use Arrow syntax. A final option would also have been to use **Modin**,
    previously known as Pandas on Ray, which allows you to use `pandas` syntax whilst
    leveraging the parallelism of Ray.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有涵盖将模型和元数据保存到 MLFlow 的内容，你可以使用我们在 *第 3 章* 中深入讨论的语法来完成。为了避免大量的通信开销，最好是将元数据临时存储为训练过程的结果，就像我们在存储预测的字典中做的那样，然后在最后将所有内容写入
    MLFlow。这意味着你不会因为与 MLFlow 服务器的通信而减慢 Ray 进程。注意，我们还可以通过使用讨论过的 Ray Dataset API 并更改转换逻辑以使用
    Arrow 语法来进一步优化这种并行处理。最后一个选择也可以是使用 **Modin**，之前被称为 Pandas on Ray，它允许你在利用 Ray 并行性的同时使用
    `pandas` 语法。
- en: Let’s now start building out the serving layer for our solution, so that we
    can use these forecasting models to generate results for other systems and users.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始构建我们解决方案的提供层，这样我们就可以使用这些预测模型为其他系统和用户生成结果。
- en: Serving the models with FastAPI
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FastAPI 提供模型
- en: The simplest and potentially most flexible approach to serving ML models in
    a microservice with Python is in wrapping the serving logic inside a lightweight
    web application. Flask has been a popular option among Python users for many years
    but now the FastAPI web framework has many advantages, which means it should be
    seriously considered as a better alternative.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，以微服务形式提供 ML 模型的最简单且可能最灵活的方法是将提供逻辑包装在一个轻量级 Web 应用程序中。Flask 多年来一直是
    Python 用户中流行的选择，但现在 FastAPI Web 框架有许多优势，这意味着它应该被认真考虑作为更好的替代方案。
- en: 'Some of the features of FastAPI that make it an excellent choice for a lightweight
    microservice are:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使 FastAPI 成为轻量级微服务优秀选择的某些特性包括：
- en: '**Data validation**: FastAPI uses and is based on the **Pydantic** library,
    which allows you to enforce type hints at runtime. This allows for the implementation
    of very easy-to-create data validation steps that make your system way more robust
    and helps avoid edge case behaviors.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证**：FastAPI 使用并基于 **Pydantic** 库，该库允许你在运行时强制执行类型提示。这允许你实现非常容易创建的数据验证步骤，使你的系统更加健壮，并有助于避免边缘情况的行为。'
- en: '**Built-in async workflows**: FastAPI gives you asynchronous task management
    out of the box with `async` and `await` keywords, so you can build the logic you
    will need in many cases relatively seamlessly without resorting to extra libraries.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内置的异步工作流程**：FastAPI 通过 `async` 和 `await` 关键字提供开箱即用的异步任务管理，因此你可以在许多情况下相对无缝地构建所需的逻辑，而无需求助于额外的库。'
- en: '**Open specifications**: FastAPI is based on several open source standards
    including the **OpenAPI REST API standard** and the **JSON Schema** declarative
    language, which helps create automatic data model documentation. These specs help
    keep the workings of FastAPI transparent and very easy to use.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放规范**：FastAPI 基于几个开源标准，包括 **OpenAPI REST API 标准** 和 **JSON Schema** 声明性语言，这有助于创建自动数据模型文档。这些规范有助于保持
    FastAPI 的工作方式透明，并且非常易于使用。'
- en: '**Automatic documentation generation**: The last point mentioned this for data
    models, but FastAPI also auto-generates documentation for your entire service
    using SwaggerUI.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动文档生成**: 上一点提到了数据模型，但 FastAPI 还使用 SwaggerUI 自动生成整个服务的文档。'
- en: '**Performance**: Fast is in the name! FastAPI uses the **Asynchronous Server
    Gateway Interface** (**ASGI**) standard, whereas other frameworks like Flask use
    the **Web Server Gateway Interface** (**WSGI**). The ASGI can process more requests
    per unit of time and does so more efficiently, as it can execute tasks without
    waiting for previous tasks to finish. The WSGI interface executes specified tasks
    sequentially and so takes longer to process requests.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**: 快速是它的名字！FastAPI 使用了 **异步服务器网关接口** (**ASGI**) 标准，而其他框架如 Flask 则使用 **Web
    服务器网关接口** (**WSGI**)。ASGI 可以在单位时间内处理更多的请求，并且效率更高，因为它可以在等待前一个任务完成之前执行任务。WSGI 接口按顺序执行指定的任务，因此处理请求需要更长的时间。'
- en: So, the above are the reasons why it might be a good idea to use FastAPI to
    serve the forecasting models in this example, but how do we go about doing that?
    That is what we will now cover.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上述内容是为什么使用 FastAPI 来提供本例中的预测模型可能是一个好主意的原因，但我们该如何着手去做呢？这正是我们现在要讨论的。
- en: 'Any microservice has to have data come into it in some specified format; this
    is called the “request.” It will then return data, known as the “response.” The
    job of the microservice is to ingest the request, execute a series of tasks that
    the request either defines or gives input for, create the appropriate output,
    and then transform that into the specified request format. This may seem basic,
    but it is important to recap and gives us the starting point for designing our
    system. It is clear that we will have to take into account the following points
    in our design:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 任何微服务都必须以某种指定的格式接收数据；这被称为“请求”。然后它将返回数据，称为“响应”。微服务的任务是摄取请求，执行请求定义或提供输入的一系列任务，创建适当的输出，然后将该输出转换为指定的请求格式。这看起来可能很基础，但回顾这一点很重要，它为我们设计系统提供了起点。很明显，在设计时，我们必须考虑以下要点：
- en: '**Request and response schemas**: Since we will be building a REST API, it
    is natural that we will specify the data model for the request and responses as
    JSON objects with associated schemas. The key when doing this is that the schemas
    are as simple as possible and that they contain all the information necessary
    for the client (the requesting service) and the server (the microservice) to perform
    the appropriate actions. Since we are building a forecasting service, the request
    object must provide enough information to allow the system to provide an appropriate
    forecast, which the upstream solution calling the service can present to users
    or perform further logic on. The response will have to contain the actual forecast
    data points or some pointer toward the location of the forecast.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**请求和响应模式**: 由于我们将构建一个 REST API，因此自然地，我们将指定请求和响应的数据模型，作为具有相关模式的 JSON 对象。在执行此操作时，关键是使模式尽可能简单，并且它们包含客户端（请求服务）和服务器（微服务）执行适当操作所需的所有必要信息。由于我们正在构建一个预测服务，请求对象必须提供足够的信息，以便系统提供适当的预测，上游调用服务的解决方案可以将其展示给用户或执行进一步的逻辑。响应将必须包含实际的预测数据点或指向预测位置的指针。'
- en: '**Compute**: The creation of the response object, in this case, a forecast,
    requires computation, as discussed in *Chapter 1*, *Introduction to ML Engineering*.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算**: 在本例中，创建响应对象（在这种情况下，是一个预测），需要计算，正如在 *第一章*，*机器学习工程导论* 中所讨论的。'
- en: A key consideration in designing ML microservices is the size of this compute
    resource and the appropriate tooling needed to execute it. As an example, if you
    are running a computer vision model that requires a large GPU in order to perform
    inference, you cannot do this on the server running the web application backend
    if that is only a small machine running a CPU. Similarly, if the inference step
    requires the ingestion of a terabyte of data, this may require us to use a parallelization
    framework like Spark or Ray running on a dedicated cluster, which by definition
    will have to be running on different machines from the serving web application.
    If the compute requirements are small enough and fetching data from another location
    is not too intense, then you may be able to run the inference on the same machine
    hosting the web application.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计机器学习微服务时的一个关键考虑因素是计算资源的大小以及执行它所需的适当工具。例如，如果你正在运行一个需要大型GPU才能进行推理的计算机视觉模型，你不能在只运行CPU的小型服务器上这样做，该服务器运行的是网络应用程序的后端。同样，如果推理步骤需要摄取一个TB的数据，这可能需要我们使用像Spark或Ray这样的并行化框架，在专用集群上运行，根据定义，它将不得不在不同的机器上运行，而不是运行服务网络应用程序的机器。如果计算需求足够小，并且从另一个位置获取数据不是太激烈，那么你可能在同一台机器上运行推理，该机器托管着网络应用程序。
- en: '**Model management**: This is an ML service, so, of course, there are models
    involved! This means, as discussed in detail in *Chapter 3*, *From Model to Model
    Factory*, we will need to implement a robust process for managing the appropriate
    model versions. The requirements for this example also mean that we have to be
    able to utilize many different models in a relatively dynamic fashion. This will
    require some careful consideration and the use of a model management tool like
    MLflow, which we met in *Chapter 3* as well. We also have to consider our strategies
    for updating and rolling back models; for example, will we use blue/green deployments
    or canary deployments, as discussed in *Chapter 5*, *Deployment Patterns and Tools*.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型管理**：这是一个机器学习服务，所以当然涉及模型！这意味着，正如我们在*第3章*中详细讨论的，*从模型到模型工厂*，我们需要实施一个健壮的过程来管理适当的模型版本。这个示例的要求还意味着我们必须能够以相对动态的方式利用许多不同的模型。这需要我们仔细考虑，并使用像MLflow这样的模型管理工具，我们也在*第3章*中提到过。我们还必须考虑我们的模型更新和回滚策略；例如，我们将使用蓝/绿部署还是金丝雀部署，正如我们在*第5章*，*部署模式和工具*中讨论的那样。'
- en: '**Performance monitoring**: For any ML system, as we have discussed at length
    throughout the book, monitoring the performance of models will be critically important,
    as will taking appropriate action to update or roll back these models. If the
    truth data for any inference cannot be immediately given back to the service,
    then this will require its own process for gathering together truth and inferences
    before performing the desired calculations on them.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**性能监控**：对于任何机器学习系统，正如我们在整本书中详细讨论的那样，监控模型的性能将至关重要，采取适当的行动来更新或回滚这些模型也同样重要。如果任何推理的真实数据不能立即返回给服务，那么这需要它自己的过程来收集真实数据和推理，然后再对它们进行所需的计算。'
- en: These are some of the important points we will have to consider as we build
    our solution. In this chapter, we will focus on points 1 and 3, as *Chapter 9*
    will cover how to build training and monitoring systems that run in a batch setting.
    Now that we know some of the things we want to factor into our solution, let’s
    get on and start building!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们构建解决方案时必须考虑的一些重要点。在本章中，我们将重点关注第1点和第3点，因为*第9章*将涵盖如何在批量设置中构建训练和监控系统。既然我们已经知道了一些我们想要纳入解决方案的因素，那么让我们开始动手构建吧！
- en: Response and request schemas
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 响应和请求模式
- en: If the client is asking for a forecast for a specific store, as we are assuming
    in the requirements, then this means that the request should specify a few things.
    First, it should specify the store, using some kind of store identifier that will
    be kept in common between the data models of the ML microservice and the client
    application.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果客户端请求特定商店的预测，正如我们在需求中假设的那样，这意味着请求应该指定一些内容。首先，它应该指定商店，使用某种类型的商店标识符，该标识符将在机器学习微服务的数据模型和客户端应用程序之间保持通用。
- en: Second, the time range for the forecast should be provided in an appropriate
    format that can be easily interpreted and serviced by the application. The systems
    should also have logic in place to create appropriate forecast time windows if
    none are provided in the request, as it is perfectly reasonable to assume if a
    client is requesting “a forecast for store X,” then we can assume some default
    behavior that provides a forecast for some time period from now into the future
    will likely be useful to the client application.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，预测的时间范围应以适当的格式提供，以便应用程序可以轻松解释并提供服务。系统还应具备逻辑来创建适当的预测时间窗口，如果请求中没有提供，这是完全合理的假设，如果客户端请求“为商店
    X 提供预测”，那么我们可以假设一些默认行为，提供从现在到未来的某个时间段的预测将可能对客户端应用程序有用。
- en: 'The simplest request JSON schema that satisfies this is then something like:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 满足这一点的最简单的请求 JSON 架构可能如下所示：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As this is a JSON object, all of the fields are strings, but they are populated
    with values that will be easily interpretable within our Python application. The
    Pydantic library will also help us to enforce data validation, which we will discuss
    later. Note that we should also allow for the client application to request multiple
    forecasts, so we should allow for this JSON to be extended to allow for lists
    of request objects:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个 JSON 对象，所有字段都是字符串类型，但它们填充了在我们 Python 应用程序中易于解释的值。Pydantic 库还将帮助我们执行数据验证，这一点我们稍后将会讨论。请注意，我们还应该允许客户端应用程序请求多个预测，因此我们应该允许这个
    JSON 扩展以允许请求对象的列表：
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As mentioned, we would like to build our application logic so that the system
    would still work even if the client only made a request specifying the `store_id`,
    and then we infer the appropriate forecast horizon to be from now to some time
    in the future.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们希望构建我们的应用程序逻辑，以便即使客户端只指定了`store_id`，系统仍然可以工作，然后我们推断适当的预测时间范围是从现在到未来的某个时间。
- en: 'This means our application should work when the following is submitted as the
    JSON body in the API call:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的应用程序应该在以下内容作为 API 调用的 JSON 主体提交时工作：
- en: '[PRE12]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To enforce these constraints on the request, we can use the Pydantic functionality
    where we inherit from the Pydantic `BaseModel` and create a data class defining
    the type requirements we have just made:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强制执行这些请求约束，我们可以使用 Pydantic 功能，通过从 Pydantic 的`BaseModel`继承并创建一个数据类来定义我们刚刚做出的类型要求：
- en: '[PRE13]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, we have enforced here that the `store_id` is a string, but we
    have allowed for the beginning and end dates for the forecast to be given as `None`.
    If the dates are not specified, we could make a reasonable assumption based on
    our business knowledge that a useful forecast time window would be from the datetime
    of the request to seven days from now. This could be something that is changed
    or even provided as a configuration variable in the application config. We will
    not deal with that particular aspect here to focus on the more the more exciting
    stuff, so this is left as fun exercise for the reader!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在这里强制执行了`store_id`是一个字符串，但我们允许预测的开始和结束日期可以给出为`None`。如果没有指定日期，我们可以根据我们的业务知识做出合理的假设，即一个有用的预测时间窗口将从请求的日期时间开始，到现在的七天。这可能是在应用程序配置中更改或提供的东西，我们在这里不会处理这个特定的方面，以便专注于更令人兴奋的内容，所以这留给读者作为有趣的练习！
- en: 'The forecasting model in our case will be based on the Prophet library, as
    discussed, and this requires an index that contains the datetimes for the forecast
    to run over. To produce this based on the request, we can write a simple helper
    function:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，预测模型将基于 Prophet 库，如前所述，这需要一个包含预测运行所需日期时间的索引。为了根据请求生成这个索引，我们可以编写一个简单的辅助函数：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This logic then allows us to create the input to the forecasting model once
    it is retrieved from the model storage layer, in our case, MLflow.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逻辑允许我们在从模型存储层检索到输入后创建预测模型，在我们的例子中，是 MLflow。
- en: 'The response object has to return the forecast in some data format, and it
    is always imperative that you return enough information for the client application
    to be able to conveniently associate the returned object with the response that
    triggered its creation. A simple schema that satisfies this would be something
    like:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 响应对象必须以某种数据格式返回预测，并且始终必须返回足够的信息，以便客户端应用程序能够方便地将返回的对象与触发其创建的响应关联起来。满足这一点的简单模式可能如下所示：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We will allow for this to be extended as a list in the same way as the request
    JSON schema. We will work with these schemas for the rest of this chapter. Now,
    let’s look at how we will manage the models in the application.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将允许以与请求JSON模式相同的方式将其扩展为列表。我们将在本章的其余部分使用这些模式。现在，让我们看看我们将如何管理应用程序中的模型。
- en: Managing models in your microservice
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的微服务中管理模型
- en: In *Chapter 3*, *From Model to Model Factory*, we discussed in detail how you
    can use MLflow as a model artifact and metadata storage layer in your ML systems.
    We will do the same here, so let’s assume that you have an MLflow Tracking server
    already running and then we just need to define our logic for interacting with
    it. If you need a refresher, feel free to revisit *Chapter 3*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章*从模型到模型工厂*中，我们详细讨论了您如何使用MLflow作为模型工件和元数据存储层在您的ML系统中。我们在这里也将这样做，所以假设您已经有一个运行的MLflow
    Tracking服务器，然后我们只需要定义与它交互的逻辑。如果您需要复习，请随时回顾第3章。
- en: 'We will need to write some logic that does the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要编写一些逻辑来完成以下操作：
- en: Checks there are models available for use in production in the MLflow server.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查在MLflow服务器中有可用于生产的模型。
- en: Retrieves a version of the model satisfying any criteria we wish to set, for
    example, that the model was not trained more than a certain number of days ago
    and that it has validation metrics within a chosen range.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索满足我们设定的任何标准的模型版本，例如，模型不是在超过一定天数前训练的，并且它在所选范围内有验证指标。
- en: Caches the model for use and reuse during the forecasting session if desired.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在预测会话期间需要，可以缓存模型以供使用和重复使用。
- en: Does all of the above for multiple models if that is required from the response
    object.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果响应对象需要，对多个模型执行上述所有操作。
- en: 'For point 1, we will have to have models tagged as ready for production in
    the MLflow model registry and then we can use the `MlflowClient()` and `mlflow
    pyfunc` functionality we met in *Chapter 3*, *From Model to Model Factory*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第1点，我们必须在MLflow模型注册表中标记模型为已准备好生产，然后我们可以使用在第3章*从模型到模型工厂*中遇到的`MlflowClient()`和`mlflow
    pyfunc`功能：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For point 2, we can retrieve the metrics for a given model by using the MLflow
    functionality we will describe below. First, using the name of the model, you
    retrieve the model’s metadata:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第2点，我们可以通过使用下面将要描述的MLflow功能来检索给定模型的指标。首先，使用模型的名称，您检索模型的元数据：
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will return a dataset like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个如下所示的数据集：
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can then use that data to retrieve the version via this object and then
    retrieve the model version metadata:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用这些数据通过此对象检索版本，然后检索模型版本元数据：
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This contains metadata that looks something like:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这包含看起来像这样的元数据：
- en: '[PRE20]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The metrics information for this model version is associated with the `run_id`,
    so we need to get that:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型版本的指标信息与`run_id`相关联，因此我们需要获取它：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The value for the `run_id` would be something like:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_id`的值可能如下所示：'
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can then use this information to get the model metrics for the specific
    run, and then perform any logic you want on top of it. To retrieve the metric
    values, you can use the following syntax:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用这些信息来获取特定运行的模型指标，并在其上执行任何您想要的逻辑。要检索指标值，您可以使用以下语法：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As an example, you could use logic like the one applied in *Chapter 2* in the
    *Continuous model performance testing* section and simply demand the root mean
    squared error is below some specified value before allowing it to be used in the
    forecasting service.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用在第2章*持续模型性能测试*部分中应用的逻辑，并简单地要求均方根误差低于某个指定的值，然后才允许它在预测服务中使用。
- en: We may also want to allow for the service to trigger retraining if the model
    is out of tolerance in terms of age; this could act as another layer of model
    management on top of any training systems put in place.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能希望允许服务在模型年龄超出容忍度时触发重新训练；这可以作为任何已实施的训练系统之上的另一层模型管理。
- en: 'If our training process is orchestrated by an Airflow DAG running on AWS MWAA,
    as we discussed in *Chapter 5*, *Deployment Patterns and Tools*, then the below
    code could be used to invoke the training pipeline:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的训练过程由运行在AWS MWAA上的Airflow DAG编排，正如我们在第5章*部署模式和工具*中讨论的那样，那么以下代码可以用来调用训练管道：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The next sections will outline how these pieces can be brought together so that
    the FastAPI service can wrap around several pieces of this logic before discussing
    how we containerize and deploy the app.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 下几节将概述如何将这些组件组合在一起，以便 FastAPI 服务可以在讨论如何容器化和部署应用程序之前，围绕这些逻辑的几个部分进行包装。
- en: Pulling it all together
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有这些整合在一起
- en: We have successfully defined our request and response schemas, and we’ve written
    logic to pull the appropriate model from our model repository; now all that is
    left to do is to tie all this together and perform the actual inference with the
    model. There are a few steps to this, which we will break down now. The main file
    for the FastAPI backend is called `app.py` and contains a few different application
    routes. For the rest of the chapter, I will show you the necessary imports just
    before each relevant piece of code, but the actual file follows the PEP8 convention
    of imports at the top of the file.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功定义了我们的请求和响应模式，并且我们已经编写了从我们的模型存储库中提取适当模型的相关逻辑；现在剩下的只是将这些整合在一起，并使用模型进行实际推理。这里有几个步骤，我们将现在分解。FastAPI
    后端的主要文件名为 `app.py`，其中包含几个不同的应用程序路由。对于本章的其余部分，我将在每个相关代码片段之前展示必要的导入，但实际的文件遵循 PEP8
    规范，即导入位于文件顶部。
- en: 'First, we define our logger and we set up some global variables to act as a
    lightweight in-memory cache of the retrieved models and service handlers:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的日志记录器，并设置一些全局变量作为检索到的模型和服务处理程序的轻量级内存缓存：
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Using global variables to pass objects between application routes is only a
    good idea if you know that this app will run in isolation and not create race
    conditions by receiving requests from multiple clients simultaneously. When this
    happens, multiple processes try and overwrite the variable. You can adapt this
    example to replace the use of global variables with the use of a cache like **Redis**
    or **Memcache** as an exercise!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全局变量在应用程序路由之间传递对象只是一种好主意，如果你知道这个应用程序将以独立方式运行，并且不会因为同时接收来自多个客户端的请求而创建竞争条件。当这种情况发生时，多个进程会尝试覆盖变量。你可以将这个例子改编一下，用缓存如
    **Redis** 或 **Memcache** 来替换全局变量的使用，作为练习！
- en: 'We then have to instantiate a `FastAPI` app object and we can define any logic
    we want to run upon startup by using the start-up lifespan event method:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要实例化一个 `FastAPI` 应用程序对象，并且可以通过使用启动生命周期事件方法来定义我们希望在启动时运行的任何逻辑：
- en: '[PRE26]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As already mentioned, FastAPI is great for supporting async workflows, allowing
    for the compute resources to be used while awaiting for other tasks to complete.
    The instantiation of the service handlers could be a slower process so this can
    be useful to adopt here. When functions that use the `async` keyword are called,
    we need to use the `await` keyword, which means that the rest of the function
    where the `async` function has been called can be suspended until a result is
    returned and the resources used for tasks elsewhere. Here we have only one handler
    to instantiate, which will handle connections to the MLflow Tracking server.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，FastAPI 非常适合支持异步工作流程，允许在等待其他任务完成时使用计算资源。服务处理程序的实例化可能是一个较慢的过程，因此在这里采用这可能是有用的。当调用使用
    `async` 关键字的函数时，我们需要使用 `await` 关键字，这意味着在调用 `async` 函数的函数中，其余部分可以暂停，直到返回结果并释放用于其他任务的任务资源。在这里，我们只有一个处理程序需要实例化，它将处理与
    MLflow 跟踪服务器的连接。
- en: 'The `registry.mlflow.handler` module is one I have written containing the `MLFlowHandler`
    class, with methods we will use throughout the app. The following are the contents
    of that module:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`registry.mlflow.handler` 模块是我编写的一个包含 `MLFlowHandler` 类的模块，其中包含我们将在整个应用程序中使用的各种方法。以下是该模块的内容：'
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, this handler has methods for checking the MLflow Tracking server
    is up and running and getting production models. You could also add methods for
    querying the MLflow API to gather the metrics data we mentioned before.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，此处理程序具有检查 MLflow 跟踪服务器是否正常运行并获取生产模型的方法。您还可以添加用于查询 MLflow API 以收集我们之前提到的度量数据的方法。
- en: 'Returning to the main `app.py` file now, I’ve written a small health check
    endpoint to get the status of the service:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到主要的 `app.py` 文件，我编写了一个小的健康检查端点来获取服务的状态：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next comes a method to get the production model for a given retail store ID.
    This function checks if the model is already available in the `global` variable
    (acting as a simple cache) and if it isn’t there, adds it. You could expand this
    method to include logic around the age of the model or any other metrics you wanted
    to use to decide whether or not to pull the model into the application:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一个获取特定零售店 ID 的生产模型的方法。此函数检查模型是否已存在于 `global` 变量中（作为简单的缓存），如果不存在，则添加它。您可以将此方法扩展到包括关于模型年龄或您想要使用的任何其他指标的逻辑，以决定是否将模型拉入应用程序：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we have the forecasting endpoint, where the client can hit this application
    with the request objects we defined before and get the forecasts based on the
    Prophet models we retrieve from MLflow. Just like in the rest of the book, I’ve
    omitted longer comments for brevity:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有预测端点，客户端可以使用我们之前定义的请求对象向此应用程序发起请求，并基于我们从 MLflow 获取的 Prophet 模型获得预测。就像本书的其他部分一样，为了简洁，我省略了较长的注释：
- en: '[PRE30]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can then run the app locally with:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以在本地运行应用程序：
- en: '[PRE31]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: And you can add the `–reload` flag if you want to develop the app while it is
    running. If you use Postman (or `curl` or any other tool of your choice) and query
    this endpoint with a request body as we described previously, see *Figure 8.2*,
    you will get an output like that shown in *Figure 8.3*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在不运行应用程序的情况下开发应用程序，可以添加 `–reload` 标志。如果您使用 Postman（或 `curl` 或您选择的任何其他工具）并使用我们之前描述的请求体查询此端点，如
    *图 8.2* 所示，您将得到类似 *图 8.3* 中所示的输出。
- en: '![](img/B19525_08_02.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_08_02.png)'
- en: 'Figure 8.2: A request to the ML microservice in the Postman app.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：Postman 应用中对 ML 微服务的请求。
- en: '![](img/B19525_08_03.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_08_03.png)'
- en: 'Figure 8.3: The response from the ML microservice when querying with Postman.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：使用 Postman 查询时 ML 微服务的响应。
- en: And that’s it, we have a relatively simple ML microservice that will return
    a Prophet model forecast for retail stores upon querying the endpoint! We will
    now move on to discussing how we can containerize this application and deploy
    it to a Kubernetes cluster for scalable serving.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们得到了一个相对简单的机器学习微服务，当查询端点时，它将返回零售店的 Prophet 模型预测！现在，我们将继续讨论如何将此应用程序容器化并部署到
    Kubernetes 集群以实现可扩展的服务。
- en: Containerizing and deploying to Kubernetes
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化和部署到 Kubernetes
- en: When we introduced Docker in *Chapter 5*, *Deployment Patterns and Tools*, we
    showed how you can use it to encapsulate your code and then run it across many
    different platforms consistently.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 *第 5 章* 中介绍 Docker 时，*部署模式和工具*，我们展示了如何使用它来封装您的代码，然后在许多不同的平台上一致地运行它。
- en: Here we will do this again, but with the idea in mind that we don’t just want
    to run the application as a singleton on a different piece of infrastructure,
    we actually want to allow for many different replicas of the microservice to be
    running simultaneously with requests being routed effectively by a load balancer.
    This means that we can take what works and make it work at almost arbitrarily
    large scales.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将再次执行此操作，但带着这样的想法：我们不仅想在不同的基础设施上以单例模式运行应用程序，实际上我们希望允许许多不同的微服务副本同时运行，并且通过负载均衡器有效地路由请求。这意味着我们可以将可行的方法扩展到几乎任意大的规模。
- en: 'We will do this by executing several steps:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过执行以下步骤来完成这项工作：
- en: Containerize the application using Docker.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Docker 容器化应用程序。
- en: Push this Docker container to Docker Hub to act as our container storage location
    (you could use another container management solution like AWS Elastic Container
    Registry or similar solutions on another cloud provider for this step).
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此 Docker 容器推送到 Docker Hub，作为我们的容器存储位置（您可以使用 AWS Elastic Container Registry
    或其他云服务提供商的类似解决方案来完成此步骤）。
- en: Create a Kubernetes cluster. We will do this locally using minikube, but you
    can do this on a cloud provider using its managed Kubernetes service. On AWS,
    this is **Elastic Kubernetes Service** (**EKS**).
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Kubernetes 集群。我们将使用 minikube 在本地执行此操作，但您也可以在云服务提供商上使用其管理的 Kubernetes 服务来完成此操作。在
    AWS 上，这是 **弹性 Kubernetes 服务**（**EKS**）。
- en: Define a service and load balancer on the cluster that can scale. Here we will
    introduce the concept of manifests for programmatically defining service and deployment
    characteristics on Kubernetes clusters.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在可以扩展的集群上定义一个服务和负载均衡器。在这里，我们将介绍在 Kubernetes 集群上通过程序定义服务和部署特性的概念。
- en: Deploy the service and test that it is working as expected.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署服务并测试其是否按预期工作。
- en: Let us now go through these steps in the next section.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在进入下一节，详细说明这些步骤。
- en: Containerizing the application
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器化应用程序
- en: 'As introduced earlier in the book, if we want to use Docker, we need to give
    instructions for how to build the container and install any necessary dependencies
    in a Dockerfile. For this application, we can use one that is based on one of
    the available FastAPI container images, assuming we have a file called `requirements.txt`
    that contains all of our Python package dependencies:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如本书前面所述，如果我们想使用 Docker，我们需要在 Dockerfile 中提供如何构建容器以及安装任何必要的依赖项的说明。对于这个应用程序，我们可以使用基于可用的
    FastAPI 容器镜像之一的一个，假设我们有一个名为 `requirements.txt` 的文件，其中包含我们所有的 Python 包依赖项：
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can then build this Docker container using the following command, where
    I have named the container `custom-forecast-service`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令构建这个 Docker 容器，其中我已将容器命名为 `custom-forecast-service`：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once this has been successfully built, we need to push it to Docker Hub. You
    can do this by logging in to Docker Hub in the terminal and then pushing to your
    account by running:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功构建，我们需要将其推送到 Docker Hub。您可以在终端中登录 Docker Hub，然后通过运行以下命令将内容推送到您的账户：
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This means that other build processes or solutions can download and run your
    container.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着其他构建过程或解决方案可以下载并运行您的容器。
- en: 'Note that before you push to Docker Hub, you can test that the containerized
    application runs by executing a command like the following, where I have included
    a platform flag in order to run the container locally on my MacBook Pro:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在您将内容推送到 Docker Hub 之前，您可以通过执行以下类似命令来测试容器化应用程序是否可以运行，其中我包含了一个平台标志，以便在我的 MacBook
    Pro 上本地运行容器：
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that we have built and shared the container, we can now work on scaling
    this up with a deployment to Kubernetes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建并分享了容器，我们可以通过部署到 Kubernetes 来扩展它。
- en: Scaling up with Kubernetes
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 扩展
- en: Working with Kubernetes can be a steep learning curve for even the most seasoned
    developers, so we will only scratch the surface here and give you enough to get
    started on your own learning journey. This section will walk you through the steps
    you need to deploy your ML microservice onto a Kubernetes cluster running locally,
    as it takes the same steps to deploy to a remotely hosted cluster (with minor
    modifications). Operating Kubernetes clusters seamlessly in production requires
    consideration of topics such as networking, cluster resource configuration and
    management, security policies, and much more. Studying all of these topics in
    detail would require an entire book in itself. In fact, an excellent resource
    to get up to speed on many of these details is *Kubernetes in Production Best
    Practices* by Aly Saleh and Murat Karsioglu. In this chapter, we will instead
    focus on understanding the most important steps to get you up and running and
    developing ML microservices using Kubernetes.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kubernetes 一起工作可能对即使是经验最丰富的开发者来说也是一个陡峭的学习曲线，所以我们在这里只会触及表面，并为您提供足够的资源，让您开始自己的学习之旅。本节将指导您完成将您的
    ML 微服务部署到本地运行的 Kubernetes 集群的步骤，因为部署到远程托管集群（进行一些小的修改）需要采取相同的步骤。在生产环境中无缝运行 Kubernetes
    集群需要考虑网络、集群资源配置和管理、安全策略等多个方面。详细研究所有这些主题需要一本完整的书。实际上，Aly Saleh 和 Murat Karsioglu
    的《Kubernetes in Production Best Practices》是一本很好的资源，可以帮助您了解许多这些细节。在本章中，我们将专注于理解您开始使用
    Kubernetes 开发 ML 微服务所需的最重要步骤。
- en: First, let’s get set up for Kubernetes development. I will use minikube here,
    as it has some handy utilities for setting up services that can be called via
    REST API calls. Previously in this book, I used kind (Kubernetes in Docker), and
    you can use this here; just be prepared to do some more work and use the documentation.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为 Kubernetes 开发做好准备。在这里，我将使用 minikube，因为它有一些方便的实用工具，可以设置可以通过 REST API
    调用服务。在这本书的先前部分，我使用了 kind（在 Docker 中运行的 Kubernetes），您也可以在这里使用它；只需准备好做一些额外的工作并使用文档。
- en: To get set up with minikube on your machine, follow the installation guide in
    the official docs for your platform at [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要在你的机器上设置 minikube，请遵循官方文档中针对您平台的安装指南，链接为 [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)。
- en: 'Once minikube is installed, you can spin up your first cluster using default
    configurations with:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 minikube，您可以使用默认配置启动您的第一个集群，命令如下：
- en: '[PRE36]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once the cluster is up and running, you can deploy the `fast-api` service to
    the cluster with the following command in the terminal:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群启动并运行，您可以在终端中使用以下命令将 `fast-api` 服务部署到集群：
- en: '[PRE37]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'where `direct-kube-deploy.yaml` is a manifest that contains the following code:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `direct-kube-deploy.yaml` 是一个包含以下代码的清单：
- en: '[PRE38]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This manifest defines a Kubernetes Deployment that creates and manages two replicas
    of a Pod template containing a container named `fast-api`, which runs the Docker
    image we created and published previously, `electricweegie/custom-forecast-service:latest`.
    It also defines resource limits for the containers running inside the Pods and
    ensures that the containers are listening on port `8000`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本清单定义了一个 Kubernetes Deployment，该 Deployment 创建并管理包含一个名为 `fast-api` 的容器的 Pod
    模板的两个副本。这个容器运行的是我们之前创建并发布的 Docker 镜像，即 `electricweegie/custom-forecast-service:latest`。它还定义了运行在
    Pod 内部容器上的资源限制，并确保容器监听端口 `8000`。
- en: 'Now that we have created a Deployment with the application in it, we need to
    expose this solution to incoming traffic, preferably through a load balancer so
    that incoming traffic can be efficiently routed to the different replicas of the
    application. To do this in minikube, you have to perform a few steps:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个包含应用程序的 Deployment，我们需要将此解决方案暴露给传入流量，最好是使用负载均衡器，以便高效地将传入流量路由到应用程序的不同副本。要在
    minikube 中完成此操作，你必须执行以下步骤：
- en: 'Network or host machine access is not provided by default to the Services running
    on the minikube cluster, so we have to create a route to expose the cluster IP
    address using the `tunnel` command:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，minikube 集群上运行的服务不提供网络或主机机访问，因此我们必须使用 `tunnel` 命令创建一个路由来公开集群 IP 地址：
- en: '[PRE39]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Open a new terminal window. This allows the tunnel to keep running, and then
    you need to create a Kubernetes Service with type `LoadBalancer` that will access
    the `deployment` we have already set up:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的终端窗口。这允许隧道持续运行，然后你需要创建一个类型为 `LoadBalancer` 的 Kubernetes 服务，该服务将访问我们已设置的
    `deployment`：
- en: '[PRE40]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can then get the external IP for accessing the Service by running:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来获取访问服务的公网 IP：
- en: '[PRE41]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This should give an output that looks something like:'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会给出类似以下输出的结果：
- en: '[PRE42]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You will then be able to use the `EXTERNAL-IP` of the load balancer service
    to hit the API, so you can navigate to Postman, or your other API development
    tool, and use `http://<EXTERNAL-IP>:8080` as the root URL for the FastAPI service
    that you have now successfully built and deployed to Kubernetes!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将能够使用负载均衡器服务的 `EXTERNAL-IP` 来访问 API，因此你可以导航到 Postman 或你的其他 API 开发工具，并使用
    `http://<EXTERNAL-IP>:8080` 作为你成功构建并部署到 Kubernetes 的 FastAPI 服务的根 URL！
- en: Deployment strategies
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署策略
- en: 'As discussed in *Chapter 5*, *Deployment Patterns and Tools*, there are several
    different strategies you can use to deploy and update your ML services. There
    are two components to this: one is the deployment strategy for the models and
    the other is the deployment strategy for the hosting application or pipelines
    that serve the models. These can both be executed in tandem as well.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 5 章*部署模式和工具*中所述，你可以使用几种不同的策略来部署和更新你的 ML 服务。这包括两个组件：一个是模型的部署策略，另一个是托管应用程序或为模型提供服务的管道的部署策略。这两个策略可以同时执行。
- en: Here we will discuss how to take the application we just deployed to Kubernetes
    and update it using canary and blue/green deployment strategies. Once you know
    how to do this for the base application, performing a similar update strategy
    for the models can be added in by specifying in the canary or blue/green deployment
    a model version that has an appropriate tag. As an example, we could use the “staging”
    stage of the model registry in MLflow to give us our “blue” model, and then upon
    transition to “green,” ensure that we have moved this model to the “production”
    stage in the model registry using the syntax outlined earlier in the chapter and
    in *Chapter 3*, *From Model to Model Factory*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论如何将我们刚刚部署到 Kubernetes 的应用程序更新，并使用金丝雀和蓝绿部署策略。一旦你学会了如何对基础应用程序进行此操作，可以通过在金丝雀或蓝绿部署中指定一个具有适当标签的模型版本，来添加对模型的类似更新策略。例如，我们可以使用
    MLflow 中模型注册表的“staging”阶段来提供我们的“蓝”模型，然后在过渡到“绿色”时，确保我们已经使用本章和第 3 章*从模型到模型工厂*中概述的语法，将此模型移动到模型注册表的“生产”阶段。
- en: As a canary deployment is a deployment of the new version of the application
    in a smaller subset of the production environment, we can create a new deployment
    manifest that enforces only one replica (this could be more on larger clusters)
    of the canary application is created and run. In this case, this only requires
    that you edit the previous manifest number of replicas to “1.”
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于金丝雀部署是在生产环境的一个较小子集中部署应用程序的新版本，我们可以创建一个新的部署清单，强制只创建和运行一个金丝雀应用的副本（在较大的集群中可能更多）。在这种情况下，这只需要你编辑之前的副本数量为“1。”
- en: 'To ensure that the canary deployment is accessible to the same load balancer,
    we have to utilize the concept of resource labels in Kubernetes. We can then deploy
    a load balancer that selects resources with the desired label. An example manifest
    for deploying such a load balancer is given below:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保金丝雀部署可以访问相同的负载均衡器，我们必须利用Kubernetes中的资源标签概念。然后我们可以部署一个选择具有所需标签的资源负载均衡器。以下是一个部署此类负载均衡器的示例清单：
- en: '[PRE43]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Or using the same minkube syntax as above:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 或者使用与上面相同的minkube语法：
- en: '[PRE44]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: After deploying this load balancer and the canary deployment, you can then implement
    monitoring of the logs on the cluster or on your model in order to determine if
    the canary is successful and should get more traffic. In that case, you would
    just update the deployment manifest to contain more replicas.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署此负载均衡器和金丝雀部署之后，你可以然后实现集群或模型上的日志监控，以确定金丝雀是否成功并且应该获得更多流量。在这种情况下，你只需更新部署清单以包含更多副本。
- en: 'Blue/green deployments will work in a very similar way; in each case, you just
    edit the Deployment manifest to label the application as blue or green, The core
    difference between blue/green and canary deployments though is that the switching
    of the traffic is a bit more abrupt, where here we can have the load balancer
    service switch production traffic to the green deployment with the following command,
    which uses the `kubectl` CLI to patch the definition of the selector in the service:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署将以非常相似的方式工作；在每种情况下，你只需编辑Deployment清单，将应用程序标记为蓝色或绿色。然而，蓝绿部署与金丝雀部署的核心区别在于流量的切换更为突然，在这里我们可以使用以下命令，该命令使用`kubectl`
    CLI来修补服务选择器的定义，将生产流量切换到绿色部署：
- en: '[PRE45]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And that is how you perform canary and blue/green deployments in Kubernetes,
    and how you can use it to try different versions of the forecasting service; give
    it a try!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你在Kubernetes中执行金丝雀和蓝/绿部署的方式，以及如何使用它来尝试不同的预测服务版本；试试看吧！
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we walked through an example of how to take the tools and techniques
    from the first seven chapters of this book and apply them together to solve a
    realistic business problem. We discussed in detail how the need for a dynamically
    triggered forecasting algorithm can lead very quickly to a design that requires
    several small services to interact seamlessly. In particular, we created a design
    with components responsible for handling events, training models, storing models,
    and performing predictions. We then walked through how we would choose our toolset
    to build to this design in a real-world scenario, by considering things such as
    appropriateness for the task at hand, as well as likely developer familiarity.
    Finally, we carefully defined the key pieces of code that would be required to
    build the solution to solve the problem repeatedly and robustly.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过一个示例展示了如何将本书前七章中的工具和技术结合起来，以解决一个实际业务问题。我们详细讨论了为什么对动态触发的预测算法的需求可以迅速导致需要多个小型服务无缝交互的设计。特别是，我们创建了一个包含处理事件、训练模型、存储模型和执行预测的组件的设计。然后，我们通过考虑诸如任务适用性以及可能的开发者熟悉度等因素，介绍了如何在现实场景中选择我们的工具集来构建这个设计。最后，我们仔细定义了构建解决方案所需的关键代码，以重复和稳健地解决问题。
- en: In the next, and final, chapter, we will build out an example of a batch ML
    process. We will name the pattern that this adheres to **Extract**, **Transform**,
    **Machine Learning**, and explore what key points should be covered in any project
    aiming to build this type of solution.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也就是最后一章，我们将构建一个批处理机器学习过程的示例。我们将命名这个模式为**提取**、**转换**、**机器学习**，并探讨任何旨在构建此类解决方案的项目应涵盖的关键点。
- en: Join our community on Discord
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mle](https://packt.link/mle)'
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code102810325355484.png)'
