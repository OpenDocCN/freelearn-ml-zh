- en: Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: In this chapter, we're going to introduce another approach to classification
    using a family of algorithms called support vector machines. They can work with
    both linear and non-linear scenarios, allowing high performance in many different
    contexts. Together with neural networks, SVMs probably represent the best choice
    for many tasks where it's not easy to find out a good separating hyperplane. For
    example, for a long time, SVMs were the best choice for MNIST dataset classification,
    thanks to the fact that they can capture very high non-linear dynamics using a
    mathematical trick, without complex modifications in the algorithm. In the first
    part, we're going to discuss the basics of linear SVM, which then will be used
    for their non-linear extensions. We'll also discuss some techniques to control
    the number of parameters and, at the end, the application of support vector algorithms
    to regression problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍另一种使用称为支持向量机算法族的方法来进行分类。它们可以处理线性和非线性场景，允许在许多不同环境中实现高性能。与神经网络一起，SVMs可能是许多难以找到良好分离超平面的任务的最佳选择。例如，由于它们可以使用数学技巧捕捉非常高的非线性动态，而不需要对算法进行复杂修改，因此SVMs长期以来一直是MNIST数据集分类的最佳选择。在第一部分，我们将讨论线性SVM的基本知识，然后将其用于它们的非线性扩展。我们还将讨论一些控制参数数量的技术，最后，将支持向量算法应用于回归问题。
- en: Linear support vector machines
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性支持向量机
- en: 'Let''s consider a dataset of feature vectors we want to classify:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个我们想要分类的特征向量数据集：
- en: '![](img/a8ebf2e9-c219-44a4-bbfd-190e1364aaef.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a8ebf2e9-c219-44a4-bbfd-190e1364aaef.png)'
- en: 'For simplicity, we assume it as a binary classification (in all the other cases,
    it''s possible to use automatically the one-versus-all strategy) and we set our
    class labels as -1 and 1:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们假设它是一个二元分类（在其他所有情况下，可以使用自动的one-versus-all策略）并将我们的类别标签设置为-1和1：
- en: '![](img/56b6c321-1019-4144-84fc-e5bd104e136a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/56b6c321-1019-4144-84fc-e5bd104e136a.png)'
- en: 'Our goal is to find the best separating hyperplane, for which the equation
    is:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到最佳分离超平面，其方程为：
- en: '![](img/d48b7499-e5f1-4593-8180-bf8e3793c2fe.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d48b7499-e5f1-4593-8180-bf8e3793c2fe.png)'
- en: 'In the following figure, there''s a bidimensional representation of such a
    hyperplane:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，有一个二维表示的此类超平面：
- en: '![](img/d2582ae4-446f-426d-9a37-3edd703ee10b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d2582ae4-446f-426d-9a37-3edd703ee10b.png)'
- en: 'In this way, our classifier can be written as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，我们的分类器可以写成：
- en: '![](img/01bbc949-57f8-4dd0-928e-de89da5ea280.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/01bbc949-57f8-4dd0-928e-de89da5ea280.png)'
- en: 'In a realistic scenario, the two classes are normally separated by a margin
    with two boundaries where a few elements lie. Those elements are called **support
    vectors**. For a more generic mathematical expression, it''s preferable to renormalize
    our dataset so that the support vectors will lie on two hyperplanes with equations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中，两个类别通常由一个包含两个边界的间隔分开，其中有一些元素位于边界上。这些元素被称为**支持向量**。为了更通用的数学表达，最好重新归一化我们的数据集，使得支持向量将位于两个具有以下方程的超平面上：
- en: '![](img/ed90dc11-fc80-474f-aace-973758eb6ecb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed90dc11-fc80-474f-aace-973758eb6ecb.png)'
- en: 'In the following figure, there''s an example with two support vectors. The
    dashed line is the original separating hyperplane:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，有一个包含两个支持向量的示例。虚线是原始的分离超平面：
- en: '![](img/93c4e356-cf65-4925-9c48-6591dcfaf927.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/93c4e356-cf65-4925-9c48-6591dcfaf927.png)'
- en: Our goal is to maximize the distance between these two boundary hyperplanes
    so as to reduce the probability of misclassification (which is higher when the
    distance is short, and there aren't two well-defined blobs as in the previous
    figure).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最大化这两个边界超平面之间的距离，以减少误分类的概率（当距离短时，误分类的概率更高，并且没有像前一个图中的两个明确分离的团块）。
- en: 'Considering that the boundaries are parallel, the distance between them is
    defined by the length of the segment perpendicular to both and connecting two
    points:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到边界是平行的，它们之间的距离由垂直于两者并连接两个点的线段的长度定义：
- en: '![](img/37e1322e-e6d5-4fbd-83d5-54ba7c0982d1.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/37e1322e-e6d5-4fbd-83d5-54ba7c0982d1.png)'
- en: 'Considering the points as vectors, therefore, we have:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将点视为向量，因此，我们有：
- en: '![](img/bde1f510-5fab-4167-8e97-cd3164a5b07f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bde1f510-5fab-4167-8e97-cd3164a5b07f.png)'
- en: 'Now, considering the boundary hyperplane equations, we get:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到边界超平面方程，我们得到：
- en: '![](img/d1a13501-f5c0-41cc-a4c6-ebe3cedbefcd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1a13501-f5c0-41cc-a4c6-ebe3cedbefcd.png)'
- en: 'The first term of the last part is equal to -1, so we solve for *t*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后部分的第一个项等于-1，因此我们解出*t*：
- en: '![](img/1d0a8ff9-e358-429c-a4a2-fb6e69eac926.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1d0a8ff9-e358-429c-a4a2-fb6e69eac926.png)'
- en: 'The distance between *x[1]* and *x[2]* is the length of the segment *t*; hence
    we get:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*x[1]* 和 *x[2]* 之间的距离是线段 *t* 的长度；因此我们得到：'
- en: '![](img/2e5930f6-b8c6-40a4-a67a-89d7ecf6d919.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2e5930f6-b8c6-40a4-a67a-89d7ecf6d919.png)'
- en: 'Now, considering all points of our dataset, we can impose the following constraint:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到我们数据集中的所有点，我们可以施加以下约束：
- en: '![](img/0637c2a1-8d11-4d8b-ac8d-0b2bff9bec9f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0637c2a1-8d11-4d8b-ac8d-0b2bff9bec9f.png)'
- en: This is guaranteed by using -1, 1 as class labels and boundary margins. The
    equality is true only for the support vectors, while for all the other points
    it will greater than 1\. It's important to consider that the model doesn't take
    into account vectors beyond this margin. In many cases, this can yield a very
    robust model, but in many datasets this can also be a strong limitation. In the
    next paragraph, we're going to use a trick to avoid this rigidness while keeping
    the same optimization technique.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过使用 -1 和 1 作为类别标签和边界边缘来保证。等式仅对支持向量成立，而对于所有其他点，它将大于 1。重要的是要考虑模型不考虑这个边缘之外的向量。在许多情况下，这可以产生一个非常鲁棒的模型，但在许多数据集中，这也可能是一个强烈的限制。在下一段中，我们将使用一个技巧来避免这种僵化，同时保持相同的优化技术。
- en: 'At this point, we can define the function to minimize in order to train a support
    vector machine:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以定义用于训练支持向量机的最小化函数：
- en: '![](img/f1ea27ad-76a9-476f-8710-3cea0273df5d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f1ea27ad-76a9-476f-8710-3cea0273df5d.png)'
- en: 'This can be further simplified (by removing the square root from the norm)
    in the following quadratic programming problem:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过从范数中移除平方根来进一步简化（以下二次规划问题）：
- en: '![](img/0a21ed94-6589-4cd0-996f-0ef0dd4b6e76.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0a21ed94-6589-4cd0-996f-0ef0dd4b6e76.png)'
- en: scikit-learn implementation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn 实现
- en: 'In order to allow the model to have a more flexible separating hyperplane,
    all scikit-learn implementations are based on a simple variant that includes so-called
    **slack variables** in the function to minimize:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许模型拥有更灵活的分离超平面，所有 scikit-learn 实现都基于一个简单变体，该变体在最小化函数中包括所谓的**松弛变量**：
- en: '![](img/7c0cd89f-8dc3-4a0e-8649-ef36e6c0df0a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c0cd89f-8dc3-4a0e-8649-ef36e6c0df0a.png)'
- en: 'In this case, the constraints become:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，约束变为：
- en: '![](img/b4198582-8d33-4738-9d96-8133c14f06e7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b4198582-8d33-4738-9d96-8133c14f06e7.png)'
- en: The introduction of the slack variables allows us to create a flexible margin
    so that some vectors belonging to a class can also be found in the opposite part
    of the hyperspace and can be included in the model training. The strength of this
    flexibility can be set using the parameter *C*. Small values (close to zero) bring
    about very hard margins, while values greater than or equal to 1 allow more and
    more flexibility (also increasing the misclassification rate). The right choice
    of *C* is not immediate, but the best value can be found automatically by using
    a grid search as seen in the previous chapters. In our examples, we keep the default
    value of 1.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 松弛变量的引入使我们能够创建一个灵活的边缘，使得属于一个类别的某些向量也可以出现在超空间的另一部分，并可以包含在模型训练中。这种灵活性的强度可以通过参数
    *C* 来设置。小值（接近零）产生非常硬的边缘，而大于或等于 1 的值允许更多的灵活性（同时也增加了误分类率）。*C* 的正确选择不是立即的，但最佳值可以通过使用网格搜索自动找到，如前几章所述。在我们的例子中，我们保持默认值
    1。
- en: Linear classification
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性分类
- en: 'Our first example is based on a linear SVM, as described in the previous section.
    We start by creating a dummy dataset with 500 vectors subdivided into two classes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个例子是基于线性 SVM，如前文所述。我们首先创建一个包含 500 个向量的虚拟数据集，这些向量被划分为两类：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the following figure, there's a plot of our dataset. Notice that some points
    overlap the two main blobs. For this reason, a positive *C* value is needed to
    allow the model to capture a more complex dynamic.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，是数据集的图表。注意，一些点重叠在两个主要团块上。因此，需要一个正的 *C* 值来允许模型捕捉更复杂的动态。
- en: '![](img/1707af43-728e-43d6-acd8-924b1cb35601.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1707af43-728e-43d6-acd8-924b1cb35601.png)'
- en: 'scikit-learn provides the `SVC` class, which is a very efficient implementation
    that can be used in most cases. We''re going to use it together with cross-validation
    to validate performance:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了 `SVC` 类，这是一个非常高效的实现，在大多数情况下都可以使用。我们将使用它与交叉验证一起验证性能：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `kernel` parameter must be set to `''linear''` in this example. In the
    next section, we''re going to discuss how it works and how it can improve the
    SVM''s performance dramatically in non-linear scenarios. As expected, the accuracy
    is comparable to a logistic regression, as this model tries to find an optimal
    linear separator. After training a model, it''s possible to get an array of support
    vectors, through the instance variable called `support_vectors_`. A plot of them,
    for our example, is shown in the following figure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`kernel`参数必须设置为`'linear'`。在下一节中，我们将讨论它是如何工作的以及它如何可以显著提高SVM在非线性场景中的性能。正如预期的那样，准确度与逻辑回归相当，因为这个模型试图找到一个最优的线性分离器。训练好模型后，可以通过名为`support_vectors_`的实例变量获取支持向量的数组。以下图显示了它们的图示：
- en: '![](img/31b9af15-46ea-4aa2-aab7-89bc88beea07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/31b9af15-46ea-4aa2-aab7-89bc88beea07.png)'
- en: As it's possible to see, they are placed in a strip along the separating line.
    The effect of `C` and the slack variables determined a movable margin that partially
    captured the existing overlap. Of course, it's impossible to separate the sets
    in a perfect way with a linear classifier and, on the other hand, most real-life
    problems are non-linear; for this reason, it's a necessary further step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，它们被放置在分离线上的一个条带中。`C`和松弛变量的影响决定了可移动的边界，部分地捕捉了现有的重叠。当然，使用线性分类器完美地分离集合是不可能的，另一方面，大多数现实生活中的问题都是非线性的；因此，这是一个必要的进一步步骤。
- en: Kernel-based classification
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于核的分类
- en: 'When working with non-linear problems, it''s useful to transform the original
    vectors by projecting them into a higher dimensional space where they can be linearly
    separated. We saw a similar approach when we discussed polynomial regression.
    SVMs also adopt the same approach, even if there''s now a complexity problem that
    we need to overcome. Our mathematical formulation becomes:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理非线性问题时，通过将原始向量投影到一个更高维的空间中，使其可以线性分离，这是一种有用的方法。我们在讨论多项式回归时看到了类似的方法。SVMs也采用相同的方法，尽管我们现在需要克服一个复杂性问题。我们的数学公式变为：
- en: '![](img/13acd1b1-0e4a-44b7-b546-76a7174f2e5d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/13acd1b1-0e4a-44b7-b546-76a7174f2e5d.png)'
- en: 'Every feature vector is now filtered by a non-linear function that can completely
    reshape the scenario. However, the introduction of such a function increased the
    computational complexity in a way that could apparently discourage this approach.
    To understand what has happened, it''s necessary to express the quadratic problem
    using Lagrange multipliers. The entire procedure is beyond the scope of this book
    (in Nocedal J., Wright S. J., *Numerical Optimization*, Springer, you can find
    a complete and formal description of quadratic programming problems); however,
    the final formulation is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征向量现在都通过一个非线性函数进行过滤，可以完全改变场景。然而，引入这样的函数增加了计算复杂度，这可能会明显地阻碍这种方法。为了理解发生了什么，有必要使用拉格朗日乘数表达二次问题。整个过程超出了本书的范围（在Nocedal
    J.，Wright S. J.的《数值优化》Springer中可以找到二次规划问题的完整和正式描述）；然而，最终的公式是：
- en: '![](img/bd4e49ed-3a01-497c-b755-2fdf06753109.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd4e49ed-3a01-497c-b755-2fdf06753109.png)'
- en: 'Therefore it''s necessary to compute the following for every couple of vectors:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每一对向量，有必要计算以下内容：
- en: '![](img/82139cea-c5ca-4297-a4eb-3b519e4733cc.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/82139cea-c5ca-4297-a4eb-3b519e4733cc.png)'
- en: 'And this procedure can be a bottleneck, unacceptable for large problems. However,
    it''s now that the so-called **kernel trick** takes place. There are particular
    functions (called kernels) that have the following property:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个程序可能成为瓶颈，对于大问题来说是不可以接受的。然而，现在所谓的**核技巧**就出现了。有一些特定的函数（称为核）具有以下特性：
- en: '![](img/ec32e87d-dc9b-44b5-bec1-d4bc04037db4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec32e87d-dc9b-44b5-bec1-d4bc04037db4.png)'
- en: In other words, the value of the kernel for two feature vectors is the product
    of the two projected vectors. With this trick, the computational complexity remains
    almost the same, but we can benefit from the power of non-linear projections even
    in a very large number of dimensions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，两个特征向量核的值是两个投影向量的乘积。通过这个技巧，计算复杂度几乎保持不变，但我们可以在非常高的维度中受益于非线性投影的力量。
- en: Excluding the linear kernel, which is a simple product, scikit-learn supports
    three different kernels that can solve many real-life problems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单的乘积的线性核之外，scikit-learn支持三种不同的核，可以解决许多现实生活中的问题。
- en: Radial Basis Function
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 径向基函数
- en: 'The RBF kernel is the default value for SVC and is based on the function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RBF核是SVC的默认值，基于以下函数：
- en: '![](img/2268fe32-9d0f-43c7-882c-9c3c51c37479.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2268fe32-9d0f-43c7-882c-9c3c51c37479.png)'
- en: The gamma parameter determines the amplitude of the function, which is not influenced
    by the direction but only by the distance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Gamma参数决定了函数的振幅，它不受方向的影响，只受距离的影响。
- en: Polynomial kernel
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式核
- en: 'The polynomial kernel is based on the function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式核基于以下函数：
- en: '![](img/54a30a9a-c292-4d46-9561-9f3d54313be7.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/54a30a9a-c292-4d46-9561-9f3d54313be7.png)'
- en: The exponent *c* is specified through the parameter degree, while the constant
    term *r* is called `coef0`. This function can easily expand the dimensionality
    with a large number of support variables and overcome very non-linear problems;
    however, the requirements in terms of resources are normally higher. Considering
    that a non-linear function can often be approximated quite well for a bounded
    area (by adopting polynomials), it's not surprising that many complex problems
    become easily solvable using this kernel.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 指数 *c* 通过参数 `degree` 指定，而常数项 *r* 被称为 `coef0`。这个函数可以通过大量的支持变量轻松地扩展维度，克服非常非线性的问题；然而，在资源方面的要求通常更高。考虑到一个非线性函数通常可以很好地逼近一个有界区域（通过采用多项式），因此许多复杂问题使用这个核函数变得容易解决并不令人惊讶。
- en: Sigmoid kernel
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid核
- en: 'The sigmoid kernel is based on this function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid核基于以下函数：
- en: '![](img/48cff76f-30b8-402d-b170-81fa7035e466.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/48cff76f-30b8-402d-b170-81fa7035e466.png)'
- en: The constant term *r* is specified through the parameter `coef0`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 常数项 *r* 通过参数 `coef0` 指定。
- en: Custom kernels
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义核函数
- en: 'Normally, built-in kernels can efficiently solve most real-life problems; however
    scikit-learn allows us to create custom kernels as normal Python functions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，内置核可以有效地解决大多数实际问题；然而，scikit-learn允许我们创建自定义核，就像普通的Python函数一样：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The function can be passed to SVC through the `kernel` parameter, which can
    assume fixed string values (`'linear'`, `'rbf'`, `'poly'` and `'sigmoid'`) or
    a callable (such as `kernel=custom_kernel`).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数可以通过SVC的 `kernel` 参数传递，该参数可以假设固定字符串值（`'linear'`, `'rbf'`, `'poly'` 和 `'sigmoid'`）或一个可调用对象（例如
    `kernel=custom_kernel`）。
- en: Non-linear examples
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性示例
- en: 'To show the power of kernel SVMs, we''re going to solve two problems. The first
    one is simpler but purely non-linear and the dataset is generated through the
    `make_circles()` built-in function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示核SVM的强大功能，我们将解决两个问题。第一个问题比较简单，但完全是非线性的，数据集是通过内置函数 `make_circles()` 生成的：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A plot of this dataset is shown in the following figure:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了该数据集的图表：
- en: '![](img/6cdb11e2-c2f8-4311-9b95-31f15d3014b1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6cdb11e2-c2f8-4311-9b95-31f15d3014b1.png)'
- en: 'As it''s possible to see, a linear classifier can never separate the two sets
    and every approximation will contain on average 50% misclassifications. A logistic
    regression example is shown here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，线性分类器永远无法分离这两个集合，每个近似值平均将包含50%的错误分类。这里展示了逻辑回归的一个例子：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As expected, the accuracy is below 50% and no other optimizations can increase
    it dramatically. Let''s consider, instead, a grid search with an SVM and different
    kernels (keeping the default values of each one):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，准确率低于50%，没有其他优化可以显著提高它。让我们考虑使用SVM和不同核函数的网格搜索（保持每个的默认值）：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected from the geometry of our dataset, the best kernel is a radial basis
    function, which yields 87% accuracy. Further refinements on `gamma` could slightly
    increase this value, but as there is a partial overlap between the two subsets,
    it's very difficult to achieve an accuracy close to 100%. However, our goal is
    not to overfit our model; it is to guarantee an appropriate level of generalization.
    So, considering the shape, a limited number of misclassifications is acceptable
    to ensure that the model captures sub-oscillations in the boundary surface.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如从数据集的几何形状所预期，最佳的核函数是径向基函数，准确率达到87%。进一步对 `gamma` 进行优化可以略微提高这个值，但由于两个子集之间有部分重叠，要达到接近100%的准确率非常困难。然而，我们的目标不是过度拟合我们的模型；而是保证一个适当的泛化水平。因此，考虑到形状，有限数量的错误分类是可以接受的，以确保模型能够捕捉到边界表面的次级振荡。
- en: 'Another interesting example is provided by the MNIST handwritten digit dataset.
    We have already seen it and classified it using linear models. Now we can try
    to find the best kernel with an SVM:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的例子是由MNIST手写数字数据集提供的。我们之前已经见过它，并使用线性模型对其进行分类。现在我们可以尝试使用SVM找到最佳的核函数：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Hence the best classifier (with almost 98% accuracy) is based on a polynomial
    kernel and a very low `C` value. This means that a non-linear transformation with
    very hard margins can easily capture the dynamics of all digits. Indeed, SVMs
    (with various internal alternatives) have always shown excellent performance with
    this dataset and their usage can easily be extended to similar problems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳分类器（几乎98%的准确性）基于多项式核和非常低的`C`值。这意味着一个非常硬边界的非线性变换可以轻松地捕捉所有数字的动态。实际上，SVM（具有各种内部替代方案）在这个数据集上始终表现出优异的性能，并且它们的用途可以轻松地扩展到类似的问题。
- en: 'Another interesting example is based on the Olivetti face dataset, which is
    not part of scikit-learn but can be automatically downloaded and set up using
    a built-in function called `fetch_olivetti_faces()`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的例子是基于Olivetti人脸数据集，这个数据集不属于scikit-learn，但可以使用内置函数`fetch_olivetti_faces()`自动下载和设置：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Through the `data_home` parameter, it is possible to specify in which local
    folder the dataset must be placed. A subset of samples is shown in the following
    figure:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`data_home`参数，可以指定数据集必须放置的本地文件夹。以下图中显示了样本的子集：
- en: '![](img/29c1cfe5-9c08-44c8-b5f2-19bac1d60f67.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29c1cfe5-9c08-44c8-b5f2-19bac1d60f67.png)'
- en: 'There are 40 different people and each of them is represented with 10 pictures
    of 64 x 64 pixels. The number of classes (40) is not high, but considering the
    similarity of many photos, a good classifier should be able to capture some specific
    anatomical details. Performing a grid search with non-linear kernels, we get:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有40个不同的人，每个人用10张64 x 64像素的图片来表示。类别的数量（40）不算高，但考虑到许多照片的相似性，一个好的分类器应该能够捕捉到一些特定的解剖细节。使用非线性核进行网格搜索，我们得到：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So the best estimator is polynomial-based with `degree=2`, and the corresponding
    accuracy is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳估计器是基于多项式的，`degree=2`，相应的准确率是：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This confirms the ability of SVM to capture non-linear dynamics even with simple
    kernels that can be computed in a very limited amount of time. It would be interesting
    for the reader to try different parameter combinations or preprocess the data
    and apply principal component analysis to reduce its dimensionality.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了SVM即使在可以非常快速计算出的简单核中也能捕捉非线性动态的能力。对于读者来说，尝试不同的参数组合或预处理数据并应用主成分分析以降低其维度性将是有趣的。
- en: Controlled support vector machines
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受控支持向量机
- en: 'With real datasets, SVM can extract a very large number of support vectors to
    increase accuracy, and that can slow down the whole process. To allow finding
    out a trade-off between precision and number of support vectors, scikit-learn
    provides an implementation called `NuSVC`, where the parameter `nu` (bounded between
    0—not included—and 1) can be used to control at the same time the number of support
    vectors (greater values will increase their number) and training errors (lower
    values reduce the fraction of errors). Let''s consider an example with a linear
    kernel and a simple dataset. In the following figure, there''s a scatter plot
    of our set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实数据集上，SVM可以提取大量的支持向量来提高准确性，这可能会减慢整个过程。为了找到精度和支持向量数量之间的权衡，scikit-learn提供了一个名为`NuSVC`的实现，其中参数`nu`（介于0—不包括—和1之间）可以用来同时控制支持向量的数量（较大的值会增加它们的数量）和训练错误（较小的值会减少错误的比例）。让我们考虑一个具有线性核和简单数据集的例子。以下图中是我们的数据集的散点图：
- en: '![](img/96bb39ac-c5b9-4100-bc07-46b56e06bff0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96bb39ac-c5b9-4100-bc07-46b56e06bff0.png)'
- en: 'Let''s start checking the number of support vectors for a standard SVM:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从检查标准SVM的支持向量数量开始：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So the model has found 242 support vectors. Let''s now try to optimize this
    number using cross-validation. The default value is 0.5, which is an acceptable
    trade-off:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型找到了242个支持向量。现在让我们尝试使用交叉验证来优化这个数量。默认值是0.5，这是一个可接受的权衡：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As expected, the behavior is similar to a standard SVC. Let''s now reduce the
    value of `nu`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，其行为类似于标准SVC。现在让我们降低`nu`的值：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this case, the number of support vectors is less than before and also the
    accuracy has been affected by this choice. Instead of trying different values,
    we can look for the best choice with a grid search:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，支持向量的数量比之前少，而且这个选择也影响了准确性。我们不必尝试不同的值，可以通过网格搜索来寻找最佳选择：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Therefore, in this case as well, the default value of 0.5 yielded the most accurate
    results. Normally, this approach works quite well, but when it's necessary to
    reduce the number of support vectors, it can be a good starting point for progressively
    reducing the value of `nu` until the result is acceptable.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，默认值 0.5 产生了最准确的结果。通常，这种方法工作得相当好，但当需要减少支持向量的数量时，它可以是一个逐步减少 `nu` 值直到结果可接受的好起点。
- en: Support vector regression
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量回归
- en: scikit-learn provides a support vector regressor based on a very simple variant
    of the algorithm already described (see the original documentation for further
    information). The real power of this approach resides in the usage of non-linear
    kernels (in particular, polynomials); however, the user is advised to evaluate
    the degree progressively because the complexity can grow rapidly, together with
    the training time.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一种基于已描述算法（请参阅原始文档以获取更多信息）的非常简单的变体支持向量回归器。这种方法真正的力量在于使用非线性核（特别是多项式核）；然而，用户应建议逐步评估度数，因为复杂性可以迅速增长，同时训练时间也会增加。
- en: 'For our example, I''ve created a dummy dataset based on a second-order noisy
    function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我创建了一个基于二次噪声函数的虚拟数据集：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The dataset in plotted in the following figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在以下图中展示：
- en: '![](img/27ae755b-e335-45e8-a74d-bc694fac9ef6.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27ae755b-e335-45e8-a74d-bc694fac9ef6.png)'
- en: 'In order to avoid a very long training process, the model is evaluated with
    `degree` set to `2`. The epsilon parameter allows us to specify a soft margin
    for predictions; if a predicted value is contained in the ball centered on the
    target value and the radius is equal to epsilon, no penalty is applied to the
    function to be minimized. The default value is 0.1:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免非常长的训练过程，模型使用`degree`设置为`2`进行评估。epsilon 参数允许我们指定预测的软边缘；如果预测值包含在以目标值为中心、半径等于
    epsilon 的球内，则不对要最小化的函数应用惩罚。默认值是 0.1：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Nocedal J., Wright S. J., *Numerical Optimization*, Springer
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Nocedal J.，Wright S. J.，*数值优化*，Springer
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how a support vector machine works in both linear
    and non-linear scenarios, starting from the basic mathematical formulation. The
    main concept is to find the hyperplane that maximizes the distance between the
    classes by using a limited number of samples (called support vectors) that are
    closest to the separation margin.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了支持向量机在线性和非线性场景下的工作原理，从基本的数学公式开始。主要概念是找到通过使用有限数量的样本（称为支持向量）来最大化类别之间的距离的超平面，这些样本是最接近分离边缘的。
- en: We saw how to transform a non-linear problem using kernel functions, which allow
    remapping of the original space to a another high-dimensional one where the problem
    becomes linearly separable. We also saw how to control the number of support vectors
    and how to use SVMs for regression problems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用核函数将非线性问题转换为线性问题，这些核函数允许将原始空间重新映射到另一个高维空间，在那里问题变得线性可分。我们还看到了如何控制支持向量的数量以及如何使用
    SVM 进行回归问题。
- en: In the next chapter, we're going to introduce another classification method
    called decision trees, which is the last one explained in this book.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种称为决策树的分类方法，这是本书中最后解释的方法。
