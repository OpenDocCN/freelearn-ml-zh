- en: 'Chapter 2: What Problems Do Feature Stores Solve?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：功能存储解决了哪些问题？
- en: In the last chapter, we discussed the different stages in the **machine learning**
    (**ML**) life cycle, the difficult and time-consuming stages of ML, and how far
    we are from an ideal world. In this chapter, we'll explore one area of ML, which
    is ML feature management. ML feature management is the process of creating features,
    storing them in persistent storage, and serving them at scale for model training
    and inference. It is one of the most important stages of ML, although it is often
    overlooked. In data science/engineering teams in the early stages of ML, the absence
    of feature management has been a major hindrance to getting their ML models to
    production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了机器学习（ML）生命周期中的不同阶段，ML的困难和耗时阶段，以及我们离理想世界还有多远。在本章中，我们将探讨ML的一个领域，即ML特征管理。ML特征管理是创建特征、将它们存储在持久存储中，并在模型训练和推理中大规模提供它们的过程。它是ML最重要的阶段之一，尽管它通常被忽视。在ML的早期阶段，数据科学/工程团队缺乏特征管理一直是将他们的ML模型投入生产的主要障碍。
- en: 'As a data scientist/ML engineer, you may have found innovative ways to store
    and retrieve features for your ML model. But mostly, the solutions we build are
    not reusable, and every solution has limitations. For example, some of us might
    be using S3 buckets to store features, whereas other data scientists in the team
    might be using transactional databases. One may be more comfortable using CSV
    files and the other might prefer using Avro or Parquet files. Due to personal
    preference and a lack of standardization, each model will probably have a different
    way of managing features. Good feature management, on the other hand, should do
    the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家/ML工程师，您可能已经找到了存储和检索ML模型特征的创新方法。但大多数情况下，我们构建的解决方案是不可重用的，每个解决方案都有局限性。例如，我们中的一些人可能正在使用S3存储桶来存储特征，而团队中的其他数据科学家可能正在使用事务数据库。一个人可能更习惯于使用CSV文件，而另一个人可能更喜欢使用Avro或Parquet文件。由于个人偏好和缺乏标准化，每个模型可能都有不同的管理特征的方式。另一方面，良好的特征管理应该做到以下几方面：
- en: Make features discoverable
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使特征可发现
- en: Lead to easy reproducibility of models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导致模型易于复现
- en: Accelerate model development and productionization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速模型开发和生产化
- en: Fuel reuse of features within and across teams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在团队内部和团队之间促进特征的重复使用
- en: Make feature monitoring easy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使特征监控变得简单
- en: The aim of this chapter is to explain how data scientists and engineers strive
    to achieve better feature management and yet fall short of expectations. We will
    review different approaches adopted by teams to bring features into production,
    common problems with these approaches, and how we can do better with a feature
    store. By the end of this chapter, you will understand how a feature store meets
    the objectives mentioned previously and provides standardization across teams.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是解释数据科学家和工程师如何努力实现更好的特征管理，但往往无法达到预期。我们将回顾团队采用的不同方法来将特征投入生产，这些方法中常见的常见问题，以及如何通过功能存储做得更好。到本章结束时，您将了解功能存储如何满足之前提到的目标，并在团队间提供标准化。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Importance of features in production
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征在生产中的重要性
- en: Ways to bring features to production
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征投入生产的方法
- en: Common problems with the approaches used for bringing features to production
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将特征投入生产的方法中常见的常见问题
- en: Feature store to the rescue
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能存储的救星
- en: Philosophy behind feature stores
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能存储背后的哲学
- en: Importance of features in production
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征在生产中的重要性
- en: Before discussing how to bring features to production, let's understand why
    features are needed in production. Let's go through an example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论如何将特征投入生产之前，让我们了解为什么在生产中需要特征。让我们通过一个例子来了解。
- en: We often use taxi and food delivery services. One of the good things about these
    services is that they tell us how long it will take for our taxi or food to arrive. Also,
    most of the time, it is approximately correct. How does it predict this accurately?
    It uses ML, of course. The ML model predicts how long it will take for the taxi
    or food to arrive. For a model like that to be successful, not only does it need
    a good feature engineering and ML algorithm, but also the most recent features.
    Though we don't know the exact feature set that the model uses, let's look at
    a couple of features that change dynamically and are very important.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常使用出租车和外卖服务。这些服务的一个好处是它们会告诉我们出租车或食物到达需要多长时间。大多数时候，这个预测是大约正确的。它是如何准确预测的呢？当然，它使用机器学习。机器学习模型预测出租车或食物到达所需的时间。为了使这样的模型成功，不仅需要良好的特征工程和机器学习算法，还需要最新的特征。尽管我们不知道模型使用的确切特征集，但让我们看看一些动态变化且非常重要的特征。
- en: With food delivery services, the major components that affect the delivery time
    are restaurants, drivers, traffic, and customers. The model probably uses a set
    of slow-changing features that are updated regularly, maybe daily or weekly, and
    a set of dynamic features that change every few minutes. The slow-changing features
    might include the average number of orders a restaurant receives at different
    times of the day from the app and in person, the average time it takes for an
    order to be ready, and so on. It might seem like these features are not slow-changing,
    but if you think about it, the average number of orders might differ based on
    restaurant location, seasonality, time of the day, day of the week, and more.
    Dynamic features include how long the last five orders took, the number of cancelations
    in the past 30 minutes, and the current number of orders for the restaurant. Similarly,
    driver features might include average order delivery time with respect to distance,
    how often the driver cancels orders, and whether the driver is picking up multiple
    orders. Apart from these features, there will be traffic features, which change
    much more dynamically.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用外卖服务时，影响配送时间的最主要因素包括餐厅、司机、交通和顾客。模型可能使用一组缓慢变化的特征，这些特征会定期更新，可能是每天或每周更新一次，以及一组每几分钟就会变化的动态特征。缓慢变化的特征可能包括餐厅在不同时间通过应用程序和亲自接收的平均订单数量、订单准备的平均时间等。这些特征可能看起来并不是缓慢变化的，但如果你仔细想想，平均订单数量可能会根据餐厅位置、季节性、一天中的时间、一周中的某一天等因素而有所不同。动态特征包括最后五笔订单所需的时间、过去30分钟内的取消订单数量，以及餐厅当前的订单数量。同样，司机特征可能包括与距离相关的平均订单配送时间、司机取消订单的频率，以及司机是否在取多个订单。除了这些特征之外，还会有交通特征，这些特征的变化更为动态。
- en: With many dynamic features in play, even if one of them is an hour old, the
    model's predictions will go off the charts. For example, if there is a crash on
    the delivery route and traffic features don't capture it and use it for inference,
    the model will predict that food will arrive more quickly than it actually will.
    Similarly, if the model cannot get the current number of orders at the restaurant,
    it will use the old value and predict a value that may be far from the truth.
    Hence the more up-to-date features a model gets, the better the predictions will
    be. Also, another thing to keep in mind is that the app will not give the features;
    the app can only give information such as the restaurant ID and the customer ID.
    The model will have to fetch the features and facts from a different location,
    ideally a feature store. Wherever the features are being fetched from, the infrastructure
    serving it must scale in and scale out based on traffic to efficiently use resources
    and also to accommodate requests at low latency with a very low percentage of
    errors, if any.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在许多动态特征，即使其中之一已经有一小时的历史，模型的预测也可能超出图表范围。例如，如果配送路线上发生车祸，交通特征没有捕捉到并用于推理，模型将预测食物会比实际到达得更快。同样，如果模型无法获取餐厅当前的订单数量，它将使用旧值并预测一个可能远离真相的值。因此，模型获取的越新特征，预测将越好。另外，还有一个需要注意的事情是，应用程序不会提供特征；应用程序只能提供诸如餐厅ID和顾客ID等信息。模型将不得不从不同的位置获取特征和事实，理想情况下是从特征存储库中获取。无论特征是从哪里获取的，为其服务的基础设施必须根据流量进行扩展和缩减，以有效地使用资源，并能够以非常低的延迟率处理请求，错误率极低，如果有的话。
- en: Just like the food delivery service, the model we built in the first chapter
    needs the features during inference and the more up to date the features are,
    the better the customer's **lifetime value** (**LTV**) prediction will be. Good
    predictions will lead to better actions, resulting in excellent customer experience,
    hence better customer affinity, and better business.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就像外卖服务一样，我们在第一章中构建的模型需要推理过程中的特征，并且特征越新，客户的**终身价值**（**LTV**）预测就会越好。良好的预测将导致更好的行动，从而带来卓越的客户体验，进而提高客户亲和力和更好的业务。
- en: Ways to bring features to production
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将特征引入生产的方法
- en: 'Now that we understand the need for features in production, let''s look at
    some traditional ways of bringing features to production. Let''s consider two
    types of pipelines: batch model pipelines and online/transactional model pipelines:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们了解了在生产中需要特征，那么让我们看看一些传统的将特征引入生产的方法。让我们考虑两种类型的管道：批量模型管道和在线/事务模型管道：
- en: '**Batch models**: These are models that are run on a schedule, such as hourly,
    daily, weekly, and so on. Two of the common batch models are forecasting and customer
    segmentation. Batch inference is easier and less complex than its counterpart
    since it doesn''t have any latency requirements; inference can run for minutes
    or hours. Batch models can use distributed computational frameworks such as Spark.
    Also, they can be run with simple infrastructure. Most ML models start as batch
    models and, over time, depending on the available infrastructure and requirements,
    they go on to become online/transactional models.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量模型**：这些是在预定时间运行的模型，例如每小时、每天、每周等。两种常见的批量模型是预测和客户细分。批量推理比其对应物更容易和更简单，因为它没有延迟要求；推理可以运行几分钟或几小时。批量模型可以使用如Spark这样的分布式计算框架。此外，它们可以用简单的基础设施运行。大多数机器学习模型最初是批量模型，随着时间的推移，根据可用的基础设施和需求，它们最终成为在线/事务模型。'
- en: Though batch models' infrastructure is simple to build and manage, these models
    have drawbacks, such as the predictions not always being up to date. Since there
    is a time lag on predictions, it might cost business. For example, let's say a
    manufacturing plant uses order forecasting models to acquire raw materials. Depending
    on the time lag of the batch forecast models, the business might need to bear
    the cost of the shortage in raw materials versus overstocking raw materials in
    the warehouse.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然批量模型的架构简单易建和管理，但这些模型存在一些缺点，例如预测并不总是最新的。由于预测存在时间滞后，可能会给业务带来成本。例如，假设一家制造厂使用订单预测模型来获取原材料。根据批量预测模型的时间滞后，企业可能需要承担原材料短缺的成本，或者在仓库中过度储备原材料。
- en: '**Online/transactional models**: Online models follow the pull paradigm; the
    prediction will be generated on demand. Online models take advantage of the current
    reality and use that for predictions. Online models are transactional, need low-latency
    serving, and should scale based on incoming traffic. A typical online model is
    a recommendation model, which could be product recommendation, design recommendation,
    and so on.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线/事务模型**：在线模型遵循拉取范式；预测将在需求时生成。在线模型利用当前现实情况并用于预测。在线模型是事务性的，需要低延迟的服务，并且应根据流量进行扩展。一个典型的在线模型是推荐模型，这可能包括产品推荐、设计推荐等等。'
- en: Though real-time prediction sounds fancy, online models face a different set
    of challenges. It is easier to build applications whose latency is 8 hours than
    it is to build an application with a latency of 100 milliseconds. The latency
    of online models is usually in milliseconds. That means the model has a few milliseconds
    to figure out what the most up-to-date value is (which means generating or getting
    the latest features for the model) and predict the outcomes. For this to happen,
    the model needs a supporting infrastructure to serve the data required for prediction.
    Online models are usually hosted as REST API endpoints, which again need scaling,
    monitoring, and more.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实时预测听起来很吸引人，但在线模型面临不同的挑战。构建一个延迟为8小时的程序比构建一个延迟为100毫秒的程序要容易。在线模型的延迟通常在毫秒级别。这意味着模型有几分钟的时间来确定最新的值（这意味着为模型生成或获取最新的特征）并预测结果。为了实现这一点，模型需要一个支持基础设施来提供预测所需的数据。在线模型通常作为REST
    API端点托管，这也需要扩展、监控等。
- en: Now that we understand the difference between batch and online models, let's
    look at how batch model pipelines work.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们了解了批量模型和在线模型之间的区别，让我们看看批量模型管道是如何工作的。
- en: Batch model pipeline
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量模型管道
- en: As discussed, batch model pipeline latency requirements can range from minutes
    to hours. Batch models usually run on a schedule, hence they will be orchestrated
    using tools such as Airflow or AWS Step Functions. Let's look at a typical batch
    model pipeline and how features are brought to production.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，批处理模型管道的延迟要求可以从几分钟到几小时不等。批处理模型通常按计划运行，因此它们将使用Airflow或AWS Step Functions等工具进行编排。让我们看看一个典型的批处理模型管道以及如何将特征带入生产。
- en: '*Figure 2.1* depicts typical batch model pipelines:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.1* 描述了典型的批处理模型管道：'
- en: '![Figure 2.1 – Batch model pipelines'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 – 批处理模型管道'
- en: '](img/B18024_02_01.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_02_01.jpg)'
- en: Figure 2.1 – Batch model pipelines
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 批处理模型管道
- en: As discussed in [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014)*, An Overview
    of the Machine Learning Life Cycle*, once the model development is complete and
    it's ready for productionization, the notebook will be refactored to remove unwanted
    code. Some data engineers also break down a single notebook into multiple logical
    steps, such as feature engineering, model training, and model prediction. The
    refactored notebooks or refactored Python scripts generated from the notebook
    are scheduled using an orchestration framework such as Airflow. In a typical pipeline,
    the first stage will read the raw data from different data sources, perform data
    cleaning, and carry out feature engineering, which will be used by subsequent
    stages in the pipeline. Once the model prediction stage is complete, the prediction
    output will be written to a persistent store, maybe a database or an S3 bucket.
    The results will be accessed from the persistent storage as and when needed. If
    a stage in the pipeline fails for any reason (such as a data accessibility issue
    or errors in the code), the pipeline will be set up to trigger an alarm and stop
    further execution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第1章*](B18024_01_ePub.xhtml#_idTextAnchor014)《机器学习生命周期概述》中所述，一旦模型开发完成并准备投入生产，笔记本将被重构以删除不需要的代码。一些数据工程师还将单个笔记本分解成多个逻辑步骤，例如特征工程、模型训练和模型预测。重构的笔记本或从笔记本生成的重构Python脚本使用Airflow等编排框架进行调度。在一个典型的管道中，第一阶段将从不同的数据源读取原始数据，执行数据清洗，并执行特征工程，这些特征将被管道的后续阶段使用。一旦模型预测阶段完成，预测输出将被写入持久存储，可能是数据库或S3存储桶。结果将在需要时从持久存储中访问。如果管道中的某个阶段由于任何原因（例如数据可访问性问题或代码错误）失败，管道将被设置为触发警报并停止进一步执行。
- en: If you haven't already noticed, in the batch model pipeline, the features are
    generated when the pipeline runs. In some cases, it also retrains a new model
    with the latest data, and in others, it uses a previously trained model and predicts
    using the features generated from the data available at the time when the pipeline
    is run. As you can see in *Figure 2.1*, every new model that is built starts at
    the raw data source, repeats the same steps, and joins the production pipeline
    list. We will discuss the problems in this approach in the later section. Let's
    look at the different ways to bring features to production in online models next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有注意到，在批处理模型管道中，特征是在管道运行时生成的。在某些情况下，它还会使用最新数据重新训练一个新模型，而在其他情况下，它使用之前训练的模型，并使用在管道运行时可用数据生成的特征进行预测。如图*图2.1*所示，每个新构建的模型都是从原始数据源开始，重复相同的步骤，并加入生产管道列表。我们将在后面的部分讨论这种方法中存在的问题。接下来，让我们看看在线模型中如何将特征带入生产的不同方法。
- en: Online model pipeline
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线模型管道
- en: Online models have the special requirement of serving features in near real
    time, as these models are customer-facing or need to make business decisions in
    real time. There are different ways to bring features to production in an online
    model. Let's discuss them one by one in this section. One thing to keep in mind
    is that these approaches are not exactly how everybody does it; they are merely
    a representation of group approaches. Different teams use different versions of
    these approaches.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在线模型有在近实时服务特征的特殊要求，因为这些模型是面向客户的或需要在实时做出业务决策。在线模型将特征带入生产的方法有很多。让我们在本节中逐一讨论它们。需要注意的是，这些方法并不完全符合每个人的做法；它们只是群体方法的表示。不同的团队使用这些方法的不同的版本。
- en: Packaging features along with models
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将特征与模型打包
- en: To deploy an online model, it will have to be packaged first. Again, there are
    different standards that teams follow depending on the tools they use. Some might
    use packing libraries such as MLflow, joblib, or ONNX. Others might package the
    model directly as the REST API Docker image. As data scientists and data engineers
    have a different set of skills, as mentioned in *Figure 1.1* of [*Chapter 1*](B18024_01_ePub.xhtml#_idTextAnchor014)*,
    An Overview of the Machine Learning Life Cycle*, the ideal approach is to provide
    data scientists with tools to package models using libraries such as MLflow, joblib,
    and ONNX, and save the model to a model registry. The data engineers can then
    use the registered model to build REST APIs and deploy it. There is also out-of-the-box
    support to deploy MLflow-packaged models as AWS SageMaker endpoints with a simple
    **command-line interface** (**CLI**) command. It also supports building REST API
    Docker images with a CLI command, which then can be deployed in any container
    environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署在线模型，首先必须将其打包。同样，团队根据他们使用的工具遵循不同的标准。有些人可能会使用打包库，如MLflow、joblib或ONNX。其他人可能会直接将模型打包为REST
    API Docker镜像。如*第1章*中的*图 1.1*所述，数据科学家和数据工程师具有不同的技能集，理想的方法是向数据科学家提供使用MLflow、joblib和ONNX等库打包模型的工具，并将模型保存到模型注册表中。然后，数据工程师可以使用注册的模型构建REST
    API并部署它。还有现成的支持，可以使用简单的**命令行界面**（**CLI**）命令将MLflow打包的模型部署为AWS SageMaker端点。它还支持使用CLI命令构建REST
    API Docker镜像，然后可以在任何容器环境中部署。
- en: 'While libraries such as MLflow and joblib provide a way to package Python objects,
    they also support adding additional dependencies if required. For example, MLflow
    provides a set of built-in flavors to support the packaging of models using ML
    libraries such as scikit-learn, PyTorch, Keras, and TensorFlow. It adds all the
    required dependencies for the ML library. Packaging models with built-in flavors
    is as simple as using the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然如MLflow和joblib之类的库提供了一种打包Python对象的方法，但它们也支持在需要时添加额外的依赖项。例如，MLflow提供了一套内置风味，以支持使用scikit-learn、PyTorch、Keras和TensorFlow等ML库打包模型。它为ML库添加了所有必需的依赖项。使用内置风味打包模型与以下代码一样简单：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Along with the required dependencies, you can package the `features.csv` file
    and load it in the `predict` method of the model. Though this might sound like
    an easy deployment option, the result of this is not far away from a batch model.
    Since features are packaged along with the model, they are static. Any change
    in the raw dataset will not affect the model unless a new version of the model
    is built with a new set of features generated from the latest data and packaged
    along with the model. However, this might be a good first step from batch to online
    models. The reason I say this is that instead of running it as a batch model,
    you have now made it a pull-based inference. Also, you have defined a REST endpoint
    input and output format for the consumer of the model. The only pending step is
    to get the latest features to the model instead of static features, which are
    packaged. Once that is implemented, the consumers of the model won't have to make
    any changes and consumers will be served with predictions using the latest available
    data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所需的依赖项外，您还可以打包`features.csv`文件，并在模型的`predict`方法中加载它。尽管这可能听起来像是一个简单的部署选项，但这种方法的结果并不远逊于批处理模型。由于特征与模型一起打包，因此它们是静态的。原始数据集的任何变化都不会影响模型，除非使用从最新数据生成的新特征集构建的新版本模型并将其打包。然而，这可能是从批处理模型到在线模型的一个很好的第一步。我之所以这么说，是因为您现在将其作为基于拉的推理来运行，而不是作为批处理模型。此外，您已经为模型的消费者定义了REST端点输入和输出格式。唯一待定的步骤是将最新特征传递给模型，而不是打包的静态特征。一旦实现这一点，模型的消费者就不需要做出任何更改，并且将使用最新可用数据进行预测。
- en: Push-based inference
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于推的推理
- en: 'Unlike pull-based inference, where the model is scored when needed, in the
    push-based inference pattern, predictions are run proactively and kept ready in
    a transactional database or key-value store so that they can be served at low
    latency when the request comes. Let''s look at a typical architecture of online
    models using push-based inference:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与需要时才评分的拉式推理不同，在基于推的推理模式中，预测是主动运行的，并保存在事务数据库或键值存储中，以便在请求到来时以低延迟提供服务。让我们看看使用基于推的推理的在线模型的典型架构：
- en: '![Figure 2.2 – Push-based inference'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2 – 基于推的推理'
- en: '](img/B18024_02_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_02_02.jpg)'
- en: Figure 2.2 – Push-based inference
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – 基于推的推理
- en: '*Figure 2.2* shows the architecture of push-based inference. The idea here
    is similar to batch model inference, but the difference is that the pipeline also
    considers the real-time dataset, which is changing dynamically. The operation
    of a push-based model works as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.2* 展示了基于推的推理架构。这里的想法与批量模型推理类似，但不同之处在于该管道还考虑了实时数据集，这些数据集是动态变化的。基于推的模型的工作方式如下：'
- en: The real-time data (in this example, user interactions with the website) will
    be captured and pushed to a queue, such as Kafka, Kinesis, or Event Hubs.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时数据（例如，用户与网站的交互）将被捕获并推送到队列，如Kafka、Kinesis或Event Hubs。
- en: The feature engineering pipelines subscribe to a specific set of topics or a
    specific set of queues depending on what data is needed to generate features of
    the model. This also depends on the tools and the architecture. There may be a
    single queue or multiple queues depending on how huge/diverse the application
    is.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程管道根据需要生成模型特征的数据订阅特定的主题集或特定的队列集。这也取决于工具和架构。根据应用的大小/多样性，可能只有一个队列或多个队列。
- en: Whenever an event of interest appears in the queue, the feature engineering
    pipeline will pick this event and regenerate the features of the model using other
    datasets.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当队列中出现感兴趣的事件时，特征工程管道将选择此事件并使用其他数据集重新生成模型的特征。
- en: Note
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Not all features are dynamic. Some features may not change very much or very
    often. For example, a customer's geographic location might not change often.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并非所有特征都是动态的。有些特征可能变化不大或不太频繁。例如，客户的地理位置可能不会经常改变。
- en: The newly generated features are used to run the prediction of the data point.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新生成的特征用于运行数据点的预测。
- en: The results are stored in a transactional database or a key-value store.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果存储在事务数据库或键值存储中。
- en: Whenever required, the website or application will query the database to get
    new predictions for the specific ID (such as `CustomerId` when serving recommendations
    for a customer on a website).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要时，网站或应用将查询数据库以获取特定ID（例如，当在网站上为顾客提供推荐时使用`CustomerId`）的新预测。
- en: This process repeats every time a new event of interest appears on the queue.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每当队列中出现新的感兴趣事件时，此过程会重复。
- en: A new pipeline will be added for every new ML model.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个新的机器学习模型都将添加一个新的管道。
- en: 'This approach might seem easy and straightforward, as the only additional requirement
    here is real-time streaming data. However, this has limitations; the whole pipeline
    will have to run within milliseconds so that the recommendations are available
    before the application makes the next query for prediction. This is achievable
    but might involve a higher cost of operationalization because this is not just
    one pipeline: every pipeline for real-time models will have to have a similar
    latency requirement. Also, this will not be a copy-and-paste infrastructure because
    each model will have a different set of requirements when it comes to incoming
    traffic. For example, a model working on order features might require fewer processing
    instances, whereas a model working with clickstream data will require more data
    processing instances. Another thing to keep in mind is that although it might
    look like they are writing data to the same database, most of the time, it involves
    different databases and different technologies.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能看起来简单直接，因为这里唯一的额外要求是实时流数据。然而，这也有局限性；整个管道必须在毫秒内运行，以便在应用进行下一次预测查询之前提供推荐。这是可行的，但可能涉及更高的运营成本，因为这不仅仅是一个管道：每个实时模型的管道都必须有类似的延迟要求。此外，这不会是一个复制粘贴的基础设施，因为每个模型在处理入站流量时都会有不同的要求。例如，处理订单特征的模型可能需要较少的处理实例，而处理点击流数据的模型可能需要更多的数据处理实例。还需要注意的是，尽管它们可能看起来像是在写入同一个数据库，但大多数情况下，涉及的是不同的数据库和不同的技术。
- en: Let's look at a better solution next.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下一个更好的解决方案。
- en: Pull-based inference
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于拉的推理
- en: 'In contrast to push-based inference, in pull-based inference, predictions are
    run at the time of the request. Instead of storing the predictions, feature sets
    of a specific model are stored in a transactional database or key-value store.
    During prediction, the feature set can be accessed with low latency. Let''s look
    at the typical architecture of a pull-based inference model and the components
    involved:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于推的推理相反，在基于拉的推理中，预测是在请求时运行的。不是存储预测，而是将特定模型的特征集存储在事务数据库或键值存储中。在预测期间，特征集可以以低延迟访问。让我们看看基于拉推理模型的典型架构和涉及的组件：
- en: '![Figure 2.3 – Using transactional/key-value store for features'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – 使用事务/键值存储进行特征'
- en: '](img/B18024_02_03.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18024_02_03.jpg)'
- en: Figure 2.3 – Using transactional/key-value store for features
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 使用事务/键值存储进行特征
- en: '*Figure 2.3* shows another way of bringing features to production: the pull-based
    mechanism. Half of the pipeline works in a similar way to the push-based inference
    that we just discussed. The difference here is that after feature engineering,
    the features are written to a transactional database or key-value store. These
    features will be kept up to date by the pipelines. Once the features are available,
    the model works as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.3* 展示了将特征引入生产的一种另一种方式：基于拉的机制。管道的一半工作方式与我们刚才讨论的基于推的推理相似。这里的区别在于，在特征工程之后，特征被写入事务数据库或键值存储。这些特征将由管道保持更新。一旦特征可用，模型的工作方式如下：'
- en: 'The model''s `predict` API would have a contract similar to the one mentioned
    here:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的 `predict` API 将具有类似于以下提到的合同：
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When the application needs to query the model, it will hit the REST endpoint
    with `entity_id`.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当应用程序需要查询模型时，它将使用 `entity_id` 打击REST端点。
- en: The model will use `entity_id` to query the key-value store to get the features
    required to score the model.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将使用 `entity_id` 查询键值存储以获取评分模型所需的特征。
- en: The features are used to score the model and return the predictions.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些特征用于评分模型并返回预测结果。
- en: This approach is ideal if you don't have the feature store infrastructure. Also,
    we will discuss this extensively in the next chapter. Again, there are a few concerns
    involved with this approach, which are the repetition of the work, deploying and
    scaling the feature engineering pipeline, and managing multiple key-value store
    infrastructures, among others.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有特征存储基础设施，这种方法是理想的。我们将在下一章中详细讨论这一点。再次强调，这种方法涉及一些问题，包括工作重复、部署和扩展特征工程管道以及管理多个键值存储基础设施等。
- en: Calculating features on demand
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按需计算特征
- en: 'Let''s discuss one last approach before we move on and discuss the problems
    with these approaches. In the approaches discussed so far, the data pipelines
    calculate the features proactively as and when the data arrives, or when the pipeline
    runs. However, it is possible to calculate the features on demand when there is
    a request for the inference. This means when the application queries the model
    for a prediction, the model will request another system for the features. The
    system uses raw data from different sources and calculates the features on demand.
    This may be the most difficult architecture to implement, but I heard on the *TWIML
    AI podcast with Sam Charrington*, *episode 326: Metaflow, A Human-Centric Framework
    for Data Science with Ville Tuulos*, that Netflix has a system that can generate
    features on demand with a latency of seconds.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论这些方法的缺点之前，让我们讨论一种最后的方法。在迄今为止讨论的方法中，数据管道在数据到达或管道运行时主动计算特征。然而，当有推理请求时，可以按需计算特征。这意味着当应用程序查询模型进行预测时，模型将请求另一个系统获取特征。该系统使用来自不同来源的原始数据，并按需计算特征。这可能是最难实现的架构，但我听说在
    *TWIML AI播客与Sam Charrington* 中，*第326集：Metaflow，Ville Tuulos的人中心数据科学框架*，Netflix有一个可以在秒级延迟下按需生成特征的系统。
- en: 'The `predict` API might look similar to the one in the last approach:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict` API 可能看起来与最后一种方法中的类似：'
- en: 'def predict(entity_id: str) -> dict'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'def predict(entity_id: str) -> dict'
- en: It then invokes the system to get features for the given entity, runs predictions
    using the features, and returns the results. As you can imagine, all of this will
    have to happen within a few seconds. The on-demand feature engineering to be executed
    in real time might require a huge infrastructure with multiple caches between
    different storage systems. Keeping these systems in sync is not an easy architecture
    design. It's just a dream infrastructure for most of us. I haven't seen one so
    far. Hopefully, we will get there soon.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后它调用系统为给定的实体获取特征，使用这些特征进行预测，并返回结果。正如你可以想象的那样，所有这些都必须在几秒钟内完成。在实时执行的需求特征工程可能需要一个巨大的基础设施，其中包含不同存储系统之间的多个缓存。保持这些系统同步并不是一个容易的架构设计。对我们大多数人来说，这只是一个梦想中的基础设施。我至今还没有看到过。希望我们很快就能实现。
- en: In this section, we discussed multiple ways to bring features into production
    for inference. There may be many other ways of achieving this, but most solutions
    are variations that revolve around one of these approaches. Now that we understand
    why and how we bring features to production, let's look at the common issues that
    these approaches have and what can be done to overcome them.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了将特征引入生产进行推理的多种方法。可能还有许多其他实现这一目标的方法，但大多数解决方案都是围绕这些方法之一的变化。现在我们了解了为什么以及如何将特征引入生产，让我们来看看这些方法常见的常见问题以及如何克服它们。
- en: Common problems with the approaches used for bringing features to production
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于将特征引入生产的常用方法的问题
- en: The approaches discussed in the previous section seem like good solutions. However,
    not only does every approach have its own technical difficulties, such as infrastructure
    sizing, keeping up with a service-level agreement (SLA), and interaction with
    different systems, but they have a few common problems as well. This is expected
    in a growing technical domain until it reaches a level of saturation. I want to
    dedicate this section to the common problems that exist in these approaches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节讨论的方法看起来像是好的解决方案。然而，每个方法不仅有其自身的技术难题，如基础设施规模、遵守服务级别协议（SLA）以及与不同系统的交互，而且它们还有一些共同的问题。在技术领域不断增长直到达到饱和水平之前，这是预料之中的。我想将本节专门用于讨论这些方法中存在的常见问题。
- en: Re-inventing the wheel
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新发明轮子
- en: One of the common problems in engineering is building something that already
    exists. The reasons for that could be many; for example, a person developing a
    solution may not know that it already exists, or the existing solution is inefficient,
    or there is a need for additional functionality. We have the same problem here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 工程中常见的一个问题就是构建已经存在的东西。造成这种情况的原因可能有很多；例如，一个正在开发解决方案的人可能不知道它已经存在，或者现有的解决方案效率低下，或者有额外的功能需求。在这里我们也遇到了同样的问题。
- en: In many organizations, data scientists work in a specific domain and with a
    team supporting them, which usually includes ML engineers, data engineers, and
    data analysts. Their goal is to get their model to production. Though the other
    teams working in parallel also have the goal of getting their model to production,
    they hardly ever collaborate with each other due to their schedules and delivery
    timelines. As discussed in the first chapter, every persona in the team has different
    skill sets, different levels of experience with the available tools, and different
    preferences. Also, data engineers on two different teams rarely have the same
    preference. This leads to each team finding a solution to productionizing their
    model, which involves building feature engineering pipelines, feature management,
    model management, and monitoring.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多组织中，数据科学家在一个特定的领域内工作，并得到一个支持他们的团队的帮助，这个团队通常包括机器学习工程师、数据工程师和数据分析师。他们的目标是让他们的模型投入生产。尽管并行工作的其他团队也有让他们的模型投入生产的目标，但由于他们的日程安排和交付时间表，他们很少相互协作。正如第一章所讨论的，团队中的每个角色都有不同的技能组合，对现有工具的经验水平不同，偏好也不同。此外，不同团队的数据工程师很少有相同的偏好。这导致每个团队都寻找一种将他们的模型投入生产的解决方案，这涉及到构建特征工程管道、特征管理、模型管理和监控。
- en: After coming up with a successful solution, even if the team (let's call it
    team-A) shares their knowledge and success with other teams, the response you
    would get would be *good to know*, *interesting*, *could be useful for us*. But
    it will never materialize into the other teams' solutions. The reason for that
    is not that other teams are indifferent to what team-A achieved. Apart from the
    knowledge, nothing that was built by team-A in many cases is reusable. The options
    that the other teams are left with are to copy the code and adapt it to their
    needs and hope it works or implement a similar-looking pipeline. Hence, most teams
    end up building their own solution for the model. The interesting thing is that
    even team-A will re-build the same pipeline for the next model they work on in
    most cases.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出一个成功的解决方案后，即使团队（我们称之为团队A）与其他团队分享他们的知识和成功，你得到的回应也只会是“知道了”，“很有趣”，“可能对我们有用”。但这永远不会转化为其他团队的解决方案。原因并不是其他团队对团队A取得的成果漠不关心。除了知识之外，团队A在很多情况下所构建的很多东西是不可重用的。其他团队剩下的选择是复制代码并适应他们的需求，希望它能工作，或者实施一个看起来相似的流程。因此，大多数团队最终都会为模型构建自己的解决方案。有趣的是，即使在大多数情况下，团队A也会为下一个他们工作的模型重新构建相同的流程。
- en: Feature re-calculation
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重新计算
- en: 'Let''s start with a question. Ask yourself this: *how much memory does your
    phone have?* Most probably you know the answer by heart. If you are not sure,
    you might check the memory in the settings and answer. Either way, if I or somebody
    else asks you the same question in an hour, I''m pretty sure you will not go back
    into the phone''s settings and check again before answering, unless you have changed
    your phone. So why are we doing this in all our ML pipelines for features?'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个问题开始。问问自己：*你的手机有多少内存？* 很可能你心里已经有了答案。如果你不确定，你可能会检查设置中的内存并回答。无论如何，如果我在一个小时后或其他人问你同样的问题，我非常确信你不会在回答之前再次进入手机的设置进行检查，除非你更换了手机。那么为什么我们在所有的机器学习流程中都要这样做特征？
- en: When you go back and look at the approaches discussed previously, all of them
    have this common problem. Let's say team-A just completed a customer LTV model
    successfully and took it to production. Now team-A has been assigned another project,
    which is to predict the next purchase day of the customer. There is a high chance
    that the features that were effective in the customer LTV model can be effective
    here as well. Though these features are being calculated periodically to support
    the production model, team-A will start again with the raw data, calculate these
    features from scratch, and use them for the model development. Not only that,
    but they replicate the whole pipeline, though there are overlaps.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当你回顾之前讨论的方法时，它们都存在一个共同的问题。假设团队A成功完成了一个客户LTV模型并将其投入生产。现在团队A被分配了另一个项目，即预测客户的下一个购买日。有很大可能性，在客户LTV模型中有效的特征在这里同样有效。尽管这些特征是定期计算以支持生产模型的，但团队A将重新从原始数据开始，从头计算这些特征，并用于模型开发。不仅如此，他们还会复制整个流程，尽管存在重叠。
- en: As a result of this re-calculation, depending on the setup and the tools that
    team-A uses, they will be wasting compute, storage, and man-hours, whereas with
    better feature management, team-A could have gotten a head start on the new project,
    which is also a cost-effective solution.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种重新计算，根据团队A的设置和使用的工具，他们将会浪费计算资源、存储空间和人力，而如果有了更好的特征管理，团队A本可以在新项目中取得领先，这同样是一个成本效益高的解决方案。
- en: Feature discoverability and sharing
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征可发现性和共享
- en: As discussed, one of the problems is recalculation within the same team. The
    other part of this problem is even bigger. That is re-calculation across the teams
    and domains. Just like in the *Re-inventing the wheel* section, where teams were
    trying to figure out how to bring ML features to production, data scientists are
    re-discovering the data and features themselves here.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一个问题是在同一团队内部进行重新计算。这个问题的另一部分甚至更大。那就是跨团队和领域的重新计算。就像在“重新发明轮子”部分中，团队试图弄清楚如何将机器学习特征带入生产一样，数据科学家在这里也在重新发现数据和特征。
- en: One of the major drivers of this is a lack of trust and discoverability. Let's
    talk about discoverability first. Whenever data scientists work on a model and
    do a great job of data mining, exploration, and feature engineering, there are
    very limited ways of sharing it, as we discussed in the first chapter. The data
    scientist can use emails and presentations to do that. However, there is no way
    for anybody to discover what's available and selectively build what is not and
    use both in the model. Even if it's possible to discover other data scientists
    work, it is not possible to use it without figuring out data access and re-calculating
    features.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中的一个主要驱动因素是缺乏信任和可发现性。我们先来谈谈可发现性。每当数据科学家在构建模型时，如果他们出色地完成了数据挖掘、探索和特征工程，那么分享这些成果的方式非常有限，正如我们在第一章中讨论的那样。数据科学家可以使用电子邮件和演示文稿来分享这些成果。然而，没有任何方式可以让任何人发现可用的资源，并选择性地构建尚未构建的内容，然后在模型中使用它们。即使有可能发现其他数据科学家的工作，但没有弄清楚数据访问和重新计算特征的情况下，也无法使用这些工作。
- en: The other driver for re-inventing the wheel in data discovery and feature engineering
    is trust. Though evidence is clear that there is a production model that uses
    the generated features, data scientists often find it difficult to trust programs
    developed by others that generate the features. Since raw data is trustworthy
    as it will have SLAs and schema validations, data scientists often end up re-discovering
    and generating the features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发现和特性工程中重新发明轮子的另一个驱动因素是信任。尽管有明确的证据表明有一个使用生成特征的生成模型在生产中运行，但数据科学家往往很难信任其他人为生成特性开发的程序。由于原始数据是可信的，因为它将具有服务等级协议和模式验证，数据科学家通常最终会重新发现并生成特性。
- en: Hence the solution required here is an application that can make the features
    generated by others discoverable, sharable, and, most importantly, trustworthy,
    that is, a person/team who owns and manages the features they produce.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里所需的解决方案是一个可以使他人生成的特性可发现、可共享，最重要的是可信赖的应用程序，即拥有和管理他们生成的特性的人/团队。
- en: Training vs Serving skew
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与服务的偏差
- en: One other common problem in ML is training and serving skew. This happens when
    the feature engineering code used to generate the features for model training
    is different from the code used to generate the features for model prediction/serving.
    This could happen for many reasons; for instance, during model training, the data
    scientist may have used PySpark for generating the features, where as while productionizing
    the pipeline, the ML/Data engineer who took over, used a different technologies
    that is required by the production infrastructure. There are few problems with
    this. One is, there are two versions of feature engineering code, and the other
    problem is this could cause the training versus serving skew since the data generated
    by two versions of the pipeline may not be same for the same raw data input.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中另一个常见问题是训练与服务的偏差。这种情况发生在用于为模型训练生成特征的特性工程代码与用于为模型预测/服务生成特征的代码不同时。这种情况可能由许多原因引起；例如，在模型训练期间，数据科学家可能使用了PySpark来生成特征，而在将管道投入生产时，接手该任务的ML/数据工程师可能使用了生产基础设施所需的不同技术。这里有几个问题。一个是存在两个版本的特性工程代码，另一个问题是这可能导致训练与服务的偏差，因为两个版本的管道生成相同原始数据输入的数据可能不同。
- en: Model reproducibility
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型可复现性
- en: Model reproducibility is one of the common issues to tackle in ML. I have heard
    the story of how after a data scientist quit his job, the model he was working
    on was lost and his team couldn't reproduce the model many times. One of the main
    reasons for this is again the lack of feature management tools. You might ask
    what the problem is in reproducing the same model when you have the history of
    raw data. Let's take a look.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可复现性是机器学习中需要解决的常见问题之一。我听说过一个故事，一位数据科学家离职后，他正在工作的模型丢失了，他的团队多次无法复现该模型。其中一个主要原因是缺乏特性管理工具。当你拥有原始数据的历史时，你可能会问复现相同模型的问题在哪里。让我们来看看。
- en: Let's say there was a data scientist, Ram, working on an ML model to recommend
    products to customers. Ram spent a month working on it and came up with a brilliant
    model. With the help of data engineers on the team, the model was deployed to
    production. But Ram was not challenged enough in this job, so he quit and moved
    on to a different firm. Unfortunately, the production system went down, Ram didn't
    follow the MLOps standards of saving the model to a registry, so the model was
    lost and could not be recovered.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个数据科学家，名叫Ram，他正在开发一个机器学习模型，用于向客户推荐产品。Ram花了一个月的时间来开发这个模型，并提出了一个出色的模型。在团队中数据工程师的帮助下，该模型被部署到生产环境中。但是，Ram在这个职位上没有得到足够的挑战，所以他辞职并跳槽到了另一家公司。不幸的是，生产系统崩溃了，Ram没有遵循MLOps标准将模型保存到注册表中，因此模型丢失且无法恢复。
- en: Now, the responsibility for rebuilding the model is given to Dee, a new data
    scientist on the team, who is smart and uses the same dataset that was used by
    Ram, and performs the same data cleansing and feature engineering as if Dee is
    a reincarnation of Ram. Unfortunately, Dee's model cannot get the same results
    as Ram's. No matter how many times Dee tries, she cannot reproduce the model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重建模型的职责落到了团队中的另一位新数据科学家Dee身上，她聪明且使用了与Ram相同的dataset，并进行了与Ram相同的数据清洗和特征工程，仿佛Dee是Ram的转世。不幸的是，Dee的模型无法得到与Ram相同的结果。无论Dee尝试多少次，她都无法重现该模型。
- en: 'One of the reasons for this is that the data has changed over time, which in
    turn affects the feature values and hence the model. There is no way to go back
    in time to produce the same features that were used the first time. As model reproducibility/repeatability
    is one of the crucial aspects of ML, we need to time travel. This means that data
    scientists should be able to go back in time and fetch the feature from a specific
    time in the past, just like in *Avengers: Endgame*, so that the models can be
    reproduced consistently.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 造成这种情况的一个原因是数据随着时间的推移发生了变化，这反过来又影响了特征值，从而影响了模型。没有办法回到过去产生第一次使用时的相同特征。由于模型的可重现性/可重复性是机器学习的一个关键方面，我们需要进行时间旅行。这意味着数据科学家应该能够回到过去，从过去某个特定时间点获取特征，就像在*复仇者联盟：终局之战*中一样，这样模型就可以一致地重现。
- en: Low latency
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低延迟
- en: One other problem that all the approaches are trying to solve is low-latency
    feature serving. The ability to provide features at low latency decides whether
    a model can be hosted as an online model or a batch model. This has problems such
    as building and managing infrastructure and keeping features up to date. As it
    doesn't make sense to set up all models to be transactional, at the same time
    there is a high chance that a feature used in one batch model could be of great
    use in a different online model. So, the ability to switch the low-latency serving
    on and off would be a great benefit to data scientists.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法试图解决的问题之一是低延迟特征服务。提供低延迟特征的能力决定了模型能否作为在线模型或批量模型托管。这涉及到构建和管理基础设施以及保持特征更新等问题。由于没有必要将所有模型都设置为事务性的，同时也有很高的可能性，一个用于批量模型的特征可能对不同的在线模型非常有用。因此，能够开关低延迟服务将极大地便利数据科学家。
- en: So far in this section, we have been through some of the common problems with
    the approaches discussed in the previous section. The question that still remains
    unanswered is what can be done to make this better? Is there a single tool or
    set of tools that exists today that can help us solve these common problems? As
    it turns out, the answer is *yes*, there is one tool that can solve all the problems
    we have talked about so far. It is called a *feature store*. In the next section,
    let's see what feature stores are, how they solve these problems, and the philosophy
    behind them.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，我们已经讨论了上一节中讨论的方法的一些常见问题。仍然悬而未决的问题是，我们能做些什么来使情况变得更好？是否存在一个或一组现有的工具，可以帮助我们解决这些常见问题？事实证明，答案是*是的*，有一个工具可以解决我们迄今为止讨论的所有问题。它被称为*特征存储*。在下一节中，我们将看看什么是特征存储，它们如何解决问题，以及背后的哲学。
- en: Feature stores to the rescue
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征存储来拯救
- en: 'Let''s begin this section with the definition of feature stores. A **feature
    store** is an operational data system for managing and serving ML features to
    models in production. It can serve feature data to models from a low-latency online
    store (for real-time prediction) or from an offline store (for scale-out batch
    scoring or model training). As the definition points out, it''s a whole package
    that helps you create and manage ML features, and accelerate the operationalization
    of models. Before we dive deeper into feature stores, let''s look at how the architecture
    of ML pipelines changes with the introduction of a feature store:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从特征存储的定义开始这一节。**特征存储**是一个用于管理和为生产中的模型提供机器学习特征的运营数据系统。它可以从低延迟的在线存储（用于实时预测）或离线存储（用于扩展批量评分或模型训练）向模型提供特征数据。正如定义所指出的，它是一个完整的包，帮助你创建和管理机器学习特征，并加速模型的运营化。在我们深入了解特征存储之前，让我们看看引入特征存储后机器学习管道架构如何变化：
- en: '![Figure 2.4 – ML pipelines with a feature store'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – 带有特征存储的机器学习管道'
- en: '](img/B18024_02_04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18024_02_04.jpg)'
- en: Figure 2.4 – ML pipelines with a feature store
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 带有特征存储的机器学习管道
- en: '*Figure 2.4* depicts the architecture of ML pipelines when you include a feature
    store. You may think that *Figure 2.4* looks the same as *Figure 2.3* and I just
    replaced a bunch of small data stores with a bigger one and called it a feature
    store. Yes, it might seem that way, but there is more to it. Unlike a traditional
    data store, a feature store has a special set of capabilities; it is not a data
    store, but rather a data system (as its definition states), and it can do much
    more than just storage and retrieval.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.4* 展示了包含特征存储的机器学习管道架构。你可能觉得 *图2.4* 和 *图2.3* 看起来一样，我只是用一个更大的存储替换了一堆小数据存储，并称之为特征存储。是的，可能看起来是这样，但还有更多。与传统数据存储不同，特征存储有一套特殊的特性；它不是一个数据存储，而是一个数据系统（正如其定义所述），并且它能做比仅仅存储和检索更多的事情。'
- en: 'With the feature store being part of the ML pipeline, this is how the entire
    pipeline works:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征存储是机器学习管道的一部分，整个管道的工作方式如下：
- en: Once the data scientist has a problem statement, the starting point will not
    be raw data anymore. It will be the feature store.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据科学家有了问题陈述，起点将不再是原始数据。它将是特征存储。
- en: The data scientist will connect to the feature store, browse through the repository,
    and use the features of interest.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学家将连接到特征存储，浏览存储库，并使用感兴趣的特性。
- en: If this is the first model, the feature store might be empty. From here, the
    data scientist will go into the discovery phase, figure out the dataset, build
    a feature engineering pipeline, and ingest the features into the feature store.
    The feature store decouples feature engineering pipelines from the rest of the
    stages in ML.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这是第一个模型，特征存储可能为空。从这里，数据科学家将进入发现阶段，确定数据集，构建特征工程管道，并将特征导入特征存储。特征存储将特征工程管道与机器学习中的其他阶段解耦。
- en: If the feature store is not empty but there are not enough features available
    in the feature store, the data scientist will discover the data of interest and
    another feature engineering pipeline will be added to sink a new set of features
    into the feature store. This approach makes the features available for the model
    that the data scientist is working on and for other data scientists who find these
    features useful in their model.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果特征存储不为空，但特征存储中可用的特征不足，数据科学家将发现感兴趣的数据，并添加另一个特征工程管道将一组新特征导入特征存储。这种方法使得特征对数据科学家正在工作的模型以及其他发现这些特征对其模型有用的数据科学家可用。
- en: Once the data scientist is happy with the feature set, the model will be trained,
    validated, and tested. If the model performance is not good, the data scientist
    will go back to discover new data and features.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据科学家对特征集满意，模型将进行训练、验证和测试。如果模型性能不佳，数据科学家将返回去发现新的数据和特征。
- en: When the model is ready to be deployed, the `predict` method of the model will
    include code to fetch the required features for generating model predictions.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当模型准备好部署时，模型的 `predict` 方法将包含用于生成模型预测所需特征的代码。
- en: The ready model will be deployed as a REST endpoint if it's an online model;
    otherwise, it will be used to perform batch predictions.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果是在线模型，准备好的模型将以REST端点部署；否则，它将用于执行批量预测。
- en: Now that we understand how the pipeline works, let's go through the problems
    we discussed in the previous section and learn how the feature store solves them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了管道的工作原理，让我们回顾上一节中讨论的问题，并了解特征存储是如何解决它们的。
- en: Standardizing ML with a feature store
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用特征存储标准化机器学习
- en: Once the feature store is standardized at the team level, though there may be
    different ways of reading data and building feature engineering pipelines, but
    beyond feature engineering, the rest of the pipeline becomes a standard implementation.
    The ML engineers and data scientists don't have to come up with new ways to bring
    features to production. After feature engineering, the data scientists and ML
    engineers will ingest the features into the feature store. The feature store,
    by definition, can serve features at low latency. All ML engineers will have to
    do after that is update their `predict` method to fetch the required features
    from the feature store and return the predictions. This not only makes the life
    of the ML engineers easy, it sometimes also offloads managing feature management
    infrastructure.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦特征存储在团队层面得到标准化，尽管可能有不同的读取数据和构建特征工程管道的方式，但超出特征工程之外，管道的其余部分成为标准实现。机器学习工程师和数据科学家不必想出新方法将特征带到生产中。在特征工程之后，数据科学家和机器学习工程师将特征摄入特征存储。根据定义，特征存储可以以低延迟提供特征。在此之后，所有机器学习工程师需要做的就是更新他们的`predict`方法，从特征存储中获取所需特征并返回预测结果。这不仅使机器学习工程师的生活变得容易，有时也减轻了管理特征管理基础设施的负担。
- en: Feature store avoids reprocessing data
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征存储避免了数据重新处理
- en: As mentioned in the definition, a feature store has an offline store, and data
    from the offline store can be retrieved for model training or batch inference.
    Model training here doesn't mean training the same model. The features ingested
    into the feature store can be used for the training of another model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如定义所述，特征存储有一个离线存储，离线存储中的数据可以用于模型训练或批量推理。这里的模型训练并不意味着训练相同的模型。输入到特征存储的特征可以用于训练另一个模型。
- en: 'Let''s take the same example we used while discussing the problem: team-A just
    completed the production deployment of the customer LTV model. The next model
    team-A starts working on is predicting the next purchase date. When the data scientists
    start working on this model, they don''t have to go back to raw data and re-calculate
    the features that were used to build the customer LTV model. The data scientist
    can connect to the feature store, which is being updated with the latest features
    of the previous model, and get the features required to train the new model. However,
    the data scientist will have to build a data cleaning and feature engineering
    pipeline for the additional features that they find useful from the raw data.
    Again, the newly added features can be reused in the next model. This makes model
    development efficient and cost-effective.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以我们在讨论问题时使用的相同例子为例：团队-A刚刚完成了客户LTV模型的投产部署。团队-A接下来要开始工作的模型是预测下一次购买日期。当数据科学家开始工作在这个模型上时，他们不必回到原始数据并重新计算构建客户LTV模型所使用的特征。数据科学家可以连接到特征存储，该存储正在更新为前一个模型的最新特征，并获取训练新模型所需的特征。然而，数据科学家将不得不为从原始数据中找到的有用特征构建数据清洗和特征工程管道。再次强调，新添加的特征可以在下一个模型中重用。这使得模型开发既高效又经济。
- en: Features are discoverable and sharable with the feature store
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征可以通过特征存储进行发现和共享
- en: In the previous paragraph, we discussed the reuse of features within a team.
    Feature stores help data scientists achieve that. The other major issue was re-calculating
    and re-discovering useful data and features across teams due to the lack of feature
    discoverability. Guess what? Feature stores solve that too. Data scientists can
    connect to a feature store and browse through the existing feature tables and
    schemas. If they find any of the existing features useful, data scientists can
    use them in the model without re-discovering or re-calculating them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一段中，我们讨论了团队内部特征的重用。特征存储帮助数据科学家实现这一点。另一个主要问题是由于缺乏特征可发现性，跨团队重新计算和重新发现有用的数据和特征。猜猜看？特征存储也能解决这个问题。数据科学家可以连接到特征存储，浏览现有的特征表和模式。如果他们发现任何现有特征有用，数据科学家可以在模型中使用它们，而无需重新发现或重新计算。
- en: Another problem involved in sharing was trust. Although feature stores don't
    solve this completely, they address it to some extent. Since the feature tables
    are created and managed by a team, the data scientists can always reach out to
    the owners to get access and also discuss other aspects, such as SLAs and monitoring.
    If you haven't noticed yet, feature stores facilitate collaboration between teams.
    This can be beneficial for both teams, with data scientists and ML engineers from
    different teams working together and sharing each other's expertise.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与共享相关的问题是信任。尽管特征存储不能完全解决这个问题，但它在一定程度上解决了它。由于特征表是由团队创建和管理的，数据科学家可以随时联系所有者以获取访问权限，并讨论其他方面，如服务等级协议和监控。如果你还没有注意到，特征存储促进了团队之间的协作。这对双方都有益，不同团队的数据科学家和机器学习工程师可以一起工作，分享彼此的专业知识。
- en: No more training versus serving skew
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 没有更多的训练与服务的偏差
- en: With Feature Store, training versus serving skew will never occur. Once the
    feature engineering is complete, the features will be ingested into the feature
    store and feature store is the source for model training. Hence, the data scientists
    will use the features in feature store to train the ML model. Once the model is
    trained and moved to production, the production model will fetch the online store
    or historical store again for model prediction. As these features are used by
    both - training and prediction, serving is generated by the same pipeline/code
    and we will never have this issue with feature store.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征存储，训练与服务的偏差将永远不会发生。一旦特征工程完成，特征将被摄入到特征存储中，特征存储是模型训练的来源。因此，数据科学家将使用特征存储中的特征来训练机器学习模型。一旦模型训练完成并部署到生产环境，生产模型将再次从在线存储或历史存储中获取数据以进行模型预测。由于这些特征同时被训练和预测使用，服务是通过相同的管道/代码生成的，我们永远不会在特征存储中遇到这个问题。
- en: Model reproducibility with feature stores
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征存储实现模型的可复现性
- en: 'The other major issue with the previously discussed architecture was model
    reproducibility. This is an issue: the data is changing frequently, which in turn
    leads to features changing and hence the model changing, though the same set of
    features is being used to build the model. The only way to solve this problem
    was to go back in time and fetch the same state data that produced the old model.
    That may be a very complex problem to solve since it will involve multiple data
    stores. However, it is possible to store generated features in such a way that
    it allows data scientists to time travel.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 之前讨论的架构中另一个主要问题是模型的可复现性。这是一个问题：数据频繁变化，这反过来又导致特征变化，进而导致模型变化，尽管使用的是相同的特征集来构建模型。解决这个问题的唯一方法就是回到过去，获取产生旧模型的相同状态数据。这可能是一个非常复杂的问题，因为它将涉及多个数据存储。然而，可以以这种方式存储生成的特征，使得数据科学家能够进行时间旅行。
- en: Yes, that is exactly what a feature store does. A feature store has an offline
    store, which stores historical data and allows users to go back in time and get
    the value of a feature at a specific point in time. With a feature store, a data
    scientist can get features from a specific time in history, so reproducing the
    model consistently is possible. Model reproducibility is no longer an issue with
    feature stores.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这正是特征存储所做的事情。特征存储有一个离线存储，用于存储历史数据，并允许用户回到过去，在特定时间点获取特征的值。使用特征存储，数据科学家可以从历史中的特定时间点获取特征，因此可以一致地复现模型。模型的可复现性不再是特征存储的问题。
- en: Serving features at low latency with feature stores
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用特征存储以低延迟提供特征
- en: Though all the solutions were able to achieve the low-latency serving in some
    way, the solutions were not uniform. ML engineers had to come up with a solution
    to solve this issue and also build and manage the infrastructure. However, having
    a feature store in the ML pipeline makes this simple and also offloads the infrastructure
    management to the other team in cases where the platform team manages the feature
    store. Even without that, having the ability to run a few commands and have the
    low-latency serving up and running is a handy tool for ML engineers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有解决方案都能以某种方式实现低延迟的服务，但解决方案并不统一。机器学习工程师必须想出一个解决方案来解决这个问题，并构建和管理基础设施。然而，在机器学习管道中拥有特征存储使得这变得简单，并且将基础设施管理卸载给其他团队，在平台团队管理特征存储的情况下。即使没有那样，能够运行几个命令并使低延迟服务上线也是机器学习工程师的一个实用工具。
- en: Philosophy behind feature stores
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征存储背后的哲学
- en: In this chapter, we have discussed different issues with ML pipelines and how
    feature stores help data scientists solve them and accelerate ML development.
    In this section, let's try to understand the philosophy behind feature stores
    and try to make sense of why having a feature store in our ML pipeline may be
    the ideal way to accelerate ML. Let's start with a real-world example as we are
    trying to build real-world experience with ML. You will be given the names of
    two phones; your job is to figure out which one is better. The names are iPhone
    13 Pro and Google Pixel 6 Pro. You have an infinite amount of time to find the
    answer; continue reading once you have the answer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了与机器学习管道相关的问题，以及特征存储如何帮助数据科学家解决这些问题并加速机器学习的发展。在本节中，让我们尝试理解特征存储背后的哲学，并尝试弄清楚为什么在我们的机器学习管道中拥有特征存储可能是加速机器学习的理想方式。让我们从一个现实世界的例子开始，因为我们正在尝试通过机器学习建立现实世界的经验。你将得到两款手机的名称；你的任务是判断哪一款更好。名称是iPhone
    13 Pro和Google Pixel 6 Pro。你有无限的时间来找到答案；在你找到答案后继续阅读。
- en: 'As Ralph Waldo Emerson said, *It''s not the destination, it is the journey*.
    Whatever your answer may be, however long you took to arrive at it, let''s look
    at how you might have arrived at it. Some of you might have got an answer right
    away, but if you haven''t used either of these phones, you probably would have
    googled `iPhone 13 Pro vs Google Pixel 6 Pro`. You would have browsed through
    a few links, which would give you a comparison of the phones:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如拉尔夫·瓦尔多·爱默生所说，*重要的不是目的地，而是旅程*。无论你的答案是什么，无论你花了多长时间到达那里，让我们看看你是如何到达那里的。有些人可能立刻就得到了答案，但如果你没有使用过这两款手机，你可能会在谷歌上搜索`iPhone
    13 Pro与Google Pixel 6 Pro对比`。你会浏览几个链接，这些链接会给你提供手机的比较：
- en: '![Figure 2.5 – iPhone 13 Pro versus Google Pixel 6 Pro'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.5 – iPhone 13 Pro versus Google Pixel 6 Pro]'
- en: '](img/B18024_02_05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18024_02_05.jpg]'
- en: Figure 2.5 – iPhone 13 Pro versus Google Pixel 6 Pro
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – iPhone 13 Pro与Google Pixel 6 Pro对比
- en: This was a great way of comparing two phones. Some of you might have done a
    lot more to arrive at the answer, but I'm sure none of us went and bought both
    phones, read through the specs provided by Apple and Google, used each of them
    for a month, and became experts with each of them before answering the question.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种很好的比较两款手机的方法。有些人可能为了得到答案做了更多的工作，但我相信我们中没有一个人去买了这两款手机，阅读了苹果和谷歌提供的规格，每个月都使用它们，并在回答问题之前成为每个手机的专家。
- en: In this task, we were smart enough to use the expertise and work done by others.
    Although there are a lot of comparisons on the internet, we chose the one that
    works for us. Not only in this task, but in most tasks, from buying a phone to
    buying a home, we try to use expert opinion to make a decision. If you look at
    it in a certain way, these are features in our decision-making. Along with the
    expert's opinion, we also include our own constraints and features, such as budget,
    memory if it's a phone, the number of seats if it's a car, and the number of rooms
    if it's a house. We use a combination of these to decide and take action. In most
    cases, this approach works, and in some cases, we might do a lot more research
    and become experts too.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，我们足够聪明，能够利用他人的专业知识和工作成果。尽管互联网上有许多比较，但我们选择了对我们有效的那一个。不仅在这个任务中，而且在大多数任务中，从购买手机到购买房屋，我们都试图利用专家意见来做出决定。如果你从某个角度来看，这些就是我们的决策特征。除了专家意见外，我们还包括我们自己的限制和特征，例如预算、如果是手机，内存大小；如果是汽车，座位数；如果是房屋，房间数。我们使用这些组合来做出决定并采取行动。在大多数情况下，这种方法是有效的，在某些情况下，我们可能需要进行更多研究，甚至成为专家。
- en: The use of feature stores in ML is an attempt to achieve something similar;
    it is like Google for data scientists. Instead of a generic search like Google,
    data scientists are looking for something specific, and are also sharing their
    expertise with other data scientists. If what is available in the feature store
    doesn't work for a data scientist, they will go to raw data, explore, understand,
    become experts in it, and come up with distinguishing features about a particular
    entity, which could be products, customers, and so on. This workflow of ML with
    feature stores will not only help data scientists use each other's expertise but
    also standardize and accelerate ML development.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中使用特征存储库是一种尝试实现类似目标的方法；它就像是数据科学家的Google。与Google的通用搜索不同，数据科学家正在寻找特定的事物，并且也在与其他数据科学家分享他们的专业知识。如果特征存储库中可用的内容不适合数据科学家，他们就会转向原始数据，进行探索、理解，成为该领域的专家，并针对特定实体（如产品、客户等）提出区分性特征。这种结合特征存储库的机器学习工作流程不仅可以帮助数据科学家利用彼此的专业知识，还可以标准化并加速机器学习的发展。
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed common problems in ML feature management, different
    architectures of productionizing ML models, and ways to bring features to production.
    We also explored the issues involved with these approaches and how feature stores
    solve these issues by standardizing practices and providing additional features
    that a traditional data store does not.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了机器学习特征管理中的常见问题、生产化机器学习模型的多种架构以及将特征带入生产的方法。我们还探讨了这些方法中涉及的问题以及特征存储库如何通过标准化实践和提供传统数据存储库不具备的额外功能来解决这些问题。
- en: Now that we understand what feature stores have to offer, in the next chapter,
    we'll get our hands dirty with feature stores and explore the terminology, features,
    typical architecture of a feature store, and much more.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了特征存储库能提供什么，在下一章中，我们将深入探讨特征存储库，并探索术语、特征、特征存储库的典型架构等内容。
- en: Further reading
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Feast documentation*: [https://docs.feast.dev/](https://docs.feast.dev/)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Feast文档*：[https://docs.feast.dev/](https://docs.feast.dev/)'
