- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Classifying Images with Deep Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度卷积神经网络进行图像分类
- en: 'In the previous chapter, we looked in depth at different aspects of the TensorFlow
    API, you became familiar with tensors and decorating functions, and you learned
    how to work with TensorFlow Estimators. In this chapter, you will now learn about
    **convolutional neural networks** (**CNNs**) for image classification. We will
    start by discussing the basic building blocks of CNNs, using a bottom-up approach.
    Then, we will take a deeper dive into the CNN architecture and explore how to
    implement CNNs in TensorFlow. In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们深入探讨了TensorFlow API的不同方面，熟悉了张量和装饰函数，学习了如何使用TensorFlow Estimators。在本章中，你将学习用于图像分类的**卷积神经网络**（**CNNs**）。我们将从讨论CNN的基本构建模块开始，采用自下而上的方法。接着，我们将深入了解CNN架构，并探索如何在TensorFlow中实现CNN。在本章中，我们将涵盖以下主题：
- en: Convolution operations in one and two dimensions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维和二维的卷积操作
- en: The building blocks of CNN architectures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN架构的构建模块
- en: Implementing deep CNNs in TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现深度CNN
- en: Data augmentation techniques for improving the generalization performance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高泛化性能的数据增强技术
- en: Implementing a face image-based CNN classifier for predicting the gender of
    a person
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基于面部图像的CNN分类器，以预测个人的性别
- en: The building blocks of CNNs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的构建模块
- en: CNNs are a family of models that were originally inspired by how the visual
    cortex of the human brain works when recognizing objects. The development of CNNs
    goes back to the 1990s, when Yann LeCun and his colleagues proposed a novel NN
    architecture for classifying handwritten digits from images (*Handwritten Digit
    Recognition with a Back-Propagation Network*, *Y*. *LeCun*, and others, *1989*,
    published at the *Neural Information Processing Systems (NeurIPS)* conference).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是一个模型家族，最初的灵感来自于人类大脑视觉皮层在识别物体时的工作方式。CNN的发展可以追溯到1990年代，当时Yann LeCun和他的同事们提出了一种新型的神经网络架构，用于从图像中分类手写数字（*Handwritten
    Digit Recognition with a Back-Propagation Network*，*Y*. *LeCun* 等人，*1989*，发表于*Neural
    Information Processing Systems (NeurIPS)*会议）。
- en: '**The human visual cortex**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**人类视觉皮层**'
- en: The original discovery of how the visual cortex of our brain functions was made
    by David H. Hubel and Torsten Wiesel in 1959, when they inserted a microelectrode
    into the primary visual cortex of an anesthetized cat. Then, they observed that
    brain neurons respond differently after projecting different patterns of light
    in front of the cat. This eventually led to the discovery of the different layers
    of the visual cortex. While the primary layer mainly detects edges and straight
    lines, higher-order layers focus more on extracting complex shapes and patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 1959年，David H. Hubel和Torsten Wiesel首次发现了我们大脑视觉皮层的功能，当时他们将一个微电极插入到麻醉猫的初级视觉皮层。然后，他们观察到，在猫眼前投射不同的光图案时，大脑神经元的反应不同。这最终导致了视觉皮层不同层次的发现。初级层主要检测边缘和直线，而更高层次则更专注于提取复杂的形状和图案。
- en: Due to the outstanding performance of CNNs for image classification tasks, this
    particular type of feedforward NN gained a lot of attention and led to tremendous
    improvements in machine learning for computer vision. Several years later, in
    2019, Yann LeCun received the Turing award (the most prestigious award in computer
    science) for his contributions to the field of artificial intelligence (AI), along
    with two other researchers, Yoshua Bengio and Geoffrey Hinton, whose names you
    have already encountered in previous chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积神经网络（CNN）在图像分类任务中的出色表现，这种特定类型的前馈神经网络（NN）受到了广泛关注，并推动了计算机视觉领域机器学习的巨大进步。几年后，2019年，Yann
    LeCun因其在人工智能（AI）领域的贡献获得了图灵奖（计算机科学领域的最高奖项），与他一起获奖的还有另外两位研究者，Yoshua Bengio和Geoffrey
    Hinton，他们的名字你在前面的章节中已经遇到过。
- en: In the following sections, we will discuss the broader concept of CNNs and why
    convolutional architectures are often described as "feature extraction layers."
    Then, we will delve into the theoretical definition of the type of convolution
    operation that is commonly used in CNNs and walk through examples for computing
    convolutions in one and two dimensions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论CNN的更广泛概念，以及为什么卷积架构通常被描述为“特征提取层”。然后，我们将深入探讨CNN中常用的卷积操作类型的理论定义，并通过一维和二维卷积计算的示例来讲解。
- en: Understanding CNNs and feature hierarchies
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解CNN和特征层次
- en: Successfully extracting **salient** (**relevant**) **features** is key to the
    performance of any machine learning algorithm and traditional machine learning
    models rely on input features that may come from a domain expert or are based
    on computational feature extraction techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 成功提取**显著**（**相关**）**特征**是任何机器学习算法性能的关键，传统的机器学习模型依赖于输入特征，这些特征可能来自领域专家或基于计算特征提取技术。
- en: 'Certain types of NNs, such as CNNs, are able to automatically learn the features
    from raw data that are most useful for a particular task. For this reason, it''s
    common to consider CNN layers as feature extractors: the early layers (those right
    after the input layer) extract **low-level features** from raw data, and the later
    layers (often **fully connected layers** like in a multilayer perceptron (MLP))
    use these features to predict a continuous target value or class label.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 某些类型的神经网络，如CNN，能够自动从原始数据中学习出对于特定任务最有用的特征。因此，通常将CNN层视为特征提取器：早期的层（即输入层之后的层）从原始数据中提取**低级特征**，而后来的层（通常是**全连接层**，如多层感知器（MLP）中的层）则使用这些特征来预测连续目标值或分类标签。
- en: Certain types of multilayer NNs, and in particular, deep convolutional NNs,
    construct a so-called **feature hierarchy** by combining the low-level features
    in a layer-wise fashion to form high-level features. For example, if we're dealing
    with images, then low-level features, such as edges and blobs, are extracted from
    the earlier layers, which are combined together to form high-level features. These
    high-level features can form more complex shapes, such as the general contours
    of objects like buildings, cats, or dogs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 某些类型的多层神经网络，尤其是深度卷积神经网络，构建了所谓的**特征层次结构**，通过逐层组合低级特征来形成高级特征。例如，如果我们处理的是图像，那么低级特征，如边缘和斑块，来自早期层，这些特征组合在一起形成高级特征。这些高级特征可以形成更复杂的形状，例如建筑物、猫或狗的轮廓。
- en: 'As you can see in the following image, a CNN computes **feature maps** from
    an input image, where each element comes from a local patch of pixels in the input
    image:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，CNN从输入图像中计算**特征图**，其中每个元素来自输入图像中局部像素块：
- en: '![](img/B13208_15_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_01.png)'
- en: (Photo by Alexander Dummer on Unsplash)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由Alexander Dummer提供，来源于Unsplash）
- en: 'This local patch of pixels is referred to as the **local receptive field**.
    CNNs will usually perform very well on image-related tasks, and that''s largely
    due to two important ideas:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个局部像素块称为**局部感受野**。CNN通常在图像相关任务上表现非常好，这主要得益于两个重要思想：
- en: '**Sparse connectivity**: A single element in the feature map is connected to
    only a small patch of pixels. (This is very different from connecting to the whole
    input image as in the case of perceptrons. You may find it useful to look back
    and compare how we implemented a fully connected network that connected to the
    whole image in *Chapter 12*, *Implementing a Multilayer Artificial Neural Network
    from Scratch*.)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏连接**：特征图中的单个元素仅连接到一个小的像素块。（这与感知器连接到整个输入图像非常不同。你可能会发现回顾并比较我们在*第12章*“*从零开始实现多层人工神经网络*”中实现的全连接网络非常有用。）'
- en: '**Parameter-sharing**: The same weights are used for different patches of the
    input image.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数共享**：相同的权重用于输入图像的不同块区域。'
- en: As a direct consequence of these two ideas, replacing a conventional, fully
    connected MLP with a convolution layer substantially decreases the number of weights
    (parameters) in the network and we will see an improvement in the ability to capture
    *salient* features. In the context of image data, it makes sense to assume that
    nearby pixels are typically more relevant to each other than pixels that are far
    away from each other.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这两种思想的直接结果，用卷积层替换传统的全连接多层感知器（MLP）大大减少了网络中的权重（参数）数量，我们将看到在捕捉*显著*特征的能力上有所提升。在图像数据的背景下，假设邻近的像素通常比远离彼此的像素更相关，这是合理的。
- en: Typically, CNNs are composed of several **convolutional** and subsampling layers
    that are followed by one or more fully connected layers at the end. The fully
    connected layers are essentially an MLP, where every input unit, *i*, is connected
    to every output unit, *j*, with weight ![](img/B13208_15_001.png) (which we covered
    in more detail in *Chapter 12*, *Implementing a Multilayer Artificial Neural Network
    from Scratch*).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN 由多个**卷积**层和子采样层组成，最后会接一个或多个全连接层。全连接层本质上是一个多层感知器（MLP），其中每个输入单元 *i* 都与每个输出单元
    *j* 连接，并且有权重 ![](img/B13208_15_001.png)（我们在*第12章*《从零开始实现多层人工神经网络》中已经详细介绍过）。
- en: Please note that subsampling layers, commonly known as **pooling layers**, do
    not have any learnable parameters; for instance, there are no weights or bias
    units in pooling layers. However, both the convolutional and fully connected layers
    have weights and biases that are optimized during training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，子采样层，通常称为**池化层**，没有任何可学习的参数；例如，池化层中没有权重或偏置单元。然而，卷积层和全连接层有权重和偏置，这些会在训练过程中进行优化。
- en: In the following sections, we will study convolutional and pooling layers in
    more detail and see how they work. To understand how convolution operations work,
    let's start with a convolution in one dimension, which is sometimes used for working
    with certain types of sequence data, such as text. After discussing one-dimensional
    convolutions, we will work through the typical two-dimensional ones that are commonly
    applied to two-dimensional images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将更详细地研究卷积层和池化层，并了解它们是如何工作的。为了理解卷积操作的原理，我们从一维卷积开始，它有时用于处理某些类型的序列数据，如文本。在讨论完一维卷积后，我们将继续探讨通常应用于二维图像的典型二维卷积。
- en: Performing discrete convolutions
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行离散卷积
- en: A **discrete convolution** (or simply **convolution**) is a fundamental operation
    in a CNN. Therefore, it's important to understand how this operation works. In
    this section, we will cover the mathematical definition and discuss some of the
    **naive** algorithms to compute convolutions of one-dimensional tensors (vectors)
    and two-dimensional tensors (matrices).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散卷积**（或简称**卷积**）是卷积神经网络（CNN）中的一个基础操作。因此，理解这个操作的原理非常重要。在本节中，我们将介绍数学定义，并讨论一些计算一维张量（向量）和二维张量（矩阵）卷积的**简单**算法。'
- en: Please note that the formulas and descriptions in this section are solely for
    understanding how convolution operations in CNNs work. Indeed, much more efficient
    implementations of convolutional operations already exist in packages such as
    TensorFlow, as you will see later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本节中的公式和描述仅用于帮助理解 CNN 中卷积操作的原理。事实上，像 TensorFlow 这样的包中已经存在更高效的卷积操作实现，正如你将在本章后面看到的那样。
- en: '**Mathematical notation**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**数学符号**'
- en: In this chapter, we will use subscripts to denote the size of a multidimensional
    array (tensor); for example, ![](img/B13208_15_002.png) is a two-dimensional array
    of size ![](img/B13208_15_003.png). We use brackets, [ ], to denote the indexing
    of a multidimensional array.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用下标来表示多维数组（张量）的大小；例如，![](img/B13208_15_002.png) 是一个大小为 ![](img/B13208_15_003.png)
    的二维数组。我们使用方括号 [ ] 来表示多维数组的索引。
- en: For example, *A*[*i*, *j*] refers to the element at index *i*, *j* of matrix
    *A*. Furthermore, note that we use a special symbol, ![](img/B13208_15_004.png),
    to denote the convolution operation between two vectors or matrices, which is
    not to be confused with the multiplication operator, *, in Python.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*A*[*i*, *j*] 表示矩阵 *A* 中索引 *i*，*j* 处的元素。此外，请注意，我们使用一个特殊符号 ![](img/B13208_15_004.png)
    来表示两个向量或矩阵之间的卷积操作，这与 Python 中的乘法操作符 * 不同。
- en: Discrete convolutions in one dimension
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一维离散卷积
- en: 'Let''s start with some basic definitions and notations that we are going to
    use. A discrete convolution for two vectors, *x* and *w*, is denoted by ![](img/B13208_15_005.png),
    in which vector *x* is our input (sometimes called **signal**) and *w* is called
    the **filter** or **kernel**. A discrete convolution is mathematically defined
    as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些基本的定义和符号开始，这些是我们接下来将使用的。两个向量 *x* 和 *w* 的离散卷积用 ![](img/B13208_15_005.png)
    表示，其中向量 *x* 是我们的输入（有时称为**信号**），而 *w* 被称为**滤波器**或**核**。离散卷积在数学上定义如下：
- en: '![](img/B13208_15_006.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_006.png)'
- en: 'As mentioned earlier, the brackets, [ ], are used to denote the indexing for
    vector elements. The index, *i*, runs through each element of the output vector,
    *y*. There are two odd things in the preceding formula that we need to clarify:
    ![](img/B13208_15_008.png) to ![](img/B13208_15_009.png) indices and negative
    indexing for *x*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，方括号[ ]用于表示向量元素的索引。索引 *i* 遍历输出向量 *y* 的每个元素。前面的公式中有两个需要澄清的奇怪之处：![](img/B13208_15_008.png)
    到 ![](img/B13208_15_009.png) 的索引和 *x* 的负索引。
- en: The fact that the sum runs through indices from ![](img/B13208_15_008.png) to
    ![](img/B13208_15_009.png) seems odd, mainly because in machine learning applications,
    we always deal with finite feature vectors. For example, if *x* has 10 features
    with indices 0, 1, 2,…, 8, 9, then indices ![](img/B13208_15_012.png) and ![](img/B13208_15_013.png)
    are out of bounds for *x*. Therefore, to correctly compute the summation shown
    in the preceding formula, it is assumed that *x* and *w* are filled with zeros.
    This will result in an output vector, *y*, that also has infinite size, with lots
    of zeros as well. Since this is not useful in practical situations, *x* is padded
    only with a finite number of zeros.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 求和从 ![](img/B13208_15_008.png) 到 ![](img/B13208_15_009.png) 的过程看起来很奇怪，主要是因为在机器学习应用中，我们通常处理的是有限的特征向量。例如，如果
    *x* 有 10 个特征，索引为 0, 1, 2,…, 8, 9，那么索引 ![](img/B13208_15_012.png) 和 ![](img/B13208_15_013.png)
    对于 *x* 来说是越界的。因此，为了正确计算前面公式中的求和，假设 *x* 和 *w* 被填充了零。这将导致输出向量 *y* 也具有无限大小，并且包含很多零。由于在实际应用中这并没有用处，因此
    *x* 仅会被填充有限数量的零。
- en: 'This process is called **zero-padding** or simply **padding**. Here, the number
    of zeros padded on each side is denoted by *p*. An example padding of a one-dimensional
    vector, *x*, is shown in the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为 **零填充**，或简称为 **填充**。这里，填充在每一侧的零的数量用 *p* 表示。下面的图示展示了一个一维向量 *x* 的填充示例：
- en: '![](img/B13208_15_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_02.png)'
- en: 'Let''s assume that the original input, *x*, and filter, *w*, have *n* and *m*
    elements, respectively, where ![](img/B13208_15_014.png). Therefore, the padded
    vector, ![](img/B13208_15_015.png), has size *n* + 2*p*. The practical formula
    for computing a discrete convolution will change to the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始输入 *x* 和滤波器 *w* 分别有 *n* 和 *m* 个元素，其中 ![](img/B13208_15_014.png)。因此，填充后的向量
    ![](img/B13208_15_015.png) 的大小为 *n* + 2*p*。计算离散卷积的实际公式将变为以下形式：
- en: '![](img/B13208_15_016.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_016.png)'
- en: 'Now that we have solved the infinite index issue, the second issue is indexing
    *x* with *i* + *m* – *k*. The important point to notice here is that *x* and *w*
    are indexed in different directions in this summation. Computing the sum with
    one index going in the reverse direction is equivalent to computing the sum with
    both indices in the forward direction after flipping one of those vectors, *x*
    or *w*, after they are padded. Then, we can simply compute their dot product.
    Let''s assume we flip (rotate) the filter, *w*, to get the rotated filter, ![](img/B13208_15_017.png).
    Then, the dot product, ![](img/B13208_15_018.png), is computed to get one element,
    *y*[*i*], where *x*[*i*: *i* + *m*] is a patch of *x* with size *m*. This operation
    is repeated like in a sliding window approach to get all the output elements.
    The following figure provides an example with *x* = [3 2 1 7 1 2 5 4] and ![](img/B13208_15_019.png)
    so that the first three output elements are computed:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们已经解决了无限索引的问题，第二个问题是使用 *i* + *m* – *k* 来索引 *x*。这里需要注意的要点是，在这个求和过程中，*x* 和
    *w* 的索引方向是不同的。用一个反向方向的索引计算和，相当于在将其中一个向量（*x* 或 *w*）填充后，翻转其中一个向量，使得两个索引都朝前计算，然后简单地计算它们的点积。假设我们将滤波器
    *w* 进行翻转（旋转），得到旋转后的滤波器，![](img/B13208_15_017.png)。然后，计算点积 ![](img/B13208_15_018.png)，得到一个元素
    *y*[*i*]，其中 *x*[*i*: *i* + *m*] 是 *x* 中大小为 *m* 的一个片段。这个操作像滑动窗口一样重复，直到计算出所有的输出元素。下图展示了一个例子，其中
    *x* = [3 2 1 7 1 2 5 4]，并且 ![](img/B13208_15_019.png)，计算得到前三个输出元素：'
- en: '![](img/B13208_15_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_03.png)'
- en: You can see in the preceding example that the padding size is zero (*p* = 0).
    Notice that the rotated filter, ![](img/B13208_15_020.png), is shifted by two
    cells each time we **shift**. This shift is another hyperparameter of a convolution,
    the **stride**, *s*. In this example, the stride is two, *s* = 2\. Note that the
    stride has to be a positive number smaller than the size of the input vector.
    We will talk more about padding and strides in the next section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在前面的示例中看到填充大小为零（*p* = 0）。注意，旋转后的滤波器 ![](img/B13208_15_020.png) 每次**平移**时都移动了两个单元。这种平移是卷积的另一个超参数，即**步幅**，*s*。在这个例子中，步幅是二，*s*
    = 2。请注意，步幅必须是小于输入向量大小的正数。我们将在下一部分详细讨论填充和步幅。
- en: '**Cross-correlation**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**互相关**'
- en: 'Cross-correlation (or simply correlation) between an input vector and a filter
    is denoted by ![](img/B13208_15_021.png) and is very much like a sibling of a
    convolution, with a small difference: in cross-correlation, the multiplication
    is performed in the same direction. Therefore, it is not a requirement to rotate
    the filter matrix, *w*, in each dimension. Mathematically, cross-correlation is
    defined as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量与滤波器之间的互相关（或简称相关）用 ![](img/B13208_15_021.png) 表示，它与卷积非常相似，唯一的区别是：在互相关中，乘法是在相同方向上进行的。因此，旋转滤波器矩阵*w*在每个维度中并不是必需的。从数学角度讲，互相关定义如下：
- en: '![](img/B13208_15_022.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_022.png)'
- en: The same rules for padding and stride may be applied to cross-correlation as
    well. Note that most deep learning frameworks (including TensorFlow) implement
    cross-correlation but refer to it as convolution, which is a common convention
    in the deep learning field.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 填充和步幅的相同规则也可以应用于互相关。请注意，大多数深度学习框架（包括 TensorFlow）实现了互相关，但称其为卷积，这在深度学习领域是一种常见的约定。
- en: Padding inputs to control the size of the output feature maps
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填充输入以控制输出特征图的大小
- en: So far, we've only used zero-padding in convolutions to compute finite-sized
    output vectors. Technically, padding can be applied with any ![](img/B13208_15_023.png).
    Depending on the choice of *p*, boundary cells may be treated differently than
    the cells located in the middle of *x*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅在卷积中使用了零填充来计算有限大小的输出向量。从技术上讲，可以使用任何 ![](img/B13208_15_023.png) 来应用填充。根据*p*的选择，边界单元可能与位于
    *x* 中间的单元有所不同。
- en: Now, consider an example where *n* = 5 and *m* = 3\. Then, with *p*=0, *x*[0]
    is only used in computing one output element (for instance, *y*[0]), while *x*[1]
    is used in the computation of two output elements (for instance, *y*[0] and *y*[1]).
    So, you can see that this different treatment of elements of *x* can artificially
    put more emphasis on the middle element, *x*[2], since it has appeared in most
    computations. We can avoid this issue if we choose *p* = 2, in which case, each
    element of *x* will be involved in computing three elements of *y*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个例子，其中*n* = 5 和 *m* = 3。那么，在*p* = 0的情况下，*x*[0] 仅用于计算一个输出元素（例如，*y*[0]），而
    *x*[1] 用于计算两个输出元素（例如，*y*[0] 和 *y*[1]）。因此，你可以看到，*x* 中元素的这种不同处理方法可以人为地更加突出中间元素 *x*[2]，因为它出现在大多数计算中。如果选择*p*
    = 2，则可以避免这个问题，在这种情况下，*x* 的每个元素都将参与计算 *y* 的三个元素。
- en: Furthermore, the size of the output, *y*, also depends on the choice of the
    padding strategy we use.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输出的大小*y*也取决于我们使用的填充策略。
- en: 'There are three modes of padding that are commonly used in practice: *full*,
    *same*, and *valid*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，常用的填充模式有三种：*全填充*、*相同填充* 和 *有效填充*：
- en: In full mode, the padding parameter, *p*, is set to *p* = *m* – 1\. Full padding
    increases the dimensions of the output; thus, it is rarely used in CNN architectures.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在全模式下，填充参数 *p* 设置为 *p* = *m* – 1。全填充增加了输出的维度，因此在CNN架构中很少使用。
- en: Same padding is usually used to ensure that the output vector has the same size
    as the input vector, *x*. In this case, the padding parameter, *p*, is computed
    according to the filter size, along with the requirement that the input size and
    output size are the same.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同填充通常用于确保输出向量与输入向量 *x* 的大小相同。在这种情况下，填充参数 *p* 是根据滤波器大小计算的，并且要求输入和输出大小相同。
- en: Finally, computing a convolution in the valid mode refers to the case where
    *p* = 0 (no padding).
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，在有效模式下计算卷积是指*p* = 0（无填充）的情况。
- en: 'The following figure illustrates the three different padding modes for a simple
    ![](img/B13208_15_024.png) pixel input with a kernel size of ![](img/B13208_15_025.png)
    and a stride of 1:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了对于一个简单的 ![](img/B13208_15_024.png) 像素输入，卷积核大小为 ![](img/B13208_15_025.png)，步长为1时的三种不同填充模式：
- en: '![](img/B13208_15_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_04.png)'
- en: The most commonly used padding mode in CNNs is same padding. One of its advantages
    over the other padding modes is that same padding preserves the size of the vector—or
    the height and width of the input images when we are working on image-related
    tasks in computer vision—which makes designing a network architecture more convenient.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNN）中，最常用的填充模式是同填充（same padding）。它相对于其他填充模式的一个优点是，它能够保持向量的大小——或者在处理与图像相关的任务时保持输入图像的高度和宽度——这使得设计网络架构更加方便。
- en: One big disadvantage of valid padding versus full and same padding, for example,
    is that the volume of the tensors will decrease substantially in NNs with many
    layers, which can be detrimental to the network performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，valid填充相对于full和same填充的一个大缺点是，在具有多个层的神经网络中，张量的体积会显著减少，这可能会对网络性能产生不利影响。
- en: In practice, it is recommended that you preserve the spatial size using same
    padding for the convolutional layers and decrease the spatial size via pooling
    layers instead. As for full padding, its size results in an output larger than
    the input size. Full padding is usually used in signal processing applications
    where it is important to minimize boundary effects. However, in the deep learning
    context, boundary effects are usually not an issue, so we rarely see full padding
    being used in practice.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，建议使用同填充（same padding）来保持卷积层的空间大小，而通过池化层来减少空间大小。至于全填充（full padding），它的大小导致输出大于输入大小。全填充通常用于信号处理应用中，在这些应用中，最小化边界效应很重要。然而，在深度学习中，边界效应通常不是问题，因此我们很少看到全填充被实际使用。
- en: Determining the size of the convolution output
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定卷积输出的大小
- en: 'The output size of a convolution is determined by the total number of times
    that we shift the filter, *w*, along the input vector. Let''s assume that the
    input vector is of size *n* and the filter is of size *m*. Then, the size of the
    output resulting from ![](img/B13208_15_026.png), with padding, *p*, and stride,
    *s*, would be determined as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的输出大小由我们沿输入向量移动滤波器 *w* 的次数决定。假设输入向量的大小为 *n*，滤波器的大小为 *m*，那么带有填充 *p* 和步长 *s*
    的输出大小可以通过以下公式确定：
- en: '![](img/B13208_15_027.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_027.png)'
- en: Here, ![](img/B13208_15_028.png) denotes the *floor* operation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_15_028.png) 表示 *向下取整* 操作。
- en: '**The floor operation**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**向下取整操作**'
- en: 'The floor operation returns the largest integer that is equal to or smaller
    than the input, for example:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 向下取整操作返回不大于输入值的最大整数，例如：
- en: '![](img/B13208_15_029.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_029.png)'
- en: 'Consider the following two cases:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两种情况：
- en: Compute the output size for an input vector of size 10 with a convolution kernel
    of size 5, padding 2, and stride 1:![](img/B13208_15_030.png)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算输入向量大小为10，卷积核大小为5，填充为2，步长为1时的输出大小：![](img/B13208_15_030.png)
- en: (Note that in this case, the output size turns out to be the same as the input;
    therefore, we can conclude this to be the same-padding mode.)
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （请注意，在这种情况下，输出大小与输入相同；因此，我们可以得出结论，这就是同填充模式。）
- en: How does the output size change for the same input vector when we have a kernel
    of size 3 and stride 2?![](img/B13208_15_031.png)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们有一个大小为3的卷积核和步长为2时，对于相同的输入向量，输出大小如何变化？![](img/B13208_15_031.png)
- en: If you are interested in learning more about the size of the convolution output,
    we recommend the manuscript *A guide to convolution arithmetic for deep learning*,
    by *Vincent Dumoulin* and *Francesco Visin*, which is freely available at [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣深入了解卷积输出的大小，我们推荐阅读 *Vincent Dumoulin* 和 *Francesco Visin* 的手稿《深度学习卷积算术指南》，该文档可以在
    [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285) 免费获取。
- en: 'Finally, in order to learn how to compute convolutions in one dimension, a
    naive implementation is shown in the following code block, and the results are
    compared with the `numpy.convolve` function. The code is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了学习如何计算一维卷积，以下代码块展示了一个简单的实现，并将其结果与 `numpy.convolve` 函数进行比较。代码如下：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So far, we have mostly focused on convolutions for vectors (1D convolutions).
    We started with the 1D case to make the concepts easier to understand. In the
    next section, we will cover 2D convolutions in more detail, which are the building
    blocks of CNNs for image-related tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注的是向量的卷积（1D卷积）。我们从1D卷积开始，以便使概念更易于理解。在下一节中，我们将更详细地介绍二维卷积，它们是卷积神经网络（CNN）在与图像相关任务中的基本构建模块。
- en: Performing a discrete convolution in 2D
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行二维离散卷积
- en: 'The concepts you learned in the previous sections are easily extendible to
    2D. When we deal with 2D inputs, such as a matrix, ![](img/B13208_15_032.png),
    and the filter matrix, ![](img/B13208_15_033.png), where ![](img/B13208_15_034.png)
    and ![](img/B13208_15_035.png), then the matrix ![](img/B13208_15_036.png) is
    the result of a 2D convolution between *X* and *W*. This is defined mathematically
    as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你在前几节中学到的概念可以很容易地扩展到二维。当我们处理二维输入，如矩阵 ![](img/B13208_15_032.png) 和滤波器矩阵 ![](img/B13208_15_033.png)，其中
    ![](img/B13208_15_034.png) 和 ![](img/B13208_15_035.png)，那么矩阵 ![](img/B13208_15_036.png)
    就是 *X* 和 *W* 之间的二维卷积的结果。数学上可以定义为如下：
- en: '![](img/B13208_15_037.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_037.png)'
- en: 'Notice that if you omit one of the dimensions, the remaining formula is exactly
    the same as the one we used previously to compute the convolution in 1D. In fact,
    all the previously mentioned techniques, such as zero-padding, rotating the filter
    matrix, and the use of strides, are also applicable to 2D convolutions, provided
    that they are extended to both dimensions independently. The following figure
    demonstrates 2D convolution of an input matrix of size ![](img/B13208_15_038.png),
    using a kernel of size ![](img/B13208_15_039.png). The input matrix is padded
    with zeros with *p* = 1\. As a result, the output of the 2D convolution will have
    a size of ![](img/B13208_15_040.png):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你省略了其中一个维度，剩下的公式与我们之前用于计算1D卷积的公式完全相同。实际上，之前提到的所有技术，如零填充、旋转滤波器矩阵和步幅的使用，也适用于二维卷积，只要它们被独立地扩展到两个维度。下图展示了对大小为
    ![](img/B13208_15_038.png) 的输入矩阵进行二维卷积，使用的卷积核大小为 ![](img/B13208_15_039.png)。输入矩阵在零填充下，*p*
    = 1。结果，二维卷积的输出将具有大小 ![](img/B13208_15_040.png)：
- en: '![](img/B13208_15_05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_05.png)'
- en: 'The following example illustrates the computation of a 2D convolution between
    an input matrix, ![](img/B13208_15_041.png), and a kernel matrix, ![](img/B13208_15_042.png),
    using padding *p* = (1, 1) and stride *s* = (2, 2). According to the specified
    padding, one layer of zeros is added on each side of the input matrix, which results
    in the padded matrix ![](img/B13208_15_043.png), as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下例说明了如何计算输入矩阵 ![](img/B13208_15_041.png) 和卷积核矩阵 ![](img/B13208_15_042.png) 之间的二维卷积，使用填充
    *p* = (1, 1) 和步幅 *s* = (2, 2)。根据指定的填充方式，输入矩阵的每一侧都会添加一层零，得到填充后的矩阵 ![](img/B13208_15_043.png)，如下所示：
- en: '![](img/B13208_15_06.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_06.png)'
- en: 'With the preceding filter, the rotated filter will be:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述滤波器，旋转后的滤波器为：
- en: '![](img/B13208_15_044.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_044.png)'
- en: 'Note that this rotation is not the same as the transpose matrix. To get the
    rotated filter in NumPy, we can write `W_rot=W[::-1,::-1]`. Next, we can shift
    the rotated filter matrix along the padded input matrix, ![](img/B13208_15_045.png),
    like a sliding window and compute the sum of the element-wise product, which is
    denoted by the ![](img/B13208_15_046.png) operator in the following figure:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种旋转与转置矩阵不同。为了在NumPy中得到旋转后的滤波器，我们可以写`W_rot=W[::-1,::-1]`。接下来，我们可以将旋转后的滤波器矩阵沿着填充的输入矩阵
    ![](img/B13208_15_045.png) 移动，像滑动窗口一样，并计算元素乘积的和，这个操作在下图中用 ![](img/B13208_15_046.png)
    表示：
- en: '![](img/B13208_15_07.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_07.png)'
- en: The result will be the ![](img/B13208_15_047.png) matrix, *Y*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是 ![](img/B13208_15_047.png) 矩阵，*Y*。
- en: 'Let''s also implement the 2D convolution according to the *naive* algorithm
    described. The `scipy.signal` package provides a way to compute 2D convolution
    via the `scipy.signal.convolve2d` function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以根据所描述的*简单*算法实现二维卷积。`scipy.signal`包提供了一种通过`scipy.signal.convolve2d`函数计算二维卷积的方法：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Efficient algorithms for computing convolution**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算卷积的高效算法**'
- en: We provided a naive implementation to compute a 2D convolution for the purpose
    of understanding the concepts. However, this implementation is very inefficient
    in terms of memory requirements and computational complexity. Therefore, it should
    not be used in real-world NN applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个简单的实现来计算二维卷积，以便帮助理解相关概念。然而，这个实现从内存需求和计算复杂度的角度来看非常低效。因此，它不应该在实际的神经网络应用中使用。
- en: One aspect is that the filter matrix is actually not rotated in most tools like
    TensorFlow. Moreover, in recent years, much more efficient algorithms have been
    developed that use the Fourier transform to compute convolutions. It is also important
    to note that in the context of NNs, the size of a convolution kernel is usually
    much smaller than the size of the input image.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一方面是，滤波矩阵在大多数工具（如TensorFlow）中实际上并不会旋转。此外，近年来，已经开发出更多高效的算法，利用傅里叶变换来计算卷积。还需要注意的是，在神经网络的背景下，卷积核的大小通常远小于输入图像的大小。
- en: For example, modern CNNs usually use kernel sizes such as ![](img/B13208_15_048.png),
    ![](img/B13208_15_049.png), or ![](img/B13208_15_050.png), for which efficient
    algorithms have been designed that can carry out the convolutional operations
    much more efficiently, such as **Winograd's minimal filtering** algorithm. These
    algorithms are beyond the scope of this book, but if you are interested in learning
    more, you can read the manuscript *Fast Algorithms for Convolutional Neural Networks*,
    by *Andrew Lavin* and *Scott Gray*, 2015, which is freely available at [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现代CNN通常使用如 ![](img/B13208_15_048.png)、![](img/B13208_15_049.png) 或 ![](img/B13208_15_050.png)
    等卷积核大小，针对这些卷积操作，已经设计了高效的算法，使得卷积操作能够更高效地执行，如**Winograd最小滤波**算法。这些算法超出了本书的范围，但如果你有兴趣了解更多，可以阅读*Andrew
    Lavin*和*Scott Gray*于2015年发布的手稿《卷积神经网络的快速算法》（*Fast Algorithms for Convolutional
    Neural Networks*），该文献可以在[https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308)免费下载。
- en: In the next section, we will discuss subsampling or pooling, which is another
    important operation often used in CNNs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论下采样或池化，这是CNN中常用的另一种重要操作。
- en: Subsampling layers
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下采样层
- en: 'Subsampling is typically applied in two forms of pooling operations in CNNs:
    **max-pooling** and **mean-pooling** (also known as **average-pooling**). The
    pooling layer is usually denoted by ![](img/B13208_15_051.png). Here, the subscript
    determines the size of the neighborhood (the number of adjacent pixels in each
    dimension) where the max or mean operation is performed. We refer to such a neighborhood
    as the **pooling size**.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样通常应用于卷积神经网络（CNN）中的两种池化操作：**最大池化**和**均值池化**（也称为**平均池化**）。池化层通常用 ![](img/B13208_15_051.png)
    表示。在这里，下标决定了执行最大或均值操作的邻域大小（每个维度中相邻像素的数量）。我们将这样的邻域称为**池化大小**。
- en: 'The operation is described in the following figure. Here, max-pooling takes
    the maximum value from a neighborhood of pixels, and mean-pooling computes their
    average:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 操作在以下图中描述。在这里，最大池化从像素的邻域中提取最大值，而均值池化计算它们的平均值：
- en: '![](img/B13208_15_08.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_08.png)'
- en: 'The advantage of pooling is twofold:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 池化的优势有两个方面：
- en: Pooling (max-pooling) introduces a local invariance. This means that small changes
    in a local neighborhood do not change the result of max-pooling.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化（最大池化）引入了局部不变性。这意味着局部邻域的微小变化不会改变最大池化的结果。
- en: 'Therefore, it helps with generating features that are more robust to noise
    in the input data. Refer to the following example, which shows that the max-pooling
    of two different input matrices, ![](img/B13208_15_052.png) and ![](img/B13208_15_053.png),
    results in the same output:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，它有助于生成对输入数据噪声更加鲁棒的特征。参见以下示例，显示了两个不同输入矩阵的最大池化结果，![](img/B13208_15_052.png)
    和 ![](img/B13208_15_053.png)，它们产生了相同的输出：
- en: '![](img/B13208_15_054.png)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B13208_15_054.png)'
- en: Pooling decreases the size of features, which results in higher computational
    efficiency. Furthermore, reducing the number of features may reduce the degree
    of overfitting as well.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化减少了特征的大小，从而提高了计算效率。此外，减少特征数量可能还会降低过拟合的程度。
- en: '**Overlapping versus non-overlapping pooling**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**重叠池化与非重叠池化**'
- en: Traditionally, pooling is assumed to be non-overlapping. Pooling is typically
    performed on non-overlapping neighborhoods, which can be done by setting the stride
    parameter equal to the pooling size. For example, a non-overlapping pooling layer,
    ![](img/B13208_15_055.png), requires a stride parameter ![](img/B13208_15_056.png).
    On the other hand, overlapping pooling occurs if the stride is smaller than the
    pooling size. An example where overlapping pooling is used in a convolutional
    network is described in *ImageNet Classification with Deep Convolutional Neural
    Networks*, by *A. Krizhevsky*, *I. Sutskever*, and *G. Hinton*, 2012, which is
    freely available as a manuscript at [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，池化假设是非重叠的。池化通常在非重叠的邻域上执行，可以通过将步幅（stride）参数设置为池化大小来实现。例如，非重叠池化层![](img/B13208_15_055.png)需要步幅参数![](img/B13208_15_056.png)。另一方面，如果步幅小于池化大小，则会发生重叠池化。在卷积网络中使用重叠池化的一个例子，详见
    *A. Krizhevsky*、*I. Sutskever* 和 *G. Hinton* 在2012年发表的论文 *ImageNet Classification
    with Deep Convolutional Neural Networks*，该论文可在[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks)免费下载。
- en: While pooling is still an essential part of many CNN architectures, several
    CNN architectures have also been developed without using pooling layers. Instead
    of using pooling layers to reduce the feature size, researchers use convolutional
    layers with a stride of 2\.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管池化仍然是许多CNN架构的一个重要组成部分，但也有一些CNN架构在没有使用池化层的情况下被开发出来。研究人员使用具有步幅为2\的卷积层来代替池化层，以减少特征的大小。
- en: 'In a sense, you can think of a convolutional layer with stride 2 as a pooling
    layer with learnable weights. If you are interested in an empirical comparison
    of different CNN architectures developed with and without pooling layers, we recommend
    reading the research article *Striving for Simplicity: The All Convolutional Net*,
    by *Jost Tobias* *Springenberg*, *Alexey Dosovitskiy*, *Thomas Brox*, and *Martin
    Riedmiller*. This article is freely available at [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '从某种意义上讲，你可以将步幅为2的卷积层视为具有可学习权重的池化层。如果你对开发有无池化层的不同CNN架构的实证比较感兴趣，我们推荐阅读研究文章 *Striving
    for Simplicity: The All Convolutional Net*，作者 *Jost Tobias Springenberg*、*Alexey
    Dosovitskiy*、*Thomas Brox* 和 *Martin Riedmiller*。该文章可以在[https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806)免费下载。'
- en: Putting everything together – implementing a CNN
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切整合——实现CNN
- en: So far, you have learned about the basic building blocks of CNNs. The concepts
    illustrated in this chapter are not really more difficult than traditional multilayer
    NNs. We can say that the most important operation in a traditional NN is matrix
    multiplication. For instance, we use matrix multiplications to compute the pre-activations
    (or net inputs), as in *z* = *Wx* + *b*. Here, *x* is a column vector (![](img/B13208_15_117.png)
    matrix) representing pixels, and *W* is the weight matrix connecting the pixel
    inputs to each hidden unit.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了CNN的基本组成部分。本章所述的概念实际上并不比传统的多层神经网络（NN）更加复杂。我们可以说，传统神经网络中最重要的操作是矩阵乘法。例如，我们使用矩阵乘法来计算预激活（或净输入），如
    *z* = *Wx* + *b*。这里，*x* 是表示像素的列向量（![](img/B13208_15_117.png)矩阵），*W* 是将像素输入与每个隐藏单元连接的权重矩阵。
- en: In a CNN, this operation is replaced by a convolution operation, as in ![](img/B13208_15_057.png),
    where *X* is a matrix representing the pixels in a ![](img/B13208_15_058.png)
    arrangement. In both cases, the pre-activations are passed to an activation function
    to obtain the activation of a hidden unit, ![](img/B13208_15_059.png), where ![](img/B13208_15_060.png)
    is the activation function. Furthermore, you will recall that subsampling is another
    building block of a CNN, which may appear in the form of pooling, as was described
    in the previous section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNN）中，这一操作通过卷积操作来替代，如图中的![](img/B13208_15_057.png)，其中 *X* 是表示像素的矩阵，采用![](img/B13208_15_058.png)排列方式。在这两种情况下，预激活值会传递到激活函数，以获得隐藏单元的激活值，即![](img/B13208_15_059.png)，其中![](img/B13208_15_060.png)是激活函数。此外，你还记得，子采样是CNN的另一个组成部分，通常以池化（pooling）的形式出现，如前一节所述。
- en: Working with multiple input or color channels
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理多个输入或颜色通道
- en: An input to a convolutional layer may contain one or more 2D arrays or matrices
    with dimensions ![](img/B13208_15_061.png) (for example, the image height and
    width in pixels). These ![](img/B13208_15_062.png) matrices are called *channels*.
    Conventional implementations of convolutional layers expect a rank-3 tensor representation
    as an input, for example a three-dimensional array, ![](img/B13208_15_063.png),
    where ![](img/B13208_15_064.png) is the number of input channels. For example,
    let's consider images as input to the first layer of a CNN. If the image is colored
    and uses the RGB color mode, then ![](img/B13208_15_065.png) (for the red, green,
    and blue color channels in RGB). However, if the image is in grayscale, then we
    have ![](img/B13208_15_066.png), because there is only one channel with the grayscale
    pixel intensity values.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输入可能包含一个或多个二维数组或矩阵，尺寸为 ![](img/B13208_15_061.png)（例如，图像的高度和宽度，以像素为单位）。这些
    ![](img/B13208_15_062.png) 矩阵被称为 *通道*。常规的卷积层实现期望输入为三维张量表示，例如三维数组 ![](img/B13208_15_063.png)，其中
    ![](img/B13208_15_064.png) 是输入通道的数量。例如，假设我们将图像作为 CNN 第一层的输入。如果图像是彩色的并使用 RGB 色彩模式，则
    ![](img/B13208_15_065.png)（表示 RGB 中的红、绿、蓝色通道）。然而，如果图像是灰度图像，则只有一个通道，即 ![](img/B13208_15_066.png)，其中包含灰度像素强度值。
- en: '**Reading an image file**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**读取图像文件**'
- en: When we work with images, we can read images into NumPy arrays using the `uint8`
    (unsigned 8-bit integer) data type to reduce memory usage compared to 16-bit,
    32-bit, or 64-bit integer types, for example.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理图像时，我们可以使用 `uint8`（无符号 8 位整数）数据类型将图像读取为 NumPy 数组，相比于 16 位、32 位或 64 位整数类型，这样可以减少内存使用。
- en: Unsigned 8-bit integers take values in the range [0, 255], which are sufficient
    to store the pixel information in RGB images, which also take values in the same
    range.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号 8 位整数的取值范围为 [0, 255]，这个范围足以存储 RGB 图像的像素信息，而 RGB 图像的像素值也在这个范围内。
- en: 'In *Chapter 13*, *Parallelizing Neural Network Training with TensorFlow*, you
    saw that TensorFlow provides a module for loading/storing and manipulating images
    via `tf.io` and `tf.image` submodules. Let''s recap how to read an image (this
    example RGB image is located in the code bundle folder that is provided with this
    chapter at [https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/code/ch15](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/code/ch15)):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第13章*，《使用 TensorFlow 并行化神经网络训练》中，你看到 TensorFlow 提供了一个模块，通过 `tf.io` 和 `tf.image`
    子模块加载/存储和操作图像。让我们回顾一下如何读取图像（这个 RGB 图像位于代码包文件夹中，章节提供的 [https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/code/ch15](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/code/ch15)）：
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When you build models and data loaders in TensorFlow, it is recommended that
    you use `tf.image` as well to read in the input images.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中构建模型和数据加载器时，建议也使用 `tf.image` 来读取输入图像。
- en: 'Now, let''s also look at an example of how we can read in an image into our
    Python session using the `imageio` package. We can install `imageio` either via
    `conda` or `pip` from the command-line terminal:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一个如何在 Python 会话中读取图像的例子，使用 `imageio` 包。我们可以通过 `conda` 或 `pip` 在命令行终端中安装
    `imageio`：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: or
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once `imageio` is installed, we can use the `imread` function to read in the
    same image we used previously by using the `imageio` package:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 `imageio`，我们可以使用 `imread` 函数通过 `imageio` 包读取我们之前使用的相同图像：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that you are familiar with the structure of input data, the next question
    is, how can we incorporate multiple input channels in the convolution operation
    that we discussed in the previous sections? The answer is very simple: we perform
    the convolution operation for each channel separately and then add the results
    together using the matrix summation. The convolution associated with each channel
    (*c*) has its own kernel matrix as *W*[:, :, *c*]. The total pre-activation result
    is computed in the following formula:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了输入数据的结构，接下来的问题是，如何将多个输入通道融入我们在前面讨论的卷积操作中？答案很简单：我们为每个通道分别执行卷积操作，然后通过矩阵求和将结果加起来。与每个通道
    (*c*) 相关的卷积有其自己的卷积核矩阵，记为 *W*[:, :, *c*]。总的预激活结果可以通过以下公式计算：
- en: '![](img/B13208_15_068.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_068.png)'
- en: 'The final result, *A*, is a feature map. Usually, a convolutional layer of
    a CNN has more than one feature map. If we use multiple feature maps, the kernel
    tensor becomes four-dimensional: ![](img/B13208_15_069.png). Here, ![](img/B13208_15_070.png)
    is the kernel size, ![](img/B13208_15_071.png) is the number of input channels,
    and ![](img/B13208_15_072.png) is the number of output feature maps. So, now let''s
    include the number of output feature maps in the preceding formula and update
    it, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果，*A*，是一个特征图。通常，CNN的卷积层有多个特征图。如果我们使用多个特征图，卷积核张量将变为四维：![](img/B13208_15_069.png)。这里，![](img/B13208_15_070.png)
    是卷积核的大小，![](img/B13208_15_071.png) 是输入通道的数量，![](img/B13208_15_072.png) 是输出特征图的数量。因此，现在我们在前面的公式中加入输出特征图的数量并更新它，如下所示：
- en: '![](img/B13208_15_073.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_073.png)'
- en: 'To conclude our discussion of computing convolutions in the context of NNs,
    let''s look at the example in the following figure, which shows a convolutional
    layer, followed by a pooling layer. In this example, there are three input channels.
    The kernel tensor is four-dimensional. Each kernel matrix is denoted as ![](img/B13208_15_074.png),
    and there are three of them, one for each input channel. Furthermore, there are
    five such kernels, accounting for five output feature maps. Finally, there is
    a pooling layer for subsampling the feature maps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们在神经网络中计算卷积的讨论，让我们来看一下下面的例子，展示了一个卷积层，后面跟着一个池化层。在这个例子中，有三个输入通道。卷积核张量是四维的。每个卷积核矩阵表示为
    ![](img/B13208_15_074.png)，并且有三个卷积核，每个输入通道一个。此外，有五个这样的卷积核，对应五个输出特征图。最后，池化层用于对特征图进行下采样：
- en: '![](img/B13208_15_09.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_09.png)'
- en: '**How many trainable parameters exist in the preceding example?**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**在上述例子中有多少可训练参数？**'
- en: 'To illustrate the advantages of convolution, **parameter-sharing**, and **sparse
    connectivity**, let''s work through an example. The convolutional layer in the
    network shown in the preceding figure is a four-dimensional tensor. So, there
    are ![](img/B13208_15_075.png) parameters associated with the kernel. Furthermore,
    there is a bias vector for each output feature map of the convolutional layer.
    Thus, the size of the bias vector is 5\. Pooling layers do not have any (trainable)
    parameters; therefore, we can write the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明卷积、**参数共享**和**稀疏连接**的优势，我们通过一个例子来说明。前面图中显示的网络中的卷积层是一个四维张量。因此，与卷积核相关的参数数量为
    ![](img/B13208_15_075.png)。此外，对于每个卷积层的输出特征图，还有一个偏置向量。因此，偏置向量的大小为5。池化层没有任何（可训练的）参数；因此，我们可以写出以下公式：
- en: '![](img/B13208_15_076.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_076.png)'
- en: If the input tensor is of size ![](img/B13208_15_077.png), assuming that the
    convolution is performed with the same-padding mode, then the output feature maps
    would be of size ![](img/B13208_15_078.png).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入张量的大小为 ![](img/B13208_15_077.png)，假设卷积采用相同填充模式，则输出特征图的大小将为 ![](img/B13208_15_078.png)。
- en: 'Note that if we use a fully connected layer instead of a convolutional layer,
    this number will be much larger. In the case of a fully connected layer, the number
    of parameters for the weight matrix to reach the same number of output units would
    have been as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们使用全连接层代替卷积层，则这个数字会大得多。在全连接层的情况下，为了达到相同数量的输出单元，权重矩阵的参数数量将如下所示：
- en: '![](img/B13208_15_079.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_079.png)'
- en: In addition, the size of the bias vector is ![](img/B13208_15_080.png) (one
    bias element for each output unit). Given that ![](img/B13208_15_081.png) and
    ![](img/B13208_15_082.png), we can see that the difference in the number of trainable
    parameters is significant.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，偏置向量的大小为 ![](img/B13208_15_080.png)（每个输出单元有一个偏置元素）。由于 ![](img/B13208_15_081.png)
    和 ![](img/B13208_15_082.png)，我们可以看到可训练参数数量的差异是显著的。
- en: 'Lastly, as was already mentioned, typically, the convolution operations are
    carried out by treating an input image with multiple color channels as a stack
    of matrices; that is, we perform the convolution on each matrix separately and
    then add the results, as was illustrated in the previous figure. However, convolutions
    can also be extended to 3D volumes if you are working with 3D datasets, for example,
    as shown in the paper *VoxNet: A 3D Convolutional Neural Network for Real-Time
    Object Recognition* (2015), by *Daniel Maturana* and *Sebastian Scherer*, which
    can be accessed at [https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf](https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，正如之前提到的，通常卷积操作是通过将具有多个颜色通道的输入图像视为矩阵堆栈来执行的；也就是说，我们分别在每个矩阵上执行卷积，然后将结果相加，正如之前的图示所示。然而，卷积也可以扩展到3D体积，如果你处理的是3D数据集，例如，如*Daniel
    Maturana*和*Sebastian Scherer*于2015年发表的论文*VoxNet: A 3D Convolutional Neural Network
    for Real-Time Object Recognition*中所示（可以通过[https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf](https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf)访问）。'
- en: In the next section, we will talk about how to regularize an NN.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何对神经网络进行正则化。
- en: Regularizing an NN with dropout
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用dropout正则化神经网络
- en: Choosing the size of a network, whether we are dealing with a traditional (fully
    connected) NN or a CNN, has always been a challenging problem. For instance, the
    size of a weight matrix and the number of layers need to be tuned to achieve a
    reasonably good performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 选择网络的大小，无论是传统的（全连接的）神经网络（NN）还是卷积神经网络（CNN），一直是一个具有挑战性的问题。例如，权重矩阵的大小和层数需要调整，以实现合理的性能。
- en: You will recall from *Chapter 14*, *Going Deeper – The Mechanics of TensorFlow*,
    that a simple network without any hidden layer could only capture a linear decision
    boundary, which is not sufficient for dealing with an exclusive or (XOR) or similar
    problem. The *capacity* of a network refers to the level of complexity of the
    function that it can learn to approximate. Small networks, or networks with a
    relatively small number of parameters, have a low capacity and are therefore likely
    to *underfit*, resulting in poor performance, since they cannot learn the underlying
    structure of complex datasets. However, very large networks may result in *overfitting*,
    where the network will memorize the training data and do extremely well on the
    training dataset while achieving a poor performance on the held-out test dataset.
    When we deal with real-world machine learning problems, we do not know how large
    the network should be *a priori*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得在*第14章*，*深入探索——TensorFlow的机制*中提到，只有一个简单的没有隐藏层的网络只能捕捉到一个线性决策边界，这不足以应对异或（XOR）或类似的问题。网络的*容量*是指它能够学习并逼近的函数的复杂度。小型网络或具有相对较少参数的网络容量较低，因此可能会*欠拟合*，导致性能较差，因为它们无法学习复杂数据集的潜在结构。然而，过大的网络可能会导致*过拟合*，即网络会记住训练数据，在训练数据集上表现极其优秀，但在保留的测试数据集上表现差劲。当我们处理现实世界的机器学习问题时，我们并不知道网络应该有多大*先验*。
- en: One way to address this problem is to build a network with a relatively large
    capacity (in practice, we want to choose a capacity that is slightly larger than
    necessary) to do well on the training dataset. Then, to prevent overfitting, we
    can apply one or multiple regularization schemes to achieve a good generalization
    performance on new data, such as the held-out test dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是构建一个容量相对较大的网络（实际上，我们希望选择一个稍微大于必要的容量）以便在训练数据集上表现良好。然后，为了防止过拟合，我们可以应用一种或多种正则化方法，以在新数据上实现良好的泛化性能，例如保留的测试数据集。
- en: 'In *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*,
    we covered L1 and L2 regularization. In the section *Tackling overfitting via
    regularization*, you saw that both techniques, L1 and L2 regularization, can prevent
    or reduce the effect of overfitting by adding a penalty to the loss that results
    in shrinking the weight parameters during training. While both L1 and L2 regularization
    can be used for NNs as well, with L2 being the more common choice of the two,
    there are other methods for regularizing NNs, such as dropout, which we discuss
    in this section. But before we move on to discussing dropout, to use L2 regularization
    within a convolutional or fully connected (dense) network, you can simply add
    the L2 penalty to the loss function by setting the `kernel_regularizer` of a particular
    layer when using the Keras API, as follows (it will then automatically modify
    the loss function accordingly):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*使用scikit-learn的机器学习分类器简介*中，我们讨论了L1和L2正则化。在*通过正则化应对过拟合*一节中，您看到，L1和L2正则化都可以通过在损失函数中加入惩罚项来防止或减少过拟合的影响，从而在训练过程中缩小权重参数。虽然L1和L2正则化同样适用于神经网络（NNs），且L2是更常见的选择，但还有其他用于正则化神经网络的方法，例如dropout（丢弃法），我们将在本节中讨论。在我们继续讨论dropout之前，要在卷积网络或全连接（稠密）网络中使用L2正则化，您可以通过在使用Keras
    API时设置某一层的`kernel_regularizer`，简单地将L2惩罚项添加到损失函数中，如下所示（它将自动相应地修改损失函数）：
- en: '[PRE6]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In recent years, **dropout** has emerged as a popular technique for regularizing
    (deep) NNs to avoid overfitting, thus improving the generalization performance
    (*Dropout: a simple way to prevent neural networks from overfitting*, by *N. Srivastava,
    G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov*, *Journal of Machine
    Learning Research 15.1*, pages 1929-1958, 2014, [http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)).
    Dropout is usually applied to the hidden units of higher layers and works as follows:
    during the training phase of an NN, a fraction of the hidden units is randomly
    dropped at every iteration with probability ![](img/B13208_15_083.png) (or keep
    probability ![](img/B13208_15_084.png)). This dropout probability is determined
    by the user and the common choice is *p* = 0.5, as discussed in the previously
    mentioned article by Nitish Srivastava and others, 2014\. When dropping a certain
    fraction of input neurons, the weights associated with the remaining neurons are
    rescaled to account for the missing (dropped) neurons.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，**dropout**成为一种流行的正则化（深度）神经网络技术，用于避免过拟合，从而提高泛化性能（*Dropout: a simple way
    to prevent neural networks from overfitting*，作者：*N. Srivastava, G. Hinton, A.
    Krizhevsky, I. Sutskever, 和 R. Salakhutdinov*，*机器学习研究杂志 15.1*，第1929-1958页，2014，[http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)）。Dropout通常应用于较高层次的隐藏单元，其工作原理如下：在神经网络的训练阶段，每次迭代时，按概率![](img/B13208_15_083.png)（或保持概率![](img/B13208_15_084.png)）随机丢弃一部分隐藏单元。这个dropout概率由用户决定，常见的选择是*p*
    = 0.5，正如Nitish Srivastava等人2014年在上述文章中讨论的。当丢弃一定比例的输入神经元时，剩余神经元的权重会进行重新缩放，以补偿丢失的（被丢弃的）神经元。'
- en: The effect of this random dropout is that the network is forced to learn a redundant
    representation of the data. Therefore, the network cannot rely on an activation
    of any set of hidden units, since they may be turned off at any time during training,
    and is forced to learn more general and robust patterns from the data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种随机dropout的效果是迫使网络学习数据的冗余表示。因此，网络不能依赖于任何一组隐藏单元的激活，因为它们可能在训练过程中随时被关闭，网络被迫从数据中学习更一般和更强健的模式。
- en: This random dropout can effectively prevent overfitting. The following figure
    shows an example of applying dropout with probability *p* = 0.5 during the training
    phase, whereby half of the neurons will become inactive randomly (dropped units
    are selected randomly in each forward pass of training). However, during prediction,
    all neurons will contribute to computing the pre-activations of the next layer.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这种随机dropout可以有效防止过拟合。下图展示了在训练阶段应用概率*p* = 0.5的dropout的示例，其中一半的神经元会随机变得不活跃（每次前向传播时随机选择丢弃的单元）。然而，在预测过程中，所有神经元都会参与计算下一层的预激活。
- en: '![](img/B13208_15_10.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_10.png)'
- en: As shown here, one important point to remember is that units may drop randomly
    during training only, whereas for the evaluation (inference) phase, all the hidden
    units must be active (for instance, ![](img/B13208_15_085.png) or ![](img/B13208_15_086.png)).
    To ensure that the overall activations are on the same scale during training and
    prediction, the activations of the active neurons have to be scaled appropriately
    (for example, by halving the activation if the dropout probability was set to
    *p* = 0.5).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，一个需要记住的重要点是，单元在训练过程中可能会随机丢失，而在评估（推理）阶段，所有的隐藏单元都必须保持激活（例如， ![](img/B13208_15_085.png)
    或 ![](img/B13208_15_086.png)）。为了确保在训练和预测时整体激活值在相同的尺度上，必须适当缩放激活的神经元（例如，如果dropout概率设置为
    *p* = 0.5，则需要将激活值减半）。
- en: However, since it is inconvenient to always scale activations when making predictions,
    TensorFlow and other tools scale the activations during training (for example,
    by doubling the activations if the dropout probability was set to *p* = 0.5).
    This approach is commonly referred to as inverse dropout.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于在做预测时总是缩放激活值不方便，TensorFlow等工具会在训练过程中缩放激活值（例如，如果dropout概率设置为 *p* = 0.5，则将激活值加倍）。这种方法通常被称为逆向dropout。
- en: While the relationship is not immediately obvious, dropout can be interpreted
    as the consensus (averaging) of an ensemble of models. As discussed in *Chapter
    7*, *Combining Different Models for Ensemble Learning*, in ensemble learning,
    we train several models independently. During prediction, we then use the consensus
    of all the trained models. We already know that model ensembles are known to perform
    better than single models. In deep learning, however, both training several models
    and collecting and averaging the output of multiple models is computationally
    expensive. Here, dropout offers a workaround, with an efficient way to train many
    models at once and compute their average predictions at test or prediction time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关系并不立即显现，但dropout可以解释为一个模型集成的共识（平均）。正如*第7章*中讨论的，*将不同模型结合用于集成学习*，在集成学习中，我们独立训练多个模型。在预测时，我们使用所有训练过的模型的共识。我们已经知道，模型集成的表现通常优于单一模型。然而，在深度学习中，训练多个模型以及收集和平均多个模型的输出计算代价很高。这里，dropout提供了一种解决方法，它可以高效地一次性训练多个模型，并在测试或预测时计算它们的平均预测。
- en: As mentioned previously, the relationship between model ensembles and dropout
    is not immediately obvious. However, consider that in dropout, we have a different
    model for each mini-batch (due to setting the weights to zero randomly during
    each forward pass).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，模型集成与dropout之间的关系并不立即显现。然而，考虑到在dropout中，我们每个小批量数据都有一个不同的模型（因为在每次前向传播时，随机将部分权重置为零）。
- en: Then, via iterating over the mini-batches, we essentially sample over ![](img/B13208_15_087.png)
    models, where *h* is the number of hidden units.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过对小批量数据进行迭代，我们本质上对 ![](img/B13208_15_087.png) 个模型进行采样，其中 *h* 是隐藏单元的数量。
- en: The restriction and aspect that distinguishes dropout from regular ensembling,
    however, is that we share the weights over these "different models", which can
    be seen as a form of regularization. Then, during "inference" (for instance, predicting
    the labels in the test dataset), we can average over all these different models
    that we sampled over during training. This is very expensive, though.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，dropout与常规集成的一个不同之处在于，我们在这些“不同的模型”之间共享权重，这可以看作是一种正则化形式。然后，在“推理”过程中（例如，预测测试数据集中的标签），我们可以对训练过程中采样的所有不同模型进行平均。这是非常昂贵的。
- en: 'Then, averaging the models, that is, computing the geometric mean of the class-membership
    probability that is returned by a model, *i*, can be computed as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，平均模型，即计算由模型 *i* 返回的类成员概率的几何平均，可以按如下方式计算：
- en: '![](img/B13208_15_088.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_088.png)'
- en: Now, the trick behind dropout is that this geometric mean of the model ensembles
    (here, *M* models) can be approximated by scaling the predictions of the last
    (or final) model sampled during training by a factor of 1/(1 – *p*), which is
    much cheaper than computing the geometric mean explicitly using the previous equation.
    (In fact, the approximation is exactly equivalent to the true geometric mean if
    we consider linear models.)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，dropout背后的技巧是，模型集成的几何平均（这里指的是 *M* 个模型）可以通过将训练过程中最后一个（或最终）模型的预测结果按 1/(1 –
    *p*) 的比例进行缩放来近似，这比使用前面公式显式计算几何平均要便宜得多。（实际上，如果我们考虑线性模型，这个近似与真实的几何平均是完全等价的。）
- en: Loss functions for classification
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类的损失函数
- en: In *Chapter 13*, *Parallelizing Neural Network Training with TensorFlow*, we
    saw different activation functions, such as ReLU, sigmoid, and tanh. Some of these
    activation functions, like ReLU, are mainly used in the intermediate (hidden)
    layers of an NN to add non-linearities to our model. But others, like sigmoid
    (for binary) and softmax (for multiclass), are added at the last (output) layer,
    which results in class-membership probabilities as the output of the model. If
    the sigmoid or softmax activations are not included at the output layer, then
    the model will compute the logits instead of the class-membership probabilities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 13 章*，*使用 TensorFlow 并行化神经网络训练* 中，我们见过不同的激活函数，如 ReLU、sigmoid 和 tanh。像 ReLU
    这样的激活函数，主要用于神经网络的中间（隐藏）层，为模型加入非线性。但其他激活函数，如 sigmoid（用于二分类）和 softmax（用于多分类），则被添加到最后的（输出）层，从而使得模型的输出为类别成员概率。如果输出层没有包含
    sigmoid 或 softmax 激活函数，那么模型将计算 logits，而非类别成员概率。
- en: Focusing on classification problems here, depending on the type of problem (binary
    versus multiclass) and the type of output (logits versus probabilities), we should
    choose the appropriate loss function to train our model. **Binary cross-entropy**
    is the loss function for a binary classification (with a single output unit),
    and **categorical cross-entropy** is the loss function for multiclass classification.
    In the Keras API, two options for categorical cross-entropy loss are provided,
    depending on whether the ground truth labels are in a one-hot encoded format (for
    example, [0, 0, 1, 0]), or provided as integer labels (for example, *y*=2), which
    is also known as "sparse" representation in the context of Keras.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重点讲解分类问题，依据问题类型（二分类与多分类）和输出类型（logits 与概率），我们应选择合适的损失函数来训练我们的模型。**二元交叉熵**是二分类（具有单一输出单元）的损失函数，而**分类交叉熵**是多分类问题的损失函数。在
    Keras API 中，分类交叉熵损失函数提供了两种选项，取决于真实标签是采用 one-hot 编码格式（例如，[0, 0, 1, 0]），还是作为整数标签（例如，*y*=2），在
    Keras 中，这也被称为“稀疏”表示。
- en: 'The following table describes three loss functions available in Keras for dealing
    with all three cases: binary classification, multiclass with one-hot encoded ground
    truth labels, and multiclass with integer (sparse) labels. Each one of these three
    loss functions also has the option to receive the predictions in the form of logits
    or class-membership probabilities:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下表描述了 Keras 中三种可用于处理以下三种情况的损失函数：二分类、多类别（使用 one-hot 编码的真实标签）和多类别（使用整数（稀疏）标签）。这三种损失函数中的每一种也可以选择接受
    logits 或类别成员概率形式的预测：
- en: '![](img/B13208_15_11.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_11.png)'
- en: Please note that computing the cross-entropy loss by providing the logits, and
    not the class-membership probabilities, is usually preferred due to numerical
    stability reasons. If we provide logits as inputs to the loss function and set
    `from_logits=True`, the respective TensorFlow function uses a more efficient implementation
    to compute the loss and derivative of the loss with respect to the weights. This
    is possible since certain mathematical terms cancel and thus don't have to be
    computed explicitly when providing logits as inputs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通常由于数值稳定性的原因，提供 logits 而非类别成员概率来计算交叉熵损失更为优选。如果我们将 logits 作为输入提供给损失函数并设置
    `from_logits=True`，相应的 TensorFlow 函数将使用更高效的实现来计算损失及损失对权重的导数。这是可能的，因为某些数学项会相互抵消，因此在提供
    logits 作为输入时，不需要显式计算这些项。
- en: 'The following code will show you how to use these three loss functions with
    two different formats, where either the logits or class-membership probabilities
    are given as inputs to the loss functions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将展示如何使用这三种损失函数，并且输入可以是 logits 或类别成员概率的两种不同格式：
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that sometimes, you may come across an implementation where a categorical
    cross-entropy loss is used for binary classification. Typically, when we have
    a binary classification task, the model returns a single output value for each
    example. We interpret this single model output as the probability of the positive
    class (for example, class 1), *P*[class = 1]. In a binary classification problem,
    it is implied that *P*[class = 0] = 1 – *P*[class = 1]; hence, we do not need
    a second output unit in order to obtain the probability of the negative class.
    However, sometimes practitioners choose to return two outputs for each training
    example and interpret them as probabilities of each class: *P*[class = 0] versus
    *P*[class = 1]. Then, in such a case, using a softmax function (instead of the
    logistic sigmoid) to normalize the outputs (so that they sum to 1) is recommended,
    and categorical cross-entropy is the appropriate loss function.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有时你可能会遇到一个实现，其中对于二分类任务使用了类别交叉熵损失。通常，当我们进行二分类任务时，模型会为每个示例返回一个单一的输出值。我们将这个单一的模型输出解释为正类的概率（例如，类别
    1），*P*[class = 1]。在二分类问题中，隐含着 *P*[class = 0] = 1 – *P*[class = 1]；因此，我们不需要第二个输出单元来获取负类的概率。然而，有时实践者会选择为每个训练示例返回两个输出，并将它们解释为每个类别的概率：*P*[class
    = 0] 与 *P*[class = 1]。在这种情况下，建议使用 softmax 函数（而不是逻辑 sigmoid）来归一化输出（使它们的和为 1），并且类别交叉熵是合适的损失函数。
- en: Implementing a deep CNN using TensorFlow
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现深度 CNN
- en: In *Chapter 14*, *Going Deeper – The Mechanics of TensorFlow*, you may recall
    that we used TensorFlow Estimators for handwritten digit recognition problems,
    using different API levels of TensorFlow. You may also recall that we achieved
    about 89 percent accuracy using the `DNNClassifier` Estimator with two hidden
    layers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第14章*中，*深入了解 – TensorFlow的机制*，你可能还记得我们使用了 TensorFlow Estimators 来解决手写数字识别问题，使用了不同的
    TensorFlow API 级别。你可能还记得我们通过使用 `DNNClassifier` Estimator 和两层隐藏层，达到了大约 89% 的准确率。
- en: Now, let's implement a CNN and see whether it can achieve a better predictive
    performance compared to the MLP (`DNNClassifier`) for classifying handwritten
    digits. Note that the fully connected layers that we saw in *Chapter 14*, *Going
    Deeper – The Mechanics of TensorFlow*, were able to perform well on this problem.
    However, in some applications, such as reading bank account numbers from handwritten
    digits, even tiny mistakes can be very costly. Therefore, it is crucial to reduce
    this error as much as possible.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个卷积神经网络（CNN），看看它是否能在手写数字分类任务中实现比多层感知机（`DNNClassifier`）更好的预测性能。请注意，在*第14章*中，我们看到的全连接层在这个问题上表现良好。然而，在某些应用中，如从手写数字中读取银行账户号码，即使是微小的错误也可能造成巨大的损失。因此，减少这个错误至关重要。
- en: The multilayer CNN architecture
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层卷积神经网络架构
- en: The architecture of the network that we are going to implement is shown in the
    following figure. The inputs are ![](img/B13208_15_089.png) grayscale images.
    Considering the number of channels (which is 1 for grayscale images) and a batch
    of input images, the input tensor's dimensions will be ![](img/B13208_15_090.png).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实现的网络架构如下图所示。输入为 ![](img/B13208_15_089.png) 灰度图像。考虑到通道数（灰度图像的通道数为 1）以及一批输入图像，输入张量的维度将是
    ![](img/B13208_15_090.png)。
- en: 'The input data goes through two convolutional layers that have a kernel size
    of ![](img/B13208_15_091.png). The first convolution has 32 output feature maps,
    and the second one has 64 output feature maps. Each convolution layer is followed
    by a subsampling layer in the form of a max-pooling operation, ![](img/B13208_15_092.png).
    Then a fully connected layer passes the output to a second fully connected layer,
    which acts as the final *softmax* output layer. The architecture of the network
    that we are going to implement is shown in the following figure:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据经过两个卷积层处理，卷积核大小为 ![](img/B13208_15_091.png)。第一个卷积层有 32 个输出特征图，第二个卷积层有 64
    个输出特征图。每个卷积层后面都有一个子采样层，采用最大池化操作，![](img/B13208_15_092.png)。然后，一个全连接层将输出传递给第二个全连接层，后者作为最终的
    *softmax* 输出层。我们将要实现的网络架构如下图所示：
- en: '![](img/B13208_15_12.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_12.png)'
- en: 'The dimensions of the tensors in each layer are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层张量的维度如下：
- en: 'Input: ![](img/B13208_15_093.png)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '输入: ![](img/B13208_15_093.png)'
- en: 'Conv_1: ![](img/B13208_15_094.png)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv_1: ![](img/B13208_15_094.png)'
- en: 'Pooling_1: ![](img/B13208_15_095.png)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pooling_1: ![](img/B13208_15_095.png)'
- en: 'Conv_2: ![](img/B13208_15_096.png)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv_2: ![](img/B13208_15_096.png)'
- en: 'Pooling_2: ![](img/B13208_15_097.png)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pooling_2: ![](img/B13208_15_097.png)'
- en: 'FC_1: ![](img/B13208_15_098.png)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC_1： ![](img/B13208_15_098.png)
- en: 'FC_2 and softmax layer: ![](img/B13208_15_099.png)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC_2 和 softmax 层： ![](img/B13208_15_099.png)
- en: For the convolutional kernels, we are using `strides=1` such that the input
    dimensions are preserved in the resulting feature maps. For the pooling layers,
    we are using `strides=2` to subsample the image and shrink the size of the output
    feature maps. We will implement this network using the TensorFlow Keras API.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积核，我们使用 `strides=1`，使输入维度在生成的特征图中得到保留。对于池化层，我们使用 `strides=2` 来对图像进行子采样，从而缩小输出特征图的尺寸。我们将使用
    TensorFlow Keras API 实现这个网络。
- en: Loading and preprocessing the data
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和预处理数据
- en: 'You will recall that in *Chapter 13*, *Parallelizing Neural Network Training
    with TensorFlow*, you learned two ways of loading available datasets from the
    `tensorflow_datasets` module. One approach is based on a three-step process, and
    a simpler method uses a function called `load`, which wraps those three steps.
    Here, we will use the first method. The three steps for loading the MNIST dataset
    are as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你会回想起在*第13章*，《*使用 TensorFlow 并行化神经网络训练*》中，你学到了两种从 `tensorflow_datasets` 模块加载可用数据集的方法。一种方法基于一个三步过程，而另一种更简单的方法是使用一个叫
    `load` 的函数，它将这三步封装起来。这里，我们将使用第一种方法。加载 MNIST 数据集的三步过程如下：
- en: '[PRE8]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The MNIST dataset comes with a pre-specified training and test dataset partitioning
    scheme, but we also want to create a validation split from the train partition.
    Notice that in the third step, we used an optional argument, `shuffle_files=False`,
    in the `.as_dataset()` method. This prevented initial shuffling, which is necessary
    for us since we want to split the training dataset into two parts: a smaller training
    dataset and a validation dataset. (Note: if the initial shuffling was not turned
    off, it would incur reshuffling of the dataset every time we fetched a mini-batch
    of data.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集提供了预先指定的训练集和测试集分割方案，但我们还希望从训练集分割出一个验证集。请注意，在第三步中，我们在 `.as_dataset()`
    方法中使用了一个可选参数 `shuffle_files=False`。这防止了数据集的初始打乱，这对于我们来说是必要的，因为我们希望将训练数据集分成两部分：一部分较小的训练集和一部分验证集。（注意：如果没有关闭初始打乱，每次获取小批量数据时，数据集都会重新打乱。）
- en: 'An example of this behavior is shown in the online contents of this chapter,
    where you can see that the number of labels in the validation datasets changes
    due to reshuffling of the train/validation splits. This can cause *false* performance
    estimation of the model, since the train/validation datasets are indeed mixed.)
    We can split the train/validation datasets as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为的一个例子展示在本章的在线内容中，在那里你可以看到，由于训练集/验证集的重新打乱，验证数据集中的标签数量发生了变化。这可能会导致模型的*错误*性能估计，因为训练集/验证集实际上是混合的。我们可以按如下方式划分训练集/验证集：
- en: '[PRE9]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, after preparing the dataset, we are ready to implement the CNN we just
    described.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在准备好数据集后，我们可以实现刚才描述的 CNN。
- en: Implementing a CNN using the TensorFlow Keras API
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Keras API 实现 CNN
- en: 'For implementing a CNN in TensorFlow, we use the Keras `Sequential` class to
    stack different layers, such as convolution, pooling, and dropout, as well as
    the fully connected (dense) layers. The Keras layers API provides classes for
    each one: `tf.keras.layers.Conv2D` for a two-dimensional convolution layer; `tf.keras.layers.MaxPool2D`
    and `tf.keras.layers.AvgPool2D` for subsampling (max-pooling and average-pooling);
    and `tf.keras.layers.Dropout` for regularization using dropout. We will go over
    each of these classes in more detail.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中实现 CNN 时，我们使用 Keras 的 `Sequential` 类来堆叠不同的层，如卷积层、池化层、Dropout 层以及全连接（密集）层。Keras
    层 API 为每个层提供了相应的类：`tf.keras.layers.Conv2D` 用于二维卷积层；`tf.keras.layers.MaxPool2D`
    和 `tf.keras.layers.AvgPool2D` 用于子采样（最大池化和平均池化）；`tf.keras.layers.Dropout` 用于通过
    Dropout 实现正则化。我们将更详细地介绍这些类。
- en: Configuring CNN layers in Keras
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Keras 中配置 CNN 层
- en: Constructing a layer with the `Conv2D` class requires us to specify the number
    of output filters (which is equivalent to the number of output feature maps) and
    kernel sizes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Conv2D` 类构建卷积层时，我们需要指定输出过滤器的数量（这相当于输出特征图的数量）和卷积核的大小。
- en: 'In addition, there are optional parameters that we can use to configure a convolutional
    layer. The most commonly used ones are the strides (with a default value of 1
    in both *x*, *y* dimensions) and padding, which could be same or valid. Additional
    configuration parameters are listed in the official documentation: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些可选的参数可以用来配置卷积层。最常用的参数是步幅（`strides`，在* x *和* y *维度上的默认值为1）和填充（padding），填充可以是`same`或`valid`。更多的配置参数列在官方文档中：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D)。
- en: It is worth mentioning that usually, when we read an image, the default dimension
    for the channels is the last dimension of the tensor array. This is called the
    "NHWC" format, where *N* stands for the number of images within the batch, *H*
    and *W* stand for height and width, respectively, and *C* stands for channels.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，通常当我们读取图像时，通道的默认维度是张量数组的最后一维。这被称为“NHWC”格式，其中*N*表示批次中的图像数量，*H*和*W*分别表示高度和宽度，*C*表示通道。
- en: Note that the `Conv2D` class assumes that inputs are in the NHWC format by default.
    (Other tools, such as PyTorch, use an NCHW format.) However, if you come across
    some data whose channels are placed at the first dimension (the first dimension
    after the batch dimension, or second dimension considering the batch dimension),
    you would need to swap the axes in your data to move the channels to the last
    dimension. Or, an alternative way to work with an NCHW-formatted input is to set
    `data_format="channels_first"`. After the layer is constructed, it can be called
    by providing a four-dimensional tensor, with the first dimension reserved for
    a batch of examples; depending on the `data_format` argument, either the second
    or the fourth dimension corresponds to the channel; and the other two dimensions
    are the spatial dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`Conv2D`类默认假设输入采用NHWC格式。（其他工具，如PyTorch，使用NCHW格式。）然而，如果你遇到一些数据，其通道位于第一维（即批次维度之后的第一维，或者考虑批次维度后为第二维），你需要在数据中交换轴，将通道移至最后一维。或者，另一种处理NCHW格式输入的方式是设置`data_format="channels_first"`。在构建该层后，可以通过提供一个四维张量来调用它，第一个维度保留作为一批样本；根据`data_format`参数，第二维或第四维对应于通道；其他两个维度是空间维度。
- en: As shown in the architecture of the CNN model that we want to build, each convolution
    layer is followed by a pooling layer for subsampling (reducing the size of feature
    maps). The `MaxPool2D` and `AvgPool2D` classes construct the max-pooling and average-pooling
    layers, respectively. The argument `pool_size` determines the size of the window
    (or neighborhood) that will be used to compute the max or mean operations. Furthermore,
    the `strides` parameter can be used to configure the pooling layer, as we discussed
    earlier.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们想要构建的CNN模型架构所示，每个卷积层后面都跟着一个池化层进行子采样（减小特征图的尺寸）。`MaxPool2D`和`AvgPool2D`类分别构造最大池化层和平均池化层。参数`pool_size`决定了用于计算最大值或均值操作的窗口（或邻域）的大小。此外，`strides`参数可以用来配置池化层，正如我们之前讨论的那样。
- en: Finally, the `Dropout` class will construct the dropout layer for regularization,
    with the argument `rate` used to determine the probability of dropping the input
    units during the training. When calling this layer, its behavior can be controlled
    via an argument named `training`, to specify whether this call will be made during
    training or during the inference.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`Dropout`类将构建用于正则化的丢弃层，参数`rate`用于确定在训练过程中丢弃输入单元的概率。调用该层时，可以通过一个名为`training`的参数来控制其行为，以指定该调用是在训练期间进行的还是在推理期间进行的。
- en: Constructing a CNN in Keras
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Keras中构建CNN
- en: 'Now that you have learned about these classes, we can construct the CNN model
    that was shown in the previous figure. In the following code, we will use the
    `Sequential` class and add the convolution and pooling layers:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了这些类，我们可以构建前面图中显示的CNN模型。在以下代码中，我们将使用`Sequential`类并添加卷积层和池化层：
- en: '[PRE10]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So far, we have added two convolution layers to the model. For each convolutional
    layer, we used a kernel of size ![](img/B13208_15_100.png) and `'same'` padding.
    As discussed earlier, using `padding='same'` preserves the spatial dimensions
    (vertical and horizontal dimensions) of the feature maps such that the inputs
    and outputs have the same height and width (and the number of channels may only
    differ in terms of the number of filters used). The max-pooling layers with pooling
    size ![](img/B13208_15_101.png) and strides of 2 will reduce the spatial dimensions
    by half. (Note that if the `strides` parameter is not specified in `MaxPool2D`,
    by default, it is set equal to the pooling size.)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已向模型中添加了两个卷积层。对于每个卷积层，我们使用了大小为 ![](img/B13208_15_100.png) 的卷积核，并采用了
    `'same'` 填充方式。如前所述，使用 `padding='same'` 保留了特征图的空间维度（纵向和横向维度），使得输入和输出具有相同的高度和宽度（而通道数仅可能因所用滤波器的数量不同而有所不同）。最大池化层的池化大小为
    ![](img/B13208_15_101.png)，步幅为 2，将空间维度减少了一半。（注意，如果在 `MaxPool2D` 中未指定 `strides`
    参数，则默认将其设置为与池化大小相等。）
- en: 'While we can calculate the size of the feature maps at this stage manually,
    the Keras API provides a convenient method to compute this for us:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以手动计算此阶段特征图的大小，但 Keras API 提供了一个方便的方法来为我们计算：
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: By providing the input shape as a tuple specified in this example, the method
    `compute_output_shape` calculated the output to have a shape (16, 7, 7, 64), indicating
    feature maps with 64 channels and a spatial size of ![](img/B13208_15_102.png).
    The first dimension corresponds to the batch dimension, for which we used 16 arbitrarily.
    We could have used `None` instead, that is, `input_shape=(None, 28, 28, 1)`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在此示例中提供元组形式的输入形状，`compute_output_shape` 方法计算得出输出的形状为 (16, 7, 7, 64)，表示具有 64
    个通道和空间大小为 ![](img/B13208_15_102.png) 的特征图。第一个维度对应于批次维度，我们任意选择了 16。我们也可以使用 `None`，即
    `input_shape=(None, 28, 28, 1)`。
- en: 'The next layer that we want to add is a dense (or fully connected) layer for
    implementing a classifier on top of our convolutional and pooling layers. The
    input to this layer must have rank 2, that is, shape [![](img/B13208_15_103.png)].
    Thus, we need to flatten the output of the previous layers to meet this requirement
    for the dense layer:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要添加的层是一个全连接层（或称密集层），用于在卷积层和池化层之上实现分类器。此层的输入必须是二维的，即形状为 [![](img/B13208_15_103.png)]。因此，我们需要将前面层的输出展平，以满足全连接层的要求：
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As the result of `compute_output_shape` indicates, the input dimensions for
    the dense layer are correctly set up. Next, we will add two dense layers with
    a dropout layer in between:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如 `compute_output_shape` 的结果所示，密集层的输入维度已正确设置。接下来，我们将添加两个密集层，中间夹一个丢弃层：
- en: '[PRE13]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The last fully connected layer, named `''fc_2''`, has 10 output units for the
    10 class labels in the MNIST dataset. Also, we use the softmax activation to obtain
    the class-membership probabilities of each input example, assuming that the classes
    are mutually exclusive, so the probabilities for each example sum to 1\. (This
    means that a training example can belong to only one class.) Based on what we
    discussed in the section *Loss functions for classification*, which loss should
    we use here? Remember that for a multiclass classification with integer (sparse)
    labels (as opposed to one-hot encoded labels), we use `SparseCategoricalCrossentropy`.
    The following code will call the `build()` method for late variable creation and
    compile the model:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层全连接层，命名为 `'fc_2'`，具有 10 个输出单元，用于 MNIST 数据集中的 10 个类别标签。此外，我们使用 softmax 激活函数来获得每个输入样本的类别归属概率，假设各类别是互斥的，因此每个样本的概率之和为
    1（这意味着一个训练样本只能属于一个类别）。根据我们在 *分类损失函数* 部分讨论的内容，我们应该使用哪种损失函数呢？记住，对于具有整数（稀疏）标签的多类分类（与独热编码标签相对），我们使用
    `SparseCategoricalCrossentropy`。以下代码将调用 `build()` 方法以进行延迟变量创建，并编译模型：
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**The Adam optimizer**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adam 优化器**'
- en: 'Note that in this implementation, we used the `tf.keras.optimizers.Adam()`
    class for training the CNN model. The Adam optimizer is a robust, gradient-based
    optimization method suited to nonconvex optimization and machine learning problems.
    Two popular optimization methods inspired Adam: `RMSProp` and `AdaGrad`.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此实现中，我们使用了 `tf.keras.optimizers.Adam()` 类来训练 CNN 模型。Adam 优化器是一种强大的基于梯度的优化方法，适用于非凸优化和机器学习问题。两个受
    Adam 启发的流行优化方法是：`RMSProp` 和 `AdaGrad`。
- en: 'The key advantage of Adam is in the choice of update step size derived from
    the running average of gradient moments. Please feel free to read more about the
    Adam optimizer in the manuscript, *Adam: A Method for Stochastic Optimization*,
    *Diederik P. Kingma* and *Jimmy Lei Ba*, 2014\. The article is freely available
    at [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 'Adam的关键优势在于选择基于梯度矩的运行平均值计算更新步长。请随意阅读更多关于Adam优化器的文献，文献名为*Adam: A Method for
    Stochastic Optimization*，作者为*Diederik P. Kingma*和*Jimmy Lei Ba*，2014年。文章可以在[https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)免费获取。'
- en: 'As you already know, we can train the model by calling the `fit()` method.
    Note that using the designated methods for training and evaluation (like `evaluate()`
    and `predict()`) will automatically set the mode for the dropout layer and rescale
    the hidden units appropriately so that we do not have to worry about that at all.
    Next, we will train this CNN model and use the validation dataset that we created
    for monitoring the learning progress:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，我们可以通过调用`fit()`方法来训练模型。请注意，使用指定的训练和评估方法（如`evaluate()`和`predict()`）将自动为dropout层设置模式并适当重新缩放隐藏单元，这样我们就不需要担心这些问题。接下来，我们将训练这个CNN模型，并使用我们为监控学习进展所创建的验证数据集：
- en: '[PRE15]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once the 20 epochs of training are finished, we can visualize the learning
    curves:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成20个训练周期，我们可以可视化学习曲线：
- en: '[PRE16]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/B13208_15_13.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_13.png)'
- en: 'As you already know from the two previous chapters, evaluating the trained
    model on the test dataset can be done by calling the `.evaluate()` method:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前两章中已经了解到的，可以通过调用`.evaluate()`方法在测试数据集上评估训练好的模型：
- en: '[PRE17]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The CNN model achieves an accuracy of 99.39%. Remember that in *Chapter 14*,
    *Going Deeper – The Mechanics of TensorFlow*, we got approximately 90% accuracy
    using the Estimator `DNNClassifier`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 该CNN模型达到了99.39%的准确率。记住，在*第14章*《*深入探讨 – TensorFlow的工作原理*》中，我们使用Estimator `DNNClassifier`时，准确率约为90%。
- en: 'Finally, we can get the prediction results in the form of class-membership
    probabilities and convert them to predicted labels by using the `tf.argmax` function
    to find the element with the maximum probability. We will do this for a batch
    of 12 examples and visualize the input and predicted labels:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以以类别成员概率的形式获得预测结果，并通过使用`tf.argmax`函数来找到最大概率的元素，从而将其转换为预测标签。我们将对一批12个示例进行操作并可视化输入和预测标签：
- en: '[PRE18]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following figure shows the handwritten inputs and their predicted labels:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了手写输入和它们的预测标签：
- en: '![](img/B13208_15_14.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_14.png)'
- en: In this set of plotted examples, all the predicted labels are correct.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一组绘制的示例中，所有的预测标签都是正确的。
- en: We leave the task of showing some of the misclassified digits, as we did in
    *Chapter 12*, *Implementing a Multilayer Artificial Neural Network from Scratch*,
    as an exercise for the reader.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示一些误分类的数字，这一任务与*第12章*《*从零实现多层人工神经网络*》中的练习类似，作为读者的练习。
- en: Gender classification from face images using a CNN
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN从面部图像进行性别分类
- en: In this section, we are going to implement a CNN for gender classification from
    face images using the CelebA dataset. As you already saw in *Chapter 13*, *Parallelizing
    Neural Network Training with TensorFlow,* the CelebA dataset contains 202,599
    face images of celebrities. In addition, 40 binary facial attributes are available
    for each image, including gender (male or female) and age (young or old).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用CelebA数据集实现一个用于从面部图像进行性别分类的CNN。如同你在*第13章*《*使用TensorFlow并行化神经网络训练*》中看到的，CelebA数据集包含了202,599张名人面部图像。此外，每张图像还包含40个二进制面部属性，包括性别（男性或女性）和年龄（年轻或年老）。
- en: Based on what you have learned so far, the goal of this section is to build
    and train a CNN model for predicting the gender attribute from these face images.
    Here, for simplicity, we will only be using a small portion of the training data
    (16,000 training examples) to speed up the training process. However, in order
    to improve the generalization performance and reduce overfitting on such a small
    dataset, we will use a technique called **data augmentation**.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 基于你目前学到的内容，本节的目标是构建并训练一个CNN模型，用于从这些面部图像中预测性别属性。为了简单起见，我们将仅使用一小部分训练数据（16,000个训练示例）来加速训练过程。然而，为了提高泛化性能并减少在如此小的数据集上的过拟合，我们将使用一种称为**数据增强**的技术。
- en: Loading the CelebA dataset
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载CelebA数据集
- en: 'First, let''s load the data similarly to how we did in the previous section
    for the MNIST dataset. CelebA data comes in three partitions: a training dataset,
    a validation dataset, and a test dataset. Next, we will implement a simple function
    to count the number of examples in each partition:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将以与上一节加载 MNIST 数据集时类似的方式加载数据。CelebA 数据集分为三个部分：训练数据集、验证数据集和测试数据集。接下来，我们将实现一个简单的函数来统计每个数据集中的样本数量：
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So, instead of using all the available training and validation data, we will
    take a subset of 16,000 training examples and 1,000 examples for validation, as
    follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不会使用所有可用的训练和验证数据，而是选择一个包含16,000个训练样本和1,000个验证样本的子集，具体如下：
- en: '[PRE20]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It is important to note that if the `shuffle_files` argument in `celeba_bldr.as_dataset()`
    was not set to `False`, we would still see 16,000 examples in the training dataset
    and 1,000 examples for the validation dataset. However, at each iteration, it
    would reshuffle the training data and take a new set of 16,000 examples. This
    would defeat the purpose, as our goal here is to intentionally train our model
    with a small dataset. Next, we will discuss data augmentation as a technique for
    boosting the performance of deep NNs.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果在`celeba_bldr.as_dataset()`中的`shuffle_files`参数没有设置为`False`，我们仍然会看到16,000个训练样本和1,000个验证样本。然而，在每次迭代中，训练数据将被重新洗牌，并选取一组新的16,000个样本。这将违背我们的目的，因为我们的目标是故意用一个小数据集来训练模型。接下来，我们将讨论数据增强作为提高深度神经网络性能的技术。
- en: Image transformation and data augmentation
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像变换与数据增强
- en: 'Data augmentation summarizes a broad set of techniques for dealing with cases
    where the training data is limited. For instance, certain data augmentation techniques
    allow us to modify or even artificially synthesize more data and thereby boost
    the performance of a machine or deep learning model by reducing overfitting. While
    data augmentation is not only for image data, there are a set of transformations
    uniquely applicable to image data, such as cropping parts of an image, flipping,
    changing the contrast, brightness, and saturation. Let''s see some of these transformations
    that are available via the `tf.image` module. In the following code block, we
    will first get five examples from the `celeba_train` dataset and apply five different
    types of transformation: 1) cropping an image to a bounding box, 2) flipping an
    image horizontally, 3) adjusting the contrast, 4) adjusting the brightness, and
    5) center-cropping an image and resizing the resulting image back to its original
    size, (218, 178). In the following code, we will visualize the results of these
    transformations, showing each one in a separate column for comparison:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强总结了一组广泛的技术，用于处理训练数据有限的情况。例如，某些数据增强技术允许我们修改或甚至人工合成更多数据，从而通过减少过拟合来提高机器学习或深度学习模型的性能。尽管数据增强不仅限于图像数据，但有一组转换方法特别适用于图像数据，如裁剪图像的一部分、翻转、调整对比度、亮度和饱和度。让我们看看通过`tf.image`模块可以使用的一些变换。在以下代码块中，我们将首先从`celeba_train`数据集中获取五个样本，并应用五种不同的变换：1）将图像裁剪到边界框，2）水平翻转图像，3）调整对比度，4）调整亮度，5）对图像进行中心裁剪，并将结果图像调整回其原始大小（218，178）。在下面的代码中，我们将可视化这些变换的结果，并将每个变换单独展示在一列中以供比较：
- en: '[PRE21]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following figure shows the results:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果图：
- en: '![](img/B13208_15_15.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_15.png)'
- en: 'In the previous figure, the original images are shown in the first row and
    their transformed version in the second row. Note that for the first transformation
    (leftmost column), the bounding box is specified by four numbers: the coordinate
    of the upper-left corner of the bounding box (here *x*=20, *y*=50), and the width
    and height of the box (width=128, height=128). Also note that the origin (the
    coordinates at the location denoted as (0, 0)) for images loaded by TensorFlow
    (as well as other packages such as `imageio`) is the upper-left corner of the
    image.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，原始图像显示在第一行，它们的变换版本显示在第二行。请注意，对于第一个变换（最左列），边界框由四个数字指定：边界框左上角的坐标（此处为*x*=20,
    *y*=50），以及框的宽度和高度（width=128, height=128）。还需要注意，对于由 TensorFlow（以及其他包如`imageio`）加载的图像，原点（即坐标为（0，0）的位置）是图像的左上角。
- en: The transformations in the previous code block are deterministic. However, all
    such transformations can also be randomized, which is recommended for data augmentation
    during model training. For example, a random bounding box (where the coordinates
    of the upper-left corner are selected randomly) can be cropped from an image,
    an image can be randomly flipped along either the horizontal or vertical axes
    with a probability of 0.5, or the contrast of an image can be changed randomly,
    where the `contrast_factor` is selected at random, but with uniform distribution,
    from a range of values. In addition, we can create a pipeline of these transformations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码块中的变换是确定性的。然而，所有这些变换也可以进行随机化，这对于模型训练中的数据增强是推荐的。例如，可以从图像中随机裁剪出一个随机边界框（左上角坐标随机选择），图像也可以以0.5的概率沿水平方向或垂直方向随机翻转，或者图像的对比度可以随机改变，`contrast_factor`
    在一个范围内随机选择，并采用均匀分布。此外，我们还可以创建这些变换的管道。
- en: 'For example, we can first randomly crop an image, then flip it randomly, and
    finally, resize it to the desired size. The code is as follows (since we have
    random elements, we set the random seed for reproducibility):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以先随机裁剪图像，然后随机翻转它，最后将其调整到所需的大小。代码如下（由于我们使用了随机元素，我们设置了随机种子以确保可重现性）：
- en: '[PRE22]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following figure shows random transformations on three example images:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了对三个示例图像进行随机变换的效果：
- en: '![](img/B13208_15_16.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_16.png)'
- en: Note that each time we iterate through these three examples, we get slightly
    different images due to random transformations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每次迭代这三个示例时，由于随机变换，得到的图像会略有不同。
- en: For convenience, we can define a wrapper function to use this pipeline for data
    augmentation during model training. In the following code, we will define the
    function `preprocess()`, which will receive a dictionary containing the keys `'image'`
    and `'attributes'`. The function will return a tuple containing the transformed
    image and the label extracted from the dictionary of attributes.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们可以定义一个包装函数，在模型训练期间使用该管道进行数据增强。在以下代码中，我们将定义函数`preprocess()`，它将接收一个包含键
    `'image'` 和 `'attributes'` 的字典。该函数将返回一个元组，包含变换后的图像和从属性字典中提取的标签。
- en: 'We will only apply data augmentation to the training examples, however, and
    not to the validation or test images. The code is as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们只会对训练样本应用数据增强，而不会对验证或测试图像进行处理。代码如下：
- en: '[PRE23]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, to see data augmentation in action, let''s create a small subset of the
    training dataset, apply this function to it, and iterate over the dataset five
    times:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了观察数据增强的实际效果，让我们创建一个小的训练数据集子集，应用这个函数，并对数据集进行五次迭代：
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This figure shows the five resulting transformations for data augmentation
    on two example images:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了对两个示例图像进行数据增强后的五种转换结果：
- en: '![](img/B13208_15_17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_17.png)'
- en: 'Next, we will apply this preprocessing function to our training and validation
    datasets. We will use an image size of `(64, 64)`. Furthermore, we will specify
    `mode=''train''` when working with training data and use `mode=''eval''` for the
    validation data so that the random elements of the data augmentation pipeline
    will be applied only to the training data:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把这个预处理函数应用于我们的训练和验证数据集。我们将使用 `(64, 64)` 的图像大小。此外，当处理训练数据时，我们将指定 `mode='train'`，而对于验证数据，我们使用
    `mode='eval'`，这样数据增强管道中的随机元素只会应用于训练数据：
- en: '[PRE25]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Training a CNN gender classifier
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个CNN性别分类器
- en: 'By now, building a model with TensorFlow''s Keras API and training it should
    be straightforward. The design of our CNN is as follows: the CNN model receives
    input images of size ![](img/B13208_15_104.png) (the images have three color channels,
    using `''channels_last''`).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，使用TensorFlow的Keras API构建和训练模型应该是很直接的。我们的CNN设计如下：CNN模型接收尺寸为![](img/B13208_15_104.png)的输入图像（这些图像有三个颜色通道，采用
    `'channels_last'`）。
- en: 'The input data goes through four convolutional layers to make 32, 64, 128,
    and 256 feature maps using filters with kernel a size of ![](img/B13208_15_105.png).
    The first three convolution layers are followed by max-pooling, ![](img/B13208_15_106.png).
    Two dropout layers are also included for regularization:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据通过四个卷积层，使用大小为![](img/B13208_15_105.png)的滤波器生成32、64、128和256个特征图。前三个卷积层后面跟着最大池化层，![](img/B13208_15_106.png)。还包括两个dropout层用于正则化：
- en: '[PRE26]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s see the shape of the output feature maps after applying these layers:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在应用这些层之后，输出特征图的形状：
- en: '[PRE27]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There are 256 feature maps (or channels) of size ![](img/B13208_15_107.png).
    Now, we can add a fully connected layer to get to the output layer with a single
    unit. If we reshape (flatten) the feature maps, the number of input units to this
    fully connected layer will be ![](img/B13208_15_108.png). Alternatively, let's
    consider a new layer, called *global average-pooling*, which computes the average
    of each feature map separately, thereby reducing the hidden units to 256\. We
    can then add a fully connected layer. Although we have not discussed global average-pooling
    explicitly, it is conceptually very similar to other pooling layers. Global average-pooling
    can be viewed, in fact, as a special case of average-pooling when the pooling
    size is equal to the size of the input feature maps.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有256个特征图（或通道），大小为 ![](img/B13208_15_107.png)。现在，我们可以添加一个全连接层，以便到达输出层，输出一个单一单元。如果我们将特征图展平（flatten），则输入到这个全连接层的单元数将是
    ![](img/B13208_15_108.png)。另外，我们可以考虑使用一个新的层，称为*全局平均池化*，它分别计算每个特征图的平均值，从而将隐藏单元减少到256。接着，我们可以添加一个全连接层。尽管我们没有明确讨论全局平均池化，但它在概念上与其他池化层非常相似。实际上，全球平均池化可以视为平均池化的特例，当池化大小等于输入特征图的大小时。
- en: 'To understand this, consider the following figure showing an example of input
    feature maps of shape ![](img/B13208_15_109.png). The channels are numbered ![](img/B13208_15_110.png).
    The global average-pooling operation calculates the average of each channel so
    that the output will have shape ![](img/B13208_15_111.png). (Note: `GlobalAveragePooling2D`
    in the Keras API will automatically squeeze the output.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，请参考下面的图，展示了输入特征图的示例，形状为 ![](img/B13208_15_109.png)。通道编号为 ![](img/B13208_15_110.png)。全局平均池化操作计算每个通道的平均值，因此输出将具有形状
    ![](img/B13208_15_111.png)。 （注意：Keras API中的`GlobalAveragePooling2D`会自动压缩输出。）
- en: 'Without squeezing the output, the shape would be ![](img/B13208_15_112.png),
    as the global average-pooling would reduce the spatial dimension of ![](img/B13208_15_113.png)
    to a ![](img/B13208_15_114.png)):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不进行输出压缩，形状将是 ![](img/B13208_15_112.png)，因为全局平均池化将把 ![](img/B13208_15_113.png)
    的空间维度减少到 ![](img/B13208_15_114.png)：
- en: '![Une image contenant objet, antenne  Description générée automatiquement](img/B13208_15_18.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含物体、天线的图片，描述自动生成](img/B13208_15_18.png)'
- en: 'Therefore, given that, in our case, the shape of the feature maps prior to
    this layer is ![](img/B13208_15_115.png), we expect to get 256 units as output,
    that is, the shape of the output will be ![](img/B13208_15_116.png). Let''s add
    this layer and recompute the output shape to verify that this is true:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到在我们案例中，这一层之前的特征图形状为 ![](img/B13208_15_115.png)，我们期望得到256个单元作为输出，也就是说，输出的形状将是
    ![](img/B13208_15_116.png)。让我们添加这个层并重新计算输出形状，以验证这一点：
- en: '[PRE28]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we can add a fully connected (dense) layer to get a single output
    unit. In this case, we can specify the activation function to be `''sigmoid''`
    or just use `activation=None`, so that the model will output the logits (instead
    of class-membership probabilities), which is preferred for model training in TensorFlow
    and Keras due to numerical stability, as discussed earlier:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以添加一个全连接（密集）层，得到一个单一的输出单元。在这种情况下，我们可以指定激活函数为 `'sigmoid'`，或者直接使用 `activation=None`，这样模型将输出logits（而不是类别概率），因为在TensorFlow和Keras中，使用logits进行模型训练更具数值稳定性，如前所述：
- en: '[PRE29]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The next step is to compile the model and, at this time, we have to decide
    what loss function to use. We have a binary classification with a single output
    unit, so that means we should use `BinaryCrossentropy`. In addition, since our
    last layer does not apply sigmoid activation (we used `activation=None`), the
    outputs of our model are logits, not probabilities. Therefore, we will also specify
    `from_logits=True` in `BinaryCrossentropy` so that the loss function applies the
    sigmoid function internally, which is, due to the underlying code, more efficient
    than doing it manually. The code for compiling and training the model is as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是编译模型，在此时我们需要决定使用什么损失函数。我们有一个二分类任务，输出单元为单一单元，所以我们应该使用 `BinaryCrossentropy`。另外，由于我们最后一层没有应用sigmoid激活（我们使用了
    `activation=None`），所以模型的输出是logits，而不是概率。因此，我们还将在 `BinaryCrossentropy` 中指定 `from_logits=True`，这样损失函数会内部应用sigmoid函数，由于底层代码的优化，这比手动执行更高效。编译和训练模型的代码如下：
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s now visualize the learning curve and compare the training and validation
    loss and accuracies after each epoch:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化学习曲线，并比较每个周期后的训练和验证损失及准确率：
- en: '[PRE31]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following figure shows the training and validation losses and accuracies:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了训练和验证的损失与准确率：
- en: '![](img/B13208_15_19.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_19.png)'
- en: 'As you can see from the learning curves, the losses for the training and validation
    have not converged to a plateau region. Based on this result, we could have continued
    training for a few more epochs. Using the `fit()` method, we can continue training
    for an additional 10 epochs as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如学习曲线所示，训练和验证的损失尚未收敛到平稳区域。基于这个结果，我们本可以继续训练更多的 epochs。使用 `fit()` 方法，我们可以按照如下方式继续训练额外的
    10 个 epochs：
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once we are happy with the learning curves, we can evaluate the model on the
    hold-out test dataset:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对学习曲线感到满意，就可以在保持的测试数据集上评估模型：
- en: '[PRE33]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we already know how to get the prediction results on some test examples
    using `model.predict()`. However, remember that the model outputs the logits,
    not probabilities. If we are interested in the class-membership probabilities
    for this binary problem with a single output unit, we can use the `tf.sigmoid`
    function to compute the probability of class 1\. (In the case of a multiclass
    problem, we would use `tf.math.softmax`.) In the following code, we will take
    a small subset of 10 examples from our pre-processed test dataset (`ds_test`)
    and run `model.predict()` to get the logits. Then, we will compute the probabilities
    of each example being from class 1 (which corresponds to *male* based on the labels
    provided in CelebA) and visualize the examples along with their ground truth label
    and the predicted probabilities. Notice that we first apply `unbatch()` to the
    `ds_test` dataset before taking 10 examples; otherwise, the `take()` method would
    return 10 batches of size 32, instead of 10 individual examples:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经知道如何使用 `model.predict()` 获取某些测试样本的预测结果。然而，请记住，模型输出的是 logits 而不是概率。如果我们对该二分类问题（具有单个输出单元）的类别成员概率感兴趣，可以使用
    `tf.sigmoid` 函数来计算类别 1 的概率（对于多类问题，我们将使用 `tf.math.softmax`）。在下面的代码中，我们将从我们预处理过的测试数据集（`ds_test`）中提取
    10 个样本，并运行 `model.predict()` 来获取 logits。然后，我们将计算每个样本属于类别 1 的概率（根据 CelebA 提供的标签，这对应于*男性*），并可视化这些样本及其真实标签和预测概率。请注意，我们首先对
    `ds_test` 数据集应用 `unbatch()`，然后再取 10 个样本，否则 `take()` 方法将返回 10 个大小为 32 的批次，而不是 10
    个单独的样本：
- en: '[PRE34]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In the following figure, you can see 10 example images along with their ground
    truth labels and the probabilities that they belong to class 1, male:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到 10 个示例图像，以及它们的真实标签和它们属于类别 1（男性）的概率：
- en: '![](img/B13208_15_20.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_15_20.png)'
- en: The probabilities of class 1 (that is, *male* according to CelebA) are provided
    below each image. As you can see, our trained model made only one error on this
    set of 10 test examples.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 类别 1（即 CelebA 数据集中标记为*男性*）的概率列在每个图像下方。如你所见，我们训练的模型在这组 10 个测试样本中只犯了一个错误。
- en: As an optional exercise, you are encouraged to try using the entire training
    dataset instead of the small subset we created. Furthermore, you can change or
    modify the CNN architecture. For example, you can change the dropout probabilities
    and the number of filters in the different convolutional layers. Also, you could
    replace the global average-pooling with a dense layer. If you are using the entire
    training dataset with the CNN architecture we trained in this chapter, you should
    be able to achieve about 97-99% accuracy.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个可选的练习，建议您尝试使用整个训练数据集，而不是我们创建的这个小子集。此外，您还可以更改或修改 CNN 架构。例如，您可以更改不同卷积层中的 dropout
    概率和滤波器数量。同时，您也可以用全连接层替代全局平均池化层。如果您使用的是本章中训练的 CNN 架构并应用整个训练数据集，您应该能够达到约 97-99%
    的准确率。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned about CNNs and their main components. We started
    with the convolution operation and looked at 1D and 2D implementations. Then,
    we covered another type of layer that is found in several common CNN architectures:
    the subsampling or so-called pooling layers. We primarily focused on the two most
    common forms of pooling: max-pooling and average-pooling.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了卷积神经网络（CNN）及其主要组件。我们从卷积操作开始，研究了 1D 和 2D 的实现方式。接着，我们介绍了 CNN 中常见的另一种层：子采样层或称为池化层。我们主要聚焦于两种最常见的池化形式：最大池化（max-pooling）和平均池化（average-pooling）。
- en: Next, putting all these individual concepts together, we implemented deep CNNs
    using the TensorFlow Keras API. The first network we implemented was applied to
    the already familiar MNIST handwritten digit recognition problem.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将所有这些单独的概念结合起来，我们使用 TensorFlow Keras API 实现了深度 CNN。我们实现的第一个网络应用于已经熟悉的 MNIST
    手写数字识别问题。
- en: Then, we implemented a second CNN on a more complex dataset consisting of face
    images and trained the CNN for gender classification. Along the way, you also
    learned about data augmentation and different transformations that we can apply
    to face images using the TensorFlow Dataset class.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在一个包含人脸图像的更复杂数据集上实现了第二个 CNN，并训练该 CNN 进行性别分类。在此过程中，你还了解了数据增强以及如何使用 TensorFlow
    Dataset 类对人脸图像进行不同的变换。
- en: In the next chapter, we will move on to **recurrent neural networks** (**RNNs**).
    RNNs are used for learning the structure of sequence data, and they have some
    fascinating applications, including language translation and image captioning.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论**递归神经网络**（**RNNs**）。RNN 用于学习序列数据的结构，并且具有一些令人着迷的应用，包括语言翻译和图像字幕生成。
