- en: Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'In this chapter we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下配方：
- en: Perceptron classifier
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机分类器
- en: Neural network – multilayer perceptron
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络 – 多层感知机
- en: Stacking with a neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与神经网络堆叠
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Neural networks and deep learning have been incredibly popular recently as they
    have solved tough problems and perhaps have become a significant part of the public
    face of artificial intelligence. Let's explore the feed-forward neural networks
    available in scikit-learn.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，神经网络和深度学习非常流行，因为它们解决了很多难题，并且可能已经成为人工智能公众面貌的重要组成部分。让我们探索scikit-learn中可用的前馈神经网络。
- en: Perceptron classifier
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机分类器
- en: With scikit-learn, you can explore the perceptron classifier and relate it to
    other classification procedures within scikit-learn. Additionally, perceptrons
    are the building blocks of neural networks, which are a very prominent part of
    machine learning, particularly computer vision.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，你可以探索感知机分类器，并将其与scikit-learn中的其他分类方法进行比较。此外，感知机是神经网络的构建模块，神经网络是机器学习中的一个重要部分，特别是在计算机视觉领域。
- en: Getting ready
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Let''s get started. The process is as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。过程如下：
- en: Load the UCI diabetes classification dataset.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载UCI糖尿病分类数据集。
- en: Split the dataset into training and test sets.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和测试集。
- en: Import a perceptron.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入感知机。
- en: Instantiate the perceptron.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化感知机。
- en: Then train the perceptron.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后训练感知机。
- en: Try the perceptron on the testing set or preferably compute `cross_val_score`.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在测试集上使用感知机，或者更好地计算`cross_val_score`。
- en: 'Load the UCI diabetes dataset:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 加载UCI糖尿病数据集：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You have loaded `X`, the set of input features, and `y`, the variable we desired
    to predict. Split `X` and `y` into testing and training sets. Do this by stratifying
    the target set, keeping the classes in balanced proportions in both training and
    testing sets:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经加载了`X`，输入特征集，以及`y`，我们希望预测的变量。将`X`和`y`划分为测试集和训练集。通过分层目标集来完成这一步，确保在训练集和测试集中类的比例平衡：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Scale the set of features. Perform the scaling operation on the training set
    only and then continue with the testing set:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对特征集进行缩放。仅在训练集上执行缩放操作，然后继续进行测试集：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Instantiate and train the perceptron on the training set:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化并在训练集上训练感知机：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Measure the cross-validation score. Pass `roc_auc` as the cross-validation
    scoring mechanism. Additionally, use a stratified k-fold by setting `cv=skf`:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量交叉验证得分。将`roc_auc`作为交叉验证评分机制。此外，通过设置`cv=skf`，使用分层K折交叉验证：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Measure the performance on the test set. Import two metrics, `accuracy_score`
    and `roc_auc_score`, from the `sklearn.metrics` module:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估性能。导入`sklearn.metrics`模块中的两个指标，`accuracy_score`和`roc_auc_score`：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The test finished relatively quickly. It performed okay, a bit worse than logistic
    regression, which was 75% accurate (this is an estimate; we cannot compare the
    logistic regression from any previous chapter because the training/testing split
    is different).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 测试很快就完成了。结果表现还行，比逻辑回归稍差，逻辑回归的准确率为75%（这是一个估计值；我们不能将本章的逻辑回归与任何之前章节中的逻辑回归做比较，因为训练集和测试集的划分不同）。
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The perceptron is a simplification of a neuron in the brain. In the following
    diagram, the perceptron receives inputs *x[1],* and *x[2]* from the left. A bias
    term*, w*[0]*,* and weights *w[1]* and *w[2]* are computed. The terms *x*[i] and
    *w*[i] form a linear function. This linear function is then passed to an activation
    function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是大脑中神经元的简化模型。在下面的图示中，感知机从左边接收输入 *x[1]* 和 *x[2]*。计算偏置项 *w[0]* 和权重 *w[1]*、*w[2]*。*x[i]*
    和 *w[i]* 组成一个线性函数。然后将这个线性函数传递给激活函数。
- en: 'In the following activation function, if the sum of the dot product of the
    weight and input vector is less than zero, an individual row is classified as
    0; otherwise it is classified as 1:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下激活函数中，如果权重与输入向量的点积之和小于零，则将单独的行分类为0；否则分类为1：
- en: '![](img/56833da2-8df6-4a37-a6cb-5133ddeda3c3.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56833da2-8df6-4a37-a6cb-5133ddeda3c3.png)'
- en: This happens in a single epoch, or iteration, passing through the perceptron.
    The process repeats through several iterations and weights are readjusted each
    time, minimizing the loss function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生在单次迭代或遍历感知机时。该过程会在多个迭代中重复，每次都会重新调整权重，从而最小化损失函数。
- en: With regard to perceptrons and the current state of neural networks, they work
    well as researchers have tried many things. In practice, they currently work well
    with the computational power available now.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于感知器和当前神经网络的状态，它们表现良好，因为研究人员已经尝试了许多方法。实际上，基于目前的计算能力，它们的表现非常好。
- en: As computing power keeps increasing, neural networks and perceptrons become
    capable of solving increasingly difficult problems and training times keep decreasing
    and decreasing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的不断提升，神经网络和感知器能够解决越来越复杂的问题，且训练时间持续缩短。
- en: There's more...
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Try running a grid search by varying the perceptron's hyperparameters. Some
    notable parameters include regularization parameters, `penalty` and `alpha`, `class_weight`,
    and `max_iter`. The `class_weight` parameter deals well with unbalanced classes
    by giving more weight to the underrepresented classes. The `max_iter` parameter refers
    to the maximum number of passes through the perceptron. In general, the higher
    its value the better, so we set it to 50\. (Note that this is the code for scikit-learn
    0.19.0\. In scikit-learn 0.18.1, use the `n_iter` parameter instead of the `max_iter`
    parameter.)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试通过调整感知器的超参数来运行网格搜索。几个显著的参数包括正则化参数`penalty`和`alpha`、`class_weight`以及`max_iter`。`class_weight`参数通过赋予代表性不足的类别更多的权重，能够很好地处理类别不平衡问题。`max_iter`参数表示感知器的最大迭代次数。一般来说，其值越大越好，因此我们将其设置为50。（请注意，这是针对scikit-learn
    0.19.0的代码。在scikit-learn 0.18.1版本中，请使用`n_iter`参数代替`max_iter`参数。）
- en: 'Try the following grid search:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试以下网格搜索：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Look at the best parameters and the best score:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 查看最佳参数和最佳得分：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Varying hyperparameters using cross-validation has improved the results. Now
    try to use bagging with a set of perceptrons as follows. Start by noticing and
    picking the best perceptron from the perceptron grid search:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证调整超参数已改善结果。现在尝试使用一组感知器进行集成学习，如下所示。首先注意并选择网格搜索中表现最好的感知器：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Perform the grid search:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 执行网格搜索：
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Look at the new cross-validation score and best parameters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 查看新的交叉验证得分和最佳参数：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Thus, a bag of perceptrons scores better than a single perceptron for this dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这个数据集，一组感知器的表现优于单一感知器。
- en: Neural network – multilayer perceptron
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络 – 多层感知器
- en: 'Using a neural network in scikit-learn is straightforward and proceeds as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中使用神经网络非常简单，步骤如下：
- en: Load the data.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Scale the data with a standard scaler.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准缩放器对数据进行缩放。
- en: Do a hyperparameter search. Begin by varying the alpha parameter.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行超参数搜索。首先调整alpha参数。
- en: Getting ready
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Load the medium-sized California housing dataset that we used in [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml),
    *Tree Algorithms and Ensembles*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们在[第9章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)中使用的中等规模的加州住房数据集，*决策树算法与集成方法*：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Bin the target variable so that the target train set and target test set are
    a bit more similar. Then use a stratified train/test split:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将目标变量进行分箱，使得目标训练集和目标测试集更为相似。然后使用分层的训练/测试拆分：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How to do it...
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Begin by scaling the input variables. Train the scaler only on the training
    data:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先对输入变量进行缩放。仅在训练数据上训练缩放器：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, perform the scaling on the test set:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在测试集上执行缩放：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, perform a randomized search (or grid search if you prefer) to find
    a reasonable value for `alpha`, one that scores well:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，执行随机搜索（或如果你喜欢，也可以进行网格搜索）来找到`alpha`的合理值，确保其得分较高：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In the context of neural networks, the single perceptrons look like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的背景下，单一感知器的结构如下所示：
- en: '![](img/a494cfb8-d5cd-4eea-8df9-c8190c0558f0.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a494cfb8-d5cd-4eea-8df9-c8190c0558f0.png)'
- en: The output is a function of a sum of the dot product of weights and inputs.
    The function *f* is the activation function and can be a sigmoid curve, for example.
    In the neural network, hyperparameter activation refers to this function. In scikit-learn,
    there are the options of identity, logistic, tanh, and relu, where logistic is
    the sigmoid curve.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是权重和输入的点积之和的函数。该函数*f*是激活函数，可以是sigmoid曲线。例如，在神经网络中，超参数激活指的就是这个函数。在 scikit-learn
    中，有identity、logistic、tanh和relu的选项，其中logistic即为sigmoid曲线。
- en: 'The whole network looks like this (the following is a diagram from the scikit
    documentation at [http://scikit-learn.org/stable/modules/neural_networks_supervised.html](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 整个网络是这样的（以下是来自scikit文档的图示，链接：[http://scikit-learn.org/stable/modules/neural_networks_supervised.html](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)）：
- en: '![](img/1eefe899-179d-46b0-81eb-c8adbda3965c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1eefe899-179d-46b0-81eb-c8adbda3965c.png)'
- en: It is instructive to use a neural network on a dataset we are familiar with,
    the California housing dataset. The California housing dataset seemed to favor
    nonlinear algorithms, particularly trees and ensembles of trees. Trees did well
    on the dataset and established a benchmark as to how well algorithms can do on
    that dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们熟悉的数据集——加利福尼亚住房数据集来训练神经网络是很有教育意义的。加利福尼亚住房数据集似乎更适合非线性算法，特别是树算法和树的集成。树算法在这个数据集上表现得很好，并为算法在该数据集上的表现建立了基准。
- en: In the end, neural networks did okay but not nearly as well as gradient boosting
    machines. Additionally, they were computationally expensive.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，神经网络表现还不错，但远不如梯度提升机好。此外，它们在计算上非常昂贵。
- en: Philosophical thoughts on neural networks
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于神经网络的哲学思考
- en: Neural networks are mathematically universal function approximators and can
    learn any function. Also, the hidden layers are often interpreted as the network
    learning the intermediate steps of a process without a human having to program
    the intermediate steps. This can come from convolutional neural networks in computer
    vision, where it is easy to see how the neural network figures out each layer.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是数学上通用的函数逼近器，可以学习任何函数。此外，隐藏层通常被解释为网络学习过程的中间步骤，而无需人工编写这些中间步骤。这可以来自计算机视觉中的卷积神经网络，在那里很容易看到神经网络如何推断出每一层。
- en: These facts make an interesting mental image and can be applied to other estimators.
    Many people do not tend to think of random forests as trees figuring out processes
    tree level by tree level, or tree by tree (perhaps because their structure is
    not as organized and random forests do not invoke visualizations of the biological
    brain). In more practical detail, if you wanted to organize random forests, you
    can limit their depth or perhaps use gradient boosting machines.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些事实构成了有趣的心智图像，并且可以应用于其他估计器。许多人往往不会把随机森林看作是树在逐步推理的过程，或者说是树与树之间的推理（也许是因为它们的结构不如有组织，而随机森林不会让人联想到生物大脑的可视化）。在更实际的细节上，如果你想组织随机森林，你可以限制它们的深度，或者使用梯度提升机。
- en: Regardless of the hard facts present or not in the idea of a neural network
    truly being intelligent, it is helpful to carry around such mental images as the
    field progresses and machines become smarter and smarter. Carry the idea around,
    yet focus on the results as they are; that's what machine learning is about now.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 无论神经网络是否真正智能这一事实如何，随着领域的进展和机器变得越来越聪明，携带这样的心智图像是有帮助的。携带这个想法，但要专注于结果；这就是现在机器学习的意义。
- en: Stacking with a neural network
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络进行堆叠
- en: The two most common meta-learning methods are bagging and boosting. Stacking
    is less widely used; yet it is powerful because one can combine models of different
    types. All three methods create a stronger estimator from a set of not-so-strong
    estimators. We tried the stacking procedure in [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml),
    *Tree Algorithms and Ensembles*. Here, we try it with a neural network mixed with
    other models.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 两种最常见的元学习方法是袋装法和提升法。堆叠法使用得较少；然而，它非常强大，因为可以将不同类型的模型结合起来。这三种方法都通过一组较弱的估计器创建了一个更强的估计器。我们在[第九章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)，*树算法与集成方法*中尝试了堆叠过程。在这里，我们尝试将神经网络与其他模型结合。
- en: 'The process for stacking is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的过程如下：
- en: Split the dataset into training and testing sets.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和测试集。
- en: Split the training set into two sets.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集划分为两部分。
- en: Train base learners on the first part of the training set.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集的第一部分上训练基础学习器。
- en: Make predictions using the base learners on the second part of the training
    set. Store these prediction vectors.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基础学习器对训练集第二部分进行预测。存储这些预测向量。
- en: Take the stored prediction vectors as inputs and the target variable as output.
    Train a higher level learner (note that we are still on the second part of the
    training set).
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将存储的预测向量作为输入，目标变量作为输出。训练一个更高层次的学习器（注意我们仍然处于训练集的第二部分）。
- en: After that, you can view the results of the overall process on the test set
    (note that you cannot select a model by viewing results on the test set).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以查看在测试集上整体过程的结果（注意，不能通过查看测试集上的结果来选择模型）。
- en: Getting ready
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Import the California housing dataset and the libraries we have been using:
    `numpy`, `pandas`, and `matplotlib`. It is a medium-sized dataset but is large
    relative to the other scikit-learn datasets:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 导入加利福尼亚州住房数据集以及我们一直使用的库：`numpy`、`pandas`和`matplotlib`。这是一个中等大小的数据集，但相对于其他scikit-learn数据集来说，它还是比较大的：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Bin the target variable to increase the balance in splitting the dataset in
    regards to the target:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将目标变量进行分箱，以提高数据集在目标变量上的平衡性：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Split the dataset `X` and `y` into three sets. `X_1` and `X_stack` refer to
    the input variables of the first and second training sets, respectively. `y_1`
    and `y_stack` refer to the output target variables of the first and second training
    sets respectively. The test set consists of `X_test_prin` and `y_test_prin`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集`X`和`y`划分为三个集合。`X_1`和`X_stack`分别表示第一个和第二个训练集的输入变量，`y_1`和`y_stack`分别表示第一个和第二个训练集的输出目标变量。测试集由`X_test_prin`和`y_test_prin`组成：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Another option is to use `StratifiedShuffleSplit` from the `model_selection`
    module in scikit-learn.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用来自scikit-learn的`model_selection`模块中的`StratifiedShuffleSplit`。
- en: How to do it...
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We are going to use three base regressors: a neural network, a single gradient
    boosting machine, and a bag of gradient boosting machines.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用三个基础回归器：一个神经网络，一个单一的梯度提升机，和一个梯度提升机的袋装集成。
- en: First base model – neural network
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个基础模型 - 神经网络
- en: 'Add a neural network by performing a cross-validated grid search on the first
    training set: `X_1`, the inputs, and `y_1` the target set. This will find the
    best parameters of the neural network for this dataset. We are only varying the
    `alpha` parameter in this example. Do not forget to scale the inputs or else the
    network will not run well:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对第一个训练集进行交叉验证的网格搜索，添加一个神经网络：`X_1`为输入，`y_1`为目标集合。这将找到该数据集的最佳神经网络参数。在此示例中，我们仅调整`alpha`参数。不要忘记对输入进行标准化，否则网络将无法很好地运行：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'View the best parameters and the best score of the grid search:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看网格搜索的最佳参数和最佳得分：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Pickle the neural network that performed the best during the grid search. This
    will save the training we have done so that we do not have to keep doing it several
    times:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持久化在网格搜索中表现最好的神经网络。这将保存我们已经完成的训练，以免我们不得不反复进行训练：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Second base model – gradient boost ensemble
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二个基础模型 - 梯度提升集成
- en: 'Perform a randomized grid search on gradient-boosted trees:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对梯度增强树进行随机网格搜索：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'View the best score and parameters:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看最佳得分和参数：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Increase the number of estimators and train:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加估算器的数量并训练：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Pickle the estimator. For convenience and reusability, the pickling code is
    wrapped into a single function:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对估算器进行持久化。为了方便和可重用性，持久化的代码被封装成一个函数：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Third base model – bagging regressor of gradient boost ensembles
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三个基础模型 - 梯度提升集成的袋装回归器
- en: 'Now, perform a small grid search for a bag of gradient-boosted trees. It is
    hard to know from a theoretical viewpoint whether this type of ensemble will do
    well. For the purpose of stacking, it will do well enough if it is not too correlated
    with the other base estimators:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，进行一个小规模的网格搜索，尝试一组梯度增强树的袋装。理论上很难判断这种集成方法是否会表现良好。对于堆叠而言，只要它与其他基础估算器的相关性不太高，它就足够好：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'View the best parameters and score:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看最佳参数和评分：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Pickle the best estimator:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持久化最佳估算器：
- en: '[PRE28]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Some functions of the stacker
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠器的一些函数
- en: 'Use functions similar to [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml),
    *Tree Algorithms and Ensembles*. The `handle_X_set` function creates a dataframe
    of the prediction vectors on the `X_stack` set. Conceptually, it refers to the
    fourth step of predictions on the second part of the training set:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用类似于[第9章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)的函数，*树算法与集成方法*。`handle_X_set`函数在`X_stack`集合上创建预测向量的数据框。概念上，它指的是对训练集第二部分进行预测的第四步：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you pickled the files previously and want to start at this step, unpickle
    the files. The following files are loaded with the correct filenames and variable
    names to perform the `handle_X_set` function:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你之前已经持久化了文件，并希望从这一步开始，解持久化文件。以下文件使用正确的文件名和变量名加载，以执行`handle_X_set`函数：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create a dataframe of predictions using the `handle_X_set` function. Print
    the Pearson correlation between the prediction vectors:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`handle_X_set`函数创建预测数据框。打印预测向量之间的皮尔逊相关性：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Meta-learner – extra trees regressor
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元学习器 – 额外树回归器
- en: 'Similar to [Chapter 9](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml), *Tree Algorithms
    and Ensembles*, train an extra tree regressor on the dataframe of predictions.
    Use `y_stack` as the target vector:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于[第9章](9fdf265d-8934-4bbb-8b3a-dd5cd2c33cc7.xhtml)，*树算法与集成方法*，在预测数据框上训练一个额外树回归器。使用`y_stack`作为目标向量：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'View the best parameters:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看最佳参数：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Train the extra trees regressor but increase the number of estimators:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练额外树回归器，但增加估计器的数量：
- en: '[PRE34]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'View the `final_etr` estimator''s cross-validation performance:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看`final_etr`估计器的交叉验证性能：
- en: '[PRE35]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'View the performance on the testing set:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看测试集上的性能：
- en: '[PRE36]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Perhaps we can increase the results even further. Place the training columns
    alongside the prediction vectors. Start by modifying the functions we have been
    using:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或许我们可以进一步提高结果。将训练列与预测向量并排放置。首先修改我们一直在使用的函数：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Continue the training of the extra trees regressor as before:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照之前的方式继续训练额外树回归器：
- en: '[PRE38]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We continue as we did previously. View the best parameters and train a model
    with more estimators:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续按之前的方式进行。查看最佳参数，并使用更多的估计器训练模型：
- en: '[PRE39]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'View cross-validation performance:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看交叉验证性能：
- en: '[PRE40]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We included the original input columns in the training of the high-level learner
    of the stacker. The cross-validation performance has increased to 0.8297 from
    0.8221\. Thus, we conclude that the model that includes the input columns is the
    best model. Now, after we have selected this model as the final best model, we
    look at the performance of the estimator on the testing set:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在堆叠器的高级学习器训练中包含了原始输入列。交叉验证性能从0.8221提高到0.8297。因此，我们得出结论，包含输入列的模型是最优模型。现在，在我们选择这个模型作为最终最佳模型后，我们查看估计器在测试集上的表现：
- en: '[PRE41]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: There's more...
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: After trying out neural networks on scikit-learn, you can try packages such
    as `skflow`, which borrows the syntax of scikit-learn yet utilizes Google's powerful
    open source TensorFlow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试过scikit-learn中的神经网络后，你可以尝试像`skflow`这样的包，它借用了scikit-learn的语法，但利用了谷歌强大的开源TensorFlow。
- en: In regards to stacking, you can try cross-validation performance and prediction
    on the whole training set `X_train_prin`, instead of splitting it into two parts,
    `X_1` and `X_stack`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 关于堆叠，你可以尝试在整个训练集`X_train_prin`上进行交叉验证性能评估和预测，而不是将其分割成两部分`X_1`和`X_stack`。
- en: A lot of packages in data science borrow heavily from either scikit-learn's
    or R's syntaxes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学中的许多包都大量借鉴了scikit-learn或R的语法。
