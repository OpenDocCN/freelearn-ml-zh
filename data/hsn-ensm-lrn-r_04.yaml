- en: Chapter 4. Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 随机森林
- en: The previous chapter introduced bagging as an ensembling technique based on
    homogeneous base learners, with the decision tree serving as a base learner. A
    slight shortcoming of the bagging method is that the bootstrap trees are correlated.
    Consequently, although the variance of predictions is reduced, the bias will persist.
    Breiman proposed randomly sampling the covariates and independent variables at
    each split, and this method then went on to help in decorrelating the bootstrap
    trees.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章介绍了袋装法作为一种基于同质基学习器的集成技术，其中决策树作为基学习器。袋装法的一个轻微不足是自助树之间存在相关性。因此，尽管预测的方差得到了降低，但偏差将持续存在。Breiman提出了在每个分割处随机抽样协变量和独立变量的方法，这种方法随后有助于去相关自助树。
- en: In the first section of this chapter, the random forest algorithm is introduced
    and illustrated. The notion of variable importance is crucial to decision trees
    and all of their variants, and a section is devoted to clearly illustrating this
    concept. Do the random forests perform better than bagging? An answer will be
    provided in the following section.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一节中，介绍了随机森林算法并进行了说明。变量重要性的概念对于决策树及其所有变体至关重要，因此有一节专门用于清晰地阐述这一概念。随机森林的表现是否优于袋装法？答案将在下一节中给出。
- en: Breiman laid out the importance of proximity plots in the context of random
    forests, and we will delve into this soon enough. An algorithm as complex as this
    will have a lot of nitty-gritty details, and some of these will be illustrated
    through programs and real data. Missing data is almost omnipresent and we will
    undertake the task of imputing missing values using random forests. Although a
    random forest is primarily a supervised learning technique, it can also be used
    for clustering observations regarding the data, and this topic will be the concluding
    section.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Breiman在随机森林的背景下阐述了邻近图的重要性，我们很快就会深入探讨这一点。如此复杂的算法将有很多细节，其中一些将通过程序和真实数据来展示。缺失数据几乎是普遍存在的，我们将承担使用随机森林插补缺失值的任务。尽管随机森林主要是一种监督学习技术，但它也可以用于对数据进行聚类，这一主题将是最后一节。
- en: 'The core topics of this chapter are the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的核心主题如下：
- en: The Random Forest algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林算法
- en: Variable importance for decision trees and random forests
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树和随机森林的变量重要性
- en: Comparing random forests with bagging
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较随机森林与袋装法
- en: Use of proximity plots
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用邻近图
- en: Random forest details, nitty-gritty, and nuances
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林的细节、细节和细微差别
- en: Handling missing data by using random forests
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用随机森林处理缺失数据
- en: Clustering with random forests
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林进行聚类
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following libraries in this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下库：
- en: '`kernlab`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernlab`'
- en: '`randomForest`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`randomForest`'
- en: '`randomForestExplainer`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`randomForestExplainer`'
- en: '`rpart`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: Random Forests
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: '[Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, generalized the decision tree using the bootstrap
    principle. Before we embark on a journey with random forests, we will quickly
    review the history of decision trees and highlight some of their advantages and
    drawbacks. The invention of decision trees followed through a culmination of papers,
    and the current form of the trees can be found in detail in Breiman, et al. (1984).
    Breiman''s method is popularly known as **C**lassification **a**nd **R**egression
    **T**rees, aka **CART**. Around the late 1970s and early 1980s, Quinlan invented
    an algorithm called C4.5 independently of Breiman. For more information, see Quinlan
    (1984). To a large extent, the current form of decision trees, bagging, and random
    forests is owed to Breiman. A somewhat similar approach is also available in an
    algorithm popularly known by the abbreviation CHAID, which stands for **Ch**i-square
    **A**utomatic **I**nteraction **D**etector. An in-depth look at CART can be found
    in Hastie, et al. (2009), and a statistical perspective can be found in Berk (2016).
    An excellent set of short notes can also be found in Seni and Elder (2010). Without
    any particular direction, we highlight some advantages and drawbacks of CART:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee "第3章。袋装")，*袋装*，通过自助原理泛化了决策树。在我们开始随机森林之旅之前，我们将快速回顾决策树的历史，并突出其一些优点和缺点。决策树的发明是通过一系列论文的累积，而树的当前形式可以在Breiman等人（1984年）的论文中找到详细信息。Breiman的方法广为人知为**分类**和**回归**树，简称**CART**。在20世纪70年代末和80年代初，Quinlan独立于Breiman发明了一种称为C4.5的算法。更多信息，请参阅Quinlan（1984）。在很大程度上，当前形式的决策树、袋装和随机森林归功于Breiman。还有一种类似的方法也存在于一个流行的缩写为CHAID的算法中，代表**Chi-square**
    **Automatic** **Interaction** **Detector**。CART的深入探讨可以在Hastie等人（2009年）的书中找到，而统计视角可以在Berk（2016年）的书中找到。Seni和Elder（2010年）也有一套出色的简短笔记。没有特定的方向，我们强调CART的一些优点和缺点：'
- en: Trees automatically address the problem of variable selection since at each
    split, they look for the variable that gives the best split in the regressand,
    and thus a tree eliminates variables that are not useful.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树自动解决变量选择问题，因为每次分割时，它们都寻找在回归变量中给出最佳分割的变量，因此树消除了无用的变量。
- en: Trees do not require data processing. This means that we don't have to consider
    transformation, rescaling, and/or weight-of-evidence preprocessing.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树不需要数据预处理。这意味着我们不必考虑转换、缩放和/或证据权重预处理。
- en: Trees are computationally scalable and the time complexity is manageable.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树在计算上是可扩展的，时间复杂度是可管理的。
- en: Trees give a metric called variable importance that is based on the contribution
    of the variable to error reduction across all the splits of the trees.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树提供了一个称为变量重要性的度量，该度量基于变量对树中所有分割的误差减少的贡献。
- en: Trees efficiently handle missing values and if an observation has a missing
    value, the tree will continue to use the available values of the observation.
    Handling missing data is often enabled by the notion of a surrogate split.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树有效地处理缺失值，如果一个观测值有缺失值，树将继续使用观测值的可用值。处理缺失数据通常是通过代理分割的概念来实现的。
- en: Trees have fewer parameters to manage, as seen in the previous chapter.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的参数较少，如前一章所示。
- en: Trees have a simple top-down interpretation.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树具有简单的自顶向下的解释。
- en: Trees with great depth tend to be almost unbiased.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度大的树往往几乎是无偏的。
- en: The interaction effect is easily identified among the variables.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量之间的交互效应很容易识别。
- en: Its drawback is that the fitted model is not continuous and it will have sharp
    edges. Essentially, trees are piecewise constant regression models.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的缺点是拟合的模型不是连续的，并且会有尖锐的边缘。本质上，树是分段常数回归模型。
- en: Trees can't approximate low interaction target functions.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树不能近似低交互的目标函数。
- en: The greedy search approach to trees results in high variance.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的贪婪搜索方法会导致高方差。
- en: 'The first extension of the trees was seen in the bagging algorithm discussed
    in the previous chapter. Suppose we have N observations. For each bootstrap sample,
    we draw N observations with replacement. How many observations are likely to be
    common between two bootstrap samples? Let''s write a simple program to find it
    first, using the simple `sample` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 树的第一种扩展可以在前一章讨论的袋装算法中看到。假设我们有N个观测值。对于每个自助样本，我们用替换法抽取N个观测值。两个自助样本之间可能有多少观测值是共同的？让我们先写一个简单的程序来找出它，使用简单的`sample`函数：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This program needs explanation. The number of *N* observations varies from 1000
    to 10000 with an increment of 1000, and we run *B = 1e3 = 1000* bootstrap iterations.
    Now, for a fixed size of *N*, we draw two samples with replacement of size *N*,
    see how many observations are common between them, and divide it by *N*. The average
    of the *B = 1000* samples is the probability of finding a common observation between
    two samples. Equivalently, it gives the common observation percentage between
    two samples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序需要解释。观测值数量 *N* 从1000到10000不等，每次增加1000，我们进行 *B = 1e3 = 1000* 次自助迭代。现在，对于固定的
    *N* 大小，我们抽取两个有放回的样本，大小为 *N*，看看它们之间有多少观测值是共同的，然后除以 *N*。*B = 1000* 个样本的平均值是两个样本之间找到共同观测值的概率。等价地，它给出了两个样本之间的共同观测值百分比。
- en: The bootstrap probability clearly shows that about 40% of observations will
    be common between any two trees. Consequently, the trees will be correlated.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自助概率清楚地表明，大约40%的观测值将在任意两棵树之间是共同的。因此，这些树将是相关的。
- en: 'In Chapter 15, Hastie, et al. (2009) points out that the bagging trees are
    IID trees and hence the expectation of any one tree is the same as the expectation
    of any other tree. Consequently, the bias of the bagged trees is the same as that
    of the individual trees. Thus, variance reduction is the only improvement provided
    by bagging. Suppose that we have B independent and identically distributed IID
    random variables with a variance of ![Random Forests](img/00179.jpeg). The sample
    average has a variance of ![Random Forests](img/00180.jpeg). However, if we know
    that the variables are only identically distributed and that there is a positive
    pairwise correlation of ![Random Forests](img/00181.jpeg), then the variance of
    the sample average is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第15章中，Hastie等人（2009年）指出，袋装树是独立同分布的树，因此任何一棵树的期望值与任何其他树的期望值相同。因此，袋装树的偏差与单个树的偏差相同。因此，方差减少是袋装提供的唯一改进。假设我们有一个B个独立同分布的随机变量，其方差为![随机森林](img/00179.jpeg)。样本平均值的方差为![随机森林](img/00180.jpeg)。然而，如果我们知道变量只是同分布的，并且存在![随机森林](img/00181.jpeg)的正相关对，那么样本平均值的方差如下：
- en: '![Random Forests](img/00182.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/00182.jpeg)'
- en: Note that as the number of B samples increase, the second term vanishes and
    the first term remains. Thus, we see that the correlatedness of the bagged trees
    restricts the benefits of averaging. This motivated Breiman to innovate in a way
    that subsequent trees will not be correlated.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，随着B样本数量的增加，第二项消失，第一项保持不变。因此，我们看到袋装树的关联性限制了平均化的好处。这促使布莱曼创新，使得后续的树不会相关。
- en: 'Breiman''s solution is that before each split, select *m < p* number of input
    variables at random for splitting. This lays the foundation of random forests,
    where we shake the data to improve the performance. Note that merely *shaking*
    does not guarantee improvement. This trick helps when we have highly nonlinear
    estimators. The formal random forest algorithm, following Hastie, et al. (2009)
    and Berk (2016), is given as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 布莱曼的解决方案是在每次分割之前，随机选择 *m < p* 个输入变量进行分割。这为随机森林奠定了基础，我们在数据中“摇动”以改善性能。请注意，仅仅“摇动”并不能保证改进。这个技巧在我们有高度非线性估计器时很有帮助。正式的随机森林算法，根据Hastie等人（2009年）和Berk（2016年）的描述，如下所示：
- en: Draw a random sample of size N with replacement from the data.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据中抽取一个大小为N的有放回的随机样本。
- en: Draw a random sample without replacement of the predictors.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从预测值中抽取一个不放回的随机样本。
- en: Construct the first recursive partition of the data in the usual way.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式构建数据的第一层递归分割。
- en: Repeat step 2 for each subsequent split until the tree is as large as desired.
    Importantly, do not prune. Compute each terminal node proportion.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2，直到树达到所需的大小。重要的是不要剪枝。计算每个终端节点的比例。
- en: Drop the out-of-bag (OOB) data down the tree and store the assigned class to
    each observation, along with each observation's predictor values.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将树外的（OOB）数据向下传递到树中，并存储每个观测值分配的类别，以及每个观测值的预测值。
- en: Repeat steps 1-5 a large number of times, say 1000.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1-5多次，例如1000次。
- en: Using only the class assigned to each observation when that observation is OOB,
    count the number of times over the trees that the observation is classified in
    one category and the number of times over trees it is classified in the other
    category.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当观测值是OOB时，仅使用分配给每个观测值的类别，计算观测值在树中被分类为一个类别和另一个类别的次数。
- en: Assign each case to a category by a majority vote over the set of trees when
    that case is OOB.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当该案例为OOB时，通过在树集合中对每个案例进行多数投票将其分配到类别。
- en: From his practical experience, Breiman recommends randomly selecting a number
    of covariates at each split as ![Random Forests](img/00183.jpeg) with a minimum
    node size of 1 for a classification problem, whereas the recommendation for a
    regression problem is ![Random Forests](img/00184.jpeg) with a minimum node size
    of 5.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从他的实践经验中，Breiman 建议在每次分割时随机选择一定数量的协变量，如 ![随机森林](img/00183.jpeg) 对于分类问题，最小节点大小为1，而对于回归问题，建议
    ![随机森林](img/00184.jpeg) 最小节点大小为5。
- en: 'We will use the `randomForest` R package for software implementation. The German
    Credit data will be used for further analysis. If you remember, in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    the accuracy obtained by using the basic classification tree was 70%. We will
    set up the German credit data using the same settings as earlier, and we will
    build the random forest:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `randomForest` R 包进行软件实现。将使用德国信用数据进行进一步分析。如果你还记得，在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术介绍")，*集成技术介绍*中，使用基本分类树获得的准确率为70%。我们将使用与之前相同的设置设置德国信用数据，并将构建随机森林：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `randomForest` function applies over the `formula` and `data` as seen earlier.
    Here, we have specified the number of trees to be 500 with `ntree=500`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`randomForest` 函数如前所述应用于 `formula` 和 `data`。在这里，我们指定了树的数量为500，使用 `ntree=500`。'
- en: 'If we compare this random result with the bagging result from the previous
    chapter, the accuracy obtained there was only `0.78`. Here, we have `p = 19` covariates,
    and so we will try to increase the number of covariates sampled for a split at
    `8`, and see how it performs:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个随机结果与上一章中的bagging结果进行比较，那里的准确率仅为 `0.78`。这里我们有 `p = 19` 个协变量，因此我们将尝试在
    `8` 处增加用于分割的协变量样本数量，并看看它的表现：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An increase of `0.01`, or about 1%, might appear meager. However, in a banking
    context, this accuracy will translate into millions of dollars. We will use the
    usual `plot` function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 增加 `0.01` 或大约1%可能看起来很少。然而，在银行环境中，这种准确率将转化为数百万美元。我们将使用常规的 `plot` 函数：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The following graph is the output of the preceding code executed using the `plot`
    function`:`
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图形是使用 `plot` 函数执行的前述代码的输出：`
- en: '![Random Forests](img/00185.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/00185.jpeg)'
- en: 'Figure 1: Error rate of the Random Forest for the German Credit data'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：德国信用数据随机森林的错误率
- en: 'We have three curves: the error rate for OOB, the error rate for the good class,
    and the error rate for the bad class. Note that the error rate stabilizes at around
    100 trees. Using the loss matrix, it might be possible to reduce the gap between
    the three curves. Ideally, the three curves should be as close as possible.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有三个曲线：OOB的错误率、好类别的错误率和坏类别的错误率。请注意，错误率在大约100棵树时稳定。使用损失矩阵，可能可以减少三条曲线之间的差距。理想情况下，三条曲线应该尽可能接近。
- en: '**Exercise**: Create random forests with the options of `split` criteria, `loss`
    matrix, `minsplit`, and different `mtry`. Examine the error rate curves and prepare
    a summary.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：创建具有 `split` 标准选项、`loss` 矩阵、`minsplit` 和不同 `mtry` 的随机森林。检查错误率曲线并准备总结。'
- en: 'Visualize the random forest! Where are the trees? Apparently, we need to do
    a lot of exercises to extract trees out of the fitted `randomForest` object. A
    new function, `plot_RF`, has been defined in the `Utilities.R` file and we will
    display it here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化随机森林！树在哪里？显然，我们需要做很多练习来从拟合的 `randomForest` 对象中提取树。在 `Utilities.R` 文件中定义了一个新的函数
    `plot_RF`，我们将在下面显示它：
- en: '![Random Forests](img/00186.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/00186.jpeg)'
- en: The `plot_RF` function first obtains the number of `$ntree` trees in the forest.
    It will then run through a `for` loop. In each iteration of the loop, it will
    extract information related to that tree with the `getTree` function and create
    a new `dendogram` object. The `dendogram` is then visualized, and is nothing but
    the tree. Furthermore, the `print` command is optional and can be muted out.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot_RF` 函数首先获取森林中的 `$ntree` 树的数量。然后它将运行一个 `for` 循环。在循环的每次迭代中，它将使用 `getTree`
    函数提取与该树相关的信息并创建一个新的 `dendogram` 对象。然后可视化 `dendogram`，它就是树。此外，`print` 命令是可选的，可以被禁用。'
- en: 'Four arbitrarily chosen trees from the forest in the PDF file are displayed
    in the following figure, Trees of the Random Forest:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中显示了PDF文件中森林中的四个任意选择的树，随机森林的树：
- en: '![Random Forests](img/00187.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/00187.jpeg)'
- en: 'Figure 2: Trees of the Random Forest'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：随机森林的树
- en: 'A quick visit is paid to the Pima Indians diabetes problem. In the accuracy
    table of [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    we could see that the accuracy for the decision tree was 0.7588, or 75.88%:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 快速浏览了Pima印第安人糖尿病问题。在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术介绍")的准确率表中，*集成技术介绍*，我们可以看到决策树的准确率为0.7588，或75.88%：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Thus, we have an improved accuracy of 0.7704 – 0.7588 = 0.0116, or about 1.2%.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的准确率提高了0.7704 - 0.7588 = 0.0116，或大约1.2%。
- en: '**Exercise**: Obtain the error rate plot of the Pima Indian Diabetes problem.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：获取Pima印第安人糖尿病问题的错误率图。'
- en: Variable importance
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量重要性
- en: Statistical models, say linear regression and logistic regression, indicate
    which variables are significant with measures such as p-value and t-statistics.
    In a decision tree, a split is caused by a single variable. If the specification
    of the number of variables for the surrogate splits, a certain variable may appear
    as the split criteria more than once in the tree and some variables may never
    appear in the tree splits at all. During each split, we select the variable that
    leads to the maximum reduction in impurity, and the contribution of a variable
    across the tree splits would also be different. The overall improvement across
    each split of the tree (by the reduction in impurity for the classification tree
    or by the improvement in the split criterion) is referred to as the *variable
    importance*. In the case of ensemble methods such as bagging and random forest,
    the variable importance is measured for each tree in the technique. While the
    concept of variable importance is straightforward, its computational understanding
    is often unclear. This is primarily because a formula or an expression is not
    given in mathematical form. The idea is illustrated next through simple code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 统计模型，例如线性回归和逻辑回归，通过p值和t统计量等指标来指示哪些变量是显著的。在决策树中，分割是由单个变量引起的。如果指定代理分割的变量数量，某个变量可能在树中多次作为分割标准出现，而某些变量可能根本不会出现在树分割中。在每次分割中，我们选择导致最大不纯度减少的变量，并且一个变量在整个树分割中的贡献也会不同。整个树分割的总体改进（通过分类树的不纯度减少或通过分割标准的改进）被称为*变量重要性*。在集成方法如bagging和随机森林的情况下，对技术中的每棵树进行变量重要性的测量。虽然变量重要性的概念很简单，但其计算理解往往不清楚。这主要是因为没有给出数学形式的公式或表达式。下面的简单代码展示了这个想法。
- en: 'The `kyphosis` dataset from the `rpart` package consists of four variables,
    and the target variable here is named `Kyphosis`, indicating the presence of the
    kyphosis type of deformation following an operation. The three explanatory variables
    are `Age`, `Number`, and `Start`. We build a classification tree with zero surrogate
    variables for the split criteria with the `maxsurrogate=0` option. The choice
    of zero surrogates ensures that we have only one variable at a split. The tree
    is set up and visualized as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`rpart`包的`kyphosis`数据集包含四个变量，这里的目标变量命名为`Kyphosis`，表示手术后存在脊柱侧弯类型的变形。三个解释变量是`Age`、`Number`和`Start`。我们使用`maxsurrogate=0`选项构建了一个没有代理变量的分割标准的分类树。零代理变量的选择确保我们在分割处只有一个变量。树被设置并如下可视化：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Variable importance](img/00188.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![变量重要性](img/00188.jpeg)'
- en: 'Figure 3: Kyphosis Classification Tree'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：脊柱侧弯分类树
- en: In the no-surrogate tree, the first split variable is `Start`, with a terminal
    leaf on the right part of the split. The left side/partition further splits again
    with the `Start` variable, with a terminal node/leaf on the left side and a split
    on the later right side. In the next two split points, we use only the `Age` variable,
    and the `Number` variable is not used anywhere in the tree. Thus, we expect the
    `Number` variable to have zero importance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在无代理树中，第一个分割变量是`Start`，分割的右侧部分有一个终端叶。左侧/分区进一步使用`Start`变量再次分割，左侧有一个终端节点/叶，右侧有一个后续分割。在接下来的两个分割点，我们只使用`Age`变量，`Number`变量在整个树中任何地方都没有使用。因此，我们预计`Number`变量的重要性为零。
- en: 'Using `$variable.importance` on the fitted classification tree, we obtain the
    variable importance of the three explanatory variables:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合的分类树上使用 `$variable.importance`，我们获得了三个解释变量的变量重要性：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As expected, the `Number` variable is not shown as having any importance. The
    importance of `Start` is given as `7.783` and `Age` as `2.961`. To understand
    how R has computed these values, run the `summary` function on the classification
    tree:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，`Number` 变量没有显示任何重要性。`Start` 的重要性给出为 `7.783`，而 `Age` 为 `2.961`。要理解 R
    如何计算这些值，请在分类树上运行 `summary` 函数：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Four lines of output have been highlighted in the summary output, and each line
    contains the information about the split, the best of improvement offered by each
    of the variables, and the variable selected at the split. Thus, for the `Start`
    variable, the first highlighted line shows the improvement at `6.762` and the
    second line shows `1.021`. By adding these, we get `6.762 + 1.021 = 7.783`, which
    is the same as the output given from the `$variable.importance` extractor. Similarly,
    the last two highlighted lines show the contribution of `Age` as `1.274 + 1.714
    = 2.961`. Thus, we have clearly outlined the computation of the variable importance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要输出中已经突出显示了四行输出，每一行都包含了关于分割、每个变量提供的最佳改进以及分割时选择的变量的信息。因此，对于 `Start` 变量，第一行突出显示了
    `6.762` 的改进，第二行显示了 `1.021`。通过将这些值相加，我们得到 `6.762 + 1.021 = 7.783`，这与从 `$variable.importance`
    提取器给出的输出相同。同样，最后两行突出显示了 `Age` 的贡献，即 `1.274 + 1.714 = 2.961`。因此，我们已经清楚地概述了变量重要性的计算。
- en: '**Exercise**: Create a new classification tree, say `KC2`, and allow a surrogate
    split. Using the `summary` function, verify the computations associated with the
    variable importance.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：创建一个新的分类树，例如 `KC2`，并允许使用代理分割。使用 `summary` 函数验证与变量重要性相关的计算。'
- en: 'The `VarImpPlot` function from the `randomForest` package gives us a dot chart
    plot of the variable importance measure:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `randomForest` 包的 `VarImpPlot` 函数为我们提供了一个变量重要性测量的点图：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A visual display is given in the following figure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图给出了一个视觉展示：
- en: '![Variable importance](img/00189.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![变量重要性](img/00189.jpeg)'
- en: 'Figure 4: Variable Importance Plots for German and Pima Indian Diabetes Random
    Forests'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：德国和皮马印第安人糖尿病随机森林的变量重要性图
- en: Thus, the five most important variables for classifying the German credit as
    good or bad are `amount`, `checking`, `age`, `duration`, and `purpose`. For the
    Pima Indian Diabetes classification, the three most important variables are `glucose`,
    `mass`, and `age`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将德国信贷分类为好或坏的五个最重要的变量是 `amount`、`checking`、`age`、`duration` 和 `purpose`。对于皮马印第安人糖尿病分类，三个最重要的变量是
    `glucose`、`mass` 和 `age`。
- en: We will look at the notion of the proximity measure next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将探讨邻近度测量的概念。
- en: Proximity plots
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邻近图
- en: According to Hastie, et al. (2009), "*one of the advertised outputs of a random
    forest is a proximity plot"* (see page 595). But what are proximity plots? If
    we have *n* observations in the training dataset, a proximity matrix of order
    ![Proximity plots](img/00190.jpeg) is created. Here, the matrix is initialized
    with all the values at 0\. Whenever a pair of observations such as OOB occur jointly
    in the terminal node of a tree, the proximity count is increased by 1\. The proximity
    matrix is visualized using the multidimensional scaling method, a concept beyond
    the scope of this chapter, where the proximity matrix is represented in two dimensions.
    The proximity plots give an indication of which points are closer to each other
    from the perspective of the random forest.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Hastie 等人（2009）的说法，"*随机森林的一个宣传输出是邻近图"*（见第595页）。但邻近图是什么？如果我们有 *n* 个观测值在训练数据集中，就会创建一个阶数为
    ![邻近图](img/00190.jpeg) 的邻近矩阵。在这里，矩阵初始化时所有值都为0。每当一对观测值，如 OOB，在树的终端节点中同时出现时，邻近计数就会增加1。邻近矩阵使用多维尺度方法进行可视化，这是一个超出本章范围的概念，其中邻近矩阵以二维形式表示。邻近图从随机森林的角度给出了哪些点彼此更近的指示。
- en: 'In the earlier creation of random forests, we had not specified the option
    of a proximity matrix. Thus, we will first create the random forest using the
    option of proximity as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期创建随机森林时，我们没有指定邻近矩阵的选项。因此，我们将首先使用邻近选项创建随机森林，如下所示：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The options `proximity=TRUE,cob.prox=TRUE` are important to obtain the `proximity`
    matrix. We then simply make use of the `MDSplot` graphical function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 `proximity=TRUE,cob.prox=TRUE` 对于获得 `proximity` 矩阵非常重要。然后我们简单地使用 `MDSplot`
    图形函数：
- en: '![Proximity plots](img/00191.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![邻近图](img/00191.jpeg)'
- en: 'Figure 5: The multidimensional plot for the proximity matrix of an RF'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：RF邻近矩阵的多维图
- en: 'It is easier to find which observation is closest to a given observation from
    the proximity data perspective, and not the Euclidean distance, using the `which.max`
    function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从邻近数据的角度来看，使用`which.max`函数更容易找到与给定观察值最近的观察值，而不是欧几里得距离：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Thus, the observations numbered `657` in the training dataset (and `962` in
    the overall dataset) are closest to the first observation. Note that the overall
    position is because of the name extracted from the sample function. The `which.max`
    function is useful for finding the maximum position in an array.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练数据集中编号为`657`的观察值（以及在整体数据集中的`962`）与第一个观察值最接近。请注意，整体位置是由于从样本函数中提取的名称造成的。`which.max`函数对于在数组中找到最大位置是有用的。
- en: It turns out that most often, the graphical display using the `MDSplot` function
    results in a similar star-shape display. The proximity matrix also helps in carrying
    out cluster analysis, as will be seen in the concluding section of the chapter.
    Next, we will cover the parameters of a random forest in more detail.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，大多数情况下，使用`MDSplot`函数的图形显示结果是一个类似星形的显示。邻近矩阵也有助于进行聚类分析，这将在本章的结尾部分看到。接下来，我们将更详细地介绍随机森林的参数。
- en: Random Forest nuances
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林的细微差别
- en: The `GC_Random_Forest.pdf` file consists of the 500 trees which serve as the
    homogeneous learners in the random forest ensemble. It is well known that a decision
    tree has a nice and clear interpretation. This is because it shows how one traverses
    the path to a terminal node. The random selection of features at each split and
    the bootstrap samples lead to the setting up of the random forest. Refer to the
    figure *Trees of the Random Forest*, which depicts trees numbered `78`, `176`,
    `395`, and `471`. The first split across the four trees is respectively `purpose`,
    `amount`, `property`, and `duration`. The second split for the first left side
    of these four trees is `employed`, `resident`, `purpose`, and `amount`, respectively.
    It is a cumbersome exercise to see which variables are meaningful over the others.
    We know that the earlier a variable appears, the higher its importance is. The
    question that then arises is, with respect to a random forest, how do we find
    the depth distribution of the variables? This and many other points are addressed
    through a powerful random forest package available as `randomForestExplainer`,
    and it is not an exaggeration that this section would not have been possible without
    this awesome package.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`GC_Random_Forest.pdf`文件包含了500棵树，这些树作为随机森林集成中的同质学习器。众所周知，决策树有一个很好且清晰的解释。这是因为它展示了如何遍历路径到一个终端节点。在每个分割点随机选择特征和自举样本导致随机森林的建立。请参考图*随机森林的树*，它描绘了编号为`78`、`176`、`395`和`471`的树。这四棵树的第一分割点分别是`目的`、`金额`、`属性`和`持续时间`。这四棵树第一左边的第二分割点分别是`雇员`、`居民`、`目的`和`金额`。看到哪些变量比其他变量更有意义是一项繁琐的练习。我们知道，变量出现得越早，其重要性就越高。随之而来的问题是，对于随机森林而言，我们如何找到变量的深度分布？这一点以及许多其他问题都通过一个强大的随机森林包`randomForestExplainer`得到了解决，而且毫不夸张地说，没有这个了不起的包，本节将无法实现。'
- en: 'By applying the `min_depth_distribution` function on the random forest object,
    we get the depth distribution of the variables. Using `plot_min_depth_distribution`,
    we then get the plot of minimum depth distribution:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对随机森林对象应用`min_depth_distribution`函数，我们得到变量的深度分布。然后使用`plot_min_depth_distribution`，我们得到最小深度分布的图：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result of the preceding code block is *Minimum Depth Distribution of German
    Random Forest*, which is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个代码块的结果是*德国随机森林的最小深度分布*，具体如下：
- en: '![Random Forest nuances](img/00192.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林的细微差别](img/00192.jpeg)'
- en: 'Figure 6: Minimum Depth Distribution of German Random Forest'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：德国随机森林的最小深度分布
- en: 'From previous figure, it is clear that the `checking` variable appears more
    often as the primary split, followed by `savings`, `purpose`, `amount`, and `duration`.
    Consequently, we get a useful depiction through the minimum depth distribution
    plot. Further analyses are possible by using the `measure_importance` function,
    which gives us various measures of importance for the variables of the random
    forest:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，`checking`变量作为主要分割点出现的频率更高，其次是`savings`、`目的`、`金额`和`持续时间`。因此，通过最小深度分布图，我们得到了一个有用的描述。可以通过使用`measure_importance`函数进行进一步的分析，它为我们提供了随机森林变量各种重要性度量：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are warned here that the random forest has not been grown with the option
    of `localImp = TRUE`, which is central to obtaining the measures. Thus, we create
    a new random forest with this option, and then run the `measure_importance` function
    on it:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里被警告，随机森林没有使用`localImp = TRUE`选项进行生长，这是获得度量的关键。因此，我们创建一个新的随机森林，然后在其上运行`measure_importance`函数：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output has a wider format, and hence we provide it in an image format and
    display the result vertically in *Analysis of Variable Importance Measure*. We
    can see that the `measure_importance` function gives a lot of information on the
    average minimum depth, number of nodes across the 500 trees that the variable
    appears as node, the average decrease in accuracy, the Gini decrease, and so on.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出格式更宽，因此我们以图像格式提供它，并在*变量重要性度量分析*中垂直显示结果。我们可以看到，`measure_importance`函数提供了关于平均最小深度、变量作为节点出现在500棵树中的节点数量、平均准确度下降、基尼减少等方面的许多信息。
- en: 'We can see from the output that if the mean minimum depth is higher, the associated
    p-value is also higher and hence the variable is insignificant. For example, the
    variables `coapp`, `depends`, `existcr`, `foreign`, and `telephon` have a higher
    mean minimum depth, and their p-value is also 1 in most cases. Similarly, lower
    values of `gini_decrease` are also associated with higher p-values, and this indicates
    the insignificance of the variables:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，如果平均最小深度较高，相关的p值也较高，因此变量不显著。例如，变量`coapp`、`depends`、`existcr`、`foreign`和`telephon`的平均最小深度较高，在大多数情况下它们的p值也是1。同样，`gini_decrease`的较低值也与较高的p值相关联，这表明变量的不显著性：
- en: '![Random Forest nuances](img/00193.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林细节](img/00193.jpeg)'
- en: 'Figure 7: Analysis of Variable Importance Measure'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：变量重要性度量分析
- en: 'The importance measure object `GC2_RF_VIM` can be used for further analyses.
    For the `no_of_nodes` measure, we can compare the various metrics from the previous
    variable importance measures. For instance, we would like to see how the `times_a_root`
    values for the variables turns out against the mean minimum depth. Similarly,
    we would like to analyze other measures. By applying the `plot_multi_way_importance`
    graphical function on this object, we get the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用重要性度量对象`GC2_RF_VIM`进行进一步分析。对于`no_of_nodes`度量，我们可以比较之前变量重要性度量中的各种指标。例如，我们想看看变量的`times_a_root`值与平均最小深度的关系。同样，我们还想分析其他度量。通过在此对象上应用`plot_multi_way_importance`图形函数，我们得到以下输出：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Random Forest nuances](img/00194.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林细节](img/00194.jpeg)'
- en: 'Figure 8: Multi-way Importance Plot for the German Credit Data'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：德国信贷数据的多元重要性图
- en: Here, the `times_a_root` values of the variables are plotted against the mean
    minimum depth, `mean_min_depth`, while keeping the number of nodes to their size.
    The non-top variables are black, while the top variables are blue. Similarly,
    we plot `gini_decrease`, `no_of_trees` and `p_value` against `mean_min_depth`
    in the preceding figure.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，变量的`times_a_root`值与平均最小深度`mean_min_depth`相对应，同时保持节点数量与其大小一致。非顶级变量为黑色，而顶级变量为蓝色。同样，我们在前一个图中将`gini_decrease`、`no_of_trees`和`p_value`与`mean_min_depth`相对应。
- en: 'The correlation between the five measures is depicted next, using the `plot_importance_ggpairs`
    function:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`plot_importance_ggpairs`函数绘制了五个度量之间的相关性，如下所示：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Random Forest nuances](img/00195.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林细节](img/00195.jpeg)'
- en: 'Figure 9: Relationship Between the Measures of Importance'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：重要性度量之间的关系
- en: Since the measures are strongly correlated, either positively or negatively,
    we need to have all five of these measures to understand random forests.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些度量高度相关，无论是正相关还是负相关，我们需要所有五个这些度量来理解随机森林。
- en: 'A great advantage of the tree structure is the interpretation of interaction
    between the variables. For instance, if the split in a parent is by one variable,
    and by another variable in the daughter node, we can conclude that there is interaction
    between these two variables. Again, the question arises for the random forests.
    Using the `important_variables` and `min_depth_interactions`, we can obtain the
    interactions among the variables of a random forest as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 树结构的优点之一是解释变量之间的交互作用。例如，如果父节点中的分割是由一个变量进行的，而在子节点中是由另一个变量进行的，我们可以得出这两个变量之间存在交互的结论。再次，对于随机森林来说，问题又出现了。使用`important_variables`和`min_depth_interactions`，我们可以获得随机森林中变量之间的交互如下：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output that will be obtained:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将获得的结果：
- en: '![Random Forest nuances](img/00196.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林的细微差别](img/00196.jpeg)'
- en: 'Figure 10: Minimum Depth Interaction for German Random Forest'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：德国随机森林的最小深度交互
- en: Thus, we can easily find the interaction variables of the random forest.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以轻松地找到随机森林的交互变量。
- en: The `randomForestExplainer` R package is very powerful and helps us to carry
    out many diagnostics after obtaining the random forests. Without post diagnostics,
    we cannot evaluate any fitted model. Consequently, the reader is advised to carry
    out most of the steps learned in this section in their implementation of random
    forests.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`randomForestExplainer` R包非常强大，帮助我们获得随机森林后执行许多诊断。没有后诊断，我们无法评估任何拟合模型。因此，建议读者在本节的实现中执行大多数学习到的步骤。'
- en: We will compare a random forest with the bagging procedure in the next section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将比较随机森林与Bagging过程。
- en: '**Exercise**: Carry out the diagnostics for the random forest built for the
    Pima Indian Diabetes problem.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：对为皮马印第安人糖尿病问题构建的随机森林进行诊断。'
- en: Comparisons with bagging
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与Bagging的比较
- en: When comparing the random forest results with the bagging counterpart for the
    German credit data and Pima Indian Diabetes datasets, we did not see much improvement
    in the accuracy over the validated partition of the data. A potential reason might
    be that the variability reduction achieved by bagging is at the optimum reduced
    variance, and that any bias improvement will not lead to an increase in the accuracy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当将随机森林的结果与德国信用数据集和皮马印第安人糖尿病数据集的Bagging对应方进行比较时，我们没有在数据的验证分区上看到准确率的明显提高。可能的原因是Bagging实现的方差减少已经达到最佳，任何偏差的改进都不会导致准确率的提高。
- en: 'We consider a dataset to be available from the R core package `kernlab`. The
    dataset is spam and it has a collection of 4601 emails with labels that state
    whether the email is spam or non-spam. The dataset has a good collection of 57
    variables derived from the email contents. The task is to build a good classifier
    for the spam identification problem. The dataset is quickly partitioned into training
    and validation partitions, as with earlier problems:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为数据集可以从R核心包`kernlab`中获取。该数据集是垃圾邮件，包含4601封带有标签的电子邮件，标签表明电子邮件是垃圾邮件还是非垃圾邮件。数据集包含从电子邮件内容中派生的57个变量的良好集合。任务是构建一个用于垃圾邮件识别问题的良好分类器。该数据集迅速划分为训练集和验证集，就像早期问题一样：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'First, we will build the simple classification tree:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将构建简单的分类树：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The classification tree gives a modest accuracy of about 90%. We will then
    apply `randomForest` and build the random forest:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树给出了大约90%的适度准确率。然后我们将应用`randomForest`并构建随机森林：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Bagging can be performed with the `randomForest` function. The trick is to
    ask the random forest to use all the variables while setting up a split. Thus,
    the choice of `mtry=ncol(spal_TestX)` will select all the variables and bagging
    is then easily performed:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`randomForest`函数进行Bagging。技巧是要求随机森林在设置分割时使用所有变量。因此，选择`mtry=ncol(spal_TestX)`将选择所有变量，然后Bagging就很容易执行：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The increase in accuracy is also reflected in the accuracy plots, as shown
    in the following figure:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率的提高也反映在准确率图中，如下所示：
- en: '![Comparisons with bagging](img/00197.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![与Bagging的比较](img/00197.jpeg)'
- en: 'Figure 11: Random Forest and Bagging Comparisons for the Spam Classification
    Problem'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：垃圾邮件分类问题的随机森林和Bagging比较
- en: We will look at some niche applications of random forests in the concluding
    two sections.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后两节中，我们将探讨随机森林的一些利基应用。
- en: Missing data imputation
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失数据插补
- en: Missing data is a menace! It pops up out of nowhere and blocks analysis until
    it is properly taken care of. The statistical technique of the expectation-maximization
    algorithm, or simply the EM algorithm, needs a lot of information on the probability
    distributions, structural relationship, and in-depth details of statistical models.
    However, an approach using the EM algorithm is completely ruled out here. Random
    forests can be used to overcome the missing data problem.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据是一个威胁！它突然出现并阻止分析，直到得到妥善处理。期望最大化算法的统计技术，或简称为EM算法，需要对概率分布、结构关系和统计模型的深入细节有大量信息。然而，在这里完全排除了使用EM算法的方法。随机森林可以用来克服缺失数据问题。
- en: We will use the `missForest` R package to fix the missing data problem whenever
    we come across it in the rest of the book. The algorithm for the `missForest`
    function and other details can be found at [https://academic.oup.com/bioinformatics/article/28/1/112/219101](https://academic.oup.com/bioinformatics/article/28/1/112/219101).
    For any variable/column with missing data, the technique is to build a random
    forest for that variable and obtain the OOB prediction as the imputation error
    estimates. Note that the function can handle continuous as well as categorical
    missing values. The creators of the package have enabled the functions with parallel
    run capability to save time.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本书的其余部分遇到缺失数据问题时，我们将使用`missForest` R包来修复。`missForest`函数的算法和其他细节可以在[https://academic.oup.com/bioinformatics/article/28/1/112/219101](https://academic.oup.com/bioinformatics/article/28/1/112/219101)找到。对于任何具有缺失数据的变量/列，技术是为此变量构建一个随机森林，并将OOB预测作为插补误差估计。请注意，该函数可以处理连续和分类的缺失值。该包的创建者已启用并行运行功能以节省时间。
- en: 'We will take a simple dataset from [https://openmv.net/info/travel-times](https://openmv.net/info/travel-times),
    and there are missing values in the data. The data consists of `13` variables
    and `205` observations. Of the `13` variables available, only the `FuelEconomy`
    variable has missing values. Let''s explore the dataset in the R terminal:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从[https://openmv.net/info/travel-times](https://openmv.net/info/travel-times)获取一个简单的数据集，并且数据中存在缺失值。数据由`13`个变量和`205`个观测值组成。在可用的`13`个变量中，只有`FuelEconomy`变量有缺失值。让我们在R终端中探索这个数据集：
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It can be seen that there are `19` observations with missing values. The `sapply`
    function tells us that all `19` observations have missing values for the `FuelEconomy`
    variable only. The `missForest` function is now deployed in action:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到有`19`个观测值存在缺失值。`sapply`函数告诉我们，所有`19`个观测值只有`FuelEconomy`变量存在缺失值。现在正在部署`missForest`函数：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We have now imputed the missing values. It needs to be noted that the imputed
    values should make sense and should not look out of place. In [Chapter 9](part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee
    "Chapter 9. Ensembling Regression Models"), *Ensembling Regression Models*, we
    will use the `missForest` function to impute a lot of missing values.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经插补了缺失值。需要注意的是，插补值应该有意义，并且不应该显得不合适。在[第9章](part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee
    "第9章。集成回归模型")中，我们将使用`missForest`函数来插补大量缺失值。
- en: '**Exercise**: How can the imputed values be validated? Use the `prodNA` function
    from the `missForest` package and puncture good values with missing data. Using
    the `missForest` function, get the imputed values and compare them with the original
    values.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：如何验证插补值？使用`missForest`包中的`prodNA`函数，并用缺失数据戳记良好值。使用`missForest`函数获取插补值，并与原始值进行比较。'
- en: The proximity matrix tells us how close the observations are from the random
    forest perspective. If we have information about the observations neighborhood,
    we can carry out a cluster analysis. As a by-product of using the proximity matrix,
    we can now also use random forests for unsupervised problems.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 邻近矩阵告诉我们从随机森林的角度来看观测值有多接近。如果我们有关于观测值邻域的信息，我们可以进行聚类分析。使用邻近矩阵的副产品，我们现在也可以使用随机森林来解决无监督问题。
- en: Clustering with Random Forest
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林聚类
- en: Random forests can be set up without the target variable. Using this feature,
    we will calculate the proximity matrix and use the OOB proximity values. Since
    the proximity matrix gives us a measure of closeness between the observations,
    it can be converted into clusters using hierarchical clustering methods.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以设置不包含目标变量。使用此功能，我们将计算邻近矩阵并使用OOB邻近值。由于邻近矩阵为我们提供了观测之间的接近度度量，因此可以使用层次聚类方法将其转换为聚类。
- en: 'We begin with the setup of `y = NULL` in the `randomForest` function. The options
    of `proximity=TRUE` and `oob.prox=TRUE` are specified to ensure that we obtain
    the required proximity matrix:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从在`randomForest`函数中设置`y = NULL`开始。指定`proximity=TRUE`和`oob.prox=TRUE`选项以确保我们获得所需的邻近矩阵：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we use the `hclust` function with the option of `ward.D2` to carry out
    the hierarchical cluster analysis on the proximity matrix of dissimilarities.
    The `cutree` function divides the `hclust` object into `k = 6` number of clusters.
    Finally, the `table` function and the visuals give an idea of how good the clustering
    has been by using the random forests:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`hclust`函数和`ward.D2`选项对距离矩阵进行层次聚类分析。`cutree`函数将`hclust`对象划分为`k = 6`个聚类。最后，`table`函数和可视化给出了使用随机森林进行聚类效果的好坏概念：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is a diagram illustrating clustering using random forests:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个说明使用随机森林进行聚类的图表：
- en: '![Clustering with Random Forest](img/00198.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机森林进行聚类](img/00198.jpeg)'
- en: 'Figure 12: Clustering Using Random Forests'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：使用随机森林进行聚类
- en: Although the clusters provided by the random forests do not fit the label identification
    problem, we will take them as a starting point. It needs to be understood that
    random forests can be used properly for cluster analysis.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机森林提供的聚类不适合标签识别问题，但我们仍将它们作为起点。需要理解的是，随机森林可以正确地用于聚类分析。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Random forests were created as an improvement on the bagging method. As an example
    of the homogeneous ensemble method, we saw how the forests help in obtaining higher
    accuracy. Visualization and variable importance for random forests were thoroughly
    detailed. We also saw a lot of diagnostic methods that can be used after fitting
    a random forest. The method was then compared with bagging. Novel applications
    of random forest for missing data imputation and cluster analysis were also demonstrated.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是为了改进袋装方法而创建的。作为一个同质集成方法的例子，我们看到了森林如何帮助提高准确性。随机森林的可视化和变量重要性被详细阐述。我们还看到了在拟合随机森林后可以使用的许多诊断方法。然后，该方法与袋装方法进行了比较。还展示了随机森林在缺失数据插补和聚类分析中的新颖应用。
- en: In the next chapter, we will look at boosting, which is a very important ensemble.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨提升，这是一种非常重要的集成方法。
