- en: Ensemble Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: In this chapter, we are going to discuss some important algorithms that exploit
    different estimators to improve the overall performance of an ensemble or committee.
    These techniques work either by introducing a medium level of randomness in every
    estimator belonging to a predefined set or by creating a sequence of estimators
    where, each new model is forced to improve the performance of the previous ones.
    These techniques allow us to reduce both the bias and the variance (thereby increasing
    validation accuracy) when employing models with a limited capacity or more prone
    to overfit the training set.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一些重要的算法，这些算法利用不同的估计器来提高集成或委员会的整体性能。这些技术要么通过在每个属于预定义集的估计器中引入中等程度的随机性来实现，要么通过创建一系列估计器，其中每个新的模型都被迫提高前一个模型的表现。这些技术使我们能够在使用有限容量或更容易过拟合训练集的模型时，减少偏差和方差（从而提高验证准确率）。
- en: 'In particular, the topics covered in the chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的特定主题如下：
- en: Introduction to ensemble learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习方法介绍
- en: A brief introduction to decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树简介
- en: Random forest and extra randomized forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林和额外的随机森林
- en: AdaBoost (algorithms M1, SAMME, SAMME.R, and R2)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost（算法M1，SAMME，SAMME.R和R2）
- en: Gradient boosting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Ensembles of voting classifiers, stacking, and bucketing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票分类器集成、堆叠和分桶
- en: Ensemble learning fundamentals
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习基础
- en: 'The main concept behind ensemble learning is the distinction between strong
    and weak learners. In particular, a strong learner is a classifier or a regressor
    which has enough capacity to reach the highest potential accuracy, minimizing
    both bias and variance (thus achieving also a satisfactory level of generalization).
    More formally, if we consider a parametrized binary classifier *f(x; θ)*, we define
    it as a strong learner if the following is true:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习背后的主要概念是强学习者和弱学习者的区别。特别是，强学习器是一个分类器或回归器，它具有足够的容量达到最高的潜在准确率，最小化偏差和方差（从而实现一个令人满意的泛化水平）。更正式地说，如果我们考虑一个参数化的二元分类器
    *f(x; θ)*，我们定义它为强学习器，如果以下条件成立：
- en: '![](img/e85388b3-c547-42a0-a706-4e9975ae1a92.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e85388b3-c547-42a0-a706-4e9975ae1a92.png)'
- en: This expression can appear cryptic; however, it's very easy to understand. It
    simply expresses the concept that a strong learner is theoretically able to achieve
    any non-null probability of misclassification with a probability greater than
    or equal to 0.5 (that is, the threshold for a binary random guess). All the models
    normally employed in Machine Learning tasks are normally strong learners, even
    if their domain can be limited (for example, a logistic regression cannot solve
    non-linear problems). On the other hand, a weak learner is a model that is generically
    able to achieve an accuracy slightly higher than a random guess, but whose complexity
    is very low (they can be trained very quickly, but can never be used alone to
    solve complex problems). There is a formal definition also in this case, but it's
    simpler to consider that the real main property of a weak learner is a limited
    ability to achieve a reasonable accuracy. In some very particular and small regions
    of the training space, a weak learner could reach a low probability of misclassification,
    but in the whole space its performance is only a little bit superior to a random
    guess. The previous one is more a theoretical definition than a practice one,
    because all the models currently available are normally quite better than a random
    oracle. However, an ensemble is defined as a set of weak learners that are trained
    together (or in a sequence) to make up a committee. Both in classification and
    regression problems, the final result is obtained by averaging the predictions
    or employing a majority vote.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式可能看起来有些晦涩；然而，它实际上非常容易理解。它仅仅表达了这样一个概念：一个强大的学习者在理论上能够以大于或等于0.5的概率（即二元随机猜测的阈值）实现任何非零误分类概率。在机器学习任务中通常使用的所有模型都是强大的学习者，即使它们的领域可能有限（例如，逻辑回归无法解决非线性问题）。另一方面，一个弱学习者是一个通常能够实现略高于随机猜测的准确率的模型，但其复杂性非常低（它们可以非常快地训练，但永远不能单独用来解决复杂问题）。在这种情况下，也存在一个正式的定义，但更简单的是考虑弱学习者的真正主要特性是有限的实现合理准确率的能力。在训练空间的某些非常特定且小的区域内，弱学习者可以达到低误分类概率，但在整个空间中，其性能仅略优于随机猜测。前一个定义更多的是理论上的，而不是实践上的，因为目前所有可用的模型通常都比随机预言者要好得多。然而，集成被定义为一起训练（或按顺序）的一组弱学习者，以形成一个委员会。在分类和回归问题中，最终结果是通过平均预测或采用多数投票来获得的。
- en: At this point, a reasonable question is—Why do we need to train many weak learners
    instead of a single strong one? The answer is double—in ensemble learning, we
    normally work with medium-strong learners (such as decision trees or **support
    vector machines** (**SVMs**)) and we use them as a committee to increase the overall
    accuracy and reduce the variance thanks to a wider exploration of the sample space.
    In fact, while a single strong learner is often able to overfit the training set,
    it's more difficult to keep a high accuracy over the whole sample subspace without
    saturating the capacity. In order to avoid overfitting, a trade-off must be found
    and the result is a less accurate classifier/regressor with a simpler separation
    hyperplane. The adoption of many weak learners (that are actually quite strong,
    because even the simplest models are more accurate than a random guess), allows
    us to force them to focus only on a limited subspace, so as to be able to reach
    a very high local accuracy with a low variance. The committee, employing an averaging
    technique, can easily find out which prediction is the most suitable. Alternatively,
    it can ask each learner to vote, assuming that a successful training process must
    always lead the majority to propose the most accurate classification or prediction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，一个合理的问题可能是——为什么我们需要训练许多弱学习者而不是一个单一的强大学习者？答案是双重的——在集成学习中，我们通常处理中等强大的学习者（如决策树或**支持向量机**（**SVMs**）），并将它们作为一个委员会来提高整体准确率并减少方差，这得益于对样本空间的更广泛探索。事实上，虽然一个强大的学习者通常能够过度拟合训练集，但要在整个样本子空间中保持高准确率而不饱和其容量则更为困难。为了避免过度拟合，必须找到一种权衡，结果是得到一个准确率较低但分离超平面更简单的分类器/回归器。采用许多弱学习者（实际上它们相当强大，因为即使是简单的模型也比随机猜测更准确），可以迫使他们只关注一个有限的子空间，从而能够以低方差达到非常高的局部准确率。委员会采用平均技术，可以轻松找出哪个预测是最合适的。或者，它可以要求每个学习者进行投票，假设成功的训练过程必须总是引导多数人提出最准确的分类或预测。
- en: 'The most common approaches to ensemble learning are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的最常见方法如下：
- en: '**Bagging (bootstrap aggregating)**: This approach trains *n* weak learners
    *fw1, fw2, ..., fwn* (very often they are decision trees) using *n* training sets
    (*D1, D2, ..., Dn*) created by randomly sampling the original dataset *D*. The
    sampling process (called **bootstrap sampling**) is normally performed with replacement,
    so as to determine different data distributions. Moreover, in many real algorithms,
    the weak learners are also initialized and trained using a medium degree of randomness.
    In this way, the probability of having clones becomes very small and, at the same,
    time it''s possible to increase the accuracy by keeping the variance under a tolerable
    threshold (thus avoiding overfitting).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**袋装（Bagging，即自助聚合）**：这种方法使用*n*个弱学习器*fw1, fw2, ..., fwn*（通常它们是决策树）来训练*n*个训练集（*D1,
    D2, ..., Dn*），这些训练集是通过随机抽样原始数据集*D*创建的。抽样过程（称为**自助抽样**）通常是通过替换来执行的，以便确定不同的数据分布。此外，在许多实际算法中，弱学习器也是使用中等程度的随机性初始化和训练的。这样，克隆的概率变得非常小，同时，通过保持方差在可容忍的阈值以下（从而避免过拟合），可以提高准确性。'
- en: '**Boosting**: This is an alternative approach that builds an incremental ensemble
    starting with a single weak learner *fw1* and adding a new one *fwi* at each iteration.
    The goal is to reweight the dataset, so as to force the new learner to focus on
    the samples that were previously misclassified. This strategy yields a very high
    accuracy because the new learners are trained with a positively-biased dataset
    that allows them to adapt to the most difficult internal conditions. However,
    in this way, the control over the variance is weakened and the ensemble can more
    easily overfit the training set. It''s possible to mitigate this problem by reducing
    the complexity of the weak learners or imposing a regularization constraint.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升（Boosting）**：这是一种替代方法，它从单个弱学习器*fw1*开始构建增量集成，并在每次迭代中添加一个新的学习器*fwi*。目标是重新加权数据集，以便迫使新的学习器关注先前被错误分类的样本。这种策略产生了非常高的准确性，因为新的学习器是在一个正偏置的数据集上训练的，这使得它们能够适应最困难的内部条件。然而，以这种方式，对方差的控制减弱了，集成更容易过拟合训练集。可以通过减少弱学习器的复杂性或施加正则化约束来减轻这个问题。'
- en: '**Stacking**: This method can be implemented in different ways but the philosophy
    is always the same—use different algorithms (normally a few strong learners) trained
    on the same dataset and filter the final result using another classifier, averaging
    the predictions or using a majority vote. This strategy can be very powerful if
    the dataset has a structure that can be partially managed with different approaches.
    Each classifier or regressor should discover some data aspects that are peculiar;
    that''s why the algorithms must be structurally different. For example, it can
    be useful to mix a decision tree with a SVM or linear and kernel models. The evaluation
    performed on the test set should clearly show the prevalence of a classifier only
    in some cases. If an algorithm is finally the only one that produces the best
    prediction, the ensemble becomes useless and it''s better to focus on a single
    strong learner.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠（Stacking）**：这种方法可以以不同的方式实现，但其哲学始终如一——使用在相同数据集上训练的不同算法（通常是几个强大的学习器），并使用另一个分类器过滤最终结果，平均预测或使用多数投票。如果数据集具有可以通过不同方法部分管理的结构，这种策略可以非常强大。每个分类器或回归器都应该发现一些独特的数据方面；这就是为什么算法必须在结构上不同。例如，将决策树与SVM或线性核模型混合可能是有用的。在测试集上进行的评估应清楚地显示在某些情况下只有分类器占主导地位。如果一个算法最终是唯一产生最佳预测的算法，那么集成就变得无用了，最好专注于单个强大的学习器。'
- en: Random forests
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'A random forest is the bagging ensemble model based on decision trees. If the
    reader is not familiar with this kind of model, I suggest reading the *Introduction
    to Machine Learning*, *Alpaydin E.*, *The MIT Press*, where a complete explanation
    can be found. However, for our purposes, it''s useful to provide a brief explanation
    of the most important concepts. A decision tree is a model that resembles a standard
    hierarchical decision process. In the majority of cases, a special family is employed,
    called binary decision trees, as each decision yields only two outcomes. This
    kind of tree is often the simplest and most reasonable choice and the training
    process (which consists in building the tree itself) is very intuitive. The root
    contains the whole dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是基于决策树的袋装集成模型。如果读者不熟悉这类模型，我建议阅读 *《机器学习导论》*，*Alpaydin E.*，*麻省理工学院出版社*，在那里可以找到完整的解释。然而，为了我们的目的，提供对最重要的概念的简要解释是有用的。决策树是一个类似于标准分层决策过程的模型。在大多数情况下，使用一个特殊的家族，称为二叉决策树，因为每个决策只产生两个结果。这种树通常是简单且合理的最佳选择，训练过程（即构建树本身）非常直观。根节点包含整个数据集：
- en: '![](img/207960e3-1f7e-440d-9a30-14e364aae79c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/207960e3-1f7e-440d-9a30-14e364aae79c.png)'
- en: 'Each level is obtained by applying a selection tuple, defined as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 每个级别是通过应用以下定义的选择元组获得的：
- en: '![](img/94702ff1-52eb-4f64-a05e-d6520cfe18fc.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/94702ff1-52eb-4f64-a05e-d6520cfe18fc.png)'
- en: 'The first index of the tuple corresponds to an input feature, while the threshold
    *t[i]* is a value chosen in the specific range of each feature. The application
    of a selection tuple leads to a split and two nodes that contain each a non-overlapping
    subset of the input dataset. In the following diagram, there''s an example of
    a slip performed at the level of the root (initial split):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 元组的第一个索引对应于一个输入特征，而阈值 *t[i]* 是在每个特征特定范围内选择的一个值。应用一个选择元组会导致一个分割和两个节点，每个节点包含输入数据集的非重叠子集。在下面的图中，有一个在根级别（初始分割）执行的分割示例：
- en: '![](img/a04f6f3f-79da-4289-b0a7-db4ddb65353d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a04f6f3f-79da-4289-b0a7-db4ddb65353d.png)'
- en: Example of initial split in a decision tree
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树初始分割的示例
- en: The set *X* is split into two subsets defined as *X11* and *X12* whose samples
    have respectively the feature with *i=2* less or greater than the threshold *ti=0.8*.
    The intuition behind classification decision trees is to continue splitting until
    the leaves contain samples belonging to a single category *yi* (these nodes are
    defined as pure). In this way, a new sample *xj* can traverse the tree with a
    computation complexity *O(log(M))* and reach a final node that determines its
    category. In a very similar way, it's possible to build regression trees whose
    output is continuous (even if, for our purposes, we are going to consider only
    classification scenarios).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 集合 *X* 被分割成两个子集，分别定义为 *X11* 和 *X12*，其样本分别具有 *i=2* 小于或大于阈值 *ti=0.8* 的特征。分类决策树背后的直觉是继续分割，直到叶子节点包含属于单个类别
    *yi* 的样本（这些节点被定义为纯净的）。这样，一个新的样本 *xj* 可以通过计算复杂度 *O(log(M))* 遍历树，并达到一个最终节点，该节点确定其类别。以非常相似的方式，可以构建输出连续的回归树（即使，为了我们的目的，我们将只考虑分类场景）。
- en: 'At this point, the main problem is how to perform each split. We cannot pick
    any feature and any threshold, because the final tree will be completely unbalanced
    and very deep. Our goal is to find the optimal selection tuple at each node considering
    the final goal, which is classification into discrete categories (the process
    is almost identical for regressions). The technique is very similar to a problem
    based on a cost function that must be minimized, but, in this case, we operate
    locally, applying an impurity measure proportional to the heterogeneity of a node.
    A high impurity indicates that samples belonging to many different categories
    are present, while an impurity equal to 0 indicates that a single category is
    present. As we need to continue splitting until a pure leaf appears, the optimal
    choice is based on a function that scores each selection tuple, allowing us to
    select the one that yields the lowest impurity (theoretically, the process should
    continue until all the leaves are pure, but normally a maximum depth is provided,
    so as to avoid excessive complexity). If there are p classes, the category set
    can be defined as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，主要问题是如何执行每个分割。我们不能随意选择任何特征和任何阈值，因为最终的树将完全不平衡且非常深。我们的目标是找到每个节点处的最佳选择元组，考虑到最终目标是分类到离散类别（对于回归过程几乎相同）。这种技术非常类似于基于必须最小化的成本函数的问题，但在这个情况下，我们局部操作，应用与节点异质性成比例的不纯度度量。高不纯度表明存在属于许多不同类别的样本，而不纯度等于
    0 则表明存在单个类别。由于我们需要继续分割直到出现纯净的叶子节点，最佳选择基于一个对每个选择元组进行评分的函数，使我们能够选择产生最低不纯度的那个（理论上，过程应该继续到所有叶子都是纯净的，但通常提供一个最大深度，以避免过度的复杂性）。如果有
    p 个类别，类别集可以定义为以下：
- en: '![](img/f687377f-feb0-482c-88b0-290fcb4a533f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f687377f-feb0-482c-88b0-290fcb4a533f.png)'
- en: 'A very common impurity measure is called **Gini impurity** and it''s based
    on the probability of a misclassification if a sample is categorized using a label
    randomly chosen from the node subset distribution. Intuitively, if all the samples
    belong to the same category, any random choice leads to a correct classification
    (and the impurity becomes *0*). On the other side, if the node contains samples
    from many categories, the probability of a misclassification increases. Formally,
    the measure is defined as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常常见的纯度度量称为 **基尼不纯度**，它基于如果使用从节点子集分布中随机选择的标签对样本进行分类时的错误分类概率。直观上，如果所有样本都属于同一类别，任何随机选择都会导致正确分类（并且不纯度变为
    *0*）。另一方面，如果节点包含来自许多类别的样本，错误分类的概率会增加。正式地，该度量定义为以下：
- en: '![](img/420ffd2d-0948-4cc2-99fe-6b3a5e64f1b1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/420ffd2d-0948-4cc2-99fe-6b3a5e64f1b1.png)'
- en: 'The subset is indicated by *Xk* and *p(j|k)* is obtained as the ratio of the
    samples belonging to the class *j* over the total number of samples. The selection
    tuple must be chosen so as to minimize the Gini impurity of the children. Another
    common approach is the cross-entropy impurity, defined as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 子集由 *Xk* 表示，而 *p(j|k)* 是通过属于类别 *j* 的样本数与样本总数的比率获得的。选择元组必须被选择以最小化子节点的基尼不纯度。另一种常见的方法是交叉熵不纯度，定义如下：
- en: '![](img/95e8f999-d39c-40ee-ad7e-405a812b0875.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/95e8f999-d39c-40ee-ad7e-405a812b0875.png)'
- en: 'The main difference between this measure and the previous one is provided by
    some fundamental information theory concepts. In particular, the goal we want
    to reach is the minimization of the uncertainty, which is measured using the *(Cross-)Entropy*.
    If we have a discrete distribution and all the samples belong to the same category,
    a random choice is can fully describe the distribution; therefore, the uncertainty
    is null. On the contrary, if, for example, we have a fair die, the probability
    of each outcome is 1/6 and the corresponding entropy is about 2.58 bits (if the
    base of the logarithm is 2). When the nodes become purer and purer, the cross-entropy
    impurity decreases and reaches 0 in an optimal scenario. Moreover, adopting the
    concept of mutual information, we can define the information gain obtained after
    a split has been performed:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一种度量方法相比，这种差异主要由一些基本的信息理论概念提供。特别是，我们想要达到的目标是最小化不确定性，这使用 *(交叉-)熵* 来衡量。如果我们有一个离散分布，并且所有样本都属于同一类别，那么随机选择可以完全描述该分布；因此，不确定性为零。相反，例如，如果我们有一个公平的骰子，每个结果的概率是
    1/6，相应的熵大约是 2.58 比特（如果对数的底是 2）。当节点变得越来越纯净时，交叉熵不纯度降低，并在最佳场景下达到 0。此外，采用互信息概念，我们可以在分割后定义获得的信息增益：
- en: '![](img/bcec7570-adf4-445f-b1db-51ee487b1e0f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bcec7570-adf4-445f-b1db-51ee487b1e0f.png)'
- en: Given a node, we want to create two children to maximize the information gain.
    In other words, by choosing the cross-entropy impurity we implicitly grow the
    tree until the information gain becomes null. Considering again the example of
    a fair die, we need 2.58 bits of information to decide which is the right outcome.
    If, instead, the die is loaded and the probability of an outcome is 1.0, we need
    no information to make a decision. In a decision tree, we'd like to resemble this
    situation, so that, when a new sample has completely traversed the tree, we don't
    need any further information to classify it. If a maximum depth is imposed, the
    final information gain cannot be null. This means that we need to pay an extra
    cost to finalize the classification. This cost is proportional to the residual
    uncertainty and should be minimized to increase the accuracy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个节点，我们希望创建两个子节点以最大化信息增益。换句话说，通过选择交叉熵不纯度，我们隐式地扩展树，直到信息增益变为零。再次考虑公平骰子的例子，我们需要2.58位信息来决定正确的结果。如果骰子是偏的，并且结果的可能性是1.0，那么我们不需要任何信息来做出决定。在决策树中，我们希望模仿这种情况，这样，当一个新的样本完全遍历树时，我们就不需要任何进一步的信息来对其进行分类。如果施加最大深度限制，最终的信息增益不能为零。这意味着我们需要支付额外的成本来完成分类。这个成本与剩余的不确定性成正比，并且应该最小化以提高精度。
- en: Other methods can also be employed (even if Gini and cross-entropy are the most
    common) and I invite the reader to check the references for further information.
    However, at this point, a consideration naturally arises. Decision trees are simple
    models (they are not weak learners!), but the procedure for building them is more
    complex than, for example, training a logistic regression or a SVM. Why are they
    so popular? One reason is already clear—they represent a structural process that
    can be shown using a diagram; however, this is not enough to justify their usage.
    Two important properties allow the employment of decision trees without any data
    preprocessing.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以采用其他方法（尽管基尼系数和交叉熵是最常见的），我邀请读者查阅参考文献以获取更多信息。然而，在这个阶段，一个自然的问题出现了。决策树是简单的模型（它们不是弱学习器！），但构建它们的程序比训练逻辑回归或SVM要复杂得多。为什么它们如此受欢迎？一个明显的原因是——它们代表了一个可以用图表展示的结构化过程；然而，这并不足以证明它们的用途。两个重要的特性使得决策树可以在没有任何数据预处理的情况下使用。
- en: In fact, it's easy to understand that, contrary to other methods, there's no
    need for any scaling or whitening and it's possible to use continuous and categorical
    features at the same time. For example, if in a bidimensional dataset a feature
    has a variance equal to 1 and the other equal to 100, the majority of classifiers
    will achieve a low accuracy; therefore, a preprocessing step becomes necessary.
    In a decision tree, a selection tuple has the same effect also when the ranges
    are very different. It goes without saying that a split can be easily performed
    considering also categorical features and there's no need, for example, to use
    techniques such as one-hot encoding (which is necessary in most cases to avoid
    generalization errors). However, unfortunately, the separation hypersurface obtained
    with a decision tree is normally much more complex than the one obtained using
    other algorithms and this drives to a higher variance with a consequential loss
    of generalization ability.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，很容易理解，与其他方法不同，不需要任何缩放或白化，并且可以同时使用连续和分类特征。例如，在一个二维数据集中，如果一个特征具有方差等于1，而另一个等于100，那么大多数分类器将只能达到低精度；因此，预处理步骤变得必要。在决策树中，选择元组在范围差异很大时也具有相同的效果。不言而喻，在考虑分类特征的情况下，可以轻松地进行分割，例如，不需要使用诸如独热编码（在大多数情况下是必要的，以避免泛化错误）等技术。然而，不幸的是，使用决策树获得的分离超曲面通常比使用其他算法获得的超曲面要复杂得多，这导致方差更高，从而降低了泛化能力。
- en: To understand the reason, it's possible to imagine a very simple bidimensional
    dataset made up of two blobs located in the second and fourth quarters. The first
    set is characterized by *(x < 0, y > 0)*, but the second one by *(x < 0, y < 0)*.
    Let's also suppose that we have a few outliers, but our knowledge about the data
    generating process is not enough to qualify them as noisy samples (the original
    distribution can have tails that are extended over the axes; for example, it may
    be a mixture of two Gaussians). In this scenario, the simplest separation line
    is a diagonal splitting the plane into two subplanes containing regions belonging
    also to the first and third quarters. However, this decision can be made only
    considering both coordinates at the same time. Using a decision tree, we need
    to split initially, for example, using the first feature and again with the second
    one. The result is a piece-wise separation line (for example, splitting the plane
    into the region corresponding to the second quarter and its complement), leading
    to a very high classification variance. Paradoxically, a better solution can be
    obtained with an incomplete tree (limiting the process, for example, to a single
    split) and with the selection of the *y*-axis as the separation line (this is
    why it's important to impose a maximum depth), but the price you pay is an increased
    bias (and a consequently worse accuracy).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解原因，可以想象一个非常简单的二维数据集，由位于第二和第四象限的两个blob组成。第一个集合的特征是*(x < 0, y > 0)*，但第二个集合的特征是*(x
    < 0, y < 0)*。让我们还假设我们有一些异常值，但我们关于数据生成过程的知识不足以将它们视为噪声样本（原始分布可以在轴上延伸尾部；例如，它可能是由两个高斯分布混合而成）。在这种情况下，最简单的分离线是分割平面的对角线，将平面分为包含第一和第三象限区域的两个子平面。然而，这个决策只能同时考虑两个坐标。使用决策树，我们需要最初，例如，使用第一个特征进行分割，然后再使用第二个特征进行分割。结果是分段分离线（例如，将平面分割为对应于第二象限及其补集的区域），导致非常高的分类方差。矛盾的是，可以通过一个不完整的树（例如，将过程限制为单个分割）和选择*y*-轴作为分离线来获得更好的解决方案（这就是为什么强制最大深度很重要的原因），但你付出的代价是增加的偏差（以及随之而来的更差的准确性）。
- en: Another important element to consider when working with decision trees (and
    related models) is the maximum depth. It's possible to grow the tree until the
    all leaves are pure, but sometimes it's preferable to impose a maximum depth (and,
    consequently, a maximum number of terminal nodes). A maximum depth equal to 1
    drives to binary models called **decision stumps**, which don't allow any interaction
    among the features (they can simply be represented as *If... Then* conditions).
    Higher values yield more terminal nodes and allow an increasing interaction among
    features (it's possible to think about a combination of many *If... Then* statements
    together with `AND` logical operators). The right value must be tuned considering
    every single problem and it's important to remember that very deep trees are more
    prone to overfitting than pruned ones.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用决策树（和相关模型）时，另一个需要考虑的重要元素是最大深度。可以生长树直到所有叶子都是纯的，但有时强制最大深度（以及随之而来的最大终端节点数）更可取。最大深度等于1导致称为**决策桩**的二进制模型，它不允许特征之间有任何交互（它们可以简单地表示为*If...
    Then*条件）。更高的值产生更多的终端节点，并允许特征之间有更多的交互（可以想到许多*If... Then*语句与`AND`逻辑运算符的组合）。正确的值必须针对每个问题进行调整，并且重要的是要记住，非常深的树比修剪过的树更容易过拟合。
- en: In some contexts, it's preferable to achieve a slightly worse accuracy with
    a higher generalization ability and, in those case, a maximum depth should be
    imposed. The common tool to determine the best value is always a grid search together
    with a cross-validation technique.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，为了获得更高的泛化能力，宁愿牺牲略微较差的准确性，在这种情况下，应该强制最大深度。确定最佳值的常用工具始终是网格搜索与交叉验证技术相结合。
- en: 'Random forests provide us with a powerful tool to solve the bias-variance trade-off
    problem. They were proposed by L. Breiman (in *Breiman L.*, *Random Forests*,
    *Machine Learning*, *45*, *2001*) and their logic is very simple. As already explained
    in the previous section, the bagging method starts with the choice of the number
    of weak learners, *Nc*. The second step is the generation of Nc datasets (called
    bootstrap samples) *D1, D2, ..., DNc*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林为我们提供了一种强大的工具来解决偏差-方差权衡问题。它们是由L. Breiman提出的（见*Breiman L.*，*随机森林*，*机器学习*，*45*，*2001*），其逻辑非常简单。正如前一部分所解释的，bagging方法首先从选择弱学习者的数量*Nc*开始。第二步是生成*Nc*个数据集（称为bootstrap
    samples）*D1, D2, ..., DNc*：
- en: '![](img/7223f375-7714-439a-99f1-c5927a22dd01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7223f375-7714-439a-99f1-c5927a22dd01.png)'
- en: 'Each decision tree is trained using the corresponding dataset using a common
    impurity criterion; however, in a random forest, in order to reduce the variance,
    the selection splits are not computed considering all the features, but only via
    a random subset containing a quite smaller number of features (common choices
    are the rounded square root, log2 or natural logarithm). This approach indeed
    weakens each learner, as the optimality is partially lost, but allows us to obtain
    a drastic variance reduction by limiting the over-specialization. At the same
    time, a bias reduction and an increased accuracy are a result of the ensemble
    (in particular for a large number of estimators). In fact, as the learners are
    trained with slightly different data distributions, the average of a prediction
    converges to the right value when *Nc → ∞* (in practice, it''s not always necessary
    to employ a very large number of decision trees, however, the correct value must
    be tuned using a grid search with cross-validation). Once all the models, represented
    with a function *di(x)*, have been trained, the final prediction can be obtained
    as an average:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个决策树都使用相应的数据集，通过一个共同的杂质标准进行训练；然而，在随机森林中，为了减少方差，选择分裂的计算不是考虑所有特征，而只通过一个包含相当少特征（常见选择是平方根的四舍五入、log2或自然对数）的随机子集。这种方法确实削弱了每个学习器，因为部分最优性丢失，但通过限制过度专业化，我们可以获得显著减少方差的效果。同时，由于集成（特别是对于大量估计量）的结果，偏差减少和精度提高。实际上，由于学习器使用略微不同的数据分布进行训练，当
    *Nc → ∞* 时，预测的平均值收敛到正确的值（在实践中，并不总是需要使用一个非常大的决策树数量，但是，必须使用带有交叉验证的网格搜索来调整正确的值）。一旦所有模型，用函数
    *di(x)* 表示，都经过训练，最终的预测可以通过平均得到：
- en: '![](img/3a7ab209-96a9-4ae5-bd03-3328a219e30f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3a7ab209-96a9-4ae5-bd03-3328a219e30f.png)'
- en: 'Alternatively, it''s possible to employ a majority vote (but only for classifications):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，也可以采用多数投票法（但仅适用于分类）：
- en: '![](img/1db39b66-3c55-486d-946e-99d8a17d431f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1db39b66-3c55-486d-946e-99d8a17d431f.png)'
- en: These two methods are very similar and, in most cases, they yield the same result.
    However, averaging is more robust and allows an improved flexibility when the
    samples are almost on the boundaries. Moreover, it can be used for both classification
    and regression tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法非常相似，并且在大多数情况下会得到相同的结果。然而，平均法在样本几乎位于边界时更加稳健，并且提供了更高的灵活性。此外，它可用于分类和回归任务。
- en: Random forests limit their randomness by picking the best selection tuple from
    a smaller sample subset. In some cases, for example, when the number of features
    is not very large, this strategy drives to a minimum variance reduction and the
    computational cost is no longer justified by the result. It's possible to achieve
    better performances with a variant called extra-randomized trees (or simply extra-trees).
    The procedure is almost the same; however, in this case, before performing a split,
    *n* random thresholds are computed (for each feature) and the one which leads
    to the least impurity is chosen. This approach further weakens the learners but,
    at the same time, reduces residual variance and prevents overfitting. The dynamic
    is not very different from many techniques such as regularization or dropout (we're
    going to discuss this approach in the next chapter); in fact, the extra-randomness
    reduces the capacity of the model, forcing it to a more linearized solution (which
    is clearly sub-optimal). The price to pay for this limitation is a consequent
    bias worsening, which, however, is compensated by the presence of many different
    learners. Even with random splits, when *Nc* is large enough, the probability
    of a wrong classification (or regression prediction) becomes smaller and smaller
    because both the average and the majority vote tend to compensate the outcome
    of trees whose structure is strongly sub-optimal in particular regions. This result
    is easier to obtain, in particular, when the number of training samples is large.
    In this case, in fact, sampling with replacement leads to slightly different distributions
    that could be considered (even if this is not formally correct) as partially and
    randomly boosted. Therefore, every weak learner will implicitly focus on the whole
    dataset with extra-attention to a smaller subset that, however, is randomly selected
    (differently from actual boosting).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过从较小的样本子集中选择最佳选择元组来限制其随机性。在某些情况下，例如，当特征数量不是非常大时，这种策略会导致方差减少最小化，而计算成本不再由结果证明是合理的。通过一个称为额外随机树（或简称额外树）的变体，可以实现更好的性能。程序几乎相同；然而，在这种情况下，在执行分割之前，会计算
    *n* 个随机阈值（对于每个特征），并选择导致最少不纯度的那个。这种方法进一步削弱了学习器，但同时减少了剩余方差并防止过拟合。这种动态与许多技术（如正则化或dropout）非常相似；事实上，额外的随机性降低了模型的容量，迫使它达到更线性的解决方案（这显然是不理想的）。为此限制所付出的代价是随之而来的偏差恶化，然而，这种恶化却因许多不同学习者的存在而得到补偿。即使使用随机分割，当
    *Nc* 足够大时，错误分类（或回归预测）的概率也会越来越小，因为平均投票和多数投票往往会补偿在特定区域结构上特别次优的树的输出。当训练样本数量很大时，这个结果更容易获得。实际上，在这种情况下，有放回的采样会导致略微不同的分布，即使这不是正式正确的，也可以被认为是部分和随机增强的。因此，每个弱学习器都会隐式地关注整个数据集，并额外关注一个较小的子集，尽管这个子集是随机选择的（与实际的增强不同）。
- en: 'The complete random forest algorithm is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 完全随机森林算法如下：
- en: Set the number of decision trees *Nc*
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置决策树的数量 *Nc*
- en: 'For *i=1* to *Nc*:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *Nc*：
- en: Create a dataset *Di* sampling with replacements from the original dataset *X*
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据集 *X* 中有放回地采样创建数据集 *Di*
- en: Set the number of features to consider during each split *Nf* (for example,
    *sqrt(n)*)
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次分割时设置要考虑的特征数量 *Nf*（例如，*sqrt(n)*）
- en: Set an impurity measure (for example, Gini impurity)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个不纯度度量（例如，基尼不纯度）
- en: Define an optional maximum depth for each tree
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每棵树定义一个可选的最大深度
- en: 'For *i=1* to *Nc*:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *Nc*：
- en: 'Random forest:'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林：
- en: Train the decision tree *di(x)* using the dataset *Di* and selecting the best
    split among *Nf* features randomly sampled
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据集 *Di* 训练决策树 *di(x)*，并从随机采样的 *Nf* 个特征中选择最佳分割
- en: 'Extra-trees:'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 额外树：
- en: Train the decision tree *di(x)* using the dataset *Di*, computing before each
    split *n* random thresholds and selecting the one that yields the least impurity
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据集 *Di* 训练决策树 *di(x)*，在每次分割前计算 *n* 个随机阈值，并选择产生最少不纯度的那个
- en: Define an output function averaging the single outputs or employing a majority
    vote
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个输出函数，它平均单个输出或采用多数投票
- en: Example of random forest with Scikit-Learn
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中的随机森林示例
- en: 'In this example, we are going to use the famous Wine dataset (178 13-dimensional
    samples split into three classes) that is directly available in Scikit-Learn.
    Unfortunately, it''s not so easy to find good and simple datasets for ensemble
    learning algorithms, as they are normally employed with large and complex sets
    that require too long a computational time. As the Wine dataset is not particularly
    complex, the first step is to assess the performances of different classifiers
    (logistic regression, decision tree, and polynomial SVM) using a k-fold cross-validation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用Scikit-Learn中直接可用的著名葡萄酒数据集（178个13维样本分为三个类别）。不幸的是，找到适合集成学习算法的好的简单数据集并不容易，因为它们通常与大型和复杂的数据集一起使用，这需要太长的计算时间。由于葡萄酒数据集并不特别复杂，第一步是使用k折交叉验证来评估不同分类器（逻辑回归、决策树和多项式SVM）的性能：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As expected, the performances are quite good, with a top value of average cross-validation
    accuracy equal to about 96% achieved by the polynomial (the default degree is
    3) SVM. A very interesting element is the performance of the decision tree, the
    worst of the set (with Gini impurity it''s lower). Even if it''s not correct,
    we can define this model as the weakest of the group and it''s a perfect candidate
    for our bagging test. We can now fit a Random Forest by instantiating the class
    `RandomForestClassifier` and selecting `n_estimators=50` (I invite the reader
    to try different values):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，性能相当好，平均交叉验证准确率最高值约为96%，由多项式SVM（默认度数为3）实现。一个非常有趣的因素是决策树的表现，它是这一组中最差的（其Gini不纯度较低）。即使它不正确，我们也可以将这个模型定义为这一组中最弱的，它是我们袋装测试的完美候选人。现在我们可以通过实例化`RandomForestClassifier`类并选择`n_estimators=50`（我邀请读者尝试不同的值）来拟合一个随机森林：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As expected, the average cross-validation accuracy is the highest, about 98.3%.
    Therefore, the random forest has successfully found a global configuration of
    decision trees, so as to specialize them in almost any region of the sample space.
    The parameter `n_jobs=cpu_count()` tells Scikit-Learn to parallelize the training
    process using all of the CPU cores available in the machine.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，平均交叉验证准确率最高，约为98.3%。因此，随机森林已经成功找到了决策树的全球配置，以便使它们几乎在任何样本空间的区域中专业化。参数`n_jobs=cpu_count()`告诉Scikit-Learn使用机器上所有可用的CPU核心并行化训练过程。
- en: 'To better understand the dynamics of this model, it''s useful to plot the cross-validation
    accuracy as a function of the number of trees:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个模型的动态，将交叉验证准确率作为树的数量函数绘制是有用的：
- en: '![](img/458c8030-ceab-46d5-89e5-8183299e051d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/458c8030-ceab-46d5-89e5-8183299e051d.png)'
- en: Cross-validation accuracy of a random forest as a function of the number of
    trees
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的交叉验证准确率作为树数量的函数
- en: It's not surprising to observe some oscillations and a plateau when the number
    of trees becomes greater at about 320\. The effect of the randomness can cause
    a performance loss, even increasing the number of learners. In fact, even if the
    training accuracy grows, the validation accuracy on different folds can be affected
    by an over-specialization. Moreover, in this case, it's very interesting to notice
    that the top accuracy is achievable with 50 trees instead of 400 or more. For
    this reason, I always suggest performing at least a grid search, in order not
    only to achieve the best accuracy but also to minimize the complexity of the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当树的数量增加到大约320时，观察到一些波动和平台期并不令人惊讶。随机性的影响可能导致性能损失，甚至增加学习者的数量。事实上，即使训练准确率增加，不同折上的验证准确率也可能受到过度专业化的影响。此外，在这个案例中，非常有趣的是，最高准确率可以通过50棵树而不是400棵或更多来实现。因此，我总是建议至少进行网格搜索，这不仅是为了达到最佳准确率，也是为了最小化模型的复杂性。
- en: 'Another important element to consider when working with decision trees and
    random forests is feature importance (also called Gini importance when this criterion
    is chosen), which is a measure proportional to the impurity reduction that a particular
    feature allows us achieve. For a decision tree, it is defined as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当与决策树和随机森林一起工作时，另一个需要考虑的重要因素是特征重要性（当选择此标准时也称为Gini重要性），这是一个与特定特征允许我们实现的杂质减少成比例的度量。对于决策树，它定义如下：
- en: '![](img/a8f75f6a-5542-415c-91c0-43f2cd907dd6.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a8f75f6a-5542-415c-91c0-43f2cd907dd6.png)'
- en: 'In the previous formula, *n(j)* denotes the number of samples reaching the
    node *j* (the sum must be extended to all nodes where the feature is chosen) and
    *ΔIi* is the impurity reduction achieved at node *j* after splitting using the
    feature *i*. In a random forest, the importance must be computed by averaging
    over all trees:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*n(j)*表示达到节点*j*（求和必须扩展到所有选择该特征的所有节点）的样本数量，而*ΔIi*是在节点*j*处使用特征*i*分割后实现的纯度降低。在随机森林中，必须通过对所有树进行平均来计算重要性：
- en: '![](img/fc2e83bd-307a-45cc-a990-135ad97e7518.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc2e83bd-307a-45cc-a990-135ad97e7518.png)'
- en: 'After fitting a model (decision tree or random forest), Scikit-Learn outputs
    the feature importance vector in the `feature_importances_ instance` variable.
    In the following graph, there''s a plot showing the importance of each feature
    (the labels can be obtained with the command `load_wine()[''feature_names''])`
    in descending order:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型（决策树或随机森林）后，Scikit-Learn在`feature_importances_`实例变量中输出特征重要性向量。在下面的图表中，有一个按降序排列的每个特征重要性的图（标签可以通过命令`load_wine()['feature_names'])`获得）：
- en: '![](img/22a4f588-560e-44c4-af5e-1dcdee9ed168.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22a4f588-560e-44c4-af5e-1dcdee9ed168.png)'
- en: Feature importances for Wine dataset
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 葡萄酒数据集的特征重要性
- en: 'We don''t want to analyze the chemical meaning of each element, but it''s clear
    that, for example, the presence of proline and the color intensity are much more
    important than the presence of non-flavonoid phenols. As the model is working
    with features that are semantically independent (it''s not the same for the pixels
    of an image), it''s possible to reduce the dimensionality of a dataset by removing
    all those features whose importance doesn''t have a high impact on the final accuracy.
    This process, called **feature selection**, should be performed using more complex
    statistical techniques, such as Chi-squared, but when a classifier is able to
    produce an importance index, it''s also possible to use a Scikit-Learn class called
    `SelectFromModel`. Passing an estimator (that can be fitted or not) and a threshold,
    it''s possible to transform the dataset by filtering out all the features whose
    value is below the threshold. Applying it to our model and setting a minimum importance
    equal to `0.02`, we get the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想分析每个元素化学意义，但很明显，例如，脯氨酸的存在和颜色强度比非黄酮酚的存在要重要得多。由于模型处理的是语义上独立的特征（图像的像素并不相同），因此可以通过移除所有对最终准确率影响不大的特征来降低数据集的维度。这个过程称为**特征选择**，应该使用更复杂的统计技术来执行，例如卡方检验，但当分类器能够产生重要性指数时，也可以使用Scikit-Learn中的一个名为`SelectFromModel`的类。通过传递一个估计器（可以是拟合的或不拟合的）和一个阈值，可以过滤掉所有低于阈值的特征值，从而转换数据集。将此应用于我们的模型并设置最小重要性为`0.02`，我们得到以下结果：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The new dataset now contains 10 features instead of the 13 of the original Wine
    dataset (for example., it's easy to verify that ash and non-flavonoid phenols
    have been removed). Of course, as for any other dimensionality reduction method,
    it's always suggested you verify the final accuracy with a cross-validation and
    make decisions only if the trade-off between loss of accuracy and complexity reduction
    is reasonable.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 新数据集现在包含10个特征，而不是原始葡萄酒数据集的13个特征（例如，很容易验证灰分和非黄酮酚已被移除）。当然，对于任何其他降维方法，总是建议您通过交叉验证来验证最终准确率，并且只有在损失准确率与降低复杂度之间的权衡是合理的情况下才做出决定。
- en: AdaBoost
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost
- en: 'In the previous section, we have seen that sampling with a replacement leads
    to datasets where the samples are randomly reweighted. However, if *M* is very
    large, most of the samples will appear only once and, moreover, all the choices
    are totally random. AdaBoost is an algorithm proposed by Schapire and Freund that
    tries to maximize the efficiency of each weak learner by employing adaptive boosting
    (the name derives from this). In particular, the ensemble is grown sequentially
    and the data distribution is recomputed at each step so as to increase the weight
    of those samples that were misclassified and reduce the weight of the ones that
    were correctly classified. In this way, every new learner is forced to focus on
    those regions that were more problematic for the previous estimators. The reader
    can immediately understand that, contrary to random forests and other bagging
    methods, boosting doesn''t rely on randomness to reduce the variance and improve
    the accuracy. Rather, it works in a deterministic way and each new data distribution
    is chosen with a precise goal. In this paragraph, we are going to consider a variant
    called **Discrete AdaBoost** (formally *AdaBoost.M1*), which needs a classifier
    whose output is thresholded (for example, *-1* and *1*). However, real-valued
    versions (whose output behaves like a probability) have been developed (a classical
    example is shown in *Additive Logistic Regression: a Statistical View of Boosting*,* Friedman
    J.*, *Hastie T.*, *Tibshirani R.*,* Annals of Statistics*, 28/1998). As the main
    concepts are always the same, the reader interested in the theoretical details
    of other variants can immediately find them in the referenced papers.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在上一节中，我们了解到使用放回抽样会导致数据集中样本被随机重新加权。然而，如果 *M* 非常大，大多数样本只会出现一次，而且所有选择都是完全随机的。AdaBoost
    是 Schapire 和 Freund 提出的一种算法，它试图通过采用自适应提升（这个名字来源于此）来最大化每个弱学习器的效率。特别是，集成是通过顺序增长，并在每一步重新计算数据分布，以便增加被错误分类的样本的权重，减少被正确分类的样本的权重。这样，每个新的学习器被迫关注那些对先前估计器来说更成问题的区域。读者可以立即理解，与随机森林和其他袋装方法不同，提升不依赖于随机性来减少方差和提高精度。相反，它以确定性的方式工作，并且每个新的数据分布都是根据精确的目标选择的。在本段中，我们将考虑一个称为
    **离散 AdaBoost**（正式名称为 *AdaBoost.M1*）的变体，它需要一个输出被阈值化的分类器（例如，*-1* 和 *1*）。然而，已经开发了实值版本（其输出行为类似于概率），例如在
    *Additive Logistic Regression: a Statistical View of Boosting* 中所示，作者为 *Friedman
    J.*，*Hastie T.*，*Tibshirani R.*，发表于 *Annals of Statistics*，1998年第28卷）。由于主要概念始终相同，对其他变体的理论细节感兴趣的读者可以立即在参考文献中找到它们。'
- en: 'For simplicity, the training dataset of **AdaBoost.M1** is defined as follow:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，**AdaBoost.M1** 的训练数据集定义为如下：
- en: '![](img/9cca3c68-157b-4ca9-ab53-baeee3d284f2.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9cca3c68-157b-4ca9-ab53-baeee3d284f2.png)'
- en: 'This choice is not a limitation because, in multi-class problems, a one-versus-the-rest
    strategy can be easily employed, even if algorithms like **AdaBoost.SAMME** guarantee
    a much better performance. In order to manipulate the data distribution, we need
    to define a weight set:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择并不是一个限制，因为在多类问题中，可以很容易地采用一对一策略，即使像 **AdaBoost.SAMME** 这样的算法保证了更好的性能。为了操作数据分布，我们需要定义一个权重集：
- en: '![](img/532490e8-b784-4762-ac4c-74c24c91f4db.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/532490e8-b784-4762-ac4c-74c24c91f4db.png)'
- en: The weight set allows defining an implicit data distribution *D(t)(x)*, which
    initially is equivalent to the original one but that can be easily reshaped by
    changing the values *wi*. Once the family and the number of estimators, *Nc*,
    have been chosen, it's possible to start the global training process. The algorithm
    can be applied to any kind of learner that is able to produce thresholded estimations
    (while the real-valued variants can work with probabilities, for example, obtained
    through the Platt scaling method).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 权重集允许定义一个隐式数据分布 *D(t)(x)*，最初它与原始数据分布等效，但可以通过改变 *wi* 的值轻松重塑。一旦选择了家族和估计器的数量 *Nc*，就可以开始全局训练过程。该算法可以应用于任何能够产生阈值估计的学习器（而实值变体可以使用概率工作，例如通过
    Platt 缩放方法获得）。
- en: 'The first instance *d1(x)* is trained with the original dataset, which means
    with the data distribution *D(1)(x)*. The next instances, instead, are trained
    with the reweighted distributions *D(2)(x), D(3)(x), ..., D(Nc)(x)*. In order
    to compute them, after each training process, the normalized weighted error sum
    is computed:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实例*d1(x)*使用原始数据集进行训练，这意味着使用数据分布*D(1)(x)*。相反，下一个实例使用重新加权的分布*D(2)(x)，D(3)(x)，...，D(Nc)(x)*进行训练。为了计算它们，在每个训练过程之后，计算归一化加权误差总和：
- en: '![](img/95ea5305-93ea-4421-a85a-25ad479a6bc2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/95ea5305-93ea-4421-a85a-25ad479a6bc2.png)'
- en: 'This value is bounded between *0* (no misclassifications) and *1* (all samples
    have been misclassified) and it''s employed to compute the estimator weight *α(t)*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此值介于*0*（没有误分类）和*1*（所有样本都被误分类）之间，并用于计算估计器权重*α(t)*：
- en: '![](img/15597875-3154-412d-a332-0d145c51c7ce.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/15597875-3154-412d-a332-0d145c51c7ce.png)'
- en: 'To understand how this function works, it''s useful to consider its plot (shown
    in the following diagram):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个函数是如何工作的，考虑其图像（如下所示）是有用的：
- en: '![](img/3267ea61-5b68-4668-b290-b59ca43cd89d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/3267ea61-5b68-4668-b290-b59ca43cd89d.png)'
- en: Estimator weight plot as a function of the normalized weighted error sum
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器权重图作为归一化加权误差总和的函数
- en: 'This diagram unveils an implicit assumption: the worst classifier is not the
    one that misclassifies all samples (*ε(t) = 1*), but a totally random binary guess
    (corresponding to *ε(t) = 0.5*). In this case, *α(t)* is null and, therefore,
    the outcome if the estimator is completely discarded. When *ε(t) < 0.5*, a boosting
    is applied (between about 0.05 and 0.5, the trend is almost linear), but it becomes
    greater than 1 only when *ε(t)* < about 0.25 (larger values drive to a penalty
    because the weight is smaller than 1). This value is a threshold to qualify an
    estimator as trusted or very strong and *α(t) → +∞* in the particular case of
    a perfect estimator (no errors).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此图揭示了一个隐含的假设：最差的分类器不是将所有样本都分类错误的那一个（*ε(t) = 1*），而是一个完全随机的二进制猜测（对应于*ε(t) = 0.5*）。在这种情况下，*α(t)*为零，因此，如果完全丢弃估计器，结果就是零。当*ε(t)
    < 0.5*时，应用提升（在约0.05和0.5之间，趋势几乎是线性的），但只有当*ε(t)* < 约0.25时，它才大于1（较大的值会导致惩罚，因为权重小于1）。这个值是一个阈值，用于判断估计器是否可信或非常强大，在完美估计器（没有错误）的特定情况下，*α(t)*
    → +∞。
- en: In practice, an upper bound should be imposed in order to avoid overflows or
    divisions by zero. Instead, when *ε(t) > 0.5*, the estimator is unacceptably weak,
    because it's worse than a random guess and the resulting boosting would be negative.
    To avoid this problem, real implementations must invert the output of such estimators,
    transforming them de facto into learners with *ε(t) < 0.5* (this is not an issue,
    as the transformation is applied to all output values in the same way). It's important
    to consider that this algorithm shouldn't be directly applied to multi-class scenarios
    because, as pointed out in *Multi-class AdaBoost*,* Zhu J.*, *Rosset S.*, *Zou
    H.*, *Hastie T.*,* 01/2006*, the threshold 0.5 corresponds to a random guess accuracy
    only for binary choices. When the number of classes is larger than two, a random
    estimator outputs a class with a probability *1/Ny* (where *Ny* is the number
    of classes) and, therefore, AdaBoost.M1 will boost the classifiers in a wrong
    way, yielding poor final accuracies (the real threshold should be *1 - 1/Ny*,
    which is larger than 0.5 when *Ny > 2*). The AdaBoost.SAMME algorithm (implemented
    by Scikit-Learn) has been proposed to solve this problem and exploit the power
    of boosting also in multi-class scenarios.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，应该施加一个上限，以避免溢出或除以零。相反，当*ε(t) > 0.5*时，估计器是不可接受的弱，因为它比随机猜测还要差，并且产生的提升将是负的。为了避免这个问题，实际实现必须反转此类估计器的输出，实际上将它们转变为*ε(t)
    < 0.5*的学习者（这不是问题，因为转换应用于所有输出值的方式相同）。重要的是要考虑，这个算法不应该直接应用于多类场景，因为，正如*多类AdaBoost*中指出的，*Zhu
    J.*，*Rosset S.*，*Zou H.*，*Hastie T.*，*01/2006*，阈值0.5仅对应于二元选择的随机猜测精度。当类的数量大于两个时，随机估计器输出一个类的概率为*1/Ny*（其中*Ny*是类的数量），因此，AdaBoost.M1将以错误的方式提升分类器，导致最终精度较差（实际的阈值应该是*1
    - 1/Ny*，当*Ny > 2*时，它大于0.5）。AdaBoost.SAMME算法（由Scikit-Learn实现）已被提出以解决这个问题，并利用提升在多类场景中的力量。
- en: 'The global decision function is defined as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 全局决策函数定义如下：
- en: '![](img/621e3cfe-d929-4bf7-93c0-31b9cb09cf09.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图像](img/621e3cfe-d929-4bf7-93c0-31b9cb09cf09.png)'
- en: In this way, as the estimators are added sequentially, the importance of each
    of them will decrease while the accuracy of *di(x)* increases. However, it's also
    possible to observe a plateau if the complexity of *X* is very high. In this case,
    many learners will have a high weight, because the final prediction must take
    into account a sub-combination of learners in order to achieve an acceptable accuracy.
    As this algorithm specializes the learners at each step, a good practice is to
    start with a small number of estimators (for example, 10 or 20) and increase the
    number until no improvement is achieved. Sometimes, a minimum number of good learners
    (like SVM or decision trees) is sufficient to reach the highest possible accuracy
    (limited to this kind of algorithm), but in some other cases, the number of estimators
    can be some thousands. Grid search and cross-validation are again the only good
    strategies to make the right choice.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，随着估计器的逐个添加，每个估计器的重要性将逐渐降低，而 *di(x)* 的准确性将提高。然而，如果 *X* 的复杂性非常高，也可能观察到平台期。在这种情况下，许多学习者的权重会很高，因为最终的预测必须考虑学习者的子组合以达到可接受的准确性。由于此算法在每一步都专门化学习者，因此一个好的做法是从少量估计器（例如，10或20）开始，直到不再获得改进为止。有时，只需要少量优秀的学习者（如SVM或决策树）就能达到最高的可能准确性（限于此类算法），但在某些其他情况下，估计器的数量可以达到数千。网格搜索和交叉验证仍然是做出正确选择的唯一好策略。
- en: 'After each training step it is necessary to update the weights in order to
    produce a boosted distribution. This is achieved using an exponential function
    (based on bipolar outputs *{-1, 1}*):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步训练之后，必须更新权重以产生增强分布。这是通过使用指数函数（基于双极输出 *{-1, 1}*）实现的：
- en: '![](img/affbe3ea-ac1a-433d-8490-f2260c52d34b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/affbe3ea-ac1a-433d-8490-f2260c52d34b.png)'
- en: 'Given a sample *x[i]*, if it has been misclassified, its weight will be increased
    considering the overall estimator weight. This approach allows a further adaptive
    behavior because a classifier with a high *α(t)* is already very accurate and
    it''s necessary a higher attention level to focus only on the (few) misclassified
    samples. On the contrary, if *α(t)* is small, the estimator must improve its overall
    performance and the over-weighting process must be applied to a large subset (therefore,
    the distribution won''t peak around a few samples, but will penalize only the
    small subset that has been correctly classified, leaving the estimator free to
    explore the remaining space with the same probability). Even if not present in
    the original proposal, it''s also possible to include a learning rate *η* that
    multiplies the exponent:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个样本 *x[i]*，如果它被错误分类，其权重将根据整体估计器权重增加。这种方法允许进一步的自适应行为，因为具有高 *α(t)* 的分类器已经非常准确，并且需要更高的关注水平来仅关注（少数）错误分类的样本。相反，如果
    *α(t)* 较小，估计器必须提高其整体性能，并且必须将过重加权过程应用于大子集（因此，分布不会围绕少数样本峰值，而只会惩罚那些被正确分类的小子集，让估计器能够以相同的概率探索剩余空间）。即使原始提案中没有提到，也可以包括一个乘以指数的学习率
    *η*：
- en: '![](img/065c5ca6-a718-4d26-94df-93c1bc6ba57d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/065c5ca6-a718-4d26-94df-93c1bc6ba57d.png)'
- en: A value *η = 1* has no effect, while smaller values have been proven to increase
    the accuracy by avoiding a premature specialization. Of course, when *η << 1*,
    the number of estimators must be increased in order to compensate the minor reweighting
    and this can drive to a training performance loss. As for the other hyperparameters,
    the right value for *η* must be discovered using a cross-validation technique
    (alternatively, if it's the only value that must be fine-tuned, it's possible
    to start with one and proceed by decreasing its value until the maximum accuracy
    has been reached).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 值 *η = 1* 没有影响，而较小的值已被证明可以通过避免过早专业化来提高准确性。当然，当 *η << 1* 时，必须增加估计器的数量以补偿轻微的重加权，这可能导致训练性能损失。至于其他超参数，必须使用交叉验证技术来发现
    *η* 的正确值（或者，如果它必须是唯一需要微调的值，可以从一个值开始，通过减少其值直到达到最大准确性）。
- en: 'The complete AdaBoost.M1 algorithm is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 AdaBoost.M1 算法如下：
- en: Set the family and the number of estimators *Nc*
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置家族和估计器的数量 *Nc*
- en: Set the initial weights *W(1)* equal to *1/M*
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始权重 *W(1)* 设置为 *1/M*
- en: Set the learning rate *η* (for example, *η = 1*)
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习率 *η*（例如，*η = 1*）
- en: Set the initial distribution *D(1)* equal to the dataset *X*
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始分布 *D(1)* 设置为数据集 *X*
- en: 'For *i=1* to *Nc*:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *Nc*：
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *D(i)*
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据分布 *D(i)* 训练第 *i* 个估计器 *di(x)*
- en: 'Compute the normalized weighted error sum *ε(i)*:'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算归一化加权误差和 *ε(i)*:'
- en: If *ε(i) > 0.5*, invert all estimator outputs
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *ε(i) > 0.5*，则反转所有估计器输出
- en: Compute the estimator weight *α(i)*
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算估计权重 *α(i)*
- en: Update the weights using the exponential formula (with or without the learning
    rate)
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指数公式（带或不带学习率）更新权重
- en: Normalize the weights
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化权重
- en: Create the global estimator applying the sign(•) function to the weighted sum *α(i)di(x)*
    (for *i=1* to *Nc*)
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建全局估计器，将 sign(•) 函数应用于加权求和 *α(i)di(x)*（对于 *i=1* 到 *Nc*）
- en: AdaBoost.SAMME
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost.SAMME
- en: 'This variant, called **Stagewise Additive Modeling using a Multi-class Exponential
    loss** (**SAMME**), was proposed by Zhu, Rosset, Zou, and Hastie in *Multi-class
    AdaBoost*,* Zhu J.*, *Rosset S.*, *Zou H.*, *Hastie T.*,* 01/2006*. The goal is
    to adapt AdaBoost.M1 in order to work properly in multi-class scenarios. As this
    is a discrete version, its structure is almost the same, with a difference in
    the estimator weight computation. Let''s consider a label dataset, *Y*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变体被称为**多类指数损失逐步加性建模**（**SAMME**），由 Zhu、Rosset、Zou 和 Hastie 在 *Multi-class
    AdaBoost* 中提出，*Zhu J.*，*Rosset S.*，*Zou H.*，*Hastie T.*，*01/2006*。目标是使 AdaBoost.M1
    能够在多类场景中正常工作。由于这是一个离散版本，其结构几乎相同，只是在估计权重计算上有所不同。让我们考虑一个标签数据集，*Y*：
- en: '![](img/93ea80f7-59b0-413b-9c59-1b65285610d9.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93ea80f7-59b0-413b-9c59-1b65285610d9.png)'
- en: 'Now, there are *p* different classes and it''s necessary to consider that a
    random guess estimator cannot reach an accuracy equal to 0.5; therefore, the new
    estimator weights are computed as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有 *p* 个不同的类别，需要考虑随机猜测估计器无法达到 0.5 的准确度；因此，新的估计器权重计算如下：
- en: '![](img/b32c95f1-abcb-4c3d-b36c-cfba45033445.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b32c95f1-abcb-4c3d-b36c-cfba45033445.png)'
- en: 'In this way, the threshold is pushed forward and *α(t)* will be zero when the
    following is true:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，阈值就会向前推进，当以下条件成立时，*α(t)* 将为零：
- en: '![](img/c30ed323-a649-4852-b175-540e93085a9b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c30ed323-a649-4852-b175-540e93085a9b.png)'
- en: 'The following graph shows the plot of *α(t)* with *p = 10*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 *α(t)* 与 *p = 10* 的关系图：
- en: '![](img/7dbe4459-d05f-4e36-a488-70cf4fb0ca60.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dbe4459-d05f-4e36-a488-70cf4fb0ca60.png)'
- en: Estimator weight plot as a function of the normalized weighted error sum when
    p = 10
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当 p = 10 时，估计权重作为归一化加权误差和的函数的图
- en: Employing this correction, the boosting process can successfully cope with multi-class
    problems without the bias normally introduced by AdaBoost.M1 when *p > 2 (α(t) >
    0* when the error is less than an actual random guess, which is a function of
    the number of classes). As the performance of this algorithm is clearly superior,
    the majority of AdaBoost implementations aren't based on the original algorithm
    anymore (as already mentioned, for example, Scikit-Learn implements AdaBoost.SAMME
    and the real-valued version AdaBoost.SAMME.R). Of course, when *p = 2*, AdaBoost.SAMME
    is exactly equivalent to AdaBoost.M1.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种修正，提升过程可以成功处理多类问题，而不会引入 AdaBoost.M1 在 *p > 2*（当误差小于实际随机猜测时，*α(t)* > 0）时通常引入的偏差。由于该算法的性能明显优于其他算法，大多数
    AdaBoost 实现不再基于原始算法（例如，Scikit-Learn 实现了 AdaBoost.SAMME 和实值版本 AdaBoost.SAMME.R）。当然，当
    *p = 2* 时，AdaBoost.SAMME 与 AdaBoost.M1 完全等价。
- en: AdaBoost.SAMME.R
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost.SAMME.R
- en: AdaBoost.SAMME.R is a variant that works with classifiers that can output prediction
    probabilities. This is normally possible employing techniques such as Platt scaling,
    but it's important to check whether a specific classifier implementation is able
    to output the probabilities without any further action. For example, SVM implementations
    provided by Scikit-Learn don't compute the probabilities unless the parameter
    `probability=True` (because they require an extra step that could be useless in
    some cases).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost.SAMME.R 是一个与能够输出预测概率的分类器一起工作的变体。这通常可以通过使用 Platt 缩放等技术来实现，但重要的是要检查特定的分类器实现是否能够在不采取任何进一步行动的情况下输出概率。例如，Scikit-Learn
    提供的 SVM 实现不会计算概率，除非参数 `probability=True`（因为它们需要额外的步骤，在某些情况下可能无益）。
- en: 'In this case, we assume that the output of each classifier is a probability
    vector:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们假设每个分类器的输出是一个概率向量：
- en: '![](img/f489c04a-b017-42e0-9615-0d5b81d71d07.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f489c04a-b017-42e0-9615-0d5b81d71d07.png)'
- en: 'Each component is the conditional probability that the *j^(th)* class is output
    given the input xi. When working with a single estimator, the winning class is
    obtained through the argmax(•) function; however, in this case, we want to re-weight
    each learner so as to obtain a sequentially grown ensemble. The basic idea is
    the same as AdaBoost.M1, but, as now we manage probability vectors, we also need
    an estimator weighting function that depends on the single sample *xi* (this function
    indeed wraps every single estimator that is now expressed as a probability vectorial
    function *pi(t)(y=i|x))*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分量是在给定输入xi的情况下，*j^(th)*类输出的条件概率。当使用单个估计器时，通过argmax(•)函数获得获胜类别；然而，在这种情况下，我们想要重新加权每个学习者，以获得一个顺序增长的集成。基本思想与AdaBoost.M1相同，但现在我们管理概率向量，我们还需要一个依赖于单个样本*xi*的估计器权重函数（这个函数实际上包装了现在表示为概率向量函数*pi(t)(y=i|x)*的每个估计器）：
- en: '![](img/6d766343-dccd-48c3-8891-3188e83156b0.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d766343-dccd-48c3-8891-3188e83156b0.png)'
- en: 'Considering the properties of logarithms, the previous expression is equivalent
    to a discrete *α(t)*; however, in this case, we don''t rely on a weighted error
    sum (the theoretical explanation is rather complex and is beyond the scope of
    this book. The reader can find it in the aforementioned paper, even if the method
    presented in the next chapter discloses a fundamental part of the logic). To better
    understand the behavior of this function, let''s consider a simple scenario with
    *p = 2*. The first case is a sample that the learner isn''t able to classify (*p=(0.5,
    0.5)*):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到对数函数的性质，前面的表达式等价于一个离散的*α(t)*；然而，在这种情况下，我们并不依赖于加权误差总和（理论解释相当复杂，超出了本书的范围。读者可以在上述论文中找到它，即使下一章介绍的方法揭示了逻辑的基本部分）。为了更好地理解这个函数的行为，让我们考虑一个简单的场景，其中*p
    = 2*。第一种情况是学习者无法分类的样本（*p=(0.5, 0.5)*）：
- en: '![](img/c1d2fce3-0e5c-4ac9-9955-d38081974cd9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c1d2fce3-0e5c-4ac9-9955-d38081974cd9.png)'
- en: 'In this case, the uncertainty is maximal and the classifier cannot be trusted
    for this sample, so the weight becomes null for all output probabilities. Now,
    let''s apply the boosting, obtaining the probability vector *p=(0.7, 0.3)*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，不确定性最大，分类器无法信任这个样本，因此所有输出概率的权重变为零。现在，让我们应用提升，得到概率向量*p=(0.7, 0.3)*：
- en: '![](img/3f301973-7a85-426d-8907-c2d27deee853.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3f301973-7a85-426d-8907-c2d27deee853.png)'
- en: 'The first class will become positive and its magnitude will increase when *p →
    1*, while the other one is the opposite value. Therefore, the functions are symmetric
    and allow working with a sum:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当*p → 1*时，第一个类别将变为正值，其幅度将增加，而另一个则是相反的值。因此，函数是对称的，允许使用总和：
- en: '![](img/cc89b32f-6c44-49fc-9ac7-65811bb4cb08.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cc89b32f-6c44-49fc-9ac7-65811bb4cb08.png)'
- en: This approach is very similar to a weighted majority vote because the winning
    class *yi* is computed taking into account not only the number of estimators whose
    output is *yi* but also their relative weight and the negative weight of the remaining
    classifiers. A class can be selected only if the strongest classifiers predicted
    it and the impact of the other learners is not sufficient to overturn this result.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与加权多数投票非常相似，因为获胜的类别*yi*的计算不仅考虑了输出*yi*的估计器的数量，还考虑了它们的相对权重和剩余分类器的负权重。只有当最强的分类器预测了该类别，并且其他学习者的影响不足以推翻这一结果时，才能选择一个类别。
- en: 'In order to update the weights, we need to consider the impact of all probabilities.
    In particular, we want to reduce the uncertainty (which can degenerate to a purely
    random guess) and force a superior attention focused on all those samples that
    have been misclassified. To achieve this goal, we need to define the *yi* and
    *p(t)(xi)* vectors, which contain, respectively, the one-hot encoding of the true
    class (for example, *(0, 0, 1, ..., 0)*) and the output probabilities yielded
    by the estimator (as a column vector). Hence, the update rule becomes as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新权重，我们需要考虑所有概率的影响。特别是，我们希望减少不确定性（这可能会退化成纯粹的随机猜测）并迫使注意力集中在所有那些被错误分类的样本上。为了实现这一目标，我们需要定义*yi*和*p(t)(xi)*向量，它们分别包含真实类别的one-hot编码（例如，*(0,
    0, 1, ..., 0)*）和估计器输出的概率（作为一个列向量）。因此，更新规则如下：
- en: '![](img/bf93290c-bb10-4179-9b2c-268f90f047dc.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bf93290c-bb10-4179-9b2c-268f90f047dc.png)'
- en: If, for example, the true vector is (1, 0) and the output probabilities are
    (0.1, 0.9), with η=1, the weight of the sample will be multiplied by about 3.16\.
    If instead, the output probabilities are (0.9, 0.1), meaning the sample has been
    successfully classified, the multiplication factor will become closer to 1\. In
    this way, the new data distribution *D(t+1)*, analogously to AdaBoost.M1, will
    be more peaked on the samples that need more attention. All implementations include
    the learning rate as a hyperparameter because, as already explained, the default
    value equal to 1.0 cannot be the best choice for specific problems. In general,
    a lower learning rate allows reducing the instability when there are many outliers
    and improves the generalization ability thanks to a slower convergence towards
    the optimum. When *η < 1*, every new distribution is slightly more focused on
    the misclassified samples, allowing the estimators to search for a better parameter
    set without big jumps (that can lead the estimator to skip an optimal point).
    However, contrary to Neural Networks that normally work with small batches, AdaBoost
    can often perform quite well also with *η=1* because the correction is applied
    only after a full training step. As usual, I recommend performing a grid search
    to select the right values for each specific problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果真实向量是(1, 0)且输出概率是(0.1, 0.9)，η=1时，样本的权重将乘以大约3.16。如果输出概率是(0.9, 0.1)，意味着样本已被成功分类，乘数将更接近1。这样，新的数据分布D(t+1)，类似于AdaBoost.M1，将在需要更多关注的样本上更加尖锐。所有实现都将学习率作为超参数，因为，如前所述，默认值等于1.0可能不是特定问题的最佳选择。一般来说，较低的学习率可以在存在许多异常值时减少不稳定性，并通过较慢的收敛速度提高泛化能力。当η
    < 1时，每个新的分布都会稍微更多地关注被错误分类的样本，允许估计器在不进行大跳跃的情况下（可能导致估计器跳过最佳点）寻找更好的参数集。然而，与通常使用小批次的神经网络不同，AdaBoost也可以在η=1时表现相当好，因为校正仅在完整训练步骤之后应用。像往常一样，我建议进行网格搜索以选择每个特定问题的正确值。
- en: 'The complete AdaBoost.SAMME.R algorithm is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的AdaBoost.SAMME.R算法如下：
- en: Set the family and the number of estimators *Nc*
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置家族和估计器数量Nc
- en: Set the initial weights *W(1)* equal to *1/M*
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始权重W(1)设置为1/M
- en: Set the learning rate *η* (for example, *η = 1*)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习率η（例如，η = 1）
- en: Set the initial distribution *D(1)* equal to the dataset *X*
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始分布D(1)设置为数据集X
- en: 'For *i=1* to *Nc*:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于i=1到Nc：
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *D(i)*
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据分布D(i)训练第i个估计器di(x)
- en: Compute the output probabilities for each class and each training sample
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个类别和每个训练样本的输出概率
- en: Compute the estimator weights *αj(i)*
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算估计器权重αj(i)
- en: Update the weights using the exponential formula (with or without the learning
    rate)
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指数公式（带或不带学习率）更新权重
- en: Normalize the weights
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化权重
- en: Create the global estimator applying the argmax(•) function to the sum *αj(i)* (for
    *i=1* to *Nc*)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对αj(i)的和（对于i=1到Nc）应用argmax(•)函数来创建全局估计器
- en: AdaBoost.R2
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost.R2
- en: 'A slightly more complex variant has been proposed by Drucker (in *Improving
    Regressors using Boosting Techniques*,* Drucker H.*,* ICML 1997*) to manage regression
    problems. The weak learners are commonly decision trees and the main concepts
    are very similar to the other variants (in particular, the re-weighting process
    applied to the training dataset). The real difference is the strategy adopted
    in order to choose the final prediction *yi* given the input sample *xi*. Assuming
    that there are Nc estimators and each of them is represented as function *dt(x)*,
    we can compute the absolute residual *ri(t)* for every input sample:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Drucker博士（在《使用提升技术改进回归器》*Drucker H.*，*ICML 1997*）提出了一种稍微复杂一些的变体来处理回归问题。弱学习器通常是决策树，其主要概念与其他变体非常相似（特别是，应用于训练数据集的重加权过程）。真正的区别在于选择最终预测yi的策略，给定输入样本xi。假设有Nc个估计器，每个估计器都表示为函数dt(x)，我们可以为每个输入样本计算绝对残差ri(t)：
- en: '![](img/ebdc8cc2-5fe8-4a61-92bb-27b556ce0f13.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ebdc8cc2-5fe8-4a61-92bb-27b556ce0f13.png)'
- en: 'Once the set *Ri* containing all the absolute residuals has been populated,
    we can compute the quantity *Sr = sup Ri* and compute the values of a cost function
    that must be proportional to the error. The common choice that is normally implemented
    (and suggested by the author himself) is a linear loss:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦包含所有绝对残差的集合 *Ri* 已被填充，我们可以计算量 *Sr = sup Ri* 并计算与误差成比例的成本函数值。通常实现的常见选择（并且作者本人也建议）是线性损失：
- en: '![](img/3ef3a9e6-36f9-47f8-b553-0aeb08f9e2fb.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3ef3a9e6-36f9-47f8-b553-0aeb08f9e2fb.png)'
- en: 'This loss is very flat and it''s directly proportional to the error. In most
    cases, it''s a good choice because it avoids premature over-specialization and
    allows the estimators to readapt their structure in a gentler way. The most obvious
    alternative is the square loss, which starts giving more importance to those samples
    whose prediction error is larger. It is defined as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种损失非常平坦，并且直接与误差成正比。在大多数情况下，这是一个好的选择，因为它避免了过早的过度专业化，并允许估计量以更温和的方式调整其结构。最明显的替代方案是平方损失，它开始给予那些预测误差较大的样本更多的重要性。它定义如下：
- en: '![](img/9257d79a-f66f-45c2-a3df-1c03dcb51383.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9257d79a-f66f-45c2-a3df-1c03dcb51383.png)'
- en: 'The last cost function is strictly related to AdaBoost.M1 and it''s exponential:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的成本函数严格相关于 AdaBoost.M1，并且是指数的：
- en: '![](img/e10feb45-b692-4c0d-aa25-ccb2a6ba9ca1.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e10feb45-b692-4c0d-aa25-ccb2a6ba9ca1.png)'
- en: This is normally a less robust choice because, as we are also going to discuss
    in the next section, it penalizes small errors in favor of larger ones. Considering
    that these functions are also employed in the re-weighting process, an exponential
    loss can force the distribution to assign very high probabilities to samples whose
    misclassification error is high, driving the estimators to become over-specialized
    with effect from the first iterations. In many cases (such as in neural networks),
    the loss functions are normally chosen according to their specific properties
    but, above all, to the ease to minimize them. In this particular scenario, loss
    functions are a fundamental part of the boosting process and they must be chosen
    considering the impact on the data distribution. Testing and cross-validation
    provide the best tool to make a reasonable decision.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是一个不太稳健的选择，因为我们还将在下一节讨论，它倾向于惩罚较小的错误，而不是较大的错误。考虑到这些函数也用于重新加权过程，指数损失可以迫使分布给那些误分类错误高的样本分配非常高的概率，从而使估计量在第一次迭代中变得过度专业化。在许多情况下（例如在神经网络中），损失函数通常根据它们的特定属性选择，但更重要的是，根据它们易于最小化的程度。在这种情况下，损失函数是提升过程的基本部分，并且必须考虑对数据分布的影响。测试和交叉验证是做出合理决策的最佳工具。
- en: 'Once the loss function has been evaluated for all training samples, it''s possible
    to build the global cost function as the weighted average of all losses. Contrary
    to many algorithms that simply sum or average the losses, in this case, it''s
    necessary to consider the structure of the distribution. As the boosting process
    reweights the samples, also the corresponding loss values must be filtered to
    avoid a bias. At the iteration *t*, the cost function is computed as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦对所有训练样本评估了损失函数，就可以构建全局成本函数，作为所有损失的加权平均值。与许多简单求和或平均损失的算法不同，在这种情况下，必须考虑分布的结构。由于提升过程重新加权样本，相应的损失值也必须过滤，以避免偏差。在迭代
    *t* 时，成本函数的计算如下：
- en: '![](img/b7adb16a-f916-4e5e-916d-bb2b39af3039.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7adb16a-f916-4e5e-916d-bb2b39af3039.png)'
- en: 'This function is proportional to the weighted errors, which can be either linearly
    filtered or emphasized using a quadratic or exponential function. However, in
    all cases, a sample whose weight is lower will yield a smaller contribution, letting
    the algorithm focus on the samples more difficult to be predicted. Consider that,
    in this case, we are working with classifications; therefore, the only measure
    we can exploit is the loss. Good samples yield low losses, hard samples yield
    proportionally higher losses. Even if it''s possible to use *C(t)* directly, it''s
    preferable to define a confidence measure:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数与加权误差成正比，这些误差可以通过线性过滤或使用二次或指数函数强调。然而，在所有情况下，权重较低的样本将产生较小的贡献，使算法能够专注于更难预测的样本。请注意，在这种情况下，我们正在处理分类；因此，我们唯一可以使用的度量是损失。好的样本产生较低的损失，困难的样本产生成比例较高的损失。即使可以直接使用
    *C(t)*，也最好定义一个置信度度量：
- en: '![](img/4f8193e2-f1a9-48c6-9d13-bb99d2ad7077.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f8193e2-f1a9-48c6-9d13-bb99d2ad7077.png)'
- en: 'This index is inversely proportional to the average confidence at the iteration
    *t*. In fact, when *C(t) → 0*, *γ(t) → 0* and when *C(t) → ∞*, *γ(t) → 1*. The
    weight update is performed considering the overall confidence and the specific
    loss value:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此指数与迭代t的平均置信度成反比。事实上，当C(t) → 0时，γ(t) → 0，当C(t) → ∞时，γ(t) → 1。权重更新是在考虑整体置信度和特定损失值的情况下进行的：
- en: '![](img/3958f502-bacd-4402-a368-9a88e4802cab.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3958f502-bacd-4402-a368-9a88e4802cab.png)'
- en: A weight will be decreased proportionally to the loss associated with the corresponding
    absolute residual. However, instead of using a fixed base, the global confidence
    index is chosen. This strategy allows a further degree of adaptability, because
    an estimator with a low confidence doesn't need to focus only on a small subset
    and, considering that *γ(t)* is bounded between 0 and 1 (worst condition), the
    exponential becomes ineffective when the cost function is very high (1x = 1),
    so that the weights remain unchanged. This procedure is not very dissimilar to
    the one adopted in other variants, but it tries to find a trade-off between global
    accuracy and local misclassification problems, providing an extra degree of robustness.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 权重将按比例减少，与相应的绝对残差损失相关。然而，而不是使用固定的基数，选择全局置信指数。这种策略允许进一步的可适应性，因为置信度低的估计器不需要只关注一个小子集，考虑到γ(t)介于0和1（最坏情况）之间，当成本函数非常高（1x
    = 1）时，指数变得无效，因此权重保持不变。这种方法与其他变体中采用的方法不太相似，但它试图在全局准确性和局部误分类问题之间找到一个折衷方案，提供额外的鲁棒性。
- en: 'The most complex part of this algorithm is the approach employed to output
    a global prediction. Contrary to classification algorithms, we cannot easily compute
    an average, because it''s necessary to consider the global confidence at each
    iteration. Drucker proposed a method based on the weighted median of all outputs.
    In particular, given a sample xi, we define the set of predictions:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法中最复杂的部分是输出全局预测所采用的方法。与分类算法不同，我们无法轻易计算平均值，因为需要考虑每次迭代的全局置信度。Drucker提出了一种基于所有输出加权中位数的方法。特别是，给定一个样本xi，我们定义预测集：
- en: '![](img/dacfd8db-452f-45d5-88ae-f1edff8371b0.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dacfd8db-452f-45d5-88ae-f1edff8371b0.png)'
- en: 'As weights, we consider the *log(1 / γ(t))*, so we can define a weight set:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 作为权重，我们考虑log(1 / γ(t))，因此我们可以定义一个权重集：
- en: '![](img/8dc09994-0ed6-4fd1-be70-75bbb1f51c47.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8dc09994-0ed6-4fd1-be70-75bbb1f51c47.png)'
- en: The final output is the median of *Y* weighted according to *Γ* (normalized
    so that the sum is 1.0). As *γ(t) → 1* when the confidence is low, the corresponding
    weight will tend to 0\. In the same way, when the confidence is high (close to
    1.0), the weight will increase proportionally and the chance to pick the output
    associated with it will be higher. For example, if the outputs are *Y = {1, 1.2,
    1.3, 2.0, 2.2, 2.5, 2.6}* and the weights are *Γ = { 0.35, 0.15, 0.12, 0.11, 0.1,
    0.09, 0.08 }*, the weighted median corresponds to the second index, therefore
    the global estimator will output 1.2 (which is, also intuitively, the most reasonable
    choice).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出是Γ加权的中位数（归一化，总和为1.0）。当γ(t) → 1时，置信度低，相应的权重将趋于0。同样，当置信度高（接近1.0）时，权重将成比例增加，选择与其相关的输出的机会将更高。例如，如果输出是Y
    = {1, 1.2, 1.3, 2.0, 2.2, 2.5, 2.6}，权重是Γ = { 0.35, 0.15, 0.12, 0.11, 0.1, 0.09,
    0.08 }，加权中位数对应于第二个索引，因此全局估计器将输出1.2（这也是直观上最合理的选择）。
- en: 'The procedure to find the median is quite simple:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找中位数的过程相当简单：
- en: The *yi(t)* must be sorted in ascending order, so that *yi(1) < yi(2) < ...
    < yi(Nc)*
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: yi(t)必须按升序排序，以便yi(1) < yi(2) < ... < yi(Nc)
- en: The set *Γ* is sorted accordingly to the index of *yi(t)* (each output *yi(t)*
    must carry its own weight)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据yi(t)的索引对集合Γ进行排序（每个输出yi(t)都必须携带其自身的权重）
- en: The set *Γ* is normalized, dividing it by its sum
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Γ进行归一化，除以其总和
- en: The index corresponding to the smallest element that splits *Γ* into two blocks
    (whose sums are less than or equal to 0.5) is selected
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择将Γ分成两个块（其和小于或等于0.5）的最小元素对应的索引
- en: The output corresponding to this index is chosen
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与该索引对应的输出
- en: 'The complete AdaBoost.R2 algorithm is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的AdaBoost.R2算法如下：
- en: Set the family and the number of estimators *Nc*
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置家族和估计器数量Nc
- en: Set the initial weights *W(1)* equal to *1/M*
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始权重W(1)设置为1/M
- en: Set the initial distribution *D(1)* equal to the dataset *X*
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始分布D(1)设置为数据集X
- en: Select a loss function *L*
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个损失函数L
- en: 'For *i=1* to *Nc*:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于i=1到Nc：
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *D(i)*
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据分布 *D(i)* 训练第 *i* 个估计量 *di(x)*
- en: Compute the absolute residuals, the loss values, and the confidence measures
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算绝对残差、损失值和置信度度量
- en: Compute the global cost function
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算全局代价函数
- en: Update the weights using the exponential formula
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指数公式更新权重
- en: Create the global estimator using the weighted median
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用加权中位数创建全局估计量
- en: Example of AdaBoost with Scikit-Learn
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost的Scikit-Learn示例
- en: Let's continue using the Wine dataset in order to analyze the performance of
    AdaBoost with different parameters. Scikit-Learn, like almost all algorithms,
    implements both a classifier `AdaBoostClassfier` (based on the algorithm SAMME
    and SAMME.R) and a regressor `AdaBoostRegressor` (based on the algorithm R2).
    In this case, we are going to use the classifier, but I invite the reader to test
    the regressor using a custom dataset or one of the built-in toy datasets. In both
    classes, the most important parameters are `n_estimators` and `learning_rate`
    (default value set to `1.0`). The default underlying weak learner is always a
    decision tree, but it's possible to employ other models creating a base instance
    and passing it through the parameter `base_estimator`. As explained in the chapter,
    real-valued AdaBoost algorithms require an output based on a probability vector.
    In Scikit-Learn, some classifiers/regressors (such as SVM) don't compute the probabilities
    unless it is explicitly required (setting the parameter `probability=True`); therefore,
    if an exception is raised, I invite you to check the documentation in order to
    learn how to force the algorithm to compute them.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用Wine数据集来分析AdaBoost在不同参数下的性能。Scikit-Learn，像几乎所有的算法一样，实现了分类器 `AdaBoostClassifier`（基于SAMME和SAMME.R算法）和回归器
    `AdaBoostRegressor`（基于R2算法）。在这种情况下，我们将使用分类器，但我邀请读者使用自定义数据集或内置的玩具数据集测试回归器。在这两个类中，最重要的参数是
    `n_estimators` 和 `learning_rate`（默认值设置为 `1.0`）。默认的底层弱学习器始终是决策树，但可以通过创建一个基础实例并通过参数
    `base_estimator` 传递它来使用其他模型。正如章节中解释的那样，实值AdaBoost算法需要一个基于概率向量的输出。在Scikit-Learn中，一些分类器/回归器（如SVM）除非明确要求（设置参数
    `probability=True`），否则不会计算概率；因此，如果出现异常，我邀请您检查文档以了解如何强制算法计算它们。
- en: 'The examples we are going to discuss have only a didactic purpose because they
    focus on a single parameter. In a real-world scenario, it''s always better to
    perform a grid search (which is more expensive), so as to analyze a set of combinations.
    Let''s start analyzing the cross-validation score as a function of the number
    of estimators (the vectors *X* and *Y* are the ones defined in the previous example):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的示例仅具有教学目的，因为它们关注单一参数。在现实世界场景中，总是更好的执行网格搜索（这更昂贵），以便分析一组组合。让我们开始分析交叉验证分数作为估计量数量的函数（向量
    *X* 和 *Y* 是在前面示例中定义的）：
- en: '[PRE3]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have considered a range starting from 10 trees and ending with 200 trees
    with steps of 10 trees. The learning rate is kept constant and equal to 0.8\.
    The resulting plot is shown in the following graph:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了从10棵树开始到200棵树结束的范围，步长为10棵树。学习率保持恒定，等于0.8。结果图如下所示：
- en: '![](img/7ac290ef-4dc9-44c8-96e3-90f49af5d90d.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7ac290ef-4dc9-44c8-96e3-90f49af5d90d.png)'
- en: 10-fold cross-validation accuracy as a function of the number of estimators
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 10折交叉验证准确率作为估计量数量的函数
- en: The maximum is reached with 50 estimators. Larger values cause performance worsening
    due to the over-specialization and a consequent variance increase. As explained
    also in other chapters, the capacity of a model must be tuned according to the
    Occam's Razor principle, not only because the resulting model can be faster to
    train, but also because a capacity excess is normally saturated, overfitting the
    training set and reducing the scope for generalization. Cross-validation can immediately
    show this effect, which, instead, can remain hidden when a standard training/test
    set split is done (above all when the samples are not shuffled).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最大值出现在50个估计量时。更大的值会导致性能下降，因为过度专业化以及随之而来的方差增加。正如在其他章节中解释的那样，模型的能力必须根据奥卡姆剃刀原则进行调整，这不仅因为结果模型可以更快地训练，而且还因为能力过剩通常会导致过拟合训练集，并减少泛化的范围。交叉验证可以立即显示出这种效果，而标准训练/测试集分割时（尤其是当样本没有打乱时），这种效果可能仍然隐藏。
- en: 'Let''s now check the performance with different learning rates (keeping the
    number of trees fixed):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查不同学习率下的性能（保持树的数量不变）：
- en: '[PRE4]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The final plot is shown in the following graph:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表如下所示：
- en: '![](img/0341b161-a7b8-4bbe-9fbc-855f635049ab.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0341b161-a7b8-4bbe-9fbc-855f635049ab.png)'
- en: 10-fold Cross-validation accuracy as a function of the learning rate (number
    of estimators = 50)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 10倍交叉验证准确率作为学习率（估计量数量=50）的函数
- en: Again, different learning rates yield different accuracies. The choice of *η
    = 0.8* seems to be the most effective, as higher and lower values lead to performance
    worsening. As explained, the learning rate has a direct impact on the re-weighting
    process. Very small values require a larger number of estimators because subsequent
    distributions are very similar. On the other side, large values can lead to a
    premature over-specialization. Even if the default value is `1.0`, I always suggest
    checking the accuracy also with smaller values. There's no golden rule for picking
    the right learning rate in every case, but it's important to remember that lower
    values allow the algorithm to smoothly adapt to fit the training set in a gentler
    way, while higher values reduce the robustness to outliers, because the samples
    that have been misclassified are immediately boosted and the probability of sampling
    them increases very rapidly. The result of this behavior is a constant focus on
    those samples that may be affected by noise, almost forgetting the structure of
    the remaining sample space.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，不同的学习率会产生不同的准确率。选择*η = 0.8*似乎是最有效的，因为更高的和更低的价值会导致性能下降。正如解释的那样，学习率对重新加权过程有直接影响。非常小的值需要更多的估计量，因为后续分布非常相似。另一方面，大的值可能导致过早的过度专业化。即使默认值是`1.0`，我也总是建议检查使用较小值时的准确率。在每种情况下选择正确的学习率没有金科玉律，但重要的是要记住，较低的值允许算法以更温和的方式平滑地适应以适应训练集，而较高的值会降低对异常值的鲁棒性，因为被错误分类的样本会立即被提升，并且采样它们的概率会非常迅速地增加。这种行为的结果是持续关注那些可能受到噪声影响的样本，几乎忘记了剩余样本空间的结构。
- en: 'The last experiment we want to make is analyzing the performance after a dimensionality
    reduction performed with **Principal Component Analysis** (**PCA**) and **Factor
    Analysis** (**FA**) (with 50 estimators and `η = 0.8`):'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要进行的最后一个实验是分析使用**主成分分析**（PCA）和**因子分析**（FA）（50个估计量和`η = 0.8`）进行的降维后的性能：
- en: '[PRE5]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting plot is shown in the following graph:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表如下所示：
- en: '![](img/00546deb-0510-444a-b198-e697936e69d2.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00546deb-0510-444a-b198-e697936e69d2.png)'
- en: 10-fold cross-validation accuracy as a function of the number of components
    (PCA and factor analysis)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 10倍交叉验证准确率作为成分数量（主成分分析PCA和因子分析FA）的函数
- en: This exercise confirms some important features analyzed in [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml),
    *EM Algorithm and Applications*. First of all, performances are not dramatically
    affected even by a 50% dimensionality reduction. This consideration is further
    confirmed by the feature importance analysis performed in the previous example.
    Decision trees can perform quite a good classification considering only 6/7 features
    because the remaining ones offer a marginal contribution to the characterization
    of a sample. Moreover, FA is almost always superior to PCA. With 7 components,
    the accuracy achieved using the FA algorithm is higher than 0.95 (very close to
    the value achieved with no reduction), while a PCA reaches this value with 12
    components. The reader should remember that PCA is a particular case of FA, with
    the assumption of homoscedastic noise. The diagram confirms that this condition
    is not acceptable with the Wine dataset. Assuming different noise variances allows
    remodeling the reduced dataset in a more accurate way, minimizing the cross-effect
    of the missing features. Even if PCA is normally the first choice, with large
    datasets, I suggest you always compare the performance with a Factor Analysis
    and choose the technique that guarantees the best result (given also that FA is
    more expensive in terms of computational complexity).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习证实了在[第5章](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml)“EM算法及其应用”中分析的一些重要特征。首先，性能即使经过50%的维度降低也不会有显著影响。这一考虑在先前的例子中进行的特征重要性分析中得到进一步证实。决策树仅考虑6/7个特征时就能执行相当好的分类，因为剩余的特征对样本的特征化贡献微乎其微。此外，因子分析（FA）几乎总是优于主成分分析（PCA）。使用7个成分，FA算法实现的准确率高于0.95（非常接近没有减少时实现的值），而PCA需要12个成分才能达到这个值。读者应该记住，PCA是FA的一个特例，假设同方差噪声。图表证实，在Wine数据集中，这种条件是不可接受的。假设不同的噪声方差允许以更准确的方式重新建模减少的数据集，最小化缺失特征的交叉效应。即使PCA通常是首选，但在大型数据集的情况下，我建议您始终比较两种技术的性能，并选择保证最佳结果的技术（考虑到FA在计算复杂度方面更昂贵）。
- en: Gradient boosting
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升
- en: 'At this point, we can introduce a more general method of creating boosted ensembles.
    Let''s choose a generic algorithm family, represented as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以引入创建提升集成的一个更通用的方法。让我们选择一个通用的算法族，如下所示：
- en: '![](img/5d3d5c8e-4f1c-4184-9ac3-9a275caf6f87.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d3d5c8e-4f1c-4184-9ac3-9a275caf6f87.png)'
- en: 'Each model is parametrized using the vector *θi* and there are no restrictions
    on the kind of method that is employed. In this case, we are going to consider
    decision trees (which is one of the most diffused algorithms when this boosting
    strategy is employed—in this case, the algorithm is known as gradient tree boosting),
    but the theory is generic and can be easily applied to more complex models, such
    as neural networks. In a decision tree, the parameter vector *θi* is made up of
    selection tuples, so the reader can think of this method as a pseudo-random forest
    where, instead of randomness, we look for extra optimality exploiting the previous
    experience. In fact, as with AdaBoost, a gradient boosting ensemble is built sequentially,
    using a technique that is formally defined as **Forward Stage-wise Additive Modeling**.
    The resulting estimator is represented as a weighted sum:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型使用向量 *θi* 进行参数化，并且对采用的方法没有限制。在这种情况下，我们将考虑决策树（当采用这种提升策略时，这是最广泛使用的算法之一——在这种情况下，该算法被称为梯度提升树），但理论是通用的，可以很容易地应用于更复杂的模型，如神经网络。在决策树中，参数向量 *θi*
    由选择元组组成，因此读者可以将这种方法视为一个伪随机森林，其中我们寻找额外的优化，而不是随机性，利用先前的经验。事实上，与AdaBoost一样，梯度提升集成是按顺序构建的，使用一种正式定义为**前向分阶段加性建模**的技术。得到的估计量表示为一个加权求和：
- en: '![](img/a543a7f1-1bdb-4f5d-b624-8e37fd1b852a.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a543a7f1-1bdb-4f5d-b624-8e37fd1b852a.png)'
- en: 'Therefore the variables to manage are the single estimator weights *αi* and
    the parameter vectors *θi*. However, we don''t have to work with the whole set,
    but with a single tuple *(αi, θi)*, without modifying the values already chosen
    during the previous iterations. The general procedure can be summarized with a
    loop:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要管理的变量是单个估计量权重 *αi* 和参数向量 *θi*。然而，我们不必处理整个集合，而只需处理单个元组 *(αi, θi)*，无需修改之前迭代中已选择的值。一般程序可以用循环来概括：
- en: The estimator sum is initialized to a null value
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估计量之和被初始化为空值
- en: 'For *i=1* to *Nc*:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *Nc*：
- en: The best *tuple(αi, θi)* is chosen and the estimator *f(x; θi)* is trained
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最佳的 *元组(αi, θi)* 并训练估计量 *f(x; θi)*
- en: '*di(x) = di-1(x) + αif(x; θi)*'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*di(x) = di-1(x) + αif(x; θi)*'
- en: The final estimator *d(x)* is output
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出最终的估计器 *d(x)*
- en: 'How is it possible to find out the best tuple? We have already presented a
    strategy for improving the performance of every learner through boosting the dataset.
    In this case, instead, the algorithm is based on a cost function that we need
    to minimize:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如何找到最佳的元组？我们已提出一种策略，通过增强数据集来提高每个学习器的性能。在这种情况下，算法基于一个我们需要最小化的成本函数：
- en: '![](img/184477c8-9617-4b24-90e9-29f44aafd30c.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/184477c8-9617-4b24-90e9-29f44aafd30c.png)'
- en: 'In particular, the generic optimal tuple is obtained as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，通用的最优元组如下获得：
- en: '![](img/65fb3164-1039-4186-92f9-63c2bfc83488.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/65fb3164-1039-4186-92f9-63c2bfc83488.png)'
- en: As the process is sequential, each estimator is optimized to improve the previous
    one's accuracy. However, contrary to AdaBoost, we are not constrained to impose
    a specific loss function (it's possible to prove that AdaBoost.M1 is equivalent
    to this algorithm with an exponential loss but the proof is beyond the scope of
    this book). As we are going to discuss, other cost functions can yield better
    performances in several different scenarios, because they avoid the premature
    convergence towards sub-optimal minima.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过程是顺序的，每个估计器都优化以提高前一个的准确性。然而，与 AdaBoost 不同，我们并不受限于强制执行特定的损失函数（可以证明 AdaBoost.M1
    与此算法的指数损失等价，但证明超出了本书的范围）。正如我们将要讨论的，其他成本函数可以在几个不同的场景中产生更好的性能，因为它们避免了过早收敛到次优最小值。
- en: The problem could be considered as solved by employing the previous formula
    to optimize each new learner; however, the `argmin(•)` function needs a complete
    exploration of the cost function space and, as `C(•)` depends on each specific
    model instance and, therefore, on *θi*, it's necessary to perform several retraining
    processes in order to find the optimal solution. Moreover, the problem is generally
    non-convex and the number of variables can be very high. Numerical algorithms
    such as L-BFGS or other quasi-Newton methods need too many iterations and a prohibitive
    computational time. It's clear that such an approach is not affordable in the
    vast majority of cases and the Gradient Boosting algorithm has been proposed as
    an intermediate solution. The idea is to find a sub-optimal solution with a gradient
    descent strategy limited to a single step for each iteration.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 可以认为通过使用前面的公式来优化每个新的学习器，问题得到了解决；然而，`argmin(•)` 函数需要对成本函数空间进行完全探索，并且由于 `C(•)`
    依赖于每个特定的模型实例，因此也依赖于 *θi*，因此有必要进行多次重新训练过程，以找到最优解。此外，问题通常是非凸的，变量的数量可能非常高。如 L-BFGS
    或其他拟牛顿方法需要太多的迭代和计算时间。很明显，在大多数情况下，这种方法是不可行的，因此提出了梯度提升算法作为中间解决方案。其想法是找到一种次优解，通过梯度下降策略限制每个迭代的单步。
- en: 'In order to present the algorithm, it''s useful to rewrite the additive model
    with an explicit reference to the optimal goal:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍算法，用明确的参考到最优目标重写加性模型是有用的：
- en: '![](img/a5ca1d40-bc8e-4be4-b5f4-81ba1e81e284.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a5ca1d40-bc8e-4be4-b5f4-81ba1e81e284.png)'
- en: 'Note that the cost function is computed carrying on all the previously trained
    models; therefore, the correction is always incremental. If the cost function
    *L* is differentiable (a fundamental condition that is not difficult to meet),
    it''s possible to compute the gradient with respect to the current additive model
    (at the *i^(th)* iteration, we need to consider the additive model obtained summing
    all the previous *i-1* models):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，成本函数是在所有先前训练的模型上计算的；因此，校正始终是增量式的。如果成本函数 *L* 是可微分的（这是一个基本条件，但并不难满足），则可以计算相对于当前加性模型（在第
    *i^(th)* 迭代时，我们需要考虑通过累加所有先前的 *i-1* 模型得到的加性模型）的梯度：
- en: '![](img/b61fc950-e403-4b93-a086-4ecd6d187c32.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b61fc950-e403-4b93-a086-4ecd6d187c32.png)'
- en: 'At this point, a new classifier can be added by moving the current additive
    model into the negative direction of the gradient:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，可以通过将当前加性模型移动到梯度的负方向来添加一个新的分类器：
- en: '![](img/7f2f22b0-32fa-43b4-abc7-08f1946018df.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7f2f22b0-32fa-43b4-abc7-08f1946018df.png)'
- en: 'We haven''t considered the parameter αi yet (nor the learning rate *η*, which
    is a constant), however the reader familiar with some basic calculus can immediately
    understand the effect of an update is to reduce the value of the global loss function
    by forcing the next model to improve its accuracy with respect to its predecessors.
    However, a single gradient step isn''t enough to guarantee an appropriate boosting
    strategy. In fact, as discussed previously, we also need to weight each classifier
    according to its ability to reduce the loss. Once the gradient has been computed,
    it''s possible to determine the best value for the weight αi with a direct minimization
    of the loss function (using a line search algorithm) computed considering the
    current additive model with *α* as an extra variable:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未考虑参数αi（以及学习率*η*，它是一个常数），然而，熟悉一些基本微积分的读者可以立即理解更新的效果是通过迫使下一个模型提高其相对于前一个模型的准确性来减少全局损失函数的值。然而，单一步的梯度步并不足以保证适当的提升策略。实际上，如前所述，我们还需要根据每个分类器减少损失的能力来加权每个分类器。一旦计算了梯度，就可以通过直接最小化损失函数（使用线搜索算法）来确定权重αi的最佳值，考虑到当前的加性模型，并将*α*作为一个额外变量：
- en: '![](img/3b76af08-36a2-41e9-b6d7-2ad04db82758.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3b76af08-36a2-41e9-b6d7-2ad04db82758.png)'
- en: When using the gradient tree boosting variant, an improvement can be achieved
    by splitting the weight *αi* into *m* sub-weights *αi(j)* associated with each
    terminal node of the tree. The computational complexity is slightly increased,
    but the final accuracy can be higher than the one obtained with a single weight.
    The reason derives from the functional structure of a tree. As the boosting forces
    a specialization in specific regions, a single weight could drive to an over-estimation
    of a learner also when a specific sample cannot be correctly classified. Instead,
    using different weights, it's possible to operate a fine-grained filtering of
    the result, accepting or discarding an outcome according to its value and to the
    properties of the specific tree.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用梯度提升树变体时，可以通过将权重*αi*分割成与树中每个终端节点关联的*m*个子权重*αi(j)*来获得改进。计算复杂度略有增加，但最终精度可以高于使用单个权重获得的精度。原因在于树的功能结构。由于提升迫使在特定区域进行专业化，单个权重可能导致学习器在特定样本无法正确分类时也进行过估计。相反，使用不同的权重，可以操作结果进行精细过滤，根据其值和特定树的性质接受或丢弃结果。
- en: This solution cannot provide the same accuracy of a complete optimization, but
    it's rather fast and it's possible to compensate for this loss using more estimators
    and a lower learning rate. Like many other algorithms, gradient boosting must
    be tuned up in order to yield the maximum accuracy with a low variance. The learning
    rate is normally quite smaller than 1.0 and its value should be found by validating
    the results and considering the total number of estimators (it's better to reduce
    it when more learners are employed). Moreover, a regularization technique could
    be added in order to prevent overfitting. When working with specific classifier
    families (such as logistic regression or neural networks), it's very easy to include
    an *L1* or *L2* penalty, but it's not so easy with other estimators. For this
    reason, a common regularization technique (implemented also by Scikit-Learn) is
    the downsampling of the training dataset. Selecting *P < N* random samples allows
    the estimators to reduce the variance and prevent overfitting. Alternatively,
    it's possible to employ a random feature selection (for gradient tree boosting
    only) as in a random forest; picking a fraction of the total number of features
    increases the uncertainty and avoids over-specialization. Of course, the main
    drawback to these techniques is a loss of accuracy (proportional to the downsampling/feature
    selection ratio) that must be analyzed in order to find the most appropriate trade-off.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案不能提供完整优化相同的准确性，但它非常快，并且可以通过使用更多的估计量和更低的学习率来补偿这种损失。像许多其他算法一样，梯度提升必须调整以获得最大精度和低方差。学习率通常远小于1.0，其值应通过验证结果并考虑估计量的总数来确定（当使用更多学习者时最好减少它）。此外，可以添加正则化技术来防止过拟合。当与特定的分类器家族（如逻辑回归或神经网络）一起工作时，包括*L1*或*L2*惩罚非常容易，但与其他估计量一起则不太容易。因此，一种常见的正则化技术（也被Scikit-Learn实现）是训练数据集的下采样。选择*P
    < N*随机样本允许估计量减少方差并防止过拟合。或者，可以采用随机特征选择（仅适用于梯度树提升），就像随机森林一样；选择总特征数的一部分会增加不确定性并避免过度专业化。当然，这些技术的缺点是精度损失（与下采样/特征选择比成比例），必须分析以找到最合适的权衡。
- en: 'Before moving to the next section, it''s useful to briefly discuss the main
    cost functions that are normally employed with this kind of algorithms. In the
    first chapter, we have presented some common cost functions, like mean squared
    error, Huber Loss (very robust in regression contexts), and cross-entropy. They
    are all valid examples, but there are other functions that are peculiar to classification
    problems. The first one is Exponential Loss, defined as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，简要讨论一下通常与这类算法一起使用的成本函数是有用的。在第一章中，我们介绍了一些常见的成本函数，如均方误差、Huber Loss（在回归上下文中非常稳健）和交叉熵。它们都是有效的例子，但还有其他特定于分类问题的函数。第一个是指数损失，定义如下：
- en: '![](img/90bfefe0-decf-497b-aab8-9add8054ad98.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![指数损失图](img/90bfefe0-decf-497b-aab8-9add8054ad98.png)'
- en: 'As pointed out by Hastie, Tibshirani and, Friedman, this function transforms
    the gradient boosting into an AdaBoost.M1 algorithm. The corresponding cost function
    has a very precise behavior that sometimes is not the most adequate to solve particular
    problems. In fact, the result of an exponential loss has a very high impact when
    the error is large, yielding distributions that are strongly peaked around a few
    samples. The subsequent classifiers can be consequently driven to over-specialize
    their structure to cope only with a small data region, with a concrete risk of
    losing the ability to correctly classify other samples. In many situations, this
    behavior is not dangerous and the final bias-variance trade-off is absolutely
    reasonable; however, there are problems where a softer loss function can allow
    a better final accuracy and generalization ability. The most common choice for
    real-valued binary classification problems is Binomial Negative Log-Likelihood
    Loss (deviance), defined as follows (in this case we are assuming that the classifier
    *f(•)* is not thresholded, but outputs a positive-class probability):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如Hastie、Tibshirani和Friedman所指出的，此函数将梯度提升转换为AdaBoost.M1算法。相应的成本函数具有非常精确的行为，有时并不适合解决特定问题。事实上，指数损失的误差很大时，其结果有很高的影响，产生围绕几个样本的强烈峰值分布。随后分类器可能会因此过度专门化其结构，仅应对小数据区域，存在失去正确分类其他样本能力的高度风险。在许多情况下，这种行为并不危险，最终的偏差-方差权衡是完全合理的；然而，有些问题中，较软的损失函数可以允许更好的最终准确性和泛化能力。对于实值二元分类问题，最常见的选择是二项式负对数似然损失（偏差），定义如下（在这种情况下我们假设分类器
    *f(•)* 没有阈值，而是输出正类概率）：
- en: '![](img/fd2ed604-b82b-4fe5-9585-c1585c579e3a.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fd2ed604-b82b-4fe5-9585-c1585c579e3a.png)'
- en: 'This loss function is the same employed in Logistic Regressions and, contrary
    to Exponential Loss, doesn''t yield peaked distributions. Two misclassified samples
    with different probabilities are boosted proportionally to the error (not the
    exponential value), so as to force the classifiers to focus on all the misclassified
    population with almost the same probability (of course, a higher probability assigned
    to samples whose error is very large is desirable, assuming that all the other
    misclassified samples have always a good chance to be selected). The natural extension
    of the Binomial Negative Log-Likelihood Loss to multi-class problems is the Multinomial
    Negative Log-Likelihood Loss, defined as follows (the classifier *f(•)* is represented
    as probability vector with *p* components):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此损失函数与逻辑回归中使用的相同，与指数损失相反，不会产生峰值分布。两个被错误分类的样本将以错误（而不是指数值）的比例进行提升，以便迫使分类器几乎以相同的概率关注所有被错误分类的群体（当然，对于错误非常大的样本分配更高的概率是可取的，假设所有其他被错误分类的样本都有很好的机会被选中）。二项式负对数似然损失的自然扩展到多类问题是多项式负对数似然损失，定义如下（分类器
    *f(•)* 表示为具有 *p* 个成分的概率向量）：
- en: '![](img/0dd27751-38b1-48d8-bb41-3f25a9466aa9.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0dd27751-38b1-48d8-bb41-3f25a9466aa9.png)'
- en: In the previous formula, the notation *Iy=j* must be interpreted as an indicator
    function, which is equal to 1 when *y=j* and 0 otherwise. The behavior of this
    loss function is perfectly analogous to the binomial variant and, in general,
    it is the default choice for classification problems. The reader is invited to
    test the examples with both exponential loss and deviance and compare the results.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，符号 *Iy=j* 必须解释为指示函数，当 *y=j* 时等于1，否则为0。此损失函数的行为与二项式变体完全类似，并且通常默认用于分类问题。读者被邀请测试使用指数损失和偏差的示例，并比较结果。
- en: 'The complete gradient boosting algorithm is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的梯度提升算法如下：
- en: Set the family and the number of estimators *Nc*
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置家族和估计器数量 *Nc*
- en: Select a loss function *L* (for example, deviance)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个损失函数 *L*（例如，偏差）
- en: Initialize the base estimator *d0(x)* as a constant (such as 0) or using another
    model
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将基础估计器 *d0(x)* 初始化为一个常数（例如0）或使用另一个模型
- en: Set the learning rate *η* (such as *η = 1*)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习率 *η*（例如 *η = 1*）
- en: 'For *i=1* to *Nc*:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *Nc*：
- en: Compute the gradient *∇d L(•)* using the additive model at the step *i-1*
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤 *i-1* 的加性模型计算梯度 *∇d L(•)*
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *{ (xi, ∇d L(yi, di-1(xi))
    }*
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据分布 *{ (xi, ∇d L(yi, di-1(xi)) }* 训练第 *i* 个估计器 *di(x)*
- en: Perform a line search to compute *αi*
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行线性搜索以计算 *αi*
- en: Add the estimator to the ensemble
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将估计器添加到集成中
- en: Example of gradient tree boosting with Scikit-Learn
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn 的梯度提升树示例
- en: 'In this example, we want to employ a gradient tree boosting classifier (class
    `GradientBoostingClassifier`) and check the impact of the maximum tree depth (`parameter
    max_depth`) on the performance. Considering the previous example, we start by
    setting `n_estimators=50` and `learning_rate=0.8`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们想要使用梯度提升树分类器（类 `GradientBoostingClassifier`）并检查最大树深度（参数 `max_depth`）对性能的影响。考虑到之前的例子，我们首先设置
    `n_estimators=50` 和 `learning_rate=0.8`：
- en: '[PRE6]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result is shown in the following diagram:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](img/c5252e92-7501-42d6-911c-b59617e11481.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5252e92-7501-42d6-911c-b59617e11481.png)'
- en: 10-fold Cross-validation accuracy as a function of the maximum tree depth
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最大树深度作为 10 折交叉验证准确率的函数
- en: 'As explained in the first section, the maximum depth of a decision tree is
    strictly related to the possibility of interaction among features. This can be
    a positive or negative aspect when the trees are employed in an ensemble. A very
    high interaction level can create over-complex separation hyperplanes and reduce
    the overall variance. In other cases, a limited interaction results in a higher
    bias. With this particular (and simple) dataset, the gradient boosting algorithm
    can achieve better performances when the max depth is 2 (consider that the root
    has a depth equal to zero) and this is partially confirmed by both the feature
    importance analysis and dimensionality reductions. In many real-world situations,
    the result of such a research could be completely different, with increased performance,
    therefore I suggest you cross-validate the results (it''s better to employ a grid
    search) starting from a minimum depth and increasing the value until the maximum
    accuracy has been achieved. With `max_depth=2`, we want now to tune up the learning
    rate, which is a fundamental parameter in this algorithm:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如第一部分所述，决策树的最大深度与特征之间的交互可能性密切相关。当这些树在集成中使用时，这可能是一个正面或负面的方面。非常高的交互水平可能会创建过度复杂的分离超平面并降低整体方差。在其他情况下，有限的交互会导致更高的偏差。在这个特定的（且简单的）数据集上，梯度提升算法在最大深度为
    2（考虑到根的深度为零）时可以达到更好的性能，这一点部分得到了特征重要性分析和降维的证实。在许多现实世界的情况下，这种研究的结果可能会有很大的不同，性能可能会提高，因此我建议从最小深度开始交叉验证结果（最好使用网格搜索），直到达到最大准确率。现在，我们想要调整学习率，这是该算法的一个基本参数：
- en: '[PRE7]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The corresponding plot is shown in the following diagram:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的图表如下所示：
- en: '![](img/0af543ef-0cee-4572-ba36-71ce1ba7e228.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0af543ef-0cee-4572-ba36-71ce1ba7e228.png)'
- en: 10-fold Cross-validation accuracy as a function of the learning rate (max depth
    equal to 2)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率（最大深度等于 2）作为 10 折交叉验证准确率的函数
- en: Unsurprisingly, gradient tree boosting outperforms AdaBoost with *η ≈ 0.9*,
    achieving a cross-validation accuracy slightly lower than 0.99\. The example is
    very simple, but it clearly shows the power of this kind of techniques. The main
    drawback is the complexity. Contrary to single models, ensembles are more sensitive
    to changes to the hyperparameters and more detailed research must be conducted
    in order to optimize the models. When the datasets are not excessively large,
    cross-validation remains the best choice. If, instead, we are pretty sure that
    the dataset represents almost perfectly the underlying data generating process,
    it's possible to shuffle it and split it into two (training/test) or three blocks
    (training/test/validation) and proceed by optimizing the hyperparameters and trying
    to overfit the test set (this expression can seem strange, but overfitting the
    test set means maximizing the generalization ability while learning perfectly
    the training set structure).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，梯度提升树在 *η ≈ 0.9* 时优于 AdaBoost，交叉验证准确率略低于 0.99。这个例子非常简单，但它清楚地展示了这种技术的能力。主要的缺点是复杂性。与单个模型不同，集成对超参数的变化更敏感，必须进行更详细的研究以优化模型。当数据集不是特别大时，交叉验证仍然是最佳选择。如果我们相当确信数据集几乎完美地代表了底层数据生成过程，那么可以对其进行洗牌并分成两个（训练/测试）或三个块（训练/测试/验证），然后通过优化超参数并尝试过度拟合测试集（这个表达可能听起来有些奇怪，但过度拟合测试集意味着在完美学习训练集结构的同时最大化泛化能力）。
- en: Ensembles of voting classifiers
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票分类器的集成
- en: 'A simpler but no less effective way to create an ensemble is based on the idea
    of exploiting a limited number of strong learners whose peculiarities allow them
    to yield better performances in particular regions of the sample space. Let''s
    start considering a set of *Nc* discrete-valued classifiers *f1(x), f2(x), ...,
    fNc(x)*. The algorithms are different, but they are all trained with the same
    dataset and output the same label set. The simplest strategy is based on a hard-voting
    approach:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集成的一个更简单但同样有效的方法是基于利用有限数量的强学习者的想法，这些学习者的特性使它们能够在样本空间的特定区域获得更好的性能。让我们首先考虑一组
    *Nc* 个离散值分类器 *f1(x), f2(x), ..., fNc(x)*。算法不同，但它们都是用相同的训练集训练的，并输出相同的标签集。最简单的策略是基于硬投票方法：
- en: '![](img/81f2b833-ad97-48b4-9fa1-893d280000be.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/81f2b833-ad97-48b4-9fa1-893d280000be.png)'
- en: 'In this case, the function *n(•)* counts the number of estimators that output
    the label *yi*. This method is rather powerful in many cases, but has some limitations.
    If we rely only on a majority vote, we are implicitly assuming that a correct
    classification is obtained by a large number of estimators. Even if, *Nc/2 + 1*
    votes are necessary to output a result, in many cases their number is much higher.
    Moreover, when k is not very large, also *Nc/2 + 1* votes imply a symmetry that
    involves a large part of the population. This condition often drives to the training
    of useless models that could be simply replaced by a single well-fitted strong
    learner. In fact, let''s suppose that the ensemble is made up of three classifiers
    and one of them is more specialized in regions where the other two can easily
    be driven to misclassifications. A hard-voting strategy applied to this ensemble
    could continuously penalize the more complex estimator in favor of the other classifiers.
    A more accurate solution can be obtained by considering real-valued outcomes.
    If each estimator outputs a probability vector, the confidence of a decision is
    implicitly encoded in the values. For example, a binary classifier whose output
    is *(0.52, 0.48)* is much more uncertain than another classifier outputting *(0.95,
    0.05)*. Applying a threshold is equivalent to flattening the probability vectors
    and discarding the uncertainty. Let''s consider an ensemble with three classifiers
    and a sample that is hard to classify because it''s very close to the separation
    hyperplane. A hard-voting strategy decides for the first class because the thresholded
    output is *(1, 1, 2)*. Then we check the output probabilities, obtaining *(0.51,
    0.49)*, *(0.52, 0.48)*, *(0.1, 0.9)*. After averaging the probabilities, the ensemble
    output becomes about (0.38, 062) and by applying `argmax(•)`, we get the second
    class as the final decision. In general, it''s also a good practice to consider
    a weighted average, so that the final class is obtained as follows (assuming the
    output of the classifier is a probability vector):'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，函数 *n(•)* 计算输出标签 *yi* 的估计器的数量。这种方法在许多情况下非常强大，但也有一些局限性。如果我们只依赖于多数投票，我们隐含地假设通过大量估计器获得了正确的分类。即使需要
    *Nc/2 + 1* 票来输出结果，在许多情况下，它们的数量要高得多。此外，当 k 不是非常大时，*Nc/2 + 1* 票也意味着涉及大量人群的对称性。这种条件往往导致训练出无用的模型，这些模型可以简单地被单个拟合良好的强学习器所取代。事实上，假设集成由三个分类器组成，其中一个在另外两个容易导致误分类的区域更为专业化。对这种集成应用硬投票策略可能会持续惩罚更复杂的估计器，以利于其他分类器。通过考虑实值结果可以获得更准确的解决方案。如果每个估计器输出一个概率向量，决策的置信度就隐含地编码在值中。例如，输出为
    *(0.52, 0.48)* 的二元分类器比输出为 *(0.95, 0.05)* 的另一个分类器要不确定得多。应用阈值相当于将概率向量展平并丢弃不确定性。让我们考虑一个由三个分类器组成的集成和一个难以分类的样本，因为它非常接近分离超平面。硬投票策略决定选择第一类，因为阈值后的输出是
    *(1, 1, 2)*。然后我们检查输出概率，得到 *(0.51, 0.49)*，*(0.52, 0.48)*，*(0.1, 0.9)*。在平均概率后，集成输出变为大约
    (0.38, 062)，通过应用 `argmax(•)`，我们得到第二类作为最终决策。一般来说，考虑加权平均也是一个好的实践，这样最终的类别可以通过以下方式获得（假设分类器的输出是一个概率向量）：
- en: '![](img/d745df4d-35df-4a4e-b758-3be4cfde5cc3.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d745df4d-35df-4a4e-b758-3be4cfde5cc3.png)'
- en: The weights can be simply equal to 1.0 if no weighting is required or they can
    reflect the level of trust we have for each classifier. An important rule is to
    avoid the dominance of a classifier in the majority of cases because it would
    be an implicit fallback to a single estimator scenario. A good voting example
    should always allow a minority to overturn a result when their confidence is quite
    higher than the majority. In this strategies, the weights can be considered as
    hyperparameters and tuned up using a grid search with cross-validation. However,
    contrary to other ensemble methods, they are not fine-grained, therefore the optimal
    value is often a compromise among some different possibilities.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不需要加权，权重可以简单地等于1.0，或者它们可以反映我们对每个分类器的信任程度。一个重要的规则是在大多数情况下避免一个分类器的支配地位，因为这会隐含地回到单个估计器场景。一个好的投票例子应该始终允许当少数派的信心远高于多数派时，推翻结果。在这种情况下，权重可以被视为超参数，并使用网格搜索与交叉验证进行调整。然而，与其他集成方法不同，它们不是细粒度的，因此最佳值通常是不同可能性之间的折衷。
- en: A slightly more complex technique is called **stacking** and consists of using
    an extra classifier as a post-filtering step. The classical approach consists
    of training the classifiers separately, then the whole dataset is transformed
    into a prediction set (based on class labels or probabilities) and the combining
    classifier is trained to associate the predictions to the final classes. Using
    even very simple models like Logistic Regressions or Perceptrons, it's possible
    to mix up the predictions so as to implement a dynamic reweighting that is a function
    of the input values. A more complex approach is feasible only when a single training
    strategy can be used to train the whole ensemble (including the combiner). For
    example, it could be employed with neural networks that, however, have already
    an implicit flexibility and can often perform quite better than complex ensembles.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一种稍微复杂的技术称为**堆叠**，它包括使用一个额外的分类器作为后过滤步骤。经典的方法包括分别训练分类器，然后将整个数据集转换为一个预测集（基于类别标签或概率），然后训练组合分类器以将预测与最终类别关联起来。即使使用像逻辑回归或感知器这样非常简单的模型，也可以混合预测，以实现一个作为输入值函数的动态重新加权。只有当可以使用单个训练策略来训练整个集成（包括组合器）时，才可行更复杂的方法。例如，它可以与神经网络一起使用，尽管神经网络已经具有隐含的灵活性，并且通常可以比复杂的集成表现得更好。
- en: Example of voting classifiers with Scikit-Learn
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中的投票分类器示例
- en: 'In this example, we are going to employ the MNIST handwritten digits dataset.
    As the concept is very simple, our goal is to show how to combine two completely
    different estimators to improve the overall cross-validation accuracy. For this
    reason, we have selected a Logistic Regression and a decision tree, which are
    structurally different. In particular, while the former is a linear model that
    works with the whole vectors, the latter is a feature-wise estimator that can
    support the decision only in particular cases (images are not made up of semantically
    consistent features, but the over-complexity of a Decision Tree can help with
    particular samples which are very close to the separation hyperplane and, therefore,
    more difficult to classify with a linear method). The first step is loading and
    normalizing the dataset (this operation is not important with a Decision Tree,
    but has a strong impact on the performances of a Logistic Regression):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用MNIST手写数字数据集。由于概念非常简单，我们的目标是展示如何结合两个完全不同的估计器来提高整体交叉验证的准确性。因此，我们选择了一个逻辑回归和一个决策树，它们在结构上是不同的。特别是，前者是一个线性模型，它使用整个向量，而后者是一个基于特征的估计器，只能在特定情况下支持决策（图像不是由语义上一致的特征组成，但决策树的过度复杂性可以帮助那些非常接近分离超平面的特定样本，因此，使用线性方法对这些样本进行分类会更困难）。第一步是加载和归一化数据集（这个操作对决策树来说并不重要，但对逻辑回归的性能有强烈的影响）：
- en: '[PRE8]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'At this point, we need to evaluate the performances of both estimators individually:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要单独评估两个估计器的性能：
- en: '[PRE9]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As expected, the Logistic Regression (∼94% accuracy) outperforms the decision
    tree (83% accuracy); therefore, a hard-voting strategy is not the best choice.
    As we trust the Logistic Regression more, we can employ soft voting with a weight
    vector set to *(0.9, 0.1)*. The class `VotingClassifier` accepts a list of tuples
    (name of the estimator, instance) that must be supplied through the `estimators`
    parameter. The strategy can be specified using parameter voting (it can be either
    "soft" or "hard") and the optional weights, using the parameter with the same
    name:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，逻辑回归（准确率约为94%）优于决策树（准确率83%）；因此，硬投票策略并不是最佳选择。由于我们更信任逻辑回归，我们可以采用带有权重向量设置为*(0.9,
    0.1)*的软投票。`VotingClassifier`类接受一个包含元组的列表（估计器的名称，实例），这些元组必须通过`estimators`参数提供。策略可以通过参数投票（可以是“软”或“硬”）和可选的权重来指定，使用具有相同名称的参数：
- en: '[PRE10]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using a soft-voting strategy, the estimator is able to outperform Logistic Regression
    by reducing the global uncertainty. I invite the reader to test this algorithm
    with other datasets, using more estimators, and try to find out the optimal combination
    using both the hard and soft voting strategies.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用软投票策略，估计器能够通过减少全局不确定性来超越逻辑回归。我邀请读者使用其他数据集测试此算法，使用更多估计器，并尝试使用硬投票和软投票策略找到最佳组合。
- en: Ensemble learning as model selection
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习作为模型选择
- en: 'This is not a proper ensemble learning technique, but it is sometimes known
    as **bucketing**. In the previous section, we have discussed how a few strong
    learners with different peculiarities can be employed to make up a committee.
    However, in many cases, a single learner is enough to achieve a good bias-variance
    trade-off but it''s not so easy to choose among the whole Machine Learning algorithm
    population. For this reason, when a family of similar problems must be solved
    (they can differ but it''s better to consider scenarios that can be easily compared),
    it''s possible to create an ensemble containing several models and use cross-validation
    to find the one whose performances are the best. At the end of the process, a
    single learner will be used, but its choice can be considered like a grid search
    with a voting system. Sometimes this technique can unveil important differences
    even using similar datasets. For example, during the development of a system,
    a first dataset (*X1, Y1*) is provided. Everybody expects that it is correctly
    sampled from an underlying data generating process p[data] and, so, a generic
    model is fitted and evaluated. Let''s imagine that a SVM achieves a very high
    validation accuracy (evaluated using a k-fold cross-validation) and, therefore,
    it is chosen as the final model. Unfortunately, a second, larger dataset (*X2,
    Y2*) is provided and the final mean accuracy worsens. We might simply think that
    the residual variance of the model cannot let it generalize correctly or, as sometimes
    happens, we can say the second dataset contains many outliers which are not correctly
    classified. The real situation is a little more complex: given a dataset, we can
    only suppose that it represents a complete data distribution. Even when the number
    of samples is very high or we use data augmentation techniques, the population
    might not represent some particular samples that will be analyzed by the system
    we are developing. Bucketing is a good way to create a security buffer that can
    be exploited whenever the scenario changes. The ensemble can be made up of completely
    different models, models belonging to the same family but differently parametrized
    (for example, different kernel SVMs) or a mixture of composite algorithms (like
    PCA + SVM, PCA + decision trees/random forests, and so on). The most important
    element is the cross-validation. As explained in the first chapter, splitting
    the dataset into training and test sets can be an acceptable solution only when
    the number of samples and their variability is high enough to justify the belief
    that it correctly represents the final data distribution. This often happens in
    deep learning, where the dimensions of the datasets are quite large and the computational
    complexity doesn''t allow retraining the model too many times. Instead, in classical
    Machine Learning contexts, cross-validation is the only way to check the behavior
    of a model when trained with a large random subset and tested on the remaining
    samples. Ideally, we''d like to observe the same performances, but it can also
    happen that the accuracy is higher in some folds and quite lower in other. When
    this phenomenon is observed and the dataset is the final one, it probably means
    that the model is not able to manage one or more regions of the sample space and
    a boosting approach could dramatically improve the final accuracy.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个合适的集成学习技术，但有时它被称为**桶化**。在前一节中，我们讨论了如何使用具有不同特性的几个强学习器来组成一个委员会。然而，在许多情况下，单个学习器就足以实现良好的偏差-方差权衡，但要在整个机器学习算法群体中选择它并不那么容易。因此，当必须解决一系列类似的问题（它们可以不同，但最好考虑可以轻松比较的场景）时，可以创建一个包含多个模型的集成，并使用交叉验证来找到表现最好的那个。在过程的最后，将使用单个学习器，但其选择可以被视为带有投票系统的网格搜索。有时，即使使用相似的数据库，这种技术也能揭示出重要的差异。例如，在系统开发过程中，提供了一个初始数据集（*X1,
    Y1*）。每个人都期望它是从潜在的数据生成过程 p[data] 中正确采样的，因此，拟合并评估了一个通用模型。让我们想象一下，SVM 实现了非常高的验证准确率（使用
    k 折交叉验证评估），因此被选为最终模型。不幸的是，提供了一个更大的第二个数据集（*X2, Y2*），最终的平均准确率变差了。我们可能会简单地认为模型的残差方差无法使其正确泛化，或者，正如有时发生的那样，我们可以说第二个数据集包含许多未被正确分类的异常值。实际情况要复杂一些：给定一个数据集，我们只能假设它代表了一个完整的数据分布。即使样本数量非常高或我们使用数据增强技术，总体可能也不代表系统将要分析的某些特定样本。桶化是一种创建安全缓冲区的好方法，可以在场景发生变化时利用。集成可以由完全不同的模型组成，属于同一家族但参数不同的模型（例如，不同的核
    SVM），或者是由复合算法（如 PCA + SVM、PCA + 决策树/随机森林等）的混合。最重要的元素是交叉验证。正如第一章中解释的那样，将数据集分为训练集和测试集可能只是一种可接受的解决方案，当样本数量及其变异性足够高，足以证明它正确地代表了最终数据分布时。这通常发生在深度学习中，数据集的维度相当大，计算复杂性不允许多次重新训练模型。相反，在经典的机器学习环境中，交叉验证是唯一检查模型在用大随机子集训练并在剩余样本上测试时的行为的方法。理想情况下，我们希望观察到相同的性能，但有时也会发生某些折叠的准确率更高，而其他折叠的准确率相当低。当观察到这种现象并且数据集是最终数据集时，这可能意味着模型无法管理样本空间的一个或多个区域，而提升方法可以显著提高最终准确率。
- en: Summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the main concepts of ensemble learning, focusing
    on both bagging and boosting techniques. In the first section, we explained the
    difference between strong and weak learners and we presented the big picture of
    how it's possible to combine the estimators to achieve specific goals.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了集成学习的核心概念，重点关注了bagging和boosting技术。在第一部分，我们解释了强学习者和弱学习者之间的区别，并展示了如何结合估计器以实现特定目标的大致图景。
- en: The next topic focused on the properties of decision trees and their main strengths
    and weaknesses. In particular, we explained that the structure of a tree causes
    a natural increase in the variance. The bagging technique called random forests
    allow mitigating this problem, improving at the same time the overall accuracy.
    A further variance reduction can be achieved by increasing the randomness and
    employing a variant called **extra randomized trees**. In the example, we have
    also seen how it's possible to evaluate the importance of each input feature and
    perform dimensionality reduction without involving complex statistical techniques.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的主题集中在决策树的性质及其主要优势和劣势上。特别是，我们解释了树的结构会导致方差的自然增加。bagging技术随机森林允许减轻这个问题，同时提高整体准确性。通过增加随机性和采用称为**额外随机树**的变体，可以实现进一步的方差减少。在示例中，我们还看到了如何评估每个输入特征的重要性，并在不涉及复杂统计技术的情况下进行降维。
- en: In the third section, we presented the most famous boosting techniques, AdaBoost,
    which is based on the concept of creating a sequential additive model, when each
    new estimator is trained using a reweighted (boosted) data distribution. In this
    way, every learner is added to focus on the misclassified samples without interfering
    with the previously added models. We analyzed the original M1 discrete variant
    and the most effective alternatives called SAMME and SAMME.R (real-valued), and
    R2 (for regressions), which are implemented in many Machine Learning packages.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三部分，我们介绍了最著名的boosting技术AdaBoost，它基于创建一个序列加性模型的概念，其中每个新估计器都是使用重新加权的（boosted）数据分布进行训练的。这样，每个学习者都被添加进来，专注于错误分类的样本，而不干扰之前添加的模型。我们分析了原始的M1离散变体以及最有效的替代方案SAMME和SAMME.R（实值），以及R2（用于回归），这些都在许多机器学习软件包中实现。
- en: After AdaBoost, we extended the concept to a generic Forward Stage-wise Additive
    Model, where the task of each new estimator is to minimize a generic cost function.
    Considering the complexity of a full optimization, a gradient descent technique
    was presented that, combined with an estimator weight line search, can yield excellent
    performances both in classification and in regression problems.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在AdaBoost之后，我们将这一概念扩展到通用的前向分阶段加性模型，其中每个新估计器的任务是使一个通用的成本函数最小化。考虑到完整优化的复杂性，提出了一种梯度下降技术，结合估计器权重线性搜索，可以在分类和回归问题中产生出色的性能。
- en: The final topics concerned how to build ensembles using a few strong learners,
    averaging their prediction or considering a majority vote. We discussed the main
    drawback of thresholded classifiers and we showed how it's possible to build a
    soft-voting model that is able to trust the estimator that show less uncertainty.
    Other useful topics are the Stacking method, which consists of using an extra
    classifier to process the prediction of each member of the ensemble and how it's
    possible to create candidate ensembles that are evaluated using a cross-validation
    technique to find out the best estimator for each specific problem.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的主题涉及如何使用少数强学习器构建集成，平均它们的预测或考虑多数投票。我们讨论了阈值分类器的主要缺点，并展示了如何构建一个软投票模型，该模型能够信任表现出较少不确定性的估计器。其他有用的主题包括Stacking方法，它包括使用额外的分类器处理集成中每个成员的预测，以及如何创建候选集成，这些集成使用交叉验证技术评估，以找出每个特定问题的最佳估计器。
- en: In the next chapter, we are going to begin discussing the most important deep
    learning techniques, introducing the fundamental concepts regarding neural networks
    and the algorithms involved in their training processes.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论最重要的深度学习技术，介绍关于神经网络及其训练过程中的基本概念。
