- en: Linear Models with scikit-learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行线性模型
- en: 'This chapter contains the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下几个步骤：
- en: Fitting a line through data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据拟合一条直线
- en: Fitting a line through data with machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习通过数据拟合一条直线
- en: Evaluating the linear regression model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估线性回归模型
- en: Using ridge regression to overcome linear regression's shortfalls
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用岭回归克服线性回归的不足
- en: Optimizing the ridge regression parameter
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化岭回归参数
- en: Using sparsity to regularize models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用稀疏性正则化模型
- en: Taking a more fundamental approach to regularization with LARS
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用更基础的方法使用LARS进行正则化
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: I conjecture that we are built to perceive linear functions very well. They
    are very easy to visualize, interpret, and explain. Linear regression is very
    old and was probably the first statistical model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我推测我们天生能很好地感知线性函数。它们非常容易可视化、解释和说明。线性回归非常古老，可能是第一个统计模型。
- en: In this chapter, we will take a machine learning approach to linear regression.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将采用机器学习方法进行线性回归。
- en: Note that this chapter, similar to the chapter on dimensionality reduction and
    PCA, involves selecting the best features using linear models. Even if you decide
    not to perform regression for predictions with linear models, you can select the
    most powerful features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这一章节与降维和PCA章节类似，涉及使用线性模型选择最佳特征。即使您决定不使用线性模型进行预测回归，也可以选择最有效的特征。
- en: Also note that linear models provide a lot of the intuition behind the use of
    many machine learning algorithms. For example, RBF-kernel SVMs have smooth boundaries,
    which when looked at up close, look like a line. Thus, SVMs are often easy to
    explain if, in the background, you remember your linear model intuition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，线性模型提供了许多机器学习算法使用背后的直觉。例如，RBF核SVM具有平滑边界，从近距离看，它们看起来像一条直线。因此，如果你记住你的线性模型直觉，解释SVM就会变得容易。
- en: Fitting a line through data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过数据拟合一条直线
- en: Now we will start with some basic modeling with linear regression. Traditional
    linear regression is the first, and therefore, probably the most fundamental model—a
    straight line through data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从线性回归的基础建模开始。传统线性回归是第一个，因此可能是最基本的模型——数据的一条直线。
- en: 'Intuitively, it is familiar to a lot of the population: a change in one input
    variable proportionally changes the output variable. It is important that many
    people will have seen it in school, or in a newspaper data graphic, or in a presentation
    at work, and so on, as it will be easy for you to explain to colleagues and investors.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，对于大多数人来说很熟悉：一个输入变量的变化会按比例改变输出变量。许多人在学校、报纸的数据图表、工作中的演示中都见过它，因此你可以很容易向同事和投资者解释它。
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The Boston dataset is perfect to play around with regression. The Boston dataset
    has the median home price of several areas in Boston. It also has other factors
    that might impact housing prices, for example, crime rate. First, import the `datasets`
    model, then we can load the dataset:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿数据集非常适合用于回归分析。波士顿数据集包括波士顿多个地区的房屋中位数价格。它还包括可能影响房价的其他因素，例如犯罪率。首先，导入`datasets`模块，然后我们可以加载数据集：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: Actually, using linear regression in scikit-learn is quite simple. The API for
    linear regression is basically the same API you're now familiar with from the
    previous chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在scikit-learn中使用线性回归非常简单。线性回归的API基本上与你从前一章已经熟悉的API相同。
- en: 'First, import the `LinearRegression` object and create an object:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`LinearRegression`对象并创建一个对象：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, it''s as easy as passing the independent and dependent variables to the
    `fit` method of `LinearRegression`:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，只需将独立变量和因变量传递给`LinearRegression`的`fit`方法即可开始：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, to get the predictions, do the following:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，要获得预测结果，请执行以下操作：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You have obtained the predictions produced by linear regression. Now, explore
    the `LinearRegression` class a bit more. Look at the residuals, the difference
    between the real target set and the predicted target set:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你已经获得了线性回归生成的预测结果。现在，进一步探索`LinearRegression`类。查看残差，即实际目标集和预测目标集之间的差异：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/0af36255-f283-4278-97a1-dc8e72212c2d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0af36255-f283-4278-97a1-dc8e72212c2d.png)'
- en: A common pattern to express the coefficients of features and their names is
    `zip(boston.feature_names, lr.coef_)`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表达特征系数及其名称的常见模式是`zip(boston.feature_names, lr.coef_)`。
- en: 'Find the coefficients of the linear regression by typing `lr.coef_`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过键入`lr.coef_`找到线性回归的系数：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, going back to the data, we can see which factors have a negative relationship
    with the outcome, and also the factors that have a positive relationship. For
    example, and as expected, an increase in the per capita crime rate by town has
    a negative relationship with the price of a home in Boston. The per capita crime
    rate is the first coefficient in the regression.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，回到数据上，我们可以看到哪些因素与结果呈负相关，哪些因素呈正相关。例如，正如预期的那样，城镇的人均犯罪率的增加与波士顿的房价呈负相关。人均犯罪率是回归分析中的第一个系数。
- en: 'You can also look at the intercept, the predicted value of the target, when
    all input variables are zero:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以查看截距，即当所有输入变量为零时目标的预测值：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you forget the names of the coefficients or intercept attributes, type `dir(lr)`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你忘记了系数或截距属性的名称，可以输入 `dir(lr)`：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For many scikit-learn predictors, parameters that consist of a word followed
    by a single `_`, such as `coef_` or `intercept_`, are of special interest. Using
    the `dir` command is a good way to check what is available within the scikit predictor
    implementation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多 scikit-learn 预测器，参数名以单词加一个下划线结尾，如 `coef_` 或 `intercept_`，这些参数特别值得关注。使用
    `dir` 命令是检查 scikit 预测器实现中可用项的好方法。
- en: How it works...
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The basic idea of linear regression is to find the set of coefficients of *v*
    that satisfy *y = Xv*, where *X* is the data matrix. It's unlikely that, for the
    given values of *X*, we will find a set of coefficients that exactly satisfy the
    equation; an error term gets added if there is an inexact specification or measurement
    error.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的基本思想是找到满足 *y = Xv* 的 *v* 系数集合，其中 *X* 是数据矩阵。对于给定的 *X* 值，通常不太可能找到一个完全满足方程的系数集合；如果存在不精确的规格或测量误差，误差项将被添加进来。
- en: Therefore, the equation becomes *y = Xv + e*, where *e* is assumed to be normally
    distributed and independent of the *X* values. Geometrically, this means that
    the error terms are perpendicular to *X*. This is beyond the scope of this book,
    but it might be worth proving E(*Xv*) = 0 for yourself.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，方程变为 *y = Xv + e*，其中 *e* 被假定为正态分布，并且与 *X* 的值独立。从几何角度来看，这意味着误差项与 *X* 垂直。虽然这超出了本书的范围，但你可能想亲自证明
    E(*Xv*) = 0。
- en: There's more...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'The `LinearRegression` object can automatically normalize (or scale) inputs:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`LinearRegression` 对象可以自动对输入进行标准化（或缩放）：'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Fitting a line through data with machine learning
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习拟合数据线
- en: 'Linear regression with machine learning involves testing the linear regression
    algorithm on unseen data. Here, we will perform 10-fold cross-validation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习的线性回归涉及在未见过的数据上测试线性回归算法。在这里，我们将进行10折交叉验证：
- en: Split the set into 10 parts
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分成10个部分
- en: Train on 9 of the parts and test on the one left over
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在9个部分上进行训练，剩下的部分用来测试
- en: Repeat this 10 times so that every part gets to be a test set once
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复这个过程10次，以便每一部分都能作为测试集一次
- en: Getting ready
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As in the previous section, load the dataset you want to apply linear regression
    to, in this case, the Boston housing dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所示，加载你想要应用线性回归的数据集，这里是波士顿住房数据集：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How to do it...
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps involved in performing linear regression are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 执行线性回归的步骤如下：
- en: 'Import the `LinearRegression` object and create an object:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `LinearRegression` 对象并创建一个实例：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Pass the independent and dependent variables to the `fit` method of `LinearRegression`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自变量和因变量传递给 `LinearRegression` 的 `fit` 方法：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, to get the 10-fold cross-validated predictions, do the following:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了获得10折交叉验证的预测结果，执行以下操作：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Looking at the residuals, the difference between the real data and the predictions,
    they are more normally distributed compared to the residuals in the previous section
    of linear regression without cross-validation:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察残差，即真实数据与预测值之间的差异，它们比前一节中没有交叉验证的线性回归的残差更接近正态分布：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following are the new residuals through cross-validation. The normal distribution
    is more symmetric than it was previously without cross-validation:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是通过交叉验证获得的新残差。与没有交叉验证时的情况相比，正态分布变得更加对称：
- en: '![](img/ff855a90-134d-47fc-8495-d3099653b95f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff855a90-134d-47fc-8495-d3099653b95f.png)'
- en: Evaluating the linear regression model
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估线性回归模型
- en: In this recipe, we'll look at how well our regression fits the underlying data.
    We fitted a regression in the last recipe, but didn't pay much attention to how
    well we actually did it. The first question after we fitted the model was clearly,
    how well does the model fit? In this recipe, we'll examine this question.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将查看我们的回归如何拟合基础数据。我们在上一节中拟合了一个回归模型，但没有太关注我们实际的拟合效果。拟合模型后，首先要问的问题显然是：模型拟合得如何？在这个步骤中，我们将深入探讨这个问题。
- en: Getting ready
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let's use the `lr` object and Boston dataset—reach back into your code from
    the *Fitting a line through data* recipe. The `lr` object will have a lot of useful
    methods now that the model has been fit.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `lr` 对象和波士顿数据集——回到你在 *数据拟合直线* 章节中的代码。现在，`lr` 对象会有很多有用的方法，因为模型已经拟合完毕。
- en: How to do it...
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Start within IPython with several imports, including `numpy`, `pandas`, and
    `matplotlib` for visualization:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 IPython 开始，导入多个库，包括 `numpy`、`pandas` 和 `matplotlib` 用于可视化：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is worth looking at a Q-Q plot. We''ll use `scipy` here because it has a
    built-in probability plot:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值得查看 Q-Q 图。我们这里使用 `scipy`，因为它有内置的概率图：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following screenshot shows the probability plot:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图展示了概率图：
- en: '![](img/7a51e9fc-0f65-42bf-84f3-dc9f5278eedc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a51e9fc-0f65-42bf-84f3-dc9f5278eedc.png)'
- en: 'Type `tuple_out[1]` and you will get the following:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入 `tuple_out[1]`，你将得到以下结果：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is a tuple of the form *(slope, intercept, r)*, where *slope* and *intercept*
    come from the least-squares fit and *r* is the square root of the coefficient
    of determination.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个形式为 *(slope, intercept, r)* 的元组，其中 *slope* 和 *intercept* 来自最小二乘拟合，而 *r*
    是决定系数的平方根。
- en: Here, the skewed values we saw earlier are a bit clearer. We can also look at
    some other metrics of the fit; **mean squared error** (**MSE**) and **mean absolute
    deviation** (**MAD**) are two common metrics. Let's define each one in Python
    and use them.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们之前看到的偏斜值变得更清晰了。我们还可以查看一些其他的拟合指标；**均方误差** (**MSE**) 和 **均值绝对偏差** (**MAD**)
    是两种常见的指标。让我们在 Python 中定义每个指标并使用它们。
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that you have seen the formulas in NumPy to get the errors, you can also
    use the `sklearn.metrics` module to get the errors quickly:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经看到了使用 NumPy 计算误差的公式，你还可以使用 `sklearn.metrics` 模块快速获取误差：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The formula for MSE is very simple:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 的公式非常简单：
- en: '![](img/aaacc3fe-3001-4375-9d72-48862ed03c4e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaacc3fe-3001-4375-9d72-48862ed03c4e.png)'
- en: 'It takes each predicted value''s deviance from the actual value, squares it,
    and then averages all the squared terms. This is actually what we optimized to
    find the best set of coefficients for linear regression. The Gauss-Markov theorem
    actually guarantees that the solution to linear regression is the best in the
    sense that the coefficients have the smallest expected squared error and are unbiased.
    In the next recipe, we''ll look at what happens when we''re OK with our coefficients
    being biased. MAD is the expected error for the absolute errors:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 它将每个预测值与实际值之间的偏差平方后再取平均，这实际上是我们优化的目标，用于找到线性回归的最佳系数。高斯-马尔科夫定理实际上保证了线性回归的解是最优的，因为系数具有最小的期望平方误差且是无偏的。在下一节中，我们将探讨当我们允许系数存在偏差时会发生什么。MAD
    是绝对误差的期望误差：
- en: '![](img/1559fcfa-4342-4a4a-bf5c-44e51a33b72d.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1559fcfa-4342-4a4a-bf5c-44e51a33b72d.png)'
- en: MAD isn't used when fitting linear regression, but it's worth taking a look
    at. Why? Think about what each one is doing and which errors are more important
    in each case. For example, with MSE, the larger errors get penalized more than
    the other terms because of the square term. Outliers have the potential to skew
    results substantially.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合线性回归时，MAD 并没有被使用，但它值得一看。为什么？想一想每个指标的作用，以及在每种情况下哪些错误更重要。例如，在 MSE 中，较大的错误会比其他项受到更多的惩罚，因为平方项的存在。离群值有可能显著地扭曲结果。
- en: There's more...
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'One thing that''s been glossed over a bit is the fact that coefficients themselves
    are random variables, therefore, they have a distribution. Let''s use bootstrapping
    to look at the distribution of the coefficient for the crime rate. Bootstrapping
    is a very common technique to get an understanding of the uncertainty of an estimate:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点被略过了，那就是系数本身是随机变量，因此它们具有分布。让我们使用自助法（bootstrapping）来查看犯罪率系数的分布。自助法是一种常见的技术，用来了解估计的不确定性：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we can look at the distribution of the coefficient:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看系数的分布：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following is the histogram that gets generated:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成的直方图：
- en: '![](img/1d793d37-edad-4092-b816-524ee5c5470d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d793d37-edad-4092-b816-524ee5c5470d.png)'
- en: 'We might also want to look at the bootstrapped confidence interval:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能需要查看自助法（bootstrapping）生成的置信区间：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is interesting; there's actually reason to believe that the crime rate
    might not have an impact on home prices. Notice how zero is within the **confidence
    interval** (**CI**) between -0.18 and 0.03, which means that it may not play a
    role. It's also worth pointing out that bootstrapping can potentially lead to
    better estimations for coefficients, because the bootstrapped mean with convergence
    to the true mean is faster than finding the coefficient using regular estimation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣；事实上，有理由相信犯罪率可能对房价没有影响。注意零值位于**置信区间**（**CI**）-0.18到0.03之间，这意味着它可能不起作用。还值得指出的是，自助法可能会带来更好的系数估计，因为自助法的均值收敛速度比使用常规估计方法找出系数要快。
- en: Using ridge regression to overcome linear regression's shortfalls
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用岭回归克服线性回归的不足
- en: In this recipe, we'll learn about ridge regression. It is different from vanilla
    linear regression; it introduces a regularization parameter to shrink coefficients.
    This is useful when the dataset has collinear factors.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习岭回归。它与普通的线性回归不同；它引入了一个正则化参数来收缩系数。当数据集包含共线性因素时，这非常有用。
- en: 'Ridge regression is actually so powerful in the presence of collinearity that
    you can model polynomial features: vectors *x*, *x*²,* x*³, ... which are highly
    collinear and correlated.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归在共线性存在的情况下非常强大，甚至可以建模多项式特征：向量 *x*、*x*²、*x*³，……这些特征高度共线且相关。
- en: Getting ready
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: Let's load a dataset that has a low effective rank and compare ridge regression
    with linear regression by way of the coefficients. If you're not familiar with
    rank, it's the smaller of the linearly independent columns and the linearly independent
    rows. One of the assumptions of linear regression is that the data matrix is full
    rank.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载一个有效秩较低的数据集，并通过系数来比较岭回归和线性回归。如果你不熟悉秩，它是线性独立列和线性独立行中的较小者。线性回归的一个假设是数据矩阵是满秩的。
- en: How to do it...
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'First, use `make_regression` to create a simple dataset with three predictors,
    but an `effective_rank` of `2`. Effective rank means that, although technically
    the matrix is full rank, many of the columns have a high degree of collinearity:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`make_regression`创建一个包含三个预测变量的简单数据集，但其`effective_rank`为`2`。有效秩意味着，尽管矩阵在技术上是满秩的，但许多列之间存在高度的共线性：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'First, let''s take a look at regular linear regression with bootstrapping from
    the previous chapter:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下上一章使用自助法的常规线性回归：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Visualize the coefficients:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化系数：
- en: '![](img/33cc9289-5560-49b7-b2d5-be71519cf3aa.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33cc9289-5560-49b7-b2d5-be71519cf3aa.png)'
- en: 'Perform the same procedure with ridge regression:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用岭回归执行相同的步骤：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Visualize the results:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化结果：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/a70ee4ac-6a94-41f1-8f06-83a51681dbba.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a70ee4ac-6a94-41f1-8f06-83a51681dbba.png)'
- en: Don't let the similar width of the plots fool you; the coefficients for ridge
    regression are much closer to zero.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 别被图表的相似宽度欺骗；岭回归的系数要接近零。
- en: 'Let''s look at the average spread between the coefficients:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看一下系数之间的平均差异：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: So, on average, the coefficients for linear regression are much higher than
    the ridge regression coefficients. This difference is the bias in the coefficients
    (forgetting, for a second, the potential bias of the linear regression coefficients).
    So then, what is the advantage of ridge regression?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，平均而言，线性回归的系数要远高于岭回归的系数。这种差异是系数中的偏差（暂时忽略线性回归系数可能存在的偏差）。那么，岭回归的优势是什么呢？
- en: 'Well, let''s look at the variance of our coefficients:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好吧，来看一下我们系数的方差：
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The variance has been dramatically reduced. This is the bias-variance trade-off
    that is so often discussed in machine learning. The next recipe will introduce
    how to tune the regularization parameter in ridge regression, which is at the
    heart of this trade-off.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 方差已经显著降低。这就是机器学习中常常讨论的偏差-方差权衡。接下来的内容将介绍如何调整岭回归中的正则化参数，这是这一权衡的核心。
- en: Optimizing the ridge regression parameter
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化岭回归参数
- en: Once you start using ridge regression to make predictions or learn about relationships
    in the system you're modeling, you'll start thinking about the choice of alpha.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你开始使用岭回归进行预测或了解你正在建模的系统中的关系，你就会开始考虑alpha的选择。
- en: For example, using ordinary least squares (**OLS**) regression might show a
    relationship between two variables; however, when regularized by an alpha, the
    relationship is no longer significant. This can be a matter of whether a decision
    needs to be taken.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用普通最小二乘 (**OLS**) 回归可能会显示两个变量之间的关系；然而，当通过 alpha 进行正则化时，这种关系不再显著。这可能是一个是否需要做出决策的问题。
- en: Getting ready
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: 'Through cross-validation, we will tune the alpha parameter of ridge regression.
    If you remember, in ridge regression, the gamma parameter is typically represented
    as alpha in scikit-learn when calling `RidgeRegression`; so, the question that
    arises is what is the best alpha? Create a regression dataset, and then let''s
    get started:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过交叉验证，我们将调整岭回归的 alpha 参数。如果你还记得，在岭回归中，gamma 参数通常在调用 `RidgeRegression` 时被表示为
    alpha，因此，出现的问题是，最优的 alpha 值是什么？创建一个回归数据集，然后我们开始吧：
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How to do it...
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: In the `linear_models` module, there is an object called `RidgeCV`, which stands
    for ridge cross-validation. This performs a cross-validation similar to **leave-one-out
    cross-validation** (**LOOCV**).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `linear_models` 模块中，有一个叫做 `RidgeCV` 的对象，代表岭回归交叉验证。它执行的交叉验证类似于 **留一交叉验证** (**LOOCV**)。
- en: 'Under the hood, it''s going to train the model for all samples except one.
    It''ll then evaluate the error in predicting this one test case:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在背后，它将为除了一个样本之外的所有样本训练模型。然后它会评估预测这个测试样本的误差：
- en: '[PRE29]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After we fit the regression, the alpha attribute will be the best alpha choice:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们拟合回归之后，alpha 属性将是最佳的 alpha 选择：
- en: '[PRE30]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the previous example, it was the first choice. We might want to hone in
    on something around `.1`:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在之前的例子中，它是第一个选择。我们可能想要集中关注 `.1` 附近的值：
- en: '[PRE31]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We could continue this hunt, but hopefully, the mechanics are clear.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续这个探索，但希望这些机制已经很清晰了。
- en: How it works...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The mechanics might be clear, but we should talk a little more about why and
    define what was meant by best. At each step in the cross-validation process, the
    model scores an error against the test sample. By default, it's essentially a
    squared error.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制可能很清楚，但我们应该再谈谈为什么，并定义最优值的含义。在交叉验证的每个步骤中，模型会对测试样本进行误差评分。默认情况下，本质上是平方误差。
- en: 'We can force the `RidgeCV` object to store the cross-validation values; this
    will let us visualize what it''s doing:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以强制 `RidgeCV` 对象存储交叉验证值；这将帮助我们可视化它所做的工作：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see, we test a bunch of points (50 in total) between `0.01` and
    `1`. Since we passed `store_cv_values` as `True`, we can access these values:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们测试了从 `0.01` 到 `1` 的一堆点（共 50 个）。由于我们将 `store_cv_values` 设置为 `True`，我们可以访问这些值：
- en: '[PRE33]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'So, we had 100 values in the initial regression and tested 50 different alpha
    values. We now have access to the errors of all 50 values. So, we can now find
    the smallest mean error and choose it as alpha:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在初始回归中我们有 100 个值，并测试了 50 个不同的 alpha 值。现在我们可以访问所有 50 个值的误差。因此，我们可以找到最小的均值误差，并选择它作为
    alpha：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This matches the best value found by the `rcv3` instance of the class `RidgeCV`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 `RidgeCV` 类的 `rcv3` 实例找到的最佳值一致：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It''s also worthwhile visualizing what''s going on. In order to do that, we''ll
    plot the mean for all 50 test alphas:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 也值得可视化正在发生的事情。为此，我们将绘制所有 50 个测试 alpha 的均值：
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/0cb03c53-80ff-4cb2-96e8-6babd594555e.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cb03c53-80ff-4cb2-96e8-6babd594555e.png)'
- en: There's more...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'If we want to use our own scoring function, we can do that as well. Since we
    looked up MAD before, let''s use it to score the differences. First, we need to
    define our loss function. We will import it from `sklearn.metrics`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用自己的评分函数，也可以这么做。由于我们之前查找了 MAD，我们就用它来评分差异。首先，我们需要定义我们的损失函数。我们将从 `sklearn.metrics`
    导入它：
- en: '[PRE37]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After we define the loss function, we can employ the `make_scorer` function
    in `sklearn`. This will take care of standardizing our function so that scikit''s
    objects know how to use it. Also, because this is a loss function and not a score
    function, the lower the better, and thus the need to let `sklearn` flip the sign
    to turn this from a maximization problem into a minimization problem:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义损失函数后，我们可以使用 `sklearn` 中的 `make_scorer` 函数。这将确保我们的函数被标准化，从而让 scikit 的对象知道如何使用它。此外，因为这是一个损失函数而不是评分函数，所以越低越好，因此需要让
    `sklearn` 翻转符号，将其从最大化问题转变为最小化问题：
- en: '[PRE38]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Continue as before to find the minimum negative MAD score:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 按照之前的方式继续寻找最小的负 MAD 分数：
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Look at the lowest score:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 查看最低的得分：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'It occurs at the alpha of `0.01`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它发生在 alpha 为 `0.01` 时：
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Bayesian ridge regression
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯岭回归
- en: Additionally, scikit-learn contains Bayesian ridge regression, which allows
    for easy estimates of confidence intervals. (Note that obtaining the following Bayesian
    ridge confidence intervals specifically requires scikit-learn Version 0.19.0 or
    higher.)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，scikit-learn还包含贝叶斯岭回归，它允许轻松估计置信区间。（注意，获取以下贝叶斯岭回归置信区间特别需要scikit-learn 0.19.0或更高版本。）
- en: 'Create a line with a slope of `3` and no intercept for simplicity:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一条斜率为`3`且没有截距的直线，简化起见：
- en: '[PRE42]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Import, instantiate, and fit the Bayesian ridge model. Note that the one-dimensional
    `X` and `y` variables have to be reshaped:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 导入、实例化并拟合贝叶斯岭回归模型。请注意，一维的`X`和`y`变量必须进行重塑：
- en: '[PRE43]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Write the following to get the error estimates on the regularized linear regression:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 编写以下代码以获取正则化线性回归的误差估计：
- en: '[PRE44]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Plot the results. The noisy data is the blue dots and the green lines approximate
    it:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制结果。噪声数据是蓝色的点，绿色的线条大致拟合它：
- en: '[PRE45]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/260b30a5-7c91-4a82-b2ef-d503cc670c5c.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/260b30a5-7c91-4a82-b2ef-d503cc670c5c.png)'
- en: As a final aside on Bayesian ridge, you can perform hyperparameter optimization
    on the parameters `alpha_1`, `alpha_2`, `lambda_1`, and `lambda_2` using a cross-validated
    grid search.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于贝叶斯岭回归的补充说明，你可以通过交叉验证网格搜索对参数`alpha_1`、`alpha_2`、`lambda_1`和`lambda_2`进行超参数优化。
- en: Using sparsity to regularize models
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用稀疏性来正则化模型
- en: The **least absolute shrinkage and selection operator** (**LASSO**) method is
    very similar to ridge regression and **l****east angle regression** (**LARS**).
    It's similar to ridge regression in the sense that we penalize our regression
    by an amount, and it's similar to LARS in that it can be used as a parameter selection,
    typically leading to a sparse vector of coefficients. Both LASSO and LARS get
    rid of a lot of the features of the dataset, which is something you might or might
    not want to do depending on the dataset and how you apply it. (Ridge regression,
    on the other hand, preserves all features, which allows you to model polynomial
    functions or complex functions with correlated features.)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小绝对收缩与选择算子**（**LASSO**）方法与岭回归非常相似，也与**最小角回归**（**LARS**）类似。它与岭回归的相似之处在于我们通过一定的惩罚量来惩罚回归，而与LARS的相似之处在于它可以作为参数选择，通常会导致一个稀疏的系数向量。LASSO和LARS都可以去除数据集中的许多特征，这取决于数据集的特点以及如何应用这些方法，这可能是你想要的，也可能不是。（而岭回归则保留所有特征，这使得你可以用来建模多项式函数或包含相关特征的复杂函数。）'
- en: Getting ready
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正在准备
- en: To be clear, LASSO regression is not a panacea. There can be computation consequences
    to using LASSO regression. As we'll see in this recipe, we'll use a loss function
    that isn't differential, and therefore requires special, and more importantly,
    performance-impairing workarounds.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，LASSO回归并非万能。使用LASSO回归可能会带来计算上的后果。正如我们在本配方中看到的，我们将使用一个不可微的损失函数，因此需要特定的、更重要的是、影响性能的解决方法。
- en: How to do it...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps involved in performing LASSO regression are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 执行LASSO回归的步骤如下：
- en: 'Let''s go back to the trusty `make_regression` function and create a dataset
    with the same parameters:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们回到可靠的`make_regression`函数，并使用相同的参数创建一个数据集：
- en: '[PRE46]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we need to import the `Lasso` object:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要导入`Lasso`对象：
- en: '[PRE47]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'LASSO contains many parameters, but the most interesting parameter is `alpha`.
    It scales the penalization term of the `Lasso` method. For now, leave it as one.
    As an aside, and much like ridge regression, if this term is zero, LASSO is equivalent
    to linear regression:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LASSO包含许多参数，但最有趣的参数是`alpha`。它用于调整`Lasso`方法的惩罚项。目前，保持它为1。顺便说一下，就像岭回归一样，如果这个项为零，LASSO就等同于线性回归：
- en: '[PRE48]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Again, let''s see how many of the coefficients remain nonzero:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次查看有多少系数保持非零：
- en: '[PRE49]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: None of our coefficients turn out to be zero, which is what we expected. Actually,
    if you run this, you might get a warning from scikit-learn that advises you to
    choose `LinearRegression`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系数没有一个变为零，这正是我们预期的。实际上，如果你运行这个，你可能会收到来自scikit-learn的警告，建议你选择`LinearRegression`。
- en: How it works...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: LASSO cross-validation – LASSOCV
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LASSO交叉验证 – LASSOCV
- en: 'Choosing the most appropriate lambda is a critical problem. We can specify
    the lambda ourselves or use cross-validation to find the best choice given the
    data at hand:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最合适的lambda是一个关键问题。我们可以自己指定lambda，或者使用交叉验证根据现有数据找到最佳选择：
- en: '[PRE50]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The LASSOCV will have, as an attribute, the most appropriate lambda. scikit-learn
    mostly uses alpha in its notation, but the literature uses lambda:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: LASSOCV将具有作为属性的最合适的lambda。scikit-learn在其符号中大多使用alpha，但文献中使用lambda：
- en: '[PRE51]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The number of coefficients can be accessed in the regular manner:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 系数的数量可以通过常规方式访问：
- en: '[PRE52]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Letting LASSOCV choose the appropriate best fit leaves us with `15` nonzero
    coefficients:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让LASSOCV选择最合适的最佳拟合，我们得到`15`个非零系数：
- en: '[PRE53]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: LASSO for feature selection
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LASSO用于特征选择
- en: LASSO can often be used for feature selection for other methods. For example,
    you might run LASSO regression to get the appropriate number of features, and
    then use those features in another algorithm.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO通常可以用于其他方法的特征选择。例如，您可以运行LASSO回归来获取合适数量的特征，然后在其他算法中使用这些特征。
- en: 'To get the features we want, create a masking array based on the columns that
    aren''t zero, and then filter out the nonzero columns to keep the features we
    want:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取我们想要的特征，创建一个基于非零列的掩码数组，然后过滤掉非零列，保留我们需要的特征：
- en: '[PRE54]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Taking a more fundamental approach to regularization with LARS
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用更基本的方法进行LARS的正则化
- en: To borrow from Gilbert Strang's evaluation of the Gaussian elimination, LARS
    is an idea you probably would've considered eventually had it not already been
    discovered by Efron, Hastie, Johnstone, and Tibshirani in their work [1].
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 借用Gilbert Strang对高斯消元法的评估，LARS是一个你可能最终会考虑的想法，除非它已经在Efron、Hastie、Johnstone和Tibshirani的工作中被发现[1]。
- en: Getting ready
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: LARS is a regression technique that is well suited to high-dimensional problems,
    that is, *p >> n*, where *p* denotes the columns or features and *n* is the number
    of samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LARS是一种回归技术，非常适合高维问题，即*p >> n*，其中*p*表示列或特征，*n*是样本的数量。
- en: How to do it...
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'First, import the necessary objects. The data we use will have 200 data points
    and 500 features. We''ll also choose low noise and a small number of informative
    features:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入必要的对象。我们使用的数据将包含200个数据点和500个特征。我们还将选择低噪声和少量的信息性特征：
- en: '[PRE55]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Since we used 10 informative features, let''s also specify that we want 10
    nonzero coefficients in LARS. We will probably not know the exact number of informative
    features beforehand, but it''s useful for learning purposes:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们使用了10个信息性特征，让我们也指定希望LARS中有10个非零系数。我们可能事先无法确切知道信息性特征的数量，但这对学习来说是有帮助的：
- en: '[PRE56]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We can then verify that LARS returns the correct number of nonzero coefficients:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以验证LARS返回正确数量的非零系数：
- en: '[PRE57]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The question then is why it is more useful to use a smaller number of features.
    To illustrate this, let''s hold out half of the data and train two LARS models,
    one with 12 nonzero coefficients and another with no predetermined amount. We
    use 12 here because we might have an idea of the number of important features,
    but we might not be sure of the exact number:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题是为什么使用更少的特征更有用。为了说明这一点，让我们保留一半的数据并训练两个LARS模型，一个具有12个非零系数，另一个没有预定数量。我们在这里使用12是因为我们可能对重要特征的数量有所了解，但我们可能不确定确切的数量：
- en: '[PRE58]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, to see how well each feature fits the unknown data, do the following:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了查看每个特征如何拟合未知数据，请执行以下操作：
- en: '[PRE59]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Look again if you missed it; the error on the test set was clearly very high.
    Herein lies the problem with high-dimensional datasets; given a large number of
    features, it's typically not too difficult to get a model of good fit on the train
    sample, but overfitting becomes a huge problem.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你错过了，再看看；测试集上的误差显然非常高。这就是高维数据集的问题所在；给定大量特征，通常不难在训练样本上得到一个拟合良好的模型，但过拟合成为了一个巨大的问题。
- en: How it works...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: LARS works by iteratively choosing features that are correlated with the residuals.
    Geometrically, correlation is effectively the least angle between the feature
    and the residuals; this is how LARS got its name.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: LARS通过反复选择与残差相关的特征来工作。在几何上，相关性实际上是特征和残差之间的最小角度；这也是LARS得名的原因。
- en: 'After choosing the first feature, LARS will continue to move in the least angle
    direction until a different feature has the same amount of correlation with the
    residuals. Then, LARS will begin to move in the combined direction of both features.
    To visualize this, consider the following graph:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 选择第一个特征后，LARS将继续沿最小角度方向移动，直到另一个特征与残差的相关性达到相同的程度。然后，LARS将开始沿这两个特征的组合方向移动。为了直观地理解这一点，考虑以下图表：
- en: '![](img/8ff7d0c4-56ec-4028-b183-b1db7a6f4f9a.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ff7d0c4-56ec-4028-b183-b1db7a6f4f9a.png)'
- en: So, we move along **x1** until we get to the point where the pull on **x1**
    by **y** is the same as the pull on **x2** by **y**. When this occurs, we move
    along the path that is equal to the angle between **x1** and **x2** divided by
    two.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们沿着**x1**移动，直到**x1**受到**y**的拉力与**x2**受到**y**的拉力相等。发生这种情况时，我们沿着一条路径移动，这条路径的角度等于**x1**和**x2**之间的角度除以二。
- en: There's more...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Much in the same way as we used cross-validation to tune ridge regression,
    we can do the same with LARS:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们使用交叉验证来调整岭回归一样，我们也可以对LARS做同样的操作：
- en: '[PRE60]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Using cross-validation will help us to determine the best number of nonzero
    coefficients to use. Here, it turns out to be as shown:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证可以帮助我们确定使用的非零系数的最佳数量。这里，结果如下：
- en: '[PRE61]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: References
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani, *Least
    angle regression*, The Annals of Statistics 32(2) 2004: pp. 407–499, doi:10.1214/009053604000000067,
    MR2060166.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bradley Efron, Trevor Hastie, Iain Johnstone, 和 Robert Tibshirani, *最小角回归*,
    《统计年鉴》32(2) 2004: 第407–499页, doi:10.1214/009053604000000067, MR2060166。'
