- en: Essential Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础知识
- en: 'So far, in previous chapters, we went through the various ETL processes available
    in GCP. In this chapter, we will start our journey of machine learning and deep
    learning through the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在之前的章节中，我们介绍了 GCP 中可用的各种 ETL 流程。在本章中，我们将通过以下主题开始我们的机器学习和深度学习之旅：
- en: Applications of machine learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习应用
- en: Supervised and unsupervised machine learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习
- en: Overview of major machine learning techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要机器学习技术的概述
- en: Data splitting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据拆分
- en: Measuring the accuracy of a model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量模型的准确性
- en: The difference between machine learning and deep learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和深度学习之间的区别
- en: Applications of deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习应用
- en: Applications of machine learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习应用
- en: Machine learning encompasses a set of techniques that learn from historical
    data. Based on the patterns learned from historical data, the machine learning
    technique predicts the probability of an event happening on a future dataset.
    Given the way in which machine learning works, there are multiple applications
    of the set of techniques. Let's explore some of them in the following sections.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习涵盖了一系列从历史数据中学习的技巧。基于从历史数据中学习到的模式，机器学习技术预测未来数据集中事件发生的概率。鉴于机器学习的工作方式，这一系列技巧有多个应用。以下几节中，我们将探讨其中的一些。
- en: Financial services
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 金融服务业
- en: 'Some applications in the field of finance are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在金融领域的某些应用如下：
- en: Identifying the riskiness of a loan/credit card applicant
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别贷款/信用卡申请人的风险程度
- en: Estimating the credit limit of a given customer
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计给定客户的信用额度
- en: Predicting whether a card transaction is a fraudulent transaction
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测卡交易是否为欺诈交易
- en: Identifying the customer segments that need to be targeted for a campaign
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别需要针对活动进行定位的客户细分市场
- en: Predicting whether a customer is likely to default in the next few months
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测客户在接下来的几个月内可能违约的可能性
- en: Recommending the right financial product that a customer should buy
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐客户应该购买的正确金融产品
- en: Retail industry
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零售行业
- en: 'The following are some applications of the different techniques of machine
    learning in the retail industry:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些机器学习不同技巧在零售行业中的应用：
- en: Predicting the next product that a customer is likely to buy
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测客户可能购买的下一种产品
- en: Estimating the optimal price point for a given product
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计给定产品的最佳价格点
- en: Forecasting the number of units a product will sell over time
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测产品随时间推移将销售的单元数量
- en: Targeting customers by bundling products for promotion
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过捆绑产品进行促销以定位客户
- en: Estimating a customer lifetime value
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计客户终身价值
- en: Telecom industry
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电信行业
- en: 'Here are a few applications of machine learning in the telecom industry:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是机器学习在电信行业中的几个应用：
- en: Predicting the likelihood of a call drop before the start of a call
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测在通话开始前通话掉话的可能性
- en: Predicting if a customer is likely to churn in the next few months
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测客户在接下来的几个月内可能流失的可能性
- en: Identifying add-ons to monthly usage that could be sold to a customer
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别可以出售给客户的附加月度使用量
- en: Identifying the customers who are less likely to pay for postpaid services
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别不太可能为后付费服务付款的客户
- en: Workforce optimization for field force effectiveness
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为现场销售人员优化劳动力
- en: Supervised and unsupervised machine learning
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习
- en: Supervised machine learning constitutes the set of techniques that work towards
    building a model that approximate a function. The function takes a set of input
    variables, which are alternatively called independent variables, and tries to
    map the input variables to the output variable, alternatively called the dependent
    variable or the label.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习构成了旨在构建一个近似函数的模型的一组技巧。该函数接受一组输入变量，这些变量也被称为独立变量，并试图将输入变量映射到输出变量，这些变量也被称为依赖变量或标签。
- en: Given that we know the label (or the value) we are trying to predict, for a
    set of input variables, the technique becomes a supervised learning problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道我们正在尝试预测的标签（或值），对于一组输入变量，该技术变成了一个监督学习问题。
- en: In a similar manner, in an unsupervised learning problem, we do not have the
    output variable that we have to predict. However, in unsupervised learning, we
    try to group the data points so that they form logical groups.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，在无监督学习问题中，我们没有必须预测的输出变量。然而，在无监督学习中，我们试图将数据点分组，以便它们形成逻辑组。
- en: 'A distinction between supervised and unsupervised learning at a high level
    can be obtained as shown in the following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下图示获得监督学习和无监督学习的高层次区别：
- en: '![](img/7c8d6266-8c0c-45ae-b8c1-2efa36c973b2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c8d6266-8c0c-45ae-b8c1-2efa36c973b2.png)'
- en: 'In the preceding diagram, the supervised learning approach can distinguish
    between the two classes, as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，监督学习方法可以区分两个类别，如下所示：
- en: '![](img/ac66330c-2d9a-48a1-8038-60bad3a57ace.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ac66330c-2d9a-48a1-8038-60bad3a57ace.png)'
- en: 'In supervised learning, there are two major objectives that can be achieved:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，有两个主要目标可以实现：
- en: Predict the probability of an event happening—classification
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测事件发生的概率——分类
- en: Estimate the value of the continuous dependent variable—regression
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计连续因变量的值——回归
- en: 'The major methods that can help in classification are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助分类的主要方法如下：
- en: Logistic regression
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Decision tree
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random forest
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Gradient boosting
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Neural network
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Along with these (except logistic regression), linear regression also helps
    in estimating a continuous variable (regression).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了逻辑回归之外，线性回归也有助于估计连续变量（回归）。
- en: 'While these techniques help in estimating a continuous variable or in predicting
    the probability of an event happening (discrete variable prediction), unsupervised
    learning helps in grouping. Grouping can be either of rows (which is a typical
    clustering technique) or of columns (a dimensionality reduction technique). The
    major methods of row groupings are:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些技术有助于估计连续变量或预测事件发生的概率（离散变量预测），但无监督学习有助于分组。分组可以是行（这是一种典型的聚类技术）或列（一种降维技术）。行分组的主要方法包括：
- en: K-means clustering
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类
- en: Hierarchical clustering
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Density-based clustering
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于密度的聚类
- en: 'The major methods of column groupings are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列分组的主要方法包括：
- en: Principal component analysis
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
- en: Row groupings result in identifying the segments of customers (observations)
    that are there in our dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列分组可以识别出数据集中存在的客户（观测值）的段。
- en: Column groupings result in reducing the number of columns. This comes in handy
    when the number of independent variables is high. Typically when this is the case,
    there could be an issue in building the model, as the number of weights that need
    to be estimated could be high. Also, there could be an issue in interpreting the
    model, as some of the independent variables could be highly correlated with each
    other. Principal component analysis or t-SNE comes in handy in such a scenario,
    where we reduce the number of independent variables without losing too much of
    the information that is present in the dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列分组可以减少列的数量。当独立变量的数量很高时，这一点很有用。通常情况下，如果出现这种情况，构建模型可能会遇到问题，因为需要估计的权重数量可能很高。此外，在解释模型时也可能出现问题，因为一些独立变量之间可能高度相关。在这种情况下，主成分分析或t-SNE很有用，因为我们可以在不丢失太多数据集中现有信息的情况下减少独立变量的数量。
- en: In the next section, we will go through an overview of all the major machine
    learning algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将概述所有主要的机器学习算法。
- en: Overview of machine learning techniques
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习技术概述
- en: Before going through an overview of the major machine learning techniques, let's
    go through the function that we would want to optimize in a regression technique
    or a classification technique.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解主要机器学习技术概述之前，让我们先看看在回归技术或分类技术中我们想要优化的函数。
- en: Objective function in regression
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归中的目标函数
- en: In a regression exercise, we estimate the continuous variable value. In such
    a scenario, our predictions can be lower than the actual value or higher; that
    is, the error value could be either positive or negative. In such a scenario,
    the objective function translates to minimizing the sum of squared values of the
    difference between the actual and predicted values of each of the observations
    in the dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归练习中，我们估计连续变量的值。在这种情况下，我们的预测可能低于实际值或高于实际值；也就是说，误差值可以是正的也可以是负的。在这种情况下，目标函数转化为最小化数据集中每个观测值实际值和预测值之间差异的平方和。
- en: 'In mathematical terms, the preceding is written as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学术语来说，前面的内容可以写成如下形式：
- en: '![](img/42d4aba6-3c58-462f-a211-fb58ffc73338.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/42d4aba6-3c58-462f-a211-fb58ffc73338.png)'
- en: 'In the given equation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中：
- en: '*SSE* stands for the *sum of squared errors*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SSE* 代表**平方误差和**'
- en: '*y* refers to the actual value of the dependent variable'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 指的是因变量的实际值'
- en: '*y''* refers to the estimated value of the dependent variable'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y''* 指的是因变量的估计值'
- en: ∑ refers to the summation of the squared errors across all the observations
    in the dataset
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∑ 表示数据集中所有观测值的平方误差之和
- en: Given the objective function, let's understand how linear regression works at
    a high level.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 给定目标函数，让我们从高层次了解线性回归是如何工作的。
- en: Linear regression
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'In linear regression, we assume a linear relationship between the independent
    variables and the dependent variable. Linear regression is represented as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们假设自变量和因变量之间存在线性关系。线性回归表示如下：
- en: '![](img/7c3f054d-cbcf-4906-9e71-32069d48bac5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c3f054d-cbcf-4906-9e71-32069d48bac5.png)'
- en: 'In the given equation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中：
- en: '*Y* is the dependent variable'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y* 是因变量'
- en: '*W* is the weight associated with the independent variable *X*'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W* 是与自变量 *X* 相关的权重'
- en: '*b* is the intercept value'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 是截距值'
- en: 'If there are multiple independent variables (let''s say two independent variables,
    *x1* and *x2*), the equation is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有多个自变量（比如说两个自变量，*x1* 和 *x2*），方程如下：
- en: '![](img/e42934ac-8357-4e22-b5c4-380b299582dc.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e42934ac-8357-4e22-b5c4-380b299582dc.png)'
- en: 'In the given equation:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中：
- en: '*w1* is the weight associated with variable *x1*'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w1* 是与变量 *x1* 相关的权重'
- en: '*w2* is the weight associated with variable *x2*'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w2* 是与变量 *x2* 相关的权重'
- en: 'A typical linear regression looks as follows, where the *x* axis is the independent
    variable and the *y* axis is the dependent variable:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的线性回归看起来如下，其中 *x* 轴是自变量，*y* 轴是因变量：
- en: '![](img/8eb8d1b7-4303-4ee3-88d2-5b99a27c004b.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8eb8d1b7-4303-4ee3-88d2-5b99a27c004b.png)'
- en: The straight line (with a certain slope and intercept) is the equation of linear
    regression.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 直线（具有特定的斜率和截距）是线性回归的方程。
- en: Note that the line in the graph is the one that minimizes the overall squared
    error.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意图中线是使整体平方误差最小的线。
- en: Decision tree
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision tree is a technique that helps us in deriving rules from data. A rule-based
    technique is very helpful in explaining how the model is supposed to work in estimating
    a dependent variable value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种帮助我们从数据中推导规则的技巧。基于规则的技巧在解释模型如何估计因变量值时非常有帮助。
- en: 'A typical decision tree looks like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的决策树如下所示：
- en: '![](img/12b92c36-714f-4dfe-a7de-4c0b3dd14e52.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12b92c36-714f-4dfe-a7de-4c0b3dd14e52.png)'
- en: 'The preceding diagram is explained as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示解释如下：
- en: '**ROOT Node:** This represents the entire population or a sample, and it is
    further divided into two or more further nodes.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根节点**：这代表整个总体或样本，并进一步分割成两个或更多的节点。'
- en: '**Splitting**: A process of dividing a node into two or more subnodes based
    on a certain rule.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割**：根据一定规则将节点分割成两个或更多子节点的过程。'
- en: '**Decision Node:** When a subnode splits into further subnodes, it is called
    **decision node.**'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策节点**：当一个子节点进一步分割成子节点时，它被称为**决策节点**。'
- en: '**Leaf/Terminal Node:** The final node in a decision tree is a leaf or terminal
    node.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叶节点/终端节点**：决策树中的最后一个节点是叶节点或终端节点。'
- en: '**Pruning:** When we remove the subnodes of a decision node, this process is
    called **pruning**. You can say it is the opposite process of splitting.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**：当我们移除决策节点的子节点时，这个过程称为**剪枝**。可以说它是分割过程的相反过程。'
- en: '**Branch/Sub-Tree:** A subsection of the entire tree is called a **branch**
    or a **sub-tree**.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分支/子树**：整个树的子部分称为**分支**或**子树**。'
- en: '**Parent and child node:** A node that is divided into subnodes is called the
    **parent node** of subnodes, whereas the subnodes are the children of the parent
    node.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**父节点和子节点**：被分割成子节点的节点称为子节点的**父节点**，而子节点是父节点的子节点。'
- en: 'Given a dependent variable and an independent variable value, we will go through
    how a decision tree works using the following dataset:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个因变量和一个自变量值，我们将通过以下数据集了解决策树是如何工作的：
- en: '| **var2** | **response** |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **var2** | **response** |'
- en: '| `0.1` | `1996` |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `0.1` | `1996` |'
- en: '| `0.3` | `839` |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `0.3` | `839` |'
- en: '| `0.44` | `2229` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `0.44` | `2229` |'
- en: '| `0.51` | `2309` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `0.51` | `2309` |'
- en: '| `0.75` | `815` |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `0.75` | `815` |'
- en: '| `0.78` | `2295` |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `0.78` | `2295` |'
- en: '| `0.84` | `1590` |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `0.84` | `1590` |'
- en: In the preceding dataset, the variable `var2` is the input variable and the
    `response` variable is the dependent variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据集中，变量 `var2` 是输入变量，而 `response` 变量是因变量。
- en: In the first step of the decision tree, we sort the input variable from lowest
    to highest and test multiple rules, one at a time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树的第一个步骤中，我们将输入变量从低到高排序，并逐一测试多个规则。
- en: In the first instance, all the observations of the dataset that have a `var2`
    value of less than `0.3` belong to the left node of a decision tree, and the other
    observations belong to the right node of the decision tree.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，所有`var2`值小于`0.3`的观测值属于决策树的左节点，其他观测值属于决策树的右节点。
- en: In a regression exercise, the predicted value of the left node is the average
    of the `response` variable for all the observations that belong to the left node.
    Similarly, the predicted value of the right node is the average of `response`
    for all the observations that belong to the right node.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归练习中，左节点的预测值是所有属于左节点的观测值的`response`变量的平均值。同样，右节点的预测值是所有属于右节点的观测值的`response`的平均值。
- en: Given a predicted value for the left node and a different predicted value for
    the observations that belong to the right node, the squared error can be calculated
    for each of the left and right nodes. The overall error for a probable rule is
    the sum of squared error in both left and right nodes.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 给定左节点的预测值和属于右节点的观测值的不同的预测值，可以计算每个左节点和右节点的平方误差。一个可能规则的总体误差是左右节点平方误差的总和。
- en: The decision rule that is implemented is the rule that has the minimum squared
    error among all the possible rules.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的决策规则是在所有可能的规则中具有最小平方误差的规则。
- en: Random forest
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random forest is an extension of decision trees. It is a forest as it is a combination
    of multiple trees, and is random as we randomly sample different observations
    for each of the decision trees.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是决策树的扩展。它是一个森林，因为它是由多个树组合而成的，并且是随机的，因为我们为每个决策树随机采样不同的观测值。
- en: A random forest works by averaging the prediction of each of the decision trees
    (which work on a sample of the original dataset).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过平均每个决策树的预测值（这些决策树在原始数据集的样本上工作）来工作。
- en: Typically, a random forest works better than a single decision tree, as the
    influence of outliers is reduced in it (because in some samples, outliers might
    not have occurred), whereas, in a decision tree, an outlier would have definitely
    occurred (if the original dataset contained an outlier).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，随机森林比单个决策树表现更好，因为在其中异常值的影响被减少了（因为在某些样本中，异常值可能没有出现），而在决策树中，异常值肯定会出现（如果原始数据集中包含异常值）。
- en: Gradient boosting
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升
- en: While a random forest works in a framework where multiple parallel trees are
    built, gradient boosting takes a different approach—building a deep framework.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当随机森林在一个构建多个并行树的框架中工作时，梯度提升采取了一种不同的方法——构建一个深度框架。
- en: The gradient in gradient boosting refers to the difference between actual and
    predicted values, and boosting refers to improvement, that is, improving the error
    over different iterations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升中的梯度指的是实际值和预测值之间的差异，而提升指的是改进，即在不同迭代中改进误差。
- en: 'Gradient boosting also leverages the way in which decision trees work in the
    following way:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升也利用了决策树工作的以下方式：
- en: Build a decision tree to estimate the dependent variable
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建决策树以估计因变量
- en: Calculate the error, that is, the difference between actual and predicted value
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算误差，即实际值和预测值之间的差异
- en: Build another decision tree that predicts the error
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建另一个预测误差的决策树
- en: Update the prediction by taking the prediction of error of the previous decision
    tree into account
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过考虑前一个决策树的预测误差来更新预测
- en: This way, gradient boosting continuously builds a decision tree that predicts
    the error of the previous decision tree and thus a depth-based framework in gradient
    boosting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，梯度提升持续构建一个预测前一个决策树误差的决策树，从而在梯度提升中构建了一个基于深度的框架。
- en: Neural network
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: A neural network provides a way to approximate nonlinear functions. Nonlinearity
    is achieved by applying activation functions on top of the summation of weighted
    input variables.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络提供了一种近似非线性函数的方法。通过在加权输入变量的和上应用激活函数来实现非线性。
- en: 'A neural network looks like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络看起来是这样的：
- en: '![](img/046bc137-9809-4623-95c1-3c16eda6b6d3.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/046bc137-9809-4623-95c1-3c16eda6b6d3.png)'
- en: The input level contains the inputs and the hidden layer contains the summation
    of the weighted input values, where each connection is associated with a weight.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层包含输入，隐藏层包含加权输入值的总和，其中每个连接都与一个权重相关。
- en: The nonlinearity is applied to the hidden layer. Typical non-linear activation
    functions could be sigmoid, tanh, or rectified linear unit.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性应用于隐藏层。典型的非线性激活函数可以是sigmoid、tanh或修正线性单元。
- en: The output level is associated with the summation of weights associated with
    each hidden unit. The optimal value of weights associated with each connection
    is obtained by adjusting the weights in such a way that the overall squared error
    value is minimized. More details of how a neural network works are provided in
    a later chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层与每个隐藏单元相关的权重之和有关。通过调整权重以使总的平方误差值最小化，获得每个连接相关的权重的最优值。关于神经网络如何工作的更多细节将在后面的章节中提供。
- en: Logistic regression
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: As discussed before, logistic regression is used to classify a prediction to
    one class or another depending on the input dataset. Logistic regression uses
    the sigmoid function to attain the probability of an event happening.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，逻辑回归用于根据输入数据集将预测分类为一类或另一类。逻辑回归使用sigmoid函数来获得事件发生的概率。
- en: 'The sigmoid curve looks like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid曲线看起来是这样的：
- en: '![](img/ba322a10-423e-4a95-8e72-2a137a06009c.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba322a10-423e-4a95-8e72-2a137a06009c.png)'
- en: Note that the output is a high probability when the *x* axis value is greater
    than 3 and the output is a very low probability when the *x* axis value is less
    than 3.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当 *x* 轴的值大于3时，输出是一个高概率，而当 *x* 轴的值小于3时，输出是一个非常低的概率。
- en: 'Logistic regression differs from linear regression in the usage of the activation
    function. While a linear regression equation would be *Y = a + b * X*, a logistic
    regression equation would be:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归与线性回归的不同之处在于激活函数的使用。线性回归方程将是 *Y = a + b * X*，而逻辑回归方程将是：
- en: '![](img/6aba77a4-c022-44f6-9801-87670cbd67b5.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6aba77a4-c022-44f6-9801-87670cbd67b5.png)'
- en: Objective function in classification
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类中的目标函数
- en: In a regression technique, we minimize the overall squared error. However, in
    a classification technique, we minimize the overall cross-entropy error.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归技术中，我们最小化总的平方误差。然而，在分类技术中，我们最小化总的交叉熵误差。
- en: 'A binary cross-entropy error is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵误差如下：
- en: '![](img/0336cc5f-d283-4c29-b27a-79ee9c4e4a73.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0336cc5f-d283-4c29-b27a-79ee9c4e4a73.png)'
- en: 'In the given equation:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的方程中：
- en: y is the actual dependent variable
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y 是实际的因变量
- en: '*p* is the probability of an event happening'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* 是事件发生的概率'
- en: For a classification exercise, all the preceding algorithms work; it's just
    that the objective function changes to cross-entropy error minimization instead
    of squared error.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类练习，所有前面的算法都适用；只是目标函数变为交叉熵误差最小化，而不是平方误差。
- en: In the case of a decision tree, the variable that belongs to the root node is
    the variable that provides the highest information gain when compared to all the
    rest of the independent variables. Information gain is defined as the improvement
    in overall entropy when the tree is split by a given variable when compared to
    no splitting.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树的情况下，属于根节点的变量是与其他所有独立变量相比提供最高信息增益的变量。信息增益定义为当树被给定变量分割时，整体熵的改善与未分割时相比。
- en: Data splitting
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'One of the key problems that need to be addressed while working on any machine
    learning model is: *how accurate can this model be once it is implemented in production
    on a future dataset?*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理任何机器学习模型时，需要解决的一个关键问题是：*一旦在未来的数据集上实施，这个模型能达到多高的准确度？*
- en: It is not possible to answer this question straight away. However, it is really
    important to obtain the buy-in from commercial teams that ultimately get benefited
    from the model build. Dividing the dataset into training and testing datasets
    comes in handy in such a scenario.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 无法立即回答这个问题。然而，从最终从模型构建中受益的商业团队那里获得认可非常重要。在这种情况下，将数据集分为训练集和测试集非常有用。
- en: The training dataset is the data that is used to build the model. The testing
    dataset is the dataset that is not seen by the model; that is, the data points
    are not used in building the model. Essentially, one can think of the testing
    dataset as the dataset that is likely to come in future. Hence, the accuracy that
    we see on the testing dataset is likely to be the accuracy of the model on the
    future dataset.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集是用于构建模型的数据。测试数据集是模型未见过的数据集；也就是说，数据点没有被用于构建模型。本质上，可以将测试数据集视为未来可能出现的数据集。因此，我们在测试数据集上看到的准确率可能是模型在未来数据集上的准确率。
- en: 'Typically, in regression, we deal with the problem of generalization/overfitting.
    The overfitting problem arises when the model is so complex that it perfectly
    fits all the data points—thus resulting in a minimal possible error rate. A typical
    example of an overfitted dataset looks like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在回归中，我们处理的是泛化/过拟合的问题。当模型变得如此复杂以至于它完美地拟合所有数据点时，过拟合问题就会出现——从而产生最小的可能错误率。一个典型的过拟合数据集的例子如下所示：
- en: '![](img/d0795b03-7685-41c0-ae81-a4de6b6a53c2.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0795b03-7685-41c0-ae81-a4de6b6a53c2.png)'
- en: From the graph dataset, one can observe that the line (colored in black) does
    not fit all the data points perfectly, while the curve (colored in blue) fits
    the points perfectly and hence has minimal error on the data points on which it
    is trained.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 从图数据集中，我们可以观察到黑色曲线并不完美地拟合所有数据点，而蓝色曲线完美地拟合了这些点，因此它在训练数据点上的错误率最小。
- en: However, the line has a better chance of being more generalizable when compared
    to the curve on a new dataset. Thus, in practice, regression/classification is
    a trade-off between generalizability and complexity of the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与新的数据集上的曲线相比，直线有更大的可能性具有更好的泛化能力。因此，在实践中，回归/分类是在模型的泛化能力和复杂性之间的一种权衡。
- en: The lower the generalizability of the model, the higher the error rate on unseen
    data points.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的泛化能力越低，未见数据点的错误率就越高。
- en: 'This phenomenon can be observed in the following graph. As the complexity of
    the model increases, the error rate of unseen data points keeps reducing till
    a point, after which it starts increasing again:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象可以在以下图表中观察到。随着模型复杂性的增加，未见数据点的错误率持续降低，直到达到一个点，之后又开始增加：
- en: '![](img/ae560c6a-9894-4e43-a8f4-a4594c219b6f.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae560c6a-9894-4e43-a8f4-a4594c219b6f.png)'
- en: The curve colored in blue is the error rate on the training dataset, and the
    curve colored in red is the testing dataset error rate.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色曲线表示训练数据集上的错误率，红色曲线表示测试数据集上的错误率。
- en: The validation dataset is used to obtain the optimal hyperparameters of the
    model. For example, in techniques such as random forest or GBM, the number of
    trees needed to build or the depth of a tree is a hyper parameter. As we keep
    changing the hyperparameter, the accuracy on unseen datasets changes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据集用于获取模型的最佳超参数。例如，在随机森林或GBM等技术中，构建所需的树的数量或树的深度是一个超参数。随着我们不断改变超参数，未见数据集上的准确率也会发生变化。
- en: However, we cannot go on varying the hyperparameter until the test dataset accuracy
    is the highest, as we would have seen the practically future dataset (testing
    dataset) in such a scenario.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不能继续改变超参数直到测试数据集的准确率最高，因为在这样的情况下，我们已经看到了实际上的未来数据集（测试数据集）。
- en: The validation dataset comes in handy in such scenarios, where we keep varying
    the hyperparameters on the training dataset until we see that the accuracy on
    the validation dataset is the highest. That would thus form the optimal hyperparameter
    combination for the model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，验证数据集非常有用，我们不断在训练数据集上改变超参数，直到我们看到验证数据集上的准确率最高。这样就会形成模型的最佳超参数组合。
- en: Measuring the accuracy of a model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量模型的准确度
- en: The methods of evaluating the accuracy of a model differ between supervised
    learning and unsupervised learning.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型准确性的方法在监督学习和无监督学习之间有所不同。
- en: In a typical linear regression (where continuous values are predicted), there
    are a couple of ways of measuring the error of the model. Typically, error is
    measured on the validation and testing datasets, as measuring error on a training
    dataset (the dataset using which a model is built) is misleading. Hence, error
    is always measured on the dataset that is not used to build a model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的线性回归（预测连续值的情况下），有几种方法可以衡量模型的误差。通常，误差是在验证集和测试集上测量的，因为在一个训练集（用于构建模型的集合）上测量误差是有误导性的。因此，误差总是在未用于构建模型的集合上测量的。
- en: Absolute error
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绝对误差
- en: 'Absolute error is defined as the absolute value of the difference between the
    forecast value and actual value. Let''s imagine a scenario as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差定义为预测值与实际值之间差异的绝对值。让我们想象以下场景：
- en: '|  | **Actual value** | **Predicted value** | **Error** | **Absolute error**
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际值** | **预测值** | **误差** | **绝对误差** |'
- en: '| **Data point 1** | 100 | 120 | 20 | 20 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **数据点 1** | 100 | 120 | 20 | 20 |'
- en: '| **Data point 2** | 100 | 80 | -20 | 20 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **数据点 2** | 100 | 80 | -20 | 20 |'
- en: '| **Overall** | 200 | 200 | 0 | 40 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| **总体** | 200 | 200 | 0 | 40 |'
- en: In the preceding scenario, we see that the overall error is 0 (as one error
    is +20 and the other is -20). If we assume that the overall error of the model
    is 0, we are missing out the fact that the model is not working well on individual
    data points.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的场景中，我们看到总体误差是0（因为一个误差是+20，另一个是-20）。如果我们假设模型的总体误差为0，我们就会忽略模型在个别数据点上表现不佳的事实。
- en: Hence, in order to avoid the issue of a positive error and negative error canceling
    each other out and thus resulting in minimal error, we consider the absolute error
    of a model, which in this case is 40; and the absolute error rate is 40/200 =
    20%.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免正误差和负误差相互抵消从而产生最小误差的问题，我们考虑模型的绝对误差，在这种情况下是40；并且绝对误差率是40/200 = 20%。
- en: Root mean square error
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方根误差
- en: 'Another approach of solving the problem of inconsistent signs of error is to
    square the error (the square of a negative number is a positive number). The scenario
    discussed previously can be translated as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 解决误差符号不一致问题的另一种方法是平方误差（负数的平方是正数）。之前讨论的场景可以翻译如下：
- en: '|  | **Actual value** | **Predicted value** | **Error** | **Squared error**
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际值** | **预测值** | **误差** | **平方误差** |'
- en: '| **Data point 1** | 100 | 120 | 20 | 400 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **数据点 1** | 100 | 120 | 20 | 400 |'
- en: '| **Data point 2** | 100 | 80 | -20 | 400 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **数据点 2** | 100 | 80 | -20 | 400 |'
- en: '| **Overall** | 200 | 200 | 0 | 800 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **总体** | 200 | 200 | 0 | 800 |'
- en: In this case, the overall squared error is 800 and root mean squared error is
    the square root of (800/2), which is 20.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，总体平方误差是800，均方根误差是（800/2）的平方根，即20。
- en: 'The accuracy in the case of a classification exercise is measured as follows:
    absolute error and RMSE are applicable when predicting continuous variables. However,
    predicting an event with discrete outcomes is a different process. Discrete event
    prediction happens in terms of probabilities; that is, the result of the model
    is a probability that certain event happens. In such cases, even though absolute
    error and RMSE can be theoretically used, there are other metrics of relevance.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类练习的情况下，准确度如下测量：绝对误差和RMSE适用于预测连续变量。然而，预测具有离散结果的事件的流程是不同的。离散事件预测以概率的形式发生；也就是说，模型的结果是某个事件发生的概率。在这种情况下，尽管绝对误差和RMSE在理论上可以使用，但还有其他相关的指标。
- en: 'A confusion matrix counts the number of instances when the model predicted
    the outcome of an event and measures it against the actual values, as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵计算模型预测事件结果的数量，并将其与实际值进行比较，如下所示：
- en: '|  | **Predicted fraud** | **Predicted non-fraud** |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测欺诈** | **预测非欺诈** |'
- en: '| **Actual fraud** | **True positive** (**TP**) | **False negative** (**FN**)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| **实际欺诈** | **真阳性**（**TP**） | **假阴性**（**FN**） |'
- en: '| **Actual non-fraud** | **False positive** (**FP**) | **True negative** (**TN**)
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **实际非欺诈** | **误报**（**FP**） | **真阴性**（**TN**） |'
- en: '*Sensitivity or TP rate or recall = TP/ (total positives) = TP/ (TP+FN)**Specificity
    or TN rate = TN/ (total negative) = TP/(FP + TN)**Precision or positive predicted
    value = TP/(TP + FP)**Accuracy = (TP + TN)/(TP + FN + FP + TN)**F1 score = 2TP/
    (2TP + FP + FN)*'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*灵敏度或TP率或召回率 = TP/（总正数）= TP/（TP+FN）* 特异性或TN率 = TN/（总负数）= TP/(FP + TN)*精确度或正预测值
    = TP/(TP + FP)*准确度 = (TP + TN)/(TP + FN + FP + TN)*F1分数 = 2TP/（2TP + FP + FN）*'
- en: A **receiver operating characteristic** (**ROC**) curve gives the relation between
    the true positive rate and false positive rate of various cutoffs. Let's say the
    model prediction is >0.8\. We assume that we should classify the prediction as
    positive. The 0.8 here is the cutoff point. Cutoffs come into the picture here
    as a model's prediction will always be a probability number—a value between 0
    and 1\. Hence, an analyst needs to bring his/her judgment in ascertaining the
    optimal cutoff.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）曲线给出了各种截止点的真正例率和假正例率之间的关系。假设模型预测值大于 0.8。我们假设我们应该将预测分类为阳性。这里的
    0.8 是截止点。在这里，截止点变得重要，因为模型的预测始终是一个概率数——一个介于 0 和 1 之间的值。因此，分析师需要将自己的判断纳入确定最佳截止点。'
- en: An ROC curve is a curve where (1-specificity) is on the *x* axis and sensitivity
    is on the *y* axis. The curve is generated by plotting the various combinations
    of sensitivity and (1-specificity) by changing the cutoff, which decides whether
    the predicted value should be a 1 or a 0.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线是一个曲线，其中（1-特异性）位于 *x* 轴上，敏感性位于 *y* 轴上。曲线是通过改变截止点（决定预测值应该是 1 还是 0）来生成敏感性（1-特异性）的各种组合而绘制的。
- en: In an ideal scenario, where data can be clearly segregated and accuracy is 100%,
    there lies a cutoff of the probability, after which the predicted value is of
    one class; it belongs to the other class for values below the cutoff. In such
    a scenario, for certain values of cutoffs, the ROC curve would be on the *y* axis
    only, that is, specificity=1\. For the rest of its length, the curve is going
    to be parallel to the *x* axis.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想场景中，数据可以清楚地分割，准确率为 100%，存在一个概率的截止点，在此之后预测值属于一个类别；低于截止点的值属于另一个类别。在这种情况下，对于某些截止点的值，ROC
    曲线将仅在 *y* 轴上，即特异性=1。对于其余长度，曲线将与 *x* 轴平行。
- en: 'A typical example of an ROC curve looks like this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 ROC 曲线看起来是这样的：
- en: '![](img/3e0d298b-6836-409b-a46c-2b08334744a8.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3e0d298b-6836-409b-a46c-2b08334744a8.png)'
- en: An ROC curve is a measure of how much better the model's performance is over
    a random guess. A random guess is where in case of a churn of 5% customers, the
    random guesser guesses that for every twenty customers, one among them will be
    labeled as a potential churner. In such a scenario, the random guess is going
    to capture 20% of all churners after randomly labeling 20% of all the customers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线是衡量模型性能相对于随机猜测有多好的一个指标。随机猜测是在 5% 的客户流失的情况下，随机猜测者猜测在每二十个客户中，将有一个被标记为潜在流失者。在这种情况下，随机猜测将在随机标记
    20% 的所有客户后捕获 20% 的所有流失者。
- en: A model's predictive power is in being able to move as close to 100% accuracy
    as possible, that is, moving away from random guesses as much as possible.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的预测能力在于尽可能接近 100% 的准确性，即尽可能远离随机猜测。
- en: '**Area under the curve** (**AUC**) is a measure of the area between the model
    curve and the random guess curve. The higher the AUC, the higher the predictive
    accuracy of the model.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线下的面积**（**AUC**）是模型曲线与随机猜测曲线之间面积的一个度量。AUC 越高，模型的预测准确性就越高。'
- en: The difference between machine learning and deep learning
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和深度学习之间的区别
- en: So far, we have looked at how various machine learning algorithms work at a
    high level. In this section, we will understand how deep learning differs from
    machine learning.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从高层次上了解了各种机器学习算法是如何工作的。在本节中，我们将了解深度学习与机器学习的区别。
- en: One of the key attributes of a machine learning task is that the inputs are
    given by the analyst or data scientist. Quite often, feature engineering plays
    a key role in improving the accuracy of the model. Moreover, if the input dataset
    is an unstructured one, feature engineering gets a lot more tricky. More often
    than not, it boils down to the knowledge of individual in deriving relevant features
    to build a more accurate model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习任务的一个关键属性是输入由分析师或数据科学家提供。通常，特征工程在提高模型准确性方面起着关键作用。此外，如果输入数据集是非结构化的，特征工程就会变得非常复杂。往往，这归结为个人在推导相关特征以构建更准确模型方面的知识。
- en: 'For example, let''s imagine a scenario where, given a set of words in a sentence,
    we are trying to predict the next word. In such a scenario, traditional machine
    learning algorithms work as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们想象一个场景，在这个场景中，给定一个句子中的单词集合，我们试图预测下一个单词。在这种情况下，传统的机器学习算法工作如下：
- en: One-hot encode each word in a sentence
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对句子中的每个单词进行独热编码
- en: Represent the input sequence of words using the one-hot encoded vector
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 one-hot 编码向量表示单词的输入序列
- en: Represent the output word, also using a one-hot encoded vector
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 one-hot 编码向量表示输出单词
- en: Build a model to predict the output word vector given the set of input words
    by optimizing for the relevant loss function
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过优化相关损失函数来构建一个模型，以预测给定输入单词集的输出单词向量
- en: 'While the preceding method works, we face three major challenges in building
    the model:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述方法有效，但在构建模型时我们面临三个主要挑战：
- en: 'Dimension of the one-hot encoded vector:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: One-hot 编码向量的维度：
- en: A piece of text is likely to have hundreds or thousands of unique words
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一段文本可能包含数百或数千个独特的单词
- en: High-dimensional data is likely to result in multiple issues—such as multicollinearity
    and the time taken to build a model
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维数据可能会引起多个问题——例如多重共线性以及构建模型所需的时间
- en: Order of words is missing in the input dataset
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据集中缺少单词的顺序
- en: 'Distance between two words is the same, irrespective of whether the words are
    similar to each other or not:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个单词之间的距离相同，无论这两个单词是否相似：
- en: For example, in a one-hot encoded vector scenario, the distance between king
    and prince would be the same as the distance between king and cheese
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在一个 one-hot 编码的向量场景中，king 和 prince 之间的距离将与 king 和 cheese 之间的距离相同
- en: 'Deep learning comes in handy in such a scenario. Using some of the techniques
    in deep learning (for example, Word2vec), we would be able to solve the following
    among the issues listed just now:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，深度学习非常有用。通过使用深度学习中的某些技术（例如 Word2vec），我们能够解决刚才列出的问题中的以下问题：
- en: Represent each word in a lower-dimensional space in such a way that words that
    are similar to each other have similar word vectors and words that are not similar
    to each other do not have similar vectors
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以一种方式在低维空间中表示每个单词，使得相似的单词具有相似的单词向量，而不相似的单词不具有相似的向量
- en: Moreover, by representing a word in a lower-dimensional space (let’s say 100),
    we would have solved the problem of high dimensionality of data
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，通过在低维空间（比如说100维）中表示一个单词，我们就解决了数据高维的问题
- en: There are multiple variants of the Word2vec technique, such as the continuous
    bag-of-words model and the continuous skip-gram model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 技术有多种变体，例如连续词袋模型和连续跳字模型。
- en: 'The architecture of a CBOW model is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW 模型的架构如下：
- en: '![](img/be7b10e6-7f0d-40fa-af6d-da43a40231fe.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be7b10e6-7f0d-40fa-af6d-da43a40231fe.png)'
- en: Note that the input vector is the one-hot encoded version (as we would have
    used in a typical machine learning model). The hidden layer neurons ensure that
    we represent the 10,000-dimensional input vector in a 300-dimensional word vector.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输入向量是 one-hot 编码版本（正如我们在典型的机器学习模型中会使用的那样）。隐藏层神经元确保我们将 10,000 维的输入向量表示为 300
    维的单词向量。
- en: The actual values in the output layer represent the one-hot encoded versions
    of the surrounding words (which form the context).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层中的实际值代表周围单词（构成上下文）的 one-hot 编码版本。
- en: Another technique in deep learning that comes in handy to solve the preceding
    problem is the **recurrent neural network** (**RNN**). An RNN works towards solving
    the sequence-of-words problem that traditional machine learning faced in the scenario
    laid out previously.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中另一种有助于解决上述问题的技术是 **循环神经网络**（**RNN**）。RNN 致力于解决传统机器学习在之前场景中面临的单词序列问题。
- en: 'RNN provides each word vector in order to predict the next word in the sequence.
    More details of how RNN works will be provided in a different chapter. The popular
    variants of the RNN technique are **long short-term memory** (**LSTM**) and **gated
    recurrent unit** (**GRU**):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 提供每个单词向量以预测序列中的下一个单词。关于 RNN 如何工作的更多细节将在另一章中提供。RNN 技术的流行变体包括 **长短期记忆**（**LSTM**）和
    **门控循环单元**（**GRU**）：
- en: '![](img/1e155073-cb6b-4175-9799-111cc8f80cba.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e155073-cb6b-4175-9799-111cc8f80cba.png)'
- en: The preceding diagram represents a typical RNN, where *x[(t-1)]*, *x[(t)]* and
    *x[(t+1)]* represent the words in each time period, *W* is the weightage associated
    with a previous word in predicting the next word, and *O[(t)]* is the output in
    time *t*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表示了一个典型的循环神经网络（RNN），其中 *x[(t-1)]*、*x[(t)]* 和 *x[(t+1)]* 代表每个时间段的单词，*W* 是用于预测下一个单词的前一个单词的权重，而
    *O[(t)]* 是时间 *t* 的输出。
- en: LSTM comes in handy when the weightage that needs to be associated with a word
    that occurred much earlier in sequence would have be high in predicting the next
    word.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要与序列中很久以前出现的单词相关联的权重很高时，LSTM非常有用。
- en: A combination of Word2vec and RNN, which are variants of neural networks, helps
    in avoiding the challenge of feature engineering with the given text data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec和RNN的组合，它们是神经网络的不同变体，有助于避免在给定的文本数据中特征工程带来的挑战。
- en: 'In order to solidify our understanding of the difference between machine learning
    and deep learning, let''s go through another example: predicting the label of
    an image.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固我们对机器学习和深度学习之间差异的理解，让我们通过另一个示例：预测图像的标签。
- en: We will use a classic example—the MNIST dataset (we will be using MNIST a lot
    more in future chapters).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个经典示例——MNIST数据集（我们将在未来的章节中更多地使用MNIST）。
- en: 'The MNIST dataset contains images of various digits, from zero to nine. Each
    image is 28 x 28 pixels in size. The task is to predict the label of the image
    by analyzing the various pixel values. A sample image in the MNIST dataset looks
    like this:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集包含从零到九的各种数字的图像。每个图像的大小为28 x 28像素。任务是通过对各种像素值进行分析来预测图像的标签。MNIST数据集中的一个示例图像如下所示：
- en: '![](img/afa8a2ff-8930-4e86-9c80-dc690995f794.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afa8a2ff-8930-4e86-9c80-dc690995f794.png)'
- en: 'Traditional machine learning solves the preceding problem as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 传统机器学习解决前面问题的方式如下：
- en: Treat each pixel as a separate variable; that is, we have a total of 784 variables
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个像素视为一个单独的变量；也就是说，我们总共有784个变量
- en: One-hot encode the label column
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对标签列进行one-hot编码
- en: Predict the probability of a label occurring
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测标签发生的概率
- en: 'The challenge with the way in which we solve the preceding problem is as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 解决前面问题的挑战如下：
- en: The model will not take pixel adjacencies into account
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型不会考虑像素的相邻关系
- en: The model will not account for translation or rotation of the image
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型不会考虑图像的平移或旋转
- en: For example, when the image is shifted appropriately, a zero could look like
    a six or vice versa. Similarly, if all the images are trained using a dataset
    that had all the numbers centered in the image but the test dataset has an image
    that is shifted slightly to the right or left, the prediction is likely to be
    inaccurate. This is because the model would have placed a weightage for each pixel.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当图像被适当地移动时，一个零可能看起来像六，反之亦然。同样，如果所有图像都是使用一个数据集进行训练，该数据集中的所有数字都集中在图像中，但测试数据集有一个图像稍微向右或向左移动，预测很可能是准确的。这是因为模型会给每个像素分配权重。
- en: In order to solve the preceding problem, a deep learning technique named **convolutional
    neural network** (**CNN**) comes in handy. A CNN works in such a way that it assigns
    weightages at a region level rather than at a pixel level. Essentially, this forms
    the convolution part of convolutional neural networks. In this way, pixel adjacencies
    are taken into account by using deep learning.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前面的问题，一种名为**卷积神经网络**（**CNN**）的深度学习技术非常有用。CNN的工作方式是它在区域级别而不是像素级别分配权重。本质上，这形成了卷积神经网络中的卷积部分。通过这种方式，使用深度学习考虑了像素的相邻关系。
- en: Similarly, translation of an image is accounted for by a technique called **max
    pooling** that is used in CNN.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，图像的平移是通过在CNN中使用的一种称为**最大池化**的技术来考虑的。
- en: 'The typical architecture of a CNN looks as follows, and more details of it
    will be explained in a later chapter:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的典型架构如下，其更多细节将在后面的章节中解释：
- en: '![](img/e774be9a-699f-4669-aa70-65924337e3b8.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e774be9a-699f-4669-aa70-65924337e3b8.png)'
- en: In the preceding diagram, the input is the image that we consider. **conv1**
    is the output when a convolution is applied between filters and input. Given that
    we apply multiple filters, we would have multiple convolutions, and **pool1**
    is the output of applying pooling on the convolution output. The process of convolution
    and pooling is applied repeatedly until we obtain the final fully connected unit,
    which is then linked to the output.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，输入是我们考虑的图像。**conv1**是应用卷积于滤波器和输入时的输出。鉴于我们应用了多个滤波器，我们将有多个卷积，**pool1**是应用池化于卷积输出时的输出。卷积和池化的过程会反复应用，直到我们获得最终的完全连接单元，然后将其连接到输出。
- en: Applications of deep learning
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: 'In the previous section, we understood why deep learning shines over machine
    learning in some applications. Let''s go through some of the applications of deep
    learning:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了为什么在某些应用中深度学习比机器学习更出色。让我们来看看深度学习的一些应用：
- en: Translation from one language to another
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一种语言翻译到另一种语言
- en: Speech-to-text conversion
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音转文本转换
- en: Image analysis in multiple industries
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个行业中的图像分析
- en: Identifying text present in images
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别图像中的文本
- en: Image and audio synthesis
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像和音频合成
- en: Personalization to predict the next movie/product that a user is likely to watch/buy
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个性化预测用户可能观看/购买的下部电影/产品
- en: Time series analysis
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列分析
- en: Detecting rare events
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测罕见事件
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we understood the major difference between supervised and unsupervised
    learning and got an overview of the major machine learning algorithms. We also
    understood the areas where deep learning algorithms shine over traditional machine
    learning algorithms, through examples of text and image analysis.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了监督学习和无监督学习之间的主要区别，并对主要的机器学习算法有了概述。我们还通过文本和图像分析的例子，了解了深度学习算法在哪些领域比传统的机器学习算法更出色。
