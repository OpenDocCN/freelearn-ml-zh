- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Making Decisions in Complex Environments with Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在复杂环境中通过强化学习做决策
- en: In the previous chapter, we focused on multimodal models for image and text
    co-learning. The last chapter of this book will be about reinforcement learning,
    which is the third type of machine learning task mentioned at the beginning of
    the book. You will see how learning from experience and learning by interacting
    with the environment differs from previously covered supervised and unsupervised
    learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们专注于图像和文本联合学习的多模态模型。本书的最后一章将讲解强化学习，这是书籍开头提到的第三类机器学习任务。你将看到从经验中学习和与环境互动学习与之前讲解的监督学习和无监督学习的区别。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Setting up the working environment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: Introducing reinforcement learning with examples
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用示例介绍强化学习
- en: Solving the FrozenLake environment with dynamic programming
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用动态规划解决 FrozenLake 环境问题
- en: Performing Monte Carlo learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行蒙特卡罗学习
- en: Solving the Taxi problem with the Q-learning algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Q-learning 算法解决出租车问题
- en: Setting up the working environment
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: Let’s get started with setting up the working environment needed for this chapter,
    including Gymnasium (which builds upon OpenAI Gym), the toolkit that gives you
    a variety of environments to develop your learning algorithms on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始设置本章所需的工作环境，包括 Gymnasium（它基于 OpenAI Gym），这是一个为你提供各种环境的工具包，帮助你开发学习算法。
- en: Introducing OpenAI Gym and Gymnasium
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 OpenAI Gym 和 Gymnasium
- en: '**OpenAI Gym** was a toolkit for developing and comparing reinforcement learning
    algorithms. It provided a collection of environments, or “tasks,” in which reinforcement
    learning agents can interact and learn. These environments range from simple grid-world
    games to complex simulations of real-world scenarios, allowing researchers and
    developers to experiment with a wide variety of reinforcement learning algorithms.
    It was developed by OpenAI, focused on building safe and beneficial **Artificial
    General Intelligence** (**AGI**).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI Gym** 是一个开发和比较强化学习算法的工具包。它提供了一系列环境或“任务”，强化学习智能体可以在这些环境中互动并学习。这些环境从简单的网格世界游戏到复杂的现实世界场景仿真不等，允许研究人员和开发者实验各种强化学习算法。它由
    OpenAI 开发，旨在构建安全且有益的 **人工通用智能**（**AGI**）。'
- en: 'Some key features of OpenAI Gym included:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 的一些关键特点包括：
- en: '**Environment interface**: Gym provided a consistent interface for interacting
    with environments, allowing agents to observe states, take actions, and receive
    rewards (we will learn about these terms).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境接口**：Gym 提供了一个一致的接口，用于与环境进行交互，使得智能体能够观察状态、采取行动并获得奖励（我们将在后文学习这些术语）。'
- en: '**Extensive collection of environments**: Gym offered a diverse set of environments,
    including classic control tasks, Atari games, robotics simulations, and more.
    This allowed researchers and developers to evaluate algorithms across various
    domains.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**丰富的环境集合**：Gym 提供了多样化的环境集合，包括经典控制任务、Atari 游戏、机器人仿真等。这使得研究人员和开发者能够在各个领域评估算法。'
- en: '**Easy-to-use API**: Gym’s API was straightforward and easy to use, making
    it accessible to both beginners and experienced researchers. Developers could
    quickly prototype and test reinforcement learning algorithms using Gym’s environments.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用的 API**：Gym 的 API 简单易用，适合初学者和有经验的研究人员。开发者可以快速原型化并测试强化学习算法，利用 Gym 提供的环境。'
- en: '**Benchmarking**: Gym facilitated benchmarking by providing standardized environments
    and evaluation metrics. This enabled researchers to compare the performance of
    different algorithms on common tasks.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准测试**：Gym 通过提供标准化的环境和评估指标，促进了基准测试。这使得研究人员能够比较不同算法在常见任务上的表现。'
- en: '**Community contributions**: Gym was an open-source project, and the community
    actively contributed new environments, algorithms, and extensions to the toolkit.
    This collaborative effort helped to continuously expand and improve Gym’s capabilities.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社区贡献**：Gym 是一个开源项目，社区积极贡献新的环境、算法和扩展功能到该工具包中。这一合作努力帮助不断扩展和改进 Gym 的能力。'
- en: Overall, OpenAI Gym served as a valuable resource for the reinforcement learning
    community, providing a standardized platform for research, experimentation, and
    benchmarking.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，OpenAI Gym 为强化学习社区提供了一个宝贵的资源，提供了一个标准化的平台用于研究、实验和基准测试。
- en: Gym was a pioneering library and set the standard for simplicity for many years.
    However, it is no longer actively maintained by the OpenAI team. Recognizing this,
    some developers took the initiative to create **Gymnasium** ([https://gymnasium.farama.org/index.html](https://gymnasium.farama.org/index.html)),
    with approval from OpenAI. Gymnasium emerged as a successor to Gym, and the original
    developers from OpenAI occasionally contribute to its development, ensuring its
    reliability and continuity. In this chapter, we will be using Gymnasium, which
    is a **maintained fork** of Gym.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Gym曾是一个开创性的库，并且在许多年里为简洁性设定了标准。然而，它不再由OpenAI团队积极维护。意识到这一点，一些开发者主动创建了**Gymnasium**（[https://gymnasium.farama.org/index.html](https://gymnasium.farama.org/index.html)），并得到了OpenAI的批准。Gymnasium作为Gym的继任者应运而生，OpenAI的原始开发者偶尔会为其开发做出贡献，确保其可靠性和持续性。在本章中，我们将使用Gymnasium，这是Gym的**维护分支**。
- en: Installing Gymnasium
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Gymnasium
- en: 'One way to install the Gymnasium library is via `pip`, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Gymnasium库的一种方式是通过`pip`，如下所示：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is recommended to install the `toy-text` extension using the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐使用以下命令安装`toy-text`扩展：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `toy-text` extension provides additional toy text-based environments, such
    as the FrozenLake environment (discussed later), for reinforcement learning experimentation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`toy-text`扩展提供了额外的基于文本的玩具环境，例如FrozenLake环境（稍后讨论），供强化学习实验使用。'
- en: 'After the installation, you can check the available Gymnasium environments
    by running the following code:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，您可以通过运行以下代码来检查可用的Gymnasium环境：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see lists of the environments at [https://gymnasium.farama.org/environments/toy_text/](https://gymnasium.farama.org/environments/toy_text/)
    and [https://gymnasium.farama.org/environments/atari/](https://gymnasium.farama.org/environments/atari/),
    including walking, moon landing, car racing, and Atari games. Feel free to play
    around with Gymnasium by going through its introduction at [https://gymnasium.farama.org/content/basic_usage/](https://gymnasium.farama.org/content/basic_usage/).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://gymnasium.farama.org/environments/toy_text/](https://gymnasium.farama.org/environments/toy_text/)和[https://gymnasium.farama.org/environments/atari/](https://gymnasium.farama.org/environments/atari/)查看环境列表，包括行走、登月、赛车和Atari游戏。您可以通过访问[https://gymnasium.farama.org/content/basic_usage/](https://gymnasium.farama.org/content/basic_usage/)来轻松体验Gymnasium的介绍。
- en: To benchmark different reinforcement learning algorithms, we need to apply them
    in a standardized environment. Gymnasium is the perfect place for this, with a
    number of versatile environments. This is similar to using datasets such as MNIST,
    ImageNet, and Thomson Reuters News as benchmarks in supervised and unsupervised
    learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对不同的强化学习算法进行基准测试，我们需要在一个标准化的环境中应用它们。Gymnasium是完美的选择，提供了多种通用环境。这类似于在监督学习和无监督学习中使用MNIST、ImageNet和汤森路透新闻等数据集作为基准。
- en: Gymnasium has an easy-to-use interface for the reinforcement learning environments
    that we can write **agents** to interact with. So what’s reinforcement learning?
    What’s an agent? Let’s see in the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium为强化学习环境提供了一个易于使用的接口，我们可以编写**智能体**与之交互。那么，什么是强化学习？什么是智能体？让我们在下一节中看看。
- en: Introducing reinforcement learning with examples
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例介绍强化学习
- en: In this chapter, we will first introduce the elements of reinforcement learning
    along with an interesting example, then will move on to how we measure feedback
    from the environment, and follow with the fundamental approaches to solve reinforcement
    learning problems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先介绍强化学习的元素，并通过一个有趣的例子来讲解，接着介绍如何衡量来自环境的反馈，并介绍解决强化学习问题的基本方法。
- en: Elements of reinforcement learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的元素
- en: You may have played Super Mario (or Sonic) when you were young. During the video
    game, you control Mario to collect coins and avoid obstacles at the same time.
    The game ends if Mario hits an obstacle or falls in a gap, and you try to get
    as many coins as possible before the game ends.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在小时候玩过超级马里奥（或索尼克）。在这个视频游戏中，你控制马里奥同时收集金币并避开障碍物。如果马里奥撞到障碍物或掉进空隙，游戏就结束了，你会尽力在游戏结束前收集尽可能多的金币。
- en: Reinforcement learning is very similar to the Super Mario game. Reinforcement
    learning is about learning what to do. It involves observing situations in the
    environment and determining the right actions in order to maximize a numerical
    reward.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与超级马里奥游戏非常相似。强化学习就是学习该做什么。它涉及观察环境中的情况，并确定正确的行动以最大化数值奖励。
- en: 'Here is the list of elements in a reinforcement learning task (we also link
    each element to Super Mario and other examples so it’s easier to understand):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是强化学习任务中各元素的列表（我们还将每个元素与超级马里奥和其他示例相连接，以便更容易理解）：
- en: '**Environment**: The environment is a task or simulation. In the Super Mario
    game, the game itself is the environment. In self-driving, the road and traffic
    are the environment. In the context of Go playing chess, the board is the environment.
    The inputs to the environment are the actions sent from the **agent** and the
    outputs are **states** and **rewards** sent to the agent.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：环境是一个任务或模拟。在超级马里奥游戏中，游戏本身就是环境。在自动驾驶中，道路和交通就是环境。在围棋下棋的上下文中，棋盘是环境。环境的输入是来自**智能体**的行动，输出是发给智能体的**状态**和**奖励**。'
- en: '**Agent**: The agent is the component that takes **actions** according to the
    reinforcement learning model. It interacts with the environment and observes the
    states to feed into the model. The goal of the agent is to solve the environment—that
    is, finding the best set of actions to maximize the rewards. The agent in the
    Super Mario game is Mario, and the autonomous vehicle is the agent for self-driving.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能体**：智能体是根据强化学习模型采取**行动**的组件。它与环境互动，并观察状态，将其输入到模型中。智能体的目标是解决环境问题——也就是说，找到一组最佳行动来最大化奖励。在超级马里奥游戏中，智能体是马里奥，在自动驾驶中，自动驾驶汽车就是智能体。'
- en: '**Action**: This is the possible movement of the agent. It is usually random
    in a reinforcement learning task at the beginning when the model starts to learn
    about the environment. Possible actions for Mario include moving left and right,
    jumping, and crouching.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动**：这是智能体可能的移动。在强化学习任务的开始阶段，当模型开始学习环境时，行动通常是随机的。超级马里奥的可能行动包括向左和向右移动、跳跃和蹲下。'
- en: '**States**: The states are the observations from the environment. They describe
    the situation in a numerical way at every time step. For a chess game, the state
    is the positions of all the pieces on the board. For Super Mario, the state includes
    the coordinates of Mario and other elements in the time frame. For a robot learning
    to walk, the position of its two legs is the state.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：状态是来自环境的观察结果。它们在每个时间步上以数字方式描述当前的情况。在国际象棋游戏中，状态是棋盘上所有棋子的位置信息。在超级马里奥中，状态包括马里奥的位置和时间帧内的其他元素。在一个学习走路的机器人中，它的两条腿的位置就是状态。'
- en: '**Rewards**: Every time the agent takes an action, it receives numerical feedback
    from the environment. The feedback is called the **reward**. It can be positive,
    negative, or zero. The reward in the Super Mario game can be, for example, +1
    if Mario collects a coin, +2 if he avoids an obstacle, -10 if he hits an obstacle,
    or 0 for other cases.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：每当智能体采取一个行动时，它会从环境中获得数值反馈，这个反馈叫做**奖励**。奖励可以是正数、负数或零。超级马里奥中的奖励可以是，例如，马里奥收集到一个金币时是+1，避开一个障碍物是+2，撞到障碍物是-10，或者在其他情况下为0。'
- en: 'The following diagram summarizes the process of reinforcement learning:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了强化学习的过程：
- en: '![](img/B21047_15_01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_01.png)'
- en: 'Figure 15.1: Reinforcement learning process'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：强化学习过程
- en: The reinforcement learning process is an iterative loop. At the beginning, the
    agent observes the initial state, *s*[0], from the environment. Then the agent
    takes an action, *a*[0], according to the model. After the agent moves, the environment
    is now in a new state, *s*[1], and it gives a feedback reward, *R*[1]. The agent
    then takes an action, *a*[1], as computed by the model with inputs *s*[1] and
    *R*[1]. This process continues until termination, completion, or for forever.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习过程是一个迭代循环。一开始，智能体从环境中观察到初始状态，*s*[0]。然后，智能体根据模型采取行动，*a*[0]。在智能体移动后，环境进入一个新的状态，*s*[1]，并给予反馈奖励，*R*[1]。然后，智能体根据模型使用*s*[1]和*R*[1]作为输入，采取行动，*a*[1]。这个过程会继续，直到终止、完成，或者永远持续下去。
- en: The goal of the reinforcement learning model is to maximize the total reward.
    So how can we calculate the total reward? Is it simply by summing up rewards at
    all the time steps? Let’s see in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习模型的目标是最大化总奖励。那么，我们如何计算总奖励呢？是不是通过将所有时间步的奖励加起来？让我们在下一节中看看。
- en: Cumulative rewards
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 累积奖励
- en: 'At time step *t*, the **cumulative rewards** (also called **returns**) *G*[1]
    can be written as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步* t *时，**累积奖励**（也叫**回报**）*G*[1]可以表示为：
- en: '![](img/B21047_15_001.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_001.png)'
- en: Here, *T* is the termination time step or infinity. *G*[t] means the total future
    reward after taking an action *a*[t] at time *t*. At each time step *t*, the reinforcement
    learning model attempts to learn the best possible action in order to maximize
    *G*[t].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*T*是终止时间步长或无限。*G*[t]表示在时间*t*采取行动*a*[t]之后的未来总奖励。在每个时间步长*t*，强化学习模型试图学习最佳的行动，以最大化*G*[t]。
- en: 'However, in many real-world cases, things don’t work this way where we simply
    sum up all future rewards. Take a look at the following example:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多现实世界的情况下，事情并非总是这样简单，我们只是把所有未来奖励加总起来。看看以下例子：
- en: Stock A rises 6 dollars at the end of day 1 and falls 5 dollars at the end of
    day 2\. Stock B falls 5 dollars on day 1 and rises 6 dollars on day 2\. After
    two days, both stocks rise 1 dollar. So, if we knew that, which one would we buy
    at the beginning of day 1? Obviously, stock A, because we won’t lose money and
    can even profit 6 dollars if we sell it at the beginning of day 2.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 股票A在第1天结束时上涨6美元，第2天结束时下跌5美元。股票B在第1天下跌5美元，第2天上涨6美元。经过两天后，两只股票都上涨了1美元。那么，如果我们知道这一点，我们在第1天开始时会选择买哪只股票？显然是股票A，因为我们不会亏损，甚至可以在第2天开始时卖出赚取6美元的利润。
- en: Both stocks have the same total reward but we favor stock A as we care more
    about immediate return than distant return. Similarly in reinforcement learning,
    we discount rewards in the distant future and the discount factor is associated
    with the time horizon. Longer time horizons should have less impact on the cumulative
    rewards. This is because longer time horizons include more irrelevant information
    and consequently are of higher variance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 两只股票的总奖励相同，但我们更倾向于股票A，因为我们更关心即时回报而非远期回报。同样，在强化学习中，我们会对远期奖励进行折扣，折扣因子与时间跨度有关。较长的时间跨度应该对累积奖励的影响较小。这是因为较长的时间跨度包含了更多无关信息，从而具有更高的方差。
- en: 'We define a discount factor ![](img/B21047_15_002.png) with a value between
    0 and 1\. We rewrite the cumulative rewards incorporating the discount factor:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个折扣因子*！[](img/B21047_15_002.png)*，其值介于0和1之间。我们重写累积奖励，结合折扣因子：
- en: '![](img/B21047_15_003.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_003.png)'
- en: As you can see, the larger the ![](img/B21047_15_004.png), the smaller the discount
    and vice versa. If ![](img/B21047_15_005.png), there is literally no discount
    and the model evaluates an action based on the sum total of all future rewards.
    If ![](img/B21047_15_006.png), the model only focuses on the immediate reward
    *R*[t][+1].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，*！[](img/B21047_15_004.png)*越大，折扣越小，反之亦然。如果*！[](img/B21047_15_005.png)*，则实际上没有折扣，模型会根据所有未来奖励的总和来评估一个行动。如果*！[](img/B21047_15_006.png)*，模型只关注即时奖励*R*[t][+1]。
- en: Now that we know how to calculate the cumulative reward, the next thing to talk
    about is how to maximize it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何计算累积奖励，接下来要讨论的是如何最大化它。
- en: Approaches to reinforcement learning
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的方法
- en: There are two main approaches to solving reinforcement learning problems, which
    are about finding the optimal actions to maximize the cumulative rewards. One
    is a policy-based approach and the other is value-based.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解决强化学习问题的主要方法有两种，目的是寻找最优的行动来最大化累积奖励。一种是基于策略的方法，另一种是基于价值的方法。
- en: Policy-based approach
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于策略的方法
- en: 'A **policy** is a function ![](img/B21047_15_007.png) that maps each input
    state to an action:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略**是一个函数 ![](img/B21047_15_007.png)，它将每个输入状态映射到一个行动：'
- en: '![](img/B21047_15_008.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_008.png)'
- en: 'It can be either deterministic or stochastic:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以是确定性的或随机的：
- en: '**Deterministic**: There is one-to-one mapping from the input state to the
    output action'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定性**：从输入状态到输出行动是单一映射'
- en: '**Stochastic**: This gives a probability distribution over all possible actions
    ![](img/B21047_15_009.png)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机性**：这给出了所有可能行动的概率分布！[](img/B21047_15_009.png)'
- en: In the **policy-based** approach, the model learns the optimal policy that maps
    each input state to the best action. The agent directly learns the best course
    of action (policy) for any situation (state) it encounters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在**基于策略**的方法中，模型学习最优策略，将每个输入状态映射到最佳行动。智能体直接学习针对它所遇到的任何情况（状态）的最佳行动（策略）。
- en: In a policy-based algorithm, the model starts with a random policy. It then
    computes the value function of that policy. This step is called the **policy evaluation
    step**. After this, it finds a new and better policy based on the value function.
    This is the **policy improvement step**. These two steps repeat until the optimal
    policy is found.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的算法中，模型从一个随机的策略开始。然后计算该策略的价值函数，这一步称为**策略评估步骤**。之后，基于价值函数，找到一个新的、更优的策略，这就是**策略改进步骤**。这两个步骤会不断重复，直到找到最优策略。
- en: Imagine you are training a race car driver. In a policy-based approach, you
    directly teach the driver the best manoeuvres (policy) to take on different parts
    of the track (states) to achieve the fastest lap time (reward). You don’t tell
    them the estimated result (reward) of each turn, but rather guide them towards
    the optimal racing line through feedback and practice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在训练一名赛车手。在基于策略的方法中，你直接教导赛车手在赛道的不同部分（状态）上采取最佳操作（策略），以实现最快的圈速（奖励）。你不会告诉他们每个弯道的估计结果（奖励），而是通过反馈和练习指导他们朝着最优赛车路线前进。
- en: Value-based approach
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于价值的方法
- en: 'The **value** *V* of a state is defined as the expected future cumulative reward
    to collect from the state:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 状态的**价值** *V* 被定义为从该状态开始收集的预期未来累计奖励：
- en: '![](img/B21047_15_010.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_010.png)'
- en: In the **value-based** approach, the model learns the optimal value function
    that maximizes the value of the input state. In other words, the agent takes an
    action to reach the state that achieves the largest value.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在**基于价值**的方法中，模型学习最大化输入状态价值的最优价值函数。换句话说，智能体采取行动到达能够实现最大价值的状态。
- en: In a value-based algorithm, the model starts with a random value function. It
    then finds a new and improved value function in an iterative manner, until it
    reaches the optimal value function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的算法中，模型从随机的价值函数开始。然后通过迭代的方式找到一个新的、更优的价值函数，直到达到最优价值函数。
- en: Now imagine you are a treasure hunter. In a value-based approach, you learn
    the estimated treasure (value) of different locations (states) in a maze. This
    helps you choose paths that lead to areas with higher potential treasure (rewards)
    without needing a pre-defined course of action (policy).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你是一个寻宝猎人。在基于价值的方法中，你学习迷宫中不同位置（状态）的估计宝藏（价值）。这帮助你选择通向宝藏（奖励）潜力更高区域的路径，而无需预定义的行动路线（策略）。
- en: We’ve learned there are two main approaches to solving reinforcement learning
    problems. In the next section, let’s see how to solve a concrete reinforcement
    learning example (FrozenLake) using a concrete algorithm, the dynamic programming
    method, in a policy-based and value-based way respectively.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，解决强化学习问题有两种主要的方法。在下一部分中，我们将分别通过基于策略和基于价值的方式，使用动态规划方法来解决一个具体的强化学习示例（FrozenLake）。
- en: Solving the FrozenLake environment with dynamic programming
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用动态规划解决 FrozenLake 环境
- en: We will focus on the policy-based and value-based dynamic programming algorithms
    in this section. But let’s start by simulating the FrozenLake environment. It
    simulates a simple grid-world scenario where an agent navigates through a grid
    of icy terrain, represented as a frozen lake, to reach a goal tile.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将重点讨论基于策略和基于价值的动态规划算法。但我们首先通过模拟 FrozenLake 环境开始。它模拟了一个简单的网格世界场景，其中智能体在一个冰面地形的网格中导航，目标是到达目标格子。
- en: Simulating the FrozenLake environment
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟 FrozenLake 环境
- en: 'FrozenLake is a typical OpenAI Gym (now Gymnasium) environment with **discrete**
    states. It is about moving the agent from the starting tile to the destination
    tile in a grid, and at the same time avoiding traps. The grid is either 4 * 4
    (FrozenLake-v1), or 8 * 8 (FrozenLake8x8-v1). There are four types of tiles in
    the grid:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: FrozenLake 是一个典型的 OpenAI Gym（现为 Gymnasium）环境，具有**离散**状态。它的目标是在一个网格中将智能体从起始格子移动到目标格子，同时避开陷阱。网格的大小为
    4 * 4（FrozenLake-v1）或 8 * 8（FrozenLake8x8-v1）。网格中有四种类型的格子：
- en: '**The starting tile**: This is state 0, and it comes with 0 reward.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**起始格子**：这是状态 0，且奖励为 0。'
- en: '**The goal tile**: It is state 15 in the 4 * 4 grid. It gives +1 reward and
    terminates an episode.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标格子**：它是 4 * 4 网格中的状态 15，提供 +1 奖励并结束一个回合。'
- en: '**The frozen tile**: In the 4 * 4 grid, states 1, 2, 3, 4, 6, 8, 9, 10, 13,
    and 14 are walkable tiles. It gives 0 reward.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冰冻格子**：在 4 * 4 网格中，状态 1、2、3、4、6、8、9、10、13 和 14 是可走的格子，给予 0 奖励。'
- en: '**The hole tile**: In the 4 * 4 grid, states 5, 7, 11, and 12 are hole tiles.
    It gives 0 reward and terminates an episode.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洞方格**：在4 * 4的网格中，状态5、7、11和12是洞方格。它会给予0奖励并终止剧集。'
- en: 'Here, an **episode** means a simulation of a reinforcement learning environment.
    It contains a list of states from the initial state to the terminal state, a list
    of actions and rewards. In the 4 * 4 FrozenLake environment, there are 16 possible
    states as the agent can move to any of the 16 tiles. And there are four possible
    actions: moving left (0), down (1), right (2), and up (3).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**剧集**指的是强化学习环境的模拟。它包含从初始状态到终止状态的状态列表、一系列动作和奖励。在4 * 4的FrozenLake环境中，代理可以移动到16个方格中的任何一个，因此有16个可能的状态。而可选的动作有四种：向左移动（0）、向下移动（1）、向右移动（2）和向上移动（3）。
- en: The tricky part of this environment is that, as the ice surface is slippery,
    the agent won’t always move in the direction it intends and can move in any other
    walkable direction or stay unmoved with certain probabilities. For example, it
    may move to the right even though it intends to move up.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境的棘手之处在于，由于冰面滑溜，代理不会总是朝着它打算的方向移动，而是可能会朝任何其他可行方向移动，或者停在原地，且这些都有一定的概率。例如，即使代理打算向上移动，它也可能会向右移动。
- en: 'Now let’s simulate the 4 * 4 FrozenLake environment by following these steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们按照以下步骤模拟4 * 4的FrozenLake环境：
- en: To simulate any OpenAI Gym environment, we need to first look up its name in
    the documentation at [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).
    We get `FrozenLake-v1` in our case.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要模拟任何OpenAI Gym环境，我们需要先在文档中查找其名称，地址为[https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)。在我们的例子中，我们得到`FrozenLake-v1`。
- en: 'We import the `gym` library and create a `FrozenLake` instance:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入`gym`库并创建一个`FrozenLake`实例：
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The environment is initialized with the `FrozenLake-v1` identifier. Additionally,
    the `render_mode` parameter is set to `rgb_array`, indicating that the environment
    should render its state as an RGB array, suitable for visualization purposes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 环境使用`FrozenLake-v1`标识符进行初始化。此外，`render_mode`参数设置为`rgb_array`，表示环境应将其状态渲染为RGB数组，适合于可视化目的。
- en: We also obtain the dimensions of the environment.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还获取了环境的维度。
- en: 'Every time we run a new episode, we need to reset the environment:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次我们运行一个新的剧集时，都需要重置环境：
- en: '[PRE4]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It means that the agent starts with state 0\. Again, there are 16 possible states,
    0, 1, …, 15.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着代理从状态0开始。同样，存在16个可能的状态，0、1、…、15。
- en: 'We render the environment to display it:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们渲染环境以显示它：
- en: '[PRE5]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you encounter any error, you may install the `pyglet` library, which embeds
    a `Matplotlib` figure within a window using canvas rendering, using the following
    command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到错误，您可以安装`pyglet`库，该库通过画布渲染将`Matplotlib`图形嵌入窗口，使用以下命令：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You will see a 4 * 4 matrix representing the FrozenLake grid and the tile (state
    0) where the agent is located:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个4 * 4的矩阵，代表FrozenLake网格，以及代理所在的方格（状态0）：
- en: '![](img/B21047_15_02.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_02.png)'
- en: 'Figure 15.2: Initial state of FrozenLake'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：FrozenLake的初始状态
- en: 'Let’s now start moving the agent around. Let’s take a right action since it
    is walkable:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们开始移动代理。我们选择向右移动，因为那是可行的：
- en: '[PRE7]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We take a “right” (2) action, but the agent moves down to state 4, at a probability
    of 33.33%, and gets 0 reward since the episode is not done yet.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行一个“向右”（2）动作，但代理以33.33%的概率向下移动到状态4，并获得0奖励，因为剧集尚未完成。
- en: 'Let’s see the rendered result:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看渲染结果：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/B21047_15_03.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_03.png)'
- en: 'Figure 15.3: Result of the agent moving right'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3：代理向右移动的结果
- en: You may get a completely different result as the agent can move right to state
    1 at a probability of 33.33%, or stay at state 0 at a probability of 33.33% due
    to the **slippery** nature of the frozen lake.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会得到完全不同的结果，因为代理有33.33%的概率直接移动到状态1，或者由于冰面**滑溜**的特性，33.33%的概率停留在状态0。
- en: In Gymnasium, “**terminated**” and “**truncated**” refer to different ways in
    which an episode can end in a reinforcement learning environment. When an episode
    is terminated, it means that the episode has ended naturally according to the
    rules of the environment. When an episode is truncated, it means that the episode
    is artificially terminated before it can end naturally.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gymnasium中，"**terminated**"和"**truncated**"指的是强化学习环境中剧集结束的不同方式。当一个剧集被terminated时，意味着该剧集按照环境规则自然结束。当一个剧集被truncated时，意味着剧集在未自然结束之前被人工终止。
- en: 'Next, we define a function that simulates a FrozenLake episode under a given
    policy and returns the total reward (as an easy start, let’s just assume discount
    factor ![](img/B21047_15_011.png)):'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，用于模拟在给定策略下的FrozenLake游戏，并返回总奖励（作为简单的开始，我们假设折扣因子为 ![](img/B21047_15_011.png)）：
- en: '[PRE9]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, `policy` is a PyTorch tensor, and `.item()` extracts the value of an element
    on the tensor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`policy`是一个PyTorch张量，`.item()`用于提取张量中元素的值。
- en: 'Now let’s play around with the environment using a random policy. We will implement
    a random policy (where random actions are taken) and calculate the average total
    reward over 1,000 episodes:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用随机策略来与环境进行互动。我们将实现一个随机策略（其中采取随机动作），并计算1,000次游戏中的平均总奖励：
- en: '[PRE10]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: On average, there is a 1.6% chance that the agent can reach the goal if we take
    random actions. This tells us it is not as easy to solve the FrozenLake environment
    as you might think.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，如果我们采取随机动作，智能体到达目标的概率为1.6%。这告诉我们，解决FrozenLake环境并不像你想象的那么容易。
- en: 'As a bonus step, you can look into the transition matrix. The **transition
    matrix T**(*s*, *a*, *s’*) contains probabilities of taking action *a* from state
    *s* then reaching *s’*. Take state 6 as an example:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为附加步骤，您可以查看转移矩阵。**转移矩阵T**(*s*, *a*, *s’*)包含从状态*s*采取动作*a*到达状态*s’*的概率。以状态6为例：
- en: '[PRE11]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The keys of the returning dictionary 0, 1, 2, 3 represent four possible actions.
    The value of a key is a list of tuples associated with the action. The tuple is
    in the format of (transition probability, new state, reward, is terminal state
    or not). For example, if the agent intends to take action 1 (down) from state
    6, it will move to state 5 (H) with 33.33% probability and receive 0 reward and
    the episode will end consequently; it will move to state 10 with 33.33% probability
    and receive 0 reward; it will move to state 7 (H) with 33.33% probability and
    receive 0 reward and terminate the episode.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 返回字典的键0、1、2、3表示四个可能的动作。每个键的值是一个与动作相关联的元组列表。元组的格式是（转移概率， 新状态，奖励，是否为终止状态）。例如，如果智能体打算从状态6采取动作1（向下），它将以33.33%的概率移动到状态5（H），并获得0奖励，随后游戏结束；它还将以33.33%的概率移动到状态10，获得0奖励；它还将以33.33%的概率移动到状态7（H），获得0奖励并终止游戏。
- en: We’ve experimented with the random policy in this section, and we only succeeded
    1.6% of the time. But this gets you ready for the next section where we will find
    the optimal policy using the value-based dynamic programming algorithm, called
    the **value iteration algorithm**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经尝试了随机策略，结果只有1.6%的成功率。但这为下一节做好了准备，在那里我们将使用基于值的动态规划算法——**值迭代算法**，来找到最优策略。
- en: Solving FrozenLake with the value iteration algorithm
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用值迭代算法解决FrozenLake问题
- en: Value iteration is an iterative algorithm. It starts with random policy values
    *V*, and then iteratively updates the values based on the **Bellman optimality
    equation** ([https://en.wikipedia.org/wiki/Bellman_equation](https://en.wikipedia.org/wiki/Bellman_equation))
    until the values converge.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代是一种迭代算法。它从随机的策略值*V*开始，然后根据**贝尔曼最优性方程**（[https://en.wikipedia.org/wiki/Bellman_equation](https://en.wikipedia.org/wiki/Bellman_equation)）反复更新值，直到这些值收敛。
- en: It is usually difficult for the values to completely converge. Hence, there
    are two criteria of convergence. One is passing a fixed number of iterations,
    such as 1,000 or 10,000\. Another one is specifying a threshold (such as 0.0001,
    or 0.00001) and we terminate the process if the changes of all values are less
    than the threshold.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 值完全收敛通常是困难的。因此，存在两种收敛标准。一个是设定固定的迭代次数，比如1,000次或10,000次。另一个是设定一个阈值（比如0.0001或0.00001），当所有值的变化小于该阈值时，我们终止过程。
- en: 'Importantly, in each iteration, instead of taking the expectation (average)
    of values across all actions, it picks the action that maximizes the policy values.
    The iteration process can be expressed as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，在每次迭代中，算法并不是对所有动作的值取期望（平均值），而是选择最大化策略值的动作。迭代过程可以表示如下：
- en: '![](img/B21047_15_012.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_012.png)'
- en: This is the representation of the Bellman equation for the state-value function
    V(s). Here, ![](img/B21047_15_013.png) is the optimal value function; ![](img/B21047_15_014.png)
    denotes the transition probability of moving to state ![](img/B21047_15_015.png)
    from state *s* by taking action *a*; and ![](img/B21047_15_016.png) is the reward
    provided in state ![](img/B21047_15_017.png) by taking action *a*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是贝尔曼方程在状态值函数V(s)中的表示。这里，![](img/B21047_15_013.png)是最优值函数；![](img/B21047_15_014.png)表示从状态*s*采取行动*a*后，转移到状态![](img/B21047_15_015.png)的转移概率；而![](img/B21047_15_016.png)是在状态![](img/B21047_15_017.png)下，通过采取行动*a*所获得的奖励。
- en: 'Once we obtain the optimal values, we can easily compute the optimal policy
    accordingly:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得最优值，就可以相应地轻松计算出最优策略：
- en: '![](img/B21047_15_018.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_018.png)'
- en: 'Let’s solve the FrozenLake environment using the value iteration algorithm
    as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用值迭代算法来求解FrozenLake环境，如下所示：
- en: 'First we set `0.99` as the discount factor, and `0.0001` as the convergence
    threshold:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们设置`0.99`为折扣因子，`0.0001`为收敛阈值：
- en: '[PRE12]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We develop the value iteration algorithm, which computes the optimal values:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开发了值迭代算法，该算法计算最优值：
- en: '[PRE13]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `value_iteration` function does the following tasks:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`value_iteration`函数执行以下任务：'
- en: Starts with policy values as all 0s
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始时，策略值设置为全0
- en: 'Updating the values based on the Bellman optimality equation:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于贝尔曼最优性方程更新值：
- en: '![](img/B21047_15_019.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_019.png)'
- en: Computing the maximal change of the values across all states
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有状态之间值的最大变化
- en: Continuing to update the values if the maximal change is greater than the convergence
    threshold
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果最大变化大于收敛阈值，则继续更新值
- en: Otherwise, terminating the iteration process and returning the last values as
    the optimal values
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，终止迭代过程，并将最后的值作为最优值返回
- en: 'We apply the algorithm to solve the FrozenLake environment along with the specified
    parameters:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用该算法解决FrozenLake环境，并使用指定的参数：
- en: '[PRE14]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Take a look at the resulting optimal values:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下结果中的最优值：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Since we have the optimal values, we can extract the optimal policy from the
    values. We develop the following function to do this:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们已经得到了最优值，可以从这些值中提取最优策略。我们开发了以下函数来实现这一点：
- en: '[PRE16]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we obtain the optimal policy based on the optimal values:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们根据最优值获得最优策略：
- en: '[PRE17]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Take a look at the resulting optimal policy:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下结果中的最优策略：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This means the optimal action in state 0 is 0 (left), 3 (up) in state 1, etc.
    This doesn’t seem very intuitive if you look at the grid. But remember that the
    grid is slippery and the agent can move in another direction than the desired
    one.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在状态0中最优动作是0（左），在状态1中是3（上），依此类推。如果你看这个网格，可能会觉得这不太直观。但记住，网格是滑的，智能体可能会朝着与期望方向不同的方向移动。
- en: 'If you doubt that it is the optimal policy, you can run 1,000 episodes with
    the policy and gauge how good it is by checking the average reward, as follows:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你怀疑这是否是最优策略，可以使用该策略运行1,000次实验，并通过检查平均奖励来评估其效果，具体如下：
- en: '[PRE19]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here, we define the `run_episode` function to simulate one episode. Then we
    print out the average reward over 1,000 episodes:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们定义了`run_episode`函数来模拟一次实验。然后，我们打印出1,000次实验的平均奖励：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Value iteration is guaranteed to converge to the optimal value function for
    a finite environment with a finite state and action space. It provides a computationally
    efficient method for solving for the optimal policy in RL problems, especially
    when the dynamics of the environment are known. Under the optimal policy computed
    by the value iteration algorithm, the agent in FrozenLake reaches the goal tile
    74% of the time.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 值迭代算法保证在有限状态和动作空间的有限环境中收敛到最优值函数。它为解决强化学习问题中的最优策略提供了一种计算上高效的方法，尤其是当环境的动态已知时。在值迭代算法计算出的最优策略下，智能体在FrozenLake中74%的时间能够到达目标方块。
- en: '**Best practice**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: The discount factor is an important parameter in RL, especially for value-based
    models. A high factor (closer to 1) makes the agent prioritize long-term rewards,
    leading to more exploration, while a low factor (closer to 0) makes it focus on
    immediate rewards. Typical tuning strategies for the discount factor include grid
    search and random search. Both could be computationally expensive for large ranges.
    Adaptive tuning is another approach, where we dynamically adjust the factor during
    training. You can start with a medium value (such as 0.9). If the agent seems
    too focused on immediate rewards, converges fast, and ignores exploration, try
    increasing the discount factor. If the agent keeps exploring and never settles
    on a good policy, try decreasing the discount factor.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子是强化学习中的一个重要参数，尤其对于基于价值的模型来说。较高的因子（接近1）使智能体优先考虑长期奖励，导致更多的探索，而较低的因子（接近0）则使其关注即时奖励。折扣因子的典型调优策略包括网格搜索和随机搜索。这两者在大范围内可能计算开销较大。自适应调优是另一种方法，其中我们在训练过程中动态调整折扣因子。你可以从一个中等的值（例如0.9）开始。如果智能体似乎过于关注即时奖励、快速收敛并忽视探索，尝试增加折扣因子。如果智能体不断探索而无法确定良好的策略，尝试减少折扣因子。
- en: Can we do something similar with the policy-based approach? Let’s see in the
    next section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否通过基于策略的方法做类似的事情？我们将在下一节中看到。
- en: Solving FrozenLake with the policy iteration algorithm
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用策略迭代算法解决FrozenLake问题
- en: The **policy iteration** algorithm has two components, policy evaluation and
    policy improvement. Similar to value iteration, it starts with an arbitrary policy
    and follows with a bunch of iterations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略迭代**算法有两个组成部分：策略评估和策略改进。与价值迭代类似，它从一个任意的策略开始，并进行多次迭代。'
- en: 'In the policy evaluation step in each iteration, we first compute the values
    of the latest policy, based on the **Bellman expectation equation**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代的策略评估步骤中，我们首先计算最新策略的值，基于**贝尔曼期望方程**：
- en: '![](img/B21047_15_020.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_020.png)'
- en: 'In the policy improvement step, we derive an improved policy based on the latest
    policy values, again based on the Bellman optimality equation:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略改进步骤中，我们基于最新的策略值推导出改进后的策略，依然基于**贝尔曼最优性方程**：
- en: '![](img/B21047_15_021.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_021.png)'
- en: These two steps repeat until the policy converges. At convergence, the latest
    policy and its value are the optimal policy and the optimal value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这两步会重复直到策略收敛。收敛时，最新的策略及其价值即为最优策略和最优价值。
- en: 'Let’s develop the policy iteration algorithm and use it to solve the FrozenLake
    environment as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开发策略迭代算法，并使用它来解决FrozenLake环境，如下所示：
- en: 'We start with the `policy_evaluation` function that computes the values of
    a given policy:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`policy_evaluation`函数开始，计算给定策略的值：
- en: '[PRE21]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The function does the following tasks:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数执行以下任务：
- en: Initializing the policy values with all 0s
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用全0初始化策略值
- en: Updating the values based on the Bellman expectation equation
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于贝尔曼期望方程更新值
- en: Computing the maximal change of the values across all states
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算所有状态下值的最大变化
- en: If the maximal change is greater than the threshold, it keeps updating the values
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果最大变化大于阈值，它会继续更新值
- en: Otherwise, it terminates the evaluation process and returns the latest values
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，它终止评估过程并返回最新的值
- en: 'Next, we develop the second component, the policy improvement, in the following
    function:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在以下函数中开发第二个组件，即策略改进：
- en: '[PRE22]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It derives a new and better policy from the input policy values based on the
    Bellman optimality equation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于贝尔曼最优性方程从输入的策略值中推导出一个新的、更好的策略。
- en: 'With both components ready, we now develop the whole policy iteration algorithm:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这两个组件，我们现在开发整个策略迭代算法：
- en: '[PRE23]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This function does the following tasks:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数执行以下任务：
- en: Initializing a random policy
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个随机策略
- en: Performing policy evaluation to update the policy values
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行策略评估以更新策略值
- en: Performing policy improvement to generate a new policy
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行策略改进以生成新的策略
- en: If the new policy is different from the old one, it updates the policy and runs
    another iteration of policy evaluation and improvement
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果新策略与旧策略不同，它会更新策略，并运行另一次策略评估和改进迭代
- en: Otherwise, it terminates the iteration process and returns the latest policy
    and its values
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，它终止迭代过程并返回最新的策略及其值
- en: 'Next, we use policy iteration to solve the FrozenLake environment:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用策略迭代来解决FrozenLake环境：
- en: '[PRE24]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we display the optimal policy and its values:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们显示最优策略及其值：
- en: '[PRE25]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We got the same results as the value iteration algorithm.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了与价值迭代算法相同的结果。
- en: '**Best practice**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Policy-based methods rely on estimating the gradient of the expected reward
    with respect to the policy parameters. In practice, we use techniques like REINFORCE,
    which uses simple Monte Carlo estimates, and **Proximal Policy Optimization**
    (**PPO**) employing stable gradient estimation. You can read more here: [https://professional.mit.edu/course-catalog/reinforcement-learning](https://professional.mit.edu/course-catalog/reinforcement-learning)
    (*Chapter 8*, *Policy Gradient Methods*).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的方法依赖于估计期望奖励相对于策略参数的梯度。实际上，我们使用像REINFORCE这样的技术，它使用简单的蒙特卡洛估计，以及**近端策略优化**（**PPO**）采用稳定的梯度估计。你可以在这里阅读更多内容：[https://professional.mit.edu/course-catalog/reinforcement-learning](https://professional.mit.edu/course-catalog/reinforcement-learning)（*第8章*，*策略梯度方法*）。
- en: 'We have just solved the FrozenLake environment with the policy iteration algorithm.
    You may wonder how to choose between the value iteration and policy iteration
    algorithms. Take a look at the following table:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚通过策略迭代算法解决了FrozenLake环境。你可能会想知道如何在价值迭代和策略迭代算法之间做选择。请查看以下表格：
- en: '![](img/B21047_15_04.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_04.png)'
- en: 'Table 15.4: Choosing between the policy iteration and value iteration algorithms'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.4：选择策略迭代和价值迭代算法之间的区别
- en: We solved a reinforcement learning problem using dynamic programming methods.
    They require a fully known transition matrix and reward matrix of an environment.
    And they have limited scalability for environments with many states. In the next
    section, we will continue our learning journey with the Monte Carlo method, which
    has no requirement of prior knowledge of the environment and is much more scalable.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过动态规划方法解决了强化学习问题。这些方法需要环境的完全已知的转移矩阵和奖励矩阵，而且对于具有大量状态的环境，其可扩展性有限。在下一部分，我们将继续学习蒙特卡洛方法，它不需要环境的先验知识，且具有更好的可扩展性。
- en: Performing Monte Carlo learning
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行蒙特卡洛学习
- en: '**Monte Carlo** (**MC**)-based reinforcement learning is a **model-free** approach,
    which means it doesn’t need a known transition matrix and reward matrix. In this
    section, you will learn about MC policy evaluation on Gymnasium’s Blackjack environment,
    and solve the environment with MC control algorithms. Blackjack is a typical environment
    with an unknown transition matrix. Let’s first simulate the Blackjack environment.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**蒙特卡洛**（**MC**）强化学习是一种**无模型**的方法，这意味着它不需要已知的转移矩阵和奖励矩阵。在这一部分，你将了解在Gymnasium的21点环境中进行蒙特卡洛策略评估，并通过蒙特卡洛控制算法解决该环境。21点是一个典型的具有未知转移矩阵的环境。首先让我们模拟21点环境。'
- en: Simulating the Blackjack environment
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟21点环境
- en: 'Blackjack is a popular card game. The game has the following rules:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 21点是一款流行的卡片游戏，游戏规则如下：
- en: The player competes against a dealer and wins if the total value of their cards
    is higher than the dealer’s and doesn’t exceed 21.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家与庄家对战，若玩家手中卡片的总值高于庄家的且不超过21点，则玩家获胜。
- en: Cards from 2 to 10 have values from 2 to 10.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2到10的牌值为2到10。
- en: Cards J, K, and Q have a value of 10.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J、K和Q牌的价值为10。
- en: The value of an ace can be either 1 or 11 (called a “usable” ace).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张王牌的价值可以是1或11（称为“可用”王牌）。
- en: At the beginning, both parties are given two random cards, but only one of the
    dealer’s cards is revealed to the player. The player can request additional cards
    (called **hit**) or stop having any more cards (called **stick**). Before the
    player calls stick, the player will lose if the sum of their cards exceeds 21
    (called **bust**). After the player sticks, the dealer keeps drawing cards until
    the sum of cards reaches 17\. If the sum of the dealer’s cards exceeds 21, the
    player will win. If neither of the two parties busts, the one with higher points
    will win or it may be a draw.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始时，双方各发两张随机牌，但庄家的牌只有一张对玩家公开。玩家可以请求更多的卡片（称为**要牌**）或停止继续要牌（称为**停牌**）。在玩家决定停牌前，如果他们的卡片总和超过21点（称为**爆牌**），则玩家会输。停牌后，庄家会继续抽牌，直到其卡片总和达到17点。如果庄家卡片的总和超过21点，玩家获胜。如果双方都没有爆牌，则点数较高的一方获胜，或者可能平局。
- en: 'The Blackjack environment ([https://gymnasium.farama.org/environments/toy_text/blackjack/](https://gymnasium.farama.org/environments/toy_text/blackjack/))
    in Gymnasium is formulated as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Gymnasium中的21点环境（[https://gymnasium.farama.org/environments/toy_text/blackjack/](https://gymnasium.farama.org/environments/toy_text/blackjack/)）的公式如下：
- en: An episode of the environment starts with two cards for each party, and only
    one from the dealer’s cards is observed.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境的回合开始时，每方各发两张牌，且仅观察到庄家的其中一张牌。
- en: An episode ends if there is a win or draw.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果出现胜利或平局，回合结束。
- en: The final reward of an episode is +1 if the player wins, -1 if the player loses,
    or 0 if there is a draw.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果玩家赢了，本回合的最终奖励为 +1；如果玩家输了，奖励为 -1；如果平局，奖励为 0。
- en: In each round, the player can take any of the two actions, hit (1) or stick
    (0).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一轮中，玩家可以选择两种动作之一，摸牌（1）或停牌（0）。
- en: 'Now let’s simulate the Blackjack environment and explore its states and actions:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们模拟 Blackjack 环境，并探索它的状态和动作：
- en: 'First, create a `Blackjack` instance:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个 `Blackjack` 实例：
- en: '[PRE26]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Reset the environment:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境：
- en: '[PRE27]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It returns the initial state (a 3-dimensional vector):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回初始状态（一个三维向量）：
- en: Player’s current points (`11` in this example)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩家当前的点数（此例中为 `11`）
- en: The points of the dealer’s revealed card (`10` in this example)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赌场公开牌的点数（此例中为 `10`）
- en: Having a usable ace or not (`False` in this example)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有可用的 A 牌（此例中为 `False`）
- en: The usable ace variable is `True` only if the player has an ace that can be
    counted as 11 without causing a bust. If the player doesn’t have an ace, or has
    an ace but it busts, this state variable will become `False`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当玩家拥有一张可以算作 11 点而不会爆掉的 A 牌时，`usable ace` 变量才为 `True`。如果玩家没有 A 牌，或者虽然有 A 牌但已经爆掉，则该状态变量会变为
    `False`。
- en: For another state example `(18, 6, True)`, it means that the player has an ace
    counted as 11 and a 7, and that the dealer’s revealed card is value 6.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以另一个状态示例 `(18, 6, True)` 来看，表示玩家拥有一张算作 11 点的 A 牌和一张 7 点，而庄家的公开牌是 6 点。
- en: 'Let’s now take some actions to see how the environment works. First, we take
    a hit action since we only have 11 points:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们采取一些动作，看看环境是如何运作的。首先，由于我们只有 11 点，我们选择摸牌动作：
- en: '[PRE28]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It returns a state `(12, 10, False)`, a 0 reward, and the episode not being
    done (meaning `False`).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个状态 `(12, 10, False)`，奖励为 0，并且该回合未结束（即 `False`）。
- en: 'Let’s take another hit since we only have 12 points:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们只有 12 点，让我们再摸一张牌：
- en: '[PRE29]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We have 13 points and think it is good enough. Then we stop drawing cards by
    taking the stick action (0):'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有 13 点，认为已经足够了。于是我们通过停牌动作（0）停止抽牌：
- en: '[PRE30]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The dealer gets some cards and beats the player. So the player loses and gets
    a -1 reward. The episode ends.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 赌场获得一些牌并击败了玩家。所以玩家输了，获得了 -1 的奖励。回合结束。
- en: Feel free to play around with the Blackjack environment. Once you feel comfortable
    with the environment, you can move on to the next section, MC policy evaluation
    on a simple policy.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以在 Blackjack 环境中进行尝试。一旦你熟悉了环境，就可以进入下一节，进行简单策略上的 MC 策略评估。
- en: Performing Monte Carlo policy evaluation
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行蒙特卡罗策略评估
- en: In the previous section, we applied dynamic programming to perform policy evaluation,
    which is the value function of a policy. However, it won’t work in most real-life
    situations where the transition matrix is not known beforehand. In this case,
    we can evaluate the value function using the MC method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们应用了动态规划来进行策略评估，也就是策略的价值函数。然而，在大多数现实情况中，由于转移矩阵事先不可知，这种方法无法使用。在这种情况下，我们可以使用
    MC 方法来评估价值函数。
- en: To estimate the value function, the MC method uses empirical mean return instead
    of expected return (as in dynamic programming). There are two approaches to compute
    the empirical mean return. One is first-visit, which averages returns **only**
    for the **first occurrence** of a state *s* among all episodes. Another one is
    every-visit, which averages returns for **every occurrence** of a state *s* among
    all episodes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计价值函数，MC 方法使用经验均值回报，而不是期望回报（如在动态规划中）。有两种方法来计算经验均值回报。一个是首次访问方法，它仅对所有回合中状态
    *s* 的**首次出现**进行回报平均。另一个是每次访问方法，它对所有回合中状态 *s* 的**每次出现**进行回报平均。
- en: Obviously, the first-visit approach has a lot less computation and is therefore
    more commonly used. I will only cover the first-visit approach in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，首次访问方法计算量较小，因此使用得更为广泛。本章只介绍首次访问方法。
- en: 'In this section, we experiment with a simple policy where we keep adding new
    cards until the total value reaches 18 (or 19, or 20 if you like). We perform
    first-visit MC evaluation on the simple policy as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实验了一种简单策略，直到总点数达到 18（或 19，或 20，如果你愿意的话）才停止抽牌。我们对这种简单策略进行了首次访问 MC 评估，如下所示：
- en: 'We first need to define a function that simulates a Blackjack episode under
    the simple policy:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要定义一个函数，模拟在简单策略下的 Blackjack 回合：
- en: '[PRE31]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In each round of an episode, the agent takes a hit if the current score is less
    than `hold_score` or a stick otherwise.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在每轮回合中，如果当前得分小于`hold_score`，代理将选择“补牌”，否则选择“停牌”。
- en: 'In the MC settings, we need to keep track of states and rewards over all steps.
    And in first-visit value evaluation, we average returns only for the first occurrence
    of a state among all episodes. We define a function that evaluates the simple
    Blackjack policy with first-visit MC:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在MC设置中，我们需要跟踪所有步骤中的状态和奖励。在首次访问值评估中，我们只对所有回合中状态首次出现时的回报进行平均。我们定义一个函数来评估简单的Blackjack策略，使用首次访问MC：
- en: '[PRE32]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The function performs the following tasks:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数执行以下任务：
- en: Running `n_episode` episodes under the simple Blackjack policy with the `run_episode`
    function
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`run_episode`函数在简单Blackjack策略下运行`n_episode`个回合
- en: For each episode, computing the `G` returns for the first visit of each state
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个回合，计算每个状态首次访问时的`G`回报
- en: For each state, obtaining the value by averaging its first returns from all
    episodes
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个状态，通过从所有回合中对其首次返回值进行平均来获得该状态的值
- en: Returning the resulting values
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回结果值
- en: Note that here we ignore states where the player busts, since we know their
    values are `-1`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里我们忽略玩家爆牌的状态，因为我们知道这些状态的值为`-1`。
- en: 'We specify the `hold_score` as `18` and the discount rate as `1` as a Blackjack
    episode is short enough, and will simulate 500,000 episodes:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`hold_score`指定为`18`，折扣率指定为`1`，因为Blackjack回合较短，并且将模拟500,000个回合：
- en: '[PRE33]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we plug in all variables to perform MC first-visit evaluation:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将所有变量代入，以执行MC首次访问评估：
- en: '[PRE34]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then print the resulting values:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们打印出结果值：
- en: '[PRE35]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We have just computed the values for all possible 280 states:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚计算了所有可能的280个状态的值：
- en: '[PRE36]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We have just experienced computing the values of 280 states under a simple policy
    in the Blackjack environment using the MC method. The transition matrix of the
    Blackjack environment is not known beforehand. Moreover, obtaining the transition
    matrix (size 280 * 280) will be extremely costly if we go with the dynamic programming
    approach. In the MC-based solution, we just need to simulate a bunch of episodes
    and compute the empirical average values. In a similar manner, we will search
    for the optimal policy in the next section.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在Blackjack环境下使用MC方法计算了280个状态在简单策略下的值。Blackjack环境的转移矩阵事先是未知的。此外，如果我们采用动态规划方法，获取转移矩阵（大小为280
    * 280）将非常昂贵。在基于MC的解决方案中，我们只需要模拟一系列的回合并计算经验平均值。以类似的方式，我们将在下一节中搜索最优策略。
- en: Performing on-policy Monte Carlo control
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行在线蒙特卡洛控制
- en: '**MC control** is used to find the optimal policy for environments with unknown
    transition matrices. There are two types of MC control, on-policy and off-policy.
    In the **on-policy approach**, we execute the policy and evaluate and improve
    it iteratively; whereas in the off-policy approach, we train the optimal policy
    using data generated by another policy.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**MC控制**用于为转移矩阵未知的环境寻找最优策略。MC控制有两种类型，在线策略和离线策略。在**在线策略方法**中，我们执行策略并迭代地评估和改进它；而在离线策略方法中，我们使用由其他策略生成的数据训练最优策略。'
- en: 'In this section, we focus on the on-policy approach. The way it works is very
    similar to the policy iteration method. It iterates between the following two
    phases, evaluation and improvement, until convergence:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们专注于在线策略方法。其工作原理与策略迭代方法非常相似。它在以下两个阶段之间迭代：评估和改进，直到收敛：
- en: In the evaluation phase, instead of evaluating the state value, we evaluate
    the **action-value**, which is commonly called the **Q-value**. Q-value *Q*(*s*,
    *a*) is the value of a state-action pair (*s*, *a*) when taking the action *a*
    in state *s* under a given policy. The evaluation can be conducted in a first-visit
    or an every-visit manner.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估阶段，我们不再评估状态值，而是评估**动作值**，通常称为**Q值**。Q值 *Q*(*s*, *a*)是状态-动作对(*s*, *a*)在给定策略下，采取动作*a*时的值。评估可以采用首次访问或每次访问的方式进行。
- en: 'In the improvement phase, we update the policy by assigning the optimal action
    in each state:'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在改进阶段，我们通过在每个状态中分配最优动作来更新策略：
- en: '![](img/B21047_15_022.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_022.png)'
- en: 'Let’s now search for the optimal Blackjack policy with on-policy MC control
    by following the steps below:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过遵循以下步骤，使用在线MC控制搜索最优的Blackjack策略：
- en: 'We start by developing a function that executes an episode by taking the best
    actions under the given Q-values:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先开发一个函数，通过在给定的Q值下执行最佳动作来完成一局游戏：
- en: '[PRE37]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This serves as the improvement phase. Specifically, it does the following tasks:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这作为改进阶段。具体来说，它执行以下任务：
- en: Initializing an episode
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个情节
- en: Taking a random action as an exploring start
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采取随机行动作为开始探索
- en: After the first action, taking actions based on the given Q-value table, that
    is ![](img/B21047_15_023.png)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一次行动后，根据给定的Q值表采取行动，即 ![](img/B21047_15_023.png)
- en: Storing the states, actions, and rewards for all steps in the episode, which
    will be used for evaluation
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储每个情节中的状态、动作和奖励，这些将用于评估
- en: 'Next, we develop the on-policy MC control algorithm:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开发了基于策略的蒙特卡罗控制算法：
- en: '[PRE38]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This function does the following tasks:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数执行以下任务：
- en: Randomly initializing the Q-values
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机初始化Q值
- en: Running `n_episode` episodes
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`n_episode`个情节
- en: For each episode, performing policy improvement and obtaining the training data;
    performing first-visit policy evaluation on the resulting states, actions, and
    rewards; and updating the Q-values
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个情节，执行策略改进并获取训练数据；对生成的状态、动作和奖励执行首次访问策略评估；并更新Q值
- en: In the end, finalizing the optimal Q-values and the optimal policy
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，确定最优的Q值和最优策略
- en: 'Now that the MC control function is ready, we compute the optimal policy:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在MC控制函数已经准备好，我们计算最优策略：
- en: '[PRE39]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Take a look at the optimal policy:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下最优策略：
- en: '[PRE40]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You may wonder if this optimal policy is really optimal and better than the
    previous simple policy (hold at 18 points). Let’s simulate 100,000 Blackjack episodes
    under the optimal policy and the simple policy respectively:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道这个最优策略是否真的是最优的，并且比之前的简单策略（在18点时停止）更好。让我们分别在最优策略和简单策略下模拟100,000个21点情节：
- en: 'We start with the function that simulates an episode under the simple policy:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从模拟简单策略下的一个情节的函数开始：
- en: '[PRE41]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we work on the simulation function under the optimal policy:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在最优策略下工作的模拟函数：
- en: '[PRE42]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We then run 100,000 episodes for both policies and keep track of their winning
    times:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们运行了100,000个情节，分别记录它们的胜利次数：
- en: '[PRE43]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We print out the results as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式打印结果：
- en: '[PRE44]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Playing under the optimal policy has a 43% chance of winning, while playing
    under the simple policy has only a 40% chance.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在最优策略下玩的胜率为43%，而在简单策略下只有40%的胜率。
- en: In this section, we solved the Blackjack environment with a model-free algorithm,
    MC learning. In MC learning, the Q-values are updated until the end of an episode.
    This could be problematic for long processes. In the next section, we will talk
    about Q-learning, which updates the Q-values for every step in an episode. You
    will see how it increases learning efficiency.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们使用无模型算法MC学习解决了21点环境。在MC学习中，Q值会在情节结束时进行更新。这可能对长时间过程有问题。在接下来的部分中，我们将讨论Q-learning，它会在情节中的每一步更新Q值。你将看到它如何提高学习效率。
- en: Solving the Blackjack problem with the Q-learning algorithm
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Q-learning算法解决21点问题
- en: Q-learning is also a model-free learning algorithm. It updates the Q-function
    for every step in an episode. We will demonstrate how Q-learning is used to solve
    the Blackjack environment.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning也是一种无模型学习算法。它在情节中的每一步更新Q函数。我们将展示如何使用Q-learning来解决21点环境。
- en: Introducing the Q-learning algorithm
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入Q-learning算法
- en: '**Q-learning** is an **off-policy** learning algorithm that optimizes the Q-values
    based on data generated by a behavior policy. The behavior policy is a greedy
    policy where it takes actions that achieve the highest returns for given states.
    The behavior policy generates learning data and the target policy (the policy
    we attempt to optimize) updates the Q-values based on the following equation:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q-learning**是一种**离策略**学习算法，它基于由行为策略生成的数据优化Q值。行为策略是一个贪婪策略，它采取在给定状态下获得最高回报的动作。行为策略生成学习数据，目标策略（我们尝试优化的策略）根据以下方程更新Q值：'
- en: '![](img/B21047_15_024.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_024.png)'
- en: Here, ![](img/B21047_15_025.png) is the resulting state after taking action
    *a* from state *s* and *r* is the associated reward. ![](img/B21047_15_026.png)
    means that the behavior policy generates the highest Q-value given state ![](img/B21047_15_027.png).
    Hyperparameters ![](img/B21047_15_028.png) and ![](img/B21047_15_029.png) are
    the learning rate and discount factor respectively. The Q-learning equation updates
    the Q-value (estimated future reward) of a state-action pair based on the current
    Q-value, the immediate reward, and the potential future rewards the agent can
    expect by taking the best possible action in the next state.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B21047_15_025.png) 是从状态 *s* 执行动作 *a* 后得到的结果状态，*r* 是相关奖励。![](img/B21047_15_026.png)
    表示行为策略在给定状态 ![](img/B21047_15_027.png) 下生成的最高Q值。超参数 ![](img/B21047_15_028.png)
    和 ![](img/B21047_15_029.png) 分别是学习率和折扣因子。Q学习方程基于当前的Q值、即时奖励和智能体通过在下一个状态中采取最佳动作所能期望的未来潜在奖励，来更新一个状态-动作对的Q值（估计的未来奖励）。
- en: Learning from the experience generated by another policy enables Q-learning
    to optimize its Q-values in every single step in an episode. We gain the information
    from a greedy policy and use this information to update the target values right
    away.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个策略生成的经验中学习，使得Q学习能够在每个回合的每一步优化其Q值。我们从贪心策略中获得信息，并立即使用这些信息更新目标值。
- en: 'One more thing to note is that the target policy is epsilon-greedy, meaning
    it takes a random action with a probability of ![](img/B21047_15_030.png) (value
    from 0 to 1) and takes a greedy action with a probability of ![](img/B21047_15_031.png)
    . The epsilon-greedy policy combines **exploitation** and **exploration**: it
    exploits the best action while exploring different actions.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一点需要注意的是，目标策略是epsilon贪心策略，这意味着它以概率 ![](img/B21047_15_030.png)（值从0到1）采取随机动作，并以概率
    ![](img/B21047_15_031.png) 采取贪心动作。epsilon贪心策略结合了**利用**和**探索**：它在探索不同动作的同时，利用最佳动作。
- en: Developing the Q-learning algorithm
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发Q学习算法
- en: 'Now it is time to develop the Q-learning algorithm to solve the Blackjack environment:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开发Q学习算法来解决Blackjack环境了：
- en: 'We start with defining the epsilon-greedy policy:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从定义epsilon贪心策略开始：
- en: '[PRE45]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Given ![](img/B21047_15_032.png) possible actions, each action is taken with
    a probability ![](img/B21047_15_033.png), and the action with the highest state-action
    value is chosen with an additional probability ![](img/B21047_15_034.png).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 ![](img/B21047_15_032.png) 可能的动作，每个动作的选择概率为 ![](img/B21047_15_033.png)，具有最高状态-动作值的动作会以额外的概率
    ![](img/B21047_15_034.png) 被选择。
- en: 'We will start with a large exploration factor ![](img/B21047_15_035.png) and
    reduce it over time until it reaches `0.1`. We define the starting and final ![](img/B21047_15_036.png)
    as follows:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从一个较大的探索因子开始 ![](img/B21047_15_035.png)，随着时间的推移逐渐减少，直到其值达到`0.1`。我们将起始值和最终值
    ![](img/B21047_15_036.png) 定义如下：
- en: '[PRE46]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we implement the Q-learning algorithm:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现Q学习算法：
- en: '[PRE47]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We initialize the action-value function Q and calculate the epsilon decay rate
    for the epsilon-greedy exploration strategy. For each episode, we let the agent
    take actions following the epsilon-greedy policy, and update the Q function for
    each step based on the off-policy learning equation. The exploration factor also
    decreases over time. We run `n_episode` episodes and finally extract the optimal
    policy from the learned action-value function Q by selecting the action with the
    maximum value for each state.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化动作值函数Q，并计算epsilon贪心探索策略的epsilon衰减率。在每个回合中，我们让智能体按照epsilon贪心策略采取行动，并根据离策略学习方程在每一步更新Q函数。探索因子也会随着时间推移而减少。我们运行`n_episode`个回合，最后通过选择每个状态下的最大Q值的动作，从学到的动作值函数Q中提取最优策略。
- en: 'We then initiate a variable to store the performance of each of 10,000 episodes,
    measured by the reward:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化一个变量来存储每个10,000个回合的表现，通过奖励来衡量：
- en: '[PRE48]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we perform Q-learning to obtain the optimal policy for the Blackjack
    problem with the following hyperparameters:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行Q学习以获得Blackjack问题的最优策略，使用以下超参数：
- en: '[PRE49]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Here, discount rate ![](img/B21047_15_037.png) and learning rate ![](img/B21047_15_038.png)
    for more exploration.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，折扣率 ![](img/B21047_15_037.png) 和学习率 ![](img/B21047_15_038.png) 用于更大的探索。
- en: 'After 10,000 episodes of learning, we plot the rolling average rewards over
    the episodes as follows:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在学习了10,000个回合后，我们绘制每个回合的滚动平均奖励，结果如下：
- en: '[PRE50]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下截图获取最终结果：
- en: '![](img/B21047_15_05.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_15_05.png)'
- en: 'Figure 15.5: Average reward over episodes'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5：每回合的平均奖励
- en: The average reward steadily rises throughout the training process, eventually
    converging. This indicates that the model becomes proficient in solving the problem
    after training.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 平均奖励在整个训练过程中稳步上升，最终收敛。这表明，模型在训练后已经能够熟练地解决问题。
- en: 'Finally, we simulate 100,000 episodes for the optimal policy we obtained using
    Q-learning and calculate the winning chance:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们模拟了 100,000 轮比赛，验证了我们使用 Q-learning 获得的最优策略，并计算了获胜机会：
- en: '[PRE51]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Playing under the optimal policy has a 42% chance of winning.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在最优策略下玩耍有 42% 的获胜机会。
- en: In this section, we solved the Blackjack problem with off-policy Q-learning.
    The algorithm optimizes the Q-values in every single step by learning from the
    experience generated by a greedy policy.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们通过离策略 Q-learning 解决了 Blackjack 问题。该算法通过从贪婪策略生成的经验中学习，在每一步优化 Q 值。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter commenced with configuring the working environment, followed by
    an examination of the core concepts of reinforcement learning, accompanied by
    practical examples. Subsequently, we delved into the FrozenLake environment, employing
    dynamic programming techniques such as value iteration and policy iteration to
    tackle it effectively. Monte Carlo learning was introduced for value estimation
    and control in the Blackjack environment. Finally, we implemented the Q-learning
    algorithm to address the same problem, providing a comprehensive overview of reinforcement
    learning techniques.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先配置了工作环境，然后探讨了强化学习的核心概念，并结合实际案例进行了讲解。接着，我们深入研究了 FrozenLake 环境，使用动态规划技术，如价值迭代和策略迭代，有效地解决了该问题。随后，我们在
    Blackjack 环境中引入了蒙特卡洛学习，用于价值估计和控制。最后，我们实现了 Q-learning 算法来解决同样的问题，全面回顾了强化学习技术。
- en: Exercises
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you try to solve the 8 * 8 FrozenLake environment with the value iteration
    or policy iteration algorithm?
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能尝试使用价值迭代或策略迭代算法解决 8 * 8 FrozenLake 环境吗？
- en: Can you implement the every-visit MC policy evaluation algorithm?
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能实现每次访问的 MC 策略评估算法吗？
- en: Join our book’s Discord space
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
- en: '![](img/New_Packt_Logo1.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/New_Packt_Logo1.png)'
- en: '[packt.com](https://www.packt.com)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[packt.com](https://www.packt.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，全面访问超过 7,000 本书籍和视频，以及行业领先的工具，帮助你规划个人发展并推进职业生涯。欲了解更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节省学习时间，更多时间进行编码，来自 4,000 多名行业专业人士的实用电子书和视频
- en: Improve your learning with Skill Plans built especially for you
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过为你量身定制的技能计划，提升你的学习效率
- en: Get a free eBook or video every month
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月免费获取一本电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，便于轻松获取重要信息
- en: Copy and paste, print, and bookmark content
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制、粘贴、打印并收藏内容
- en: At [www.packt.com](https://www.packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [www.packt.com](https://www.packt.com)，你还可以阅读一系列免费的技术文章，注册各种免费的电子邮件通讯，并享受
    Packt 书籍和电子书的独家折扣和优惠。
- en: Other Books You May Enjoy
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你可能喜欢的其他书籍
- en: 'If you enjoyed this book, you may be interested in these other books by Packt:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这本书，你可能对 Packt 出版的其他书籍感兴趣：
- en: '[![](img/978-1-80107-430-8.png)](https://www.packtpub.com/en-us/product/mastering-pytorch-9781801074308)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-80107-430-8.png)](https://www.packtpub.com/en-us/product/mastering-pytorch-9781801074308)'
- en: '**Mastering PyTorch – Second Edition**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**《精通 PyTorch - 第二版》**'
- en: Ashish Ranjan Jha
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: Ashish Ranjan Jha
- en: 'ISBN: 978-1-80107-430-8'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ISBN：978-1-80107-430-8
- en: Implement text, vision, and music generation models using PyTorch
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现文本、视觉和音乐生成模型
- en: Build a deep Q-network (DQN) model in PyTorch
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中构建深度 Q 网络（DQN）模型
- en: Deploy PyTorch models on mobile devices (Android and iOS)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在移动设备（Android 和 iOS）上部署 PyTorch 模型
- en: Become well versed in rapid prototyping using PyTorch with fastai
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟练掌握使用 fastai 在 PyTorch 中进行快速原型设计
- en: Perform neural architecture search effectively using AutoML
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AutoML 高效执行神经架构搜索
- en: Easily interpret machine learning models using Captum
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Captum轻松解释机器学习模型
- en: Design ResNets, LSTMs, and graph neural networks (GNNs)
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计ResNet、LSTM和图神经网络（GNN）
- en: Create language and vision transformer models using Hugging Face
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hugging Face创建语言和视觉转换模型
- en: '[![](img/978-1-83546-231-7.png)](https://www.packtpub.com/en-us/product/building-llm-powered-applications-9781835462317)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](img/978-1-83546-231-7.png)](https://www.packtpub.com/en-us/product/building-llm-powered-applications-9781835462317)'
- en: '**Building LLM Powered Applications**'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建LLM驱动的应用程序**'
- en: Valentina Alto
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 瓦伦蒂娜·阿尔托
- en: 'ISBN: 978-1-83546-231-7'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 'ISBN: 978-1-83546-231-7'
- en: Explore the core components of LLM architecture, including encoder-decoder blocks
    and embeddings
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索LLM架构的核心组件，包括编码器-解码器块和嵌入层
- en: Understand the unique features of LLMs like GPT-3.5/4, Llama 2, and Falcon LLM
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解GPT-3.5/4、Llama 2和Falcon LLM等LLM的独特特性
- en: Use AI orchestrators like LangChain, with Streamlit for the frontend
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AI协调器，如LangChain，结合Streamlit进行前端开发
- en: Get familiar with LLM components such as memory, prompts, and tools
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉LLM的组件，如内存、提示和工具
- en: Learn how to use non-parametric knowledge and vector databases
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用非参数知识和向量数据库
- en: Understand the implications of LFMs for AI research and industry applications
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LFMs对AI研究和行业应用的影响
- en: Customize your LLMs with fine tuning
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过微调定制你的LLM
- en: Learn about the ethical implications of LLM-powered applications
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解LLM驱动应用程序的伦理影响
- en: Packt is searching for authors like you
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Packt正在寻找像你这样的作者
- en: If you’re interested in becoming an author for Packt, please visit [authors.packtpub.com](https://authors.packtpub.com)
    and apply today. We have worked with thousands of developers and tech professionals,
    just like you, to help them share their insight with the global tech community.
    You can make a general application, apply for a specific hot topic that we are
    recruiting an author for, or submit your own idea.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣成为Packt的作者，请访问[authors.packtpub.com](https://authors.packtpub.com)并今天就申请。我们已经与成千上万的开发者和技术专家合作，帮助他们将见解分享给全球技术社区。你可以提交一个通用申请，申请我们正在招聘作者的特定热门话题，或者提交你自己的创意。
- en: Share your thoughts
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享你的想法
- en: Now you’ve finished *Python Machine Learning By Example - Fourth Edition*, we’d
    love to hear your thoughts! If you purchased the book from Amazon, please [click
    here to go straight to the Amazon review page](https://packt.link/r/1835085628)
    for this book and share your feedback or leave a review on the site that you purchased
    it from.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了*《Python机器学习实例 - 第四版》*，我们非常希望听到你的想法！如果你是通过亚马逊购买的这本书，请[点击这里直接前往亚马逊的评论页面](https://packt.link/r/1835085628)，分享你的反馈或在购买网站上留下评论。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论对我们和技术社区非常重要，能帮助我们确保提供优质的内容。
