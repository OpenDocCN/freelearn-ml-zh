- en: '*Chapter 1*: Foundational Concepts of Explainability Techniques'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：可解释性技术的基础概念'
- en: As more and more organizations have started to adopt **Artificial Intelligence**
    (**AI**) and **Machine Learning** (**ML**) for their critical business decision-making
    process, it becomes an immediate expectation to interpret and demystify **black-box
    algorithms** to increase their adoption. AI and ML are being increasingly utilized
    for determining our day-to-day experiences across multiple areas, such as banking,
    healthcare, education, recruitment, transport, and supply chain. But the integral
    role played by AI and ML models has led to the growing concern of business stakeholders
    and consumers about the lack of transparency and interpretability as these black-box
    algorithms are highly subjected to human bias; particularly for high-stake domains,
    such as healthcare, finance, legal, and other critical industrial operations,
    model explainability is a prerequisite.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的组织开始采用人工智能（AI）和机器学习（ML）来支持他们的关键业务决策过程，立即期望能够解释和揭示黑盒算法，以增加其采用率。AI 和 ML
    正在越来越多地被用于确定我们在多个领域的日常体验，如银行、医疗保健、教育、招聘、交通和供应链。但 AI 和 ML 模型所扮演的基石角色导致了商业利益相关者和消费者对透明度和可解释性的日益关注，因为这些黑盒算法高度受到人类偏见的影响；特别是在高风险领域，如医疗保健、金融、法律和其他关键工业操作，模型可解释性是一个先决条件。
- en: As the benefits of AI and ML can be significant, the question is, how can we
    increase its adoption despite the growing concerns? Can we even address these
    concerns and democratize the use of AI and ML? And how can we make AI more explainable
    for critical industrial applications in which black-box models are not trusted?
    Throughout this book, we will try to learn the answers to these questions and
    apply these concepts and ideas to solve practical problems!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能（AI）和机器学习（ML）的益处可能非常显著，问题在于，我们如何增加其采用率，尽管存在越来越多的担忧？我们甚至能否解决这些担忧，并使 AI
    和 ML 的使用民主化？以及我们如何使 AI 对关键工业应用更加可解释，在这些应用中，黑盒模型不受信任？在这本书中，我们将尝试找到这些问题的答案，并将这些概念和思想应用于解决实际问题！
- en: 'In this chapter, you will learn about the foundational concepts of **Explainable
    AI** (**XAI**) so that the terms and concepts used in future chapters are clear,
    and it will be easier to follow and implement some of the advanced explainability
    techniques discussed later in this book. This will give you the required theoretical
    knowledge needed to understand and implement the practical techniques discussed
    in later chapters. The chapter focuses on the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解可解释人工智能（XAI）的基础概念，以便未来章节中使用的术语和概念清晰易懂，这将使你更容易跟随和实施本书后面讨论的一些高级可解释性技术。这将为你提供理解和实施后面章节中讨论的实用技术所需的理论知识。本章重点介绍以下主要主题：
- en: Introduction to XAI
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XAI 简介
- en: Defining explanation methods and approaches
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义解释方法和途径
- en: Evaluating the quality of explainability methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估可解释性方法的质量
- en: Now, let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: Introduction to XAI
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XAI 简介
- en: '**XAI** is the most effective practice to ensure that AI and ML solutions are
    transparent, trustworthy, responsible, and ethical so that all regulatory requirements
    on algorithmic transparency, risk mitigation, and a fallback plan are addressed
    efficiently. AI and ML explainability techniques provide the necessary visibility
    into how these algorithms operate at every stage of their solution life cycle,
    allowing end users to understand *why* and *how* queries are related to the outcome
    of AI and ML models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**XAI** 是确保 AI 和 ML 解决方案透明、可信、负责任和道德的最有效实践，从而有效地解决算法透明度、风险缓解和后备计划的监管要求。AI 和
    ML 可解释性技术提供了必要的可见性，以了解这些算法在其解决方案生命周期中的每个阶段是如何运作的，使用户能够理解 AI 和 ML 模型的查询与结果之间的 *原因*
    和 *方式*。'
- en: Understanding the key terms
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解关键术语
- en: Usually, for ML models, for addressing the *how* questions, we use the term
    *interpretability*, and for addressing the *why* questions, we use the term *explainability*.
    In this book, the terms **model interpretability** and **model explainability**
    are interchangeably used. However, for providing *human-friendly* holistic explanations
    of the outcome of ML models, we will need to make ML algorithms both interpretable
    and explainable, thus allowing the end users to easily comprehend the decision-making
    process of these models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于机器学习模型，为了回答*如何*的问题，我们使用术语*可解释性*，为了回答*为什么*的问题，我们使用术语*可解释性*。在这本书中，术语**模型可解释性**和**模型可解释性**是互换使用的。然而，为了提供机器学习模型结果的*人性化*整体解释，我们需要使机器学习算法既可解释又可解释，从而使用户能够轻松理解这些模型的决策过程。
- en: In most scenarios, ML models are considered as black-boxes, where we feed in
    any training data and it is expected to predict on new, unseen data. Unlike conventional
    programming, where we program specific instructions, an ML model automatically
    tries to learn these instructions from the data. As illustrated in *Figure 1.1*,
    when we try to find out the rationale for the model prediction, we do not get
    enough information!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，机器学习模型被视为黑盒，我们输入任何训练数据，并期望它能够对新、未见过的数据进行预测。与传统的编程不同，在传统的编程中，我们编写特定的指令，而机器学习模型会自动尝试从数据中学习这些指令。如图*1.1*所示，当我们试图找出模型预测的依据时，我们得不到足够的信息！
- en: '![Figure 1.1 – Conventionally, black-box models do not provide any rationale
    behind predictions'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 – 传统上，黑盒模型不提供预测背后的任何依据'
- en: '](img/B18216_01_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_01.jpg)'
- en: Figure 1.1 – Conventionally, black-box models do not provide any rationale behind
    predictions
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 传统上，黑盒模型不提供预测背后的任何依据
- en: Now, let's understand the impact of incorrect predictions and inaccurate ML
    models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解错误预测和不准确的机器学习模型的影响。
- en: Consequences of poor predictions
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差异预测的后果
- en: Traditionally, all ML models were believed to be magical black-boxes that can
    automatically decipher interesting patterns and insights from the data and provide
    *silver bullet* outcomes! As compared to conventional rule-based computer programs,
    which are limited by the intelligence of the programmer, well-trained ML algorithms
    are considered to provide rich insights and accurate predictions even in complex
    situations. But the fact is, all ML models suffer from *bias*, which can be due
    to the **inductive bias** of the algorithm itself, or it can be due to the presence
    of bias in the dataset used for training the model. In practice, there can be
    other reasons, such as **data drift**, **concept drift**, and **overfitted** or
    **underfitted** models, for which model predictions can go wrong. As the famous
    British statistician George E.P. Box once said, *"All models are wrong, but some
    are useful"*; all statistical, scientific, and ML models can give incorrect outcomes
    if the initial assumptions of these methods are not consistent. Therefore, it
    is important for us to know why an ML model predicted a specific outcome, what
    can be done if it is wrong, and how the predictions can be improved.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，所有机器学习模型都被认为是神奇的黑色盒子，可以自动从数据中解析出有趣的模式和见解，并提供*银弹*般的解决方案！与受程序员智力限制的传统基于规则的计算机程序相比，经过良好训练的机器学习算法被认为即使在复杂情况下也能提供丰富的见解和准确的预测。但事实是，所有机器学习模型都存在*偏差*，这可能是由于算法本身的**归纳偏差**，也可能是由于用于训练模型的数据库中存在偏差。在实践中，还可能有其他原因，如**数据漂移**、**概念漂移**、**过拟合**或**欠拟合**的模型，这些原因可能导致模型预测出错。正如著名的英国统计学家乔治·E·P·博克斯曾经说过：“所有模型都是错误的，但有些是有用的”；所有统计的、科学的和机器学习模型都可能在初始假设不一致的情况下给出错误的结论。因此，了解为什么机器学习模型预测了特定的结果，如果它是错误的，应该怎么做，以及如何改进预测，对我们来说非常重要。
- en: '*Figure 1.2* illustrates a collection of news headlines highlighting the failure
    of AI algorithms towards producing fair and unbiased outcomes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.2*展示了新闻标题的集合，突出了人工智能算法在产生公平和无偏见结果方面的失败。'
- en: '![Figure 1.2 – Growing concern of bias and lack of fairness of ML models being
    reported frequently'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2 – 偏见和缺乏公平性的机器学习模型日益受到关注'
- en: '](img/B18216_01_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_02.jpg)'
- en: Figure 1.2 – Growing concern of bias and lack of fairness of ML models being
    reported frequently
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 偏见和缺乏公平性的机器学习模型日益受到关注
- en: Before completely agreeing with me on the necessity of model explainability,
    let me try to give some practical examples of low-stake and high-stake domains
    to understand the consequences of poor predictions. Weather forecasting is one
    of the classical forecasting problems that is extremely challenging (as it depends
    on multiple dynamic factors) where ML is extensively used, and the ability of
    ML algorithms to consider multiple parameters of different types makes it more
    efficient than standard statistical models to predict the weather. Despite having
    highly accurate forecast models, there are times when weather forecasting algorithms
    might miss the prediction of rainfall, even though it starts raining after a few
    minutes! But the consequences of such a poor prediction might not be so severe,
    and moreover, most people do not blindly rely on automated weather predictions,
    thus making weather forecasting a low-stake domain problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全同意我对模型可解释性必要性的观点之前，让我尝试给出一些低风险和高风险领域的实际例子，以了解不良预测的后果。天气预报是极具挑战性的经典预测问题之一（因为它依赖于多个动态因素），机器学习在这里得到了广泛的应用，机器学习算法考虑多种不同类型的参数的能力使其比标准统计模型在预测天气方面更加高效。尽管拥有高度准确的预测模型，但有时天气预报算法可能会错过降雨的预测，尽管几分钟后就开始下雨了！但这样的不良预测的后果可能并不严重，而且大多数人并不盲目依赖自动化的天气预报，这使得天气预报成为一个低风险领域问题。
- en: Similarly, for another low-stake domain, such as a content recommendation system,
    even if an ML algorithm provides an irrelevant recommendation, at the most, the
    end users might spend more time explicitly searching for relevant content. While
    the overall experience of the end user might be impacted, still, there is no severe
    loss of life or livelihood. Hence, the need for model explainability is not critical
    for low-stake domains, but providing explainability to model predictions does
    make the automated intelligent systems more trustworthy and reliable for end users,
    thus increasing AI adoption by enhancing the end user experience.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于另一个低风险领域，例如内容推荐系统，即使机器学习算法提供了不相关的推荐，最多用户可能只是会花更多时间明确地搜索相关内容。尽管最终用户的整体体验可能会受到影响，但仍然没有严重的生活或生计损失。因此，对于低风险领域，模型可解释性的需求并不关键，但为模型预测提供可解释性确实使自动智能系统对最终用户更加可靠和可信，从而通过提升用户体验来增加人工智能的采用率。
- en: Now, let me give an example where the consequences of poor predictions led to
    a severe loss of reputation and valuation of a company, impacting many lives!
    In November 2021, an American online real estate marketplace company called *Zillow*
    ([https://www.zillow.com/](https://www.zillow.com/)) reported having lost over
    40% of its stock value, and the home-buying division *Offers* lost over $300 million
    because of its failure to detect the unpredictability of their home price forecasting
    algorithms (for more information, please refer to the sources mentioned in the
    *References* section). In order to compensate for the loss, Zillow had to take
    drastic measures of cutting down its workforce and several thousands of families
    were impacted.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我举一个例子，其中不良预测的后果导致了公司声誉和估值的严重损失，影响了众多人的生活！2021年11月，一家名为*Zillow*的美国在线房地产交易平台公司报告称，其股价价值损失超过40%，其购房部门*Offers*因未能检测到其房价预测算法的不确定性而损失了超过3亿美元（更多详情请参阅*参考文献*部分）。为了弥补损失，Zillow不得不采取大幅裁员等措施，数千个家庭受到了影响。
- en: Similarly, multiple technology companies have been accused of using highly biased
    AI algorithms that could result in social unrest due to racial or gender discrimination.
    One such incident happened in 2015 when Google Photos made a massive racist blunder
    by automatically tagging an African-American couple as *Gorilla* (please look
    into the sources mentioned in the *References* section for more information).
    Although these blunders were unintentional and mostly due to biased datasets or
    non-generalized ML models, the consequences of these incidents can create massive
    social, economic, and political havoc. Bias in ML models in other high-stake domains,
    such as healthcare, credit lending, and recruitment, continuously reminds us of
    the need for more transparent solutions and XAI solutions on which end users can
    rely.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，多家科技公司被指控使用高度偏见的AI算法，这些算法可能导致因种族或性别歧视而引发的社会动荡。2015年发生的一起事件中，谷歌照片自动将一对非洲裔美国人夫妇标记为*大猩猩*，造成了大规模的种族主义错误（请参阅*参考文献*部分中提到的来源以获取更多信息）。尽管这些错误是无意的，并且大多是由于有偏见的数据集或未泛化的机器学习模型造成的，但这些事件的后果可能会造成巨大的社会、经济和政治混乱。在其他高风险领域，如医疗保健、信贷贷款和招聘中，机器学习模型中的偏见持续提醒我们，需要更多透明的解决方案和XAI解决方案，以便最终用户可以依赖。
- en: 'As illustrated in *Figure 1.3*, the consequences of poor predictions highlight
    the importance of XAI, which can provide early indicators to prevent loss of reputation,
    money, life, or livelihood due to the failure of AI algorithms:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图1.3*所示，不良预测的后果突出了XAI的重要性，它可以提供早期指标，以防止因AI算法失败而导致的声誉、金钱、生命或生计的损失：
- en: '![Figure 1.3 – Common consequences of poor prediction of ML models'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.3 – 机器学习模型预测不良的常见后果'
- en: '](img/B18216_01_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_01_03.jpg]'
- en: Figure 1.3 – Common consequences of poor prediction of ML models
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 机器学习模型预测不良的常见后果
- en: Now, let's try to summarize the need for model explainability in the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在下一节中尝试总结模型可解释性的需求。
- en: Summarizing the need for model explainability
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述模型可解释性的需求
- en: 'In the previous section, we learned that the consequences of poor predictions
    can impact many lives in high-risk domains, and even in low-risk domains the end
    user''s experience can be affected. *Samek and Binder''s* work in *Tutorial on
    Interpretable Machine Learning, MICCAI''18*, highlights the main necessity of
    model explainability. Let me try to summarize the key reasons why model explainability
    is essential:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解到不良预测的后果可能影响高风险领域的许多生命，甚至在低风险领域，最终用户的使用体验也可能受到影响。*Samek和Binder*在*MICCAI'18*的*可解释机器学习教程*中强调了模型可解释性的主要必要性。让我尝试总结模型可解释性之所以重要的关键原因：
- en: '**Verifying and debugging ML systems**: As we have seen some examples where
    wrong model decisions can be costly and dangerous, model explainability techniques
    help us to verify and validate ML systems. Having an interpretation for incorrect
    predictions helps us to debug the root cause and provides a direction to fix the
    problem. We will discuss the different stages of an explainable ML system in more
    detail in [*Chapter 10*](B18216_10_ePub.xhtml#_idTextAnchor209), *XAI Industry
    Best Practices*.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证和调试机器学习系统**：正如我们所看到的，错误的模型决策可能代价高昂且危险，模型可解释性技术帮助我们验证和验证机器学习系统。对错误预测的解释有助于我们调试根本原因，并提供解决问题的方向。我们将在[*第10章*](B18216_10_ePub.xhtml#_idTextAnchor209)
    *XAI行业最佳实践*中更详细地讨论可解释机器学习系统的不同阶段。'
- en: '**Using user-centric approaches to improve ML models**: XAI provides a mechanism
    to include human experience and intuition to improve ML systems. Traditionally,
    ML models are evaluated based on prediction error. Using such evaluation approaches
    to improve ML models doesn''t add any transparency and may not be robust and efficient.
    However, using explainability approaches, we can use human experience to verify
    predictions and understand whether model-centric or data-centric approaches are
    further needed to improve the ML model. *Figure 1.4* compares a classical ML system
    with an explainable ML system:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用以用户为中心的方法来改进机器学习模型**：XAI提供了一种机制，将人类经验和直觉纳入机器学习系统。传统上，机器学习模型是根据预测误差来评估的。使用这种评估方法来改进机器学习模型不会增加任何透明度，并且可能不是稳健和高效的。然而，使用可解释性方法，我们可以使用人类经验来验证预测，并了解是否需要进一步采用以模型为中心或以数据为中心的方法来改进机器学习模型。*图1.4*比较了一个经典的机器学习系统与一个可解释的机器学习系统：'
- en: '![Figure 1.4 – Comparison between classical ML and explainable ML approach'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.4 – 经典机器学习与可解释机器学习方法的比较'
- en: '](img/B18216_01_04.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_01_04.jpg]'
- en: Figure 1.4 – Comparison between classical ML and explainable ML approach
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 经典机器学习与可解释机器学习方法的比较
- en: '**Learning new insights**: ML is considered to automatically unravel interesting
    insights and patterns from data that are not obvious to human beings. Explainable
    ML provides us with a mechanism to understand the rationale behind the insights
    and patterns automatically picked up by the model and allows us to study these
    patterns in detail to make new discoveries.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习新的见解**：机器学习被认为能够自动从数据中揭示出对人类来说并不明显的有趣见解和模式。可解释机器学习为我们提供了一种机制，使我们能够理解模型自动选择的见解和模式背后的理由，并允许我们详细研究这些模式以进行新的发现。'
- en: '**Compliance with legislation**: Many regulatory bodies, such as the **General
    Data Protection Regulation** (**GDPR**) and the **California Consumer Privacy
    Act** (**CCPA**), have expressed severe concerns about the lack of explainability
    in AI. So, growing global AI regulations have empowered individuals with the right
    to demand an explanation of automated decision-making systems that can affect
    them. Model explainability techniques try to ensure ML models are compliant with
    proposed regulatory laws, thereby promoting fairness, accountability, and transparency.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符合法规要求**：许多监管机构，如**通用数据保护条例**（**GDPR**）和**加利福尼亚消费者隐私法案**（**CCPA**），对人工智能缺乏可解释性表示严重关切。因此，日益增长的全球人工智能法规赋予了个人要求对其产生影响的自动化决策系统进行解释的权利。模型可解释性技术试图确保机器学习模型符合拟议的监管法律，从而促进公平、责任和透明度。'
- en: '![Figure 1.5 – (Top) Screenshots of tweets from the European Commission, highlighting
    the right to demand explanations. (Bottom) Table showing some important regulatory
    laws established for making automated decision systems explainable, transparent,
    accountable, and fair'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.5 – (上) 欧洲委员会推文的截图，突出强调要求解释的权利。(下) 表格显示为使自动化决策系统可解释、透明、负责任和公平而建立的一些重要监管法律'
- en: '](img/B18216_01_05_.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_05_.jpg)'
- en: Figure 1.5 – (Top) Screenshots of tweets from the European Commission, highlighting
    the right to demand explanations. (Bottom) Table showing some important regulatory
    laws established for making automated decision systems explainable, transparent,
    accountable, and fair
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – (上) 欧洲委员会推文的截图，突出强调要求解释的权利。(下) 表格显示为使自动化决策系统可解释、透明、负责任和公平而建立的一些重要监管法律
- en: The need for model explainability can be visualized in the following diagram
    of the **FAT model of explainable ML** as provided in the book *Interpretable
    Machine Learning with Python* by *Serg Masís*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可解释性的需求可以通过书中提供的**FAT模型可解释机器学习**的以下图表来可视化，该图表来自Serg Masís的《用Python进行可解释机器学习》一书。
- en: '![Figure 1.6 – FAT model of explainable ML (from Interpretable Machine Learning
    with Python by Serg Masís)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.6 – 可解释机器学习FAT模型（来自Serg Masís的《用Python进行可解释机器学习》）'
- en: '](img/B18216_01_06.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_06.jpg)'
- en: Figure 1.6 – FAT model of explainable ML (from Interpretable Machine Learning
    with Python by Serg Masís)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 可解释机器学习FAT模型（来自Serg Masís的《用Python进行可解释机器学习》）
- en: '*Figure 1.6* shows the pyramid that forms the FAT model of explainable ML system
    for increasing AI adoption. Let us discuss about defining explanation methods
    and approaches in the next section.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.6* 展示了构成可解释机器学习系统FAT模型的金字塔，用于提高人工智能的采用率。让我们在下一节讨论定义解释方法和途径。'
- en: Defining explanation methods and approaches
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义解释方法和途径
- en: In this section, let's try to understand some key concepts required for understanding
    and applying various explainability techniques and approaches.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们尝试理解理解和应用各种可解释性技术和方法所需的一些关键概念。
- en: Dimensions of explainability
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性的维度
- en: 'Adding to the concepts presented at *MICCAI''18 from Tutorial on Interpretable
    Machine Learning* by *Samek and Binder*, when we talk about the problem of demystifying
    black-box algorithms, there are four different dimensions through which we can
    address this problem, as can be seen in the following diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Samek和Binder在*MICCAI'18*上的教程《可解释机器学习》中提出的概念基础上，当我们谈论揭开黑盒算法神秘面纱的问题时，有四个不同的维度可以通过以下图表来解决这个问题：
- en: '![Figure 1.7 – Four dimensions of explainability'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7 – 可解释性的四个维度'
- en: '](img/B18216_01_07.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_07.jpg)'
- en: Figure 1.7 – Four dimensions of explainability
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 – 可解释性的四个维度
- en: 'Now, let''s learn about these dimensions in detail:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细了解这些维度：
- en: '**Data**: The dimension of explainability revolves around the underlying data
    that is being modeled. Understanding the data, identifying its limitations and
    relevant components, and forming certain hypotheses are crucial to setting up
    the correct expectations. A robust data curation process, analyzing data purity,
    and the impact of adversarial effects on the data are other key exercises done
    for obtaining explainable outcomes.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：可解释性的维度围绕着正在建模的底层数据。理解数据、识别其局限性和相关组件，以及形成某些假设对于建立正确的预期至关重要。一个稳健的数据整理过程、分析数据纯度和对抗性效应对数据的影响是获取可解释结果的其他关键练习。'
- en: '**Model**: Model-based interpretability techniques often help us to understand
    how the input data is mapped to the output predictions and be aware of some limitations
    and assumptions of the ML algorithms used. For example, the Naïve Bayes algorithm
    used for ML classification assumes that the presence of a certain feature is completely
    independent and unrelated to the presence of any other features. So, knowing about
    these *inductive biases* of ML algorithms helps us to understand and anticipate
    any prediction error or limitations of the ML models.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：基于模型的可解释性技术通常帮助我们理解输入数据是如何映射到输出预测的，并意识到所使用的机器学习算法的一些局限性和假设。例如，用于机器学习分类的朴素贝叶斯算法假设某个特征的存在是完全独立且与任何其他特征的存在无关的。因此，了解机器学习算法的这些*归纳偏差*有助于我们理解和预测任何预测错误或机器学习模型的局限性。'
- en: '**Outcomes**: The outcome of explainability is about understanding *why* a
    certain prediction or decision is made by an ML model. Although data and model
    interpretability is quite crucial, most ML experts and end-users focus on making
    the final model predictions interpretable.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：可解释性的结果在于理解为什么一个特定的预测或决策是由机器学习模型做出的。尽管数据和模型的可解释性非常重要，但大多数机器学习专家和最终用户更关注使最终模型预测可解释。'
- en: '**End users**: The final dimension of explainability is all about creating
    the right level of abstraction and including the right amount of details for the
    final consumers of the ML models so that the outcomes are reliable and trustworthy
    for any non-technical end-user and empower them to understand the decision-making
    process of black-box algorithms.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终用户**：可解释性的最后一个维度是关于为机器学习模型的最终消费者创建正确的抽象层次和包括适当数量的细节，以便结果对任何非技术最终用户都是可靠和可信的，并使他们能够理解黑盒算法的决策过程。'
- en: Explainability to AI/ML algorithms is provided with respect to one or more dimensions
    of explainability. Next, let's discuss about addressing the key questions of explainability.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性针对一个或多个可解释性的维度提供给AI/ML算法。接下来，让我们讨论如何解决可解释性的关键问题。
- en: Addressing key questions of explainability
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决可解释性的关键问题
- en: 'Now that we understand the different dimensions of explainability, let''s discuss
    what is needed to make ML models explainable. In order to make ML algorithms explainable,
    the following are the key questions that we should try to address:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了可解释性的不同维度，让我们讨论一下要使机器学习模型可解释需要什么。为了使机器学习算法可解释，以下是我们应该尝试解决的问题：
- en: '*What do we understand from the data?*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们从数据中理解了什么？*'
- en: The very first step is all about the data. Before even proceeding with the AI
    and ML modeling, we should spend enough time analyzing and exploring the data.
    The goal is always to look for gaps, inconsistencies, potential biases, or hypotheses
    that might impact or create challenges while modeling the data and generating
    the predictions. This helps us to know what is expected and how certain aspects
    of the data can contribute toward solving the business problem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步完全是关于数据。在开始AI和机器学习建模之前，我们应该花足够的时间分析和探索数据。目标始终是寻找差距、不一致性、潜在的偏差或假设，这些可能会影响或给建模和生成预测带来挑战。这有助于我们了解期望是什么以及数据的某些方面如何有助于解决业务问题。
- en: '*How is the model created?*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型是如何创建的？*'
- en: We need to understand how transparent the algorithm is and what kind of relationship
    the algorithm can capture when modeling the data during the modeling process.
    This is the step where we try to understand the inductive bias of the algorithms
    and then try to relate this to the initial hypothesis or observations obtained
    while exploring the data. For example, linear models will not model the data efficiently
    if the data has some quadratic or cycle patterns observed using visualization-based
    data exploration methods. The prediction error is expected to be higher. So, if
    it is unclear how the algorithm builds a model of the training data, these algorithms
    are less transparent and, hence, less interpretable.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解算法的透明度如何，以及算法在建模过程中建模数据时可以捕捉到什么样的关系。这是我们试图理解算法的归纳偏置并尝试将其与探索数据时获得的初始假设或观察结果相关联的步骤。例如，如果数据中存在一些通过基于可视化的数据探索方法观察到的二次或周期模式，线性模型将无法有效地建模数据。预测误差预计会更高。因此，如果算法如何构建训练数据模型不清楚，这些算法的透明度较低，因此可解释性也较低。
- en: '*What do we know about the global interpretability of the trained model?*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于训练模型的全球可解释性，我们了解什么？*'
- en: Understanding the global model interpretability is always challenging. It is
    about getting a holistic view of the underlying features used, knowing the important
    features, how sensitive the model is toward changes in the key feature values,
    and what kind of complex interactions are happening inside the model. This is
    especially hard to achieve in practice for complex deep learning models that have
    millions of parameters to learn and several hundreds of layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 理解全局模型的可解释性始终具有挑战性。这关乎获得底层特征的全面视角，了解重要特征，了解模型对关键特征值变化的敏感度，以及模型内部发生的复杂交互类型。这对于具有数百万个学习参数和数百层复杂深度学习模型来说，在实践中尤其难以实现。
- en: '*What is the influence of different parts of the model on the final prediction?*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型的不同部分对最终预测有什么影响？*'
- en: Different parts of an ML model might impact the final prediction in a different
    way. Especially for deep neural network models, each layer tries to learn different
    types of features. When model predictions are incorrect, understanding how different
    parts of a model can affect or control the final outcome is very important. So,
    explainability techniques can unravel insights from different parts of a model
    and help debug and observe the algorithm's robustness for different data points.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的不同部分可能以不同的方式影响最终预测。特别是对于深度神经网络模型，每一层都试图学习不同类型的特征。当模型预测错误时，了解模型的不同部分如何影响或控制最终结果非常重要。因此，可解释性技术可以从模型的不同部分揭示洞察力，并帮助调试和观察算法对不同数据点的鲁棒性。
- en: '*Why did the model make a specific prediction for a single record and a batch
    of records?*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么模型对单个记录和一批记录做出特定的预测？*'
- en: The most important aspect of explainability is understanding why the model is
    making a specific prediction and not something else. So, certain local and global
    explanation techniques are applied, which either consider the impact of individual
    features or even the collective impact of multiple features on the outcome. Usually,
    these explainability techniques are applied for single instances of the data and
    a batch of data instances to understand whether the observations are consistent.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的最重要方面是理解模型为何做出特定的预测而不是其他预测。因此，应用了某些局部和全局的解释技术，这些技术要么考虑单个特征的影响，要么甚至考虑多个特征对结果的整体影响。通常，这些可解释性技术应用于单个数据实例和一批数据实例，以了解观察结果是否一致。
- en: '*Does the outcome match the expectation of the end user?*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结果是否符合最终用户的期望？*'
- en: The final step is always providing user-centric explanations. This means explainability
    is all about comparing the outcome with end users' predictions based on common
    sense and human intuition. If the model forecast matches the user's prediction,
    providing a reasonable explanation includes justifying the dominant factors for
    the specific outcome. But suppose the model forecasting is not matching the user's
    prediction. In that case, a good explanation tries to justify what changes could
    have happened in the input observations to get a different outcome.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步总是提供以用户为中心的解释。这意味着解释性全部关于将结果与基于常识和人类直觉的用户预测进行比较。如果模型的预测与用户的预测相符，提供合理的解释只需证明导致特定结果的主要因素。但如果模型的预测与用户的预测不符，那么一个好的解释会尝试证明输入观察中可能发生了哪些变化，以得到不同的结果。
- en: For example, let's say, considering usual weekday traffic congestion, the time
    taken to reach from office to home for me is 30 minutes. But if it is raining,
    I would expect the vehicles on the road to move slowly and traffic congestion
    to be higher, and hence might expect it to take longer to reach home. Now, if
    an AI application predicts the time to get home as still 30 minutes, I might not
    trust this prediction as this is counter-intuitive.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设，考虑到通常的工作日交通拥堵，我从办公室到家的通勤时间是30分钟。但如果下雨，我会预计路上的车辆会缓慢移动，交通拥堵会更高，因此可能会预计到家的时间会更长。现在，如果AI应用预测到家的时间仍然是30分钟，我可能不会相信这个预测，因为这不符合直觉。
- en: Now, let's say that the algorithm was accurate in its forecast. However, the
    justification provided to me was about the movement of the vehicles on my route,
    and the AI app just mentioned that the vehicles on my route are moving at the
    same speed as on other days. Does this explanation really help me to understand
    the model predictions? No, it doesn't. But suppose the application mentions that
    there are fewer vehicles on the route than found on typical days. In that case,
    I would easily understand that the number of vehicles is fewer due to the rain
    and hence the time to destination is still the same as usual on weekdays.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设算法在预测上是准确的。然而，提供的理由是关于我路线上的车辆移动情况，AI应用只是提到我路线上的车辆移动速度与其他日子相同。这种解释真的能帮助我理解模型预测吗？不，它不能。但如果应用提到路线上的车辆比典型日子少，那么我会很容易理解车辆数量减少是因为下雨，因此到达目的地的时间仍然与平时工作日相同。
- en: My own recommendation is that, after training and validating an ML model, always
    try to seek answers to these questions as an initial step in interpreting the
    working of black-box models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是，在训练和验证机器学习模型后，始终尝试在解释黑盒模型的工作原理时将这些问题的答案作为第一步。
- en: Understanding different types of explanation methods
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解不同类型的解释方法
- en: 'In the previous section, we discussed some key questions to address when designing
    and using robust explainability methods. In this section, we will discuss various
    types of explanation methods, considering the four dimensions of explainability
    used in ML:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了在设计和使用鲁棒的解释方法时需要解决的一些关键问题。在本节中，我们将讨论各种类型的解释方法，考虑到在机器学习中使用的四个解释性维度：
- en: '**Local explainability** and **global explainability**: ML model explainability
    can be done for single local instances of the data to understand how a certain
    range of values or specific categorical value can be related to the final prediction.
    This is called local explainability. Global model explainability is used to explain
    the behavior of the entire model or certain important features as a whole that
    contribute toward a specific set of model outcomes.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部可解释性**和**全局可解释性**：机器学习模型的可解释性可以针对数据中的单个局部实例进行，以了解某个范围内的值或特定的分类值如何与最终预测相关。这被称为局部可解释性。全局模型可解释性用于解释整个模型的行为或某些重要特征的整体行为，这些特征共同导致一组特定的模型结果。'
- en: '**Intrinsic explainability** and **extrinsic explainability**: Some ML models,
    such as linear models, simple decision trees, and heuristic algorithms, are intrinsically
    explainable as we clearly know the logic or the mathematical mapping of the input
    and output that the algorithm applies, whereas extrinsic or post hoc explainability
    is about first training an ML model on given data and then using certain model
    explainability techniques separately to understand and interpret the model''s
    outcome.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内在可解释性**和**外在可解释性**：一些机器学习模型，如线性模型、简单的决策树和启发式算法，由于其逻辑或输入输出之间的数学映射是清晰的，因此本质上是可以解释的。而外在或事后可解释性则是指首先在给定数据上训练一个机器学习模型，然后使用某些模型可解释性技术分别来理解和解释模型的输出。'
- en: '**Model-specific explainability** and **model-agnostic explainability**: When
    we use certain explainability methods that are applicable for any specific algorithm,
    then these are model-specific approaches. For example, visualization of the tree
    structure in decision tree models is only specific to the decision tree algorithm
    and hence comes under the model-specific explainability method. Model-agnostic
    methods are used to provide explanations to any ML model irrespective of the algorithm
    being used. Mostly, these are post hoc analysis methods, used after the trained
    ML model is obtained, and usually, these methods are not aware of the internal
    model structure and weights. In this book, we will mostly focus on model-agnostic
    explainability methods, which are not dependent on any particular algorithm.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型特定可解释性**和**模型无关可解释性**：当我们使用适用于任何特定算法的某些可解释性方法时，这些就是模型特定方法。例如，决策树模型中树结构的可视化仅限于决策树算法，因此属于模型特定可解释性方法。模型无关方法用于向任何机器学习模型提供解释，无论使用的是哪种算法。大多数情况下，这些是事后分析方法，在训练好的机器学习模型获得后使用，通常这些方法并不了解模型的内部结构和权重。在这本书中，我们将主要关注模型无关可解释性方法，这些方法不依赖于任何特定的算法。'
- en: '**Model-centric explainability** and **data-centric explainability**: Conventionally,
    the majority of explanation methods are model-centric, as these methods try to
    interpret how the input features and target values are being modeled by the algorithm
    and how the specific outcomes are obtained. But with the latest advancement in
    the space of data-centric AI, ML experts and researchers are also investigating
    explanation methods around the data used for training the models, which are known
    as data-centric explainability. Data-centric methods are used to understand whether
    the data is consistent, well curated, and well suited for solving the underlying
    problem. Data profiling, detection of data and concept drifts, and adversarial
    robustness are certain specific data-centric explainability approaches that we
    will be discussing in more detail in [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053),
    *Data-Centric Approaches*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型中心可解释性**和**数据中心可解释性**：传统上，大多数解释方法都是模型中心的，因为这些方法试图解释算法如何对输入特征和目标值进行建模，以及如何获得特定的结果。但随着数据中心AI领域的最新进展，机器学习专家和研究人员也在研究围绕用于训练模型的训练数据的解释方法，这被称为数据中心可解释性。数据中心方法用于了解数据是否一致、精心整理，以及是否适合解决潜在问题。数据概要、数据漂移和概念漂移的检测以及对抗鲁棒性是我们将在[*第3章*](B18216_03_ePub.xhtml#_idTextAnchor053)“数据中心方法”中更详细讨论的某些特定数据中心可解释性方法。'
- en: We will discuss all these types of explainability methods in later chapters
    of the book.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后续章节中讨论所有这些类型的可解释性方法。
- en: Understanding the accuracy interpretability trade-off
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解准确性与可解释性之间的权衡
- en: For an ideal scenario, we would want our ML models to be highly accurate and
    highly interpretable so that any non-technical business stakeholder or end user
    can understand the rationale behind the model predictions. But in practice, achieving
    highly accurate and interpretable models is extremely difficult, and there is
    always a trade-off between accuracy and interpretability.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，我们希望我们的机器学习模型既高度准确又高度可解释，以便任何非技术性的业务利益相关者或最终用户都能理解模型预测背后的逻辑。但在实践中，实现高度准确和可解释的模型极其困难，准确性和可解释性之间始终存在权衡。
- en: For example, to perform radiographic image classification, intrinsically interpretable
    ML algorithms, such as decision trees, might not be able to give efficient and
    generalized results, whereas more complex deep convolutional neural networks,
    such as DenseNet, might be more efficient and robust for modeling radiographic
    image data. But DenseNet is not intrinsically interpretable, and explaining the
    algorithm's working to any non-technical end user can be pretty complicated and
    challenging. So, highly accurate models, such as deep neural networks, are non-linear
    and more complex and can capture complex relationships and patterns from the data,
    but achieving interpretability is difficult for these models. Highly interpretable
    models, such as linear regression and decision trees, are primarily linear and
    less complex, but these are limited to learning only linear or less-complex patterns
    from the data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了进行放射学图像分类，本质上可解释的机器学习算法，如决策树，可能无法提供高效和通用的结果，而更复杂的深度卷积神经网络，如DenseNet，可能对建模放射学图像数据更为高效和稳健。但是，DenseNet本质上不可解释，向任何非技术性最终用户解释算法的工作原理可能相当复杂和具有挑战性。因此，高度精确的模型，如深度神经网络，是非线性和更复杂的，可以从数据中捕捉复杂的关联和模式，但这些模型的可解释性很难实现。高度可解释的模型，如线性回归和决策树，主要是线性和较简单的，但它们仅限于从数据中学习线性或较简单的模式。
- en: Now, the question is, is it better to go with highly accurate models or highly
    interpretable models? I would say that the correct answer is, *it depends!* It
    depends on the problem being solved and on the consumers of the model. For high-stake
    domains, where the consequences of poor predictions are severe, I would recommend
    going for more interpretable models even if accuracy is being sacrificed. Any
    rule-based heuristic model that is highly interpretable can be very effective
    in such situations. But if the problem is well studied, and getting the least
    prediction error is the main goal (such as in any academic use case or any ML
    competitions) such that the consequences of poor prediction will not create any
    significant damage, then going for highly accurate models can be preferred. In
    most industrial problems, it is essential to keep the right balance of model accuracy
    and interpretability to promote AI adoption.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是，是选择高度精确的模型还是高度可解释的模型更好？我会说，正确的答案是，*这取决于！* 这取决于要解决的问题和模型的消费者。对于高风险领域，其中预测错误的后果严重，即使牺牲准确性，我也建议选择更可解释的模型。任何高度可解释的基于规则的启发式模型在这种情况下可能非常有效。但是，如果问题研究得很好，并且最小化预测误差是主要目标（如任何学术用例或任何机器学习竞赛），那么预测错误的后果不会造成任何重大损害，那么选择高度精确的模型可能是首选。在大多数工业问题中，保持模型准确性和可解释性之间的正确平衡对于促进人工智能的采用至关重要。
- en: '*Figure 1.10* illustrates the accuracy-interpretability trade-off of popular
    ML algorithms:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.10* 展示了流行机器学习算法的准确性-可解释性权衡：'
- en: '![Figure 1.8 – Accuracy-interpretability trade-off diagram'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8 – 准确性-可解释性权衡图'
- en: '](img/B18216_01_08.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18216_01_08.jpg]'
- en: Figure 1.8 – Accuracy-interpretability trade-off diagram
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 – 准确性-可解释性权衡图
- en: Now that we have a fair idea of the accuracy-interpretability trade-off, let's
    try to understand how to evaluate the quality of explainability methods.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对准确性-可解释性权衡有了相当的了解，让我们尝试理解如何评估可解释性方法的质量。
- en: Evaluating the quality of explainability methods
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估可解释性方法的质量
- en: Explainability is subjective and may vary from person to person. The key question
    is *How do we determine whether one approach is better than the other?* So, in
    this section, let's discuss certain criteria to consider for evaluating explainability
    techniques for ML systems.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是主观的，可能因人而异。关键问题是*我们如何确定一种方法是否优于另一种方法？* 因此，在本节中，让我们讨论评估机器学习系统可解释性技术时应考虑的某些标准。
- en: Criteria for good explainable ML systems
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优秀可解释机器学习系统的标准
- en: 'As explainability for ML systems is a very subjective topic, first let''s try
    to understand some key criteria for good human-friendly explanations. In his book
    *Interpretable Machine Learning*, *Christoph Molnar*, the author, has also tried
    to emphasize the importance of good human-friendly explanations after thorough
    research, which I will try to mention in a condensed form considering modern,
    industrial, explainable ML systems:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习系统的可解释性是一个非常主观的话题，首先让我们尝试理解一些良好的人性化解释的关键标准。在《可解释机器学习》一书中，作者**克里斯托夫·莫拉尔**经过深入研究后，也强调了良好的人性化解释的重要性，我将尝试以简化的形式提及，考虑到现代、工业化的可解释机器学习系统：
- en: '**Coherence with a priori knowledge**: Consistency with prior beliefs of end
    users is an important criterion of explainable ML systems. If any explanation
    contradicts a priori knowledge of human beings, then humans tend to have less
    trust in such explanations. However, it is challenging to introduce prior knowledge
    of humans into ML models. But human-friendly explainable ML systems should try
    to provide explanations surrounding certain features that have direct and less
    complex relationships with the outcome, such that the relationship is coherent
    with the prior beliefs of the end users.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与先验知识的一致性**：与最终用户的先验信念的一致性是可解释机器学习系统的重要标准。如果任何解释与人类先验知识相矛盾，那么人类往往会对这样的解释缺乏信任。然而，将人类的先验知识引入机器学习模型是具有挑战性的。但是，人性化的可解释机器学习系统应该尝试提供围绕某些特征的解释，这些特征与结果有直接且不太复杂的关系，从而使关系与最终用户的先验信念一致。'
- en: For example, for predicting the presence of diabetes, the measure of blood glucose
    level has a direct relationship, which is consistent with prior human beliefs.
    If the blood glucose level is higher than usual, this might indicate diabetes
    for the patient, although diabetes can also be due to certain genetic factors
    or other reasons. Similarly, high blood glucose levels can also be momentary and
    a high blood glucose level doesn't always mean that the patient has diabetes.
    But as the explanation is consistent with a priori knowledge, end users will have
    more trust in such explanations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于预测糖尿病的存在，血糖水平的测量与人类先前的信念有直接关系，这是符合人类先前的信念的。如果血糖水平高于正常值，这可能会表明患者患有糖尿病，尽管糖尿病也可能是由某些遗传因素或其他原因引起的。同样，高血糖水平也可能是瞬时的，高血糖水平并不总是意味着患者患有糖尿病。但是，由于解释与先验知识一致，最终用户会对这样的解释有更多的信任。
- en: '**Fidelity**: Another key factor for providing a holistic explanation is the
    truthfulness of the explanation, which is also termed the fidelity of ML models.
    Explanations with high fidelity can approximate the holistic prediction of the
    black-box models, whereas low-fidelity explanations can interpret a local data
    instance or a specific subset of the data. For example, for doing sales forecasting,
    providing explanations based on just the trend of historical data doesn''t give
    a complete picture, as other factors, such as production capacity, market competition,
    and customer demand, might influence the outcome of the model. Fidelity plays
    a key role, especially for doing a detailed root cause analysis, but too many
    details may not be useful for common users, unless requested.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忠实度**：提供全面解释的另一个关键因素是解释的真实性，这也被称为机器模型的忠实度。高忠实度的解释可以近似黑盒模型的全面预测，而低忠实度的解释可以解释局部数据实例或数据的一个特定子集。例如，在进行销售预测时，仅基于历史数据的趋势提供解释并不能给出完整的画面，因为其他因素，如生产能力、市场竞争和客户需求，可能会影响模型的结果。忠实度在详细根本原因分析中起着关键作用，但过多的细节可能对普通用户没有帮助，除非有要求。'
- en: '**Abstraction**: Good human-friendly explanations are always expected in a
    concise and abstract format. Too many complicated details can also impact the
    experience of end users. For example, for weather forecasting, if the model predicts
    a high probability of rain, the concise and abstract explanation can be that it
    is cloudy now and raining within 5 kilometers of the current location, so there
    is a high probability of rain.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象性**：良好的人性化解释总是期望以简洁和抽象的格式呈现。过多的复杂细节也可能影响最终用户的体验。例如，对于天气预报，如果模型预测有很高的降雨概率，简洁和抽象的解释可以是：现在天空多云，当前位置5公里范围内正在下雨，因此有很高的降雨概率。'
- en: But if the model includes details related to precipitation level, humidity,
    and wind speed, which might also be important for the prediction of rainfall,
    these additional details are complex and difficult to comprehend, and hence not
    human-friendly. So, good, human-friendly explanations include the appropriate
    amount of abstraction to simplify the understanding for the end users. End users
    mostly prefer concise explanations, but detailed explanations might be needed
    when doing root cause analysis for model predictions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果模型包括与降水水平、湿度和风速相关的细节，这些细节可能对降雨预测也很重要，这些额外的细节复杂且难以理解，因此不人性化。所以，好的、人性化的解释包括适当程度的抽象，以简化最终用户的理解。最终用户大多更喜欢简洁的解释，但在进行模型预测的根本原因分析时可能需要详细解释。
- en: '**Contrastive explanations**: Good, human-friendly explanations are not about
    understanding the inner workings of the models but mostly about comparing the
    *what-if* scenarios. Suppose the outcome is continuous numerical data as in the
    case of regression problems. In that case, a good explanation for predictions
    includes comparing with another instance''s prediction that is significantly higher
    or lower. Similarly, a good explanation for classification problems is about comparing
    the current prediction with other possible outcomes. But contrastive explanations
    are application-dependent as it requires a point of comparison, although understanding
    the *what-if* scenarios helps us to understand the importance of certain key features
    and how these features are related to the target variable.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比解释**：好的、人性化的解释并不是关于理解模型的内部运作，而是主要关于比较“如果...会怎样”的情景。假设结果是连续的数值数据，就像回归问题中的情况一样。在这种情况下，一个好的预测解释包括与另一个实例的预测进行比较，该预测显著更高或更低。同样，一个好的分类问题解释是关于将当前预测与其他可能的输出进行比较。但对比解释是应用相关的，因为它需要一个比较点，尽管理解“如果...会怎样”的情景有助于我们理解某些关键特征的重要性以及这些特征与目标变量之间的关系。'
- en: For example, for a use case of employee churn prediction, if the model predicts
    that the employee is likely to leave the organization, then contrastive explanations
    try to justify the model's decision by comparing it with an instance's prediction
    where the employee is expected to stay in the organization and compare the values
    of the key features used to model the data. So, the explanation method might convey
    that since the salary of the employee who is likely to leave the organization
    is much lower than that of employees who are likely to stay within the organization,
    the model predicted that the employee is expected to leave the organization.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于员工流失预测的用例，如果模型预测员工可能会离开组织，那么对比解释试图通过将其与一个预期员工将留在组织中的实例的预测进行比较来证明模型的决策，并比较用于建模数据的关键特征值。因此，解释方法可能会传达，由于可能离开组织的员工的薪水远低于可能留在组织内的员工的薪水，模型预测该员工预计会离开组织。
- en: '**Focusing on the abnormality**: This may sound counter-intuitive, but human
    beings try to seek explanations for events that are not expected and not obvious.
    Suppose there is an abnormal observation in the data, such as a rare categorical
    value or an anomalous continuous value that can influence the outcome. In that
    case, it should be included in the explanation. Even if other normal and consistent
    features have the same influence on the model outcome, still including the abnormality
    holds higher importance in terms of human-friendly explanation.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注异常情况**：这听起来可能有些反直觉，但人类试图寻找对那些不可预期和不明显的事件的解释。假设数据中存在异常观察，比如一个罕见的分类值或一个可能影响结果的异常连续值。在这种情况下，它应该包含在解释中。即使其他正常且一致的特征对模型结果有相同的影响，仍然包含异常性在人性化的解释中具有更高的重要性。'
- en: For example, say we are predicting the price of cars based on their configuration,
    let's say the mode of operation is *electric*, which is a rare observation compared
    to *gasoline*. Both of these categories might have the same influence on the final
    model prediction. Still, the model explanation should include the rare observation,
    as end users are more interested in abnormal observations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们正在根据汽车的配置预测汽车的价格，比如说操作模式是*电动*，这与*汽油*相比是一种罕见的观察。这两个类别可能对最终模型预测有相同的影响。然而，模型解释应该包括罕见的观察，因为最终用户对异常观察更感兴趣。
- en: '**Social aspect**: The social aspect of model explainability determines the
    abstraction level and the content of explanations. The social aspect depends on
    the level of understanding of the specific target audience and might be difficult
    to generalize and introduce during the model explainability method. For example,
    suppose a stock-predicting ML model designed to prescribe actions to users suggests
    *shorting* a particular stock. In that case, end users outside the finance domain
    may find it difficult to understand. But instead, if the model suggests selling
    *a stock at the current price without possessing it and buying it back after 1
    month when the price is expected to fall*, any non-technical user might comprehend
    the model suggestions easily. So, good explanations consider the social aspect,
    and often, user-centric design principles of **Human-Computer Interaction** (**HCI**)
    are utilized to design good, explainable ML systems that consider the social aspect.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社会方面**：模型可解释性的社会方面决定了解释的抽象层次和内容。社会方面取决于特定目标受众的理解水平，可能在模型可解释性方法中难以概括和引入。例如，假设一个旨在向用户建议行动的股票预测机器学习模型建议*做空*某只特定股票。在这种情况下，非金融领域的最终用户可能难以理解。但相反，如果模型建议*在当前价格卖出不持有的股票，并在一个月后价格预期下跌时买回*，任何非技术用户都可能轻松理解模型建议。因此，好的解释应考虑社会方面，并且通常，利用**人机交互**（**HCI**）的用户中心设计原则来设计考虑社会方面的良好、可解释的机器学习系统。'
- en: Now that we have a fair idea of the key criteria for good explanations, in the
    next section, we will discuss some auxiliary criteria that are equally important
    while building explainable ML systems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对良好解释的关键标准有了大致的了解，在下一节中，我们将讨论在构建可解释的机器学习系统时同样重要的辅助标准。
- en: Auxiliary criteria of XAI for ML systems
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XAI的辅助标准用于机器学习系统
- en: 'Good explanations are not limited to the key criteria discussed previously,
    but there are a few auxiliary criteria of XAI as discussed by *Doshi-Velez* and
    *Kim* in their work *Towards A Rigorous Science of Interpretable Machine Learning*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的解释不仅限于之前讨论的关键标准，还有XAI的一些辅助标准，如*Doshi-Velez*和*Kim*在他们关于*迈向可解释机器学习的严谨科学*的工作中所讨论的那样：
- en: '**Unbiasedness**: Model explainability techniques should also look for the
    presence of any form of bias in the data or the model. So, one of the key goals
    of XAI is to make ML models unbiased and fair. For example, for predicting credit
    card fraud, the explainability approach should investigate the importance of demographic
    information related to the gender of the customer for the model''s decision-making
    process. If the importance of gender information is high, that means that the
    model is biased toward a particular gender.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无偏性**：模型可解释性技术还应寻找数据或模型中任何形式的偏见。因此，XAI的一个关键目标是使机器学习模型无偏见且公平。例如，对于预测信用卡欺诈，可解释性方法应调查与客户性别相关的人口信息对于模型决策过程的重要性。如果性别信息的重要性很高，这意味着模型对特定性别存在偏见。'
- en: '**Privacy**: Explainability methods should comply with data privacy measures,
    and hence any sensitive information should not be used for the model explanations.
    Mainly for providing personalized explanations, ensuring compliance with data
    privacy can be very important.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：可解释性方法应遵守数据隐私措施，因此不应使用任何敏感信息进行模型解释。主要为了提供个性化的解释，确保遵守数据隐私可能非常重要。'
- en: '**Causality**: Model explainability approaches should try to look for any causal
    relationships so that the end users are aware that due to any perturbation, there
    can be changes in model predictions for production systems.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因果关系**：模型可解释性方法应尝试寻找任何因果关系，以便最终用户意识到由于任何扰动，生产系统的模型预测可能会有所变化。'
- en: '**Robustness**: Methods such as sensitivity analysis help to understand how
    robust and consistent a model prediction is with respect to its feature values.
    If small changes in input features lead to a significant shift in model predictions,
    it shows that the model is not robust or stable.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：如敏感性分析等方法有助于了解模型预测与其特征值之间的鲁棒性和一致性。如果输入特征的小幅变化导致模型预测发生显著变化，这表明模型不是鲁棒或稳定的。'
- en: '**Trust**: One of the key goals of XAI is to increase AI adoption by increasing
    the trust of the end users. So, all explainability methods should make black-box
    ML models more transparent and interpretable so that the end users can trust and
    rely on them. If explanation methods don''t meet the criteria of good explanations,
    as discussed in the *Criteria for good explainable ML systems* section, it might
    not help to increase the trust of its consumers.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任**：XAI的一个关键目标是通过对最终用户增加信任来增加AI的采用率。因此，所有可解释性方法都应使黑盒机器学习模型更加透明和可解释，以便最终用户可以信任并依赖它们。如果解释方法不符合在“良好可解释机器学习系统标准”部分讨论的良好解释标准，那么它可能不会帮助增加其消费者的信任。'
- en: '**Usable**: XAI methods should try to make AI models more usable. Hence, it
    should provide information to the users to accomplish the task. For example, counterfactual
    explanations might suggest a loan applicant pays their credit card bill on time
    for the next 2 months and clear off their previous debts before applying for a
    new loan so that their loan application is not rejected.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性**：XAI方法应努力使AI模型更易于使用。因此，它应向用户提供完成任务所需的信息。例如，反事实解释可能会建议贷款申请人支付未来2个月的信用卡账单，并在申请新贷款之前清偿之前的债务，这样他们的贷款申请就不会被拒绝。'
- en: Next, we will need to understand various levels of evaluating explainable ML
    systems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要了解评估可解释机器学习系统各个层面的不同水平。
- en: Taxonomy of evaluation levels for explainable ML systems
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释机器学习系统评估层面的分类
- en: Now that we have discussed the key criteria for designing and evaluating good
    explainable ML systems, let's discuss the taxonomy of evaluation methods for judging
    the quality of explanations. In their work *Towards A Rigorous Science of Interpretable
    Machine Learning*, *Doshi-Velez and* *Kim* mentioned three major types of evaluation
    approaches that we will try to understand in this section. Since explainable ML
    systems are to be designed with user-centric design principles of HCI, human beings
    evaluating real tasks play a central role in assessing the quality of explanation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了设计和评估良好可解释机器学习系统的关键标准，那么让我们讨论评估解释质量的评估方法分类。在他们的工作《迈向可解释机器学习的严谨科学》中，Doshi-Velez和Kim提到了三种主要的评估方法类型，我们将在本节中尝试理解。由于可解释机器学习系统应按照以用户为中心的人类-计算机交互（HCI）设计原则进行设计，因此评估真实任务的评估者在评估解释质量中扮演着核心角色。
- en: 'But human evaluation mechanisms can have their own challenges, such as different
    types of human bias and being more time- and other resource-consuming, and can
    have other compounding factors that can lead to inconsistent evaluation. Hence,
    human evaluation experiments should be well designed and should be used only when
    needed and otherwise not. Now, let''s look at the three major types of evaluation
    approaches:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但人类评估机制可能有其自身的挑战，例如不同类型的人类偏见，以及更耗时和消耗其他资源，还可能有其他导致评估不一致的复合因素。因此，人类评估实验应设计得很好，并且只有在必要时才使用，否则不应使用。现在，让我们看看三种主要的评估方法类型：
- en: '**Application-grounded evaluation**: This evaluation method involves including
    the explainability techniques in a real product or application, thus allowing
    the conduction of human subject experiments, in which real end users are involved
    to perform certain experiments. Although the experiment setup cost is high and
    time-consuming, building an almost finished product and then allowing domain experts
    to test has its benefits. It will enable the researcher to evaluate the quality
    of the explanation with respect to the end task of the system, thus providing
    ways to quickly identify errors or limitations of the explainability methods.
    This evaluation principle is consistent with the evaluation methods used in HCI,
    and explainability is infused within the entire system responsible for solving
    the user''s keep problem and helping the user meet its end goal.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于应用的评估方法**：这种评估方法涉及将可解释性技术集成到实际产品或应用中，从而允许进行涉及真实最终用户的实验，这些用户将执行某些实验。尽管实验设置成本高且耗时，但构建一个几乎完成的产品然后允许领域专家进行测试有其好处。这将使研究人员能够评估解释的质量相对于系统的最终任务，从而提供快速识别可解释性方法错误或局限性的方法。这种评估原则与在人类-计算机交互（HCI）中使用的评估方法一致，可解释性被融入到整个负责解决用户持续问题和帮助用户实现最终目标的系统中。'
- en: For example, to evaluate the quality of explanation methods of an AI software
    for automated skin cancer detection from images, dermatologists can be approached
    to directly test the objective for which the AI software is built. If the explanation
    methods are successful, then such a solution can be scaled up easily. In terms
    of the industrial perspective, since getting a perfect finished product can be
    time-consuming, the better approach is to build a robust prototype or a **Minimum
    Viable Product** (**MVP**) so that the domain expert testing the system gets a
    better idea of how the finished product will be.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了评估一款AI软件从图像中自动检测皮肤癌的解释方法的质量，皮肤科医生可以被邀请直接测试AI软件所构建的目标。如果解释方法成功，那么这样的解决方案可以很容易地扩展。从工业角度来看，由于获得完美的成品可能耗时，更好的方法是构建一个健壮的原型或**最小可行产品（MVP**），这样领域专家在测试系统时就能更好地了解成品将如何。
- en: '**Human-grounded evaluation**: This evaluation method involves conducting human
    subject experiments with non-expert novice users on more straightforward tasks
    rather than domain experts. Getting domain experts can be time-consuming and expensive,
    so human-grounded evaluation experiments are easier and less costly to set up.
    The tasks are also simplified sometimes and usually for certain use cases where
    generalization is important, these methods are very helpful. **A/B testing**,
    **counterfactual simulations**, and **forward simulations** are certain popular
    evaluation methods used for human-grounded evaluation.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于人类的评估**：这种方法涉及对非专家新手用户进行更直接的任务的人类受试者实验，而不是领域专家。获取领域专家可能耗时且昂贵，因此基于人类的评估实验更容易设置且成本更低。有时任务也会简化，通常在需要泛化的某些用例中，这些方法非常有帮助。**A/B测试**、**反事实模拟**和**正向模拟**是用于基于人类评估的一些流行评估方法。'
- en: In XAI, A/B testing provides different types of explanations to the user, where
    the user is asked to select the best one with the higher quality of explanation.
    Then, based on the final aggregated votes, and using other metrics such as click-through
    rate, screen hovering time, and time to task completion, the best method is decided.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在可解释人工智能（XAI）中，A/B测试向用户提供不同类型的解释，用户被要求选择解释质量更高的最佳选项。然后，基于最终汇总的投票，以及使用其他指标如点击率、屏幕悬停时间和完成任务的时间，来确定最佳方法。
- en: For counterfactual simulation methods, human subjects are presented with the
    input and output of the model with the model explanations for a certain number
    of data samples and are asked to provide certain changes to the input features
    in order to change the model's final outcome to a specific range of values or
    a specific category. In the forward simulation method, human subjects are provided
    with the model inputs and their corresponding explanation methods and then asked
    to simulate the model prediction without looking at the ground-truth values. Then,
    the error metric used to find the difference between human-predicted outcomes
    with the ground-truth labels can be used as a quantitative way to evaluate the
    quality of explanation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反事实模拟方法，人类受试者被展示模型的输入和输出以及一定数量的数据样本的解释，并被要求提供某些输入特征的更改，以改变模型的最终结果到特定的值范围或特定类别。在正向模拟方法中，人类受试者被提供模型输入及其相应的解释方法，然后被要求在不查看真实值的情况下模拟模型预测。然后，用于找到人类预测结果与真实标签之间差异的错误指标可以作为评估解释质量的一种定量方法。
- en: '**Functionality-grounded evaluation**: This evaluation method doesn''t involve
    any human subject experiments, and proxy tasks are used to evaluate the quality
    of explanation. These experiments are more feasible and less expensive to set
    up than the other two, and especially for use cases where human subject experiments
    are restricted and unethical, this is an alternative approach. This approach works
    well when the type of algorithm was already tested in human-level evaluation.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于功能性的评估**：这种方法不涉及任何人类受试者实验，使用代理任务来评估解释的质量。这些实验比其他两种方法更可行且成本更低，特别是在人类受试者实验受限且不道德的用例中，这是一种替代方法。当算法类型已经在人类水平评估中测试过时，这种方法效果良好。'
- en: For example, linear regression models are easily interpretable and end users
    can efficiently understand the working of the model. So, using a linear regression
    model for use cases such as sales forecasting can help us to understand the overall
    trend of the historical data and how the forecasted values are related to the
    trend.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，线性回归模型易于解释，最终用户可以有效地理解模型的工作原理。因此，使用线性回归模型进行销售预测等用例可以帮助我们理解历史数据的整体趋势以及预测值与趋势之间的关系。
- en: '*Figure 1.9* summarizes the taxonomy of the evaluation level for explainable
    ML systems:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.9*总结了可解释机器学习系统评估层级的分类：'
- en: '![Figure 1.9 – Taxonomy of evaluation level for the explainable ML system'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.9 – 可解释机器学习系统的评估层级分类'
- en: '](img/B18216_01_09.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_09.jpg)'
- en: Figure 1.9 – Taxonomy of evaluation level for the explainable ML system
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 – 可解释机器学习系统的评估层级分类
- en: Apart from the methods discussed here, for determining the quality of explanations,
    other metrics such as description length of explanation, the complexity of the
    features used in the explanation, and cognitive processing time required to understand
    the provided explanation, are often used. We will discuss them in more detail
    in [*Chapter 11*](B18216_11_ePub.xhtml#_idTextAnchor217), *End User-Centered Artificial
    Intelligence*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这里讨论的方法之外，为了确定解释的质量，还经常使用其他指标，如解释的描述长度、用于解释的特征的复杂性以及理解提供的解释所需的认知处理时间。我们将在[*第11章*](B18216_11_ePub.xhtml#_idTextAnchor217)《以最终用户为中心的人工智能》中更详细地讨论它们。
- en: 'In this chapter, so far you have come across many new concepts for ML explainability
    techniques. The mind-map diagram in *Figure 1.10* gives a nice summary of the
    various terms and concepts discussed in this chapter:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，到目前为止，你已经遇到了许多关于机器学习可解释性技术的全新概念。*图1.10*中的思维导图对本章讨论的各种术语和概念提供了一个很好的总结：
- en: '![Figure 1.10 – Mind-map diagram of machine learning explainability techniques'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.10 – 机器学习可解释性技术的思维导图'
- en: '](img/B18216_01_10.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_01_10.jpg)'
- en: Figure 1.10 – Mind-map diagram of machine learning explainability techniques
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 机器学习可解释性技术的思维导图
- en: I strongly recommend all of you make yourselves familiar with the jargon used
    in this mind-map diagram as we will be using it throughout the book! Let's summarize
    what we have discussed in the summary section.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你们熟悉这个思维导图中使用的术语，因为我们将在此书中使用它！让我们总结一下在摘要部分中讨论的内容。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, you should now understand what XAI is and why it
    is so important. You have learned about the various terms and concepts related
    to explainability techniques, which we will frequently use throughout this book.
    You have also learned about certain key criteria of human-friendly explainable
    ML systems and different approaches to evaluating the quality of the explainability
    techniques. In the next chapter, we will focus on various types of model explainability
    methods for structured and unstructured data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章之后，你现在应该已经理解了XAI（可解释人工智能）是什么以及为什么它如此重要。你已经了解了与可解释性技术相关的各种术语和概念，这些术语和概念我们将在此书中频繁使用。你还了解了一些关键的人性化可解释机器学习系统的标准以及评估可解释性技术质量的不同方法。在下一章中，我们将专注于结构化和非结构化数据的各种模型可解释性方法。
- en: References
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please refer to the following resources to gain additional information:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下资源以获取更多信息：
- en: '*The $300m flip flop: how real-estate site Zillow''s side hustle went badly
    wrong*, *theguardian.com*: [https://www.theguardian.com/business/2021/nov/04/zillow-homes-buying-selling-flip-flop](https://www.theguardian.com/business/2021/nov/04/zillow-homes-buying-selling-flip-flop)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《3亿美元的双面胶：房地产网站Zillow的副业为何出错》，*theguardian.com*：[https://www.theguardian.com/business/2021/nov/04/zillow-homes-buying-selling-flip-flop](https://www.theguardian.com/business/2021/nov/04/zillow-homes-buying-selling-flip-flop)'
- en: '*Zillow says it''s closing homebuying business, cutting 25% of workforce; earnings
    miss estimates*, *cnbc.com*: [https://www.cnbc.com/2021/11/02/zillow-shares-plunge-after-announcing-it-will-close-home-buying-business.html](https://www.cnbc.com/2021/11/02/zillow-shares-plunge-after-announcing-it-will-close-home-buying-business.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Zillow表示将关闭购房业务，裁员25%，收益未达到预期*，*cnbc.com*：[https://www.cnbc.com/2021/11/02/zillow-shares-plunge-after-announcing-it-will-close-home-buying-business.html](https://www.cnbc.com/2021/11/02/zillow-shares-plunge-after-announcing-it-will-close-home-buying-business.html)'
- en: '*Google Photos Tags Two African-Americans As Gorillas Through Facial Recognition
    Software*, *forbes.com*: [https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/](https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过面部识别软件将两名非洲裔美国人标记为大猩猩的谷歌照片*, *forbes.com*: [https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/](https://www.forbes.com/sites/mzhang/2015/07/01/google-photos-tags-two-african-americans-as-gorillas-through-facial-recognition-software/)'
- en: '*Google apologises for Photos app''s racist blunder*, *bbc.com*: [https://www.bbc.com/news/technology-33347866](https://www.bbc.com/news/technology-33347866)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*谷歌就照片应用中的种族错误道歉*, *bbc.com*: [https://www.bbc.com/news/technology-33347866](https://www.bbc.com/news/technology-33347866)'
- en: '*Tutorial on Interpretable Machine Learning, MICCAI''18, by Samek and Binder*:
    [http://www.heatmapping.org/slides/2018_MICCAI.pdf](http://www.heatmapping.org/slides/2018_MICCAI.pdf)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释机器学习教程，MICCAI''18，由Samek和Binder编写*: [http://www.heatmapping.org/slides/2018_MICCAI.pdf](http://www.heatmapping.org/slides/2018_MICCAI.pdf)'
- en: '[*Chapter 1*](B18216_01_ePub.xhtml#_idTextAnchor014) *–* *Interpretation, Interpretability,
    and Explainability and Why Does It All Matter?* by *Serg Masis, Packt Publishing
    Ltd.:* [https://www.amazon.com/Interpretable-Machine-Learning-Python-hands/dp/180020390X](https://www.amazon.com/Interpretable-Machine-Learning-Python-hands/dp/180020390X)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第一章*](B18216_01_ePub.xhtml#_idTextAnchor014) *–* *解释、可解释性和可解释性以及为什么这一切都如此重要？*
    by *塞尔吉·马西斯，Packt Publishing Ltd.:* [https://www.amazon.com/Interpretable-Machine-Learning-Python-hands/dp/180020390X](https://www.amazon.com/Interpretable-Machine-Learning-Python-hands/dp/180020390X)'
- en: '*Interpretable Machine Learning*, *Christoph Molnar:* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可解释机器学习*, *克里斯托夫·莫拉尔:* [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
- en: '*Towards A Rigorous Science of Interpretable Machine Learning by Doshi-Velez
    and Kim*: [https://arxiv.org/abs/1702.08608](https://arxiv.org/abs/1702.08608)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过Doshi-Velez和Kim的研究，迈向可解释机器学习的严谨科学*: [https://arxiv.org/abs/1702.08608](https://arxiv.org/abs/1702.08608)'
