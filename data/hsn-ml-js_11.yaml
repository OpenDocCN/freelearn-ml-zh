- en: Using Machine Learning in Real-Time Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在实时应用中使用机器学习
- en: Throughout this book, you have learned many ML algorithms and techniques. What
    remains, however, is to deploy these algorithms into real-world applications.
    This chapter is dedicated to those pieces of advice related to using ML in the
    real world, in real applications, and in production environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你已经学习了许多机器学习算法和技术。然而，剩下的工作是将这些算法部署到现实世界的应用中。本章专门讨论与在现实世界、实际应用和生产环境中使用机器学习相关的建议。
- en: There are many differences between idealized usage of ML algorithms and real-world
    usage. In our examples, we both train and execute models in one step, in response
    to one command. We assume that the models do not need to be serialized, saved,
    or reloaded in any way. We have not thought about user interface responsiveness,
    executing on mobile devices, or building API interfaces between clients and servers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理想化的机器学习算法使用与实际使用之间存在许多差异。在我们的示例中，我们一步训练和执行模型，响应一个命令。我们假设模型不需要以任何方式序列化、保存或重新加载。我们没有考虑用户界面的响应性、在移动设备上执行或构建客户端和服务器之间的API接口。
- en: Real applications may also have a scope several orders of magnitude larger than
    the examples we've discussed. How do you train an ANN with billions of data points
    in a dataset? How do you collect, store, and process that amount of information?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 真实应用的范围可能比我们讨论的例子大几个数量级。你如何在一个包含数十亿数据点的数据集中训练一个人工神经网络（ANN）？你如何收集、存储和处理这么多的信息？
- en: 'In this chapter, we''ll discuss the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Frontend architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端架构
- en: Backend architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后端架构
- en: Data pipelining
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道
- en: Tools and services you can use to build a production ML system
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用来构建生产级机器学习系统的工具和服务
- en: Serializing models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列化模型
- en: Our examples throughout this book have built, trained, and tested models only
    to destroy them a millisecond later. We can get away with this because our examples
    use limited training data and, at worst, take only a few minutes to train. Production
    applications will typically use much more data and require more time to train.
    In production applications, the trained model itself is a valuable asset that
    should be stored, saved, and loaded on demand. In other words, our models must
    be serializable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的示例仅构建、训练和测试模型，然后在毫秒后将其销毁。我们之所以能够这样做，是因为我们的示例使用的是有限的训练数据，最坏的情况也只需要几分钟就能完成训练。在实际应用中，通常会使用更多的数据，并且需要更多的时间来训练。在生产应用中，训练好的模型本身是一项宝贵的资产，应该根据需要存储、保存和加载。换句话说，我们的模型必须是可序列化的。
- en: Serialization itself is typically not a difficult issue. Models are essentially
    a compressed version of the training data. Some models can indeed be very large,
    but they will still be a fraction of the size of the data that trained them. What
    makes the topic of serialization challenging is that it opens up many other architectural
    questions that you will have to consider, the first being the question of where
    and how to store the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化本身通常不是一个难题。模型本质上是对训练数据的压缩版本。一些模型确实可能非常大，但它们仍然只是训练它们的数据大小的一小部分。使序列化问题变得具有挑战性的是，它引发了许多其他架构问题，你必须考虑的第一个问题就是模型存储的位置和方式。
- en: Disappointingly, there's no right answer. Models can be stored nearly anywhere
    depending on their size, complexity, frequency of use, available technology, and
    so on. Naive Bayes classifiers require only the storage of token and document
    counts and use only key/value lookups with no advanced querying, so a single Redis
    server can host a huge classifier trained on billions of documents. Very large
    models can be serialized into a dedicated database, perhaps even a dedicated graph
    database cluster. Moderately sized models may be serialized as JSON or a binary
    format and stored in a database BLOB field, hosted on a file server or API such
    as Amazon S3, or stored in browser local storage if it is small enough.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 令人失望的是，没有正确答案。模型可以根据其大小、复杂性、使用频率、可用技术等因素存储在几乎任何地方。朴素贝叶斯分类器只需要存储标记和文档计数，并且仅使用键/值查找，没有高级查询，因此单个Redis服务器可以托管一个在数十亿文档上训练的巨大分类器。非常大的模型可以序列化到一个专用数据库中，甚至可能是一个专用的图数据库集群。中等大小的模型可以序列化为JSON或二进制格式，并存储在数据库的BLOB字段中，托管在文件服务器或API（如Amazon
    S3）上，或者如果足够小，可以存储在浏览器本地存储中。
- en: Most ML libraries have serialization and deserialization built in, as ultimately
    this functionality is dependent on the implementation details of the library.
    Most libraries include methods such as `save()` and `load()`, though you will
    want to refer to the documentation of the specific library you're using.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习库都内置了序列化和反序列化功能，因为最终这种功能依赖于库的实现细节。大多数库包括`save()`和`load()`等方法，但你仍需参考你所使用的特定库的文档。
- en: Make sure to include serialization functionality when writing your own libraries.
    If you want to support multiple storage backends, it would be best to decouple
    the serialization functionality from the core logic and implement a driver and
    interface architecture instead.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在编写自己的库时包含序列化功能。如果你想支持多个存储后端，最好将序列化功能与核心逻辑解耦，并实现一个驱动程序和接口架构。
- en: This is just the first of the questions we'll need to answer now that we have
    a serializable model. Serializable models are also portable, which means they
    can be moved from machine to machine. You can download a pretrained model onto
    a smartphone for offline usage, for instance. Your JavaScript application can
    use a web worker to download and maintain a ready-to-use model for speech detection,
    ask for microphone permission, and make a website navigable solely by voice commands—all
    through a Chrome extension.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们现在需要回答的第一个问题，因为我们已经有一个可序列化的模型。可序列化模型也是可移植的，这意味着它们可以从一台机器移动到另一台机器。例如，你可以将预训练模型下载到智能手机上进行离线使用。你的JavaScript应用程序可以使用Web
    Worker下载并维护一个用于语音检测的现成模型，请求麦克风权限，并通过Chrome扩展仅通过语音命令使网站可导航。
- en: In this section, we'll discuss the various architectural considerations that
    arise once your model is serializable and portable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一旦模型可序列化和可移植后出现的各种架构考虑因素。
- en: Training models on the server
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在服务器上训练模型
- en: Due to the time, data, processing power, and memory requirements involved in
    training sophisticated models, it's often desirable to train models on the server
    rather than the client. Depending on the use case, even the evaluation of models
    may need to occur on the server.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练复杂模型涉及的时间、数据、处理能力和内存需求，通常在服务器上而不是在客户端训练模型是可取的。根据用例，模型的评估也可能需要在服务器上完成。
- en: There are a few paradigms to consider in terms of where to train and where to
    evaluate models. Your options, in general, will be to train and evaluate fully
    on the server, train and evaluate fully on the client, or to train on the server
    but evaluate on the client. Let's explore some examples of each paradigm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑模型训练和评估的位置方面，有几个范例需要考虑。一般来说，你的选择将是完全在服务器上训练和评估，完全在客户端训练和评估，或者是在服务器上训练但在客户端评估。让我们探讨每个范例的一些示例。
- en: The simplest implementation is to both train and evaluate models on the server.
    The main advantage of this approach is that you get to determine and control the
    entire execution environment of the model. You can easily analyze the server load
    required to train and execute a model and scale your servers as necessary. It's
    easier for a server you fully control to get access to a large corpus of training
    data, as the data is most likely in a database that you also control. You won't
    have to worry about which version of JavaScript your clients are running or whether
    you will have access to the client's GPU for training. Training and executing
    models on the server also means that there is no additional load on the client
    machine due to the model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的实现方式是在服务器上同时训练和评估模型。这种方法的优点在于你可以决定并控制模型的整个执行环境。你可以轻松分析训练和执行模型所需的服务器负载，并根据需要调整服务器规模。由于数据很可能存储在你也控制的数据库中，因此完全控制的服务器更容易访问大量训练数据。你不必担心客户端运行的是哪种版本的JavaScript，或者你是否能够访问客户端的GPU进行训练。在服务器上训练和执行模型还意味着由于模型的存在，客户端机器不会增加额外的负载。
- en: The primary downside of a fully server-side approach is that it requires a well-designed
    and robust API. If you have an application which requires quick response times
    for a model evaluation, you will need to ensure that your API can serve results
    quickly and reliably. This approach also means that offline evaluation of models
    is not possible; the client will require a connection to your server in order
    for anything to work. Most applications or products billed as **Software as a
    Service** (**SaaS**) will use the server-side model, and this approach should
    be the first one you consider if you are providing a paid service to customers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 完全服务器端方法的缺点主要是需要设计良好的、健壮的API。如果你有一个需要快速响应时间的模型评估的应用，你需要确保你的API能够快速且可靠地提供服务。这种方法还意味着无法进行离线模型评估；客户端需要连接到你的服务器才能使任何操作生效。大多数被称为**软件即服务**（**SaaS**）的应用或产品将使用服务器端模型，如果你在向客户提供付费服务，这种方法应该是你首先考虑的。
- en: Models can conversely be fully trained and evaluated on the client. In this
    case, the client itself will need access to the training data and it will need
    sufficient processing power to train a model. This approach is generally not appropriate
    for models that require large training sets or long training times, as there is
    no way to ensure that the client's device will be able to process the data. You
    will also have to contend with older devices which may not have a GPU or the processing
    power to train even simple models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，模型也可以在客户端完全进行训练和评估。在这种情况下，客户端本身需要访问训练数据，并且需要足够的处理能力来训练模型。这种方法通常不适用于需要大量训练集或长时间训练时间的模型，因为没有办法确保客户端的设备能够处理数据。你还得应对那些可能没有GPU或处理能力训练甚至简单模型的旧设备。
- en: However, client-side training and evaluation is a good approach for applications
    which require a high level of data privacy or data ownership in cases where the
    training data originates from the device itself. Restricting the processing to
    the client device ensures that the user's data is not transmitted to any third-party
    server and can be deleted directly by the user. Applications such as fingerprint
    scanning, biometrics analysis, location data analysis, phone call analysis, and
    so on, are good candidates for a fully client-side approach. This approach also
    ensures that models can be trained and evaluated offline, with no need for an
    internet connection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于训练数据来自设备本身且需要高度数据隐私或数据所有权的应用来说，客户端训练和评估是一个很好的方法。将处理限制在客户端设备上可以确保用户数据不会被传输到任何第三方服务器，并且可以直接由用户删除。指纹扫描、生物识别分析、位置数据分析、电话分析等应用是采用完全客户端方法的良好候选者。这种方法还确保了模型可以在离线状态下进行训练和评估，无需互联网连接。
- en: A hybrid approach can blend the best of both worlds in some cases. Advanced
    models that require a lot of training data can be trained on a server and serialized.
    A client, when it first connects to your application, can download and store the
    trained model for offline usage. The client itself becomes responsible for evaluating
    the model, but does not need to train the model in this case.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，混合方法可以将两者的优点结合起来。需要大量训练数据的高级模型可以在服务器上训练并序列化。客户端在首次连接到你的应用时，可以下载并存储训练好的模型以供离线使用。客户端本身负责评估模型，但在此情况下不需要训练模型。
- en: The hybrid approach allows you to train and periodically update sophisticated
    models on the server. A serialized model is much smaller than the original training
    data, and therefore can be delivered to a client for offline evaluation. As long
    as both the client and the server use compatible libraries or algorithms (that
    is, `TensorFlow.js` on both sides), the client can take advantage of the processing
    power of the server for training but use its own offline processing capabilities
    for the much less demanding evaluation step.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法允许你在服务器上训练和定期更新复杂模型。序列化模型比原始训练数据小得多，因此可以发送到客户端进行离线评估。只要客户端和服务器使用兼容的库或算法（即，两边都使用`TensorFlow.js`），客户端就可以利用服务器的处理能力进行训练，但在对评估步骤要求较低的情况下，使用自己的离线处理能力。
- en: An example use case for the hybrid model is speech or image recognition, perhaps
    for an AI assistant or **Augmented Reality** (**AR**) application. In the case
    of an AR application, the server is responsible for maintaining millions of training
    images and training (for example) an RNN to classify objects. Once the training
    is complete, this model can be serialized, stored, and downloaded by the client.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型的示例用例包括语音或图像识别，可能是用于人工智能助手或**增强现实（AR**）应用程序。在AR应用程序的情况下，服务器负责维护数百万个训练图像并训练（例如）一个RNN来分类物体。一旦训练完成，这个模型就可以被序列化、存储并由客户端下载。
- en: Let's imagine an AR application that connects to the device's camera and displays
    an annotated video feed that identifies objects. When the application first starts, the
    client downloads the AR RNN model and stores it in the device's local storage
    along with version information. When the video feed first starts, the application
    retrieves the model from storage and deserializes it into the client's own RNN
    implementation. Ideally, the client's RNN implementation will use the same library
    and version as the library on the server.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个增强现实（AR）应用程序，该程序连接到设备的摄像头并显示一个标注的视频流，用于识别物体。当应用程序首次启动时，客户端会下载AR RNN模型并将其存储在设备的本地存储中，同时存储版本信息。当视频流首次启动时，应用程序从存储中检索模型并将其反序列化到客户端自己的RNN实现中。理想情况下，客户端的RNN实现将使用与服务器上相同的库和版本。
- en: In order to classify and annotate every frame of the video, the client would
    need to do all the necessary work in just 16 ms (for 60 FPS video). This is achievable,
    but in practice not every frame is used for classification; 1 of every 3 frames
    (50 ms apart) would suffice. The hybrid approach shines here; the application
    would suffer a major performance penalty if each frame of a video had to be uploaded
    to a server, evaluated, and then returned. Even with a fantastic model performance—a
    model that evaluates in, say, 5 ms—you could experience an additional 100 ms lag
    due to the round-trip time required by an HTTP request.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对视频的每一帧进行分类和标注，客户端需要在仅仅16毫秒内（对于60 FPS的视频）完成所有必要的工作。这是可行的，但在实践中并非每一帧都用于分类；每3帧中就有1帧（相隔50毫秒）就足够了。混合方法在这里表现出色；如果视频的每一帧都需要上传到服务器、评估然后返回，应用程序将遭受严重的性能损失。即使模型性能非常出色——例如，模型评估需要5毫秒——你也可能因为HTTP请求所需的往返时间而额外体验100毫秒的延迟。
- en: Under the hybrid approach, the client does not need to ship the image to a server
    for evaluation but instead can evaluate the image immediately, based on the previously
    trained model now loaded into memory. A well-designed client will periodically
    check the server for updates to the model and update it when necessary, but will
    still allow outdated models to run offline. Users are happiest when applications *just
    work*, and the hybrid model gives you both performance and resilience. Servers
    are relied upon only for tasks that can happen asynchronously, such as downloading
    updated models or sending information back to the server.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合方法下，客户端不需要将图像发送到服务器进行评估，而是可以直接根据现在加载到内存中的先前训练模型立即评估图像。一个设计良好的客户端会定期检查服务器以获取模型更新，并在必要时更新它，但仍然允许过时的模型离线运行。当应用程序“正常工作”时，用户最满意，混合模型为你提供了性能和弹性。服务器仅用于可以异步进行的任务，例如下载更新模型或将信息发送回服务器。
- en: The hybrid approach, therefore, is best for use cases where a large, sophisticated
    model is needed but the evaluation of the model either needs to happen very quickly
    or offline. This is not a hard-and-fast rule, of course. There are many other
    situations where a hybrid approach makes the most sense; if you have many clients
    and cannot afford the server resources to process all their evaluations, you might
    use the hybrid approach to offload your processing responsibilities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，混合方法最适合需要大型、复杂模型但模型评估需要非常快速或离线进行的用例。当然，这不是一个绝对规则。还有许多其他情况下，混合方法最为合理；如果你有多个客户端且无法承担服务器资源来处理所有他们的评估，你可能使用混合方法来卸载你的处理责任。
- en: Care must be taken when designing a client application that performs model training
    or evaluation. While evaluation is a lot faster than training, it is still nontrivial
    and may cause UI performance issues on the client if not implemented correctly.
    In the next section, we'll look at a modern web browser feature called **web workers**
    that can be used to perform processing in a standalone thread, keeping your UI
    responsive.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计执行模型训练或评估的客户端应用程序时，必须格外小心。虽然评估比训练快得多，但如果实现不当，它仍然是非平凡的，可能会在客户端引起 UI 性能问题。在下一节中，我们将探讨一个现代网络浏览器功能，称为
    **web workers**，它可以用于在独立线程中执行处理，保持你的 UI 响应。
- en: Web workers
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web workers
- en: If you're developing for a web browser application, you'll certainly want to
    use a web worker to manage the model in the background. Web workers are a browser-specific
    feature intended to allow background processing, which is exactly what we want
    when we're dealing with large models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在为网络浏览器应用程序开发，你当然会想使用 web worker 在后台管理模型。Web workers 是一个浏览器特定功能，旨在允许后台处理，这正是我们在处理大型模型时想要的。
- en: Web workers can interact with `XMLHttpRequest`, `IndexedDB`,and `postMessage`.
    A web worker can download a model from the server with `XMLHttpRequest`, store
    it locally with `IndexedDB`, and communicate with the UI thread with `postMessage`.
    These three tools, used together, provide a complete foundation for a responsive,
    performant, and potentially offline experience. Other JavaScript platforms, such
    as React Native, also have similar faculties for HTTP requests, data storage,
    and interprocess communication.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Web workers 可以与 `XMLHttpRequest`、`IndexedDB` 和 `postMessage` 交互。Web worker 可以使用
    `XMLHttpRequest` 从服务器下载模型，使用 `IndexedDB` 本地存储它，并使用 `postMessage` 与 UI 线程通信。这三个工具结合使用，为响应式、高性能以及可能离线体验提供了完整的基础。其他
    JavaScript 平台，如 React Native，也具有类似的 HTTP 请求、数据存储和进程间通信功能。
- en: Web workers can be combined with other browser-specific features such as **service
    workers** and device APIs to provide a full offline experience. Service workers
    can cache specific assets for offline use or intelligently switch between online
    and offline evaluation. The browser extension platforms, as well as mobile platforms
    such as React Native, also provide a number of mechanisms for supporting cached
    data, background threads, and offline use.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Web workers 可以与其他浏览器特定功能（如 **service workers** 和设备 API）结合使用，以提供完整的离线体验。Service
    workers 可以缓存特定资产以供离线使用，或智能地在在线和离线评估之间切换。浏览器扩展平台以及如 React Native 这样的移动平台也提供了一系列机制来支持缓存数据、后台线程和离线使用。
- en: 'Regardless of the platform, the concepts are the same: the application should
    download and upload data asynchronously when an internet connection is available;
    the application should cache (and version) everything it needs to function, like
    a pretrained model; and the application should evaluate the model independently
    of the UI.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是哪个平台，概念都是相同的：当有互联网连接时，应用程序应该异步下载和上传数据；应用程序应该缓存（并版本控制）它需要运行的任何内容，如预训练模型；并且应用程序应该独立于
    UI 评估模型。
- en: It would be easy to mistakenly assume that a model is small and fast enough
    to run in the same thread as the UI. If your average evaluation time is only 5
    ms and you only need one evaluation every 50 ms, it's tempting to become complacent
    and skip the additional detail of evaluating your model in a separate thread.
    However, the range of devices on the market today make it so you cannot even assume
    an order-of-magnitude similarity in performance. If you've tested your application
    on a modern phone with a GPU, for example, you may not be able to accurately assess
    how it will perform on an older phone's CPU. The evaluation time might jump from
    5 ms to 100 ms. In a poorly designed application this will result in UI lag or
    freezing, but in a well-designed application, the UI will remain responsive but
    with less frequent updates.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 容易错误地假设模型足够小且运行速度快，可以与 UI 在同一线程中运行。如果你的平均评估时间仅为 5 毫秒，并且你每 50 毫秒只需要进行一次评估，那么可能会变得自满，并跳过在单独线程中评估模型的额外细节。然而，市场上各种设备的范围使得你甚至不能假设性能上有数量级的相似性。例如，如果你在一个带有
    GPU 的现代手机上测试了你的应用程序，你可能无法准确评估它在旧手机 CPU 上的性能。评估时间可能会从 5 毫秒跳到 100 毫秒。在设计不良的应用程序中，这会导致
    UI 延迟或冻结，但在设计良好的应用程序中，UI 将保持响应，但更新频率较低。
- en: Fortunately, web workers and the `postMessage` API are simple to use. The `IndexedDB`
    API is a low-level API and may be daunting to use initially, but there are many
    user-friendly libraries that abstract the details away. The specific manner in
    which you download and store the pretrained model is solely dependent on the implementation
    details of your application and the specific ML algorithm you've chosen. Smaller
    models can be serialized as JSON and stored in `IndexedDB`; more advanced models
    can be integrated directly into `IndexedDB`. Make sure to include a mechanism
    for comparing version information in your server-side API; you should have a way
    to ask the server what the current version of the model is and be able to compare
    that to your own copy so that you can invalidate and update the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Web Worker和`postMessage` API使用简单。`IndexedDB` API是一个低级API，最初可能难以使用，但有许多用户友好的库可以抽象出细节。你下载和存储预训练模型的具体方式完全取决于你应用程序的实现细节和所选的具体机器学习算法。较小的模型可以序列化为JSON并存储在`IndexedDB`中；更高级的模型可以直接集成到`IndexedDB`中。确保在你的服务器端API中包含一个比较版本信息的机制；你应该有一种方法可以询问服务器当前模型的版本，并将其与自己的副本进行比较，以便可以使其无效并更新模型。
- en: Put some thought into the design of your web worker's message-passing API as
    well. You will use the `postMessage`API (available in all major browsers) for
    communication between the UI thread and the background thread. This communication
    should, at the very least, include some way to check on the status of the model
    and a way to send a data point to the model for evaluation. But you'll also want
    to look forward to future functionality and make your API flexible and future-proof.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计你的Web Worker的消息传递API时也要多加思考。你将使用`postMessage`API（在所有主流浏览器中都可用）来在UI线程和后台线程之间进行通信。这种通信至少应该包括检查模型状态的方法以及向模型发送数据点以供评估的方法。但你也会希望展望未来的功能，并使你的API灵活且具有前瞻性。
- en: Two examples of functionality you might want to plan for are continually improving
    models, that retrain themselves based on user feedback, and per-user models that
    learn the behaviors or preferences of individual users.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要计划的功能示例包括持续改进的模型，这些模型根据用户反馈重新训练，以及针对每个用户的模型，这些模型学习单个用户的行为或偏好。
- en: Continually improving and per-user models
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续改进和针对每个用户的模型
- en: Throughout the life cycle of your application, it's likely that the end user
    is going to interact with your model in some way. Often, this interaction can
    be used as feedback to further train the model. The interaction may also be used
    to customize the model to the user, tailoring it to their own interests and behaviors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在你应用程序的生命周期中，最终用户很可能会以某种方式与你的模型进行交互。通常，这种交互可以用作进一步训练模型的反馈。这种交互还可以用来根据用户的需求定制模型，以适应他们的兴趣和行为。
- en: A good example of both concepts is the spam filter. A spam filter should continually
    improve as users mark messages as spam. Spam filters are most powerful when they
    have lots of data points to train against, and this data can come from other users
    of the application. Any time a user marks a message as spam, that knowledge should
    be applied to the model and other users should also be able to enjoy the automatic
    improvement in their own spam filtering.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 两个概念的良例是垃圾邮件过滤器。垃圾邮件过滤器应该随着用户将消息标记为垃圾邮件而不断改进。当垃圾邮件过滤器拥有大量数据点用于训练时，它们最为强大，而这些数据可以来自应用程序的其他用户。每当用户将一条消息标记为垃圾邮件时，这种知识应该应用于模型，并且其他用户也应该能够享受到他们自己垃圾邮件过滤器的自动改进。
- en: Spam filters are also a good example of models that should be customizable per
    user. What I think is spam may not be the same as what you think is spam. I aggressively
    mark marketing emails and newsletters that I haven't signed up for as spam, but
    other users may want to see those types of messages in their own inbox. At the
    same time, there are messages that everyone agrees are spam, so it would be good
    to design our application to use a central, continually updating model that can
    be locally refined to better fit the behavior of the specific user.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤器也是应该针对每个用户定制的模型的良例。我认为是垃圾邮件的东西可能和你认为的不同。我积极地标记那些我没有注册的营销邮件和新闻通讯为垃圾邮件，但其他用户可能希望在自己的收件箱中看到这些类型的消息。同时，有些消息是每个人都同意是垃圾邮件的，因此设计我们的应用程序以使用一个中央、持续更新的模型会很好，这个模型可以本地优化以更好地适应特定用户的行为。
- en: Bayesian classifiers fit this description very well, as Bayes' theorem is designed
    to be updated by new information. In [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml), *Classification
    Algorithms*, we discussed an implementation of the Naive Bayes classifier that
    handles rare words gracefully. In that scheme, a weight factor skewed word probabilities
    towards neutral so that rare words wouldn't influence the model too strongly.
    A per-user spam filter can use this same technique but instead of skewing words
    towards neutrality, we can skew them towards the central model's probability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯分类器非常适合这种描述，因为贝叶斯定理是为了通过新信息进行更新而设计的。在[第5章](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml)，“分类算法”中，我们讨论了Naive
    Bayes分类器的实现，该实现能够优雅地处理稀有词汇。在该方案中，一个权重因子将词汇概率偏向中性，这样稀有词汇就不会对模型产生过强的干扰。一个针对用户的垃圾邮件过滤器可以使用同样的技术，但不是将词汇偏向中性，而是偏向中心模型的概率。
- en: The rare word weight factor, in this usage, becomes a weight factor that balances
    the central model against the local model. The larger you make the weight factor,
    the more important the central model becomes and the longer it will take for the
    user to affect the local model. A smaller weight factor will be more responsive
    to user feedback, but may also cause irregularities in performance. In a typical
    rare word implementation the weight factor is in the range from 3 to 10\. In a
    per-user model, however, the weight factor should be larger—perhaps 50 – 1,000—in
    consideration of the fact that the central model is trained by millions of examples
    and should not be easily overridden by just a handful of local examples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种用法中，稀有词汇的权重因子变成了一个平衡中心模型和本地模型的权重因子。你使权重因子越大，中心模型就越重要，用户影响本地模型所需的时间就越长。较小的权重因子将更敏感于用户反馈，但也可能导致性能的不规律。在典型的稀有词汇实现中，权重因子在3到10的范围内。然而，在针对用户的模型中，权重因子应该更大——可能是50到1,000，考虑到中心模型是由数百万个示例训练的，不应该轻易被少量本地示例所覆盖。
- en: Care must be taken when sending data back to the server for continual model
    improvements. You should not transmit the email message back to the server, as
    that would create an unnecessary security risk—*especially* if your product is
    not an email hosting provider but only an email client. If you are also the email
    hosting provider, then you can simply send the email ID back to the server to
    be marked as spam and given to the model for training; the client and the server
    will maintain their own models separately. If you are not the email hosting provider,
    then you should take extra care to secure your user's data. If you must transmit
    a token stream back to the server, then you should encrypt it in transit as well
    as anonymize it. You may also consider using a tokenizer that salts and hashes
    the tokens (for example, with sha1 or hmac) after tokenizing and stemming the
    content. The classifier will work just as well with hashed data as it does with
    readable data, but will add an additional layer of obfuscation. Finally, make
    sure that the HTTP request and raw token data is not logged. Once the data enters
    the model (in the form of token counts) it is sufficiently anonymized, but make
    sure there is no way a spy can relate a specific token stream to a specific user.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据发送回服务器以进行持续模型改进时，必须小心谨慎。你不应该将电子邮件消息发送回服务器，因为这会创建一个不必要的安全风险——*尤其是*如果你的产品不是一个电子邮件托管服务提供商，而只是一个电子邮件客户端。如果你也是电子邮件托管服务提供商，那么你可以简单地发送电子邮件ID回服务器，将其标记为垃圾邮件并供模型训练；客户端和服务器将分别维护自己的模型。如果你不是电子邮件托管服务提供商，那么你应该格外小心，确保用户数据的安全。如果你必须将令牌流发送回服务器，那么你应该在传输过程中对其进行加密，并对其进行匿名化。你也可以考虑使用一个在分词和词干提取后对令牌进行盐化和散列的标记器（例如，使用sha1或hmac）。分类器在处理散列数据时与处理可读数据一样有效，但会添加一个额外的混淆层。最后，确保不要记录HTTP请求和原始令牌数据。一旦数据以令牌计数的形式进入模型，它就足够匿名化了，但请确保间谍无法将特定的令牌流与特定的用户联系起来。
- en: Naive Bayes classifiers are not the only models that can be continually updated
    or customized per user, of course. Continual updating of a model is possible with
    most ML algorithms. If a user indicates that an RNN got an image classification
    wrong, that user's data point can be added to the model's training set and the
    model can either be fully retrained at periodic intervals, or can be batch-updated
    with newer training examples.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，朴素贝叶斯分类器并不是唯一可以持续更新或根据用户定制的模型。大多数机器学习算法都支持模型的持续更新。如果一个用户指出一个循环神经网络（RNN）在图像分类上犯了错误，那么这个用户的数据点可以被添加到模型的训练集中，模型可以定期完全重新训练，或者可以与新训练示例一起批量更新。
- en: Some algorithms support truly live updates of the model. The Naive Bayes classifier
    requires only an update to token and document counts, which might even be stored
    in memory. The knn and k-means algorithms similarly allow data points to be added
    to the model at any time. Some ANNs, like those used in reinforcement learning,
    also rely on live feedback.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法支持真正实时的模型更新。朴素贝叶斯分类器只需要更新标记和文档计数，这些甚至可能存储在内存中。knn和k-means算法类似地允许在任何时候将数据点添加到模型中。一些用于强化学习的ANN（人工神经网络）也依赖于实时反馈。
- en: Other algorithms are better updated periodically in batches. These algorithms
    typically rely on gradient descent or stochastic methods and require a feedback
    loop over many examples during training; examples are ANNs and random forests.
    An ANN model can indeed be retrained with a single data point, but batch training
    is far more effective. Be careful not to overfit models as you update them; too
    much training is not always a good thing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法更适合定期批量更新。这些算法通常依赖于梯度下降或随机方法，并在训练期间需要许多示例的反馈循环；例如，ANN和随机森林。确实可以使用单个数据点重新训练ANN模型，但批量训练更有效。在更新模型时，请注意不要过拟合模型；过多的训练并不总是好事。
- en: In some cases, it is better to fully retrain a model based on the updated training
    set. One reason to do this is to avoid overfitting short-term trends in training
    data. By fully retraining a model, you ensure that recent training examples have
    the same weight as old training examples; this may or may not be desired. If models
    are periodically and automatically retrained, make sure that the training algorithm
    is looking at the right signals. It should be able to balance accuracy, loss,
    and variance in order to develop reliable models. As much of ML training is stochastic
    in nature, there is no guarantee that two training runs will finish to the same
    level of quality or in similar amounts of time. Your training algorithm should
    control for these factors and be able to discard bad models if necessary, for
    instance if a target accuracy or loss was not achieved within a maximum limit
    on the number of training epochs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，最好基于更新的训练集完全重新训练模型。这样做的一个原因是为了避免训练数据中的短期趋势过拟合。通过完全重新训练模型，你可以确保最近的训练示例与旧的训练示例具有相同的权重；这可能是或可能不是所希望的。如果模型定期自动重新训练，请确保训练算法正在查看正确的信号。它应该能够平衡准确性、损失和方差，以开发可靠的模型。由于机器学习训练在很大程度上是随机的，因此不能保证两次训练运行将以相同的质量或相似的时间完成。你的训练算法应该控制这些因素，并在必要时能够丢弃不良模型，例如，如果在最大训练轮数限制内没有达到目标准确性或损失。
- en: 'A new question arises at this point: how do you collect, store, and process
    gigabytes or terabytes of training data? How and where do you store and distribute
    serialized models to clients? How do you collect new training examples from millions
    of users? This topic is called data pipelining, which we''ll discuss next.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，一个新的问题出现了：你如何收集、存储和处理数GB或TB的训练数据？你如何以及在哪里存储和分发序列化模型给客户？你如何从数百万用户那里收集新的训练示例？这个话题被称为数据管道，我们将在下一节讨论。
- en: Data pipelines
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道
- en: When developing a production ML system, it's not likely that you will have the
    training data handed to you in a ready-to-process format. Production ML systems
    are typically part of larger application systems, and the data that you use will
    probably originate from several different sources. The training set for an ML
    algorithm may be a subset of your larger database, combined with images hosted
    on a **Content Delivery Network** (**CDN**) and event data from an Elasticsearch
    server. In our examples, we have been given an isolated training set, but in the
    real world we will need to generate the training set in an automated and repeatable
    manner.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发生产级ML系统时，你不太可能得到以可处理格式提供的训练数据。生产级ML系统通常是更大应用程序系统的一部分，你使用的数据可能来自多个不同的来源。ML算法的训练集可能是你更大数据库的一个子集，结合存储在**内容分发网络**（**CDN**）上的图像和来自Elasticsearch服务器的的事件数据。在我们的示例中，我们得到了一个隔离的训练集，但在现实世界中，我们需要以自动化和可重复的方式生成训练集。
- en: The process of ushering data through various stages of a life cycle is called **data
    pipelining**. Data pipelining may include data selectors that run SQL or Elasticsearch
    queries for objects, event subscriptions which allow data to flow in from event-or
    log-based data, aggregations, joins, combining data with data from third-party
    APIs, sanitization, normalization, and storage.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据引导通过生命周期各个阶段的过程被称为**数据管道**。数据管道可能包括运行SQL或Elasticsearch查询的对象选择器，允许基于事件或日志的数据流入的事件订阅，聚合，连接，将数据与第三方API的数据结合，净化，标准化和存储。
- en: In an ideal implementation, the data pipeline acts as an abstraction layer between
    the larger application environment and the ML process. The ML algorithm should
    be able to read the output of the data pipeline without any knowledge of the original
    source of the data, similar to our examples. Under this approach, the ML algorithm
    will not need to understand the implementation details of the application; it
    is the pipeline itself that is responsible for knowing how the application is
    built.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的实现中，数据管道充当了更大应用程序环境和ML过程之间的抽象层。ML算法应该能够读取数据管道的输出，而不需要了解数据的原始来源，类似于我们的示例。在这种方法下，ML算法不需要了解应用程序的实现细节；管道本身负责知道应用程序是如何构建的。
- en: 'As there are many possible data sources and infinite ways to architect an application,
    there is no one-size-fits-all data pipeline. However, most data pipelines will
    contain these components, which we will discuss in the following sections:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可能存在许多可能的数据源和无限多的应用程序架构方式，没有一种数据管道可以适用于所有情况。然而，大多数数据管道将包含以下组件，我们将在接下来的章节中讨论：
- en: Data querying and event subscription
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据查询和事件订阅
- en: Data joining or aggregation
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据连接或聚合
- en: Transformation and normalization
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换和标准化
- en: Storage and delivery
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和交付
- en: Let's take a look at each of these concepts and introduce some tools and techniques
    that can achieve them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些概念，并介绍一些可以实现它们的工具和技术。
- en: Data querying
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据查询
- en: Imagine an application such as Disqus, which is an embeddable comment form that
    website owners can use to add comment functionality to blog posts or other pages.
    The primary functionality of Disqus is to allow users to like or leave comments
    on posts, however, as an additional feature and revenue stream, Disqus can make
    content recommendations and display them alongside sponsored content. The content
    recommendation system is an example of an ML system that is only one feature of
    a larger application.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下像Disqus这样的应用程序，它是一个可嵌入的评论表单，网站所有者可以使用它来为博客文章或其他页面添加评论功能。Disqus的主要功能是允许用户对帖子进行点赞或留言，然而，作为一个额外的功能和收入来源，Disqus可以提供内容推荐并在赞助内容旁边展示它们。内容推荐系统是一个ML系统的例子，它是更大应用程序的一个功能。
- en: A content recommendation system in an application such as Disqus does not necessarily
    need to interact with the comment data, but might use the user's likes history
    to generate recommendations similar to the current page. Such a system would also
    need to analyze the text content of the liked pages and compare that to the text
    content of all pages in the network in order to make recommendations. Disqus does
    not need the post's content in order to provide comment functionality, but does
    need to store metadata about the page (like its URL and title) in its database.
    The post content may therefore not reside in the application's main database,
    though the likes and page metadata would likely be stored there.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在Disqus这样的应用中的内容推荐系统并不一定需要与评论数据交互，但可能会使用用户的喜欢历史来生成与当前页面类似的推荐。这样的系统还需要分析喜欢页面的文本内容，并将其与网络中所有页面的文本内容进行比较，以便做出推荐。Disqus不需要帖子的内容来提供评论功能，但需要在数据库中存储关于页面的元数据（如URL和标题）。因此，帖子内容可能不会存储在应用程序的主数据库中，尽管喜欢和页面元数据可能会存储在那里。
- en: A data pipeline built around Disqus's recommendation system needs first to query
    the main database for pages the user has liked—or pages that were liked by users
    who liked the current page—and return their metadata. In order to find similar
    content, however, the system will need to use the text content of each liked post.
    This data might be stored in a separate system, perhaps a secondary database such
    as MongoDB or Elasticsearch, or in Amazon S3 or some other data warehouse. The
    pipeline will need to retrieve the text content based on the metadata returned
    by the main database, and associate the content with the metadata.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在Disqus推荐系统周围的数据管道首先需要查询主数据库以获取用户喜欢的页面——或者喜欢当前页面的用户所喜欢的页面——并返回它们的元数据。然而，为了找到类似的内容，系统将需要使用每个喜欢帖子的文本内容。这些数据可能存储在单独的系统，比如MongoDB或Elasticsearch这样的二级数据库，或者Amazon
    S3或其他数据仓库中。该管道需要根据主数据库返回的元数据检索文本内容，并将内容与元数据关联起来。
- en: This is an example of multiple data selectors or data sources in the early stages
    of a data pipeline. One data source is the primary application data, which stores
    post and likes metadata. The other data source is a secondary server which stores
    the post's text content.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在数据管道早期阶段的一个多数据选择器或数据源的例子。一个数据源是主要应用程序数据，它存储帖子和喜欢元数据。另一个数据源是二级服务器，它存储帖子的文本内容。
- en: The next step in this pipeline might involve finding a number of candidate posts
    similar to the ones the user has liked, perhaps through a request to Elasticsearch
    or some other service that can find similar content. Similar content is not necessarily
    the correct content to serve, however, so these candidate articles will ultimately
    be ranked by an (hypothetical) ANN in order to determine the best content to display.
    In this example, the input to the data pipeline is the current page and the output
    from the data pipeline is a list of, say, 200 similar pages that the ANN will
    then rank.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道的下一步可能涉及找到与用户喜欢的帖子相似的一批候选帖子，这可能通过请求Elasticsearch或其他能够找到相似内容的服务来实现。然而，相似的内容并不一定是正确的内容来提供，因此这些候选文章最终将由一个（假设的）人工神经网络（ANN）进行排名，以确定要显示的最佳内容。在这个例子中，数据管道的输入是当前页面，输出是数据管道的一个列表，例如200个相似的页面，然后ANN将对这些页面进行排名。
- en: If all the necessary data resides in the primary database, the entire pipeline
    can be achieved with an SQL statement and some JOINs. Even in this case, care
    should be taken to develop a degree of abstraction between the ML algorithm and
    the data pipeline, as you may decide to update the application's architecture
    in the future. In other cases, however, the data will reside in separate locations
    and a more considered pipeline should be developed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有必要的数据都驻留在主数据库中，整个管道可以通过一个SQL语句和一些JOIN操作来实现。即使在这种情况下，也应该在机器学习算法和数据管道之间开发一定程度的抽象，因为您可能决定在未来更新应用程序的架构。然而，在其他情况下，数据将驻留在不同的位置，因此需要开发一个更周全的管道。
- en: There are many ways to build this data pipeline. You could develop a JavaScript
    module that performs all the pipeline tasks, and in some cases, you could even
    write a bash script using standard Unix tools to accomplish the task. On the other
    end of the complexity spectrum, there are purpose-built tools for data pipelining
    such as *Apache Kafka* and *AWS Pipeline*. These systems are designed modularly
    and allow you to define a specific data source, query, transformation, and aggregation
    modules as well as the workflows that connect them. In AWS Pipeline, for instance,
    you define *data nodes* that understand how to interact with the various data
    sources in your application.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个数据管道有许多方法。你可以开发一个执行所有管道任务的 JavaScript 模块，在某些情况下，你甚至可以使用标准的 Unix 工具编写 bash
    脚本来完成任务。在复杂性的另一端，有专门用于数据管道的工具，如 *Apache Kafka* 和 *AWS Pipeline*。这些系统设计为模块化，允许你定义特定的数据源、查询、转换和聚合模块，以及连接它们的流程。例如，在
    AWS Pipeline 中，你定义 *数据节点*，这些节点了解如何与你的应用程序中的各种数据源进行交互。
- en: The earliest stage of a pipeline is typically some sort of data query operation.
    Training examples must be extracted from a larger database, keeping in mind that
    not every record in a database is necessarily a training example. In the case
    of a spam filter, for instance, you should only select messages that have been
    marked as spam or not spam by a user. Messages that were automatically marked
    as spam by a spam filter should probably not be used for training, as that might
    cause a positive feedback loop that ultimately causes an unacceptable false positive
    rate.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的最早阶段通常是某种数据查询操作。必须从更大的数据库中提取训练示例，同时考虑到数据库中的每条记录并不一定是训练示例。例如，在垃圾邮件过滤器的情况下，你应该只选择被用户标记为垃圾邮件或非垃圾邮件的消息。那些被垃圾邮件过滤器自动标记为垃圾邮件的消息可能不应该用于训练，因为这可能会引起正反馈循环，最终导致不可接受的误报率。
- en: Similarly, you may want to prevent users that have been blocked or banned by
    your system from influencing your model training. A bad actor could intentionally
    mislead an ML model by taking inappropriate actions on their own data, so you
    should disqualify these data points as training examples.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，你可能想阻止被你的系统阻止或禁止的用户影响你的模型训练。一个恶意行为者可能会通过对自己数据进行不适当的行为来故意误导机器学习模型，因此你应该将这些数据点作为训练示例排除。
- en: Alternatively, if your application is such that recent data points should take
    precedence over older training points, your data query operation might set a time-based
    limit on the data to use for training, or select a fixed limit ordered reverse
    chronologically. No matter the situation, make sure you carefully consider your
    data queries as they are an essential first step in your data pipeline.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你的应用程序要求最近的数据点应该比旧的数据点优先考虑，你的数据查询操作可能需要对用于训练的数据设置基于时间限制，或者选择一个按时间顺序逆序排列的固定限制。无论情况如何，确保你仔细考虑你的数据查询，因为它们是你数据管道中的基本第一步。
- en: Not all data needs to come from database queries, however. Many applications
    use a *pub/sub* or event subscription architecture to capture streaming data.
    This data could be activity logs aggregated from a number of servers, or live
    transaction data from a number of sources. In these cases, an event subscriber
    will be an early part of your data pipeline. Note that event subscription and
    data querying are not mutually exclusive operations. Events that come in through
    a pub/sub system can still be filtered based on various criteria; this is still
    a form of data querying.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有数据都需要来自数据库查询。许多应用程序使用 *pub/sub* 或事件订阅架构来捕获流数据。这些数据可能是来自多个服务器的活动日志聚合，或者来自多个来源的实时交易数据。在这些情况下，事件订阅者将是你的数据管道的早期部分。请注意，事件订阅和数据查询不是互斥的操作。通过
    pub/sub 系统传入的事件仍然可以根据各种标准进行过滤；这仍然是一种数据查询的形式。
- en: One potential issue with an event subscription model arises when it's combined
    with a batch-training scheme. If you require 5,000 data points but receive only
    100 per second, your pipeline will need to maintain a buffer of data points until
    the target size is reached. There are various message-queuing systems that can
    assist with this, such as RabbitMQ or Redis. A pipeline requiring this type of
    functionality might hold messages in a queue until the target of 5,000 messages
    is achieved, and only then release the messages for batch processing through the
    rest of the pipeline.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当事件订阅模型与批量训练方案结合时，可能会出现一个潜在问题。如果你需要5,000个数据点，但每秒只收到100个，你的管道需要维护一个数据点的缓冲区，直到达到目标大小。有各种消息队列系统可以协助完成这项工作，例如RabbitMQ或Redis。需要这种功能的管道可能会在队列中保留消息，直到达到5,000条消息的目标，然后才将消息释放到管道的其余部分进行批量处理。
- en: In the case that data is collected from multiple sources, it most likely will
    need to be joined or aggregated in some manner. Let's now take a look at a situation
    where data needs to be joined to data from an external API.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是从多个来源收集的，它很可能需要以某种方式连接或聚合。现在让我们看看需要将数据与外部API数据连接的情况。
- en: Data joining and aggregation
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据连接和聚合
- en: Let's return to our example of the Disqus content recommendation system. Imagine
    that the data pipeline is able to query likes and post metadata directly from
    the primary database, but that no system in the applications stores the post's
    text content. Instead, a microservice was developed in the form of an API that
    accepts a post ID or URL and returns the page's sanitized text content.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的Disqus内容推荐系统示例。想象一下，数据管道能够直接从主数据库查询点赞和帖子元数据，但没有系统在应用程序中存储帖子的文本内容。相反，开发了一个以API形式存在的微服务，该API接受帖子ID或URL，并返回页面的净化文本内容。
- en: In this case, the data pipeline will need to interact with the microservice
    API in order to get the text content for each post. This approach is perfectly
    valid, though if the frequency of post content requests is high, some caching
    or storage should probably be implemented.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据管道需要与微服务API交互，以获取每个帖子的文本内容。这种方法是完全有效的，尽管如果帖子内容请求的频率很高，可能需要实施一些缓存或存储。
- en: The data pipeline will need to employ an approach similar to the buffering of
    messages in the event subscription model. The pipeline can use a message queue
    to queue posts that still require content, and make requests to the content microservice
    for each post in the queue until the queue is depleted. As each post's content
    is retrieved it is added to the post metadata and stored in a separate queue for
    completed requests. Only when the source queue is depleted and the sink queue
    is full should the pipeline move on to the next step.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道需要采用与事件订阅模型中消息缓冲类似的方法。管道可以使用消息队列来排队仍需要内容的帖子，并对队列中的每个帖子向内容微服务发出请求，直到队列耗尽。随着每个帖子内容的检索，它被添加到帖子元数据中，并存储在单独的队列中，用于完成请求。只有当源队列耗尽且目标队列满时，管道才应继续下一步。
- en: Data joining does not necessarily need to involve a microservice API. If the
    pipeline collects data from two separate sources that need to be combined, a similar
    approach can be employed. The pipeline is the only component that needs to understand
    the relationship between the two data sources and formats, leaving both the data
    sources and the ML algorithm to operate independently of those details.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据连接不一定需要涉及微服务API。如果管道从两个需要合并的独立来源收集数据，可以采用类似的方法。管道是唯一需要理解两个数据源和格式之间关系的组件，让数据源和机器学习算法独立于这些细节进行操作。
- en: The queue approach also works well when a data aggregation is required. An example
    of this situation is a pipeline in which the input is streaming input data and
    the output is token counts or value aggregations. Using a message queue is desirable
    in these situations as most message queues ensure that a message can be consumed
    only once, therefore preventing any duplication by the aggregator. This is especially
    valuable when the event stream is very high frequency, such that tokenizing each
    event as it comes in would lead to backups or server overload.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要数据聚合时，队列方法也工作得很好。这种情况的一个例子是，输入是流式输入数据，输出是标记计数或值聚合的管道。在这些情况下，使用消息队列是可取的，因为大多数消息队列确保消息只能被消费一次，从而防止聚合器产生任何重复。当事件流非常高频时，这一点尤其有价值，因为将每个事件作为它到来时进行标记可能会导致备份或服务器过载。
- en: Because message queues ensure that each message is consumed only once, high-frequency
    event data can stream directly into a queue where messages are consumed by multiple
    workers in parallel. Each worker might be responsible for tokenizing the event
    data and then pushing the token stream to a different message queue. The message
    queue software ensures that no two workers process the same event, and each worker
    can operate as an independent unit that is only concerned with tokenization.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于消息队列确保每条消息只被消费一次，高频事件数据可以直接流入一个队列，其中消息由多个并行工作的进程消费。每个工作进程可能负责对事件数据进行分词，然后将分词流推送到不同的消息队列。消息队列软件确保没有两个工作进程处理相同的消息，每个工作进程可以作为独立单元运行，只关注分词。
- en: As the tokenizers push their results onto a new message queue, another worker
    can consume those messages and aggregate token counts, delivering its own results
    to the next step in the pipeline every second or minute or 1,000 events, whatever
    is appropriate for the application. The output of this style of pipeline might
    be fed into a continually updating Bayesian model, for example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当分词器将结果推送到新的消息队列时，另一个工作进程可以消费这些消息并汇总分词计数，每秒、每分钟或每1,000个事件（以适用于应用程序的方式）将自身的结果传递到管道的下一步。这种风格管道的输出可能被输入到一个持续更新的贝叶斯模型中，例如。
- en: One benefit of a data pipeline designed in this manner is performance. If you
    were to attempt to subscribe to high-frequency event data, tokenize each message,
    aggregate token counts, and update a model all in one system, you might be forced
    to use a very powerful (and expensive) single server. The server would simultaneously
    need a high-performance CPU, lots of RAM, and a high-throughput network connection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式设计的数据管道的一个好处是性能。如果您试图订阅高频事件数据，对每条消息进行分词，汇总分词计数，并更新模型，您可能被迫使用一个非常强大（且昂贵）的单个服务器。服务器同时需要高性能CPU、大量RAM和高吞吐量网络连接。
- en: By breaking up the pipeline into stages, however, you can optimize each stage
    of the pipeline for its specific task and load condition. The message queue that
    receives the source event stream needs only to receive the event stream but does
    not need to process it. The tokenizer workers do not necessarily need to be high-performance
    servers, as they can be run in parallel. The aggregating queue and worker will
    process a large volume of data but will not need to retain data for longer than
    a few seconds and therefore may not need much RAM. The final model, which is a
    compressed version of the source data, can be stored on a more modest machine.
    Many components of the data pipeline can be built of commodity hardware simply
    because a data pipeline encourages modular design.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过将管道分解为阶段，您可以针对每个阶段的特定任务和负载条件进行优化。接收源事件流的消息队列只需要接收事件流，但不需要处理它。分词工作进程不一定是高性能服务器，因为它们可以并行运行。汇总队列和工作进程将处理大量数据，但不需要保留数据超过几秒钟，因此可能不需要太多RAM。最终的模型，即源数据的压缩版本，可以存储在更普通的机器上。由于数据管道鼓励模块化设计，因此数据管道的许多组件可以用通用硬件构建。
- en: In many cases, you will need to transform your data from format to format throughout
    the pipeline. That could mean converting from native data structures to JSON,
    transposing or interpolating values, or hashing values. Let's now discuss several
    types of data transformations that may occur in the data pipeline.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，您需要在管道中从一种格式转换数据到另一种格式。这可能意味着将原生数据结构转换为JSON，转置或插值值，或对值进行哈希处理。现在让我们讨论在数据管道中可能发生的几种数据转换类型。
- en: Transformation and normalization
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换和归一化
- en: As your data makes its way through a pipeline, it may need to be converted into
    a structure compatible with your algorithm's input layer. There are many possible
    transformations that can be performed on the data in the pipeline. For example,
    in order to protect sensitive user data before it reaches a token-based classifier,
    you might apply a cryptographic hashing function to the tokens so that they are
    no longer human readable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的数据通过管道传输时，可能需要将其转换为与算法输入层兼容的结构。在管道中的数据可以进行许多可能的转换。例如，为了在数据到达基于分词的分类器之前保护敏感用户数据，您可能需要对分词应用加密哈希函数，这样它们就不再是人类可读的。
- en: More typically, the types of transformations will be related to sanitization,
    normalization, or transposition. A sanitization operation might involve removing
    unnecessary whitespace or HTML tags, removing email addresses from a token stream,
    and removing unnecessary fields from the data structure. If your pipeline has
    subscribed to an event stream as the source of the data and the event stream attaches
    source server IP addresses to event data, it would be a good idea to remove these
    values from the data structure, both in order to save space and to minimize the
    surface area for potential data leaks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 更典型的情况是，转换类型将与清理、归一化或转置相关。清理操作可能涉及删除不必要的空白或HTML标签，从标记流中删除电子邮件地址，以及从数据结构中删除不必要的字段。如果你的管道已订阅事件流作为数据源，并且事件流将源服务器IP地址附加到事件数据中，那么从数据结构中移除这些值是一个好主意，这样既可以节省空间，也可以最大限度地减少潜在数据泄露的表面积。
- en: Similarly, if email addresses are not necessary for your classification algorithm,
    the pipeline should remove that data so that it interacts with the fewest possible
    servers and systems. If you've designed a spam filter, you may want to look into
    using only the domain portion of the email address instead of the fully qualified
    address. Alternately, the email addresses or domains may be hashed by the pipeline
    so that the classifier can still recognize them but a human cannot.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你的分类算法不需要电子邮件地址，那么管道应该移除这些数据，以便它与尽可能少的服务器和系统交互。如果你设计了一个垃圾邮件过滤器，你可能想考虑只使用电子邮件地址的域名部分而不是完全合格的地址。或者，电子邮件地址或域名可以通过管道进行哈希处理，这样分类器仍然可以识别它们，但人类却不能。
- en: Make sure to audit your data for other potential security and privacy issues
    as well. If your application collects the end user's IP address as part of its
    event stream, but the classifier does not need that data, remove it from the pipeline
    as early as possible. These considerations are becoming ever more important with
    the implementation of new European privacy laws, and every developer should be
    aware of privacy and compliance concerns.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 确保审查数据中的其他潜在安全和隐私问题。如果你的应用程序在事件流中收集最终用户的IP地址，但分类器不需要这些数据，那么应尽早将其从管道中移除。随着新欧洲隐私法律的实施，这些考虑因素变得越来越重要，每个开发者都应该意识到隐私和合规问题。
- en: A common category of data transformation is normalization. When working with
    a range of numerical values for a given field or feature, it's often desirable
    to normalize the range such that it has a known minimum and maximum bound. One
    approach is to normalize all values of the same field to the range [0,1], using
    the maximum encountered value as the divisor (for example, the sequence *1, 2,
    4* can be normalized to *0.25, 0.5, 1*). Whether data needs to be normalized in
    this manner will depend entirely on the algorithm that consumes the data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换的常见类别之一是归一化。当处理给定字段或特征的数值范围时，通常希望将范围归一化，使其具有已知的最小和最大边界。一种方法是将同一字段的全部值归一化到[0,1]的范围内，使用遇到的最高值作为除数（例如，序列*1,
    2, 4*可以归一化为*0.25, 0.5, 1*）。数据是否需要以这种方式归一化完全取决于消耗数据的算法。
- en: Another approach to normalization is to convert values into percentiles. In
    this scheme, very large outlying values will not skew the algorithm too drastically.
    If most values lie between 0 and 100 but a few points include values such as 50,000,
    an algorithm may give outsized precedence to the large values. If the data is
    normalized as a percentile, however, you are guaranteed to not have any values
    exceeding 100 and the outliers are brought into the same range as the rest of
    the data. Whether or not this is a good thing depends on the algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种归一化的方法是转换值成为百分位数。在这个方案中，非常大的异常值不会使算法产生太大的偏差。如果大多数值位于0到100之间，但少数点包括像50,000这样的值，算法可能会给予大值过大的优先级。然而，如果数据以百分位数归一化，那么你保证不会有任何超过100的值，异常值也会被纳入与数据其他部分相同的范围。这好不好取决于算法。
- en: The data pipeline is also a good place to calculate derived or second-order
    features. Imagine a random forest classifier that uses Instagram profile data
    to determine if the profile belongs to a human or a bot. The Instagram profile
    data will include fields such as the user's followers count, friends count, posts
    count, website, bio, and username. A random forest classifier will have difficulty
    using those fields in their original representations, however, by applying some
    simple data transformations, you can achieve accuracies of 90%.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道也是计算派生或二阶特征的好地方。想象一个随机森林分类器，它使用Instagram个人资料数据来确定个人资料属于人类还是机器人。Instagram个人资料数据将包括用户的关注者数量、朋友数量、帖子数量、网站、简介和用户名。然而，随机森林分类器在使用这些字段的原有表示时可能会遇到困难，但是通过应用一些简单的数据转换，你可以达到90%的准确率。
- en: In the Instagram case, one type of helpful data transformation is calculating
    ratios. Followers count and friends count, as separate features or signals, may
    not be useful to the classifier since they are treated somewhat independently.
    But the friends-to-followers *ratio* can turn out to be a very strong signal that
    may expose bot users. An Instagram user with 1,000 friends doesn't raise any flags,
    nor would an Instagram user with 50 followers; treated independently, these features
    are not strong signals. However, an Instagram user with a friends-to-followers
    ratio of 20 (or 1,000/50) is almost certainly a bot designed to follow other users.
    Similarly, a ratio such as posts-versus-followers or posts-versus-friends may
    end up being a stronger signal than any of those features independently.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在Instagram的情况下，一种有用的数据转换是计算比率。关注者数量和粉丝数量作为单独的特征或信号，可能对分类器没有太大帮助，因为它们被处理得相对独立。但是，朋友与关注者的比率可能成为一个非常强烈的信号，可能会暴露出机器人用户。一个有1,000个朋友的Instagram用户不会引起任何警报，同样，一个有50个粉丝的Instagram用户也不会；独立处理，这些特征不是强烈的信号。然而，一个朋友与关注者比率为20（或1,000/50）的Instagram用户几乎肯定是一个设计来关注其他用户的机器人。同样，像帖子与关注者比或帖子与朋友比这样的比率可能最终比任何单独的特征都强。
- en: Text content such as the Instagram user's profile bio, website, or username
    is made useful by deriving second-order features from them as well. A classifier
    may not be able to do anything with a website's URL, but perhaps a Boolean *has_profile_website*
    feature can be used as a signal instead. If, in your research, you notice that
    usernames of bots tend to have a lot of numbers in them, you can derive features
    from the username itself. One feature can calculate the ratio of letters to numbers
    in the username, another Boolean feature can represent whether the username has
    a number at the end or beginning, and a more advanced feature could determine
    if dictionary words were used in the username or not (therefore distinguishing
    between `@themachinelearningwriter` and something gibberish like `@panatoe234`).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 文本内容，如Instagram用户的个人资料简介、网站或用户名，通过从它们中提取二阶特征也能变得有用。分类器可能无法对网站的URL做任何事情，但也许可以用一个布尔值特征*has_profile_website*作为信号。如果在你的研究中，你注意到机器人的用户名中往往有很多数字，你可以从用户名本身提取特征。一个特征可以计算用户名中字母与数字的比例，另一个布尔值特征可以表示用户名是否以数字开头或结尾，一个更高级的特征可以确定用户名中是否使用了字典中的单词（因此区分`@themachinelearningwriter`和像`@panatoe234`这样的乱码）。
- en: Derived features can be of any level of sophistication or simplicity. Another
    simple feature could be whether the Instagram profile contains a URL in the profile
    bio field (as opposed to the dedicated website field); this can be detected with
    a regex and the Boolean value used as the feature. A more advanced feature could
    automatically detect whether the language used in the user's content is the same
    as the language specified by the user's locale setting. If the user claims they're
    in France but always writes captions in Russian it may indeed be a Russian living
    in France, but when combined with other signals like a friends-to-followers ratio
    far from 1, this information may be indicative of a bot user.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的特征可以是任何复杂度或简单度。另一个简单的特征可能是Instagram个人资料是否在个人资料简介字段中包含URL（与专门的网站字段相对）；这可以通过正则表达式检测，布尔值用作特征。一个更高级的特征可以自动检测用户内容中使用的语言是否与用户指定的地区设置相同。如果用户声称他们在法国，但总是用俄语写标题，这确实可能是一个住在法国的俄罗斯人，但结合其他信号，如关注者与粉丝的比例远非1，这些信息可能表明是一个机器人用户。
- en: There are lower level transformations that may need to be applied to the data
    in the pipeline as well. If the source data is in an XML format but the classifier
    requires JSON formatting, the pipeline should take responsibility for the parsing
    and conversion of formats.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些低级转换可能需要应用于管道中的数据。如果源数据是XML格式，但分类器需要JSON格式，则管道应负责解析和格式转换。
- en: Other mathematical transformations may also be applied. If the native format
    of the data is row-oriented but the classifier needs column-oriented data, the
    pipeline can perform a vector transposition operation as part of the processing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以应用其他数学转换。如果数据的原生格式是面向行的，但分类器需要面向列的数据，则管道可以在处理过程中执行向量转置操作。
- en: Similarly, the pipeline can use mathematical interpolation to fill in missing
    values. If your pipeline subscribes to events emitted by a suite of sensors in
    a laboratory setting and a single sensor goes offline for a couple of measurements,
    it may be reasonable to interpolate between the two known values in order to fill
    in the missing data. In other cases, missing values can be replaced with the population's
    mean or median value. Replacing missing values with a mean or median will often
    result in the classifier deprioritizing that feature for that data point, as opposed
    to breaking the classifier by giving it a null value.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，管道可以使用数学插值来填充缺失值。如果你的管道订阅了实验室环境中一套传感器发出的事件，并且单个传感器在几次测量中离线，那么在两个已知值之间进行插值以填充缺失数据可能是合理的。在其他情况下，缺失值可以用总体均值或中位数来替换。用均值或中位数替换缺失值通常会导致分类器优先考虑该数据点的特征，而不是通过提供一个空值来破坏分类器。
- en: 'In general, there are two things to consider in terms of transformation and
    normalization within a data pipeline. The first is the mechanical details of the
    source data and the target format: XML data must be transformed to JSON, rows
    must be converted to columns, images must be converted from JPEG to BMP formats,
    and so on. The mechanical details are not too tricky to work out, as you will
    already be aware of the source and target formats required by the system.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在数据管道中的转换和归一化方面，有两个方面需要考虑。第一个是源数据和目标格式的机械细节：XML数据必须转换为JSON，行必须转换为列，图像必须从JPEG格式转换为BMP格式，等等。这些机械细节并不太复杂，因为你已经知道系统所需的源和目标格式。
- en: The other consideration is the semantic or mathematical transformation of your
    data. This is an exercise in feature selection and feature engineering, and is
    not as straightforward as the mechanical transformation. Determining which second-order
    features to derive is both art and science. The art is coming up with new ideas
    for derived features, and the science is to rigorously test and experiment with
    your work. In my experience with Instagram bot detection, for instance, I found
    that the letters-to-numbers ratio in Instagram usernames was a very weak signal.
    I abandoned that idea after some experimentation in order to avoid adding unnecessary
    dimensionality to the problem.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是您数据的语义或数学转换。这是一个特征选择和特征工程练习，并不像机械转换那样直接。确定要推导出哪些二阶特征既是一门艺术也是一门科学。艺术在于提出新的衍生特征想法，而科学在于严格测试和实验你的工作。以我在Instagram机器人检测方面的经验为例，我发现Instagram用户名中的字母与数字比例是一个非常微弱的信号。经过一些实验后，我放弃了这个想法，以避免给问题添加不必要的维度。
- en: At this point, we have a hypothetical data pipeline that collects data, joins
    and aggregates it, processes it, and normalizes it. We're almost done, but the
    data still needs to be delivered to the algorithm itself. Once the algorithm is
    trained, we might also want to serialize the model and store it for later use.
    In the next section, we'll discuss a few considerations to make when transporting
    and storing training data or serialized models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个假设的数据管道，它收集数据，将其连接和聚合，处理它，并将其归一化。我们几乎完成了，但数据仍需要交付给算法本身。一旦算法被训练，我们可能还希望序列化模型并存储它以供以后使用。在下一节中，我们将讨论在传输和存储训练数据或序列化模型时需要考虑的一些因素。
- en: Storing and delivering data
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储和交付数据
- en: 'Once your data pipeline has applied all the necessary processing and transformations,
    it has one task left to do: deliver the data to your algorithm. Ideally, the algorithm
    will not need to know about the implementation details of the data pipeline. The
    algorithm should have a single location that it can interact with in order to
    get the fully processed data. This location could be a file on disk, a message
    queue, a service such as Amazon S3, a database, or an API endpoint. The approach
    you choose will depend on the resources available to you, the topology or architecture
    of your server system, and the format and size of the data.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的数据处理管道完成了所有必要的处理和转换，它剩下的任务就是将数据传递给你的算法。理想情况下，算法不需要了解数据管道的实现细节。算法应该有一个单一的位置可以与之交互，以获取完全处理过的数据。这个位置可能是一个磁盘上的文件，一个消息队列，一个如Amazon
    S3这样的服务，一个数据库，或者一个API端点。你选择的方法将取决于你可用的资源，你的服务器系统的拓扑或架构，以及数据的格式和大小。
- en: Models that are trained only periodically are typically the simplest case to
    handle. If you're developing an image recognition RNN that learns labels for a
    number of images and only needs to be retrained every few months, a good approach
    would be to store all the images as well as a manifest file (relating image names
    to labels) in a service such as Amazon S3 or a dedicated path on disk. The algorithm
    would first load and parse the manifest file and then load the images from the
    storage service as needed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 只定期训练的模型通常是处理起来最简单的情况。如果你正在开发一个图像识别RNN，它学习大量图像的标签，并且只需要每几个月重新训练一次，一个很好的方法是将所有图像以及一个清单文件（将图像名称与标签相关联）存储在Amazon
    S3或磁盘上的专用路径上。算法首先加载并解析清单文件，然后根据需要从存储服务加载图像。
- en: Similarly, an Instagram bot detection algorithm may only need to be retrained
    every week or every month. The algorithm can read training data directly from
    a database table or a JSON or CSV file stored on S3 or a local disk.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，一个Instagram机器人检测算法可能只需要每周或每月重新训练一次。算法可以直接从数据库表、存储在S3或本地磁盘上的JSON或CSV文件中读取训练数据。
- en: It is rare to have to do this, but in some exotic data pipeline implementations
    you could also provide the algorithm with a dedicated API endpoint built as a
    microservice; the algorithm would simply query the API endpoint first for a list
    of training point references, and then request each in turn from the API.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况很少发生，但在一些特殊的数据管道实现中，你也可以为算法提供一个作为微服务构建的专用API端点；算法会首先查询API端点以获取训练点引用的列表，然后依次从API请求每个引用。
- en: Models which require online updates or near-real-time updates, on the other
    hand, are best served by a message queue. If a Bayesian classifier requires live
    updates, the algorithm can subscribe to a message queue and apply updates as they
    come in. Even when using a sophisticated multistage pipeline, it is possible to
    process new data and update a model in fractions of a second if you've designed
    all the components well.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 需要在线更新或近似实时更新的模型，另一方面，最好通过消息队列来提供服务。如果一个贝叶斯分类器需要实时更新，算法可以订阅消息队列，并在更新到来时应用它们。即使使用复杂的分阶段管道，如果你设计好了所有组件，处理新数据和更新模型也可能在几秒钟内完成。
- en: 'Returning to the spam filter example, we can design a highly performant data
    pipeline like so: first, an API endpoint receives feedback from a user. In order
    to keep the user interface responsive, this API endpoint is responsible only for
    placing the user''s feedback into a message queue and can finish its task in under
    a millisecond. The data pipeline in turn subscribes to the message queue, and
    in another few milliseconds is made aware of a new message. The pipeline then
    applies a few simple transformations to the message, like tokenizing, stemming,
    and potentially even hashing the tokens.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回到垃圾邮件过滤器示例，我们可以设计一个高性能的数据管道，如下所示：首先，一个API端点接收用户的反馈。为了保持用户界面的响应性，这个API端点只负责将用户的反馈放入消息队列，并且可以在不到一毫秒内完成其任务。然后，数据处理管道订阅消息队列，在另几个毫秒内就会知道有新消息。管道随后对消息应用一些简单的转换，如分词、词干提取，甚至可能对标记进行散列。
- en: 'The next stage of the pipeline transforms the token stream into a hashmap of
    tokens and their counts (for example, from *hey hey there* to *{hey: 2, there:
    1}*); this avoids the need for the classifier to update the same token''s count
    more than once. This stage of processing will only require another couple of milliseconds
    at worst. Finally, the fully processed data is placed in a separate message queue
    which the classifier subscribes to. Once the classifier is made aware of the data
    it can immediately apply the updates to the model. If the classifier is backed
    by Redis, for instance, this final stage will also require only a few milliseconds.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '管道下一步将把标记流转换成标记及其计数的哈希表（例如，从 *hey hey there* 转换为 *{hey: 2, there: 1}*）；这样可以避免分类器需要多次更新同一个标记的计数。这一处理阶段在最坏的情况下也只需额外几毫秒。最后，完全处理后的数据被放置在一个单独的消息队列中，分类器会订阅这个队列。一旦分类器意识到数据，它就可以立即将更新应用到模型上。如果分类器由
    Redis 支持，例如，这一最终阶段也只需几毫秒。'
- en: The entire process we have described, from the time the user's feedback reaches
    the API server to the time the model is updated, may only require 20 ms. Considering
    that communication over the internet (or any other means) is limited by the speed
    of light, the best-case scenario for a TCP packet making a round-trip between
    New York and San Francisco is 40 ms; in practice, the average cross-country latency
    for a good internet connection is about 80 ms. Our data pipeline and model is
    therefore capable of updating itself based on user feedback a full 20 ms before
    the user will even receive their HTTP response.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所描述的整个过程，从用户反馈到达 API 服务器到模型更新的时间，可能只需要 20 毫秒。考虑到互联网（或任何其他方式）的通信速度受光速限制，纽约和旧金山之间
    TCP 数据包往返的最佳情况场景是 40 毫秒；在实际操作中，良好互联网连接的平均跨国家延迟约为 80 毫秒。因此，我们的数据管道和模型能够在用户甚至收到他们的
    HTTP 响应之前 20 毫秒就根据用户反馈进行自我更新。
- en: Not every application requires real-time processing. Managing separate servers
    for an API, a data pipeline, message queues, a Redis store, and hosting the classifier
    might be overkill both in terms of effort and budget. You'll have to determine
    what's best for your use case.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个应用程序都需要实时处理。为 API、数据管道、消息队列、Redis 存储和分类器托管分别管理服务器，在努力和预算方面可能都是过度的。您需要确定最适合您用例的方案。
- en: The last thing to consider is not related to the data pipeline but rather the
    storage and delivery of the model itself, in the case of a hybrid approach where
    a model is trained on the server but evaluated on the client. The first question
    to ask yourself is whether the model is considered public or private. Private
    models should not be stored on a public Amazon S3 bucket, for instance; instead,
    the S3 bucket should have access control rules in place and your application will
    need to procure a signed download link with an expiration time (the S3 API assists
    with this).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要考虑的不是数据管道相关的问题，而是模型本身的存储和交付，特别是在混合方法中，模型在服务器上训练但在客户端评估的情况下。首先要问自己的问题是模型是否被认为是公共的还是私有的。例如，私有模型不应存储在公共的
    Amazon S3 存储桶中；相反，S3 存储桶应设置访问控制规则，并且您的应用程序需要获取一个带有过期时间的签名下载链接（S3 API 可以帮助完成这项工作）。
- en: The next consideration is how large the model is and how often it will be downloaded
    by clients. If a public model is downloaded frequently but updated infrequently,
    it might be best to use a CDN in order to take advantage of edge caching. If your
    model is stored on Amazon S3, for example, then the Amazon CloudFront CDN would
    be a good choice.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个考虑因素是模型的大小以及客户端下载模型的频率。如果公共模型经常被下载但更新不频繁，使用 CDN 以利用边缘缓存可能是最好的选择。例如，如果您的模型存储在
    Amazon S3 上，那么 Amazon CloudFront CDN 将是一个不错的选择。
- en: Of course, you can always build your own storage and delivery solution. In this
    chapter, I have assumed a cloud architecture, however if you have a single dedicated
    or collocated server then you may simply want to store the serialized model on
    disk and serve it either through your web server software or through your application's
    API. When dealing with large models, make sure to consider what will happen if
    many users attempt to download the model simultaneously. You may inadvertently
    saturate your server's network connection if too many people request the file
    at once, you might overrun any bandwidth limits set by your server's ISP, or you
    might end up with your server's CPU stuck in I/O wait while it moves data around.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你总是可以构建自己的存储和交付解决方案。在本章中，我假设了一个云架构，然而如果你只有一个专用的或共址服务器，你可能只想将序列化的模型存储在磁盘上，并通过你的网络服务器软件或应用程序的API提供服务。在处理大型模型时，确保考虑如果许多用户同时尝试下载模型会发生什么。如果太多人同时请求文件，你可能会无意中饱和服务器的网络连接，你可能会超出服务器ISP设置的任何带宽限制，或者你可能会发现服务器的CPU在移动数据时陷入I/O等待状态。
- en: As mentioned previously, there's no one-size-fits-all solution for data pipelining.
    If you're a hobbyist developing applications for fun or just a few users, you
    have lots of options for data storage and delivery. If you're working in a professional
    capacity on a large enterprise project, however, you will have to consider all
    aspects of the data pipeline and how they will impact your application's performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，没有一种适合所有情况的数据管道解决方案。如果你是一个为了乐趣或仅仅为几个用户开发应用程序的爱好者，你有很多数据存储和交付的选择。然而，如果你在一个大型企业项目中以专业身份工作，你将不得不考虑数据管道的所有方面以及它们如何影响应用程序的性能。
- en: I will offer one final piece of advice to the hobbyists reading this section.
    While it's true that you don't need a sophisticated, real-time data pipeline for
    hobby projects, you should build one anyway. Being able to design and build real-time
    data pipelines is a highly marketable and valuable skill that not many people
    possess, and if you're willing to put in the practice to learn ML algorithms then
    you should also practice building performant data pipelines. I'm not saying that
    you should build a big, fancy data pipeline for every single hobby project—just
    that you should do it a few times, using several different approaches, until you're
    comfortable not just with the concepts but also the implementation. Practice makes
    perfect, and practice means getting your hands dirty.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我将给阅读这一部分的爱好者提供一条最后的建议。虽然对于爱好项目来说，你确实不需要一个复杂的、实时的数据处理管道，但你仍然应该构建一个。能够设计和构建实时的数据处理管道是一项非常具有市场价值和稀缺的技能，而且很多人都不具备这项技能。如果你愿意投入实践去学习机器学习算法，那么你也应该练习构建性能良好的数据处理管道。我并不是说你应该为每一个爱好项目都构建一个庞大而复杂的数据处理管道——只是说你应该尝试几次，使用几种不同的方法，直到你不仅对概念感到舒适，也对实现感到舒适。熟能生巧，而实践意味着亲自动手。
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed a number of practical matters related to ML applications
    in production. Learning ML algorithms is, of course, central to building an ML
    application, but there's much more to building an application than simply implementing
    an algorithm. Applications ultimately need to interact with users across a variety
    of devices, so it is not enough to consider only what your application does —you
    must also plan for how and where it will be used.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了与生产中机器学习应用相关的许多实际问题。学习机器学习算法当然是构建机器学习应用的核心，但构建应用远不止简单地实现算法。应用最终需要与各种设备上的用户进行交互，因此，仅仅考虑你的应用能做什么是不够的——你还必须计划它将如何以及在哪里被使用。
- en: We began the chapter with a discussion about serializable and portable models,
    and you learned about the different architectural approaches to the training and
    evaluation of models. We discussed the fully server-side approach (common with
    SaaS products), the fully client-side approach (useful for sensitive data), and
    a hybrid approach by which a model is trained on the server but evaluated on the
    client. You also learned about web workers, which are a useful browser-specific
    feature that you can use to ensure a performant and responsive UI when evaluating
    models on the client.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章开始时讨论了可序列化和可移植的模型，并学习了模型训练和评估的不同架构方法。我们讨论了完全服务器端的方法（常见于SaaS产品），完全客户端的方法（对于敏感数据很有用），以及一种混合方法，即模型在服务器上训练但在客户端评估。你还学习了关于Web
    Workers的内容，这是一个有用的浏览器特定功能，你可以使用它来确保在客户端评估模型时有一个性能良好且响应迅速的用户界面。
- en: We also discussed models which continually update or get periodically retrained,
    and various approaches to communicating feedback between the client and the server.
    You also learned about per-user models, or algorithms which can be trained by
    a central source of truth but refined by an individual user's specific behaviors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了持续更新或定期重新训练的模型，以及客户端和服务器之间传递反馈的各种方法。你还学习了关于按用户模型，或者可以由一个中心真实来源训练但可以通过个别用户的特定行为进行优化的算法。
- en: Finally, you learned about data pipelines and various mechanisms that manage
    the collection, combining, transformation, and delivery of data from one system
    to the next. A central theme in our data pipeline discussion was the concept of
    using the data pipeline as a layer of abstraction between ML algorithms and the
    rest of your production systems.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了关于数据管道以及各种管理数据从一系统到下一系统收集、组合、转换和交付的机制。在我们对数据管道的讨论中，一个中心主题是使用数据管道作为机器学习算法和其余生产系统之间的一层抽象。
- en: 'The final topic I''d like to discuss is one that many ML students are curious
    about: how exactly do you choose the right ML algorithm for a given problem? Experts
    in ML typically develop an intuition that guides their decisions, but that intuition
    can take years to form. In the next chapter, we''ll discuss practical techniques
    you can use to narrow in on the appropriate ML algorithm to use for any given
    problem.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要讨论的最后一个话题是许多机器学习学生都好奇的：你究竟是如何为特定问题选择正确的机器学习算法的？机器学习专家通常发展出一种指导他们决策的直觉，但这种直觉可能需要数年才能形成。在下一章中，我们将讨论你可以使用的实用技术，以缩小针对任何给定问题的适当机器学习算法的选择范围。
