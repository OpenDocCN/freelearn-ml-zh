- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Building an Extract, Transform, Machine Learning Use Case
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建提取、转换、机器学习用例
- en: Similar to *Chapter 8*, *Building an Example ML Microservice*, the aim of this
    chapter will be to try to crystallize a lot of the tools and techniques we have
    learned about throughout this book and apply them to a realistic scenario. This
    will be based on another use case introduced in *Chapter 1*, *Introduction to
    ML Engineering*, where we imagined the need to cluster taxi ride data on a scheduled
    basis. So that we can explore some of the other concepts introduced throughout
    the book, we will assume as well that for each taxi ride, there is also a series
    of textual data from a range of sources, such as traffic news sites and transcripts
    of calls between the taxi driver and the base, joined to the core ride information.
    We will then pass this data to a **Large Language Model** (**LLM**) for summarization.
    The result of this summarization can then be saved in the target data location
    alongside the basic ride date to provide important context for any downstream
    investigations or analysis of the taxi rides. We will also build on our previous
    knowledge of Apache Airflow, which we will use as the orchestration tool for our
    pipelines, by discussing some more advanced concepts to make your Airflow jobs
    more robust, maintainable, and scalable. We will explore this scenario so that
    we can outline the key decisions we would make if building a solution in the real
    world, as well as discuss how to implement it by leveraging what has been covered
    in other chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与*第八章*，*构建一个示例ML微服务*类似，本章的目标将是尝试将我们在本书中学习到的许多工具和技术结晶化，并将它们应用于一个实际场景。这将是基于*第一章*，*机器学习工程导论*中引入的另一个用例，在那里我们想象了定期对出租车行程数据进行聚类的需求。为了探索本书中介绍的其他一些概念，我们还将假设对于每次出租车行程，还有一系列来自各种来源的文本数据，例如交通新闻网站和出租车司机与基地之间的通话记录，这些数据与核心行程信息合并。然后我们将这些数据传递给**大型语言模型**（**LLM**）进行总结。总结的结果可以随后保存在目标数据位置，与基本的行程日期一起提供对任何下游调查或分析出租车行程的重要背景信息。我们还将基于我们之前对Apache
    Airflow的知识，将其用作我们管道的编排工具，通过讨论一些更高级的概念来使你的Airflow作业更加健壮、可维护和可扩展。我们将探索这个场景，以便概述在现实世界中构建解决方案时我们会做出的关键决策，以及讨论如何通过利用其他章节中介绍的内容来实现它。
- en: This use case will allow us to explore what is perhaps the most used pattern
    in **machine learning** (**ML**) solutions across the world—that of the batch
    inference process. Due to the nature of retrieving, transforming, and then performing
    ML on data, I have termed this **Extract, Transform, Machine Learning** (**ETML**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本用例将使我们能够探索可能是全球范围内在**机器学习**（**ML**）解决方案中最常用的模式——批量推理过程。由于检索、转换并在数据上执行机器学习的特性，我将这种模式称为**提取、转换、机器学习**（**ETML**）。
- en: 'We will work through this example in the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下几节中逐步分析这个示例：
- en: Understanding the batch processing problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解批量处理问题
- en: Designing an ETML solution
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计ETML解决方案
- en: Selecting the tools
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择工具
- en: Executing the build
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行构建
- en: All of these topics will help us understand the particular decisions and steps
    we need to take in order to build a successful ETML solution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些主题都将帮助我们理解在构建成功的ETML解决方案时，我们需要做出的特定决策和步骤。
- en: In the next section, we will revisit the high-level problem introduced in *Chapter
    1*, *Introduction to ML Engineering*, and explore how to map the business requirements
    to technical solution requirements, given everything we have learned in the book
    so far.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾在*第一章*，*机器学习工程导论*中引入的总体问题，并探讨如何根据我们在本书中迄今为止所学到的所有内容，将业务需求映射到技术解决方案需求。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'As in the other chapters, to create the environment to run the code examples
    in this chapter you can run:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如同其他章节一样，为了运行本章中的代码示例，你可以执行以下操作：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will include installs of Airflow, PySpark, and some supporting packages.
    For the Airflow examples, we can just work locally, and assume that if you want
    to deploy to the cloud, you can follow the details given in *Chapter 5*, *Deployment
    Patterns and Tools*. If you have run the above `conda` command then you will have
    installed Airflow locally, along with PySpark and the Airflow PySpark connector
    package, so you can run Airflow as standalone with the following command in the
    terminal:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这将包括Airflow、PySpark和一些支持包的安装。对于Airflow示例，我们可以在本地工作，并假设如果你想要部署到云端，可以遵循*第五章*，*部署模式和工具*中给出的细节。如果你已经运行了上述`conda`命令，那么你已经在本地安装了Airflow、PySpark和Airflow
    PySpark连接器包，因此你可以在终端中使用以下命令以独立方式运行Airflow：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will then instantiate a local database and all relevant Airflow components.
    There will be a lot of output to the terminal, but near the end of the first phase
    of output, you should be able to spot details about the local server that is running,
    including a generated user ID and password. See *Figure 9.1* for an example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这将实例化一个本地数据库和所有相关的Airflow组件。终端会有很多输出，但在第一次输出的最后阶段，你应该能够找到有关正在运行的本地服务器的详细信息，包括生成的用户ID和密码。请参见*图9.1*的示例。
- en: '![A screenshot of a computer code  Description automatically generated with
    low confidence](img/B19525_09_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![计算机代码屏幕截图  描述自动生成，置信度低](img/B19525_09_01.png)'
- en: 'Figure 9.1: Example of local login details created upon running Airflow in
    standalone mode. As the message says, do not use this mode for production!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：在独立模式下运行Airflow时创建的本地登录详情示例。正如消息所说，不要在生产环境中使用此模式！
- en: If you navigate to the URL provided (in the second line of the screenshot you
    can see that the app is `Listening at http://0.0.0.0:8080`), you will see a page
    like that shown in *Figure 9.2*, where you can use the local username and password
    to log in (see *Figure 9.3*). When you log in to the standalone version of Airflow,
    you are presented with many examples of DAGs and jobs that you can base your own
    workloads on.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你导航到提供的URL（在截图的第二行中你可以看到应用正在`Listening at http://0.0.0.0:8080`），你会看到一个像*图9.2*中显示的页面，在那里你可以使用本地用户名和密码登录（见*图9.3*）。当你登录到Airflow的独立版本时，你会看到许多DAGs和作业的示例，你可以基于这些示例构建自己的工作负载。
- en: '![](img/B19525_09_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_09_02.png)'
- en: 'Figure 9.2: The login page of the standalone Airflow instance running on a
    local machine.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：在本地机器上运行的独立Airflow实例的登录页面。
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19525_09_03.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度中等](img/B19525_09_03.png)'
- en: 'Figure 9.3: The landing page after logging in to the standalone Airflow instance.
    The page has been populated with a series of example DAGs.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：登录到独立Airflow实例后的着陆页面。页面已填充了一系列示例DAGs。
- en: Now that we have done some of the preliminary setup, let’s move on to discussing
    the details of the problem we will try to solve before we build out our solution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了一些初步设置，让我们在构建解决方案之前，讨论我们将尝试解决的问题的细节。
- en: Understanding the batch processing problem
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解批量处理问题
- en: 'In *Chapter 1*, *Introduction to ML Engineering*, we saw the scenario of a
    taxi firm that wanted to analyze anomalous rides at the end of every day. The
    customer had the following requirements:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*机器学习工程导论*中，我们看到了一家出租车公司每天结束时分析异常行程的场景。客户有以下要求：
- en: Rides should be clustered based on ride distance and time, and anomalies/outliers
    identified.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应根据行程距离和时间对行程进行聚类，并识别异常值/离群值。
- en: Speed (distance/time) was not to be used, as analysts would like to understand
    long-distance rides or those with a long duration.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度（距离/时间）不应使用，因为分析师希望了解长途行程或持续时间较长的行程。
- en: The analysis should be carried out on a daily schedule.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析应按日计划进行。
- en: The data for inference should be consumed from the company’s data lake.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理所需的数据应从公司的数据湖中获取。
- en: The results should be made available for consumption by other company systems.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果应可供其他公司系统消费。
- en: 'Based on the description in the introduction to this chapter, we can now add
    some extra requirements:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本章引言中的描述，我们现在可以添加一些额外的要求：
- en: The system’s results should contain information on the rides classification
    as well as a summary of relevant textual data.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统的结果应包含有关行程分类以及相关文本数据的摘要。
- en: Only anomalous rides need to have textual data summarized.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有异常行程需要总结文本数据。
- en: 'As we did in *Chapter 2*, *The Machine Learning Development Process*, and *Chapter
    8*, *Building an Example ML Microservice*, we can now build out some user stories
    from these requirements, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第2章“机器学习开发过程”和第8章“构建示例ML微服务”中所做的那样，我们现在可以根据这些需求构建一些用户故事，如下所示：
- en: '**User story 1**: As an operations analyst or data scientist, I want to be
    given clear labels of rides that are anomalous when considering their ride times
    in minutes and ride distances in miles so that I can perform further analysis
    and modeling on the volume of anomalous rides. The criteria for what counts as
    an anomaly should be determined by an appropriate ML algorithm, which defines
    an anomaly with respect to the other rides for the same day.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事1**：作为一名运营分析师或数据科学家，我希望在考虑每趟行程的分钟数和英里数时，能够得到异常行程的清晰标签，以便我可以对异常行程的量进行进一步的分析和建模。判断异常的标准应由适当的机器学习算法确定，该算法根据同一天的其它行程定义异常。'
- en: '**User story 2**: As an operations analyst or data scientist, I want to be
    provided with a summary of relevant textual data so that I can do further analysis
    and modeling on the reasons for some rides being anomalous.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事2**：作为一名运营分析师或数据科学家，我希望得到相关文本数据的摘要，以便我可以对一些行程异常的原因进行进一步的分析和建模。'
- en: '**User story 3**: As an internal application developer, I want all output data
    sent to a central location, preferably in the cloud, so that I can easily build
    dashboards and other applications with this data.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事3**：作为一名内部应用程序开发者，我希望所有输出数据都发送到一个中央位置，最好是云上，这样我就可以轻松地使用这些数据构建仪表板和其他应用程序。'
- en: '**User story 4**: As an operations analyst or data scientist, I would like
    to receive a report every morning by 09.00\. This report should clearly show which
    rides were anomalous or “normal” as defined by the selected ML algorithm. This
    will enable me to update my analyses and provide an update to the logistics managers.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户故事4**：作为一名运营分析师或数据科学家，我希望能每天早上9:00之前收到一份报告。这份报告应清楚地显示哪些行程是异常的或根据所选机器学习算法定义的“正常”的。这将使我能够更新我的分析并向物流经理提供更新。'
- en: User story 1 should be taken care of by our general clustering approach, especially
    since we are using the **Density-Based Spatial Clustering of Applications with
    Noise** (**DBSCAN**) algorithm, which provides a label of *-1* for outliers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 用户故事1应由我们的通用聚类方法处理，尤其是因为我们正在使用**基于密度的空间聚类应用噪声**（**DBSCAN**）算法，该算法为异常值提供标签*-1*。
- en: User story 2 can be accommodated by leveraging the capabilities of LLMs that
    we discussed in *Chapter 7*, *Deep Learning, Generative AI, and LLMOps*. We can
    send the textual data we are given as part of the input batch to a GPT model with
    an appropriately formatted prompt; the prompt formatting can be done with LangChain
    or vanilla Python logic.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用户故事2可以通过利用我们在第7章“深度学习、生成式AI和LLMOps”中讨论的LLM功能来满足。我们可以将作为输入批次一部分提供的文本数据发送到具有适当格式提示的GPT模型；提示格式化可以使用LangChain或纯Python逻辑完成。
- en: User story 3 means that we have to push the results to a location on the cloud
    that can then be picked up either by a data engineering pipeline or a web application
    pipeline. To make this as flexible as possible, we will push results to an assigned
    **Amazon Web Services** (**AWS**) **Simple Storage Service** (**S3**) bucket.
    We will initially export the data in the **JavaScript Object Notation** (**JSON**)
    format, which we have already met in several chapters, as this is a format that
    is often used in application development and can be read in by most data engineering
    tools.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 用户故事3意味着我们必须将结果推送到云上的某个位置，然后可以被数据工程管道或Web应用程序管道获取。为了尽可能灵活，我们将结果推送到一个指定的**亚马逊网络服务**（**AWS**）**简单存储服务**（**S3**）桶。我们最初将以**JavaScript对象表示法**（**JSON**）格式导出数据，这在几个章节中已经提到，因为这是一种常用于应用开发且大多数数据工程工具都能读取的格式。
- en: The final user story, user story 4, gives us guidance on the scheduling we require
    for the system. In this case, the requirements mean we should run a daily batch
    job.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的用户故事4为我们提供了对系统所需调度指导。在这种情况下，需求意味着我们应该运行一个每日批量作业。
- en: Let’s tabulate these thoughts in terms of some ML solution technical requirements,
    as shown in *Table 9.1*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些想法按照一些机器学习解决方案的技术要求进行表格化，如表9.1所示。
- en: '| **User Story** | **Details** | **Technical Requirements** |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **用户故事** | **详细信息** | **技术要求** |'
- en: '| 1 | As an operations analyst or data scientist, I want to be given clear
    labels of rides that have anomalously long ride times or distances so that I can
    perform further analysis and modeling on the volume of anomalous rides. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 作为一名运营分析师或数据科学家，我希望得到具有异常长骑行时间或距离的骑行清晰的标签，以便我可以对异常骑行量进行进一步的分析和建模。 |'
- en: Algorithm type = anomaly detection/clustering/outlier detection.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法类型 = 异常检测/聚类/离群值检测。
- en: Features = ride time and distance.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征 = 骑行时间和距离。
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 2 | As an operations analyst or data scientist, I want to be provided with
    a summary of relevant textual data so that I can do further analysis and modeling
    on the reasons for some rides being anomalous. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 作为一名运营分析师或数据科学家，我希望得到相关文本数据的摘要，以便我可以对一些骑行异常的原因进行进一步的分析和建模。 |'
- en: Algorithm type = text summarization.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法类型 = 文本摘要。
- en: Potential models = transformers like BERT, and LLMs like GPT models.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的模型 = 类似BERT的转换器，以及类似GPT模型的LLMs。
- en: Input requirements = formatted prompts.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入需求 = 格式化提示。
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| 3 | As an internal application developer, I want all output data sent to
    a central location, preferably in the cloud, so that I can easily build dashboards
    and other applications with this data. | System output destination = S3 on AWS.
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 作为内部应用程序开发者，我希望所有输出数据都发送到一个中央位置，最好是云中，这样我就可以轻松地使用这些数据构建仪表板和其他应用程序。 |
    系统输出目的地 = AWS上的S3。 |'
- en: '| 4 | As an operations analyst or data scientist, I would like to see the output
    data for the previous day’s rides every morning so that I can update my analyses
    and provide an update to the logistics managers. | Batch frequency = daily. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 作为一名运营分析师或数据科学家，我希望在每天早上看到前一天骑行的输出数据，以便我可以更新我的分析并向物流经理提供更新。 | 批次频率 =
    每天。 |'
- en: 'Table 9.1: Translating user stories to technical requirements.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1：将用户故事转换为技术需求。
- en: 'The process of taking some user stories and translating them into potential
    technical requirements is a very important skill for an ML engineer, and it can
    really help speed up the design and implementation of a potential solution. For
    the rest of the chapter, we will use the information in *Table 9.1*, but to help
    you practice this skill, can you think of some other potential user stories for
    the scenario given and what technology requirements these might translate to?
    Here are some thoughts to get you started:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将一些用户故事转换为潜在的技术需求的过程是机器学习工程师的一项非常重要的技能，它确实可以帮助加快潜在解决方案的设计和实施。在本章的其余部分，我们将使用*表9.1*中的信息，但为了帮助您练习这项技能，您能想到一些其他可能的用户故事以及这些故事可能转换成的技术需求吗？以下是一些启发性的思考：
- en: A data scientist in the firm may want to try and build models to predict customer
    satisfaction based on a variety of features of the rides, including times and
    perhaps any of the traffic issues mentioned in the textual data. How often may
    they want this data? What data would they need? What would they do with it specifically?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司的数据科学家可能想要尝试构建模型，根据骑行的时间和其他特征（包括文本数据中提到的任何交通问题）来预测客户满意度。他们可能多久需要一次这样的数据？他们需要哪些数据？他们具体会如何使用这些数据？
- en: The developers of the mobile app in the firm may want to have forecasts of expected
    ride times based on traffic and weather conditions to render for users. How could
    they do this? Can the data come in batches, or should it be an event-driven solution?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司移动应用的开发者可能希望根据交通和天气条件为用户提供预期的骑行时间预测。他们如何做到这一点？数据可以批量提供，还是应该是一个事件驱动的解决方案？
- en: Senior management may want reports compiled of the firm’s performance across
    several variables in order to make decisions. What sort of data may they want
    to see? What ML models would provide more insight? How often does the data need
    to be prepared, and what solutions could the results be shown in?
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级管理层可能希望看到公司跨多个变量的表现报告，以便做出决策。他们可能希望看到哪些类型的数据？哪些机器学习模型能提供更多见解？数据需要多久准备一次，结果可以展示在哪些解决方案中？
- en: Now that we have done some of the initial work required to understand what we
    need the system to do and how it may do it, we can now move on to bringing these
    together into some initial designs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了一些了解系统需要做什么以及它可能如何做的初步工作，我们现在可以开始将这些内容整合到一些初步的设计中。
- en: Designing an ETML solution
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计ETML解决方案
- en: 'The requirements clearly point us to a solution that takes in some data and
    augments it with ML inference, before outputting the data to a target location.
    Any design we come up with must encapsulate these steps. This is the description
    of any ETML solution, and this is one of the most used patterns in the ML world.
    In my opinion it will remain important for a long time to come as it is particularly
    suited to ML applications where:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 需求明确指出我们需要一个解决方案，该解决方案接收一些数据，并使用机器学习推理对其进行增强，然后将数据输出到目标位置。我们提出的任何设计都必须封装这些步骤。这是任何ETML解决方案的描述，这也是机器学习世界中应用最广泛的模式之一。在我看来，它将在未来一段时间内保持重要，因为它特别适合机器学习应用，其中：
- en: '**Latency is not critical**: If you can afford to run on a schedule and there
    are no high-throughput or low-latency response time requirements, then running
    as an ETML batch is perfectly acceptable.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟不是关键**：如果您可以按计划运行，并且没有高吞吐量或低延迟响应时间的要求，那么作为ETML批量运行是完全可接受的。'
- en: '**You need to batch the data for algorithmic reasons**: A great example of
    this is the clustering approach we will use here. There are ways to perform clustering
    in an online setting, where the model is continually updated as new data comes
    in, but some approaches are simpler if you have all the relevant data taken together
    in the relevant batch. Similar arguments can apply to deep learning models, which
    will require large batches of data to be processed on GPUs in parallel for maximum
    efficiency.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您需要批量处理数据以进行算法原因**：这里我们将使用的聚类方法是一个很好的例子。在在线环境中执行聚类的确有方法，其中模型会随着新数据的到来而不断更新，但如果你将所有相关数据一起放在相关批次中，某些方法会更简单。类似的论点也适用于深度学习模型，这些模型需要在大批量的数据上并行处理GPU以实现最大效率。'
- en: '**You do not have event-based or streaming mechanisms available**: Many organizations
    may still operate in batch mode simply because they have to! It can require investment
    to move to appropriate platforms that work in a different mode, and this may not
    have always been made available.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**您没有事件或流机制可用**：许多组织可能仍然在批量模式下运行，仅仅是因为他们必须这样做！转移到适当平台以采用不同模式可能需要投资，而这可能并不总是可行的。'
- en: '**It is simpler**: Related to the previous point, getting event-based or streaming
    systems set up can take some learning for your team, whereas batching is relatively
    intuitive and easy to get started with.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**这更简单**：与前面的观点相关，为您的团队设置基于事件或流系统可能需要一些学习，而批量处理相对直观且易于开始。'
- en: Now, let’s start discussing the design. The key elements our design has to cover
    were articulated in *Table 9.1*. We can then start to build out a design diagram
    that covers the most important aspects, including starting to pin down which technologies
    are used for which processes. *Figure 9.4* shows a simplified design diagram that
    starts to do this and shows how we can use an Airflow pipeline to pull data from
    an S3 bucket, storing our clustered data in S3 as an intermediate data storage
    step, before proceeding to summarize the text data using the LLM and exporting
    the final result to our final target S3 location.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始讨论设计。我们的设计必须涵盖的关键要素已在*表9.1*中阐述。然后我们可以开始构建一个设计图，涵盖最重要的方面，包括开始确定用于哪些流程的技术。*图9.4*显示了一个简化的设计图，开始这样做，并展示了我们如何使用Airflow管道从S3存储桶中提取数据，将我们的聚类数据存储在S3作为中间数据存储步骤，然后再使用LLM摘要文本数据，并将最终结果导出到我们的最终目标S3位置。
- en: '![](img/B19525_09_04.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_09_04.png)'
- en: 'Figure 9.4: High-level design for the ETML clustering and summarization system.
    Steps 1–3 of the overall pipelines are the clustering steps and 4–6 are the summarization
    steps.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：ETML聚类和摘要系统的整体设计。整体流程的步骤1-3是聚类步骤，4-6是摘要步骤。
- en: In the next section, we will look at some potential tools we can use to solve
    this problem, given everything we have learned in previous chapters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨一些潜在的工具体现在我们学习了前几章的内容后可以用来解决这个问题。
- en: Selecting the tools
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择工具
- en: For this example, and pretty much whenever we have an ETML problem, our main
    considerations boil down to a few simple things, namely the selection of the interfaces
    we need to build, the tools we need to perform the transformation and modeling
    at the scale we require, and how we orchestrate all of the pieces together. The
    next few sections will cover each of these in turn.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，以及几乎每次我们遇到ETML问题时，我们的主要考虑因素归结为几个简单的事情，即选择我们需要构建的接口、我们需要用于在所需规模上执行转换和建模的工具，以及我们如何编排所有这些部件。接下来的几节将依次介绍这些内容。
- en: Interfaces and storage
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接口和存储
- en: When we execute the extract and load parts of ETML, we need to consider how
    to interface with the systems that store our data. It is important that whichever
    database or data technology we extract from, we use the appropriate tools to extract
    at whatever scale and pace we need. In this example, we can use S3 on AWS for
    our storage; our interfacing can be taken care of by the AWS `boto3` library and
    the AWS CLI. Note that we could have selected a few other approaches, some of
    which are listed in *Table 9.2* along with their pros and cons.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行 ETML 的提取和加载部分时，我们需要考虑如何与存储我们数据的系统进行接口。重要的是，无论我们从哪个数据库或数据技术中提取，我们都应使用适当的工具以所需的规模和速度进行提取。在这个例子中，我们可以使用
    AWS 上的 S3 进行我们的存储；我们的接口可以由 AWS `boto3` 库和 AWS CLI 来处理。请注意，我们还可以选择其他一些方法，其中一些在
    *表 9.2* 中列出，并附有它们的优缺点。
- en: '| **Potential Tools** | **Pros** | **Cons** |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **潜在工具** | **优点** | **缺点** |'
- en: '| The AWS CLI, S3, and `boto3` |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| AWS CLI、S3 和 `boto3` |'
- en: Relatively simple to use, and extensive documentation.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对简单易用，并有广泛的文档。
- en: Connects to a wide variety of other AWS tools and services.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到广泛的 AWS 工具和服务。
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Not cloud-agnostic.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非云无关。
- en: Not applicable to other environments or technologies.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于其他环境或技术。
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SQL database and JDBC/ODBC connector |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SQL 数据库和 JDBC/ODBC 连接器 |'
- en: Relatively tool-agnostic.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对工具无关。
- en: Cross-platform and cloud.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨平台和云。
- en: Optimized storage and querying possible.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化存储和查询是可能的。
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Requires data modeling and database administration.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要数据建模和数据库管理。
- en: Not optimized for unstructured data.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适用于非结构化数据。
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Vendor-supplied cloud data warehouse via their API |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 通过他们的 API 提供的供应商云数据仓库 |'
- en: Often good documentation and examples to follow.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常有良好的文档和示例可供参考。
- en: Good optimizations in place.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有良好的优化。
- en: Modern platforms have good connectivity to other well-used platforms.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代平台与其他广泛使用的平台有良好的连接性。
- en: Managed services available across multiple clouds.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在多个云中提供托管服务。
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Requires data modeling and database administration.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要数据建模和数据库管理。
- en: Can sometimes support unstructured data but is not always easy to implement.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时可以支持非结构化数据，但实现起来并不总是容易。
- en: Can be expensive.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能很昂贵。
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 9.2: Data storage and interface options for the ETML solution with some
    potential pros and cons.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.2：ETML 解决方案的数据存储和接口选项，以及一些潜在的优缺点。
- en: Based on these options, it seems like using the AWS CLI, S3 and the `boto3`
    package will be the simplest mechanism that provides the most flexibility in this
    scenario. In the next section, we will consider the decisions we must make around
    the scalability of our modeling approach. This is very important when working
    with batches of data, which could be extremely large in some cases.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些选项，使用 AWS CLI、S3 和 `boto3` 包似乎将是提供最大灵活性的最简单机制。在下一节中，我们将考虑围绕我们建模方法可扩展性必须做出的决策。当处理可能极其庞大的数据批次时，这一点非常重要。
- en: Scaling of models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型扩展
- en: 'In *Chapter 6*, *Scaling Up*, we discussed some of the mechanisms to scale
    up our analytics and ML workloads. We should ask ourselves whether any of these,
    or even other methods, apply to the use case at hand and use them accordingly.
    This works the other way too: if we are looking at relatively small amounts of
    data, there is no need to provision a large amount of infrastructure, and there
    may be no need to spend time creating very optimized processing. Each case should
    be examined on its own merits and within its own context.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 6 章*，*扩展规模* 中，我们讨论了一些扩展我们的分析和 ML 工作负载的机制。我们应该问自己是否这些或甚至其他方法适用于当前的使用案例，并相应地使用它们。这也同样适用：如果我们正在查看相对较小的数据量，就没有必要配置大量基础设施，可能也不需要花费时间创建非常优化的处理。每个案例都应根据自己的优点和背景进行审查。
- en: We have listed some of the options for the clustering part of the solution and
    their pros and cons in *Table 9.3*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *表 9.3* 中列出了一些解决方案聚类部分的选项及其优缺点。
- en: '| **Potential Tools** | **Pros** | **Cons** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **潜在工具** | **优点** | **缺点** |'
- en: '| Spark ML | Can scale to very large datasets. | Requires cluster management.Can
    have large overheads for smaller datasets or processing requirements.Relatively
    limited algorithm set. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Spark ML | 可以扩展到非常大的数据集。 | 需要集群管理。对于较小的数据集或处理需求可能有较大的开销。算法集相对有限。 |'
- en: '| Spark with pandas **User-Defined Function** (**UDF**) | Can scale to very
    large datasets.Can use any Python-based algorithm. | Might not make sense for
    some problems where parallelization is not easily applicable. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 使用 pandas **用户定义函数**（**UDF**）的 Spark | 可以扩展到非常大的数据集。可以使用任何基于 Python 的算法。
    | 对于某些问题，可能不适用于并行化。 |'
- en: '| Scikit-learn | Well known by many data scientists.Can run on many different
    types of infrastructure.Small overheads for training and serving. | Not very inherently
    scalable. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Scikit-learn | 许多数据科学家都很熟悉。可以在许多不同类型的基础设施上运行。训练和服务的开销很小。 | 并非本质上可扩展。 |'
- en: '| Ray AIR or Ray Serve | Relatively easy-to-use API.Good integration with many
    ML libraries. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Ray AIR 或 Ray Serve | 相对容易使用的 API。与许多机器学习库的良好集成。 |'
- en: Requires cluster management with new types of clusters (Ray clusters).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要使用新型集群（Ray 集群）进行集群管理。
- en: Newer skillset for ML engineers.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对机器学习工程师来说，需要掌握新的技能集。
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 9.3: Options for the modeling part of the ETML solution, with their pros
    and cons and with a particular focus on scalability and ease of use.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.3：ETML 解决方案建模部分的选项，包括其优缺点，特别关注可扩展性和易用性。
- en: Given these options, and assuming that the data volumes are not large for this
    example, we can comfortably stick with the Scikit-learn modeling approach, as
    this provides maximum flexibility and will likely be most easily usable by the
    data scientists in the team. It should be noted that the conversion of the Scikit-learn
    code to using a pandas UDF in Spark can be accomplished at a later date, with
    not too much work, if more scalable behavior is required.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些选项，并且假设数据量对于这个例子来说并不大，我们可以舒适地坚持使用 Scikit-learn 建模方法，因为这提供了最大的灵活性，并且可能最容易为团队中的数据科学家所使用。需要注意的是，如果需要更多可扩展的行为，Scikit-learn
    代码转换为在 Spark 中使用 pandas UDF 可以在稍后日期完成，工作量不会太大。
- en: As explained above, however, clustering is only one part of the “ML” in this
    ETML solution, the other being the text summarization part. Some potential options
    and pros and cons are shown in *Table 9.4*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如上所述，聚类只是 ETML 解决方案中“机器学习”的一部分，另一部分是文本摘要部分。一些潜在选项及其优缺点在 *表 9.4* 中展示。
- en: '| **Potential Tools** | **Pros** | **Cons** |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| **潜在工具** | **优点** | **缺点** |'
- en: '| GPT-X models from OpenAI (or another vendor) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| OpenAI 的 GPT-X 模型（或另一个供应商） |'
- en: Simple to use – we’ve already met this in *Chapter 7*, *Deep Learning, Generative
    AI, and LLMOps*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单易用 – 我们已经在 *第 7 章*，*深度学习、生成式 AI 和 LLMOps* 中遇到过。
- en: Potentially the most performant models available.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能是可用的最高性能的模型。
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can become expensive.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会变得昂贵。
- en: Not as in control of the model.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型的控制不如开源 LLM 强。
- en: Lack of visibility of data and model lineage.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和模型的可视性不足。
- en: '|'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Open-source LLMs |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 开源 LLM |'
- en: More transparent lineage of the data and model.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和模型的可追溯性更透明。
- en: More stable (you are in control of the model).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更稳定（你控制着模型）。
- en: '|'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Large infrastructure requirements.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要大量基础设施。
- en: Requires very specialized skills if optimization is required.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要优化，则需要非常专业的技能。
- en: More operational management is required (LLMOps).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要更多的运营管理（LLMOps）。
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Any other non-LLM deep learning model, e.g., a BERT variant |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 任何其他非 LLM 的深度学习模型，例如，BERT 变体 |'
- en: Can be easier to set up than some open-source LLMs.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比一些开源的 LLM 更容易设置。
- en: Extensively studied and documented.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广泛研究和记录。
- en: Easier to retrain and fine-tune (smaller models).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更容易重新训练和微调（较小的模型）。
- en: Vanilla MLOps applicable.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用 Vanilla MLOps。
- en: May not require prompt engineering.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不需要提示工程。
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: More operational overhead than an API call (but less than hosting an LLM).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比 API 调用有更多的运营开销（但比托管 LLM 少）。
- en: Less performant.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能较低。
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 9.4: Tooling options for the text summarization component of the EMTL
    solution.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.4：EMTL 解决方案文本摘要组件的工具选项。
- en: Now that we have explored the tooling decisions we have to make around scalable
    ML models, we will move on to another important topic for ETML solutions—how we
    manage the scheduling of batch processing.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了关于可扩展机器学习模型我们需要做出的工具决策，我们将转向 ETML 解决方案中的另一个重要主题——我们如何管理批量处理的调度。
- en: Scheduling of ETML pipelines
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ETML 管道调度
- en: 'The kind of batch process that ETML corresponds to is often something that
    ties in quite nicely with daily batches, but given the other two points outlined
    previously, we may need to be careful about when we schedule our jobs—for example,
    a step in our pipeline may need to connect to a production database that does
    not have a read replica (a copy of the database specifically just for reading).
    If this were the case, then we may cause major performance issues for users of
    any solutions utilizing that database if we start hammering it with queries at
    9 a.m. on a Monday morning. Similarly, if we run overnight and want to load into
    a system that is undergoing other batch upload processes, we may create resource
    contention, slowing down the process. There is no *one-size-fits-all* answer here;
    it is just important to consider your options. We look at the pros and cons of
    using some of the tools we have met throughout the book for the scheduling and
    job management of this problem in the following table:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ETML对应的批量处理类型通常与日常批量处理很好地结合，但鉴于前面提到的其他两点，我们可能需要小心安排我们的作业时间——例如，我们的管道中的一个步骤可能需要连接到一个没有读副本（仅用于读取的数据库副本）的生产数据库。如果是这种情况，那么如果我们周一早上9点开始对该数据库进行大量查询，我们可能会对使用该数据库的任何解决方案的用户造成重大性能问题。同样，如果我们整夜运行并希望将数据加载到正在经历其他批量上传过程的系统中，我们可能会造成资源争用，从而减慢处理速度。这里没有**一刀切**的答案；重要的是要考虑你的选项。我们在以下表格中查看了一些我们在本书中遇到的工具在处理此类问题的调度和作业管理方面的优缺点：
- en: '| **Potential Tools** | **Pros** | **Cons** |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **潜在工具** | **优点** | **缺点** |'
- en: '| Apache Airflow |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Apache Airflow |'
- en: Good scheduling management.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的调度管理。
- en: Relatively easy-to-use API.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对易于使用的API。
- en: Good documentation.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的文档。
- en: Cloud-hosted services are available, such as AWS **Managed Workflows for Apache
    Airflow** (**MWAA**).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供云托管服务，例如AWS的**Apache Airflow托管工作流**（**MWAA**）。
- en: Flexible for use across ML, data engineering, and other workloads.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ML、数据工程和其他工作负载中具有灵活性。
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can take time to test pipelines.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试管道可能需要花费时间。
- en: Cloud services like MWAA can be expensive.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如MWAA之类的云服务可能成本较高。
- en: Airflow is relatively general-purpose (potentially also a pro), and does not
    have too much specific functionality for ML workloads.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Airflow相对通用（可能也是一个优点），并且没有太多针对ML工作负载的特定功能。
- en: '|'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ZenML |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| ZenML |'
- en: Relatively easy-to-use API.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对易于使用的API。
- en: Good documentation.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的文档。
- en: Designed for ML engineers.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为ML工程师设计。
- en: Multiple useful MLOps integrations.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个有用的MLOps集成。
- en: A cloud option is available.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供云选项。
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can take time to test pipelines.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试管道可能需要花费时间。
- en: Slightly steeper learning curve compared to Airflow.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比Airflow，学习曲线稍微陡峭一些。
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Kubeflow |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Kubeflow |'
- en: Relatively easy-to-use API.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相对易于使用的API。
- en: Good documentation.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的文档。
- en: Simplifies the use of Kubernetes substantially if that is required.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，可以显著简化Kubernetes的使用。
- en: '|'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Requires an AWS variant for use on AWS.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要AWS变体才能在AWS上使用。
- en: Slightly steeper learning curve than the others.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比其他工具，学习曲线稍微陡峭一些。
- en: Can be harder to debug at times due to Kubernetes under the hood.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于底层使用Kubernetes，有时可能更难调试。
- en: '|'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 9.5: Pros and cons of using Apache Airflow to manage our scheduling.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.5：使用Apache Airflow管理我们的调度的优缺点。
- en: Given what we have in *Tables 9.3*, *9.4*, and *9.5*, all of the options considered
    have really strong pros and not too many cons. This means that there are many
    possible combinations of technology that will allow us to solve our problems.
    The requirements for our problem have stipulated that we have relatively small
    datasets that need to be processed in batches every day, first with some kind
    of clustering or anomaly detection algorithm before further analysis using an
    LLM. We can see that selecting `scikit-learn` for the modeling package, a GPT
    model from OpenAI called via the API, and Apache Airflow for orchestration can
    fit the bill. Again, this is not the only combination we could have gone with.
    It might be fun for you to take the example we work through in the rest of the
    chapter and try some of the other tools. Knowing multiple ways to do something
    is a key skill for an ML engineer that can help you adapt to many different situations.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在*表9.3*、*9.4*和*9.5*中的内容，所有考虑的选项都有很强的优点，而不是太多缺点。这意味着有许多可能的技术组合可以帮助我们解决问题。我们问题的要求规定，我们需要处理相对较小的数据集，每天需要批量处理，首先使用某种聚类或异常检测算法，然后再使用LLM进行进一步分析。我们可以看到，选择`scikit-learn`作为建模包，通过API调用的OpenAI的GPT模型，以及Apache
    Airflow进行编排，可以满足需求。再次强调，这并不是我们唯一可能选择的组合。你可能觉得有趣，可以尝试本章后面我们讨论的例子，并尝试一些其他工具。知道多种完成某事的方法是ML工程师的关键技能，这可以帮助你适应许多不同的情况。
- en: The next section will discuss how we can proceed with the execution of the solution,
    given this information.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将讨论，根据这些信息，我们如何进行解决方案的执行。
- en: Executing the build
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行构建
- en: Execution of the build, in this case, will be very much about how we take the
    **proof-of-concept** code shown in *Chapter 1*, *Introduction to ML Engineering*,
    and then split this out into components that can be called by another scheduling
    tool such as Apache Airflow.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，构建过程的执行将非常依赖于我们如何将*第一章*，*机器学习工程导论*中展示的**概念验证**代码拆分成可以被其他调度工具（如Apache Airflow）调用的组件。
- en: This will provide a showcase of how we can apply some of the ML engineering
    skills we learned throughout the book. In the next few sections, we will focus
    on how to build out an Airflow pipeline that leverages a series of different ML
    capabilities, creating a relatively complex solution in just a few lines of code.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这将展示我们如何应用我们在整本书中学到的某些ML工程技能。在接下来的几节中，我们将专注于如何构建一个利用一系列不同ML能力的Airflow管道，仅用几行代码就创建一个相对复杂的解决方案。
- en: Building an ETML pipeline with advanced Airflow features
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用高级Airflow功能构建ETML管道
- en: We already discussed Airflow in detail in *Chapter 5*, *Deployment Patterns
    and Tools*, but there we covered more of the details around how to deploy your
    DAGs on the cloud. Here we will focus on building in more advanced capabilities
    and control flows into your DAGs. We will work locally here on the understanding
    that when you want to deploy, you can use the process outlined in *Chapter 5*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第五章*，*部署模式和工具*中详细讨论了Airflow，但那时我们更多地覆盖了如何在云上部署你的DAGs的细节。在这里，我们将专注于在DAGs中构建更高级的功能和控制流程。我们在这里本地工作，基于这样的理解：当你想要部署时，你可以使用*第五章*中概述的过程。
- en: 'First we will look at some good DAG design practices. Many of these are direct
    applications of some of the good software engineering practices we discussed throughout
    the book; for a good review of some of these you can go back to *Chapter 4*, *Packaging
    Up*. Here we will emphasize how these can be applied to Airflow:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨一些良好的DAG设计实践。其中许多是我们在整本书中讨论的一些良好软件工程实践的直接应用；为了对这些进行良好回顾，你可以回到*第四章*，*打包*。在这里，我们将强调这些如何应用于Airflow：
- en: '**Embody separation of concerns for your tasks**: As discussed in *Chapter
    4*, separation of concerns is all about ensuring that specific pieces of code
    or software perform specific functions with minimal overlap. This can also be
    phrased as “atomicity,” using the analogy of building up your solution with “atoms”
    of specific, focused functionality. At the level of Airflow DAGs we can embody
    this principle by ensuring our DAGs are built of tasks that have one clear job
    to do in each case. So, in this example we clearly have the “extract,” “transform,”
    “ML,” and “load” stages, for which it makes sense to have specific tasks in each
    case. Depending on the complexity of those tasks they may even be split further.
    This also helps us to create good control flows and error handling, as it is far
    easier to anticipate, test, and manage failure modes for smaller, more atomic
    pieces of code. We will see this in practice with the code examples in this section.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将关注点分离应用于你的任务**：如第4章所述，关注点分离主要是确保特定的代码块或软件执行特定的功能，且重叠最小。这也可以用“原子性”来表述，即用具体的、专注的功能“原子”来构建你的解决方案。在Airflow
    DAGs的层面，我们可以通过确保我们的DAGs由每个案例都有一个明确任务的作业来体现这一原则。因此，在这个例子中，我们清楚地有“提取”、“转换”、“机器学习”和“加载”阶段，对于每个阶段都有特定的任务是合理的。根据这些任务的复杂性，它们甚至可能进一步拆分。这也帮助我们创建良好的控制流程和错误处理，因为对于更小、更原子化的代码块来说，预测、测试和管理故障模式要容易得多。我们将在本节中的代码示例中看到这一点。'
- en: '**Use retries**: You can pass in several arguments to Airflow steps that help
    control how the task will operate under different circumstances. An important
    aspect of this is the concept of “retries,” which tells the task to, you guessed
    it, try the process again if there is a failure. This is a nice way to build in
    some resiliency to your system, as there could be all sorts of reasons for a temporary
    failure in a process, such as a drop in network connectivity if it includes a
    REST API call over HTTP. You can also introduce delays between retries and even
    exponential backoff, which is when your retries have increasing time delays between
    them. This can help if you hit an API with rate limits, for example, where the
    exponential backoff will mean the system is allowed to hit the endpoint again.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用重试**：你可以向Airflow步骤传递几个参数，这些参数有助于控制任务在不同情况下的操作。这个概念的一个重要方面是“重试”，它告诉任务在出现失败时再次尝试该过程。这是一种在系统中内置一些弹性的好方法，因为一个过程中的暂时性失败可能有各种原因，例如，如果它包括通过HTTP的REST
    API调用，那么网络连接可能会下降。你还可以在重试之间引入延迟，甚至指数退避，即重试之间的时间延迟逐渐增加。如果你遇到有速率限制的API，这可能很有帮助，例如，指数退避意味着系统被允许再次访问端点。'
- en: '**Mandate idempotency in your DAGs**: Idempotency is the quality of code that
    returns the same result when run multiple times on the same inputs. It can easily
    be assumed that most programs work this way but that is definitely not the case.
    We have made extensive use of Python objects in this book that contain internal
    states, for example, ML models in `scikit-learn` or neural networks in PyTorch.
    This means that it should not be taken for granted that idempotency is a feature
    of your solution. Idempotency is very useful, especially in ETML pipelines, because
    it means if you need to perform retries then you know that this will not lead
    to unexpected side effects. You can enforce idempotency at the level of your DAG
    by enforcing it at the level of your tasks that make up the DAG. The challenge
    for an EMTL application is that we obviously have ML models, which I’ve just mentioned
    can be a challenge for this concept! So, some thought is required to make sure
    that retries and the ML steps of your pipeline can play nicely together.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在你的DAGs中强制执行幂等性**：幂等性是指代码在多次运行相同输入时返回相同结果的质量。很容易假设大多数程序都是这样工作的，但这绝对不是事实。在这本书中，我们已经广泛使用了包含内部状态的Python对象，例如`scikit-learn`中的机器学习模型或PyTorch中的神经网络。这意味着不能想当然地认为幂等性是解决方案的一个特性。幂等性非常有用，尤其是在ETML管道中，因为它意味着如果你需要执行重试，你知道这不会导致意外的副作用。你可以在DAG的层面通过强制执行组成DAG的任务的幂等性来强制执行幂等性。对于EMTL应用程序来说，挑战在于我们显然有机器学习模型，正如我刚才提到的，这可能会对这一概念构成挑战！因此，需要一些思考来确保重试和管道的机器学习步骤可以很好地协同工作。'
- en: '**Leverage the Airflow operators and provider package ecosystem**: Airflow
    comes with a large list of operators to perform all sorts of tasks, and there
    are several packages designed to help integrate with other tools called *provider
    packages*. The advice here is to **use them**. This speaks to the points discussed
    in *Chapter 4*, *Packaging Up,* about “not reinventing the wheel” and ensures
    that you can focus on building the appropriate logic you need for your workloads
    and system and not on creating boilerplate integrations. For example, we can use
    the Spark provider package. We can install it with:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用Airflow操作符和提供者包生态系统**：Airflow附带了一系列操作符，可以执行各种任务，还有几个包旨在帮助与其他工具集成，称为*提供者包*。这里的建议是**使用它们**。这涉及到*第4章*，*打包*中讨论的“不要重复造轮子”的观点，并确保你可以专注于构建适合你的工作负载和系统的适当逻辑，而不是创建样板式的集成。例如，我们可以使用Spark提供者包。我们可以通过以下方式安装它：'
- en: '[PRE2]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then in a DAG we can submit a Spark application, for example one contained
    within a script called `spark-script.py`, for a run with:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，在DAG中，我们可以提交一个Spark应用程序，例如一个名为`spark-script.py`的脚本中包含的应用程序，以以下方式运行：
- en: '[PRE3]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Use** `with DAG() as dag`: In the example above you will see that we used
    this pattern. This context manager pattern is one of the three main ways you can
    define a DAG, the other two being using the DAG constructor and passing it to
    all tasks in your pipeline or using a decorator to convert a function to a DAG.
    The use of a context manager means, as in any usage of it in Python, that any
    resources defined within the context are cleaned up correctly even if the code
    block exists with an exception. The mechanism that uses the constructor requires
    passing `dag=dag_name` into every task you define in the pipeline, which is quite
    laborious. Using the decorator is quite clean for basic DAGs but can become quite
    hard to read and maintain if you build more complex DAGs.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用** `with DAG() as dag`：在上面的例子中，你会看到我们使用了这种模式。这种上下文管理器模式是你可以定义DAG的三个主要方法之一，其他两种是使用DAG构造函数并将它传递给管道中的所有任务，或者使用装饰器将函数转换为DAG。使用上下文管理器的意思是，就像在Python中任何使用它的场合一样，任何在上下文中定义的资源都会被正确清理，即使代码块因为异常而存在。使用构造函数的机制要求将`dag=dag_name`传递到管道中你定义的每个任务中，这相当繁琐。使用装饰器对于基本的DAG来说相当干净，但如果构建更复杂的DAG，它可能会变得难以阅读和维护。'
- en: '**Remember to test (!)**: After reading the rest of this book and becoming
    a confident ML engineer, I can hear you shouting at the page, “What about testing?!”,
    and you would be right to shout. Our code is only as good as the tests we can
    run on it. Luckily for us Airflow provides some out-of-the-box functionality that
    enables local testing and debugging of your DAGs. For debugging in your IDE or
    editor, if your DAG is called `dag`, like in the example above, all you need to
    do is add the below snippet to your DAG definition file to run the DAG in a local,
    serialized Python process within your chosen debugger. This does not run the scheduler;
    it just runs the DAG steps in a single process, which means it fails fast and
    gives quick feedback to the developer:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记住要测试（!）**：在阅读完这本书的其余部分并成为自信的机器学习工程师之后，我可以听到你在页面上大喊，“关于测试呢？！”，你是对的，应该大喊。我们的代码只有在我们能对其运行的测试中才是好的。幸运的是，Airflow提供了一些开箱即用的功能，使得可以在本地测试和调试你的DAG。对于在IDE或编辑器中进行调试，如果你的DAG名为`dag`，就像上面的例子一样，你只需要将以下片段添加到你的DAG定义文件中，就可以在选择的调试器中运行DAG，在本地序列化的Python进程中。这不会运行调度器；它只是在单个进程中运行DAG步骤，这意味着它会快速失败，并给开发者提供快速反馈：'
- en: '[PRE4]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can also use `pytest` like we did in *Chapter 4*, *Packaging Up*, and elsewhere
    in the book.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们也可以像在*第4章*，*打包*，以及书中的其他地方所做的那样使用`pytest`。
- en: Now that we have discussed some of the important concepts we can use, we will
    start to build up the Airflow DAG in some detail, and we will try and do this
    in a way that shows you how to build some resiliency into the solution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了一些我们可以使用的重要概念，我们将开始详细构建Airflow DAG，并且我们将尝试以展示如何将一些弹性构建到解决方案中的方式来完成这项工作。
- en: 'First, it is important to note that, for this example, we will actually perform
    the ETML process twice: once for the clustering component and once for the text
    summarization. Doing it this way means that we can use *intermediary storage*
    in between the steps, in this case, AWS S3 again, in order to introduce some resiliency
    into the system. This is so because if the second step fails, it doesn’t mean
    the first step’s processing is lost. The example we will walk through does this
    in a relatively straightforward way, but as always in this book, remember that
    the concepts can be extended and adapted to use tooling and processes of your
    choice, as long as the fundamentals remain solid.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，重要的是要注意，对于这个例子，我们实际上将执行 ETML 流程两次：一次用于聚类组件，一次用于文本摘要。这样做意味着我们可以在步骤之间使用 *中间存储*，在这种情况下，再次使用
    AWS S3，以便在系统中引入一些弹性。这是因为如果第二步失败，并不意味着第一步的处理丢失。我们将以相对直接的方式展示这个例子，但就像本书中始终强调的那样，请记住，这些概念可以扩展和适应您选择的工具和流程，只要基本原理保持稳固。
- en: 'Let’s start building this DAG! It is a relatively short one that only contains
    two tasks; we will show the DAG first and then expand on the details:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建这个 DAG！这是一个相对简短的 DAG，只包含两个任务；我们将首先展示 DAG，然后详细说明：
- en: '[PRE5]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`extract_cluster_load_task`: This task will extract the input data from the
    appropriate S3 bucket and perform some clustering using DBSCAN, before loading
    the original data joined to the model output to the intermediary storage location.
    For simplicity we will use the same bucket for intermediate storage, but this
    could be any location or solution that you have connectivity to.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_cluster_load_task`: 此任务将从适当的 S3 存储桶中提取输入数据，并使用 DBSCAN 进行一些聚类，然后将原始数据与模型输出连接到中间存储位置。为了简单起见，我们将使用相同的存储桶作为中间存储，但这可以是任何您有连接性的位置或解决方案。'
- en: '`extract_summarize_load_task`: Similarly, the first step here is to extract
    the data from S3 using the boto3 library. The next step is to take the data and
    then call the appropriate LLM to perform summarization on the text fields selected
    in the data, specifically those that contain information on the local news, weather,
    and traffic reports for the day of the batch run.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_summarize_load_task`: 同样，这里的第一个步骤是使用 boto3 库从 S3 中提取数据。接下来的步骤是将数据取出，然后调用适当的
    LLM 对数据中选定的文本字段进行摘要，特别是那些包含关于批次运行当天当地新闻、天气和交通报告的信息的字段。'
- en: You will have noticed upon reading the DAG that the reason the DAG definition
    is so short is that we have abstracted away most of the logic into subsidiary
    modules, in line with the principles of keeping it simple, separating our concerns,
    and applying modularity. See *Chapter 4*, *Packaging Up*, for an in-depth discussion
    of these and other important concepts.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读 DAG 后，您可能会注意到 DAG 定义之所以如此简短，是因为我们将大部分逻辑抽象到了辅助模块中，这符合保持简单、分离关注点和应用模块化的原则。请参阅
    *第 4 章*，*打包*，以深入了解这些和其他重要概念。
- en: 'The first component that we use in the DAG is the functionality contained in
    the `Clusterer` class under `utils.cluster`. The full definition of this class
    is given below. I have omitted standard imports for brevity:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 DAG 中使用的第一个组件是 `utils.cluster` 下的 `Clusterer` 类的功能。此类的完整定义如下。为了简洁起见，我省略了标准导入：
- en: '[PRE6]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the constructor of the class contains a reference to a default set
    of `model_params`. These can be read in from a configuration file. I have included
    them here for visibility. The actual clustering and labeling method within the
    class is relatively simple; it just standardizes the incoming features, applies
    the DBSCAN clustering algorithm, and then exports the originally extracted dataset,
    which now includes the cluster labels. One important point to note is that the
    features used for clustering are supplied as a list so that this can be extended
    in the future if desired, or if richer data for clustering is available, simply
    by altering the `op_kwargs` argument supplied to the first task’s `PythonOperator`
    object in the DAG.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，类的构造函数包含对默认 `model_params` 集合的引用。这些可以从配置文件中读取。我已在此处包含它们以供参考。类内的实际聚类和标记方法相对简单；它只是标准化传入的特征，应用
    DBSCAN 聚类算法，然后导出最初提取的数据集，现在包括聚类标签。一个需要注意的重要点是，用于聚类的特征以列表的形式提供，这样就可以在未来扩展，或者如果可用更丰富的聚类数据，只需通过更改提供给
    DAG 中第一个任务的 `PythonOperator` 对象的 `op_kwargs` 参数即可。
- en: After the first task, which uses the `Clusterer` class, runs successfully, a
    JSON is produced giving the source records and their cluster labels. Two random
    examples from the produced file are given in *Figure 9.5*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个任务成功运行后，该任务使用 `Clusterer` 类，会生成一个 JSON 文件，其中包含源记录及其聚类标签。在 *图 9.5* 中给出了生成的文件中的两个随机示例。
- en: '![A picture containing text, screenshot, font, document  Description automatically
    generated](img/B19525_09_05.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、字体、文档的图片，自动生成描述](img/B19525_09_05.png)'
- en: 'Figure 9.5: Two example records of the data produced after the clustering step
    in the ETML pipeline.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：ETML 流程中聚类步骤后生成的两个示例记录。
- en: 'You may have noticed that, at the top of this example, another utility class
    is imported, the `Extractor` class from the `utils.extractor` module. This is
    simply a wrapper around some `boto3` functionality and is defined below:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，在这个示例的顶部，还导入了另一个实用工具类，即来自 `utils.extractor` 模块的 `Extractor` 类。这只是一个围绕一些
    `boto3` 功能的包装器，定义如下：
- en: '[PRE7]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let’s move on to the definition of the other class used in the DAG, the
    `LLMSummarizer` from the `utils.summarize` module:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续定义 DAG 中使用的另一个类，即来自 `utils.summarize` 模块的 `LLMSummarizer` 类：
- en: '[PRE8]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can see that this class follows a similar design pattern to the one used
    in the `Clusterer` class, only now the method that we utilize has the role of
    prompting an OpenAI LLM of our choosing, using a standard template that we have
    hardcoded. Again, this prompt template can be extracted out into a configuration
    file that is packaged up with the solution, but is shown here for visibility.
    The prompt asks that the LLM summarizes the relevant pieces of contextual information
    supplied, concerning local news, weather, and traffic reports, so that we have
    a concise summary that can be used for downstream analysis or to render in a user
    interface. A final important point to note here is that the method to generate
    the summary, which wraps a call to OpenAI APIs, has a `try except` clause that
    will allow for a fallback to a different model if the first model call experiences
    any issues. At the time of writing in May 2023, OpenAI APIs still show some brittleness
    when it comes to latency and rate limits, so steps like this allow you to build
    in more resilient workflows.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，这个类遵循与 `Clusterer` 类相似的设计模式，但现在我们利用的方法是提示我们选择的 OpenAI LLM，使用我们硬编码的标准模板。同样，这个提示模板可以提取出来，与解决方案一起打包到一个配置文件中，但在这里展示是为了可见性。提示要求
    LLM 概括提供的有关当地新闻、天气和交通报告的相关上下文信息，以便我们有一个简洁的摘要，可用于下游分析或用于用户界面。最后，需要注意的是，生成摘要的方法，它封装了对
    OpenAI API 的调用，有一个 `try except` 子句，如果第一个模型调用遇到任何问题，将允许回退到不同的模型。截至 2023 年 5 月，OpenAI
    API 在延迟和速率限制方面仍然显示出一些脆弱性，因此这样的步骤允许您构建更健壮的工作流程。
- en: An example output from the application of the `LLMSummarizer` class when running
    the DAG is given in *Figure 9.6*.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 DAG 时应用 `LLMSummarizer` 类的一个示例输出在 *图 9.6* 中给出。
- en: '![A picture containing text, screenshot, font  Description automatically generated](img/B19525_09_06.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、字体的图片，自动生成描述](img/B19525_09_06.png)'
- en: 'Figure 9.6: An example output from the LLMSummarizer; you can see that it takes
    in the news, weather, and traffic information and produces a concise summary that
    can help any downstream consumers of this data understand what it means for overall
    traffic conditions.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：LLMSummarizer 的一个示例输出；您可以看到它接收新闻、天气和交通信息，并生成一个简洁的摘要，可以帮助任何下游消费者理解整体交通状况的含义。
- en: A potential area of optimization in this piece of code is around the prompt
    template used, as there is the potential for some nice prompt engineering to try
    and shape the output from the LLM to be more consistent. You could also use a
    tool like LangChain, which we met in *Chapter 7*, *Deep Learning, Generative AI,
    and LLMOps*, to perform more complex prompting of the model. I leave this as a
    fun exercise for the reader.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，一个潜在的优化区域是围绕所使用的提示模板，因为有可能进行一些很好的提示工程，以尝试使 LLM 的输出更加一致。您还可以使用我们在 *第 7
    章* 中遇到的工具 LangChain，进行更复杂的模型提示。我将这留作读者的一项有趣练习。
- en: Now that we have defined all the logic for our DAG and the components it uses,
    how will we actually configure this to run, even in standalone mode? When we deployed
    a DAG to **MWAA**, the AWS-hosted and managed Airflow solution, in *Chapter 4*,
    you may recall that we had to send our DAG to a specified bucket that was then
    read in by the system.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们 DAG 及其使用的所有逻辑，我们实际上将如何配置它以运行，即使在独立模式下？当我们将在 *第 4 章* 中部署 DAG 到 **MWAA**，AWS
    托管和管理的 Airflow 解决方案时，您可能还记得，我们必须将我们的 DAG 发送到一个指定的存储桶，然后由系统读取。
- en: 'For your own hosted or local Airflow instances the same point applies; this
    time we need to send the DAG to a `dags` folder, which is located in the `$AIRFLOW_HOME`
    folder. If you have not explicitly configured this for your installation of Airflow
    you will use the default, which is typically in a folder called `airflow` under
    your `home` directory. To find this and lots of other useful information you can
    execute the following command, which produces the output shown in *Figure 9.7*:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您自己的托管或本地 Airflow 实例，同样适用；这次我们需要将 DAG 发送到 `$AIRFLOW_HOME` 文件夹中的 `dags` 文件夹。如果您没有明确为您的
    Airflow 安装配置此文件夹，您将使用默认设置，通常位于您 `home` 目录下的名为 `airflow` 的文件夹中。要找到这个以及其他许多有用的信息，您可以执行以下命令，该命令将产生如
    *图 9.7* 所示的输出：
- en: '[PRE9]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B19525_09_07.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序屏幕截图  描述自动生成，置信度中等](img/B19525_09_07.png)'
- en: 'Figure 9.7: The output from the airflow info command.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：airflow info 命令的输出。
- en: Once you have found the location of the `$AIRFLOW_HOME` folder, if there isn’t
    a folder called `dags` already, create one. For simple, self-contained DAGs that
    do not use sub-modules, all you need to do to mock up deployment is to copy the
    DAG over to this folder, similar to how we sent our DAG to S3 in the example with
    MWAA in *Chapter 5*. Since we use multiple sub-modules in this example we can
    either decide to install them as a package, using the techniques developed in
    *Chapter 4*, *Packaging Up*, and make sure that it is available in the Airflow
    environment, or we can simply send the sub-modules into the same `dags` folder.
    For simplicity that is what we will do here, but please consult the official Airflow
    documentation for details on this.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您找到了 `$AIRFLOW_HOME` 文件夹的位置，如果还没有名为 `dags` 的文件夹，请创建一个。对于简单、自包含且不使用子模块的 DAG，您要做的只是将
    DAG 复制到这个文件夹中，就像我们在 *第 5 章* 中的例子中将 DAG 发送到 S3 一样。由于我们在本例中使用了多个子模块，我们可以选择使用在 *第
    4 章* 中开发的打包技术将它们作为包安装，并确保它们在 Airflow 环境中可用，或者我们可以简单地将子模块发送到同一个 `dags` 文件夹。为了简化，我们将这样做，但请查阅官方
    Airflow 文档以获取有关此方面的详细信息。
- en: Once we have copied over the code, if we access the Airflow UI, we should be
    able to see our DAG like in *Figure 9.8*. As long as the Airflow server is running,
    the DAG will run on the supplied schedule. You can also trigger manual runs in
    the UI for testing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们复制了代码，如果我们访问 Airflow UI，我们应该能够看到我们的 DAG，就像在 *图 9.8* 中所示。只要 Airflow 服务器正在运行，DAG
    将按照提供的计划运行。您也可以在 UI 中手动触发运行以进行测试。
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B19525_09_08.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度中等](img/B19525_09_08.png)'
- en: 'Figure 9.8: The ETML DAG in the Airflow UI.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：Airflow UI 中的 ETML DAG。
- en: Running the DAG will result in the intermediary and final output JSON files
    being created in the S3 bucket, as shown in *Figure 9.9*.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 DAG 将导致在 S3 存储桶中创建中间和最终输出 JSON 文件，如图 *图 9.9* 所示。
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B19525_09_09.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成，置信度低](img/B19525_09_09.png)'
- en: 'Figure 9.9: The successful run of the DAG creates the intermediate and final
    JSON files.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：DAG 成功运行创建中间和最终 JSON 文件。
- en: And with that, we have now built an ETML pipeline that takes in some taxi ride
    data, clusters this based on ride distance and time, and then performs text summarization
    on some contextual information using an LLM.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们现在已经构建了一个 ETML 管道，它接收一些出租车行程数据，根据行程距离和时间进行聚类，然后使用 LLM 对一些上下文信息进行文本摘要。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter has covered how to apply a lot of the techniques learned in this
    book, in particular from *Chapter 2*, *The Machine Learning Development Process*,
    *Chapter 3*, *From Model to Model Factory*, *Chapter 4*, *Packaging Up*, and *Chapter
    5*, *Deployment Patterns and Tools*, to a realistic application scenario. The
    problem, in this case, concerned clustering taxi rides to find anomalous rides
    and then performing NLP on some contextual text data to try and help explain those
    anomalies automatically. This problem was tackled using the ETML pattern, which
    I offered up as a way to rationalize typical batch ML engineering solutions. This
    was explained in detail. A design for a potential solution, as well as a discussion
    of some of the tooling choices any ML engineering team would have to go through,
    was covered. Finally, a deep dive into some of the key pieces of work that would
    be required to make this solution production-ready was performed. In particular
    we showed how you can use good object-orientated programming techniques to wrap
    ML functionality spanning the Scikit-learn package, the AWS `boto3` library, and
    OpenAI APIs to use LLMs to create some complex functionality. We also explored
    in detail how to use more complex features of Airflow to orchestrate these pieces
    of functionality in ways that are resilient.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了如何将本书中学到的许多技术应用到实际应用场景中，特别是从第二章《机器学习开发过程》、第三章《从模型到模型工厂》、第四章《打包》和第五章《部署模式和工具》中学习的技术。这个问题涉及将出租车行程进行聚类以寻找异常行程，然后对一些上下文文本数据进行自然语言处理（NLP），以尝试自动解释这些异常。这个问题是通过ETML模式解决的，我提出这个模式是为了合理化典型的批量机器学习工程解决方案。这一点已经详细解释。还涉及了一个潜在解决方案的设计，以及讨论了任何机器学习工程团队都必须经历的某些工具选择。最后，深入研究了使该解决方案达到生产就绪所需的一些关键工作。特别是，我们展示了如何使用良好的面向对象编程技术来封装跨越Scikit-learn包、AWS
    `boto3`库和OpenAI API的机器学习功能，以使用LLMs创建一些复杂的功能。我们还详细探讨了如何使用Airflow的更复杂功能来编排这些功能，使其具有弹性。
- en: With that, you have not only completed this chapter but also the second edition
    of *Machine Learning Engineering with Python*, so congratulations! Throughout
    this book, we have covered a wide variety of topics related to ML engineering,
    from how to build your teams and what development processes could look like, all
    the way through to packaging, scaling, scheduling, deploying, testing, logging,
    and a whole bunch of stuff in between. We have explored AWS and managed cloud
    services, we have performed deep dives into open-source technologies that give
    you the ability to orchestrate, pipeline, and scale, and we have also explored
    the exciting new world of LLMs, generative AI, and LLMOps.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你已经不仅完成了这一章节，还完成了《Python机器学习工程》的第二版，所以恭喜你！在这本书中，我们涵盖了与机器学习工程相关的广泛主题，从如何组建团队以及可能的发展流程，一直到打包、扩展、调度、部署、测试、日志记录以及其中的一系列其他内容。我们探讨了AWS和管理的云服务，深入研究了开源技术，这些技术赋予你编排、管道化和扩展的能力，我们还探索了令人兴奋的新领域：大型语言模型（LLMs）、生成式AI和LLMOps。
- en: There are so many topics to cover in this fast-moving, ever-changing, and exhilarating
    world of ML engineering and MLOps that one book could never do the whole field
    justice. This second edition, however, has attempted to improve upon the first
    by providing some more breadth and depth in areas I think will be important to
    help develop the next generation of ML engineering talent. It is my hope that
    you come away from reading this book not only feeling well equipped but also as
    excited as I am every day to go out and build the future. If you work in this
    space, or you are moving into it, then you are doing so at what I believe is a
    truly unique time in history. As ML systems are required to keep becoming more
    powerful, pervasive, and performant, I believe the demand for ML engineering skills
    is only going to grow. I hope this book has given you the tools you need to take
    advantage of that, and that you have enjoyed the ride as much as I have!
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个快速变化、不断发展和令人兴奋的机器学习工程和MLOps世界中，有如此多的主题需要探讨，以至于一本书根本无法对整个领域做出公正的评价。然而，这一版试图通过提供一些我认为对培养下一代机器学习工程人才重要领域的更多广度和深度来改进第一版。我希望你在阅读这本书后不仅感到装备齐全，而且像我每天出去构建未来一样兴奋。如果你在这个领域工作，或者你正在进入这个领域，那么你正在经历的是我认为历史上一个真正独特的时间。随着机器学习系统需要不断变得更加强大、普遍和高效，我相信对机器学习工程技能的需求只会增长。我希望这本书已经为你提供了利用这一点的工具，并且你享受这段旅程的程度和我一样！
- en: Join our community on Discord
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mle](https://packt.link/mle)'
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code102810325355484.png)'
