- en: Chapter 9. Ensemble Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。集成方法
- en: In this chapter, we take a step back from learning new models and instead think
    about how several trained models can work together as an ensemble, in order to
    produce a single model that is more powerful than the involved models, individually.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们暂时放慢学习新模型的速度，而是思考如何将几个训练好的模型作为一个集成一起工作，以产生一个比单个模型更强大的单一模型。
- en: The first type of ensemble that we will study uses different samples of the
    same dataset in order to train multiple versions of the same model. These models
    then vote on the correct answer for a new observation and an average or majority
    decision is made, depending on the type of problem. This process is known as bagging,
    which is short for bootstrap aggregation. Another approach to combine models is
    boosting. This essentially involves training a chain of models and assigning weights
    to observations that were incorrectly classified or fell far from their predicted
    value so that successive models are forced to prioritize them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的第一种集成类型使用相同数据集的不同样本来训练同一模型的多个版本。然后，这些模型对新观测的正确答案进行投票，并基于问题的类型做出平均或多数决策。这个过程被称为bagging，即bootstrap
    aggregation的缩写。另一种结合模型的方法是boosting。这本质上涉及训练一系列模型，并为被错误分类或远低于其预测值的观测分配权重，以便后续模型被迫优先考虑它们。
- en: As methods, bagging and boosting are fairly general and have been applied with
    a number of different types of models. Decision trees, studied in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-Based Methods*, are particularly
    suited to ensemble methods. So much so, that a particular type of tree-based ensemble
    model has its own name--the random forest. Random forests offer significant improvements
    over the single decision tree and are generally considered to be very powerful
    and flexible models, as we shall soon discover. In this chapter, we'll revisit
    some of the datasets we analyzed in previous chapters and see if we can improve
    performance by applying some of the principles we learn here.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作为方法，Bagging和Boosting相当通用，并且已经应用于多种不同类型的模型。在第6章（[第6章](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "第6章。支持向量机"））中研究的决策树，*基于树的模型*，特别适合于集成方法。如此之甚，以至于一种特定的基于树的集成模型有它自己的名字——随机森林。随机森林在单棵决策树之上提供了显著的改进，通常被认为是非常强大和灵活的模型，我们很快就会发现这一点。在本章中，我们将回顾我们在前几章中分析的一些数据集，看看我们是否可以通过应用在这里学到的某些原则来提高性能。
- en: Bagging
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging
- en: The focus of this chapter is on combining the results from different models
    in order to produce a single model that will outperform individual models on their
    own. **Bagging** is essentially an intuitive procedure for combining multiple
    models trained on the same dataset, by using majority voting for classification
    models and average value for regression models. We'll present this procedure for
    the classification case, and later show how this is easily extended to handle
    regression models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是结合不同模型的结果，以产生一个能够优于单个模型的单一模型。**Bagging**是一种直观的将多个在同一数据集上训练的模型结合起来的过程，对于分类模型使用多数投票，对于回归模型使用平均值。我们将首先介绍这个分类案例的过程，稍后展示如何轻松扩展以处理回归模型。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Bagging procedure for binary classification**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**二元分类的Bagging过程**'
- en: '**Inputs**:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：'
- en: '*data*: The input data frame containing the input features and a column with
    the binary output label.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据*：包含输入特征和具有二进制输出标签列的输入数据框。'
- en: '*M*: An integer, representing the number of models that we want to train.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*M*：一个整数，表示我们想要训练的模型数量。'
- en: '**Output**:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '*models*: A set of Μ trained binary classifier models.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型*：一组M个训练好的二元分类器模型。'
- en: '**Method**:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**：'
- en: 1\. Create a random sample of size *n*, where *n* is the number of observations
    in the original dataset, with replacement. This means that some of the observations
    from the original training set will be repeated and some will not be chosen at
    all. This process is known as **bootstrapping**, **bootstrap sampling**, or **bootstrap
    resampling**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 创建一个大小为 *n* 的随机样本，其中 *n* 是原始数据集中的观测数，带有替换。这意味着原始训练集中的某些观测将被重复，而某些观测则根本不会被选中。这个过程被称为**bootstrapping**、**bootstrap
    sampling**或**bootstrap resampling**。
- en: 2\. Train a classification model using this sampled dataset. Typically, we opt
    not to use regularization or shrinkage methods designed to reduce overfitting
    in this step, because the aggregating process used at the end will be used to
    smooth out overfitting.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 使用这个样本数据集来训练一个分类模型。通常，我们在这个步骤中不选择使用正则化或收缩方法来减少过拟合，因为最终使用的聚合过程将用于平滑过拟合。
- en: 3\. For each observation in the sampled dataset, record the class assigned by
    the model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 对于样本数据集中的每个观察结果，记录模型分配的类别。
- en: 4\. Repeat this process *M* times in order to train *M* models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 重复这个过程 *M* 次，以训练 *M* 个模型。
- en: 5\. For every observation in the original training dataset, compute the predicted
    class via a majority vote across the different models. For example, suppose M
    = 61 and through bootstrap sampling, a particular observation appears in the training
    data for 50 of the models. If 37 of these predict class 1 for this observation
    and 13 predict class -1, by majority vote the overall prediction will be class
    1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 对于原始训练数据集中的每个观察结果，通过不同模型之间的多数投票来计算预测的类别。例如，假设 M = 61，通过自助抽样，一个特定的观察结果出现在50个模型中的训练数据中。如果有37个模型预测这个观察结果为类别1，而13个模型预测为类别-1，那么通过多数投票，整体预测将是类别1。
- en: 6\. Compute the model's accuracy using the labels provided by the training set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 使用训练集提供的标签来计算模型的准确率。
- en: In a nutshell, all we are effectively doing is training the same model on *M*
    different versions of the input training set (created through sampling with replacement)
    and averaging the result.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们实际上只是在 *M* 个不同的输入训练集版本（通过有放回抽样创建）上训练相同的模型，并平均结果。
- en: A legitimate question to ask would be, how many distinct observations do we
    get each time we sample with replacement? On average, we end up with 63 percent
    of the distinct observations in every sample that we make. To understand where
    this comes from, consider that because we are sampling with replacement, the probability
    of not picking out a particular observation, *x[1]*, during sampling is just the
    result of *n* failed Bernoulli trials.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个合理的问题可能是，每次我们进行有放回的抽样时，我们会得到多少不同的观察结果？平均而言，我们得到的每个样本中都有63%的不同观察结果。为了理解这是如何产生的，考虑一下，因为我们是有放回地抽样，所以在抽样过程中没有抽到特定观察结果
    *x[1]* 的概率，仅仅是 *n* 次伯努利试验失败的结果。
- en: '![Bagging](img/00158.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging](img/00158.jpeg)'
- en: This number also happens to be the average proportion of observations that are
    not selected across the entire training dataset because we multiply and divide
    the previous expression by *n* to compute this quantity. The numerical result
    of this expression can be approximated by *e-1*, which is roughly 37 percent.
    Consequently, the average proportion of observations that are selected is around
    63 percent. This number is just an average, of course, and is more accurate for
    larger values of *n*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字也恰好是整个训练数据集中未选择的观察结果的平均比例，因为我们通过将前面的表达式乘以 *n* 并除以 *n* 来计算这个量。这个表达式的数值结果可以近似为
    *e-1*，大约是37%。因此，所选观察结果的平均比例大约是63%。这个数字只是一个平均值，当然，对于更大的 *n* 值来说更准确。
- en: Margins and out-of-bag observations
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 间隔和袋外观察结果
- en: Let's imagine that for a particular observation, *x[1]*, 85 percent of our models
    predict the correct class and the remaining 15 percent predict the incorrect class.
    Let's also imagine that we have another observation, *x[2]*, for which the analogous
    percentages are 53 percent and 47 percent. Clearly, our intuition suggests that
    we should be more confident about the classification of the former observation
    compared to the latter observation. Put differently, the difference between the
    classification proportions, also known as the **margin** (similar to but not to
    be confused with the margin used for support vector machines) is a good indicator
    of the confidence of our classification.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设对于特定的观察结果 *x[1]*，85%的模型预测正确的类别，而剩下的15%预测错误的类别。让我们再假设有一个另一个观察结果 *x[2]*，其相应的百分比是53%和47%。显然，我们的直觉表明，我们应该对前者的分类比后者的分类更有信心。换句话说，分类比例之间的差异，也称为**间隔**（类似于但不要与支持向量机中使用的间隔混淆），是我们分类置信度的一个良好指标。
- en: The 70 percent margin of observation *x[1]* is much larger than the 6 percent
    margin of observation *x[2]* and thus, we believe more strongly in our ability
    to correctly classify the former observation. In general, what we are hoping for
    is a classifier that has a large margin for all the observations. We are less
    optimistic about the generalization abilities of a classifier that has a small
    margin for more than a handful of observations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到的70%的误差范围 *x[1]* 比观察到的6%的误差范围 *x[2]* 大得多，因此，我们对我们正确分类前者的能力更有信心。一般来说，我们希望的是一个对所有观察都有大误差范围的分类器。对于只有少数观察有较小误差范围的分类器的泛化能力，我们不太乐观。
- en: One thing the reader may have noticed here is that in generating the set of
    predicted values for each model, we are using the same data on which the model
    was trained. If we look closely at *step 3* of the procedure, we are classifying
    the same sampled data that we used in *step 2* to train the model. Even though
    we are eventually relying on using an averaging process at the end in order to
    obtain the estimated accuracy of the bagged classifier for unseen data, we haven't
    actually used any unseen data at any step along the way.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可能已经注意到，在生成每个模型的预测值集时，我们使用了训练模型相同的同一数据。如果我们仔细观察程序的*步骤3*，我们会用我们在*步骤2*中用于训练模型的相同采样数据进行分类。尽管我们最终依赖于在最后使用平均过程来获得未见数据的袋装分类器的估计准确率，但我们实际上在过程中任何步骤都没有使用任何未见数据。
- en: Remember that in *step 1* we constructed a sample of the training data with
    which to train our model. From the original dataset, we refer to the observations
    that were not chosen for a particular iteration of the procedure as the **out-of-bag**
    (**OOB**) observations. These observations are therefore not used in the training
    of the model at that iteration. Consequently, instead of relying on the observations
    used to train the models at every step, we can actually use the OOB observations
    to record the accuracy of a particular model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在*步骤1*中，我们构建了一个训练数据的样本来训练我们的模型。从原始数据集中，我们将未选择用于该程序特定迭代的观察称为**袋外观察**（**OOB**）。因此，这些观察在该迭代中未用于模型的训练。因此，我们实际上可以使用OOB观察来记录特定模型的准确率。
- en: In the end, we average over all the OOB accuracy rates to obtain an average
    accuracy. This average accuracy is far more likely to be a realistic and objective
    estimate of the performance of the bagged classifier on unseen data. For a particular
    observation, the assigned class is thus decided as the majority vote over all
    classifiers for which the observation was not picked in their corresponding training
    sample.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算所有OOB准确率的平均值以获得平均准确率。这个平均准确率更有可能是对未见数据上袋装分类器性能的现实和客观估计。对于特定的观察，所分配的类别是所有在对应训练样本中没有选择该观察的分类器的多数投票结果。
- en: The samples generated from sampling the original dataset with replacement, known
    as **bootstrapped samples**, are similar to drawing multiple samples from the
    same distribution. As we are trying to estimate the same target function using
    a number of different samples instead of just one, the averaging process reduces
    the variance of the result. To see this, consider trying to estimate the mean
    of a set of observations drawn from the same distribution and all mutually independent
    of each other. More formally, these are known as **independent and identically
    distributed** (**iid**) observations. The variance of the mean of these observations
    is ![Margins and out-of-bag observations](img/00159.jpeg).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始数据集有放回地抽取的样本，称为**自助样本**，类似于从同一分布中抽取多个样本。由于我们试图使用多个不同的样本而不是一个来估计相同的目标函数，平均过程减少了结果的变化。为了看到这一点，考虑尝试估计从同一分布中抽取的一组观察的平均值，并且所有观察都是相互独立的。更正式地说，这些被称为**独立同分布**（**iid**）的观察。这些观察的平均值的方差是![误差范围和袋外观察](img/00159.jpeg)。
- en: This shows that as the number of observations increases, the variance decreases.
    Bagging tries to achieve the same behavior for the function we are trying to model.
    We don't have truly independent training samples, and are instead forced to use
    bootstrapped samples, but this thought experiment should be enough to convince
    us that, in principle, bagging has the potential to reduce the variance of the
    model. At the same time, this averaging process is a form of smoothing over any
    localized bumps in the function that we are trying to estimate. Assuming that
    the target regression function or classification boundary that we are trying to
    estimate is actually smooth, then bagging may also reduce the bias of our model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明随着观测值的增加，方差会减小。袋装法试图为我们试图建模的函数实现相同的行为。我们并没有真正独立的训练样本，而是被迫使用自助样本，但这个思想实验应该足以让我们相信，原则上，袋装法有可能减少模型的方差。同时，这种平均过程是对我们试图估计的函数中任何局部峰值的平滑。假设我们试图估计的目标回归函数或分类边界实际上是平滑的，那么袋装法也可能减少我们模型的偏差。
- en: Predicting complex skill learning with bagging
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用袋装法预测复杂技能学习
- en: Bagging and boosting are both very popular with the tree-based models that we
    studied in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-Based Methods*. There are many notable
    implementations to apply these approaches to methodologies such as CART for building
    trees.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法和提升法都非常受我们[第6章](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "第6章。支持向量机")中研究的基于树的模型的欢迎。有许多值得注意的实现将这些方法应用于构建树的方法，如CART。
- en: The `ipred` package, for example, contains an implementation to build a bagged
    predictor for trees built with `rpart()`. We can experiment with the `bagging()`
    function that this package provides. To do this, we specify the number of bagged
    trees to make using the `nbagg` parameter (default is `25`) and indicate that
    we want to compute accuracy using the OOB samples by setting the `coob` parameter
    to `TRUE`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`ipred`包包含了一个用于构建由`rpart()`构建的树的袋装预测器的实现。我们可以尝试这个包提供的`bagging()`函数。为此，我们指定要构建的袋装树的数量，使用`nbagg`参数（默认为`25`），并通过将`coob`参数设置为`TRUE`来指示我们想要使用OOB（Out-of-Bag）样本来计算准确率。
- en: 'We will do this for our complex skill learning dataset from the previous chapter,
    using the same training data frame:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一章中的复杂技能学习数据集，并使用相同的训练数据框进行此操作：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As we can see, the SSE on the test set is less than the lowest SSE that we saw
    when tuning a single tree. Increasing the number of bagged iterations, however,
    does not seem to improve this performance substantially. We will revisit this
    dataset again later.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，测试集上的SSE（均方误差）低于我们在调整单个树时看到的最低SSE。然而，增加袋装迭代次数似乎并没有显著提高这种性能。我们稍后还会再次访问这个数据集。
- en: Predicting heart disease with bagging
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用袋装法预测心脏病
- en: 'The prototypical use case for bagging is the decision tree; however, it is
    important to remember that we can use this method with a variety of different
    models. In this section, we will show how we can build a bagged logistic regression
    classifier. We built a logistic regression classifier for the Statlog Heart dataset
    in [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*. Now, we will repeat that
    experiment but use bagging in order to see if we can improve our results. To begin
    with, we''ll draw our samples with replacement and use these to train our models:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用袋装法的典型用例是决策树；然而，重要的是要记住我们可以使用这种方法与各种不同的模型。在本节中，我们将展示如何构建一个袋装逻辑回归分类器。我们在[第3章](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "第3章。线性回归")中为Statlog Heart数据集构建了一个逻辑回归分类器，*逻辑回归*。现在，我们将重复那个实验，但使用袋装法来查看我们是否可以改进我们的结果。首先，我们将用放回抽样来抽取样本，并使用这些样本来训练我们的模型：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In our code, the data frames `heart_train` and `heart_test` are referring to
    the same data frames that we prepared in [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*. We begin by deciding on
    the number of models that we will train and setting the appropriate value of `M`.
    Here, we have used an initial value of `11`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，数据框`heart_train`和`heart_test`指的是我们在[第3章](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "第3章。线性回归")中准备的数据框，*逻辑回归*。我们首先决定我们将训练的模型数量，并设置适当的`M`值。在这里，我们使用了初始值`11`。
- en: Note that it is a good idea to use an odd number of models with bagging, so
    that during the majority voting process there can never be a tie with binary classification.
    For reproducibility, we set a vector of seeds that we will use. This is simply
    a counter from an arbitrarily chosen starting seed value of `70000`. The `sample_vectors`
    matrix in our code contains a matrix where the columns are the indexes of randomly
    selected rows from the training data with replacement. Note that the rows are
    numbered 1 through 230 in the training data, making the sampling process easy
    to code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用奇数个模型进行袋装是一种好方法，因为在多数投票过程中，二元分类永远不会出现平局。为了可重复性，我们设置了一个种子向量，我们将使用它。这只是一个从任意选择的起始种子值`70000`开始的计数器。我们代码中的`sample_vectors`矩阵包含一个矩阵，其中列是随机选择的训练数据行的索引。请注意，在训练数据中，行号从1到230，这使得采样过程易于编码。
- en: 'Next, we''ll define a function that creates a single logistic regression model
    given a sampling vector of indices to use with our training data frame:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个函数，该函数根据用于训练数据框的索引采样向量创建单个逻辑回归模型：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the last line of the preceding code, we iterate through the columns of the
    `sample_vectors` matrix we produced earlier and supply them as an input to our
    logistic regression model training function, `train_1glm()`. The resulting models
    are then stored in our final list variable, `models`. This now contains 11 trained
    models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一行的代码中，我们遍历了我们之前产生的`sample_vectors`矩阵的列，并将它们作为输入提供给我们的逻辑回归模型训练函数`train_1glm()`。然后，这些模型被存储在我们的最终列表变量`models`中。现在它包含了11个训练好的模型。
- en: 'As the first method of evaluating our models, we are going to use the data
    on which each individual model was trained. To that end, we''ll construct the
    `bags` variable that is a list of these data frames, this time with unique indexes,
    as we don''t want to use any duplicate rows from the bootstrap sampling process
    in the evaluation. We''ll also add a new column called `ID` to these data frames
    that stores the original row names from the `heart_train` data frame. We''ll see
    why we do this shortly:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为评估我们模型的第一种方法，我们将使用训练每个单独模型所用的数据。为此，我们将构建一个名为`bags`的变量，它是一个包含这些数据框的列表，这次带有唯一的索引，因为我们不想在评估中使用任何来自自助抽样过程的重复行。我们还将为这些数据框添加一个名为`ID`的新列，该列存储来自`heart_train`数据框的原始行名。我们很快就会看到我们为什么要这样做：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now have a list of models and a list of data frames that they were trained
    on, the latter without duplicate observations. From these two, we can create a
    list of predictions. For each training data frame, we will tack on a new column
    called `PREDICTIONS {m}`, where `{m}` will be the number of the model being used
    to make the predictions. Consequently, the first data frame in the bags list will
    have a predictions column called `PREDICTIONS 1`. The second data frame will have
    a predictions column called `PREDICTIONS 2`, the third will have one called `PREDICTIONS
    3`, and so on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个模型列表和一个它们训练过的数据框列表，后者没有重复的观测值。从这两个列表中，我们可以创建一个预测列表。对于每个训练数据框，我们将在其中添加一个名为`PREDICTIONS
    {m}`的新列，其中`{m}`将是用于进行预测的模型的编号。因此，`bags`列表中的第一个数据框将有一个名为`PREDICTIONS 1`的预测列。第二个数据框将有一个名为`PREDICTIONS
    2`的预测列，第三个将有一个名为`PREDICTIONS 3`的预测列，依此类推。
- en: 'The following call produces a new set of data frames as just described, but
    only keeping the `PREDICTIONS{m}` and `ID` columns, and these data frames are
    stored as a list in the variable `training_predictions`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下调用产生了一组新的数据框，正如之前所描述的，但只保留`PREDICTIONS{m}`和`ID`列，并且这些数据框被存储在变量`training_predictions`中作为一个列表：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we want to merge all of these data frames onto a single data frame, where
    the rows are the rows of the original data frame (and thus, correspond to the
    observations in the dataset) and the columns are the predictions made by each
    model on the observations. Where a particular row (observation) was not selected
    by the sampling process to train a particular model, it will have an `NA` value
    in the column corresponding to the predictions that that model makes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要将这些数据框合并到一个单独的数据框中，其中行是原始数据框的行（因此，对应于数据集中的观测值），列是每个模型对观测值做出的预测。如果一个特定的行（观测值）没有被采样过程选中来训练特定的模型，它将在对应于该模型做出的预测的列中有一个`NA`值。
- en: Just to be clear, recall that each model is making predictions only on the observations
    that were used to train it and so the number of predictions that each model makes
    is smaller than the total number of observations available in our starting data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，请记住，每个模型只对其用于训练的观测值做出预测，因此每个模型做出的预测数量小于我们起始数据中可用的观测值总数。
- en: 'As we have stored the original row numbers of the `heart_train` data frame
    in the `ID` column of every data frame created in the previous step, we can merge
    using this column. We use the `Reduce()` function along with the `merge()` function
    in order to merge all the data frames in our `training_predictions` variable into
    one new data frame. Here is the code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将原始`heart_train`数据框的行号存储在每个在之前步骤创建的数据框的`ID`列中，我们可以使用这个列进行合并。我们使用`Reduce()`函数以及`merge()`函数来将我们的`training_predictions`变量中的所有数据框合并到一个新的数据框中。以下是代码：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s have a look at the first few lines and columns of this aggregated data
    frame:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个聚合数据框的前几行和列：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first column is the `ID` row that was used to merge the data frame. The
    numbers in this column are the row numbers of the observations from the starting
    training data frame. The `PREDICTIONS 1` column contains the predictions that
    the first model makes. We can see that this model had rows `1`, `2`, `5`, and
    `6` as part of its training data. For the first row, the model predicts class
    `1` and for the other three rows, it predicts class `0`. Rows `3` and `4` were
    not part of its training data and so there are two `NA` values. This reasoning
    can be used to understand the remaining columns, which correspond to the next
    three models trained.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列是用于合并数据框的`ID`行。这个列中的数字是从起始训练数据框中观测值的行号。`PREDICTIONS 1`列包含第一个模型做出的预测。我们可以看到，这个模型将行`1`、`2`、`5`和`6`作为其训练数据的一部分。对于第一行，模型预测类别`1`，而对于其他三行，它预测类别`0`。行`3`和`4`不是其训练数据的一部分，因此有两个`NA`值。这种推理可以用来理解剩余的列，这些列对应于接下来训练的三个模型。
- en: 'With this data frame constructed, we can now produce our training data predictions
    for the whole bagged model using a majority vote across each row of the preceding
    data frame. Once we have these, we merely need to match the predictions with the
    labeled values of the corresponding rows of the original `heart_train` data frame
    and compute our accuracy:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建了这个数据框之后，我们现在可以使用前一个数据框的每一行中的多数投票来生成整个袋装模型的训练数据预测。一旦我们有了这些预测，我们只需将预测与原始`heart_train`数据框相应行的标签值相匹配，并计算我们的准确度：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We now have our first accuracy measure for our bagged model--91.7 percent. This
    is analogous to measuring the accuracy on our training data. We will now repeat
    this process using the OOB observations for each model to compute the OOB accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了我们的第一个袋装模型的准确度衡量指标——91.7%。这与在训练数据上测量准确度相似。我们将现在重复这个过程，使用每个模型的OOB观测值来计算OOB准确度。
- en: There is one caveat here, however. In our data, the ECG column is a factor with
    three levels, one of which, level 1, is very rare. As a result of this, when we
    draw bootstrap samples from the original training data, we may encounter samples
    in which this factor level never appears. When that happens, the `glm()` function
    will think this factor only takes two levels, and the resulting model will be
    unable to make predictions when it encounters an observation with a value for
    the ECG factor that it has never seen before.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个需要注意的地方。在我们的数据中，ECG列是一个有三个级别的因素，其中之一，即级别1，非常罕见。因此，当我们从原始训练数据中抽取bootstrap样本时，可能会遇到这个因素级别从未出现过的样本。当这种情况发生时，`glm()`函数会认为这个因素只有两个级别，并且当它遇到一个它以前从未见过的ECG因素值的观测值时，生成的模型将无法做出预测。
- en: 'To handle this situation, we need to replace the level 1 value of this factor
    with an `NA` value for the OOB observations, if the model they correspond to did
    not have at least one observation with an ECG factor level of 1 in its training
    data. Essentially, for simplicity, we will just not attempt to make a prediction
    for these problematic observations when they arise. With this in mind, we will
    define a function to compute the OOB observations for a particular sample and
    then use this to find the OOB observations for all our samples:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种情况，我们需要将这个因素的第一级值替换为`NA`值，对于OOB观察值，如果它们对应的模型在其训练数据中没有至少一个具有ECG因素级别1的观察值。本质上，为了简单起见，我们将在这些问题观察值出现时，根本不尝试对这些观察值进行预测。考虑到这一点，我们将定义一个函数来计算特定样本的OOB观察值，然后使用这个函数来找到所有样本的OOB观察值：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will use our `glm_predictions()` function to compute predictions using
    our out-of-bag samples. The remainder of the process is identical to what we did
    earlier:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用我们的`glm_predictions()`函数来使用我们的OOB样本计算预测。其余的过程与之前所做的相同：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As expected, we see that our OOB accuracy, which is a better measure of performance
    on unseen data, is lower than the training data accuracy. In the last line of
    the previous code sample, we excluded `NA` values when computing the out-of-bag
    accuracy. This is important because it is possible that a particular observation
    may appear in all the bootstrap samples and therefore will never be available
    for an OOB prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，我们看到我们的OOB准确率，这是对未见数据性能的更好衡量，低于训练数据准确率。在前一个代码样本的最后一条线中，我们在计算OOB准确率时排除了`NA`值。这是很重要的，因为可能有一个特定的观察值出现在所有的bootstrap样本中，因此永远不会用于OOB预测。
- en: Equally, our fix for the rare level of the ECG factor means that even if an
    observation is not selected by the sampling process, we may still not be able
    to make a prediction for it. The reader should verify that only one observation
    happens to produce an `NA` value because of the combination of the two phenomena
    just described.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们对ECG因素罕见级别的修复意味着即使一个观察值没有被采样过程选中，我们可能仍然无法对其做出预测。读者应该验证只有由于上述两种现象的结合，才有一个观察值产生了`NA`值。
- en: 'Finally, we''ll repeat this process a third time using the `heart_test` data
    frame to obtain the test set accuracy:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`heart_test`数据框重复这个过程第三次，以获得测试集准确率：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The accuracy on the test set seems lower than what we found without a bagged
    model. This is not necessarily bad news for us, since the test set is very small.
    In fact, the difference between the performance of this bagged model and the original
    model trained in [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*, is 32/40 compared to 36/40,
    which is to say it is only worse by four observations in 40.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的准确率似乎低于我们没有使用Bagged模型时找到的准确率。这对我们来说不一定是坏消息，因为测试集非常小。事实上，这个Bagged模型与[第3章](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "第3章。线性回归")中训练的原始模型（逻辑回归）的性能差异是32/40比36/40，也就是说，它只比40个观察值中的四个更差。
- en: In a real-world situation, we generally want to have a much larger test set
    to estimate our unseen accuracy. In fact, because of this, we are more inclined
    to believe our OOB accuracy measurement, which is done over a larger number of
    observations and averaged over many models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们通常希望有一个更大的测试集来估计我们的未见准确率。事实上，正因为如此，我们更倾向于相信我们的OOB准确率测量，这是在更多的观察值上进行的，并且平均了多个模型。
- en: 'Bagging is actually very useful for us in this scenario as it gives us a model
    for which we can have a better estimate of the test accuracy, using the OOB observations
    because the test set is so small. As a final demonstration, we run the previous
    code a number of times with different values of *M* and store the results in a
    data frame:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Bagging实际上对我们非常有用，因为它为我们提供了一个模型，我们可以用它来更好地估计测试准确率，这是因为测试集非常小，我们可以使用OOB（Out-of-Bag）观察值。作为最后的演示，我们多次运行之前的代码，使用不同的*M*值，并将结果存储在一个数据框中：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This table shows us that the test accuracy fluctuates around 80 percent. This
    isn't that surprising given the small size of our test set of only 40 observations.
    For the training accuracy, we see that we are fluctuating around 91 percent. The
    OOB accuracy, which is far more stable as an accuracy measure, shows us that the
    expected performance of the model is around 85 percent. As the number of models
    increases, we don't see much of an improvement over 11 models, though for most
    real-world datasets, we would usually see some improvement before tapering off.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此表显示，测试准确率在80%左右波动。考虑到我们的测试集只有40个观测值，这并不令人惊讶。对于训练准确率，我们看到它在91%左右波动。作为准确度衡量标准，OOB准确率（Out-of-Bag）远更稳定，它显示模型预期的性能大约在85%左右。随着模型数量的增加，我们并没有看到超过11个模型后的显著改进，尽管对于大多数实际数据集，我们通常会在性能下降之前看到一些改进。
- en: Although our example focused exclusively on bagging for classification problems,
    the move to regression problems is relatively straightforward. Instead of using
    majority votes for a particular observation, we use the average value of the target
    function predicted by the individual models. Bagging is not always guaranteed
    to provide a performance improvement on a model. For starters, we should note
    that it makes sense to use bagging only when we have a nonlinear model. As the
    bagging process is performing an average (a linear operation) over the models
    generated, we will not see any improvements with linear regression, for example,
    because we aren't increasing the expressive power of our model. The next section
    talks about some other limitations of bagging.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的例子专注于分类问题上的袋装法，但转向回归问题相对简单。我们不是对特定观测值使用多数投票，而是使用个别模型预测的目标函数的平均值。袋装法并不总是保证在模型上提供性能改进。首先，我们应该注意，只有在我们有一个非线性模型时，使用袋装法才有意义。由于袋装过程在对生成的模型进行平均（线性操作），因此我们不会看到线性回归等线性模型有任何改进，因为我们没有增加我们模型的表达能力。下一节将讨论袋装法的其他局限性。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on bagging, consult the original paper of *Leo Breiman*
    titled *Bagging Predictors*, published in 1996 in the journal *Machine Learning*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 关于袋装法的更多信息，请参阅*Leo Breiman*于1996年在《Machine Learning》期刊上发表的原论文《Bagging Predictors》。
- en: Limitations of bagging
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋装法的局限性
- en: So far, we've only explored the upside of using bagging, but in some cases it
    may turn out not to be a good idea. Bagging involves taking the average across
    predictions made by several models, which are trained on bootstrapped samples
    of the training data. This averaging process smoothens the overall output, which
    may reduce bias when the target function is smooth. Unfortunately, if the target
    function is not smooth, we may actually introduce bias by using bagging.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只探讨了使用袋装法的优点，但在某些情况下，它可能不是一个好主意。袋装法涉及对多个模型在训练数据的bootstrap样本上训练后做出的预测进行平均。这个过程平滑了整体输出，当目标函数是平滑的时，可能会减少偏差。不幸的是，如果目标函数不是平滑的，我们实际上可能会通过使用袋装法引入偏差。
- en: Another way that bagging introduces bias is when one of the output classes is
    very rare. Under those circumstances, the majority voting system tends to be biased
    towards the more common class. Other problems may arise in relation to the sampling
    process itself. As we have already learned, when some categorical features include
    values that are rare, these may not appear at all in some of the bootstrap samples.
    When this happens, the models built for these samples will be unable to make a
    prediction when they encounter this new feature level in their test set.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法引入偏差的另一种方式是当其中一个输出类别非常罕见时。在这种情况下，多数投票系统往往会偏向于更常见的类别。与采样过程本身相关的问题也可能出现。正如我们已经学到的，当一些分类特征包含罕见值时，这些值可能根本不会出现在某些bootstrap样本中。当这种情况发生时，为这些样本构建的模型在遇到测试集中的这个新特征级别时将无法做出预测。
- en: High leverage points, which are highly influential in determining the model's
    output function compared to other points, can also be a problem. If a bootstrap
    sample is drawn that does not include one or more high leverage points, the resulting
    trained model will be quite different compared to when they are included. Therefore,
    bagging performance depends on how often these particular observations are sampled
    in order to win the majority vote. Due to this fact, our ensemble model will have
    a high variance in the presence of high leverage points. For a given dataset,
    we can often predict if we are in this situation by looking for outliers and highly
    skewed features.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 高杠杆点，与其他点相比，在确定模型输出函数时具有高度影响力，也可能成为问题。如果抽取的bootstrap样本不包括一个或多个高杠杆点，那么得到的训练模型将与包含这些点时的模型大不相同。因此，袋装法的性能取决于这些特定观察结果被采样的频率，以便赢得多数投票。由于这个事实，我们的集成模型在高杠杆点存在时将具有高方差。对于给定的数据集，我们通常可以通过寻找异常值和高度偏斜的特征来预测我们是否处于这种情况。
- en: We must also remember that the different models we build are not truly independent
    of each other in the strict sense because they still use the same set of input
    features. The averaging process would have been more effective if the models were
    independent. Also, bagging does not help when the type of model that we are using
    predicts a functional form that is very far from the true form of the target function.
    When this happens, training multiple models of this type merely reproduces the
    systematic errors across the different models. Put differently, bagging works
    better when we have low bias and high variance models as the averaging process
    is primarily designed to reduce the variance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须记住，我们构建的不同模型在严格意义上并不是真正相互独立的，因为它们仍然使用相同的输入特征集。如果模型是独立的，平均过程将更有效。此外，当使用的模型类型预测的函数形式与目标函数的真实形式相差甚远时，袋装法不起作用。当这种情况发生时，训练多个此类模型仅仅是在不同模型中复制系统误差。换句话说，当我们的模型具有低偏差和高方差时，袋装法效果更好，因为平均过程主要是设计用来减少方差的。
- en: Finally, and this applies to ensemble models in general, we tend to lose the
    explanative power of our model. We saw an example of explanative power in linear
    regression where each model parameter (regression coefficient) corresponded to
    the amount of change in the output, for a unit increase in the corresponding feature.
    Decision trees are another example of a model with high explanatory power. Using
    bagging loses this benefit because of the majority voting process and so we cannot
    directly relate our inputs to the predicted output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这适用于所有集成模型，我们往往会失去模型的可解释性。我们在线性回归中看到了可解释性的一个例子，其中每个模型参数（回归系数）对应于输出变化量，对于相应特征的单位增加。决策树是另一个具有高可解释性的模型例子。使用袋装法会失去这种好处，因为多数投票过程，所以我们不能直接将输入与预测输出联系起来。
- en: Boosting
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升法
- en: '**Boosting** offers an alternative take on the problem of how to combine models
    together to achieve greater performance. In particular, it is especially suited
    to **weak learners**. Weak learners are models that produce an accuracy that is
    better than a model that randomly guesses, but not by much. One way to create
    a weak learner is to use a model whose complexity is configurable.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升法**为如何组合模型以实现更好的性能提供了另一种思路。特别是，它非常适合**弱学习器**。弱学习器是那些产生的准确率优于随机猜测模型的模型，但提升并不大。创建弱学习器的一种方法是用一个复杂度可配置的模型。'
- en: For example, we can train a multilayer perceptron network with a very small
    number of hidden layer neurons. Similarly, we can train a decision tree, but only
    allow the tree to comprise a single node, resulting in a single split in the input
    data. This special type of decision tree is known as a **stump**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以用一个非常小的隐藏层神经元数量来训练一个多层感知器网络。同样，我们可以训练一个决策树，但只允许树包含一个节点，从而在输入数据中产生一个单一的分割。这种特殊的决策树被称为**桩**。
- en: When we looked at bagging, the key idea was to take a set of random bootstrapped
    samples of the training data and then train multiple versions of the same model
    using these different samples. In the classical boosting scenario, there is no
    random component, as all the models use all of the training data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑袋装法时，关键思想是取一组训练数据的随机重采样样本，然后使用这些不同的样本训练同一模型的多个版本。在经典的提升法场景中，没有随机成分，因为所有模型都使用了所有训练数据。
- en: For classification, boosting works by building a model on the training data
    and then measuring the classification accuracy on that training data. The individual
    observations that were misclassified by the model are given a larger weight than
    those that were correctly classified, and then the model is retrained again using
    these new weights. This is then repeated multiple times, each time adjusting the
    weights of individual observations based on whether they were correctly classified
    or not in the last iteration.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，提升法通过在训练数据上构建模型，然后在该训练数据上测量分类准确性来实现。模型误分类的个别观察值比正确分类的观察值赋予更大的权重，然后使用这些新权重重新训练模型。然后重复多次，每次根据观察值在上一次迭代中是否被正确分类来调整个别观察值的权重。
- en: To combat overfitting, the ensemble classifier is built as a weighted average
    of all the models trained in this sequence, with the weights usually being proportional
    to the classification accuracy of each individual model. As we are using the entire
    training data, there are no OOB observations, so the accuracy in each case is
    measured using the training data itself. Regression with boosting is usually done
    by adjusting the weights of observation based on some measure of the distance
    between the predicted value and the labeled value.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对抗过拟合，集成分类器被构建为在此序列中训练的所有模型的加权平均，权重通常与每个单独模型的分类精度成比例。由于我们使用的是全部训练数据，因此没有OOB（Out-of-Bag）观察值，所以每个案例的准确性使用训练数据本身来衡量。使用提升法进行回归通常是通过根据预测值和标签值之间的距离度量来调整观察值的权重。
- en: AdaBoost
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost
- en: Continuing our focus on classification problems, we now introduce **AdaBoost**,
    which is short for **adaptive boosting**. In particular, we will focus on **Discrete
    AdaBoost**, as it makes predictions on binary classes. We will use `-1` and `1`
    as the class labels. **Real AdaBoost** is an extension of AdaBoost, in which the
    outputs are the class probabilities. In our version of AdaBoost, all of the training
    data is used; however, there are other versions of AdaBoost in which the training
    data is also sampled. There are also multiclass extensions of AdaBoost as well
    as extensions that are suited to regression-type problems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 继续关注分类问题，我们现在介绍**AdaBoost**，即**自适应提升**。特别是，我们将关注**离散AdaBoost**，因为它对二进制类别进行预测。我们将使用`-1`和`1`作为类别标签。**真实AdaBoost**是AdaBoost的扩展，其输出是类别概率。在我们的AdaBoost版本中，使用了所有训练数据；然而，AdaBoost还有其他版本，其中也使用了训练数据的采样。AdaBoost还有多类扩展以及适合回归类型问题的扩展。
- en: AdaBoost for binary classification
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaBoost二分类
- en: 'The following outlines the specifics – inputs, outputs, and methods used by
    AdaBoost:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下概述了AdaBoost的具体细节——输入、输出和使用的算法：
- en: '**Inputs**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**:'
- en: '*data*: The input data frame containing the input features and a column with
    the binary output label'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*data*: 包含输入特征和具有二进制输出标签的列的输入数据框'
- en: '*M*: An integer, representing the number of models that we want to train'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M*: 一个整数，表示我们想要训练的模型数量'
- en: '**Output**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**:'
- en: '*models*: A series of *Μ* trained models'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*models*: 一系列*Μ*个已训练的模型'
- en: '*alphas*: A vector of *M* model weights'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*alphas*: 一个包含*M*个模型权重的向量'
- en: '**Method**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法**:'
- en: Initialize a vector of observation weights, *w*, of length *n* with entries
    *w[i]* *= 1/n*. This vector will be updated in every iteration.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个长度为*n*的观察权重向量*w*，其中条目*w[i]* *= 1/n*。此向量将在每次迭代中更新。
- en: Using the current value of the observation weights and all the data in the training
    set, train a classifier model *G[m]*.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前观察权重值和训练集中的所有数据，训练一个分类器模型*G[m]*。
- en: Compute the weighted error rate as the sum of all misclassified observations
    multiplied by their observation weights, divided by the sum of the weight vector.
    Following our usual convention of using *x[i]* as an observation and *y[i]* as
    its label, we can express this using the following equation:![AdaBoost for binary
    classification](img/00160.jpeg)
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算加权错误率，即将所有误分类观察值的加权总和除以权重向量的总和。按照我们通常的约定，使用*x[i]*作为观察值，*y[i]*作为其标签，我们可以用以下方程表示：![AdaBoost二分类](img/00160.jpeg)
- en: We then set the model weight for this model, *a[m]*, as the logarithm of the
    ratio between the accuracy and error rates. In a formula, this is:![AdaBoost for
    binary classification](img/00161.jpeg)
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将此模型的模型权重*a[m]*设置为准确率和错误率比值的对数。用公式表示，这是：![AdaBoost二分类](img/00161.jpeg)
- en: We then update the observation weights vector, *w*, for the next iteration.
    Incorrectly classified observations have their weight multiplied by ![AdaBoost
    for binary classification](img/00162.jpeg), thereby increasing their weight for
    the next iteration. Correctly classified observations have their weight multiplied
    by ![AdaBoost for binary classification](img/00163.jpeg), thereby reducing their
    weight for the next iteration.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随后更新下一次迭代的观察权重向量，*w*。错误分类的观察值其权重乘以![AdaBoost二分类](img/00162.jpeg)，从而增加其在下一次迭代中的权重。正确分类的观察值其权重乘以![AdaBoost二分类](img/00163.jpeg)，从而减少其在下一次迭代中的权重。
- en: Renormalize the weights vector so that the sum of the weights is 1.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置权重向量，使得权重的总和为1。
- en: Repeat steps two through six *M* times in order to produce *M* models.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤二至六 *M* 次，以生成 *M* 个模型。
- en: Define our ensemble classifier as the sign of the weighted sum of the outputs
    of all the boosted models:![AdaBoost for binary classification](img/00164.jpeg)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们的集成分类器为所有提升模型输出的加权总和的符号：![AdaBoost二分类](img/00164.jpeg)
- en: Predicting atmospheric gamma ray radiation
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测大气伽马射线辐射
- en: In order to study boosting in action, in this section we'll introduce a new
    prediction problem from the field of atmospheric physics. More specifically, we
    will analyze the patterns made by radiation on a telescope camera in order to
    predict whether a particular pattern came from gamma rays leaking into the atmosphere,
    or from regular background radiation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究提升算法的实际应用，在本节中，我们将从大气物理学领域引入一个新的预测问题。更具体地说，我们将分析辐射在望远镜相机上形成的模式，以预测特定模式是否来自伽马射线泄漏到大气中，或来自常规背景辐射。
- en: 'Gamma rays leave distinctive elliptical patterns and so we can create a set
    of features to describe these. The dataset we will use is the *MAGIC Gamma Telescope
    dataset*, hosted by the *UCI Machine Learning* repository at [http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope).
    Our data consists of 19,020 observations of the following attributes:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 伽马射线会留下独特的椭圆形图案，因此我们可以创建一组特征来描述这些图案。我们将使用的数据集是*MAGIC伽马望远镜数据集*，由*UCI机器学习*存储库托管，[http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope)。我们的数据包括19,020个以下属性的观察值：
- en: '| Column name | Type | Definition |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `FLENGTH` | Numerical | The major axis of the ellipse (mm) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `FLENGTH` | 数值 | 椭圆的主轴（mm） |'
- en: '| `FWIDTH` | Numerical | The minor axis of the ellipse (mm) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `FWIDTH` | 数值 | 椭圆的次轴（mm） |'
- en: '| `FSIZE` | Numerical | Logarithm to the base ten of the sum of the content
    of all pixels in the camera photo |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `FSIZE` | 数值 | 相机照片中所有像素内容之和的以10为底的对数 |'
- en: '| `FCONC` | Numerical | Ratio of the sum of the two highest pixels over `FSIZE`
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| `FCONC` | 数值 | 两个最高像素之和与`FSIZE`的比值 |'
- en: '| `FCONC1` | Numerical | Ratio of the highest pixel over `FSIZE` |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `FCONC1` | 数值 | 最高像素与`FSIZE`的比值 |'
- en: '| `FASYM` | Numerical | Distance from the highest pixel to the center, projected
    onto the major axis (mm) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `FASYM` | 数值 | 最高像素到中心的距离，投影到主轴上（mm） |'
- en: '| `FM3LONG` | Numerical | Third root of the third moment along the major axis
    (mm) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| `FM3LONG` | 数值 | 主轴上第三矩的立方根（mm） |'
- en: '| `FM3TRANS` | Numerical | Third root of the third moment along the minor axis
    (mm) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `FM3TRANS` | 数值 | 次轴上第三矩的立方根（mm） |'
- en: '| `FALPHA` | Numerical | Angle of the major axis with the vector to the origin
    (degrees) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `FALPHA` | 数值 | 主轴与原点向量的夹角（度） |'
- en: '| `FDIST` | Numerical | Distance from the origin to the center of the ellipse
    (mm) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `FDIST` | 数值 | 原点到椭圆中心的距离（mm） |'
- en: '| `CLASS` | Binary | Gamma rays (*g*) or Background Hadron Radiation (*b*)
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `CLASS` | 二进制 | 伽马射线 (*g*) 或背景质子辐射 (*b*) |'
- en: 'First, we will load the data into a data frame called `magic`, recoding the
    `CLASS` output variable to use classes `1` and `-1` for gamma rays and background
    radiation, respectively:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据加载到名为`magic`的数据框中，将`CLASS`输出变量重新编码为使用类`1`和`-1`分别代表伽马射线和背景辐射：
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we''ll split our data frame into a training data and a test data frame
    using our typical 80-20 split:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据框拆分为训练数据框和测试数据框，使用典型的80-20拆分：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model that we are going to use with boosting is a simple multilayer perceptron
    with a single hidden layer. Harkening back to [Chapter 4](part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 4. Generalized Linear Models"), *Neural Networks*, we know that the `nnet`
    package is perfect for this task. With neural networks, we often get superior
    accuracy when we normalize the inputs and so before training any models, we will
    carry out this preprocessing step:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要用于提升的模型是一个简单的单隐藏层多层感知器。回顾 [第 4 章](part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7
    "第 4 章。广义线性模型")，*神经网络*，我们知道 `nnet` 包非常适合这项任务。在使用神经网络时，我们通常在归一化输入后获得更高的准确率，因此在我们训练任何模型之前，我们将执行此预处理步骤：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Boosting is designed to work best with weak learners, and for this reason,
    we are going to use a very small number of hidden neurons in our hidden layer.
    Concretely, we will begin with the simplest possible multilayer perceptron that
    uses a single hidden neuron. To understand the effect of using boosting, we will
    establish our baseline performance by training a single neural network and measuring
    its accuracy. We can do this as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 提升旨在与弱学习器配合使用效果最佳，因此我们在隐藏层中将使用非常少的隐藏神经元。具体来说，我们将从使用单个隐藏神经元的可能的最简单多层感知器开始。为了了解使用提升的效果，我们将通过训练单个神经网络并测量其准确率来建立基线性能。我们可以这样做：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This establishes that we have a baseline accuracy of around 79.5 percent. This
    isn''t too bad, but we are going to use boosting to see if we can improve it.
    To that end, we are going to write our own function, `AdaBoostNN()`, which will
    take as input a data frame, the name of the output variable, the number of single
    hidden layer neural network models we want to build, and finally the number of
    hidden units these neural networks will have. This function will then implement
    the AdaBoost algorithm that we previously described and finally return a list
    of models and their weights. Here is the function:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的基线准确率约为 79.5%。这并不太糟糕，但我们将使用提升来查看是否可以改进它。为此，我们将编写自己的函数 `AdaBoostNN()`，该函数将接受一个数据框、输出变量的名称、我们想要构建的单隐藏层神经网络模型的数量，以及最后这些神经网络将拥有的隐藏单元数。然后该函数将实现我们之前描述的
    AdaBoost 算法，并最终返回模型及其权重的列表。以下是该函数：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Before proceeding, we will work our way through the function to understand what
    each line is doing. We first initialize empty lists of models and model weights
    (`alphas`). We also compute the number of observations in our training data, storing
    this in the variable `n`. The name of the output column provided is then used
    to create a formula that describes the neural network that we will build.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将逐步分析函数以了解每一行的作用。我们首先初始化空的模型和模型权重列表（`alphas`）。我们还计算训练数据中的观测数，并将此存储在变量
    `n` 中。然后使用提供的输出列名称创建一个公式，描述我们将构建的神经网络。
- en: In our dataset, this formula will be `CLASS ~ .`, which means that the neural
    network will compute `CLASS` as a function of all the other columns as input features.
    We then initialize our weights vector, as we did in *step 1* of AdaBoost, and
    define our loop that will run for *M* iterations in order to build *M* models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，这个公式将是 `CLASS ~ .`，这意味着神经网络将计算 `CLASS` 作为所有其他列作为输入特征的函数。然后我们初始化我们的权重向量，就像在
    AdaBoost 的 *步骤 1* 中做的那样，并定义我们的循环，该循环将运行 *M* 次迭代以构建 *M* 个模型。
- en: In every iteration, the first step is to use the current setting of the weights
    vector to train a neural network using as many hidden units as specified in the
    input, `hidden_units`. We then compute a vector of predictions that this model
    generates on the training data using the `predict()` function. By comparing these
    predictions to the output column of the training data, we calculate the errors
    that the current model makes on the training data. This then allows us to compute
    the error rate. According to *step 4* of the AdaBoost algorithm, this error rate
    is set as the weight of the current model. Finally, the observation weights to
    be used in the next iteration of the loop are updated according to whether each
    observation was correctly classified using *step 5* of the AdaBoost algorithm.
    The weight vector is then normalized and we are ready to begin the next iteration.
    After completing *M* iterations, we output a list of models and their corresponding
    model weights.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，第一步是使用当前权重向量的设置，使用输入中指定的隐藏单元数量训练一个神经网络。然后我们使用`predict()`函数计算模型在训练数据上生成的预测向量。通过将这些预测与训练数据的输出列进行比较，我们计算出当前模型在训练数据上的错误。这然后允许我们计算错误率。根据AdaBoost算法的第4步，这个错误率被设置为当前模型的权重。最后，根据AdaBoost算法的第5步，根据每个观察是否被正确分类来更新下一次迭代循环中使用的观察权重。然后权重向量被归一化，我们就可以开始下一次迭代。完成*M*次迭代后，我们输出一个包含模型及其相应模型权重的列表。
- en: 'We now have a function that is able to train our ensemble classifier using
    AdaBoost, but we also need a function to make predictions. This function will
    take in the output list produced by our training function, `AdaBoostNN()`, along
    with a test dataset. We''ve called this function `AdaBoostNN.predict()` and it
    is shown here:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个函数能够使用AdaBoost训练我们的集成分类器，但我们还需要一个函数来进行预测。这个函数将接收我们的训练函数`AdaBoostNN()`生成的输出列表以及一个测试数据集。我们把这个函数命名为`AdaBoostNN.predict()`，如下所示：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this function, we first extract the models and the model weights from the
    list produced by our previous function. We then create a matrix of predictions,
    where each column corresponds to the vector of predictions made by a particular
    model. Thus, we will have as many columns in this matrix as models that we used
    for boosting.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们首先从之前函数生成的列表中提取模型和模型权重。然后我们创建一个预测矩阵，其中每一列对应于特定模型做出的预测向量。因此，在这个矩阵中，我们将有与用于提升的模型数量一样多的列。
- en: We then multiply the predictions produced by each model with their corresponding
    model weight. For example, every prediction from the first model is in the first
    column of the prediction matrix and will have its value multiplied by the first
    model weight *α1*. Finally, in the last step, we reduce our matrix of weighted
    observations into a single vector of observations by summing the weighted predictions
    for each observation and taking the sign of the result. This vector of predictions
    is then returned by our function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将每个模型生成的预测与相应的模型权重相乘。例如，第一个模型的每个预测都在预测矩阵的第一列，并且其值将乘以第一个模型权重*α1*。最后，在最后一步中，我们将加权观察值的矩阵简化为单个观察值向量，通过将每个观察值的加权预测相加并取结果的符号来实现。然后，这个预测向量由我们的函数返回。
- en: 'As an experiment, we will train 10 neural network models with a single hidden
    unit and see if boosting improves accuracy:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作为实验，我们将使用单个隐藏单元训练10个神经网络模型，看看提升是否能够提高准确率：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Boosting 10 models seems to give us a marginal improvement in accuracy, but
    perhaps training more models might make more of a difference. We are also interested
    in the relationship between the complexity of our weak learner, as measured by
    the number of hidden neurons, and the performance benefits we can expect from
    boosting on this dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 提升使用10个模型似乎在准确率上只带来轻微的提高，但也许训练更多的模型可能会带来更大的差异。我们还对弱学习器的复杂性（通过隐藏神经元的数量来衡量）与在数据集上提升可以预期的性能收益之间的关系感兴趣。
- en: 'The following plot shows the results of experimenting with our functions using
    different numbers of models as well as hidden neurons:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了使用不同数量的模型以及隐藏神经元进行实验的结果：
- en: '![Predicting atmospheric gamma ray radiation](img/00165.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![预测大气伽马射线辐射](img/00165.jpeg)'
- en: For the neural networks with one hidden unit, as the number of boosting models
    increases, we see an improvement in accuracy, but after 100 models, this tapers
    off and is actually slightly less for 200 models. The improvement over the baseline
    of a single model is substantial for these networks. When we increase the complexity
    of our learner by having a hidden layer with three hidden neurons, we get a much
    smaller improvement in performance. At 200 models, both ensembles perform at a
    similar level, indicating that at this point, our accuracy is being limited by
    the type of model we are training.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只有一个隐藏单元的神经网络，随着提升模型的数量增加，我们看到了准确性的提高，但达到100个模型后，这种提高逐渐减弱，对于200个模型来说实际上略有下降。对于这些网络，与单个模型基线相比，改进是显著的。当我们通过具有三个隐藏神经元的隐藏层增加学习者的复杂性时，我们在性能上的改进要小得多。在200个模型时，这两个集成表现水平相似，这表明在这个点上，我们的准确性受到我们正在训练的模型类型的限制。
- en: Note
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The original AdaBoost algorithm was presented by *Freund* and *Schapire* in
    the journal of *Computer and System Sciences* in a 1997 paper titled *A Decision-theoretic
    generalization of on-line learning and an application to boosting*. This is a
    good place to start learning more about AdaBoost.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的AdaBoost算法由*Freund*和*Schapire*在1997年发表在《Computer and System Sciences》期刊上的论文《A
    Decision-theoretic generalization of on-line learning and an application to boosting》中提出。这是学习AdaBoost的一个好起点。
- en: Predicting complex skill learning with boosting
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用提升预测复杂技能学习
- en: We will revisit our Skillcraft dataset in this section--this time in the context
    of another boosting technique known as **stochastic gradient boosting**. The main
    characteristic of this method is that in every iteration of boosting, we compute
    a gradient in the direction of the errors that are made by the model trained in
    the current iteration.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新审视我们的Skillcraft数据集--这次是在另一种称为**随机梯度提升**的增强技术背景下。这种方法的主要特点是，在提升的每一轮迭代中，我们计算模型在当前迭代中犯的错误方向上的梯度。
- en: 'This gradient is then used in order to guide the construction of the model
    that will be added in the next iteration. Stochastic gradient boosting is commonly
    used with decision trees, and a good implementation in R can be found in the `gbm`
    package, which provides us with the `gbm()` function. For regression problems,
    we need to specify the `distribution` parameter to be `gaussian`. In addition,
    we can specify the number of trees we want to build (which is equivalent to the
    number of iterations of boosting) via the `n.trees` parameter, as well as a `shrinkage`
    parameter that is used to control the algorithm''s learning rate:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个梯度随后被用来指导在下一轮迭代中添加的模型构建。随机梯度提升通常与决策树一起使用，在R语言中，`gbm`包提供了一个良好的实现，它提供了`gbm()`函数。对于回归问题，我们需要指定`distribution`参数为`gaussian`。此外，我们可以通过`n.trees`参数指定我们想要构建的树的数量（这相当于提升的迭代次数），以及一个用于控制算法学习率的`shrinkage`参数：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more about how stochastic gradient boosting works, a good source to
    consult is the paper titled *Stochastic Gradient Boosting*. This was written by
    *Jerome H. Friedman* and appears in the February 2002 issue of the journal *Computational
    Statistics & Data Analysis*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于随机梯度提升的工作原理，一个很好的参考资料是标题为《随机梯度提升》的论文。这篇论文由*Jerome H. Friedman*撰写，发表在2002年2月的期刊《Computational
    Statistics & Data Analysis》上。
- en: 'In order to make predictions with this setup, we need to use the `gbm.perf()`
    function, whose job it is to take the boosted model we built and pick out the
    optimal number of boosting iterations. We can then provide this to our `predict()`
    function in order to make predictions on our test data. To measure the SSE on
    our test set, we will use the `compute_SSE()` function that we wrote in [Chapter
    6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7 "Chapter 6. Support
    Vector Machines"), *Tree-based Methods*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这种设置进行预测，我们需要使用`gbm.perf()`函数，其任务是取我们构建的提升模型并挑选出最佳的提升迭代次数。然后我们可以将这个结果提供给我们的`predict()`函数，以便对我们的测试数据进行预测。为了测量测试集中的SSE，我们将使用我们在[第6章](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "第6章。基于树的方法")中编写的`compute_SSE()`函数：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: A bit of experimentation has revealed that we can't get substantially better
    results than this by allowing the algorithm to iterate over more trees. Despite
    this, we are already performing better using this method than both the single
    and bagged tree classifiers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许算法在更多的树上迭代，我们发现无法显著提高结果。尽管如此，我们使用这种方法已经比单树和袋装树分类器表现得更好。
- en: Limitations of boosting
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升法的局限性
- en: Boosting is a very powerful technique that continues to receive a lot of attention
    and research, but it is not without its limitations. Boosting relies on combining
    weak learners together. In particular, we can expect to get the most out of boosting
    when the models that are used are not already complex models themselves. We already
    saw an example of this with neural networks, by noting that the more complex architecture
    of three hidden neurons gives a better learner to begin with than the simpler
    architecture of a single hidden neuron.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法是一种非常强大的技术，它持续受到很多关注和研究，但它并非没有局限性。提升法依赖于将弱学习器结合起来。特别是，当使用的模型本身不是复杂模型时，我们可以期待从提升法中获得最大的收益。我们已经通过注意到具有三个隐藏神经元的更复杂架构在开始时比具有单个隐藏神经元的简单架构提供更好的学习器，看到了这种效果的例子。
- en: Combining weak learners may be a way to reduce overfitting, but this is not
    always effective. By default, boosting uses all of its training data and progressively
    tries to correct mistakes that it makes without any penalizing or shrinkage criterion
    (although the individual models trained may themselves be regularized). Consequently,
    boosting can sometimes overfit.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将弱学习器结合起来可能是一种减少过拟合的方法，但这并不总是有效的。默认情况下，提升法使用其所有的训练数据，并逐步尝试纠正它所犯的错误，而不进行任何惩罚或收缩标准（尽管训练的个别模型本身可能已经进行了正则化）。因此，提升法有时可能会过拟合。
- en: Finally, a very important limitation is that many boosting algorithms have a
    symmetric loss function. Specifically, there is no distinction that is made in
    classification between a false positive classification error and a false negative
    classification error. Every type of error is treated the same when the observation
    weights are updated.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个非常重要的局限性是许多提升算法具有对称损失函数。具体来说，在分类中，对错误正分类和错误负分类没有进行区分。当更新观察权重时，所有类型的错误都被同等对待。
- en: In practice, this might not be desirable, in that one of the two errors may
    be more costly. For example, on the website for our **Major Atmospheric Gamma
    Imaging Cherenkov** (**MAGIC**) telescope dataset, the authors state that a false
    positive of detecting gamma rays where there are none, is worse than a false negative
    of misclassifying gamma rays as background radiation. Cost-sensitive extensions
    of boosting algorithms have been proposed, however.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这可能不是所希望的，因为两种错误中的一种可能代价更高。例如，在我们的**主要大气伽马成像切伦科夫**（**MAGIC**）望远镜数据集网站上，作者表示，在没有任何伽马射线的情况下检测到伽马射线是比将伽马射线错误分类为背景辐射的错误更糟糕的。然而，已经提出了提升算法的成本敏感扩展。
- en: Random forests
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: The final ensemble model that we will discuss in this chapter is unique to tree-based
    models and is known as the **random forest**. In a nutshell, the idea behind random
    forests stems from an observation on bagging trees. Let's suppose that the actual
    relationship between the features and the target variable can be adequately described
    with a tree structure. It is quite likely that during bagging with moderately
    sized bootstrapped samples, we will keep picking the same features to split on
    high up in the tree.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将要讨论的最终集成模型是树模型特有的，被称为**随机森林**。简而言之，随机森林背后的想法源于对袋装树的观察。假设特征与目标变量之间的实际关系可以用树结构充分描述。在用适度大小的自助样本进行袋装时，我们很可能会在树的高层不断选择相同的特征进行分割。
- en: For example, in our Skillcraft dataset, we expect to see `APM` as the feature
    that will be chosen at the top of most of the bagged trees. This is a form of
    tree correlation that essentially impedes our ability to derive the variance reduction
    benefits from bagging. Put differently, the different tree models that we build
    are not truly independent of each other because they will have many features and
    split points in common. Consequently, the averaging process at the end will be
    less successful in reducing the ensemble variance.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的Skillcraft数据集中，我们预计在大多数袋装树的最顶层会选择`APM`作为特征。这是一种树相关形式，本质上阻碍了我们从袋装中获得方差减少的好处。换句话说，我们构建的不同树模型并不是真正相互独立的，因为它们将有许多共同的特征和分割点。因此，最终的平均过程在减少集成方差方面将不太成功。
- en: To counteract this effect, the random forest algorithm introduces an element
    of randomization in the tree construction process. Just as with bagging, random
    forests involve building a number of trees with bootstrapped samples and using
    the average of their predictions to form the ensemble prediction. When we construct
    individual trees, however, the random forest algorithm imposes a constraint.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抵消这种影响，随机森林算法在树构建过程中引入了随机化元素。就像在袋装法中一样，随机森林涉及构建多个使用自助样本的树，并使用它们预测的平均值来形成集成预测。然而，当我们构建单个树时，随机森林算法施加了一个约束。
- en: At each node in the tree, we draw a random sample of size *mtry* from the total
    number of input features. Whereas in regular tree construction, we consider all
    the features at each node to determine which one to split on, with random forests,
    we only consider features from the sample we created for that node. We can often
    use a relatively small number for *mtry*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在树的每个节点上，我们从所有输入特征中随机抽取大小为 *mtry* 的样本。而在常规的树构建过程中，我们在每个节点考虑所有特征以确定要分割的特征，而在随机森林中，我们只考虑为该节点创建的样本中的特征。我们通常可以使用一个相对较小的
    *mtry* 值。
- en: The number of trees we build, in combination with the fact that each tree has
    several nodes, is often enough to ensure that the more important features are
    sampled a sufficient number of times. Various heuristics have been proposed for
    choosing appropriate values for this parameter, such as one third or the square
    root of the total number of features available.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的树的数量，加上每棵树有几个节点的事实，通常足以确保重要的特征被足够多次地采样。已经提出了各种启发式方法来选择此参数的适当值，例如可用特征总数的1/3或平方根。
- en: This sampling step effectively forces the structure of the bagged trees to be
    different from each other and offers a number of different benefits. Feature sampling
    allows us to consider input features that are successful in splitting the data
    for only a small range of the target variable. These locally relevant features
    are rarely chosen without the sampling constraint because we usually prefer features
    that form good overall splits of the data at a given node in the tree. Nonetheless,
    we may want to include these features in our model if we don't want to overlook
    local variations in the output.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个采样步骤有效地迫使袋装树的结构彼此不同，并提供了许多不同的好处。特征采样使我们能够考虑在目标变量的小范围内成功分割数据的输入特征。这些局部相关特征在没有采样约束的情况下很少被选中，因为我们通常更喜欢在树中的给定节点处形成良好的整体分割的特征。尽管如此，如果我们不希望忽略输出中的局部变化，我们可能仍然希望将这些特征包含在我们的模型中。
- en: Similarly, sampling input features is useful when we have correlated input features.
    Regular tree construction tends to favor only one of the features from a correlated
    set while ignoring the rest despite the fact that the resulting splits from even
    highly correlated features are not exactly the same. When we sample input features
    we are less likely to have correlated features compete with each other and so
    we can choose a wider range of features to use with our model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当我们的输入特征相关时，采样输入特征是有用的。常规的树构建往往只偏向于相关集合中的一个特征，而忽略其余的特征，尽管即使是非常相关的特征产生的分割结果也不完全相同。当我们采样输入特征时，我们不太可能让相关的特征相互竞争，因此我们可以选择更广泛的特征范围来与我们的模型一起使用。
- en: In general, the randomized nature of the sampling process is designed to combat
    overfitting because we can think of this process as applying regularization on
    the impact of each input feature. Overfitting can still be a problem if we happen
    to have too many input features unrelated to the target variable compared to those
    that are related, but this is a fairly rare scenario. Random forests in general
    scale quite favorably with the number of input features, precisely because of
    this sampling process that doesn't require us to consider all the features when
    splitting at each node. In particular, this model is a good choice when the number
    of features exceeds the number of observations. Finally, the sampling process
    mitigates the cost of constructing a large number of trees again because we consider
    a subset of input features when deciding on how to split at each node. The number
    of trees is another tuning parameter that we must decide on in a random forest
    model; it is very common to build anywhere between several hundred and a few thousand
    trees.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，采样过程的随机性被设计用来对抗过拟合，因为我们可以把这个过程看作是对每个输入特征影响进行正则化的应用。如果我们碰巧有太多与目标变量无关的输入特征，与相关的特征相比，过拟合仍然可能成为一个问题，但这种情况相当罕见。随机森林在一般情况下与输入特征的数量有很好的扩展性，这正是由于这种采样过程，它不需要我们在每个节点分裂时考虑所有特征。特别是，当特征数量超过观测数量时，这个模型是一个很好的选择。最后，采样过程减轻了构建大量树的成本，因为我们决定在每个节点如何分裂时只考虑输入特征的一个子集。树的数量是随机森林模型中我们必须决定的另一个调整参数；通常情况下，我们会构建几百到几千棵树。
- en: 'In R, we can use the `randomForest` package in order to train random forest
    models. The `randomForest()` function takes in a formula and a training data frame,
    as well as a number of other optional parameters. Of particular interest is the
    `ntree` parameter, which controls the number of trees that will be built for the
    ensemble, and the `mtry` parameter, which is the number of features sampled for
    use at each node for splitting. These parameters should be tuned by trying out
    different configurations, and we can use the `tune()` function from the `e1071`
    package to do just that:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中，我们可以使用`randomForest`包来训练随机森林模型。`randomForest()`函数接受一个公式和一个训练数据框，以及一些其他可选参数。特别值得注意的是`ntree`参数，它控制将构建多少棵树用于集成，以及`mtry`参数，它是每个节点分裂时用于分裂的特征样本数量。这些参数应该通过尝试不同的配置来调整，我们可以使用`e1071`包中的`tune()`函数来完成这项工作：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The results show that on this dataset, the best parameter combination is to
    train 1,000 trees and use a value of 6 for `mtry`. This last value corresponds
    to one third of the number of input features, which is the typical heuristic for
    regression problems. The SSE value on our test set is almost identical to what
    we obtained using gradient boosting.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，在这个数据集上，最佳的参数组合是训练1,000棵树并使用`mtry`的值为6。这个最后的值对应于输入特征数量的三分之一，这是回归问题的典型启发式方法。我们的测试集上的SSE值几乎与使用梯度提升法获得的结果相同。
- en: The importance of variables in random forests
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林中变量的重要性
- en: We already discussed the fact that ensemble models do not, in general, have
    explanative power. For random forests, it turns out that we can still measure
    variable importance scores for the different input features by tallying and keeping
    track of the reductions in our error function across all the trees in the ensemble.
    In this way, we can obtain an analogous plot to the one we obtained for a single
    tree when we looked at this dataset in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-based Methods*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了集成模型通常没有解释能力的事实。对于随机森林，我们发现我们仍然可以通过对所有集成中的树进行错误函数减少的计数和跟踪来测量不同输入特征的变量重要性得分。通过这种方式，我们可以获得与我们在[第6章](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "第6章。支持向量机")“基于树的算法”中查看此数据集时获得的单个树的类似图。
- en: 'To compute variable importance, we use the `importance()` function and plot
    the results:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算变量重要性，我们使用`importance()`函数并绘制结果：
- en: '![The importance of variables in random forests](img/00166.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林中变量的重要性](img/00166.jpeg)'
- en: Looking at this plot, we can see that `APM` and `ActionLatency` are once again
    the most important features, but their order is reversed. We also see that `TotalHours`
    is now third in importance, significantly higher than what we saw before in a
    single tree.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个图表，我们可以看到`APM`和`ActionLatency`再次是最重要的特征，但它们的顺序颠倒了。我们还看到`TotalHours`现在在重要性上排名第三，比之前在单个树中看到的要高得多。
- en: We have explored the Skillcraft dataset using a number of different methods,
    but each time we treated this as a regression problem and measured our accuracy
    using the SSE. Our target variable is the league index, which tells us the gaming
    league in which a player competes. As such, it is actually an ordered factor.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用多种不同的方法探索了Skillcraft数据集，但每次我们都将其视为回归问题，并使用SSE来衡量我们的准确度。我们的目标变量是联赛指数，它告诉我们玩家在哪个游戏联赛中竞争。因此，它实际上是一个有序因子。
- en: As we've already seen before, models whose output is an ordered factor can be
    tricky to train as well as assess. For example, perhaps a more appropriate method
    of assessing our model would be to first round our model's numerical output so
    that we obtain a prediction on an actual player league. Then, we could assess
    the model using a weighted classification error rate that more heavily penalizes
    a predicted league index that is very far from the actual league index. We leave
    this as an exercise for the reader.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所见，输出为有序因子的模型在训练和评估时可能很棘手。例如，评估我们模型的一个更合适的方法可能是首先将我们模型的数值输出四舍五入，以便我们获得一个关于实际玩家联赛的预测。然后，我们可以使用加权分类误差率来评估模型，该误差率对预测的联赛指数与实际联赛指数差异很大的情况给予更重的惩罚。我们将此作为读者的练习。
- en: One of the issues that we often face when we model the problem as a regression
    problem is that we have no way to force the output to predict across the full
    range of the original levels. In our particular dataset, for example, we might
    never predict the lowest or highest league. For some suggestions on alternative
    ways to model ordered factors with regression trees, there is an insightful paper
    published in 2000 by *Kramer* and others, titled *Prediction of Ordinal Classes
    Using Regression Trees*. This appears in the *34th* issue of *Fundamentals Informaticae*
    by *IOS Press*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将问题建模为回归问题时，我们经常面临的一个问题是无法强迫输出预测覆盖原始级别的完整范围。例如，在我们的特定数据集中，我们可能永远不会预测最低或最高的联赛。关于使用回归树以其他方式建模有序因子的建议，有一篇由Kramer和其他人于2000年发表的见解深刻的论文，标题为*使用回归树预测有序类别*。这篇论文发表在*IOS
    Press*出版的*Fundamentals Informaticae*的*第34期*。
- en: Note
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For random forests, the original reference is a 2001 paper by *Leo Breiman*,
    titled *Random Forests*, published in the journal *Machine Learning*. Besides
    this reference, a fantastic chapter with numerous examples appears in the book
    *Statistical Learning from a Regression Perspective*, *Richard A. Derk*, *Springer*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机森林，原始参考文献是2001年由Leo Breiman发表在*Machine Learning*期刊上的论文*Random Forests*。除了这个参考文献之外，书中还有一个精彩的章节，包含了许多示例，标题为*从回归视角的统计学习*，作者是Richard
    A. Derk，出版社是Springer。
- en: XGBoost
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: Along the same vein as AdaBoost, and with yet another twist on gradient boosting,
    is **Extreme Gradient Boosting** (**XGBoost**). XGBoost is a library of functions
    designed and optimized specifically for boosting trees algorithms. It is a generally
    advanced tool kit that yields impressive results, but does takes some time to
    understand.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与AdaBoost类似，并在梯度提升上又有新的变化，是**极端梯度提升**（**XGBoost**）。XGBoost是一个专门设计和优化的函数库，用于提升树算法。它是一个通常高级的工具包，能产生令人印象深刻的成果，但理解它需要一些时间。
- en: XGBoost is based upon the quite well known gradient boosting framework, but
    it is more efficient. Specifically, XGBoost leverages system optimization concepts
    such as out of core computations, parallelization, cache optimization, and distributed
    computing to create a faster and more flexible tool for learning tree ensembles.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost基于广为人知的梯度提升框架，但效率更高。具体来说，XGBoost利用了系统优化概念，如离核计算、并行化、缓存优化和分布式计算，以创建一个更快、更灵活的学习树集成工具。
- en: Note
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Basically, XGBoost is built to perform the sequential process of *boosting*
    by utilizing all cores of a machine as it recursively divides data into parts,
    retaining the first part to be used as the test data, then reintegrating the first
    part to the dataset and retaining the second part, do a training and repeat, and
    so on.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，XGBoost是通过利用机器的所有核心，在递归地将数据划分为部分的同时执行序列化过程*提升*而构建的，保留第一部分作为测试数据，然后将第一部分重新整合到数据集中，保留第二部分，进行训练并重复，依此类推。
- en: In addition, XGBoost has many parameters that can be customized and is extendable
    and therefore is much more flexible. Other advantages offered by XGBoost include
    regularization, customizable parameters, deeper tree pruning, and built-in cross
    validation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，XGBoost 具有许多可定制的参数，且可扩展，因此具有更大的灵活性。XGBoost 提供的其他优势包括正则化、可定制的参数、更深入的树剪枝和内置交叉验证。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we deviated from our usual pattern of learning a new type of
    model and instead focused on techniques to build ensembles of models that we have
    seen before. We discovered that there are numerous ways to combine models in a
    meaningful way, each with its own advantages and limitations. Our first technique
    for building ensemble models was bagging. The central idea behind bagging is that
    we build multiple versions of the same model using bootstrap samples of the training
    data. We then average the predictions made by these models in order to construct
    our overall prediction. By building many different versions of the model we can
    smooth out errors made due to overfitting and end up with a model that has reduced
    variance.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们偏离了我们通常的学习新模型的方式，而是专注于构建之前见过的模型集成技术。我们发现，有无数种有意义地组合模型的方法，每种方法都有其自身的优势和局限性。我们构建集成模型的第一种技术是
    bagging。bagging 的核心思想是，我们使用训练数据的自助样本构建多个相同模型的版本。然后，我们平均这些模型做出的预测，以构建我们的总体预测。通过构建多个不同版本的模型，我们可以平滑掉由于过拟合而产生的错误，并最终得到一个方差较小的模型。
- en: A different approach to building model ensembles uses all of the training data
    and is known as boosting. Here, the defining characteristic is to train a sequence
    of models, but each time we weigh each observation with a different weight depending
    on whether we classified that observation correctly in the previous model. There
    are many variants of boosting and we presented two of the most well-known algorithms,
    AdaBoost and stochastic gradient boosting (as well as mentioning perhaps the newer,
    more efficient tree learner, XGBoost). The averaging process that operates over
    the predictions made by individual models to compute the final prediction often
    weighs each model by its performance.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型集成的另一种方法使用所有训练数据，称为 boosting。在这里，定义特征是训练一系列模型，但每次我们根据我们在前一个模型中是否正确分类了该观察结果，以不同的权重来权衡每个观察结果。boosting
    有许多变体，我们介绍了两种最著名的算法，即 AdaBoost 和随机梯度提升（以及提到可能更新、更高效的树学习器 XGBoost）。在计算最终预测时，对个别模型做出的预测进行平均的过程通常根据每个模型的表现来权衡。
- en: Traditional texts that present bagging and boosting introduce them in the context
    of decision trees. There is good reason for this, as the decision tree is the
    prototypical model for which bagging and boosting have been applied. Boosting
    in particular works best on models that are weak learners and decision trees can
    easily be made into weak learners by significantly restricting their size and
    complexity during construction.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍 bagging 和 boosting 的传统文本通常在决策树的环境中介绍它们。这有很好的理由，因为决策树是 bagging 和 boosting
    应用的典型模型。特别是，boosting 在弱学习器模型上效果最好，通过在构建过程中显著限制其大小和复杂性，决策树可以很容易地变成弱学习器。
- en: At the same time, however, this often leaves the reader with a view that ensemble
    methods only work for decision trees, or without any experience in how they can
    be applied to other methods. In this chapter, we emphasized how these techniques
    are general and how they can be used with a number of different types of models.
    Consequently, we applied these techniques to models that we have seen before,
    such as neural networks and logistic regression.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这往往让读者产生一种观点，即集成方法仅适用于决策树，或者没有经验了解它们如何应用于其他方法。在本章中，我们强调了这些技术的通用性以及它们可以与多种不同类型的模型一起使用。因此，我们将这些技术应用于我们之前见过的模型，例如神经网络和逻辑回归。
- en: The final type of ensemble model that we studied was the random forest. This
    is a very popular and powerful algorithm based on bagging decision trees. The
    key breakthrough behind this model is the use of an input feature sampling procedure,
    which limits the choice of features that are available to split on during the
    construction of each tree. In doing this, the model reduces the correlation between
    trees, captures significant localized variations in the output, and improves the
    degree of variance reduction in the final result. Another key benefit of this
    model is that it scales well with a larger number of input features. For our real-world
    Skillcraft dataset, we discovered that random forests and stochastic gradient
    boosting produced the best performance.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的最后一种集成模型是随机森林。这是一个基于袋装决策树的非常流行且强大的算法。这个模型背后的关键突破是使用输入特征采样程序，这限制了在构建每棵树时可用于分裂的特征选择。通过这样做，模型减少了树之间的相关性，捕捉了输出中的显著局部变化，并提高了最终结果的方差减少程度。这个模型的另一个关键好处是它能够很好地扩展到更多的输入特征。对于我们的实际Skillcraft数据集，我们发现随机森林和随机梯度提升产生了最佳性能。
- en: In the next chapter, we will introduce another type of model with a distinct
    structure known as the probabilistic graphical model. These models use a graphical
    structure in order to explicitly represent the conditional independence between
    input features. Probabilistic graphical models find applications across a wide
    variety of predictive tasks from spam email identification to DNA sequence labeling.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种具有独特结构的模型，称为概率图模型。这些模型使用图形结构来明确表示输入特征之间的条件独立性。概率图模型在各种预测任务中都有应用，从垃圾邮件识别到DNA序列标记。
