- en: Boosting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升
- en: The second generative method we will discuss is boosting. Boosting aims to combine
    a number of weak learners into a strong ensemble. It is able to reduce bias, but
    also variance. Here, weak learners are individual models that perform slightly
    better than random. For example, in a classification dataset with two classes
    and an equal number of instances belonging to each class, a weak learner will
    be able to classify the dataset with an accuracy of slightly more than 50%.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第二种生成方法是提升。提升旨在将多个弱学习器结合成一个强大的集成。它能够减少偏差，同时也能降低方差。在这里，弱学习器是指那些表现略好于随机预测的独立模型。例如，在一个包含两个类别且每个类别的实例数量相等的分类数据集上，弱学习器的准确率会稍微高于
    50%。
- en: In this chapter, we will present two classic boosting algorithms, Gradient Boosting
    and AdaBoost. Furthermore, we will explore the use of scikit-learn implementations
    for classification and regression. Finally, we will experiment with a recent boosting
    algorithm and its implementation, XGBoost.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍两种经典的提升算法：梯度提升（Gradient Boosting）和 AdaBoost。此外，我们还将探讨使用 scikit-learn
    实现进行分类和回归。最后，我们将实验一种近期的提升算法及其实现——XGBoost。
- en: 'The main topics covered are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主要主题如下：
- en: The motivation behind using boosting ensembles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提升集成的动机
- en: The various algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种算法
- en: Leveraging scikit-learn to create boosting ensembles in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 scikit-learn 在 Python 中创建提升集成
- en: Utilizing the XGBoost library for Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 XGBoost 库进行 Python 编程
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解 Python 语法和约定。最后，熟悉 NumPy 库将有助于读者理解一些自定义算法实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter06)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter06)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2ShWstT](http://bit.ly/2ShWstT).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以观看代码演示：[http://bit.ly/2ShWstT](http://bit.ly/2ShWstT)。
- en: AdaBoost
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost
- en: AdaBoost is one of the most popular boosting algorithms. Similar to bagging,
    the main idea behind the algorithm is to create a number of uncorrelated weak
    learners and then combine their predictions. The main difference with bagging
    is that instead of creating a number of independent bootstrapped train sets, the
    algorithm sequentially trains each weak learner, assigns weights to all instances,
    samples the next train set based on the instance's weights, and repeats the whole
    process. As a base learner algorithm, usually decision trees consisting of a single
    node are used. These decision trees, with a depth of a single level, are called
    **decision stumps**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是最流行的提升算法之一。与袋装法类似，该算法的主要思想是创建若干个无关的弱学习器，然后将它们的预测结果结合起来。与袋装法的主要区别在于，算法不是创建多个独立的自助法训练集，而是顺序地训练每一个弱学习器，给所有实例分配权重，基于实例的权重采样下一组训练集，然后重复整个过程。作为基学习器算法，通常使用由单一节点构成的决策树。这些深度为一层的决策树被称为**决策桩**。
- en: Weighted sampling
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权采样
- en: Weighted sampling is the sampling process were each candidate has a corresponding
    weight, which determines its probability of being sampled. The weights are normalized,
    in order for their sum to equal one. Then, the normalized weights correspond to
    the probability that any individual will be sampled. For a simple example with
    three candidates, assuming weights of 1, 5, and 10, the following table depicts
    the normalized weights and the corresponding probability that any candidate will
    be chosen.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 加权采样是指每个候选者都有一个相应的权重，这个权重决定了该候选者被采样的概率。权重经过归一化处理，使得它们的总和为 1。然后，归一化后的权重对应每个候选者被选中的概率。以下表格展示了一个简单示例，其中有三个候选者，权重分别为
    1、5 和 10，并展示了归一化权重以及相应的候选者被选中的概率。
- en: '| Candidate | Weight | Normalized weight | Probability |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 候选者 | 权重 | 归一化权重 | 概率 |'
- en: '| 1 | 1 | 0.0625 | 6.25% |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0.0625 | 6.25% |'
- en: '| 2 | 5 | 0.3125 | 31.25% |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 5 | 0.3125 | 31.25% |'
- en: '| 3 | 10 | 0.625 | 62.50% |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 10 | 0.625 | 62.50% |'
- en: Instance weights to probabilities
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实例权重转为概率
- en: Creating the ensemble
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集成模型
- en: 'Assuming a classification problem, the AdaBoost algorithm can be described
    on a high-level basis, from its basic steps. For regression purposes, the steps
    are similar:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是一个分类问题，AdaBoost 算法可以从其基本步骤高层次地描述。对于回归问题，步骤类似：
- en: Initialize all of the train set instance's weights equally, so their sum equals
    1.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有训练集实例的权重，使它们的总和等于 1。
- en: Generate a new set by sampling with replacement, according to the weights.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据权重进行有放回的采样，生成一个新的数据集。
- en: Train a weak learner on the sampled set.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在采样集上训练弱学习器。
- en: Calculate its error on the original train set.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算它在原始训练集上的错误率。
- en: Add the weak learner to the ensemble and save its error rate.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将弱学习器添加到集成模型中并保存其错误率。
- en: Adjust the weights, increasing the weights of misclassified instances and decreasing
    the weights of correctly classified instances.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整权重，增加错误分类实例的权重，减少正确分类实例的权重。
- en: Repeat from *Step 2*.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *步骤 2* 重复。
- en: The weak learners are combined by voting. Each learner's vote is weighted, according
    to its error rate.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 弱学习器通过投票组合，每个学习器的投票按其错误率加权。
- en: 'The whole process is depicted in the following diagram:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程如下面的图所示：
- en: '![](img/9001110e-758f-4230-aea5-a7077ac7453e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9001110e-758f-4230-aea5-a7077ac7453e.png)'
- en: The process of creating the ensemble for the nth learner
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为第 n 个学习器创建集成模型的过程
- en: 'In essence, this makes each new classifier focus on the instances that the
    previous learners could not handle correctly. Assuming a binary classification
    problem, we may start with a dataset that looks like the following diagram:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这使得每个新的分类器都专注于前一个学习器无法正确处理的实例。假设是一个二分类问题，我们可以从如下图所示的数据集开始：
- en: '![](img/3dbe0e7c-9aa8-4189-866c-201c38baac6b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3dbe0e7c-9aa8-4189-866c-201c38baac6b.png)'
- en: Our initial dataset
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始数据集
- en: 'Here, all weights are equal. The first decision stump decides to partition
    the problem space as follows. The dotted line represents the decision boundary.
    The two black **+** and **-** symbols denote the sub-space that the decision stump
    classifies every instance as positive or negative, respectively. This leaves two
    misclassified instances. These instance weights will be increased, while all other
    weights will be decreased:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，所有权重都相等。第一个决策树桩决定按如下方式划分问题空间。虚线代表决策边界。两个黑色的 **+** 和 **-** 符号表示决策树桩将每个实例分类为正类或负类的子空间。这留下了两个错误分类的实例。它们的实例权重将被增加，而其他所有权重将被减少：
- en: '![](img/8d4ca482-4848-4991-8382-8f7fb365979c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d4ca482-4848-4991-8382-8f7fb365979c.png)'
- en: The first decision stump's space partition and errors
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个决策树桩的空间划分和错误
- en: 'By creating another dataset, where the two misclassified instances are dominant
    (they may be included several times, as we sample with replacement and their weights
    are larger than the other instances), the second decision stump partitions the
    space, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建另一个数据集，其中两个错误分类的实例占主导地位（由于我们进行有放回的采样并且它们的权重大于其他实例，它们可能会被多次包含），第二个决策树桩按如下方式划分空间：
- en: '![](img/d2e1c6fb-3344-4296-9ab2-196542b77cf5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2e1c6fb-3344-4296-9ab2-196542b77cf5.png)'
- en: The second decision stump's space partition and errors
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个决策树桩的空间划分和错误
- en: 'Finally, after repeating the process for a third decision stump, the final
    ensemble has partitioned the space as depicted in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在重复第三个决策树桩的过程后，最终的集成模型按如下图所示划分了空间：
- en: '![](img/9290b515-08ea-41cf-8f59-160bdef9c1c5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9290b515-08ea-41cf-8f59-160bdef9c1c5.png)'
- en: The final ensemble's partition of the problem space
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最终集成模型的空间划分
- en: Implementing AdaBoost in Python
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中实现 AdaBoost
- en: 'In order to better understand how AdaBoost works, we will present a basic implementation
    in Python. We will use the breast cancer classification dataset for this example.
    As always, we first load the libraries and data:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 AdaBoost 是如何工作的，我们将展示一个基本的 Python 实现。我们将使用乳腺癌分类数据集作为示例。像往常一样，我们首先加载库和数据：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We then create the ensemble. First, we declare the ensemble's size and the base
    learner type. As mentioned earlier, we use decision stumps (decision trees only
    a single level deep).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建集成模型。首先，声明集成模型的大小和基础学习器类型。如前所述，我们使用决策树桩（决策树仅有一层）。
- en: 'Furthermore, we create a NumPy array for the data instance weights, the learners''
    weights, and the learners'' errors:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们为数据实例的权重、学习器的权重和学习器的错误创建了一个 NumPy 数组：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For each base learner, we will create a `deepcopy` of the original classifier,
    train it on a sample dataset, and evaluate it. First, we create the copy and sample
    with replacement from the original test set, according to the instance''s weights:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个基本学习器，我们将创建一个原始分类器的`deepcopy`，在一个样本数据集上训练它，并进行评估。首先，我们创建副本并根据实例的权重，从原始测试集中进行有放回的抽样：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then fit the learner on the sampled dataset and predict on the original
    train set. We use the `predictions` to see which instances are correctly classified
    and which instances are misclassified:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在采样数据集上拟合学习器，并在原始训练集上进行预测。我们使用`predictions`来查看哪些实例被正确分类，哪些实例被误分类：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the following, the weighted errors are classified. Both `errors` and `corrects`
    are lists of Booleans (`True` or `False`), but Python handles them as 1 and 0\.
    This allows us to multiply element-wise with `data_weights`. The learner''s error
    is then calculated with the average weighted error:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，权重误差被分类。`errors`和`corrects`都是布尔值列表（`True`或`False`），但Python将它们处理为1和0。这使得我们可以与`data_weights`进行逐元素相乘。然后，学习器的误差通过加权误差的平均值计算得出：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, the learner''s weight can be calculated as half the natural logarithm
    of the weighted accuracy over the weighted error. In turn, we can use the learner''s
    weight to calculate the new data weights. For erroneously classified instances,
    the new weight equals the natural exponent of the old weight times the learner''s
    weight. For correctly classified instances, the negative multiple is used instead.
    Finally, the new weights are normalized and the base learner is added to the `base_learners`
    list:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，学习器的权重可以通过加权准确率与加权误差的自然对数的一半来计算。接下来，我们可以使用学习器的权重来计算新的数据权重。对于误分类的实例，新权重等于旧权重乘以学习器权重的自然指数。对于正确分类的实例，则使用负倍数。最后，新的权重进行归一化，基本学习器被添加到`base_learners`列表中：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In order to make predictions with the ensemble, we combine each individual
    prediction through a weighted majority voting. As this is a binary classification
    problem, if the weighted average is more than `0.5`, the instance is classified
    as `0`; otherwise, it''s classified as `1`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用集成进行预测，我们通过加权多数投票将每个单独的预测结果结合起来。由于这是一个二分类问题，如果加权平均值大于`0.5`，则实例被分类为`0`；否则，它被分类为`1`：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The final accuracy achieved by this ensemble is 95%.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该集成方法最终实现的准确率为95%。
- en: Strengths and weaknesses
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势与劣势
- en: 'Boosting algorithms are able to reduce both bias and variance. For a long time,
    they were considered immune to overfitting, but in fact they can overfit, although
    they are extremely robust. One possible explanation is that the base learners,
    in order to classify outliers, create very strong and complicated rules that rarely
    fit other instances. In the following diagram, an example is depicted. The ensemble
    has generated a set of rules in order to correctly classify the outlier, but the
    rules are so strong that only an identical example (that is, with the exact same
    feature values) could fit into the sub-space defined by the rules:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法能够同时减少偏差和方差。长期以来，它们被认为能够免疫过拟合，但事实上，它们也有可能过拟合，尽管它们非常健壮。一种可能的解释是，基本学习器为了分类异常值，创建了非常强大且复杂的规则，这些规则很少能适应其他实例。在下面的图示中，给出了一个示例。集成方法生成了一组规则来正确分类异常值，但这些规则如此强大，以至于只有一个完全相同的例子（即，具有完全相同特征值）才能适应由规则定义的子空间：
- en: '![](img/74776589-d19e-405e-9f91-67f8937812d0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74776589-d19e-405e-9f91-67f8937812d0.png)'
- en: Generated rules for an outlier
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为异常值生成的规则
- en: One disadvantage of many boosting algorithms is that they are not easily parallelized,
    as the models are created in a sequential fashion. Furthermore, they pose the
    usual problems of ensemble learning techniques, such as reduction in interpretability
    and additional computational costs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 许多提升算法的一个缺点是它们难以并行化，因为模型是顺序生成的。此外，它们还存在集成学习技术的常见问题，例如可解释性的降低和额外的计算成本。
- en: Gradient boosting
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting is another boosting algorithm. It is a more generalized boosting
    framework compared to AdaBoost, which also makes it more complicated and math-intensive.
    Instead of trying to emphasize problematic instances by assigning weights and
    resampling the dataset, gradient boosting builds each base learner on the previous
    learner's errors. Furthermore, gradient boosting uses decision trees of varying
    depths. In this section, we will present gradient boosting, without delving much
    into the math involved. Instead, we will present the basic concepts, as well as
    a custom Python implementation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是另一种提升算法。与 AdaBoost 相比，它是一个更广泛的提升框架，这也使得它更复杂且需要更多数学推导。梯度提升不是通过分配权重并重新采样数据集来强调有问题的实例，而是通过构建每个基本学习器来纠正前一个学习器的误差。此外，梯度提升使用不同深度的决策树。在这一部分，我们将介绍梯度提升，而不深入探讨其中的数学原理。相反，我们将介绍基本概念以及一个自定义的
    Python 实现。
- en: Creating the ensemble
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集成模型
- en: The gradient boosting algorithm (for regression purposes) starts by calculating
    the mean of the target variable for the train set and uses it as an initial prediction.
    Then, it calculates the difference of each instance's target from the prediction
    (mean), in order to calculate the error. These errors are also called **pseudo-residuals**.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法（用于回归目的）从计算训练集目标变量的均值开始，并将其作为初始预测值。然后，计算每个实例目标与预测值（均值）的差异，以便计算误差。这些误差也称为**伪残差**。
- en: 'Following that, it creates a decision tree that tries to predict the pseudo-residuals.
    By repeating this process, a number of times, the whole ensemble is created. Similar
    to AdaBoost, gradient boosting assigns a weight to each tree. Contrary to AdaBoost,
    this weight does not depend on the tree''s performance. Instead, it is a constant
    term, which is called **learning rate**. Its purpose is to increase the ensemble''s
    generalization ability, by restricting its over-fitting power. The algorithm''s
    steps are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，它创建一个决策树，尝试预测伪残差。通过重复这个过程若干次，整个集成模型被构建出来。类似于 AdaBoost，梯度提升为每棵树分配一个权重。与 AdaBoost
    不同的是，这个权重并不依赖于树的表现，而是一个常数项，这个常数项称为**学习率**。它的目的是通过限制过拟合的能力来提高集成模型的泛化能力。算法的步骤如下：
- en: Define the learning rate (smaller than 1) and the ensemble's size.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义学习率（小于 1）和集成模型的大小。
- en: Calculate the train set's target mean.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集的目标均值。
- en: Using the mean as a very simple initial prediction, calculate each instance's
    target difference from the mean. These errors are called pseudo-residuals.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均值作为非常简单的初始预测，计算每个实例目标与均值的差异。这些误差称为伪残差。
- en: Build a decision tree, by using the original train set's features and the pseudo-residuals
    as targets.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用原始训练集的特征和伪残差作为目标，构建决策树。
- en: Make predictions on the train set, using the decision tree (we try to predict
    the pseudo-residuals). Multiply the predicted values by the learning rate.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用决策树对训练集进行预测（我们尝试预测伪残差）。将预测值乘以学习率。
- en: Add the multiplied values to the previously stored predicted values. Use the
    newly calculated values as predictions.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将乘积值加到之前存储的预测值上，使用新计算的值作为新的预测。
- en: Calculate the new pseudo-residuals using the calculated predictions.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计算得到的预测值来计算新的伪残差。
- en: Repeat from *Step 4* until the desired ensemble size is achieved.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*步骤 4*开始重复，直到达到所需的集成模型大小。
- en: Note that in order to produce the final ensemble's predictions, each base learner's
    prediction is multiplied by the learning rate and added to the previous learner's
    prediction. The calculated mean can be regarded as the first base learner's prediction.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了产生最终集成模型的预测，每个基本学习器的预测值会乘以学习率，并加到前一个学习器的预测上。计算出的均值可以视为第一个基本学习器的预测值。
- en: 'At each step *s*, for a learning rate *lr*, the prediction is calculated as
    follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步 *s* 中，对于学习率 *lr*，预测值计算如下：
- en: '![](img/5f94494d-65fa-4f84-a396-2b9fb0b4e72a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f94494d-65fa-4f84-a396-2b9fb0b4e72a.png)'
- en: 'The residuals are calculated as the difference from the actual target value
    *t*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 残差计算为实际目标值 *t* 与预测值的差异：
- en: '![](img/78a02149-b4ed-4ee2-9fbc-f0f792bd2e71.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78a02149-b4ed-4ee2-9fbc-f0f792bd2e71.png)'
- en: 'The whole process is depicted in the following diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程如下面的图所示：
- en: '![](img/136039d5-8c1c-4600-a6e8-a6e166d33c8f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/136039d5-8c1c-4600-a6e8-a6e166d33c8f.png)'
- en: Steps to create a gradient boosting ensemble
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 创建梯度提升集成模型的步骤
- en: Further reading
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'As this is a hands-on book, we will not go further into the mathematical aspect
    of the algorithm. Nonetheless, for the mathematically curious or inclined, we
    recommend the following papers. The first is a more regression-specific framework,
    while the second is more general:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本实战书籍，我们不会深入探讨算法的数学方面。然而，对于数学上有兴趣的人，我们推荐以下论文。第一篇是更具体的回归框架，而第二篇则更加一般化：
- en: 'Friedman, J.H., 2001\. Greedy function approximation: a gradient boosting machine. *Annals
    of statistics*, pp.1189-1232.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman, J.H., 2001\. 贪婪函数逼近：梯度提升机。《统计学年鉴》，pp.1189-1232。
- en: Mason, L., Baxter, J., Bartlett, P.L. and Frean, M.R., 2000\. Boosting algorithms
    as gradient descent. In *Advances in neural information processing systems *(pp.
    512-518).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mason, L., Baxter, J., Bartlett, P.L. 和 Frean, M.R., 2000\. 提升算法作为梯度下降方法。在《神经信息处理系统进展》中（第512-518页）。
- en: Implementing gradient boosting in Python
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中实现梯度提升
- en: 'Although gradient boosting can be complex and mathematically intensive, if
    we focus on conventional regression problems, it can be quite simple. In order
    to demonstrate this, we present a custom implementation in Python, using standard
    scikit-learn decision trees. For our implementation, we will use the diabetes
    regression dataset. First, we load the libraries and data, and set the seed for
    NumPy''s random number generator:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管梯度提升可能很复杂且需要数学知识，但如果我们专注于传统的回归问题，它可以变得非常简单。为了证明这一点，我们在 Python 中使用标准的 scikit-learn
    决策树实现了一个自定义的例子。对于我们的实现，我们将使用糖尿病回归数据集。首先，加载库和数据，并设置 NumPy 的随机数生成器的种子：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Following this, we define the ensemble's size, learning rate, and the Decision
    Tree's maximum depth. Furthermore, we create a list to store the individual base
    learners, as well as a NumPy array to store the previous predictions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义集成模型的大小、学习率和决策树的最大深度。此外，我们创建一个列表来存储各个基础学习器，以及一个 NumPy 数组来存储之前的预测。
- en: 'As mentioned earlier, our initial prediction is the train set''s target mean.
    Instead of defining a maximum depth, we could also define a maximum number of
    leaf nodes by passing the `max_leaf_nodes=3` argument to the constructor:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的初始预测是训练集的目标均值。除了定义最大深度外，我们还可以通过将 `max_leaf_nodes=3` 参数传递给构造函数来定义最大叶节点数：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The next step is to create and train the ensemble. We start by calculating
    the pseudo-residuals, using the previous predictions. We then create a deep copy
    of the base learner class and train it on the train set, using the pseudo-residuals
    as targets:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建和训练集成模型。我们首先计算伪残差，使用之前的预测。然后我们创建基础学习器类的深层副本，并在训练集上使用伪残差作为目标进行训练：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we use the trained base learner in order to predict the pseudo-residuals
    on the train set. We multiply the predictions by the learning rate and add them
    to our previous predictions. Finally, we append the base learner to the `base_learners`
    list:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用训练好的基础学习器在训练集上预测伪残差。我们将预测乘以学习率，加到之前的预测上。最后，我们将基础学习器追加到 `base_learners`
    列表中：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In order to make predictions with our ensemble and evaluate it, we use the
    test set''s features in order to predict pseudo-residuals, multiply them by the
    learning rate, and add them to the train set''s target mean. It is important to
    use the original train set''s mean as a starting point, because each tree predicts
    deviation from that original mean:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用我们的集成模型进行预测和评估，我们使用测试集的特征来预测伪残差，将其乘以学习率，然后加到训练集的目标均值上。重要的是要使用原始训练集的均值作为起始点，因为每棵树都预测相对于那个原始均值的偏差：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The algorithm is able to achieve an R-squared value of 0.59 and an MSE of 2253.34
    with this particular setup.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法能够通过这种特定设置实现0.59的R平方值和2253.34的均方误差。
- en: Using scikit-learn
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn
- en: Although for educational purposes it is useful to code our own algorithms, scikit-learn
    has some very good implementations for both classification and regression problems.
    In this section, we will go through the implementations, as well as see how we
    can extract information about the generated ensembles.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管出于教育目的编写自己的算法很有用，但 scikit-learn 在分类和回归问题上有一些非常好的实现。在本节中，我们将介绍这些实现，并看看如何提取生成的集成模型的信息。
- en: Using AdaBoost
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AdaBoost
- en: Scikit-learn's Adaboost implementations exist in the `sklearn.ensemble` package,
    in the `AdaBoostClassifier` and `AdaBoostRegressor` classes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 中的 AdaBoost 实现位于 `sklearn.ensemble` 包中的 `AdaBoostClassifier` 和
    `AdaBoostRegressor` 类中。
- en: 'Like all scikit-learn classifiers, we use the `fit` and `predict` functions
    in order to train the classifier and predict on the test set. The first parameter
    is the base classifier that the algorithm will use. The `algorithm="SAMME"` parameter
    forces the classifier to use a discrete boosting algorithm. For this example,
    we use the hand-written digits recognition problem:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有 scikit-learn 分类器一样，我们使用`fit`和`predict`函数来训练分类器并在测试集上进行预测。第一个参数是算法将使用的基本分类器。`algorithm="SAMME"`参数强制分类器使用离散提升算法。对于这个例子，我们使用手写数字识别问题：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in an ensemble with 81% accuracy on the test set. One advantage
    of using the provided implementation is that we can access and plot each individual
    base learner''s errors and weights. We can access them through `ensemble.estimator_errors_`
    and `ensemble.estimator_weights_`, respectively. By plotting the weights, we can
    gauge where the ensemble stops to benefit from additional base learners. By creating
    an ensemble of 1,000 base learners, we see that from approximately the 200 base
    learners mark, the weights are stabilized. Thus, there is little point in adding
    more than 200\. This is further confirmed by the fact that the ensemble of size
    1,000 achieves an 82% accuracy, a small increase over the 81% achieved with 200
    base learners:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了在测试集上准确率为 81% 的集成。使用提供的实现的一个优势是，我们可以访问并绘制每个单独的基本学习器的误差和权重。我们可以通过`ensemble.estimator_errors_`和`ensemble.estimator_weights_`分别访问它们。通过绘制权重，我们可以评估集成在哪些地方停止从额外的基本学习器中获益。通过创建一个由
    1,000 个基本学习器组成的集成，我们可以看到大约从 200 个基本学习器开始，权重已经稳定。因此，再增加超过 200 个基本学习器几乎没有意义。通过事实也得到了进一步证实：1,000
    个基本学习器的集成达到了 82% 的准确率，比使用 200 个基本学习器时提高了 1%。
- en: '![](img/5ff5bdde-8046-483f-be0d-5d27d68ae112.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ff5bdde-8046-483f-be0d-5d27d68ae112.png)'
- en: Base learner weights for an ensemble of 1,000 base learners
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 1,000 个基本学习器的集成基本学习器权重
- en: 'The regression implementation adheres to the same principles. Here, we test
    the algorithm on the diabetes dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 回归实现遵循相同的原理。这里，我们在糖尿病数据集上测试该算法：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The ensemble generates an R-squared of 0.59 and an MSE of 2256.5\. By plotting
    the weights of the base learners, we see that the algorithm has stopped early,
    due to negligible improvement in predictive power, after the 151^(st) base learner.
    This is indicated by the zero valued weights in the plot. Furthermore, by printing
    the length of `ensemble.estimators_`, we observe that its length is only 151\.
    This is the equivalent of the `base_learners` list in our implementation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 集成生成的 R 平方为 0.59，均方误差（MSE）为 2256.5。通过绘制基本学习器的权重，我们可以看到算法由于预测能力的改进微不足道，在第 151
    个基本学习器之后提前停止。这可以通过图中的零权重值看出。此外，通过打印`ensemble.estimators_`的长度，我们观察到它的长度仅为 151。这与我们实现中的`base_learners`列表等效：
- en: '![](img/61ab17f3-3d12-412e-ba29-ef48a5fad8bc.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61ab17f3-3d12-412e-ba29-ef48a5fad8bc.png)'
- en: Base learner weights for the regression Adaboost
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 回归 Adaboost 的基本学习器权重
- en: Using gradient boosting
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度提升
- en: 'Scikit-learn also implements gradient boosting regression and classification.
    They too are included in the `ensemble` package, under `GradientBoostingRegressor`
    and `GradientBoostingClassifier`, respectively. The two classes store the errors
    at each step, in the `train_score_` attribute of the object. Here, we present
    an example for the diabetes regression dataset. The train and validation processes
    follow the scikit-learn standard, using the `fit` and `predict` functions. The
    only parameter that needs to be specified is the learning rate, which is passed
    to the `GradientBoostingRegressor` constructor through the `learning_rate` parameter:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 还实现了梯度提升回归和分类。这两者也被包含在`ensemble`包中，分别为`GradientBoostingRegressor`和`GradientBoostingClassifier`。这两个类在每一步存储误差，保存在对象的`train_score_`属性中。这里，我们展示了一个糖尿病回归数据集的例子。训练和验证过程遵循
    scikit-learn 的标准，使用`fit`和`predict`函数。唯一需要指定的参数是学习率，它通过`learning_rate`参数传递给`GradientBoostingRegressor`构造函数：
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The ensemble achieves an R-squared of 0.44 and an MSE of 3092\. Furthermore,
    if we use matplotlib to plot `ensemble.train_score_`, we can see that diminishing
    returns appear after around 20 base learners. If we further analyze the errors,
    by calculating the improvements (difference between base learners), we see that
    after 25 base learners there are cases where adding a base learner worsens the
    performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型达到了0.44的R平方值和3092的均方误差（MSE）。此外，如果我们使用matplotlib绘制`ensemble.train_score_`，可以看到大约在20个基学习器之后，收益递减现象出现。如果进一步分析误差，通过计算改进（基学习器之间的差异），我们发现，在25个基学习器之后，添加新的基学习器可能会导致性能下降。
- en: 'Although on average the performance continues to increase, after 50 base learners
    there is no significant improvement. Thus, we repeat the experiment, with `ensemble_size
    = 50`, yielding an R-squared of 0.61 and an MSE of 2152:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管平均性能持续提高，但在使用50个基学习器后，性能没有显著改进。因此，我们重复实验，设定`ensemble_size = 50`，得到了0.61的R平方值和2152的均方误差（MSE）：
- en: '![](img/37438d81-ae5f-4ac4-a072-0f60b54b397e.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37438d81-ae5f-4ac4-a072-0f60b54b397e.png)'
- en: Errors and differences for gradient boost regression
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升回归的误差与差异
- en: 'For the classification example, we use the hand-written digit classification
    dataset. Again, we define the `n_estimators` and `learning_rate` parameters:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类示例，我们使用手写数字分类数据集。同样，我们定义了`n_estimators`和`learning_rate`参数：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The accuracy achieved with the specific ensemble size is 89%. By plotting the
    errors and their differences, we see that there are again diminishing returns,
    but there are no cases where performance significantly drops. Thus, we do not
    expect a predictive performance improvement by reducing the ensemble size.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特定集成大小达到的准确率为89%。通过绘制误差及其差异，我们再次看到收益递减现象，但没有出现性能显著下降的情况。因此，我们不期待通过减少集成大小来提高预测性能。
- en: XGBoost
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost
- en: 'XGBoost is a boosting library with parallel, GPU, and distributed execution
    support. It has helped many machine learning engineers and data scientists to
    win Kaggle.com competitions. Furthermore, it provides an interface that resembles
    scikit-learn''s interface. Thus, someone already familiar with the interface is
    able to quickly utilize the library. Additionally, it allows for very fine control
    over the ensemble''s creation. It supports monotonic constraints (that is, the
    predicted value should only increase or decrease, relative to a specific feature),
    as well as feature interaction constraints (for example, if a decision tree creates
    a node that splits by age, it should not use sex as a splitting feature for all
    children of that specific node). Finally, it adds an additional regularization
    parameter, gamma, which further reduces the overfitting capabilities of the generated
    ensemble. The corresponding paper is Chen, T. and Guestrin, C., 2016, August.
    Xgboost: A scalable tree boosting system. In *Proceedings of the 22nd acm sigkdd
    international conference on knowledge discovery and data mining* (pp. 785-794).
    ACM.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 'XGBoost是一个支持并行、GPU和分布式执行的提升库。它帮助许多机器学习工程师和数据科学家赢得了Kaggle.com的竞赛。此外，它提供了一个类似于scikit-learn接口的API。因此，已经熟悉该接口的人可以快速利用这个库。此外，它允许对集成的创建进行非常精细的控制。它支持单调约束（即，预测值应当根据特定特征只增加或减少），以及特征交互约束（例如，如果一个决策树创建了一个按年龄分裂的节点，那么它不应当对该节点的所有子节点使用性别作为分裂特征）。最后，它增加了一个额外的正则化参数gamma，进一步减少了生成集成模型的过拟合能力。相关论文为Chen,
    T. 和 Guestrin, C., 2016年8月，Xgboost: A scalable tree boosting system. 见《*第22届ACM
    SIGKDD国际知识发现与数据挖掘大会论文集*》，（第785-794页）。ACM。'
- en: Using XGBoost for regression
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用XGBoost进行回归
- en: 'We will present a simple regression example with XGBoost, using the diabetes
    dataset. As it will be shown, its usage is quite simple and similar to the scikit-learn
    classifiers. XGBoost implements regression with `XGBRegressor`. The constructor
    has a respectably large number of parameters, which are very well-documented in
    the official documentation. In our example, we will use the `n_estimators`, `n_jobs`,
    `max_depth`, and `learning_rate` parameters. Following scikit-learn''s conventions,
    they define the ensemble size, the number of parallel processes, the tree''s maximum
    depth, and the learning rate, respectively:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用糖尿病数据集展示一个简单的回归示例。正如所示，其使用方法非常简单，类似于scikit-learn的分类器。XGBoost通过`XGBRegressor`实现回归。该构造函数包含大量参数，并且在官方文档中有详细的说明。在我们的示例中，我们将使用`n_estimators`、`n_jobs`、`max_depth`和`learning_rate`参数。按照scikit-learn的约定，它们分别定义了集成的大小、并行处理的数量、树的最大深度以及学习率：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The rest of the code evaluates the generated `ensemble`, and is similar to
    any of the previous examples:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码评估生成的`ensemble`，与之前的任何示例类似：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: XGBoost achieves an R-squared of 0.65 and an MSE of 1932.9, the best performance
    out of all the boosting methods we tested and implemented in this chapter. Furthermore,
    we did not fine-tune any of its parameters, which further displays its modeling
    power.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 的 R-squared 为 0.65，MSE 为 1932.9，是我们在本章中测试和实现的所有提升方法中表现最好的。此外，我们并未对其任何参数进行微调，这进一步显示了它的建模能力。
- en: Using XGBoost for classification
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 XGBoost 进行分类
- en: 'For classification purposes, the corresponding class is implemented in `XGBClassifier`.
    The constructor''s parameters are the same as the regression implementation. For
    our example, we use the hand-written digit classification problem. We set the
    `n_estimators` parameter to `100` and `n_jobs` to `4`. The rest of the code follows
    the usual template:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，相应的类是 `XGBClassifier`。构造函数的参数与回归实现相同。以我们的示例为例，我们使用的是手写数字分类问题。我们将 `n_estimators`
    参数设置为 `100`，`n_jobs` 设置为 `4`。其余的代码遵循常规模板：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The ensemble correctly classifies the test set with 89% accuracy, also the highest
    achieved for any boosting algorithm.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该集成方法以 89% 的准确率正确分类了测试集，也是所有提升算法中表现最好的。
- en: Other boosting libraries
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他提升方法库
- en: Two other boosting libraries that are gaining popularity are Microsoft's LightGBM
    and Yandex' CatBoost. Both of these libraries can match (and even outperform)
    XGBoost, under certain circumstances. Nonetheless, XGBoost is the best of all
    three out of the box, without the need of fine-tuning and special data treatment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个越来越流行的提升方法库是微软的 LightGBM 和 Yandex 的 CatBoost。在某些情况下，这两个库的性能可以与 XGBoost 相媲美（甚至超过）。尽管如此，XGBoost
    在所有三者中仍然是最优秀的，无需微调和特殊的数据处理。
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we presented one of the most powerful ensemble learning techniques,
    boosting. We presented two popular boosting algorithms, AdaBoost and gradient
    boosting. We presented custom implementations for both algorithms, as well as
    usage examples for the scikit-learn implementations. Furthermore, we briefly presented
    XGBoost, a library dedicated to regularized, distributed boosting. XGBoost was
    able to outperform all other methods and implementations on both regression as
    well as classification problems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了最强大的集成学习技术之一——提升方法。我们介绍了两种流行的提升算法，AdaBoost 和梯度提升。我们提供了这两种算法的自定义实现，以及 scikit-learn
    实现的使用示例。此外，我们还简要介绍了 XGBoost，这是一个专注于正则化和分布式提升的库。XGBoost 在回归和分类问题中都能超越所有其他方法和实现。
- en: AdaBoost creates a number of base learners by employing weak learners (slightly
    better than random guessing). Each new base learner is trained on a weighted sample
    from the original train set. Weighted sampling from a dataset assigns a weight
    to each instance and then samples from the dataset, using the weights in order
    to calculate the probability that each instance will be sampled.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 通过使用弱学习器（略优于随机猜测）来创建多个基础学习器。每个新的基础学习器都在来自原始训练集的加权样本上进行训练。数据集的加权抽样为每个实例分配一个权重，然后根据这些权重从数据集中抽样，以计算每个实例被抽样的概率。
- en: The data weights are calculated based on the previous base learner's errors.
    The base learner's error is also used to calculate the learner's weight. The base
    learners' predictions are combined through voting, using each learner's weight.
    Gradient boosting builds its ensemble by training each new base learner using
    the previous prediction's errors as a target. The initial prediction is the train
    dataset's target mean. Boosting methods cannot be parallelized in the degree that
    bagging methods can be. Although robust to overfitting, boosting methods can overfit.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据权重是基于前一个基础学习器的错误计算的。基础学习器的错误还用于计算学习器的权重。通过投票的方式结合基础学习器的预测结果，投票时使用每个学习器的权重。梯度提升通过训练每个新的基础学习器，使用前一次预测的错误作为目标，来构建其集成方法。初始预测是训练数据集的目标均值。与袋装方法相比，提升方法无法在相同程度上并行化。尽管提升方法对过拟合具有较强的鲁棒性，但它们仍然可能会过拟合。
- en: In scikit-learn, AdaBoost implementations store the individual learners' weights,
    which can be used to identify the point where additional base learners do not
    contribute to the ensemble's predictive power. Gradient Boosting implementations
    store the ensemble's error at each step (base learner), which can also help to
    identify an optimal number of base learners. XGBoost is a library dedicated to
    boosting, with regularization capabilities that further reduce the overfitting
    ability of the ensembles. XGBoost is frequently a part of winning machine learning
    models in many Kaggle competitions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，AdaBoost 的实现存储了各个学习器的权重，这些权重可以用来识别额外的基学习器不再对整体集成的预测能力有贡献的点。梯度提升实现在每一步（基学习器）都存储了集成的误差，这也有助于确定最佳的基学习器数量。XGBoost
    是一个专注于提升（boosting）的库，具有正则化能力，进一步减少集成模型的过拟合能力。XGBoost 经常成为许多 Kaggle 竞赛中获胜的机器学习模型的一部分。
