- en: '*Chapter 5*: XGBoost Unveiled'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：XGBoost 揭示'
- en: In this chapter, you will finally see **Extreme Gradient Boosting**, or **XGBoost**,
    as it is. XGBoost is presented in the context of the machine learning narrative
    that we have built up, from decision trees to gradient boosting. The first half
    of the chapter focuses on the theory behind the distinct advancements that XGBoost
    brings to tree ensemble algorithms. The second half focuses on building XGBoost
    models within the **Higgs Boson Kaggle Competition**, which unveiled XGBoost to
    the world.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将最终看到**极限梯度提升**（Extreme Gradient Boosting），或称为**XGBoost**。XGBoost 是在我们构建的机器学习叙事框架中呈现的，从决策树到梯度提升。章节的前半部分聚焦于
    XGBoost 带给树集成算法的独特进展背后的理论。后半部分则聚焦于在**Higgs 博士 Kaggle 竞赛**中构建 XGBoost 模型，正是这个竞赛让
    XGBoost 向全世界展示了它的强大。
- en: Specifically, you will identify speed enhancements that make XGBoost faster,
    discover how XGBoost handles missing values, and learn the mathematical derivation
    behind XGBoost's **regularized parameter selection**. You will establish model
    templates for building XGBoost classifiers and regressors. Finally, you will look
    at the **Large Hadron Collider**, where the Higgs boson was discovered, where
    you will weigh data and make predictions using the original XGBoost Python API.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，你将识别出使 XGBoost 更加快速的速度增强，了解 XGBoost 如何处理缺失值，并学习 XGBoost 的**正则化参数选择**背后的数学推导。你将建立构建
    XGBoost 分类器和回归器的模型模板。最后，你将了解**大型强子对撞机**（Large Hadron Collider），即希格斯玻色子发现的地方，在那里你将使用原始的
    XGBoost Python API 来加权数据并进行预测。
- en: 'This chapter covers the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及以下主要内容：
- en: Designing XGBoost
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计 XGBoost
- en: Analyzing XGBoost parameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析 XGBoost 参数
- en: Building XGBoost models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 XGBoost 模型
- en: Finding the Higgs boson – case study
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找希格斯玻色子 – 案例研究
- en: Designing XGBoost
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计 XGBoost
- en: XGBoost is a significant upgrade from gradient boosting. In this section, you
    will identify the key features of XGBoost that distinguish it from gradient boosting
    and other tree ensemble algorithms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是相较于梯度提升算法的重大升级。在本节中，你将识别 XGBoost 的关键特性，这些特性使它与梯度提升和其他树集成算法区分开来。
- en: Historical narrative
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史叙事
- en: With the acceleration of big data, the quest to find awesome machine learning
    algorithms to produce accurate, optimal predictions began. Decision trees produced
    machine learning models that were too accurate and failed to generalize well to
    new data. Ensemble methods proved more effective by combining many decision trees
    via **bagging** and **boosting**. A leading algorithm that emerged from the tree
    ensemble trajectory was gradient boosting.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据的加速发展，寻找能产生准确、最优预测的优秀机器学习算法的探索开始了。决策树生成的机器学习模型过于精确，无法很好地泛化到新数据。集成方法通过**集成**和**提升**组合多棵决策树，证明了更加有效。梯度提升是从树集成算法轨迹中出现的领先算法。
- en: The consistency, power, and outstanding results of gradient boosting convinced
    Tianqi Chen from the University of Washington to enhance its capabilities. He
    called the new algorithm XGBoost, short for **Extreme Gradient Boosting**. Chen's
    new form of gradient boosting included built-in regularization and impressive
    gains in speed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升的一致性、强大功能和出色结果使得华盛顿大学的陈天奇（Tianqi Chen）决定增强其能力。他将这一新算法命名为 XGBoost，代表**极限梯度提升**（Extreme
    Gradient Boosting）。陈天奇的新型梯度提升算法包含内建的正则化，并在速度上取得了显著提升。
- en: 'After finding initial success in Kaggle competitions, in 2016, Tianqi Chen
    and Carlos Guestrin authored *XGBoost: A Scalable Tree Boosting System* to present
    their algorithm to the larger machine learning community. You can check out the
    original paper at [https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf).
    The key points are summarized in the following section.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Kaggle 竞赛中取得初步成功后，2016年，陈天奇和卡洛斯·格斯特林（Carlos Guestrin）共同撰写了 *XGBoost: A Scalable
    Tree Boosting System*，向更广泛的机器学习社区介绍了他们的算法。你可以在[https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf)上查看原文。以下部分总结了其中的关键要点。'
- en: Design features
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计特性
- en: As indicated in [*Chapter 4*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093),
    *From Gradient Boosting to XGBoost*, the need for faster algorithms is evident
    when dealing with big data. The *Extreme* in *Extreme Gradient Boosting* means
    pushing computational limits to the extreme. Pushing computational limits requires
    knowledge not just of model-building but also of disk-reading, compression, cache,
    and cores.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第4章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)所示，*从梯度提升到 XGBoost*，在处理大数据时，对更快算法的需求显而易见。*极限*（Extreme）在*极限梯度提升*（Extreme
    Gradient Boosting）中意味着将计算极限推向极致。推动计算极限不仅需要构建模型的知识，还需要了解磁盘读取、压缩、缓存和核心等方面的知识。
- en: Although the focus of this book remains on building XGBoost models, we will
    take a glance under the hood of the XGBoost algorithm to distinguish key advancements,
    such as handling missing values, speed gains, and accuracy gains that make XGBoost
    faster, more accurate, and more desirable. Let's look at these key advancements
    next.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的重点仍然是构建 XGBoost 模型，但我们将窥探一下 XGBoost 算法的内部机制，以区分其关键进展，如处理缺失值、提升速度和提高准确度，这些因素使
    XGBoost 更快、更准确、并且更具吸引力。接下来，让我们看看这些关键进展。
- en: Handling missing values
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: You spent significant time in [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022),
    *Machine Learning Landscape*, practicing different ways to correct **null values**.
    This is an essential skill for all machine learning practitioners.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你在[*第1章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)《机器学习概述》中花费了大量时间，练习了不同的方法来处理**空值**。这是所有机器学习从业者必须掌握的基本技能。
- en: XGBoost, however, is capable of handling missing values for you. There is a
    `missing` hyperparameter that can be set to any value. When given a missing data
    point, XGBoost scores different split options and chooses the one with the best
    results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，XGBoost 能够为你处理缺失值。它有一个名为 `missing` 的超参数，可以设置为任何值。遇到缺失数据时，XGBoost 会对不同的分割选项进行评分，并选择结果最好的那个。
- en: Gaining speed
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升速度
- en: 'XGBoost was specifically designed for speed. Speed gains allow machine learning
    models to build more quickly which is especially important when dealing with millions,
    billions, or trillions of rows of data. This is not uncommon in the world of big
    data, where each day, industry and science accumulate more data than ever before.
    The following new design features give XGBoost a big edge in speed over comparable
    ensemble algorithms:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是专门为速度设计的。速度的提升使得机器学习模型能够更快速地构建，这在处理百万、十亿或万亿行数据时尤为重要。在大数据的世界里，这并不罕见，因为每天，工业和科学领域都会积累比以往更多的数据。以下新的设计功能使
    XGBoost 在速度上相比同类集成算法具有显著优势：
- en: '**Approximate split-finding algorithm**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近似分割查找算法**'
- en: '**Sparsity aware split-finding**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏感知分割查找**'
- en: '**Parallel computing**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行计算**'
- en: '**Cache-aware access**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存感知访问**'
- en: '**Block compression and sharding**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**块压缩与分片**'
- en: Let's learn about these features in a bit more detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这些功能。
- en: Approximate split-finding algorithm
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 近似分割查找算法
- en: Decision trees need optimal splits to produce optimal results. A *greedy algorithm*
    selects the best split at each step and does not backtrack to look at previous
    branches. Note that decision tree splitting is usually performed in a greedy manner.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树需要最优的分割才能产生最优结果。*贪心算法*在每一步选择最佳的分割，并且不会回溯查看之前的分支。请注意，决策树分割通常以贪心的方式进行。
- en: XGBoost presents an exact greedy algorithm in addition to a new approximate
    split-finding algorithm. The split-finding algorithm uses **quantiles**, percentages
    that split data, to propose candidate splits. In a global proposal, the same quantiles
    are used throughout the entire training, and in a local proposal, new quantiles
    are provided for each round of splitting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 提出了一个精确的贪心算法，并且增加了一种新的近似分割查找算法。分割查找算法使用**分位数**（用于拆分数据的百分比）来提出候选的分割点。在全局提议中，整个训练过程中使用相同的分位数；而在局部提议中，每一轮分割都会提供新的分位数。
- en: A previously known algorithm, **quantile sketch**, works well with equally weighted
    datasets. XGBoost presents a novel weighted quantile sketch based on merging and
    pruning with a theoretical guarantee. Although the mathematical details of this
    algorithm are beyond the scope of this book, you are encouraged to check out the
    appendix of the original XGBoost paper at [https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个已知的算法，**分位数草图**，在数据集权重相等的情况下表现良好。XGBoost 提出了一个新型的加权分位数草图，基于合并和修剪，并提供了理论保证。虽然该算法的数学细节超出了本书的范围，但你可以参考
    XGBoost 原始论文的附录，链接请见[https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf)。
- en: Sparsity-aware split finding
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏感知分割查找
- en: '`pd.get_dummies` to transform **categorical columns** into **numerical columns**.
    This resulted in a larger dataset with many values of 0\. This method of converting
    categorical columns into numerical columns, where 1 indicates presence and 0 indicates
    absence, is generally referred to as one-hot encoding. You will gain practice
    with one-hot-encoding in [*Chapter 10*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *XGBoost Model Deployment*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pd.get_dummies`将**类别列**转换为**数值列**。这会导致数据集增大，并产生许多值为 0 的条目。将类别列转换为数值列，其中 1
    表示存在，0 表示不存在，这种方法通常称为独热编码。你将在[*第10章*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230)中练习独热编码，*XGBoost模型部署*。
- en: Sparse matrices are designed to only store data points with non-zero and non-null
    values. This saves valuable space. A sparsity-aware split indicates that when
    looking for splits, XGBoost is faster because its matrices are sparse.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏矩阵旨在仅存储具有非零且非空值的数据点，这样可以节省宝贵的空间。稀疏感知的分裂意味着，在寻找分裂时，XGBoost 更快，因为它的矩阵是稀疏的。
- en: 'According to the original paper, *XGBoost: A Scalable Tree Boosting System*,
    the sparsity-aware split-finding algorithm performed 50 times faster than the
    standard approach on the **All-State-10K** dataset.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '根据原始论文《XGBoost: A Scalable Tree Boosting System》，稀疏感知的分裂查找算法在**All-State-10K**数据集上比标准方法快
    50 倍。'
- en: Parallel computing
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 并行计算
- en: Boosting is not ideal for **parallel computing** since each tree depends on
    the results of the previous tree. There are opportunities, however, where parallelization
    may take place.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 增强方法并不适合**并行计算**，因为每棵树都依赖于前一棵树的结果。然而，仍然有一些并行化的机会。
- en: Parallel computing occurs when multiple computational units are working together
    on the same problem at the same time. XGBoost sorts and compresses the data into
    blocks. These blocks may be distributed to multiple machines, or to external memory
    (out of core).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 并行计算发生在多个计算单元同时协同处理同一个问题时。XGBoost 将数据排序并压缩为块。这些块可以分发到多个机器，或者分发到外部内存（即核心外存储）。
- en: Sorting the data is faster with blocks. The split-finding algorithm takes advantage
    of blocks and the search for quantiles is faster due to blocks. In each of these
    cases, XGBoost provides parallel computing to expedite the model-building process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用块排序数据速度更快。分裂查找算法利用了块，因而查找分位数的速度也更快。在这些情况下，XGBoost 提供了并行计算，以加速模型构建过程。
- en: Cache-aware access
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓存感知访问
- en: 'The data on your computer is separated into **cache** and **main memory**.
    The cache, what you use most often, is reserved for high-speed memory. The data
    that you use less often is held back for lower-speed memory. Different cache levels
    have different orders of magnitude of latency, as outlined here: [https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机上的数据被分为**缓存**和**主内存**。缓存是你最常使用的部分，保留给高速内存。较少使用的数据则保留在较低速的内存中。不同的缓存级别有不同的延迟量级，详细信息可以参见：[https://gist.github.com/jboner/2841832](https://gist.github.com/jboner/2841832)。
- en: 'When it comes to gradient statistics, XGBoost uses **cache-aware prefetching**.
    XGBoost allocates an internal buffer, fetches the gradient statistics, and performs
    accumulation with mini batches. According to *XGBoost: A Scalable Tree Boosting
    System*, prefetching lengthens read/write dependency and reduces runtimes by approximately
    50% for datasets with a large number of rows.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在梯度统计方面，XGBoost 使用**缓存感知预取**。XGBoost 分配一个内部缓冲区，提取梯度统计数据，并通过小批量进行累积。根据《XGBoost:
    A Scalable Tree Boosting System》中的描述，预取延长了读写依赖，并将大规模数据集的运行时间减少了大约 50%。'
- en: Block compression and sharding
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块压缩与分片
- en: XGBoost delivers additional speed gains through **block compression** and **block
    sharding**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 通过**块压缩**和**块分片**提供额外的速度提升。
- en: Block compression helps with computationally expensive disk reading by compressing
    columns. Block sharding decreases read times by sharding the data into multiple
    disks that alternate when reading the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 块压缩通过压缩列来帮助计算密集型磁盘读取。块分片通过将数据分片到多个磁盘，并在读取数据时交替使用，减少了读取时间。
- en: Accuracy gains
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确性提升
- en: XGBoost adds built-in regularization to achieve accuracy gains beyond gradient
    boosting. **Regularization** is the process of adding information to reduce variance
    and prevent overfitting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 增加了内建正则化，以在梯度提升之外实现准确性提升。**正则化**是通过增加信息来减少方差并防止过拟合的过程。
- en: Although data may be regularized through hyperparameter fine-tuning, regularized
    algorithms may also be attempted. For example, `Ridge` and `Lasso` are regularized
    machine learning alternatives to `LinearRegression`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据可以通过超参数微调进行正则化，但也可以尝试使用正则化算法。例如，`Ridge` 和 `Lasso` 是 `LinearRegression` 的正则化机器学习替代方法。
- en: XGBoost includes regularization as part of the learning objective, as contrasted
    with gradient boosting and random forests. The regularized parameters penalize
    complexity and smooth out the final weights to prevent overfitting. XGBoost is
    a regularized version of gradient boosting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 将正则化作为学习目标的一部分，与梯度提升和随机森林不同。正则化参数通过惩罚复杂性并平滑最终权重来防止过拟合。XGBoost 是一种正则化版本的梯度提升。
- en: In the next section, you will meet the math behind the learning objective of
    XGBoost, which combines regularization with the loss function. While you don't
    need to know the math to use XGBoost effectively, mathematical knowledge may provide
    a deeper understanding. You can skip the next section if desired.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将接触到 XGBoost 学习目标背后的数学原理，它将正则化与损失函数结合在一起。虽然你不需要知道这些数学内容来有效地使用 XGBoost，但掌握数学知识可能会让你对其有更深的理解。如果愿意，你可以跳过下一节。
- en: Analyzing XGBoost parameters
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析 XGBoost 参数
- en: In this section, we will analyze the parameters that XGBoost uses to create
    state-of-the-art machine learning models with a mathematical derivation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过数学推导分析 XGBoost 用于创建最先进机器学习模型的参数。
- en: We will maintain the distinction between parameters and hyperparameters as presented
    in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision Trees
    in Depth*. Hyperparameters are chosen before the model is trained, whereas parameters
    are chosen while the model is being trained. In other words, the parameters are
    what the model learns from the data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持与[*第 2 章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入解析》中的参数和超参数的区分。超参数是在模型训练之前选择的，而参数则是在模型训练过程中选择的。换句话说，参数是模型从数据中学习到的内容。
- en: The derivation that follows is taken from the XGBoost official documentation,
    *Introduction to Boosted Trees*, at [https://xgboost.readthedocs.io/en/latest/tutorials/model.html](https://xgboost.readthedocs.io/en/latest/tutorials/model.html).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下推导来自 XGBoost 官方文档，*Boosted Trees 介绍*，网址为 [https://xgboost.readthedocs.io/en/latest/tutorials/model.html](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)。
- en: Learning objective
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'The learning objective of a machine learning model determines how well the
    model fits the data. In the case of XGBoost, the learning objective consists of
    two parts: the **loss function** and the **regularization term**.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的学习目标决定了模型与数据的拟合程度。对于 XGBoost，学习目标包括两个部分：**损失函数** 和 **正则化项**。
- en: 'Mathematically, XGBoost''s learning objective may be defined as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，XGBoost 的学习目标可以定义如下：
- en: '![](img/Formula_05_001.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_001.jpg)'
- en: Here, ![](img/Formula_05_002.png) is the loss function, which is the **Mean
    Squared Error** (**MSE**) for regression, or the log loss for classification,
    and ![](img/Formula_05_003.png) is the regularization function, a penalty term
    to prevent over-fitting. Including a regularization term as part of the objective
    function distinguishes XGBoost from most tree ensembles.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_05_002.png) 是损失函数，表示回归的**均方误差**（**MSE**），或分类的对数损失，![](img/Formula_05_003.png)
    是正则化函数，一个用于防止过拟合的惩罚项。将正则化项作为目标函数的一部分使 XGBoost 与大多数树集成方法有所区别。
- en: Let's look at the objective function in more detail, by considering the MSE
    for regression.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑回归的均方误差（MSE）更详细地看一下目标函数。
- en: Loss function
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The loss function, defined as the MSE for regression, can be written in summation
    notation, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义为回归的均方误差（MSE）的损失函数可以用求和符号表示，如下所示：
- en: '![](img/Formula_05_004.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_004.jpg)'
- en: Here, ![](img/Formula_05_005.png) is the target value for the ![](img/Formula_05_006.png)th
    row and ![](img/Formula_05_007.png) is the value predicted by the machine learning
    model for the ![](img/Formula_05_008.png)th row. The summation symbol, ![](img/Formula_05_009.png),
    indicates that all rows are summed starting with ![](img/Formula_05_010.png) and
    ending with ![](img/Formula_05_011.png), the number of rows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_05_005.png) 是第 ![](img/Formula_05_006.png) 行的目标值，![](img/Formula_05_007.png)
    是机器学习模型为第 ![](img/Formula_05_008.png) 行预测的值。求和符号 ![](img/Formula_05_009.png) 表示从
    ![](img/Formula_05_010.png) 开始到 ![](img/Formula_05_011.png) 结束，所有行的求和。
- en: 'The prediction, ![](img/Formula_05_012.png), for a given tree requires a function
    that starts at the tree root and ends at a leaf. Mathematically, this can be expressed
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的树，预测值 ![](img/Formula_05_012.png) 需要一个从树根开始，到叶子结束的函数。数学上可以表达为：
- en: '![](img/Formula_05_013.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_013.jpg)'
- en: Here, *x*i is a vector whose entries are the columns of the ![](img/Formula_05_014.png)th
    row and ![](img/Formula_05_015.png) means that the function ![](img/Formula_05_016.png)
    is a member of ![](img/Formula_05_017.png), the set of all possible CART functions.
    **CART** is an acronym for **Classification And Regression Trees**. CART provides
    a real value for all leaves, even for classification algorithms.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*i 是一个向量，其条目为第 ![](img/Formula_05_014.png) 行的列，![](img/Formula_05_015.png)
    表示函数 ![](img/Formula_05_016.png) 是 ![](img/Formula_05_017.png) 的成员，后者是所有可能的 CART
    函数集合。**CART** 是 **分类与回归树**（**Classification And Regression Trees**）的缩写。CART 为所有叶子节点提供了一个实值，即使对于分类算法也是如此。
- en: 'In gradient boosting, the function that determines the prediction for the ![](img/Formula_05_006.png)th
    row includes the sum of all previous functions, as outlined in [*Chapter 4*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093),
    *From Gradient Boosting to XGBoost*. Therefore, it''s possible to write the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度提升中，决定第 ![](img/Formula_05_006.png) 行预测的函数包括所有之前函数的和，如[*第4章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)《从梯度提升到
    XGBoost》中所述。因此，可以写出以下公式：
- en: '![](img/Formula_05_019.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_019.jpg)'
- en: Here, *T* is the number of boosted trees. In other words, to obtain the prediction
    for the ![](img/Formula_05_020.png)th tree, sum the predictions of the previous
    trees in addition to the prediction for the new tree. The notation ![](img/Formula_05_021.png)
    insists that the functions belong to ![](img/Formula_05_022.png), the set of all
    possible CART functions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*T* 是提升树的数量。换句话说，为了获得第 ![](img/Formula_05_020.png) 棵树的预测结果，需要将之前所有树的预测结果与新树的预测结果相加。符号
    ![](img/Formula_05_021.png) 表明这些函数属于 ![](img/Formula_05_022.png)，即所有可能的 CART 函数集合。
- en: 'The learning objective for the ![](img/Formula_05_023.png)th boosted tree can
    now be rewritten as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第 ![](img/Formula_05_023.png) 棵提升树的学习目标现在可以重写如下：
- en: '![](img/Formula_05_024.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_024.jpg)'
- en: Here, ![](img/Formula_05_025.png) is the general loss function of the ![](img/Formula_05_026.png)th
    boosted tree and ![](img/Formula_05_027.png) is the regularization term.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_05_025.png) 是第 ![](img/Formula_05_026.png) 棵提升树的总损失函数，![](img/Formula_05_027.png)
    是正则化项。
- en: Since boosted trees sum the predictions of previous trees, in addition to the
    prediction of the new tree, it must be the case that ![](img/Formula_05_028.png).
    This is the idea behind additive training.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提升树会将之前树的预测结果与新树的预测结果相加，因此必须满足 ![](img/Formula_05_028.png)。这就是加法训练的思想。
- en: 'By substituting this into the preceding learning objective, we obtain the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 将此代入之前的学习目标，我们得到以下公式：
- en: '![](img/Formula_05_029.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_029.png)'
- en: 'This can be rewritten as follows for the least square regression case:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最小二乘回归情况，可以将其重写如下：
- en: '![](img/Formula_05_030.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_030.jpg)'
- en: 'Multiplying the polynomial out, we obtain the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 展开多项式后，我们得到以下公式：
- en: '![](img/Formula_05_031.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_031.jpg)'
- en: Here, ![](img/Formula_05_032.png) is a constant term that does not depend on
    ![](img/Formula_05_033.png). In terms of polynomials, this is a quadratic equation
    with the variable ![](img/Formula_05_034.png). Recall that the goal is to find
    an optimal value of ![](img/Formula_05_035.png), the optimal function mapping
    the roots (samples) to the leaves (predictions).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_05_032.png) 是一个常数项，不依赖于 ![](img/Formula_05_033.png)。从多项式的角度来看，这是一个关于变量
    ![](img/Formula_05_034.png) 的二次方程。请记住，目标是找到一个最优值 ![](img/Formula_05_035.png)，即最优函数，它将根节点（样本）映射到叶节点（预测值）。
- en: 'Any sufficiently smooth function, such as second-degree polynomial (quadratic),
    can be approximated by a **Taylor polynomial**. XGBoost uses Newton''s method
    with a second-degree Taylor polynomial to obtain the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 任何足够光滑的函数，例如二次多项式（quadratic），都可以通过**泰勒多项式**来逼近。XGBoost 使用牛顿法与二次泰勒多项式来得到以下公式：
- en: '![](img/Formula_05_036.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_036.jpg)'
- en: 'Here, ![](img/Formula_05_037.png) and ![](img/Formula_05_038.png) can be written
    as the following partial derivatives:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_05_037.png) 和 ![](img/Formula_05_038.png) 可以写作以下偏导数：
- en: '![](img/Formula_05_039.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_039.png)'
- en: '![](img/Formula_05_040.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_040.png)'
- en: For a general discussion of how XGBoost uses the **Taylor expansion**, check
    out [https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion](https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解 XGBoost 如何使用 **泰勒展开**，请查阅 [https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion](https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion)。
- en: XGBoost implements this learning objective function by taking a solver that
    uses only ![](img/Formula_05_041.png) and ![](img/Formula_05_038.png) as input.
    Since the loss function is general, the same inputs can be used for regression
    and classification.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 通过使用仅需要 ![](img/Formula_05_041.png) 和 ![](img/Formula_05_038.png) 作为输入的求解器来实现这一学习目标函数。由于损失函数是通用的，相同的输入可以用于回归和分类。
- en: This leaves the regularization function, ![](img/Formula_05_043.png).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这留下了正则化函数，![](img/Formula_05_043.png)。
- en: Regularization function
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化函数
- en: 'Let ![](img/Formula_05_044.png) be the vector space of leaves. Then, ![](img/Formula_05_045.png),
    the function mapping the tree root to the leaves, can be recast in terms of ![](img/Formula_05_044.png),
    as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设 ![](img/Formula_05_044.png) 为叶子的向量空间。那么，![](img/Formula_05_045.png)，即将树根映射到叶子的函数，可以用
    ![](img/Formula_05_044.png) 来重新表示，形式如下：
- en: '![](img/Formula_05_047.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_047.png)'
- en: Here, *q* is the function assigning data points to leaves and *T* is the number
    of leaves.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*q* 是将数据点分配给叶子的函数，*T* 是叶子的数量。
- en: 'After practice and experimentation, XGBoost settled on the following as the
    regularization function where ![](img/Formula_05_048.png) and ![](img/Formula_05_049.png)
    are penalty constants to reduce overfitting:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 经过实践和实验，XGBoost 确定了以下作为正则化函数，其中 ![](img/Formula_05_048.png) 和 ![](img/Formula_05_049.png)
    是用于减少过拟合的惩罚常数：
- en: '![](img/Formula_05_050.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_050.png)'
- en: Objective function
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标函数
- en: 'Combining the loss function with the regularization function, the learning
    objective function becomes the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将损失函数与正则化函数结合，学习目标函数变为以下形式：
- en: '![](img/Formula_05_051.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_051.png)'
- en: 'We can define the set of indices of data points assigned to the ![](img/Formula_05_052.png)th
    leaf as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义分配给 ![](img/Formula_05_052.png) 叶子的数据显示点的索引集，如下所示：
- en: '![](img/Formula_05_053.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_053.png)'
- en: 'The objective function can then be written as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数可以写成如下形式：
- en: '![](img/Formula_05_054.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_054.png)'
- en: 'Finally, setting the ![](img/Formula_05_055.png) and ![](img/Formula_05_056.png),
    after rearranging the indices and combining like terms, we obtain the final form
    of the objective function, which is the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过设置 ![](img/Formula_05_055.png) 和 ![](img/Formula_05_056.png)，在重新排列索引并合并相似项后，我们得到目标函数的最终形式，即：
- en: '![](img/Formula_05_057.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_057.png)'
- en: 'Minimizing the objective function by taking the derivative with respect to
    ![](img/Formula_05_058.png) and setting the left side equal to zero, we obtain
    the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对目标函数求导并关于 ![](img/Formula_05_058.png) 令左侧为零，我们得到如下结果：
- en: '![](img/Formula_05_059.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_059.png)'
- en: 'This can be substituted back into the objection function to give the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以代回到目标函数中，得到以下结果：
- en: '![](img/Formula_05_060.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_060.png)'
- en: This is the result XGBoost uses to determine how well the model fits the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 XGBoost 用来确定模型与数据拟合程度的结果。
- en: Congratulations on making it through a long and challenging derivation!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了一个漫长且具有挑战性的推导过程！
- en: Building XGBoost models
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 XGBoost 模型
- en: In the first two sections, you learned how XGBoost works under the hood with
    parameter derivations, regularization, speed enhancements, and new features such
    as the `missing` parameter to compensate for null values.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两部分中，你学习了 XGBoost 如何在底层工作，包括参数推导、正则化、速度增强以及新的特性，如 `missing` 参数用于补偿空值。
- en: In this book, we primarily build XGBoost models with scikit-learn. The scikit-learn
    XGBoost wrapper was released in 2019\. Before full immersion with scikit-learn,
    building XGBoost models required a steeper learning curve. Converting NumPy arrays
    to `dmatrices`, for instance, was mandatory to take advantage of the XGBoost framework.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们主要使用 scikit-learn 构建 XGBoost 模型。scikit-learn 的 XGBoost 封装器在 2019 年发布。在完全使用
    scikit-learn 之前，构建 XGBoost 模型需要更陡峭的学习曲线。例如，必须将 NumPy 数组转换为 `dmatrices`，才能利用 XGBoost
    框架。
- en: In scikit-learn, however, these conversions happen behind the scenes. Building
    XGBoost models in scikit-learn is very similar to building other machine learning
    models in scikit-learn, as you have experienced throughout this book. All standard
    scikit-learn methods, such as `.fit`, and `.predict`, are available, in addition
    to essential tools such as `train_test_split`, `cross_val_score`, `GridSearchCV`,
    and `RandomizedSearchCV`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 scikit-learn 中，这些转换是后台自动完成的。在 scikit-learn 中构建 XGBoost 模型与构建其他机器学习模型非常相似，正如你在本书中所经历的那样。所有标准的
    scikit-learn 方法，如 `.fit` 和 `.predict` 都可以使用，此外还包括如 `train_test_split`、`cross_val_score`、`GridSearchCV`
    和 `RandomizedSearchCV` 等重要工具。
- en: In this section, you will develop templates for building XGBoost models. Going
    forward, these templates can be referenced as starting points for building XGBoost
    classifiers and regressors.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将开发用于构建 XGBoost 模型的模板。以后，这些模板可以作为构建 XGBoost 分类器和回归器的起点。
- en: 'We will build templates for two classic datasets: the **Iris dataset** for
    classification and the **Diabetes dataset** for regression. Both datasets are
    small, built into scikit-learn, and have been tested frequently throughout the
    machine learning community. As part of the model-building process, you will explicitly
    define default hyperparameters that give XGBoost great scores. These hyperparameters
    are explicitly defined so that you can learn what they are in preparation for
    adjusting them going forward.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为两个经典数据集构建模板：用于分类的 **Iris 数据集** 和用于回归的 **Diabetes 数据集**。这两个数据集都很小，内置于 scikit-learn，并且在机器学习社区中经常被测试。作为模型构建过程的一部分，你将显式定义默认的超参数，这些超参数使
    XGBoost 模型得分优秀。这些超参数被显式定义，以便你了解它们是什么，并为将来调整它们做好准备。
- en: The Iris dataset
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Iris 数据集
- en: The Iris dataset, a staple of the machine learning community, was introduced
    by statistician Robert Fischer in 1936\. Its easy accessibility, small size, clean
    data, and symmetry of values have made it a popular choice for testing classification
    algorithms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Iris 数据集，机器学习领域的一个重要数据集，由统计学家 Robert Fischer 于 1936 年引入。它易于访问、数据量小、数据干净，且具有对称的数值，这些特点使其成为测试分类算法的热门选择。
- en: 'We will introduce the Iris dataset by downloading it directly from scikit-learn
    using the `datasets` library with the `load_iris()` method, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用 `datasets` 库中的 `load_iris()` 方法直接从 scikit-learn 下载 Iris 数据集，如下所示：
- en: '[PRE0]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Scikit-learn datasets are stored as `pandas` DataFrames are used more for data
    analysis and data visualization. Viewing NumPy arrays as DataFrames requires the
    `pandas` `DataFrame` method. This scikit-learn dataset is split into predictor
    and target columns in advance. Bringing them together requires concatenating the
    NumPy arrays with the code `np.c_` before conversion. Column names are also added,
    as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 数据集以 `pandas` DataFrame 的形式存储，后者更多用于数据分析和数据可视化。将 NumPy 数组视为 DataFrame
    需要使用 `pandas` 的 `DataFrame` 方法。这个 scikit-learn 数据集在预先划分为预测列和目标列后，合并它们需要用 `np.c_`
    来连接 NumPy 数组，然后再进行转换。列名也会被添加，如下所示：
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can view the first five rows of the DataFrame using `df.head()`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `df.head()` 查看 DataFrame 的前五行：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The resulting DataFrame will look like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果 DataFrame 将如下所示：
- en: '![Figure 5.1 – The Iris dataset](img/B15551_05_01.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – Iris 数据集](img/B15551_05_01.jpg)'
- en: Figure 5.1 – The Iris dataset
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – Iris 数据集
- en: The predictor columns are self-explanatory, measuring sepal and petal length
    and width. The target column, according to the scikit-learn documentation, [https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html),
    consists of three different iris flowers, **setosa**, **versicolor**, and **virginica**.
    There are 150 rows.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 预测列的含义不言自明，分别衡量萼片和花瓣的长度与宽度。目标列根据 scikit-learn 文档， [https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)，包含三种不同的鸢尾花：**setosa**、**versicolor**
    和 **virginica**。数据集包含 150 行。
- en: 'To prepare the data for machine learning, import `train_test_split`, then split
    the data accordingly. You can use the original NumPy arrays, `iris[''data'']`
    and `iris[''target'']`, as inputs for `train_test_split`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备机器学习所需的数据，导入 `train_test_split`，然后相应地划分数据。你可以使用原始的 NumPy 数组 `iris['data']`
    和 `iris['target']` 作为 `train_test_split` 的输入：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have split the data, let's build the classification template.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经划分了数据，接下来让我们构建分类模板。
- en: XGBoost classification template
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost 分类模板
- en: 'The following template is for building an XGBoost classifier, assuming the
    dataset has already been split into `X_train`, `X_test`, `y_train`, and `y_test`
    sets:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模板用于构建 XGBoost 分类器，假设数据集已经分为 `X_train`、`X_test`、`y_train` 和 `y_test`：
- en: 'Import `XGBClassifier` from the `xgboost` library:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `xgboost` 库中导入 `XGBClassifier`：
- en: '[PRE4]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Import a classification scoring method as needed.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要导入分类评分方法。
- en: 'While `accuracy_score` is standard, other scoring methods, such as `auc` (**Area
    Under Curve**), will be discussed later:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然 `accuracy_score` 是标准评分方法，但其他评分方法，如 `auc`（**曲线下的面积**），将在后面讨论：
- en: '[PRE5]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Initialize the XGBoost classifier with hyperparameters.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用超参数初始化 XGBoost 分类器。
- en: 'Fine-tuning hyperparameters is the focus of [*Chapter 6*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136),
    *XGBoost Hyperparameters*. In this chapter, the most important default hyperparameters
    are explicitly stated ahead:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数微调是 [*第 6 章*](B15551_06_Final_NM_ePUB.xhtml#_idTextAnchor136) 的重点，*XGBoost
    超参数*。在本章中，最重要的默认超参数已明确列出：
- en: '[PRE6]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The brief descriptions of the preceding hyperparameters are as follows:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述超参数的简要描述如下：
- en: 'a) `booster=''gbtree''`: The booster is the `''gbtree''` stands for gradient
    boosted tree, the XGBoost default base learner. It''s uncommon but possible to
    work with other base learners, a strategy we employ in [*Chapter 8*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189),
    *XGBoost Alternative Base Learners*.'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `booster='gbtree'`：`booster` 的 `'gbtree'` 代表梯度提升树，是 XGBoost 的默认基础学习器。尽管不常见，但也可以使用其他基础学习器，这是一种我们在
    [*第 8 章*](B15551_08_Final_NM_ePUB.xhtml#_idTextAnchor189) 中使用的策略，*XGBoost 替代基础学习器*。
- en: 'b) `objective=''multi:softprob''`: Standard options for the objective can be
    viewed in the XGBoost official documentation, [https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html),
    under *Learning Task Parameters*. The `multi:softprob` objective is a standard
    alternative to `binary:logistic` when the dataset includes multiple classes. It
    computes the probabilities of classification and chooses the highest one. If not
    explicitly stated, XGBoost will often find the right objective for you.'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `objective='multi:softprob'`：标准的目标选项可以在 XGBoost 官方文档中查看，[https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)，在
    *学习任务参数* 部分。`multi:softprob` 目标是当数据集包含多个类别时，作为 `binary:logistic` 的标准替代方案。它计算分类的概率，并选择最高的一个。如果没有明确指定，XGBoost
    通常会为你找到合适的目标。
- en: 'c) `max_depth=6`: The `max_depth` of a tree determines the number of branches
    each tree has. It''s one of the most important hyperparameters in making balanced
    predictions. XGBoost uses a default of `6`, unlike random forests, which don''t
    provide a value unless explicitly programmed.'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `max_depth=6`：树的 `max_depth` 决定了每棵树的分支数量。它是做出平衡预测时最重要的超参数之一。XGBoost 默认值为
    `6`，不同于随机森林，后者不会提供值，除非明确编程。
- en: 'd) `learning_rate=0.1`: Within XGBoost, this hyperparameter is often referred
    to as `eta`. This hyperparameter limits the variance by reducing the weight of
    each tree to the given percentage. The `learning_rate` hyperparameter was explored
    in detail in [*Chapter 4*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093), *From
    Gradient Boosting to XGBoost*.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `learning_rate=0.1`：在 XGBoost 中，这个超参数通常被称为 `eta`。该超参数通过减少每棵树的权重来限制方差，达到给定的百分比。`learning_rate`
    超参数在 [*第 4 章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093) 中进行了详细探讨，*从梯度提升到
    XGBoost*。
- en: 'e) `n_estimators=100`: Popular among ensemble methods, `n_estimators` is the
    number of boosted trees in the model. Increasing this number while decreasing
    `learning_rate` can lead to more robust results.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) `n_estimators=100`：在集成方法中非常流行，`n_estimators` 是模型中的提升树数量。增加这个数量并降低 `learning_rate`
    可以得到更稳健的结果。
- en: Fit the classifier to the data.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类器拟合到数据上。
- en: 'This is where the magic happens. The entire XGBoost system, the details explored
    in the previous two sections, the selection of optimal parameters, including regularization
    constraints, and speed enhancements, such as the approximate split-finding algorithm,
    and blocking and sharding all occur during this one powerful line of scikit-learn
    code:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是魔法发生的地方。整个 XGBoost 系统，包括前两节中探讨的细节，最佳参数选择，包括正则化约束，以及速度增强，例如近似分裂查找算法，以及阻塞和分片，都会在这行强大的
    scikit-learn 代码中发生：
- en: '[PRE7]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Predict the *y* values as `y_pred`:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *y* 值预测为 `y_pred`：
- en: '[PRE8]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Score the model by comparing `y_pred` against `y_test`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 `y_pred` 与 `y_test` 进行比较来对模型进行评分：
- en: '[PRE9]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Display your results:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示你的结果：
- en: '[PRE10]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Unfortunately, there is no official list of Iris dataset scores. There are too
    many to compile in one place. An initial score of `97.4` percent on the Iris dataset
    using default hyperparameters is very good (see [https://www.kaggle.com/c/serpro-iris/leaderboard](https://www.kaggle.com/c/serpro-iris/leaderboard)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Iris 数据集没有官方的得分列表，得分太多无法在一个地方汇总。使用默认超参数，在 Iris 数据集上的初始得分为 `97.4` 百分比，非常不错（参见
    [https://www.kaggle.com/c/serpro-iris/leaderboard](https://www.kaggle.com/c/serpro-iris/leaderboard)）。
- en: The XGBoost classifier template provided in the preceding paragraphs is not
    meant to be definitive, but rather a starting point going forward.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 前面段落中提供的 XGBoost 分类器模板并非最终版本，而是未来构建模型的起点。
- en: The Diabetes dataset
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 糖尿病数据集
- en: Now that you are becoming familiar with scikit-learn and XGBoost, you are developing
    the ability to build and score XGBoost models fairly quickly. In this section,
    an XGBoost regressor template is provided using `cross_val_score` with scikit-learn's
    Diabetes dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了 scikit-learn 和 XGBoost，你正逐步培养起快速构建和评分 XGBoost 模型的能力。在本节中，提供了一个使用 `cross_val_score`
    的 XGBoost 回归器模板，并应用于 scikit-learn 的糖尿病数据集。
- en: 'Before building the template, import the predictor columns as `X` and the target
    columns as `y`, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模板之前，请导入预测列为 `X`，目标列为 `y`，如下所示：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have imported the predictor and target columns, let's start building
    the template.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已导入预测列和目标列，让我们开始构建模板。
- en: The XGBoost regressor template (cross-validation)
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost 回归器模板（交叉验证）
- en: 'Here are the essential steps to build an XGBoost regression model in scikit-learn
    using cross-validation, assuming that the predictor columns, `X`, and the target
    column, `y`, have been defined:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在 scikit-learn 中使用交叉验证构建 XGBoost 回归模型的基本步骤，假设已定义预测列 `X` 和目标列 `y`：
- en: 'Import `XGBRegressor` and `cross_val_score`:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `XGBRegressor` 和 `cross_val_score`：
- en: '[PRE12]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Initialize `XGBRegressor`.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 `XGBRegressor`。
- en: 'Here, we initialize `XGBRegressor` with `objective=''reg:squarederror''`, the
    MSE. The most important hyperparameter defaults are explicitly given:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们初始化 `XGBRegressor`，并设置 `objective='reg:squarederror'`，即均方误差（MSE）。最重要的超参数默认值已明确给出：
- en: '[PRE13]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Fit and score the regressor with `cross_val_score`.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cross_val_score` 进行回归器的拟合和评分。
- en: 'With `cross_val_score`, fitting and scoring are done in one step using the
    model, the predictor columns, the target column, and the scoring as inputs:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 `cross_val_score`，拟合和评分在一个步骤中完成，输入包括模型、预测列、目标列和评分：
- en: '[PRE14]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Display the results.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果。
- en: 'Scores for regression are commonly displayed as the **Root Mean Squared Error**
    (**RMSE**) to keep the units the same:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回归得分通常以**均方根误差** (**RMSE**) 展示，以保持单位一致：
- en: '[PRE15]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The result is as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE16]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Without a baseline of comparison, we have no idea what that score means. Converting
    the target column, `y`, into a `pandas` DataFrame with the `.describe()` method
    will give the quartiles and the general statistics of the predictor column, as
    follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 没有比较基准，我们无法理解该得分的意义。将目标列 `y` 转换为 `pandas` DataFrame 并使用 `.describe()` 方法将显示预测列的四分位数和一般统计数据，如下所示：
- en: '[PRE17]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the expected output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '![Figure 5.2 – Describing the statistics of y, the Diabetes target column](img/B15551_05_02.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 描述 y，糖尿病目标列的统计数据](img/B15551_05_02.jpg)'
- en: Figure 5.2 – Describing the statistics of y, the Diabetes target column
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 描述 y，糖尿病目标列的统计数据
- en: A score of `63.124` is less than 1 standard deviation, a respectable result.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 得分为 `63.124` 小于 1 个标准差，这是一个可敬的结果。
- en: You now have XGBoost classifier and regressor templates that can be used for
    building models going forward.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你拥有了可以用于构建模型的 XGBoost 分类器和回归器模板。
- en: Now that you are accustomed to building XGBoost models in scikit-learn, it's
    time for a deep dive into high energy physics.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经习惯在 scikit-learn 中构建 XGBoost 模型，是时候深入探索高能物理学了。
- en: Finding the Higgs boson – case study
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找希格斯玻色子 – 案例研究
- en: In this section, we will review the Higgs Boson Kaggle Competition, which brought
    XGBoost into the machine learning spotlight. In order to set the stage, the historical
    background is given before moving on to model development. The models that we
    build include a default model provided by XGBoost at the time of the competition
    and a reference to the winning solution provided by Gabor Melis. Kaggle accounts
    are not required for this text, so we will not take the time to show you how to
    make submissions. We have provided guidelines if you are interested.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾希格斯玻色子 Kaggle 竞赛，该竞赛使 XGBoost 成为机器学习的焦点。为了铺垫背景，在进入模型开发之前，首先提供历史背景。我们构建的模型包括当时
    XGBoost 提供的默认模型和 Gabor Melis 提供的获胜解决方案的参考。本节内容无需 Kaggle 账户，因此我们不会花时间展示如何提交结果。如果你感兴趣，我们已提供了相关指南。
- en: Physics background
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物理背景
- en: In popular culture, the Higgs boson is known as the *God particle*. Theorized
    by Peter Higgs in 1964, the Higgs boson was introduced to explain why particles
    have mass.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在大众文化中，希格斯玻色子被称为*上帝粒子*。该粒子由彼得·希格斯在1964年提出，用于解释为什么粒子具有质量。
- en: The search to find the Higgs boson culminated in its discovery in 2012 in the
    **Large Hadron Collider** at CERN (Geneva, Switzerland). Nobel Prizes were awarded
    and the Standard Model of physics, the model that accounts for every force known
    to physics except for gravity, stood taller than ever before.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找希格斯玻色子的过程最终在2012年通过**大型强子对撞机**在瑞士日内瓦的 CERN 发现达到了高潮。诺贝尔奖颁发了，物理学的标准模型，即解释所有已知物理力（除了重力之外）的模型，变得比以往任何时候都更加重要。
- en: 'The Higgs boson was discovered by smashing protons into each other at extremely
    high speeds and observing the results. Observations came from the **ATLAS** detector,
    which records data resulting from *hundreds of millions of proton-proton collisions
    per second*, according to the competition''s technical documentation, *Learning
    to discover: the Higgs boson machine learning challenge*, [https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '希格斯玻色子是通过以极高速度碰撞质子并观察结果来发现的。观测数据来自**ATLAS**探测器，该探测器记录了*每秒数亿次质子-质子碰撞*所产生的数据，具体内容可以参考竞赛的技术文档《Learning
    to discover: the Higgs boson machine learning challenge》，[https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf)。'
- en: After discovering the Higgs boson, the next step was to precisely measure the
    characteristics of its decay. The ATLAS experiment found the Higgs boson decaying
    into two **tau** particles from data wrapped in background noise. To better understand
    the data, ATLAS called upon the machine learning community.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在发现希格斯玻色子后，下一步是精确测量其衰变特性。ATLAS 实验通过从背景噪声中提取的数据发现希格斯玻色子衰变成两个**tau**粒子。为了更好地理解数据，ATLAS
    寻求了机器学习社区的帮助。
- en: Kaggle competitions
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle 竞赛
- en: The Kaggle competition is a machine learning competition designed to solve a
    particular problem. Machine learning competitions became famous in 2006 when Netflix
    offered 1 million dollars to anyone who could improve upon their movie recommendations
    by 10%. In 2009, the 1 million dollar prize was awarded to *BellKor*'s *Pragmatic
    Chaos* team ([https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/](https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 竞赛是一种旨在解决特定问题的机器学习竞赛。机器学习竞赛在2006年变得有名，当时 Netflix 提供了100万美元奖励给任何能够改进其电影推荐系统10%的人。2009年，100万美元奖金被颁发给了*BellKor*的*Pragmatic
    Chaos*团队 ([https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/](https://www.wired.com/2009/09/bellkors-pragmatic-chaos-wins-1-million-netflix-prize/))。
- en: Many businesses, computer scientists, mathematicians, and students became aware
    of the increasing value that machine learning held in society. Machine learning
    competitions became hot, with mutual benefits going to company hosts and machine
    learning practitioners. Starting in 2010, many early adopters went to Kaggle to
    try their hand at machine learning competitions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 许多企业、计算机科学家、数学家和学生开始意识到机器学习在社会中的日益重要性。机器学习竞赛逐渐火热，企业主和机器学习从业者都从中获得了互利的好处。从2010年开始，许多早期采用者前往
    Kaggle 参与机器学习竞赛。
- en: In 2014, Kaggle announced the *Higgs Boson Machine Learning Challenge* with
    ATLAS ([https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson)).
    With a $13,000 prize pool, 1,875 teams entered the competition.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Kaggle宣布了*希格斯玻色子机器学习挑战赛*与ATLAS合作（[https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson)）。比赛奖金池为13,000美元，共有1,875支队伍参加了比赛。
- en: In Kaggle competitions, training data is provided, along with a required scoring
    method. Teams build machine learning models on the training data before submitting
    their results. The target column of the test data is not provided. Multiple submissions
    are permitted, however, and scores are returned so that competitors can improve
    upon their models before the final date.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle比赛中，提供了训练数据以及所需的评分方法。团队在训练数据上构建机器学习模型，然后提交结果。测试数据的目标列不会提供。然而，允许多次提交，参赛者可以在最终日期之前不断优化自己的模型。
- en: Kaggle competitions are fertile ground for testing machine learning algorithms.
    Unlike in industry, Kaggle competitions draw thousands of competitors, making
    the machine learning models that win prizes very well tested.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle比赛是测试机器学习算法的沃土。与工业界不同，Kaggle比赛吸引了成千上万的参赛者，这使得获奖的机器学习模型经过了非常充分的测试。
- en: XGBoost and the Higgs challenge
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost与希格斯挑战
- en: XGBoost was released to the general public on March 27, 2014, 6 months before
    the Higgs challenge. In the competition, XGBoost soared, helping competitors climb
    the Kaggle leaderboard while saving valuable time.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost于2014年3月27日公开发布，早于希格斯挑战赛6个月。在比赛中，XGBoost大放异彩，帮助参赛者在Kaggle排行榜上攀升，同时节省了宝贵的时间。
- en: Let's access the data to see what the competitors were working with.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们访问数据，看看参赛者们在使用什么数据。
- en: Data
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: 'Instead of using the data provided by Kaggle, we use the original data provided
    by the CERN open data portal where it originated: [http://opendata.cern.ch/record/328](http://opendata.cern.ch/record/328).
    The difference between the CERN data and the Kaggle data is that the CERN dataset
    is significantly larger. We will select the first 250,000 rows and make some modifications
    to match the Kaggle data.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用源自CERN开放数据门户的原始数据，而不是Kaggle提供的数据：[http://opendata.cern.ch/record/328](http://opendata.cern.ch/record/328)。CERN数据与Kaggle数据的区别在于，CERN数据集要大得多。我们将选择前250,000行，并进行一些修改以匹配Kaggle数据。
- en: You can download the CERN Higgs boson dataset directly from [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接从[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter05)下载CERN希格斯玻色子数据集。
- en: 'Read the `atlas-higgs-challenge-2014-v2.csv.gz` file into a `pandas` DataFrame.
    Please note that we are selecting the first 250,000 rows only, and the `compression=gzip`
    parameter is used since the dataset is zipped as a `csv.gz` file. After accessing
    the data, view the first five rows, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 将`atlas-higgs-challenge-2014-v2.csv.gz`文件读取到`pandas`数据框中。请注意，我们仅选择前250,000行，并且使用`compression=gzip`参数，因为数据集是以`csv.gz`文件形式压缩的。读取数据后，查看前五行，如下所示：
- en: '[PRE18]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The far-right columns of the output should be as shown in the following screenshot:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最右边几列应与以下截图所示相同：
- en: '![Figure 5.3 – CERN Higgs boson data – Kaggle columns included](img/B15551_05_03.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – CERN希格斯玻色子数据 – 包含Kaggle列](img/B15551_05_03.jpg)'
- en: Figure 5.3 – CERN Higgs boson data – Kaggle columns included
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – CERN希格斯玻色子数据 – 包含Kaggle列
- en: Notice the `Kaggleset` and `KaggleWeight` columns. Since the Kaggle dataset
    was smaller, Kaggle used a different number for their weight column which is denoted
    in the preceding diagram as `KaggleWeight`. The `t` value under `Kaggleset` indicates
    that it's part of the training set for the Kaggle dataset. In other words, these
    two columns, `Kaggleset` and `KaggleWeight`, are columns in the CERN dataset designed
    to include information that will be used for the Kaggle dataset. In this chapter,
    we will restrict our subset of the CERN data to the Kaggle training set.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`Kaggleset`和`KaggleWeight`列。由于Kaggle数据集较小，Kaggle在其权重列中使用了不同的数字，该列在前面的图中表示为`KaggleWeight`。`Kaggleset`下的`t`值表示它是Kaggle数据集的训练集的一部分。换句话说，这两列，`Kaggleset`和`KaggleWeight`，是CERN数据集中的列，用于包含将被用于Kaggle数据集的信息。在本章中，我们将限制CERN数据的子集为Kaggle训练集。
- en: 'To match the Kaggle training data, let''s delete the `Kaggleset` and `Weight`
    columns, convert `KaggleWeight` into `''Weight''`, and move the `''Label''` column
    to the last column, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了匹配Kaggle训练数据，我们将删除`Kaggleset`和`Weight`列，将`KaggleWeight`转换为`'Weight'`，并将`'Label'`列移到最后一列，如下所示：
- en: '[PRE19]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'One way to move the `Label` column is to store it as a variable, delete the
    column, and add a new column by assigning it to the new variable. Whenever assigning
    a new column to a DataFrame, the new column appears at the end:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一种移动`Label`列的方法是将其存储为一个变量，删除该列，然后通过将其分配给新变量来添加新列。每当将新列分配给DataFrame时，新列会出现在末尾：
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now that all changes have been made, the CERN data matches the Kaggle data.
    Go ahead and view the first five rows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有更改已经完成，CERN数据与Kaggle数据一致。接下来，查看前五行数据：
- en: '[PRE21]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the left side of the expected output:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是期望输出的左侧部分：
- en: '![Figure 5.4 – CERN Higgs boson data – physics columns](img/B15551_05_04.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – CERN 希格斯玻色子数据 – 物理列](img/B15551_05_04.jpg)'
- en: Figure 5.4 – CERN Higgs boson data – physics columns
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – CERN 希格斯玻色子数据 – 物理列
- en: Many columns are not shown, and an unusual value of `-999.00` occurs in multiple
    places.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 许多列没有显示，并且出现了`-999.00`这一不寻常的值。
- en: The columns beyond `EventId` include variables prefixed with `PRI`, which stands
    for *primitives*, which are values directly measured by the detector during collisions.
    By contrast, columns labeled `DER` are numerical derivations from these measurements.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`EventId`之后的列包含以`PRI`为前缀的变量，`PRI`代表*原始值*，即在碰撞过程中由探测器直接测量的值。相比之下，标记为`DER`的列是从这些测量值中得出的数值推导。'
- en: 'All column names and types are revealed by `df.info()`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 所有列名和类型可以通过`df.info()`查看：
- en: '[PRE22]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is a sample of the output, with the middle columns truncated to save space:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出的一个示例，中间的列已被截断以节省空间：
- en: '[PRE23]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'All columns have non-null values, and only the final column, `Label`, is non-numerical.
    The columns can be grouped as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 所有列都有非空值，只有最后一列`Label`是非数字类型。列可以按如下方式分组：
- en: 'Column `0` : `EventId` – irrelevant for the machine learning model.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列 `0`：`EventId` – 对于机器学习模型无关。
- en: 'Columns `1-30`: Physics columns derived from LHC collisions. Details for these
    columns can be found in the link to the technical documentation at [http://higgsml.lal.in2p3.fr/documentation](http://higgsml.lal.in2p3.fr/documentation).
    These are the machine learning predictor columns.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列 `1-30`：来自LHC碰撞的物理列。这些列的详细信息可以在[http://higgsml.lal.in2p3.fr/documentation](http://higgsml.lal.in2p3.fr/documentation)中的技术文档链接中找到。这些是机器学习的预测列。
- en: 'Column `31` : `Weight` – this column is used to scale the data. The issue here
    is that Higgs boson events are very rare, so a machine learning model with 99.9
    percent accuracy may not be able to find them. Weights compensate for this imbalance,
    but weights are not available for the test data. Strategies for dealing with weights
    will be discussed later in this chapter, and in [*Chapter 7*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161),
    *Discovering Exoplanets with XGBoost*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列 `31`：`Weight` – 该列用于对数据进行缩放。问题在于希格斯玻色子事件非常稀有，因此一个99.9%准确率的机器学习模型可能无法找到它们。权重弥补了这一不平衡，但测试数据中没有权重。处理权重的策略将在本章后续部分讨论，并在[*第7章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161)中讨论，*使用XGBoost发现外星行星*。
- en: 'Column `32`: `Label` – this is the target column, labeled `s` for signal and
    `b` for background. The training data has been simulated from real data, so there
    are many more signals than otherwise would be found. The signal is the occurrence
    of the Higgs boson decay.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列 `32`：`Label` – 这是目标列，标记为`s`表示信号，`b`表示背景。训练数据是从实际数据模拟而来，因此信号比实际情况下更多。信号是指希格斯玻色子衰变的发生。
- en: 'The only issue with the data is that the target column, `Label`, is not numerical.
    Convert the `Label` column into a numerical column by replacing the `s` values
    with `1` and the `b` values with `0`, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的唯一问题是目标列`Label`不是数字类型。通过将`s`值替换为`1`，将`b`值替换为`0`，将`Label`列转换为数字列，如下所示：
- en: '[PRE24]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now that all columns are numerical with non-null values, you can split the
    data into predictor and target columns. Recall that the predictor columns are
    indexed 1–30 and the target column is the last column, indexed `32` (or -1). Note
    that the `Weight` column should not be included because it''s not available for
    the test data:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有列都变为数字类型并且没有空值，你可以将数据拆分为预测列和目标列。回顾一下，预测列的索引是1–30，目标列是最后一列，索引为`32`（或 -1）。注意，`Weight`列不应包含在内，因为测试数据中没有该列：
- en: '[PRE25]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Scoring
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打分
- en: The Higgs Challenge is not your average Kaggle competition. In addition to the
    difficulty of understanding high energy physics for feature engineering (a route
    we will not pursue), the scoring method is not standard. The Higgs Challenge requires
    optimizing the **Approximate Median Significance** (**AMS**).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Higgs 挑战不是普通的 Kaggle 竞赛。除了理解高能物理以进行特征工程（这不是我们将要追求的路径）的难度外，评分方法也不是标准的。Higgs 挑战要求优化
    **近似中位数显著性** (**AMS**)。
- en: 'The AMS is defined as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: AMS 的定义如下：
- en: '![](img/Formula_05_061.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_061.png)'
- en: Here, ![](img/Formula_05_062.png) is the true positive rate, ![](img/Formula_05_063.png)
    is the false positive rate, and ![](img/Formula_05_064.png) is a constant regularization
    term given as `10`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_05_062.png) 是真阳性率，![](img/Formula_05_063.png) 是假阳性率，而 ![](img/Formula_05_064.png)
    是一个常数正则化项，其值为 `10`。
- en: Fortunately, XGBoost provided an AMS scoring method for the competition, so
    it does not need to be formally defined. A high AMS results from many true positives
    and few false negatives. Justification for the AMS instead of other scoring methods
    is given in the technical documentation at [http://higgsml.lal.in2p3.fr/documentation](http://higgsml.lal.in2p3.fr/documentation).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，XGBoost 为比赛提供了一种 AMS 评分方法，因此不需要正式定义。高 AMS 的结果来自许多真阳性和很少的假阴性。关于为何选择 AMS
    而不是其他评分方法的理由在技术文档中说明，地址为 [http://higgsml.lal.in2p3.fr/documentation](http://higgsml.lal.in2p3.fr/documentation)。
- en: Tip
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: It's possible to build your own scoring methods, but it's not usually needed.
    In the rare event that you need to build your own scoring method, you can check
    out [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)
    for more information.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 可以构建自己的评分方法，但通常不需要。在极少数需要构建自定义评分方法的情况下，你可以查看 [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)
    获取更多信息。
- en: Weights
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重
- en: Before building a machine learning model for the Higgs boson, it's important
    to understand and utilize weights.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 Higgs 玻色子的机器学习模型之前，了解并利用权重非常重要。
- en: In machine learning, weights can be used to improve the accuracy of imbalanced
    datasets. Consider the `s` (signal) and `b` (background) columns in the Higgs
    challenge. In reality, `s` << `b`, so signals are very rare among the background
    noise. Let's say, for example, that signals are 1,000 times rarer than background
    noise. You can create a weight column where `b` = 1 and `s` = 1/1000 to compensate
    for this imbalance.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，权重可以用来提高不平衡数据集的准确性。考虑 Higgs 挑战中的 `s`（信号）和 `b`（背景）列。实际上，`s` << `b`，所以信号在背景噪声中非常稀少。例如，信号比背景噪声少
    1,000 倍。你可以创建一个权重列，其中 `b` = 1，`s` = 1/1000 以补偿这种不平衡。
- en: According to the technical documentation of the competition, the weight column
    is a `s` (signal) events.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 根据比赛的技术文档，权重列定义为 `s`（信号）事件。
- en: 'The weights should first be scaled to match the test data since the test data
    provides the expected number of signal and background events generated by the
    test set. The test data has 550,000 rows, more than twice the 250,000 rows (`len(y)`)
    provided by the training data. Scaling weights to match the test data can be achieved
    by multiplying the weight column by the percentage of increase, as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先应将权重按比例缩放以匹配测试数据，因为测试数据提供了测试集生成的信号和背景事件的预期数量。测试数据有 550,000 行，比训练数据提供的 250,000
    行（`len(y)`）多两倍以上。将权重按比例缩放以匹配测试数据可以通过将权重列乘以增加百分比来实现，如下所示：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, XGBoost provides a hyperparameter, `scale_pos_weight`, which takes the
    scaling factor into account. The scaling factor is the sum of the weights of the
    background noises divided by the sum of the weight of the signal. The scaling
    factor can be computed using `pandas` conditional notation, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，XGBoost 提供了一个超参数 `scale_pos_weight`，它考虑了缩放因子。缩放因子是背景噪声权重之和除以信号权重之和。可以使用
    `pandas` 的条件符号来计算缩放因子，如下所示：
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding code, `df[df['Label']==1]` narrows the DataFrame down to rows
    where the `Label` column equals `1`, then `np.sum` adds the values of these rows
    using the `test_Weight` column.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`df[df['Label']==1]` 缩小了 DataFrame 到 `Label` 列等于 `1` 的行，然后 `np.sum` 使用
    `test_Weight` 列的值加总这些行的值。
- en: 'Finally, to see the actual rate, divide `b` by `s`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要查看实际比率，将 `b` 除以 `s`：
- en: '[PRE28]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In summary, the weights represent the expected number of signal and background
    events generated by the data. We scale the weights to match the size of the test
    data, then divide the sum of the background weights by the sum of the signal weights
    to establish the `scale_pos_weight=b/s` hyperparameter.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，权重代表数据生成的预期信号和背景事件的数量。我们将权重缩放以匹配测试数据的大小，然后将背景权重之和除以信号权重之和，以建立 `scale_pos_weight=b/s`
    超参数。
- en: Tip
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: For a more detailed discussion on weights, check out the excellent introduction
    from KDnuggets at [https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html](https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关权重的更多详细讨论，请查看 KDnuggets 提供的精彩介绍：[https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html](https://www.kdnuggets.com/2019/11/machine-learning-what-why-how-weighting.html)。
- en: The model
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: It's time to build an XGBoost model to predict the signal – that is, the simulated
    occurrences of the Higgs boson decay.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候构建一个 XGBoost 模型来预测信号——即模拟的希格斯玻色子衰变事件。
- en: At the time of the competition, XGBoost was new, and the scikit-learn wrapper
    was not yet available. Even today (2020), the majority of information online about
    implementing XGBoost in Python is pre-scikit-learn. Since you are likely to encounter
    the pre-scikit-learn XGBoost Python API online, and this is what all competitors
    used in the Higgs Challenge, we present code using the original Python API in
    this chapter only.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在比赛开始时，XGBoost 是一种新工具，且 scikit-learn 的封装还没有发布。即便到了今天（2020年），关于在 Python 中实现 XGBoost
    的大多数信息仍然是在 scikit-learn 发布之前的版本。由于你可能会在线遇到 pre-scikit-learn 版本的 XGBoost Python
    API，而且这正是所有参与者在 Higgs Challenge 中使用的版本，因此本章只展示了使用原始 Python API 的代码。
- en: 'Here are the steps to build an XGBoost model for the Higgs Challenge:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为 Higgs Challenge 构建 XGBoost 模型的步骤：
- en: 'Import `xgboost` as `xgb`:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `xgboost` 为 `xgb`：
- en: '[PRE29]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Initialize the XGBoost model as a `-999.0` are unknown values. Instead of converting
    these values into the median, mean, mode, or other null replacement, in XGBoost,
    unknown values can be set to the `missing` hyperparameter. During the model build
    phase, XGBoost automatically chooses the value leading to the best split.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 XGBoost 模型时，将 `-999.0` 设置为未知值。在 XGBoost 中，未知值可以通过 `missing` 超参数来设置，而不是将这些值转换为中位数、均值、众数或其他空值替代。在模型构建阶段，XGBoost
    会自动选择最佳的拆分值。
- en: 'The `weight` hyperparameter can equal the new column, `df[''test_Weight'']`,
    as defined in the `weight` section:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`weight` 超参数可以等于新列 `df[''test_Weight'']`，如 `weight` 部分所定义：'
- en: '[PRE30]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Set additional hyperparameters.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置其他超参数。
- en: 'The hyperparameters that follow are defaults provided by XGBoost for the competition:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下超参数是 XGBoost 为竞赛提供的默认值：
- en: 'a) Initialize a blank dictionary called `param`:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 初始化一个名为 `param` 的空字典：
- en: '[PRE31]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: b) Define the objective as `'binary:logitraw'`.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 将目标定义为 `'binary:logitraw'`。
- en: 'This means a binary model is created from logistic regression probabilities.
    This objective defines the model as a classifier and allows a ranking of the target
    column, which is required of submissions for this particular Kaggle competition:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这意味着一个二分类模型将从逻辑回归概率中创建。这个目标将模型定义为分类器，并允许对目标列进行排序，这是此特定 Kaggle 比赛提交所要求的：
- en: '[PRE32]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'c) Scale the positive examples using the background weights divided by the
    signal weights. This will help the model perform better on the test set:'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 使用背景权重除以信号权重对正样本进行缩放。这有助于模型在测试集上表现得更好：
- en: '[PRE33]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'd) The learning rate, `eta`, is given as `0.1`:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 学习率 `eta` 设置为 `0.1`：
- en: '[PRE34]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'e) `max_depth` is given as `6`:'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) `max_depth` 设置为 `6`：
- en: '[PRE35]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'f) Set the scoring method as `''auc''` for display purposes:'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 将评分方法设置为 `'auc'`，以便显示：
- en: '[PRE36]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Although the AMS score will be printed, the evaluation metric is given as `auc`,
    which stands for `auc` is the true positive versus false positive curve that is
    perfect when it equals `1`. Similar to accuracy, `auc` is a standard scoring metric
    for classification, although it's often superior to accuracy since accuracy is
    limited for imbalanced datasets, as discussed in [*Chapter 7*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161),
    *Discovering Exoplanets with XGBoost*.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然会打印 AMS 分数，但评估指标设置为 `auc`，即 `auc` 是描述真正例与假正例的曲线，当其值为 `1` 时是完美的。与准确率类似，`auc`
    是分类问题的标准评分指标，尽管它通常优于准确率，因为准确率对于不平衡数据集来说有局限性，正如在[*第7章*](B15551_07_Final_NM_ePUB.xhtml#_idTextAnchor161)《使用
    XGBoost 发现系外行星》中所讨论的那样，*难以遗忘*。
- en: 'Create a list of parameters that includes the preceding items, along with the
    evaluation metric (`auc`) and `ams@0.15`, XGBoost''s implementation of the AMS
    score using a 15% threshold:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个参数列表，包含前面提到的内容，并加上评估指标（`auc`）和`ams@0.15`，XGBoost实现的使用15%阈值的AMS分数：
- en: '[PRE37]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create a watchlist that includes the initialized classifier and `''train''`
    so that you can view scores as the trees continue to boost:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个观察列表，包含初始化的分类器和`'train'`，这样你就可以在树继续提升的过程中查看分数：
- en: '[PRE38]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Set the number of boosting rounds to `120`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将提升轮次设置为`120`：
- en: '[PRE39]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Train and save the model. Train the model by placing the parameter list, the
    classifier, the number of rounds, and the watchlist as inputs. Save the model
    using the `save_model` method so that you do not have to go through a time-consuming
    training process a second time. Then, run the code and watch how the scores improve
    as the trees are boosted:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练并保存模型。通过将参数列表、分类器、轮次和观察列表作为输入来训练模型。使用`save_model`方法保存模型，这样你就不必再经过耗时的训练过程。然后，运行代码并观察随着树的提升，分数如何提高：
- en: '[PRE40]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The end of your results should have the following output:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的结果应该包括以下输出：
- en: '[PRE41]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Congratulations on building an XGBoost classifier that can predict Higgs boson
    decay!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你构建了一个能够预测希格斯玻色子衰变的XGBoost分类器！
- en: The model performs with `94.58` percent `auc`, and an AMS of `5.9`. As far as
    the AMS is concerned, the top values of the competition were in the upper threes.
    This model achieves an AMS of around `3.6` when submitted with the test data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的`auc`为`94.58`百分比，AMS为`5.9`。就AMS而言，竞赛的最高值在三分之三的上方。该模型在提交测试数据时，AMS大约为`3.6`。
- en: The model that you just built was provided as a baseline by Tanqi Chen for XGBoost
    users during the competition. The winner of the competition, Gabor Melis, used
    this baseline to build his model. As can be seen from viewing the winning solution
    at [https://github.com/melisgl/higgsml](https://github.com/melisgl/higgsml) and
    clicking on **xgboost-scripts**, changes made to the baseline model are not significant.
    Melis, like most Kaggle competitors, also performed feature engineering to add
    more relevant columns to the data, a practice we will address in [*Chapter 9*](B15551_09_Final_NM_ePUB.xhtml#_idTextAnchor211),
    *XGBoost Kaggle Masters*.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚构建的模型是由Tanqi Chen为XGBoost用户在竞赛期间提供的基准模型。竞赛的获胜者，Gabor Melis，使用这个基准模型来构建他的模型。从查看获胜解决方案[https://github.com/melisgl/higgsml](https://github.com/melisgl/higgsml)并点击**xgboost-scripts**可以看出，对基准模型所做的修改并不显著。Melis和大多数Kaggle参赛者一样，也进行了特征工程，向数据中添加了更多相关列，这是我们将在[*第9章*](B15551_09_Final_NM_ePUB.xhtml#_idTextAnchor211)中讨论的内容，*XGBoost
    Kaggle大师*。
- en: It is possible to build and train your own model after the deadline and submit
    it through Kaggle. For Kaggle competitions, submissions must be ranked, properly
    indexed, and delivered with the Kaggle API topics that require further explanation.
    If you want to submit models for the actual competition, the XGBoost ranking code,
    which you may find helpful, is available at [https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py](https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在截止日期之后，你是可以自己构建和训练模型，并通过Kaggle提交的。对于Kaggle竞赛，提交必须经过排名、正确索引，并且必须使用Kaggle API主题进行进一步说明。如果你希望提交实际竞赛的模型，XGBoost排名代码对你可能有所帮助，可以在[https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py](https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-pred.py)找到。
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how XGBoost was designed to improve the accuracy
    and speed of gradient boosting with missing values, sparse matrices, parallel
    computing, sharding, and blocking. You learned the mathematical derivation behind
    the XGBoost objective function that determines the parameters for gradient descent
    and regularization. You built `XGBClassifier` and `XGBRegressor` templates from
    classic scikit-learn datasets, obtaining very good scores. Finally, you built
    the baseline model provided by XGBoost for the Higgs Challenge that led to the
    winning solution and lifted XGBoost into the spotlight.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学习了XGBoost是如何通过处理缺失值、稀疏矩阵、并行计算、分片和块等技术，来提高梯度提升的准确性和速度。你学习了XGBoost目标函数背后的数学推导，该函数决定了梯度下降和正则化的参数。你使用经典的scikit-learn数据集构建了`XGBClassifier`和`XGBRegressor`模板，获得了非常好的分数。最后，你构建了XGBoost为希格斯挑战提供的基准模型，这个模型最终引领了获胜解决方案，并将XGBoost推到了聚光灯下。
- en: Now that you have a solid understanding of the overall narrative, design, parameter
    selection, and model-building templates of XGBoost, in the next chapter, you will
    fine-tune XGBoost's hyperparameters to achieve optimal scores.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经对XGBoost的整体叙述、设计、参数选择和模型构建模板有了扎实的理解，在下一章中，你将微调XGBoost的超参数，以达到最佳评分。
