- en: '*Chapter 7*: Advanced Feature Extraction with NLP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：使用NLP的高级特征提取'
- en: In the previous chapters, we learned about many standard transformation and
    preprocessing approaches within the Azure Machine Learning service as well as
    typical labeling techniques using the Azure Machine Learning Data Labeling service.
    In this chapter, we want to go one step further to extract semantic features from
    textual and categorical data—a problem that users often face when training ML
    models. This chapter will describe the foundations of feature extraction with
    **Natural Language Processing** (**NLP**). This will help you to practically implement
    semantic embeddings using NLP for your ML pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了在Azure机器学习服务中许多标准的转换和预处理方法，以及使用Azure机器学习数据标注服务进行典型标注技术的应用。在本章中，我们希望更进一步，从文本和分类数据中提取语义特征——这是用户在训练机器学习模型时经常遇到的问题。本章将描述使用**自然语言处理**（**NLP**）进行特征提取的基础。这将帮助您在实际的机器学习管道中实现使用NLP的语义嵌入。
- en: First, we will take a look at the differences between *textual*, *categorical*,
    *nominal*, and *ordinal* data. This classification will help you to decide the
    best feature extraction and transformation technique per feature type. Later,
    we will look at the most common transformations for categorical values, namely
    **label encoding** and **one-hot encoding**. Both techniques will be compared
    and tested to understand the different use cases and applications for both techniques.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨**文本**、**分类**、**名义**和**有序**数据之间的差异。这种分类将帮助您根据特征类型决定最佳的特征提取和转换技术。稍后，我们将查看分类值最常见的转换方法，即**标签编码**和**独热编码**。这两种技术将被比较和测试，以了解这两种技术的不同用例和应用。
- en: Next, we will tackle the numerical embedding of textual data. To achieve this,
    we will build a simple **bag-of-words** model, using a **count vectorizer**. To
    sanitize the input, we will build an NLP pipeline consisting of a **tokenizer**,
    stop word removal, **stemming**, and **lemmatization**. We will learn how these
    different techniques affect a sample dataset step by step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将处理文本数据的数值嵌入。为了实现这一点，我们将构建一个简单的**词袋**模型，使用**计数向量器**。为了净化输入，我们将构建一个包含**分词器**、停用词去除、**词干提取**和**词形还原**的NLP管道。我们将逐步学习这些不同的技术如何影响样本数据集。
- en: Following this, we will replace the word count method with a much better word
    frequency weighting approach—the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) algorithm. This will help you to compute the importance of words
    when given a whole corpus of documents by weighting the occurrence of a term in
    one document over the frequency in the corpus. Additionally, we will look at **Singular
    Value Decomposition** (**SVD**) for reducing the size of the term dictionary.
    As a next step, we will improve the term embedding quality by leveraging word
    semantics, and we will look under the hood of semantic embeddings such as **Global
    Vectors** (**GloVe**) and **Word2Vec**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，我们将用一种更好的词频加权方法——**词频-逆文档频率**（**TF-IDF**）算法来替换词计数方法。这将帮助您在给定整个文档集合的情况下，通过加权一个文档中术语的出现频率相对于文档集合中的频率来计算单词的重要性。此外，我们将探讨**奇异值分解**（**SVD**）以减少术语字典的大小。作为下一步，我们将通过利用词义来提高术语嵌入的质量，并深入了解语义嵌入，如**全局向量**（**GloVe**）和**Word2Vec**。
- en: In the last section, we will take a look at current state-of-the-art language
    models that are based on sequence-to-sequence deep neural networks with over 100
    million parameters. We will train a small end-to-end model using **Long Short-Term
    Memory** (**LSTM**), perform word embedding and sentiment analysis using **Bidirectional
    Encoder Representations from Transformers** (**BERT**), and compare both custom
    solutions to Azure's text analytics capabilities in Cognitive Services.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节，我们将探讨基于序列到序列深度神经网络且超过一亿参数的当前最先进的语言模型。我们将使用**长短期记忆**（**LSTM**）训练一个小的端到端模型，使用**双向编码器表示从Transformer**（**BERT**）进行词嵌入和情感分析，并将这两种自定义解决方案与Azure认知服务中的文本分析能力进行比较。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding categorical data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类数据
- en: Building a simple bag-of-words model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建简单的词袋模型
- en: Leveraging term importance and semantics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用术语重要性和语义
- en: Implementing end-to-end language models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现端到端语言模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create categorical encodings, create semantic embeddings, train an end-to-end
    model, and perform classic NLP preprocessing steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建分类编码、创建语义嵌入、训练端到端模型以及执行经典的NLP预处理步骤：
- en: '`azureml-sdk 1.34.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: '`azureml-widgets 1.34.0`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-widgets 1.34.0`'
- en: '`tensorflow 2.6.0`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow 2.6.0`'
- en: '`numpy 1.19.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy 1.19.5`'
- en: '`pandas 1.3.2`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.2`'
- en: '`scikit-learn 0.24.2`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn 0.24.2`'
- en: '`nltk 3.6.2`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk 3.6.2`'
- en: '`genism 3.8.3`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gensim 3.8.3`'
- en: Similar to previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，您可以使用本地Python解释器或Azure Machine Learning中托管的笔记本环境来执行此代码。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07)。
- en: Understanding categorical data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类数据
- en: '**Categorical data** comes in many forms, shapes, and meanings. It is extremely
    important to understand what type of data you are dealing with—is it a string,
    text, or numeric value disguised as a categorical value? This information is essential
    for data preprocessing, feature extraction, and model selection.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类数据**以多种形式、形状和意义存在。了解你正在处理的数据类型至关重要——它是一个字符串、文本还是伪装成分类值的数值？这些信息对于数据预处理、特征提取和模型选择至关重要。'
- en: In this section, first, we will take a look at the different types of categorical
    data—namely *ordinal*, *nominal*, and *text*. Depending on the type, you can use
    different methods to extract information or other valuable data from it. Please
    bear in mind that categorical data is ubiquitous, whether it is in an ID column,
    a nominal category, an ordinal category, or a free-text field. It's worth mentioning
    that the more information you have on the data, the easier the preprocessing is.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先，我们将查看不同类型的分类数据——即*顺序*、*名义*和*文本*。根据类型，你可以使用不同的方法从中提取信息或其他有价值的数据。请记住，分类数据无处不在，无论是ID列、名义类别、顺序类别还是自由文本字段。值得一提的是，你对数据的了解越多，预处理就越容易。
- en: Next, we will actually preprocess the ordinal and nominal categorical data by
    transforming it into numerical values. This is a required step when you want to
    use an ML algorithm later on that can't interpret categorical data, which is true
    for most algorithms except, for example, decision tree-based approaches. Most
    other algorithms can only operate (for example, compute a loss function) on a
    numeric value and so a transformation is required.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过将其转换为数值来实际预处理顺序和名义分类数据。当你想要使用不能解释分类数据的机器学习算法时，这是一个必要的步骤，这对于大多数算法都是真实的，例如基于决策树的算法。大多数其他算法只能对数值值进行操作（例如，计算损失函数），因此需要进行转换。
- en: Comparing textual, categorical, and ordinal data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较文本、分类和顺序数据
- en: Many ML algorithms, such as support vector machines, neural networks, linear
    regression, and more, can only be applied to numeric data. However, in real-world
    datasets, we often find non-numeric columns, such as columns that contain textual
    data. The goal of this chapter is to transform textual data into numeric data
    as an advanced feature extraction step, which allows us to plug the processed
    data into any ML algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法，如支持向量机、神经网络、线性回归等，只能应用于数值数据。然而，在现实世界的数据集中，我们经常发现非数值列，例如包含文本数据的列。本章的目标是将文本数据转换为数值数据，作为高级特征提取步骤，这样我们就可以将处理后的数据插入到任何机器学习算法中。
- en: 'When working with real-world data, you will be confronted with many different
    types of textual and/or categorical data. To optimize ML algorithms, you need
    to understand the differences in order to apply different preprocessing techniques
    to the different types. But first, let''s define the three different textual data
    types:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理现实世界数据时，你将面临许多不同类型的文本和/或分类数据。为了优化机器学习算法，你需要了解这些差异，以便对不同的类型应用不同的预处理技术。但首先，让我们定义三种不同的文本数据类型：
- en: '*Textual data*: Free text'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本数据*：自由文本'
- en: '*Categorical nominal data*: Non-orderable categories'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类名义数据*：不可排序的类别'
- en: '*Categorical ordinal data*: Orderable categories'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有序类别数据*：可排序的类别'
- en: The difference between textual data and categorical data is that, in textual
    data, we want to capture semantic similarities (that is, the similarity in the
    meaning of the words), whereas, in categorical data, we want to differentiate
    between a small number of variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据和类别数据之间的区别在于，在文本数据中，我们想要捕捉语义相似性（即词语的意义相似性），而在类别数据中，我们想要区分少数几个变量。
- en: The difference between categorical nominal data and categorical ordinal data
    is that nominal data cannot be ordered (all categories have the same weight),
    whereas ordinal categories can be logically ordered on an ordinal scale.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有序类别数据和有序类别数据之间的区别在于，名义数据不能排序（所有类别具有相同的权重），而有序类别可以在有序尺度上逻辑排序。
- en: '*Figure 7.1* shows an example dataset of comments on news articles, where the
    first column, named `statement`, is a textual field, the column named `topic`
    is a nominal category, and `rating` is an ordinal category:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1* 展示了一个新闻文章评论的示例数据集，其中第一列，命名为 `statement`，是一个文本字段，名为 `topic` 的列是一个名义类别，而
    `rating` 是一个有序类别：'
- en: '![Figure 7.1 – Comparing different textual data types ](img/B17928_07_001.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 比较不同的文本数据类型](img/B17928_07_001.jpg)'
- en: Figure 7.1 – Comparing different textual data types
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 比较不同的文本数据类型
- en: Understanding the differences between these data representations is essential
    to find the proper embedding technique afterward. It seems quite natural to replace
    ordinal categories with an ordinal numeric scale and to embed nominal categories
    in an orthogonal space. On the contrary, it's not obvious how to embed textual
    data into a numerical space where the semantics are preserved—this will be covered
    in the later sections of this chapter that deal with NLP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些数据表示之间的差异对于之后找到适当的嵌入技术至关重要。用有序数值尺度替换有序类别似乎很自然，将名义类别嵌入到正交空间中。相反，将文本数据嵌入到保留语义的数值空间中并不明显——这将在本章后面的部分中介绍，这部分内容涉及NLP。
- en: 'Please note that instead of categorical values, you will also see continuous
    numeric variables representing categorical information, for example, IDs from
    a dimension or lookup table. Although these are numeric values, you should consider
    treating them as categorical nominal values, if possible. Here is an example dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了类别值之外，你还会看到表示类别信息的连续数值变量，例如来自维度或查找表的ID。尽管这些是数值，但如果可能的话，你应该考虑将它们作为类别名义值处理。以下是一个示例数据集：
- en: '![Figure 7.2 – Comparing numerical categorical values ](img/B17928_07_002.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 比较数值类别值](img/B17928_07_002.jpg)'
- en: Figure 7.2 – Comparing numerical categorical values
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 比较数值类别值
- en: In this example, we can see that the `sensorId` value is a numeric value that
    should be interpreted as a categorical nominal value instead of a numeric value
    by default because it doesn't have a numeric meaning. What do you get when you
    subtract `sensorId` `2` from `sensorId` `1`? Is `sensorId` `10` 10 times larger
    than `sensorId` `1`? These are the typical questions to ask to discover and encode
    these categorical values. We will discover, in [*Chapter 9*](B17928_09_ePub.xhtml#_idTextAnchor152),
    *Building ML Models Using Azure Machine Learning*, that by specifying that these
    values are categorical, a gradient-boosted tree model can optimize these features
    instead of treating them as continuous variables.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以看到 `sensorId` 值是一个数值，应该将其解释为类别名义值，而不是默认的数值，因为它没有数值意义。当你从 `sensorId`
    `1` 减去 `sensorId` `2` 时，你得到什么？`sensorId` `10` 是 `sensorId` `1` 的10倍大吗？这些问题是发现和编码这些类别值的典型问题。我们将在
    [*第9章*](B17928_09_ePub.xhtml#_idTextAnchor152)，*使用Azure机器学习构建ML模型* 中发现，通过指定这些值是类别数据，梯度提升树模型可以优化这些特征，而不是将它们作为连续变量处理。
- en: Transforming categories into numeric values
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将类别转换为数值
- en: 'Let''s start by converting categorical variables (both ordinal and nominal)
    into numeric values. In this section, we will look at two common techniques for
    categorical encoding: **label encoding** and **one-hot encoding** (also called
    *dummy coding*). While **label encoding** replaces a categorical feature column
    with a numerical feature column, **one-hot encoding** uses multiple columns (where
    the number of columns equals the number of unique values) to encode a single feature.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从将分类变量（序数和名义）转换为数值开始。在本节中，我们将探讨两种常见的分类编码技术：**标签编码**和**独热编码**（也称为*虚拟编码*）。虽然**标签编码**用一个数值特征列替换分类特征列，**独热编码**则使用多个列（列的数量等于唯一值的数量）来编码一个单一特征。
- en: Both techniques are applied in the same way. During the training iteration,
    these techniques find all of the unique values in a feature column and assign
    them a specific numeric value (multidimensional value for one-hot encoding). As
    a result, a lookup dictionary defining this replacement is stored in the encoder.
    When the encoder is applied, the values in the applied column are transformed
    (replaced) using the lookup dictionary. If the list of possible values is known
    beforehand, most implementations allow the encoder to initialize the lookup dictionary
    directly from the list of known values, rather than finding the unique values
    in the training set. This has the benefit of specifying the order of the values
    in the dictionary, so orders the encoded values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术以相同的方式进行应用。在训练迭代过程中，这些技术会找到特征列中的所有唯一值，并给它们分配一个特定的数值（对于独热编码，是一个多维数值）。结果，一个定义这种替换的查找字典存储在编码器中。当应用编码器时，应用列中的值会使用查找字典进行转换（替换）。如果事先知道可能的值列表，大多数实现允许编码器直接从已知值的列表初始化查找字典，而不是在训练集中找到唯一值。这有利于指定字典中值的顺序，从而对编码值进行排序。
- en: Important Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note that it's often possible that certain categorical feature values
    in the test set don't appear in the training set and, hence, are not stored in
    the lookup dictionary. So, you should add a default category to your encoder that
    can also transform unseen values into numeric values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，通常可能存在某些分类特征值在测试集中没有出现在训练集中，因此没有存储在查找字典中。因此，你应该在你的编码器中添加一个默认类别，该类别也可以将未见过的值转换为数值。 '
- en: 'Now, we will use two different categorical data columns, one ordinal and one
    nominal category, to showcase the different encodings. *Figure 7.3* shows a nominal
    feature, `topic`, which could represent a list of articles by a news agency:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用两个不同的分类数据列，一个是序数类别，另一个是名义类别，来展示不同的编码。*图7.3* 显示了一个名义特征`topic`，它可能代表一个新闻机构的文章列表：
- en: '![Figure 7.3 – Nominal categorical data ](img/B17928_07_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 名义分类数据](img/B17928_07_003.jpg)'
- en: Figure 7.3 – Nominal categorical data
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 名义分类数据
- en: '*Figure 7.4* contains the ordinal category of `rating`; it could represent
    a feedback form for purchased articles on a website:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.4* 包含了`rating`的序数类别；它可能代表一个网站购买文章的反馈表单：'
- en: '![Figure 7.4 – Ordinal categorical data ](img/B17928_07_004.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 序列分类数据](img/B17928_07_004.jpg)'
- en: Figure 7.4 – Ordinal categorical data
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 序列分类数据
- en: To preserve the meaning of the categories, we require different preprocessing
    techniques for the different categorical data types. First, we take a look at
    the *label encoder*. The label encoder assigns an incrementing value to each unique
    categorical value in a feature column. So, it transforms categories into a numeric
    value between `0` and `N-1`, where `N` represents the number of unique values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保留类别的含义，我们需要为不同的分类数据类型采用不同的预处理技术。首先，我们来看一下*标签编码器*。标签编码器为特征列中的每个唯一分类值分配一个递增的值。因此，它将类别转换为介于`0`和`N-1`之间的数值，其中`N`代表唯一值的数量。
- en: 'Let''s test the label encoder in the `topic` column within the first table.
    We train the encoder on the data and replace the `topic` column with a numeric
    topic ID. Here is an example snippet to train the label encoder and transform
    the dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在第一个表中的`topic`列中测试标签编码器。我们在数据上训练编码器，并用数值主题ID替换`topic`列。以下是一个训练标签编码器并转换数据集的示例片段：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 7.5* shows the results of the preceding transformation. Each topic
    was encoded as a numerical increment, `topicId`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.5* 显示了先前转换的结果。每个主题都被编码为一个数值增量，`topicId`：'
- en: '![Figure 7.5 – Label-encoded topics ](img/B17928_07_005.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 标签编码的主题](img/B17928_07_005.jpg)'
- en: Figure 7.5 – Label-encoded topics
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 标签编码的主题
- en: 'The generated lookup table for `topicId` is shown in *Figure 7.6*. This lookup
    dictionary was learned by the encoder during the `fit()` method and can be applied
    to categorical data using the `transform()` method:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`topicId`生成的查找表如图*7.6*所示。这个查找字典是在`fit()`方法期间由编码器学习到的，可以使用`transform()`方法应用于分类数据：'
- en: '![Figure 7.6 – A lookup dictionary for topics ](img/B17928_07_006.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 主题查找字典](img/B17928_07_006.jpg)'
- en: Figure 7.6 – A lookup dictionary for topics
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 主题的查找字典
- en: As you can see in the previous screenshots, encoding nominal data with labels
    is easy and straightforward. However, the resulting numerical data has different
    mathematical properties from the distinct nominal categories. So, let's find out
    how this method works for ordinal data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几个截图所示，使用标签对名义数据进行编码既简单又直接。然而，生成的数值数据具有与不同的名义类别不同的数学属性。因此，让我们找出这种方法对有序数据是如何工作的。
- en: 'In the next example, we naïvely apply the label encoder to the ratings dataset.
    The encoder is trained by iterating the training data in order to create the lookup
    dictionary:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，我们天真地将标签编码器应用于评分数据集。编码器通过迭代训练数据来训练，以创建查找字典：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Figure 7.7* shows the result of the encoded ratings as `ratingId`, which is
    very similar to the previous example. However, in the case of ratings, the numerical
    properties of the ratings data are similar to the ordinal properties of the categorical
    ratings:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.7*显示了编码后的评分结果作为`ratingId`，这与前面的例子非常相似。然而，在评分的情况下，评分数据的数值属性与分类评分的有序属性相似：'
- en: '![Figure 7.7 – Label-encoded ratings ](img/B17928_07_007.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 标签编码的评分](img/B17928_07_007.jpg)'
- en: Figure 7.7 – Label-encoded ratings
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 标签编码的评分
- en: 'Additionally, let''s look at the lookup dictionary, in *Figure 7.8*, that the
    encoder learned from the input data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们看看编码器从输入数据中学习到的查找字典，如图*7.8*所示：
- en: '![Figure 7.8 – The lookup dictionary for ratings ](img/B17928_07_008.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 评分的查找字典](img/B17928_07_008.jpg)'
- en: Figure 7.8 – The lookup dictionary for ratings
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 评分的查找字典
- en: 'Do you see something odd in the autogenerated lookup dictionary? Due to the
    order of the categorical values in the training data, we created a numeric list
    with the following order:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你在自动生成的查找字典中看到什么奇怪的地方了吗？由于训练数据中分类值的顺序，我们按照以下顺序创建了一个数字列表：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is probably not what we anticipated when applying a label encoder to an
    ordinal categorical value. The ordering we would be looking for is similar to
    the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不是我们在将标签编码器应用于有序分类值时所预期的结果。我们希望寻找的顺序类似于以下内容：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In order to create a label encoder with the right order, we can pass the ordered
    list of categorical values to the encoder. This would create a more meaningful
    encoding, as shown in *Figure 7.9*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建具有正确顺序的标签编码器，我们可以将分类值的有序列表传递给编码器。这将创建一个更有意义的编码，如图*7.9*所示：
- en: '![Figure 7.9 – Label-encoded ratings with custom order ](img/B17928_07_009.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 带有自定义顺序的标签编码的评分](img/B17928_07_009.jpg)'
- en: Figure 7.9 – Label-encoded ratings with custom order
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 带有自定义顺序的标签编码的评分
- en: 'To achieve this in Python, we have to use pandas'' categorical ordinal variable,
    which is a special kind of label encoder that requires a list of ordered categories
    as input:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中实现这一点，我们必须使用pandas的分类顺序变量，这是一种特殊的标签编码器，它需要一个有序分类列表作为输入：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Under the hood, we implicitly created the following lookup dictionary for the
    encoder by passing the categories directly to it in order:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，我们通过直接将类别传递给编码器来隐式地创建了以下查找字典：
- en: '![Figure 7.10 – A lookup dictionary for ratings with custom orders ](img/B17928_07_010.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 带有自定义顺序的评分查找字典](img/B17928_07_010.jpg)'
- en: Figure 7.10 – A lookup dictionary for ratings with custom orders
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 带有自定义顺序的评分的查找字典
- en: As you can see in the preceding example, a label encoder can be quickly applied
    to any categorical data without much afterthought. The result of the label encoder
    is a single numerical feature and a categorical lookup table. Additionally, we
    can see, in the examples with topics and ratings, that label encoding is more
    suitable for ordinal data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，标签编码器可以迅速应用于任何分类数据，无需过多思考。标签编码器的结果是单个数值特征和分类查找表。此外，我们还可以看到，在主题和评分的示例中，标签编码更适合有序数据。
- en: Important Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The key takeaway is that the label encoder is great for encoding ordinal categorical
    data. You also learned that the order of elements matters, and so it is good practice
    to manually pass the categories to the encoder in the correct order.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的收获是标签编码器非常适合编码有序分类数据。你也了解到元素的顺序很重要，因此将类别按正确顺序手动传递给编码器是一个好的实践。
- en: Orthogonal embedding using one-hot encoding
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用独热编码的正交嵌入
- en: 'In the second part of this section, we will take a look at the `N`, where `N`
    represents the number of unique values. This vector contains only zeros, except
    for one column that contains `1` and represents the column for this specific value.
    Here is a code snippet showing you how to apply the one-hot encoder to the `articles`
    dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的第二部分，我们将探讨`N`的含义，其中`N`代表唯一值的数量。这个向量除了包含一个列值为`1`的列，代表这个特定值所在的列外，其余列都包含`0`。以下是一个代码片段，展示了如何将独热编码器应用于`articles`数据集：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the preceding code is shown in *Figure 7.11*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出显示在*图7.11*中：
- en: '![Figure 7.11 – One-hot-encoded articles ](img/B17928_07_011.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 独热编码的文章](img/B17928_07_011.jpg)'
- en: Figure 7.11 – One-hot-encoded articles
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 独热编码的文章
- en: 'The lookup dictionary for one-hot encoding has `N+1` columns, where `N` is
    the number of unique values in the encoded column. As we can see in the lookup
    dictionary in *Figure 7.12*, all N-dimensional vectors in the dictionary are orthogonal
    and of an equal length, `1`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码的查找字典有`N+1`列，其中`N`是编码列中唯一值的数量。正如我们在*图7.12*中的查找字典中可以看到的那样，字典中的所有N维向量都是正交的，长度相等，为`1`：
- en: '![Figure 7.12 – The lookup dictionary for articles ](img/B17928_07_012.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 – 文章的查找字典](img/B17928_07_012.jpg)'
- en: Figure 7.12 – The lookup dictionary for articles
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 文章的查找字典
- en: 'Now, let''s compare this technique with ordinal data and apply one-hot encoding
    to the ratings table. The result is shown in *Figure 7.13*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这种技术与有序数据进行比较，并将独热编码应用于评分表。结果显示在*图7.13*中：
- en: '![Figure 7.13 – One-hot-encoded ratings ](img/B17928_07_013.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – 独热编码的评分](img/B17928_07_013.jpg)'
- en: Figure 7.13 – One-hot-encoded ratings
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 独热编码的评分
- en: In the preceding figure, we can see that even if the original category values
    are ordinal, the encoded values can no longer be sorted, and so, this property
    is lost after the numeric encoding. Therefore, we can conclude that one-hot encoding
    is great for nominal categorical values where the number of unique values is small.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到，即使原始的类别值是有序的，编码后的值也无法排序，因此，在数值编码后，这个属性就丢失了。因此，我们可以得出结论，独热编码非常适合唯一值数量较少的名称分类值。
- en: So far, we've learned how to embed nominal and ordinal categorical values into
    numeric values by using a lookup dictionary and one-dimensional or N-dimensional
    numeric embedding. However, we discovered that it is somewhat limited in many
    aspects, such as the number of unique categories and capabilities to embed free
    text. In the following sections, we will learn how to extract words using a simple
    NLP pipeline.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何通过使用查找字典和一维或N维数值嵌入将名称和有序分类值嵌入到数值中。然而，我们发现它在许多方面都有一定的局限性，例如唯一类别的数量和嵌入自由文本的能力。在接下来的几节中，我们将学习如何使用简单的NLP管道提取单词。
- en: Semantics and textual values
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义和文本值
- en: It's worth taking the time to understand that a categorical value and a textual
    value are not the same. Although they might both be stored as a string and could
    have the same data type in your dataset, usually, a categorical value represents
    a finite set of categories, whereas a text value can hold any textual information.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花时间去理解的是，分类值和文本值并不相同。尽管它们可能都存储为字符串，并且在你的数据集中可能有相同的数据类型，但通常，分类值代表一组有限的类别，而文本值可以包含任何文本信息。
- en: So, why is this distinction important? Once you preprocess your categorical
    data and embed it into a numerical space, nominal categories will often be implemented
    as orthogonal vectors. You will not automatically be able to compute a distance
    from category A to category B or create a semantic meaning between the categories.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这种区分为什么很重要呢？一旦你预处理了分类数据并将其嵌入到数值空间中，名称类别通常会被实现为正交向量。你将无法自动计算类别A到类别B的距离或创建类别之间的语义意义。
- en: However, with textual data, usually, you start feature extraction with a different
    approach that assumes that you will find similar terms in the same text feature
    of your dataset samples. You can use this information to compute meaningful similarity
    scores between two textual columns; for example, to measure the number of words
    that are in common.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于文本数据，通常您会采用不同的方法来开始特征提取，该方法假设您将在数据集样本的相同文本特征中找到相似术语。您可以使用这些信息来计算两个文本列之间的有意义相似度得分；例如，测量共同单词的数量。
- en: Therefore, we recommend that you thoroughly check what kind of categorical values
    you have and how you are aiming to preprocess them. Also, a great exercise is
    to compute the similarity between two rows and see whether it matches your prediction.
    Let's take a look at a simple textual preprocessing approach using a dictionary-based
    bag-of-words embedding.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议您彻底检查您有哪些类型的分类值以及您打算如何预处理它们。此外，一个很好的练习是计算两行之间的相似度，看看它是否与您的预测相符。让我们看看使用基于字典的词袋嵌入的简单文本预处理方法。
- en: Building a simple bag-of-words model
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建简单的词袋模型
- en: In this section, we will look at a surprisingly simple concept to tackle the
    shortcomings of label encoding for textual data using a technique called bag-of-words,
    which will build a foundation for a simple NLP pipeline. Don't worry if these
    techniques look too simple when you read through them; we will gradually build
    on top of them with tweaks, optimizations, and improvements to build a modern
    NLP pipeline.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一个惊人的简单概念，即使用称为词袋的技术来解决标签编码在文本数据中的不足，这将为一个简单的NLP管道打下基础。当您阅读这些技术时，如果它们看起来太简单，请不要担心；我们将通过调整、优化和改进逐步构建现代NLP管道。
- en: A naïve bag-of-words model using counting
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用计数构建的简单词袋模型
- en: In this section, the main concept that we will build is the bag-of-words model.
    It is a very simple concept; that is, it involves modeling any document as a collection
    of words that appear in a given document with the frequency of each word. Hence,
    we throw away sentence structure, word order, punctuation marks, and more and
    reduce the documents to a raw count of words. Following this, we can vectorize
    this word count into a numeric vector representation, which can then be used for
    ML, analysis, document comparisons, and much more. While this word count model
    sounds very simple, we will encounter quite a few language-specific obstacles
    along the way that we will need to resolve.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建的主要概念是词袋模型。这是一个非常简单的概念；也就是说，它涉及将任何文档建模为包含在给定文档中的单词集合，每个单词的频率。因此，我们丢弃句子结构、单词顺序、标点符号等，并将文档简化为单词的原始计数。在此基础上，我们可以将这个单词计数向量化为一个数值向量表示，然后可以用于机器学习、分析、文档比较等等。虽然这个单词计数模型听起来非常简单，但在路上我们将会遇到很多语言特定的障碍，我们需要解决。
- en: 'Let''s get started and define a sample document that we will transform throughout
    this section:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始并定义一个示例文档，我们将在这个部分对其进行转换：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Applying a naïve word count to the document gives us our first (too simple)
    bag-of-words model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将简单的单词计数应用于文档为我们提供了我们的第一个（过于简单）词袋模型：
- en: '![Figure 7.14 – A naïve bag-of-words model ](img/B17928_07_014.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 一个简单的词袋模型](img/B17928_07_014.jpg)'
- en: Figure 7.14 – A naïve bag-of-words model
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 - 一个简单的词袋模型
- en: However, there are many problems with a naïve approach such as the preceding
    one. We have mixed different punctuation marks, notations, nouns, verbs, adverbs,
    and adjectives in different declinations, conjugations, tenses, and cases. Therefore,
    we have to build a pipeline to clean and normalize the data using NLP. In this
    section, we will build a pipeline with the following cleaning steps before feeding
    the data into a **count vectorizer** that, ultimately, counts the word occurrences
    and collects them in a feature vector.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像前面那样简单的方法有很多问题。我们混合了不同的标点符号、符号、名词、动词、副词和形容词的不同变形、屈折、时态和格。因此，我们必须构建一个管道来使用NLP清理和标准化数据。在本节中，在将数据输入到**计数向量器**之前，我们将构建以下清理步骤的管道，该向量器最终会计算单词出现次数并将它们收集到特征向量中。
- en: Tokenization – turning a string into a list of words
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词 - 将字符串转换为单词列表
- en: 'The first step in building the pipeline is to separate a corpus into documents
    and a document into words. This process is called `nltk`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 构建管道的第一步是将语料库分为文档，将文档分为单词。这个过程被称为`nltk`：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code will output a list of tokens that contains words and punctuation
    marks:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码将输出一个包含单词和标点符号的标记列表：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When you execute the preceding code snippet, `nltk` will download the pretrained
    punctuation model in order to run the word tokenizer. The output of the tokenizer
    is the words and punctuation marks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行前面的代码片段时，`nltk`将下载预训练的标点模型以运行分词器。分词器的输出是单词和标点符号。
- en: 'In the next step, we will remove the punctuation marks as they are not relevant
    for the subsequent *stemming* process. However, we will bring them back for *lemmatization*
    later in this section:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将移除标点符号，因为它们对于随后的*词形还原*过程不相关。然而，我们将在本节稍后将其恢复：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result will only contain the words of the original document without any
    punctuation marks:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将只包含原始文档中的单词，没有任何标点符号：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, we used the `word.islanum()` function to only extract
    alphanumeric tokens and make them all lowercase. The preceding list of words already
    looks much better than the initial naïve model. However, it still contains a lot
    of unnecessary words, such as *the*, *we*, *had*, and more, which don't convey
    any information.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`word.isalnum()`函数来仅提取字母数字标记并将它们全部转换为小写。前面的单词列表已经比最初的原始模型好得多。然而，它仍然包含许多不必要的词，如*the*、*we*、*had*等，这些词不传达任何信息。
- en: 'In order to filter out the noise for a specific language, it makes sense to
    remove these words that often appear in texts and don''t add any semantic meaning
    to the text. It is common practice to remove these so-called `nltk` library in
    Python:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了过滤掉特定语言的噪声，有道理移除那些经常出现在文本中且不增加任何语义意义的词。在Python中，移除这些词是常见的做法，使用`nltk`库：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now the resulting list only contains words that are not stop words:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在得到的列表只包含不是停用词的单词：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code gives us a nice pipeline where we end up with only the semantically
    meaningful words. We can take this list of words to the next step and apply a
    more sophisticated transformation/normalization to each word. If we applied the
    count vectorizer at this stage, we would end up with the simple bag-of-words model,
    as shown in *Figure 7.15*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码为我们提供了一个很好的管道，我们最终只得到具有语义意义的词。我们可以将这个词表带到下一步，并对每个词应用更复杂的转换/归一化。如果我们在这个阶段应用计数向量器，我们最终会得到如图7.15所示的简单词袋模型：
- en: '![Figure 7.15 – A simple bag-of-words model ](img/B17928_07_015.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – 一个简单的词袋模型](img/B17928_07_015.jpg)'
- en: Figure 7.15 – A simple bag-of-words model
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 一个简单的词袋模型
- en: As you can see in the previous figure, the list of terms that are included in
    the bag-of-words model is already far cleaner than the naïve example. This is
    because it doesn't contain any punctuation marks or stop words.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，词袋模型中包含的术语列表已经比原始示例干净得多。这是因为它不包含任何标点符号或停用词。
- en: You might ask what qualifies a word as a stop word other than it occurring relatively
    often in a piece of text? Well, that's an excellent question! We can measure the
    importance of each word in the current context compared to its occurrences across
    the text using the **TF-IDF** method, which will be discussed in the *Measuring
    the importance of words using TF-IDF* section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，除了在文本中相对频繁出现之外，什么使一个词成为停用词？嗯，这是一个非常好的问题！我们可以使用**TF-IDF**方法来衡量每个词在当前上下文中的重要性，与它在整个文本中的出现频率进行比较，这将在*使用TF-IDF衡量词的重要性*部分进行讨论。
- en: Stemming – the rule-based removal of affixes
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取 – 基于规则的词缀移除
- en: In the next step, we want to normalize affixes—word endings to create plurals
    and conjugations. You can see that with each step, we are diving deeper into the
    concept of a single language—in this case, English. However, when applying these
    steps to a different language, it's likely that completely different transformations
    will need to be used. This is what makes NLP such a difficult field.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们想要归一化词缀——单词的结尾以创建复数和动词变位。你可以看到，随着每一步的进行，我们都在更深入地探讨单一语言的概念——在这个案例中，是英语。然而，当将这些步骤应用于不同的语言时，可能需要使用完全不同的转换。这就是为什么NLP是一个如此困难的领域。
- en: 'Removing the affixes of words to obtain the stem of a word is also called **stemming**.
    Stemming refers to a rule-based (heuristic) approach to transform each occurrence
    of a word into its word stem. Here is a simple example of some expected transformations:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 移除单词的词缀以获得词根也称为**词干提取**。词干提取是指将每个单词的出现转换为它的词根的基于规则（启发式）方法。以下是一些预期的转换示例：
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see in the preceding example, such a heuristic approach for stemming
    has to be built specifically for each language. This is generally true for all
    other NLP algorithms as well. For the sake of brevity, in this book, we will only
    discuss English examples.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，这种针对词根的启发式方法必须为每种语言专门构建。这对于所有其他NLP算法也是普遍适用的。为了简洁起见，在这本书中，我们只将讨论英语示例。
- en: 'A popular algorithm for stemming in English is Porter''s algorithm, which defines
    five sequential reduction rules, such as removing *-ed*, *-ing*, -*ate*, *-tion*,
    *-ence*, *-ance*, and more, from the end of words. The `nltk` library comes with
    an implementation of Porter''s stemming algorithm:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 英语中一个流行的词根化算法是Porter算法，它定义了五个连续的缩减规则，例如从单词末尾移除*ed*、*ing*、*ate*、*tion*、*ence*、*ance*等。`nltk`库包含Porter词根化算法的实现：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting list of words after stemming looks like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 词根化后的单词列表看起来像这样：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, we simply apply `stemmer` to each word in the tokenized
    document. The bag-of-words model after this step is shown in *Figure 7.16*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只是简单地将`stemmer`应用于分词文档中的每个单词。经过这一步骤后的词袋模型如图*7.16*所示：
- en: '![Figure 7.16 – The bag-of-words model after stemming ](img/B17928_07_016.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16 – 词根化后的词袋模型](img/B17928_07_016.jpg)'
- en: Figure 7.16 – The bag-of-words model after stemming
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 词根化后的词袋模型
- en: While this algorithm works well with affixes, it can't avoid normalizing conjugations
    and tenses. This will be our next problem to tackle using lemmatization.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个算法与词缀配合得很好，但它无法避免对动词的变形和时态进行规范化。这是我们接下来要解决的问题，我们将使用词形还原来解决。
- en: Lemmatization – dictionary-based word normalization
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词形还原 – 基于词典的词规范化
- en: 'When looking at the stemming examples, we can already see the limitations of
    that approach. For example, what would happen with irregular verb conjugations—such
    as *are*, *am*, or *is*—that should all be normalized to the same word, *be*?
    This is exactly what lemmatization tries to solve using a pretrained set of vocabulary
    and conversion rules, called lemmas. The **lemmas** are stored in a lookup dictionary
    and look similar to the following transformations:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看词根化示例时，我们已能看出该方法的局限性。例如，对于像*are*、*am*或*is*这样的不规则动词变形，它们都应该被规范化为同一个词*be*，会发生什么？这正是词形还原试图通过使用预训练的词汇集和转换规则（称为词元）来解决的问题。**词元**存储在查找字典中，类似于以下转换：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'There is one very important point to make when discussing lemmatization. Each
    lemma needs to be applied to the correct word type, hence a lemma for nouns, verbs,
    adjectives, and more. The reason for this is that a word can be either a noun
    or a verb in the past tense. In our example, `ground` could come from the noun
    *ground* or the verb *grind*; `left` could be an adjective or the past tense of
    *leave*. So, we also need to extract the word type from the word in a sentence—this
    process is called `nltk` library has us covered once again. To estimate the correct
    POS tag, we also need to provide the punctuation mark:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论词形还原时，有一个非常重要的观点需要提出。每个词元都需要应用于正确的词型，因此名词、动词、形容词等都有词元。这样做的原因是，一个词可以是名词或动词的过去时。在我们的例子中，`ground`可能来自名词*ground*或动词*grind*；`left`可能是形容词或*leave*的过去时。因此，我们还需要从句子中的单词中提取词型——这个过程称为`nltk`库再次为我们提供了支持。为了估计正确的POS标签，我们还需要提供标点符号：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here are the resulting POS tags:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的POS标签：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The POS tags describe the word type of each token in the document. You can
    find a complete list of tags using the `nltk.help.upenn_tagset()` command. Here
    is an example of how to do so from the command line:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: POS标签描述了文档中每个标记的词型。您可以使用`nltk.help.upenn_tagset()`命令找到完整的标签列表。以下是从命令行执行此操作的示例：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding command will print the list of POS tags:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将打印出POS标签列表：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The POS tags also include tenses for verbs and other very useful information.
    However, for the lemmatization in this section, we only need to know the word
    type—*noun*, *verb*, *adjective*, or *adverb*. One possible choice of lemmatizer
    is the WordNet lemmatizer in `nltk`. WordNet is a lexical database of English
    words that groups them into groups of concepts and word types.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: POS标签还包括动词和其他非常有用的时态信息。然而，在本节的词形还原中，我们只需要知道单词类型——*名词*、*动词*、*形容词*或*副词*。一个可能的词形还原器选择是`nltk`中的WordNet词形还原器。WordNet是一个英语词汇数据库，它将单词分组到概念和词型组中。
- en: 'To apply the lemmatizer to the output of the stemming, we need to filter the
    POS tags by punctuation marks and stop words, similar to the previous preprocessing
    step. Then, we can use the word tags for the resulting words. Let''s apply the
    lemmatizer using `nltk`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要将词形还原器应用于词干分析的结果，我们需要通过标点符号和停用词过滤POS标签，类似于之前的预处理步骤。然后，我们可以使用结果单词的词标签。让我们使用`nltk`应用词形还原器：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The code outputs the lemmatized words:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出了词形还原后的单词：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding list of words looks a lot cleaner than what we found in previous
    models. This is because we normalized the tenses of the verbs and transformed
    them into their infinitive form. The resulting bag-of-words model is shown in
    *Figure 7.17*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 上述单词列表看起来比我们在之前的模型中找到的干净得多。这是因为我们对动词的时态进行了归一化，并将它们转换成不定式形式。得到的词袋模型在*图7.17*中显示：
- en: '![Figure 7.17 – The bag-of-words model after lemmatization ](img/B17928_07_017.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图7.17 – 词袋模型在词形还原后的样子](img/B17928_07_017.jpg)'
- en: Figure 7.17 – The bag-of-words model after lemmatization
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 词形还原后的词袋模型
- en: This technique is extremely helpful for cleaning up irregular forms of words
    in your dataset. However, it works based on rules—called lemmas—and, hence, it
    can only be used for languages and words where such lemmas are available.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术对于清理数据集中单词的不规则形式非常有帮助。然而，它基于规则——称为词元——因此，它只能用于有此类词元的语言和单词。
- en: A bag-of-words model in scikit-learn
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn中的词袋模型
- en: Finally, we can put all our previous steps together to create a state-of-the-art
    NLP preprocessing pipeline to normalize the input documents and run them through
    a count vectorizer so that we can transform them into a numeric feature vector.
    Doing so for multiple documents allows us to easily compare the semantics of the
    document in a numerical space. We could compute cosine similarities on the document's
    feature vectors to compute their similarity, plug them into a supervised classification
    method, or perform clustering on the resulting document concepts.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们之前的所有步骤结合起来，创建一个最先进的自然语言处理预处理流程，以归一化输入文档，并通过计数向量器运行它们，以便我们可以将它们转换成数值特征向量。对多个文档这样做，我们可以轻松地在数值空间中比较文档的语义。我们可以计算文档特征向量之间的余弦相似度来计算它们的相似度，将它们插入到监督分类方法中，或者对生成的文档概念进行聚类。
- en: 'To recap, let''s take a look at the final pipeline for the simple bag-of-words
    model. I want to emphasize that this model is only the start of our journey in
    feature extraction using NLP. We performed the following steps for normalization:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，让我们看看简单词袋模型的最终流程。我想强调的是，这个模型只是我们使用自然语言处理进行特征提取旅程的开始。我们进行了以下步骤进行归一化：
- en: Tokenization
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词
- en: Removing punctuation marks
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: Removing stop words
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除停用词
- en: Stemming
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatization with POS tagging
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于POS标签的词形还原
- en: 'In the last step, we applied `CountVectorizer` in scikit-learn. This will count
    the occurrences of each word, create a global corpus of words, and output a sparse
    feature vector of word frequencies. Here is the sample code to pass the preprocessed
    data from `nltk` to `CountVectorizer`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们在scikit-learn中应用了`CountVectorizer`。这将计算每个词的出现次数，创建一个全局单词语料库，并输出一个包含单词频率的稀疏特征向量。以下是将预处理数据从`nltk`传递到`CountVectorizer`的示例代码：
- en: '[PRE23]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The transformed bag-of-words model contains coordinates and counts:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的词袋模型包含坐标和计数：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The coordinates refer to the `(document id, term id)` pair, whereas the count
    refers to the term frequency. To better understand this output, we can also look
    at the internal vocabulary of the model. The `vocabulary_` parameter contains
    a lookup dictionary for the term ids:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标指的是`(文档ID，术语ID)`对，而计数指的是术语频率。为了更好地理解这个输出，我们还可以查看模型的内部词汇表。`vocabulary_`参数包含术语ID的查找字典：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The code outputs the model''s word dictionary:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出了模型的单词字典：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the preceding example, we transform the preprocessed document back into a
    string before passing it to `CountVectorizer`. The reason for this is that `CountVectorizer`
    comes with some configurable preprocessing techniques out of the box, such as
    tokenization, stop word removal, and more. For this demonstration, we want to
    apply it to the preprocessed data. The output of the transformation is a sparse
    feature vector containing the term frequencies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们在将其传递到`CountVectorizer`之前将预处理文档转换回字符串。这样做的原因是`CountVectorizer`自带一些可配置的预处理技术，例如分词、停用词删除等。对于这个演示，我们想将其应用于预处理数据。转换的输出是一个包含术语频率的稀疏特征向量。
- en: Let's find out how we can combine multiple terms with semantic concepts.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来找出如何将多个术语与语义概念相结合。
- en: Leveraging term importance and semantics
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用术语重要性和语义
- en: Everything we have done up to now has been relatively simple and based on word
    stems or so-called tokens. The bag-of-words model was nothing but a dictionary
    of tokens counting the occurrence of tokens per field. In this section, we will
    take a look at a common technique to further improve matching between documents
    using n-gram and skip-gram combinations of terms.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所做的一切都相对简单，并且基于词干或所谓的标记。词袋模型只不过是一个标记字典，它按字段统计标记的出现次数。在本节中，我们将探讨一种常见的技巧，通过术语的n-gram和skip-gram组合来进一步改进文档之间的匹配。
- en: Combining terms in multiple ways will explode your dictionary. This will turn
    into a problem if you have a large corpus; for instance, 10 million words. Hence,
    we will look at a common preprocessing technique to reduce the dimensionality
    of a large dictionary through SVD.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以多种方式组合术语将使你的词典爆炸。如果你有一个大语料库，比如一千万个单词，这就会变成一个问题。因此，我们将探讨一种常见的预处理技术，通过SVD来降低大型词典的维度。
- en: While, now, this approach is a lot more complicated, it is still based on a
    bag-of-words model that already works well on a large corpus, in practice. However,
    of course, we can do better and try to understand the importance of words. Therefore,
    we will tackle another popular technique in NLP to compute the importance of terms.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然，现在，这种方法要复杂得多，但它仍然基于一个在大语料库上已经工作得很好的词袋模型。然而，当然，我们可以做得更好，并尝试理解词语的重要性。因此，我们将探讨NLP中另一种流行的技术来计算术语的重要性。
- en: Generalizing words using n-grams and skip-grams
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用n-gram和skip-gram进行词语泛化
- en: In the previous pipeline, we considered each word on its own without any context.
    However, as we all know, context matters a lot in language. Sometimes, words belong
    together and only make sense in context rather than on their own. To introduce
    this context into the same type of algorithm, we will introduce **n-grams** and
    **skip-grams**. Both techniques are heavily used in NLP for preprocessing datasets
    and extracting relevant features from text data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的管道中，我们考虑了每个单词本身，没有任何上下文。然而，众所周知，上下文在语言中非常重要。有时，词语在一起才有意义，而不是单独存在。为了将这种上下文引入同类型的算法，我们将引入**n-gram**和**skip-gram**。这两种技术都在NLP中广泛用于预处理数据集和从文本数据中提取相关特征。
- en: 'Let''s start with n-grams. An `N` consecutive entities (that is, characters,
    words, or tokens) of an input dataset. Here are some examples for computing the
    n-grams in a list of characters:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从n-gram开始。一个输入数据集的`N`个连续实体（即字符、单词或标记）。以下是一些在字符列表中计算n-gram的示例：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is an example using the built-in `ngram_range` parameter in scikit-learn''s
    `CountVectorizer` to generate multiple n-grams for the input data:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例，使用scikit-learn的`CountVectorizer`中的内置`ngram_range`参数来为输入数据生成多个n-gram：
- en: '[PRE28]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As you can see, the vocabulary now contains both the 1-gram and 2-gram representations
    of each term:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，词汇现在包含每个术语的1-gram和2-gram表示：
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the preceding code, we can see that instead of the original words, we now
    have a combination of two consecutive words in our trained vocabulary.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以看到，我们现在在训练词汇中拥有两个连续词语的组合，而不是原始词语。
- en: 'We can extend the concept of n-grams to also allow the model to skip words.
    This a great option, if we for example want to perform a 2-gram, but in one of
    our samples there is an adjective in-between two words and in the other those
    words are directly next to each other. To achieve this, we need a method that
    allows us to define how many words it is allowed to skip to find matching words.
    Here is an example using the same characters as before:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将n-gram的概念扩展到允许模型跳过词语。如果我们想要执行一个2-gram，但其中一个样本中两个词语之间有一个形容词，而在另一个样本中这些词语是直接相邻的，这是一个很好的选项。为了实现这一点，我们需要一种方法来定义我们允许跳过多少个词语来找到匹配的词语。以下是一个使用之前相同字符的示例：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Luckily, we find the generalized version of n-grams implemented in `nltk` as
    the `nltk.skipgrams` method. Setting the skip distance to `0` results in the traditional
    n-gram algorithm. We can apply it to our original dataset:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在`nltk`中找到了n-gram的通用版本，即`nltk.skipgrams`方法。将跳过距离设置为`0`会导致传统的n-gram算法。我们可以将其应用于我们的原始数据集：
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Similar to the 2-gram example, the method produces a list of combinations of
    paired terms. However, in this case, we allowed one skip word to be present between
    those pairs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与2-gram示例类似，该方法产生了一组成对术语的组合列表。然而，在这种情况下，我们在这些对之间允许存在一个跳过的单词：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code, we can observe that skip-grams can generate a lot of
    additional useful feature dimensions for the NLP model. In real-world scenarios,
    both techniques are often used because the individual word order plays a big role
    in the semantics.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们可以观察到skip-grams可以为NLP模型生成大量的额外有用特征维度。在现实场景中，这两种技术通常都会使用，因为单个单词的顺序在语义中起着重要作用。
- en: However, the explosion of new feature dimensions could be devastating if the
    input documents are, for example, all websites from the web or large documents.
    Therefore, we also need a way to avoid an explosion of the dimensions while capturing
    all of the semantics from the input data. We will tackle this challenge in the
    next section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果输入文档是来自网络的所有网站或大型文档，新特征维度的爆炸可能会造成灾难。因此，我们还需要一种方法来避免维度爆炸，同时捕获输入数据中的所有语义。我们将在下一节中解决这个挑战。
- en: Reducing word dictionary size using SVD
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SVD减小词字典大小
- en: A common problem with NLP is the vast number of words in a corpus and, hence,
    exploding dictionary sizes. In the previous example, we saw that the size of the
    dictionary defines the size of the orthogonal term vector. Therefore, a dictionary
    size of 20,000 terms would result in 20,000-dimensional feature vectors. Even
    without any n-gram enrichment, this feature vector dimension is too large to be
    processed on standard PCs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的一个常见问题是语料库中的单词数量庞大，因此字典大小会爆炸。在先前的例子中，我们看到字典的大小定义了正交项向量的大小。因此，20,000个术语的字典大小将导致20,000维的特征向量。即使没有任何n-gram丰富，这个特征向量维度也太大，无法在标准PC上处理。
- en: Therefore, we need an algorithm to reduce the dimensions of the generated `CountVectorizer`
    while preserving the present information. Optimally, we would only remove redundant
    information from the input data and project it onto a lower-dimensional space
    while preserving all of the original information.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要一个算法来减小生成的`CountVectorizer`的维度，同时保留现有信息。理想情况下，我们只会从输入数据中移除冗余信息，并将其投影到低维空间，同时保留所有原始信息。
- en: The PCA transformation would be a great fit for our solution and help us to
    transform the input data into lower linearly unrelated dimensions. However, computing
    the eigenvalues requires a symmetric matrix (the same number of rows and columns),
    which, in our case, we don't have. Hence, we can use the SVD algorithm, which
    generalizes the eigenvector computation to non-symmetric matrices. Due to its
    numeric stability, it is often used in NLP and information retrieval systems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: PCA变换非常适合我们的解决方案，并帮助我们将输入数据转换成更低维度的线性无关维度。然而，计算特征值需要一个对称矩阵（行数和列数相同），在我们的情况下，我们没有这样的矩阵。因此，我们可以使用SVD算法，它将特征向量计算推广到非对称矩阵。由于其数值稳定性，它通常用于NLP和信息检索系统中。
- en: The usage of SVD in NLP applications is also called **Latent Semantic Analysis**
    (**LSA**), as the principal components can be interpreted as concepts in a latent
    feature space. The SVD embedding transforms the high-dimensional feature vector
    into a lower-dimensional concept space. Each dimension in the concept space is
    constructed by a linear combination of term vectors. By dropping the concepts
    with the smallest variance, we also reduce the dimensions of the resulting concept
    space to something that is a lot smaller and easier to handle. Typical concept
    spaces have 10s to 100s of dimensions, while word dictionaries usually have over
    100,000.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: SVD在NLP应用中的使用也被称为**潜在语义分析**（**LSA**），因为主成分可以解释为潜在特征空间中的概念。SVD嵌入将高维特征向量转换成低维概念空间。概念空间中的每个维度都是由术语向量的线性组合构成的。通过丢弃方差最小的概念，我们也减小了结果概念空间的维度，使其变得小得多，更容易处理。典型的概念空间有10到100个维度，而单词字典通常有超过100,000个。
- en: 'Let''s look at an example using the `TruncatedSVD` implementation from `sklearn`.
    The SVD is implemented as a transformer class, and so, we need to call `fit_transform()`
    to fit a dictionary and transform it using the same step. The SVD is configured
    to only keep the components with the highest variance using the `n_components`
    argument:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过 `sklearn` 的 `TruncatedSVD` 实现来查看一个示例。SVD 被实现为一个转换器类，因此我们需要调用 `fit_transform()`
    来拟合一个字典并使用相同的步骤进行转换。SVD 使用 `n_components` 参数配置为仅保留方差最高的成分：
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we perform the LSA on the `X_train_counts` data and the
    output of `CountVectorizer` using SVD. We configure the SVD to only keep the first
    five components with the highest variance.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 SVD 对 `X_train_counts` 数据和 `CountVectorizer` 的输出进行 LSA。我们配置 SVD
    只保留方差最高的前五个成分。
- en: 'By reducing the dimensionality of your dataset, you lose information. Thankfully,
    we can compute the amount of variance in the remaining dataset using the trained
    SVD object, as shown in the following example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低数据集的维度，你会丢失信息。幸运的是，我们可以使用训练好的 SVD 对象来计算剩余数据集中方差的数量，如下例所示：
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding command outputs the variance as a number between 0 and 1, where
    1 means that the SVD transformation is an exact lossless mapping of the original
    data into the latent space:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将方差输出为一个介于 0 和 1 之间的数字，其中 1 表示 SVD 变换是原始数据到潜在空间的精确无损映射：
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In this case, with only five components, the SVD retained 20% of the variance
    of the original dataset.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，仅使用五个成分，SVD 保留了原始数据集 20% 的方差。
- en: Important Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Depending on the task, we usually aim to preserve more than 80–90% of the original
    variance after the latent transformation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务的不同，我们通常的目标是在潜在变换后保留超过 80-90% 的原始方差。
- en: In the previous code example, we computed the variance of the data that is preserved
    after the transformation to the configured number of components. Hence, we can
    now increase or reduce the number of components in order to keep a specific percentage
    of the information in the transformed data. This is a very helpful operation and
    is used in many practical NLP implementations.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们计算了转换后保留的数据的方差。因此，我们现在可以增加或减少成分的数量，以保持转换数据中特定百分比的信息。这是一个非常有用的操作，并在许多实际的
    NLP 应用中得到了使用。
- en: Note that we are still using the original word dictionary from the bag-of-words
    model. One particular downside of this model is that the more often a term occurs,
    the higher its count (and, therefore, weight) will get. This is a problem because,
    now, any term that is not a stop word and appears often in the text will receive
    a high weight—independent of the importance of the term within a certain document.
    Therefore, we introduce another extremely popular preprocessing technique—**TF-IDF**.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们仍在使用词袋模型的原始单词字典。这个模型的一个特定缺点是，一个术语出现的频率越高，它的计数（以及因此的权重）就越高。这是一个问题，因为现在，任何不是停用词且在文本中频繁出现的术语都将获得高权重——无论该术语在特定文档中的重要性如何。因此，我们引入了另一个极其流行的预处理技术——**TF-IDF**。
- en: Measuring the importance of words using TF-IDF
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 测量单词的重要性
- en: One particular downside of the bag-of-words approach is that we simply count
    the absolute number of words in a context without checking whether the word generally
    appears frequently across all documents. A term that appears in every document
    might not be relevant for our model, as it contains less information and more
    often it appears across other documents. Hence, an important technique in text
    mining is to compute the importance of a certain word in a given context.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋方法的一个特定缺点是，我们仅仅计算一个上下文中单词的绝对数量，而不检查该单词是否在所有文档中普遍出现。一个在所有文档中都出现的术语可能对我们模型来说并不相关，因为它包含的信息较少，并且更频繁地出现在其他文档中。因此，在文本挖掘中，计算给定上下文中某个单词的重要性是一项重要的技术。
- en: 'Therefore, instead of an absolute count of terms in a context, we want to compute
    the number of terms in the context relative to a corpus of documents. By doing
    so, we will give higher weight to terms that appear only in a certain context,
    and reduce the amount of weight given to terms that appear in many different documents.
    This is exactly what the TF-IDF algorithm does. It is easy to compute a weight
    (*w*) for a term (*t*) in a document (*d*) according to the following equation:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望计算一个上下文中术语的相对数量，而不是上下文中术语的绝对计数。通过这样做，我们将给只出现在特定上下文中的术语赋予更高的权重，并减少给出现在许多不同文档中的术语的权重。这正是
    TF-IDF 算法所做的事情。根据以下方程式，很容易计算文档中术语 (*t*) 的权重 (*w*)：
- en: '![](img/Formula_07_001.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![公式 07_001](img/Formula_07_001.png)'
- en: While the term frequency (*f*t) counts all of the terms in a document, the inverse
    document frequency is computed by dividing the total number of documents (*N*)
    by the counts of a term in all documents (*f*d). The *IDF* term is usually log-transformed,
    as the total count of a term across all documents can get quite large.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词频 (*f*t) 计算了文档中的所有术语，但逆文档频率是通过将总文档数 (*N*) 除以所有文档中术语的计数 (*f*d*) 来计算的。*IDF*
    术语通常进行对数变换，因为所有文档中术语的总数可能相当大。
- en: 'In the following example, we will not use the TF-IDF function directly. Instead,
    we will use `TfidfVectorizer`, which does the counting and then applies the TF-IDF
    function to the result in one step. Again, the function is implemented as a `sklearn`
    transformer, and hence, we call `fit_transform()` to train and transform the dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们不会直接使用 TF-IDF 函数。相反，我们将使用 `TfidfVectorizer`，它在一步中完成计数并将 TF-IDF 函数应用于结果。再次强调，该函数作为
    `sklearn` 转换器实现，因此我们调用 `fit_transform()` 来训练和转换数据集：
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The result is formatted in a similar manner to the earlier example containing
    `(document id, term id)` pairs and their TF-IDF values:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的格式与前面的示例类似，包含 `(document id, term id)` 对及其 TF-IDF 值：
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the preceding code, we apply `TfidfVectorizer` directly, which returns the
    same result as using `CountVectorizer` and `TfidfTransformer` combined. We transform
    a dataset containing the words of the bag-of-words model and return the TF-IDF
    values. We can also return the terms for each TF-IDF value:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们直接应用 `TfidfVectorizer`，它返回与使用 `CountVectorizer` 和 `TfidfTransformer`
    结合相同的结果。我们转换包含词袋模型中单词的数据集，并返回 TF-IDF 值。我们还可以为每个 TF-IDF 值返回术语：
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code returns the vocabulary of the model:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回模型的词汇表：
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this example, we can see that `ground` gets a TF-IDF value of `0.667`, whereas
    all the other terms receive a value of `0.333`. This count will now scale relatively
    when more documents are added to the corpus—hence, if the word `hold` were to
    be included again, the TF-IDF value would decrease.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们可以看到 `ground` 获得了 TF-IDF 值为 `0.667`，而所有其他术语的值均为 `0.333`。当向语料库中添加更多文档时，这个计数将相对缩放——因此，如果单词
    `hold` 再次出现，TF-IDF 值将降低。
- en: In any real-world pipeline, we would always use all the techniques presented
    in this chapter—tokenization, stop word removal, stemming, lemmatization, n-grams/skip-grams,
    TF-IDF, and SVD—combined in a single pipeline. The result would be a numeric representation
    of n-grams/skip-grams of tokens weighted by importance and transformed into a
    latent semantic space. Using these techniques for your first NLP pipeline will
    get you quite far, as you can now capture a lot of information from your textual
    data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何实际的管道中，我们都会始终使用本章中介绍的所有技术——分词、停用词去除、词干提取、词形还原、n-gram/skip-gram、TF-IDF 和 SVD——结合在一个单一的管道中。结果将是一个由重要性加权的
    n-gram/skip-gram 的标记的数值表示，并转换到潜在语义空间。使用这些技术进行你的第一个 NLP 管道将让你走得很远，因为你现在可以从你的文本数据中捕获大量信息。
- en: So far, we have learned how to numerically encode many kinds of categorical
    and textual values by using either one-dimensional or N-dimensional labels or
    counting and weighting word stems and character combinations. While many of these
    methods work well in many situations where you require simple numeric embedding,
    they all have a serious limitation—they don't encode semantics. Let's take a look
    at how we can extract the semantic meaning of text in the same pipeline.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用一维或 N 维标签、计数和加权词干和字符组合来数值化许多种类的分类和文本值。虽然许多这些方法在需要简单数值嵌入的许多情况下都表现良好，但它们都有一个严重的限制——它们不编码语义。让我们看看我们如何在同一个管道中提取文本的语义意义。
- en: Extracting semantics using word embeddings
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词嵌入提取语义
- en: When computing the similarity of news, you would imagine that topics such as
    tennis, *Formula 1*, or *soccer* would be semantically more similar to each other
    than topics such as politics, economics, or science. Yet, in terms of the previously
    discussed techniques, all encoded categories are seen as semantically the same.
    In this section, we will discuss a simple method of semantic embedding, which
    is also called **word embedding**.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算新闻的相似性时，你会想象到像网球、*一级方程式*或*足球*这样的主题在语义上比像政治、经济或科学这样的主题更相似。然而，在之前讨论的技术中，所有编码的分类都被视为在语义上是相同的。在本节中，我们将讨论一种简单的语义嵌入方法，这也可以称为**词嵌入**。
- en: The previously discussed pipeline using LSA transforms multiple documents into
    terms and then transforms those terms into semantic concepts that can be compared
    with other documents. However, the semantic meaning is based on the term occurrences
    and importance—there is no measurement of semantics between individual terms.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 之前讨论的管道使用LSA将多个文档转换为术语，然后将这些术语转换为可以与其他文档比较的语义概念。然而，语义意义基于术语出现和重要性——没有对单个术语之间的语义进行测量。
- en: 'Hence, what we are looking for is an embedding of terms into numerical multidimensional
    space such that each word represents one point in this space. This allows us to
    compute a numerical distance between multiple words in this space in order to
    compare the semantic meaning of two words. The most interesting benefit of word
    embeddings is that algebra on the word embeddings is not only numerically possible
    but also makes sense. Consider the following example:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们寻找的是将术语嵌入到数值多维空间中的嵌入，这样每个单词就代表这个空间中的一个点。这使我们能够计算这个空间中多个单词之间的数值距离，以比较两个单词的语义意义。词嵌入最有趣的好处是，在词嵌入上的代数运算不仅数值上是可能的，而且是有意义的。考虑以下示例：
- en: '[PRE40]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can create such an embedding by mapping a corpus of words on an N-dimensional
    numeric space and optimizing the numeric distance based on the word semantics—for
    example, based on the distance between words in a corpus. The resulting optimization
    outputs a dictionary of words in the corpus and their numeric N-dimensional representation.
    In this numeric space, words have the same, or at least similar, properties as
    in the semantic space. A great benefit is that these embeddings can be trained
    unsupervised, so no training data has to be labeled.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将单词语料库映射到N维数值空间，并根据单词语义优化数值距离（例如，基于语料库中单词之间的距离）来创建这样的嵌入。结果优化输出语料库中单词及其N维数值表示的字典。在这个数值空间中，单词具有与语义空间中相同或至少相似的属性。一个巨大的好处是，这些嵌入可以无监督地训练，因此不需要标记的训练数据。
- en: 'One of the first embeddings is called **Word2Vec** and is based on a continuous
    bag-of-words model or a continuous skip-gram model to count and measure the words
    in a window. Let''s try this functionality and perform a semantic word embedding
    using Word2Vec:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的嵌入之一被称为**Word2Vec**，它基于连续的词袋模型或连续的跳字模型来计数和测量窗口中的单词。让我们尝试这个功能，并使用Word2Vec进行语义词嵌入：
- en: 'The best Python implementation for word embeddings is **Gensim**, which we
    will also use here. We need to feed our tokens into the model in order to train
    it:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最好的Python词嵌入实现是**Gensim**，我们也将在这里使用它。我们需要将我们的标记输入到模型中以便训练它：
- en: '[PRE41]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the preceding code, we load the `Word2Vec` model and initialize it with the
    list of tokens from the previous sections, which is stored in the `words` variable.
    The `size` attribute defines the dimension of the resulting vectors, and the `window`
    parameter decides how many words we should consider per window. Once the model
    has been trained, we can simply look up the word embedding in the model's dictionary.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们加载了`Word2Vec`模型，并用之前章节中存储在`words`变量中的标记列表初始化它。`size`属性定义了结果向量的维度，`window`参数决定了我们应该考虑多少个单词作为每个窗口。一旦模型被训练，我们就可以简单地在该模型的字典中查找词嵌入。
- en: The code will automatically train the embedding on the set of tokens we provided.
    The resulting model stores the word-to-vector mapping in the `wv` property. Optimally,
    we also use a large corpus or pretrained model that is either provided by `gensim`
    or another NLP library, such as `NLTK`, to train the embedding and fine-tune it
    with a smaller dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将自动在我们提供的标记集上训练嵌入。结果模型将单词到向量的映射存储在`wv`属性中。理想情况下，我们还使用一个大型语料库或预训练模型，该模型由`gensim`或另一个NLP库（如`NLTK`）提供，以训练嵌入并使用较小的数据集进行微调。
- en: 'Next, we can use the trained model to embed all the terms from our document
    using the Word2Vec embedding. However, this will result in multiple vectors as
    each word returns its own embedding. Therefore, you need to combine all the vectors
    into a single vector using the mathematical mean of all the embeddings. This procedure
    is quite similar to the one used to generate a concept in LSA. Also, other reduction
    techniques are possible; for example, weighing the individual embedding vectors
    using their TF-IDF values:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用训练好的模型通过Word2Vec嵌入将我们文档中的所有术语嵌入。然而，这将导致多个向量，因为每个单词都返回其自己的嵌入。因此，你需要使用所有嵌入的数学平均值将所有向量组合成一个单一的向量。这个过程与用于生成LSA中概念的类似过程非常相似。此外，还有其他可能的缩减技术；例如，使用TF-IDF值对单个嵌入向量进行加权：
- en: '[PRE42]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the preceding function, we compute the mean from all the word embedding vectors
    of the terms—this is called a **mean embedding**, and it represents the concept
    of this document in the embedding space. If a word is not found in the embedding,
    we need to replace it with zeros in the computation.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们计算所有术语的词嵌入向量的平均值——这被称为**平均嵌入**，它代表了文档在嵌入空间中的概念。如果一个单词在嵌入中未找到，我们需要在计算中将它替换为零。
- en: You can use such a semantic embedding for your application by downloading a
    pretrained embedding, for example, on the Wikipedia corpus. Then, you can loop
    through your sanitized input tokens and look up the words in the dictionary of
    the numeric embedding.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过下载预训练嵌入，例如在维基百科语料库上，来使用此类语义嵌入为您的应用程序。然后，您可以遍历您的清洗过的输入标记，并在数字嵌入的字典中查找单词。
- en: 'GloVe is another popular technique for encoding words as numerical vectors,
    developed by Stanford University. In contrast to the continuous window-based approach,
    it uses global word-to-word co-occurrence statistics to determine the linear relationships
    between words:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe是另一种流行的将单词编码为数值向量的技术，由斯坦福大学开发。与基于连续窗口的方法相比，它使用全局单词到单词共现统计来确定单词之间的线性关系：
- en: 'Let''s take a look at the pretrained 6 B tokens embedding trained on Wikipedia
    and the Gigaword news archive:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看在维基百科和Gigaword新闻档案上训练的预训练6B标记嵌入：
- en: '[PRE43]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the preceding code, we only open and parse the pretrained word embedding
    in order to store the word and vectors in a lookup dictionary.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只打开并解析预训练的词嵌入，以便将单词和向量存储在查找字典中。
- en: 'Then, we use the dictionary to look up tokens in our training data and merge
    them by computing the mean of all GloVe vectors:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用这个字典在我们的训练数据中查找标记，并通过计算所有GloVe向量的平均值来合并它们：
- en: '[PRE44]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding code works very similar to before and returns one vector per word,
    which is aggregated by taking their mean at the end. Again, this corresponds with
    a semantic concept using all the tokens of the training data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码与之前非常相似，每个单词返回一个向量，最后通过取平均值进行聚合。再次强调，这与使用训练数据中所有标记的语义概念相对应。
- en: Gensim provides other popular models for semantic embeddings, such as *doc2word*,
    *fastText*, and *GloVe*. The `gensim` Python library is a great place for utilizing
    these pretrained embeddings or for training your own models. Now you can replace
    your bag-of-words model with a mean embedding of the word vectors to also capture
    word semantics. However, your pipeline will still be built out of many tunable
    components.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim提供了其他流行的语义嵌入模型，如*doc2word*、*fastText*和*GloVe*。`gensim` Python库是利用这些预训练嵌入或训练您自己的模型的绝佳场所。现在您可以用单词向量的平均嵌入替换您的词袋模型，以捕获词义。然而，您的管道仍然由许多可调组件构建。
- en: In the next section, we will take a look at building end-to-end state-of-the-art
    language models and reusing some of the language features from Azure Cognitive
    Services.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨构建端到端最先进的语言模型以及重用Azure认知服务中的一些语言特征。
- en: Implementing end-to-end language models
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现端到端语言模型
- en: In the previous sections, we trained and concatenated multiple pieces to implement
    a final algorithm where most of the individual steps need to be trained as well.
    Lemmatization contains a dictionary of conversion rules. Stop words are stored
    in the dictionary. Stemming needs rules for each language and word that the embedding
    needs to train—TF-IDF and SVD are only computed on your training data but are
    independent of each other.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们训练和连接了多个部分以实现一个最终算法，其中大多数单个步骤也需要进行训练。词形还原包含一个转换规则字典。停用词存储在字典中。词干提取需要为每种语言和每个需要嵌入训练的单词制定规则——TF-IDF
    和 SVD 仅在训练数据上计算，但彼此独立。
- en: This is a similar problem to the traditional computer vision approach, which
    we will discuss in more depth in [*Chapter 10*](B17928_10_ePub.xhtml#_idTextAnchor165),
    *Training Deep Neural Networks on Azure*, where many classic algorithms are combined
    into a pipeline of feature extractors and classifiers. Similar to breakthroughs
    of end-to-end models trained via gradient descent and backpropagation in computer
    vision, deep neural networks—especially sequence-to-sequence models—have replaced
    the classical approach of performing each step of the transformation and training
    process manually.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个类似于传统计算机视觉方法的问题，我们将在[*第 10 章*](B17928_10_ePub.xhtml#_idTextAnchor165)“在
    Azure 上训练深度神经网络”中更深入地讨论，其中许多经典算法被组合成一个特征提取器和分类器的管道。类似于计算机视觉中通过梯度下降和反向传播训练的端到端模型的突破，深度神经网络——尤其是序列到序列模型——已经取代了手动执行每个转换和训练步骤的经典方法。
- en: In this section, first, we will take a look at improving our previous model
    using custom embedding and an LSTM implementation to model a token sequence. This
    will give you a good understanding of how we are moving from an individual preprocessor-based
    pipeline to a full end-to-end approach using deep learning.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先，我们将查看如何通过自定义嵌入和 LSTM 实现来改进我们之前的模型，以对标记序列进行建模。这将帮助你更好地理解我们是如何从基于单个预处理器管道的个体方法过渡到使用深度学习的完整端到端方法的。
- en: Sequence-to-sequence models are models based on encoders and decoders that are
    trained on a variable set of inputs. This encoder/decoder architecture is used
    for a variety of tasks, such as machine translation, image captioning, and summarization.
    A nice benefit of these models is that you can reuse the encoder part of this
    network to convert a set of inputs into a fixed-set numerical representation of
    the encoder.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型是基于在可变输入集上训练的编码器和解码器模型。这种编码器/解码器架构用于各种任务，如机器翻译、图像标题和摘要。这些模型的优点之一是，你可以重用这个网络中的编码器部分，将一组输入转换为编码器的固定集数值表示。
- en: Next, we will look at the state-of-the-art language representation models and
    discuss how they can be used for feature engineering and the preprocessing of
    your text data. We will use BERT to perform sentiment analysis and numeric embedding.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨最先进的语言表示模型，并讨论它们如何用于特征工程和文本数据的预处理。我们将使用 BERT 进行情感分析和数值嵌入。
- en: Finally, we will also look at reusing the Azure Cognitive Services APIs for
    text analytics to carry out advanced modeling and feature extraction, such as
    text or sentence sentiment, keywords, or entity recognition. This is a nice approach
    because you can leverage the know-how and amount of training data from Microsoft
    to perform complex text analytics using a simple HTTP request.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将探讨如何重用 Azure 认知服务 API 进行文本分析，以执行高级建模和特征提取，例如文本或句子情感、关键词或实体识别。这是一个很好的方法，因为你可以利用微软的知识和大量训练数据，通过简单的
    HTTP 请求来执行复杂的文本分析。
- en: The end-to-end learning of token sequences
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记序列的端到端学习
- en: Instead of concatenating different pieces of algorithms into a single pipeline,
    we want to build and train an end-to-end model that can train the word embedding,
    pre-form latent semantic transformation, and capture sequential information in
    the text in a single model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想将不同的算法片段连接成一个单一的管道，而是想要构建和训练一个端到端模型，该模型可以训练词嵌入、预形式潜在语义转换，并在单个模型中捕获文本中的顺序信息。
- en: 'The benefit of such a model is that each processing step can be fine-tuned
    for the user''s prediction task in a single combined optimization process:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的优点是，每个处理步骤都可以在单个联合优化过程中针对用户的预测任务进行微调：
- en: 'The first part of the pipeline will look extremely similar to the previous
    sections. We will build a tokenizer that converts documents into sequences of
    tokens that are then transformed into a numerical model based on the token sequence.
    Then, we will use `pad_sequences` to align all of the documents to the same length:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道的第一部分将看起来与前几节非常相似。我们将构建一个标记化器，将文档转换为标记序列，然后将其转换为基于标记序列的数值模型。然后，我们将使用`pad_sequences`将所有文档对齐到相同的长度：
- en: '[PRE45]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In the next step, we will build a simple model using Keras, an embedding layer,
    and an LSTM layer to capture token sequences. The embedding layer will perform
    a similar operation to GloVe, where the words will be embedded into a semantic
    space. The LSTM cell will ensure that we are comparing sequences of words instead
    of single words at a time. Then, we will use a dense layer with a *softmax* activation
    to implement a classifier head:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，我们将使用Keras构建一个简单的模型，包括一个嵌入层和一个LSTM层来捕获标记序列。嵌入层将执行类似于GloVe的操作，将单词嵌入到语义空间中。LSTM单元将确保我们比较的是单词序列而不是单个单词。然后，我们将使用一个带有*softmax*激活函数的密集层来实现分类头：
- en: '[PRE46]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As you can see in the preceding function, we build a simple neural network using
    three layers (that is, `Embedding`, `LSTM`, and `Dense`) and a `softmax` activation
    for classification. This means that in order to train this model, we would also
    need a classification problem to be solved at the same time. Hence, we do need
    labeled training data to perform analysis using this approach. In the next section,
    we will examine how sequence-to-sequence models are used in input-output text
    sequences to learn an implicit text representation.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的函数所示，我们使用三个层（即`Embedding`、`LSTM`和`Dense`）和一个`softmax`激活函数构建了一个简单的神经网络，用于分类。这意味着为了训练此模型，我们还需要同时解决一个分类问题。因此，我们需要标记的训练数据来使用这种方法进行分析。在下一节中，我们将探讨序列到序列模型是如何在输入输出文本序列中用于学习隐式文本表示的。
- en: State-of-the-art sequence-to-sequence models
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最前沿的序列到序列模型
- en: In recent years, another type of model has replaced the traditional NLP pipelines—transformer-based
    models. These types of models are fully end-to-end and use sequence-to-sequence
    mapping, positional encoding, and multi-head attention layers. This allows the
    models to look forward and backward in a text, pay attention to specific patterns,
    and learn tasks fully end to end. As you might be able to tell, these models have
    complex architectures and usually have well over 100 million or over 1 billion
    parameters.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在近年来，另一种类型的模型已经取代了传统的NLP管道——基于transformer的模型。这些类型的模型是完全端到端的，并使用序列到序列映射、位置编码和多头注意力层。这使得模型能够在文本中向前和向后查看，关注特定模式，并完全端到端地学习任务。正如你可能已经猜到的，这些模型具有复杂的架构，通常有超过一亿或超过十亿的参数。
- en: Sequence-to-sequence models are now state of the art for many complex end-to-end
    NLP problems such as classification (for example, sentiment or text analysis),
    language understanding (for example, entity recognition), translation, text generation,
    summarization, and more.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型现在在许多复杂的端到端自然语言处理（NLP）问题中处于最前沿，例如分类（例如，情感或文本分析）、语言理解（例如，实体识别）、翻译、文本生成、摘要等等。
- en: One popular sequence-to-sequence model is BERT, which, today, exists in many
    different variations and configurations. Models based on the BERT architecture
    seem to perform particularly well but have already been outperformed by newer
    updated architectures, tuned parameters, or models with more training data.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的序列到序列模型是BERT，今天，它存在许多不同的变体和配置。基于BERT架构的模型似乎表现特别出色，但已经被更新的架构、调整的参数或具有更多训练数据的模型所超越。
- en: The easiest way to get started using these new NLP models is with the *Hugging
    Face* `transformers` library, which provides end-to-end models (or pipelines)
    along with pretrained tokenizers and models. The `transformers` library implements
    all model architectures for both *TensorFlow* and *PyTorch*. The models can be
    easily consumed and used in an application, trained from scratch, or fine-tuned
    using domain-specific custom training data.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些新的NLP模型的最简单方法是通过*Hugging Face*的`transformers`库，该库提供了端到端模型（或管道）以及预训练的标记器和模型。`transformers`库实现了*TensorFlow*和*PyTorch*的所有模型架构。这些模型可以轻松地在应用程序中使用，从头开始训练，或者使用特定领域的自定义训练数据进行微调。
- en: 'The following example shows how to implement sentiment analysis using the default
    `sentiment-analysis` pipeline, which, at the time of writing, uses the `TFDistilBertForSequenceClassification`
    model:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用默认的 `sentiment-analysis` 流程实现情感分析，该流程在撰写本文时使用 `TFDistilBertForSequenceClassification`
    模型：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As you can see in the previous example, it's very simple to use a pretrained
    model for an end-to-end prediction task. These three lines of code can easily
    be integrated into your feature extraction pipeline to enrich your training data
    with sentiments.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，使用预训练模型进行端到端预测任务非常简单。这三行代码可以轻松集成到你的特征提取流程中，以丰富你的训练数据中的情感。
- en: Besides end-to-end models, another popular application of NLP is to provide
    semantic embeddings for textual data during preprocessing. This can also be implemented
    using the `transformers` library and any of the many supported models.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 除了端到端模型之外，NLP 的另一个流行应用是在预处理文本数据时提供语义嵌入。这也可以使用 `transformers` 库和许多支持的模型之一来实现。
- en: 'To do this, first, we initialize a pretrained tokenizer for BERT. This will
    help us to split the input data into the correct format for the BERT model:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，首先，我们初始化一个预训练的 BERT 分词器。这将帮助我们将输入数据分割成适合 BERT 模型的正确格式：
- en: '[PRE48]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once we have transformed the input into a token sequence, we can evaluate the
    BERT model. To retrieve the numerical embedding, we need to understand the latent
    state of the encoder, which we can retrieve using the `last_hidden_state` property:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将输入转换为标记序列，我们就可以评估 BERT 模型。要检索数值嵌入，我们需要理解编码器的潜在状态，我们可以使用 `last_hidden_state`
    属性来检索：
- en: '[PRE49]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The last hidden layer contains the latent representation of the model, which
    we can now use as a semantic numerical representation in our model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的隐藏层包含模型的潜在表示，我们现在可以用作模型中的语义数值表示：
- en: '[PRE50]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The key takeaway from these models is that they use an encoder/decoder-based
    architecture, which allows us to simply borrow the encoder to embed text into
    a semantic numerical feature space. Hence, a common approach is to download the
    pretrained model and perform a forward pass through the encoder part of the network.
    The fixed-sized numerical output can now be used as a feature vector for any other
    model. This is a common preprocessing step and a good trade-off for using a state-of-the-art
    language model for numerical embedding.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的关键启示是它们使用基于编码器/解码器的架构，这使我们能够简单地借用编码器将文本嵌入到语义数值特征空间中。因此，一个常见的方法是下载预训练模型，并通过网络的编码器部分进行正向传递。现在，固定大小的数值输出可以用作任何其他模型的特征向量。这是一个常见的预处理步骤，也是使用最先进的语言模型进行数值嵌入的良好权衡。
- en: Text analytics using Azure Cognitive Services
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Azure 认知服务的文本分析
- en: A good approach in many engineering disciplines is to not reinvent the wheel
    when many other companies have already solved the same problem far better than
    you will ever be able to solve it. This might be the case for basic text analytics
    and text understanding tasks that Microsoft has developed, implemented, and trained
    and now offers as a service.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多工程学科中，一个好的方法是不重复造轮子，因为许多其他公司已经比你更好地解决了相同的问题。对于微软开发、实施和训练的基本文本分析和文本理解任务，现在作为服务提供的情况可能也是如此。
- en: What if I told you that when working with Azure, text understanding features
    such as sentiment analysis, key phrase extraction, language detection, named entity
    recognition, and the extraction of **Personally Identifiable Information** (**PII**)
    is just one request away? Azure provides the Text Analytics API as part of Cognitive
    Services, which will solve all of these problems for you.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我告诉你，当使用 Azure 时，文本理解功能，如情感分析、关键词提取、语言检测、命名实体识别以及**个人身份信息**（**PII**）的提取，只需一个请求即可？Azure
    提供的 Text Analytics API 作为认知服务的一部分，将为您解决所有这些问题。
- en: This won't solve the need to transform a piece of text into numerical values,
    but it will make it easier to extract semantics from your text. One example would
    be to perform a key phrase extraction or sentiment analysis using Cognitive Services
    as an additional feature engineering step, instead of implementing your own NLP
    pipeline.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不能解决将文本转换为数值的需求，但它会使从文本中提取语义变得更加容易。一个例子就是使用认知服务作为额外的特征工程步骤，执行关键词提取或情感分析，而不是实现自己的
    NLP 流程。
- en: 'Let''s implement a function that returns the sentiment for a given document
    using the Text Analytics API of Cognitive Services. This is great when you want
    to enrich your data with additional attributes, such as overall sentiment, in
    the text. Let''s start by setting up all the parameters we will need to call the
    Cognitive Services API:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个函数，使用认知服务的文本分析API返回给定文档的情感。当你想在文本中添加额外的属性，如整体情感时，这非常棒。让我们首先设置所有需要调用认知服务API的参数：
- en: '[PRE51]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we define the content and metadata of the request. We create a `payload`
    object that contains a single document and the text we want to analyze:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义请求的内容和元数据。我们创建一个包含单个文档和要分析的文本的`payload`对象：
- en: '[PRE52]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we need to send the payload, heads, and parameters to the Cognitive
    Services API:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将payload、头部和参数发送到认知服务API：
- en: '[PRE53]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code looks very similar to the computer vision example that we
    saw in [*Chapter 2*](B17928_02_ePub.xhtml#_idTextAnchor034), *Choosing the Right
    Machine Learning Service in Azure*. In fact, it uses the same API but just a different
    endpoint for Text Analytics and, in this case, sentiment analysis functionality.
    Let''s run this code and look at the output, which looks very similar to the following
    snippet:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码看起来与我们在[*第2章*](B17928_02_ePub.xhtml#_idTextAnchor034)中看到的计算机视觉示例非常相似，即*在Azure中选择合适的机器学习服务*。事实上，它使用的是相同的API，但只是为文本分析和在此情况下情感分析功能使用了不同的端点。让我们运行这段代码并查看输出，输出看起来与以下片段非常相似：
- en: '[PRE54]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We can observe that the JSON response contains a sentiment classification for
    each document (`positive`, `neutral`, and `negative`) as well as numeric confidence
    scores for each class. Also, you can see that the resulting documents are stored
    in an array and marked with an `id` value. Hence, you can send multiple documents
    to this API using an ID to identify each document.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，JSON响应包含每个文档的情感分类（`积极`、`中性`和`消极`）以及每个类别的数字置信度分数。此外，你可以看到，生成的文档存储在一个数组中，并标记了一个`id`值。因此，你可以使用ID来标识每个文档，向此API发送多个文档。
- en: Using custom pretrained language models is great, but for standardized text
    analytics, we can simply reuse Cognitive Services. Microsoft has invested tons
    of resources into the research and production of these language models, which
    you can use for your own data pipelines for a relatively small amount of money.
    Therefore, if you prefer using a managed service instead of running your customer
    transformer model, you should try this Text Analytics API.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义预训练的语言模型很棒，但对于标准化的文本分析，我们可以简单地重用认知服务。微软在研究和生产这些语言模型上投入了大量的资源，你可以用相对较少的费用为自己的数据管道使用这些模型。因此，如果你更喜欢使用托管服务而不是运行自己的客户transformer模型，你应该尝试这个文本分析API。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to preprocess textual and categorical nominal
    and ordinal data using state-of-the-art NLP techniques.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用最先进的NLP技术对文本和分类的定名和有序数据进行预处理。
- en: You can now build a classical NLP pipeline with stop word removal, *lemmatization*
    and *stemming*, *n-grams*, and count term occurrences using a *bag-of-words* model.
    We used *SVD* to reduce the dimensionality of the resulting feature vector and
    to generate lower-dimensional topic encoding. One important tweak to the count-based
    bag-of-words model is to compare the relative term frequencies of a document.
    You learned about the *TF-IDF* function and can use it to compute the importance
    of a word in a document compared to the corpus.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用停用词去除、*词形还原*和*词干提取*、*n-gram*和计数词项出现来构建一个经典的NLP管道，使用*词袋模型*。我们使用*SVD*来降低结果特征向量的维度，并生成低维度的主题编码。对基于计数的词袋模型的一个重要调整是比较文档中术语的相对频率。你学习了*TF-IDF*函数，并可以使用它来计算一个词在文档中的重要性，与语料库相比。
- en: In the following section, we looked at *Word2Vec* and *GloVe*, which are pretrained
    dictionaries of numeric word embeddings. Now you can easily reuse a pretrained
    word embedding for commercial NLP applications with great improvements and accuracy
    due to the semantic embedding of words.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们探讨了*Word2Vec*和*GloVe*，它们是预训练的数字词嵌入字典。现在，你可以轻松地重用预训练的词嵌入，在商业NLP应用中实现显著的改进和准确性，这得益于词的语义嵌入。
- en: Finally, we finished the chapter by looking at a state-of-the-art approach to
    language modeling, using end-to-end language representations, such as *BERT* and
    BERT-based architectures, which are trained as sequence-to-sequence models. The
    benefit of these models is that you can reuse the encoder to transform a sequence
    of text into a numerical representation, which is a very common task during feature
    extraction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过研究一种最先进的方法来结束这一章节，该方法使用端到端语言表示，例如 *BERT* 和基于 BERT 的架构，这些架构被训练为序列到序列模型。这些模型的好处是你可以重用编码器将一系列文本转换为数值表示，这在特征提取过程中是一个非常常见的任务。
- en: In the next chapter, we will look at how to train an ML model using Azure Machine
    Learning, applying everything we have learned so far.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用 Azure Machine Learning 训练一个机器学习模型，应用我们迄今为止所学的一切。
