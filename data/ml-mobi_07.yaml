- en: Spam Message Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件检测
- en: This chapter will provide you with an overview of **natural language processing**
    (**NLP**) and discuss how NLP can be combined with machine learning to provide
    solutions to problems. Then, the chapter will take a real-world use case of doing
    spam message detection by utilizing NLP, combined with the linear SVM classification
    model. The program will be implemented as a mobile application using Core ML for
    iOS.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为您提供一个关于自然语言处理（NLP）的概述，并讨论如何将NLP与机器学习相结合，以提供解决方案。然后，本章将探讨一个利用NLP和线性SVM分类模型进行垃圾邮件检测的真实世界用例。程序将作为iOS的Core
    ML移动应用程序实现。
- en: To handle text in machine learning algorithms, we will go through the various
    NLP techniques that will be used on the text data to make it ready for learning
    algorithms. Once the text is prepared, we will see how we can classify it using
    the linear SVM model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在机器学习算法中处理文本，我们将探讨将在文本数据上使用的各种NLP技术，以便使其准备好用于学习算法。一旦文本准备好，我们将看到如何使用线性SVM模型对其进行分类。
- en: '**Problem definition**: The bulk SMS message data is provided, and these messages
    need to be classified as spam or non-spam messages.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题定义**：提供了大量短信消息数据，这些消息需要被分类为垃圾邮件或非垃圾邮件。'
- en: 'We will be covering the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding NLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自然语言处理（NLP）
- en: Understanding the linear SVM algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线性SVM算法
- en: 'Solving the problem using linear SVM in Core ML:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Core ML中的线性SVM解决该问题：
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术要求
- en: How to create the model file using scikit-learn
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用scikit-learn创建模型文件
- en: Testing the model
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: Importing the scikit-learn model into the Core ML project
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将scikit-learn模型导入Core ML项目
- en: Writing an iOS mobile application, using the scikit-learn model in it, and doing
    spam message detection
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn模型编写iOS移动应用程序，并对其进行垃圾邮件检测
- en: Understanding NLP
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自然语言处理（NLP）
- en: NLP is a huge topic, and it is beyond the scope of this book to go into detail
    on the subject. However, in this section, we will go through the high-level details
    of NLP and try to understand the key concepts required to prepare and process
    the textual data using NLP, in order to make it ready for consumption by machine
    learning algorithms for prediction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是一个庞大的主题，本书的范围并不包括对这个主题的详细探讨。然而，在本节中，我们将探讨NLP的高级细节，并尝试理解使用NLP准备和处理文本数据所需的关键概念，以便使其准备好由机器学习算法进行预测。
- en: Introducing NLP
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍自然语言处理（NLP）
- en: Huge, unstructured textual data is getting generated on a daily basis. Social
    media, websites such as Twitter and Facebook, and communication apps, such as
    WhatsApp, generate an enormous volume of this unstructured data daily—not to mention
    the volume created by blogs, news articles, product reviews, service reviews,
    advertisements, emails, and SMS. So, to summarize, there is **huge data **(in
    TBS).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每天都会产生大量非结构化文本数据。社交媒体、如Twitter和Facebook的网站，以及如WhatsApp的通讯应用，每天都会产生大量的这种非结构化数据——更不用说博客、新闻文章、产品评论、服务评论、广告、电子邮件和短信产生的数据量了。所以，总结来说，存在大量的数据（以TB为单位）。
- en: 'However, it is not possible for a computer to get any insight from this data
    and to carry out specific actions based on the insights, directly from this huge
    data, because of the following reasons:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于以下原因，计算机无法从这些数据中获得任何洞察力，并直接根据这些洞察力执行特定操作，因为这些数据量巨大：
- en: The data is unstructured
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是非结构化的
- en: The data cannot be understood directly without preprocessing
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有预处理，数据无法直接理解
- en: This data cannot be directly fed in an unprocessed form into any ML algorithms
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些数据不能以未经处理的形式直接输入到任何机器学习算法中
- en: To make this data more meaningful and to derive information from it, we use
    NLP. The field of study that focuses on the interactions between human language
    and computers is called **NLP**. NLP is a branch of data science that is closely
    related to computational linguistics. It deals with the science of the computer –
    analyzing, understanding, and deriving information from human natural language-based
    data, which is usually unstructured like text, speech, and so on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些数据更有意义并从中提取信息，我们使用自然语言处理（NLP）。专注于人类语言与计算机之间交互的研究领域被称为**自然语言处理（NLP**）。NLP是数据科学的一个分支，与计算语言学密切相关。它涉及计算机科学——分析、理解和从基于人类自然语言的数据（通常是文本、语音等非结构化数据）中提取信息。
- en: Through NLP, computers can analyze and derive meaning from human language and
    do many useful things. By utilizing NLP, many complex tasks, such as an automatic
    summary of huge documents, translations, relationship extraction between a different
    mass of unstructured data, sentiment analysis, and speech recognition, can be
    accomplished.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过NLP，计算机可以分析和从人类语言中提取意义，并执行许多有用的事情。通过利用NLP，可以完成许多复杂任务，例如自动总结大量文档、翻译、不同大量非结构化数据之间的关系提取、情感分析和语音识别。
- en: 'For computers to understand and analyze human language, we need to analyze
    the sentence in a more structured manner and understand the core of it. In any
    sentence, we need to understand three core things:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让计算机理解和分析人类语言，我们需要以更结构化的方式分析句子并理解其核心。在任何句子中，我们需要理解三个核心内容：
- en: '**Semantic information**: This relates to the meaning of the sentence. This
    is the specific meaning of the words in the sentence, for example, *The k**ite
    flies*. Here, we don''t know whether the kite is man-made or a bird.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义信息**：这关系到句子的含义。这是句子中单词的具体含义，例如，*风筝飞翔*。在这里，我们不知道风筝是人造的还是鸟类的。'
- en: '**Syntactic information**: This relates to the structure of the sentence. This
    is the specific syntactic meaning of the words in a sentence. *Sreeja saw Geetha
    with candy*. Here, we are not sure who has the candy: Sreeja or Geetha?'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句法信息**：这关系到句子的结构。这是句子中单词的具体句法含义。*Sreeja用糖果看到了Geetha*。在这里，我们不确定谁有糖果：Sreeja还是Geetha？'
- en: '**Pragmatic information (context)**: This relates to the context (linguistic
    or non-linguistic) of the sentence. This is the specific context in which the
    words in the sentence are used. For example, *He is out* in the context of baseball
    and healthcare is different.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语用信息（上下文）**：这关系到句子的上下文（语言或非语言）。这是句子中单词使用的具体上下文。例如，*他在外面*在棒球和医疗保健的上下文中是不同的。'
- en: 'However, computers cannot analyze and recognize sentences as humans do. Therefore,
    there is a well-defined way to enable computers to perform text processing. Here
    are the main steps involved in that exercise:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算机不能像人类那样分析和识别句子。因此，有一个明确的方法可以使计算机执行文本处理。以下是该练习涉及的主要步骤：
- en: '**Preprocessing**: This step deals with removing all the noise from the sentence,
    so the only information critical in the context of the sentence is retained for
    the next step. For example, language stop words ("noise"), such as *is*, *the*,
    or *an*, can be removed from the sentence for further processing. When processing
    the sentence, the human brain doesn''t take into consideration the noise that''s
    present in the language. Similarly, the computer can be fed with noiseless text
    for further processing.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：这一步涉及从句子中移除所有噪声，以便只保留对句子上下文至关重要的信息供下一步使用。例如，语言停用词（“噪声”），如*是*、*这*或*一个*，可以从句子中移除以进行进一步处理。在处理句子时，人脑不会考虑语言中存在的噪声。同样，计算机可以接收无噪声文本以进行进一步处理。'
- en: '**Feature engineering**: For the computer to process the preprocessed text,
    it needs to know the key features of the sentence. This is what is accomplished
    through the feature engineering step.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征工程**：为了计算机能够处理预处理后的文本，它需要了解句子的关键特征。这是通过特征工程步骤实现的。'
- en: '**NLP processing**: With the human language converted into a feature matrix,
    the computer can perform NLP processing, which could either be classification,
    sentiment analysis, or text matching.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**NLP处理**：将人类语言转换为特征矩阵后，计算机可以执行NLP处理，这可能是分类、情感分析或文本匹配。'
- en: Now, let's try to understand the high-level activities that would be performed
    in each of these steps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试理解在每个步骤中将要执行的高级活动。
- en: Text-preprocessing techniques
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本预处理技术
- en: 'Before we can process text, it needs to be preprocessed. Preprocessing would
    deal with the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们处理文本之前，它需要先进行预处理。预处理将涉及以下内容：
- en: Removing noise from the text under consideration
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从考虑的文本中移除噪声
- en: Normalizing the sentence
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化句子
- en: Standardizing the sentence
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化句子
- en: There can be additional steps, such as a grammar check or spellcheck, based
    on the requirements.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需求，可能会有额外的步骤，例如语法检查或拼写检查。
- en: Removing noise
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除噪声
- en: Any text present in the sentence that may not be relevant to the context of
    the data can be termed noise.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中出现的任何可能不与数据上下文相关的文本都可以称为噪声。
- en: For example, this can include language stop words (commonly used words in a
    language – *is*, *am*, *the*, *of*, and *in*), URLs or links, social media entities
    (mentions, hashtags), and punctuation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这可以包括语言停用词（在语言中常用词汇 – *is*、*am*、*the*、*of*和*in*）、URL或链接、社交媒体实体（提及、标签）、以及标点符号。
- en: To remove the noise from the sentence, the general approach is to maintain a
    dictionary of noise words and then iterate through the tokens of the sentence
    under consideration against this dictionary and remove matching stop words. The
    dictionary of noise words is updated frequently to cover all possible noise.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从句子中去除噪声，一般的方法是维护一个噪声词字典，然后迭代考虑的句子中的标记与该字典进行对比，并移除匹配的停用词。噪声词字典会频繁更新以覆盖所有可能的噪声。
- en: Normalization
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规范化
- en: The disparities of words in sentences are converted into a normalized form.
    The words in a sentence may vary, such as *sing*, *singer*, *sang*, or *singing*,
    but they all would more or less fit into the same context and could be standardized.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中单词的差异被转换为规范化的形式。句子中的单词可能有所不同，如*sing*、*singer*、*sang*或*singing*，但它们或多或少都能适应相同的上下文，并且可以被标准化。
- en: 'There are different ways to normalize sentences:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来规范化句子：
- en: '**Stemming: **A basic rule-based process of stripping the suffixes (*-ing*,
    *-ly*, *-es*, *-s*) from a word.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：从单词中去除后缀（如*ing*、*ly*、*es*、*s*）的基本基于规则的过程。'
- en: '**Lemmatization: **The more sophisticated procedure to identify the root form
    of a word. It involves a more complex process of verifying the semantics and syntax.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原**：识别单词词根的更复杂的过程。它涉及一个更复杂的过程来验证语义和句法。'
- en: Standardization
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化
- en: This step involves standardizing the sentence to make sure it contains tokens
    that are from the standard language dictionary only and not anything else, such
    as hashtags, colloquial words, and so on. All these are removed in this step.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤涉及标准化句子，以确保它只包含来自标准语言字典的标记，而不包含任何其他内容，如标签、俚语词汇等。所有这些都在此步骤中被移除。
- en: Feature engineering
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Now that the text has been processed, the next step to arrange the features
    from the text so that they can be fed into any machine learning algorithm to carry
    out classification, clustering, and so on. There are various methods to convert
    the text into a feature matrix, and we will go through some of them in this section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文本已经被处理，下一步是将文本中的特征排列好，以便它们可以被输入到任何机器学习算法中执行分类、聚类等操作。有各种方法将文本转换为特征矩阵，我们将在本节中介绍其中一些。
- en: Entity extraction
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体提取
- en: Here, the key entities from the sentence that would be used for NLP processing
    are extracted. **Named entity recognition** (**NER**) is one such method, where
    the entities could be named entities, such as that of a place, person, or monument.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，从句子中提取出用于NLP处理的关键实体。**命名实体识别**（NER）是其中一种方法，其中的实体可以是地点、人物或纪念碑等命名实体。
- en: Topic modeling
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: This is another method, where the topics are identified from the corpus of text.
    The topics can be single words, patterns of words, or sequences of co-occurring
    words. Based on a number of words in the topic, these could be called **N-Gram.** So,
    based on context and repeatability, bigrams and trigrams could be used as features.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一种方法，从文本语料库中识别主题。主题可以是单个单词、单词模式或共现单词序列。根据主题中的单词数量，这些可以被称为**N-gram**。因此，基于上下文和重复性，可以使用双词和三词作为特征。
- en: Bag-of-words model
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: A bag-of-words model is a representation of text that describes the occurrence
    of words within a document. It involves the representation of known words and
    a measure of the presence of known words in the document. The model is more centered
    around the occurrence of known words in the document, and not about the order
    of words or the structure of words in the document.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是对文本的表示，描述了文档中单词的出现情况。它涉及已知单词的表示和已知单词在文档中的出现度量的计算。该模型更侧重于文档中已知单词的出现，而不是单词的顺序或文档中单词的结构。
- en: Statistical Engineering
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计工程
- en: Text data can also be represented as numerical values using various techniques.
    **Term Frequency-Inverse Document Frequency** (**TF-IDF**) for a huge corpus of
    text documents is an important technique in this class.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据也可以使用各种技术表示为数值。对于大量文本文档，**词频-逆文档频率**（TF-IDF）是此类中的重要技术。
- en: TF–IDF
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF–IDF
- en: TF-IDF is a weighted model  that's used to convert the text documents into vector
    models on the basis of the occurrence of words in the documents without considering
    the exact ordering of text in the document.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是一种加权模型，它基于文档中单词的出现次数将文本文档转换为向量模型，而不考虑文档中文本的确切顺序。
- en: Let's consider a set of N text documents and any one document to be D. Then,
    we define the following.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一组 N 个文本文档和任意一个文档 D。然后，我们定义以下内容。
- en: TF
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF
- en: 'This measures how frequently a term occurs in a document. Since every document
    is a different length, it is possible that a term would appear more in long documents
    than shorter ones. Thus, the TF is often divided by the document length to normalize
    it:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量了一个术语在文档中出现的频率。由于每个文档的长度不同，一个术语可能在长文档中比在短文档中出现得更频繁。因此，TF 通常会被除以文档长度以进行归一化：
- en: '*TF(t) = (Number of times term t appears in a document(D))/(Total number of
    terms in the document(N))*.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(t) = (文档 D 中术语 t 出现的次数)/(文档中的总词数 N)*.'
- en: Inverse Document Frequency (IDF)
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆文档频率 (IDF)
- en: 'This measures how important a term is for the corpus. While computing TF, all
    terms are considered equally important. However, it is common thinking that stop
    words occur more often, but they are less important as far as NLP is concerned.
    Thus, there is a need to bring down the importance of common terms and bring up
    the importance of rare terms, hence the IDF, which is calculated as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量了一个术语对于语料库的重要性。在计算 TF 时，所有术语都被视为同等重要。然而，通常认为停用词出现得更频繁，但就 NLP 而言，它们的重要性较低。因此，有必要降低常见术语的重要性，提高罕见术语的重要性，这就是
    IDF，其计算如下：
- en: '*IDF(t) = log_e(Total number of documents/Number of documents with term t in
    it)*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF(t) = log_e(文档总数/包含术语 t 的文档数)*'
- en: TF-IDF
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: 'The TF IDF formula gives the relative importance of a term in a corpus (list
    of documents), given by the following formula:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 公式给出了一个术语在语料库（文档列表）中的相对重要性，其公式如下：
- en: '![](img/2f33724e-a938-4bcc-a947-1a91644caded.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f33724e-a938-4bcc-a947-1a91644caded.png)'
- en: 'Where:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*tf*[*i,j*] = number of occurence of *i* in *j*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tf*[*i,j*] = *i* 在 *j* 中的出现次数'
- en: '*df[i]* = number of documents containing *i*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*df[i]* = 包含 *i* 的文档数'
- en: '*N* = total number of document'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* = 文档总数'
- en: Consider a document that contains 1,000 words, wherein the word rat appears
    3 times. The **term frequency** (**TF**) for rat is then (3/1000=) 0.003\. Now,
    in 10,000 documents, the word cat appears in 1,000 of them. Therefore, the **inverse
    document frequency** (**IDF**) is calculated as log(10000/1000) = 1\. Thus, the
    TF-IDF weight is the product of these quantities is 0.003 * 1 = 0.12.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含 1,000 个单词的文档，其中单词“rat”出现了 3 次。那么“rat”的**词频**（**TF**）就是 (3/1000=) 0.003。现在，在
    10,000 个文档中，单词“cat”出现在其中的 1,000 个。因此，**逆文档频率**（**IDF**）计算为 log(10000/1000) = 1。因此，TF-IDF
    权重是这些数量的乘积，即 0.003 * 1 = 0.12。
- en: The words or features in the text corpus could also be organized as feature
    vectors for easy feeding into the next step of NLP processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 文本语料库中的单词或特征也可以组织成特征向量，以便于将其输入到 NLP 处理的下一步。
- en: Classifying/clustering the text
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对文本进行分类/聚类
- en: The last step is to actually carry out classification or clustering using the
    feature engineered matrix or word vectors. We could use any classification algorithm
    and feed the feature vector to carry out classification or clustering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是实际使用特征工程矩阵或词向量进行分类或聚类。我们可以使用任何分类算法并将特征向量输入以执行分类或聚类。
- en: Similar to carrying out the clustering, different similarity measures could
    be used, such as Cosine Distance or Levenshtein distance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与执行聚类类似，可以使用不同的相似度度量，例如余弦距离或 Levenshtein 距离。
- en: Understanding linear SVM algorithm
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解线性 SVM 算法
- en: In [Chapter 2](1b52495b-c6cb-4197-8fcd-a1e764c1f1c2.xhtml), *Supervised and
    Unsupervised Learning Algorithms*, we covered the SVM algorithm and now have an
    idea of how the SVM model works. A linear support vector machine or linear SVM is
    a linear classifier that tries to find a hyperplane with the largest margin that
    splits the input space into two regions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 2 章](1b52495b-c6cb-4197-8fcd-a1e764c1f1c2.xhtml) “监督和非监督学习算法”中，我们介绍了 SVM
    算法，现在我们已经了解了 SVM 模型的工作原理。线性支持向量机或线性 SVM 是一种线性分类器，它试图找到一个具有最大边缘的超平面，将输入空间分割成两个区域。
- en: A hyperplane is a generalization of a plane. In one dimension, a hyperplane
    is called a **point**. In two dimensions, it is a line. In three dimensions, it
    is a plane. In more dimensions, you can call it a hyperplane.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面是平面的推广。在一维中，超平面被称为**点**。在二维中，它是一条线。在三维中，它是一个平面。在更多维度中，你可以称之为超平面。
- en: 'As we saw, the goal of SVM is to identify the hyperplane that tries to find
    the largest margin that splits the input space into two regions. If the input
    space is linearly separable, it is easy to separate them. However, in real life,
    we find that the input space is very non-linear:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，SVM的目标是识别一个试图找到最大间隔的超平面，将输入空间分成两个区域。如果输入空间是线性可分的，那么很容易将它们分开。然而，在现实生活中，我们发现输入空间非常非线性：
- en: '![](img/53a98c4a-8f3d-4517-9e7e-c004a1a91794.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53a98c4a-8f3d-4517-9e7e-c004a1a91794.png)'
- en: In the preceding scenario, the SVM can help us separate the red and blue balls
    by using what is called a **Kernel Trick**, which is the method of using a linear
    classifier to solve a non-linear problem.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的场景中，SVM可以通过所谓的**核技巧**帮助我们分离红色和蓝色球，核技巧是使用线性分类器来解决非线性问题的方法。
- en: The kernel function is applied to each data instance to map the original non-linear
    observations into a higher-dimensional space in which they become separable.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数应用于每个数据实例，将原始的非线性观测映射到一个更高维的空间，在这个空间中它们变得可分离。
- en: 'The most popular kernel functions available are as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的核函数如下：
- en: The linear kernel
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性核
- en: The polynomial kernel
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式核
- en: The RBF (Gaussian) kernel
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBF（高斯）核
- en: The string kernel
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串核
- en: The linear kernel is often recommended for text classification, as most text
    classification problems need to be categorized into two classes. In our example,
    we also want to classify the SMS messages into spam and non-spam.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 线性核通常被推荐用于文本分类，因为大多数文本分类问题都需要将文本分类为两类。在我们的例子中，我们还想将短信消息分类为垃圾邮件和非垃圾邮件。
- en: Solving the problem using linear SVM in Core ML
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Core ML中使用线性SVM解决问题
- en: In this section, we are going to look at how we can solve the spam message detection
    problem using all the concepts we have gone through in this chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何使用本章中介绍的所有概念来解决垃圾邮件检测问题。
- en: We are going to take a bunch of SMS messages and attempt to classify them as
    spam or non-spam. This is a classification problem and we will use the linear
    SVM algorithm to perform this, considering the advantages of using this algorithm
    for text classification.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选取一些短信消息并尝试将它们分类为垃圾邮件或非垃圾邮件。这是一个分类问题，我们将使用线性SVM算法来完成这个任务，考虑到使用此算法进行文本分类的优势。
- en: We are going to use NLP techniques to convert the data-SMS messages into a feature
    vector to feed into the linear SVM algorithm. We are going to use the scikit-learn
    vectorizer methods to transform the SMS messages into the TF-IDF vector, which
    could be fed into the linear SVM model to perform SMS spam detection (classification
    into spam and non-spam).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用NLP技术将数据-SMS消息转换为特征向量，以输入到线性SVM算法中。我们将使用scikit-learn的vectorizer方法将短信消息转换为TF-IDF向量，该向量可以输入到线性SVM模型中，以执行短信垃圾邮件检测（将短信分类为垃圾邮件和非垃圾邮件）。
- en: About the data
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于数据
- en: The data that we are using to create the model that detects the spam messages
    is taken from [http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/), which
    contains 747 spam message samples, along with 4,827 non-spam messages.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于创建检测垃圾邮件模型的数据来自[http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/)，其中包含747个垃圾邮件样本，以及4,827条非垃圾邮件。
- en: 'These messages are taken from different sources and labeled with the category
    of spam and non-spam. If you open the downloaded file in Notepad or any text editor,
    it will be in the following format:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些消息来自不同的来源，并标记为垃圾邮件和非垃圾邮件的类别。如果你在记事本或任何文本编辑器中打开下载的文件，它将具有以下格式：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding sample, we can see that every line starts with the category
    name and is followed by the actual message.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们可以看到每一行都以类别名称开头，后面跟着实际的消息。
- en: Technical requirements
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To create a model to classify a message as spam or non-spam, we need a library
    that is capable of doing so. Here, we've selected scikit-Learn.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个将消息分类为垃圾邮件或非垃圾邮件的模型，我们需要一个能够做到这一点的库。在这里，我们选择了scikit-Learn。
- en: 'To write this application, you need to have the Python3+ version installed
    on your desktop, and Xcode 9+ must be installed on your Mac machine. If you don''t
    have either of these, please check the appendix of this book to learn how to get
    them. Once you have installed Python in your machine, execute the following commands
    to get the required packages:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写这个应用，你需要在你的桌面电脑上安装Python3+版本，并且必须在你的Mac机器上安装Xcode 9+。如果你没有这些之一，请查阅本书的附录来学习如何获取它们。一旦你在你的机器上安装了Python，执行以下命令以获取所需的包：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the preceding code, we installed scikit-learn to get access to the algorithms
    and NumPy as the scikit-learn requires it, and pandas (*pandas* is an open source,
    BSD-licensed library providing high-performance, easy-to-use data structures and
    data analysis tools for the Python programming) to read the model from the file
    and core-ML tools to generate a Core ML model file.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们安装了scikit-learn以获取访问算法的权限，以及NumPy，因为scikit-learn需要它，还有pandas（*pandas*是一个开源的BSD许可库，为Python编程提供高性能、易于使用的数据结构和数据分析工具）来从文件中读取模型和core-ML工具来生成Core
    ML模型文件。
- en: Now, download `SMSSpamCollection.txt`, a plain text file from the model link
    stated in the preceding section, onto your disk and put it in your `project` folder.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，下载`SMSSpamCollection.txt`，一个从上一节中提到的模型链接的纯文本文件，并将其下载到你的磁盘上，然后放入你的`project`文件夹中。
- en: Creating the Model file using Scikit Learn
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit Learn创建模型文件
- en: 'In your project folder, create a python file with the following code to create
    a model file:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的项目文件夹中，创建一个包含以下代码的python文件来创建一个模型文件：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Test the fitted model, we can append the following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 测试拟合的模型，我们可以添加以下代码：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Upon executing the preceding program, it will show you whether the given message
    is spam or non-spam.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的程序后，它将显示给定的消息是垃圾邮件还是非垃圾邮件。
- en: Converting the scikit-learn model into the Core ML model
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将scikit-learn模型转换为Core ML模型
- en: In the preceding section, we created our model to classify the messages as spam and
    non-spam. Now, let's convert that into the Core ML model so that we can use that
    in an IOS app.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们创建了我们的模型来将消息分类为垃圾邮件和非垃圾邮件。现在，让我们将其转换为Core ML模型，以便我们可以在iOS应用中使用它。
- en: 'To create a core-ML model, append the following lines to the preceding code
    and run them. This will create a `.mlmodel` file:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建Core ML模型，将以下行添加到前面的代码中并运行它们。这将创建一个`.mlmodel`文件：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, you can take the generated `SpamMessageClassifier.mlmodel` file and use
    this in your Xcode.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以获取生成的`SpamMessageClassifier.mlmodel`文件，并在你的Xcode中使用它。
- en: Writing the iOS application
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写iOS应用
- en: 'You can get the code for the iOS project in our GitHub repository ([https://github.com/PacktPublishing/Machine-Learning-for-Mobile](https://github.com/PacktPublishing/Machine-Learning-for-Mobile)).
    Once you download the project and open the project in Xcode, you will find the
    directory structure:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们的GitHub仓库中获取iOS项目的代码（[https://github.com/PacktPublishing/Machine-Learning-for-Mobile](https://github.com/PacktPublishing/Machine-Learning-for-Mobile)）。一旦你下载了项目并在Xcode中打开它，你将找到以下目录结构：
- en: '![](img/130e69e7-19e1-4d52-baf7-dd837444d45e.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/130e69e7-19e1-4d52-baf7-dd837444d45e.png)'
- en: 'In this, I want to explain the important files to you. Main. Storyboard is
    having the UI design for the app:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想向你解释一些重要的文件。Main. Storyboard包含了应用的UI设计：
- en: '![](img/d7fdf840-e9a4-4f6d-84c3-ac32757e3fc9.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7fdf840-e9a4-4f6d-84c3-ac32757e3fc9.png)'
- en: 'Here, we have two labels, one button, and one text box. The two labels are
    a heading label and on result label. Button to submit the input and get the result.
    And we have a textbox to give a message as input. Here, the main processing is
    written in the `controller.swift` view:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有两个标签，一个按钮和一个文本框。两个标签是一个标题标签和一个结果标签。按钮用于提交输入并获取结果。我们还有一个文本框来输入消息。在这里，主要处理是在`controller.swift`视图中编写的：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When you run the app in the simulator of Xcode, it will generate the following
    results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在Xcode的模拟器中运行应用时，它将生成以下结果：
- en: '![](img/0d8d52c5-36b8-456b-8ee5-9dab19a0155f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d8d52c5-36b8-456b-8ee5-9dab19a0155f.png)'
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went through many things, such as, understanding NLP at
    a high level. There are various steps involved in NLP, such as text preprocessing,
    as well as techniques to carry this out, such as feature engineering and methods
    to perform feature engineering and classification or clustering of the feature
    vectors. We also looked into the linear SVM algorithm in which we went through
    the details of the SVM algorithm, the kernel function, and how it is more applicable
    to text classification.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了众多内容，例如，从高层次理解自然语言处理（NLP）。NLP涉及多个步骤，例如文本预处理，以及执行这些步骤的技术，如特征工程和执行特征工程以及特征向量分类或聚类的方
    法。我们还研究了线性SVM算法，其中我们详细介绍了SVM算法、核函数以及它如何更适用于文本分类。
- en: We solved our problem using linear SVM in Core ML and we also saw a practical
    example of performing spam message detection using the linear SVM algorithm model
    that we developed in scikit learn and converted into a Core ML model. We wrote
    an iOS application using the converted Core ML model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用线性SVM在Core ML中解决了我们的问题，并且我们还看到了一个使用我们在scikit learn中开发的线性SVM算法模型并转换为Core
    ML模型进行垃圾邮件检测的实际示例。我们使用转换后的Core ML模型编写了一个iOS应用程序。
- en: In the next chapter, we will be introduced to another ML framework, Fritz, which
    tries to solve the common problems that we see in model deployment and upgrades,
    and the unification of handling ML models across mobile OS platforms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一个机器学习框架Fritz，它试图解决我们在模型部署和升级中常见的各种问题，以及在不同移动操作系统平台间统一处理机器学习模型的方法。
