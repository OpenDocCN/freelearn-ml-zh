- en: 'Chapter 6: Training Natural Language Processing Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章：训练自然语言处理模型
- en: In the previous chapter, you learned how to use SageMaker's built-in algorithms
    for **computer vision** (**CV**) to solve problems including image classification,
    object detection, and semantic segmentation.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，您学习了如何使用 SageMaker 的内置算法进行 **计算机视觉**（**CV**）任务，解决包括图像分类、物体检测和语义分割等问题。
- en: '**Natural language processing** (**NLP**) is another very promising field in
    ML. Indeed, NLP algorithms have proven very effective in modeling language and
    extracting context from unstructured text. Thanks to this, applications such as
    search and translation applications and chatbots are now commonplace.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是机器学习中另一个非常有前景的领域。事实上，NLP 算法在建模语言和从非结构化文本中提取上下文方面已经证明非常有效。凭借这一点，像搜索和翻译应用程序以及聊天机器人等应用程序如今已经非常普遍。'
- en: 'In this chapter, you will learn about built-in algorithms designed specifically
    for NLP tasks and we''ll discuss the types of problems that you can solve with
    them. As in the previous chapter, we''ll also cover in great detail how to prepare
    real-life datasets such as Amazon customer reviews. Of course, we''ll train and
    deploy models too. We will cover all of this under the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习专门为 NLP 任务设计的内置算法，我们将讨论您可以用它们解决哪些类型的问题。与前一章一样，我们还将详细讲解如何准备实际数据集，如亚马逊客户评论。当然，我们还将训练和部署模型。我们将在以下主题中覆盖所有内容：
- en: Discovering the NLP built-in algorithms in Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现亚马逊 SageMaker 中的 NLP 内置算法
- en: Preparing natural language datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备自然语言数据集
- en: Using the built-in algorithms for NLP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的 NLP 算法
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an **Amazon Web Services** (**AWS**) account to run the examples
    included in this chapter. If you haven't got one already, please browse to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个 **Amazon Web Services**（**AWS**）账户才能运行本章中的示例。如果您还没有账户，请访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建一个。您还应该了解 AWS 免费套餐（[https://aws.amazon.com/free/](https://aws.amazon.com/free/)），它允许您在一定的使用限制内免费使用许多
    AWS 服务。
- en: You will need to install and configure the AWS **command-line interface** (**CLI**)
    tool for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要为您的账户安装并配置 AWS **命令行界面**（**CLI**）工具（[https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)）。
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory but
    strongly encouraged, as it includes many projects that we will need (Jupyter,
    `pandas`, `numpy`, and more).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个可用的 Python 3.x 环境。虽然安装 Anaconda 发行版（[https://www.anaconda.com/](https://www.anaconda.com/)）不是必需的，但强烈推荐安装，因为它包括我们将需要的许多项目（Jupyter、`pandas`、`numpy`
    等）。
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可以在 GitHub 上找到，链接为 [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。您需要安装
    Git 客户端才能访问这些代码（[https://git-scm.com/](https://git-scm.com/)）。
- en: Discovering the NLP built-in algorithms in Amazon SageMaker
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现亚马逊 SageMaker 中的 NLP 内置算法
- en: 'SageMaker includes four NLP algorithms, enabling **supervised learning** (**SL**)
    and **unsupervised learning** (**UL**) scenarios. In this section, you''ll learn
    about these algorithms, what kinds of problems they solve, and what their training
    scenarios are. Let''s have a look at an overview of the algorithms we''ll be discussing:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 包含四种 NLP 算法，支持 **有监督学习**（**SL**）和 **无监督学习**（**UL**）场景。在本节中，您将了解这些算法，它们能解决哪些问题，以及它们的训练场景。让我们先来看一下我们将讨论的算法概述：
- en: '**BlazingText** builds text classification models (SL) or computes word vectors
    (UL). **BlazingText** is an Amazon-invented algorithm.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BlazingText** 构建文本分类模型（SL）或计算词向量（UL）。**BlazingText** 是亚马逊发明的算法。'
- en: '**LDA** builds UL models that group a collection of text documents into topics.
    This technique is called **topic modeling**.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LDA** 构建 UL 模型，将一组文本文档分组为多个主题。这种技术称为 **主题建模**。'
- en: '**NTM** is another **topic modeling** algorithm based on neural networks, and
    it gives you more insight into how topics are built.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NTM** 是另一个基于神经网络的**主题建模**算法，它能让你更深入地了解主题是如何构建的。'
- en: '**Sequence to Sequence** (**seq2seq**) builds **deep learning** (**DL**) models,
    predicting a sequence of output tokens from a sequence of input tokens.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sequence to Sequence**（**seq2seq**）构建**深度学习**（**DL**）模型，从一系列输入标记预测一系列输出标记。'
- en: Discovering the BlazingText algorithm
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现BlazingText算法
- en: The BlazingText algorithm was invented by Amazon. You can read more about it
    at [https://dl.acm.org/doi/10.1145/3146347.3146354](https://dl.acm.org/doi/10.1145/3146347.3146354).
    BlazingText is an evolution of **FastText**, a library for efficient text classification
    and representation learning developed by Facebook ([https://fasttext.cc](https://fasttext.cc)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText算法是由亚马逊发明的。你可以在[https://dl.acm.org/doi/10.1145/3146347.3146354](https://dl.acm.org/doi/10.1145/3146347.3146354)阅读更多关于它的内容。BlazingText是**FastText**的演进，FastText是Facebook开发的一个高效文本分类和表示学习库
    ([https://fasttext.cc](https://fasttext.cc))。
- en: It lets you train text classification models, as well as computing **word vectors**.
    Also called **embeddings**, **word vectors** are the cornerstone of many NLP tasks,
    such as finding word similarities, word analogies, and so on. **Word2Vec** is
    one of the leading algorithms to compute these vectors ([https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)),
    and it's the one BlazingText implements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它让你能够训练文本分类模型，并计算**词向量**。词向量也叫做**嵌入**，是许多自然语言处理（NLP）任务的基石，比如寻找词语相似度、词语类比等。**Word2Vec**
    是计算这些向量的领先算法之一 ([https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781))，而BlazingText正是实现了这一算法。
- en: The main improvement of BlazingText is its ability to train on **graphics processing
    unit** (**GPU**) instances, where FastText only supports **central processing
    unit** (**CPU**) instances.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText的主要改进是能够在**图形处理单元**（**GPU**）实例上进行训练，而FastText只支持**中央处理单元**（**CPU**）实例。
- en: 'The speed gain is significant, and this is where its name comes from: "blazing"
    is faster than "fast"! If you''re curious about benchmarks, you''ll certainly
    enjoy this blog post: [https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 速度提升是显著的，这也是它名字的来源：“blazing”比“fast”更快！如果你对基准测试感兴趣，一定会喜欢这篇博客文章：[https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/)。
- en: Finally, BlazingText is fully compatible with FastText. Models can be very easily
    exported and tested, as you will see later in the chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，BlazingText与FastText完全兼容。你可以非常轻松地导出并测试模型，正如你稍后在本章中将看到的那样。
- en: Discovering the LDA algorithm
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现LDA算法
- en: This UL algorithm uses a generative technique, named **topic modeling**, to
    identify topics present in a large collection of text documents. It was first
    applied to ML in 2003 ([http://jmlr.csail.mit.edu/papers/v3/blei03a.html](http://jmlr.csail.mit.edu/papers/v3/blei03a.html)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个UL算法使用一种生成技术，叫做**主题建模**，来识别大规模文本文档集合中的主题。它首次应用于机器学习是在2003年 ([http://jmlr.csail.mit.edu/papers/v3/blei03a.html](http://jmlr.csail.mit.edu/papers/v3/blei03a.html))。
- en: 'Please note that LDA is not a classification algorithm. You pass it the number
    of topics to build, not the list of topics you expect. To paraphrase Forrest Gump:
    "*Topic modeling is like a box of chocolates, you never know what you''re gonna
    get."*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LDA并不是一个分类算法。你给它传递的是要构建的主题数量，而不是你期望的主题列表。用福雷斯特·甘普的话说：“*主题建模就像一盒巧克力，你永远不知道自己会得到什么。*”
- en: LDA assumes that every text document in a collection was generated from several
    latent (meaning "hidden") topics. A topic is represented by a word probability
    distribution. For each word present in a collection of documents, this distribution
    gives the probability that the word appears in documents generated by this topic.
    For example, in a "finance" topic, the distribution would yield high probabilities
    for words such as "revenue," "quarter," or "earnings," and low probabilities for
    "ballista" or "platypus" (or so I should think).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LDA假设文档集合中的每个文本都是由几个潜在（即“隐藏”）主题生成的。一个主题通过词语概率分布来表示。对于文档集合中的每个词，这个分布给出该词在由该主题生成的文档中出现的概率。例如，在“金融”主题中，分布会为“收入”、“季度”或“收益”等词汇给出较高的概率，而“弩炮”或“鸭嘴兽”等词汇则会有较低的概率（我想应该是这样）。
- en: Topic distributions are not considered independently. They are represented by
    a **Dirichlet distribution**, a multivariate generalization of univariate distributions
    ([https://en.wikipedia.org/wiki/Dirichlet_distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)).
    This mathematical object gives the algorithm its name.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 主题分布不是独立考虑的。它们通过**Dirichlet分布**表示，这是单变量分布的多变量推广（[https://en.wikipedia.org/wiki/Dirichlet_distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)）。这个数学对象赋予了算法它的名称。
- en: Given the number of words in the vocabulary and the number of latent topics,
    the purpose of the LDA algorithm is to build a model that is as close as possible
    to an ideal Dirichlet distribution. In other words, it will try to group words
    so that distributions are as well formed as possible and match the specified number
    of topics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定词汇表中的词语数量和潜在主题的数量，LDA算法的目的是建立一个尽可能接近理想Dirichlet分布的模型。换句话说，它会尝试将词语分组，使得分布尽可能良好，并与指定的主题数量匹配。
- en: 'Training data needs to be carefully prepared. Each document needs to be converted
    into a **bag-of-words** (**BoW**) representation: each word is replaced by a pair
    of integers, representing a unique word **identifier** (**ID**) and the word count
    in the document. The resulting dataset can be saved either to **comma-separated
    values** (**CSV**) format or to **RecordIO-wrapped protobuf** format, a technique
    we already studied with **factorization machines** in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据需要仔细准备。每个文档需要转换为**词袋**（**BoW**）表示：每个词语被一对整数替换，表示一个唯一的词语**标识符**（**ID**）和文档中该词的出现次数。最终的数据集可以保存为**逗号分隔值**（**CSV**）格式或**RecordIO包装的protobuf**格式，这是一种我们在[**第4章**](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)《训练机器学习模型》中已经学习过的技术。
- en: Once the model has been trained, we can score any document and get a score per
    topic. The expectation is that documents containing similar words should have
    similar scores, making it possible to identify their top topics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以对任何文档进行评分，并为每个主题打分。预期是，包含相似词语的文档应该具有相似的得分，从而使我们能够识别它们的主要主题。
- en: Discovering the NTM algorithm
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索NTM算法
- en: 'NTM is another algorithm for topic modeling. You can read more about it at
    [https://arxiv.org/abs/1511.06038](https://arxiv.org/abs/1511.06038). The following
    blog post also sums up the key elements of the paper: [https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: NTM是另一种主题建模算法。你可以在[https://arxiv.org/abs/1511.06038](https://arxiv.org/abs/1511.06038)上阅读更多关于它的信息。以下博客文章也总结了论文的关键要素：[https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/)。
- en: As with LDA, documents need to be converted to a BoW representation, and the
    dataset can be saved to either CSV or RecordIO-wrapped protobuf format.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与LDA一样，文档需要转换为BoW表示，并且数据集可以保存为CSV格式或RecordIO包装的protobuf格式。
- en: For training, NTM uses a completely different approach based on neural networks
    and—more precisely—on an encoder architecture ([https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)).
    In true DL fashion, the encoder trains on mini-batches of documents. It tries
    to learn their latent features by adjusting network parameters through backpropagation
    and optimization.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，NTM采用一种完全不同的方法，基于神经网络——更准确地说，是基于编码器架构（[https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder)）。真正的深度学习方式是，编码器在文档的小批量上进行训练。它通过反向传播和优化调整网络参数，试图学习它们的潜在特征。
- en: 'Unlike LDA, NTM can tell us which words are the most impactful in each topic.
    It also gives us two per-topic metrics, **word embedding topic coherence** (**WETC**)
    and **topic uniqueness** (**TU**). These are outlined in more detail here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与LDA不同，NTM可以告诉我们每个主题中最具影响力的词语。它还为每个主题提供了两个度量指标，**词嵌入主题一致性**（**WETC**）和**主题独特性**（**TU**）。这些在这里有更详细的描述：
- en: WETC tells us how semantically close the topic words are. This value is between
    0 and 1; the higher, the better. It's computed using the **cosine similarity**
    ([https://en.wikipedia.org/wiki/Cosine_similarity](https://en.wikipedia.org/wiki/Cosine_similarity))
    of the corresponding word vectors in a pretrained **Global Vectors** (**GloVe**)
    model (another algorithm similar to Word2Vec).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WETC 告诉我们主题词之间的语义相似度。该值介于 0 和 1 之间，值越高表示越好。它是通过预训练的 **全球向量** (**GloVe**) 模型中的对应词向量的
    **余弦相似度** ([https://en.wikipedia.org/wiki/Cosine_similarity](https://en.wikipedia.org/wiki/Cosine_similarity))
    计算得到的（这是另一个类似于 Word2Vec 的算法）。
- en: TU tells us how unique the topic is—that is to say, whether its words are found
    in other topics or not. Again, the value is between 0 and 1, and the higher the
    score, the more unique the topic is.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TU 告诉我们该主题的独特性——也就是说，其词汇是否在其他主题中出现过。该值介于 0 和 1 之间，得分越高，主题越独特。
- en: Once the model has been trained, we can score documents and get a score per
    topic.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们就可以对文档进行评分，并为每个主题获得评分。
- en: Discovering the seq2sea algorithm
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现 seq2sea 算法
- en: The seq2seq algorithm is based on **long short-term memory** (**LSTM**) neural
    networks ([https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)).
    As its name implies, seq2seq can be trained to map one sequence of tokens to another.
    Its main application is machine translation, training on large bilingual corpora
    of text, such as the **Workshop on Statistical Machine Translation** (**WMT**)
    dataset ([http://www.statmt.org/wmt20/](http://www.statmt.org/wmt20/)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq 算法基于 **长短期记忆** (**LSTM**) 神经网络 ([https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215))。顾名思义，seq2seq
    可以被训练成将一系列标记映射到另一系列标记。它的主要应用是机器翻译，训练大规模的双语语料库，例如 **统计机器翻译研讨会** (**WMT**) 数据集 ([http://www.statmt.org/wmt20/](http://www.statmt.org/wmt20/))。
- en: In addition to the implementation available in SageMaker, AWS has also packaged
    the seq2seq algorithm into an open source project, **AWS Sockeye** ([https://github.com/awslabs/sockeye](https://github.com/awslabs/sockeye)),
    which also includes tools for dataset preparation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在 SageMaker 中提供的实现外，AWS 还将 seq2seq 算法打包成了一个开源项目，**AWS Sockeye** ([https://github.com/awslabs/sockeye](https://github.com/awslabs/sockeye))，该项目还包括数据集准备工具。
- en: I won't cover seq2seq in this chapter. It would take too many pages to get into
    the appropriate level of detail, and there's no point in just repeating what's
    already available in the Sockeye documentation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会介绍 seq2seq。详细讲解会占用太多篇幅，而且没有必要重复 Sockeye 文档中已经包含的内容。
- en: You can find a seq2seq example in the notebook available at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de).
    Unfortunately, it uses the low-level `boto3` **application programming interface**
    (**API**), which we will cover in [*Chapter 12*](B17705_12_Final_JM_ePub.xhtml#_idTextAnchor260),
    *Automating Machine Learning Workflows*. Still, it's a valuable read, and you
    won't have too much trouble figuring things out.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de)
    找到一个 seq2seq 示例。不幸的是，它使用了低级别的 `boto3` **应用程序编程接口** (**API**)，我们将在 [*第 12 章*](B17705_12_Final_JM_ePub.xhtml#_idTextAnchor260)，*自动化机器学习工作流*
    中讲解。尽管如此，它仍然是一本有价值的读物，而且你不会遇到太多麻烦就能搞明白。
- en: Training with NLP algorithms
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 NLP 算法进行训练
- en: Just as for CV algorithms, training is the easy part, especially with the SageMaker
    **software development kit** (**SDK**). By now, you should be familiar with the
    workflow and the APIs, and we'll keep using them in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 CV 算法一样，训练是相对简单的，尤其是在使用 SageMaker **软件开发工具包** (**SDK**) 时。到目前为止，你应该已经熟悉了工作流程和
    API，并且我们将在本章中继续使用它们。
- en: Preparing data for NLP algorithms is another story. First, real-life datasets
    are generally pretty bulky. In this chapter, we'll work with millions of samples
    and hundreds of millions of words. Of course, they need to be cleaned, processed,
    and converted to the format expected by the algorithm.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为 NLP 算法准备数据是另一回事。首先，现实生活中的数据集通常非常庞大。在本章中，我们将处理数百万个样本和数亿个单词。当然，这些数据需要清洗、处理，并转换为算法所期望的格式。
- en: 'As we go through the chapter, we''ll use the following techniques:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下技术：
- en: Loading and cleaning data with the `pandas` library ([https://pandas.pydata.org](https://pandas.pydata.org))
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`库加载和清理数据（[https://pandas.pydata.org](https://pandas.pydata.org)）
- en: Removing stop words and lemmatizing with the **Natural Language Toolkit** (**NLTK**)
    library ([https://www.nltk.org](https://www.nltk.org))
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**自然语言工具包**（**NLTK**）库去除停用词并进行词形还原（[https://www.nltk.org](https://www.nltk.org)）
- en: Tokenizing with the `spaCy` library ([https://spacy.io/](https://spacy.io/))
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`spaCy`库进行分词（[https://spacy.io/](https://spacy.io/)）
- en: Building vocabularies and generating BoW representations with the `gensim` library
    ([https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/))
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`gensim`库构建词汇表并生成BoW表示（[https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)）
- en: Running data processing jobs with **Amazon SageMaker Processing**, which we
    studied in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030), *Handling
    Data Preparation Techniques*
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**Amazon SageMaker Processing**运行数据处理任务，我们在[*第二章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)中学习过，*处理数据准备技术*
- en: Granted—this isn't an NLP book, and we won't go extremely far in processing
    data. Still, this will be quite fun, and hopefully an opportunity to learn about
    popular open source tools for NLP.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当然——这不是一本NLP书籍，我们不会深入数据处理。但这将是一次非常有趣的学习机会，希望能了解一些流行的开源NLP工具。
- en: Preparing natural language datasets
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备自然语言数据集
- en: For the CV algorithms in the previous chapter, data preparation focused on the
    technical format required for the dataset (**image** format, **RecordIO**, or
    **augmented manifest**). The images themselves weren't processed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的CV算法中，数据准备侧重于数据集所需的技术格式（**图像**格式、**RecordIO**或**增强清单**）。图像本身没有被处理。
- en: Things are quite different for NLP algorithms. The text needs to be heavily
    processed, converted, and saved in the right format. In most learning resources,
    these steps are abbreviated or even ignored. Data is already "automagically" ready
    for training, leaving the reader frustrated and sometimes dumbfounded on how to
    prepare their own datasets.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NLP算法，情况完全不同。文本需要经过大量处理、转换，并以正确的格式保存。在大多数学习资源中，这些步骤通常会被简化或忽略。数据已经“自动”准备好进行训练，这让读者感到沮丧，有时甚至不知从何处开始准备自己的数据集。
- en: No such thing here! In this section, you'll learn how to prepare NLP datasets
    in different formats. Once again, get ready to learn a lot!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有这种情况！在本节中，您将学习如何准备不同格式的NLP数据集。再一次，准备好学习大量内容吧！
- en: Let's start with preparing data for BlazingText.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从为BlazingText准备数据开始。
- en: Preparing data for classification with BlazingText
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用BlazingText准备分类数据
- en: 'BlazingText expects labeled input data in the same format as FastText, outlined
    here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText期望标签输入数据与FastText相同的格式，详见此处：
- en: A plaintext file, with one sample per line.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个纯文本文件，每行一个样本。
- en: 'Each line has two fields, as follows:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每行包含两个字段，如下所示：
- en: a) A label in the form of `__label__LABELNAME__`
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 以`__label__LABELNAME__`形式的标签
- en: b) The text itself, formed into space-separated tokens (words and punctuation)
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 文本本身，形成以空格分隔的词元（单词和标点符号）
- en: Let's get to work and prepare a customer review dataset for sentiment analysis
    (positive, neutral, or negative). We'll use the **Amazon Customer Reviews** dataset
    available at [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).
    That should be more than enough real-life data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始工作，准备一个用于情感分析（正面、中立或负面）的客户评论数据集。我们将使用**Amazon Customer Reviews**数据集，可以在[https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)找到。那应该是充足的现实数据。
- en: 'Before starting, please make sure that you have enough storage space. Here,
    I''m using a notebook instance with 10 **gigabytes** (**GB**) of storage. I''ve
    also picked a C5 instance type to run the processing steps faster. We''ll proceed
    as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保您有足够的存储空间。在这里，我使用的是具有10 **千兆字节**（**GB**）存储的笔记本实例。我还选择了一个C5实例类型，以更快地运行处理步骤。我们将按以下方式进行：
- en: 'Let''s download the camera reviews by running the following code:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过运行以下代码来下载相机评论：
- en: '[PRE0]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load the data with `pandas`, ignoring any line that causes an error. We
    also drop any line with missing values. The code is illustrated in the following
    snippet:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`pandas`加载数据，忽略任何会引发错误的行，并删除任何包含缺失值的行。以下代码演示了这一过程：
- en: '[PRE1]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We print the data shape and the column names, like this:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印数据的形状和列名，如下所示：
- en: '[PRE2]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This gives us the following output:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE3]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '1.8 million lines! We keep 100,000, which is enough for our purpose. We also
    drop all columns except `star_rating` and `review_body`, as illustrated in the
    following code snippet:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 180 万行！我们保留了 100,000 行，足够满足我们的需求。我们还删除了除 `star_rating` 和 `review_body` 之外的所有列，以下代码片段展示了这一过程：
- en: '[PRE4]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Based on star ratings, we add a new column named `label`, with labels in the
    proper format. You have to love how `pandas` makes this so simple. Then, we drop
    the `star_rating` column, as illustrated in the following code snippet:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于星级评分，我们新增了一个名为 `label` 的列，标签格式已正确。你不得不佩服 `pandas` 让这一切变得如此简单。然后，我们删除了 `star_rating`
    列，以下代码片段展示了这一过程：
- en: '[PRE5]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'BlazingText expects labels at the beginning of each line, so we move the `label`
    column to the front, as follows:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BlazingText 期望每行的开头是标签，因此我们将 `label` 列移到前面，如下所示：
- en: '[PRE6]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The data should now look like this:![Figure 6.1 – Viewing the dataset
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据现在应该像这样：![图 6.1 – 查看数据集
- en: '](img/B17705_06_1.jpg)'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_06_1.jpg)'
- en: Figure 6.1 – Viewing the dataset
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.1 – 查看数据集
- en: 'BlazingText expects space-separated tokens: each word and each punctuation
    sign must be space-separated from the next. Let''s use the handy `punkt` tokenizer
    from the `nltk` library. Depending on the instance type you''re using, this could
    take a couple of minutes. Here''s the code you''ll need:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BlazingText 期望标记符号之间用空格分隔：每个单词和标点符号必须与下一个单词或符号用空格分隔开。让我们使用 `nltk` 库中的便捷 `punkt`
    分词器。根据你使用的实例类型，这可能需要几分钟。下面是你需要的代码：
- en: '[PRE7]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We join tokens into a single string, which we also convert to lowercase, as
    follows:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将标记符号连接成一个字符串，并将其转换为小写，如下所示：
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The data should now look like this (notice that all tokens are correctly space-separated):![Figure
    6.2 – Viewing the tokenized dataset
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据现在应该像这样（请注意，所有的标记符号都已正确地用空格分隔）：![图 6.2 – 查看标记化后的数据集
- en: '](img/B17705_06_2.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_06_2.jpg)'
- en: Figure 6.2 – Viewing the tokenized dataset
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.2 – 查看标记化后的数据集
- en: 'Finally, we split the dataset for training (95%) and validation (5%), and we
    save both splits as plaintext files, as illustrated in the following code snippet:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将数据集拆分为训练集（95%）和验证集（5%），并将这两个拆分保存为纯文本文件，以下代码片段展示了这一过程：
- en: '[PRE9]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Opening one of the files, you should see plenty of lines similar to this one:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开其中一个文件，你应该会看到许多类似这样的行：
- en: '[PRE10]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The data preparation wasn't too bad, was it? Still, tokenization ran for a minute
    or two. Now, imagine running it on millions of samples. Sure, you could fire up
    a larger environment in **SageMaker Studio**. You'd also pay more for as long
    as you're using it, which would probably be wasteful if only this one step required
    extra computing muscle. In addition, imagine having to run the same script on
    many other datasets. Do you want to do this manually again and again, waiting
    20 minutes every time and hoping your notebook doesn't crash? Certainly not, I
    should say!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备其实并不算太糟糕，对吧？不过，标记化过程运行了一两分钟。现在，想象一下如果是在数百万个样本上运行这个过程会怎样。没错，你可以启动一个更大的环境来运行它，在
    **SageMaker Studio** 上运行。不过，你还得为使用它支付更多费用，特别是当只有这一道步骤需要额外计算力时，这显然是浪费的。另外，想象一下如果你需要对许多其他数据集运行同样的脚本。你会想一次又一次地手动做这些事情，等待
    20 分钟，每次还要希望你的笔记本不会崩溃吗？我想应该不会！
- en: You already know the answer to both problems. It's **Amazon SageMaker Processing**,
    which we studied in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030),
    *Handling Data Preparation Techniques*. You should have the best of both worlds,
    using the smallest and least-expensive environment possible for experimentation,
    and running on-demand jobs when you need more resources. Day in, day out, you'll
    save money and get the job done faster.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道这两个问题的答案了。那就是 **Amazon SageMaker Processing**，我们在 [*第二章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)《数据准备技术》中学过这个内容。你应该能够两全其美，在最小且最便宜的环境中进行实验，并在需要更多资源时运行按需作业。日复一日，你将节省成本，并更快完成任务。
- en: Let's move this processing code to SageMaker Processing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些处理代码移到 SageMaker Processing。
- en: Preparing data for classification with BlazingText, version 2
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为使用 BlazingText 进行分类准备数据，第 2 版
- en: 'We''ve covered this in detail in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030),
    *Handling Data Preparation Techniques*, so I''ll go faster this time. We''ll proceed
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第二章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)《数据准备技术》中已经详细讨论了这个内容，所以这次我会讲得更快一些。我们将按如下步骤进行：
- en: 'We upload the dataset to **Simple Storage Service** (**S3**), as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集上传到 **简单存储服务** (**S3**)，如下所示：
- en: '[PRE11]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We define the processor by running the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过运行以下代码来定义处理器：
- en: '[PRE12]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We run the processing job, passing the processing script and its arguments,
    as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行处理作业，传递处理脚本及其参数，如下所示：
- en: '[PRE13]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The abbreviated preprocessing script is shown in the following code snippet.
    The full version is in the GitHub repository for the book. We first install the
    `nltk` package, as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简化的预处理脚本如下所示，完整版本可以在本书的GitHub仓库中找到。我们首先安装`nltk`包，如下所示：
- en: '[PRE14]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We read the command-line arguments, as follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取命令行参数，如下所示：
- en: '[PRE15]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We read the input dataset and process it, as follows:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取输入数据集并进行处理，如下所示：
- en: '[PRE16]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we split it for training and validation, and save it into two text
    files, as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将数据分为训练集和验证集，并将其保存到两个文本文件中，如下所示：
- en: '[PRE17]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, it doesn't take much to convert manual processing code into
    a SageMaker Processing job. You can actually reuse most of the code too, as it
    deals with generic topics such as command-line arguments, inputs, and outputs.
    The only trick is using `subprocess.call` to install dependencies inside the processing
    container.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，将手动处理的代码转化为SageMaker处理作业并不难。你实际上还可以重用大部分代码，因为它涉及一些通用的主题，比如命令行参数、输入和输出。唯一的技巧是使用`subprocess.call`在处理容器内安装依赖项。
- en: Equipped with this script, you can now process data at scale as often as you
    want, without having to run and manage long-lasting notebooks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了这个脚本后，你现在可以按需大规模处理数据，而不必运行和管理长期的笔记本。
- en: 'Now, let''s prepare data for the other BlazingText scenario: word vectors!'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为另一个BlazingText场景——词向量——准备数据！
- en: Preparing data for word vectors with BlazingText
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用BlazingText准备词向量数据
- en: 'BlazingText lets you compute word vectors easily and at scale. It expects input
    data in the following format:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText让你能够轻松且大规模地计算词向量。它期望输入数据为以下格式：
- en: A plaintext file, with one sample per line.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个纯文本文件，每行一个样本。
- en: Each sample must have space-separated tokens (words and punctuations).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个样本必须具有以空格分隔的标记（词语和标点符号）。
- en: 'Let''s process the same dataset as in the previous section, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理与上一节相同的数据集，如下所示：
- en: 'We''ll need the `spaCy` library, so let''s install it along with its English
    language model, like this:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要`spaCy`库，因此让我们安装它并一起安装其英语语言模型，如下所示：
- en: '[PRE18]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We load the data with `pandas`, ignoring any line that causes an error. We
    also drop any line with missing values. We should have more than enough data anyway.
    Here''s the code you''ll need:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`pandas`加载数据，忽略任何导致错误的行。我们还会删除任何包含缺失值的行。无论如何，我们的数据量应该足够多。以下是你需要的代码：
- en: '[PRE19]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We keep 100,000 lines, and we also drop all columns except `review_body`, as
    illustrated in the following code snippet:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们保留100,000行，并且删除除了`review_body`之外的所有列，如下所示：
- en: '[PRE20]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We write a function to tokenize reviews with `spaCy`, and we apply it to the
    `DataFrame`. This step should be noticeably faster than `nltk` tokenization in
    the previous example, as `spaCy` is based on `cython` ([https://cython.org](https://cython.org)).
    The code is illustrated in the following snippet:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们编写一个函数来使用`spaCy`对评论进行分词，并将其应用到`DataFrame`中。这个步骤应该比前一个例子中使用`nltk`分词要明显更快，因为`spaCy`基于`cython`（[https://cython.org](https://cython.org)）。以下是代码示例：
- en: '[PRE21]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The data should now look like this:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在数据应该是这样的：
- en: '![Figure 6.3 – Viewing the tokenized dataset'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.3 – 查看分词后的数据集'
- en: '](img/B17705_06_3.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_06_3.jpg)'
- en: Figure 6.3 – Viewing the tokenized dataset
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.3 – 查看分词后的数据集
- en: 'Finally, we save the reviews to a plaintext file, as follows:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将评论保存到一个纯文本文件中，如下所示：
- en: '[PRE22]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Opening this file, you should see one tokenized review per line, as illustrated
    in the following code snippet:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开这个文件，你应该看到每行一个分词后的评论，正如以下代码片段所示：
- en: '[PRE23]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here too, we should really be running these steps using SageMaker Processing.
    You'll find the corresponding notebook and preprocessing script in the GitHub
    repository for the book.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们也应该使用SageMaker处理来运行这些步骤。你可以在这本书的GitHub仓库中找到相应的笔记本和预处理脚本。
- en: Now, let's prepare data for the LDA and NTM algorithms.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为LDA和NTM算法准备数据。
- en: Preparing data for topic modeling with LDA and NTM
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为LDA和NTM准备主题建模数据
- en: In this example, we will use the **Million News Headlines** dataset ([https://doi.org/10.7910/DVN/SYBGZL](https://doi.org/10.7910/DVN/SYBGZL)),
    which is also available in the GitHub repository. As the name implies, it contains
    a million news headlines from the Australian news source *ABC*. Unlike product
    reviews, headlines are in very short sentences. Building a topic model should
    be an interesting challenge!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 **百万新闻头条** 数据集（[https://doi.org/10.7910/DVN/SYBGZL](https://doi.org/10.7910/DVN/SYBGZL)），该数据集也可以在
    GitHub 仓库中找到。顾名思义，它包含来自澳大利亚新闻源 *ABC* 的一百万条新闻头条。与产品评论不同，头条新闻通常是非常简短的句子。构建一个主题模型将是一个有趣的挑战！
- en: Tokenizing data
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词数据
- en: 'As you would expect, both algorithms require a tokenized dataset, so we''ll
    proceed as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所预料的，两个算法都需要一个已经分词的数据集，所以我们按以下步骤进行：
- en: 'We''ll need the `nltk` and `gensim` libraries, so let''s install them, as follows:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装 `nltk` 和 `gensim` 库，步骤如下：
- en: '[PRE24]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we''ve downloaded the dataset, we load it entirely with `pandas`, like
    this:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集后，我们使用 `pandas` 完整加载它，如下所示：
- en: '[PRE25]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The data should look like this:![Figure 6.4 – Viewing the tokenized dataset
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据应该是这样的：![图 6.4 – 查看已分词的数据集
- en: '](img/B17705_06_4.jpg)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_06_4.jpg)'
- en: Figure 6.4 – Viewing the tokenized dataset
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.4 – 查看已分词的数据集
- en: 'It''s sorted by date, and we shuffle it as a precaution. We then drop the `date`
    column by running the following code:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据按日期排序，并且为了安全起见我们将其打乱。然后，我们运行以下代码删除 `date` 列：
- en: '[PRE26]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We write a function to clean up and process the headlines. First, we get rid
    of all punctuation signs and digits. Using `nltk`, we also remove stop words—namely,
    words that are extremely common and don''t add any context, such as "this," "any,"
    and so on. In order to reduce the vocabulary size while keeping context, we could
    apply either **stemming** (https://en.wikipedia.org/wiki/Stemming) or **lemmatization**
    ([https://en.wikipedia.org/wiki/Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)),
    two popular NLP techniques. Let''s go with the latter here. Depending on your
    instance type, this could run for several minutes. Here''s the code you''ll need:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写一个函数来清理和处理头条新闻。首先，我们去除所有标点符号和数字。使用 `nltk`，我们还会去除停用词——即那些非常常见且不添加任何上下文的单词，例如“this”，“any”等。为了减少词汇量并保持上下文，我们可以应用
    **词干提取**（https://en.wikipedia.org/wiki/Stemming）或 **词形还原**（[https://en.wikipedia.org/wiki/Lemmatisation](https://en.wikipedia.org/wiki/Lemmatisation)），这两种是流行的自然语言处理技术。这里我们选择后者。根据你的实例类型，这个过程可能需要几分钟。以下是你需要的代码：
- en: '[PRE27]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once processed, the data should look like this:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据处理后应该是这样的：
- en: '![Figure 6.5 – Viewing the lemmatized dataset'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 查看已词形还原的数据集'
- en: '](img/B17705_06_5.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_06_5.jpg)'
- en: Figure 6.5 – Viewing the lemmatized dataset
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 查看已词形还原的数据集
- en: Now the reviews have been tokenized, we need to convert them to a BoW representation,
    replacing each word with a unique integer ID and its frequency count.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在评论已经分词，我们需要将其转换为 BoW 表示，将每个单词替换为一个唯一的整数 ID 和其频率计数。
- en: Converting data to a BoW representation
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据转换为 BoW 表示
- en: 'We will convert the reviews into a BoW representation using the following steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤将评论转换为 BoW 表示：
- en: 'The `gensim` library has exactly what we need! We build a **dictionary**, a
    list of all words present in the document collection, using the following code:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gensim` 库正好包含了我们需要的功能！我们使用以下代码构建一个**字典**，它包含了文档集合中的所有单词：'
- en: '[PRE28]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The dictionary looks like this:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字典看起来是这样的：
- en: '[PRE29]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This number feels very high. If we have too many dimensions, training will be
    very long, and the algorithm may have trouble fitting the data; for example, NTM
    is based on a neural network architecture. The input layer will be sized based
    on the number of tokens, so we need to keep them reasonably low. It will speed
    up training and help the encoder learn a manageable number of latent features.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个数字看起来很高。如果维度太多，训练将会非常长，算法可能会很难拟合数据；例如，NTM 是基于神经网络架构的。输入层的大小将根据分词的数量来决定，因此我们需要保持维度合理。这样可以加速训练，并帮助编码器学习一个可管理的潜在特征数量。
- en: 'We could go back and clean the headlines some more. Instead, we use a `gensim`
    function that removes extreme words—outlier words that are either extremely rare
    or extremely frequent. Then, taking a bold bet, we decide to restrict the vocabulary
    to the top 512 remaining words. Yes—that''s less than 1%. Here''s the code to
    do this:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以回头再清理一下头条新闻。不过，我们使用 `gensim` 的一个函数来移除极端单词——即那些非常稀有或非常频繁的异常词。然后，冒着风险，我们决定将词汇限制为剩余的前
    512 个单词。是的——这不到 1%。以下是执行此操作的代码：
- en: '[PRE30]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We write the vocabulary to a text file. Not only does this help us check what
    the top words are, but we''ll also pass this file to the NTM algorithm as an extra
    **channel**. You''ll see why this is important when we train the model. The code
    to do this is illustrated in the following snippet:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将词汇表写入一个文本文件。这不仅有助于我们检查哪些单词最重要，而且我们还将把这个文件作为额外的**通道**传递给NTM算法。训练模型时，你会明白为什么这很重要。执行此操作的代码示例如下：
- en: '[PRE31]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We use the dictionary to build a BoW for each headline. It''s stored in a new
    column called `tokens`. When we''re done, we drop the text review. The code is
    illustrated in the following snippet:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用字典为每个标题构建一个词袋（BoW）。它被存储在一个名为`tokens`的新列中。当我们完成后，会删除文本评论。代码示例如下：
- en: '[PRE32]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The data should now look like this:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，数据应如下所示：
- en: '![Figure 6.6 – Viewing the BoW dataset'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – 查看BoW数据集'
- en: '](img/B17705_06_6.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_06_6.jpg)'
- en: Figure 6.6 – Viewing the BoW dataset
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 查看BoW数据集
- en: 'As you can see, each word has been replaced with its unique ID and its frequency
    count in the review. For instance, the last line tells us that word #11 is present
    once, word #12 is present once, and so on.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每个单词已经被替换为其唯一的ID以及在评论中的频率。例如，最后一行告诉我们单词#11出现一次，单词#12出现一次，依此类推。
- en: The data processing is now complete. The last step is to save it to the appropriate
    input format.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理现已完成。最后一步是将其保存为适当的输入格式。
- en: Saving input data
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存输入数据
- en: NTM and LDA expect data in either a CSV format or a RecordIO-wrapped protobuf
    format. Just as with the **factorization matrix** example in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*, the data we're working with is quite sparse.
    Any given review only contains a small number of words from the vocabulary. As
    CSV is a dense format, we would end up with a huge amount of zero-frequency words.
    Not a good idea!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: NTM和LDA期望数据以CSV格式或RecordIO封装的protobuf格式提供。正如在[*第4章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)中关于**矩阵分解**的例子所讨论的，*训练机器学习模型*，我们正在处理的数据非常稀疏。任何给定的评论只包含词汇表中的少数几个单词。由于CSV格式是稠密格式，我们最终会得到大量的零频率单词。这可不是什么好主意！
- en: 'Once again, we''ll use `lil_matrix`, a `SciPy`. It will have as many lines
    as we have reviews, and as many columns as we have words in the dictionary. We''ll
    proceed as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用`lil_matrix`，一个`SciPy`库。它的行数与评论数相等，列数与字典中的单词数相等。我们按以下方式进行：
- en: 'We create the sparse matrix, like this:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按如下方式创建稀疏矩阵：
- en: '[PRE33]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We write a function to add a headline to the matrix. For each token, we simply
    write its frequency in the appropriate column, as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写一个函数，将标题添加到矩阵中。对于每个令牌，我们只需在相应的列中写下其频率，如下所示：
- en: '[PRE34]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then iterate over headlines and add them to the matrix. Quick note: we can''t
    use row index values, as they might be larger than the number of lines. The code
    is illustrated in the following snippet:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们遍历标题并将其添加到矩阵中。快速提示：我们不能使用行索引值，因为它们可能大于行数。代码示例如下：
- en: '[PRE35]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The last step is to write this matrix into a memory buffer in `protobuf` format
    and upload it to S3 for future use, as follows:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的步骤是将这个矩阵写入`protobuf`格式的内存缓冲区，并上传到S3以供将来使用，如下所示：
- en: '[PRE36]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Building the (1000000, 512) matrix takes a few minutes. Once it''s been uploaded
    to S3, we can see that it''s only 42 **megabytes** (**MB**). Lil'' matrix indeed.
    The code is illustrated in the following snippet:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建（1000000，512）的矩阵需要几分钟。上传到S3后，我们可以看到它只有42 **兆字节**（**MB**）。确实是小矩阵。代码示例如下：
- en: '[PRE37]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This concludes the data preparation for LDA and NTM. Now, let's see how we can
    use text datasets prepared with **SageMaker Ground Truth**.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是LDA和NTM的数据准备工作。现在，让我们看看如何使用**SageMaker Ground Truth**准备的文本数据集。
- en: Using datasets labeled with SageMaker Ground Truth
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker Ground Truth标记的数据集
- en: As discussed in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030),
    *Handling Data Preparation Techniques*, SageMaker Ground Truth supports text classification
    tasks. We could definitely use its output to build a dataset for FastText or BlazingText.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*第2章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)中讨论的，*数据准备技术处理*，SageMaker
    Ground Truth支持文本分类任务。我们完全可以使用它的输出构建一个用于FastText或BlazingText的数据集。
- en: 'First, I ran a quick text classification job on a few sentences, applying one
    of two labels: "`aws_service`" if the sentence mentions an AWS service, and "`no_aws_service`"
    if it doesn''t.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我快速运行了一个文本分类任务，应用了两种标签之一：“`aws_service`”如果句子提到AWS服务，和“`no_aws_service`”如果没有提到。
- en: 'Once the job is complete, I can fetch the **augmented manifest** from S3\.
    It''s in **JavaScript Object Notation Lines** (**JSON Lines**) format, and here''s
    one of its entries:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦任务完成，我可以从 S3 获取 **增强后的清单**。它是 **JavaScript 对象表示法行**（**JSON Lines**）格式，下面是其中一个条目：
- en: '[PRE38]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Shall we write a bit of Python code to put this in BlazingText format? Of course!
    Here we go:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来写点 Python 代码，把它转成 BlazingText 格式怎么样？当然可以！来看看：
- en: 'We load the augmented manifest directly from S3, as follows:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们直接从 S3 加载增强后的清单，如下所示：
- en: '[PRE39]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The data looks like this:![Figure 6.7 – Viewing the labeled dataset
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据看起来像这样：![图 6.7 – 查看标注过的数据集](img/B17705_06_7.jpg)
- en: '](img/B17705_06_7.jpg)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_06_7.jpg)'
- en: Figure 6.7 – Viewing the labeled dataset
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.7 – 查看标注过的数据集
- en: 'The label is buried in the `my-text-classification-job-metadata` column. We
    extract it into a new column, as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签隐藏在 `my-text-classification-job-metadata` 列中。我们将其提取到一个新列，如下所示：
- en: '[PRE40]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The data now looks like that shown in the following screenshot. From then on,
    we can apply tokenization, and so on. That was easy, wasn't it?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据现在看起来像下面的截图所示。从此以后，我们可以应用分词等操作。很简单，不是吗？
- en: '![Figure 6.8 – Viewing the processed dataset'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8 – 查看处理过的数据集](img/B17705_06_7.jpg)'
- en: '](img/B17705_06_8.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_06_8.jpg)'
- en: Figure 6.8 – Viewing the processed dataset
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 查看处理过的数据集
- en: Now, let's build NLP models!
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建 NLP 模型！
- en: Using the built-in algorithms for NLP
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内置的 NLP 算法
- en: In this section, we're going to train and deploy models with BlazingText, LDA,
    and NTM. Of course, we'll use the datasets prepared in the previous section.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 BlazingText、LDA 和 NTM 训练并部署模型。当然，我们将使用前一节准备的数据集。
- en: Classifying text with BlazingText
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BlazingText 进行文本分类
- en: 'BlazingText makes it extremely easy to build a text classification model, especially
    if you have no NLP skills. Let''s see how, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText 让构建文本分类模型变得极其简单，尤其是如果你没有 NLP 技能。让我们看看，以下是步骤：
- en: 'We upload the training and validation datasets to S3\. Alternatively, we could
    use the output paths returned by a SageMaker Processing job. The code is illustrated
    in the following snippet:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将训练和验证数据集上传到 S3。或者，我们可以使用 SageMaker 处理作业返回的输出路径。代码如下所示：
- en: '[PRE41]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We configure the `Estimator` function for BlazingText, as follows:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为 BlazingText 配置 `Estimator` 函数，如下所示：
- en: '[PRE42]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We set a single hyperparameter, telling BlazingText to train in supervised
    mode, as follows:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置一个超参数，告诉 BlazingText 以监督模式进行训练，如下所示：
- en: '[PRE43]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We define channels, setting the content type to `text/plain`, and then we launch
    the training, as follows:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义通道，设置内容类型为 `text/plain`，然后启动训练，如下所示：
- en: '[PRE44]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We get a validation accuracy of 88.4%, which is quite good in the absence of
    any hyperparameter tweaking. We then deploy the model to a small CPU instance,
    as follows:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到了 88.4% 的验证准确率，在没有调整任何超参数的情况下，这个结果相当不错。然后，我们将模型部署到一个小型 CPU 实例，如下所示：
- en: '[PRE45]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Once the endpoint is up, we send three tokenized samples for prediction, asking
    for all three labels, as follows:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点启动，我们将发送三个已分词的样本进行预测，并请求所有三个标签，如下所示：
- en: '[PRE46]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Printing the response, we see that the three samples were correctly categorized,
    as illustrated here:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印响应后，我们可以看到三个样本已正确分类，如下图所示：
- en: '[PRE47]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As usual, we delete the endpoint once we''re done by running the following
    code:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，一旦完成，我们通过运行以下代码删除端点：
- en: '[PRE48]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now, let's train BlazingText to compute word vectors.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练 BlazingText 来计算词向量。
- en: Computing word vectors with BlazingText
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BlazingText 计算词向量
- en: The code is almost identical to the previous example, with only two differences.
    First, there is only one channel, containing training data. Second, we need to
    set BlazingText to UL mode.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码几乎与前一个示例相同，只有两个不同点。首先，只有一个通道，包含训练数据。其次，我们需要将 BlazingText 设置为 UL 模式。
- en: 'BlazingText supports the training modes implemented in Word2Vec: **skipgram**
    and **continuous BoW** (**CBOW**). It adds a third mode, **batch_skipgram**, for
    faster distributed training. It also supports **subword embeddings**, a technique
    that makes it possible to return a word vector for words that are misspelled or
    not part of the vocabulary.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: BlazingText 支持在 Word2Vec 中实现的训练模式：**skipgram** 和 **连续的 BoW**（**CBOW**）。它还添加了一种第三种模式，**batch_skipgram**，用于更快的分布式训练。它还支持
    **子词嵌入**，这是一种可以为拼写错误或不在词汇表中的词返回词向量的技术。
- en: 'Let''s go for skipgram with subword embeddings. We leave the dimension of vectors
    unchanged (the default is 100). Here''s the code you''ll need:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用 skipgram 和子词嵌入。我们保持向量的维度不变（默认是 100）。以下是所需的代码：
- en: '[PRE49]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Unlike other algorithms, there is nothing to deploy here. The model artifact
    is in S3 and can be used for downstream NLP applications.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他算法不同，这里没有需要部署的内容。模型文件存储在 S3 中，并且可以用于下游 NLP 应用程序。
- en: Speaking of which, BlazingText is compatible with FastText, so how about trying
    to load the models we just trained into FastText?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这里，BlazingText 与 FastText 兼容，那么如何将我们刚训练的模型加载到 FastText 中呢？
- en: Using BlazingText models with FastText
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BlazingText 模型与 FastText
- en: 'First, we need to compile FastText, which is extremely simple. You can even
    do it on a notebook instance without having to install anything. Here''s the code
    you''ll need:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要编译 FastText，这非常简单。你甚至可以在笔记本实例上进行编译，无需安装任何东西。下面是你需要的代码：
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Let's first try our classification model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试我们的分类模型。
- en: Using a BlazingText classification model with FastText
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 BlazingText 分类模型与 FastText
- en: 'We will try the model using the following steps:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤来尝试该模型：
- en: 'We copy the model artifact from S3 and extract it as follows:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 S3 复制模型文件并进行解压，方法如下：
- en: '[PRE51]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We load `model.bin` with FastText, as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 FastText 加载 `model.bin`，方法如下：
- en: '[PRE52]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We predict samples and view their top class, as follows:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们预测样本并查看它们的 top 类别，方法如下：
- en: '[PRE53]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We exit with *Ctrl* + *C*. Now, let's explore our vectors.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 *Ctrl* + *C* 退出。现在，让我们来探索我们的向量。
- en: Using BlazingText word vectors with FastText
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 BlazingText 词向量与 FastText
- en: 'We will now use FastText with the vectors, as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 FastText 和向量，方法如下：
- en: 'We copy the model artifact from S3 and we extract it, like this:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 S3 复制模型文件并解压，方法如下：
- en: '[PRE54]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can explore word similarities. For example, let''s look for words that are
    closest to "telephoto". This could help us improve how we handle search queries
    or how we recommend similar products. Here''s the code you''ll need:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以探索词语相似度。例如，让我们找出与“远摄”最相近的词汇。这可能有助于我们改进搜索查询的处理方式或推荐相似产品的方式。以下是你需要的代码：
- en: '[PRE55]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can also look for analogies. For example, let''s ask our model the following
    question: What''s the Canon equivalent for the Nikon D3300 camera? The code is
    illustrated in the following snippet:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以寻找类比。例如，假设我们问我们的模型以下问题：尼康 D3300 相机的佳能等效型号是什么？代码如下所示：
- en: '[PRE56]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: According to our model, you should consider the XSI and 700D cameras!
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们的模型，你应该考虑 XSI 和 700D 相机！
- en: As you can see, word vectors are amazing and BlazingText makes it easy to compute
    them at any scale. Now, let's move on to topic modeling, another fascinating subject.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，词向量非常强大，BlazingText 使得在任何规模下计算词向量都变得非常容易。现在，让我们继续进行主题建模，这是另一个令人着迷的主题。
- en: Modeling topics with LDA
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LDA 进行主题建模
- en: 'In a previous section, we prepared a million news headlines, and we''re now
    going to use them for topic modeling with LDA, as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们准备了百万条新闻标题，现在我们将使用它们进行主题建模，采用 LDA 方法，具体如下：
- en: 'First, we define useful paths by running the following code:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过运行以下代码定义有用的路径：
- en: '[PRE57]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We configure the `Estimator` function, like this:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置 `Estimator` 函数，如下所示：
- en: '[PRE58]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We set hyperparameters: how many topics we want to build (10), how many dimensions
    the problem has (the vocabulary size), and how many samples we''re training on.
    Optionally, we can set a parameter named `alpha0`. According to the documentation:
    "*Small values are more likely to generate sparse topic mixtures and large values
    (greater than 1.0) produce more uniform mixtures."* Let''s set it to 0.1 and hope
    that the algorithm can indeed build well-identified topics. Here''s the code you''ll
    need:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置超参数：希望构建的主题数量（10），问题的维度（词汇大小）以及我们正在训练的样本数量。可选地，我们可以设置一个名为 `alpha0` 的参数。根据文档：“*较小的值更容易生成稀疏的主题混合，而较大的值（大于
    1.0）则生成更加均匀的混合。*”我们将其设置为 0.1，并希望算法能够构建出良好识别的主题。以下是所需的代码：
- en: '[PRE59]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We launch the training. As RecordIO is the default format expected by the algorithm,
    we don''t need to define channels. The code is illustrated in the following snippet:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动训练。由于 RecordIO 是该算法期望的默认格式，我们无需定义通道。代码如下所示：
- en: '[PRE60]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Once training is complete, we deploy to a small CPU instance, as follows:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们将模型部署到一个小型 CPU 实例，方法如下：
- en: '[PRE61]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Before we send samples for prediction, we need to process them just like we
    processed the training set. We write a function that takes care of this: building
    a sparse matrix, filling it with BoW, and saving to an in-memory protobuf buffer,
    as follows:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在发送样本进行预测之前，我们需要像处理训练集一样处理它们。我们编写一个函数来完成这个任务：构建稀疏矩阵，填充 BoW，并保存到内存中的 protobuf
    缓冲区，方法如下：
- en: '[PRE62]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Please note that we need the dictionary here. This is why the corresponding
    SageMaker Processing job saved a pickled version of it, which we could later unpickle
    and use.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们在这里需要字典。这就是为什么相应的SageMaker处理任务保存了它的pickle版本，我们可以稍后将其反序列化并使用它。
- en: 'Then, we define a Python array containing five headlines, named `samples`.
    These are real headlines I copied from the ABC news website at [https://www.abc.net.au/news/](https://www.abc.net.au/news/).
    The code is illustrated in the following snippet:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个包含五个标题的Python数组，命名为`samples`。这些是真实的标题，我从ABC新闻网站上复制的，网址是[https://www.abc.net.au/news/](https://www.abc.net.au/news/)。代码示例如下：
- en: '[PRE63]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s process and predict them, as follows:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们处理并预测它们，如下所示：
- en: '[PRE64]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The response contains a score vector for each review (extra decimals have been
    removed for brevity). Each vector reflects a mix of topics, with a score per topic.
    All scores add up to 1\. The code is illustrated in the following snippet:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 响应中包含每个评论的得分向量（为了简洁起见，已删除额外的小数）。每个向量反映了各个主题的混合，并为每个主题分配得分。所有得分总和为1。代码示例如下：
- en: '[PRE65]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This isn''t easy to read. Let''s print the top topic and its score, as follows:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这并不容易读取。让我们打印出主要主题及其得分，如下所示：
- en: '[PRE66]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This prints out the following result:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下结果：
- en: '[PRE67]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'As usual, we delete the endpoint once we''re done, as follows:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，训练完成后我们会删除终端节点，如下所示：
- en: '[PRE68]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Interpreting LDA results is not easy, so let's be careful here. No wishful thinking!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 解释LDA结果并不容易，所以我们在这里要小心。不要有过度的幻想！
- en: We see that each headline has a definite topic, which is good news. Apparently,
    LDA was able to identify solid topics, maybe thanks to the low `alpha0` value.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们看到每个标题都有一个明确的主题，这是好消息。显然，LDA能够识别出稳固的主题，也许得益于较低的`alpha0`值。
- en: The top topics for unrelated headlines are different, which is promising.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与无关的标题相比，主要的主题是不同的，这很有前景。
- en: The last two headlines are both about sports and their top topic is the same,
    which is another good sign.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后的两个标题都是关于体育的，它们的主要主题是相同的，这是另一个好兆头。
- en: All five reviews scored zero on topics 5, 6, 8, and 9\. This probably means
    that other topics have been built, and we would need to run more examples to discover
    them.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有五条评论在主题5、6、8和9上得分为零。这可能意味着已经构建了其他主题，我们需要运行更多示例来发现它们。
- en: Is this a successful model? Probably. Can we be confident that topic 0 is about
    world affairs, topic 1 about sports, and topic 2 about sports? Not until we've
    predicted a few thousand more reviews and checked that related headlines are assigned
    to the same topic.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个成功的模型吗？可能是。我们能确定主题0是关于世界事务的，主题1是关于体育的，主题2也是关于体育的吗？直到我们预测了几千个评论并检查相关标题是否被分配到同一主题，才能得出结论。
- en: As mentioned at the beginning of the chapter, LDA is not a classification algorithm.
    It has a mind of its own and it may build totally unexpected topics. Maybe it
    will group headlines according to sentiment or city names. It all depends on the
    distribution of these words inside the document collection.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头提到的，LDA并不是一种分类算法。它有自己的“思想”，可能会构建出完全出乎意料的主题。也许它会根据情感或城市名称将标题进行分组。一切取决于这些词语在文档集合中的分布。
- en: Wouldn't it be nice if we could see which words "weigh" more in a certain topic?
    That would certainly help us understand the topics a little better. Enter NTM!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够看到哪些词语在某个主题中“更重要”，那该有多好呢？这无疑会帮助我们更好地理解这些主题。于是，NTM应运而生！
- en: Modeling topics with NTM
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NTM建模主题
- en: 'This example is very similar to the previous one. We''ll just highlight the
    differences, and you''ll find a full example in the GitHub repository for the
    book. Let''s get into it, as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子与前一个非常相似。我们将仅强调其中的不同之处，完整的示例可以在书籍的GitHub仓库中找到。让我们开始吧，如下所示：
- en: 'We upload the **vocabulary file** to S3, like this:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将**词汇文件**上传到S3，像这样：
- en: '[PRE69]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We select the NTM algorithm, as follows:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择NTM算法，如下所示：
- en: '[PRE70]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Once we''ve configured the `Estimator` function, we set the hyperparameters,
    as follows:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们配置了`Estimator`函数，就可以设置超参数，如下所示：
- en: '[PRE71]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We launch training, passing the vocabulary file in the `auxiliary` channel,
    as follows:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们启动训练，将词汇文件传递到`auxiliary`通道，如下所示：
- en: '[PRE72]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'When training is complete, we see plenty of information in the training log.
    First, we see the average WETC and TU scores for the 10 topics, as follows:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成后，我们可以在训练日志中看到大量信息。首先，我们看到10个主题的平均WETC和TU分数，如下所示：
- en: '[PRE73]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: These are decent results. Topic unicity is high, and the semantic distance between
    topic words is average.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果不错。主题的独特性很高，主题词之间的语义距离适中。
- en: For each topic, we see its WETC and TU scores, as well as its top words—that
    is to say, the words that have the highest probability of appearing in documents
    associated with this topic.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个话题，我们看到它的WETC和TU分数，以及它的关键词——即在与该话题相关的文档中最有可能出现的词汇。
- en: Let's look at each one in detail and try to put names to topics.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看每个话题，尝试为它们命名。
- en: 'Topic 0 is pretty obvious, I think. Almost all words are related to crime,
    so let''s call it `crime`. You can see this topic here:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 第0个话题相当明显，我认为。几乎所有的词汇都与犯罪相关，所以我们称它为`crime`。你可以在这里查看该话题：
- en: '[PRE74]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The following topic 1 is a little fuzzier. How about `legal`? Have a look at
    it here:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的第1个话题有些模糊。怎么样，叫它`legal`？请在这里查看它：
- en: '[PRE75]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Topic 2 is about accidents and fires. Let''s call it `disaster`. You can see
    the topic here:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 第2个话题是关于事故和火灾的。我们称它为`disaster`。你可以在这里查看该话题：
- en: '[PRE76]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Topic 3 is obvious: `sports`. The TU score is the highest, showing that sports
    articles use a very specific vocabulary found nowhere else, as we can see here:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 第3个话题显而易见：`sports`。TU分数最高，表明体育文章使用的是非常特定的词汇，其他地方找不到，正如我们在这里看到的那样：
- en: '[PRE77]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Topic 4 is a strange mix of weather information and natural resources. It has
    the lowest WETC and the lowest TU score too. Let''s call it `unknown1`. Have a
    look at it here:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 第4个话题是天气信息和自然资源的奇怪混合。它的WETC分数和TU分数都是最低的。我们称它为`unknown1`。请在这里查看它：
- en: '[PRE78]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Topic 5 is about world affairs, it seems. Let''s call it `international`. You
    can see the topic here:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 第5个话题似乎是关于世界事务的。我们称它为`international`。你可以在这里查看该话题：
- en: '[PRE79]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Topic 6 feels like local news, as it contains abbreviations for Australian
    regions: `qld` is Queensland, `ta` is Tasmania, `nsw` is New South Wales, and
    so on. Let''s call it `local`. The topic is shown here:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 第6个话题感觉像是地方新闻，因为它包含了澳大利亚地区的缩写：`qld`是昆士兰，`ta`是塔斯马尼亚，`nsw`是新南威尔士州，等等。我们称它为`local`。该话题如下：
- en: '[PRE80]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Topic 7 is a no-brainer: `finance`. It has the highest WETC score, showing
    that its words are closely related from a semantic point of view. Topic unicity
    is also very high, and we would probably see the same for domain-specific topics
    on medicine or engineering. Have a look at the topic here:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 第7个话题显而易见：`finance`。它具有最高的WETC分数，表明从语义角度来看，它的词汇密切相关。话题的独特性也非常高，我们可能在医学或工程等领域特定话题中看到相同的情况。请查看该话题：
- en: '[PRE81]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Topic 8 is about politics, with a bit of crime thrown in. Some people would
    say that''s actually the same thing. As we already have a `crime` topic, we''ll
    name this one `politics`. Have a look at the topic here:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 第8个话题是关于政治的，带有一些犯罪元素。有些人会说，这实际上是同一回事。由于我们已经有了一个`crime`话题，我们将这个命名为`politics`。请查看该话题：
- en: '[PRE82]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Topic 9 is another mixed bag. It''s hard to say whether it''s about farming
    or missing people! Let''s go with `unknown2`. You can see the topic here:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 第9个话题是一个混合的例子。很难说它是关于农业还是失踪人员！我们称之为`unknown2`。你可以在这里看到该话题：
- en: '[PRE83]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'All things considered, that''s a pretty good model: 8 clear topics out of 10\.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，这是一个相当不错的模型：10个话题中有8个非常清晰。
- en: 'Let''s define our list of topics and run our sample headlines through the model
    after deploying it, as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的话题列表，并在部署模型后运行示例头条新闻，如下所示：
- en: '[PRE84]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We use the following function to print the top three topics and their score:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下函数来打印前3个话题及其分数：
- en: '[PRE85]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Here''s the output:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE86]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Headlines 0, 2, 3, and 4 are right on target. That's not surprising given how
    strong these topics are.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 头条0、2、3和4完全符合预期。这并不令人惊讶，因为这些话题的强度非常高。
- en: Headline 1 scores very high on the topic we called `legal`. Maybe Adelaide passengers
    should sue the train company? Seriously, we would need to find other matching
    headlines to get a better sense of what the topic is really about.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 头条1在我们称为`legal`的话题中得分很高。也许阿德莱德的乘客应该起诉火车公司？说真的，我们需要找到其他匹配的头条新闻，以便更好地了解该话题的真实含义。
- en: As you can see, NTM makes it easier to understand what topics are about. We
    could improve the model by processing the vocabulary file, adding or removing
    specific words to influence topics, increasing the number of topics, fiddling
    with `alpha0`, and so on. My intuition tells me that we should really see a "weather"
    topic in there. Please experiment and see if you want to make it appear.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，NTM使得理解话题变得更加容易。我们可以通过处理词汇文件来改进模型，添加或移除特定词汇来影响话题，增加话题数目，调整`alpha0`等。我直觉上认为，我们应该在其中看到一个“天气”话题。请尝试一下，看看是否能让它出现。
- en: 'If you''d like to run another example, you''ll find interesting techniques
    in this notebook:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想运行另一个示例，你可以在这个笔记本中找到有趣的技巧：
- en: '[https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb)'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb)'
- en: Summary
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: NLP is a very exciting topic. It's also a difficult one because of the complexity
    of language in general, and due to how much processing is required to build datasets.
    Having said that, the built-in algorithms in SageMaker will help you get good
    results out of the box. Training and deploying models are straightforward processes,
    which leaves you more time to explore, understand, and prepare data.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是一个非常激动人心的主题。它也是一个困难的主题，因为语言本身的复杂性，以及构建数据集所需的处理工作量。尽管如此，SageMaker中的内置算法将帮助你轻松获得良好的结果。训练和部署模型是直接的过程，这为你提供了更多时间来探索、理解和准备数据。
- en: In this chapter, you learned about the BlazingText, LDA, and NTM algorithms.
    You also learned how to process datasets using popular open source tools such
    as `nltk`, `spaCy`, and `gensim`, and how to save them in the appropriate format.
    Finally, you learned how to use the SageMaker SDK to train and deploy models with
    all three algorithms, as well as how to interpret results. This concludes our
    exploration of built-in algorithms.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了BlazingText、LDA和NTM算法。你还学习了如何使用流行的开源工具，如`nltk`、`spaCy`和`gensim`，来处理数据集，并将其保存为适当的格式。最后，你学习了如何使用SageMaker
    SDK训练和部署这三种算法的模型，以及如何解读结果。这标志着我们对内置算法的探索的结束。
- en: In the next chapter, you will learn how to use built-in ML frameworks such as
    **scikit-learn**, **TensorFlow**, **PyTorch**, and **Apache MXNet**.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用内置的机器学习框架，如**scikit-learn**、**TensorFlow**、**PyTorch**和**Apache
    MXNet**。
