- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Interpretation Methods for Multivariate Forecasting and Sensitivity Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元预测和敏感性分析的解释方法
- en: Throughout this book, we have learned about various methods we can use to interpret
    supervised learning models. They can be quite effective at assessing models while
    also uncovering their most influential predictors and their hidden interactions.
    But as the term supervised learning suggests, these methods can only leverage
    known samples and permutations based on these known samples’ distributions. However,
    when these samples represent the past, things can get tricky! As the Nobel laureate
    in physics Niels Bohr famously quipped, “Prediction is very difficult, especially
    if it’s about the future.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们学习了我们可以用来解释监督学习模型的各种方法。它们在评估模型的同时，也能揭示其最有影响力的预测因子及其隐藏的相互作用。但是，正如“监督学习”这个术语所暗示的，这些方法只能利用已知的样本以及基于这些已知样本分布的排列。然而，当这些样本代表过去时，事情可能会变得复杂！正如诺贝尔物理学奖获得者尼尔斯·玻尔著名地打趣说，“预测是非常困难的，尤其是如果它关乎未来。”
- en: Indeed, when you see data points fluctuating in a time series, they may appear
    to be rhythmically dancing in a predictable pattern – at least in the best-case
    scenarios. Like a dancer moving to a beat, every repetitive movement (or frequency)
    can be attributed to seasonal patterns, while a gradual change in volume (or amplitude)
    is attributed to an equally predictable trend. The dance is inevitably misleading
    because there are always missing pieces of the puzzle that slightly shift the
    data points, such as a delay in a supplier’s supply chain causing an unexpected
    dent in today’s sales figures. To make matters worse, there are also unforeseen
    catastrophic once-in-a-decade, once-in-a-generation, or simply once-ever events
    that can radically make the somewhat understood movement of a time series unrecognizable,
    similar to a ballroom dancer having a seizure. For instance, in 2020, sales forecasts
    everywhere, either for better or worse, were rendered useless by COVID-19!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当你看到时间序列中的数据点波动时，它们可能看起来像是在按照可预测的模式有节奏地跳舞——至少在最佳情况下是这样。就像一个舞者随着节奏移动，每一次重复的动作（或频率）都可以归因于季节性模式，而音量（或振幅）的逐渐变化则可以归因于同样可预测的趋势。这种舞蹈不可避免地具有误导性，因为总会有一些缺失的拼图碎片稍微改变数据点，比如供应商供应链中的延迟导致今天销售数据的意外下降。更糟糕的是，还有不可预见的、十年一遇、一代人一遇或甚至一次性的灾难性事件，这些事件可以彻底改变人们对时间序列运动的一些理解，就像一个舞厅舞者在抽搐。例如，在2020年，由于COVID-19，无论好坏，各地的销售预测都变得毫无用处！
- en: We could call this an extreme outlier event, but we must recognize that models
    weren’t built to predict these momentous events because they were trained on almost
    entirely likely occurrences. Not predicting these unlikely yet most consequential
    events is why we shouldn’t place so much trust in forecasting models to begin
    with, especially without discussing certainty or confidence bounds.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种情况称为极端异常事件，但我们必须认识到，模型并不是为了预测这些重大事件而构建的，因为它们几乎完全基于可能发生的情况进行训练。未能预测这些不太可能但后果严重的意外事件，这就是我们一开始就不应该过度依赖预测模型的原因，尤其是在没有讨论确定性或置信区间的情况下。
- en: This chapter will examine a multivariate forecasting problem with **Long Short-Term
    Memory** (**LSTM**) models. We will first assess the models with traditional interpretation
    methods, followed by the **Integrated Gradient** method we learned about in *Chapter
    7*, *Visualizing Convolutional Neural Networks*, to generate our model’s local
    attributions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨一个使用**长短期记忆**（**LSTM**）模型的多元预测问题。我们将首先使用传统的解释方法评估模型，然后使用我们在第7章“可视化卷积神经网络”中学习的**集成梯度**方法来生成我们模型的局部属性。
- en: 'But more importantly, we will understand the LSTM’s learning process and limitations
    better. We will then employ a prediction approximator method and SHAP’s `KernelExplainer`
    for both global and local interpretation. Lastly, *forecasting and uncertainty
    are intrinsically linked*, and *sensitivity analysis* is a family of methods designed
    to measure the uncertainty of the model’s output in relation to its input, so
    it’s very useful in forecasting scenarios. We will also study two such methods:
    **Morris** for *factor prioritization* and **Sobol** for *factor fixing*, which
    involves cost sensitivity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 但更重要的是，我们将更好地理解LSTM的学习过程和局限性。然后，我们将采用预测近似方法以及SHAP的`KernelExplainer`进行全局和局部解释。最后，*预测和不确定性是内在相关的*，*敏感性分析*是一系列旨在衡量模型输出不确定性相对于其输入的方法，因此在预测场景中非常有用。我们还将研究两种这样的方法：**Morris**用于*因素优先级排序*和**Sobol**用于*因素固定*，这涉及到成本敏感性。
- en: 'The following are the main topics we are going to cover:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将要讨论的主要主题：
- en: Assessing time series models with traditional interpretation methods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统解释方法评估时间序列模型
- en: Generating LSTM attributions with integrated gradients
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用积分梯度生成LSTM属性
- en: Computing global and local attributions with SHAP’s `KernelExplainer`
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SHAP的`KernelExplainer`计算全局和局部属性
- en: Identifying influential features with factor prioritization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因素优先级识别有影响力的特征
- en: Quantifying uncertainty and cost sensitivity with factor fixing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因素固定量化不确定性和成本敏感性
- en: Let’s begin!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `sklearn`,
    `tensorflow`, `matplotlib`, `seaborn`, `alibi`, `distython`, `shap`, and `SALib`
    libraries. Instructions on how to install all these libraries can be found in
    this book’s preface.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`tensorflow`、`matplotlib`、`seaborn`、`alibi`、`distython`、`shap`和`SALib`库。如何安装所有这些库的说明可以在本书的序言中找到。
- en: 'The code for this chapter is located here: [https://packt.link/b6118](https://packt.link/b6118).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/b6118](https://packt.link/b6118)。
- en: The mission
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: Highway traffic congestion is a problem that’s affecting cities across the world.
    As the number of vehicles per capita steadily increases across the developing
    world with not enough road and parking infrastructure to keep up with it, congestion
    has been increasing at alarming levels. In the United States, the vehicle per
    capita statistic is among the highest in the world (838 per 1,000 people in 2019).
    For this reason, US cities represent 62 out of the 381 cities worldwide with at
    least a 15% congestion level.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 高速公路交通拥堵是一个影响世界各地的城市的问题。随着发展中国家每千人车辆数量的稳步增加，而道路和停车基础设施不足以跟上这一增长，拥堵水平已经达到了令人担忧的程度。在美国，每千人车辆统计数字是世界上最高的之一（2019年为每千人838辆）。因此，美国城市占全球381个城市中至少有15%拥堵水平的62个城市。
- en: 'Minneapolis is one such city (see *Figure 9.1*) where that threshold was recently
    surpassed and keeps rising. To put this metropolitan area into context, congestion
    levels are extremely severe at above 50%, but moderate-level congestion (15-25%)
    is already a warning sign of bad congestion to come. It’s challenging to reverse
    congestion once it reaches 25% because any infrastructure improvement will be
    costly to implement without disrupting traffic even further. One of the worst
    congestion points is between the twin cities of Minneapolis and St. Paul throughout
    the Interstate 94 (I-94) highway, which congests alternate routes as commuters
    try to cut travel time. Knowing this, the mayors of both cities have obtained
    some federal funding to expand the highway:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼阿波利斯就是这样一座城市（参见*图9.1*），那里的阈值最近已经超过并持续上升。为了将这个大都市地区置于适当的背景中，拥堵水平在50%以上时极为严重，但中等程度的拥堵（15-25%）已经是未来可能出现严重拥堵的预警信号。一旦拥堵达到25%，就很难逆转，因为任何基础设施的改善都将非常昂贵，而且还会进一步扰乱交通。最严重的拥堵点之一是在明尼阿波利斯和圣保罗这对双城之间的94号州际公路（I-94），当通勤者试图缩短旅行时间时，这会拥堵替代路线。了解这一点后，这两座城市的市长们已经获得了一些联邦资金来扩建这条公路：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_09_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_09_01.png)'
- en: 'Figure 9.1: TomTom’s 2019 traffic index for Minneapolis'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：TomTom的2019年明尼阿波利斯交通指数
- en: The mayors want to be able to tout a completed expansion as a joint accomplishment
    to get reelected for a second term. However, they are well aware that a noisy,
    dirty, and obstructive expansion can be a big nuisance for commuters, so the construction
    project could backfire politically if it’s not made nearly invisible. Therefore,
    they have stipulated that the construction company prefabricates as much as possible
    elsewhere and assembles only during low-volume hours. These hours have less than
    1,500 vehicles per hour. They can also only work on one direction of the highway
    at a time and only block no more than half of its lanes when they are working
    on it. To ensure compliance with these stipulations, they will fine the company
    if they are blocking more than a quarter of the highway any time that volume is
    above this threshold, at a rate of $15 per vehicle.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 市长们希望能够吹嘘一个完成的扩建项目作为共同成就，以便在第二任期内再次当选。然而，他们很清楚，一个嘈杂、脏乱和阻碍交通的扩建项目可能会给通勤者带来很大的麻烦，因此，如果扩建项目不是几乎看不见，它可能会在政治上适得其反。因此，他们规定建筑公司尽可能在其他地方预制，并在低流量时段进行组装。这些时段的每小时交通量少于1,500辆车。他们一次只能在一个方向的高速公路上工作，并且在他们工作时只能阻挡不超过一半的车道。为了确保遵守这些规定，如果他们在任何交通量超过这个阈值时阻挡超过四分之三的高速公路，他们将对公司处以每辆车15美元的罚款。
- en: In addition to that, if the construction crew are on-site blocking half the
    highway while traffic is over 1,500 vehicles per hour, it will cost them $5,000
    a day. To put this into perspective, blocking during a typical peak hour could
    cost the construction company $67,000 per hour, plus the $5,000 daily fee! The
    local authorities will use **Automated Traffic Recorder** (**ATR**) stations along
    the route to monitor traffic volume, as well as local traffic police to register
    when lanes are getting blocked for construction.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，如果施工队伍在每小时交通量超过1,500辆车时在现场阻挡一半的高速公路，他们每天将花费5,000美元。为了更直观地了解这一点，在典型的交通高峰时段阻挡可能会使建筑公司每小时损失67,000美元，再加上每天5,000美元的费用！当地当局将使用沿路线的**自动交通记录器**（**ATR**）站点来监控交通流量，以及当地交通警察来记录施工时车道被阻挡的情况。
- en: The project has been planned as a 2-year construction project; the first year
    will expand the westbound lanes on the I-94 route, while the second will expand
    the eastbound lanes. The on-site portion of the construction will only occur from
    May through October because snow is less likely to delay construction during these
    months. Throughout the rest of the year, they will focus on pre-fabrication. They
    will attempt to work weekdays only because the workers union negotiated generous
    overtime pay for weekends. Therefore, weekend construction will happen only if
    there are significant delays. However, the union agreed to work holidays May through
    October for the same rate.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目已被规划为一个为期2年的建设项目；第一年将在I-94路线的西行车道进行扩建，而第二年将扩建东行车道。施工现场的建设仅从5月到10月进行，因为在这几个月里下雪不太可能延误施工。在整个余下的年份，他们将专注于预制。他们将尝试只在工作日工作，因为工人联盟为周末谈判了慷慨的加班费。因此，只有在有重大延误的情况下，周末才会进行施工。然而，工会同意在5月至10月期间以相同的费率在节假日工作。
- en: 'The construction company doesn’t want to take any risks! Therefore, they need
    a model to predict traffic for the I-94 route and, more importantly, to understand
    what factors create uncertainty and possibly increase costs. They have hired a
    machine learning expert to do this: you!'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑公司不想承担任何风险！因此，他们需要一个模型来预测I-94路线的交通流量，更重要的是，要了解哪些因素会创造不确定性并可能增加成本。他们已经聘请了一位机器学习专家来完成这项工作：就是你！
- en: The ATR data provided by the construction company includes hourly traffic volumes
    up to September 2018, as well as weather data at the same timescale. It only consists
    of the westbound lanes because that expansion will come first.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 建筑公司提供的ATR数据包括截至2018年9月的每小时交通量，以及同一时间尺度的天气数据。它只包括西行车道，因为那部分扩建将首先进行。
- en: The approach
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'You have trained a stateful **Bidirectional LSTM** model with almost four years’
    worth of data (October 2012 – September 2016). You reserved the last year for
    testing (September 2017–2018) and the prior year to that for validation (September
    2016 –2017). This made sense because the combined testing and validation datasets
    align well with the highway expansion project’s expected conditions (March – November).
    You wondered about using other splitting schemes that leveraged only the data
    representative of these conditions, but you didn’t want to reduce the training
    data so drastically, and maybe they might need it for winter predictions after
    all. A look-back window defines how much past data a time series model has access
    to. You chose 168 hours (1 week) as the look-back window size. Given the stateful
    nature of the model, as the model moves forward in the training data, it can learn
    daily and weekly seasonality, as well as some trends and patterns that can only
    be observed across several weeks. You also trained another two models. You have
    outlined the following steps to meet the client’s expectations:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经使用近四年的数据（2012年10月 – 2016年9月）训练了一个有状态的**双向LSTM**模型。您保留了最后一年用于测试（2017年9月–2018年）和之前一年用于验证（2016年9月
    –2017年）。这样做是有道理的，因为测试和验证数据集与高速公路扩建项目预期的条件（3月 – 11月）相吻合。您曾考虑使用仅利用这些条件数据的其他分割方案，但您不想如此大幅度地减少训练数据，也许它们最终还是可能需要用于冬季预测。回望窗口定义了时间序列模型可以访问多少过去数据。您选择了168小时（1周）作为回望窗口大小。鉴于模型的这种有状态性质，随着模型在训练数据中的前进，它可以学习每日和每周的季节性，以及一些只能在几周内观察到的趋势和模式。您还训练了另外两个模型。您概述了以下步骤以满足客户期望：
- en: With *RMSE*, *regression plots*, *confusion matrices*, and much more, you will
    access the models’ predictive performance and, more importantly, how the error
    is distributed.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *RMSE*、*回归图*、*混淆矩阵* 等等，您将访问模型的预测性能，更重要的是，了解误差的分布情况。
- en: With *integrated gradients*, you will understand if you took the best modeling
    strategy since it can help you visualize each of the model’s pathways to a decision,
    and help you choose a model based on that.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *集成梯度*，您将了解是否采取了最佳建模策略，因为它可以帮助您可视化模型到达决策的每一条路径，并帮助您根据这一点选择模型。
- en: With *SHAP’s* `KernelExplainer` and a prediction approximation method, you will
    derive both a global and local understanding of what features matter to the chosen
    model.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *SHAP的* `KernelExplainer` 和预测近似方法，您将推导出对所选模型有重要意义的特征的全局和局部理解。
- en: With *Morris sensitivity analysis*, you will identify *factor prioritization*,
    which ranks factors (in other words, features) by how much they can drive output
    variability.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *Morris敏感性分析*，您将识别 *因子优先级*，它根据它们可以驱动输出变异性的程度对因素（换句话说，特征）进行排序。
- en: With *Sobol sensitivity analysis*, you will compute *factor fixing*, which helps
    determine what factors aren’t influential. It does this by quantifying the input
    factors’ contributions and interactions to the output’s variability. With this,
    you can understand what factors may have the most effect on potential fines and
    costs, thus producing a variance-based cost-sensitivity analysis.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *Sobol敏感性分析*，您将计算 *因子固定*，这有助于确定哪些因素不具有影响力。它是通过量化输入因素对输出变异性的贡献和相互作用来做到这一点的。有了这个，您可以了解哪些因素可能对潜在的罚款和成本影响最大，从而产生基于变异性的成本敏感性分析。
- en: The preparation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You can find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/09/Traffic_compact1.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/09/Traffic_compact1.ipynb).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处找到此示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/09/Traffic_compact1.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/09/Traffic_compact1.ipynb)。
- en: Loading the libraries
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you will need to install the following libraries:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mldatasets` 用于加载数据集'
- en: '`pandas` and `numpy` to manipulate the dataset'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 和 `numpy` 用于操作数据集'
- en: '`tensorflow` to load the model'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow` 用于加载模型'
- en: '`sklearn` (scikit-learn), `matplotlib`, `seaborn`, `alibi`, `distython`, `shap`,
    and `SALib` to create and visualize the interpretations'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`、`matplotlib`、`seaborn`、`alibi`、`distython`、`shap` 和 `SALib`
    用于创建和可视化解释'
- en: 'You should load all of them first:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该首先加载所有这些内容：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s check that TensorFlow has loaded the right version by using the `print(tf.__version__)`
    command. It should be 2.0 or above.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`print(tf.__version__)`命令来检查TensorFlow是否加载了正确的版本。它应该是2.0或更高版本。
- en: Understanding and preparing the data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There should be over 52,000 records and 16 columns. We can verify this with
    `traffic_df.info()`. The output should check out. All the features are numerical
    and have no missing values, and the categorical features have already been one-hot
    encoded for us.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 应该有超过52,000条记录和16列。我们可以使用`traffic_df.info()`来验证这一点。输出应该符合预期。所有特征都是数值型的，没有缺失值，并且分类特征已经为我们进行了独热编码。
- en: The data dictionary
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据字典
- en: 'There are only nine features, but they become 16 columns because of categorical
    encoding:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分类编码，只有九个特征，但它们变成了16列：
- en: '`dow`: Ordinal; day of the week starting with Monday (between 0 and 6)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dow`: 序数型；以星期一开始的星期几（介于0到6之间）'
- en: '`hr`: Ordinal; hour of the day (between 0 and 23)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hr`: 序数型；一天中的小时（介于0到23之间）'
- en: '`temp`: Continuous; average temperature in Celsius (between-30 and 37)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temp`: 连续型；摄氏度平均温度（介于-30到37之间）'
- en: '`rain_1h`: Continuous; mm of rainfall occurred in the hour (between 0 and 21)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rain_1h`: 连续型；该小时发生的降雨量（介于0到21毫米之间）'
- en: '`snow_1h`: Continuous; cm of snow (when converted to liquid form) occurred
    in the hour (between 0 and 2.5)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`snow_1h`: 连续型；该小时发生的雪量（当转换为液体形式时）（介于0到2.5厘米之间）'
- en: '`cloud_coverage`: Continuous; percentage of cloud coverage (between 0 and 100)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud_coverage`: 连续型；云层覆盖率百分比（介于0到100之间）'
- en: '`is_holiday`: Binary; is the day a national or state holiday when it occurs
    Monday to Friday (1 for yes, 0 for no)?'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_holiday`: 二元型；该天是星期一到星期五的国家或州假日吗？（1表示是，0表示否）？'
- en: '`traffic_volume`: Continuous; the target feature capturing traffic volume'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`traffic_volume`: 连续型；捕获交通量的目标特征'
- en: '`weather`: Categorical; a short description of the weather during that hour
    (Clear | Clouds | Haze | Mist | Rain | Snow | Unknown | Other)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weather`: 分类；该小时天气的简短描述（晴朗 | 云层 | 雾 | 薄雾 | 雨 | 雪 | 未知 | 其他）'
- en: Understanding the data
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解数据
- en: The first step in understanding a time series problem is understanding the target
    variable. This is because it determines how you approach everything else, from
    data preparation to modeling. The target variable is likely to have a special
    relationship with time, such as a seasonal movement or a trend.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理解时间序列问题的第一步是理解目标变量。这是因为它决定了你如何处理其他所有事情，从数据准备到建模。目标变量可能与时间有特殊的关系，例如季节性变动或趋势。
- en: Understanding weeks
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解周
- en: 'First, we can sample one 168-hour period from every season to understand the
    variance a bit better between days of the week, and then get an idea of how they
    could vary across seasons and holidays:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以从每个季节中采样一个168小时的时间段，以更好地理解一周中每天之间的方差，然后了解它们如何在季节和假日之间变化：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code generates the plots shown in *Figure 9.2*. If you read them
    from left to right, you’ll see that they all start with Wednesday and end with
    Tuesday of the following week. Every day of the week starts and ends at a low
    point, with a high point in between. Weekdays tend to have two peaks corresponding
    to morning and afternoon rush hour, while weekends only have one mid-afternoon
    bump:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了*图9.2*中显示的图表。如果你从左到右阅读它们，你会看到它们都是从星期三开始，以下一周的星期二结束。每周的每一天都是从低点开始和结束，中间有一个高点。工作日通常有两个高峰，对应早晨和下午高峰时段，而周末只有一个下午的峰值：
- en: '![](img/B18406_09_02.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_09_02.png)'
- en: 'Figure 9.2: Several sample weekly periods for traffic_volume representing each
    season'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：代表每个季节的几个交通量样本周周期
- en: There are some major outliers, such as Saturday October 31, which is basically
    Halloween and is not an official holiday. Also, February 2 (a Tuesday) was the
    beginning of a severe snowstorm, and the period in the late summer is much more
    chaotic than the other sample weeks. It turns out that in that year, the state
    fair occurred. Like Halloween, it’s not a federal or a regional holiday, but it’s
    important to note that the fairgrounds are located halfway between Minneapolis
    and St. Paul. You’ll also notice that on Friday July 29, there’s a midnight bump
    in traffic, which can be attributed to this being a big day for Minneapolis concerts.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些主要异常值，例如10月31日星期六，这基本上是万圣节，并不是官方的节假日。还有2月2日（星期二）是严重的暴风雪的开始，而夏末的时期比其他样本周要混乱得多。结果发现，那一年州博览会发生了。像万圣节一样，它既不是联邦节日也不是地区节日，但重要的是要注意博览会场地位于明尼阿波利斯和圣保罗之间的一半。你还会注意到，在7月29日星期五午夜时，交通量有所上升，这可以归因于这是明尼阿波利斯音乐会的大日子。
- en: Trying to explain these inconsistencies while comparing periods in your time
    series is a good exercise as it helps you figure out what variables to add to
    your model, or at least know what is missing. In our case, we know our `is_holiday`
    variable doesn’t include days such as Halloween or the entire state fair week,
    nor do we have a variable for big music or sporting events. To produce a more
    robust model, it would be advisable to look for reliable external data sources
    and add more features that cover all these possibilities, not to mention validate
    the existing variables. For now, we will work with what we’ve got.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较时间序列中的各个时期时，试图解释这些不一致性是一个很好的练习，因为它有助于你确定需要添加到模型中的变量，或者至少知道缺少了什么。在我们的案例中，我们知道我们的`is_holiday`变量不包括万圣节或整个州博览会周，也没有针对大型音乐或体育赛事的变量。为了构建一个更稳健的模型，寻找可靠的外部数据源并添加更多覆盖所有这些可能性的特征是明智的，更不用说验证现有变量了。目前，我们将使用我们拥有的数据。
- en: Understanding days
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解日子
- en: It is crucial for the highway expansion project to understand what traffic looks
    like for the average workday. The construction crew will be working on weekdays
    only (Monday to Friday) unless they experience delays, in which case they will
    also work weekends. We must also make a distinction between holidays and other
    weekdays because these are likely to be different.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高速公路扩建项目来说，了解平均工作日的交通状况至关重要。施工队伍只在工作日（周一至周五）工作，除非遇到延误，在这种情况下，他们也会在周末工作。我们还必须区分节假日和其他工作日，因为这些可能有所不同。
- en: 'To this end, we will create a DataFrame (`weekend_df`) and engineer a new column
    (`type_of_day`) that codes hours as being part of a “Holiday,” “Weekday,” or “Weekend.”
    Then, we can group by this column and the `hr` column, and aggregate with `mean`
    and standard deviation (`std`). We can then `pivot` so that we have one column
    with the average and standard deviations traffic volumes for every `type_of_day`
    category, where the rows represent the hours of the day (`hr`). Then, we can plot
    the resulting DataFrame. We can create intervals with the standard deviations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们将创建一个DataFrame（`weekend_df`）并创建一个新列（`type_of_day`），将小时编码为“假日”、“工作日”或“周末”。然后，我们可以按此列和`hr`列进行分组，并使用`mean`和标准差（`std`）进行聚合。然后我们可以进行`pivot`，以便我们有一个列，其中包含每个`type_of_day`类别的平均交通量和标准差，其中行代表一天中的小时数（`hr`）。然后，我们可以绘制结果DataFrame。我们可以创建包含标准差的区间：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding snippet results in the following plot. It represents the hourly
    average, but there’s quite a bit of variation, which is why the construction company
    is proceeding with caution. There are horizontal lines that have been plotted
    representing each of the thresholds:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段产生了以下图表。它表示每小时平均交通量，但变化很大，这就是为什么建筑公司正在谨慎行事。图中绘制了代表每个阈值的水平线：
- en: 5,300 for full capacity.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量满载时为5,300。
- en: 2,650 for half-capacity, after which the construction company will get fined
    the daily amount specified.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半容量时为2,650，之后建筑公司将因每日指定金额被罚款。
- en: 1,500 is the no-construction threshold, after which the construction company
    will get fined the hourly amount specified.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无施工阈值是1,500，之后建筑公司将因每小时指定金额被罚款。
- en: 'They only want to work Monday to Friday during the hours that are typically
    below the 1,500 threshold. These five hours would be 11 p.m. (the day before)
    to 5 a.m. If they had to work weekends, this schedule would typically be delayed
    until 1 a.m. and end at 6 a.m. There’s considerably less variance during weekdays,
    so it’s understandable why the construction company is adamant about only working
    weekdays. During these hours, holidays appear to be similar to weekends, but holidays
    tend to vary even more than weekends, which is potentially even more problematic:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 他们只想在通常低于1500阈值的小时内工作，周一到周五。这五个小时将是晚上11点（前一天）到早上5点。如果他们必须周末工作，这个时间表通常会推迟到凌晨1点，并在早上6点结束。在工作日，变化相对较小，所以建筑公司坚持只在工作日工作是可以理解的。在这些小时里，节假日看起来与周末相似，但节假日的变化甚至比周末更大，这可能是更成问题的情况：
- en: '![Chart  Description automatically generated](img/B18406_09_03.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_09_03.png)'
- en: 'Figure 9.3: The average hourly traffic volume for holidays, weekdays, and weekends,
    with intervals'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：节假日、工作日和周末的平均每小时交通量，以及间隔
- en: Usually, for a project like this, you would explore the predictor variables
    to the extent we have done with the target. This book is about model interpretation,
    so we will learn about the predictors by interpreting the models. But before we
    get to the models, we must prepare the data for them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于这样的项目，你会探索预测变量，就像我们对目标所做的那样。这本书是关于模型解释的，所以我们将通过解释模型来了解预测变量。但在我们到达模型之前，我们必须为它们准备数据。
- en: Data preparation
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'The first data preparation step is to split it into train, validation, and
    test sets. Please note that the test dataset comprises the last 52 weeks (`2184`
    hours), while the validation dataset comprises the 52 weeks before that, so it
    starts at `4368` and ends `2184` hours before the last row of the DataFrame:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步数据准备是将数据分割成训练集、验证集和测试集。请注意，测试数据集包括最后52周（`2184`小时），而验证数据集包括之前的52周，因此它从`4368`小时开始，到DataFrame最后一行之前的`2184`小时结束：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that the DataFrame has been split, we can plot it to ensure that its parts
    are split as intended. We can do so with the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在DataFrame已经被分割，我们可以绘制它以确保其部分是按照预期分割的。我们可以使用以下代码来完成：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code produces *Figure 9.4*. It shows that almost 4 years of data
    was allocated for the training dataset, and a year to validate and test each.
    We won’t reference the validation dataset from this point on during this exercise
    because it was only instrumental during training to assess the model’s predictive
    performance after every epoch.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图9.4*。它显示，训练数据集分配了近4年的数据，而验证和测试各分配了一年。在这个练习中，我们将不再引用验证数据集，因为它只是在训练期间作为工具来评估模型在每个epoch后的预测性能。
- en: '![Chart  Description automatically generated with low confidence](img/B18406_09_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成，置信度低](img/B18406_09_04.png)'
- en: 'Figure 9.4: Time series split into train, validation, and test sets'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：时间序列分割为训练集、验证集和测试集
- en: The next step is to min-max normalize the data. We are doing this because larger
    values lead to slower learning for all neural networks in general and LSTMs are
    very prone to **exploding and vanishing gradients**. Relatively uniform and small
    numbers can help counter these problems. We will discuss this later in this chapter,
    but basically, the network becomes either numerically unstable or ineffective
    at reaching a global minimum.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对数据进行min-max归一化。我们这样做是因为较大的值会导致所有神经网络的学习速度变慢，而LSTM非常容易发生**梯度爆炸和消失**。相对均匀且较小的数字可以帮助解决这些问题。我们将在本章后面讨论这个问题，但基本上，网络要么在数值上不稳定，要么在达到全局最小值方面无效。
- en: 'We can min-max normalize with `MinMaxScaler` from the `scikit` package. For
    now, all we will do is `fit` the scaler so that we can use them whenever we need
    them. We will create a scaler for our target (`traffic_volume`) called `y_scaler`
    and another for the rest of the variables (`X_scaler`) with the entire dataset,
    so that transformations are consistent no matter what part you are using, be it
    `train`, `valid`, or `test`. All the `fit` process does is save the formula to
    make each variable fit between zero and one:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scikit`包中的`MinMaxScaler`进行min-max归一化。目前，我们只会对归一化器进行`fit`操作，以便我们可以在需要时使用它们。我们将为我们的目标（`traffic_volume`）创建一个名为`y_scaler`的归一化器，并为其余变量（`X_scaler`）创建另一个归一化器，使用整个数据集，以确保无论使用哪个部分（`train`、`valid`或`test`），转换都是一致的。所有的`fit`过程只是保存公式，使每个变量适合在零和一之间：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will `transform` both our train and test datasets with our scaler,
    creating *y* and *X* pairs for each:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用我们的缩放器 `transform` 我们的训练和测试数据集，为每个创建 *y* 和 *X* 对：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: However, for a time series model, the *y* and *X* pairs we created aren’t useful
    because each observation is a timestep. And each timestep is more than the features
    that occur for that timestep, but to a certain extent what happens before it,
    called lags. For instance, say if we predict traffic based on 168 lagged observations,
    for every label, we will need the previous 168 hours of each feature. Therefore,
    you have to generate an array for every timestep, as well as its lags. Fortunately,
    `keras` has a function called `TimeseriesGenerator` that takes your *X* and *y*
    and produces a generator that feeds the data to your model. You must specify a
    certain `length`, which is the number of lagged observations (also known as the
    **lookback window**). The default `batch_size` is one, but we are using 24 because
    the client prefers to get forecasts 24 hours at a time, and also training and
    inference are much faster with a larger batch size.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于时间序列模型，我们创建的 *y* 和 *X* 对并不有用，因为每个观测值都是一个时间步长。每个时间步长不仅仅是该时间步长发生的特征，而且在一定程度上是它之前发生的事情，称为滞后。例如，如果我们根据
    168 个滞后观测值预测交通，对于每个标签，我们将需要每个特征的之前 168 小时的数据。因此，你必须为每个时间步长以及其滞后生成一个数组。幸运的是，`keras`
    有一个名为 `TimeseriesGenerator` 的函数，它接受你的 *X* 和 *y* 并生成一个生成器，该生成器将数据馈送到你的模型。你必须指定一个特定的
    `length`，这是滞后观测值的数量（也称为 **lookback window**）。默认的 `batch_size` 是一个，但我们使用 24，因为客户更喜欢一次获取
    24 小时的预测，而且使用更大的批次大小进行训练和推理要快得多。
- en: 'Naturally, when you need to forecast tomorrow, you will need tomorrow’s weather,
    but you can complete the timesteps with weather forecasts:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，当你需要预测明天时，你需要明天的天气，但你可以用天气预报来补充时间步长：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Any model that was trained with a 1-week look-back window and 24-hour batch
    size will need this generator. Each generator is a list of tuples corresponding
    to each batch. Index 0 of this tuple is the *X* feature array, while index 1 is
    the *y* label array. Therefore, the first number output is the length of the list,
    which is the number of batches. The dimensions of the *X* and *y* array follow.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 任何使用 1 周滞后窗口和 24 小时批次大小训练的模型都需要这个生成器。每个生成器是与每个批次对应的元组的列表。这个元组的索引 0 是 *X* 特征数组，而索引
    1 是 *y* 标签数组。因此，输出的第一个数字是列表的长度，即批次的数量。*X* 和 *y* 数组的维度随后。
- en: For instance, `gen_train` has 1,454 batches, and each batch has 24 timesteps,
    with a length of 168 and 15 features. The shape of the predicted labels expected
    from these 24 timesteps is `(24,1)`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`gen_train` 有 1,454 个批次，每个批次有 24 个时间步长，长度为 168，有 15 个特征。从这些 24 个时间步长中预期的预测标签的形状是
    `(24,1)`。
- en: 'Lastly, before moving forward with handling models and stochastic interpretation
    methods, let’s attempt to make things more reproducible by initializing our random
    seeds:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在继续处理模型和随机解释方法之前，让我们尝试通过初始化我们的随机种子来使事情更具可重复性：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Loading the LSTM model
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载 LSTM 模型
- en: 'We can quickly load the model and output its summary like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速加载模型并像这样输出其摘要：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, let’s assess the `LSTM_traffic_168_compact1` model using traditional interpretation
    methods.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用传统的解释方法评估 `LSTM_traffic_168_compact1` 模型。
- en: Assessing time series models with traditional interpretation methods
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用传统的解释方法评估时间序列模型
- en: A time series regressor model can be evaluated as you would evaluate any regression
    model; that is, using metrics derived from the **mean squared error** or the **R-squared**
    score. There are, of course, cases in which you will need to use a metric with
    medians, logs, deviances, or absolute values. These models don’t require any of
    this.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列回归模型可以像评估任何回归模型一样进行评估；也就是说，使用来自 **均方误差** 或 **R-squared** 分数的指标。当然，在某些情况下，你可能需要使用具有中位数、对数、偏差或绝对值的指标。这些模型不需要任何这些。
- en: Using standard regression metrics
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用标准的回归指标
- en: The `evaluate_reg_mdl` function can evaluate the model, output some standard
    regression metrics, and plot them. The parameters for this model are the fitted
    model (`lstm_traffic_mdl`), `X_train` (`gen_train`), `X_test` (`gen_test`), `y_train`,
    and `y_test`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate_reg_mdl` 函数可以评估模型，输出一些标准的回归指标，并绘制它们。此模型的参数是拟合的模型 (`lstm_traffic_mdl`)，`X_train`
    (`gen_train`)，`X_test` (`gen_test`)，`y_train` 和 `y_test`。'
- en: Optionally, we can specify a `y_scaler` so that the model is evaluated with
    the labels’ inverse transformed, which makes the plot and **root mean square error**
    (**RMSE**) much easier to interpret. Another optional parameter that is very much
    necessary, in this case, is `y_truncate=True` because our `y_train` and `y_test`
    are of larger dimensions than the predicted labels. This discrepancy happens because
    the first prediction occurs several timesteps after the first timestep in the
    dataset due to the look-back window. Therefore, we would need to deduct these
    timesteps from `y_train` in order to match the length of `gen_train`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以指定一个`y_scaler`，以便模型使用标签的逆变换进行评估，这使得绘图和**均方根误差**（**RMSE**）更容易理解。在这种情况下，另一个非常必要的可选参数是`y_truncate=True`，因为我们的`y_train`和`y_test`的维度比预测标签大。这种差异发生是因为由于回望窗口，第一次预测发生在数据集的第一个时间步之后。因此，我们需要从`y_train`中减去这些时间步，以便与`gen_train`的长度匹配：
- en: We will now evaluate both models with the following code. To observe the prediction’s
    progress as it happens, we will use `predopts={"verbose":1}`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用以下代码评估这两个模型。为了观察预测的进度，我们将使用`predopts={"verbose":1}`。
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/B18406_09_05.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_09_05.png)'
- en: 'Figure 9.5: Predictive performance evaluations for the “LSTM_traffic_168_compact1”
    model'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：“LSTM_traffic_168_compact1”模型的预测性能评估
- en: 'We can also evaluate the model by comparing observed versus predicted traffic.
    It would be helpful to break down the error by the hour and type of day too. To
    this end, we can create DataFrames with these values – one for each model. But
    first, we must truncate the DataFrame (`-y_test_pred.shape[0]`) so that it matches
    the length of the predictions array, and we won’t need all the columns, so we
    are providing indexes for only those we are interested in: `traffic_volume` is
    #7 but we also will want `dow` (#0), `hr` (#1), and `is_holiday` (#6). We will
    rename `traffic_volume` to `actual_traffic` and create a new column called `predicted_traffic`
    with our predictions. Then, we will engineer a `type_of_day` column, as we did
    previously, which tells us if it’s a holiday, weekday, or weekend. Finally, we
    can drop the `dow` and `is_holiday` columns since we won’t need them:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过比较观察到的与预测的交通来评估模型。按小时和类型分解错误可能会有所帮助。为此，我们可以创建包含这些值的DataFrames - 每个模型一个。但首先，我们必须截断DataFrame（`-y_test_pred.shape[0]`），以便它与预测数组的长度匹配，我们不需要所有列，所以我们只提供我们感兴趣的索引：`traffic_volume`是第7列，但我们还希望有`dow`（第0列）、`hr`（第1列）和`is_holiday`（第6列）。我们将`traffic_volume`重命名为`actual_traffic`，并创建一个名为`predicted_traffic`的新列，其中包含我们的预测。然后，我们将创建一个`type_of_day`列，就像我们之前做的那样，它告诉我们是否是节假日、工作日还是周末。最后，我们可以删除`dow`和`is_holiday`列，因为我们不再需要它们：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can quickly review the contents of the DataFrames by simply running a cell
    with `evaluate_df`. It should have 4 columns.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过简单地运行一个带有`evaluate_df`的单元格来快速查看DataFrames的内容。它应该有4列。
- en: Predictive error aggregations
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测误差聚合
- en: 'It may be that some days and times of day are more prone to predictive errors.
    To get a better sense of how these errors are distributed across time, we can
    plot RMSE on an hourly basis segmented by `type_of_day`. To do this, we must first
    define an `rmse` function and then group each of the models’ evaluated DataFrames
    by `type_of_day` and `hr` and use the `apply` function to aggregate using the
    `rmse` function. We can then pivot to ensure that each `type_of_day` has a column
    with the RMSEs on an hourly basis. We can then average these columns and store
    them in a series:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是某些日期和时间更容易出现预测误差。为了更好地了解这些误差如何在时间上分布，我们可以按小时分段绘制`type_of_day`的RMSE。为此，我们必须首先定义一个`rmse`函数，然后按`type_of_day`和`hr`对每个模型的评估DataFrame进行分组，并使用`apply`函数通过`rmse`函数进行聚合。然后我们可以通过转置来确保每个`type_of_day`都有一个按小时显示RMSE的列。然后我们可以平均这些列并将它们存储在一个系列中：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have DataFrames with the hourly RMSEs for holidays, weekdays, and
    weekends, as well as the average for these “types” of day, we can plot them using
    the `evaluate_by_hr` DataFrame. We will also create dotted horizontal lines with
    the averages for each `type_of_day` from the `mean_by_daytype` `pandas` series:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了包含节假日、工作日和周末每小时RMSE的DataFrames，以及这些“类型”的日平均数，我们可以使用`evaluate_by_hr` DataFrame来绘制它们。我们还将创建带有每个`type_of_day`平均值的虚线水平线，这些平均值来自`mean_by_daytype`
    `pandas`系列：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code generated the plot shown in *Figure 9.6*. As we can see,
    the model has a high RMSE for holidays. However, the model could be overestimating
    the traffic volume, and overestimating is not as bad as underestimating in this
    particular use case because underestimating can lead to annoying commuters with
    traffic delays and additional costs from fines:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图9.6*中显示的图表。正如我们所见，该模型在假日有很高的RMSE。然而，该模型可能高估了交通量，而在这种特定情况下，高估不如低估糟糕，因为低估可能导致交通延误和额外的罚款成本：
- en: '![Chart, line chart  Description automatically generated](img/B18406_09_06.png)Figure
    9.6: Hourly RMSE segmented by type_of_day for the “LSTM_traffic_168_compact1”
    model'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，折线图，描述自动生成](img/B18406_09_06.png)图9.6：“LSTM_traffic_168_compact1”模型按type_of_day类型划分的小时RMSE'
- en: Evaluating the model like a classification problem
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将模型评估视为分类问题
- en: 'Indeed, just like classification problems can have false positives and false
    negatives and one is more costly than the other, you can frame any regression
    problem with concepts such as underestimation and overestimation. This framing
    is especially useful when one is more costly than the other. If you have clearly
    defined thresholds, as we have for this project, you can evaluate any regression
    problem as you would a classification one. We will assess it with a confusion
    matrix with half-capacity and no-construction thresholds. To accomplish this,
    we can use `np.where` to get binary arrays for when the actuals and predictions
    surpass each threshold. We can then use the `compare_confusion_matrices` function
    to compare the confusion matrices for the model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，就像分类问题可以有假阳性和假阴性，其中一个是比另一个更昂贵的，你可以用诸如低估和过度估计等概念来构建任何回归问题。这种构建特别有用，当其中一个比另一个更昂贵时。如果你有明确定义的阈值，就像我们在这个项目中做的那样，你可以像评估分类问题一样评估任何回归问题。我们将使用半容量和“无施工”阈值混淆矩阵来评估它。为了完成这项任务，我们可以使用`np.where`来获取实际值和预测值超过每个阈值的二进制数组。然后我们可以使用`compare_confusion_matrices`函数来比较模型的混淆矩阵：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Chart  Description automatically generated](img/B18406_09_07.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_09_07.png)'
- en: 'Figure 9.7: Confusion matrices for going over half and the no-construction
    threshold for the “LSTM_traffic_168_compact1” model'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：“LSTM_traffic_168_compact1”模型的超过半容量和“无施工”阈值的混淆矩阵
- en: We are most interested in the percentage of false negatives (bottom-left quadrant)
    because predicting no traffic beyond the threshold when, in fact, it did rise
    above it, will lead to a steep fine. On the other hand, the cost of false positives
    is in preemptively leaving the construction site when traffic didn’t rise above
    the threshold after all. It’s better to be safe than sorry, though! If you compare
    false negatives for the “no-construction” threshold (0.85%), it’s less than a
    third of that of the half-capacity threshold (3.08%). Ultimately, what matters
    most is the no-construction threshold because the idea is to stop construction
    before it gets close to half-capacity.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最感兴趣的是假阴性（左下象限）的百分比，因为当实际上交通量超过了阈值时，预测没有超过阈值将导致高额罚款。另一方面，假阳性的成本在于在交通量实际上没有超过阈值的情况下提前离开施工现场。尽管如此，安全总是比后悔好！如果你比较“无施工”阈值的假阴性（0.85%），它不到半容量阈值（3.08%）的三分之一。最终，最重要的是无施工阈值，因为目的是在接近半容量之前停止施工。
- en: Now that we have leveraged traditional methods to understand the model’s decisions,
    let’s move on to some more advanced model-agnostic methods.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经利用传统方法来理解模型的决策，让我们继续探讨一些更高级的模型无关方法。
- en: Generating LSTM attributions with integrated gradients
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成梯度生成LSTM归因
- en: We first learned about **integrated gradients** (**IG**) in *Chapter 7*, *Visualizing
    Convolutional Neural Networks*. Unlike the other gradient-based attribution methods
    studied in that chapter, path-integrated gradients is not contingent on convolutional
    layers, nor is it limited to classification problems.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次在*第7章*，*可视化卷积神经网络*中了解到**集成梯度**（**IG**）。与该章节中研究的其他基于梯度的归因方法不同，路径集成梯度不依赖于卷积层，也不限于分类问题。
- en: In fact, since it computes the gradients of the output concerning the inputs
    averaged along the path, the input and output could be anything! It is common
    to use integrated gradients with **Convolutional Neural Networks** (**CNNs**)
    and **Recurrent Neural Networks** (**RNNs**), like the one we are interpreting
    in this chapter. Frankly, when you see an IG LSTM example online, it has an embedding
    layer and is an NLP classifier, but IG could be used very effectively for LSTMs
    that even process sounds or genetic data!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，因为它计算了输出相对于输入沿路径平均的梯度，所以输入和输出可以是任何东西！通常与**卷积神经网络**（**CNNs**）和**循环神经网络**（**RNNs**）一起使用整合梯度，就像我们在本章中解释的那样。坦白说，当你在网上看到IG
    LSTM的例子时，它有一个嵌入层，是一个NLP分类器，但IG对于甚至处理声音或遗传数据的LSTMs也非常有效！
- en: 'The integrated gradient explainer and the explainers that we will use moving
    forward can access any part of the traffic dataset. First, let’s create a generator
    for all of it:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 整合梯度解释器和我们将继续使用的解释器可以访问交通数据集的任何部分。首先，让我们为所有这些创建一个生成器：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Integrated gradients is a local interpretation method. So, let’s get a few
    sample “instances of interest” we can interpret. We know holidays may require
    specialized logic, so let’s see if our model picks up on the importance of `is_holiday`
    for one example (`holiday_afternoon_s`). Also, mornings are a concern, especially
    mornings with a larger than average rush hour because of weather conditions, so
    we have one example for that (`peak_morning_s`). Lastly, a hot day might have
    more traffic, especially on a weekend (`hot_Saturday_s`):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 整合梯度是一种局部解释方法。所以，让我们获取一些我们可以解释的“感兴趣的样本实例”。我们知道假日可能需要专门的逻辑，所以让我们看看我们的模型是否注意到了`is_holiday`在某个例子（`holiday_afternoon_s`）中的重要性。早晨也是一个问题，尤其是由于天气条件，早晨的拥堵时间比平均水平更长，所以我们有一个例子（`peak_morning_s`）。最后，一个炎热的日子可能会有更多的交通，尤其是在周末（`hot_Saturday_s`）：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now that we have created some instances, let’s instantiate our explainers.
    `IntegratedGradients` from the `alibi` package only requires a deep learning model,
    but it is recommended to set a number of steps (`n_steps`) for the integral approximation
    and `internal_batch_size`. We will instantiate an explainer for our model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一些实例，让我们实例化我们的解释器。来自`alibi`包的`IntegratedGradients`只需要一个深度学习模型，但建议为积分近似设置步骤数（`n_steps`）和内部批次大小。我们将为我们的模型实例化一个解释器：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Before we iterate our samples and the explainers, it is important to realize
    how we need to input the sample to the explainer because it will need a batch
    of 24\. To this end, we will have to get the index of the sample once we’ve deducted
    the lookback window (`nidx`). Then, you can obtain the batch for this sample from
    the generator (`gen_all`). Each batch includes 24 timesteps, so you floor `nidx`
    by 24 (`nidx//24`) to get the batch’s position for that sample. Once you’ve got
    the batch for the sample (`batch_X`) and printed the shape `(24, 168, 15)`, it
    shouldn’t surprise you that the first number is 24\. Of course, we will need to
    get the index of the sample within the batch (`nidx%24`) to obtain the data for
    that sample:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对样本和解释器进行迭代之前，重要的是要意识到我们需要如何将样本输入到解释器中，因为它需要一个包含24个样本的批次。为此，一旦我们减去回望窗口（`nidx`），我们就必须获取样本的索引。然后，你可以从生成器（`gen_all`）中获取该样本的批次。每个批次包含24个时间步长，所以你需要将`nidx`向下取整到24（`nidx//24`），以获取该样本的批次位置。一旦你获取了该样本的批次（`batch_X`）并打印了形状（`24,
    168, 15`），第一个数字是24这一点不应该让你感到惊讶。当然，我们还需要获取批次内样本的索引（`nidx%24`），以获取该样本的数据：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `for` loop will use the previously explained method to locate the batch
    for the sample `(batch_X`). This `batch_X` is inputted into the `explain` function.
    This is because this is a regression problem and there’s no target class; that
    is, `target=None`. Once the explanation is produced, the `attributions` property
    will have the attributions for the entire batch. We can only obtain this for the
    sample and `transpose` it to produce an image that has this shape: `(15, lb)`.
    The rest of the code in the `for` loop simply obtains the labels to use in the
    tick marks and then plots an image stretched out to fit the dimensions of our
    `figure`, along with its labels:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`for`循环将使用之前解释的方法来定位样本的批次（`batch_X`）。这个`batch_X`被输入到`explain`函数中。这是因为这是一个回归问题，没有目标类别；也就是说，`target=None`。一旦产生了解释，`attributions`属性将包含整个批次的属性。我们只能获取样本的属性，并将其`transpose`以产生一个形状为`(15,
    lb)`的图像。`for`循环中的其余代码只是获取用于刻度的标签，然后绘制一个图像，该图像拉伸以适应我们的`figure`维度，以及其标签：'
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding code will generate the plots shown in *Figure 9.8*. On the *y*-axis,
    you can see the variable names, while on the *x*-axis, you can see the dates corresponding
    to the lookback window for the sample in question. The rightmost part of the *x*-axis
    is the sample’s date, and as you move left, you go backward in time. For instance,
    the holiday afternoon sample was 4 p.m. September 3 and there is one week’s worth
    of lookback, so each tick mark backward is a day before that date.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将生成*图9.8*中显示的图表。在*y*轴上，您可以看到变量名称，而在*x*轴上，您可以看到对应于所讨论样本回望窗口的日期。*x*轴的最右侧是样本的日期，随着您向左移动，您会向时间后退。例如，假日下午的样本是9月3日下午4点，有一周的回望，所以每个向后的刻度代表该日期前一天。
- en: '![Graphical user interface, application, table  Description automatically generated](img/B18406_09_08.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，表格  自动生成的描述](img/B18406_09_08.png)'
- en: 'Figure 9.8: Annotated integrated gradients attribution map for all samples
    for the “LSTM_traffic_168_compact1” model'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：对于“LSTM_traffic_168_compact1”模型所有样本的注释集成梯度归因图
- en: You can tell by the intensity in the attribution maps in *Figure 9.8* which
    hour/variables mattered for the prediction. The color bar to the right of each
    attribution map can serve as a key. Negative numbers in red correspond to a negative
    correlation, while positive numbers in blue correspond to a positive correlation.
    However, something that is pretty evident is the tendency for intensities to fade
    as each map goes backward in time. Since it’s bidirectional, this happens from
    both ends. What is surprising is how fast this happens.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看*图9.8*中的归因图强度来判断哪些小时/变量对预测很重要。每个归因图右侧的颜色条可以用作参考。红色中的负数表示负相关，而蓝色中的正数表示正相关。然而，一个相当明显的是，随着每个图向时间后退，强度往往会减弱。由于它是双向的，所以这种情况发生在两端。令人惊讶的是，这个过程发生得有多快。
- en: Let’s start from the bottom. For “Hot Saturday,” day of the week, hour, temperature,
    and clear weather play an important role in this prediction increasingly as you
    get closer to the predicted time (midday Saturday). The day started cooler, which
    explains how there’s a patch of red before the blue in the temperature feature.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从底部开始。对于“热周六”，随着您接近预测时间（周六中午），星期几、小时、温度和晴朗的天气在这个预测中扮演的角色越来越重要。天气开始较凉爽，这解释了为什么在温度特征中红色区域出现在蓝色区域之前。
- en: For “Peak Morning,” attributions make sense since it was clear after it had
    been previously rainy and cloudy, which caused the rush hour to peak quickly rather
    than increase slowly. To a certain degree, the LSTM has learned that only recent
    weather matters – no more than two or three days’ worth. However, that is not
    the only reason the integrated gradients fade. They also fade because of the **vanishing
    gradient problem**. This problem occurs during backpropagation because the gradient
    values are multiplied by the weight matrices in each step, so gradients can exponentially
    decrease to zero.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“高峰早晨”，归因是有意义的，因为它在之前有雨和多云之后变得晴朗，这导致高峰时段迅速达到顶峰而不是缓慢增加。在一定程度上，LSTM已经学会了只有最近的天气才重要——不超过两三天。然而，集成梯度减弱的原因不仅仅是这一点。它们也因为**梯度消失问题**而减弱。这个问题发生在反向传播过程中，因为梯度值在每一步都要乘以权重矩阵，所以梯度可以指数级减少到零。
- en: LSTMs are organized in a very long sequence, making the network ever more ineffective
    at capturing dependencies in the long term. Fortunately, these LSTMs are **stateful**,
    which means they string batches in a sequence by leveraging states from the previous
    batch. **Statefulness** ensures learning from a long sequence, despite vanishing
    gradients. This is why when we observe the attribution map for “Holiday Afternoon,”
    there are negative attributions for `is_holiday`, which makes sense to anticipate
    no rush hour. It turns out September 3 (Labor Day) is nearly two months after
    the previous holiday (Independence Day), which is a more festive holiday. Is it
    possible that the model is picking up on these patterns?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM被组织在一个非常长的序列中，这使得网络在长期捕捉依赖关系方面越来越无效。幸运的是，这些LSTM是**有状态的**，这意味着它们通过利用前一个批次的状态将批次按顺序连接起来。**状态性**确保了从长序列中学习，尽管存在梯度消失问题。这就是为什么当我们观察“假日下午”的归因图时，对于`is_holiday`有负归因，这是预料到没有高峰时段的合理原因。结果证明，9月3日（劳动节）距离前一个假日（独立日）近两个月，而独立日是一个更盛大的节日。模型是否可能捕捉到这些模式呢？
- en: We could try subcategorizing holidays by their traffic patterns to see if that
    helps the model identify them. We could also make rolling aggregations of previous
    weather conditions to make it easier for the model to pick up on recent weather
    patterns. Weather patterns span hours, so it is intuitive to aggregate, not to
    mention easier to interpret. Interpretation methods can point us in the right
    direction as to how to improve models, and there’s certainly a lot of room for
    improvement.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试根据交通模式对假日进行子分类，看看这是否能帮助模型识别它们。我们还可以对以前的天气条件进行滚动汇总，以便模型更容易地捕捉到最近的天气模式。天气模式跨越数小时，因此汇总是直观的，而且更容易解释。解释方法可以为我们指明如何改进模型的方向，当然，改进的空间很大。
- en: Next, we will take a stab at a permutation-based method!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试一种基于排列的方法！
- en: Computing global and local attributions with SHAP’s KernelExplainer
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SHAP的KernelExplainer计算全局和局部归因
- en: Permutation methods make changes to the input to assess how much difference
    they will make to a model’s output. We first discussed this in *Chapter 4*, *Global
    Model-Agnostic interpretation methods*, but if you recall, there’s a coalitional
    framework to perform these permutations that will produce the average marginal
    contribution for each feature across different coalitions of features. This process’s
    outcome is **Shapley** **values**, which have essential mathematical properties
    such as additivity and symmetry. Unfortunately, Shapley values are costly to compute
    for datasets that aren’t small, so the SHAP library has approximation methods.
    One of these methods is `KernelExplainer`, which we also explained in *Chapter
    4* and used in *Chapter 5*, *Local Model-Agnostic Interpretation Methods*. It
    approximates the Shapley values with a weighted local linear regression, just
    like LIME does.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 排列方法通过对输入进行修改来评估它们将对模型输出产生多大的影响。我们首次在*第4章*，*全局模型无关解释方法*中讨论了这一点，但如果你还记得，有一个联盟框架可以执行这些排列，从而为不同特征的联盟产生每个特征的边际贡献的平均值。这个过程的结果是**Shapley**
    **值**，它具有如加法和对称性等基本数学性质。不幸的是，对于不是特别小的数据集，Shapley值的计算成本很高，所以SHAP库有近似方法。其中一种方法就是`KernelExplainer`，我们在*第4章*中也解释了它，并在*第5章*，*局部模型无关解释方法*中使用它。它使用加权局部线性回归来近似Shapley值，就像LIME所做的那样。
- en: Why use KernelExplainer?
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用KernelExplainer？
- en: We have a deep learning model, so why aren’t we using SHAP’s `DeepExplainer`
    as we did with the CNN in *Chapter 7*, *Visualizing Convolutional Neural Networks*?
    DeepExplainer adapted the DeepLIFT algorithm to approximate the Shapley values.
    It works very well with any feedforward network that’s used for tabular data,
    CNNs, and RNNs with an embedding layer, such as those used for an NLP classifier,
    or even to detect genomic sequences. It gets trickier for multivariate time series
    because DeepExplainer doesn’t know what to do with the input’s three-dimensional
    array. Even if it did, it includes data for previous timesteps, so you cannot
    permute one timestep without considering the previous ones. For instance, if the
    permutation dictates that the temperature is five degrees lower, shouldn’t that
    affect all the previous timestep’s temperatures up to a certain number of hours?
    And what if it’s 20 degrees lower? Doesn’t that mean it’s likely in a different
    season with entirely different weather – perhaps more clouds and snow as well?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个深度学习模型，那么为什么我们不使用SHAP的`DeepExplainer`，就像我们在*第7章*，*可视化卷积神经网络*中使用的CNN一样呢？DeepExplainer将DeepLIFT算法改编来近似Shapley值。它与任何用于表格数据、CNN和具有嵌入层的RNN（例如用于NLP分类器或用于检测基因组序列的RNN）都配合得非常好。对于多元时间序列，它变得更加复杂，因为DeepExplainer不知道如何处理输入的三维数组。即使它知道，它还包括了之前时间步的数据，因此你无法在不考虑之前时间步的情况下对单个时间步进行排列。例如，如果排列规定温度降低五度，这不应该影响之前数小时内的所有温度吗？如果温度降低20度呢？这不意味着它可能处于不同的季节，并且天气完全不同——也许还有更多的云和雪？
- en: SHAP’s `KernelExplainer` can receive any arbitrary black box `predict` function.
    It also makes assumptions about the input dimensions. Fortunately, we can change
    the input data before it permutes it, making it seem to the `KernelExplainer`
    like it’s dealing with a tabular dataset. The arbitrary `predict` function doesn’t
    have to simply call the model’s `predict` function – it can change data both on
    the way in and on the way out!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP的`KernelExplainer`可以接收任何任意的黑盒`predict`函数。它还对输入维度做出了一些假设。幸运的是，我们可以在它排列之前更改输入数据，使其对`KernelExplainer`来说，就像它正在处理一个表格数据集一样。任意的`predict`函数不必简单地调用模型的`predict`函数——它可以在输入和输出过程中更改数据！
- en: Defining a strategy to get it to work with a multivariate time series model
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个策略使其与多元时间序列模型一起工作
- en: To mimic likely past weather patterns based on the permutated input data, we
    could create a generative model or something to that effect. This strategy will
    help us to generate a variety of past timesteps that fit the permutated timestep,
    as well as to generate images for a specific class. Although this would likely
    lead to more accurate predictions, we won’t use this strategy because it’s incredibly
    time-consuming.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模仿基于排列输入数据的可能过去天气模式，我们可以创建一个生成模型或类似的东西。这种策略将帮助我们生成适合排列时间步的多种过去时间步，以及为特定类别生成图像。尽管这可能会导致更准确的预测，但我们不会使用这种策略，因为它非常耗时。
- en: 'Instead, we will find the time series data that best suits the permutated input
    with existing examples from our `gen_all` generator. There are distance metrics
    we can use to find the one that is closest to the permutated input. However, we
    must place some guardrails because if the permutation is for a Saturday at 5 a.m.
    with a temperature of 27 degrees Celsius and 90 percent cloud coverage, the closest
    observation to this one could be on a Friday at 7 a.m., but regardless of the
    weather traffic, it would be completely different. Therefore, we can implement
    a filter function that ensures that it only finds the closest observations for
    the same `dow`, `is_holiday`, and `hr`. The filter function can also clean up
    the permutated sample to remove or modify anything nonsensical for the model,
    such as a continuous value for a categorical feature:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将使用`gen_all`生成器中的现有示例来找到最适合排列输入的时间序列数据。我们可以使用距离度量来找到最接近排列输入的那个。然而，我们必须设置一些限制，因为如果排列是在周六早上5点，温度为27摄氏度，云量为90%，那么最接近的观察可能是在周五早上7点，但无论天气交通如何，它都会完全不同。因此，我们可以实现一个过滤器函数，确保它只找到相同`dow`、`is_holiday`和`hr`的最近观察。过滤器函数还可以清理排列样本，删除或修改模型中任何无意义的部分，例如分类特征的连续值：
- en: '![](img/B18406_09_09.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_09_09.png)'
- en: 'Figure 9.9: Permutation approximation strategy'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：排列近似策略
- en: '*Figure 9.9* depicts the rest of the process where it uses a distance function
    to find the closest observation to the modified permutated sample. This function
    returns the closest observation index, but the model can’t predict on singular
    observations (or timesteps), so it requires its past hourly history up to the
    lookback window. For this reason, it retrieves the right batch from the generator
    and makes a prediction on that, but the predictions will be on a different scale,
    so they need to be inverse transformed with `y_scaler`. Once the `predict` function
    has iterated through all the samples and made predictions for them and rescaled
    them, it sends them back to the `KernelExplainer`, which outputs their SHAP values.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.9*展示了使用距离函数找到修改后的排列样本最近观察的过程。此函数返回最近的观察索引，但模型不能对单个观察（或时间步）进行预测，因此它需要其过去直到`lookback`窗口的小时历史。因此，它从生成器中检索正确的批次并对其进行预测，但预测将处于不同的尺度上，因此它们需要使用`y_scaler`进行逆变换。一旦`predict`函数迭代了所有样本并对它们进行了预测和缩放，它将它们发送回`KernelExplainer`，该工具输出它们的SHAP值。'
- en: Laying the groundwork for the permutation approximation strategy
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为排列近似策略打下基础
- en: You can define a custom filter function (`filt_fn`). It takes a `pandas` DataFrame
    with the entire dataset (`X_df`) you want to filter from, as well as the permutated
    sample (`x`) for filtering and the length of the `lookback` window.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以定义一个自定义的过滤器函数（`filt_fn`）。它接受一个包含整个数据集（`X_df`）的`pandas` DataFrame，您希望从中过滤，以及用于过滤的排列样本（`x`）和`lookback`窗口的长度。
- en: 'The function can also modify the permutated sample. In this case, we have to
    do this because so many features of the model are discrete, but the permutation
    process makes them continuous. As we mentioned previously, all the filtering does
    is protect the distance function from finding a nonsensical closest sample to
    the permutated sample by limiting the options:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数还可以修改排列后的样本。在这种情况下，我们必须这样做，因为模型中有许多特征是离散的，但排列过程使它们变得连续。正如我们之前提到的，所有过滤操作所做的只是通过限制选项来保护距离函数，防止它找到排列样本的非合理最近样本：
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If you refer to *Figure 9.9*, after the filter function, the next thing we ought
    to define is the distance function. We could use any standard distance function
    accepted by `scipy.spatial.distance.cdist`, such as “Euclidean,” “cosine,” or
    “Hamming.” The problem with these standard distance functions is that they either
    work well with continuous or discrete variables but not both. We have both in
    this dataset!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你参考 *图 9.9*，在过滤器函数之后，我们接下来应该定义的是距离函数。我们可以使用 `scipy.spatial.distance.cdist`
    接受的任何标准距离函数，例如“欧几里得”、“余弦”或“汉明”。这些标准距离函数的问题在于，它们要么与连续变量很好地工作，要么与离散变量很好地工作，但不能两者都很好地工作。我们在这个数据集中两者都有！
- en: Fortunately, some alternatives exist that can handle both, such as **Heterogeneous
    Euclidean-Overlap Metric** (**HEOM**) and **Heterogeneous Value Difference Metric**
    (**HVDM**). Both methods apply different distance metrics, depending on the nature
    of the variable. HEOM uses a normalized Euclidean ![](img/B18406_09_001.png) for
    continuous and , for discrete, “overlap” distance; that is, a distance of zero
    if the same and one otherwise.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，存在一些可以处理这两种情况的替代方案，例如**异构欧几里得-重叠度量**（**HEOM**）和**异构值差异度量**（**HVDM**）。这两种方法根据变量的性质应用不同的距离度量。HEOM
    使用归一化的欧几里得距离 ![](img/B18406_09_001.png) 对连续变量，对离散变量使用“重叠”距离；即如果相同则为零距离，否则为 1。
- en: HVDM is more complicated because, for continuous variables, it’s the absolute
    distance between both values, divided by the standard deviation of the feature
    in question times four ![](img/B18406_09_002.png)), which is a great distance
    metric for handling outliers. For discrete variables, it uses a normalized **value
    difference metric**, which is based on the difference between the conditional
    probability of both values.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: HVDM 更复杂，因为对于连续变量，它是两个值之间的绝对距离，除以所涉及特征的均方根的四倍 ![](img/B18406_09_002.png))，这是一个处理异常值的好距离度量。对于离散变量，它使用归一化的**值差异度量**，这是基于两个值的条件概率之间的差异。
- en: Even though HVDM is better than HEOM for datasets with many continuous values,
    it is overkill in this case. Once the dataset has been filtered by day of the
    week (`dow`) and hour (`hr`), the remaining discrete features are all binary,
    so “overlap” distance is ideal, and for the three remaining continuous features
    (`temp`, `rain_1h`, `snow_1h`, and `cloud_coverage`), Euclidean distance should
    suffice. `distython` has an `HEOM` distance method, and all it requires is a background
    dataset (`X_df.values`) and the indexes of the categorical features (`cat_idxs`).
    We can programmatically identify these features with an `np.where` command.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管HVDM对于具有许多连续值的集合数据集比HEOM更好，但在这种情况下却是过度设计。一旦数据集通过星期几 (`dow`) 和小时 (`hr`) 过滤，剩余的离散特征都是二进制的，因此“重叠”距离是理想的，而对于剩下的三个连续特征（`temp`、`rain_1h`、`snow_1h`
    和 `cloud_coverage`），欧几里得距离应该足够。`distython` 有一个 `HEOM` 距离方法，它只需要一个背景数据集 (`X_df.values`)
    和分类特征的索引 (`cat_idxs`)。我们可以使用 `np.where` 命令编程识别这些特征。
- en: 'If you want to verify that these are the right ones, run `print(cat_idxs)`
    in a cell. Only indexes 2, 3, 4, and 5 should be omitted:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想验证这些是否是正确的，请在单元格中运行 `print(cat_idxs)`。只有索引 2、3、4 和 5 应该被省略：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can create a `lambda` function that puts everything depicted in *Figure
    9.9* together. It leverages a function called `approx_predict_ts` that takes care
    of the entire pipeline. It takes our filter function (`filt_fn`), distance function
    (`heom_dist.heom`), generator (`gen_all`), and fitted model (`lstm_traffic_mdl`),
    and chains them together, as described in *Figure 9.9*. It also scales the data
    with our scalers (`X_scaler` and `y_scaler`). Distance is computed on transformed
    features for higher accuracy, and the predictions are reverse transformed on the
    way out:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个 `lambda` 函数，将 *图 9.9* 中描述的所有内容放在一起。它利用一个名为 `approx_predict_ts` 的函数来处理整个流程。它接受我们的过滤器函数
    (`filt_fn`)、距离函数 (`heom_dist.heom`)、生成器 (`gen_all`) 和拟合的模型 (`lstm_traffic_mdl`)，并将它们链接在一起，如
    *图 9.9* 所示。它还使用我们的缩放器 (`X_scaler` 和 `y_scaler`) 对数据进行缩放。距离是在转换后的特征上计算的，以提高准确性，并且预测在输出过程中进行反向转换：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now use the prediction function with `KernelExplainer`, but it should
    be done on samples that are most representative of the construction crew’s expected
    working conditions; that is, they plan to work March through November only, preferably
    on weekdays and in low-traffic hours. To this end, let’s create a DataFrame (`working_season_df`)
    that only includes these months and initializes a `KernelExplainer` with `predict_fn`
    and the k-means of the DataFrame as background data:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`KernelExplainer`的预测函数，但应该在最能代表施工队预期工作条件的样本上进行；也就是说，他们计划在3月到11月工作，最好是工作日和交通量低的时间。为此，让我们创建一个只包括这些月份的DataFrame（`working_season_df`），并使用`predict_fn`和DataFrame的k-means作为背景数据初始化一个`KernelExplainer`：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can now produce SHAP values for a random set of observations of the `working_season_df`
    DataFrame.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以为`working_season_df` DataFrame的随机观测值集生成SHAP值。
- en: Computing the SHAP values
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算SHAP值
- en: 'We will sample 48 observations from it. `KernelExplainer` is rather slow, especially
    when it’s using our approximation method. To get an optimal global interpretation,
    it is best to use a high number of observations but also a high `nsamples`, which
    is the number of times we need to reevaluate the model when explaining each prediction.
    Unfortunately, having 50 of each would cause the explainer to take many hours
    to run, depending on your available compute, so we will use `nsamples=10`. You
    can look at SHAP’s progress bar and adjust it accordingly. Once it’s done, it
    will produce a feature importance `summary_plot` containing the SHAP values:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从其中采样48个观测值。`KernelExplainer`相当慢，尤其是在使用我们的近似方法时。为了获得最佳的全球解释，最好使用大量的观测值，同时也要使用高`nsamples`，这是在解释每个预测时需要重新评估模型次数的数量。不幸的是，如果每种都有50个，那么解释器运行起来将需要数小时，这取决于你的可用计算资源，所以我们将会使用`nsamples=10`。你可以查看SHAP的进度条并相应地调整。一旦完成，它将生成包含SHAP值的特征重要性`summary_plot`：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding code plots the summary shown in the following graph. Not surprisingly,
    `hr` and `dow` are the most important features, followed by some weather features.
    Strangely enough, temperature and rain don’t seem to weigh in on the predictions,
    but late spring through fall may not be a significant factor. Or maybe more observations
    and a higher `nsample` will yield a better global interpretation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码绘制了以下图形中显示的摘要。不出所料，`hr`和`dow`是最重要的特征，其次是某些天气特征。奇怪的是，温度和降雨似乎并没有在预测中起到作用，但晚春到秋季可能不是一个显著因素。或者，也许更多的观测值和更高的`nsample`将产生更好的全球解释：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18406_09_10.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![包含图形用户界面的图片描述自动生成](img/B18406_09_10.png)'
- en: 'Figure 9.10: SHAP summary plot based on the SHAP values produced by 48 sampled
    observations'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：基于48个采样观测值产生的SHAP摘要图
- en: 'We can do the same with the instances of interest we chose in the previous
    section for local interpretations. Let’s iterate through all these data points.
    Then, we can produce a single `shap_values`, but this time with `nsamples=80`,
    and then generate a `force_plot` for each one:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用上一节中选择的感兴趣实例进行相同的操作，以进行局部解释。让我们遍历所有这些数据点。然后，我们可以生成一个单一的`shap_values`，但这次使用`nsamples=80`，然后为每个生成一个`force_plot`：
- en: '[PRE29]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code generates the plots shown in *Figure 9.11*. “Holiday afternoon”
    has the hour (`hr=16`) pushing toward a higher prediction, while the fact that
    it’s a Monday (`dow=0`) and a holiday (`is_holiday=1`) is a driving force in the
    opposite direction. On the other hand, “Peak Morning” is mostly peak due to the
    hour (`hr=8.0`), but it has a high `cloud_coverage`, affirmative `weather_Clouds`,
    and yet no rain (`rain_1h=0.0`). Lastly, “Hot Saturday” has the day of the week
    (`dow=5`) pushing for a lower value, but the abnormally high value is mostly due
    to it being midday with no rain and clouds. Strangely, higher than normal temperature
    is not one of the factors:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图9.11*中显示的图形。“假日午后”的小时数(`hr=16`)推动预测值升高，而它是星期一(`dow=0`)和假日(`is_holiday=1`)的事实则推动预测值向相反方向移动。另一方面，“高峰早晨”主要由于小时数(`hr=8.0`)而处于高峰状态，但它有高`cloud_coverage`，肯定的`weather_Clouds`，而且没有降雨(`rain_1h=0.0`)。最后，“炎热的周六”由于星期数(`dow=5`)推动值降低，但异常高的值主要由于它是中午没有降雨和云层。奇怪的是，高于正常温度不是影响因素之一：
- en: '![Timeline  Description automatically generated](img/B18406_09_11.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![时间线描述自动生成](img/B18406_09_11.png)'
- en: 'Figure 9.11: Force plots generated with SHAP values using nsamples=80 for a
    Holiday Afternoon, Peak Morning, and Hot Saturday'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：使用SHAP值和nsamples=80生成的力图，用于假日午后、高峰早晨和炎热周六
- en: With SHAP’s game theory-based approach, we can gauge how many permutations for
    the existing observations marginally vary the predicted outcome across many possible
    coalitions of features. However, this approach can be very limiting because our
    background data’s existing variance shapes our understanding of outcome variance.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SHAP基于博弈论的方法，我们可以衡量现有观察值的排列如何使预测结果在许多可能特征联盟中边际变化。然而，这种方法可能非常有限，因为我们的背景数据中现有的方差塑造了我们对于结果方差的理解。
- en: In the real world, *variability is often determined by what is NOT represented
    in your data – but infinitesimally plausible*. For instance, reaching 25°C (77°F)
    before 5 a.m. in a Minneapolis summer is not a common occurrence, but with global
    warming, it could become frequent, so we would want to simulate how it could impact
    traffic patterns. Forecasting models are particularly prone to risk, so simulating
    is a crucial interpretation component to assess this uncertainty. A better understanding
    of uncertainty can yield more robust models and directly inform decisions. Next,
    we will discuss how we can produce simulations with sensitivity analysis methods.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，*变异性通常由数据中未表示的内容决定——但可能性极小*。例如，在明尼阿波利斯夏季凌晨5点之前达到25°C（77°F）并不常见，但随着全球变暖，它可能会变得频繁，因此我们想要模拟它如何影响交通模式。预测模型特别容易受到风险的影响，因此模拟是评估这种不确定性的关键解释组成部分。对不确定性的更好理解可以产生更稳健的模型，并直接指导决策。接下来，我们将讨论我们如何使用敏感性分析方法产生模拟。
- en: Identifying influential features with factor prioritization
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用因素优先级识别有影响力的特征
- en: The **Morris method** is one of several global sensitivity analysis methods
    that range from simple **Fractional factorial** to complicated **Monte Carlo filtering**.
    Morris is somewhere on this spectrum, falling into two categories. It uses **one-at-a-time
    sampling**, which means that only one value changes between consecutive simulations.
    It’s also an **Elementary Effects** (**EE**) method, which means that it doesn’t
    quantify the exact effect of a factor in a model but rather gauges its importance
    and relationship with other factors. By the way, **factor** is just another word
    for a feature or variable that’s commonly used in applied statistics. To be consistent
    with the related theory, we will use this word in this and the next section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**莫里斯方法**是几种全局敏感性分析方法之一，范围从简单的**分数因子**到复杂的**蒙特卡洛过滤**。莫里斯位于这个光谱的某个位置，分为两个类别。它使用**一次一个采样**，这意味着在连续模拟之间只有一个值发生变化。它也是一个**基本效应**（**EE**）方法，这意味着它不量化模型中因素的确切效应，而是衡量其重要性和与其他因素的关系。顺便说一句，**因素**只是另一个在应用统计学中常用的特征或变量的名称。为了与相关理论保持一致，我们将在本节和下一节中使用这个词汇。'
- en: Another property of Morris is that it’s less computationally expensive than
    the variance-based methods we will study next. It can provide more insights than
    simpler and less costly methods such as regression-, derivative-, or factorial-based
    ones. It can’t quantify effects precisely but can identify those with negligible
    or interaction effects, making it an ideal method for screening factors when the
    number of factors is low. Screening is also known as **factor prioritization**
    because it can prioritize your factors by how they are classified.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 莫里斯的另一个特性是，它比我们接下来将要研究的基于方差的计算方法更节省计算资源。它可以提供比回归、导数或基于因子的简单且成本较低的方法更多的见解。它不能精确量化效应，但可以识别那些具有可忽略或交互效应的效应，这使得它成为在因素数量较少时筛选因素的理想方法。筛选也被称为**因素优先级**，因为它可以根据它们的分类来优先考虑因素。
- en: Computing Morris sensitivity indices
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算莫里斯敏感性指数
- en: The Morris method derives a distribution of elementary effects that it associates
    with an individual factor. Each EE distribution has a mean (*µ*) and a standard
    deviation (*σ*). These two statistics are what help map the factors into different
    classifications. The mean could be negative when the model is non-monotonic, so
    a Morris method variation adjusts for this with absolute values (*µ*^*) so that
    it is more manageable to interpret. We will use this variation here.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 莫里斯方法推导出与单个因素相关联的基本效应分布。每个基本效应分布都有一个平均值（*µ*）和标准差（*σ*）。这两个统计数据有助于将因素映射到不同的分类中。当模型非单调时，平均值可能是负数，因此莫里斯方法的一个变体通过绝对值（*µ*^*）进行调整，以便更容易解释。我们在这里将使用这种变体。
- en: 'Now, let’s limit the scope of this problem to make it more manageable. The
    traffic uncertainties the construction crew will face will be ongoing from May
    to October, Monday to Friday, from 11 p.m. to 5 a.m. Therefore, we can take the
    `working_season_df` DataFrame and subset it further to produce a working hours
    one (`working_hrs_df`) that we can `describe`. We will include the 1%, 50%, and
    99% percentiles to understand where the median and outliers lie:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将此问题的范围限制在更易于管理的范围内。施工队将面临的道路交通不确定性将持续从5月到10月，周一至周五，晚上11点到凌晨5点。因此，我们可以从`working_season_df`
    DataFrame中进一步提取子集，以生成一个工作小时DataFrame（`working_hrs_df`），我们可以对其进行`describe`。我们将包括1%、50%和99%的百分位数，以了解中位数和异常值所在的位置：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code produced the table in *Figure 9.12*. We can use this table
    to extract the ranges we will use for our features in the simulation. Typically,
    we would use plausible values that have exceeded the existing maximums or minimums.
    For most models, any feature value can be increased or decreased beyond its known
    limits, and since the model learned a monotonic relationship, it can infer a realistic
    outcome. For instance, it might learn that rain beyond a certain point will increasingly
    diminish traffic. Then, say you want to simulate a severe flood with, say, 30
    mm of rain per hour; it can accurately predict no traffic:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图9.12*中的表格。我们可以使用这张表格来提取我们在模拟中使用的特征范围。通常，我们会使用超过现有最大值或最小值的合理值。对于大多数模型，任何特征值都可以在其已知限制之外增加或减少，并且由于模型学习到了单调关系，它可以推断出合理的结局。例如，它可能学习到超过某个点的降雨量将逐渐减少交通。那么，假设你想模拟每小时30毫米的严重洪水；它可以准确预测无交通：
- en: '![](img/B18406_09_12.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_09_12.png)'
- en: 'Figure 9.12: Summary statistics for the period that the construction crew plans
    to work through'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：施工队计划工作期间的汇总统计
- en: However, because we are using a prediction approximation method that samples
    from historical values, we are limited to how far we can push the boundaries outside
    of the known. For this reason, we will use the 1% and 99% percentile values as
    our limits. We should note that this is an important caveat for any findings,
    especially for features that could plausibly extend beyond these limits, such
    as `temp`, `rain_1h`, and `snow_1h`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，因为我们使用的是从历史值中采样的预测近似方法，所以我们受到如何将边界推到已知范围之外的限制。因此，我们将使用1%和99%的百分位数值作为我们的限制。我们应该注意，这对于任何发现来说都是一个重要的注意事项，特别是对于可能超出这些限制的特征，例如`temp`、`rain_1h`和`snow_1h`。
- en: 'Another thing to note from the summary of *Figure 9.12* is that many weather-related
    binary features are very sparse. You can tell by their extremely low mean. Each
    factor that’s added to the sensitivity analysis simulation slows it down, so we
    will only take the top three; that is, `weather_Clear`, `weather_Clouds`, and
    `weather_Rain`. These factors are specified along with the other six factors in
    a “problem” dictionary (`morris_problem`), which has their corresponding `names`,
    `bounds`, and `groups`. Now, `bounds` is critical because it denotes what ranges
    of values will be simulated for each factor. We will use [0,4] (Monday to Friday)
    for `dow` and [-1,4] (11 p.m. to 4 a.m.) for `hr`. The filter function automatically
    translates negative hours into hours from the day before so that -1 on a Tuesday
    is equivalent to 23 on a Monday. The rest of the bounds were informed by the percentiles.
    Note that `groups` all have factors in the same group, except for the three weather
    ones:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图9.12*的总结中，我们还需要注意的一点是，许多与天气相关的二元特征非常稀疏。你可以通过它们的极低平均值来判断。每个添加到敏感性分析模拟中的因素都会减慢其速度，因此我们只会选择前三个；即`weather_Clear`、`weather_Clouds`和`weather_Rain`。这些因素与其他六个因素一起在“问题”字典（`morris_problem`）中指定，其中包含它们的对应`names`、`bounds`和`groups`。现在，`bounds`是关键，因为它表示每个因素将模拟哪些值范围。我们将使用[0,4]（周一至周五）作为`dow`的值，以及[-1,4]（晚上11点到凌晨4点）作为`hr`的值。过滤器函数自动将负小时转换为前一天的小时，因此周二的一1相当于周一的23。其余的界限是由百分位数确定的。请注意，`groups`中的所有因素都属于同一组，除了三个天气因素：
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once the dictionary has been defined, we can generate Morris method samples
    with `SALib's` `sample` method. In addition to the dictionary, it takes a number
    of trajectories (`256`) and levels (`num_levels=4`). The method uses a grid with
    factors and levels to construct the trajectories for which inputs are randomly
    moved **one at a time** (**OAT**). What is important to heed here is that more
    levels add more resolution to this grid, potentially making for a better analysis.
    However, this can be very time-consuming. It’s better to start with a ratio between
    the number of trajectories and levels of 25:1 or higher.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了字典，我们就可以使用 `SALib` 的 `sample` 方法生成Morris方法样本。除了字典外，它还需要轨迹数量（`256`）和级别（`num_levels=4`）。该方法使用因素和级别的网格来构建输入随机逐个移动的轨迹（**OAT**）。这里需要注意的重要一点是，更多的级别会增加这个网格的分辨率，可能使分析更好。然而，这可能会非常耗时。最好从轨迹数量和级别之间的比例25:1或更高开始。
- en: 'Then, you can decrease this ratio progressively. In other words, if you have
    enough compute, you can make `num_levels` match the number of trajectories, but
    if you have this much compute available, you could try `optimal_trajectories=True`.
    However, given that we have groups, `local_optimization` would have to be `False`.
    The output of `sample` is an array that is one column for each factor and (*G*
    + 1) × *T* rows (where *G* is the number of groups and *T* is the number of trajectories).
    We have eight groups and 256 trajectories, so `print` should output a shape of
    2,304 rows and 10 columns:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以逐步降低这个比例。换句话说，如果你有足够的计算能力，你可以让 `num_levels` 与轨迹数量相匹配，但如果你有这么多可用的计算能力，你可以尝试
    `optimal_trajectories=True`。然而，鉴于我们有组，`local_optimization` 必须设置为 `False`。`sample`
    的输出是一个数组，每个因素一列，(*G* + 1) × *T* 行（其中 *G* 是组数，*T* 是轨迹数）。我们有八个组，256个轨迹，所以 `print`
    应该输出一个2,304行10列的形状：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Given that the `predict` function will only work with 15 factors, we should
    modify the samples to fill the remaining five factors with zeroes. We use zeroes
    because that is the median value for these features. Medians are least likely
    to increase traffic, but you ought to tailor your default values on a case-by-case
    basis. If you recall our **Cardiovascular Disease** (**CVD**) example from *Chapter
    2*, *Key Concepts of Interpretability*, the feature value that would increase
    CVD risk was sometimes the minimum or maximum.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `predict` 函数只与15个因素一起工作，我们应该修改样本，用零填充剩余的五个因素。我们使用零，因为这这些特征的中位数。中位数最不可能增加交通量，但你应该根据具体情况调整默认值。如果你还记得我们
    **第2章** 的 **心血管疾病** （**CVD**）示例，*可解释性关键概念*，增加CVD风险的特性值有时是最小值或最大值。
- en: 'The `np.hstack` function can concatenate the array horizontally so that three
    zero factors follow the samples for the first eight factors. Then, there’s a lonely
    ninth sample factor corresponding to `weather_Rain`, followed by two zero factors.
    The resulting array should have the same number of rows as before but 15 columns:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.hstack` 函数可以将数组水平拼接，使得前八个因素之后跟着三个零因素。然后，有一个孤独的第九个样本因素对应于 `weather_Rain`，接着是两个零因素。结果数组应该与之前一样行数，但列数为15：'
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `numpy` array known as `morris_sample_mod` now has the Morris samples in
    a shape that can be understood by our `predict` function. If this was a model
    that had been trained on a tabular dataset, we could just leverage the model’s
    `predict` function. However, just as we did with SHAP, we have to use the approximation
    method. This time, we won’t use `predict_fn` because we want to set one additional
    option, `progress_bar=True`, in `approx_predict_ts`. Everything else will remain
    the same. The progress bar will come in handy because this should take a while.
    Run the cell and take a coffee break:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为 `morris_sample_mod` 的 `numpy` 数组现在以我们的 `predict` 函数可以理解的形式包含了Morris样本。如果这是一个在表格数据集上训练过的模型，我们就可以直接利用模型的
    `predict` 函数。然而，就像我们使用SHAP一样，我们必须使用近似方法。这次，我们不会使用 `predict_fn`，因为我们想在 `approx_predict_ts`
    中设置一个额外的选项，`progress_bar=True`。其他一切都将保持不变。进度条将很有用，因为这可能需要一段时间。运行单元格，休息一下喝杯咖啡：
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To produce a sensitivity analysis with `SALib''s` `analyze` function, all you
    need is your problem dictionary (`morris_problem`), the original Morris samples
    (`morris_sample`), and the predictions we just produced with those samples (`morris_preds`).
    There’s an optional confidence interval level argument (`conf_level`), but the
    default of 0.95 is good. It uses resamples to compute this confidence level, which
    is 1,000 by default. This setting can also be changed with an optional `num_resamples`
    argument:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`SALib`的`analyze`函数进行敏感性分析，你需要你的问题字典（`morris_problem`），原始的Morris样本（`morris_sample`），以及我们用这些样本生成的预测（`morris_preds`）。还有一个可选的置信区间水平参数（`conf_level`），但默认的0.95是好的。它使用重采样来计算这个置信水平，默认为1,000。这个设置也可以通过可选的`num_resamples`参数来改变：
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Analyzing the elementary effects
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析基本影响
- en: '`analyze` will return a dictionary with the Morris sensitivity indices, including
    the mean (*µ*) and standard deviation (*σ*) elementary effect, as well as the
    absolute value of the mean (*µ*^*). It’s easier to appreciate these values in
    a tabular format so that we can place them into a DataFrame and sort and color-code
    them according to *µ*^*, which can be interpreted as the overall importance of
    the factor. *σ*, on the other hand, is how much the factor interacts with other
    ones:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`analyze`将返回一个包含Morris敏感性指数的字典，包括平均值（*µ*）和标准差（*σ*）的基本影响，以及平均值（*µ*^*）的绝对值。在表格格式中更容易欣赏这些值，这样我们就可以将它们放入DataFrame中，并根据*µ*^*排序和着色，*µ*^*可以解释为因素的整体重要性。另一方面，*σ*表示因素与其它因素的交互程度：'
- en: '[PRE36]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code outputs the DataFrame depicted in *Figure 9.13*. You can
    tell that `is_holiday` is one of the most important factors, at least during the
    bounds specified in the problem definition (`morris_problem`). Another thing to
    note is that weather does have an absolute mean elementary effect but inconclusive
    interaction effects. Groups are challenging to assess, especially when they are
    sparse binary factors:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出了*图9.13*中展示的DataFrame。你可以看出`is_holiday`是其中最重要的因素之一，至少在问题定义中指定的范围（`morris_problem`）内是这样。还有一点需要注意，天气确实有绝对的基本影响，但交互效应并不确定。组别很难评估，尤其是当它们是稀疏的二进制因素时：
- en: '![Timeline  Description automatically generated](img/B18406_09_13.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![时间线  自动生成的描述](img/B18406_09_13.png)'
- en: 'Figure 9.13: The elementary effects decomposition of the factors'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13：因素的基本影响分解
- en: 'The DataFrame in the preceding figure is not the best way to visualize the
    elementary effects. When there are not too many factors, it’s easier to plot them.
    `SALib` comes with two plotting methods. The horizontal bar plot (`horizontal_bar_plot`)
    and covariance plot (`covariance_plot`) can be placed side by side. The covariance
    plot is excellent, but it doesn’t annotate the areas it delineates. We will learn
    about these next. So, solely for instructional purposes, we will use `text` to
    place the annotations:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图中的DataFrame不是可视化基本影响的最佳方式。当因素不多时，更容易绘制它们。`SALib`提供了两种绘图方法。水平条形图（`horizontal_bar_plot`）和协方差图（`covariance_plot`）可以并排放置。协方差图非常好，但它没有注释它所界定的区域。我们将在下一节中了解这些。因此，仅出于教学目的，我们将使用`text`来放置注释：
- en: '[PRE37]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The preceding code produces the plots shown in *Figure 9.14*. The bar plot
    on the left ranks the factors by *µ*^*, while the lines sticking out of each bar
    signify their corresponding confidence bands. The covariance plot to the right
    is a scatter plot with *µ*^* on the *x*-axis and *σ* on the *y*-axis. Therefore,
    the farther right the point is, the more important it is, while the further up
    it is in the plot, the more it interacts with other factors and becomes increasingly
    less monotonic. Naturally, this means that factors that don’t interact much and
    are mostly monotonic ones comply with linear regression assumptions, such as linearity
    and multicollinearity. However, the spectrum between linear and non-linear or
    non-monotonic is determined diagonally by the ratio of *σ* and *µ*^*:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了*图9.14*中显示的图表。左边的条形图按*µ*^*对因素进行排序，而每根从条形中伸出的线表示相应的置信区间。右边的协方差图是一个散点图，*µ*^*位于*x*轴上，*σ*位于*y*轴上。因此，点越往右，它就越重要，而它在图中越往上，它与其它因素的交互就越多，就越不单调。自然地，这意味着那些交互不多且主要单调的因素符合线性回归的假设，如线性性和多重共线性。然而，线性与非线性或非单调之间的范围由*σ*和*µ*^*的比率对角确定：
- en: '![Chart, scatter chart  Description automatically generated](img/B18406_09_14.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B18406_09_14.png)'
- en: 'Figure 9.14: A bar and covariance plot depicting the elementary effects'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14：表示基本效应的条形图和协方差图
- en: You can tell by the preceding covariance plot that all the factors are non-linear
    or non-monotonic. `hr` is by far the most important, with the following two (`dow`
    and `temp`) clustered relatively nearby, followed by `weather` and `is_holiday`.
    The `weather` group is not on the plot because interactivity was inconclusive,
    yet `cloud_coverage`, `rain_1h`, and `snow_1h` are considerably more interactive
    than important on their own.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过前面的协方差图看出，所有因素都是非线性或非单调的。`hr`无疑是其中最重要的，其次是接下来的两个因素（`dow`和`temp`）相对靠近，然后是`weather`和`is_holiday`。`weather`组没有在图中显示，因为交互性结果不确定，但`cloud_coverage`、`rain_1h`和`snow_1h`的交互性比它们单独重要得多。
- en: Elementary effects help us understand how to classify our factors in accordance
    with their effects on model outcomes. However, it’s not a robust method to properly
    quantify their effects or those derived from factor interactions. For that, we
    would have to turn to a variance-based global method that uses a probabilistic
    framework to decompose the output’s variance and trace it back to the inputs.
    Those methods include **Fourier Amplitude Sensitivity Test** (**FAST**) and **Sobol**.
    We will study the latter approach next.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 基本效应帮助我们理解如何根据它们对模型结果的影响来分类我们的因素。然而，这并不是一个稳健的方法来正确量化它们的影响或由因素相互作用产生的影响。为此，我们必须转向使用概率框架分解输出方差并将其追溯到输入的基于方差的全局方法。这些方法包括**傅里叶振幅敏感性测试**（**FAST**）和**Sobol**。我们将在下一节研究后一种方法。
- en: Quantifying uncertainty and cost sensitivity with factor fixing
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用因素固定量化不确定性和成本敏感性
- en: With the Morris indices, it became evident that all the factors are non-linear
    or non-monotonic. There’s a high degree of interactivity between them – as expected!
    It should be no surprise that climate factors (`temp`, `rain_1h`, `snow_1h`, and
    `cloud_coverage`) are likely multicollinear with `hr`. There are also patterns
    to be found between `hr`, `is_holiday`, and `dow` and the target. Many of these
    factors most definitely don’t have a monotonic relationship with the target. We
    know this already. For instance, traffic doesn’t consistently increase as hours
    increase throughout the day. That’s not the case for days of the week either!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Morris指数，很明显，所有因素都是非线性或非单调的。它们之间有很高的交互性——正如预期的那样！气候因素（`temp`、`rain_1h`、`snow_1h`和`cloud_coverage`）很可能与`hr`存在多重共线性。在`hr`、`is_holiday`、`dow`和目标之间也存在一些模式。许多这些因素肯定与目标没有单调关系。我们已经知道了这一点。例如，交通在一天中的小时数增加时并不总是增加。情况对一周中的某一天也是如此！
- en: However, we didn’t know to what degree `is_holiday` and `temp` impacted the
    model, particularly during the crew’s working hours, which was an important insight.
    That being said, factor prioritization with Morris indices is usually to be taken
    as a starting point or “first setting” because once you ascertain that there are
    interaction effects, it’s best if you disentangle them. To this end, there’s a
    “second setting” called **factor fixing**. We can quantify the variance and, by
    doing so, the uncertainty brought on by all the factors.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不知道`is_holiday`和`temp`对模型的影响程度，尤其是在机组人员的工作时间内，这是一个重要的见解。话虽如此，使用Morris指数进行因素优先级排序通常被视为起点或“第一设置”，因为一旦确定存在交互效应，最好是解开它们。为此，有一个“第二设置”，称为**因素固定**。我们可以量化方差，通过这样做，量化所有因素带来的不确定性。
- en: Only **variance-based methods** can quantify these effects in a statistically
    rigorous fashion. **Sobol sensitivity analysis** is one of these methods, which
    means that it decomposes the model’s output variance into percentages and attributes
    it to the model’s inputs and interactions. Like Morris, it has a sampling step,
    as well as a sensitivity index estimation step.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 只有**基于方差的方法**才能以统计严谨的方式量化这些效应。**Sobol敏感性分析**是这些方法之一，这意味着它将模型的输出方差分解成百分比，并将其归因于模型的输入和交互。像Morris一样，它有一个采样步骤，以及一个敏感性指数估计步骤。
- en: Unlike Morris, the sampling doesn’t follow a series of levels but the input
    data’s distribution. It uses a **quasi-Monte Carlo method**, where it samples
    points in hyperspace that follow the inputs’ probability distributions. **Monte
    Carlo** methods are a family of algorithms that perform random sampling, often
    for optimization or simulation. They seek shortcuts on problems that would be
    impossible to solve with brute force or entirely deterministic approaches. Monte
    Carlo methods are common in sensitivity analysis precisely for this reason. Quasi-Monte
    Carlo methods have the same goal. However, they converge faster because they use
    a deterministic low-discrepancy sequence instead of using a pseudorandom one.
    The Sobol method uses the **Sobol sequence**, devised by the same mathematician.
    We will use another sampling scheme derived from Sobol’s, called Saltelli’s.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与Morris不同，采样不遵循一系列级别，而是遵循输入数据的分布。它使用**准蒙特卡洛方法**，在超空间中采样点，这些点遵循输入的概率分布。**蒙特卡洛方法**是一系列执行随机采样的算法，通常用于优化或模拟。它们寻求在用蛮力或完全确定性的方法无法解决的问题上的捷径。蒙特卡洛方法在敏感性分析中很常见，正是出于这个原因。准蒙特卡洛方法有相同的目标。然而，它们收敛得更快，因为它们使用确定性低偏差序列而不是使用伪随机序列。Sobol方法使用**Sobol序列**，由同一位数学家设计。我们将使用另一种从Sobol派生出的采样方案，称为Saltelli的。
- en: Once the samples have been produced, Monte Carlo estimators compute the variance-based
    sensitivity indices. These indices are capable of quantifying non-linear non-additive
    effects and second-order indices, which relate to the interaction between two
    factors. Morris can reveal interactivity in your model, but not precisely how
    it is manifested. Sobol can tell you what factors are interacting and to what
    degree.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成样本，蒙特卡洛估计器就会计算基于方差的敏感性指数。这些指数能够量化非线性非加性效应和第二阶指数，这些指数与两个因素之间的相互作用相关。Morris可以揭示模型中的交互性，但不能精确地说明它是如何表现的。Sobol可以告诉你哪些因素在相互作用以及相互作用的程度。
- en: Generating and predicting on Saltelli samples
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成和预测Saltelli样本
- en: 'To begin a Sobol sensitivity analysis with `SALib`, we must first define a
    problem. We’ll do the same as we did with Morris. This time, we will reduce the
    factors because we realized that the `weather` grouping led to inconclusive results.
    We should include the least sparse of all the weather factors; that is, `weather_Clear`.
    And since Sobol uses a probabilistic framework, there’s no harm in expanding the
    bounds to their minimum and maximum values for `temp`, `rain_1h`, and `cloud_coverage`,
    as seen in *Figure 9.12*:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`SALib`开始Sobol敏感性分析，我们必须首先定义一个问题。我们将与Morris做同样的事情。这次，我们将减少因素，因为我们意识到`weather`分组导致了不确定的结果。我们应该包括所有天气因素中最稀疏的；即`weather_Clear`。由于Sobol使用概率框架，将`temp`、`rain_1h`和`cloud_coverage`的范围扩展到它们的最大和最小值是没有害处的，如图*9.12*所示：
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Generating the samples should look familiar too. The Saltelli `sample` function
    requires the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 生成样本看起来也应该很熟悉。Saltelli的`sample`函数需要以下内容：
- en: A problem statement (`sobol_problem`)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题陈述（`sobol_problem`）
- en: A number of samples to produce per factor (`300`)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个因素要生成的样本数量（`300`）
- en: Second-order indices to compute (`calc_second_order=True`)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二阶索引以进行计算（`calc_second_order=True`）
- en: 'Given that we want the interactions, the output of `sample` is an array that
    has one column for each factor and ![](img/B18406_09_003.png) rows (where *N*
    is the number of samples and *F* is the number of factors). We have eight factors
    and 256 samples per factor, so `print` should output a shape of 4,608 rows and
    8 columns. First, we will modify it, as we did previously, with `hstack` to add
    the 7 empty factors needed to make the predictions, resulting in 15 columns instead:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要交互作用，`sample`的输出是一个数组，其中每一列代表一个因素，有![](img/B18406_09_003.png)行（其中*N*是样本数量，*F*是因素数量）。我们有八个因素，每个因素有256个样本，所以`print`应该输出4,608行和8列的形状。首先，我们将像之前一样使用`hstack`修改它，添加7个空因素以进行预测，从而得到15列：
- en: '[PRE39]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, let’s predict on these samples. This should take a while, so it’s coffee
    time once more:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对这些样本进行预测。这可能需要一些时间，所以又是咖啡时间：
- en: '[PRE40]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Performing Sobol sensitivity analysis
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行Sobol敏感性分析
- en: For Sobol sensitivity analysis (`analyze`), all you need is a problem statement
    (`sobol_problem`) and the model outputs (`saltelli_preds`). But the predictions
    don’t tell the story of uncertainty. Sure, there’s variance in the predicted traffic,
    but that traffic is only a problem once it exceeds 1,500\. Uncertainty is something
    you want to relate to risk or reward, costs or revenue, loss or profit – something
    tangible you can connect to your problem.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Sobol敏感性分析（`analyze`），你所需要的只是一个问题陈述（`sobol_problem`）和模型输出（`saltelli_preds`）。但是预测并不能讲述不确定性的故事。当然，预测的交通流量有方差，但只有当交通量超过1,500时，这个问题才会出现。不确定性是你想要与风险或回报、成本或收入、损失或利润相关联的东西——一些你可以与你问题相关联的实质性东西。
- en: 'First, we must assess if there’s any risk at all. To get an idea of whether
    the predicted traffic in the samples exceeded the no-construction threshold during
    working hours, we can use `print(max(saltelli_preds[:,0]))`. The maximum traffic
    level should be somewhere in the neighborhood of 1,800-1,900, which means that
    there’s at least some risk that the construction company will pay a fine. Instead
    of using the predictions (`saltelli_preds`) as the model’s output, we can create
    a simple binary array with ones when it exceeded 1,500 and zero otherwise. We
    will call this `costs`, and then run the `analyze` function with it. Note that
    `calc_second_order=True` is also set here. It will throw an error if `sample`
    and `analyze` don’t have a consistent setting. Like with Morris, there’s an optional
    confidence interval level argument (`conf_level`), but the default of 0.95 is
    good:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须评估是否存在任何风险。为了了解样本中的预测交通量是否在工作时间内超过了无建设阈值，我们可以使用`print(max(saltelli_preds[:,0]))`。最大交通水平应该在1,800-1,900左右，这意味着至少存在一些风险，即建筑公司将会支付罚款。我们不必使用预测（`saltelli_preds`）作为模型的输出，我们可以创建一个简单的二进制数组，当它超过1,500时为1，否则为0。我们将称之为`costs`，然后使用它运行`analyze`函数。注意，这里也设置了`calc_second_order=True`。如果`sample`和`analyze`没有一致的设置，它将抛出一个错误。与Morris一样，有一个可选的置信区间水平参数（`conf_level`），但默认的0.95是好的：
- en: '[PRE41]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`analyze` will return a dictionary with the Sobol sensitivity indices, including
    the first-order (`S1`), second-order (`S2`), and total-order (`ST`) indices, as
    well as the total confidence bounds (`ST_conf`). The indices correspond to percentages,
    but the totals won’t necessarily add up unless the model is additive. It’s easier
    to appreciate these values in a tabular format so that we can place them into
    a DataFrame and sort and color-code them according to the total, which can be
    interpreted as the overall importance of the factor. However, we will leave the
    second-order indices out because they are two-dimensional and akin to a correlation
    plot:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`analyze`将返回一个包含Sobol敏感性指数的字典，包括一阶（`S1`）、二阶（`S2`）和总阶（`ST`）指数，以及总置信区间（`ST_conf`）。这些指数对应于百分比，但除非模型是加性的，否则总数不一定相加。在表格格式中更容易欣赏这些值，这样我们可以将它们放入DataFrame中，并根据总数进行排序和着色，总数可以解释为因素的整体重要性。然而，我们将省略二阶指数，因为它们是二维的，类似于相关图：'
- en: '[PRE42]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding code outputs the DataFrame depicted in *Figure 9.15*. You can
    tell that `temp` and `is_holiday` are in the top four, at least during the bounds
    specified in the problem definition (`sobol_problem`). Another thing to note is
    that `weather_Clear` does have more of an effect on its own, but `rain_1h` and
    `cloud_coverage` seem to have no effect on the potential cost because they have
    zero total first-order indices:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码输出了*图9.15*中展示的DataFrame。你可以看出`temp`和`is_holiday`至少在问题定义中指定的边界内排在前面四位。另一个需要注意的事情是`weather_Clear`确实对其自身有更大的影响，但`rain_1h`和`cloud_coverage`似乎对潜在成本没有影响，因为它们的总一阶指数为零：
- en: '![Timeline  Description automatically generated](img/B18406_09_15.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![时间线描述自动生成](img/B18406_09_15.png)'
- en: 'Figure 9.15: Sobol global sensitivity indices for the eight factors'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：八个因素的Sobol全局敏感性指数
- en: 'Something interesting about the first-order values is how low they are, suggesting
    that interactions account for most of the model output variance. We can easily
    produce a heatmap with second-order indices to corroborate this. It’s the combination
    of these indices and the first-order ones that add up to the totals:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 关于一阶值的一些有趣之处在于它们有多低，这表明交互作用占模型输出方差的大部分。我们可以很容易地使用二阶索引来证实这一点。这些索引和一阶索引的组合加起来就是总数：
- en: '[PRE43]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The preceding code outputs the heatmap in *Figure 9.16*:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码输出了*图9.16*中的热图：
- en: '![Chart, waterfall chart  Description automatically generated](img/B18406_09_16.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图 描述自动生成](img/B18406_09_16.png)'
- en: 'Figure 9.16: Sobol second-order indices for the eight factors'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16：八个因素的Sobol二阶指数
- en: Here, you can tell that `is_holiday` and `weather_Clear` are the two factors
    that contribute the most to the output variance with the highest absolute value
    of 0.26\. `dow` and `hr` have sizable interactions with all the factors.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以知道`is_holiday`和`weather_Clear`是两个对输出方差贡献最大的因素，其绝对值最高为0.26。`dow`和`hr`与所有因素都有相当大的相互作用。
- en: Incorporating a realistic cost function
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入一个现实成本函数
- en: Now, we can create a cost function that takes our inputs (`saltelli_sample`)
    and outputs (`saltelli_preds`) and computes how much the twin cities would fine
    the construction company, plus any additional costs the additional traffic could
    produce.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个成本函数，它接受我们的输入（`saltelli_sample`）和输出（`saltelli_preds`），并计算双城将对建筑公司罚款多少，以及额外的交通可能产生的任何额外成本。
- en: 'It is better to do this if both the input and outputs are in the same array
    because we will need details from both to calculate the costs. We can use `hstack`
    to join the samples and their corresponding predictions, producing an array with
    eight columns (`saltelli_sample_preds`). We can then define a cost function that
    can compute the costs (`cost_fn`), given an array with these nine columns:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入和输出都在同一个数组中，这样做会更好，因为我们需要从两者中获取详细信息来计算成本。我们可以使用`hstack`将样本及其对应的预测结果连接起来，生成一个包含八个列的数组（`saltelli_sample_preds`）。然后我们可以定义一个成本函数，它可以计算包含这些九个列的数组的成本（`cost_fn`）：
- en: '[PRE44]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We know that the half-capacity threshold wasn’t exceeded for any sample predictions,
    so we won’t even bother to include the daily penalty in the function. Besides
    that, the fines are $15 per vehicle that exceeds the hourly no-construction threshold.
    In addition to these fines, to be able to leave on time, the construction company
    estimates additional costs: $1,500 in extra wages if the threshold is exceeded
    at 4 a.m. and $4,500 more on Fridays to speed up the moving of their equipment
    because it can’t stay on the highway shoulder during weekends. Once we have the
    cost function, we can iterate through the combined array (`saltelli_sample_preds`),
    calculating costs for each sample. List comprehension can do this efficiently:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，对于任何样本预测，半容量阈值都没有超过，所以我们甚至不需要在函数中包含每日罚款。除此之外，罚款是每辆超过每小时无施工阈值的车辆15美元。除了这些罚款之外，为了能够按时离开，建筑公司估计额外的成本：如果凌晨4点超过阈值，额外工资为1,500美元，周五额外4,500美元以加快设备移动速度，因为周末不能停在高速公路的路肩上。一旦我们有了成本函数，我们就可以遍历组合数组（`saltelli_sample_preds`），为每个样本计算成本。列表推导可以有效地完成这项工作：
- en: '[PRE45]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `print` statement should output a cost somewhere between $170,000 and $200,000\.
    But not to worry! The construction crew only plans to work about 195 days on-site
    per year and 5 hours each day, for a total of 975 hours. However, there are 4,608
    samples, which means that there are almost 5 years’ worth of predicted costs due
    to excess traffic. In any case, the point of calculating these costs is to figure
    out how they relate to the model’s inputs. More years’ worth of samples means
    tighter confidence intervals:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`print`语句应该输出一个介于17万美元和20万美元之间的成本。但不必担心！建筑队每年只计划在现场工作大约195天，每天5小时，总共975小时。然而，有4,608个样本，这意味着由于交通过多，几乎有5年的预测成本。无论如何，计算这些成本的目的在于了解它们与模型输入的关系。更多的样本年意味着更紧密的置信区间：'
- en: '[PRE46]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We can now perform the analysis again but with `costs2`, and we can save the
    analysis into a `factor_fixing2_sa` dictionary. Lastly, we can produce a new sorted
    and color-coded DataFrame with this dictionary’s values, as we did previously
    for *Figure 9.15*, which generates the output shown in *Figure 9.17*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以再次进行分析，但使用`costs2`，并将分析保存到`factor_fixing2_sa`字典中。最后，我们可以使用这个字典的值生成一个新的排序和彩色编码的DataFrame，就像我们之前为*图9.15*所做的那样，这将生成*图9.17*中的输出。
- en: 'As you can tell by *Figure 9.17* once the actual costs have been factored in,
    `dow`, `hr`, and `is_holiday` become riskier factors, while `snow_1h` and `temp`
    become less relevant when compared to *Figure 9.15*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从*图9.17*中可以看出，一旦实际成本被考虑在内，`dow`、`hr`和`is_holiday`成为更具风险的因素，而与*图9.15*相比，`snow_1h`和`temp`变得不那么相关：
- en: '![Timeline  Description automatically generated](img/B18406_09_17.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![时间线 描述自动生成](img/B18406_09_17.png)'
- en: 'Figure 9.17: Sobol global sensitivity indices for the eight factors using the
    realistic cost function'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17：使用现实成本函数计算八个因素的Sobol全局敏感性指数
- en: 'One thing that is hard to appreciate with a table is the confidence intervals
    of the sensitivity indices. For that, we can use a bar plot, but first, we must
    convert the entire dictionary into a DataFrame so that `SALib''s` plotting function
    can plot it:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 用表格难以欣赏的是敏感性指数的置信区间。为此，我们可以使用条形图，但首先，我们必须将整个字典转换成一个DataFrame，以便`SALib`的绘图函数可以绘制它：
- en: '[PRE47]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code generates the bar plot in *Figure 9.18*. The 95% confidence
    interval for `dow` is much larger than for other important factors, which shouldn’t
    be surprising considering how much variance there is between days of the week.
    Another interesting insight is how `weather_Clear` has negative first-order effects,
    so the positive total-order indices are entirely attributed to second-order ones,
    which expand the confidence interval:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了*图9.18*中的条形图。`dow`的95%置信区间比其他重要因素大得多，考虑到一周中各天之间的差异很大，这并不令人惊讶。另一个有趣的见解是`weather_Clear`具有负一阶效应，因此正的总阶指数完全归因于二阶指数，这扩大了置信区间：
- en: '![Chart, scatter chart  Description automatically generated](img/B18406_09_18.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B18406_09_18.png)'
- en: 'Figure 9.18: Bar plot with the Sobol sensitivity total-order indices and their
    confidence intervals using a realistic cost function'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18：使用现实成本函数绘制的条形图，包含Sobol敏感性总阶指数及其置信区间
- en: 'To understand how, let’s plot the heatmap shown in *Figure 9.16* again but
    this time using `factor_fixing2_sa` instead of `factor_fixing_sa`. The heatmap
    in *Figure 9.19* should depict how the realistic costs reflect the interactions
    in the model:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何，让我们再次绘制*图9.16*所示的散点图，但这次使用`factor_fixing2_sa`而不是`factor_fixing_sa`。*图9.19*中的散点图应该描绘出模型中成本的现实反映：
- en: '![Chart, waterfall chart  Description automatically generated](img/B18406_09_19.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图  自动生成的描述](img/B18406_09_19.png)'
- en: 'Figure 9.19: Sobol second-order indices for seven factors while factoring a
    more realistic cost function'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19：在考虑更现实的成本函数时，七个因素的Sobol二阶指数
- en: The preceding heatmap shows similar salient interactions to those in *Figure
    9.16* but they’re much more nuanced since there are more shades. It becomes evident
    that `weather_Clear` has a magnifying effect when combined with `is_holiday`,
    and a tempering effect for `dow` and `hr`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的散点图显示了与*图9.16*中相似的显著交互，但由于有更多的阴影，它们更加细腻。很明显，`weather_Clear`与`is_holiday`结合时具有放大作用，而对`dow`和`hr`则有调和作用。
- en: Mission accomplished
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: The mission was to train a traffic prediction model and understand what factors
    create uncertainty and possibly increase costs for the construction company. We
    can conclude a significant portion of the potential $35,000/year in fines can
    be attributed to the `is_holiday` factor. Therefore, the construction company
    should rethink working holidays. There are only seven or eight holidays between
    March and November, and they could cost more because of the fines than working
    on a few Sundays instead. With this caveat, the mission was successful, but there’s
    still a lot of room for improvement.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是训练一个交通预测模型，并了解哪些因素会创造不确定性，并可能增加建筑公司的成本。我们可以得出结论，潜在的35,000美元/年的罚款中有很大一部分可以归因于`is_holiday`因素。因此，建筑公司应该重新考虑工作假日。三月至十一月之间只有七个或八个假日，由于罚款，它们可能比在几个星期日工作成本更高。考虑到这个警告，任务已经成功，但仍有很多改进的空间。
- en: Of course, these conclusions are for the `LSTM_traffic_168_compact1` model –
    which we can compare with other models. Try replacing the `model_name` at the
    beginning of the notebook with `LSTM_traffic_168_compact2`, an equally small but
    significantly more robust model, or `LSTM_traffic_168_optimal`, a larger slightly
    better-performing model, and re-running the notebook. Or glance at the notebooks
    named `Traffic_compact2` and `Traffic_optimal`, which already have been re-run
    with these corresponding models. You will find that it is possible to train and
    select models that manage uncertain inputs much better. That being said, improvement
    doesn’t always come by simply selecting a better model.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些结论是针对`LSTM_traffic_168_compact1`模型——我们可以将其与其他模型进行比较。尝试将笔记本开头的`model_name`替换为`LSTM_traffic_168_compact2`，这是一个同样小巧但显著更稳健的模型，或者`LSTM_traffic_168_optimal`，这是一个更大但表现略好的模型，并重新运行笔记本。或者浏览名为`Traffic_compact2`和`Traffic_optimal`的笔记本，这些笔记本已经使用相应的模型重新运行。你会发现，可以训练和选择能够更好地管理不确定输入的模型。话虽如此，改进并不总是通过简单地选择更好的模型就能实现。
- en: For instance, one thing that could be covered in further depth is the true impact
    of `temp`, `rain_1h`, and `snow_1h`. Our prediction approximation method precluded
    Sobol from testing the effect of extreme weather events. If we modified the model
    to train on aggregated weather features at single timesteps and built in some
    guardrails, we could simulate weather extremes with Sobol. And the “third setting”
    of sensitivity analysis, known as factor mapping, could help pinpoint how exactly
    some factor values affect the predicted outcome, leading to a sturdier cost-benefit
    analysis, but we won’t cover that in this chapter.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以进一步深入探讨的是`temp`、`rain_1h`和`snow_1h`的真正影响。我们的预测近似方法排除了Sobol测试极端天气事件的影响。如果我们修改模型以在单个时间步长上训练聚合的天气特征，并内置一些安全措施，我们就可以使用Sobol模拟天气极端情况。而且，敏感性分析的“第三设置”，即因素映射，可以帮助精确指出某些因素值如何影响预测结果，从而进行更稳健的成本效益分析，但这一点我们不会在本章中涉及。
- en: 'Throughout *Part Two* of this book, we explored an ecosystem of interpretation
    methods: global and local; model-specific and model-agnostic; permutation-based
    and sensitivity-based. There’s no shortage of interpretation methods to choose
    from for any machine learning use case. However, it cannot be stressed enough
    that *NO method is perfect*. Still, they can complement each other to approximate
    a better understanding of your machine learning solution and the problem it aims
    to solve.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第二部分，我们探讨了多种解释方法的生态系统：全局和局部；针对特定模型和非特定模型；基于排列和基于敏感度的。对于任何机器学习用例，可供选择的方法并不缺乏。然而，必须强调的是，**没有任何方法是完美的**。尽管如此，它们可以相互补充，以更接近地理解您的机器学习解决方案及其旨在解决的问题。
- en: 'This chapter’s focus on certainty in forecasting was designed to shed light
    on a particular problem in the machine learning community: overconfidence. *Chapter
    1*, *Interpretation, Interpretability, Explainability; and Why Does It All Matter?*,
    in the *A business case of interpretability* section, described the many biases
    that infest human decision-making. These biases are often fueled by overconfidence
    in domain knowledge or our models’ impressive results. And these impressive results
    cloud us from grasping the limitations of our models as the public distrust of
    AI increases.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 本章关注预测中的确定性，旨在揭示机器学习社区中的一个特定问题：过度自信。在“可解释性的商业案例”部分的*第一章*，《解释、可解释性、可解释性；以及这一切为什么都重要？》，描述了充斥在人类决策中的许多偏见。这些偏见通常是由对领域知识或我们模型令人印象深刻的成果的过度自信所驱动的。而这些令人印象深刻的成果使我们无法理解我们模型的局限性，随着公众对AI的不信任增加，这一点变得更加明显。
- en: 'As we discussed in *Chapter 1*, *Interpretation, Interpretability, Explainability;
    and Why Does It All Matter?*, machine learning is only meant to tackle *incomplete
    problems*. Otherwise, we might as well use deterministic and procedural programming
    like those found in closed-loop systems. The best we can do to solve an incomplete
    problem is an incomplete solution, which should be optimized to solve as much
    of it as possible. Whether through gradient descent, least-squares estimation,
    or splitting and pruning a decision tree, machine learning doesn’t produce a model
    that generalizes perfectly. That lack of completeness in machine learning is precisely
    why we need interpretation methods. In a nutshell: models learn from our data,
    and we can learn a lot from our models, but only if we interpret them!'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第一章*中讨论的，*解释、可解释性、可解释性；以及为什么这一切都很重要？*，机器学习仅用于解决*不完整问题*。否则，我们不如使用在闭环系统中发现的确定性程序编程。解决不完整问题的最佳方法是一个不完整的解决方案，它应该被优化以尽可能多地解决它。无论是通过梯度下降、最小二乘估计还是分割和修剪决策树，机器学习不会产生一个完美泛化的模型。机器学习中的这种不完整性正是我们需要解释方法的原因。简而言之：模型从我们的数据中学习，我们可以从我们的模型中学到很多，但只有当我们解释它们时才能做到！
- en: Interpretability doesn’t stop there, though. Model interpretations can drive
    decisions and help us understand model strengths and weaknesses. However, often,
    there are problems in the data or models themselves that can make them less interpretable.
    In *Part Three* of this book, we’ll learn how to tune models and the training
    data for interpretability by reducing complexity, mitigating bias, placing guardrails,
    and enhancing reliability.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可解释性并不止于此。模型解释可以驱动决策并帮助我们理解模型的优势和劣势。然而，数据或模型本身的问题有时会使它们变得难以解释。在本书的*第三部分*中，我们将学习如何通过降低复杂性、减轻偏差、设置护栏和增强可靠性来调整模型和训练数据以提高可解释性。
- en: 'Statistician George E.P. Box famously quipped that “*all models are wrong,
    but some are useful*.” Perhaps they aren’t always wrong, but humility is required
    from machine learning practitioners to accept that even high-performance models
    should be subject to scrutiny and our assumptions about them. Uncertainty with
    machine learning models is expected and shouldn’t be a source of shame or embarrassment.
    This leads us to another takeaway from this chapter: that uncertainty comes with
    ramifications, be it costs or profit lift, and that we can gauge these with sensitivity
    analysis.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家George E.P. Box曾著名地开玩笑说，“*所有模型都是错误的，但有些是有用的*。”也许它们并不总是错误的，但机器学习从业者需要谦逊地接受，即使是高性能模型也应受到审查，并且我们对它们的假设也应受到质疑。机器学习模型的不确定性是可以预期的，不应该是羞耻或尴尬的来源。这使我们得出本章的另一个结论：不确定性伴随着后果，无论是成本还是利润提升，我们可以通过敏感性分析来衡量这些后果。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, you should understand how to assess a time series
    model’s predictive performance, know how to perform local interpretations for
    them with integrated gradients, and know how to produce both local and global
    attributions with SHAP. You should also know how to leverage sensitivity analysis
    factor prioritization and factor fixing for any model.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该了解如何评估时间序列模型的预测性能，知道如何使用集成梯度对他们进行局部解释，以及如何使用SHAP产生局部和全局归因。你还应该知道如何利用敏感性分析因子优先级和因子固定来优化任何模型。
- en: In the next chapter, we will learn how to reduce the complexity of a model and
    make it more interpretable with feature selection and engineering.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何通过特征选择和工程来降低模型的复杂性，使其更具可解释性。
- en: Dataset and image sources
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集和图像来源
- en: 'TomTom, 2019, Traffic Index: [https://nonews.co/wp-content/uploads/2020/02/TomTom2019.pdf](https://nonews.co/wp-content/uploads/2020/02/TomTom2019.pdf)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TomTom，2019年，交通指数：[https://nonews.co/wp-content/uploads/2020/02/TomTom2019.pdf](https://nonews.co/wp-content/uploads/2020/02/TomTom2019.pdf)
- en: 'UCI Machine Learning Repository, 2019, Metro Interstate Traffic Volume Data
    Set: [https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCI机器学习仓库，2019年，都市州际交通流量数据集：[https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)
- en: Further reading
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Wilson, D.R., and Martinez, T., 1997, *Improved Heterogeneous Distance Functions*.
    J. Artif. Int. Res. 6-1\. pp.1-34: [https://arxiv.org/abs/cs/9701101](https://arxiv.org/abs/cs/9701101)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson, D.R. 和 Martinez, T.，1997年，*改进的异构距离函数*。J. Artif. Int. Res. 6-1\. 第1-34页：[https://arxiv.org/abs/cs/9701101](https://arxiv.org/abs/cs/9701101)
- en: 'Morris, M., 1991, *Factorial sampling plans for preliminary computational experiments*.
    Quality Engineering, 37, 307-310: [https://doi.org/10.2307%2F1269043](https://doi.org/10.2307%2F1269043)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Morris, M., 1991, *《初步计算实验的因子抽样计划》*. Quality Engineering, 37, 307-310: [https://doi.org/10.2307%2F1269043](https://doi.org/10.2307%2F1269043)'
- en: 'Saltelli, A., Tarantola, S., Campolongo, F., and Ratto, M., 2007, *Sensitivity
    analysis in practice: A guide to assessing scientific models*. Chichester: John
    Wiley & Sons.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saltelli, A., Tarantola, S., Campolongo, F., and Ratto, M., 2007, *《实践中的敏感性分析：评估科学模型指南》*.
    Chichester: John Wiley & Sons.'
- en: 'Sobol, I.M., 2001, *Global sensitivity indices for nonlinear mathematical models
    and their Monte Carlo estimates*. MATH COMPUT SIMULAT,55(1–3),271-280: [https://doi.org/10.1016/S0378-4754(00)00270-6](https://doi.org/10.1016/S0378-4754(00)00270-6)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sobol, I.M., 2001, *《非线性数学模型的全球敏感性指数及其蒙特卡洛估计》*. MATH COMPUT SIMULAT, 55(1–3),
    271-280: [https://doi.org/10.1016/S0378-4754(00)00270-6](https://doi.org/10.1016/S0378-4754(00)00270-6)'
- en: 'Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and S. Tarantola,
    2010, *Variance based sensitivity analysis of model output. Design and estimator
    for the total sensitivity index*. Computer Physics Communications, 181(2):259-270:
    [https://doi.org/10.1016/j.cpc.2009.09.018](https://doi.org/10.1016/j.cpc.2009.09.018
    )'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and S. Tarantola,
    2010, *《模型输出的方差敏感性分析：总敏感性指数的设计和估计器》*. Computer Physics Communications, 181(2):259-270:
    [https://doi.org/10.1016/j.cpc.2009.09.018](https://doi.org/10.1016/j.cpc.2009.09.018)'
- en: Learn more on Discord
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的 Discord 社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_9.xhtml)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_9.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code107161072033138125.png)'
