- en: 5\. Supervised Learning – Key Steps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 监督学习 – 关键步骤
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will dive deep into the concept of neural networks and describe
    the processes of forward and backpropagation. We will solve a supervised learning
    classification problem using a neural network and analyze the results of the neural
    network by performing error analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨神经网络的概念，并描述前向传播和反向传播的过程。我们将使用神经网络解决一个监督学习分类问题，并通过执行误差分析来分析神经网络的结果。
- en: By the end of this chapter, you will be able to train a network to solve a classification
    problem and fine-tune some of the hyperparameters of the network to improve its
    performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够训练一个网络来解决分类问题，并微调网络的一些超参数以提高其性能。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In the preceding chapter, we explored three machine learning algorithms to solve
    supervised learning tasks, either for classification or regression. In this chapter,
    we will explore one of the most popular machine learning algorithms nowadays,
    artificial neural networks, which belong to a subgroup of machine learning called
    deep learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节中，我们探讨了三种机器学习算法，用于解决监督学习任务，无论是分类还是回归问题。在本章中，我们将探讨当前最流行的机器学习算法之一——人工神经网络，它属于一种叫做深度学习的机器学习子集。
- en: '**Artificial neural networks** (**ANNs**), also known as **Multilayer Perceptrons**
    (**MLPs**), have become increasingly popular mostly because they present a complex
    algorithm that can approach almost any challenging data problem. Even though the
    theory was developed decades back, during the 1940s, such networks are becoming
    more popular now, thanks to all the improvements in technology that allow for
    the gathering of large amounts of data, as well as the developments in computer
    infrastructure that allow the training of complex algorithms with large amounts
    of data.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**），也被称为 **多层感知机**（**MLPs**），由于它们呈现出一种复杂的算法，可以处理几乎任何具有挑战性的数据问题，因此越来越受欢迎。尽管这一理论在
    20 世纪 40 年代就已被提出，但随着技术的进步，尤其是数据收集的能力和计算基础设施的发展，现在这些网络变得更加流行，它们能够在大量数据的基础上训练复杂的算法。'
- en: Due to this, the following chapter will focus on introducing ANNs, their different
    types, and the advantages and disadvantages that they present. Additionally, an
    ANN will be used to predict the income of an individual based on demographic and
    financial information from the individual, as per the previous chapter, in order
    to present the differences in the performance of ANNs in comparison to the other
    supervised learning algorithms.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，接下来的章节将重点介绍人工神经网络（ANNs）、它们的不同类型以及它们所呈现的优缺点。此外，按照前一章节的内容，将使用一个人工神经网络根据个人的
    demographic 和 financial 信息预测其收入，以展示人工神经网络与其他监督学习算法在性能上的差异。
- en: Artificial Neural Networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Although there are several machine learning algorithms available to solve data
    problems, as we have already stated, ANNs have become increasingly popular among
    data scientists, on account of their ability to find patterns in large and complex
    datasets that cannot be interpreted by humans.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有多种机器学习算法可用于解决数据问题，但正如我们已经提到的，人工神经网络（ANNs）因其能够在大型复杂数据集中发现模式，而逐渐受到数据科学家的青睐，这些模式是人类无法解释的。
- en: The **neural** part of the name refers to the resemblance of the architecture
    of the model to the anatomy of the human brain. This part is meant to replicate
    a human being's ability to learn from historical data by transferring bits of
    data from neuron to neuron until an outcome is reached.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经**这个词部分指的是模型结构与人脑解剖结构的相似性。这一部分旨在复制人类通过将数据从一个神经元传递到另一个神经元，直到得出结果的方式来学习历史数据的能力。'
- en: 'In the following diagram, a human neuron is displayed, where A represents the
    **dendrites** that receive input information from other neurons, B refers to the
    **nucleus** of the neuron that processes the information, and C represents the
    **axon** that oversees the process of passing the processed information to the
    next neuron:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图示中，展示了一个人类神经元，其中 A 表示接收来自其他神经元输入信息的 **树突**，B 代表处理信息的 **细胞核**，C 表示负责将处理后的信息传递给下一个神经元的
    **轴突**：
- en: '![Figure 5.1: Visual representation of a human neuron'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：人类神经元的可视化表示'
- en: '](img/B15781_05_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_01.jpg)'
- en: 'Figure 5.1: Visual representation of a human neuron'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：人类神经元的可视化表示
- en: Moreover, the **artificial** part refers to the actual learning process of the
    model, where the main objective is to minimize the error in the model. This is
    an artificial learning process, considering that there is no real evidence regarding
    how human neurons process the information that they receive, and hence the model
    relies on mathematical functions that map an input to a desired output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，**人工**部分指的是模型的实际学习过程，其主要目标是最小化模型的误差。这是一个人工学习过程，因为没有确凿的证据表明人类神经元如何处理它们接收到的信息，因此模型依赖于将输入映射到期望输出的数学函数。
- en: How Do ANNs Work?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络是如何工作的？
- en: 'Before we dive into the process that is followed by an ANN, let''s start by
    looking at its main components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解神经网络的过程之前，让我们先看一下其主要组成部分：
- en: '`X`, as it contains all the data from the dataset (each instance with its features).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X`，它包含了数据集中所有的数据（每个实例及其特征）。'
- en: '**Hidden layers**: This layer is in charge of processing the input data in
    order to find patterns that are useful for making a prediction. The ANN can have
    as many hidden layers as desired, each with as many neurons (units) as required.
    The first layers are in charge of the simpler patterns, while the layers at the
    end search for the more complex ones.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：这一层负责处理输入数据，以便发现有助于做出预测的模式。神经网络可以有任意数量的隐藏层，每层有所需的神经元（单位）数。前几层负责处理简单的模式，而后面几层则负责寻找更复杂的模式。'
- en: The hidden layers use a set of variables that represent weights and biases in
    order to help train the network. The values for the weights and biases are used
    as the variables that change in each iteration to approximate the prediction to
    the ground truth. This will be explained later.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐藏层使用一组表示权重和偏差的变量来帮助训练网络。权重和偏差的值作为在每次迭代中变化的变量，用来将预测结果逼近实际值。稍后将详细解释这一过程。
- en: '`Y_hat`, this layer is the prediction made by the model, based on the data
    received from the hidden layers. This prediction is presented in the form of a
    probability, where the class label with a higher probability is the one selected
    as the prediction.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Y_hat`，这一层是模型基于从隐藏层接收到的数据所做的预测。该预测以概率的形式呈现，其中具有较高概率的类别标签被选为预测结果。'
- en: 'The following diagram illustrates the architecture of the preceding three layers,
    where the circles under 1 denote the neurons in the input layer, the ones under
    2 represent the neurons of 2 hidden layers (each layer represented by a column
    of circles), and finally, the circles under 3 are the neurons of the output layer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了前三层的架构，其中1下方的圆圈表示输入层的神经元，2下方的圆圈表示两个隐藏层的神经元（每层由一列圆圈表示），最后，3下方的圆圈表示输出层的神经元：
- en: '![Figure 5.2: Basic architecture of an ANN'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：神经网络的基本结构'
- en: '](img/B15781_05_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_02.jpg)'
- en: 'Figure 5.2: Basic architecture of an ANN'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：神经网络的基本结构
- en: As an analogy, consider a manufacturing process for building car parts. Here,
    the input layer consists of the raw materials, which, in this case, may be aluminum.
    The initial steps of the process involve polishing and cleaning the material,
    which can be seen as the first couple of hidden layers. Next, the material is
    bent to achieve the shape of the car part, which is handled by the deeper hidden
    layers. Finally, the part is delivered to the client, which can be considered
    to be the output layer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为类比，考虑一个制造汽车零部件的过程。在这里，输入层由原材料组成，这些原材料可能是铝。过程的初步步骤包括抛光和清洁材料，可以视为前几层隐藏层。接下来，材料被弯曲以实现汽车部件的形状，这由更深的隐藏层处理。最后，部件交付给客户，这可以视为输出层。
- en: Considering these steps, the main objective of the manufacturing process is
    to achieve a final part that highly resembles the part that the process aimed
    to build, meaning that the output, `Y_hat`, should maximize its similarity to
    `Y` (the ground truth) for a model to be considered a good fit to the data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些步骤，制造过程的主要目标是实现一个最终部件，该部件高度类似于该过程旨在构建的部件，这意味着输出 `Y_hat` 应最大化与 `Y`（实际值）的相似性，才能认为模型与数据相拟合。
- en: 'The actual methodology to train an ANN is an iterative process comprised of
    the following steps: forward propagation, calculation of the cost function, backpropagation,
    and weights and biases updates. Once the weights and biases are updated, the process
    starts again until the number of iterations is met.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 ANN 的实际方法是一个迭代过程，包括以下步骤：正向传播、计算成本函数、反向传播、以及权重和偏置的更新。一旦权重和偏置更新完成，过程将重新开始，直到满足迭代次数要求。
- en: Let's explore each of the steps of the iteration process in detail.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨迭代过程中的每个步骤。
- en: Forward Propagation
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正向传播
- en: 'The input layer feeds the initial information to the ANN. The processing of
    the data is done by propagating data bits through the depth (number of hidden
    layers) and width (number of units in each layer) of the network. The information
    is processed by each neuron in each layer using a linear function, coupled with
    an activation function that aims to break the linearity, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层将初始信息传递给 ANN。数据的处理是通过在网络的深度（隐藏层的数量）和宽度（每层单元的数量）中传播数据比特完成的。每一层中的每个神经元都使用线性函数处理信息，并结合激活函数来打破线性关系，过程如下：
- en: '![Figure 5.3: The linear and activation functions used by an ANN'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：ANN 使用的线性和激活函数'
- en: '](img/B15781_05_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_03.jpg)'
- en: 'Figure 5.3: The linear and activation functions used by an ANN'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：ANN 使用的线性和激活函数
- en: Here, *W*1 and *b*1 are a matrix and a vector containing the weights and biases,
    respectively, and serve as the variables that can be updated through the iterations
    to train the model. *Z*1 is the linear function for a given neuron, and *A*1 is
    the outcome from the unit after applying an activation function (represented by
    the sigma symbol) to the linear one.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W*1 和 *b*1 分别是包含权重和偏置的矩阵和向量，作为可以通过迭代更新的变量来训练模型。*Z*1 是给定神经元的线性函数，*A*1 是在应用激活函数（用
    sigma 符号表示）后得到的单位输出。
- en: 'The preceding two formulas are calculated for each neuron in each layer, where
    the value of *X* for the hidden layers (other than the input layer) is replaced
    by the output of the previous layer (*A*n), as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的两个公式会针对每个层中的每个神经元进行计算，其中隐藏层（除了输入层）的 *X* 值将被替换为上一层的输出（*A*n），如下所示：
- en: '![Figure 5.4: The values calculated for the second layer of the ANN'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：ANN 第二层计算的值'
- en: '](img/B15781_05_04.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_04.jpg)'
- en: 'Figure 5.4: The values calculated for the second layer of the ANN'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：ANN 第二层计算的值
- en: Finally, the output from the last hidden layer is fed to the output layer, where
    the linear function is once again calculated, along with an activation function.
    The outcome from this layer, after some processing as required, is the one that
    will be compared against the ground truth in order to evaluate the performance
    of the algorithm before moving on to the next iteration.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，来自最后一个隐藏层的输出被传送到输出层，在那里再次计算线性函数，并结合激活函数进行处理。经过必要的处理后，该层的输出将与真实值进行比较，以评估算法的性能，然后才会进入下一次迭代。
- en: The values of the weights for the first iteration are randomly initialized between
    0 and 1, while the values for the biases can be set to 0 initially. Once the first
    iteration is run, the values will be updated, so that the process can start again.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次迭代的权重值会在 0 和 1 之间随机初始化，而偏置值可以初始设置为 0。一旦第一次迭代运行，权重和偏置值将被更新，从而使过程重新开始。
- en: The activation function can be of different types. Some of the most common ones
    are the **Rectified Linear Unit** (**ReLU**), the **Hyperbolic tangent** (**tanh**),
    and the **Sigmoid** and **Softmax** functions, which will be explained in a subsequent
    section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数可以有不同的类型。一些常见的激活函数包括**修正线性单元**（**ReLU**）、**双曲正切**（**tanh**）、以及**Sigmoid**和**Softmax**函数，后续部分将对这些函数进行解释。
- en: Cost Function
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本函数
- en: Considering that the final objective of the training process is to build a model
    based on a given set of data that maps an expected output, it is particularly
    important to measure the model's ability to estimate a relation between `X` and
    `Y` by comparing the differences between the predicted value (`Y_hat`) and the
    ground truth (`Y`). This is accomplished by calculating the cost function (also
    known as the **loss function**) to determine how poor the model's predictions
    are. The cost function is calculated for each iteration to measure the progress
    of the model along the iteration process, with the objective of finding the values
    for the weights and biases that minimize the cost function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到训练过程的最终目标是基于给定的数据集构建一个模型，以映射预期输出，特别重要的是通过比较预测值（`Y_hat`）和真实值（`Y`）之间的差异，来衡量模型估计
    `X` 和 `Y` 之间关系的能力。这是通过计算损失函数（也称为**损失函数**）来完成的，目的是确定模型预测的准确性。每次迭代都会计算损失函数，以衡量模型在迭代过程中的进展，目标是找到能够最小化损失函数的权重和偏置值。
- en: For classification tasks, the cost function most commonly used is the **cross-entropy
    cost function**, where the higher the value of the cost function, the greater
    the divergence between the predicted and actual values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，最常用的损失函数是**交叉熵损失函数**，其中损失函数的值越大，预测值与实际值之间的差异越大。
- en: 'For a binary classification task, that is, tasks with only two class output
    labels, the cross-entropy cost function is calculated as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类任务，也就是只有两个类别输出标签的任务，交叉熵损失函数的计算公式如下：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, *y* would be either 1 or 0 (either of the two class labels), *y*hat would
    be the probability calculated by the model, and *log* would be the natural logarithm.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y* 要么是 1，要么是 0（两个类别标签中的一个），*y*hat 是模型计算的概率，*log* 是自然对数。
- en: 'For a multiclass classification task, the formula is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类别分类任务，公式如下：
- en: '![Figure 5.5: The cost function for a multiclass classification task'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5：多类别分类任务的损失函数'
- en: '](img/B15781_05_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_05.jpg)'
- en: 'Figure 5.5: The cost function for a multiclass classification task'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：多类别分类任务的损失函数
- en: Here, *c* represents a class label and *M* refers to the total number of class
    labels.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*c* 表示类别标签，*M* 是类别标签的总数。
- en: Once the cost function is calculated, the training process proceeds to perform
    the backpropagation step, which will be explained in the following section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦损失函数计算完成，训练过程将进入反向传播步骤，接下来将解释这一过程。
- en: Moreover, for regression tasks, the cost function would be the RMSE, which was
    explained in *Chapter 3*, *Supervised Learning – Key Steps*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于回归任务，损失函数将是均方根误差（RMSE），这一点在*第 3 章*，*监督学习——关键步骤*中已解释。
- en: Backpropagation
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: The backpropagation procedure was introduced as part of the training process
    of ANNs to make learning faster. It basically involves calculating the partial
    derivatives of the cost function with respect to the weights and biases along
    the network. The objective of this is to minimize the cost function by changing
    the weights and the biases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播过程作为人工神经网络（ANNs）训练过程的一部分，引入了以加快学习速度。它基本上涉及计算损失函数关于权重和偏置的偏导数，并沿网络传播。其目标是通过调整权重和偏置来最小化损失函数。
- en: Considering that the weights and biases are not directly contained in the cost
    function, a chain rule is used to propagate the error from the cost function backward
    until it reaches the first layers of the network. Next, a weighted average of
    the derivatives is calculated, which is used as the value to update the weights
    and biases before running a new iteration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到权重和偏置并不直接包含在损失函数中，使用链式法则将误差从损失函数反向传播，直到到达网络的第一层。接下来，计算偏导数的加权平均值，并将其作为更新权重和偏置的值，然后进行新一轮的迭代。
- en: There are several algorithms that can be used to perform backpropagation, but
    the most common one is **gradient descent**. Gradient descent is an optimization
    algorithm that tries to find some local or global minimum of a function, which,
    in this case, is the cost function. It does so by determining the direction in
    which the model should move to reduce the error.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种算法可以用于执行反向传播，但最常见的算法是**梯度下降**。梯度下降是一种优化算法，它试图找到函数的局部或全局最小值，在这里，它的目标是损失函数。它通过确定模型应移动的方向来减少误差，从而实现这一目标。
- en: 'For instance, the following diagram displays an example of the training process
    of an ANN through the different iterations, where the job of backpropagation is
    to determine the direction in which the weights and biases should be updated,
    so that the error can continue to be minimized until it reaches a minimum point:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下图示展示了ANN训练过程中的一个示例，通过不同的迭代，其中反向传播的任务是确定权重和偏差应该更新的方向，从而使误差继续最小化，直到达到最小点：
- en: '![Figure 5.6: Example of the iterative process of training an ANN'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6：ANN训练的迭代过程示例'
- en: '](img/B15781_05_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_06.jpg)'
- en: 'Figure 5.6: Example of the iterative process of training an ANN'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：ANN训练的迭代过程示例
- en: 'It is important to highlight that backpropagation does not always find the
    global minima, since it stops updating once it has reached the lowest point in
    a slope, regardless of any other regions. For instance, consider the following
    diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，反向传播并不总是能找到全局最小值，因为一旦它到达坡道的最低点，它就会停止更新，而不管其他区域如何。例如，考虑以下图示：
- en: '![Figure 5.7: Examples of minimum points'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：最小点的示例'
- en: '](img/B15781_05_07.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_07.jpg)'
- en: 'Figure 5.7: Examples of minimum points'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：最小点的示例
- en: Although all three points can be considered minimum points when compared to
    the points to their left and right, only one of them is the global minima.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与左侧和右侧的点相比，所有三个点都可以视为最小点，但其中只有一个是全局最小值。
- en: Updating the Weights and Biases
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新权重和偏差
- en: 'Taking the derivatives'' average that was calculated during backpropagation,
    the final step of an iteration is to update the values of the weights and biases.
    This process is done using the following formula for updating weights and biases:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算反向传播过程中得到的导数平均值，迭代的最后一步是更新权重和偏差的值。这个过程使用以下公式来更新权重和偏差：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, the old values are those used to perform the forward propagation step,
    the derivative rate is the value obtained from the backpropagation step, which
    is different for the weights and the biases, and the learning rate is a constant
    that is used to neutralize the effect of the derivative rate, so that the changes
    in the weights and biases are small and smooth. This has been proven to help reach
    the lowest point more quickly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，旧值是用于执行前向传播步骤的值，导数率是从反向传播步骤中得到的值，对于权重和偏差有所不同，学习率是用于中和导数率影响的常数，使得权重和偏差的变化保持小而平滑。这已被证明有助于更快地达到最低点。
- en: Once the weights and the biases have been updated, the entire process starts
    again.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦权重和偏差被更新，整个过程将重新开始。
- en: Understanding the Hyperparameters
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解超参数
- en: 'Hyperparameters, as you have seen so far, are parameters that can be fine-tuned
    to improve the accuracy of a model. For neural networks, hyperparameters can be
    classified into two main groups:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，超参数是可以通过微调来提高模型准确性的参数。对于神经网络，超参数可以分为两大类：
- en: Those that alter the structure of the network
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些改变网络结构的参数
- en: Those that modify the process to train it
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些修改训练过程的参数
- en: An important part of building an ANN is the process of fine-tuning the hyperparameters
    by performing error analysis and by playing around with the hyperparameters that
    help to solve the condition that is affecting the network. As a general reminder,
    networks suffering from high bias can usually be improved by creating bigger networks
    or training for longer durations of time (that is, more iterations), whereas networks
    suffering from high variance can benefit from the addition of more training data
    or by introducing a regularization technique, which will be explained in a subsequent
    section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 构建ANN的一个重要部分是通过执行误差分析并调整有助于解决影响网络的条件的超参数来进行微调。一般提醒一下，出现高偏差的网络通常可以通过创建更大的网络或训练更长时间（即更多的迭代次数）来改进，而出现高方差的网络则可以通过增加更多的训练数据或引入正则化技术来改善，后者将在后续章节中解释。
- en: Considering that the number of hyperparameters that can be changed for training
    an ANN is large, the most commonly used ones will be explained in the following sections.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于可以更改用于训练ANN的大量超参数，接下来将解释最常用的那些超参数。
- en: Number of Hidden Layers and Units
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层和单元的数量
- en: The number of hidden layers and the number of units in each layer can be set
    by the researcher, as mentioned previously. Again, there is no exact science to
    select this number, and, on the contrary, the selection of this number is part
    of the fine-tuning process to test different approximations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，隐藏层的数量以及每层的单元数量可以由研究人员设定。同样地，选择这些数量并没有确切的科学方法，相反，这一选择是微调过程中测试不同近似值的一部分。
- en: Nonetheless, when selecting the number of hidden layers, some data scientists
    lean toward an approach wherein multiple networks are trained, each with an extra
    layer. The model with the lowest error is the one with the correct number of hidden
    layers. Unfortunately, this approach does not always work well, as more complex
    data problems do not really show a difference in performance through simply changing
    the number of hidden layers, regardless of the other hyperparameters.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在选择隐藏层的数量时，一些数据科学家倾向于采用一种方法，即训练多个网络，每个网络多一个隐藏层。误差最小的模型即为具有正确隐藏层数量的模型。不幸的是，这种方法并不总是奏效，因为对于更复杂的数据问题，单纯通过改变隐藏层数量并不会显著改善性能，无论其他超参数如何。
- en: On the other hand, there are several techniques that you can use to choose the
    number of units in a hidden layer. It is common for data scientists to choose
    the initial values for both of these hyperparameters based on similar research
    papers that are available online. This means that a good starting point would
    be copying the architecture of networks that have been successfully used for projects
    in a similar field, and then, through error analysis, fine-tuning the hyperparameters
    to improve performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有多种技术可以用于选择隐藏层中的单元数量。数据科学家常常根据网上的类似研究论文来选择这两个超参数的初始值。这意味着一个好的起点是复制在类似领域的项目中成功使用的网络架构，然后通过误差分析，微调超参数以提高性能。
- en: Nonetheless, it is important to consider the fact that based on research activity,
    deeper networks (networks with many hidden layers) outperform wider networks (networks
    with many units in each layer).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，根据研究活动，深度网络（具有多个隐藏层的网络）往往优于宽度网络（每层具有多个单元的网络）。
- en: Activation Functions
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'As mentioned previously, the activation function is used to introduce non-linearity
    to the model. The most commonly used activation functions are the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，激活函数用于为模型引入非线性。最常用的激活函数包括以下几种：
- en: '**ReLU**: The output of this function is either 0 or the number derived from
    the linear function, whichever is higher. This means that the output will be the
    raw number it receives whenever this number is above 0, otherwise, the output
    would be 0.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU**：该函数的输出为 0 或来自线性函数的数值，以较大的值为准。也就是说，当输入值大于 0 时，输出就是该输入值本身；否则，输出为 0。'
- en: '**Tanh**: This function consists of the division of the hyperbolic sine by
    the hyperbolic cosine of the input. The output is a number between -1 and 1.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tanh**：该函数由输入的双曲正弦与双曲余弦的商组成。输出为介于 -1 和 1 之间的数值。'
- en: '**Sigmoid**: The function has an S-shape. It takes the input and converts it
    into a probability. The output from this function is between 0 and 1.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：该函数呈 S 形。它将输入值转化为概率。该函数的输出值介于 0 和 1 之间。'
- en: '**Softmax**: Similar to the sigmoid function, this calculates the probability
    of the input, with the difference being that the Softmax function can be used
    for multiclass classification tasks as it is capable of calculating the probability
    of a class label in reference to the others.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：与 sigmoid 函数类似，Softmax 计算输入的概率，不同之处在于 Softmax 函数可以用于多类别分类任务，因为它能够计算一个类别标签相对于其他类别的概率。'
- en: The selection of an activation function should be done by considering that,
    conventionally, both the ReLU and the Hyperbolic tangent (tanh) activation functions
    are used for all of the hidden layers, with ReLU being the most popular one among
    scientists due to its performance in relation to the majority of data problems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择应考虑到，通常情况下，ReLU 和双曲正切（tanh）激活函数被用于所有隐藏层，其中 ReLU 由于其在大多数数据问题中的性能，成为科学家们最常用的选择。
- en: Moreover, the Sigmoid and the Softmax activation functions should be used for
    the output layer, as their outcome is in the form of a probability. The Sigmoid
    activation function is used for binary classification problems, as it only outputs
    the probability for two class labels, whereas the Softmax activation function
    can be used for either binary or multiclass classification problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Sigmoid和Softmax激活函数应该用于输出层，因为它们的输出形式是概率。Sigmoid激活函数用于二分类问题，因为它只输出两个类别标签的概率，而Softmax激活函数则可以用于二分类或多分类问题。
- en: Regularization
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization is a technique used in machine learning to improve a model that
    is suffering from overfitting, which means that this hyperparameter is mostly
    used when it is strictly required, and its main objective is to increase the generalization
    ability of the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是机器学习中用于改善过拟合模型的一种技术，过拟合意味着当模型过度拟合训练数据时，这个超参数通常在严格要求时才使用，其主要目的是增加模型的泛化能力。
- en: 'There are different regularization techniques, but the most common ones are
    the L1, L2, and dropout techniques. Although scikit-learn only supports L2 for
    its MLP classifier, brief explanations of the three forms of regularization are
    as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种正则化技术，但最常见的有L1、L2和Dropout技术。尽管scikit-learn仅支持L2正则化用于其MLP分类器，但以下是三种正则化形式的简要说明：
- en: The L1 and L2 techniques add a regularization term to the cost function as a
    way of penalizing high weights that may be affecting the performance of the model.
    The main difference between these approaches is that the regularization term for
    L1 is the absolute value of the magnitude of the weights, while for L2, it is
    the squared magnitude of the weights. For regular data problems, L2 has proven
    to work better, while L1 is mainly popular for feature extraction tasks since
    it creates sparse models.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1和L2技术通过在成本函数中添加正则化项来惩罚可能影响模型性能的高权重。这两种方法的主要区别在于，L1的正则化项是权重的绝对值，而L2的正则化项是权重的平方大小。对于常规数据问题，L2已被证明效果更好，而L1主要在特征提取任务中流行，因为它能够创建稀疏模型。
- en: Dropout, on the other hand, refers to the model's ability to drop out some units
    in order to ignore their output during a step in the iteration, which simplifies
    the neural network. The dropout value is set between 0 and 1, and it represents
    the percentage of units that will be ignored. The units that are ignored are different
    in each iteration step.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout则指模型通过丢弃一些单元，在迭代步骤中忽略它们的输出，从而简化神经网络。Dropout值在0到1之间设置，表示将被忽略的单元的比例。每次迭代步骤中被忽略的单元都是不同的。
- en: Batch Size
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量大小
- en: Another hyperparameter to be tuned during the construction of an ANN is the
    batch size. This refers to the number of instances to be fed to the neural network
    during an iteration, which will be used to perform a forward and a backward pass
    through the network. For the next iteration, a new set of instances will be used.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 构建人工神经网络（ANN）时需要调整的另一个超参数是批量大小。它指的是在每次迭代中传入神经网络的实例数量，这些实例将用于执行前向传播和反向传播。对于下一次迭代，将使用一组新的实例。
- en: This technique also helps to improve the model's ability to generalize to the
    training data because, in each iteration, it is fed with new combinations of instances,
    which is useful when dealing with an overfitted model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术还帮助提高模型对训练数据的泛化能力，因为在每次迭代中，模型都会接收到新的实例组合，这在处理过拟合模型时非常有用。
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As per the result of many years of research, a good practice is to set the batch
    size to a value that is a multiple of 2\. Some of the most common values are 32,
    64, 128, and 256.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据多年的研究结果，一个好的实践是将批量大小设置为2的倍数。一些常见的值包括32、64、128和256。
- en: Learning Rate
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate, as explained previously, is introduced to help determine
    the size of the steps that the model will take to get to the local or global minima
    in each iteration. The lower the learning rate, the slower the learning process
    of the network, but this results in better models. On the other hand, the larger
    the learning rate, the faster the learning process of the model; however, this
    may result in a model not converging.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，学习率用于帮助确定模型在每次迭代中向局部或全局最小值前进的步长。学习率越低，网络的学习过程越慢，但这会导致更好的模型。另一方面，学习率越大，模型的学习过程越快，但这可能导致模型无法收敛。
- en: Note
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The default learning rate value is usually set to 0.001.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的学习率值通常设置为0.001。
- en: Number of Iterations
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迭代次数
- en: A neural network is trained through an iterative process, as mentioned previously.
    Therefore, it is necessary to set the number of iterations that the model will
    perform. The best way to set up the ideal number of iterations is to start low,
    between 200 and 500, and increase it, in the event that the plot of the cost function
    over each iteration shows a decreasing line. Needless to say, the larger the number
    of iterations, the longer it takes to train a model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是通过迭代过程进行训练的，如前所述。因此，需要设定模型将执行的迭代次数。设置理想迭代次数的最佳方式是从较低的值开始，介于200到500之间，并在每次迭代的成本函数图显示递减趋势时增加它。不言而喻，迭代次数越大，训练模型的时间也越长。
- en: Additionally, increasing the number of iterations is a technique known to address
    underfitted networks. This is because it gives the network more time to find the
    right weights and biases that generalize to the training data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，增加迭代次数是解决欠拟合网络的一种技术。这是因为它给网络更多时间来找到适用于训练数据的正确权重和偏置。
- en: Applications of Neural Networks
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的应用
- en: In addition to the preceding architecture, a number of new architectures have
    emerged over time, thanks to the popularity of neural networks. Some of the most
    popular ones are **convolutional neural networks**, which can handle the processing
    of images by using filters as layers, and **recurrent neural networks**, which
    are used to process sequences of data such as text translations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述的架构外，随着神经网络的流行，出现了许多新的架构。其中最流行的一些是**卷积神经网络**，它可以通过使用滤波器作为层来处理图像，以及**递归神经网络**，它用于处理像文本翻译这样的数据序列。
- en: On account of this, the applications of neural networks extend to almost any
    data problem, ranging from simple to complex. While a neural network is capable
    of finding patterns in really large datasets (either for classification or regression
    tasks), they are also known for effectively handling challenging problems, such
    as the autonomous abilities of self-driving cars, the construction of chatbots,
    and the recognition of faces.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络的应用几乎涵盖了所有数据问题，从简单到复杂。虽然神经网络能够在非常大的数据集中找到模式（无论是分类任务还是回归任务），但它们也以有效处理一些具有挑战性的问题而闻名，例如自动驾驶汽车的自主能力、聊天机器人构建以及面部识别。
- en: Limitations of Neural Networks
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的局限性
- en: 'Some of the limitations of training neural networks are as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的一些局限性如下：
- en: The training process takes time. Regardless of the hyperparameters used, they
    generally take time to converge.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程需要时间。无论使用什么超参数，通常都需要时间才能收敛。
- en: They need very large datasets in order to work better. Neural networks are meant
    for larger datasets, as their main advantage is their ability to find patterns
    within millions of values.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们需要非常大的数据集才能更好地工作。神经网络适用于较大的数据集，因为它们的主要优势在于能够在数百万个值中找到模式。
- en: They are considered a black box as there is no actual knowledge of how the network
    arrives at a result. Although the math behind the training process is clear, it
    is not possible to know what assumptions the model makes while being trained.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们被视为黑箱，因为我们无法实际了解网络是如何得出结果的。尽管训练过程背后的数学原理是清晰的，但无法知道模型在训练过程中做出了哪些假设。
- en: The hardware requirements are large. Again, the greater the complexity of the
    problem, the larger the hardware requirements.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件要求较高。同样，问题的复杂性越大，硬件要求也越大。
- en: Although ANNs can be applied to almost any data problem, due to their limitations,
    it is always a good practice to test other algorithms when dealing with simpler
    data problems. This is important because applying neural networks to data problems
    that can be solved by simpler models makes the costs outweigh the benefits.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人工神经网络几乎可以应用于任何数据问题，但由于其局限性，在处理简单数据问题时，测试其他算法始终是一个好习惯。这一点非常重要，因为将神经网络应用于那些可以通过更简单模型解决的数据问题，会导致成本大于收益。
- en: Applying an Artificial Neural Network
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用人工神经网络
- en: Now that you know the components of an ANN, as well as the different steps that
    it follows to train a model and make predictions, let's train a simple network
    using the scikit-learn library.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了人工神经网络（ANN）的组成部分，以及它训练模型和做出预测的不同步骤，接下来让我们使用scikit-learn库训练一个简单的网络。
- en: In this topic, scikit-learn's neural network module will be used to train a
    network using the datasets used in the previous chapter's exercises and activities
    (that is, the Fertility Dataset and the Processed Census Income Dataset). It is
    important to mention that scikit-learn is not the most appropriate library for
    neural networks, as it does not currently support many types of neural networks,
    and its performance over deeper networks is not as good as other neural network
    specialized libraries, such as TensorFlow and PyTorch.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本主题中，将使用scikit-learn的神经网络模块，通过上一章的练习和活动中使用的数据集（即生育数据集和处理后的普查收入数据集）来训练一个网络。需要提到的是，scikit-learn并不是最适合做神经网络的库，因为它目前不支持许多类型的神经网络，并且在处理更深层次的网络时，性能不如其他专注于神经网络的库，比如TensorFlow和PyTorch。
- en: The neural network module in scikit-learn currently supports an MLP for classification,
    an MLP for regression, and a Restricted Boltzmann Machine architecture. Considering
    that the case study consists of a classification task, the MLP for classifications
    will be used.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的神经网络模块目前支持用于分类的MLP、用于回归的MLP以及受限玻尔兹曼机（Restricted Boltzmann Machine）架构。考虑到本案例研究是一个分类任务，因此将使用用于分类的MLP。
- en: Scikit-Learn's Multilayer Perceptron
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scikit-Learn的多层感知器（MLP）
- en: An MLP is a supervised learning algorithm that, as the name indicates, uses
    multiple layers (hidden layers) to learn a non-linear function that translates
    the input values into output, either for classification or regression. As we explained
    previously, the job of each unit of a layer is to transform the data received
    from the previous layer by calculating a linear function and then applying an
    activation function to break the linearity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: MLP是一种监督学习算法，顾名思义，它使用多个层（隐藏层）来学习一个非线性函数，将输入值转换为输出，无论是用于分类还是回归。如前所述，每个层的单元的工作是通过计算一个线性函数并应用激活函数来打破线性关系，从而转化从前一层接收到的数据。
- en: It is important to mention the fact that an MLP has a non-convex loss function
    that, as mentioned previously, signifies that there may be multiple local minima.
    This means that different initializations of the weights and biases will result
    in different trained models, which, in turn, indicates different accuracy levels.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提到的是，MLP具有一个非凸的损失函数，正如前面提到的，这意味着可能存在多个局部最小值。这意味着不同的权重和偏差初始化将导致不同的训练模型，这也意味着不同的准确性水平。
- en: The MLP classifier in scikit-learn has around 20 different hyperparameters associated
    with the architecture or the learning process, which can be altered in order to
    modify the training process of the network. Fortunately, all of these hyperparameters
    have set default values, which allows us to run an initial model without much
    effort. The results from this model can then be used to tune the hyperparameters
    as required.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的MLP分类器有大约20个与架构或学习过程相关的超参数，可以调整这些超参数来修改网络的训练过程。幸运的是，所有这些超参数都有预设的默认值，这使得我们可以轻松地运行一个初始模型。然后可以根据需要调整这些超参数，以优化模型。
- en: 'To train an MLP classifier, it is required that you input two arrays: first,
    the `X` input of dimensions (`n_samples`, `n_features`) containing the training
    data, and then the `Y` input of dimensions (`n_sample`) that contains the label
    values for each sample.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个MLP分类器，需要输入两个数组：首先是`X`输入，其维度为（`n_samples`，`n_features`），包含训练数据；然后是`Y`输入，其维度为（`n_samples`），包含每个样本的标签值。
- en: Similar to the algorithms that we looked at in the previous chapter, the model
    is trained using the `fit` method, and then predictions can be obtained by using
    the `predict` method on the trained model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前一章中查看的算法类似，模型是通过`fit`方法进行训练的，然后可以通过在训练好的模型上使用`predict`方法来获取预测结果。
- en: 'Exercise 5.01: Applying the MLP Classifier Class'
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习5.01：应用MLP分类器类
- en: In this exercise, you will train a model using scikit-learn's MLP to solve a
    classification task that consists of determining whether the fertility of the
    subjects has been affected by their demographics, their environmental conditions,
    and their previous medical conditions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，您将使用scikit-learn的MLP来训练一个模型，解决一个分类任务，该任务包括确定受试者的生育能力是否受其人口统计特征、环境条件和既往病史的影响。
- en: Note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For the exercises and activities within this chapter, you will need to have
    Python 3.7, NumPy, Jupyter, pandas, and scikit-learn installed on your system.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中的练习和活动，您需要在系统中安装Python 3.7、NumPy、Jupyter、pandas和scikit-learn。
- en: 'Open a Jupyter Notebook to implement this exercise. Import all the necessary
    elements to read the dataset and to calculate a model''s accuracy, as well as
    scikit-learn''s `MLPClassifier` class:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Jupyter Notebook 实现这个练习。导入所有必要的元素以读取数据集并计算模型的准确性，以及 scikit-learn 的 `MLPClassifier`
    类：
- en: '[PRE2]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the Fertility Dataset from the previous chapter, read the `.csv` file.
    Make sure that you add the `header` argument equal to `None` to the `read_csv`
    function, considering that the dataset does not contain a header row:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一章的生育率数据集，读取 `.csv` 文件。确保将 `header` 参数设置为 `None`，传递给 `read_csv` 函数，因为该数据集没有包含头行：
- en: '[PRE3]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Split the dataset into `X` and `Y` sets in order to separate the features data
    from the label values:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分成 `X` 和 `Y` 两个集合，以便将特征数据与标签值分开：
- en: '[PRE4]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Instantiate the `MLPClassifier` class from the `neural_network` module of scikit-learn
    and use the `fit` method to train a model. When instantiating the model, leave
    all the hyperparameters at their default values, but add a `random_state` argument
    equal to `101` to ensure that you get the same results as the one shown in this
    exercise:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 的 `neural_network` 模块实例化 `MLPClassifier` 类，并使用 `fit` 方法训练模型。在实例化模型时，保持所有超参数为默认值，但添加
    `random_state` 参数，设置为 `101`，以确保你得到与本练习中显示的相同结果：
- en: '[PRE5]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Address the warning that appears after running the `fit` method:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理运行 `fit` 方法后出现的警告：
- en: '![Figure 5.8: Warning message displayed after running the fit method'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.8：运行 `fit` 方法后显示的警告信息'
- en: '](img/B15781_05_08.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_05_08.jpg)'
- en: 'Figure 5.8: Warning message displayed after running the fit method'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.8：运行 `fit` 方法后显示的警告信息
- en: As you can see, the warning specifies that after running the default number
    of iterations, which is `200`, the model has not reached convergence.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，警告指出在运行默认迭代次数`200`次后，模型尚未收敛。
- en: 'To address this issue, try higher values for the iterations until the warning
    stops appearing. To change the number of iterations, add the `max_iter` argument
    inside the parentheses during the instantiation of the model:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了解决这个问题，尝试使用更高的迭代次数，直到警告不再出现。要更改迭代次数，请在实例化模型时，在括号内添加 `max_iter` 参数：
- en: '[PRE6]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Furthermore, the output beneath the warning explains the values used for all
    of the hyperparameters of the MLP.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，警告下方的输出解释了 MLP 所有超参数使用的值。
- en: 'Finally, perform a prediction by using the model that you trained previously,
    for a new instance with the following values for each feature: `−0.33`, `0.69`,
    `0`, `1`, `1`, `0`, `0.8`, `0`, `0.88`.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用你之前训练的模型对一个新的实例进行预测，该实例的每个特征值如下：`−0.33`，`0.69`，`0`，`1`，`1`，`0`，`0.8`，`0`，`0.88`。
- en: 'Use the following code:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用以下代码：
- en: '[PRE7]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The model's prediction is equal to `N`, that is, the model predicts the person
    with the specified features to have a normal diagnosis.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的预测结果为 `N`，即模型预测该具有指定特征的人为正常诊断。
- en: 'Calculate the accuracy of your model, based on the predictions it achieves
    over the `X` variable, as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据模型在 `X` 变量上的预测，计算你的模型准确性，如下所示：
- en: '[PRE8]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The accuracy of your model is equal to `98%`.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的模型准确率为 `98%`。
- en: Note
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2BaKHRe](https://packt.live/2BaKHRe).
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见 [https://packt.live/2BaKHRe](https://packt.live/2BaKHRe)。
- en: You can also run this example online at [https://packt.live/37tTxpv](https://packt.live/37tTxpv).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是 [https://packt.live/37tTxpv](https://packt.live/37tTxpv)。你必须执行整个
    Notebook 才能得到预期结果。
- en: You have successfully trained and evaluated the performance of an MLP model.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经成功地训练并评估了 MLP 模型的性能。
- en: 'Activity 5.01: Training an MLP for Our Census Income Dataset'
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.01：为我们的普查收入数据集训练一个 MLP
- en: 'With the objective of comparing the performance of the algorithms trained in
    the previous chapter with the performance of a neural network, for this activity,
    we will continue to work with the Preprocessed Census Income Dataset. Consider
    the following scenario: your company is continually offering a course for employees
    to improve their abilities, and you have recently learned about neural networks
    and their power. You have decided to build a network to model the dataset that
    you were given previously in order to test whether a neural network is better
    at predicting a person''s income based on their demographic data.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 目的是将上一章节训练的算法性能与神经网络的性能进行比较，针对本活动，我们将继续使用预处理的收入数据集。假设以下情境：你的公司一直在为员工提供提升技能的课程，而你最近学习了神经网络及其强大功能。你决定构建一个网络来建模之前给定的数据集，测试神经网络在根据人口统计数据预测个人收入方面是否优于其他模型。
- en: Note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Start this activity using the preprocessed dataset from the previous chapter:
    `census_income_dataset_preprocessed.csv`. You can also find the preprocessed dataset
    on this book''s GitHub repository at [https://packt.live/2UQIthA](https://packt.live/2UQIthA).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一章节的预处理数据集开始本活动：`census_income_dataset_preprocessed.csv`。你也可以在本书的GitHub仓库找到该预处理数据集：[https://packt.live/2UQIthA](https://packt.live/2UQIthA)。
- en: 'Perform the following steps to complete this activity:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来完成此活动：
- en: Import all the elements required to load and split a dataset, train an MLP,
    and to measure accuracy.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有加载和划分数据集、训练MLP以及测量准确度所需的元素。
- en: Using the preprocessed Census Income Dataset, separate the features from the
    target, creating the variables `X` and `Y`.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预处理的收入数据集，将特征与目标变量分开，创建`X`和`Y`变量。
- en: Divide the dataset into training, validation, and testing sets, using a split
    ratio of 10%.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集、验证集和测试集，使用10%的划分比例。
- en: Note
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Remember to continue using a `random_state` argument equal to `101` when performing
    the dataset split in order to set a seed to arrive at the same results as the
    ones in this book.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记得在进行数据集划分时继续使用`random_state`参数等于`101`，以便设置种子并获得与本书中相同的结果。
- en: Instantiate the `MLPClassifier` class from scikit-learn and train the model
    with the training data.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn中实例化`MLPClassifier`类，并使用训练数据训练模型。
- en: Leave all the hyperparameters at their default values. Again, use a `random_state`
    equal to 101.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将所有超参数保持为默认值。再次使用`random_state`等于101。
- en: Although a warning will appear specifying that, with the given iterations, no
    convergence was reached, leave the warning unaddressed, since hyperparameter fine-tuning
    will be explored in the following sections of this chapter.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管会出现警告，提示在给定的迭代次数下未达到收敛，但无需处理该警告，因为超参数微调将在本章的后续部分进行探讨。
- en: Calculate the accuracy of the model for all three sets (training, validation,
    and testing).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在所有三组（训练集、验证集和测试集）上的准确度。
- en: Note
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 240.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第240页找到。
- en: 'The accuracy score for the three sets should be as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 三组的准确度应如下所示：
- en: Train sets = 0.8465
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练集 = 0.8465
- en: Dev sets = 0.8246
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开发集 = 0.8246
- en: Test sets = 0.8415
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试集 = 0.8415
- en: Performance Analysis
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能分析
- en: In the following section, we will first perform error analysis using the accuracy
    metric as a tool to determine the condition that is affecting (in greater proportion)
    the performance of the algorithm. Once the model is diagnosed, the hyperparameters
    can be tuned to improve the overall performance of the algorithm. The final model
    will be compared to those that were created during the previous chapter in order
    to determine whether a neural network outperforms the other models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将首先进行错误分析，使用准确度指标作为工具，确定影响算法性能的主要因素。一旦诊断出模型问题，就可以调整超参数以提升算法的整体性能。最终模型将与上一章节中创建的模型进行比较，以确定神经网络是否优于其他模型。
- en: Error Analysis
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误分析
- en: 'Using the accuracy score calculated in *Activity 5.01*, *Training an MLP for
    Our Census Income Dataset*, we can calculate the error rates for each of the sets
    and compare them against one another to diagnose the condition that is affecting
    the model. To do so, a Bayes error equal to 1% will be assumed, considering that
    other models in the previous chapter were able to achieve an accuracy level of
    over 97%:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在*活动 5.01*中计算的准确率，即*为我们的普查收入数据集训练 MLP*，我们可以计算每个数据集的误差率并进行比较，从而诊断影响模型的条件。为此，将假定贝叶斯误差为
    1%，因为前一章中的其他模型已能实现超过 97% 的准确率：
- en: '![Figure 5.9: Accuracy score and error rate of the network'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：网络的准确率和误差率'
- en: '](img/B15781_05_09.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_09.jpg)'
- en: 'Figure 5.9: Accuracy score and error rate of the network'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：网络的准确率和误差率
- en: Note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Considering *Figure 5.9*, remember that in order to detect the condition that
    is affecting the network, it is necessary to take an error rate and, from that,
    subtract the value of the error rate above it. The biggest positive difference
    is the one that we use to diagnose the model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到*图 5.9*，记住为了检测影响网络的条件，需要取一个误差率，并从中减去上面的误差率。最大的正差异就是我们用来诊断模型的差异。
- en: According to the column of differences, it is evident that the biggest difference
    is found between the error rate in the training set and the Bayes error. Based
    on this, it is possible to conclude that the model is suffering from *high bias*,
    which, as explained in previous chapters, can be handled by training a bigger
    network and/or training for longer periods of time (a higher number of iterations).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 根据差异列，显而易见训练集中的误差率与贝叶斯误差之间存在最大差异。基于这一点，可以得出结论，模型正遭遇*高偏差*，正如前几章所解释的那样，可以通过训练更大的网络和/或进行更长时间的训练（增加迭代次数）来解决这一问题。
- en: Hyperparameter Fine-Tuning
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数微调
- en: Through error analysis, it was possible to determine that the network is suffering
    from high bias. This is highly important as it indicates the actions that need
    to be taken in order to improve the performance of the model in greater proportion.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过误差分析，我们确定了网络存在高偏差。这非常重要，因为它表明了需要采取的行动，以便更大幅度地提高模型的性能。
- en: 'Considering that both the number of iterations and the size of the network
    (number of layers and units) should be changed using a trial-and-error approach,
    the following experiments will be performed:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到迭代次数和网络大小（层数和单元数）应使用试错法进行调整，以下实验将会进行：
- en: '![Figure 5.10: Suggested experiments to tune the hyperparameters'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10：调整超参数的建议实验'
- en: '](img/B15781_05_10.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_10.jpg)'
- en: 'Figure 5.10: Suggested experiments to tune the hyperparameters'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：调整超参数的建议实验
- en: Note
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Some experiments may take longer to run due to their complexity. For instance,
    Experiment 3 will take longer than Experiment 2.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实验可能需要更长时间来运行，因为它们较为复杂。例如，实验 3 比实验 2 花费的时间更长。
- en: The idea behind these experiments is to be able to test different values for
    the different hyperparameters in order to find out whether an improvement can
    be achieved. If the improvements achieved through these experiments are significant,
    further experiments should be considered.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的目的是测试不同超参数的不同值，以便找出是否能取得改进。如果这些实验所获得的改进显著，应考虑进行进一步的实验。
- en: 'Similar to adding the `random_state` argument to the initialization of the
    MLP, the change in the values of the number of iterations and the size of the
    network can be achieved using the following code, which shows the values for Experiment
    3:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与在 MLP 初始化中添加 `random_state` 参数类似，迭代次数和网络大小的变化可以通过以下代码实现，该代码显示了实验 3 的值：
- en: '[PRE9]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To find what term to use in order to change each hyperparameter, visit scikit-learn's
    `MLPClassifier` page at [http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 若要找出调整每个超参数的术语，请访问 scikit-learn 的 `MLPClassifier` 页面：[http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)。
- en: As you can see in the preceding snippet, the `max_iter` argument is used to
    set the number of iterations to run during the training of the network. The `hidden_layer_sizes`
    argument is used to both set the number of hidden layers and set the number of
    units in each. For instance, in the preceding example, by setting the argument
    to `(100,100,100)`, the architecture of the network is of 3 hidden layers, each
    with 100 units. Of course, this architecture also includes the required input
    and output layers.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在上面的片段中所看到的，`max_iter`参数用于设置网络训练期间运行的迭代次数。`hidden_layer_sizes`参数用于设置隐藏层的数量和每个隐藏层的单元数。例如，在上面的示例中，通过将参数设置为`(100,100,100)`，网络的架构为3个隐藏层，每个隐藏层有100个单元。当然，这种架构还包括所需的输入和输出层。
- en: Note
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Using the example to train a network with the configurations of Experiment 3,
    you are encouraged to try to execute the training process for the configurations
    of Experiment 1 and 2.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实验3的配置来训练网络的示例，鼓励您尝试执行实验1和2的配置的训练过程。
- en: 'The accuracy scores from running the preceding experiments can be seen in the
    following table:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从运行上述实验中得到的准确度分数如下表所示：
- en: '![Figure 5.11: Accuracy scores for all experiments'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.11：所有实验的准确度分数'
- en: '](img/B15781_05_11.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_11.jpg)'
- en: 'Figure 5.11: Accuracy scores for all experiments'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11：所有实验的准确度分数
- en: Note
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Keep in mind that the main purpose behind tuning the hyperparameters is to decrease
    the difference between the error rate of the training set and the Bayes error,
    which is why most of the analysis is done by considering only this value.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，调整超参数的主要目的是减少训练集的误差率与贝叶斯误差之间的差异，这就是为什么大部分分析只考虑这个值。
- en: Through an analysis of the accuracy scores of the experiments, it can be concluded
    that the best configuration of hyperparameters is the one used during Experiment
    2\. Additionally, it is possible to conclude that there is most likely no point
    in trying other values for the number of iterations, considering that increasing
    the number of iterations did not have a positive effect on the performance of
    the algorithm.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析实验的准确度分数，可以得出结论，最佳的超参数配置是实验2中使用的配置。此外，可以得出结论，增加迭代次数对算法性能没有积极影响，因此很可能没有必要尝试其他迭代次数的值。
- en: 'Nonetheless, in order to test the width of the hidden layers, the following
    experiments will be considered, using the selected values for the number of iterations
    and the number of hidden layers of Experiment 2, but varying the number of units
    in each layer:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管为了测试隐藏层的宽度，将考虑以下实验，使用实验2中选择的迭代次数和隐藏层数量的值，但会改变每层的单元数：
- en: '![Figure 5.12: Suggested experiments to vary the width of the network'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.12：建议的实验以改变网络宽度'
- en: '](img/B15781_05_12.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_12.jpg)'
- en: 'Figure 5.12: Suggested experiments to vary the width of the network'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：建议的实验以改变网络宽度
- en: 'The accuracy score of the two experiments is shown, followed by an explanation
    of the logic behind them:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 展示了两个实验的准确度分数，随后解释了它们背后的逻辑：
- en: '![Figure 5.13: Accuracy scores for the second round of experiments'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.13：第二轮实验的准确度分数'
- en: '](img/B15781_05_13.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_05_13.jpg)'
- en: 'Figure 5.13: Accuracy scores for the second round of experiments'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：第二轮实验的准确度分数
- en: It can be seen that the accuracy for both experiments decreases for all sets
    of data, in comparison to the initial model. By observing these values, it can
    be concluded that the performance of Experiment 2 is the highest in terms of testing
    sets, which leaves us with a network that iterates for 500 steps, with one input
    and output layer and two hidden layers with 100 units each.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，与初始模型相比，所有数据集的两个实验的准确度都有所下降。通过观察这些数值，可以得出结论，实验2在测试集方面的性能最佳，这使我们得到一个迭代500步的网络，具有一个输入和输出层以及两个每个有100个单元的隐藏层。
- en: Note
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is no ideal way to test the different configurations of hyperparameters.
    The only important thing to consider is that the focus is centered on those hyperparameters
    that solve the condition that is affecting the network in a greater proportion.
    Feel free to try more experiments if you wish.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理想的方法来测试超参数的不同配置。唯一需要考虑的重要事项是，重点放在那些解决影响网络的条件的超参数上。如果愿意，可以尝试更多实验。
- en: Considering the accuracy scores of all three sets of Experiment 2 to calculate
    the error rate, the biggest difference is still between the training set error
    and the Bayes error. This means that the model may not be the best fit for the
    dataset, considering that the training set error could not be brought closer to
    the minimum possible error margin.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑实验 2 的三个数据集的准确率得分来计算误差率，最大的差异仍然是在训练集误差和贝叶斯误差之间。这意味着考虑到训练集误差无法接近最小可能误差边际，该模型可能并不最适合该数据集。
- en: Note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3e2O8bS](https://packt.live/3e2O8bS).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3e2O8bS](https://packt.live/3e2O8bS)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需要在本地运行。
- en: Model Comparison
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型比较
- en: When more than one model has been trained, the final step related to the process
    of creating a model is a comparison between the models in order to choose the
    one that best represents the training data in a generalized way, so that it works
    well over unseen data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练了多个模型时，创建模型过程的最后一步是对模型进行比较，以选择最能以一种泛化方式代表训练数据的模型，从而能在未见数据上表现良好。
- en: The comparison, as mentioned previously, must be done by using only the metric
    that was selected to measure the performance of the models for the data problem.
    This is important, considering that one model can perform very differently for
    each metric, so the model that maximizes the performance with the ideal metric
    should be selected.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，比较必须仅使用选择的度量标准来衡量模型在数据问题上的表现。这一点非常重要，因为一个模型在每个度量标准上的表现可能会大相径庭，因此应选择在理想度量标准下最大化表现的模型。
- en: Although the metric is calculated on all three sets of data (training, validation,
    and testing) in order to be able to perform error analysis, for most cases, comparison
    and selection should be done by prioritizing the results obtained with the testing
    set. This is mainly due to the purpose of the sets, considering that the training
    set is used to create the model, the validation set is used to fine-tune the hyperparameters,
    and finally, the testing set is used to measure the overall performance of the
    model on unseen data.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该度量标准是在所有三个数据集（训练集、验证集和测试集）上计算的，以便能够进行误差分析，但在大多数情况下，比较和选择应优先考虑使用测试集获得的结果。这主要是因为各数据集的目的不同，训练集用于创建模型，验证集用于微调超参数，最终，测试集用于衡量模型在未见数据上的整体表现。
- en: Taking this into account, the model with a superior performance on the testing
    set, after having improved all models to their fullest potential, will be the
    one that performs best on unseen data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，在对所有模型进行充分优化后，测试集上表现最优的模型将在未见数据上表现最佳。
- en: 'Activity 5.02: Comparing Different Models to Choose the Best Fit for the Census
    Income Data Problem'
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.02：比较不同模型以选择最适合人口收入数据问题的模型
- en: 'Consider the following scenario: after training four different models with
    the available data, you have been asked to perform an analysis to choose the model
    that best suits the case study.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下场景：在使用可用数据训练四个不同的模型后，你被要求进行分析以选择最适合案例研究的模型。
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following activity is mainly analytical. Use the results obtained from the
    activities in the previous chapter, as well as the activity in the current chapter.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下活动主要是分析性的。请使用上一章活动中获得的结果，以及本章中的活动。
- en: 'Perform the following steps to compare the different models:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来比较不同的模型：
- en: Open the Jupyter Notebooks that you used to train the models.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你用于训练模型的 Jupyter Notebook。
- en: 'Compare the four models, based only on their accuracy scores. Fill in the details
    in the following table:![Figure 5.14: Accuracy scores of all four models for the
    Census Income Dataset'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅根据准确率得分比较四个模型。请在下表中填写详细信息：![图 5.14：所有四个模型在人口收入数据集上的准确率得分
- en: '](img/B15781_05_14.jpg)'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_05_14.jpg)'
- en: 'Figure 5.14: Accuracy scores of all four models for the Census Income Dataset'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.14：所有四个模型在人口收入数据集上的准确率得分
- en: On the basis of the accuracy scores, identify the model with the best performance.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据准确率得分，识别表现最好的模型。
- en: Note
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 242.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 242 页找到。
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter mainly focused on ANNs (the MLP, in particular), which have become
    increasingly important in the field of machine learning due to their ability to
    tackle highly complex data problems that usually use extremely large datasets
    with patterns that are impossible to see with the human eye.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要聚焦于人工神经网络（特别是多层感知器，MLP），它们在机器学习领域变得越来越重要，因为它们能够处理高度复杂的数据问题，这些问题通常需要使用极其庞大的数据集，并且这些数据集的模式是肉眼无法识别的。
- en: The main objective is to emulate the architecture of the human brain by using
    mathematical functions to process data. The process that is used to train an ANN
    consists of a forward propagation step, the calculation of a cost function, a
    backpropagation step, and the updating of the different weights and biases that
    help to map the input values to an output.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 主要目标是通过使用数学函数来模拟人脑的结构，以处理数据。训练人工神经网络的过程包括前向传播步骤、成本函数的计算、反向传播步骤以及更新不同的权重和偏置，这些权重和偏置帮助将输入值映射到输出。
- en: In addition to the variables of the weights and biases, ANNs have multiple hyperparameters
    that can be tuned to improve the performance of the network, which can be done
    by modifying the architecture or training process of the algorithm. Some of the
    most popular hyperparameters are the size of the network (in terms of hidden layers
    and units), the number of iterations, the regularization term, the batch size,
    and the learning rate.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权重和偏置的变量外，人工神经网络还有多个可以调整的超参数，以改善网络的性能。这可以通过修改算法的架构或训练过程来实现。一些最常见的超参数包括网络的大小（隐藏层和单元的数量）、迭代次数、正则化项、批量大小和学习率。
- en: Once these concepts were covered, we created a simple network to tackle the
    Census Income Dataset problem that was introduced in the previous chapter. Next,
    by performing error analysis, we fine-tuned some of the hyperparameters of the
    network to improve its performance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这些概念被讲解完毕，我们就创建了一个简单的网络来解决上一章介绍的“人口普查收入数据集”问题。接下来，通过执行误差分析，我们微调了网络的一些超参数，以提高其性能。
- en: In the next chapter, we will learn how to develop an end-to-end machine learning
    solution, starting from the understanding of the data and training of the model,
    as seen thus far, and ending with the process of saving a trained model in order
    to be able to make future use of it.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何开发一个端到端的机器学习解决方案，从理解数据和训练模型开始（如前所述），最终到保存训练好的模型，以便将来可以再次使用它。
