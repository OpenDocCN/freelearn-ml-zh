- en: Section 5 – Clustering and Dimensionality Reduction with Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5节 – 使用无监督学习进行聚类和降维
- en: The last two chapters of this book examines unsupervised learning models. These
    are models where there is no target to predict. Even without a target there are
    many insights that can be gleaned from our data. Dimension reduction with principal
    component analysis (PCA) allows us to capture the variance of our features with
    fewer components than the original number of features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的后两章探讨了无监督学习模型。这些模型中没有预测的目标。即使没有目标，我们也可以从数据中获得许多见解。使用主成分分析（PCA）进行降维可以让我们用比原始特征数量更少的组件来捕捉特征的变化。
- en: The components created with PCA can be used for visualizations, or to identify
    processes that are important but cannot really be captured well by each feature.
    PCA can also be used when we need to reduce the feature space in a supervised
    learning model. We will demonstrate how to create and evaluate a PCA in the next
    chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用主成分分析（PCA）创建的组件可以用于可视化，或者用于识别那些虽然重要但无法被每个特征很好地捕捉到的过程。当我们需要在监督学习模型中减少特征空间时，也可以使用PCA。我们将在下一章中演示如何创建和评估PCA。
- en: Clustering helps us group instances by those which have more in common with
    each other than with those in any other group. This often reveals relationships
    that are not otherwise obvious. We look at two popular clustering algorithms,
    K-means and DBSCAN, in this chapter. Clustering works well when we are able to
    find the right hyperparameter values for our model -- the number of clusters (k)
    for k-means, and the value of epsilon for DBSCAN, which determines the size of
    the radius around core instances in a cluster. We will go over choosing the best
    hyperparameter values for these clustering algorithms in the final chapter of
    this book.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类帮助我们根据实例之间的相似性将实例分组，这些实例与其他任何组中的实例相比有更多的共同点。这通常揭示了其他情况下不明显的关系。在本章中，我们将探讨两种流行的聚类算法，K-means和DBSCAN。当我们能够找到适合我们模型的正确超参数值时，聚类效果会很好——对于k-means是簇的数量（k），对于DBSCAN是epsilon的值，它决定了簇中核心实例周围半径的大小。我们将在本书的最后一章中讨论如何为这些聚类算法选择最佳的超参数值。
- en: 'This section comprises the following chapters:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括以下章节：
- en: '[*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170), *Principal Component
    Analysis*'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第15章*](B17978_15_ePub.xhtml#_idTextAnchor170)，*主成分分析*'
- en: '[*Chapter 16*](B17978_16_ePub.xhtml#_idTextAnchor177), *K-Means and DBSCAN
    Clustering*'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第16章*](B17978_16_ePub.xhtml#_idTextAnchor177)，*K-Means和DBSCAN聚类*'
