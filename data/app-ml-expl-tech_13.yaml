- en: '*Chapter 10*: XAI Industry Best Practices'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：XAI行业最佳实践'
- en: In the first section of this book, we discussed various concepts related to
    **Explainable AI** (**XAI**). These concepts were established through years of
    research, considering various application domains of **artificial intelligence**
    (**AI**). However, the need for XAI for industrial applications has been felt
    very recently as AI adoption in industrial use cases is increasing. Unfortunately,
    the general awareness of XAI for industrial use cases is still lacking due to
    certain challenges and gaps in how to implement human-friendly explainability
    methods.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一部分，我们讨论了与**可解释人工智能**（**XAI**）相关的各种概念。这些概念是通过多年的研究建立的，考虑了人工智能的多种应用领域。然而，随着人工智能在工业用例中的采用增加，最近才感觉到对XAI在工业应用中的需求。不幸的是，由于某些挑战和如何实施人性化的可解释性方法的差距，XAI在工业用例中的普遍认识仍然不足。
- en: In *Section 2*, *Practical Problem Solving*, we covered many XAI Python frameworks
    that are popularly used for interpreting the working of **machine learning** (**ML**)
    models. However, only understanding how to apply the XAI Python frameworks in
    practice is not sufficient for industrial problems. Industrial problems require
    solutions that are scalable and sustainable. So, it is very important for us to
    discuss the best practices of XAI for scalable and sustainable AI solutions used
    for industrial problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2节*，*实际问题解决*中，我们介绍了许多流行的XAI Python框架，这些框架通常用于解释**机器学习**（**ML**）模型的工作原理。然而，仅仅了解如何在实践中应用XAI
    Python框架是不够的，工业问题需要可扩展和可持续的解决方案。因此，对于我们来说，讨论XAI在可扩展和可持续的AI解决方案中的最佳实践非常重要，这些解决方案用于解决工业问题。
- en: Over the years, XAI has evolved a lot. From being a topic of academic research,
    XAI is now a powerful tool in the toolkit of AI and ML industrial practitioners.
    However, XAI has many open challenges on which the research community is still
    working to bring AI closer to end users. So, we will discuss the existing challenges
    of XAI and the general recommendations for designing an explainable ML system
    while considering the open challenges. Also, the quality of AI/ML systems is as
    good as the quality of the underlying data. Therefore, we will also focus on the
    importance of adopting a data-first approach for model explainability.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，XAI已经发展了很多。从学术研究的主题，XAI现在已成为AI和ML工业实践者工具箱中的强大工具。然而，XAI在许多开放挑战上仍然存在，研究社区仍在努力工作，以使AI更接近最终用户。因此，我们将讨论XAI的现有挑战和设计可解释ML系统的通用建议，同时考虑开放挑战。此外，AI/ML系统的质量与底层数据的质量一样好。因此，我们还将关注采用以数据为先的方法对于模型可解释性的重要性。
- en: The XAI research community believes that XAI is a multi-disciplinary perspective
    that should be centered around the end user. So, we will discuss the concept of
    **interactive machine learning** (**IML**) to create high user engagement for
    industrial AI systems. Finally, we will cover the importance of providing actionable
    suggestions and insights using AI/ML as an approach to decipher the complex nature
    of AI models, thereby making AI explainable and increasing users' trust.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: XAI研究社区认为，XAI是一个多学科视角，应该以最终用户为中心。因此，我们将讨论**交互式机器学习**（**IML**）的概念，以创造高用户参与度的工业AI系统。最后，我们将介绍使用AI/ML作为方法来揭示AI模型复杂性的重要性，从而使AI可解释并增加用户的信任。
- en: 'Unlike the previous chapters, in this chapter, we will not focus on the practical
    applications or learn a new XAI framework. Instead, our goal is to understand
    the best practices of XAI for industrial use cases. So, in this chapter, we are
    going to discuss the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章不同，在本章中，我们不会关注实际应用或学习新的XAI框架。我们的目标是了解XAI在工业用例中的最佳实践。因此，在本章中，我们将讨论以下主题：
- en: Open challenges of XAI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XAI的开放挑战
- en: Guidelines for designing explainable ML systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计可解释机器学习系统的指南
- en: Adopting a data-first approach for explainability
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用以数据为先的方法进行可解释性
- en: Emphasizing IML for explainability
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强调以可解释性为目的的交互式机器学习（IML）
- en: Emphasizing prescriptive insights for explainability
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强调可解释性的规范性见解
- en: So, let's find out more about these topics next.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们进一步了解这些主题。
- en: Open challenges of XAI
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XAI的开放挑战
- en: 'As briefly discussed, there have been some significant advances in the field
    of XAI. XAI is no longer just a topic of academic research; the availability of
    XAI frameworks has made XAI an essential tool for industrial practitioners. But
    are these frameworks sufficient to increase AI adoption? Unfortunately, the answer
    is no. XAI is yet to mature further as there are certain open challenges that,
    once resolved, can significantly bridge the gap between AI and the end user. Let''s
    discuss these open challenges next:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如简要讨论的那样，XAI领域已经取得了一些显著的进展。XAI不再仅仅是学术研究的主题；XAI框架的可用性使得XAI成为工业实践者的重要工具。但这些框架足以增加AI的采用率吗？不幸的是，答案是否定的。XAI还需要进一步成熟，因为有一些开放性的挑战，一旦解决，可以显著缩小AI与最终用户之间的差距。让我们接下来讨论这些开放性的挑战：
- en: '*Shifting focus between the model developer and the end user*: After exploring
    many XAI frameworks throughout this book, you might have also felt that the explainability
    provided by most of the frameworks requires technical knowledge of ML, mathematics,
    or statistics to truly understand the working of the model. This is because the
    explainability methods or algorithms were primarily designed for ML experts or
    model developers.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在模型开发者和最终用户之间转移焦点*：在本书中探索了许多XAI框架之后，你可能也感觉到，大多数框架提供的可解释性需要ML、数学或统计学的技术知识才能真正理解模型的运作。这是因为可解释性方法或算法主要是为ML专家或模型开发者设计的。'
- en: As more and more end users start utilizing AI models and systems, the need for
    non-technical human-friendly explanations is growing. So, for industrial applications,
    dynamically shifting the focus between the model developer and the end user is
    a challenge.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的最终用户开始使用AI模型和系统，对非技术性、人性化的解释的需求正在增长。因此，对于工业应用来说，在模型开发者和最终用户之间动态转移焦点是一个挑战。
- en: To a non-technical end user, a simple explanation method such as feature importance
    visualization can become really complicated unless explicit information is provided.
    In order to mitigate this challenge, the general recommendation is to design user-centric
    AI systems. Similar to any software application or system, the user should be
    involved in the development process early on to understand their requirements
    and include their expertise while designing the application and not post-production
    of the application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非技术性的最终用户来说，除非提供明确的信息，否则像特征重要性可视化这样的简单解释方法可能会变得非常复杂。为了减轻这一挑战，一般的建议是设计以用户为中心的AI系统。类似于任何软件应用或系统，用户应该在开发初期就参与其中，了解他们的需求，并在设计应用时包括他们的专业知识，而不是在应用生产之后。
- en: '*Lack of stakeholder participation*: From the previous point, although the
    recommended action is to involve the end users early on in the development process
    of the AI system, onboarding a stakeholder in the development process can also
    be a challenge. For most industrial use cases, AI solutions are developed in isolation
    without involving the final stakeholder(s). Following the design principles from
    the field of **Human-Computer Interaction** (**HCI**), the user should be involved
    in the loop during the development process.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏利益相关者参与*：从上一个观点来看，尽管建议尽早让最终用户参与AI系统的开发过程，但让利益相关者加入开发过程也可能是一个挑战。对于大多数工业用例，AI解决方案是在不涉及最终利益相关者的情况下独立开发的。根据人机交互（**HCI**）领域的原则，用户应该在开发过程中参与其中。'
- en: Considering high stake domains such as healthcare, finance, legal and regulatory,
    getting stakeholders and domain experts can be an extremely tedious and expensive
    process. The stakeholder's availability can be a challenge. Their interest or
    motivation to participate in the development process can be low even with necessary
    incentives and compensation. Due to these difficulties in onboarding end users
    into the development process, designing a user-centric AI system is difficult.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到高度敏感的领域，如医疗保健、金融、法律和监管，获取利益相关者和领域专家可能是一个极其繁琐且昂贵的流程。利益相关者的可用性可能是一个挑战。即使有必要的激励和补偿，他们的兴趣或参与开发过程的动机也可能很低。由于这些困难，将最终用户纳入开发过程，设计以用户为中心的AI系统变得困难。
- en: The most recommended action to tackle this challenge is through a collaboration
    between industry and academia. Usually, academic institutions such as medical
    schools, law schools, or other universities have broader access to real participants
    or students who belong to the respective fields and can be *pseudo* participants.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 应对这一挑战的最推荐行动是通过行业和学术界的合作。通常，如医学院、法学院或其他大学等学术机构可以更广泛地接触到各自领域的真实参与者或学生，他们可以成为*伪*参与者。
- en: 'The following diagram illustrates how XAI is a multi-disciplinary perspective:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了XAI是一个多学科视角：
- en: '![](img/B18216_10_001.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18216_10_001.jpg)'
- en: Figure 10.1 – XAI is a multi-disciplinary perspective
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – XAI是一个多学科视角
- en: '*Application-specific challenges*: Different application domains need explainability
    of different types. For example, in an AI-based loan approval system, influence-based
    or example-based feature explanations can be really helpful. However, for an application
    to detect COVID-19 infections from X-ray images, highlighting or localizing the
    region of the infection can be more helpful. So, each application can have its
    own requirement and definition of explainability and, thus, any general XAI framework
    might not be very effective.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特定应用的挑战*：不同的应用领域需要不同类型的可解释性。例如，在一个基于人工智能的贷款审批系统中，基于影响或基于示例的特征解释可能非常有帮助。然而，对于从X射线图像中检测COVID-19感染的应用，突出或定位感染区域可能更有帮助。因此，每个应用都可能有其自己的可解释性要求和定义，因此任何通用的XAI框架可能并不非常有效。'
- en: '*Lack of quantitative evaluation metrics*: The quantitative evaluation of explanation
    methods has been an important research topic. Unfortunately, there is still no
    tool or framework that exists that can quantitatively evaluate the quality of
    explanation methods. This is mostly because many diverse AI algorithms are at
    work on different types of data. Consequently, there are many definitions of model
    explainability and many approaches for XAI. So, it is very hard to generalize
    quantitative evaluation metrics that can work with all of the different explanation
    methods.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏定量评估指标*：解释方法的定量评估一直是重要的研究课题。不幸的是，目前还没有工具或框架可以定量评估解释方法的质量。这主要是因为许多不同的AI算法正在处理不同类型的数据。因此，有许多关于模型可解释性的定义和许多XAI的方法。因此，很难概括出适用于所有不同解释方法的定量评估指标。'
- en: 'Currently, qualitative evaluation methods such as *Trust*, *Usefulness*, *Actionability*,
    *Coherence with prior beliefs*, *Impact*, and more are used. To learn more about
    these metrics, take a look at *Understanding Machines: Explainable AI* from *Accenture
    Labs*, which is available at [https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf](https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf).
    Additionally, take a look at *Explanation in Artificial Intelligence: Insights
    from the Social Sciences* from *Tim Miller*, which is available at [https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，正在使用定性评估方法，如*信任*、*实用性*、*可操作性*、*与先验信念的一致性*、*影响*等。要了解更多关于这些指标的信息，请参阅*Accenture
    Labs*的*Understanding Machines: Explainable AI*，可在[https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf](https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf)找到。此外，还可以查看*Explanation
    in Artificial Intelligence: Insights from the Social Sciences*，由*Tim Miller*撰写，可在[https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf)找到。'
- en: The qualitative evaluation methods are, indeed, user-centric and use the principles
    of HCI to collect feedback from the end user, but usually, quantitative metrics
    are more useful when comparing different methods. However, I am hopeful that tools
    such as *Quantus* ([https://github.com/understandable-machine-intelligence-lab/Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)),
    which is used to evaluate explanation methods for neural networks, will mature
    significantly in a few years and it will be easier to evaluate explanation methods.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 定性评估方法确实是用户中心的，并使用人机交互（HCI）的原则从最终用户那里收集反馈，但通常，在比较不同方法时，定量指标更有用。然而，我希望能有像*Quantus*
    ([https://github.com/understandable-machine-intelligence-lab/Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus))这样的工具，它用于评估神经网络解释方法，在几年内会显著成熟，这将使评估解释方法变得更加容易。
- en: '*Lack of actionable explanations*: Most explanation methods don''t provide
    actionable insights to the end user. So, designing explainable AI/ML systems that
    can provide actionable explanations can be challenging. Counterfactual explanations,
    what-if analysis, and interactive visualization-based explanations are the only
    explanation methods that allow the user to observe the change in outcome when
    the input features are altered. I would recommend increasing the usage of these
    actionable explanation methods to develop explainable AI/ML systems.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏可操作的解释*：大多数解释方法不向最终用户提供可操作的见解。因此，设计能够提供可操作解释的可解释AI/ML系统可能具有挑战性。反事实解释、假设分析以及基于交互式可视化的解释是唯一允许用户观察当输入特征改变时结果变化的解释方法。我建议增加这些可操作解释方法的使用，以开发可解释的AI/ML系统。'
- en: '*Lack of contextual explanations*: Any ML algorithm that is deployed in production
    depends on the specific use case and the underlying data. Due to this, there is
    always a trade-off between explainability, model performance, fairness, and privacy.
    So, understanding the context of explainability is an existing challenge that
    any general XAI framework cannot provide accurately. So, the recommendation to
    mitigate this challenge is to design personalized explainable ML systems for a
    specific use case rather than a generalized implementation.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏上下文解释*：任何在生产中部署的机器学习算法都依赖于特定的用例和底层数据。因此，在可解释性、模型性能、公平性和隐私性之间总是存在权衡。因此，理解可解释性的上下文是任何通用XAI框架都无法准确提供的现有挑战。因此，缓解这一挑战的建议是为特定用例设计个性化的可解释机器学习系统，而不是通用的实现。'
- en: 'If you want to explore more in this area, you can take a look at *Verma et
    al.''s* work, *Pitfalls of Explainable ML: An Industry Perspective* ([https://arxiv.org/abs/2106.07758](https://arxiv.org/abs/2106.07758)),
    to learn more about the typical challenges of XAI. All of these open challenges
    are interesting research problems that you can explore to help the research community
    progress in this field. Now that we have discussed the open challenges of XAI,
    next, let''s discuss the guidelines for designing explainable ML systems for industrial
    use cases, considering the open challenges.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在这个领域进行更多探索，可以查看*Verma等人*的工作，《可解释机器学习的陷阱：行业视角》([https://arxiv.org/abs/2106.07758](https://arxiv.org/abs/2106.07758))，了解更多关于XAI典型挑战的信息。所有这些开放挑战都是有趣的研究问题，你可以探索这些问题，以帮助研究界在这个领域取得进步。现在我们已经讨论了XAI的开放挑战，接下来，让我们讨论针对工业用例设计可解释机器学习系统的指南，同时考虑这些开放挑战。
- en: Guidelines for designing explainable ML systems
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计可解释机器学习系统的指南
- en: 'In this section, we will discuss the recommended guidelines for designing an
    explainable ML system from an industry perspective while considering the open
    challenges of XAI, as discussed in the previous section. All of these guidelines
    have been carefully collated from various publications, conference keynotes, and
    panel discussions from various experts in the field of XAI, ML, and software systems.
    It is true that every ML and AI problem is unique in its own way, and so, it is
    hard to generalize any recommendations. But many AI organizations have adopted
    the following list of guidelines for designing explainable and user-friendly ML
    systems:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从行业角度讨论设计可解释机器学习系统的推荐指南，同时考虑上一节中讨论的XAI的开放挑战。所有这些指南都经过精心整理，来自各种出版物、会议主题演讲和来自XAI、机器学习和软件系统领域的专家的圆桌讨论。确实，每个机器学习和人工智能问题都有其独特之处，因此很难泛化任何建议。但许多人工智能组织已经采用了以下列表中的指南来设计可解释且用户友好的机器学习系统：
- en: '*Identify the target audience of XAI and their usability context*: The definition
    of explainability depends on the user using the AI system. *Arrieta et al*., in
    their work *Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities,
    and Challenges toward Responsible AI*, have highlighted the importance of identifying
    the target audience of XAI when designing explainable AI systems.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确定XAI的目标受众及其可用性环境*：可解释性的定义取决于使用AI系统的用户。*Arrieta等人*在他们的工作《可解释人工智能（XAI）：概念、分类、机会和挑战，迈向负责任的人工智能》中强调了在设计可解释AI系统时确定XAI目标受众的重要性。'
- en: An AI system can have different audiences such as technical stakeholders (that
    is, data scientists, ML experts, product owners, and developers), business stakeholders
    (that is, managers and executive leaders), domain experts (that is, doctors, lawyers,
    insurance agents, and more), legal and regulatory agencies, and non-technical
    end users. Every audience might have a different need for explainability, so accordingly,
    the explanation methods should try to address the best needs of the audience.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个AI系统可以拥有不同的受众，例如技术利益相关者（即数据科学家、机器学习专家、产品所有者和开发者）、商业利益相关者（即经理和执行领导者）、领域专家（即医生、律师、保险代理等）、法律和监管机构，以及非技术最终用户。每个受众可能对可解释性有不同的需求，因此，解释方法应尽量满足受众的最佳需求。
- en: As a preliminary step, identifying the target audience of the explainable system
    along with the situation or context in which they are going to use the system
    helps a lot in the design process. For example, for medical experts relying on
    ML models to predict the risk of diabetes, the choice of explanation methods depends
    on their actual needs. If their need is to suggest actions to improve the health
    conditions of diabetic patients, then counterfactual examples can be really useful.
    However, if their purpose is to find out the factors that are leading to the increase
    in the risk of diabetes, then feature-based explanations methods are more relevant.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为初步步骤，确定可解释系统的目标受众以及他们将要使用系统的情境或背景，在设计中非常有帮助。例如，对于依赖机器学习模型预测糖尿病风险的医疗专家，解释方法的选择取决于他们的实际需求。如果他们的需求是建议改善糖尿病患者健康状况的措施，那么反事实示例可能非常有用。然而，如果他们的目的是找出导致糖尿病风险增加的因素，那么基于特征的解释方法更为相关。
- en: '*Figure 10.2* illustrates the various target audience of explainable AI systems:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.2* 展示了可解释AI系统的各种目标受众：'
- en: '![](img/B18216_10_002.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18216_10_002.jpg)'
- en: Figure 10.2 – Identifying the target audience of XAI
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 确定XAI的目标受众
- en: '*Shortlisting the XAI techniques based on the user''s needs*: Once the target
    audience and their usability context have been identified, along with the necessary
    technical details about the type of the dataset (for instance, tabular, images,
    or textual) and the ML algorithm used for training the model, shortlisting a list
    of possible explanation methods, as covered in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, is very important.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*根据用户需求选择XAI技术*：一旦确定了目标受众及其可用性背景，以及关于数据集类型（例如，表格、图像或文本）和用于训练模型的机器学习算法的必要技术细节，根据[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)中所述的*模型可解释性方法*，选择一系列可能的解释方法非常重要。'
- en: These shortlisted explanation methods should fit in with the software system
    that the target users will use to interact with the AI models. This means the
    explanation techniques should be well integrated with the software applications
    or interfaces and should even be considered during the design process of the software
    for a consistent user experience.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预选的解释方法应与目标用户将用于与AI模型交互的软件系统相匹配。这意味着解释技术应与软件应用或界面良好集成，甚至应在软件设计过程中考虑，以确保一致的用户体验。
- en: '*Human-centered XAI: An iterative process of translating and evaluating XAI
    in specific domains involving the end user*. Similar to the design life cycle
    of a software system using HCI, XAI is also an iterative process. It should be
    human-centered and should be evaluated continuously to assess the impact. In the
    *User-centric system design using XAI* section of [*Chapter 11*](B18216_11_ePub.xhtml#_idTextAnchor217), *End
    User-Centered Artificial Intelligence,* I have included other important aspects
    to consider for a human-centered XAI design process.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以人为本的XAI：在特定领域涉及最终用户的翻译和评估XAI的迭代过程*。类似于使用人机交互(HCI)的软件系统设计生命周期，XAI也是一个迭代过程。它应以人为中心，并应持续评估以评估影响。在[*第11章*](B18216_11_ePub.xhtml#_idTextAnchor217)的*使用XAI进行以用户为中心的系统设计*部分，*以最终用户为中心的人工智能*中，我包括了以人为本的XAI设计过程中需要考虑的其他重要方面。'
- en: '*The importance of the feedback loop in XAI*: All explainable AI systems should
    have the option to capture the end user''s feedback to assess the impact, relevance,
    effectiveness, and trust of the explanations provided by the system. It is never
    possible to consider all edge cases and all preferences of the end users during
    the design and the initial development process. But using the feedback loop, developers
    can collect specific feedback about the explanation methods and modify them if
    needed.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*XAI中反馈循环的重要性*：所有可解释人工智能系统都应该有捕捉最终用户反馈的选项，以评估系统提供的解释的影响、相关性、有效性和可信度。在设计以及初始开发过程中，永远不可能考虑所有边缘情况和最终用户的所有偏好。但是，通过使用反馈循环，开发者可以收集关于解释方法的特定反馈，并在必要时对其进行修改。'
- en: '*The importance of scalability in the design process*: Similar to serving ML
    models for production systems, explainability should also be served in modular
    and scalable approaches. The best way to serve model explanations is by designing
    **scalable web APIs** to be deployed in centralized cloud servers. So, when XAI
    is implemented in practice, do make sure that the explanations are being served
    through web APIs so that they can be easily integrated with any software interface
    or application.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*设计过程中可扩展性的重要性*：与为生产系统提供机器学习模型类似，可解释性也应该以模块化和可扩展的方式提供。提供模型解释的最佳方式是通过设计**可扩展的Web
    API**，以便在集中式云服务器上部署。因此，当XAI在实践中得到实施时，务必确保解释是通过Web API提供的，这样它们就可以轻松地集成到任何软件界面或应用程序中。'
- en: '*Toggling between the data, the interface, and actionable insights*: It has
    been observed by many experts that, for end users, their satisfaction with the
    model explanation method is a trade-off between how well the explanation is being
    connected to the underlying dataset (or their prior beliefs), how the users are
    able to interact with the ML system to gain more confidence in it, and how well
    the explanations encourage them to take actions to get their desired output. **Data-centric
    XAI**, **IML**, and **actionable explanations** are broader research topics that
    should be considered when designing the explainable AI system for industrial use
    cases.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在数据、界面和可操作见解之间切换*：许多专家观察到，对于最终用户来说，他们对模型解释方法的满意度是在以下方面之间的权衡：解释如何与底层数据集（或他们的先验信念）相连接，用户如何与机器学习系统互动以增强对其的信心，以及解释如何鼓励他们采取行动以获得期望的输出。**数据为中心的可解释XAI**、**IML**和**可操作解释**是设计工业用例的可解释人工智能系统时应考虑的更广泛的研究主题。'
- en: So, we have learned about the open challenges of XAI and discussed the design
    guidelines considering the open challenges. We now have a fair idea of what to
    consider when designing explainable ML systems. Next, let's elaborate on the last
    recommended guideline in the upcoming sections to carefully understand why it
    is important. Let's start our discussion with the importance of using a data-centric
    approach for explainability.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经了解了XAI的开放挑战，并讨论了在设计时考虑这些开放挑战的设计指南。现在，我们对在设计可解释的机器学习系统时需要考虑的内容有了相当的了解。接下来，让我们在接下来的章节中详细阐述最后一条推荐的指南，以仔细理解为什么它很重要。让我们从使用以数据为中心的方法来提高可解释性的重要性开始讨论。
- en: Adopting a data-first approach for explainability
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用以数据为中心的方法来提高可解释性
- en: In [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053), *Data-Centric Approaches*,
    we discussed the importance and various techniques of **Data-Centric XAI**. Now,
    in this section, we will elaborate on how adopting a data-first approach for explainability
    helps in gaining users' trust in industrial use cases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B18216_03_ePub.xhtml#_idTextAnchor053) *数据为中心的方法*中，我们讨论了**数据为中心的可解释XAI**的重要性和各种技术。现在，在本节中，我们将阐述采用以数据为中心的方法来提高可解释性如何有助于在工业用例中获得用户的信任。
- en: Data-centric AI is based on the fundamental idea that *the quality of the ML
    model is as good as the quality of the underlying dataset used for training the
    model*. For industrial use cases, dealing with poor-quality datasets is a major
    challenge for most data scientists. Unfortunately, data quality is often ignored
    as data scientists and ML experts are expected to cast their *magic* of ML to
    build models that are close to 100% accurate. Consequently, ML experts simply
    try to follow **model-centric approaches** such as tuning hyperparameters or using
    complex algorithms to boost model performance. Even if the model performance increases
    slightly, with the increase in complexity, explainability decreases. The lack
    of explainability increases the skepticism of the business stakeholders. Also,
    issues relating to data quality such as the presence of *data anomalies*, *data
    leakage*, *data drift*, and other issues, as discussed in [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053),
    *Data-Centric Approaches*, significantly increase. *In that case, what do we do?*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以数据为中心的人工智能基于这样一个基本理念：*机器学习模型的质量与用于训练模型的底层数据集的质量一样好*。对于工业用例，处理质量差的数据集是大多数数据科学家面临的主要挑战。不幸的是，数据质量通常被忽视，因为数据科学家和机器学习专家被期望施展他们的*魔法*，构建接近100%准确的模型。因此，机器学习专家简单地尝试遵循**以模型为中心的方法**，例如调整超参数或使用复杂算法来提高模型性能。即使模型性能略有提高，随着复杂性的增加，可解释性会降低。可解释性的缺乏增加了业务利益相关者的怀疑。此外，与数据质量相关的问题，如*数据异常*、*数据泄露*、*数据漂移*和其他问题，如在第3章“以数据为中心的方法”中讨论的那样，显著增加。*在这种情况下，我们该怎么办呢？*
- en: The answer is to adopt a data-centric approach to explain the ML process. Using
    data-centric explainability methods such as **exploratory data analysis** (**EDA**),
    we can extract insights about the dataset such as any interesting patterns, correlations,
    monotonicity, or trends from the features used in the dataset. EDA and data analysis
    between the training data and the inference data also helps you to identify data
    quality issues. If there are issues in the dataset, it is always recommended that
    you inform the business stakeholder about the limitations of poor data quality
    and set the expectations correctly about the model performance. So, even if the
    model predictions are not correct, the business stakeholder will understand the
    limitations instead of doubting the ML system.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案是采用以数据为中心的方法来解释机器学习过程。使用以数据为中心的可解释性方法，例如**探索性数据分析**（**EDA**），我们可以从数据集中提取有关数据集的见解，例如任何有趣的模式、相关性、单调性或趋势，这些都是在数据集中使用的特征。在训练数据和推理数据之间的EDA和数据分析也有助于您识别数据质量问题。如果数据集中存在问题，始终建议您向业务利益相关者告知数据质量差的局限性，并正确设置关于模型性能的期望。因此，即使模型预测不正确，业务利益相关者也会理解局限性，而不是怀疑机器学习系统。
- en: But *why don't we try out the other XAI frameworks and methods covered throughout
    this book*? *How would adopting a data-first approach for explainability help*?
    Well, you can and you should try out other relevant XAI methods if applicable,
    but data-centric explainability is always easier to explain to a non-technical
    user. Especially, with the *data profiling method*, as discussed in [*Chapter
    3*](B18216_03_ePub.xhtml#_idTextAnchor053), *Data-Centric Approaches*, we can
    identify the range of values of features present in the dataset for each category
    (if there is a classification problem) or each bin of the prediction variable
    (if there is a regression problem) and compare the model predictions with the
    profiled values. Simple comparisons with the profiled values are easier to understand
    as compared to complicated mathematical concepts such as *Shapley values* or other
    algorithms used in XAI frameworks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们为什么不尝试一下本书中涵盖的其他XAI框架和方法呢？采用以数据为中心的可解释性方法会有什么帮助呢？好吧，如果您适用，您可以尝试其他相关XAI方法，但以数据为中心的可解释性总是更容易向非技术用户解释。特别是，通过讨论在[*第3章*](B18216_03_ePub.xhtml#_idTextAnchor053)“以数据为中心的方法”中的*数据概要方法*，我们可以确定数据集中每个类别（如果有分类问题）或预测变量的每个区间（如果有回归问题）中特征的值范围，并将模型预测与概要值进行比较。与复杂的数学概念（如*Shapley值*）或其他在XAI框架中使用的算法相比，与概要值的简单比较更容易理解。
- en: Another reason why data-centric approaches are preferable is because of the
    user's trust in historical data. Generally, it is observed that most stakeholders
    have more trust in historical data as compared to AI models. For example, in the
    spring season, if an AI weather forecasting model predicts the occurrence of snowfall,
    most end users would be hesitant to trust the prediction. That's because spring
    is always associated with sunshine and flowers blooming due to the observations
    throughout the world over many years. But if the model also indicates the occurrence
    of snowfall in the last few years during the same time or even indicates that
    there was snowfall in close proximity in the last few days, the user's trust would
    be greater. So, it is recommended that you, first, explore data-centric explainability
    and then look at other explainability methods for any industrial ML problems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据为中心的方法更可取的另一个原因是用户对历史数据的信任。一般来说，观察到大多数利益相关者对历史数据的信任比对AI模型的信任更高。例如，在春季，如果AI天气预报模型预测降雪，大多数最终用户可能会犹豫是否相信预测。这是因为春季总是与阳光和花朵盛开联系在一起，这是由于多年来全球的观察。但如果模型也表明在过去的几年里同一时间发生了降雪，或者甚至表明在过去的几天里附近有降雪，用户的信任就会更大。因此，建议您首先探索以数据为中心的可解释性，然后查看其他可解释性方法来解决任何工业机器学习问题。
- en: 'The following diagram illustrates how data-centric XAI can be very close to
    the natural ways of providing explainability, thereby improving the ease of understanding:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表说明了数据为中心的XAI如何非常接近提供可解释性的自然方式，从而提高理解的便捷性：
- en: '![Figure 10.3 – The importance of a data-centric approach for explainability'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 以数据为中心的方法对于可解释性的重要性]'
- en: '](img/B18216_10_003.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片/B18216_10_003.jpg]'
- en: Figure 10.3 – The importance of a data-centric approach for explainability
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 以数据为中心的方法对于可解释性的重要性
- en: Next, we will discuss interactive ML to boost the end user's trust.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论交互式机器学习来增强最终用户的信任。
- en: Emphasizing IML for explainability
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强调以可解释性为中心的IML
- en: IML is the paradigm of designing intelligent user interfaces to facilitate ML
    and AI algorithms with the help of user interactions. Using IML to steer the usage
    of ML systems to increase the trust of the end user has been an important research
    topic for the AI and HCI research community over the last few years. Many works
    of research literature recommend using IML to increase user engagement for AI
    systems. *Recent Research Advances on Interactive Machine Learning* by *Jiang
    et al*. ([https://arxiv.org/abs/1811.04548](https://arxiv.org/abs/1811.04548))
    talks about some of the significant progress that has been made in the field of
    IML and how it is closely associated with the increasing trust and transparency
    of ML algorithms.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: IML是设计智能用户界面的范例，通过用户交互来促进机器学习和AI算法。在过去几年中，使用IML引导ML系统使用，以增加最终用户的信任，一直是AI和HCI研究社区的重要研究课题。许多研究文献推荐使用IML来增加AI系统的用户参与度。“交互式机器学习的最新研究进展”由Jiang等人撰写。（[https://arxiv.org/abs/1811.04548](https://arxiv.org/abs/1811.04548)）讨论了在IML领域取得的重大进展以及它与ML算法的信任和透明度增加的紧密关联。
- en: 'IML is another interesting approach that is used by the XAI community to explain
    ML models. Even in frameworks such as *DALEX* and *Explainerdashboards*, as covered
    in [*Chapter 9*](B18216_09_ePub.xhtml#_idTextAnchor172), *Other Popular XAI Frameworks*,
    providing interactive dashboards and web interfaces that end users can interact
    with to explore the data, model, and predictions are considered as a way for model
    explainability. IML helps the user in the following ways:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: IML是XAI社区用来解释机器学习的另一种有趣的方法。即使在*DALEX*和*Explainerdashboards*等框架中，如在第9章[*其他流行的XAI框架*](B18216_09_ePub.xhtml#_idTextAnchor172)中所述，提供交互式仪表板和用户可以与之交互的Web界面，也被视为模型可解释性的方式。IML以以下方式帮助用户：
- en: Explore the dataset through graphs and visuals, thereby making it easier for
    the user to observe and remember key insights from the data.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过图表和视觉探索数据集，从而使用户更容易观察和记住数据中的关键见解。
- en: Gain more confidence about the ML systems, as the intelligent user interfaces
    allow the user to make changes and observe the outcome. It makes it easier for
    the user to figure out how the model behaves while considering any changes in
    input.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对ML系统更有信心，因为智能用户界面允许用户进行更改并观察结果。这使得用户在考虑任何输入更改时更容易了解模型的行为。
- en: Typically, what-if analysis and local explainability are improved when interactive
    interfaces are provided.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，当提供交互式界面时，假设分析和局部可解释性会得到改善。
- en: IML gives more control to the user to explore the system, and IML usually considers
    a user-centric design process for providing customized interfaces tailor-made
    for a specific use case.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互式机器学习（IML）赋予用户更多的控制权来探索系统，IML通常考虑以用户为中心的设计流程，以提供针对特定用例量身定制的界面。
- en: 'In short, IML improves the user experience and, thus, helps to boost the adoption
    of AI models. I would strongly recommend using interactive user interfaces as
    part of explainable ML systems along with serving model explainability using modularized
    web APIs. You can read the following article to find out more about the usefulness
    of IML for business problems: [https://hub.packtpub.com/what-is-interactive-machine-learning/](https://hub.packtpub.com/what-is-interactive-machine-learning/).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，交互式机器学习（IML）改善了用户体验，从而有助于提高AI模型的采用率。我强烈建议将交互式用户界面作为可解释机器学习系统的一部分，同时使用模块化的Web
    API来提供模型的可解释性。您可以阅读以下文章，了解更多关于交互式机器学习（IML）在解决商业问题方面的有用性：[https://hub.packtpub.com/what-is-interactive-machine-learning/](https://hub.packtpub.com/what-is-interactive-machine-learning/)。
- en: 'The following diagram illustrates the difference between conventional ML and
    IML:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了传统机器学习与交互式机器学习之间的差异：
- en: '![Figure 10\. 4 – Comparing conventional ML with IML'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 10.4 – Comparing conventional ML with IML]'
- en: '](img/B18216_10_004.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_10_004.jpg]'
- en: Figure 10\. 4 – Comparing conventional ML with IML
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 比较传统机器学习与交互式机器学习
- en: As you can see from the preceding diagram, using IML, the end user can directly
    interact with the intelligent user interface to get predictions, explanations,
    and insights. Next, let's discuss the importance of prescriptive insights for
    explainable ML systems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的图中所见，使用交互式机器学习（IML），最终用户可以直接与智能用户界面交互，以获取预测、解释和见解。接下来，让我们讨论规范性见解对于可解释机器学习系统的重要性。
- en: Emphasizing prescriptive insights for explainability
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强调规范性见解的可解释性
- en: Prescriptive insight is a popular jargon used in data analysis. It means providing
    actionable recommendations derived from the dataset to achieve the desired outcome.
    It is often considered to be a catalyst in the entire process of data-driven decision-making.
    In the context of XAI, explanation methods such as *counterfactual examples*,
    *data-centric XAI*, and *what-if analysis* are prominently used for providing
    actionable suggestions to the user.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 规范性见解是数据分析中常用的术语。它意味着从数据集中提供可操作的推荐，以实现期望的结果。它通常被认为是在数据驱动决策过程的整个过程中起到催化剂的作用。在XAI的背景下，如**反事实示例**、**数据中心的XAI**和**假设分析**等解释方法被广泛用于向用户提供可操作的建议。
- en: Along with counterfactuals, the concept of **actionable recourse in ML** is
    also used for generating prescriptive insights. **Actionable recourse** is the
    ability of a user to alter the prediction of an ML model by modifying the features
    that are actionable. But *how is it different from counterfactuals?* Actionable
    recourse can be considered to be an extension of the idea of counterfactual examples,
    which uses actionable features instead of all the features present in the dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 除了反事实之外，**机器学习中的可操作回溯**这一概念也被用于生成规范性见解。**可操作回溯**是指用户通过修改可操作的特征来改变机器学习模型的预测的能力。但**它与反事实有何不同呢**？可操作回溯可以被视为反事实示例想法的扩展，它使用可操作的特征而不是数据集中所有现存的特性。
- en: Now, *what do we mean by actionable features?* Considering a practical scenario,
    it is not feasible for us to change all the features present in a dataset in any
    direction to reach the desired outcome. For example, features such as *age*, *gender*,
    and *race* cannot be changed in any direction to obtain the desired output. Unfortunately,
    algorithms used for generating counterfactual examples do not consider the practical
    feasibility of changing a feature.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**我们所说的可操作特征是什么意思呢**？考虑一个实际场景，我们不可能在数据集中改变所有特征以任何方向达到期望的结果。例如，如**年龄**、**性别**和**种族**这样的特征无法在任何方向上改变以获得期望的输出。不幸的是，用于生成反事实示例的算法并没有考虑改变特征的实际可行性。
- en: Let's suppose that an ML model is being used to estimate the risk of diabetes.
    For a diabetic patient, if we want to use counterfactual examples to recommend
    how to reduce the risk of diabetes, it is not practically feasible for the patient
    to reduce their age by 10 years or change their gender to decrease the risk. So,
    these are non-actionable features. Even though theoretically altering these features
    can change the model prediction, it is not practical to change these features.
    Therefore, the concept of actionable recourse is more like a controlled counterfactual
    generation process that is applied to actionable features and considers a practically
    feasible boundary condition for the feature values.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个机器学习模型正在被用来估计糖尿病的风险。对于一个糖尿病患者来说，如果我们想使用反事实示例来推荐如何降低糖尿病的风险，患者实际上不可能通过减少10岁年龄或改变性别来降低风险，这是不切实际的。因此，这些是非操作性的特征。尽管理论上改变这些特征可以改变模型的预测，但实际上改变这些特征是不切实际的。因此，可操作回溯的概念更像是一个应用于可操作特征的受控反事实生成过程，并考虑了特征值的实际可行边界条件。
- en: To generate prescriptive insights, I would recommend that you use actionable
    recourse as it considers the practical feasibility and difficulty of altering
    a feature value to get the desired outcome. You can find more about actionable
    recourse from *Ustun et al.'s* work, *Actionable Recourse in Linear Classification*
    ([https://arxiv.org/abs/1809.06514](https://arxiv.org/abs/1809.06514)), along
    with their GitHub project at [https://github.com/ustunb/actionable-recourse](https://github.com/ustunb/actionable-recourse).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成规范性洞察，我建议你使用可操作回溯，因为它考虑了改变特征值以获得期望结果的实际可行性和难度。你可以从*Ustun等人*的工作中了解更多关于可操作回溯的信息，*线性分类中的可操作回溯*([https://arxiv.org/abs/1809.06514](https://arxiv.org/abs/1809.06514))，以及他们的GitHub项目[https://github.com/ustunb/actionable-recourse](https://github.com/ustunb/actionable-recourse)。
- en: 'But *are prescriptive insights really necessary in XAI*? Well, the answer is
    *yes*! The following list of reasons explains why prescriptive insights are important
    in XAI:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但在XAI中，*规范性洞察真的必要吗*？答案是*是的*！以下列出的原因解释了为什么规范性洞察在XAI中很重要：
- en: Prescriptive insights are actions suggested to the user to get the desired result.
    In most industrial use cases, explainability is incomplete if the user is unaware
    of how to reach their desired outcome.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范性洞察是为用户提供以获得期望结果的建议的行动。在大多数工业用例中，如果用户不知道如何达到他们的期望结果，可解释性是不完整的。
- en: Generating prescriptive insights is a proactive method for explaining the working
    of ML models. That's because it allows the user to take necessary proactive actions
    rather than trusting the passive explanations provided to them.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成规范性洞察是解释机器学习模型工作原理的一种主动方法。这是因为它允许用户采取必要的主动行动，而不是信任他们被动收到的解释。
- en: It increases the user's faith in the system by giving a sense of control over
    the system. Using actionable explanations, the user is empowered to alter the
    model prediction.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过给予用户对系统的控制感，它增加了用户对系统的信任。使用可操作的解释，用户被赋予了改变模型预测的能力。
- en: It increases the ability of business stakeholders for making data-driven decisions
    for the organization.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它增加了企业利益相关者做出数据驱动决策的能力。
- en: 'These are the main reasons why you should always consider generating explanations
    that are actionable when designing explainable AI systems for industrial problems.
    *Figure 10.5* illustrates how prescriptive insights using XAI can provide actionable
    recommendations for the user to get their desired outcome:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是你为什么在设计用于工业问题的可解释人工智能系统时，始终应考虑生成可操作解释的主要原因。*图10.5*展示了使用XAI的规范性洞察如何为用户提供可操作的建议，以获得他们期望的结果：
- en: '![Figure 10.5 – The importance of prescriptive insights for explainability'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5 – 可解释性中规范性洞察的重要性'
- en: '](img/B18216_10_005.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_10_005.jpg)'
- en: Figure 10.5 – The importance of prescriptive insights for explainability
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 可解释性中规范性洞察的重要性
- en: With this, we have arrived at the end of this chapter. Let's summarize the topics
    discussed next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经到达了本章的结尾。让我们总结一下接下来讨论的主题。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on the best practices for designing explainable AI systems
    for industrial problems. In this chapter, we discussed the open challenges of
    XAI and the necessary design guidelines for explainable ML systems, considering
    the open challenges. We also highlighted the importance of considering data-centric
    approaches of explainability, IML, and prescriptive insights for designing explainable
    AI/ML systems.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了为工业问题设计可解释人工智能系统的最佳实践。在本章中，我们讨论了XAI的开放挑战和可解释机器学习系统的必要设计指南，考虑到开放挑战。我们还强调了在设计中考虑以数据为中心的可解释性、IML和规范性洞察的重要性。
- en: If you are a technical expert, architect, or business leader responsible for
    using AI to solve industrial problems, this chapter has helped you to learn some
    of the most important guidelines for designing explainable AI/ML systems considering
    the open challenges in XAI. If you are a researcher in the field of AI or HCI,
    some of the open challenges discussed in the chapter could be interesting research
    topics to consider. Finding solutions to these challenges can lead to significant
    progress in the field of XAI.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一位负责使用人工智能解决工业问题的技术专家、架构师或业务领导者，这一章节帮助你学习了在设计可解释人工智能/机器学习系统时考虑XAI中的开放挑战的一些最重要的指导原则。如果你是人工智能或人机交互领域的学者，章节中讨论的一些开放挑战可能成为值得考虑的研究课题。解决这些挑战可以在XAI领域取得重大进展。
- en: In the next chapter, we will cover the principles of **End User-Centered Artificial
    Intelligence** to bridge the AI-end user gap.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍**以最终用户为中心的人工智能**的原则，以弥合人工智能与最终用户之间的差距。
- en: References
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For additional information about the topics covered in this chapter, please
    refer to the following resources:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解本章涉及主题的更多信息，请参阅以下资源：
- en: '*Pitfalls of Explainable ML: An Industry Perspective*: [https://arxiv.org/abs/2106.07758](https://arxiv.org/abs/2106.07758)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《可解释机器学习的陷阱：行业视角》*：[https://arxiv.org/abs/2106.07758](https://arxiv.org/abs/2106.07758)'
- en: 'The Quantus framework in GitHub: [https://github.com/understandable-machine-intelligence-lab/Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub上的Quantus框架：[https://github.com/understandable-machine-intelligence-lab/Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)
- en: '*Explanation in Artificial Intelligence: Insights from the Social Sciences*:
    [https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《人工智能中的解释：来自社会科学的见解》*：[https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf)'
- en: '*Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network
    Explanations*: [https://arxiv.org/abs/2202.06861](https://arxiv.org/abs/2202.06861)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Quantus：用于神经网络解释负责任的评估的可解释人工智能工具包》*：[https://arxiv.org/abs/2202.06861](https://arxiv.org/abs/2202.06861)'
- en: '*Understanding Machines: Explainable AI* from *Accenture Labs*: [https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf](https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《理解机器：可解释人工智能》*，来自 *Accenture Labs*：[https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf](https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf)'
- en: '*Actionable Recourse in Linear Classification* by *Ustun et al*:[https://arxiv.org/abs/1809.06514](https://arxiv.org/abs/1809.06514)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《线性分类中的可操作救济》*，作者：*Ustun et al*：[https://arxiv.org/abs/1809.06514](https://arxiv.org/abs/1809.06514)'
- en: 'Actionable recourse in ML: [https://github.com/ustunb/actionable-recourse](https://github.com/ustunb/actionable-recourse)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML中的可操作救济：[https://github.com/ustunb/actionable-recourse](https://github.com/ustunb/actionable-recourse)
- en: '*Questioning the AI: Informing Design Practices for Explainable AI User Experiences*
    by *Liao et al*: [https://dl.acm.org/doi/10.1145/3313831.3376590](https://dl.acm.org/doi/10.1145/3313831.3376590)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《质疑人工智能：为可解释人工智能用户体验提供设计实践的信息》*，作者：*Liao et al*：[https://dl.acm.org/doi/10.1145/3313831.3376590](https://dl.acm.org/doi/10.1145/3313831.3376590)'
- en: '*Advances and Open Questions in Explainable AI (XAI): A practical perspective
    from an HCI researcher* by *Q. Vera Liao*: [http://qveraliao.com/aaai_panel.pdf](http://qveraliao.com/aaai_panel.pdf)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《可解释人工智能（XAI）的进展与开放性问题：来自人机交互研究者的实用视角》*，作者：*Q. Vera Liao*：[http://qveraliao.com/aaai_panel.pdf](http://qveraliao.com/aaai_panel.pdf)'
