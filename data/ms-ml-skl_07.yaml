- en: Chapter 7. Dimensionality Reduction with PCA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 PCA 降维
- en: In this chapter, we will discuss a technique for reducing the dimensions of
    data called **Principal Component Analysis** (**PCA**). Dimensionality reduction
    is motivated by several problems. First, it can be used to mitigate problems caused
    by the curse of dimensionality. Second, dimensionality reduction can be used to
    compress data while minimizing the amount of information that is lost. Third,
    understanding the structure of data with hundreds of dimensions can be difficult;
    data with only two or three dimensions can be visualized easily. We will use PCA
    to visualize a high-dimensional dataset in two dimensions, and build a face recognition
    system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论一种叫做**主成分分析**（**PCA**）的降维技术。降维的动机来源于几个问题。首先，它可以用来缓解由维度灾难引发的问题。其次，降维可以在最小化信息丢失的情况下压缩数据。第三，理解具有数百维度的数据结构是困难的；而仅有两三个维度的数据则容易进行可视化。我们将使用PCA在二维空间中可视化一个高维数据集，并构建一个人脸识别系统。
- en: An overview of PCA
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA 概述
- en: Recall from [Chapter 3](ch03.html "Chapter 3. Feature Extraction and Preprocessing"),
    *Feature Extraction and Preprocessing*, that problems involving high-dimensional
    data can be affected by the curse of dimensionality. As the dimensions of a data
    set increases, the number of samples required for an estimator to generalize increases
    exponentially. Acquiring such large data may be infeasible in some applications,
    and learning from large data sets requires more memory and processing power. Furthermore,
    the sparseness of data often increases with its dimensions. It can become more
    difficult to detect similar instances in high-dimensional space as all of the
    instances are similarly sparse.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第3章](ch03.html "第3章：特征提取与预处理")，*特征提取与预处理*，我们知道涉及高维数据的问题可能会受到维度灾难的影响。随着数据集维度的增加，估计器需要的样本数会指数增长。获取如此庞大的数据在某些应用中可能是不可行的，而且从大数据集中学习需要更多的内存和处理能力。此外，随着数据维度的增加，数据的稀疏性通常会增加。在高维空间中，检测相似实例变得更加困难，因为所有实例都呈现稀疏特征。
- en: Principal Component Analysis, also known as the Karhunen-Loeve Transform, is
    a technique used to search for patterns in high-dimensional data. PCA is commonly
    used to explore and visualize high-dimensional data sets. It can also be used
    to compress data, and process data before it is used by another estimator. PCA
    reduces a set of possibly-correlated, high-dimensional variables to a lower-dimensional
    set of linearly uncorrelated synthetic variables called **principal components**.
    The lower-dimensional data will preserve as much of the variance of the original
    data as possible.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析，也叫做卡尔霍嫩-洛夫变换（Karhunen-Loeve Transform），是一种用于在高维数据中寻找模式的技术。PCA 常用于探索和可视化高维数据集。它还可以用来压缩数据，并在数据被另一个估计器使用之前对其进行处理。PCA
    将一组可能相关的高维变量减少到一组低维的、线性不相关的合成变量，称为**主成分**。低维数据将尽可能保留原始数据的方差。
- en: PCA reduces the dimensions of a data set by projecting the data onto a lower-dimensional
    subspace. For example, a two dimensional data set could be reduced by projecting
    the points onto a line; each instance in the data set would then be represented
    by a single value rather than a pair of values. A three-dimensional dataset could
    be reduced to two dimensions by projecting the variables onto a plane. In general,
    an *n*-dimensional dataset can be reduced by projecting the dataset onto a *k*-dimensional
    subspace, where *k* is less than *n*. More formally, PCA can be used to find a
    set of vectors that span a subspace, which minimizes the sum of the squared errors
    of the projected data. This projection will retain the greatest proportion of
    the original data set's variance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 通过将数据投影到一个低维子空间来减少数据集的维度。例如，一个二维数据集可以通过将数据点投影到一条线上来降低维度；数据集中的每个实例将由一个单一的值表示，而不是一对值。一个三维数据集可以通过将变量投影到一个平面上来降低到二维。一般来说，一个*n*维的数据集可以通过将数据集投影到一个*k*维子空间来降低维度，其中*k*小于*n*。更正式地说，PCA
    可以用来找到一组向量，这些向量构成一个子空间，最小化投影数据的平方误差和。这个投影将保留原始数据集方差的最大比例。
- en: 'Imagine that you are a photographer for a gardening supply catalog, and that
    you are tasked with photographing a watering can. The watering can is three-dimensional,
    but the photograph is two-dimensional; you must create a two-dimensional representation
    that describes as much of the watering can as possible. The following are four
    possible pictures that you could use:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一本园艺用品目录的摄影师，负责拍摄一个洒水壶。洒水壶是三维的，但照片是二维的；你必须创造一个二维的表示，尽可能多地描述洒水壶。以下是你可以使用的四张可能的照片：
- en: '![An overview of PCA](img/8365OS_07_01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![PCA概述](img/8365OS_07_01.jpg)'
- en: In the first photograph, the back of the watering can is visible, but the front
    cannot be seen. The second picture is angled to look directly down the spout of
    the watering can; this picture provides information about the front of the can
    that was not visible in the first photograph, but now the handle cannot be seen.
    The height of the watering can cannot be discerned from the bird's eye view of
    the third picture. The fourth picture is the obvious choice for the catalog; the
    watering can's height, top, spout, and handle are all discernible in this image.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一张照片中，可以看到洒水壶的背面，但看不到前面。第二张照片是从洒水壶的喷口正下方拍摄的；这张照片提供了第一张照片中看不到的壶前面的信息，但现在手柄不可见。从第三张图片的鸟瞰视角，无法辨别洒水壶的高度。第四张照片是目录中最明显的选择；这张图片清楚地展示了洒水壶的高度、顶部、喷口和手柄。
- en: 'The motivation of PCA is similar; it can project data in a high-dimensional
    space to a lower-dimensional space that retains as much of the variance as possible.
    PCA rotates the data set to align with its principal components to maximize the
    variance contained within the first several principal components. Assume that
    we have the data set that is plotted in the following figure:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的动机是类似的；它可以将高维空间中的数据投影到一个低维空间，并尽可能保留方差。PCA旋转数据集，使其与主要成分对齐，以最大化前几个主成分中的方差。假设我们有如下图所示的数据集：
- en: '![An overview of PCA](img/8365OS_07_02.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![PCA概述](img/8365OS_07_02.jpg)'
- en: The instances approximately form a long, thin ellipse stretching from the origin
    to the top right of the plot. To reduce the dimensions of this data set, we must
    project the points onto a line. The following are two lines that the data could
    be projected onto. Along which line do the instances vary the most?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实例大致形成了一个从原点到图表右上角的长而细的椭圆。为了减少这个数据集的维度，我们必须将这些点投影到一条线上。以下是两个可以进行投影的线条。沿哪条线，实例变化最多？
- en: '![An overview of PCA](img/8365OS_07_03.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![PCA概述](img/8365OS_07_03.jpg)'
- en: 'The instances vary more along the dashed line than the dotted line. In fact,
    the dashed line is the first principal component. The second principal component
    must be orthogonal to the first principal component; that is, the second principal
    component must be statistically independent, and will appear to be perpendicular
    to the first principal component when it is plotted, as shown in the following
    figure:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实例沿着虚线比沿着点线变化得更多。实际上，虚线是第一个主成分。第二个主成分必须与第一个主成分正交；也就是说，第二个主成分必须是统计独立的，当绘制时，第二主成分将垂直于第一个主成分，如下图所示：
- en: '![An overview of PCA](img/8365OS_07_04.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![PCA概述](img/8365OS_07_04.jpg)'
- en: Each subsequent principal component preserves the maximum amount of the remaining
    variance; the only constraint is that each must be orthogonal to the other principal
    components.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每个后续的主成分保留剩余方差的最大部分；唯一的限制是，每个主成分必须与其他主成分正交。
- en: Now assume that the data set is three dimensional. The scatter plot of the points
    looks like a flat disc that has been rotated slightly about one of the axes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设数据集是三维的。这些点的散点图看起来像一个稍微绕着一个轴旋转的平面圆盘。
- en: '![An overview of PCA](img/8365OS_07_05.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![PCA概述](img/8365OS_07_05.jpg)'
- en: The points can be rotated and translated such that the tilted disk lies almost
    exactly in two dimensions. The points now form an ellipse; the third dimension
    contains almost no variance and can be discarded.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些点可以旋转和平移，使得倾斜的平面几乎完全位于二维空间中。现在这些点形成了一个椭圆；第三维几乎没有方差，可以丢弃。
- en: PCA is most useful when the variance in a data set is distributed unevenly across
    the dimensions. Consider a three-dimensional data set with a spherical convex
    hull. PCA cannot be used effectively with this data set because there is equal
    variance in each dimension; none of the dimensions can be discarded without losing
    a significant amount of information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PCA（主成分分析）在数据集的方差在各维度之间分布不均时最为有效。考虑一个三维数据集，其具有球形凸包。由于每个维度的方差相等，PCA无法有效地应用于该数据集；任何维度都无法丢弃，否则会丢失大量信息。
- en: It is easy to visually identify the principal components of data sets with only
    two or three dimensions. In the next section, we will discuss how to calculate
    the principal components of high-dimensional data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可视化方式，很容易识别只有二维或三维的数据集的主成分。在下一节中，我们将讨论如何计算高维数据的主成分。
- en: Performing Principal Component Analysis
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行主成分分析
- en: There are several terms that we must define before discussing how principal
    component analysis works.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论主成分分析如何工作之前，我们必须先定义几个术语。
- en: Variance, Covariance, and Covariance Matrices
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方差、协方差和协方差矩阵
- en: 'Recall that **variance** is a measure of how a set of values are spread out.
    Variance is calculated as the average of the squared differences of the values
    and mean of the values, as per the following equation:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，**方差**是衡量一组值分布的程度。方差是值与均值的平方差的平均值，按以下公式计算：
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_06.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![方差、协方差和协方差矩阵](img/8365OS_07_06.jpg)'
- en: '**Covariance** is a measure of how much two variables change together; it is
    a measure of the strength of the correlation between two sets of variables. If
    the covariance of two variables is zero, the variables are uncorrelated. Note
    that uncorrelated variables are not necessarily independent, as correlation is
    only a measure of linear dependence. The covariance of two variables is calculated
    using the following equation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**协方差**是衡量两个变量一起变化的程度；它是衡量两个变量集之间相关性强度的度量。如果两个变量的协方差为零，则这两个变量不相关。请注意，不相关的变量不一定是独立的，因为相关性仅仅是线性依赖性的度量。两个变量的协方差使用以下公式计算：'
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_07.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![方差、协方差和协方差矩阵](img/8365OS_07_07.jpg)'
- en: 'If the covariance is nonzero, the sign indicates whether the variables are
    positively or negatively correlated. When two variables are positively correlated,
    one increases as the other increases. When variables are negatively correlated,
    one variable decreases relative to its mean as the other variable increases relative
    to its mean. A **covariance** **matrix** describes the covariance values between
    each pair of dimensions in a data set. The element ![Variance, Covariance, and
    Covariance Matrices](img/8365OS_07_33.jpg) indicates the covariance of the ![Variance,
    Covariance, and Covariance Matrices](img/8365OS_07_34.jpg) and ![Variance, Covariance,
    and Covariance Matrices](img/8365OS_07_35.jpg) dimensions of the data. For example,
    a covariance matrix for a three-dimensional data is given by the following matrix:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果协方差非零，符号表示变量之间是正相关还是负相关。当两个变量正相关时，一个变量随着另一个变量的增加而增加。当变量负相关时，一个变量相对于其均值减少，而另一个变量相对于其均值增加。**协方差**
    **矩阵**描述数据集中每对维度之间的协方差值。元素![方差、协方差和协方差矩阵](img/8365OS_07_33.jpg)表示数据的![方差、协方差和协方差矩阵](img/8365OS_07_34.jpg)和![方差、协方差和协方差矩阵](img/8365OS_07_35.jpg)维度的协方差。例如，一个三维数据的协方差矩阵如下所示：
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_08.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![方差、协方差和协方差矩阵](img/8365OS_07_08.jpg)'
- en: 'Let''s calculate the covariance matrix for the following data set:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算以下数据集的协方差矩阵：
- en: '| 2 | 0 | −1.4 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | −1.4 |'
- en: '| 2.2 | 0.2 | −1.5 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2.2 | 0.2 | −1.5 |'
- en: '| 2.4 | 0.1 | −1 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 2.4 | 0.1 | −1 |'
- en: '| 1.9 | 0 | −1.2 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 1.9 | 0 | −1.2 |'
- en: 'The means of the variables are 2.125, 0.075, and -1.275\. We can then calculate
    the covariances of each pair of variables to produce the following covariance
    matrix:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的均值为2.125、0.075和-1.275。然后，我们可以计算每一对变量的协方差，得到以下协方差矩阵：
- en: '![Variance, Covariance, and Covariance Matrices](img/8365OS_07_09.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![方差、协方差和协方差矩阵](img/8365OS_07_09.jpg)'
- en: 'We can verify our calculations using NumPy:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NumPy验证我们的计算：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Eigenvectors and eigenvalues
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征向量和特征值
- en: 'A vector is described by a **direction** and **magnitude**, or length. An **eigenvector**
    of a matrix is a non-zero vector that satisfies the following equation:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个向量由**方向**和**大小**（或长度）来描述。矩阵的**特征向量**是一个非零向量，满足以下方程：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_10.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_10.jpg)'
- en: In the preceding equation, ![Eigenvectors and eigenvalues](img/8365OS_07_30.jpg)
    is an eigenvector, *A* is a square matrix, and ![Eigenvectors and eigenvalues](img/8365OS_07_31.jpg)
    is a scalar called an **eigenvalue**. The direction of an eigenvector remains
    the same after it has been transformed by *A*; only its magnitude has changed,
    as indicated by the eigenvalue; that is, multiplying a matrix by one of its eigenvectors
    is equal to scaling the eigenvector. The prefix *eigen* is the German word for
    *belonging to* or *peculiar to*; the eigenvectors of a matrix are the vectors
    that *belong* to and characterize the structure of the data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![特征向量和特征值](img/8365OS_07_30.jpg) 是一个特征向量，*A* 是一个方阵，![特征向量和特征值](img/8365OS_07_31.jpg)
    是一个称为**特征值**的标量。特征向量的方向在通过 *A* 变换后保持不变；只有其大小发生了变化，这一变化由特征值表示；即，用矩阵与其特征向量相乘相当于对特征向量进行缩放。前缀
    *eigen* 是德语单词，意思是 *属于* 或 *特有的*；矩阵的特征向量是 *属于* 数据结构并表征数据结构的向量。
- en: Eigenvectors and eigenvalues can only be derived from square matrices, and not
    all square matrices have eigenvectors or eigenvalues. If a matrix does have eigenvectors
    and eigenvalues, it will have a pair for each of its dimensions. The principal
    components of a matrix are the eigenvectors of its covariance matrix, ordered
    by their corresponding eigenvalues. The eigenvector with the greatest eigenvalue
    is the first principal component; the second principal component is the eigenvector
    with the second greatest eigenvalue, and so on.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量和特征值只能从方阵中得出，并非所有方阵都有特征向量或特征值。如果一个矩阵有特征向量和特征值，那么每个维度都会有一对对应的特征向量和特征值。矩阵的主成分是其协方差矩阵的特征向量，按其对应的特征值排序。特征值最大的特征向量是第一个主成分；第二个主成分是特征值第二大的特征向量，依此类推。
- en: 'Let''s calculate the eigenvectors and eigenvalues of the following matrix:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算以下矩阵的特征向量和特征值：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_11.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_11.jpg)'
- en: 'Recall that the product of *A* and any eigenvector of *A* must be equal to
    the eigenvector multiplied by its eigenvalue. We will begin by finding the eigenvalues,
    which we can find using the following characteristic equations:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，*A* 和任何 *A* 的特征向量相乘的结果必须等于特征向量与其特征值的乘积。我们将从找到特征值开始，特征值可以通过以下特征方程求得：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_12.jpg)![Eigenvectors and eigenvalues](img/8365OS_07_13.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_12.jpg)![特征向量和特征值](img/8365OS_07_13.jpg)'
- en: 'The characteristic equation states that the determinant of the matrix, that
    is, the difference between the data matrix and the product of the identity matrix
    and an eigenvalue is zero:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 特征方程指出，矩阵的行列式，也就是数据矩阵与单位矩阵与特征值的乘积之间的差为零：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_14.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_14.jpg)'
- en: 'Both of the eigenvalues for this matrix are equal to **-1**. We can now use
    the eigenvalues to solve the eigenvectors:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的两个特征值都是**-1**。我们现在可以使用特征值来求解特征向量：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_10.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_10.jpg)'
- en: 'First, we set the equation equal to zero:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将方程设置为零：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_12.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_12.jpg)'
- en: 'Substituting our values for *A* produces the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *A* 的值代入后得到如下结果：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_15.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_15.jpg)'
- en: We can then substitute the first eigenvalue in our first eigenvalue to solve
    the eigenvectors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将第一个特征值代入我们的第一个特征值中来求解特征向量。
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_36.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_36.jpg)'
- en: 'The preceding equation can be rewritten as a system of equations:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程可以改写为一组方程：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_16.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_16.jpg)'
- en: 'Any non-zero vector that satisfies the preceding equations, such as the following,
    can be used as an eigenvector:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 任何满足前面方程的非零向量，如下所示，都可以作为特征向量：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_17.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_17.jpg)'
- en: 'PCA requires unit eigenvectors, or eigenvectors that have a length equal to
    **1**. We can normalize an eigenvector by dividing it by its norm, which is given
    by the following equation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PCA需要单位特征向量，或者说长度为**1**的特征向量。我们可以通过将特征向量除以其范数来归一化它，范数由以下公式给出：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_18.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_18.jpg)'
- en: 'The norm of our vector is equal to the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向量的范数等于以下值：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_19.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_19.jpg)'
- en: 'This produces the following unit eigenvector:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下单位特征向量：
- en: '![Eigenvectors and eigenvalues](img/8365OS_07_20.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![特征向量和特征值](img/8365OS_07_20.jpg)'
- en: 'We can verify that our solutions for the eigenvectors are correct using NumPy.
    The `eig` function returns a tuple of the eigenvalues and eigenvectors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NumPy验证我们对特征向量的解是否正确。`eig`函数返回一个包含特征值和特征向量的元组：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Dimensionality reduction with Principal Component Analysis
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用主成分分析进行降维
- en: 'Let''s use principal component analysis to reduce the following two-dimensional
    data set to one dimension:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用主成分分析（PCA）将以下二维数据集降维为一维：
- en: '| x1 | x2 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| x1 | x2 |'
- en: '| --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.9 | 1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 | 1 |'
- en: '| 2.4 | 2.6 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 2.4 | 2.6 |'
- en: '| 1.2 | 1.7 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 | 1.7 |'
- en: '| 0.5 | 0.7 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.7 |'
- en: '| 0.3 | 0.7 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 0.7 |'
- en: '| 1.8 | 1.4 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 1.8 | 1.4 |'
- en: '| 0.5 | 0.6 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.6 |'
- en: '| 0.3 | 0.6 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 0.6 |'
- en: '| 2.5 | 2.6 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 2.5 | 2.6 |'
- en: '| 1.3 | 1.1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1.3 | 1.1 |'
- en: 'The first step of PCA is to subtract the mean of each explanatory variable
    from each observation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的第一步是从每个观测值中减去每个解释变量的均值：
- en: '| x1 | x2 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| x1 | x2 |'
- en: '| --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.9 - 1.17 = -0.27 | 1 - 1.3 = -0.3 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 0.9 - 1.17 = -0.27 | 1 - 1.3 = -0.3 |'
- en: '| 2.4 - 1.17 = 1.23 | 2.6 - 1.3 = 1.3 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 2.4 - 1.17 = 1.23 | 2.6 - 1.3 = 1.3 |'
- en: '| 1.2 - 1.17 = 0.03 | 1.7 - 1.3 = 0.4 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1.2 - 1.17 = 0.03 | 1.7 - 1.3 = 0.4 |'
- en: '| 0.5 - 1.17 = -0.67 | -0.7 - 1.3 = 0.6 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 - 1.17 = -0.67 | -0.7 - 1.3 = 0.6 |'
- en: '| 0.3 - 1.17 = -0.87 | -0.7 - 1.3 = 0.6 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 - 1.17 = -0.87 | -0.7 - 1.3 = 0.6 |'
- en: '| 1.8 - 1.17 = 0.63 | 1.4 - 1.3 = 0.1 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 1.8 - 1.17 = 0.63 | 1.4 - 1.3 = 0.1 |'
- en: '| 0.5 - 1.17 = -0.67 | 0.6 - 1.3 = -0.7 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 - 1.17 = -0.67 | 0.6 - 1.3 = -0.7 |'
- en: '| 0.3 - 1.17 = -0.87 | 0.6 - 1.3 = -0.7 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 - 1.17 = -0.87 | 0.6 - 1.3 = -0.7 |'
- en: '| 2.5 - 1.17 = 1.33 | 2.6 - 1.3 = 1.3 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 2.5 - 1.17 = 1.33 | 2.6 - 1.3 = 1.3 |'
- en: '| 1.3 - 1.17 = 0.13 | 1.1 - 1.3 = -0.2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1.3 - 1.17 = 0.13 | 1.1 - 1.3 = -0.2 |'
- en: Next, we must calculate the principal components of the data. Recall that the
    principal components are the eigenvectors of the data's covariance matrix ordered
    by their eigenvalues. The principal components can be found using two different
    techniques. The first technique requires calculating the covariance matrix of
    the data. Since the covariance matrix will be square, we can calculate the eigenvectors
    and eigenvalues using the approach described in the previous section. The second
    technique uses singular value decomposition of the data matrix to find the eigenvectors
    and square roots of the eigenvalues of the covariance matrix. We will work through
    an example using the first technique, and then describe the second technique that
    is used by scikit-learn's implementation of PCA.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须计算数据的主成分。回顾一下，主成分是按其特征值排序的数据协方差矩阵的特征向量。主成分可以通过两种不同的技术来找到。第一种技术需要计算数据的协方差矩阵。由于协方差矩阵是方阵，我们可以使用上一节中描述的方法计算特征向量和特征值。第二种技术则利用数据矩阵的奇异值分解（SVD）来找到协方差矩阵的特征向量和特征值的平方根。我们将首先通过第一种方法完成一个示例，然后描述scikit-learn中PCA实现所使用的第二种方法。
- en: 'The following matrix is the covariance matrix for the data:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下矩阵是数据的协方差矩阵：
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_21.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![使用主成分分析进行降维](img/8365OS_07_21.jpg)'
- en: 'Using the technique described in the previous section, the eigenvalues are
    1.250 and 0.034\. The following are the unit eigenvectors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一节中描述的技术，特征值为1.250和0.034。以下是单位特征向量：
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_22.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![使用主成分分析进行降维](img/8365OS_07_22.jpg)'
- en: 'Next, we will project the data onto the principal components. The first eigenvector
    has the greatest eigenvalue and is the first principal component. We will build
    a transformation matrix in which each column of the matrix is the eigenvector
    for a principal component. If we were reducing a five-dimensional data set to
    three dimensions, we would build a matrix with three columns. In this example,
    we will project our two-dimensional data set onto one dimension, so we will use
    only the eigenvector for the first principal component. Finally, we will find
    the dot product of the data matrix and transformation matrix. The following is
    the result of projecting our data onto the first principal component:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把数据投影到主成分上。第一个特征向量具有最大的特征值，是第一个主成分。我们将构建一个变换矩阵，其中矩阵的每一列都是一个主成分的特征向量。如果我们将一个五维数据集降维到三维，我们将构建一个包含三列的矩阵。在这个例子中，我们将把二维数据集投影到一维，因此我们只会使用第一个主成分的特征向量。最后，我们将计算数据矩阵和变换矩阵的点积。以下是将数据投影到第一个主成分后的结果：
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_23.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析降维](img/8365OS_07_23.jpg)'
- en: 'Many implementations of PCA, including the one of scikit-learn, use singular
    value decomposition to calculate the eigenvectors and eigenvalues. SVD is given
    by the following equation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 PCA 的实现，包括 scikit-learn 中的实现，使用奇异值分解（SVD）来计算特征向量和特征值。SVD 由以下方程给出：
- en: '![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_24.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析降维](img/8365OS_07_24.jpg)'
- en: The columns of ![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_27.jpg)
    are called left singular vectors of the data matrix, the columns of ![Dimensionality
    reduction with Principal Component Analysis](img/8365OS_07_28.jpg) are its right
    singular vectors, and the diagonal entries of ![Dimensionality reduction with
    Principal Component Analysis](img/8365OS_07_29.jpg) are its singular values. While
    the singular vectors and values of a matrix are useful in some applications of
    signal processing and statistics, we are only interested in them as they relate
    to the eigenvectors and eigenvalues of the data matrix. Specifically, the left
    singular vectors are the eigenvectors of the covariance matrix and the diagonal
    elements of ![Dimensionality reduction with Principal Component Analysis](img/8365OS_07_29.jpg)
    are the square roots of the eigenvalues of the covariance matrix. Calculating
    SVD is beyond the scope of this chapter; however, eigenvectors found using SVD
    should be similar to those derived from a covariance matrix.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![主成分分析降维](img/8365OS_07_27.jpg) 的列称为数据矩阵的左奇异向量，![主成分分析降维](img/8365OS_07_28.jpg)
    的列是其右奇异向量，![主成分分析降维](img/8365OS_07_29.jpg) 的对角元素是其奇异值。虽然矩阵的奇异向量和奇异值在信号处理和统计学的某些应用中很有用，但我们关注它们仅仅是因为它们与数据矩阵的特征向量和特征值有关。具体来说，左奇异向量是协方差矩阵的特征向量，![主成分分析降维](img/8365OS_07_29.jpg)
    的对角元素是协方差矩阵特征值的平方根。计算 SVD 超出了本章的范围；然而，使用 SVD 得到的特征向量应该与从协方差矩阵推导出的特征向量相似。'
- en: Using PCA to visualize high-dimensional data
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主成分分析（PCA）可视化高维数据
- en: It is easy to discover patterns by visualizing data with two or three dimensions.
    A high-dimensional dataset cannot be represented graphically, but we can still
    gain some insights into its structure by reducing it to two or three principal
    components.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将数据可视化为二维或三维图形，可以轻松发现数据中的模式。高维数据集无法直接图形化表示，但我们仍然可以通过将数据降维至两到三个主成分来获得其结构的一些洞察。
- en: 'Collected in 1936, Fisher''s Iris data set is a collection of fifty samples
    from each of the three species of Iris: Iris setosa, Iris virginica, and Iris
    versicolor. The explanatory variables are measurements of the length and width
    of the petals and sepals of the flowers. The Iris dataset is commonly used to
    test classification models, and is included with scikit-learn. Let''s reduce the
    `iris` dataset''s four dimensions so that we can visualize it in two dimensions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 费舍尔的鸢尾花数据集于1936年收集，是来自三种鸢尾花物种的每种各50个样本的集合：鸢尾花 Setosa、鸢尾花 Virginica 和鸢尾花 Versicolor。解释变量是花瓣和萼片的长度和宽度的测量值。鸢尾花数据集通常用于测试分类模型，并且包含在
    scikit-learn 中。让我们将 `iris` 数据集的四个维度降到二维，以便可视化：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First, we load the built-in iris data set and instantiate a `PCA` estimator.
    The `PCA` class takes a number of principal components to retain as a hyperparameter.
    Like the other estimators, `PCA` exposes a `fit_transform()` method that returns
    the reduced data matrix:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载内置的鸢尾花数据集，并实例化一个`PCA`估计器。`PCA`类接受一个要保留的主成分数量作为超参数。像其他估计器一样，`PCA`暴露了一个`fit_transform()`方法，返回降维后的数据矩阵：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we assemble and plot the reduced data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们组装并绘制了降维后的数据：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The reduced instances are plotted in the following figure. Each of the dataset's
    three classes is indicated by its own marker style. From this two-dimensional
    view of the data, it is clear that one of the classes can be easily separated
    from the other two overlapping classes. It would be difficult to notice this structure
    without a graphical representation. This insight can inform our choice of classification
    model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 缩减后的实例在下图中绘制。数据集的三个类别分别用其自己的标记样式表示。从这个数据的二维视图中，可以清楚地看出一个类别可以很容易地与其他两个重叠的类别分开。没有图形表示，很难注意到这种结构。这一洞察可以影响我们选择分类模型。
- en: '![Using PCA to visualize high-dimensional data](img/8365OS_07_25.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![使用PCA可视化高维数据](img/8365OS_07_25.jpg)'
- en: Face recognition with PCA
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA进行面部识别
- en: 'Now let''s apply PCA to a face-recognition problem. Face recognition is the
    supervised classification task of identifying a person from an image of his or
    her face. In this example, we will use a data set called *Our Database of Faces*
    from AT&T Laboratories, Cambridge. The data set contains ten images each of forty
    people. The images were created under different lighting conditions, and the subjects
    varied their facial expressions. The images are gray scale and 92 x 112 pixels
    in dimension. The following is an example image:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将PCA应用于面部识别问题。面部识别是一种监督分类任务，其目标是从面部图像中识别出一个人。在本例中，我们将使用来自AT&T实验室剑桥分部的*我们的面部数据库*数据集。该数据集包含四十个人的每个人的十张图像。这些图像在不同的光照条件下创建，并且主体改变了他们的面部表情。图像是灰度的，尺寸为92
    x 112像素。以下是一个示例图像：
- en: '![Face recognition with PCA](img/8365OS_07_26.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![使用PCA进行面部识别](img/8365OS_07_26.jpg)'
- en: While these images are small, a feature vector that encodes the intensity of
    every pixel will have 10,304 dimensions. Training from such high-dimensional data
    could require many samples to avoid over-fitting. Instead, we will use PCA to
    compactly represent the images in terms of a small number of principal components.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些图像很小，但编码每个像素强度的特征向量将具有10,304个维度。从这样高维数据的训练可能需要许多样本以避免过拟合。因此，我们将使用PCA来以少数主成分紧凑地表示图像。
- en: 'We can reshape the matrix of pixel intensities for an image into a vector,
    and create a matrix of these vectors for all of the training images. Each image
    is a linear combination of this data set''s principal components. In the context
    of face recognition, these principal components are called **eigenfaces**. The
    eigenfaces can be thought of as standardized components of faces. Each face in
    the data set can be expressed as some combination of the eigenfaces, and can be
    approximated as a combination of the most important eigenfaces:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将图像的像素强度矩阵重塑为一个向量，并为所有训练图像创建这些向量的矩阵。每个图像都是这个数据集主成分的线性组合。在面部识别的上下文中，这些主成分被称为**特征脸**。特征脸可以被看作是标准化的面部组件。数据集中的每张脸都可以表示为一些特征脸的组合，并且可以近似为最重要的特征脸的组合：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We begin by loading the images into `NumPy` arrays, and reshaping their matrices
    into vectors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将图像加载到`NumPy`数组中，并将它们的矩阵重塑为向量：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then randomly split the images into training and test sets, and fit the
    `PCA` object on the training set:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将图像随机分割为训练集和测试集，并在训练集上拟合`PCA`对象：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We reduce all of the instances to 150 dimensions and train a logistic regression
    classifier. The data set contains forty classes; scikit-learn automatically creates
    binary classifiers using the one versus all strategy behind the scenes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有实例缩减为150个维度并训练一个逻辑回归分类器。数据集包含四十个类别；scikit-learn在幕后自动使用一对所有策略创建二进制分类器：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we evaluate the performance of the classifier using cross-validation
    and a test set. The average per-class F1 score of the classifier trained on the
    full data was 0.94, but required significantly more time to train and could be
    prohibitively slow in an application with more training instances:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用交叉验证和测试集评估分类器的性能。在完整数据集上训练的分类器的每类平均F1分数为0.94，但训练所需的时间明显更长，在具有更多训练实例的应用中可能会变得非常缓慢：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is the output of the script:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是脚本的输出：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we examined the problem of dimensionality reduction. High-dimensional
    data cannot be visualized easily. High-dimensional data sets may also suffer from
    the curse of dimensionality; estimators require many samples to learn to generalize
    from high-dimensional data. We mitigated these problems using a technique called
    principal component analysis, which reduces a high-dimensional, possibly-correlated
    data set to a lower-dimensional set of uncorrelated principal components by projecting
    the data onto a lower-dimensional subspace. We used principal component analysis
    to visualize the four-dimensional Iris data set in two dimensions, and build a
    face-recognition system. In the next chapter, we will return to supervised learning.
    We will discuss an early classification algorithm called the perceptron, which
    will prepare us to discuss more advanced models in the last few chapters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了降维问题。高维数据难以进行可视化。高维数据集还可能遭遇维度灾难；估计器需要大量样本才能从高维数据中学习并进行泛化。我们通过使用一种叫做主成分分析的技术来缓解这些问题，该技术通过将数据投影到低维子空间，将一个高维、可能相关的数据集降维为一组不相关的主成分。我们使用主成分分析将四维Iris数据集可视化为二维，并构建了一个人脸识别系统。在下一章中，我们将回到监督学习。我们将讨论一种早期的分类算法——感知机，这将为我们在最后几章中讨论更高级的模型做准备。
