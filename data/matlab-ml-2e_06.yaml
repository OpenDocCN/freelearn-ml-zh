- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Deep Learning and Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习和卷积神经网络
- en: '**Deep learning** (**DL**) is a **machine learning** (**ML**) technology based
    on multilayer **artificial neural networks** (**ANNs**) that has allowed many
    applications to reach a high degree of accuracy. **Deep NNs** (**DNNs**) are capable
    of modeling and have the ability to capture intricate connections between input
    and output information. Among the highly effective uses, **computer vision** (**CV**)
    stands out, encompassing activities such as categorization, image regression,
    and the identification of objects. As an illustration, an advanced NN can produce
    a stratified portrayal of entities, wherein each entity is recognized by a collection
    of features taking the shape of visual basics, such as specific contours, directed
    lines, surface details, and repetitive designs. **Convolutional** **networks**
    (**CNNs**) are characterized by convolutional layers, which use filters to analyze
    data in a local region and produce an activation map. These activation maps are
    then processed by pooling layers, which aggregate the low-resolution data to reduce
    the dimensionality of the representation and make processing more computationally
    efficient. The convolutional and pooling layers are then alternated several times
    until the image is represented by a low-resolution activation map. In this chapter,
    we will learn the basic concepts of DL and discover how to implement an algorithm
    based on CNNs in the MATLAB environment.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）是一种基于多层**人工神经网络**（**ANNs**）的**机器学习**（**ML**）技术，它使许多应用达到了高精度。**深度神经网络**（**DNNs**）能够建模，并具有捕捉输入和输出信息之间复杂联系的能力。在高度有效的应用中，**计算机视觉**（**CV**）尤为突出，包括分类、图像回归和物体识别等活动。例如，一个高级神经网络可以产生实体的分层描绘，其中每个实体都由一组特征组成，这些特征以视觉基础的形式出现，如特定的轮廓、有向线条、表面细节和重复图案。**卷积****网络**（**CNNs**）的特点是具有卷积层，这些层使用过滤器在局部区域分析数据并生成激活图。然后，这些激活图由池化层处理，池化层将低分辨率数据聚合起来，以降低表示的维度并使处理更加计算高效。卷积层和池化层交替多次，直到图像由低分辨率的激活图表示。在本章中，我们将学习深度学习的基本概念，并了解如何在MATLAB环境中实现基于CNN的算法。'
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding DL basic concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习基本概念
- en: Exploring DL models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习模型
- en: Approaching CNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接近CNNs
- en: Building a CNN in MATLAB
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MATLAB中构建CNN
- en: Exploring the model’s results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索模型的结果
- en: Discovering DL architectures
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现深度学习架构
- en: As always, I thank the technical reviewers for their helpful advice on improving
    my chapter. I will try to apply them as they were proposed to me.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我要感谢技术审稿人为我章节提供的有益建议。我将尝试按照他们给我的建议来应用。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will introduce ML basic concepts. To understand these topics,
    a basic knowledge of algebra and mathematical modeling is needed. A working knowledge
    of the MATLAB environment is also required.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍机器学习的基本概念。要理解这些主题，需要具备代数和数学建模的基本知识。同时，也需要熟悉MATLAB环境。
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中的MATLAB代码，您需要以下文件（可在GitHub上找到：[https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)）：
- en: '`CNNPistachioClassification.m`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNNPistachioClassification.m`'
- en: '`PistachioShort.zip`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PistachioShort.zip`'
- en: Understanding DL basic concepts
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习基本概念
- en: '**DL** is a branch of ML based on using algorithms to model high-level abstractions
    about data. This discipline is part of a range of approaches that aim to learn
    methods for representing data. For example, an observation such as an image can
    be described in different ways: as a vector of intensity values for each pixel,
    or more abstractly as a set of edges or regions that have shapes or significant
    features. Some of these possible representations may prove more effective than
    others in facilitating the process of training another ML system.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**DL**是机器学习的一个分支，它基于使用算法来建模关于数据的高级抽象。这个学科是旨在学习数据表示方法的一系列方法之一。例如，一个观察结果，如图像，可以用不同的方式描述：作为每个像素强度值的向量，或者更抽象地，作为一组具有形状或显著特征的边缘或区域集合。这些可能的表示中，一些可能比其他表示更有效地促进训练另一个机器学习系统的过程。'
- en: For automatically identifying and extracting relevant features from raw data,
    we can use automated feature extraction to eliminate the need for manual **feature
    engineering** (**FE**). This process streamlines ML tasks and improves model performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从原始数据自动识别和提取相关特征，我们可以使用自动特征提取来消除手动**特征工程**（**FE**）的需求。这个过程简化了机器学习任务并提高了模型性能。
- en: Automated feature extraction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动特征提取
- en: In this context, one of the central aspects of DL is the development of learning
    algorithms that specialize in automatically extracting significant features from
    a dataset, which can then be used to train ML systems. This is a significant contribution,
    considering that without such techniques, these features would have to be manually
    generated and vetted prior to training. The basic concept of DL is to successively
    subject the input data to different levels of processing, the result of which
    is the emergence of features. Consider an image of a cat. Automated feature extraction
    can identify features such as the cat’s eyes, ears, and whiskers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，深度学习（DL）的一个核心方面是开发专门从数据集中自动提取显著特征的算法，这些特征随后可用于训练机器学习（ML）系统。考虑到如果没有这些技术，这些特征在训练之前必须手动生成并验证，这是一个重要的贡献。深度学习的基本概念是对输入数据进行不同级别的连续处理，其结果是特征的出现。考虑一张猫的图片。自动特征提取可以识别出猫的眼睛、耳朵和胡须等特征。
- en: 'In the field of NNs, the concept of DL was introduced with the creation of
    so-called **DNNs**. The operating principle is like that of traditional NNs but
    with one obvious difference: the significant increase in the number of intermediate
    layers of hidden neurons. As with classical NNs, DNNs are capable of modeling
    complex relationships between input and output data.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络（NN）领域，深度学习的概念随着所谓的**深度神经网络（DNN**）的创建而引入。其工作原理类似于传统的神经网络，但有一个明显的区别：隐藏神经元中间层的数量显著增加。与经典神经网络一样，DNN能够模拟输入和输出数据之间的复杂关系。
- en: One of the most successful areas of application is CV, which includes tasks
    such as classification, image regression, and object detection. In the latter
    task, a DNN can build a layered representation of objects, in which each object
    is identified by a set of features that appear in the form of primitive visual
    elements, such as specific edges, oriented lines, textures, and recurring patterns.
    This modeling ability is rooted in a large number of hidden layers of neurons.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最成功的应用领域之一是计算机视觉（CV），包括分类、图像回归和目标检测等任务。在目标检测任务中，DNN可以构建一个分层表示对象，其中每个对象由一组以原始视觉元素（如特定边缘、方向线、纹理和重复模式）的形式出现的特征来识别。这种建模能力源于大量隐藏层中的神经元。
- en: Training a DNN
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练深度神经网络（DNN）
- en: 'As far as the training phase is concerned, it is still possible to use the
    well-established backpropagation algorithm. As with traditional NNs, DNNs can
    suffer from the age-old problem of overfitting. To mitigate this situation, **regularization
    techniques** are usually employed, which intervene in the optimization process
    during the training phase. Among the most frequently employed methods are the
    following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就训练阶段而言，仍然可以使用成熟的反向传播算法。与传统的神经网络一样，DNN可能会遭受古老的过拟合问题。为了减轻这种情况，通常采用**正则化技术**，这些技术会在训练阶段的优化过程中介入。最常用的方法包括以下几种：
- en: '**L2 regularization** (**weight decay**), which influences the optimizer’s
    operation by adding the **L2 norm** of the network weights, multiplied by a specific
    constant, to the loss function. The L2 norm, also known as the Euclidean norm,
    is a mathematical function that measures the length of a vector. It is defined
    as the square root of the sum of the squares of the vector’s components. The L2
    norm is often used in ML and statistics to measure the distance between two vectors.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2正则化**（**权重衰减**），通过向损失函数中添加网络权重的**L2范数**（也称为欧几里得范数），乘以一个特定的常数，来影响优化器的操作。L2范数是一个数学函数，用于衡量向量的长度。它定义为向量各分量平方和的平方根。L2范数在机器学习和统计学中常用于衡量两个向量之间的距离。'
- en: 'The formula for the L2 norm of a vector *x* is as follows:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 向量 *x* 的 L2 范数公式如下：
- en: '||x||2 = sqrt(∑ (x _ i) ^ 2)'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '||x||2 = sqrt(∑ (x _ i) ^ 2)'
- en: 'Please refer here as follows:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请按以下方式参考：
- en: '*||x||_2* is the L2 norm of *x*'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*||x||_2* 是 *x* 的 L2 范数'
- en: '*x_i* is the i-th component of *x*'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x_i* 是 *x* 的第 i 个分量'
- en: '**L1 regularization** (**sparsity**), functioning similarly to the former but
    using the **L1 norm**. The L1 norm is often used in ML and statistics to measure
    the distance between two vectors. It is also known as the Manhattan norm or the
    Taxicab norm because it corresponds to the distance traveled by a taxicab moving
    along city streets. The formula for the L1 norm of a vector *x* is:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1正则化**（**稀疏性**），与之前的功能类似，但使用**L1范数**。L1范数常用于机器学习和统计学中，用于衡量两个向量之间的距离。它也被称为曼哈顿范数或出租车范数，因为它对应于出租车在城市街道上行驶的距离。向量
    *x* 的L1范数公式为：'
- en: '||x||1 = ∑ |x _ i|'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '||x||1 = ∑ |x _ i|'
- en: 'Here:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里：
- en: '*||x||_1* is the L1 norm of *x*'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*||x||_1* 是 *x* 的L1范数'
- en: '*x_i* is the i-th component of *x*'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x_i* 是 *x* 的第 i 个分量'
- en: '**Dropout**, where, during each training step, a set number of randomly chosen
    neurons within the hidden layers are turned off, preventing their outputs from
    affecting subsequent neurons.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**，在每次训练步骤中，隐藏层中随机选择的一组神经元被关闭，防止它们的输出影响后续神经元。'
- en: All these techniques serve to lessen the interdependencies between the network
    and the training data samples, effectively curbing overfitting. Although backpropagation
    training provides a solid solution due to its simplicity and tendency to converge
    toward better local minima compared to other methods, it can pose considerable
    challenges in deep networks concerning actual computation. Indeed, an array of
    **hyperparameters** must be considered in DNNs, encompassing factors such as dimensions
    (in terms of layer count and units per layer), learning rate, initial weights,
    and the optimization of these parameters—a process that can become unwieldy in
    terms of time and computational resources.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术都是为了减少网络和训练数据样本之间的相互依赖性，有效地遏制过拟合。尽管反向传播训练由于其简单性和相对于其他方法更倾向于收敛到更好的局部最小值，提供了一个可靠的解决方案，但它可以在深层网络的实际计算中带来相当大的挑战。实际上，在深度神经网络中必须考虑一系列**超参数**，包括维度（就层计数和每层的单元数而言）、学习率、初始权重以及这些参数的优化——这个过程在时间和计算资源方面可能会变得难以控制。
- en: Various solutions have been suggested in this context, including the utilization
    of mini-batches of data to expedite training. However, a substantial breakthrough
    has been the advancement of GPUs with increasingly powerful computational capabilities
    in recent years. In network operations, the primary computational tasks involve
    matrix and vector operations, which align well with parallel implementation on
    GPU hardware.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下已经提出了各种解决方案，包括使用数据的小批量来加速训练。然而，近年来GPU计算能力的显著提升是一个重大的突破。在网络操作中，主要的计算任务涉及矩阵和向量运算，这非常适合在GPU硬件上并行实现。
- en: Another issue stemming from gradient descent-based techniques, accentuated by
    the intricate structure of deep networks, is the **vanishing gradient problem**.
    This challenge originates from gradient computation within the chain and the network’s
    substantial number of layers. Typically used activation functions tend to generate
    gradients with minute values, typically within the range of [-1, 1]. Due to the
    chain computation, this results in the multiplication of many small values when
    calculating the gradient in the initial layers of an *n*-level network. Consequently,
    the gradient decreases exponentially with *n*, causing the initial layers to learn
    at a sluggish pace.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个由基于梯度下降的技术引起的问题，由于深层网络的复杂结构而加剧，是**梯度消失问题**。这个挑战源于链中的梯度计算和网络的大量层。通常使用的激活函数倾向于生成具有微小值的梯度，通常在[-1,
    1]的范围内。由于链式计算，当在*n*-级网络的初始层计算梯度时，这会导致许多小值的乘积。因此，梯度随着*n*呈指数级下降，导致初始层的学习速度缓慢。
- en: Another solution provides the preliminary training in an unsupervised way of
    one level of the network at a time, to then carry out a final complete training
    by means of backpropagation. Another way around the problem, encouraged by the
    development of GPUs in recent times, is the use of faster hardware to counteract
    what is the main symptom of the problem—namely, the slowness of the training process.
    Another point against DNNs, in the case of **supervised learning** (**SL**), is
    represented by the very large amount of sample data (including the desired output)
    necessary for the network to reach the required results at the end of the training.
    This represents a significant obstacle since, for certain tasks, the production
    of the expected outputs for each example is an operation that can take a very
    long time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是逐级以无监督方式对网络进行初步训练，然后通过反向传播进行最终完整训练。近年来，随着GPU的发展，另一种解决方法，即使用更快的硬件来对抗问题的主要症状——即训练过程的缓慢。在**监督学习**（**SL**）的情况下，反对深度神经网络（DNNs）的另一个观点是，网络在训练结束时达到所需结果所需的非常大的样本数据量（包括期望的输出）。这代表了一个重大障碍，因为对于某些任务来说，为每个示例生成期望的输出是一个可能需要非常长时间的操作。
- en: After introducing DL, let’s now see the different types of DL available.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了深度学习之后，现在让我们看看可用的不同类型的深度学习。
- en: Exploring DL models
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度学习模型
- en: 'Various types of DL architectures and techniques have been developed to tackle
    different tasks and challenges. Here are some examples:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对不同的任务和挑战，已经开发出各种类型的深度学习架构和技术。以下是一些例子：
- en: '**CNNs**: Mainly used for image and video analysis, CNNs are designed to learn
    spatial hierarchies of features automatically and adaptively from input data.
    They have been highly successful in tasks such as image classification, object
    detection, and image segmentation.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）：主要用于图像和视频分析，CNNs被设计来自动和自适应地从输入数据中学习特征的空间层次结构。它们在图像分类、目标检测和图像分割等任务中取得了高度成功。'
- en: '**Recurrent NNs** (**RNNs**): RNNs are well suited for tasks involving sequences,
    such as **natural language processing** (**NLP**) and speech recognition. They
    have an internal memory that allows them to maintain information about previous
    inputs, making them effective for handling sequential data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）：RNNs非常适合涉及序列的任务，如**自然语言处理**（**NLP**）和语音识别。它们具有内部记忆，能够保持关于先前输入的信息，这使得它们在处理序列数据时非常有效。'
- en: '**Long short-term memory** (**LSTM**) **networks**: A type of RNN, LSTMs are
    designed to overcome the vanishing gradient problem in training deep networks.
    They are particularly useful for capturing long-range dependencies in sequential
    data, making them popular for tasks such as language modeling and machine translation.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）网络：作为一种循环神经网络（RNN），LSTMs被设计用来克服训练深度网络时的梯度消失问题。它们特别适用于捕捉序列数据中的长距离依赖关系，因此在语言建模和机器翻译等任务中非常受欢迎。'
- en: '**Generative adversarial networks** (**GANs**): GANs consist of two NNs—the
    generator and the discriminator—engaged in a game-like setting. GANs are used
    for generating new data samples that are like a given dataset. They have been
    used for image generation, style transfer, and data augmentation.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）：GANs由两个神经网络组成——生成器和判别器，它们在一种类似游戏的设置中进行互动。GANs用于生成与给定数据集类似的新数据样本。它们已被用于图像生成、风格迁移和数据增强。'
- en: '**Autoencoders** (**AEs**): AEs are used for **unsupervised learning** (**UL**)
    and data compression. They consist of an encoder that maps the input data to a
    latent space and a decoder that reconstructs the input from the latent representation.
    AEs find applications in dimensionality reduction, denoising, and anomaly detection.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器**（**AEs**）：AEs用于**无监督学习**（**UL**）和数据压缩。它们由一个编码器组成，该编码器将输入数据映射到潜在空间，以及一个解码器，该解码器从潜在表示中重建输入。AEs在降维、去噪和异常检测中找到应用。'
- en: '**Transformer networks**: Transformers have gained popularity in NLP tasks.
    They utilize self-attention mechanisms to process input data in parallel, making
    them efficient for handling long-range dependencies in sequences. The **Bidirectional
    Encoder Representations from Transformers** (**BERT**) model is a prominent example.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer网络**：Transformer在自然语言处理任务中获得了流行。它们利用自注意力机制并行处理输入数据，这使得它们在处理序列中的长距离依赖关系时效率很高。**从Transformer中获得的双向编码器表示**（**BERT**）模型是一个突出的例子。'
- en: '**Capsule networks**: Capsule networks aim to improve the robustness of NNs
    to variations in object poses and viewpoints. They use *capsules* to represent
    different aspects of an object, allowing them to capture hierarchical relationships
    between features.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**胶囊网络**：胶囊网络旨在提高神经网络（NNs）对物体姿态和视角变化的鲁棒性。它们使用**胶囊**来表示物体的不同方面，允许它们捕获特征之间的层次关系。'
- en: '**Neural Turing machines** (**NTMs**): These architectures combine NNs with
    external memory, enabling them to perform algorithmic tasks. They are designed
    to learn algorithmic procedures and have been used for tasks such as sorting and
    associative recall.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经图灵机**（**NTMs**）：这些架构将神经网络与外部内存相结合，使它们能够执行算法任务。它们被设计来学习算法过程，并已被用于排序和关联回忆等任务。'
- en: '**Graph NNs** (**GNNs**): GNNs operate on graph-structured data, making them
    suitable for tasks such as node classification, link prediction, and graph-level
    classification. They have applications in social network analysis, molecular chemistry,
    and recommendation systems.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图神经网络**（**GNNs**）：GNNs在图结构数据上运行，使它们适用于节点分类、链接预测和图级分类等任务。它们在社交网络分析、分子化学和推荐系统中都有应用。'
- en: These are just a few examples of the diverse range of DL architectures and techniques
    available, each tailored to specific tasks and data types. For example, CNNs are
    designed for image and spatial data analysis using convolutional layers to automatically
    learn hierarchical features and patterns from images. Characteristic claims are
    image classification, object detection, and image segmentation. RNNs are suited
    for sequential and temporal data analysis, maintaining the memory of previous
    inputs through recurrent connections and enabling them to capture sequential patterns.
    Typical applications are NLP, speech recognition, and time-series prediction.
    LSTM networks are an improved version of RNNs to address the vanishing gradient
    problem, incorporating memory cells to selectively retain and update information
    over long sequences. Applications of these technologies provide language modeling,
    machine translation, and **sentiment analysis** (**SA**). GANs are used to generate
    new data samples that resemble a given dataset using a generator and a discriminator
    network that compete in a minimax game. This type of DL is particularly suitable
    for image generation, style transfer, and data augmentation. Finally, AEs as UL
    for data compression and feature extraction comprise an encoder to map input data
    to a lower-dimensional latent space and a decoder to reconstruct input data. Dimensionality
    reduction, denoising, and anomaly detection are representative applications for
    these algorithms.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是深度学习（DL）架构和技术的多样性的几个例子，每个都是针对特定任务和数据类型定制的。例如，卷积神经网络（CNNs）旨在通过卷积层自动从图像中学习层次特征和模式，用于图像和空间数据分析。其典型应用包括图像分类、目标检测和图像分割。循环神经网络（RNNs）适用于序列和时间数据分析，通过循环连接保持先前输入的记忆，使它们能够捕获序列模式。典型应用包括自然语言处理（NLP）、语音识别和时间序列预测。长短期记忆网络（LSTM）是RNNs的改进版本，用于解决梯度消失问题，通过引入记忆单元在长序列中选择性保留和更新信息。这些技术的应用包括语言建模、机器翻译和**情感分析**（SA）。生成对抗网络（GANs）使用生成器和判别器网络来生成与给定数据集相似的新数据样本，这两个网络在最小-最大游戏中竞争。这种类型的深度学习特别适合图像生成、风格迁移和数据增强。最后，作为数据压缩和特征提取的通用学习（UL）的自动编码器（AEs）包括一个编码器将输入数据映射到低维潜在空间，以及一个解码器来重建输入数据。降维、去噪和异常检测是这些算法的代表性应用。
- en: The field of DL continues to evolve, with new architectures and approaches emerging
    to address various challenges across different domains.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域持续发展，新的架构和方法不断涌现，以解决不同领域中的各种挑战。
- en: At this point, we can delve into the main technology of DL—namely, CNNs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以深入探讨深度学习的主要技术——即卷积神经网络（CNNs）。
- en: Approaching CNNs
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接近CNNs
- en: 'As outlined in [*Chapter 5*](B21156_05.xhtml#_idTextAnchor105), *Introducing
    Artificial Neural Networks Modeling,* ANNs draw their inspiration from biological
    NNs. These ANNs aim to replicate human cognitive processes by emulating the mechanisms
    observed in natural NNs. They serve the purpose of estimating or approximating
    functions that might rely on numerous inputs, many of which could be unfamiliar.
    ANNs are typically conceptualized as networks of interconnected neurons, facilitating
    the exchange of messages. Each connection possesses an associated weight, the
    value of which can be adjusted through learning from experience. This adaptive
    characteristic empowers NNs to accommodate diverse input types and facilitates
    their capacity to learn (*Figure 6**.1*):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第5章*](B21156_05.xhtml#_idTextAnchor105)中概述的，*介绍人工神经网络建模*，人工神经网络（ANNs）从生物神经网络中汲取灵感。这些ANN旨在通过模拟自然神经网络中观察到的机制来复制人类的认知过程。它们旨在估计或近似可能依赖于众多输入的函数，其中许多可能是陌生的。ANNs通常被构想为相互连接的神经元网络，促进信息的交换。每个连接都有一个相关的权重，其值可以通过从经验中学习进行调整。这种适应性特征使神经网络能够适应不同的输入类型，并促进它们的学习能力（*图6**.1*）：
- en: '![Figure 6.1 – ANN architecture with hidden layers](img/B21156_06_01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 带有隐藏层的ANN架构](img/B21156_06_01.jpg)'
- en: Figure 6.1 – ANN architecture with hidden layers
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 带有隐藏层的ANN架构
- en: 'ANNs define a neuron as the **central processing unit** (**CPU**) that executes
    a mathematical operation to produce a single output from a set of input values.
    The neuron’s output is determined by the weighted sum of inputs and an added bias.
    Each neuron undertakes a straightforward task: activation occurs if the cumulative
    signal surpasses a specified activation threshold. The previous diagram illustrates
    a basic ANN structure, which essentially embodies CNNs. Indeed, like the latter,
    CNNs consist of interconnected neurons linked by weighted branches (weights);
    the networks’ training parameters remain the weights and biases.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs将神经元定义为执行数学运算以从一组输入值产生单个输出的**中央处理单元**（**CPU**）。神经元的输出由输入的加权和以及一个附加的偏置决定。每个神经元执行一个简单的任务：如果累积信号超过一个指定的激活阈值，则发生激活。前面的图示展示了基本的ANN结构，这基本上代表了CNN。确实，就像后者一样，CNN由通过加权分支（权重）相互连接的神经元组成；网络的训练参数是权重和偏置。
- en: Within CNNs, the inter-neuron connectivity pattern takes inspiration from the
    arrangement observed in the animal world’s visual cortex. Neurons within this
    section of the brain (visual cortex) react to specific stimuli within limited
    observation zones referred to as receptive fields. These fields partially overlap
    to encompass the entire field of vision. The response of an individual neuron
    to stimuli within its receptive field can be approximated mathematically through
    a convolution operation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，神经元之间的连接模式受到了动物世界视觉皮层中观察到的排列的启发。大脑的这一部分（视觉皮层）中的神经元对称为感受野的有限观察区域内的特定刺激做出反应。这些区域部分重叠，以涵盖整个视野。单个神经元对其感受野内刺激的反应可以通过卷积运算进行数学近似。
- en: 'All aspects pertaining to NN training, encompassing forward/backward propagation
    and weight updates, hold true in this context. Additionally, an entire CNN utilizes
    a singular differentiable cost function. However, CNNs are based on a particular
    assumption: their input possesses a specific data structure, such as an image.
    This characteristic empowers them to incorporate tailored architectural elements
    for enhanced processing of such data.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所有与NN训练相关的内容，包括前向/反向传播和权重更新，在此情况下都适用。此外，整个CNN使用一个单一的可微成本函数。然而，CNN基于一个特定的假设：它们的输入具有特定的数据结构，例如图像。这一特性使它们能够结合定制的架构元素，以增强此类数据的处理。
- en: CNNs adopt a **fully connected** (**FC**) **architecture**, where each neuron
    in each layer connects to all neurons in the preceding layer (excluding bias neurons).
    Generally, these architectures do not scale effectively when confronted with expanding
    input data sizes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CNN采用**全连接**（**FC**）**架构**，其中每一层的每个神经元都与前一层的所有神经元连接（不包括偏置神经元）。通常，当面临输入数据大小的扩展时，这些架构无法有效地进行扩展。
- en: 'The most common layers in a CNN are:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中最常见的层是：
- en: Convolutional layer
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layer
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: '**Rectified linear** **units** (**ReLUs**)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性** **单元** （**ReLUs**）'
- en: FC layer
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC层
- en: The function of each layer is addressed in detail in the following subsections.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每个层的功能将在以下小节中详细说明。
- en: Usually, a CNN is composed of multiple sequential levels of convolution and
    subsampling (pooling), which are subsequently succeeded by one or more FC final
    levels, especially in the context of classification tasks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN由多个连续的卷积和子采样（池化）层组成，随后是1个或多个FC最终层，特别是在分类任务的情况下。
- en: For addressing real-world challenges, these procedures can be merged and repeated
    as required. For instance, you can incorporate two, three, or even multiple layers
    of convolution. Additionally, you have the flexibility to apply pooling operations
    repeatedly to downsize the data dimensions. As mentioned earlier, a variety of
    levels are typically employed within a CNN. Certain layers encompass adjustable
    training parameters (weight and bias), whereas others are designed to execute
    predefined functions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对现实世界的挑战，这些程序可以根据需要合并和重复。例如，你可以包含两层、三层甚至更多层的卷积。此外，你还可以灵活地多次应用池化操作以减小数据维度。如前所述，CNN中通常使用多种级别。某些层包含可调整的训练参数（权重和偏差），而其他层则设计为执行预定义的功能。
- en: Convolutional layer
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层
- en: In a CNN, the primary layer type is the **convolutional layer**, and including
    one or more of these layers within a CNN is indispensable. In practical terms,
    the parameters of a convolutional layer are tied to a collection of adaptable
    filters. In contrast to CNNs, convolutional layers are structured in three dimensions,
    encompassing width, height, and depth.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，主要层类型是**卷积层**，在CNN中包含一个或多个这些层是必不可少的。从实际的角度来看，卷积层的参数与一系列可适应的滤波器相关联。与CNN不同，卷积层在三个维度上结构化，包括宽度、高度和深度。
- en: Every filter occupies a small spatial area (along width and height dimensions)
    while spanning the complete depth of the input volume it is applied to. In forward
    propagation, each filter is shifted—or, more precisely, convolved—across the input
    volume’s width and height, generating a two-dimensional activation map (also known
    as a feature map) specific to that filter. While the filter traverses the input
    region, a scalar product operation takes place between the filter values and those
    of the corresponding input section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个滤波器占据一个小的空间区域（沿宽度和高度维度），同时跨越它所应用的输入体积的完整深度。在正向传播中，每个滤波器在输入体积的宽度和高度上移动——更准确地说，是卷积——生成一个特定于该滤波器的二维激活图（也称为特征图）。当滤波器穿越输入区域时，滤波器值与相应输入部分的值之间发生标量积操作。
- en: Conceptually, the network’s objective is to learn activated filters that detect
    specific functionalities within distinct spatial regions of the input. Combining
    all these feature maps (for every filter) along the depth dimension forms the
    output volume of a convolutional layer. Each element within this volume can be
    interpreted as the output of a neuron observing a confined input area, and it
    shares its parameters with other neurons within the same feature map. This sharing
    stems from their common origin—the application of the same filter.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，网络的目的是学习激活滤波器，这些滤波器可以检测输入的不同空间区域中的特定功能。将这些特征图（对于每个滤波器）沿深度维度组合形成卷积层的输出体积。这个体积中的每个元素可以解释为观察有限输入区域的神经元的输出，并且它与同一特征图中的其他神经元共享其参数。这种共享源于它们的共同起源——应用相同的滤波器。
- en: 'The mathematical steps for filtering are as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤的数学步骤如下：
- en: Align the filter and the image patch.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对齐滤波器和图像块。
- en: Multiply each image pixel by the corresponding feature pixel.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个图像像素乘以相应的特征像素。
- en: Sum the products.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求这些乘积的和。
- en: Divide each sum by the total number of pixels in the feature.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个和除以特征中像素的总数。
- en: 'Here is the formula for the output of a convolutional layer:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是卷积层输出的公式：
- en: Output = ∑ F(k, l) * I(i + k, j + l)) / S(k, l)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 = ∑ F(k, l) * I(i + k, j + l)) / S(k, l)
- en: 'Here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: F(k, l) is the feature map element in (k, l)position
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F(k, l)是(k, l)位置的特征图元素
- en: I(i + k, j + l)) ) is the image element in(i + k, j + l)position
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I(i + k, j + l)) ) 是在(i + k, j + l)位置的图像元素
- en: S(k, l) is the sum of the feature pixel
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S(k, l)是特征像素的和
- en: 'In summary, key points to emphasize include:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，需要强调的关键点包括：
- en: '**Local receptive field**: Neurons in a layer are connected to a small segment
    of the input, known as a local receptive field. Each connection learns a weight.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部感受野**：层中的神经元连接到输入的一个小段，称为局部感受野。每个连接学习一个权重。'
- en: '**Shared weights**: Since significant features (such as edges or blobs) can
    appear anywhere in an image, neurons within the same layer share weights. This
    implies that identical features are recognized by all neurons in the layer, even
    if they’re positioned at different input points.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享权重**：由于显著特征（如边缘或块）可以出现在图像的任何位置，同一层的神经元共享权重。这意味着即使它们位于不同的输入点，层中的所有神经元也能识别出相同的特征。'
- en: '**Convolution**: Identical weight patterns are employed at various positions.
    The outcome of convolution is termed a **feature map**.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积**：在各个位置使用相同的权重模式。卷积的结果被称为**特征图**。'
- en: Each filter captures a specific feature from the preceding layer. Therefore,
    training multiple convolutional filters is necessary to extract diverse features.
    Each filter produces a feature map highlighting distinct characteristics.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每个过滤器从前一层捕获特定的特征。因此，需要训练多个卷积过滤器来提取不同的特征。每个过滤器生成一个突出显示独特特性的特征图。
- en: Pooling layer
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: These layers are systematically integrated into a network to periodically diminish
    the spatial dimensions (width and height) of ongoing representations and volumes
    within a specific stage of the network. This serves a dual purpose of minimizing
    the number of parameters, reducing computational time, and keeping a check on
    overfitting. The role of a **pooling layer** is to operate on each depth slice
    of the input volume independently to resize it spatially.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层系统地集成到网络中，以定期减少网络特定阶段的表示和体积的空间维度（宽度和高度）。这起到了双重作用：最小化参数数量，减少计算时间，并监控过拟合。**池化层**的作用是对输入体积的每个深度切片独立操作，以在空间上调整其大小。
- en: 'For each feature obtained during the convolutional process, a matrix is constructed,
    and the maximum value within each selected matrix is identified to compress the
    entire input. The steps are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积过程中获得的每个特征，都会构建一个矩阵，并识别每个选定矩阵中的最大值，以压缩整个输入。步骤如下：
- en: Choose a window size (typically `2` or `3`).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择窗口大小（通常是`2`或`3`）。
- en: Select a stride distance for pixel movement (usually 2). In a CNN, the stride
    distance in a pooling layer determines how much the pooling operation slides across
    the input feature map. A higher stride distance means that the pooling operation
    will skip over more pixels, resulting in a smaller output feature map. A lower
    stride distance means that the pooling operation will slide more carefully across
    the input feature map, resulting in a larger output feature map.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择像素移动的步距（通常是`2`）。在CNN中，池化层的步距决定了池化操作在输入特征图上滑动的程度。较大的步距意味着池化操作会跳过更多像素，导致输出特征图更小。较小的步距意味着池化操作会在输入特征图上更仔细地滑动，导致输出特征图更大。
- en: Slide the window across the filtered images.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将窗口滑动到过滤后的图像上。
- en: For each window, determine the maximum value.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个窗口，确定最大值。
- en: Illustratively, this method partitions an input image into groups of squares,
    and for each resulting segment, the highest value is extracted as the output.
    CNNs additionally incorporate pooling layers, strategically placed right after
    the convolutional layers. A pooling layer dissects the input into sections and
    chooses a representative value, employing methods such as max pooling or average
    pooling. The **max pooling** layer captures the highest features detected by preceding
    convolutional layers. The output layer assesses the presence of a potential feature
    within a region of the previous layers, albeit without exact spatial coordinates.
    The objective is to enable subsequent layers to process larger data sections.
    Max pooling expedites convergence rates, thereby enabling the selection of highly
    invariant features that enhance generalization performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，这种方法将输入图像分割成方块组，并对每个生成的片段，提取最高值作为输出。CNN还包含池化层，这些层被巧妙地放置在卷积层之后。池化层将输入分割成部分，并选择一个代表性值，采用最大池化或平均池化等方法。**最大池化层**捕获了先前卷积层检测到的最高特征。输出层评估前一层区域中潜在特征的存在，尽管没有精确的空间坐标。目标是使后续层能够处理更大的数据部分。最大池化加速了收敛速度，从而能够选择高度不变的特征，以增强泛化性能。
- en: 'The integration of a pooling layer offers the following benefits:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的集成提供了以下好处：
- en: Reduces subsequent layer computations
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了后续层的计算量
- en: Enhances feature robustness concerning spatial positioning
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强了关于空间定位的特征鲁棒性
- en: Operates under the premise that after identifying a particular feature, its
    precise location within the input becomes less significant than its general position
    relative to other features
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在识别特定特征后，其相对于输入的精确位置不如其相对于其他特征的一般位置重要。
- en: Average pooling serves as a downsampling operation that consolidates the representation
    of features within patches of the feature map. Typically, it is implemented in
    2x2 patches of the feature map with a stride of (2, 2). The process entails computing
    the average for each patch of the feature map, indicating that every 2x2 square
    in the feature map is downsampled to its average value. To illustrate, consider
    the 6x6 feature map resulting from the line detector convolutional filter discussed
    in the preceding section. We can manually explore the application of the average
    pooling operation to the initial line of that feature map.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 平均池化是一种下采样操作，它将特征图中的特征表示在特征图块中整合。通常，它是在2x2的特征图块中实现的，步长为(2, 2)。这个过程包括计算特征图每个块的平均值，这意味着特征图中的每个2x2正方形都被下采样到其平均值。为了说明，考虑前一小节中讨论的线检测卷积滤波器产生的6x6特征图。我们可以手动探索平均池化操作对该特征图第一行的应用。
- en: In a typical CNN architecture, convolution layers and pooling layers are interleaved
    in a recurring pattern.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的CNN架构中，卷积层和池化层以重复的模式交替出现。
- en: ReLUs
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLUs
- en: '**ReLUs** serve as crucial activation functions within NNs. Their frequent
    utilization occurs multiple times within a single network, often following each
    convolutional layer. A ReLU layer is composed of neurons that execute the function
    *f(x) = max(0, x)*. Incorporating these layers enhances the network’s non-linearity
    while maintaining the convolutional levels’ receptive fields unchanged.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLUs**作为神经网络中的关键激活函数，在单个网络中频繁使用，通常在每个卷积层之后出现多次。ReLU层由执行函数*f(x) = max(0,
    x)*的神经元组成。引入这些层增强了网络的非线性，同时保持卷积层的感受野不变。'
- en: ReLUs are favored over alternative functions such as hyperbolic tangent or sigmoid,
    primarily due to their capacity to accelerate the training process significantly
    without substantially compromising generalization accuracy. By employing ReLU
    layers, network training becomes remarkably swifter, all while preserving comparable
    performance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUs比双曲正切或Sigmoid等替代函数更受欢迎，主要是因为它们能够显著加速训练过程，同时不会大幅降低泛化精度。通过使用ReLU层，网络训练变得非常快，同时保持相似的性能。
- en: 'These layer types lack adjustable parameters, thus executing a fixed function.
    Furthermore, they don’t possess any adaptable parameters either. Layers devoid
    of tunable parameters simplify the backward propagation process: errors computed
    up to that point, stemming from the subsequent layer, are propagated backward
    to the preceding layer.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层类型没有可调整的参数，因此执行固定函数。此外，它们也不具备任何可适应的参数。没有可调整参数的层简化了反向传播过程：到该点为止计算出的错误，源于后续层，被反向传播到前一层。
- en: FC layer
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FC层
- en: This layer type mirrors the structure of any layer found in a traditional ANN
    featuring an FC architecture. In an **FC layer**, each neuron establishes connections
    with all neurons from the preceding layer, specifically interacting with their
    activations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种层类型与具有FC架构的传统ANN中发现的任何层的结构相似。在**FC层**中，每个神经元都与前一层的所有神经元建立连接，特别是与它们的激活进行交互。
- en: 'Unlike what has been observed thus far in CNNs, this layer type does not adhere
    to the principle of local connectivity. An FC layer establishes connections spanning
    the entire input volume, which naturally results in a multitude of connections.
    The sole adjustable parameter in this layer type is the number of neurons, *K*,
    that constitute it. The fundamental characteristic of an FC layer can be summarized
    as follows: linking its K neurons with the entire input volume and calculating
    the activation for each of these K neurons.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 与迄今为止在CNN中观察到的不同，这种层类型不遵循局部连接原则。FC层建立跨越整个输入体积的连接，这自然导致大量连接。这种层类型中唯一的可调整参数是该层由多少个神经元*K*组成。FC层的基本特征可以总结如下：将其K个神经元与整个输入体积相连接，并计算这些K个神经元中的每个神经元的激活。
- en: In practice, the outcome will be a singular 1 x 1 x K vector, encapsulating
    the computed activations. The transition from an input volume, organized in three
    dimensions, to a singular output vector in one dimension (1 x 1 x K) after implementing
    an FC layer signifies that the utilization of additional convoluted layers becomes
    infeasible. In the realm of CNNs, FC layers primarily serve the purpose of consolidating
    information amassed up to that point, presenting it as a singular value (the activation
    of one of its neurons). This singular value subsequently forms the basis for subsequent
    computations in the final classification process.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，结果将是一个单一的1 x 1 x K向量，封装了计算的激活值。从三维组织输入体积到实现FC层后的一维单一输出向量（1 x 1 x K），这表明使用额外的卷积层变得不可行。在CNN领域，FC层主要起到整合到目前为止收集到的信息的作用，将其呈现为单一值（其神经元之一的活动）。这个单一值随后成为最终分类过程中后续计算的基础。
- en: Having thoroughly examined each element of a CNN, it is now fitting to delve
    into the overarching structure of a complete CNN.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在彻底检查了CNN的每个元素之后，现在适合深入研究一个完整CNN的整体结构。
- en: Building a CNN in MATLAB
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在MATLAB中构建CNN
- en: In this section, we will see how to train a CNN for image classification, starting
    from the images as input layers. The overall architecture of a CNN typically comprises
    a sequence of convolutional layers, interspersed with ReLU layers and, when appropriate,
    standardization and pooling layers. Ultimately, the network concludes with a series
    of FC layers leading to the output layer.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何从图像作为输入层开始训练CNN进行图像分类。CNN的整体架构通常包括一系列卷积层，穿插着ReLU层，以及适当的标准化和池化层。最终，网络以一系列FC层结束，这些层通向输出层。
- en: 'CNNs consist of two primary types of neurons:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: CNN由两种主要类型的神经元组成：
- en: '**Processing neurons**: These neurons undertake the responsibility of processing
    specific sections of the input image through convolution functions. Their primary
    role involves extracting distinctive features from the input data.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理神经元**：这些神经元负责通过卷积函数处理输入图像的特定部分。它们的主要作用是从输入数据中提取独特特征。'
- en: '**Aggregation or pooling neurons**: These neurons aggregate the input data
    and reduce its dimensions through subsampling, enhancing efficiency for subsequent
    layers’ processing.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合或池化神经元**：这些神经元通过子采样聚合输入数据并减少其维度，提高了后续层处理效率。'
- en: 'By assembling the output values from a given layer, it becomes possible to
    reconstruct an intermediate image that serves as a foundation for subsequent layers’
    operations:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过组合给定层的输出值，可以重建一个中间图像，该图像作为后续层操作的起点：
- en: '![Figure 6.2 – CNN architecture](img/B21156_06_02.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – CNN架构](img/B21156_06_02.jpg)'
- en: Figure 6.2 – CNN architecture
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – CNN架构
- en: The fundamental concept involves commencing with a sizable image and progressively
    condensing the data through incremental steps until a singular outcome is achieved.
    As the number of convolutional passages increases, the NN’s capacity to comprehend
    and handle intricate functions amplifies.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 基本概念涉及从一个大图像开始，通过逐步的增量步骤逐渐压缩数据，直到达到单一的结果。随着卷积通道数量的增加，神经网络理解和处理复杂函数的能力增强。
- en: 'The configuration of a basic CNN can be succinctly described through the following
    components:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 基本CNN的配置可以通过以下组件简要描述：
- en: An input layer tasked with acquiring input elements, such as images
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责获取输入元素（如图像）的输入层
- en: A sequence of convolutional layers, occasionally punctuated by a ReLU layer
    and, when deemed appropriate, pooling layers
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列卷积层，偶尔被ReLU层打断，并在适当的时候使用池化层
- en: A series of FC layers
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列FC层
- en: An output layer that furnishes the final results
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供最终结果的输出层
- en: Recent research has indicated that the significance of FC layers might be less
    substantial. Nevertheless, for the present context, the outlined structure represents
    the typical architecture of a CNN.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，FC层的重要性可能较小。然而，在当前背景下，概述的结构代表了CNN的典型架构。
- en: '**Object recognition** refers to the capacity to locate a specific object within
    a series of images or videos. Humans effortlessly identify diverse objects in
    images, even when those objects appear differently. Moreover, objects can be distinguished
    even when only parts of them are visible. However, this remains a challenging
    task for the broader field of CV.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标识别**指的是在一系列图像或视频中定位特定对象的能力。人类在图像中轻松识别各种对象，即使这些对象以不同的方式出现。此外，即使只看到对象的一部分，也可以区分对象。然而，这仍然是计算机视觉领域的挑战性任务。'
- en: In each image, every object boasts numerous intriguing features that can be
    extracted to craft a depiction of it. This portrayal subsequently serves to distinguish
    the object when attempting to spot it amid multiple objects in a test image. Ensuring
    the features taken from the reference image are unaffected by differences in image
    scale, disturbances, lighting variations, and distortions is crucial for establishing
    dependable recognition.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在每张图像中，每个对象都拥有许多引人入胜的特征，可以从中提取出来以描绘该对象。这种描绘随后用于在测试图像中的多个对象中识别该对象时进行区分。确保从参考图像中提取的特征不受图像尺度、干扰、光照变化和畸变的影响，对于建立可靠的识别至关重要。
- en: CNNs are particularly well suited for this endeavor, providing algorithms that
    excel in the identification of objects with remarkable accuracy. This is because
    they are able to learn features from images that are invariant to translation,
    scale, and rotation. This is due to the use of convolutional layers, which apply
    a filter to a small region of the input image and then slide the filter across
    the image. This allows the network to learn features that are specific to different
    parts of the image, regardless of where they are located. Additionally, pooling
    layers are used to reduce the dimensionality of the feature maps, which makes
    the network more efficient to train and run.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）特别适合这项任务，提供了在对象识别方面表现出色的算法。这是因为它们能够从图像中学习到对平移、尺度和旋转不变的特性。这是由于使用了卷积层，它们将过滤器应用于输入图像的小区域，然后将过滤器在图像上滑动。这使得网络能够学习到特定于图像不同部分的特征，而不管它们位于何处。此外，还使用了池化层来降低特征图的维度，这使得网络在训练和运行时更加高效。
- en: For CNN training, we will use Pistachio image dataset. The dataset offers data
    for analyzing the species of pistachio using a large collection of photos.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CNN 训练，我们将使用 Pistachio 图像数据集。该数据集提供了一组大量照片，用于分析开心果的品种。
- en: 'It encompasses a total of 2,148 images categorized into 2 classes—`Kirmizi`
    and `Siirt`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含总共 2,148 张图像，分为 2 个类别——`Kirmizi` 和 `Siirt`：
- en: 'To start, we must load the dataset in the MATLAB workspace. The dataset is
    available in ZIP format on the site linked in the *Technical requirements* section.
    To facilitate the reader’s work, the images have been divided according to class
    into folders named according to the category and resized to reduce the weight
    of the data. These folders are available for download on the book’s GitHub site.
    Once the file has been downloaded and the contents extracted, it will be enough
    to define the path of the root folder. At this point, it will be possible to upload
    the data to the MATLAB workspace as follows:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须在 MATLAB 工作空间中加载数据集。数据集以 ZIP 格式存储在“技术要求”部分链接的网站上。为了方便读者工作，图像已根据类别划分到文件夹中，并按类别命名，同时调整大小以减轻数据重量。这些文件夹可在本书的
    GitHub 网站上下载。一旦文件下载并解压，只需定义根文件夹的路径即可。此时，就可以按照以下方式将数据上传到 MATLAB 工作空间：
- en: '[PRE0]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To load the data, we used the `imageDatastore()` function, which employs an
    `ImageDatastore` entity to oversee a compilation of image files, with each singular
    image fitting within memory, although the complete image collection might not.
    Constructing an `ImageDatastore` entity involves utilizing the `imageDatastore`
    function, where you can define its attributes. Subsequently, you can import and
    manipulate the data using the functions associated with the entity. Three parameters
    were used, as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了加载数据，我们使用了 `imageDatastore()` 函数，该函数使用 `ImageDatastore` 实体来管理一系列图像文件，尽管每个单独的图像可以适应内存，但整个图像集合可能不行。构建
    `ImageDatastore` 实体涉及使用 `imageDatastore` 函数，其中可以定义其属性。随后，可以使用与实体关联的函数导入和处理数据。使用了三个参数，如下所示：
- en: '`Location= ''C:\MatlabScript\PistachioShort''`: This is the location of the
    dataset. To replicate the example, the reader has to simply substitute the path
    of the folder in which the dataset was stored.'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Location= ''C:\MatlabScript\PistachioShort''`：这是数据集的位置。为了复制示例，读者只需简单地替换存储数据集的文件夹的路径。'
- en: '`IncludeSubfolders`: The inclusion of subfolders can be controlled using the
    `IncludeSubfolders` name-value argument, which accepts either `true` or `false`.
    Opt for `true` to encompass all files and subfolders within each main folder,
    or select `false` to solely encompass files within each main folder. When `IncludeSubfolders`
    is not explicitly specified, the default value is `false`.'
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IncludeSubfolders`：可以通过`IncludeSubfolders`名称-值参数控制子文件夹的包含，该参数接受`true`或`false`。选择`true`以包含每个主文件夹内的所有文件和子文件夹，或选择`false`以仅包含每个主文件夹内的文件。当未明确指定`IncludeSubfolders`时，默认值为`false`。'
- en: '`LabelSource`: The origin of label data is determined through the `LabelSource`
    name-value argument, which accepts either `none` or `foldernames`. By specifying
    `none`, the `Labels` property remains devoid of labels. On the other hand, designating
    `foldernames` results in the assignment of labels based on the folder names, which
    are then stored within the `Labels` property. Subsequent adjustments to the labels
    can be made by directly accessing the `Labels` property. It’s important to note
    that the `LabelSource` name-value argument cannot be utilized when a `FileSet`
    object serves as the file or folder location.'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LabelSource`：标签数据的来源通过`LabelSource`名称-值参数确定，该参数接受`none`或`foldernames`。通过指定`none`，`Labels`属性将保持无标签状态。另一方面，指定`foldernames`会导致根据文件夹名称分配标签，这些标签随后存储在`Labels`属性中。可以通过直接访问`Labels`属性来对标签进行后续调整。需要注意的是，当`FileSet`对象作为文件或文件夹位置时，不能使用`LabelSource`名称-值参数。'
- en: 'We can now view some of the images loaded through a random process:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在可以通过随机过程查看一些加载的图像：
- en: '[PRE1]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this piece of code, we have used the `randperm()` function, which generates
    a row vector comprising `9` distinct random integers chosen from the range of
    `1` to `2148`. Each number generated was used as an index to identify an image
    file path stored in the `Data.Files` property.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，我们使用了`randperm()`函数，该函数生成一个包含`9`个从`1`到`2148`范围内选取的不同随机整数的行向量。每个生成的数字都被用作索引，以识别存储在`Data.Files`属性中的图像文件路径。
- en: 'The following output was returned:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下输出已返回：
- en: '![Figure 6.3 – Image samples from PistachioShort dataset](img/B21156_06_03.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – PistachioShort数据集的图像样本](img/B21156_06_03.jpg)'
- en: Figure 6.3 – Image samples from PistachioShort dataset
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – PistachioShort数据集的图像样本
- en: 'We are now able to determine the image count in each category. The `ClassItemsNumber`
    variable represents a table containing the labels and the corresponding number
    of images associated with each of them:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在能够确定每个类别的图像数量。`ClassItemsNumber`变量代表一个包含标签及其各自相关联的图像数量的表格：
- en: '[PRE2]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We used the `countEachLabel()` function that produces a summary table indicating
    the labels within the `Data` dataset and the respective count of associated files
    for each label.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`countEachLabel()`函数，该函数生成一个汇总表，显示`Data`数据集中的标签及其各自关联文件的计数。
- en: 'The following table was printed:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格已打印：
- en: '[PRE3]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, there are two categories, and there are enough images in each
    category.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，有两个类别，并且每个类别中都有足够的图像。
- en: 'To start, it is necessary to divide the data at our disposal into two subsets;
    the first will be used for training, and the second for algorithm validation.
    Data splitting is a crucial step in ML and data analysis whereby a dataset is
    divided into two or more subsets for the purpose of training, validation, and
    testing of a model. We have `2148` samples divided into `1232` Kirmizi species
    and `916` Siirt species. Usually, the data would be split into 80% for the training
    and 20% for validation. For this purpose, we decide to use 700 samples for the
    training and the remainder for validation:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，有必要将我们可用的数据分为两个子集；第一个将用于训练，第二个用于算法验证。数据拆分是机器学习和数据分析中的一个关键步骤，其中将数据集分为两个或多个子集，用于模型的训练、验证和测试。我们有`2148`个样本，分为`1232`个Kirmizi物种和`916`个Siirt物种。通常，数据会被分成80%用于训练，20%用于验证。为此，我们决定使用700个样本进行训练，其余用于验证：
- en: '[PRE4]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To do this, the `splitEachLabel()` function was used: this function divides
    the image files in the Data dataset into two distinct datastores—namely, `DataTrain`
    and `DataValidation`. The freshly created `DataTrain` datastore encompasses the
    initial `TrainSamples` value from every category, while the `DataValidation` datastore
    holds the remaining files from each category. `TrainSamples` can either be a fractional
    value between `0` and `1`, denoting the proportion of files to allocate to `DataTrain`,
    or an integer representing the exact count of files to be placed in `DataTrain`
    for each category.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了做到这一点，使用了 `splitEachLabel()` 函数：这个函数将数据集中的图像文件分为两个不同的数据存储——即 `DataTrain` 和
    `DataValidation`。新创建的 `DataTrain` 数据存储包含了每个类别的初始 `TrainSamples` 值，而 `DataValidation`
    数据存储则包含了每个类别的剩余文件。`TrainSamples` 可以是一个介于 `0` 和 `1` 之间的分数值，表示分配给 `DataTrain` 的文件比例，或者是一个整数，表示每个类别要放入
    `DataTrain` 的确切文件数。
- en: 'Now, we can start building our CNN. As anticipated, a CNN is formed by a series
    of layers connected to each other. To get started, you need to use a layer to
    import your input data:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始构建我们的 CNN。正如预期的那样，CNN 是由一系列相互连接的层组成的。要开始，你需要使用一个层来导入你的输入数据：
- en: '[PRE5]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `layers` variable is an array containing a list of layers of the CNN and
    defines the architecture of NNs for DL. As a first layer, we set the `imageInputLayer`
    layer: this layer serves as an image input, feeding 2D images into a NN while
    also implementing data normalization. This layer provides an image input layer
    and defines the unchangeable `InputSize` attribute.'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`layers` 变量是一个包含 CNN 层列表的数组，它定义了用于深度学习的神经网络架构。作为第一层，我们设置了 `imageInputLayer`
    层：这个层作为图像输入，将 2D 图像输入到神经网络中，同时实现数据归一化。这个层提供了一个图像输入层，并定义了不可更改的 `InputSize` 属性。'
- en: The `InputSize` attribute is a depiction of the input data’’ dimensions, represented
    as a 1D array of integers `[h w c]`, where `h`, `w`, and `c` denote the height,
    width, and number of channels respectively. In this case, we have an RGB image
    with height = 64 and width = 64.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`InputSize` 属性是输入数据维度的表示，表示为一个整数数组 `[h w c]`，其中 `h`、`w` 和 `c` 分别表示高度、宽度和通道数。在这种情况下，我们有一个高度和宽度均为
    64 的 RGB 图像。'
- en: 'After the input layer, we set the first block of three layers in sequence:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在输入层之后，我们按顺序设置了第一个由三个层组成的块：
- en: '[PRE6]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Downsampling at a 2:1 ratio and stride = 2 indicate a shift by 2 columns and
    2 rows in the pooling operation. A 2D max pooling layer conducts downsampling
    by partitioning the input into rectangular pooling regions and subsequently determining
    the maximum value within each region.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以 2:1 的比例下采样和步长 = 2 表示在池化操作中移动 2 列和 2 行。一个 2D 最大池化层通过将输入划分为矩形池化区域并随后确定每个区域内的最大值来实现下采样。
- en: 'Now, we will apply a second block of layers like the first, changing the parameters:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将应用第二个与第一个相似的层块，改变参数：
- en: '[PRE7]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that, a new pooling layer is applied:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，应用了一个新的池化层：
- en: '[PRE8]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And finally, a third block of layers:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，第三个层块：
- en: '[PRE9]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In an FC layer, the input is multiplied by a weight matrix and then augmented
    with a bias vector. A bias vector is a set of values added to the weighted input
    of a neuron before applying the activation function. It plays a crucial role in
    adjusting neuron activation thresholds and enabling the network to represent complex
    relationships in data. The `Size` parameter output specifies the output size;
    we are classifying a two-species pistachio, so the size is `2`. This layer category
    replicates the configuration of layers typically present in a conventional ANN
    that employs an FC design. Within an FC layer, each neuron establishes connections
    with every neuron from the previous layer, engaging directly with their respective
    activations. softmaxLayer
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在全连接层（FC layer）中，输入被乘以一个权重矩阵，然后通过一个偏置向量进行增强。偏置向量是一组在应用激活函数之前添加到神经元加权输入中的值。它在调整神经元激活阈值和使网络能够表示数据中的复杂关系方面起着至关重要的作用。`Size`
    参数输出指定了输出大小；我们正在对两种类型的开心果进行分类，所以大小是 `2`。这个层类别复制了在采用全连接设计的传统人工神经网络中通常存在的层的配置。在全连接层中，每个神经元与前一层的每个神经元建立连接，直接参与它们的激活。
- en: '`softmaxLayer` is a type of layer used in NNs, specifically designed to apply
    the `softmax` function to the input. The `softmax` function is often used in classification
    tasks to convert raw scores or logits into a probability distribution over multiple
    classes. This layer is commonly utilized as the final layer in a NN for multi-class
    classification, where the output values are transformed into probabilities that
    sum up to `1`, making it easier to interpret the model’s predictions. classificationLayer];'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`softmaxLayer`是神经网络中使用的层类型，专门设计用于对输入应用`softmax`函数。`softmax`函数通常用于分类任务，将原始分数或logits转换为多个类别的概率分布。此层通常用作神经网络的多类分类的最终层，其中输出值被转换为概率，这些概率之和为`1`，使得模型的预测更容易解释。classificationLayer];'
- en: The classification layer computes cross-entropy loss for both standard and weighted
    classification tasks involving mutually exclusive classes. The layer automatically
    determines the number of classes based on the dimension of the output from the
    preceding layer.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类层计算标准分类任务和涉及互斥类的加权分类任务的交叉熵损失。该层根据前一层的输出维度自动确定类别的数量。
- en: 'Right now, we have to set the option of the CNN:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们必须设置CNN的选项：
- en: '[PRE10]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `trainingOptions()` function was used to set several options relevant to
    the training of the network. The following options were set:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用了`trainingOptions()`函数来设置与网络训练相关的几个选项。以下选项被设置：
- en: '`sdm`: Solver for training the NN; we have used the `InitialLearnRate`: The
    initial learning rate employed in training is a positive numerical value. When
    the learning rate is excessively low, the training process might become prolonged.
    Conversely, if the learning rate is excessively high, the training could yield
    suboptimal results or diverge.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sdm`: 用于训练神经网络的求解器；我们使用了`InitialLearnRate`：训练中使用的初始学习率是一个正数值。当学习率过低时，训练过程可能会变得漫长。相反，如果学习率过高，训练可能会产生次优结果或发散。'
- en: '`MaxEpochs`: The maximum number of training epochs is defined as a positive
    integer. In the context of gradient descent optimization using mini-batches, an
    iteration signifies a single step aimed at minimizing the loss function. An epoch
    refers to the full iteration of the training algorithm across the entire training
    dataset.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxEpochs`: 最大训练周期的数量定义为正整数。在梯度下降优化中使用小批量时，一个迭代表示旨在最小化损失函数的单步。一个周期是指训练算法在整个训练数据集上的完整迭代。'
- en: '`Shuffle`: Option for data shuffling. For this, we employed the practice of
    shuffling the training data before every training epoch, and similarly, shuffling
    the validation data before each validation of the NN.'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shuffle`: 数据洗牌选项。为此，我们在每个训练周期之前对训练数据进行洗牌，同样，在每次神经网络验证之前对验证数据进行洗牌。'
- en: '`ValidationData`: The validation data to be utilized during training is defined
    as either a datastore, a table, or an array of cells containing the predictors
    and validation responses.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidationData`: 在训练期间要使用的验证数据定义为数据存储、表或包含预测因子和验证响应的单元格数组。'
- en: '`ValidationFrequency`: The NN’s validation rate is expressed as a positive
    integer, representing the count of interactions. `ValidationFrequency` signifies
    the interval, in terms of iterations, at which validation metrics are assessed.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidationFrequency`: 神经网络的验证率用正整数表示，表示交互次数。`ValidationFrequency`表示验证指标评估的迭代间隔。'
- en: '`Verbose`: For displaying training progress information.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Verbose`: 用于显示训练进度信息。'
- en: '`Plots`: Upon configuring the `training-progress` training option in `trainingOptions`
    and initiating the network training using `trainNetwork()`, a plot is generated.
    This plot showcases training metrics for each iteration. Each iteration involves
    an estimation of the gradient and an adjustment of the network parameters. If
    you provide validation data within `trainingOptions`, the plot will additionally
    present validation metrics whenever the network undergoes validation by the `trainNetwork`
    process.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Plots`: 在配置`trainingOptions`中的`training-progress`训练选项并使用`trainNetwork()`启动网络训练后，将生成一个图表。此图表展示了每个迭代的训练指标。每个迭代涉及梯度的估计和网络参数的调整。如果你在`trainingOptions`中提供了验证数据，图表将还会在`trainNetwork`过程进行网络验证时，额外展示验证指标。'
- en: 'Finally, we have to train the network:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须训练网络：
- en: '[PRE11]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `trainNetwork()` function facilitates the training of DNNs. This training
    can be executed on either a CPU or a GPU. For tasks such as image classification
    and image regression, it’s possible to train a single NN concurrently using multiple
    GPUs or a local/remote parallel pool. However, the employment of GPUs or parallel
    processing necessitates the presence of Parallel Computing Toolbox. Additionally,
    utilizing a GPU for DL mandates the availability of a compatible GPU device.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`trainNetwork()` 函数简化了深度神经网络（DNN）的训练过程。这种训练可以在CPU或GPU上执行。对于图像分类和图像回归等任务，可以使用多个GPU或本地/远程并行池同时训练单个神经网络。然而，使用GPU或并行处理需要Parallel
    Computing Toolbox。此外，使用GPU进行深度学习（DL）需要兼容的GPU设备。'
- en: 'The following plot was returned to the `trainNetwork()` function:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面的情节被返回到`trainNetwork()`函数：
- en: '![Figure 6.4 – Training process](img/B21156_06_04.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 训练过程](img/B21156_06_04.jpg)'
- en: Figure 6.4 – Training process
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 训练过程
- en: 'The plot in *Figure 6**.4* will be progressively updated throughout the training
    process. In this way, it will be possible to check how the algorithm manages to
    adjust the weights to reach convergence. The plot in *Figure 6**.4* consists of
    three curves:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.4* 中的情节将在整个训练过程中逐步更新。这样，就可以检查算法如何调整权重以达到收敛。*图6**.4* 中的情节由三条曲线组成：'
- en: '**Training error**: This curve shows the average error of the network on the
    training data as a function of the number of training epochs. Epochs are the number
    of times the network is trained on the entire training dataset. A low training
    error indicates that the network is learning well from the training data.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练误差**：此曲线显示了网络在训练数据上的平均误差作为训练轮次数量的函数。轮次是网络在整个训练数据集上训练的次数。低训练误差表明网络正在很好地从训练数据中学习。'
- en: '**Validation error**: This curve shows the average error of the network on
    the validation data as a function of the number of training epochs. Validation
    data is data that the network was not trained on, and it is used to evaluate how
    well the network generalizes to unseen data. A low validation error indicates
    that the network is learning to generalize well to new data.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证误差**：此曲线显示了网络在验证数据上的平均误差作为训练轮次数量的函数。验证数据是网络未训练过的数据，用于评估网络对未见数据的泛化能力。低验证误差表明网络正在学习很好地泛化到新数据。'
- en: '**Network weights**: This curve shows the magnitude of the weights of the network
    as a function of the number of training epochs. The weights are the parameters
    that the network learns from the training data. A stable weight curve indicates
    that the network is not overfitting to the training data.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络权重**：此曲线显示了网络权重的大小作为训练轮次数量的函数。权重是网络从训练数据中学习的参数。稳定的权重曲线表明网络没有过度拟合训练数据。'
- en: After seeing how to train a CNN in MATLAB, we now need to learn how to interpret
    the results and use the validation metrics correctly.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了如何在MATLAB中训练CNN之后，我们现在需要学习如何解释结果并正确使用验证指标。
- en: Exploring the model’s results
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索模型的结果
- en: Evaluating results is an essential part of any CNN implementation process. This,
    of course, is true for any algorithm based on ML. Evaluation metrics are quantitative
    measures used to assess the performance and quality of a model, algorithm, or
    system in various tasks, such as ML, data analysis, and optimization. These metrics
    provide a way to objectively quantify how well a model is performing and to compare
    different models or approaches.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果是任何卷积神经网络（CNN）实现过程中的一个重要部分。当然，这对于任何基于机器学习的算法来说也是正确的。评估指标是用于评估模型、算法或系统在不同任务（如机器学习、数据分析、优化）中的性能和质量的定量度量。这些指标提供了一种客观量化模型表现好坏的方法，并比较不同的模型或方法。
- en: The type of metric to adopt obviously depends on the type of algorithm we are
    implementing; in the previous section, we implemented a CNN for the classification
    of the pistachio species. So, let’s take a look at the metrics available for this
    type of algorithm.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 采用的指标类型显然取决于我们正在实现的算法类型；在前一节中，我们实现了一个用于开心果物种分类的CNN。因此，让我们看看这种类型算法可用的指标。
- en: 'For a classification task, we can use the following metrics:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，我们可以使用以下指标：
- en: '**Accuracy**: The proportion of correctly classified instances out of the total
    instances'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：正确分类的实例占总实例的比例'
- en: '**Precision**: The ratio of true positive predictions to the total number of
    positive predictions, indicating the accuracy of positive predictions'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**：真正阳性预测与总阳性预测数之比，表明阳性预测的准确性'
- en: '**Recall (Sensitivity)**: The ratio of true positive predictions to the total
    number of actual positives, indicating the model’s ability to identify positive
    cases'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率（灵敏度）**：真正阳性预测与实际阳性总数之比，表明模型识别阳性病例的能力'
- en: '**F1-score**: The harmonic mean of precision and recall, providing a balance
    between them'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**：精确率和召回率的调和平均数，提供两者之间的平衡'
- en: '**Receiver Operating Characteristic (ROC) curve**: A graph showing the trade-off
    between true positive rate and false positive rate at various thresholds'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收者操作特征（ROC）曲线**：显示在不同阈值下真正阳性率和假阳性率之间的权衡'
- en: '**Area Under the ROC Curve (AUC-ROC)**: A metric that quantifies the overall
    performance of a binary classification model'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROC曲线下的面积（AUC-ROC）**：一个量化二元分类模型整体性能的指标'
- en: 'Let us try to deepen the concept of accuracy. Accuracy is a fundamental evaluation
    metric used in classification tasks to measure the proportion of correctly predicted
    instances out of the total instances in a dataset. It provides an overall view
    of how well a classification model is performing. The accuracy formula is:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试深化准确率的概念。准确率是分类任务中用于衡量数据集中正确预测实例比例的基本评估指标，提供了对分类模型性能的整体看法。准确率公式是：
- en: Accuracy =  Number of Correct Predictions  ______________________  Total Number
    of Predictions
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 = 正确预测数 / 总预测数
- en: 'For example, if you have a binary classification problem with 100 instances,
    and your model correctly predicts 90 of them, the accuracy would be:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一个包含100个实例的二进制分类问题，并且你的模型正确预测了其中的90个，那么准确率将是：
- en: Accuracy =  90 _ 100  = 0.90, or 90%
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 = 90 / 100 = 0.90，或90%
- en: While accuracy is a simple and intuitive metric, it might not be suitable in
    cases of imbalanced classes, where one class significantly outnumbers the other.
    In such situations, a high accuracy could be misleading if the model is performing
    well on the dominant class but poorly on the minority class. In these cases, precision,
    recall, F1-score, and other metrics might provide a more accurate representation
    of the model’s performance. Also, in this case, it is important to point out that
    when the dataset is not sufficiently balanced between the classes, the classification
    result may not be applied to all the classes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然准确率是一个简单直观的指标，但在类别不平衡的情况下可能不适用，其中一类显著多于另一类。在这种情况下，如果模型在多数类上表现良好而在少数类上表现不佳，高准确率可能会产生误导。在这些情况下，精确率、召回率、F1分数和其他指标可能更能准确地反映模型的性能。此外，在这种情况下，重要的是指出，当数据集在类别之间不平衡时，分类结果可能不适用于所有类别。
- en: 'So, let’s try to apply this metric to the case study analyzed in the previous
    section for the classification of the pistachio species. We have already trained
    the network, and at this point, we can use it to make classifications using data
    never seen before by the algorithm:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们尝试将此指标应用于上一节中分析的案例研究，用于杏仁种类的分类。我们已经训练了网络，在这个阶段，我们可以使用它来对算法从未见过的数据进行分类：
- en: 'To carry out the validation, we start by classifying the collected images in
    the `DataValidation` subset:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进行验证，我们首先对`DataValidation`子集中的收集到的图像进行分类：
- en: '[PRE12]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This function employs a trained DNN to classify data. Predictions can be generated
    using the trained network on either a CPU or GPU. Utilizing a GPU necessitates
    both a *Parallel Computing Toolbox™ license* and a compatible GPU device. Two
    parameters are passed: the CNN trained (`CNnet`) and the input dataset (`DataValidation`).'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此函数使用训练好的深度神经网络（DNN）对数据进行分类。可以使用训练好的网络在CPU或GPU上生成预测。使用GPU需要**并行计算工具箱™许可证**和兼容的GPU设备。传递了两个参数：训练好的卷积神经网络（`CNnet`）和输入数据集（`DataValidation`）。
- en: 'After that, we must extract the label of the input data; this information is
    contained in the image datastore. We have only to use the `labels` parameter,
    as follows:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们必须提取输入数据的标签；此信息包含在图像数据存储中。我们只需使用`labels`参数，如下所示：
- en: '[PRE13]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s calculate the accuracy using the formula previously introduced:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用之前引入的公式来计算准确率：
- en: '[PRE14]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following result is returned:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE15]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: An accuracy of 86.95% was obtained; this tells us that the CNN-based algorithm
    can correctly classify 86.95% of the pistachio images we passed it. We also recall
    that these images had never been used by the network training algorithm.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 获得了86.95%的准确率；这告诉我们，基于CNN的算法可以正确分类我们传递给它的86.95%的开心果图像。我们还记得这些图像从未被网络训练算法使用过。
- en: To better understand the performance of the model, we can draw a confusion matrix.
    A confusion matrix is a tabular representation used in the field of ML and statistics
    to evaluate the performance of a classification model. It provides a comprehensive
    view of how well the model’s predictions align with the actual classes in a dataset.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地理解模型的性能，我们可以绘制一个混淆矩阵。混淆矩阵是机器学习和统计学领域中用于评估分类模型性能的表格表示。它提供了一个全面的视角，展示了模型的预测与数据集中实际类别之间的匹配程度。
- en: 'A confusion matrix is organized into four categories:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混淆矩阵分为四个类别：
- en: '**True Positives** (**TP**): Instances that are correctly predicted as positive'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例** (**TP**): 被正确预测为正例的实例'
- en: '**True Negatives** (**TN**): Instances that are correctly predicted as negative'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例** (**TN**): 被正确预测为负例的实例'
- en: '**False Positives** (**FP**): Instances that are incorrectly predicted as positive
    when they are actually negative (**Type** **I error**)'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例** (**FP**): 实际为负例但被错误预测为正例的实例（**I型错误**）'
- en: '**False Negatives** (**FN**): Instances that are incorrectly predicted as negative
    when they are actually positive (**Type** **II error**)'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例** (**FN**): 实际为正例但被错误预测为负例的实例（**II型错误**）'
- en: 'The matrix format looks like this:'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 矩阵格式如下所示：
- en: '|  | **Predicted Positive** | **Predicted Negative** |'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | **预测正例** | **预测负例** |'
- en: '| **Actual Positive** | True Positives | False Negatives |'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| **实际正例** | 真正例 | 假正例 |'
- en: '| **Actual Negative** | False Positives | True Negatives |'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| **实际负例** | 假正例 | 真负例 |'
- en: Figure 6.5 – Confusion matrix terms’ meaning
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 混淆矩阵术语的含义
- en: 'In MATLAB, we can use the following command:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在MATLAB中，我们可以使用以下命令：
- en: '[PRE16]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function generates a confusion matrix chart using true labels stored in
    `DataValLabel` and predicted labels stored in `CNNPredLabel`. It then returns
    a `ConfusionMatrixChart` object.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用存储在`DataValLabel`中的真实标签和存储在`CNNPredLabel`中的预测标签生成混淆矩阵图表。然后它返回一个`ConfusionMatrixChart`对象。
- en: 'The following results are printed:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 打印以下结果：
- en: '[PRE17]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following output was returned:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下输出：
- en: '![Figure 6.6 – Confusion matrix](img/B21156_06_05.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 混淆矩阵](img/B21156_06_05.jpg)'
- en: Figure 6.6 – Confusion matrix
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 混淆矩阵
- en: In that matrix, rows correspond to the true classes, and columns correspond
    to the predicted classes. Cells along the diagonal represent correctly classified
    instances, while off-diagonal cells represent instances that were classified incorrectly.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个矩阵中，行对应于真实类别，列对应于预测类别。对角线上的单元格代表正确分类的实例，而对角线外的单元格代表被错误分类的实例。
- en: 'We can also show the accuracy calculation from the confusion matrix table as
    this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从混淆矩阵表中展示准确率的计算如下：
- en: (TP + TN) / (Totalvalidationdata) = (204 + 169) / 916 = 0.8695
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (TP + TN) / (Totalvalidationdata) = (204 + 169) / 916 = 0.8695
- en: In *Figure 6**.6*, we can notice that false negatives are greater than false
    positives. In some applications, false negatives are more serious than false positives.
    For example, in the context of medical diagnosis, it is generally more important
    to avoid missing a disease than to incorrectly flag a healthy person as having
    a disease. However, in other applications, false positives may be more serious.
    For example, in the context of fraud detection, it is generally more important
    to avoid incorrectly accusing an innocent person of fraud than to miss a fraudulent
    transaction.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6**.6中，我们可以注意到假负例的数量大于假正例。在某些应用中，假负例可能比假正例更严重。例如，在医疗诊断的背景下，避免漏诊疾病通常比错误地将健康人标记为患病更重要。然而，在其他应用中，假正例可能更严重。例如，在欺诈检测的背景下，避免错误地将无辜的人指控为欺诈通常比错过欺诈交易更重要。
- en: After seeing how to implement a CNN and correctly interpret the results, we
    can now explore some of the many DL architectures available.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了如何实现CNN和正确解释结果之后，我们现在可以探索许多可用的深度学习架构。
- en: Discovering DL architectures
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现深度学习架构
- en: DL models are essentially multi-layered NNs, which refers to NNs that comprise
    multiple hidden layers (at least two) structured hierarchically. This hierarchical
    arrangement facilitates the sharing and reuse of information. Across this hierarchy,
    one can pinpoint features while disregarding unnecessary intricacies, thereby
    enhancing invariance. Within the realm of multi-level ML, deeper tiers acquire
    inputs from the outputs of prior layers and execute more complex transformations
    and abstractions. This layering approach to learning draws inspiration from the
    information processing and learning methods of mammalian brains, enabling them
    to react to external stimuli.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: DL模型本质上是由多层神经网络组成的，这指的是由多个隐藏层（至少两层）以层次结构排列的神经网络。这种层次结构安排促进了信息的共享和重用。在这个层次结构中，可以在忽略不必要的复杂性同时定位特征，从而增强不变性。在多级机器学习的领域内，更深层的层级从先前层的输出获取输入，并执行更复杂的转换和抽象。这种学习分层方法是从哺乳动物大脑的信息处理和学习方法中汲取灵感，使它们能够对外部刺激做出反应。
- en: DL architectures are the fundamental blueprints that underlie the construction
    of DNNs, enabling them to effectively learn and represent complex patterns and
    features from data. These architectures define the layout, connections, and flow
    of information within the network, determining how data is transformed and features
    are extracted at various layers. Several notable DL architectures have been developed
    over the years, each designed to tackle specific types of tasks and data. In the
    following sections, we will address the most used.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: DL架构是构建深度神经网络的基本蓝图，使它们能够有效地从数据中学习并表示复杂的模式和特征。这些架构定义了网络中的布局、连接和信息流，决定了数据在各个层如何被转换和特征如何被提取。多年来已经开发出几种显著的DL架构，每个架构都是为了解决特定类型的任务和数据而设计的。在接下来的章节中，我们将讨论最常用的架构。
- en: Understanding RNNs
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解RNN
- en: '**Feedforward NNs** (**FNNs**) operate by providing input data to the network
    that is then transformed into output. In SL scenarios, the output typically represents
    a label that corresponds to the input. These algorithms essentially link raw data
    with specific categories by recognizing underlying patterns. In contrast, recurrent
    networks incorporate not only the current input data being fed into the network
    but also information they have accumulated over time.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈神经网络**（**FNNs**）通过向网络提供输入数据，然后将其转换为输出而运行。在监督学习场景中，输出通常代表与输入相对应的标签。这些算法本质上通过识别潜在模式将原始数据与特定类别相联系。相比之下，循环网络不仅包含被输入到网络中的当前数据，还包括它们随时间积累的信息。'
- en: An RNN embodies a neural model with bidirectional information flow. Unlike feedforward
    networks where signal propagation only moves unidirectionally from inputs to outputs,
    RNNs exhibit a different behavior. In RNNs, signal propagation can traverse between
    neural layers, from a prior layer to a subsequent one, among neurons within the
    same layer, or even from a neuron to itself. A decision made by an RNN at a given
    moment influences its subsequent decisions. Consequently, RNNs possess two sources
    of input—the present and the recent past—combining to determine responses to new
    data, mirroring how people make decisions in everyday life.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: RNN体现了一个具有双向信息流的神经网络模型。与信号传播仅从输入到输出单向移动的前馈网络不同，RNN表现出不同的行为。在RNN中，信号传播可以在神经网络层之间穿越，从一个先前层到后续层，在同一层内的神经元之间，甚至从一个神经元到它自己。RNN在某一时刻做出的决策会影响其后续决策。因此，RNN有两个输入来源——当前和近期过去——结合以确定对新数据的响应，这与人们在日常生活中做出决策的方式相似。
- en: 'The key distinction between recurrent and feedforward networks lies in the
    **feedback loop** that connects RNNs to their past decisions, momentarily utilizing
    their output as input. This aspect underscores the memory aspect of RNNs. The
    inclusion of memory in NNs serves a purpose: there is intrinsic information in
    the sequence itself, and RNNs leverage this information to perform tasks that
    feedforward networks are unable to accomplish.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络和前馈网络之间的关键区别在于连接RNN到其过去决策的**反馈循环**，暂时利用其输出作为输入。这一点强调了RNN的记忆方面。在神经网络中包含记忆有其目的：序列本身具有内在信息，RNN利用这些信息执行前馈网络无法完成的任务。
- en: Access to this memory occurs based on content rather than specific addresses
    or locations. One approach entails considering memory content as the activation
    patterns on nodes within an RNN. The concept involves initiating the network with
    an activation pattern that partially or noisily represents the desired memory
    content, allowing the network to stabilize around the required content. An RNN
    belongs to the category of NNs where there exists at least one feedback connection
    between neurons, forming a directed cycle within the network’s structure.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 访问此内存是基于内容而非特定的地址或位置。一种方法是将内存内容视为RNN（递归神经网络）中节点上的激活模式。该概念涉及使用部分或噪声地表示所需内存内容的激活模式来初始化网络，从而使网络围绕所需内容稳定。RNN属于NNs（神经网络）的范畴，其中至少存在一个神经元之间的反馈连接，在网络的内部结构中形成一个有向循环。
- en: '*Figure 6**.7* illustrates a standard RNN with connections linking the hidden
    layer to the output layer:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.7* 展示了一个标准的RNN，其中连接将隐藏层与输出层相连：'
- en: '![Figure 6.7 – RNN architecture](img/B21156_06_06.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – RNN架构](img/B21156_06_06.jpg)'
- en: Figure 6.7 – RNN architecture
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – RNN架构
- en: In the preceding diagram depicting a recurrent network, the hidden layer’s weights
    are defined using both the input and output layers. Essentially, we can view RNNs
    as a variation of ANNs, with differences in the number of hidden layers and the
    flow of data patterns. RNNs exhibit distinct data flow patterns due to the cyclic
    connections between neurons. Unlike feedforward networks, RNNs can leverage internal
    memory during their computations. RNNs belong to the class of ANNs that incorporate
    connections between hidden layers, which are carried forward across time to facilitate
    the learning of sequences.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面表示循环网络的图中，隐藏层的权重使用输入层和输出层共同定义。本质上，我们可以将RNN视为ANNs的一种变体，它们在隐藏层的数量和数据模式流动方面有所不同。由于神经元之间的循环连接，RNNs表现出独特的数据流动模式。与前馈网络不同，RNNs可以在计算过程中利用内部记忆。RNNs属于一类ANNs，它们包含隐藏层之间的连接，这些连接随时间传递以促进序列的学习。
- en: RNNs derive their strength and effectiveness from their unique mechanism of
    storing and flowing data across varying time intervals. These networks are adept
    at identifying patterns in data sequences, making them valuable tools for prediction
    and forecasting. Their versatility spans multiple domains, encompassing text,
    images, speech, and time series data. Positioned among the potent members of the
    ANN family, RNNs embody the intricacies of the biological brain, encompassing
    both memory and computational capabilities.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs从其独特的机制中获得了力量和有效性，该机制可以在不同的时间间隔存储和流动数据。这些网络擅长识别数据序列中的模式，使它们成为预测和预测的有价值工具。它们的适用性跨越多个领域，包括文本、图像、语音和时间序列数据。作为ANN家族中强大的成员之一，RNNs体现了生物大脑的复杂性，包括记忆和计算能力。
- en: Analyzing LSTM networks
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析LSTM网络
- en: A specific form of RNN is the LSTM network, originally conceptualized by Hochreiter
    and Schmidhuber in 1997\. This architecture has garnered renewed attention within
    the realm of DL due to its immunity to the vanishing gradient problem and its
    proven excellence in practical results and performance.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的一种特定形式是LSTM（长短期记忆）网络，最初由Hochreiter和Schmidhuber在1997年提出。由于其对梯度消失问题的免疫性以及其在实际结果和性能方面的证明优势，这种架构在深度学习领域重新引起了关注。
- en: The vanishing gradient problem poses challenges in training ANNs through gradient-based
    learning techniques. Methods such as backpropagation adjust weights based on error
    gradients. However, these gradients can diminish exponentially as they propagate
    deeper into the network, sometimes rendering them exceedingly small and preventing
    weight adjustments. In extreme cases, this can halt network training altogether.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题在通过基于梯度的学习技术训练ANNs（人工神经网络）时提出了挑战。例如，反向传播方法根据误差梯度调整权重。然而，这些梯度在传播到网络深处时会呈指数级减小，有时变得极其微小，从而阻止权重的调整。在极端情况下，这甚至可能完全停止网络训练。
- en: LSTM networks offer an ideal solution for predicting and classifying sequential
    data, outperforming numerous traditional ML methodologies. A significant example
    is Google’s transition in 2012 from **hidden Markov models** (**HMMs**) to DNNs
    for voice recognition. By 2015, Google had adopted LSTM-based RNNs combined with
    **Connectionist Temporal Classification** (**CTC**) for improved voice recognition.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络为预测和分类序列数据提供了一个理想的解决方案，优于许多传统的ML方法。一个显著的例子是，谷歌在2012年从**隐马尔可夫模型**（**HMMs**）转向DNN进行语音识别。到2015年，谷歌已经采用了基于LSTM的RNN，并结合**连接主义时序分类**（**CTC**）来提高语音识别。
- en: CTC serves as an output and scoring function for training RNNs. An LSTM network’s
    strength lies in its ability to capture long-term dependencies among data, enhancing
    its utility in contexts such as speech recognition where understanding sentence
    context is crucial.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: CTC作为训练RNN的输出和评分函数。LSTM网络的优势在于其捕捉数据长期依赖关系的能力，增强了其在语音识别等场景中的实用性，在这些场景中理解句子上下文至关重要。
- en: An LSTM network comprises interconnected LSTM cells, each featuring input, output,
    and **forget gates**. These gates facilitate writing, reading, and resetting functions
    on the cell memory. Rather than binary, the gates are analogical, typically managed
    by sigmoid activation functions that map values to a range (0, 1). This multiplicative
    nature empowers the cells to retain information over extended periods. The input
    gate controls whether the current state is combined with incoming input, while
    the forget gate resets the cell’s state when its value drops to zero. The output
    gate determines whether the cell’s content is retrieved or not, influencing the
    network’s overall output.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LSTM网络由相互连接的LSTM单元组成，每个单元都有输入、输出和**遗忘门**。这些门使得在单元内存上执行写入、读取和重置功能成为可能。与二进制不同，这些门是模拟的，通常由sigmoid激活函数管理，将值映射到范围（0，1）。这种乘性特性使单元能够在较长的时间内保留信息。输入门控制当前状态是否与传入的输入结合，而遗忘门在值降至零时重置单元的状态。输出门确定是否检索单元的内容，影响网络的总体输出。
- en: 'The following diagram explains an LSTM unit:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了LSTM单元：
- en: '![Figure 6.8 – LSTM architecture](img/B21156_06_07.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – LSTM架构](img/B21156_06_07.jpg)'
- en: Figure 6.8 – LSTM architecture
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – LSTM架构
- en: NN-based approaches possess significant potency, enabling the extraction of
    inherent data characteristics and relationships. Notably, LSTM networks have demonstrated
    impressive real-world efficacy, boasting remarkable recognition rates. However,
    one drawback is that NNs function as **black-box models**. This means their behavior
    lacks predictability, and it’s impossible to decipher the underlying logic by
    which they process data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 基于NN的方法具有显著的潜力，能够提取内在数据特征和关系。值得注意的是，LSTM网络已经展示了令人印象深刻的实际效果，拥有显著的识别率。然而，一个缺点是NNs作为**黑盒模型**。这意味着它们的行为缺乏可预测性，无法解码它们处理数据的基本逻辑。
- en: Introducing transformer models
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍转换器模型
- en: '**Transformer models** are a revolutionary architecture in the field of NLP
    and beyond. Introduced in 2017, transformers have since become a foundational
    framework for a wide range of tasks, from language translation to image generation.
    The key innovation in transformer models is the self-attention mechanism. Traditional
    models process sequences in a linear manner, which limits their ability to capture
    long-range dependencies. In contrast, transformers employ self-attention to weigh
    the importance of different words in a sequence with respect to each other, enabling
    them to consider global context and dependencies.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换器模型**是NLP领域乃至更广泛的领域的革命性架构。2017年推出以来，转换器已经成为了从语言翻译到图像生成等一系列任务的基石框架。转换器模型的关键创新是自注意力机制。传统模型以线性方式处理序列，这限制了它们捕捉长距离依赖的能力。相比之下，转换器使用自注意力来衡量序列中不同单词相对于彼此的重要性，使它们能够考虑全局上下文和依赖关系。'
- en: The transformer architecture consists of an encoder and a decoder, each comprising
    multiple layers. The encoder processes the input data, while the decoder generates
    the output. **Attention mechanisms** within these layers allow the model to focus
    on relevant parts of the input or output sequence.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器架构由一个编码器和一个解码器组成，每个都包含多个层。编码器处理输入数据，而解码器生成输出。这些层中的**注意力机制**允许模型关注输入或输出序列的相关部分。
- en: 'Transformers can be applied with success in various applications:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器在各种应用中都可以成功应用：
- en: '**Machine translation**: Models such as *transformers* and *BERT* have set
    new benchmarks in machine translation, allowing for more accurate and contextually
    meaningful translations'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：例如 *transformers* 和 *BERT* 这样的模型在机器翻译领域设定了新的基准，使得翻译更加准确和具有语境意义'
- en: '**Language understanding**: BERT captures bidirectional context, significantly
    improving language understanding tasks'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言理解**：BERT 捕获双向上下文，显著提高了语言理解任务的效果'
- en: '**Text generation**: **Generative Pre-trained Transformer** (**GPT**) models
    generate coherent and contextually relevant text, finding applications in chatbots,
    content generation, and more'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：**生成预训练Transformer**（**GPT**）模型生成连贯且具有语境相关性的文本，在聊天机器人、内容生成等领域得到应用'
- en: '**Image generation**: **Vision Transformer** (**ViT**) extends transformers
    to images, achieving competitive performance in image classification and generation
    tasks'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成**：**视觉Transformer**（**ViT**）将 transformers 扩展到图像领域，在图像分类和生成任务中取得了有竞争力的性能'
- en: One of the reasons behind transformers’ success is their parallelizable structure,
    which speeds up training. Additionally, the attention mechanism enables capturing
    relationships without explicitly defining sequence lengths.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器成功的原因之一是其可并行化的结构，这加快了训练速度。此外，注意力机制使得能够捕捉关系，而无需显式地定义序列长度。
- en: Despite their immense capabilities, transformers have their own challenges,
    such as a high computational cost for large models and the potential for generating
    biased or inappropriate content. Researchers continue to refine and adapt the
    transformer architecture to address these issues and push the boundaries of AI
    capabilities.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管变压器具有巨大的能力，但它们也面临一些挑战，例如大型模型的高计算成本以及生成有偏见或不适当内容的潜在可能性。研究人员继续改进和调整变压器架构，以解决这些问题并推动人工智能能力的边界。
- en: Summary
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned the basic concepts of DL and discovered how to implement
    a CNN algorithm in the MATLAB environment. First, we looked at how DL enables
    automated feature extraction, then we looked at how to train a deep network, and
    then we got a taste of the most popular DL architectures.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了深度学习（DL）的基本概念，并发现了如何在 MATLAB 环境中实现 CNN 算法。首先，我们探讨了深度学习如何实现自动特征提取，然后我们探讨了如何训练深度网络，接着我们体验了最受欢迎的深度学习架构。
- en: We then focused on CNN to analyze it in detail. We learned about the different
    layers that make up this network and what functions these layers perform. We then
    saw in practice how to implement a CNN in the MATLAB environment for image classification
    of pistachio nuts. We learned how to correctly import the image database, how
    to draw the architecture of the network with the different layers one after the
    other, and how to set the network parameters. Finally, we saw how to use evaluation
    metrics for the correct interpretation of the results.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们专注于 CNN 并对其进行了详细分析。我们了解了构成这个网络的不同层以及这些层执行的功能。然后，我们看到了如何在 MATLAB 环境中实现 CNN
    以进行开心果的图像分类。我们学习了如何正确导入图像数据库，如何依次绘制网络的不同层架构，以及如何设置网络参数。最后，我们看到了如何使用评估指标来正确解释结果。
- en: In the last section, we introduced some of the most used networks, which will
    be the subject of more detailed study and application in the following chapters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们介绍了最常用的网络，这些网络将在接下来的章节中更详细地研究和应用。
- en: In the next chapter, we will understand the basic concepts of NLP, how to use
    corpora and word and sentence tokenization, and how to build a model to label
    sentences, and finally, we will implement gradient boosting techniques using MATLAB.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将了解自然语言处理（NLP）的基本概念，如何使用语料库和单词及句子分词，以及如何构建一个用于标注句子的模型，最后，我们将使用 MATLAB
    实现梯度提升技术。
- en: 'Part 3: Machine Learning in Practice'
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：机器学习实践
- en: In this part, we will take a journey into the specialized domains of MATLAB
    application, where we will reveal the immense potential of the software in various
    fields.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，我们将探索 MATLAB 应用领域的专业领域，揭示该软件在各个领域的巨大潜力。
- en: Explore the enchanting world of natural language processing using MATLAB, where
    language intricacies are deciphered, enabling machines to comprehend and respond
    to human communication. Transitioning into the visual realm, [*Chapter 8*](B21156_08.xhtml#_idTextAnchor167)
    opens doors to the manipulation and understanding of visual data, bringing pixels
    to life through sophisticated algorithms. Delve into the intricate patterns of
    [*Chapter 9*](B21156_09.xhtml#_idTextAnchor184), where historical data paves the
    way for informed predictions and strategic decision-making. Then, witness the
    power of [*Chapter 10*](B21156_10.xhtml#_idTextAnchor202), where personalized
    suggestions and recommendations emerge from intricate algorithms, enhancing user
    experiences. Concluding our exploration, immerse yourself in the realm of [*Chapter
    11*](B21156_11.xhtml#_idTextAnchor223), where subtle deviations in data patterns
    are unveiled, providing a critical edge in identifying outliers and potential
    threats. Join us as we navigate through these diverse applications, unlocking
    the capabilities of MATLAB in shaping the future of data analysis and decision
    support.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MATLAB探索自然语言处理的迷人世界，在这里语言的复杂性被解码，使机器能够理解和回应人类的交流。过渡到视觉领域，[*第8章*](B21156_08.xhtml#_idTextAnchor167)为我们打开了操作和理解视觉数据的大门，通过复杂的算法使像素变得生动。深入[*第9章*](B21156_09.xhtml#_idTextAnchor184)的复杂模式，历史数据为明智的预测和战略决策铺平了道路。然后，见证[*第10章*](B21156_10.xhtml#_idTextAnchor202)的力量，其中个性化的建议和推荐从复杂的算法中产生，增强了用户体验。在探索的结尾，沉浸在[*第11章*](B21156_11.xhtml#_idTextAnchor223)的领域中，揭示数据模式中的微妙偏差，为识别异常和潜在威胁提供了关键优势。加入我们，我们将导航这些多样化的应用，解锁MATLAB在塑造数据分析与决策支持未来中的作用。
- en: 'This part has the following chapters:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 7*](B21156_07.xhtml#_idTextAnchor145), *Natural Language Processing
    Using MATLAB*'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B21156_07.xhtml#_idTextAnchor145), *使用MATLAB进行自然语言处理*'
- en: '[*Chapter 8*](B21156_08.xhtml#_idTextAnchor167), *MATLAB for Image Processing
    and Computer Vision*'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B21156_08.xhtml#_idTextAnchor167), *MATLAB图像处理和计算机视觉*'
- en: '[*Chapter 9*](B21156_09.xhtml#_idTextAnchor184), *Time Series Analysis and
    Forecasting with MATLAB*'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B21156_09.xhtml#_idTextAnchor184), *使用MATLAB进行时间序列分析和预测*'
- en: '[*Chapter 10*](B21156_10.xhtml#_idTextAnchor202), *MATLAB Tools for Recommender
    Systems*'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B21156_10.xhtml#_idTextAnchor202), *MATLAB推荐系统工具*'
- en: '[*Chapter 11*](B21156_11.xhtml#_idTextAnchor223), *Anomaly Detection in MATLAB*'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B21156_11.xhtml#_idTextAnchor223), *MATLAB中的异常检测*'
