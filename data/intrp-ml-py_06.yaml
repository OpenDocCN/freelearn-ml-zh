- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Anchors and Counterfactual Explanations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 锚定和反事实解释
- en: In previous chapters, we learned how to attribute model decisions to features
    and their interactions with state-of-the-art global and local model interpretation
    methods. However, the decision boundaries are not always easy to define or interpret
    with these methods. Wouldn’t it be nice to be able to derive human-interpretable
    rules from model interpretation methods? In this chapter, we will cover a few
    human-interpretable, local, classification-only model interpretation methods.
    We will first learn how to use scoped rules called **anchors** to explain complex
    models with statements such as *if X conditions are met, then Y is the outcome*.
    Then, we will explore **counterfactual** explanations that follow the form *if
    Z conditions aren’t met, then Y is not the outcome*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何使用最先进的全局和局部模型解释方法将模型决策归因于特征及其与状态的交互。然而，决策边界并不总是容易定义或解释。如果能够从模型解释方法中推导出人类可解释的规则，那岂不是很好？在本章中，我们将介绍一些人类可解释的、局部的、仅分类的模型解释方法。我们首先将学习如何使用称为**锚定**的限定规则来解释复杂模型，例如使用如下语句*如果满足X条件，则Y是结果*。然后，我们将探讨遵循*如果Z条件不满足，则Y不是结果*形式的**反事实**解释。
- en: 'These are the main topics we are going to cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将在本章中要涵盖的主要主题：
- en: Understanding anchor explanations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解锚定解释
- en: Exploring counterfactual explanations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索反事实解释
- en: Let’s begin!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `sklearn`,
    `catboost`, `matplotlib`, `seaborn`, `alibi`, `tensorflow`, `shap`, and `witwidget`
    libraries. Instructions on how to install all of these libraries are in the *Preface*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`catboost`、`matplotlib`、`seaborn`、`alibi`、`tensorflow`、`shap`和`witwidget`库。如何安装所有这些库的说明在*序言*中。
- en: 'The code for this chapter is located here: [https://packt.link/tH0y7](https://packt.link/tH0y7).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/tH0y7](https://packt.link/tH0y7)。
- en: The mission
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: In the United States, for the last two decades, private companies and nonprofits
    have developed criminal **Risk Assessment Instruments/Tools** (**RAIs**), most
    of which employ statistical models. As many states can no longer afford their
    large prison populations, these methods have increased in popularity, *guiding*
    judges and parole boards through every step of the prison system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在美国，在过去二十年里，私营公司和非营利组织开发了刑事**风险评估工具/仪器**（**RAIs**），其中大部分采用统计模型。由于许多州无法承担其庞大的监狱人口，这些方法越来越受欢迎，*引导*法官和假释委员会通过监狱系统的每一步。
- en: These are high-impact decisions that can determine if a person is released from
    prison. Can we afford for these decisions to be wrong? Can we accept the recommendations
    from these systems without understanding why they were made? Worst of all, we
    don’t exactly know how an assessment was made. The risk is usually calculated
    with a white-box model, but, in practice, a black-box model is used because it
    is proprietary. Predictive performance is also relatively low, with median AUC
    scores for a sample of nine tools ranging between 0.57 and 0.74 according to the
    paper *Performance of Recidivism Risk Assessment Instruments in U.S. Correctional
    Settings*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是影响重大的决策，可能决定一个人是否被释放出监狱。我们能否承担这些决策出错的风险？我们能否在不理解为什么这些系统会做出这样的推荐的情况下接受这些系统的建议？最糟糕的是，我们并不确切知道评估是如何进行的。风险通常使用白盒模型进行计算，但在实践中，由于是专有的，通常使用黑盒模型。根据论文《美国矫正环境中再犯风险评估工具的性能》的数据，预测性能也相对较低，九个工具样本的中位AUC分数在0.57到0.74之间。
- en: Even though traditional statistical methods are still the norm for criminal
    justice models, to improve performance, some researchers have proposed leveraging
    more complex models, such as Random Forest with larger datasets. Far from being
    science fiction drawn from *Minority Report* or *Black Mirror*, in some countries,
    scoring people based on their likelihood of engaging in antisocial, or even antipatriotic,
    behavior with big data and machine learning is already a reality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统的统计方法仍然是刑事司法模型的标准，但为了提高性能，一些研究人员提出了利用更复杂的模型，例如具有更大数据集的随机森林。这并非来自《少数派报告》或《黑镜》的科学幻想，在一些国家，基于大数据和机器学习对人们参与反社会，甚至反爱国行为的可能性进行评分已经成为现实。
- en: As more and more AI solutions attempt to make life-changing predictions about
    us with our data, fairness must be properly assessed, and all its ethical and
    practical implications must be adequately discussed. *Chapter 1*, *Interpretation,
    Interpretability, and Explainability; and Why Does It All Matter?*, covered how
    fairness is an integral concept for machine learning interpretation. You can evaluate
    fairness in any model, but fairness is especially difficult when it involves human
    behavior. The dynamics between human psychological, neurological, and sociological
    factors are extremely complicated. In the context of predicting criminal behavior,
    it boils down to what factors are potentially to blame for a crime, as it wouldn’t
    be fair to include anything else in a model, and how these factors interact.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的AI解决方案试图用我们的数据做出改变我们生活的预测，公平性必须得到适当的评估，并且所有其伦理和实际影响都必须得到充分的讨论。*第一章*，*解释、可解释性和可解释性；以及这一切为什么都如此重要？*，讨论了公平性是机器学习解释的一个基本概念。你可以在任何模型中评估公平性，但涉及人类行为时，公平性尤其困难。人类心理、神经和社会因素之间的动态极其复杂。在预测犯罪行为的背景下，这归结为哪些因素可能对犯罪负有责任，因为将其他任何东西包含在模型中都是不公平的，以及这些因素如何相互作用。
- en: Quantitative criminologists are still debating the best predictors of criminality
    and their root causes. They’re also debating whether it is ethical to *blame*
    a criminal for these factors to begin with. Thankfully, demographic traits such
    as race, gender, and nationality are no longer used in criminal risk assessments.
    But this doesn’t mean that these methods are no longer biased. Scholars recognize
    the problem and are proposing solutions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 定量犯罪学家仍在争论犯罪性的最佳预测因素及其根本原因。他们也在争论是否应该从一开始就*责怪*罪犯这些因素。幸运的是，人口统计特征如种族、性别和国籍不再用于犯罪风险评估。但这并不意味着这些方法不再有偏见。学者们认识到这个问题，并提出了解决方案。
- en: This chapter will examine racial bias in one of the most widely used risk assessment
    tools. Given this topic’s sensitive and relevant nature, it was essential to provide
    a modicum of context about criminal risk assessment tools and how machine learning
    and fairness connect with all of them. We won’t go into much more detail, but
    understanding the context is important to appreciate how machine learning could
    perpetuate structural inequality and unfair biases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨在广泛使用的风险评估工具中存在的种族偏见。鉴于这一主题的敏感性和相关性，提供一些关于犯罪风险评估工具以及机器学习和公平性如何与它们所有方面相关联的背景信息是至关重要的。我们不会深入更多细节，但理解背景对于欣赏机器学习如何持续加剧结构性不平等和不公平偏见是很重要的。
- en: Now, let’s introduce you to your mission for this chapter!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们向您介绍本章的任务！
- en: Unfair bias in recidivism risk assessments
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 再犯风险评估中的不公平偏见
- en: Imagine a scenario of an investigative journalist writing an article on an African
    American defendant detained while awaiting trial. A tool called **Correctional
    Offender Management Profiling for Alternative Sanction** (**COMPAS**) deemed him
    as being at risk of recidivism. **Recidivism** is when someone relapses into criminal
    behavior. And the score convinced the judge that the defendant had to be detained
    without considering any other arguments or testimonies. He was locked up for many
    months, and in the trial, was found not guilty. Over five years have passed since
    the trial, and he hasn’t been accused of any crime. You could say the prediction
    for recidivism was a false positive.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个调查记者正在撰写一篇关于一名非洲裔美国被告的文章，该被告在审判前被拘留。一个名为**矫正犯人管理配置文件以替代制裁**（**COMPAS**）的工具认为他有再犯的风险。**再犯**是指某人重新陷入犯罪行为。这个分数让法官相信被告必须被拘留，而不考虑任何其他论点或证词。他被关押了数月，在审判中被判无罪。自审判以来已经过去了五年，他没有被指控犯有任何罪行。可以说，对再犯的预测是一个假阳性。
- en: 'The journalist has reached out to you because she would like to ascertain with
    data science whether there was unfair bias in this case. The COMPAS risk assessment
    is computed using 137 questions ([https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)).
    It includes questions such as the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 记者联系你是因为她希望用数据科学来确定这个案例中是否存在不公平的偏见。COMPAS风险评估是通过137个问题计算的（[https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)）。它包括以下问题：
- en: “Based on the screener’s observations, is this person a suspected or admitted
    gang member?”
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “根据筛查者的观察，这个人是不是被怀疑或承认的帮派成员？”
- en: “How often have you moved in the last 12 months?”
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “在过去12个月内，你多久搬过一次家？”
- en: “How often do you have barely enough money to get by?”
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你多久才有足够的钱维持生计？”
- en: Psychometric LIKERT scale questions such as “I have never felt sad about things
    in my life.”
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 心理测量学李克特量表问题，例如“我从未因为生活中的事情感到悲伤。”
- en: Even though race is not one of the questions, many of these questions may correlate
    with race. Not to mention, in some cases, they can be more a question of subjective
    opinion than fact, and thus be prone to bias.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管种族不是问题之一，但许多这些问题可能与种族相关。更不用说，在某些情况下，它们可能更多的是主观意见的问题而不是事实，因此容易产生偏见。
- en: The journalist cannot provide you with the 137 answered questions or the COMPAS
    model because this data is not publicly available. However, thankfully, all defendants’
    demographic and recidivism data for the same county in Florida is available.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 记者无法提供您137个已回答的问题或 COMPAS 模型，因为这些数据不是公开可用的。然而，幸运的是，佛罗里达州同一县的所有被告的人口统计和再犯数据是可用的。
- en: The approach
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'You have decided to do the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经决定做以下事情：
- en: '**Train a proxy model**: You don’t have the original features or model, but
    all is not lost because you have the COMPAS scores—the labels. And we also have
    features relevant to the problem we can connect to these labels with models. By
    approximating the COMPAS model via the proxies, you can assess the fairness of
    the COMPAS decisions. In this chapter, we will train a CatBoost model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练代理模型**：您没有原始特征或模型，但并非一切尽失，因为您有 COMPAS 分数——标签。我们还有与这些问题相关的特征，我们可以通过模型将这些标签连接起来。通过通过代理近似
    COMPAS 模型，您可以评估 COMPAS 决策的公平性。在本章中，我们将训练一个 CatBoost 模型。'
- en: '**Anchor explanations**: Using this method will unearth insights into why the
    proxy model makes specific predictions using a series of rules called anchors,
    which tell you where the decision boundaries lie. The boundaries are relevant
    for our mission because we want to know why the defendant has been wrongfully
    predicted to recidivate. It’s an approximate boundary to the original model, but
    there’s still some truth to it.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anchor 解释**：使用这种方法将揭示代理模型为何使用一系列称为锚点的规则做出特定预测的见解，这些规则告诉您决策边界在哪里。边界对我们任务的相关性在于我们想知道为什么被告被错误地预测会再犯。这是一个对原始模型的近似边界，但其中仍有一些真实性。'
- en: '**Counterfactual explanations**: While Anchor explains why a decision was made,
    counterfactuals can be useful to examine why a decision was not made. This is
    particularly useful in inspecting the fairness of decisions. We will use an unbiased
    method to find counterfactuals and then use the **What-If Tool** (**WIT**) to
    explore counterfactuals and fairness further.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反事实解释**：虽然 Anchor 解释了决策为何被做出，但反事实可以用来检查决策为何没有被做出。这在检查决策的公平性方面特别有用。我们将使用无偏方法来找到反事实，然后使用
    **What-If Tool** (**WIT**) 来进一步探索反事实和公平性。'
- en: The preparations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/06/Recidivism.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/06/Recidivism.ipynb).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在这里找到这个示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/06/Recidivism.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/06/Recidivism.ipynb)。
- en: Loading the libraries
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mldatasets` 用于加载数据集'
- en: '`pandas` and `numpy` to manipulate the dataset'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `pandas` 和 `numpy` 来操作数据集
- en: '`sklearn` (scikit-learn), and `catboost` to split the data and fit the models'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `sklearn` (scikit-learn) 和 `catboost` 来分割数据并拟合模型
- en: '`matplotlib`, `seaborn`, `alibi`, `tensorflow`, `shap`, and `witwidget` to
    visualize the interpretations'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `matplotlib`、`seaborn`、`alibi`、`tensorflow`、`shap` 和 `witwidget` 来可视化解释
- en: 'You should load all of them first:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该首先加载所有这些：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s check that TensorFlow has loaded the right version with `print(tf.__version__)`.
    It should be 2.0 or above. We should also disable eager execution and verify that
    it worked with this command. The output should say that it’s `False`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 `print(tf.__version__)` 检查 TensorFlow 是否加载了正确的版本。它应该是 2.0 或更高版本。我们还应该禁用
    eager 执行并验证它是否通过此命令完成。输出应该显示为 `False`：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Understanding and preparing the data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We load the data like this into a DataFrame called `recidivism_df`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据以这种方式加载到名为 `recidivism_df` 的 DataFrame 中：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There should be almost 15,000 records and 23 columns. We can verify this was
    the case with `info()`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 应该有大约15,000条记录和23列。我们可以使用`info()`来验证这一点：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following output checks out. All features are numeric with no missing values,
    and categorical features have already been one-hot-encoded for us:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出是正确的。所有特征都是数值型，没有缺失值，并且分类特征已经为我们进行了独热编码：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The data dictionary
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据字典
- en: 'There are only nine features, but they become 22 columns because of the categorical
    encoding:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然只有九个特征，但由于分类编码，它们变成了22列：
- en: '`age`: Continuous; the age of the defendant (between 18 and 96).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`: 连续；被告的年龄（介于18和96之间）。'
- en: '`juv_fel_count`: Ordinal; the number of juvenile felonies (between 0 and 2).'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`juv_fel_count`: 序数；少年重罪的次数（介于0和2之间）。'
- en: '`juv_misd_count`: Ordinal; the number of juvenile misdemeanors (between 0 and
    1).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`juv_misd_count`: 序数；少年轻罪的次数（介于0和1之间）。'
- en: '`juv_other_count`: Ordinal; the number of juvenile convictions that are neither
    felonies nor misdemeanors (between 0 and 1).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`juv_other_count`: 序数；既不是重罪也不是轻罪的少年定罪的次数（介于0和1之间）。'
- en: '`priors_count`: Ordinal; the number of prior crimes committed (between 0 and
    13).'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`priors_count`: 序数；已犯前科的数量（介于0和13之间）。'
- en: '`is_recid`: Binary; did the defendant recidivate within 2 years (1 for yes,
    0 for no)?'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_recid`: 二进制；被告在2年内是否再犯（1表示是，0表示否）？'
- en: '`sex`: Categorical; the gender of the defendant.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sex`: 分类；被告的性别。'
- en: '`race`: Categorical; the race of the defendant.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`race`: 分类；被告的种族。'
- en: '`c_charge_degree`: Categorical; the degree of what the defendant is currently
    being charged with. The United States classifies criminal offenses as felonies,
    misdemeanors, and infractions, ordered from most serious to least. These are subclassified
    in the form of degrees, which go from 1^(st) (most serious offenses) to 3^(rd)
    or 5^(th) (least severe). However, even though this is standard for federal offenses,
    it is tailored to state law. For felonies, Florida ([http://www.dc.state.fl.us/pub/scoresheet/cpc_manual.pdf](http://www.dc.state.fl.us/pub/scoresheet/cpc_manual.pdf))
    has a level system that determines the severity of a crime regardless of the degree,
    and this goes from 10 (most severe) to 1 (least).'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c_charge_degree`: 分类；被告目前被指控的程度的分类。美国将犯罪行为分为重罪、轻罪和违章，从最严重到最轻微的顺序排列。这些以级别的形式进一步分类，从1^(st)（最严重的罪行）到3^(rd)或5^(th)（最轻微的罪行）。然而，尽管这是联邦罪行的标准，但它是根据州法律定制的。对于重罪，佛罗里达州([http://www.dc.state.fl.us/pub/scoresheet/cpc_manual.pdf](http://www.dc.state.fl.us/pub/scoresheet/cpc_manual.pdf))有一个级别制度，它根据犯罪的程度来确定犯罪的严重性，而不考虑级别，从10（最严重）到1（最轻微）。'
- en: The categories of this feature are prefixed with *F* for felonies and *M* for
    misdemeanors. They are followed by a number, which is a level for felonies and
    a degree for misdemeanors.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该特征的类别以*F*开头表示重罪，以*M*开头表示轻罪。它们后面跟着一个数字，这是重罪的级别和轻罪的度数。
- en: '`compas_score`: Binary; COMPAS scores defendants as “low,” “medium,” or “high”
    risk. In practice, “medium” is often treated as “high” by decision-makers, so
    this feature has been converted to binary to reflect this behavior: 1: high/medium
    risk and 0: low risk.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`compas_score`: 二进制；COMPAS将被告的风险评估为“低”、“中”或“高”。在实践中，“中”通常被决策者视为“高”，因此该特征已被转换为二进制以反映这种行为：1：高风险/中风险，0：低风险。'
- en: Examining predictive bias with confusion matrices
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用混淆矩阵检查预测偏差
- en: 'There are two binary features in the dataset. The first one is the recidivism
    risk prediction made by COMPAS (`compas_score`). The second one (`is_recid`) is
    the *ground truth* because it’s what happened within 2 years of the defendant’s
    arrest. Just as you would with the prediction of any model against its training
    labels, you can build confusion matrices with these two features. scikit-learn
    can produce one with the `confusion_matrix` function (`cf_matrix`), and we can
    then create a Seaborn `heatmap` with it. Instead of plotting the number of **True
    Negatives** (**TNs**), **False Positives** (**FPs**), **False Negatives** (**FNs**),
    and **True Positives** (**TPs**), we can plot percentages with a simple division
    (`cf_matrix/np.sum(cf_matrix)`). The other parameters of `heatmap` only assist
    with formatting:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有两个二元特征。第一个是由COMPAS做出的再犯风险预测（`compas_score`）。第二个（`is_recid`）是*真实情况*，因为它是在被告被捕后的两年内发生的事情。就像你会用任何模型的预测与其训练标签进行比较一样，你可以用这两个特征构建混淆矩阵。scikit-learn可以使用`confusion_matrix`函数（`cf_matrix`）生成一个，然后我们可以用Seaborn的`heatmap`创建它。我们不是通过简单的除法（`cf_matrix/np.sum(cf_matrix)`）来绘制**真阴性**（**TNs**）、**假阳性**（**FPs**）、**假阴性**（**FNs**）和**真阳性**（**TPs**）的数量，而是绘制百分比。`heatmap`的其他参数仅用于格式化：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code outputs *Figure 6.1*. The top-right corner is FPs, which
    are nearly one-fifth of all predictions, and together with the FNs in the bottom-left
    corner, they make up over two-thirds:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了*图6.1*。右上角是假阳性（FP），占所有预测的五分之一左右，与左下角的假阴性（FN）一起，占超过三分之二：
- en: '![](img/B18406_06_01.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_06_01.png)'
- en: 'Figure 6.1: Confusion matrix between the predicted risk of recidivism (compas_score)
    and the ground truth (is_recid)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：预测再犯风险（compas_score）和真实情况（is_recid）之间的混淆矩阵
- en: '*Figure 6.1* shows that the COMPAS model’s predictive performance is not very
    good, especially if we assume that criminal justice decision-makers are taking
    medium or high risk assessments at face value. It also tells us that FP and FNs
    occur at a similar rate. Nevertheless, simple visualizations such as the confusion
    matrix obscure predictive disparities between subgroups of a population. We can
    quickly compare disparities between two subgroups that historically have been
    treated differently by the United States criminal justice system. To this end,
    we first subdivide our DataFrame into two DataFrames: one for Caucasians (`recidivism_c_df`)
    and another for African Americans (`recidivism_aa_df`). Then we can generate confusion
    matrices for each DataFrame and plot them side by side with the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1* 显示，COMPAS模型的预测性能并不很好，尤其是如果我们假设刑事司法决策者正在将中等或高风险评估视为表面价值。它还告诉我们，假阳性（FP）和假阴性（FN）的发生率相似。尽管如此，像混淆矩阵这样的简单可视化会掩盖人口子群体之间的预测差异。我们可以快速比较两个历史上一直受到美国刑事司法系统不同对待的子群体之间的差异。为此，我们首先将我们的DataFrame细分为两个DataFrame：一个用于白人（`recidivism_c_df`）和另一个用于非裔美国人（`recidivism_aa_df`）。然后我们可以为每个DataFrame生成混淆矩阵，并使用以下代码将它们并排绘制：'
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/B18406_06_02.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_06_02.png)'
- en: 'Figure 6.2: Comparison of the confusion matrices for the predicted risk of
    recidivism (compas_score) and the ground truth (is_recid) between African Americans
    and Caucasians in the dataset'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：数据集中非洲裔美国人和白人在预测再犯风险（compas_score）和真实情况（is_recid）之间的混淆矩阵比较
- en: Instead of eyeballing it by looking at the plots, we can measure the **False
    Positive Rate** (**FPR**), which is the ratio between these two measures (FP /
    (FP + TN)) at risk of recidivism more often.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是通过查看图表来直观地评估它，而是可以测量**假阳性率**（**FPR**），这是两个度量之间的比率（FP / (FP + TN)），在再犯风险较高的情况下更为常见。
- en: Data preparation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: Before we move on to the modeling and interpretation, we have one last step.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续进行建模和解释之前，我们还有最后一步。
- en: 'Since `prepare=True` for the data loading, all we do now is split the data
    into a training and test dataset. As usual, it is critical to set your random
    states so that all your findings are reproducible. We will then set `y` to be
    our target variable (`compas_score`) and set `X` as every other feature except
    for `is_recid`, because this is the ground truth. Lastly, we split `y` and `X`
    into train and test datasets as we have before:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据加载时`prepare=True`，我们现在所做的一切就是将数据分为训练集和测试集。像往常一样，设置随机状态以确保所有发现都是可重复的至关重要。然后我们将`y`设置为我们的目标变量（`compas_score`），将`X`设置为除`is_recid`之外的所有其他特征，因为这是真实情况。最后，我们将`y`和`X`分为训练集和测试集，就像我们之前做的那样：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let’s get started!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: Modeling
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: First, let’s quickly train the model we will use throughout this chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速训练本章将使用的模型。
- en: '**Proxy models** are a means to emulate output from a black-box model just
    like **global surrogate models**, which we covered in *Chapter 4*, *Global Model-Agnostic
    Interpretation Methods*. So, are they the same thing? In machine learning, surrogate
    and proxy are terms that are often used interchangeably. However, semantically,
    surrogacy relates to substitution and proxy relates more to a representation.
    So, we call these proxy models to distinguish that we don’t have the exact training
    data. Therefore, you only represent the original model because you cannot substitute
    it. For the same reason, unlike interpretation with surrogates, which is best
    served by simpler models, a proxy is best suited to complex models that can make
    up for the difference in training data with complexity.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理模型**是一种模拟黑盒模型输出的方法，就像我们在*第4章*中提到的**全局代理模型**一样。那么，它们是同一回事吗？在机器学习中，代理和代理是经常可以互换使用的术语。然而，从语义上讲，代理与替代相关，而代理更多地与表示相关。因此，我们称这些代理模型是为了区分我们没有确切的训练数据。因此，你只能代表原始模型，因为你不能替代它。出于同样的原因，与代理的解读不同，后者最适合简单的模型，而代理最适合复杂的模型，这些模型可以用复杂性来弥补训练数据中的差异。'
- en: We will train a **CatBoost** classifier. For those of you who aren’t familiar
    with CatBoost, it’s an efficient boosted ensemble tree method. It’s similar to
    **LightGBM**, except it uses a new technique called **Minimal Variance Sampling**
    (**MVS**) instead of **Gradient-Based One-Side Sampling** (**GOSS**). Unlike LightGBM,
    it grows trees in a balanced fashion. It’s called CatBoost because it can automatically
    encode categorical features, and it’s particularly good at tackling overfitting,
    with unbiased treatment of categorical features and class imbalances. We won’t
    go into a whole lot of detail, but it was chosen for this exercise for those reasons.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个**CatBoost**分类器。对于那些不熟悉CatBoost的人来说，它是一种高效的提升集成树方法。它与**LightGBM**类似，但使用了一种称为**最小方差抽样**（**MVS**）的新技术，而不是**基于梯度的单侧抽样**（**GOSS**）。与LightGBM不同，它以平衡的方式生长树。它被称为CatBoost，因为它可以自动编码分类特征，并且特别擅长处理过拟合，对分类特征和类别不平衡的处理是无偏的。我们不会过多地详细介绍，但出于这些原因，它被选为这次练习。
- en: 'As a tree-based model class, you can specify a maximum `depth` value for `CatBoostClassifier`.
    We are setting a relatively high `learning_rate` value and a lower `iterations`
    value (the default is 1,000). Once we have used `fit` on the model, we can evaluate
    the results with `evaluate_class_mdl`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基于树的模型类别，你可以为`CatBoostClassifier`指定一个最大`深度`值。我们设置了一个相对较高的`学习率`值和一个较低的`迭代`值（默认为1,000）。一旦我们对模型使用了`fit`，我们就可以用`evaluate_class_mdl`来评估结果：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can appreciate the output of `evaluate_class_mdl` for our CatBoost model
    in *Figure 6.3*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图6.3*中欣赏到我们的CatBoost模型的`evaluate_class_mdl`输出：
- en: '![Chart, line chart  Description automatically generated](img/B18406_06_03.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_06_03.png)'
- en: 'Figure 6.3: Predictive performance of our CatBoost model'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：CatBoost模型的预测性能
- en: From the optics of fairness, we care more about FPs than FNs because it’s more
    unfair to put an *innocent* person in prison than it is to leave a *guilty* person
    on the streets. Therefore, we should aspire to have higher *precision* than *recall*.
    *Figure 6.3* confirms this, as well as a healthy ROC curve, ROC-AUC, and MCC.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从公平性的角度出发，我们更关心假阳性（FPs）而不是假阴性（FNs），因为将一个**无辜**的人关进监狱比让一个**有罪**的人留在街上更不公平。因此，我们应该追求比召回率更高的**精确度**。*图6.3*证实了这一点，以及一个健康的ROC曲线、ROC-AUC和MCC。
- en: The predictive performance for the model is reasonably accurate considering
    it’s a *proxy model* meant to only approximate the real thing with different,
    yet related, data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到它是一个仅用不同但相关的数据近似真实事物的**代理模型**，该模型的预测性能是相当准确的。
- en: Getting acquainted with our “instance of interest”
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熟悉我们的“实例”
- en: 'The journalist reached out to you with a case in mind: the African American
    defendant who was falsely predicted to have a high risk of recidivism. This case
    is #`5231` and is your main *instance of interest*. Since our focus is racial
    bias, we’d like to compare it with similar instances but of different races. To
    that end, we found case #`10127` (Caucasian) and #`2726` (Hispanic).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 记者带着一个案例找到你：一个被错误预测有高再犯风险的非洲裔美国被告。这个案例是#`5231`，是你的主要**实例**。由于我们的重点是种族偏见，我们想将其与不同种族的类似案例进行比较。为此，我们找到了案例#`10127`（白人）和#`2726`（西班牙裔）。
- en: 'We can look at the data for all three. Since we will keep referring to these
    instances throughout this chapter, let’s first save the indexes of the African
    American (`idx_aa`), Hispanic (`idx_h`), and Caucasian (`idx_c`) cases. Then,
    we can subset the test dataset by these indexes. Since we have to make sure that
    our predictions match, we will concatenate this subsetted test dataset to the
    true labels (`y_test`) and the CatBoost predictions (`y_test_cb_pred`):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看这三个实例的数据。由于我们将在本章中多次引用这些实例，让我们首先保存非洲裔美国人(`idx_aa`)、西班牙裔(`idx_h`)和白色人种(`idx_c`)案例的索引。然后，我们可以通过这些索引对测试数据集进行子集划分。由于我们必须确保我们的预测匹配，我们将这个子集化的测试数据集连接到真实标签(`y_test`)和CatBoost预测(`y_test_cb_pred`)：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code produces the DataFrame in *Figure 6.4*. You can tell that
    the predictions match the true labels, and our main *instance of interest* was
    the only one predicted as a medium or high risk of recidivism. Besides race, the
    only other differences are with `c_charge_degree` and one minor age difference:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成了*图6.4*中的DataFrame。你可以看出预测与真实标签相匹配，我们主要关注的实例是唯一一个被预测为中等或高风险再犯的实例。除了种族之外，唯一的其他差异是与`c_charge_degree`以及一个微小的年龄差异：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_06_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_06_04.png)'
- en: 'Figure 6.4: Observations #5231, #10127, and #2726 side by side with feature
    differences highlighted'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：观察#5231、#10127和#2726并排展示，特征差异被突出显示
- en: Throughout this chapter, we will pay close attention to these differences to
    see whether they played a large role in producing the prediction difference. All
    the methods we will cover will complete the picture of what can determine or change
    the proxy model’s decision, and, potentially, the COMPAS model by extension. Now
    that we have completed the setup, we will move forward with employing the interpretation
    methods.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将密切关注这些差异，以了解它们是否在产生预测差异中发挥了重要作用。我们将涵盖的所有方法都将完善决定或改变代理模型决策，以及可能通过扩展的COMPAS模型的决定的图景。现在我们已经完成了设置，我们将继续使用解释方法。
- en: Understanding anchor explanations
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解锚点解释
- en: In *Chapter 5*, *Local Model-Agnostic Interpretation Methods*, we learned that
    **LIME** trains a local surrogate model (specifically a **weighted sparse linear
    model**) on a **perturbed** version of your dataset in the **neighborhood** of
    your *instance of interest*. The result is that you approximate a **local decision
    boundary** that can help you interpret the model’s prediction for it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*局部模型无关解释方法*中，我们了解到**LIME**在数据集的**扰动**版本上训练一个局部代理模型（具体来说是一个**加权稀疏线性模型**），在**实例邻域**中。结果是，你近似了一个**局部决策边界**，这可以帮助你解释该模型对其的预测。
- en: Like LIME, **anchors** are also derived from a model-agnostic perturbation-based
    strategy. However, they are not about the *decision boundary* but the **decision
    region**. Anchors are also known as **scoped rules** because they list some **decision
    rules** that apply to your instance and its *perturbed* neighborhood. This neighborhood
    is also known as the **perturbation space**. An important detail is to what extent
    the rules apply to it, known as **precision**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与LIME一样，**锚点**也是从一个模型无关的基于扰动的策略中派生出来的。然而，它们不是关于**决策边界**，而是关于**决策区域**。锚点也被称为**范围规则**，因为它们列出了一些适用于你的实例及其**扰动**邻域的**决策规则**。这个邻域也被称为**扰动空间**。一个重要的细节是规则在多大程度上适用于它，这被称为**精确度**。
- en: Imagine the neighborhood around your instance. You would expect the points to
    have more similar predictions the closer you got to your instance, right? So,
    if you had decision rules that defined these predictions, the smaller the area
    surrounding your instance, the more precise your rules. This concept is called
    **coverage**, which is the percentage of your *perturbation space* that yields
    a specific *precision*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你实例周围的社区。你会期望当你离你的实例越近时，点之间的预测越相似，对吧？所以，如果你有定义这些预测的决策规则，你实例周围的区域越小，你的规则就越精确。这个概念被称为**覆盖率**，它是指你的**扰动空间**中产生特定**精确度**的百分比。
- en: Unlike LIME, anchors don’t fit a local surrogate model to explain your chosen
    instance’s prediction. Instead, they explore possible candidate decision rules
    using an algorithm called **Kullback-Leibler divergence Lower and Upper Confidence
    Bounds** (**KL-LUCB**), which is derived from a **Multi-Armed Bandit** (**MAB**)
    algorithm.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LIME 不同，锚点不拟合局部代理模型来解释你选择的实例的预测。相反，它们使用称为**Kullback-Leibler 散度下界和上界置信度**（**KL-LUCB**）的算法探索可能的候选决策规则，该算法源于**多臂老虎机**（**MAB**）算法。
- en: MABs are a family of *reinforcement learning algorithms* about maximizing the
    payoff when you have limited resources to explore all unknown possibilities. The
    algorithm originated from understanding how casino slot machine players could
    maximize their payoff by playing multiple machines. It’s called multi-armed bandit
    because slot machine players are known as one-armed bandits. Yet players don’t
    know which machine will yield the highest payoff, can’t try all of them at once,
    and have finite funds. The trick is to learn how to balance exploration (trying
    unknown slot machines) with exploitation (using those you already have reasons
    to prefer).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: MABs 是一种关于在资源有限的情况下探索所有未知可能性的最大回报的**强化学习算法**家族。该算法起源于理解赌场老虎机玩家如何通过玩多个机器来最大化他们的回报。它被称为多臂老虎机，因为老虎机玩家被称为单臂老虎机。然而，玩家不知道哪台机器会产生最高的回报，不能一次性尝试所有机器，而且资金有限。技巧在于学习如何平衡探索（尝试未知的老虎机）与利用（使用那些你已经有一定理由偏好的机器）。
- en: In the anchors’ case, each slot machine is a potential decision rule, and the
    payoff is how much precision it yields. The KL-LUCB algorithm uses confidence
    regions based on the **Kullback-Leibler divergence** between the distributions
    to find the decision rule with the highest precision sequentially, yet efficiently.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在锚点的情况下，每个老虎机都是一个潜在的决策规则，回报是它产生的精确度。KL-LUCB 算法使用基于分布之间的**Kullback-Leibler 散度**的置信区域，依次找到具有最高精确度的决策规则，同时效率很高。
- en: Preparations for anchor and counterfactual explanations with alibi
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用证据准备锚点和反事实解释
- en: 'Several small steps need to be performed to help the `alibi` library produce
    human-friendly explanations. The first one pertains to the prediction since the
    model may output a 1 or 0, but it’s easier to understand a prediction by its name.
    To help us with this, we need a list with the class names where the 0 position
    matches our negative class name and 1 matches the positive one:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 需要执行几个小步骤来帮助 `alibi` 库生成人类友好的解释。第一步与预测有关，因为模型可能输出 1 或 0，但通过名称理解预测更容易。为了帮助我们，我们需要一个包含类名的列表，其中
    0 位置匹配我们的负类名，1 匹配正类名：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let’s create a `numpy` array with our main *instance of interest* and
    print it out. Please note that the single-dimension array needs to be expanded
    (`np.expand_dims`) so that it’s understood by `alibi`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个包含我们主要**感兴趣实例**的 `numpy` 数组并将其打印出来。请注意，单维数组需要扩展（`np.expand_dims`），以便
    `alibi` 能够理解：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code outputs an array with the 21 features, of which 12 were
    the result of **One-Hot Encoding** (**OHE**):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出一个包含 21 个特征的数组，其中 12 个是**独热编码**（**OHE**）的结果：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A problem with making human-friendly explanations arises when you have OHE categories.
    To both the machine learning model and the explainer, each OHE feature is separate
    from the others. Still, to the human interpreting the outcomes, they cluster together
    as categories of their original features.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有独热编码类别时，制作人类友好的解释会出现问题。对于机器学习模型和解释器来说，每个独热特征都是独立的。然而，对于解释结果的人类来说，它们会聚集成它们原始特征的类别。
- en: 'The `alibi` library has several utility functions to deal with this problem,
    such as `ohe_to_ord`, which takes a one-hot-encoded instance and puts it in an
    ordinal format. To use this function, we first define a dictionary `(cat_vars_ohe`)
    that tells `alibi` where the categorical variables are in our features and how
    many categories each one has. For instance, in our data, gender starts at the
    5^(th) index and has two categories, which is why our `cat_vars_ohe` dictionary
    begins with `5: 2`. Once you have this dictionary, `ohe_to_ord` can take your
    instance (`X_test_eval`) and output it in ordinal format, where each categorical
    variable takes up a single feature. This utility function will prove useful for
    Alibi’s counterfactual explanations, where the explainer will need this dictionary
    to map categorical features together:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`alibi`库有几个实用函数来处理这个问题，例如`ohe_to_ord`，它将一个独热编码的实例转换为序数格式。要使用此函数，我们首先定义一个字典`(cat_vars_ohe)`，告诉`alibi`我们的特征中分类变量的位置以及每个变量有多少个类别。例如，在我们的数据中，性别从第5个索引开始，有两个类别，这就是为什么我们的`cat_vars_ohe`字典从`5:
    2`开始。一旦你有了这个字典，`ohe_to_ord`就可以将你的实例（`X_test_eval`）转换为序数格式，其中每个分类变量占据一个单独的特征。这个实用函数对于Alibi的逆事实解释非常有用，因为解释器需要这个字典来映射分类特征：'
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code outputs the following array:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了以下数组：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For when it’s in ordinal format, Alibi will need a dictionary that provides
    names for each category and a list of feature names:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据处于序数格式时，Alibi需要一个字典，为每个类别提供名称以及特征名称列表：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'However, Alibi’s anchor explanations use the data as it is provided to our
    models. We are using OHE data, so we need a category map for that format. Of course,
    the OHE features are all binary, so they only have two “categories” each:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Alibi的锚点解释使用的是提供给我们的模型的数据。我们正在使用OHE数据，因此我们需要一个针对该格式的类别映射。当然，OHE特征都是二进制的，因此它们每个只有两个“类别”：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Local interpretations for anchor explanations
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 锚点解释的局部解释
- en: 'All Alibi explainers require a `predict` function, so we create a `lambda`
    function called `predict_cb_fn` for our CatBoost model. Please note that we are
    using `predict_proba` for the classifier’s probabilities. Then, to initialize
    `AnchorTabular`, we also provide it with our features’ names as they are in our
    OHE dataset and the category map (`category_map_ohe`). Once it has initialized,
    we fit it with our training data:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Alibi解释器都需要一个`predict`函数，因此我们为我们的CatBoost模型创建了一个名为`predict_cb_fn`的`lambda`函数。请注意，我们正在使用`predict_proba`来获取分类器的概率。然后，为了初始化`AnchorTabular`，我们还提供了我们的特征名称，这些名称与我们的OHE数据集和类别映射（`category_map_ohe`）中的名称一致。一旦初始化完成，我们就用我们的训练数据来拟合它：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Before we leverage the explainer, it’s good practice to check that the anchor
    “holds.” In other words, we should check that the MAB algorithm found decision
    rules that help explain the prediction. To verify this, you use the `predictor`
    function to check that the prediction is the same as the one you expect for this
    instance. Right now, we are using `idx_aa`, which is the case of the African American
    defendant:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们利用解释器之前，检查锚点“是否稳固”是一个好的实践。换句话说，我们应该检查MAB算法是否找到了有助于解释预测的决策规则。为了验证这一点，你使用`predictor`函数来检查预测结果是否与这个实例预期的结果相同。目前，我们使用`idx_aa`，这是非洲裔美国被告的情况：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code outputs the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了以下内容：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can proceed to use the `explain` function to generate an explanation for
    our instance. We can set our precision threshold to `0.85`, which means we expect
    the predictions on anchored observations to be the same as our instance at least
    85% of the time. Once we have an explanation, we can print the anchors as well
    as their precision and coverage:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续使用`explain`函数来为我们实例生成解释。我们可以将我们的精确度阈值设置为`0.85`，这意味着我们期望在锚定观察到的预测至少85%的时间与我们的实例相同。一旦我们有了解释，我们还可以打印锚点以及它们的精确度和覆盖率：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following output was generated by the preceding code. You can tell that
    `age`, `priors_count`, and `race_African-American` are factors at 86% precision.
    Impressively, this rule applies to almost a third of all the perturbation space’s
    instances with a coverage of 0.29:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出是由上述代码生成的。你可以看出`age`、`priors_count`和`race_African-American`是精确度为86%的因素。令人印象深刻的是，这条规则适用于几乎三分之一的扰动空间实例，覆盖率为0.29：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can try the same code but with a 5% bump in the precision threshold set
    to 0.9\. We observe the same three anchors that were generated from the previous
    example with three additional anchors:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试相同的代码，但将精确度阈值提高5%，设置为0.9。我们观察到与上一个示例中生成的相同三个锚点，以及另外三个额外的锚点：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Interestingly enough, although precision did increase by a few percentage points,
    coverage stayed the same. At this level of precision, we may confirm that race
    is a significant factor because being African American is an anchor, but so is
    not being Caucasian. Another factor was `c_charge_degree`. The explanation reveals
    that being accused of a first-degree misdemeanor or third-level felony would have
    been better. Understandably, a seventh-level felony is a more serious charge than
    these two.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，尽管精确度提高了几个百分点，但覆盖率保持不变。在这个精确度水平上，我们可以确认种族是一个重要因素，因为非洲裔美国人是一个锚点，但非白人也是如此。另一个因素是`c_charge_degree`。解释表明，被指控为一级轻罪或三级重罪会更好。可以理解的是，七级重罪比这两个罪更严重。
- en: 'Another way of understanding why a model made a specific prediction is by looking
    for a similar datapoint that had the opposite prediction and examining why the
    alternative decision was made. The decision boundary crosses between both points,
    so it’s helpful to contrast decision explanations from both sides of the boundary.
    This time, we will use `idx_c`, which is the case for the Caucasian defendant
    with a threshold of 85% and which outputs the anchors as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解模型为何做出特定预测的方法是寻找具有相反预测的相似数据点，并检查为什么做出了替代决策。决策边界跨越了这两个点，因此对比边界两边的决策解释是有帮助的。这次，我们将使用`idx_c`，这是白人被告的案例，阈值为85%，并输出以下锚点：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first anchor is `priors_count <= 2.00`, but on the other side of the boundary,
    the first two anchors were `age <= 25.00` and `priors_count > 0.00`. In other
    words, for an African American under or equal to the age of 25, any number of
    priors is enough to categorize them as having a medium/high risk of recidivism
    (86% of the time). On the other hand, a Caucasian person will be predicted as
    low risk if the priors don’t exceed two and they were not accused of a first-degree
    misdemeanor (89% of the time and with 58% coverage). These decision rules suggest
    a biased decision based on race with different standards applied to different
    racial groups. A double standard is when different rules are applied when, in
    principle, the situation is the same. In this case, the different rules for `priors_count`
    and the absence of `age` as a factor for Caucasians constitute double standards.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个锚点是`priors_count <= 2.00`，但在边界另一侧，前两个锚点是`age <= 25.00`和`priors_count > 0.00`。换句话说，对于一个25岁或以下的非洲裔美国人，任何数量的先例都足以将他们归类为有中等/高风险的再犯可能性（86%的时间）。另一方面，如果一个白人没有超过两个先例，并且没有被指控为一级轻罪，那么他们将被预测为低风险（89%的时间和58%的覆盖率）。这些决策规则表明，基于种族的偏见决策，对不同种族群体应用了不同的标准。双重标准是在原则上情况相同的情况下，应用不同的规则。在这种情况下，`priors_count`的不同规则和缺乏将`age`作为白人的因素构成了双重标准。
- en: 'We can now try a Hispanic defendant (`idx_h`) to observe whether double standards
    are also found in this instance. We just run the same code as before but replace
    `idx_c` with `idx_h`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以尝试一个西班牙裔被告（`idx_h`），以观察是否在这个实例中也发现了双重标准。我们只需运行之前相同的代码，但将`idx_c`替换为`idx_h`：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The explanations for the Hispanic defendant confirm the different standard with
    `priors_count` and that `race` continues to be a strong factor since there’s one
    anchor for not being African American and another one for being Hispanic.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于西班牙裔被告的解释证实了`priors_count`的不同标准，并且由于有一个锚点用于不是非洲裔美国人，另一个锚点用于西班牙裔，所以“种族”继续是一个强有力的因素。
- en: For specific model decisions, anchor explanations answer the question *why?*
    However, by comparing similar instances that are only slightly different but have
    different predictions, we have explored the question *what if?* In the next section,
    we will expand on this question further.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具体的模型决策，锚点解释回答了“为什么？”这个问题。然而，通过比较只有细微差别但预测结果不同的相似实例，我们已经探讨了“如果...会怎样？”这个问题。在下一节中，我们将进一步探讨这个问题。
- en: Exploring counterfactual explanations
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索反事实解释
- en: '**Counterfactuals** are an integral part of human reasoning. How many of us
    have muttered the words “If I had done *X* instead, my outcome *y* would have
    been different”? There’s always one or two things that, if done differently, could
    lead to the outcomes we prefer!'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**反事实**是人类推理的一个基本组成部分。我们中有多少人低声说过“如果我做了*X*而不是这样，我的结果*y*就会不同”？总有一些事情，如果我们做得不同，可能会导致我们更喜欢的结果！'
- en: In machine learning outcomes, you can leverage this way of reasoning to make
    for extremely human-friendly explanations where we can explain decisions in terms
    of what would need to change to get the opposite outcome (the **counterfactual
    class**). After all, we are often interested in knowing how to make a negative
    outcome better. For instance, how do you get your denied loan application approved
    or decrease your risk of cardiovascular disease from high to low? However, hopefully,
    answers to those questions aren’t a huge list of changes. You prefer the smallest
    number of changes required to change your outcome.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的结果中，你可以利用这种推理方式来制作极其人性化的解释，我们可以用需要改变什么才能得到相反结果（**反事实类别**）来解释决策。毕竟，我们通常对了解如何使负面结果变得更好感兴趣。例如，你如何让你的贷款申请被批准，或者将你的心血管疾病风险从高降低到低？然而，希望这些问题的答案不是一大串需要改变的事情。你更喜欢最小的改变数量来改变你的结果。
- en: Regarding fairness, counterfactuals are an important interpretation method,
    in particular when there are elements involved that *we can’t change* or shouldn’t
    have to change. For instance, if you perform exactly the same job and have the
    same level of experience as your co-worker, you expect to have the same salary,
    right? If you and your spouse share the same assets and credit history but have
    different credit scores, you have to wonder why. Does it have to do with gender,
    race, age, or even political affiliations? Whether it’s a compensation, credit
    rating, or recidivism risk model, you’d hope that similar people were treated
    similarly.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 关于公平性，反事实是一种重要的解释方法，特别是在涉及我们无法改变或不应改变的因素时。例如，如果你和你同事做完全相同的工作，并且拥有相同的工作经验水平，你期望得到相同的薪水，对吧？如果你和你的配偶拥有相同的资产和信用记录，但信用评分不同，你不得不怀疑这是为什么。这和性别、种族、年龄，甚至是政治派别有关吗？无论是薪酬、信用评级还是再犯风险模型，你希望类似的人得到类似对待。
- en: Finding counterfactuals is not particularly hard. All we have to do is change
    our *instance of interest* slightly until it changes the outcome. And maybe there’s
    an instance already in the dataset just like that!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 找到反事实并不特别困难。我们只需要稍微改变我们的“目标实例”直到它改变结果。也许数据集中已经有一个类似的实例！
- en: In fact, you could say that the three instances we examined with anchors in
    the previous section are close enough to be counterfactuals of each other, except
    for the Caucasian and Hispanic cases, which have the same outcome. But the Caucasian
    and Hispanic instances were “*cherry-picked*” by looking for datapoints with the
    same criminal history but different races to the *instance of interest*. Perhaps
    by comparing similar points, mostly except for race, we limited the scope in such
    a way that we confirmed what we hoped to confirm, which is that race matters to
    the model’s decision-making.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，可以说我们在上一节中用锚点考察的三个实例彼此之间非常接近，可以视为对方的反事实，除了白人和西班牙裔案例，它们有相同的结局。但白人和西班牙裔实例是通过寻找与目标实例具有相同犯罪历史但不同种族的数据点来“*挑选*”的。也许通过比较相似点，除了种族之外，我们这样限制了范围，从而证实了我们希望证实的事情，那就是种族对模型的决策有影响。
- en: This is an example of *selection bias*. After all, counterfactuals are inherently
    selective because they focus on a few feature changes. And even with a few features,
    there are so many possible permutations that change the outcome, which means that
    a single point could have hundreds of counterfactuals. And not all of these will
    tell a consistent story. This phenomenon is called the **Rashomon effect**. It
    is named after a famous Japanese movie about a murder mystery. And as we have
    come to expect from murder mysteries, witnesses have different recollections of
    what happened. But in the same way that it’s difficult to rely on a single witness,
    you cannot rely on a single counterfactual. Also, in the same way that great detectives
    are trained to look for clues everywhere in connection to the scene of a crime
    (even if it contradicts their instincts), counterfactuals shouldn’t be “cherry-picked”
    because they conveniently tell the story we want them to tell.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个**选择偏差**的例子。毕竟，反事实本质上是具有选择性的，因为它们关注几个特征的变化。即使只有几个特征，也有许多可能的排列组合会改变结果，这意味着一个点可能有数百个反事实。而且，并非所有这些都会讲述一个一致的故事。这种现象被称为**罗生门效应**。它是以一部著名的日本谋杀悬疑电影命名的。正如我们期待谋杀悬疑电影中的情况一样，目击者对所发生的事情有不同的回忆。但同样，正如伟大的侦探被训练去寻找与犯罪现场相关的线索（即使这与他们的直觉相矛盾）一样，反事实不应该“挑选”，因为它们方便地讲述了我们希望它们讲述的故事。
- en: Fortunately, there are algorithmic ways of looking for counterfactual instances
    in an unbiased manner. Typically, these involve finding the closest points with
    different outcomes, but there are different ways of measuring the distance between
    points. For starters, there’s the **L1** distance (also known as the **Manhattan
    distance**) and **L2** distance (also known as the **Euclidean distance**), among
    many others. But there’s also the question of normalizing the distances because
    not all features have the same scale. Otherwise, they would be biased against
    features with smaller scales, such as one-hot-encoded features. There are many
    normalization schemes to choose from too. You could use **standard deviation**,
    **min-max scaling**, or even **median absolute deviation** [9].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些算法方法可以在无偏的方式下寻找反事实实例。通常，这些方法涉及找到具有不同结果的最近点，但测量点之间距离的方法有很多。首先，有**L1**距离（也称为**曼哈顿距离**）和**L2**距离（也称为**欧几里得距离**），以及其他许多方法。但还有关于距离归一化的一个问题，因为并非所有特征都具有相同的尺度。否则，它们会对尺度较小的特征（如独热编码的特征）产生偏见。也有许多归一化方案可供选择。你可以使用**标准差**、**最小-最大缩放**，甚至**中值绝对偏差**[9]。
- en: In this section, we will explain and use one advanced counterfactual finding
    method. Then, we will explore Google’s **What If Tool** (**WIT**). It has a simple
    L1- and L2-based counterfactual finder, which is limited to the dataset but makes
    up for it with other useful interpretation features.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释并使用一种高级的反事实寻找方法。然后，我们将探索谷歌的**假设工具**（**WIT**）。它有一个简单的基于L1和L2的反事实寻找器，虽然它在数据集上有限制，但通过其他有用的解释功能来弥补这一点。
- en: Counterfactual explanations guided by prototypes
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 由原型引导的反事实解释
- en: 'The most sophisticated counterfactual finding algorithms do the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最复杂反事实寻找算法做以下事情：
- en: '**Loss**: These leverage a *loss function* that helps us optimize finding the
    counterfactuals closest to our *instance of interest*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**: 这些方法利用一个**损失函数**来帮助我们优化寻找与我们的**感兴趣实例**最接近的反事实。'
- en: '**Perturbation**: These tend to operate with a *perturbation space* much like
    anchors do, changing as few features as possible. Please note that counterfactuals
    don’t have to be real points in your dataset. That would be far too limiting.
    Counterfactuals exist in the realm of the possible, not of the necessarily known.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扰动**: 这些通常在类似于锚点的**扰动空间**中操作，尽可能少地改变特征。请注意，反事实不必是数据集中的真实点。那会限制得太厉害。反事实存在于可能性的领域，而不是必然已知的领域。'
- en: '**Distribution**: However, counterfactuals have to be realistic, and therefore,
    interpretable. For example, a loss function could help determine that `age < 0`
    alone is enough to make any medium-/high-risk instance low-risk. This is why counterfactuals
    should lie close to the statistical distributions of your data, especially *class-specific
    distributions*. They also should not be biased against smaller-scale features,
    namely categorical variables.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布**：然而，反事实必须是现实的，因此可解释的。例如，损失函数可以帮助确定`年龄 < 0`本身就足以使任何中/高风险实例变为低风险。这就是为什么反事实应该靠近你的数据的统计分布，特别是*类特定分布*。它们也不应该对较小规模的特性，即分类变量有偏见。'
- en: '**Speed**: These run fast enough to be useful in real-world scenarios.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：这些运行足够快，可以在现实场景中有用。'
- en: Alibi’s **Counterfactuals Guided by Prototypes** (`CounterFactualProto`) has
    all these properties. It has a loss function that includes both L1 (*Lasso*) and
    L2 (*Ridge*) regularization as a linear combination, just like **Naïve Elastic
    Net** does ![](img/B18406_06_001.png) but with a weight (![](img/B18406_03_014.png))
    only on the L1 term. The clever part of this algorithm is that it can (optionally)
    use an *autoencoder* to understand the distributions. We will leverage one in
    *Chapter 7*, *Visualizing Convolutional Neural Networks*. However, what’s important
    to note here is that autoencoders, in general, are neural networks that learn
    a compressed representation of your training data. This method incorporates loss
    terms from the autoencoder, such as one for the nearest prototype. A prototype
    is the dimensionality-reduced representation of the counterfactual class.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Alibi的**原型引导的反事实**(`CounterFactualProto`)具有所有这些特性。它有一个损失函数，包括L1 (*Lasso*) 和
    L2 (*Ridge*) 正则化作为线性组合，就像**朴素弹性网络**一样 ![img/B18406_06_001.png]，但只在L1项上有权重(![](img/B18406_03_014.png))。这个算法的巧妙之处在于它可以（可选地）使用一个*自动编码器*来理解分布。我们将在*第7章*，*可视化卷积神经网络*中利用它。然而，这里重要的是要注意，自动编码器通常是一类神经网络，它学习训练数据的压缩表示。这种方法结合了来自自动编码器的损失项，例如最近的原型。原型是反事实类降维后的表示。
- en: If an autoencoder is not available, the algorithm uses a tree often used for
    multidimensional search (*k-d trees*) instead. With this tree, the algorithm can
    efficiently capture the class distributions and choose the nearest prototype.
    Once it has the prototype, the perturbations are guided by it. Incorporating a
    prototype loss term in the loss function ensures that the resulting perturbations
    will be close enough to the prototype that is in distribution for the counterfactual
    class. Many modeling classes and interpretation methods overlook the importance
    of treating continuous and categorical features differently.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有自动编码器可用，算法将使用一个常用于多维搜索的树（*k-d trees*）。使用这棵树，算法可以有效地捕获类分布并选择最近的原型。一旦它有了原型，扰动就会由它引导。在损失函数中引入原型损失项确保产生的扰动足够接近反事实类的分布中的原型。许多建模类和解释方法忽略了处理连续和分类特征不同的重要性。
- en: '`CounterFactualProto` can use two different distance metrics to compute the
    pairwise distances between categories of a categorical variable: **Modified Value
    Difference Metric** (**MVDM**) and **Association-Based Distance Metric** (**ABDM**)
    and can even combine both. Another way in which `CounterFactualProto` ensures
    meaningful counterfactuals is by limiting permutated features to predefined ranges.
    We can use the minimum and maximum values of features to generate a tuple of arrays
    (`feature_range`):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`CounterFactualProto`可以使用两种不同的距离度量来计算分类变量的类别之间的成对距离：**修改后的值差异度量**(**MVDM**)和**基于关联的距离度量**(**ABDM**)，甚至可以结合两者。`CounterFactualProto`确保有意义的反事实的另一种方式是通过限制排列特征到预定义的范围内。我们可以使用特征的最小值和最大值来生成一个数组元组(`feature_range`)：'
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code outputs two arrays – the first one with the minimum and
    the second with the maximum of all features:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出两个数组——第一个包含所有特征的最小值，第二个包含所有特征的最大值：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now instantiate an explainer with `CounterFactualProto`. As arguments,
    it requires the model’s `predict` function (`predict_cb_fn`), the shape of the
    instance you want to explain (`X_test_eval.shape`), the maximum amount of optimization
    iterations to perform (`max_iterations`), and the feature range for perturbed
    instances (`feature_range`). Many hyperparameters can be chosen, including the
    ![](img/B18406_03_014.png) weight to apply to the L1 loss (`beta`) and the θ weight
    to apply to the prototype loss (`theta`). Also, you must specify whether to use
    the *k-d tree* or not (`use_kdtree`) when the autoencoder model isn’t provided.
    Once the explainer is instantiated, you fit it to the test dataset. We are specifying
    the distance metric for categorical features (`d_type`) as the combination of
    ABDM and MVDM:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`CounterFactualProto`实例化一个解释器。作为参数，它需要模型的`predict`函数(`predict_cb_fn`)、你想要解释的实例的形状(`X_test_eval.shape`)、要执行的优化迭代次数的最大值(`max_iterations`)以及扰动实例的特征范围(`feature_range`)。可以选择许多超参数，包括应用于L1损失的权重(![](img/B18406_03_014.png)
    `beta`)和应用于原型损失的θ权重(`theta`)。此外，当自动编码器模型未提供时，你必须指定是否使用*k-d树*(`use_kdtree`)。一旦实例化了解释器，你将其拟合到测试数据集。我们指定分类特征的距离度量(`d_type`)为ABDM和MVDM的组合：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Creating an explanation with an explainer is similar to how it was with anchors.
    Just pass the instance (`X_test_eval`) to the `explain` function. However, outputting
    the results is not as straightforward, mainly because of the features converting
    between one-hot-encoded and ordinal, and interactions among the features. The
    documentation for Alibi ([https://docs.seldon.io/projects/alibi/](https://docs.seldon.io/projects/alibi/))
    has a detailed example of how this is done.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用解释器创建解释与使用主播类似。只需将实例(`X_test_eval`)传递给`explain`函数即可。然而，输出结果并不那么直接，主要是因为特征在one-hot编码和序数之间的转换，以及特征之间的交互。Alibi([https://docs.seldon.io/projects/alibi/](https://docs.seldon.io/projects/alibi/))的文档中有一个详细的示例，说明了如何进行此操作。
- en: 'We will instead use a utility function called `describe_cf_instance` that does
    this for us using the *instance of interest* (`X_test_eval`), explanation (`cf_cb_explanation`),
    class names (`class_names`), one-hot-encoded category locations (`cat_vars_ohe`),
    category map (`category_map`), and feature names (`feature_names`):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`describe_cf_instance`的实用函数来完成这项工作，该函数使用*感兴趣的实例*(`X_test_eval`)、解释(`cf_cb_explanation`)、类名(`class_names`)、one-hot编码的类别位置(`cat_vars_ohe`)、类别映射(`category_map`)和特征名称(`feature_names`)：
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following output was produced by the preceding code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是由前面的代码生成的：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can appreciate from the output that the *instance of interest* (“original”)
    has a 53.26% probability of being *Medium/High Risk*, but the counterfactual is
    barely on the *Low Risk* side with 50.03%! A counterfactual that is slightly on
    the other side is what we would like to see because that likely means that it
    is as close as possible to our *instance of interest*. There are four feature
    differences between them, three of which are categorical (`sex`, `race`, and `c_charge_degree`).
    The fourth difference is the `priors_count` numerical feature, which is treated
    as continuous since the explainer doesn’t know it’s discrete. In any case, the
    relationship should be *monotonic*, meaning an increase in one variable is consistent
    with a decrease or increase in the other. In this case, fewer priors should always
    mean lower risk, which means we can interpret the 1.90 as a 1 because if 0.1 fewer
    priors helped reduce the risk, a whole prior should also do so.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从输出中看出，*感兴趣的实例*（“原始”）有53.26%的概率属于*中/高风险*，但反事实几乎在*低风险*一边，只有50.03%！我们希望看到的是稍微偏向另一边的反事实，因为这可能意味着它与我们*感兴趣的实例*尽可能接近。它们之间有四个特征差异，其中三个是分类的（`sex`、`race`和`c_charge_degree`）。第四个差异是`priors_count`数值特征，由于解释器不知道它是离散的，因此将其视为连续的。无论如何，关系应该是*单调的*，这意味着一个变量的增加与另一个变量的减少或增加一致。在这种情况下，较少的先验应该总是意味着较低的风险，这意味着我们可以将1.90解释为1，因为如果少0.1个先验有助于降低风险，那么整个先验也应该如此。
- en: A more powerful insight derived from `CounterFactualProto`'s output is that
    two demographic features were present in the closest counterfactual to this feature.
    One was found with a method that is designed to follow our classes’ statistical
    distributions and isn’t biased against or in favor of specific types of features.
    And even though it is surprising to see Asian females in our counterfactual because
    it doesn’t fit the narrative that Caucasian males are getting preferential treatment,
    it is concerning to realize that `race` appears in the counterfactual at all.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `CounterFactualProto` 的输出中得出的更有力的见解是，两个人口统计特征出现在与该特征最接近的反事实中。其中一个是通过一种旨在遵循我们类别统计分布的方法发现的，这种方法不会对特定类型的特征产生偏见或偏袒。尽管看到亚洲女性在我们的反事实中很令人惊讶，因为它不符合白人男性得到优先待遇的叙述，但意识到
    `种族` 在反事实中出现的任何情况都令人担忧。
- en: The Alibi library has several counterfactual finding methods, including one
    that leverages reinforcement learning. Alibi also uses *k-d trees* for its trust
    score, which I highly recommend as well! The trust score measures the agreement
    between any classifier and a modified nearest neighbors classifier. The reasoning
    behind this is that a model’s predictions should be consistent on a local level
    to be trustworthy. In other words, if you and your neighbor are almost the same
    in every way, why would you be treated differently?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Alibi 库有几种反事实发现方法，包括一种利用强化学习的方法。Alibi 还使用 *k-d 树* 来计算其信任分数，我也强烈推荐！信任分数衡量任何分类器与修改后的最近邻分类器之间的协议。背后的推理是，一个模型的预测应该在局部层面上保持一致，才能被认为是可信的。换句话说，如果你和你的邻居在各个方面几乎都一样，为什么你会被不同对待？
- en: Counterfactual instances and much more with WIT
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WIT 中的反事实实例以及更多
- en: 'Google’s WIT is a very versatile tool. It requires very little input or preparation
    and opens up in your Jupyter or Colab notebook as an interactive dashboard with
    three tabs:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的 WIT 是一个非常通用的工具。它需要的输入或准备非常少，并在您的 Jupyter 或 Colab 笔记本中以交互式仪表板的形式打开，有三个选项卡：
- en: '**Datapoint editor**: To visualize your datapoints, edit them, and explain
    their predictions.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据点编辑器**：为了可视化您的数据点，编辑它们，并解释它们的预测。'
- en: '**Performance**: To see high-level model performance metrics (for all regression
    and classification models). For binary classification, this tab is called **Performance
    and Fairness** because, in addition to high-level metrics, predictive fairness
    can be compared between your dataset’s feature-based slices.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：要查看高级模型性能指标（适用于所有回归和分类模型）。对于二元分类，此选项卡称为 **性能和公平性**，因为除了高级指标外，还可以比较您数据集基于特征的切片之间的预测公平性。'
- en: '**Features**: To view general feature statistics.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：查看一般特征统计。'
- en: Given that the **Features** tab doesn’t relate to model interpretations, we
    will explore only the first two in this section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 **特征** 选项卡与模型解释无关，我们将只在本节中探索前两个。
- en: Configuring WIT
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 WIT
- en: 'Optionally, we can enrich our interpretations in WIT by creating attributions,
    which are values that explain how much each feature contributes to each prediction.
    You could use any method to generate attributions, but we will use SHAP. We covered
    SHAP first in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*. Since
    we will interpret our CatBoost model in the WIT dashboard, the SHAP explainer
    that is most suitable is `TreeExplainer`, but `DeepExplainer` would work for the
    neural network (and `KernelExplainer` for both). To initialize `TreeExplainer`,
    we need to pass the fitted model (`fitted_cb_mdl`):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以在 WIT 中通过创建归因来丰富我们的解释，这些归因是解释每个特征对每个预测贡献多少的值。您可以使用任何方法生成归因，但我们将使用 SHAP。我们在
    *第 4 章*，*全局模型无关解释方法* 中首先介绍了 SHAP。由于我们将在 WIT 仪表板中解释我们的 CatBoost 模型，因此最合适的 SHAP
    解释器是 `TreeExplainer`，但 `DeepExplainer` 适用于神经网络（`KernelExplainer` 适用于两者）。要初始化 `TreeExplainer`，我们需要传递拟合的模型（`fitted_cb_mdl`）：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: WIT requires all the features in the dataset (including the labels). We will
    use the test dataset, so you could concatenate `X_test` and `y_test`, but even
    those two exclude the ground truth feature (`is_recid`). One way of getting all
    of them is to subset `recidivism_df` with the test dataset indexes (`y_test.index`).
    WIT also needs our data and columns in list format so we can save them as variables
    for later use (`test_np` and `cols_l`).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: WIT 需要数据集中的所有特征（包括标签）。我们将使用测试数据集，因此您可以连接 `X_test` 和 `y_test`，但即使这两个也排除了地面真相特征（`is_recid`）。获取所有这些特征的一种方法是从
    `recidivism_df` 中提取测试数据集的索引（`y_test.index`）。WIT 还需要我们的数据和列的列表格式，以便我们可以将它们保存为变量以供以后使用（`test_np`
    和 `cols_l`）。
- en: 'Lastly, for predictions and attributions, we will need to remove our ground
    truth (`is_recid`) and classification label (`compas_score`), so let’s save the
    index of these columns (`delcol_idx`):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于预测和归因，我们需要移除我们的真实情况（`is_recid`）和分类标签（`compas_score`），所以让我们保存这些列的索引（`delcol_idx`）：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: WIT has several useful functions for customizing the dashboard, such as setting
    a custom distance metric (`set_custom_distance_fn`), displaying class names instead
    of numbers (`set_label_vocab`), setting a custom `predict` function (`set_custom_predict_fn`),
    and a second `predict` function to compare two models (`compare_custom_predict_fn`).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: WIT有几个用于自定义仪表板的有用函数，例如设置自定义距离度量（`set_custom_distance_fn`）、显示类名而不是数字（`set_label_vocab`）、设置自定义`predict`函数（`set_custom_predict_fn`）以及第二个用于比较两个模型的`predict`函数（`compare_custom_predict_fn`）。
- en: 'In addition to `set_label_vocab`, we are going to only use a custom `predict`
    function (`custom_predict_with_shap`). All it needs to function is to take an
    array with your `examples_np` dataset and produce some predictions (`preds`).
    However, we first must remove features that we want in the dashboard but weren’t
    used for the training (`delcol_idx`). This function’s output is a dictionary with
    the predictions stored in a `predictions` key. But we’d also like some attributions,
    which is why we need an `attributions` key in that dictionary. Therefore, we take
    our SHAP explainer and generate `shap_values`, which is a `numpy` array. However,
    attributions need to be a list of dictionaries to be understood by the WIT dashboard.
    To this end, we iterate `shap_output` and convert each observation’s SHAP values
    array into a dictionary (`attrs`) and then append this to a list (`attributions`):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`set_label_vocab`之外，我们还将只使用自定义的`predict`函数（`custom_predict_with_shap`）。它只需要一个包含你的`examples_np`数据集的数组并产生一些预测（`preds`）。然而，我们首先必须移除那些我们想在仪表板中显示但未用于训练的特征（`delcol_idx`）。这个函数的输出是一个字典，其中预测存储在`predictions`键中。但我们还希望有一些归因，这就是为什么我们需要在字典中有一个`attributions`键。因此，我们使用SHAP解释器生成`shap_values`，这是一个`numpy`数组。然而，归因需要是一个字典列表，以便WIT仪表板能够理解。为此，我们迭代`shap_output`并将每个观察的SHAP值数组转换为字典（`attrs`），然后将这个字典追加到一个列表（`attributions`）中：
- en: '[PRE32]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Before we build the WIT dashboard, it’s important to note that to find our
    *instance of interest* in the dashboard, we need to know its position within the
    `numpy` array provided to WIT because these don’t have indexes as `pandas` DataFrames
    do. To find the position, all we need to do is provide the `get_loc` function
    with the index:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建WIT仪表板之前，重要的是要注意，为了在仪表板中找到我们的*感兴趣实例*，我们需要知道它在WIT提供的`numpy`数组中的位置，因为这些没有像`pandas`数据框那样的索引。为了找到位置，我们只需要向`get_loc`函数提供索引：
- en: '[PRE33]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code outputs as `2910`, so we can take note of this number. Building
    the WIT dashboard is fairly straightforward now. We first initialize a config
    (`WitConfigBuilder`) with our test dataset in `numpy` format (`test_np`) and our
    list of features (`cols_l`). Both are converted to lists with `tolist()`. Then,
    we set our custom `predict` function with `set_custom_predict_fn` and our target
    feature (`is_recid`) and provide our class names. We will use the ground truth
    this time to evaluate fairness from the perspective of what really happened. Once
    the config is initialized, the widget (`WitWidget`) builds the dashboard with
    it. You can optionally provide a height (the default is 1,000 pixels):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出为`2910`，因此我们可以注意这个数字。现在构建WIT仪表板相当直接。我们首先使用测试数据集（以`numpy`格式`test_np`）和我们的特征列表（`cols_l`）初始化一个配置（`WitConfigBuilder`）。这两个都通过`tolist()`转换为列表。然后，我们使用`set_custom_predict_fn`设置我们的自定义`predict`函数以及我们的目标特征（`is_recid`）并提供我们的类名。这次我们将使用真实情况来评估公平性。一旦配置初始化，小部件（`WitWidget`）就会用它来构建仪表板。你可以选择提供高度（默认为1,000像素）：
- en: '[PRE34]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Datapoint editor
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据点编辑器
- en: 'In *Figure 6.5*, you can see the WIT dashboard with its three tabs. We will
    first explore the first tab (**Datapoint editor**). It has **Visualize** and **Edit**
    panes on the left, and on the right, it can show you either **Datapoints** or
    **Partial dependence plots**. When you have **Datapoints** selected, you can visualize
    the datapoints in many ways using the controls in the upper right (the highlighted
    area *A*). What we have done in *Figure 6.5* is set the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.5*中，你可以看到带有三个标签页的WIT仪表板。我们将首先探索第一个标签页（**数据点编辑器**）。它左侧有**可视化**和**编辑**面板，右侧可以显示**数据点**或**部分依赖图**。当你选择**数据点**时，你可以使用右上角的控制（高亮区域*A*）以多种方式可视化数据点。我们在*图6.5*中设置如下：
- en: '**Binning** | **X-axis**: `c_charge_degree_(F7)`'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱** | **X轴**：`c_charge_degree_(F7)`'
- en: '**Binning** | **Y-axis**: `compas_score`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱** | **Y轴**：`compas_score`'
- en: '**Color By**: `race_African-American`'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按**：`race_African-American`'
- en: Everything else stays the same.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其他所有内容保持不变。
- en: 'These settings resulted in all our datapoints being neatly organized into two
    rows and two columns and color-coded by African American or not. The right column
    is for those with a level 7 charge degree, and the upper row is for those with
    a *Medium/High Risk* COMPAS score. We can look for datapoint `2910` in this subgroup
    (*B*) by clicking on the top-rightmost item. It should appear in the **Edit**
    pane (*C*). Interestingly enough, the SHAP attributions for this datapoint are
    three times higher for `age` than they are for `race_African-American`. But still,
    race altogether is second to age in importance. Also, notice that in the **Infer**
    pane, you see the predicted probability for *Medium/High Risk* is approximately
    83%:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置导致我们的所有数据点都被整齐地组织成两行两列，并按非洲裔美国人或非非洲裔美国人进行着色编码。右侧列是为那些具有7级电荷程度的个体，而上方行是为那些具有*中等/高风险*
    COMPAS分数的个体。我们可以通过点击最右上方的项目来查找这个子组（*B*）中的数据点`2910`。它应该出现在**编辑**面板（*C*）中。有趣的是，这个数据点的SHAP归因对于`age`是`race_African-American`的三倍。然而，种族整体在重要性上仍然排在年龄之后。此外，请注意，在**推断**面板中，您可以看到*中等/高风险*的预测概率约为83%：
- en: '![Graphical user interface, treemap chart  Description automatically generated](img/B18406_06_05.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，树状图图表  自动生成的描述](img/B18406_06_05.png)'
- en: 'Figure 6.5: WIT dashboard with our instance of interest'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：WIT仪表板以及我们感兴趣的实例
- en: 'WIT can find the nearest counterfactual using L1 or L2 distances. And it can
    use either feature values or attributions to calculate the distances. As mentioned
    earlier, WIT can also include a custom distance-finding function if you add it
    to the configuration. For now, we will select **L2** with the **Feature value**.
    In *Figure 6.6*, these options appear in the highlighted *A* area. Once you choose
    a distance metric and enable **Nearest counterfactual**, it appears side by side
    with our *instance of interest* (area *B*), and it compares their predictions
    as shown in *Figure 6.6* (area *C*). You can sort the features by **Absolute attribution**
    for a clearer understanding of feature importance on a local level. The counterfactual
    is only 3 years older but has zero priors instead of two, yet that was enough
    to reduce the **Medium/High Risk** to nearly 5%:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: WIT可以使用L1或L2距离找到最近的反事实。它可以使用特征值或归因来计算距离。如前所述，如果您将其添加到配置中，WIT还可以包含一个自定义的距离查找函数。目前，我们将选择**L2**和**特征值**。在*图6.6*中，这些选项出现在高亮的*A*区域。一旦您选择了一个距离度量并启用了**最近的反事实**，它就会与我们的*实例实例*（区域*B*）并排显示，并比较它们的预测，如图*图6.6*（区域*C*）所示。您可以通过**绝对归因**对特征进行排序，以便更清晰地了解局部层面的特征重要性。反事实仅比我们感兴趣的实例大3年，但没有先验值，而是有两个，但这足以将**中等/高风险**降低到近5%：
- en: '![Graphical user interface  Description automatically generated](img/B18406_06_06.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成的描述](img/B18406_06_06.png)'
- en: 'Figure 6.6: How to find the nearest counterfactual in WIT'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：如何在WIT中找到最近的反事实
- en: 'While both our *instance of interest* and counterfactual remain selected, we
    can visualize them along with all other points. By doing this, you take insights
    from local interpretations and can create enough context for global understandings.
    For instance, let’s change our visualization settings to the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的*实例实例*和反事实都保持选中状态时，我们可以将它们与其他所有点一起可视化。通过这样做，您可以从局部解释中获得见解，并为全局理解创造足够的内容。例如，让我们将我们的可视化设置更改为以下内容：
- en: '**Binning** | **X-axis**: `Inference label`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱** | **X轴**：`推理标签`'
- en: '**Binning** | **Y-axis**: `(none)`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分箱** | **Y轴**：`(无)`'
- en: '**Scatter** | **X-axis**: `age`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**散点图** | **X轴**：`age`'
- en: '**Scatter** | **Y-axis**: `priors_count`'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**散点图** | **Y轴**：`priors_count`'
- en: Everything else stays the same.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其他所有内容保持不变。
- en: 'The result of this visualization is depicted in *Figure 6.7*. You can tell
    that the **Low Risk** bins’ points tend to hover in the lower end of `priors_count`.
    Both bins show that `prior_count` and `age` have a slight correlation, although
    this is substantially more pronounced in the **Medium/High Risk** bin. However,
    what is most interesting is the sheer density of African American datapoints deemed
    **Medium/High Risk** in `age` ranging 18–25 and with `prior_count` below three,
    compared to those in the **Low Risk** bin. It suggests that both lower `age` and
    higher `priors_count` increases risk more for African Americans than others:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化的结果是*图6.7*所展示的。您可以看到，**低风险**箱中的点倾向于在`priors_count`的较低端徘徊。两个箱体都表明`prior_count`和`年龄`有轻微的相关性，尽管这在**中/高风险**箱中更为明显。然而，最有趣的是，与**低风险**箱中的数据点相比，在`年龄`为18-25岁且`priors_count`低于三的非洲裔美国人数据点的纯粹密度被认为是**中/高风险**。这表明，对于非洲裔美国人来说，较低的`年龄`和较高的`priors_count`比其他人更容易增加风险：
- en: '![Graphical user interface, chart, scatter chart  Description automatically
    generated](img/B18406_06_07.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，图表，散点图描述自动生成](img/B18406_06_07.png)'
- en: 'Figure 6.7: Visualizing age versus priors_count in WIT'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：在WIT中可视化年龄与priors_count的关系
- en: 'We can try creating our own counterfactuals by editing the datapoint. What
    happens when we reduce `priors_count` to one? The answer to this question is depicted
    in *Figure 6.8*. Once you make the change and click on the **Predict** button
    in the **Infer** pane, it adds an entry to the last prediction history in the
    **Infer** pane. You can tell in **Run #2** that the risk reduces nearly to 33.5%,
    down nearly 50%!'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过编辑数据点来尝试创建自己的反事实。当我们把`priors_count`减少到一的时候会发生什么？这个问题的答案在*图6.8*中展示。一旦您做出更改并点击**推断**面板中的**预测**按钮，它将在**推断**面板的最后预测历史中添加一个条目。您可以在**运行#2**中看到，风险几乎降低到33.5%，下降了近50%！
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_06_08.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_06_08.png)'
- en: 'Figure 6.8: Editing the datapoint to decrease priors_count in WIT'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：在WIT中编辑数据点以减少priors_count
- en: 'Now, what happens if `age` is only 2 years older but there are two priors?
    In *Figure 6.9*, **Run #3** tells you that it barely made it inside the **Low
    Risk** score:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果`年龄`只比之前多2年，但有两次先验，会发生什么？在*图6.9*中，**运行#3**告诉你它几乎达到了**低风险**评分：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_06_09.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_06_09.png)'
- en: 'Figure 6.9: Editing the datapoint to increase the age in WIT'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：在WIT中编辑数据点以增加年龄
- en: 'Another feature that the **Datapoint editor** tab has is **Partial dependence
    plots**, which we covered in *Chapter 4*, *Global Model-Agnostic Interpretation
    Methods*. If you click on this radio button, it will modify the right pane to
    look like *Figure 6.10*. By default, if a datapoint is selected, the PDPs are
    local, meaning they pertain to the chosen datapoint. But you can switch to global.
    In any case, it’s best to sort plots by variation as done in *Figure 6.10*, where
    `age` and `priors_count` have the highest variation. Interestingly, neither of
    them is monotonic, which doesn’t make sense. The model should learn that an increase
    in `priors_count` should consistently increase risk. It should be the same with
    a decrease in `age`. After all, academic research shows that crime tends to peak
    in the mid-20s and that higher priors increase the likelihood of recidivism. The
    relationship between these two variables is also well understood, so perhaps some
    data engineering and monotonic constraints could make sure a model was consistent
    with known phenomena rather than learning the inconsistencies in the data that
    lead to unfairness. We will cover this in *Chapter 12*, *Monotonic Constraints
    and Model Tuning for Interpretability*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据点编辑器**标签页还具有另一个功能，即**部分依赖图**，这在*第4章*，*全局模型无关解释方法*中有所介绍。如果您点击此单选按钮，它将修改右侧面板以类似于*图6.10*的外观。默认情况下，如果选择了一个数据点，PDPs是局部的，意味着它们与所选数据点相关。但您可以切换到全局。无论如何，最好按变化排序图表，就像在*图6.10*中那样，其中`年龄`和`priors_count`变化最大。有趣的是，这两个变量都不是单调的，这没有意义。模型应该学会`priors_count`的增加应始终增加风险。当`年龄`减少时，情况应该相同。毕竟，学术研究表明，犯罪倾向于在20多岁时达到顶峰，而较高的先验会增加再犯的可能性。这两个变量之间的关系也已被充分理解，因此，也许一些数据工程和单调约束可以确保模型与已知现象一致，而不是学习导致不公平的数据不一致性。我们将在*第12章*，*单调约束和模型调优以提高可解释性*中介绍这一点：'
- en: '![Chart, line chart  Description automatically generated](img/B18406_06_10.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_06_10.png)'
- en: 'Figure 6.10: Local partial dependence plot for age and priors_count'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：年龄和先验计数局部部分依赖图
- en: Is there something that can be done to improve fairness in a model that has
    already been trained? Indeed, there is. The **Performance & Fairness** tab can
    help with that.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经训练好的模型，有没有什么方法可以提高其公平性？确实有。**性能与公平性**选项卡可以帮助你做到这一点。
- en: Performance & Fairness
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能与公平性
- en: 'When you click on the **Performance & Fairness** tab, you will see that it
    has **Configure** and **Fairness** panes on the left. And on the right, you can
    explore the overall performance of the model (see *Figure 6.11*). In the upper
    part of this pane, it has **False Positives (%)**, **False Negatives (%)**, **Accuracy
    (%)**, and **F1** fields. If you expand the pane, it shows the ROC curve, PR curve,
    confusion matrix, and mean attributions – the average Shapley values. We covered
    these terms in the previous chapters of this book either directly or indirectly,
    except for the PR curve. The **Precision-Recall** (**PR**) is very much like the
    ROC curve, except it plots precision against recall instead of TPR versus FPR.
    In this plot, precision is expected to decrease as recall decreases. Unlike ROC,
    it’s considered worse than a coin toss when the line is close to the *x*-axis,
    and it’s best suited to imbalanced classification problems:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击**性能与公平性**选项卡时，你会看到它左侧有**配置**和**公平性**面板。在右侧，你可以探索模型的总体性能（见*图6.11*）。在这个面板的上部，有**误报率（%）**、**漏报率（%）**、**准确率（%）**和**F1**字段。如果你展开面板，它会显示ROC曲线、PR曲线、混淆矩阵和平均归因——Shapley值的平均值。我们在本书的前几章中直接或间接地介绍了这些术语，除了PR曲线。**精确率-召回率**（**PR**）与ROC曲线非常相似，但它绘制的是精确率对召回率，而不是TPR对FPR。在这个图中，预期精确率会随着召回率的降低而降低。与ROC不同，当线接近*x*轴时，它被认为比抛硬币还差，它最适合不平衡分类问题：
- en: '![Graphical user interface, chart  Description automatically generated](img/B18406_06_11.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，图表  自动生成的描述](img/B18406_06_11.png)'
- en: 'Figure 6.11: Performance & Fairness tab initial view'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：性能与公平性选项卡初始视图
- en: A classification model will output probabilities that an observation belongs
    to a class label. We usually take every observation above or equal to 0.5 to belong
    to the positive class. Otherwise, we predict it to belong to the negative class.
    This threshold is called the **classification threshold**, and you don’t always
    have to use the standard 0.5.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类模型将输出一个观察值属于某个类别标签的概率。我们通常将所有大于或等于0.5的观察值归为正类。否则，我们预测它属于负类。这个阈值被称为**分类阈值**，你不必总是使用标准的0.5。
- en: 'There are many cases in which it is appropriate to perform **threshold tuning**.
    One of the most compelling reasons is imbalanced classification problems because
    often models optimize performance on accuracy alone but end up with bad recall
    or precision. Adjusting the threshold will improve the metric you care most about:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，进行**阈值调整**是合适的。其中一个最令人信服的原因是不平衡的分类问题，因为通常模型只优化准确率，但最终召回率或精确度不佳。调整阈值将提高你最关心的指标：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_06_12.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  自动生成的描述](img/B18406_06_12.png)'
- en: 'Figure 6.12: Slicing performance metrics by race_African-American'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：按种族划分的性能指标_African-American
- en: 'Another primary reason to adjust thresholds is for fairness. To this end, you
    need to examine the metric you most care about across different slices of your
    data. In our case, **False Positives (%)** is where we can appreciate unfairness
    the most. For instance, take a look at *Figure 6.12*. In the **Configure** pane,
    we can slice the data by `race_African-American`, and to the right of it, we can
    see what we observed at the beginning of this chapter, which is that FPs for African
    Americans are substantially higher than for other segments. One way to fix this
    is through an automatic optimization method such as **Demographic parity** or
    **Equal opportunity**. If you are to use one of these, it’s best to adjust **Cost
    Ratio (FP/FN)** to tell the optimizer that FPs are worth more than FNs:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 调整阈值的另一个主要原因是公平性。为此，你需要检查你最关心的指标在不同数据切片中的表现。在我们的案例中，**假阳性百分比**是我们最能感受到不公平的地方。例如，看看*图6.12*。在**配置**面板中，我们可以通过`race_African-American`来切片数据，在其右侧，我们可以看到本章开头观察到的结果，即非洲裔美国人的FPs显著高于其他群体。解决这个问题的一种方法是通过自动优化方法，如**人口统计平衡**或**平等机会**。如果你要使用其中之一，最好调整**成本比率（FP/FN）**来告诉优化器FP比FN更有价值：
- en: '![Chart  Description automatically generated with medium confidence](img/B18406_06_13.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成，置信度中等](img/B18406_06_13.png)'
- en: 'Figure 6.13: Adjusting the classification threshold for the dataset sliced
    by race_African-American'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：调整按种族_African-American划分的数据集的分类阈值
- en: We can also adjust thresholds manually using the default **Custom thresholds**
    setting (see *Figure 6.13*). For these slices, if we want approximate parity with
    our FPs, we should use 0.78 as our threshold for when `race_African-American=1`.
    The drawback is that FNs will increase for this group, not achieving parity on
    that end. A cost ratio would help determine whether 14.7% in FPs justifies 24.4%
    in FNs, but to do this, we would have to understand the average costs involved.
    We will examine odds calibration methods further in *Chapter 11*, *Bias Mitigation
    and Causal Inference Methods*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用默认的**自定义阈值**设置手动调整阈值（参见*图6.13*）。对于这些切片，如果我们想要与我们的FPs（假阳性）近似相等，我们应该将`race_African-American=1`时的阈值设为0.78。缺点是对于这个群体，FNs（假阴性）会增加，无法在那一端实现平衡。成本比率可以帮助确定14.7%的FPs是否足以证明24.4%的FNs是合理的，但要做到这一点，我们必须了解平均成本。我们将在*第11章*，*偏差缓解和因果推断方法*中进一步探讨概率校准方法。
- en: Mission accomplished
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: This chapter’s mission was to see whether there was unfair bias in predicting
    whether a particular defendant would recidivate. We demonstrated that the FPR
    for African American defendants is 1.87 times higher than for Caucasian defendants.
    This disparity was confirmed with WIT, indicating that the model in question is
    much more likely to misclassify the positive class based on race. However, this
    is a global interpretation method, so it doesn’t answer our question regarding
    a specific defendant. Incidentally, in *Chapter 11*, *Bias Mitigation and Causal
    Inference Methods*, we will cover other global interpretation methods for unfairness.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的任务是查看在预测特定被告是否会再犯时是否存在不公平的偏见。我们证明了非洲裔美国被告的FPR（假阳性率）是白人被告的1.87倍。这种差异通过WIT（加权集成测试）得到证实，表明所讨论的模型更有可能基于种族错误地将正类分类。然而，这是一个全局解释方法，所以它没有回答我们关于特定被告的问题。顺便提一下，在*第11章*，*偏差缓解和因果推断方法*中，我们将介绍其他用于不公平性的全局解释方法。
- en: 'To ascertain whether the model was racially biased against the defendant in
    question, we leveraged anchor and counterfactual explanations – they both output
    race as a primary feature in their explanations. The anchor did it with relatively
    high precision and coverage, and *Counterfactuals Guided by Prototypes* found
    that the closest decision has a different race. That being said, in both cases,
    race wasn’t the only feature in the explanations. The features usually included
    any or all of the following: `priors_count`, `age`, `charge_degree`, and `sex`.
    The inconsistent rules involving the first three regarding `race` suggests double
    standards and the involvement of `sex` suggests intersectionality. **Double standards**
    are when rules are applied unfairly to different groups. **Intersectionality**
    is how overlapping identities create different systems of interconnected modes
    of discrimination. However, we know that females of all races are less likely
    to recidivate according to academic research. Still, we have to ask ourselves
    whether females have a structural advantage that makes them privileged in this
    context. A healthy dose of skepticism can help, since when it comes to bias, there’s
    usually a more elaborate dynamic going on than meets the eye. The bottom line
    is that despite all the other factors that interplay with race, and provided that
    there’s no relevant criminological information that we are missing, yes—there’s
    racial bias involved in this particular prediction.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定该模型是否对所涉及的被告存在种族偏见，我们利用了锚定和反事实解释——它们都在解释中将种族作为主要特征。锚定以相对较高的精确度和覆盖率完成，而*原型引导的反事实*发现最接近的决定具有不同的种族。尽管如此，在这两种情况下，种族都不是解释中唯一的特征。通常包括以下任何或所有特征：`priors_count`、`age`、`charge_degree`和`sex`。涉及前三个与`race`相关的不一致规则暗示了双重标准，而涉及`sex`则暗示了交叉性。**双重标准**是指规则对不同群体不公平地应用。**交叉性**是指重叠的身份如何创建不同的相互关联的歧视模式。然而，根据学术研究，我们知道所有种族的女性再犯的可能性较低。尽管如此，我们仍需问自己，女性是否具有结构性优势，使她们在这个背景下享有特权。适量的怀疑态度可能会有所帮助，因为当涉及到偏见时，通常存在比表面现象更复杂的动态。底线是，尽管所有其他与种族相关的因素都在相互作用，并且假设我们没有遗漏相关的犯罪学信息，是的——在这个特定的预测中存在种族偏见。
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, you should know how to leverage anchors, to understand
    the decision rules that impact a classification, and counterfactuals, to grasp
    what needs to change for the predicted class to change. You also learned how to
    assess fairness using confusion matrices and Google’s WIT. In the next chapter,
    we will study interpretation methods for **Convolutional Neural Networks** (**CNNs**).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章之后，你应该知道如何利用锚定来理解影响分类的决定规则，以及如何使用反事实来掌握预测类别需要改变的内容。你还学习了如何使用混淆矩阵和谷歌的WIT来评估公平性。在下一章中，我们将研究**卷积神经网络**（**CNNs**）的解释方法。
- en: Dataset sources
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集来源
- en: ProPublica Data Store, 2019, COMPAS Recidivism Risk Score Data and Analysis.
    Originally retrieved from [https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ProPublica数据存储库，2019，COMPAS再犯风险评分数据和分析。最初从[https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)检索。
- en: Further reading
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Desmarais, S.L., Johnson, K.L., and Singh, J.P., 2016, *Performance of recidivism
    risk assessment instruments in U.S. correctional settings*. Psychol Serv;13(3):206-222:
    [https://doi.org/10.1037/ser0000075](https://doi.org/10.1037/ser0000075)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Desmarais, S.L., Johnson, K.L., 和 Singh, J.P., 2016, *美国矫正环境中再犯风险评估工具的性能*.
    心理服务;13(3):206-222: [https://doi.org/10.1037/ser0000075](https://doi.org/10.1037/ser0000075)'
- en: 'Berk, R., Heidari, H., Jabbari, S., Kearns, M., and Roth, A., 2017, *Fairness
    in Criminal Justice Risk Assessments: The State of the Art*. Sociological Methods
    & Research.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berk, R., Heidari, H., Jabbari, S., Kearns, M., 和 Roth, A., 2017, *刑事司法风险评估中的公平性：现状*.
    社会学方法与研究。
- en: 'Angwin, J., Larson, J., Mattu, S., and Kirchner, L., 2016, *Machine Bias. There’s
    software used across the county to predict future criminals*: [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angwin, J., Larson, J., Mattu, S.，和 Kirchner, L.，2016，*机器偏见。全国范围内用于预测未来罪犯的软件*：[https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
- en: 'Ribeiro, M.T., Singh, S., and Guestrin, C., 2018, *Anchors: High-Precision
    Model-Agnostic Explanations*. Proceedings of the AAAI/ACM Conference on AI, Ethics,
    and Society: [https://doi.org/10.1609/aaai.v32i1.11491](https://doi.org/10.1609/aaai.v32i1.11491)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribeiro, M.T., Singh, S., and Guestrin, C., 2018, *锚点：高精度模型无关解释*. AAAI/ACM人工智能、伦理与社会会议论文集:
    [https://doi.org/10.1609/aaai.v32i1.11491](https://doi.org/10.1609/aaai.v32i1.11491)'
- en: 'Rocque, M., Posick, C., & Hoyle, J., 2015, *Age and Crime*. The encyclopedia
    of crime and punishment, 1–8: [https://doi.org/10.1002/9781118519639.wbecpx275](https://doi.org/10.1002/9781118519639.wbecpx275)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rocque, M., Posick, C., & Hoyle, J., 2015, *年龄与犯罪*. 犯罪与惩罚百科全书，1–8: [https://doi.org/10.1002/9781118519639.wbecpx275](https://doi.org/10.1002/9781118519639.wbecpx275)'
- en: 'Dhurandhar, A., Chen, P., Luss, R., Tu, C., Ting, P., Shanmugam, K., and Das,
    P., 2018, *Explanations based on the Missing: Towards Contrastive Explanations
    with Pertinent Negatives*. NeurIPS: [https://arxiv.org/abs/1802.07623](https://arxiv.org/abs/1802.07623)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dhurandhar, A., Chen, P., Luss, R., Tu, C., Ting, P., Shanmugam, K., and Das,
    P., 2018, *基于缺失的解释：向相关负样本的对比解释迈进*. NeurIPS: [https://arxiv.org/abs/1802.07623](https://arxiv.org/abs/1802.07623)'
- en: 'Jiang, H., Kim, B., and Gupta, M.R., 2018, *To Trust Or Not To Trust A Classifier*.
    NeurIPS: [https://arxiv.org/pdf/1805.11783.pdf](https://arxiv.org/pdf/1805.11783.pdf)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang, H., Kim, B., and Gupta, M.R., 2018, *是否信任分类器*. NeurIPS: [https://arxiv.org/pdf/1805.11783.pdf](https://arxiv.org/pdf/1805.11783.pdf)'
- en: Learn more on Discord
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入本书的 Discord 社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_6.xhtml)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_6.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code107161072033138125.png)'
