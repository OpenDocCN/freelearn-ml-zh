- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Deploying, Monitoring, and Scaling in Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生产环境中的部署、监控和扩展
- en: Some people may read this book from beginning to end to gain an overall understanding
    of as many concepts as possible in the realm of AI/ML on Google Cloud, while others
    may use it as a reference, whereby they pick it up and read certain chapters on
    specific topics whenever they need to work with those topics as part of a project
    or client engagement. If you’ve been reading this book from the beginning, then
    you have come a long way, and we have journeyed together through the majority
    of the **ML model development life cycle** (**MDLC**). While model training is
    what often gets the most attention in the press – and that is where a lot of the
    magic happens – you know by now that training is just one piece of the overall
    life cycle.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会从头到尾阅读这本书，以尽可能多地了解Google Cloud在AI/ML领域的概念，而其他人可能会将其作为参考，当他们需要作为项目或客户合作的一部分处理特定主题时，他们会挑选并阅读某些章节。如果您从本书的开头就开始阅读，那么您已经走了很长的路，我们已经一起走过了**ML模型开发生命周期**（**MDLC**）的大部分旅程。虽然模型训练通常是媒体关注的焦点——这也是许多魔法发生的地方——但现在您已经知道，训练只是整个生命周期的一部分。
- en: When we’ve trained and tested our models, and we believe they’re ready to be
    exposed to our clients, we need to find a way to host them so that they can be
    used accordingly. In this chapter, we will dive into that part of the process
    in more detail, including some of the challenges that exist when it comes to hosting
    and managing models and monitoring them on an ongoing basis to ensure that they
    stay relevant and perform optimally in perpetuity. We’ll begin by discussing how
    we can host our models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练和测试了我们的模型，并且相信它们已经准备好向我们的客户展示时，我们需要找到一种方式来托管它们，以便它们可以被相应地使用。在本章中，我们将更详细地探讨这一过程的部分，包括在托管和管理模型以及持续监控它们以确保它们保持相关性并持续优化性能时存在的挑战。我们将从讨论我们如何托管我们的模型开始。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: How do I make my models available to my applications?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何使我的模型可供我的应用程序使用？
- en: Fundamental concepts for serving models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的模型的基本概念
- en: A/B testing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A/B测试
- en: Common challenges of serving models in production
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产环境中服务的模型常见挑战
- en: Monitoring models in production
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控生产环境中的模型
- en: Optimizing for AI/ML at the edge
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在边缘优化AI/ML
- en: How do I make my models available to my applications?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我如何使我的模型可供我的应用程序使用？
- en: We introduced this concept in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    and we talked about the various things you would need to do to host a model on
    your own, such as setting up all of the required infrastructure, including load
    balancers, routers, switches, cables, servers, and storage, among other things,
    and then managing all of that infrastructure on an ongoing basis. This would require
    a lot of your time and resources.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中介绍了这个概念，并讨论了您需要执行的各种操作来在自己的服务器上托管模型，例如设置所有必需的基础设施，包括负载均衡器、路由器、交换机、电缆、服务器和存储等，然后持续管理这些基础设施。这需要您投入大量的时间和资源。
- en: Luckily, all of that stuff was in the old days, and you no longer need to do
    any of that. This is because Google Cloud provides the Vertex AI prediction service,
    which enables you to host models in production within minutes, using infrastructure
    that is all managed for you by Google.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，所有这些都在过去，您现在不再需要做任何这些事情。这是因为Google Cloud提供了Vertex AI预测服务，它使您能够在几分钟内使用由Google为您管理的基础设施在生产环境中托管模型。
- en: For completeness, I will also mention that if you would like to host your models
    on Google Cloud without using Vertex, numerous other Google Cloud services can
    be used for that purpose, such as **Google Compute Engine** (**GCE**), **Google
    Kubernetes Engine** (**GKE**), **Google App Engine** (**GAE**), Cloud Run, and
    Cloud Functions. We described all of these services, as well as some pointers
    on how to choose between them, in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我还会提到，如果您想在Google Cloud上托管模型而不使用Vertex，还有许多其他Google Cloud服务可以用于此目的，例如**Google
    Compute Engine**（**GCE**）、**Google Kubernetes Engine**（**GKE**）、**Google App Engine**（**GAE**）、Cloud
    Run和Cloud Functions。我们在[*第三章*](B18143_03.xhtml#_idTextAnchor059)中描述了所有这些服务，以及一些关于如何在这之间做出选择的提示。
- en: Remember that choosing the right platform to host your ML models depends on
    your specific use case and requirements. Factors such as scalability, latency,
    costs, development effort, and operational management all play a role in choosing
    the best solution. You may also make certain decisions based on the framework
    you’re using to build your ML models. For example, you might want to use TensorFlow
    Serving if you’re building models in TensorFlow, or TorchServe if you’re building
    models in PyTorch.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，选择合适的平台来托管你的机器学习模型取决于你的具体用例和需求。可扩展性、延迟、成本、开发努力和运营管理都在选择最佳解决方案中发挥作用。你还可以根据你用来构建机器学习模型的框架做出某些决定。例如，如果你在TensorFlow中构建模型，你可能想使用TensorFlow
    Serving；如果你在PyTorch中构建模型，你可能想使用TorchServe。
- en: In most cases, my recommendation would be to start with a service that is dedicated
    and optimized for the task at hand, and in the case of building and hosting ML
    models, Vertex AI is that service. In this chapter, we will deploy our first model
    using Vertex AI, but before we dive into the hands-on activities, we’ll introduce
    some important concepts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我的建议是开始使用一个专门且针对当前任务进行优化的服务，在构建和托管机器学习模型的情况下，Vertex AI就是这样的服务。在本章中，我们将使用Vertex
    AI部署我们的第一个模型，但在我们深入实际操作之前，我们将介绍一些重要概念。
- en: Fundamental concepts for serving models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型服务的根本概念
- en: In this section, we will introduce some important topics related to how we can
    host our models so that our clients can interact with them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些与我们如何托管模型以便我们的客户与之交互相关的重要主题。
- en: Online and offline model serving
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线与离线模型服务
- en: 'In ML, there are generally two options we have for serving predictions from
    models: **online** serving (also known as **real-time** serving) and **offline**
    serving (also known as **batch** serving). The high-level use cases associated
    with each of these methods are **online (or real-time) inference** and **offline
    (or batch) inference**, respectively. Let’s take a few minutes to introduce these
    methods and understand their use cases.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们有两种从模型中提供预测的选项：**在线**服务（也称为**实时**服务）和**离线**服务（也称为**批量**服务）。与每种方法相关的高级用例分别是**在线（或实时）推理**和**离线（或批量）推理**。让我们花几分钟时间介绍这些方法并了解它们的用例。
- en: Online/real-time model serving
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线/实时模型服务
- en: As the name suggests, in the case of real-time model serving, the model needs
    to respond “in real time” to prediction requests, which usually means that a client
    (perhaps a customer, or some other system) needs to receive an inference response
    as quickly as possible, and may be waiting synchronously for a response from the
    model. An example of this would be a fraud detection system for credit card transactions.
    As you can imagine, credit card companies want to detect possible fraudulent transactions
    as quickly as possible – ideally during the transaction process, in which case
    they could prevent the transaction from processing completely, if possible. It
    would not be as useful for them to check for fraudulent transactions at some arbitrary
    point later.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，在实时模型服务的情况下，模型需要“实时”响应预测请求，这通常意味着客户端（可能是客户或某些其他系统）需要尽可能快地收到推理响应，并且可能正在同步等待模型的响应。一个这样的例子就是信用卡交易的反欺诈系统。正如你可以想象的那样，信用卡公司希望尽可能快地检测可能的欺诈交易——理想情况下是在交易过程中，这样他们就可以在可能的情况下阻止交易完全处理。在之后的某个任意时间点检查欺诈交易对他们来说可能就不那么有用。
- en: Considering that online inference requests usually require a response to be
    returned as quickly as possible, factors such as ensuring low latency, handling
    high request volumes, and providing a reliable, always-available service are some
    of the main challenges in this area.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在线推理请求通常需要尽快返回响应，确保低延迟、处理高请求量以及提供可靠且始终可用的服务是该领域的一些主要挑战。
- en: 'In this case, when we refer to low latency, we mean that the prediction response
    time is usually in the order of milliseconds or, at the very most, seconds. The
    actual response time requirements will depend on the business use case. For example,
    some users may accept needing to wait for a few seconds for their credit card
    transaction to be approved or rejected, but another example of real-time inference
    is the use of ML models in self-driving cars, and in that scenario, for example,
    a car must be able to respond to its environment in milliseconds if it needs to
    suddenly take some kind of action, such as avoiding an unexpected obstacle that
    comes into its path. *Figure 10**.1* shows an example of a batch prediction workflow
    that we will implement in the practical exercises in this chapter. We will explain
    each of the components that are depicted in detail. In *Figure 10**.1*, the solid
    lines represent steps we will explicitly perform in the practical exercises, whereas
    the dotted lines represent the steps that Vertex AI will perform automatically
    on our behalf:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，当我们提到低延迟时，我们指的是预测响应时间通常在毫秒级别，或者最多几秒钟。实际的响应时间要求将取决于业务用例。例如，一些用户可能可以接受等待几秒钟来批准或拒绝他们的信用卡交易，但另一个实时推理的例子是自动驾驶汽车中机器学习模型的使用，在这种情况下，例如，如果汽车需要突然采取某种行动，比如避开突然出现在其路径上的意外障碍物，它必须能够在毫秒级别对其环境做出反应。*图10.1*展示了我们将在本章的实践练习中实施的批预测工作流程的示例。我们将详细解释图中展示的每个组件。在*图10.1*中，实线代表我们在实践练习中明确执行的步骤，而虚线代表Vertex
    AI将代表我们自动执行的步骤：
- en: '![Figure 10.1: Online prediction](img/B18143_10_1.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1：在线预测](img/B18143_10_1.jpg)'
- en: 'Figure 10.1: Online prediction'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：在线预测
- en: 'The steps outlined in *Figure 10**.1* are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.1*中概述的步骤如下：'
- en: Train the model in our notebook.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的笔记本中训练模型。
- en: Save the resulting model artifacts to Google Cloud Storage.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的模型工件保存到Google Cloud Storage。
- en: Register the model details in the Vertex AI Model Registry (explained in more
    detail in this chapter).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI模型注册表中注册模型详细信息（在本章中更详细地解释）。
- en: Create an endpoint to host and serve our model.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个端点来托管和提供我们的模型。
- en: The Vertex AI Online Prediction service (explained in more detail in this chapter)
    fetches our model’s details from the Vertex AI Model Registry.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI在线预测服务（在本章中更详细地解释）从Vertex AI模型注册表中检索我们模型的详细信息。
- en: The Vertex AI Online Prediction service fetches our saved model from Google
    Cloud Storage.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI在线预测服务从Google Cloud Storage检索我们保存的模型。
- en: We send a prediction request to our model and receive a response. In this case,
    we are sending the request from our Vertex AI Workbench notebook, but it’s important
    to note that when our model is hosted on an endpoint, prediction requests could
    be sent from any client application that can access that endpoint.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们向我们的模型发送预测请求并接收响应。在这种情况下，我们是从我们的Vertex AI Workbench笔记本发送请求的，但重要的是要注意，当我们的模型托管在端点时，预测请求可以来自任何可以访问该端点的客户端应用程序。
- en: The Vertex AI Online Prediction service saves the prediction inputs, outputs,
    and other details to Google Cloud BigQuery. This is an optional feature that we
    can enable so that we can perform analytical queries on the inputs, outputs, and
    other details related to our model’s predictions.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI在线预测服务将预测输入、输出和其他详细信息保存到Google Cloud BigQuery。这是一个可选功能，我们可以启用它，以便我们可以对输入、输出以及与我们模型预测相关的其他细节执行分析查询。
- en: It should also be noted that in the case of online model serving, predictions
    are generally made on demand, and usually for a single instance or a small batch
    of instances. In this case, your model and its serving infrastructure need to
    be able to quickly react to sudden – and possibly unexpected – changes in inference
    traffic volume.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 还应注意的是，在在线模型服务的情况下，预测通常是按需进行的，通常针对单个实例或一小批实例。在这种情况下，您的模型及其服务基础设施需要能够快速应对推理流量量的突然——可能是意外的——变化。
- en: Offline/batch model serving
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离线/批处理模型服务
- en: 'Given our description of online model serving, it may have become obvious that
    offline serving means that no client is waiting for an immediate response in real
    time. In fact, rather than our model receiving on-demand inference requests from
    individual clients, we can feed many input observations into our model in large
    batches, which it can process over longer periods; perhaps hours or even days,
    depending on the business case. The predictions produced by our models can then
    be stored and used later, rather than being acted on immediately. Examples of
    batch inference use cases include predicting the next day’s stock prices or sending
    out targeted emails to users based on their predicted preferences. *Figure 10**.2*
    shows an example of a batch prediction workflow that we will implement in the
    practical exercises in this chapter. In *Figure 10**.2*, the solid lines represent
    steps we will explicitly perform in the practical exercises, whereas the dotted
    lines represent the steps that Vertex AI will perform automatically on our behalf:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对在线模型服务的描述，可能已经很明显，离线服务意味着没有客户端在实时等待即时响应。实际上，而不是我们的模型从单个客户端接收按需推理请求，我们可以将许多输入观测值以大批次的形式输入到我们的模型中，它可以处理更长的时间；可能是几个小时甚至几天，具体取决于业务案例。我们的模型产生的预测可以随后存储并用于以后，而不是立即采取行动。批量推理用例的例子包括预测第二天股票价格或根据预测的偏好向用户发送定向电子邮件。*图
    10**.2* 展示了我们将在本章的实践练习中实施的批量预测工作流程的示例。在 *图 10**.2* 中，实线表示我们将明确在实践练习中执行的步骤，而虚线表示
    Vertex AI 将代表我们自动执行的步骤：
- en: '![Figure 10.2: Batch prediction](img/B18143_10_2.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2：批量预测](img/B18143_10_2.jpg)'
- en: 'Figure 10.2: Batch prediction'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：批量预测
- en: 'The steps outlined in *Figure 10**.2* are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10**.2* 中概述的步骤如下：'
- en: Train the model in our notebook.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的笔记本中训练模型。
- en: Save the resulting model artifacts to Google Cloud Storage.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的模型工件保存到 Google Cloud Storage。
- en: Register the model details in the Vertex AI Model Registry.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Vertex AI 模型注册表中注册模型详细信息。
- en: Save the test data in Google Cloud Storage. This will be used as the input data
    in our batch prediction job later.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将测试数据保存到 Google Cloud Storage。这将在我们后续的批量预测作业中用作输入数据。
- en: Create a batch prediction job.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个批量预测作业。
- en: The Vertex AI Batch Prediction service (explained in more detail in this chapter)
    fetches our model’s details from the Vertex AI Model Registry.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI 批量预测服务（在本章中更详细地解释）从 Vertex AI 模型注册表中获取我们模型的详细信息。
- en: The Vertex AI Batch Prediction service fetches our saved model from Google Cloud
    Storage.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI 批量预测服务从 Google Cloud Storage 获取我们保存的模型。
- en: The Vertex AI Batch Prediction service fetches our input data from Google Cloud
    Storage.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI 批量预测服务从 Google Cloud Storage 获取我们的输入数据。
- en: The Vertex AI Batch Prediction runs a batch prediction job, using our model
    and input data.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI 批量预测运行一个批量预测作业，使用我们的模型和输入数据。
- en: The Vertex AI Batch Prediction service saves the prediction outputs to Google
    Cloud Storage.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vertex AI 批量预测服务将预测输出保存到 Google Cloud Storage。
- en: Rather than being optimized for low latency, batch prediction systems are generally
    optimized to handle a large number of instances at once (that is, high throughput),
    and therefore usually fall into the category of large-scale distributed computing
    use cases, which can benefit from parallelized execution, and can be scheduled
    to automatically execute periodically (for example, once a day), or can be triggered
    by an event (for example, when a new batch of data is available).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与优化低延迟不同，批量预测系统通常优化以一次性处理大量实例（即高吞吐量），因此通常属于大规模分布式计算用例，可以从并行执行中受益，并且可以安排定期自动执行（例如，每天一次），或者可以由事件触发（例如，当有新的数据批次可用时）。
- en: It’s important to note that the decision between online and offline serving
    is not always strictly binary, and you may find that a combination of online and
    offline serving best meets your needs. For example, you may use offline serving
    to generate large-scale reports, while also using online serving to make real-time
    predictions in user-facing applications. In either case, both online and offline
    serving require the model to be deployed in a serving infrastructure. This infrastructure
    is responsible for loading the model, receiving prediction requests, making predictions
    using the model, and returning the predictions. In Google Cloud, there are various
    tools and platforms available to assist with this, such as TensorFlow Serving,
    and the Vertex AI prediction service, which we will discuss in more detail in
    subsequent sections in this chapter. First, however, let’s introduce another tool
    that is important for managing our models as part of our end-to-end model development
    life cycle.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在线和离线服务的决策并不总是严格二元的，你可能会发现在线和离线服务的组合最能满足你的需求。例如，你可能会使用离线服务生成大规模报告，同时使用在线服务在面向用户的应用中进行实时预测。在任一情况下，在线和离线服务都需要将模型部署在服务基础设施中。该基础设施负责加载模型、接收预测请求、使用模型进行预测，并返回预测结果。在Google
    Cloud中，有各种工具和平台可用于协助此任务，例如TensorFlow Serving和Vertex AI预测服务，我们将在本章后续部分详细讨论。然而，首先，让我们介绍另一个作为我们端到端模型开发生命周期一部分的重要工具。
- en: Vertex AI Model Registry
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI 模型注册库
- en: 'Earlier in this book, we made an analogy between the traditional **software
    development life cycle** (**SDLC**) and the MDLC. To dive into this analogy in
    more depth, let’s consider some important tools in the SDLC process:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的早期部分，我们将传统的**软件开发生命周期**（**SDLC**）与MDLC进行了类比。为了更深入地探讨这个类比，让我们考虑SDLC过程中的几个重要工具：
- en: '**Shared repositories**: As a part of the SDLC process, we usually want to
    store our code and related artifacts in registries or repositories that can be
    accessed by multiple contributors who need to collaborate on a specific development
    project. Such repositories often include metadata that helps describe certain
    aspects of the code assets so that people can easily understand how those assets
    were developed, as well as how they are used.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享仓库**：作为SDLC过程的一部分，我们通常希望将我们的代码和相关工件存储在注册库或仓库中，以便多个需要协作进行特定开发项目的贡献者可以访问。这样的仓库通常包括帮助描述代码资产某些方面的元数据，以便人们可以轻松理解这些资产是如何开发的，以及它们是如何被使用的。'
- en: '**Version control**: As contributors make changes to code assets in a given
    project, we want to ensure that we are tracking such changes and contributions
    and that all contributors can easily access that information. This also enables
    us to roll back to previous versions if we notice issues in a newly deployed version
    of our software.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制**：当贡献者对特定项目中的代码资产进行更改时，我们希望确保我们正在跟踪这些更改和贡献，并且所有贡献者都可以轻松访问这些信息。这也使我们能够在发现软件新部署版本中的问题时回滚到之前的版本。'
- en: Experience has taught us that similar tools are required when we want to efficiently
    deploy and manage ML models, especially when doing so at a large scale (remember
    that some companies may have thousands of ML models, owned by hundreds of different
    teams).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 经验告诉我们，当我们想要高效地部署和管理机器学习模型时，需要类似的工具，尤其是在大规模部署时（记住，一些公司可能拥有成千上万的机器学习模型，由数百个不同的团队拥有）。
- en: Also, bear in mind that data science projects are often highly experimental,
    in which the data science teams may try out lots of different algorithms, datasets,
    and hyperparameter values, training lots of different models to see which options
    produce the best results. This is especially true in the early stages of a data
    science project, but this also often holds true on an ongoing basis, whereby the
    data scientists constantly strive to improve the models even after they have been
    deployed to a production environment. They do this to keep abreast of emerging
    trends in the industry and produce better results.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，数据科学项目通常高度实验性，数据科学团队可能会尝试许多不同的算法、数据集和超参数值，训练许多不同的模型，以查看哪些选项能产生最佳结果。这在数据科学项目的早期阶段尤其如此，但这种情况也常常持续存在，数据科学家即使在模型部署到生产环境之后，也会不断努力改进模型。他们这样做是为了跟上行业的新趋势，并产生更好的结果。
- en: 'Google Cloud’s Vertex AI platform includes a Model Registry service that allows
    us to manage our ML models in a centralized place, making it much easier for us
    to track how models are developed, even when multiple teams are contributing to
    the development of those models. Let’s take a look at some of the Model Registry’s
    important features:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 的 Vertex AI 平台包括一个模型注册服务，它允许我们在一个集中的位置管理我们的机器学习模型，这使得我们能够更容易地跟踪模型的发展情况，即使有多个团队在贡献这些模型的发展。让我们来看看模型注册的一些重要功能：
- en: '**Model versioning**: The Model Registry allows us to create multiple versions
    of a model, where each version can correspond to a different set of training parameters
    or a different set of training data. This helps us keep track of different experiments
    or deployments.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型版本控制**：模型注册允许我们创建多个版本的模型，其中每个版本可以对应不同的训练参数集或不同的训练数据集。这有助于我们跟踪不同的实验或部署。'
- en: '**Model metadata**: For each model, we can record metadata such as the model’s
    description, the input and output schemas, the labels (useful for categorization),
    and the metrics (useful for comparing models). For each version, we can record
    additional metadata such as the description, the runtime version (corresponding
    to the version of the Vertex AI platform services), the Python version, the machine
    type used for serving, and the serving settings.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型元数据**：对于每个模型，我们可以记录元数据，例如模型的描述、输入和输出模式、标签（用于分类的有用信息）和指标（用于比较模型的有用信息）。对于每个版本，我们可以记录额外的元数据，例如描述、运行时版本（对应于
    Vertex AI 平台服务的版本）、Python 版本、用于服务的机器类型以及服务设置。'
- en: '**Model artifacts**: These are the artifacts that are used to produce the model.
    They can be stored in Google Cloud Storage and linked to the model in the registry.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型工件**：这些是用于生成模型的工件。它们可以存储在 Google Cloud Storage 中，并链接到注册表中的模型。'
- en: '**Access control**: We can control who can view, edit, and deploy models in
    the registry through Google Cloud’s **Identity and Access Management** (**IAM**)
    system.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制**：我们可以通过 Google Cloud 的 **身份和访问管理**（**IAM**）系统来控制谁可以查看、编辑和部署注册表中的模型。'
- en: It’s also important to understand that the Model Registry is well-integrated
    with other Vertex AI components. For example, we can use the Vertex AI training
    service to train a model and then automatically upload the trained model to the
    registry, after which we can deploy the model to the Google Cloud Vertex AI prediction
    service, which we’ll describe next. We can compare model versions against each
    other and easily change which versions are deployed to production. We can also
    automate all of those steps using Vertex AI Pipelines, something we’ll explore
    in the next chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解模型注册与其他 Vertex AI 组件的良好集成。例如，我们可以使用 Vertex AI 训练服务来训练一个模型，然后自动将训练好的模型上传到注册表中，之后我们可以将模型部署到
    Google Cloud Vertex AI 预测服务，我们将在下一节中描述。我们可以比较模型版本，并轻松更改部署到生产中的版本。我们还可以使用 Vertex
    AI Pipelines 自动化所有这些步骤，我们将在下一章中探讨这一点。
- en: Vertex AI prediction service
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI 预测服务
- en: The Google Cloud Vertex AI prediction service is an offering within the Vertex
    AI ecosystem that makes it easy for us to host ML models and serve them to our
    clients, thus supporting both batch and online model-serving use cases. It’s a
    managed service, so when we use it to host our models, we don’t need to worry
    about managing the servers and infrastructure required to do so; the service will
    automatically scale the required infrastructure and computing resources up and
    down based on the amount of traffic being sent to our models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI 预测服务是 Vertex AI 生态系统中的一个服务，它使我们能够轻松托管机器学习模型并向我们的客户提供服务，从而支持批量模型服务和在线模型服务用例。这是一个托管服务，因此当我们用它来托管我们的模型时，我们不需要担心管理所需的服务器和基础设施；该服务将根据发送到我们模型的流量量自动扩展所需的基础设施和计算资源。
- en: As we mentioned in the previous section, it integrates with Vertex AI Model
    Registry so that we can easily control which versions of our models are deployed
    to production, and it also integrates with many other Google Cloud services, such
    as Vertex AI Pipelines, which allows us to automate the development and deployment
    of our models, and Google Cloud Operations Suite, which provides integrated logging
    and monitoring functionality.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一节中提到的，它集成了 Vertex AI 模型注册，使我们能够轻松控制哪些版本的模型被部署到生产中，并且它还集成了许多其他 Google Cloud
    服务，例如 Vertex AI Pipelines，它允许我们自动化模型的开发和部署，以及 Google Cloud Operations Suite，它提供集成的日志记录和监控功能。
- en: 'We will perform hands-on activities in which we will use the Vertex AI Model
    Registry and the Vertex AI prediction service to store and serve models in Google
    Cloud shortly, but first, let’s cover one more concept that’s important in the
    context of model deployment and management: **A/B testing**.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Google Cloud 中使用 Vertex AI 模型注册和 Vertex AI 预测服务进行实际操作，以存储和提供模型，但首先，让我们再覆盖一个在模型部署和管理方面非常重要的概念：**A/B
    测试**。
- en: A/B testing
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: A/B 测试
- en: A/B testing is the practice of testing one model (model A) against another model
    (model B) to see which one performs better. While the term could technically apply
    to testing and comparing any models, the usual scenario is to test a new version
    of a model to improve model performance concerning the business objective.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试是通过测试一个模型（模型A）与另一个模型（模型B）的性能来比较哪个模型表现更好。虽然这个术语在技术上可以应用于测试和比较任何模型，但通常的场景是测试模型的新版本，以改善模型在业务目标方面的性能。
- en: 'Vertex AI allows us to deploy more than one model to a single endpoint, as
    well as control the amount of traffic that is served by each model by using the
    `traffic_split` variable, as shown in *Figure 10**.3*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 允许我们将多个模型部署到单个端点，并通过使用 `traffic_split` 变量来控制每个模型所服务的流量量，如图 *图10.3*
    所示：
- en: '![Figure 10.3: A/B configuration using Vertex AI’s traffic_split](img/B18143_10_3.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3：使用 Vertex AI 的 traffic_split 进行 A/B 配置](img/B18143_10_3.jpg)'
- en: 'Figure 10.3: A/B configuration using Vertex AI’s traffic_split'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：使用 Vertex AI 的 traffic_split 进行 A/B 配置
- en: As you will see in the practical exercises in this chapter, if we don’t set
    any value for the `traffic_split` variable, the default behavior is to keep all
    traffic directed to the original model that was already deployed to our endpoint.
    This is a safety mechanism that prevents unexpected behavior in terms of how our
    models serve traffic from our clients. The `traffic_split` configuration gives
    us very granular control over how much traffic we want to send to each deployed
    model or model version. For example, we could set the `traffic_split` configuration
    so that it suddenly starts sending all traffic to our new model by allocating
    100% of the traffic to that model, which would effectively perform an in-place
    replacement of our model. However, we may want to test our new model version with
    a small subset of our production traffic before completely replacing our prior
    model version, which equates to the idea of **canary testing** in software development.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在本章的实际练习中将会看到的，如果我们不对 `traffic_split` 变量设置任何值，默认行为是将所有流量都导向已经部署到我们端点的原始模型。这是一个安全机制，可以防止我们的模型在服务客户流量方面出现意外行为。`traffic_split`
    配置使我们能够非常细致地控制我们希望发送到每个已部署模型或模型版本的流量量。例如，我们可以设置 `traffic_split` 配置，使其突然将所有流量发送到我们的新模型，通过将100%的流量分配给该模型，从而有效地替换我们的模型。然而，我们可能希望在完全替换先前的模型版本之前，用我们生产流量的一小部分来测试我们的新模型版本，这在软件开发中相当于**金丝雀测试**的概念。
- en: When we determine that our new model is behaving as intended, we can gradually
    (or suddenly, depending on the business requirements) change the `traffic_split`
    variable to send more (or all) traffic to the new model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们确定我们的新模型表现符合预期时，我们可以逐渐（或根据业务需求突然）更改 `traffic_split` 变量，将更多（或全部）流量发送到新模型。
- en: Now that we’ve covered many of the important concepts related to hosting and
    serving models in production, let’s see how this works in the real world by deploying
    a model using Vertex AI.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了与在生产环境中托管和提供模型相关的许多重要概念，让我们通过使用 Vertex AI 部署一个模型来看看这在现实世界中是如何工作的。
- en: We’ve prepared a Vertex AI Workbench notebook that will walk you through all
    of the steps required to do this. Again, we can use the same Vertex AI Workbench
    managed notebook instance that we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left-hand side of the screen, navigate to the `Chapter-10` directory
    and open the `deployment-prediction.ipynb` notebook. You can choose TensorFlow
    2 (Local) as the kernel. Again, you can run each cell in the notebook by selecting
    the cell and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant
    code, the notebook contains markdown text that describes what the code is doing.
    I recommend only executing the model training and deployment sections, and A/B
    testing, and then reading through some more of the topics in this chapter before
    proceeding to the other activities in the notebook.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备了一个 Vertex AI Workbench 笔记本，它会指导你完成所有必要的步骤。再次强调，我们可以使用我们在 [*第5章*](B18143_05.xhtml#_idTextAnchor168)
    中创建的相同的 Vertex AI Workbench 管理笔记本实例来完成这个任务。请在笔记本实例上打开 JupyterLab。在屏幕左侧的目录浏览器中，导航到
    `Chapter-10` 目录并打开 `deployment-prediction.ipynb` 笔记本。你可以选择 TensorFlow 2 (Local)
    作为内核。同样，你可以通过选择单元格并按键盘上的 *Shift* + *Enter* 来运行笔记本中的每个单元格。除了相关的代码外，笔记本还包含描述代码正在做什么的
    Markdown 文本。我建议只执行模型训练和部署部分，以及 A/B 测试，然后在继续进行笔记本中的其他活动之前，阅读本章的一些更多主题。
- en: We must also enable a feature named `prediction-request-response-logging` in
    the notebook, which will log our models’ responses for the prediction requests
    that are received. We can save those responses in a Google Cloud BigQuery table,
    which enables us to perform analysis on the prediction responses from each of
    our models and see how they are performing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须在笔记本中启用一个名为 `prediction-request-response-logging` 的功能，该功能将记录我们模型对收到的预测请求的响应。我们可以将这些响应保存到
    Google Cloud BigQuery 表中，这样我们就可以对每个模型的预测响应进行分析，并查看它们的性能。
- en: Once you have completed the model training and deployment sections of the notebook
    (or if you’d like to just keep reading for now), you can move on to the next section,
    where we will discuss the kinds of challenges that companies usually run into
    when deploying, serving, and managing models in production.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成了笔记本中的模型训练和部署部分（或者如果你现在只想继续阅读），你就可以进入下一部分，我们将讨论公司在生产中部署、提供和管理模型时通常会遇到哪些挑战。
- en: Common challenges of serving models in production
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产中部署模型时的常见挑战
- en: Deploying and hosting ML models in production often comes with numerous challenges.
    If you’re developing and serving just one model, you may encounter some of these
    challenges, but if you are developing tens, hundreds, or thousands of models,
    then you will likely run into the majority of these challenges and concerns.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中部署和托管机器学习模型通常伴随着许多挑战。如果你只是开发和提供单个模型，你可能会遇到一些这些挑战，但如果你正在开发和提供数十、数百或数千个模型，那么你很可能会遇到这些挑战和担忧的大多数。
- en: Deployment infrastructure
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署基础设施
- en: Choosing the right infrastructure to host ML models, setting it up, and managing
    it can be complex, particularly in hybrid or multi-cloud environments. Again,
    Google Cloud Vertex AI takes care of all of this for us automatically, but without
    such cloud offerings, many companies find this to be perhaps one of the most challenging
    aspects of any data science project.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的托管机器学习模型的基础设施、设置和管理它可能很复杂，尤其是在混合或多云环境中。再次强调，Google Cloud Vertex AI 会自动为我们处理所有这些，但没有这样的云服务，许多公司发现这可能是有数据科学项目中最具挑战性的方面之一。
- en: Model availability and scaling in production
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型在生产和扩展中的可用性
- en: This is an extension of deployment infrastructure management. As demand increases,
    our model needs to serve more predictions. The ability to scale services up and
    down based on demand is crucial and can be difficult to manage manually. The Vertex
    AI Autoscaling feature enables us to do this easily. All we have to do is specify
    the minimum and maximum number of machines that we want to run for each model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是部署基础设施管理的扩展。随着需求的增加，我们的模型需要提供更多的预测。根据需求进行服务扩展和缩减的能力至关重要，并且可能很难手动管理。Vertex
    AI 自动扩展功能使我们能够轻松地做到这一点。我们只需指定我们希望为每个模型运行的机器的最小和最大数量即可。
- en: For example, if we know that we always need at least three nodes running to
    handle our regularly expected traffic, but we sometimes get increased levels of
    traffic up to double the normal amount, we could specify that we want Vertex AI
    to always run at least three machines, and to automatically scale up to a maximum
    of six machines when needed.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们知道我们始终需要至少运行三个节点来处理我们通常预期的流量，但有时流量会增加到正常水平的两倍，我们可以指定我们希望 Vertex AI 总是运行至少三台机器，并在需要时自动扩展到最多六台机器。
- en: We can specify our autoscaling preferences by configuring the `minReplicaCount`
    and `maxReplicaCount` variables, which are elements of the machine specification
    for our deployed model. We’ve provided steps on how to do this in the Jupyter
    Notebook that is associated with this chapter. Note that we can specify these
    details per model, not just per endpoint. This enables us to scale each of our
    models independently, which gives us flexibility, depending on our needs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过配置 `minReplicaCount` 和 `maxReplicaCount` 变量来指定我们的自动扩展偏好，这些变量是我们部署的模型机器规格的元素。我们已经在与本章相关的
    Jupyter Notebook 中提供了如何操作的步骤。请注意，我们可以按模型指定这些细节，而不仅仅是按端点。这使我们能够独立扩展我们的每个模型，这为我们提供了灵活性，取决于我们的需求。
- en: Vertex AI also gives us the flexibility to either deploy multiple models to
    the same endpoint or to create separate, dedicated endpoints for each model. If
    we have completely different types of models for different use cases, then we
    would typically deploy those models to separate endpoints. At this point, you
    might be wondering why we would ever want to deploy multiple models to a single
    endpoint. The most common reason for doing this is when we want to implement A/B
    testing. Again, we’ve covered the steps for implementing A/B testing use cases
    in the Jupyter Notebook that accompanies this chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 也为我们提供了灵活性，可以选择将多个模型部署到同一个端点，或者为每个模型创建独立的、专门的端点。如果我们针对不同的用例有完全不同类型的模型，那么我们通常会把这些模型部署到独立的端点。在这个时候，你可能想知道为什么我们想要将多个模型部署到单个端点。这样做最常见的原因是我们想要实施
    A/B 测试。同样，我们已经在伴随本章的 Jupyter Notebook 中介绍了实施 A/B 测试用例的步骤。
- en: Data quality
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: Often, the data that’s used in a production environment may contain errors or
    may not be as clean as the data used for model training, which could lead to inaccurate
    predictions. As a result, we may need to implement data processing steps in production
    that prepare and clean up the data “on-the-fly” at prediction time. In general,
    any data transformation techniques that we may have applied when preparing the
    data for training also need to be applied at inference time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在生产环境中使用的数据可能包含错误，或者可能没有用于模型训练的数据那么干净，这可能导致预测不准确。因此，我们可能需要在生产中实施数据预处理步骤，在预测时“实时”准备和清理数据。一般来说，我们在准备训练数据时可能应用的任何数据转换技术也需要在推理时应用。
- en: Model/data/concept drift
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型/数据/概念漂移
- en: 'In addition to preparing and cleaning up our data, if you think back to some
    of the data exploration activities we performed in previous chapters of this book,
    you may remember that one important aspect of our data is the statistical distribution
    of the variables in our dataset. Examples include the mean value of each variable
    in the dataset, the observed range between the minimum and maximum values of each
    variable, or the kinds of values of each variable, such as discreet or continuous.
    Collectively, we can refer to these characteristics as the “shape” of our data.
    *Figure 10**.4* shows some examples of different types of data distributions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准备和清理我们的数据之外，如果你回想一下本书前几章中我们执行的一些数据探索活动，你可能记得我们数据的一个重要方面是数据集中变量的统计分布。例如，包括数据集中每个变量的平均值、每个变量的最小值和最大值之间的观察范围，或者每个变量的值类型，如离散或连续。总的来说，我们可以将这些特征称为我们数据的“形状”。*图
    10.4* 展示了一些不同类型的数据分布的示例：
- en: '![Figure 10.4: Data distributions  (source: https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)](img/B18143_10_4.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4：数据分布  (来源：https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)](img/B18143_10_4.jpg)'
- en: 'Figure 10.4: Data distributions (source: https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：数据分布 (来源：https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)
- en: In an ideal world, the shape of the data that is used to train our model should
    be the same as the shape of the data that the model is expected to encounter in
    production. For this reason, we usually want to use real-world data (that is,
    data that was previously observed in production) to train our models. However,
    over time, the shape of the data that’s observed in production might begin to
    deviate from the shape of the data that was used to train the model. This can
    happen suddenly, such as when a global pandemic drastically changes the purchasing
    behavior of consumers within a short period (for example, everybody is suddenly
    purchasing toilet paper and hand sanitizer, rather than fashion and cosmetic products),
    or seasonally, or it could occur more gradually due to natural evolutionary changes
    in a given business domain or market.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想的世界里，用于训练我们模型的 数据形状 应该与模型在生产中预期遇到的数据形状相同。因此，我们通常希望使用现实世界的数据（即，在生产中之前观察到的数据）来训练我们的模型。然而，随着时间的推移，生产中观察到的数据形状可能会开始偏离用于训练模型的数据形状。这种情况可能突然发生，例如，当全球大流行在短时间内急剧改变消费者的购买行为（例如，每个人都突然购买卫生纸和洗手液，而不是时尚和化妆品），或者季节性变化，或者它可能由于特定业务领域或市场的自然进化变化而逐渐发生。
- en: In any case, such a deviation is generally referred to as “drift,” or more specifically
    **model drift**, **data drift**, or **concept drift**. Model drift refers to the
    overall degradation of the model’s objective performance over time, and it can
    be caused by factors such as data drift or concept drift. Data drift is when the
    statistical distribution of the data in production differs from the statistical
    distribution of the data that was used to train our model. On the other hand,
    concept drift is when the relationships change between the input features and
    the target variable that a model is trying to predict. This means that even if
    the input data stays the same, the underlying relationships between the variables
    may change over time, which can affect the accuracy of our model. The example
    of consumer behavior changing during a pandemic is an instance of concept drift.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，这种偏差通常被称为“漂移”，或者更具体地说，是**模型漂移**、**数据漂移**或**概念漂移**。模型漂移指的是模型目标性能随时间整体下降，它可能由数据漂移或概念漂移等因素引起。数据漂移是指生产中数据的统计分布与用于训练我们模型的数据的统计分布不同。另一方面，概念漂移是指模型试图预测的输入特征与目标变量之间的关系发生变化。这意味着即使输入数据保持不变，变量之间的潜在关系也可能随时间变化，这可能会影响我们模型的准确性。大流行期间消费者行为变化的例子是概念漂移的一个实例。
- en: Regardless of the type of drift, it can lead to degrading model performance.
    As such, we need to constantly evaluate the performance of our models to ensure
    they consistently meet our business requirements. If we find that the performance
    of our models is degrading, we need to assess whether it is due to drift, and
    if we detect that it is, we need to take corrective measures accordingly. We will
    discuss such measures in more detail later.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 无论漂移的类型如何，它都可能导致模型性能下降。因此，我们需要持续评估我们模型的性能，以确保它们始终满足我们的业务需求。如果我们发现我们模型的性能正在下降，我们需要评估这是否是由于漂移造成的，如果我们检测到它是，我们需要相应地采取纠正措施。我们将在稍后更详细地讨论这些措施。
- en: Security and privacy
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全和隐私
- en: Ensuring that the data used by our model complies with security and privacy
    regulations can be complex, particularly in industries dealing with sensitive
    data. Depending on the industry and use case, this can be considered one of the
    most important aspects of model development and management.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 确保我们模型使用的数据符合安全和隐私法规可能很复杂，尤其是在处理敏感数据的行业中。根据行业和用例，这可以被认为是模型开发和管理的最重要方面之一。
- en: Model interpretability
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型可解释性
- en: Often, models are treated as “black box” implementations, in which case we don’t
    have much visibility into how the model is operating internally, making it difficult
    to understand how they’re making predictions. This can cause issues, especially
    in regulated industries where explanations are required. We will explore this
    topic in a lot more detail later in this book.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型被视为“黑盒”实现，在这种情况下，我们无法深入了解模型内部的工作方式，这使得理解它们是如何进行预测的变得困难。这可能会引起问题，尤其是在需要解释的监管行业中。我们将在本书的稍后部分更详细地探讨这个话题。
- en: Tracking ML model metadata
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪机器学习模型元数据
- en: As we mentioned earlier, ML model development usually involves some experimentation,
    in which data scientists evaluate different versions of their datasets, as well
    as different algorithms, hyperparameter values, and other components of the development
    process. Also, the development process often involves collaboration among multiple
    team members or multiple separate teams. Considering that some companies develop
    and deploy hundreds or even thousands of ML models, it’s important to track all
    of these experiments and development iterations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，机器学习模型开发通常涉及一些实验，数据科学家会评估他们数据集的不同版本，以及不同的算法、超参数值和其他开发过程组件。此外，开发过程通常涉及多个团队成员或多个独立团队之间的协作。考虑到一些公司开发和部署了数百甚至数千个机器学习模型，跟踪所有这些实验和开发迭代是很重要的。
- en: For example, if we have a model deployed in production, and we observe that
    the model is making inaccurate or inappropriate predictions, then we need to understand
    why that model is behaving as it is. To do that, we need to know all of the steps
    that were performed to create that specific model version, as well as all of the
    inputs and outputs associated with the development of that model, such as the
    version of the input dataset that was used to train the model, as well as the
    algorithm and hyperparameter values used during the training process. We refer
    to this information as the metadata of the model. Without this, it would be very
    difficult to understand why our model behaves the way it does.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个在生产中部署的模型，并且观察到该模型正在做出不准确或不适当的预测，那么我们需要了解为什么该模型会这样表现。为了做到这一点，我们需要知道创建该特定模型版本所执行的所有步骤，以及与该模型开发相关的所有输入和输出，例如用于训练模型的输入数据集版本，以及训练过程中使用的算法和超参数值。我们将这些信息称为模型的元数据。没有这些信息，很难理解我们的模型为什么会这样表现。
- en: This concept is somewhat linked to the topic of model interpretability but also
    includes additional aspects of ML model development.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念与模型可解释性的主题有些关联，但也包括机器学习模型开发的其他方面。
- en: Integration with existing systems
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与现有系统的集成
- en: Most companies have various software systems that were developed or procured
    over many years, and integrating the ML model into these systems can be complex.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司都有各种软件系统，这些系统是在多年的时间里开发和采购的，将机器学习模型集成到这些系统中可能很复杂。
- en: Monitoring
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: Setting up robust monitoring for the performance of ML models in production
    can be a challenge if you manage the infrastructure yourself. We need to constantly
    monitor the model’s predictive performance to ensure it is still valid and hasn’t
    degraded over time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你自己管理基础设施，为生产中的机器学习模型设置稳健的监控可能是一个挑战。我们需要持续监控模型的预测性能，以确保它仍然有效，并且随着时间的推移没有退化。
- en: This is such an important aspect of model management that we will dedicate the
    next section of this chapter specifically to this topic.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对模型管理的一个重要方面，因此我们将在本章的下一节专门讨论这个主题。
- en: Monitoring models in production
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控生产中的模型
- en: Our work isn’t over once we’ve developed and deployed a model – we need to track
    the model’s performance and its overall health over time and make adjustments
    if we observe that the model performance deteriorates. In this section, we’ll
    discuss some of the characteristics of our models that we typically need to monitor.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开发和部署模型后的工作还没有结束——我们需要随着时间的推移跟踪模型的表现和整体健康状况，并在观察到模型性能下降时进行调整。在本节中，我们将讨论我们通常需要监控的一些模型特征。
- en: Objective model performance
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标模型性能
- en: 'Not surprisingly, the most prominent aspect of our model that we need to monitor
    is how it performs concerning the objective it was created to achieve. We discussed
    objective metrics in previous chapters of this book, such as **Mean Squared Error**
    (**MSE**), Accuracy, F1 score, and AUC-ROC, among others. An example of AUC-ROC
    is depicted in *Figure 10**.5* for reference:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，我们需要监控的我们模型最突出的方面是它在其创建目标方面的表现。我们在本书的前几章讨论了目标指标，例如**均方误差**（**MSE**）、准确率、F1分数和AUC-ROC等。AUC-ROC的一个例子在*图10.5*中展示，供参考：
- en: '![Figure 10.5: AUC-ROC](img/B18143_10_5.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5：AUC-ROC](img/B18143_10_5.jpg)'
- en: 'Figure 10.5: AUC-ROC'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：AUC-ROC
- en: Objective metrics tell us how our model is performing in terms of the main purpose
    our model is intended to serve, such as predicting housing prices or identifying
    cats in photographs. If we notice a sustained degradation in these metrics beyond
    a certain threshold that we deem acceptable to the needs of the business, then
    we usually need to take corrective action.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目标指标告诉我们我们的模型在实现其主要目的方面表现如何，例如预测房价或识别照片中的猫。如果我们注意到这些指标在某个我们认为对业务需求可接受的阈值以上持续下降，那么我们通常需要采取纠正措施。
- en: A common cause of degradation of model performance is data drift, which we discussed
    in the previous section. If we find that data drift has occurred, a corrective
    action is often to retrain our model with updated data that matches the shape
    or distribution of the data that is currently being observed by the model in production.
    This may require gathering fresh data from our production systems or other appropriate
    sources.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能下降的常见原因是数据漂移，我们已在上一节中讨论过。如果我们发现发生了数据漂移，纠正措施通常是使用与模型在生产中当前观察到的数据形状或分布相匹配的更新数据重新训练我们的模型。这可能需要从我们的生产系统或其他适当的来源收集新鲜数据。
- en: Monitoring specifically for data drift
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专门监测数据漂移
- en: Monitoring for data drift involves comparing the statistical properties of the
    incoming data with those of the training data. In this case, we analyze each feature
    or variable in the data, and we compare the distribution of the incoming data
    with the distribution of the training data. In addition to comparing simple descriptive
    statistics such as the mean, median, mode, and standard deviation, there are also
    some specific statistical tests we can evaluate, such as the Kullback-Leibler
    divergence, the Kolmogorov-Smirnov test, or the chi-squared test. We will discuss
    these mechanisms in more detail later in this book.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 监测数据漂移涉及比较传入数据的统计属性与训练数据的统计属性。在这种情况下，我们分析数据中的每个特征或变量，并将传入数据的分布与训练数据的分布进行比较。除了比较简单的描述性统计量，如均值、中位数、众数和标准差之外，还有一些特定的统计检验我们可以评估，例如Kullback-Leibler散度、Kolmogorov-Smirnov检验或卡方检验。我们将在本书的后面部分更详细地讨论这些机制。
- en: Vertex AI Model Monitoring provides automated tests that can detect different
    varieties of drift. Specifically, it can check for **feature skew and drift**
    and **attribution skew and drift**, both of which we’ll describe next. It also
    provides model explanation functionality, which we’ll explore in great detail
    later in this book.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI模型监控提供了自动测试，可以检测不同类型的漂移。具体来说，它可以检查**特征偏斜和漂移**以及**归因偏斜和漂移**，这两者我们将在下文中进行描述。它还提供了模型解释功能，我们将在本书的后面部分详细探讨。
- en: Skew is also called **training-serving skew**, and it refers to a scenario in
    which the distribution of feature data in production differs from the distribution
    of feature data that was used to train the model. To detect this type of skew,
    we generally need to have access to the original training data since Vertex AI
    Model Monitoring will compare the distribution of the training data against what
    is seen in the inference requests that are sent to our model in production.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 偏斜也被称为**训练-服务偏斜**，它指的是生产环境中特征数据的分布与用于训练模型的特征数据分布不同的情况。为了检测这种偏斜，我们通常需要访问原始训练数据，因为Vertex
    AI模型监控会将训练数据的分布与发送到我们生产中模型的推理请求中看到的分布进行比较。
- en: '**Prediction drift** refers to the scenario in which the feature data in production
    changes over time. In the absence of access to the original training data, we
    can still turn on drift detection to check the input data for changes over time.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测漂移**指的是生产中的特征数据随时间变化的情况。在没有访问原始训练数据的情况下，我们仍然可以开启漂移检测来检查输入数据随时间的变化。'
- en: It’s important to note that some amount of drift and skew are likely tolerable,
    but we generally need to determine and configure thresholds beyond which we need
    to take corrective actions. These thresholds depend on the needs of the business
    for our particular use case.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，一些程度的漂移和偏斜可能是可容忍的，但通常我们需要确定和配置阈值，超过这些阈值我们就需要采取纠正措施。这些阈值取决于我们特定用例的业务需求。
- en: Anomalous model behavior
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常模型行为
- en: Anomalies could be spikes in prediction requests, unusual values of input features,
    or unusual prediction response values. For example, if a model that is built for
    fraud detection starts flagging an unusually high number of transactions as fraudulent,
    it may be an anomaly and warrant further investigation. Anomalies may also occur
    due to data drift, or due to temporary changes in the environment.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 异常可能包括预测请求的峰值、输入特征的异常值或异常的预测响应值。例如，如果一个用于欺诈检测的模型开始标记异常高数量的交易为欺诈，这可能是一个异常，需要进一步调查。异常也可能由于数据漂移或环境中的临时变化而发生。
- en: Resource utilization
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源利用率
- en: This includes monitoring aspects such as CPU usage, memory usage, network I/O,
    and others to ensure that the model’s serving infrastructure is operating within
    its capacity. These metrics are important indicators for determining when to scale
    resources up or down based on factors such as the amount of traffic currently
    being sent to our models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括监控CPU使用率、内存使用率、网络I/O等方面，以确保模型的服务基础设施在其容量内运行。这些指标是确定何时根据当前发送到我们模型的流量量等因素增加或减少资源的重要指标。
- en: Model bias and fairness
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型偏差和公平性
- en: We need to ensure that our models are making fair predictions on an ongoing
    basis. This can involve tracking fairness metrics and checking for bias in our
    models’ predictions. We will explore this topic in much more detail in the next
    chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保我们的模型持续进行公平的预测。这可能包括跟踪公平性指标和检查模型预测中的偏差。我们将在下一章中更详细地探讨这个话题。
- en: These are some of the main aspects of our models that we need to monitor. The
    exact items that need to be prioritized depend on our business case.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们需要监控的模型的主要方面。具体需要优先考虑的项目取决于我们的业务案例。
- en: Now that we’ve talked about many of the most common topics in the space of model
    monitoring, let’s discuss what to do if we detect that our model’s performance
    is degrading.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了模型监控领域中最常见的许多话题，让我们讨论一下如果检测到我们的模型性能下降，我们应该怎么做。
- en: Addressing model performance degradation
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决模型性能下降问题
- en: If we find that the values of the metrics we’re monitoring for our models are
    showing a decline in performance, there are some actions we can take to correct
    the situation, and possibly prevent such a scenario from occurring in the future.
    This section discusses some relevant corrective actions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发现我们监控的模型指标值显示出性能下降，我们可以采取一些措施来纠正这种情况，并可能防止未来发生此类情况。本节讨论了一些相关的纠正措施。
- en: Perhaps the most effective way to ensure that our models stay up to date and
    are performing optimally is to implement a robust MLOps pipeline. This includes
    **continuous integration and continuous deployment** (**CI/CD**) for ML models,
    and a major component of this is to regularly train our models on new data that
    is seen in production – this is referred to as **continuous training**. For this
    purpose, we need to implement a mechanism for capturing data in production, and
    then automatically update our models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 确保我们的模型保持最新并表现最佳的最有效方法可能是实施一个健壮的MLOps管道。这包括机器学习模型的**持续集成和持续部署**（**CI/CD**），其主要组成部分是定期使用生产中看到的新数据训练我们的模型——这被称为**持续训练**。为此，我们需要实施一个机制来捕获生产中的数据，然后自动更新我们的模型。
- en: We could either update our models periodically (for example, every night or
    every month), or we could trigger retraining to occur based on outputs from a
    Vertex AI Model Monitoring job. For example, if a Model Monitoring job detects
    that drift has occurred beyond an acceptable threshold, an automated notification
    could be sent, and it could automatically kick off a pipeline to train and deploy
    a new version of our model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定期更新我们的模型（例如，每晚或每月一次），或者根据Vertex AI模型监控作业的输出触发重新训练。例如，如果模型监控作业检测到漂移超出了可接受的阈值，可以发送自动通知，并自动启动一个管道来训练和部署我们模型的新版本。
- en: The next chapter is dedicated entirely to the topic of MLOps, so we will dive
    into these concepts in much more detail there.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将完全致力于MLOps的话题，因此我们将更详细地探讨这些概念。
- en: In the meantime, if you’d like to switch from theory to practical learning,
    now would be a good time to execute the *Model Monitoring* section of the Vertex
    AI Workbench notebook that accompanies this chapter. Otherwise, let’s continue
    with the rest of the topics in this chapter, which include optimizing for AI/ML
    use cases at the edge.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果您想从理论转向实践学习，现在执行本章附带的Vertex AI Workbench笔记本中的*模型监控*部分将是一个好时机。否则，让我们继续本章的其余部分，其中包括在边缘优化AI/ML用例。
- en: Optimizing for AI/ML at the edge
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在边缘优化AI/ML
- en: Serving ML models at the edge refers to running your models directly on user
    devices such as smartphones or IoT devices. The term “edge” is based on traditional
    network architecture terminology, in which the core of the network is in the network
    owner’s data centers, and the edge of the network is where user devices connect
    to the network. Running models and other types of systems at the edge can provide
    benefits such as lower latency, increased privacy, and reduced server costs. However,
    edge devices usually have limited computing power, so we may need to make some
    changes to our models for them to run efficiently on those devices. There are
    several things we can do to optimize our models to run at the edge, all of which
    we will discuss in this section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘提供ML模型指的是直接在用户设备上运行您的模型，例如智能手机或物联网设备。术语“边缘”基于传统的网络架构术语，其中网络的核心位于网络所有者的数据中心，而网络的边缘是用户设备连接到网络的地方。在边缘运行模型和其他类型的系统可以提供诸如降低延迟、增加隐私和减少服务器成本等好处。然而，边缘设备通常计算能力有限，因此我们可能需要对模型进行一些调整，以便它们在这些设备上高效运行。我们可以做几件事情来优化我们的模型，以便它们在边缘运行，所有这些内容我们将在本节中讨论。
- en: Model optimization
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型优化
- en: Let’s start by discussing what kinds of measures we can take to optimize our
    models so that they can be used at the edge.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先讨论一下我们可以采取哪些措施来优化我们的模型，以便它们可以在边缘使用。
- en: Model selection
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型选择
- en: First, we should try to choose lightweight models that can still provide good
    performance on our objectives. For example, decision trees and linear models often
    require less memory and computational power than deep learning models. Of course,
    our model selection process also depends on our business needs. Sometimes, we
    will need larger models to achieve the required objective. As such, this recommendation
    for optimizing AI/ML workloads at the edge is simply a first-step guideline. We
    will discuss more advanced strategies next.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该尝试选择轻量级的模型，这些模型仍然能在我们的目标上提供良好的性能。例如，决策树和线性模型通常比深度学习模型需要的内存和计算能力更少。当然，我们的模型选择过程也取决于我们的业务需求。有时，我们需要更大的模型来实现所需的目标。因此，这个关于在边缘优化AI/ML工作负载的建议只是一个初步的指导方针。我们将在下一节讨论更高级的策略。
- en: Model pruning
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型剪枝
- en: 'Pruning is a technique for reducing the size of a model by removing parameters
    (for example, “weight pruning”) or whole neurons (for example, “neuron pruning”)
    that contribute the least to the model’s performance. An example of neuron pruning
    is depicted in *Figure 10**.6*, in which a neuron has been removed from each hidden
    layer (as represented by the red X covering each of the removed neurons):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝是一种通过移除对模型性能贡献最小的参数（例如，“权重剪枝”）或整个神经元（例如，“神经元剪枝”）来减小模型尺寸的技术。图10.6中展示了神经元剪枝的一个例子，其中每个隐藏层都移除了一个神经元（如图中覆盖每个被移除神经元的红色X所示）：
- en: '![Figure 10.6: Neuron pruning](img/B18143_10_6.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6：神经元剪枝](img/B18143_10_6.jpg)'
- en: 'Figure 10.6: Neuron pruning'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：神经元剪枝
- en: The resulting pruned model requires less memory and computational resources.
    If we remove too many weights or neurons, then it could affect the accuracy of
    our model, so the idea, of course, is to find a balance that reduces the computational
    resources required, with minimal impact on accuracy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 结果剪枝后的模型需要更少的内存和计算资源。如果我们移除太多的权重或神经元，那么可能会影响我们模型的准确性，因此，当然，我们的想法是找到一个平衡点，以最小化对准确性的影响，同时减少所需的计算资源。
- en: Model quantization
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型量化
- en: Quantization is a way of reducing the numerical precision of the model’s weights.
    For example, weights might be stored as 32-bit floating-point numbers during training,
    but they can often be quantized as 8-bit integers for inference without a significant
    reduction in performance. This reduces the memory requirements and computational
    cost of the model. This is particularly useful in the context of **Large Language
    Models** (**LLMs**), which can have hundreds of billions of weights. We will cover
    this in detail in the *Generative AI* section of this book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种降低模型权重数值精度的方法。例如，权重在训练期间可能以32位浮点数存储，但在推理时通常可以量化为8位整数，而不会显著降低性能。这减少了模型的内存需求和计算成本。这在**大型语言模型**（**LLMs**）的背景下尤其有用，这些模型可能拥有数百亿个权重。我们将在本书的“生成式AI”部分详细讨论这一点。
- en: Knowledge distillation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: This technique involves training a smaller model, sometimes referred to as a
    “student” model, to mimic the behavior of a larger, “teacher” model, or ensemble
    of models. The smaller model is trained to produce outputs as similar as possible
    to those of the larger model so that it can perform a somewhat similar task, with
    perhaps a tolerable reduction in model accuracy. Again, we need to find a balance
    in the trade-off between model size reduction and reduction in accuracy. Distillation
    is also particularly useful in the context of LLMs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术涉及训练一个较小的模型，有时被称为“学生”模型，以模仿一个较大、被称为“教师”模型或模型集合的行为。较小的模型被训练以产生尽可能接近较大模型的输出，以便它能执行类似的任务，或许在模型精度上有所可接受的降低。再次强调，我们需要在模型尺寸减小和精度降低之间找到平衡。在LLMs的背景下，蒸馏技术尤其有用。
- en: Making use of efficient model architectures
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用高效模型架构
- en: Some model architectures are designed to be efficient on edge devices. For example,
    MobileNet and EfficientNet are efficient variants of **convolutional neural networks**
    (**CNNs**) that are suitable for mobile devices.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型架构被设计为在边缘设备上高效。例如，MobileNet和EfficientNet是**卷积神经网络**（**CNNs**）的高效变体，适合移动设备。
- en: Now that we’ve discussed some changes that we can make to our models to optimize
    them for edge use cases, let’s take a look at what other kinds of mechanisms we
    can use for this purpose.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了一些我们可以对模型进行的修改，以优化它们在边缘用例上的性能，让我们看看我们还可以使用哪些其他类型的机制来实现这一目的。
- en: Optimization beyond model techniques
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型技术之外的优化
- en: All of the previous optimization techniques involved making changes to our models
    to make them run more efficiently on edge devices. There are also additional measures
    we can take, such as converting trained models into other formats that are optimized
    for edge devices or optimizing the hardware that runs our models at the edge.
    Let’s discuss these mechanisms in a bit more detail.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的先前优化技术都涉及对我们的模型进行修改，使其在边缘设备上运行得更高效。我们还可以采取其他措施，例如将训练好的模型转换为针对边缘设备优化的其他格式，或者优化在边缘运行我们的模型的硬件。让我们更详细地讨论这些机制。
- en: Hardware-specific optimization
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件特定优化
- en: Depending on the specific hardware on the edge device (for example, CPU, GPU,
    **tensor processing unit** (**TPU**), and so on), different optimization strategies
    can be used. For example, some libraries provide tools for optimizing computation
    graphs based on specific hardware.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据边缘设备上的特定硬件（例如，CPU、GPU、**张量处理单元**（**TPU**）等），可以使用不同的优化策略。例如，一些库提供了基于特定硬件优化计算图的工具。
- en: Specialized libraries
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专用库
- en: Libraries such as TensorFlow Lite and the **Open Neural Network Exchange** (**ONNX**)
    Runtime can convert models into a format optimized for edge devices, further reducing
    the memory footprint and increasing the speed of models. We’ll discuss these libraries
    in more detail in this section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如TensorFlow Lite和**开放神经网络交换**（**ONNX**）运行时之类的库可以将模型转换为针对边缘设备优化的格式，进一步减少模型的内存占用并提高其速度。我们将在本节中更详细地讨论这些库。
- en: TensorFlow Lite
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorFlow Lite
- en: TensorFlow Lite is a set of tools provided by TensorFlow to help us run models
    on mobile, embedded, and IoT devices. It does this by converting our TensorFlow
    models into a more efficient format for use on such edge devices. It also includes
    tools for optimizing the model’s size and performance, as well as for implementing
    hardware acceleration. We’ve used TensorFlow Lite to convert our model; this can
    be found in the Jupyter Notebook that accompanies this chapter.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Lite 是 TensorFlow 提供的一套工具，帮助我们运行移动、嵌入式和物联网设备上的模型。它是通过将我们的 TensorFlow
    模型转换为更高效的格式，以便在边缘设备上使用来实现的。它还包括优化模型大小和性能的工具，以及实现硬件加速的工具。我们已经使用 TensorFlow Lite
    转换我们的模型；这可以在本章所附的 Jupyter Notebook 中找到。
- en: Edge TPU Compiler
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Edge TPU Compiler
- en: Google’s Edge TPU Compiler is a tool for compiling models to run on Google’s
    Edge TPUs, which are designed for running TensorFlow Lite models on edge devices.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的 Edge TPU Compiler 是一种用于将模型编译到 Google 的 Edge TPUs 上运行的工具，这些 Edge TPUs
    是为在边缘设备上运行 TensorFlow Lite 模型而设计的。
- en: ONNX Runtime
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: ONNX is an open format for representing ML models that enables models to be
    transferred between various ML frameworks. It also provides a cross-platform inference
    engine called the ONNX Runtime, which includes support for various kinds of hardware
    accelerators and is designed to provide fast inference and lower the resource
    requirements of ML models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 是一种用于表示机器学习模型的开放格式，它使得模型能够在各种机器学习框架之间进行迁移。它还提供了一个跨平台的推理引擎，称为 ONNX Runtime，它包括对各种硬件加速器的支持，旨在提供快速的推理并降低机器学习模型对资源的需求。
- en: TensorRT
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorRT
- en: TensorRT is a deep learning model optimizer and runtime library developed by
    NVIDIA for deploying neural network models on GPUs, particularly on NVIDIA’s embedded
    platform called Jetson.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT 是 NVIDIA 开发的一种深度学习模型优化器和运行时库，用于在 GPU 上部署神经网络模型，尤其是在 NVIDIA 的嵌入式平台 Jetson
    上。
- en: TVM
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TVM
- en: Apache TVM is an open source ML compiler stack that aims to enable efficient
    deployment of ML models on a variety of hardware platforms. TVM supports model
    inputs from various deep learning frameworks, including TensorFlow, Keras, PyTorch,
    ONNX, and others.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Apache TVM 是一个开源的机器学习编译器堆栈，旨在使机器学习模型在各种硬件平台上高效部署成为可能。TVM 支持从各种深度学习框架中获取模型输入，包括
    TensorFlow、Keras、PyTorch、ONNX 以及其他框架。
- en: These are just some of the tools that exist for optimizing ML models so that
    they can run at the edge. With the constant proliferation of new types of technological
    devices, edge optimization is an active area of research, and new tools and mechanisms
    continue to be developed.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是优化机器学习模型以便在边缘运行的一些工具。随着新型技术设备的不断涌现，边缘优化是一个活跃的研究领域，新的工具和机制仍在不断开发中。
- en: 'In our hands-on activities in the Jupyter Notebook that accompanies this chapter,
    we used TensorFlow Lite to optimize our model, after which we stored the optimized
    model in Google Cloud Storage. From there, we can easily deploy our model to any
    device that supports the TensorFlow Lite interpreter. A list of supported platforms
    is provided in the TensorFlow Lite documentation, which also contains a lot of
    additional useful information on how TensorFlow Lite works in greater detail:
    [https://www.tensorflow.org/lite/guide/inference#supported_platforms](https://www.tensorflow.org/lite/guide/inference#supported_platforms).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章所附的 Jupyter Notebook 的动手活动中，我们使用了 TensorFlow Lite 来优化我们的模型，之后我们将优化后的模型存储在
    Google Cloud Storage 中。从那里，我们可以轻松地将我们的模型部署到任何支持 TensorFlow Lite 解释器的设备。TensorFlow
    Lite 文档中提供了一份支持平台列表，其中还包含大量关于 TensorFlow Lite 如何在更详细层面工作的有用信息：[https://www.tensorflow.org/lite/guide/inference#supported_platforms](https://www.tensorflow.org/lite/guide/inference#supported_platforms)。
- en: At this point, we’ve covered a lot of different topics related to model deployment.
    Let’s take some time to recap what we’ve learned.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了与模型部署相关的许多不同主题。让我们花些时间回顾一下我们学到了什么。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed various Google Cloud services for hosting ML models,
    such as Vertex AI, Cloud Functions, GKE, and Cloud Run. We differentiated between
    online and offline model serving, whereby online serving is used for real-time
    predictions, and offline serving is used for batch predictions. Then, we explored
    common challenges in deploying ML models, such as data/model drift, scaling, monitoring,
    performance, and keeping our models up to date. We also introduced specific components
    of Vertex AI that make it easier for us to deploy and manage models, such as the
    Vertex AI Model Registry, the Vertex AI prediction service, and Vertex AI Model
    Monitoring.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了各种用于托管机器学习模型的 Google Cloud 服务，例如 Vertex AI、Cloud Functions、GKE 和 Cloud
    Run。我们区分了在线和离线模型服务，其中在线服务用于实时预测，而离线服务用于批量预测。然后，我们探讨了部署机器学习模型时常见的挑战，例如数据/模型漂移、扩展、监控、性能以及保持模型更新。我们还介绍了
    Vertex AI 的特定组件，这些组件使我们的模型部署和管理更加容易，例如 Vertex AI 模型注册表、Vertex AI 预测服务和 Vertex
    AI 模型监控。
- en: Specifically, we dived quite deep into monitoring models in production, focusing
    on data drift and model drift. We discussed mechanisms to combat these drifts,
    such as automated continuous training.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们深入探讨了生产环境中监控模型，重点关注数据漂移和模型漂移。我们讨论了对抗这些漂移的机制，例如自动连续训练。
- en: Next, we explained A/B testing for comparing two versions of a model, and we
    discussed optimizing ML models for edge deployment using methods such as model
    pruning and quantization, as well as libraries and tools for optimizing our models,
    such as TensorFlow Lite.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们解释了 A/B 测试，用于比较模型的两个版本，并讨论了使用模型剪枝和量化等方法优化边缘部署的机器学习模型，以及用于优化我们的模型的库和工具，例如
    TensorFlow Lite。
- en: At this point, we have covered all the major steps in the MDLC. Our next topic
    of focus will be how to automate the entire life cycle using MLOps. Join us in
    the next chapter, where you’ll continue your journey of becoming an expert AI/ML
    solutions architect, in which you have already come a very long way and are making
    great progress!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了 MDLC 中的所有主要步骤。我们接下来关注的主题将是如何使用 MLOps 自动化整个生命周期。请加入我们，在下一章中，你将继续你的旅程，成为一名专家
    AI/ML 解决方案架构师，在这个过程中你已经走了很长的路，并且取得了巨大的进步！
