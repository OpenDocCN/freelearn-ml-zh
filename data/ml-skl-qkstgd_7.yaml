- en: Clustering Data with Unsupervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无监督机器学习进行数据聚类
- en: Most of the data that you will encounter out in the wild will not come with
    labels. It is impossible to apply supervised machine learning techniques when
    your data does not come with labels. Unsupervised machine learning addresses this
    issue by grouping data into clusters; we can then assign labels based on those
    clusters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您在实际应用中遇到的大部分数据都不会带有标签。如果数据没有标签，您无法应用有监督的机器学习技术。无监督机器学习通过将数据分组为聚类来解决此问题；然后我们可以基于这些聚类分配标签。
- en: Once the data has been clustered into a specific number of groups, we can proceed
    to give those groups labels. Unsupervised machine learning is the first step that
    you, as the data scientist, will have to implement, before you can apply supervised
    machine learning techniques (such as classification) to make meaningful predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被聚类成特定数量的组，我们就可以继续为这些组分配标签。无监督学习是您作为数据科学家需要实施的第一步，之后才能应用有监督的机器学习技术（如分类）进行有意义的预测。
- en: A common application of the unsupervised machine learning algorithm is customer
    data, which can be found across a wide range of industries. As a data scientist,
    your job is to find groups of customers that you can segment and deliver targeted
    products and advertisements to.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督机器学习算法的一个常见应用是客户数据，这些数据可以在各行各业中找到。作为数据科学家，您的工作是找到可以细分的客户群体，并向其推送有针对性的产品和广告。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下主题：
- en: The k-means algorithm and how it works internally, in order to cluster unlabeled
    data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 算法及其内部工作原理，用于对无标签数据进行聚类
- en: Implementing the k-means algorithm in scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现 k-means 算法
- en: Using feature engineering to optimize unsupervised machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征工程优化无监督机器学习
- en: Cluster visualization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类可视化
- en: Going from unsupervised to supervised machine learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从无监督学习到有监督学习
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, NumPy ≥ 1.15.1, Matplotlib ≥ 3.0.0, Pydotplus ≥ 2.0.2, Image ≥ 3.1.2,
    Seaborn ≥ 0.9.0, and SciPy ≥ 1.1.0 installed on your system.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在系统中安装 Python 3.6 或更高版本，Pandas ≥ 0.23.4，Scikit-learn ≥ 0.20.0，NumPy ≥ 1.15.1，Matplotlib
    ≥ 3.0.0，Pydotplus ≥ 2.0.2，Image ≥ 3.1.2，Seaborn ≥ 0.9.0 和 SciPy ≥ 1.1.0。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb)[.](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb)[.](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_07.ipynb)'
- en: 'Check out the following video to see the code in action:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请观看以下视频，查看代码的实际应用：
- en: '[http://bit.ly/2qeEJpI](http://bit.ly/2qeEJpI)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2qeEJpI](http://bit.ly/2qeEJpI)'
- en: The k-means algorithm
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 算法
- en: In this section, you will learn about how the k-means algorithm works under
    the hood, in order to cluster data into groups that make logical sense.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习 k-means 算法的工作原理，以便将数据聚类成有逻辑意义的组。
- en: 'Let''s consider a set of points, as illustrated in the following diagram:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一组点，如下图所示：
- en: '![](img/9d2492a0-3cb0-4ab3-8685-d523c395d4dd.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d2492a0-3cb0-4ab3-8685-d523c395d4dd.png)'
- en: A random set of points
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一组随机点
- en: Assignment of centroids
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质心分配
- en: 'The first step that the algorithm takes is to assign a set of random centroids.
    Assuming that we want to find two distinct clusters or groups, the algorithm can
    assign two centroids, as shown in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的第一步是分配一组随机质心。假设我们要找到两个不同的聚类或组，算法可以分配两个质心，如下图所示：
- en: '![](img/a4f79e48-a120-4fdb-a6c9-ca18b6d3d209.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f79e48-a120-4fdb-a6c9-ca18b6d3d209.png)'
- en: Centroids, represented by stars
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由星号表示的质心
- en: In the preceding diagram, the stars represent the centroids of the algorithm.
    Note that in this case, the clusters' centers perfectly fit the two distinct groups.
    This is the most ideal case. In reality, the means (or centroids) are assigned
    randomly, and, with every iteration, the cluster centroids move closer to the
    center of the two groups.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，星号代表算法的质心。请注意，在这种情况下，聚类的中心完美地符合两个不同的组。这是最理想的情况。实际上，均值（或质心）是随机分配的，并且在每次迭代中，聚类的质心都会向两个组的中心靠近。
- en: The algorithm is known as the k-means algorithm, as we try to find the mean
    of a group of points as the centroid. Since the mean can only be computed for
    a set of numeric points, such clustering algorithms only work with numerical data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法被称为k-means算法，因为我们试图找到一组点的均值作为质心。由于均值只能针对一组数值点计算，因此这种聚类算法只能处理数值数据。
- en: 'In reality, the process of grouping these points into two distinct clusters
    is not this straightforward. A visual representation of the process can be illustrated
    as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，将这些点分组为两个不同的聚类并不像看起来那么简单。该过程的可视化表示可以如下所示：
- en: '![](img/97902875-2328-417d-89d6-f0a4c32d7342.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97902875-2328-417d-89d6-f0a4c32d7342.png)'
- en: The process of assigning centroids in the k-means algorithm
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法中分配质心的过程
- en: In the preceding diagram, the process of assigning the random centroids begins
    in the upper-left corner. As we go down and toward the upper-right corner, note
    how the centroids move closer to the center of the two distinct groups. In reality,
    the algorithm does not have an optimal endpoint at which it stops the iteration.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，随机分配质心的过程从左上角开始。随着我们向下并朝右上角移动，请注意质心如何逐渐靠近两个不同组的中心。实际上，算法没有一个最佳的终止点来停止迭代。
- en: When does the algorithm stop iterating?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法什么时候停止迭代？
- en: 'Typically, the algorithm looks for two metrics, in order to stop the iteration
    process:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，算法会寻找两个度量标准，以停止迭代过程：
- en: The distance between the distinct groups (or clusters) that are formed
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形成的不同组（或聚类）之间的距离
- en: The distance between each point and the centroid of a cluster
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个点与聚类质心之间的距离
- en: The optimal case of cluster formation is when the distance between the distinct
    groups or clusters are as large as possible, while the distances between each
    point and the centroid of a cluster are as small as possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类形成的最佳情况是，当不同组或聚类之间的距离尽可能大，而每个点与聚类质心之间的距离尽可能小。
- en: Implementing the k-means algorithm in scikit-learn
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现k-means算法
- en: Now that you understand how the k-means algorithm works internally, we can proceed
    to implement it in scikit-learn. We are going to work with the same fraud detection
    dataset that we used in all of the previous chapters. The key difference is that
    we are going to drop the target feature, which contains the labels, and identify
    the two clusters that are used to detect fraud.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了k-means算法的内部工作原理，我们可以继续在scikit-learn中实现它。我们将使用在之前章节中使用的相同的欺诈检测数据集。关键的区别是，我们将丢弃包含标签的目标特征，并识别用于检测欺诈的两个聚类。
- en: Creating the base k-means model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建基础k-means模型
- en: 'In order to load the dataset into our workspace and drop the target feature
    with the labels, we use the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据集加载到工作空间并丢弃包含标签的目标特征，我们使用以下代码：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we can implement the k-means algorithm with two cluster means. The choice
    of using two cluster means is arbitrary in nature, since we know that there should
    be two distinct clusters as a result of two labels: fraud and not fraud transactions.
    We can do this by using the following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以实现具有两个聚类均值的k-means算法。选择使用两个聚类均值是任意的，因为我们知道应该有两个不同的聚类，分别对应两个标签：欺诈和非欺诈交易。我们可以通过以下代码来实现：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the preceding code, first, we import the `KMeans` package from scikit-learn
    and initialize a model with two clusters. We then fit this model to the data by
    using the `.fit()`function. This results in a set of labels as the output. We
    can extract the labels by using the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，首先，我们从scikit-learn中导入`KMeans`包并初始化一个具有两个聚类的模型。然后，我们使用`.fit()`函数将该模型拟合到数据上。这将产生一组标签作为输出。我们可以使用以下代码提取这些标签：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output produced by the preceding code is an array of labels for each mobile
    transaction, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成的输出是每个移动交易的标签数组，如下所示：
- en: '![](img/9d6d2097-7765-4adb-8135-6402afc75127.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d6d2097-7765-4adb-8135-6402afc75127.png)'
- en: Array of labels
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数组
- en: Now that we have a set of labels, we know which cluster each transaction falls
    into. Mobile transactions that have a label of `0` fall into one group, while
    transactions that have a label of `1` fall into the second group.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一组标签，我们知道每个交易属于哪个聚类。标签为 `0` 的移动交易属于一组，而标签为 `1` 的交易属于第二组。
- en: The optimal number of clusters
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳的聚类数量
- en: While explaining how the k-means algorithm works, we mentioned how the algorithm
    terminates once it finds the optimal number of clusters. When picking clusters
    arbitrarily using scikit-learn, this is not always the case. We need to find the
    optimal number of clusters, in this case.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释 k-means 算法如何工作的过程中，我们提到过，当算法找到最佳的聚类数量时，它会终止。然而，当使用 scikit-learn 随机选择聚类时，这种情况并不总是成立。在这种情况下，我们需要找到最佳的聚类数量。
- en: One way that we can do this is by a measure known as **inertia.** Inertia measures
    how close the data points in a cluster are to its centroid. Obviously, a lower
    inertia signifies that the groups or clusters are tightly packed, which is good.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一种被称为**惯性**的度量来实现这一点。惯性度量的是聚类中数据点与其质心的接近程度。显然，较低的惯性意味着组或聚类紧密地聚集在一起，这样是比较好的。
- en: 'In order to compute the inertia for the model, we use the following code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算模型的惯性值，我们使用以下代码：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code produced an inertia value of *4.99 × 10 ^ 17*, which is extremely
    large with respect to the other values of inertia produced by different numbers
    of clusters (explained as follows), and is not a good value of inertia. This suggests
    that the individual data points are spread out, and are not tightly packed together.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生了一个惯性值为 *4.99 × 10 ^ 17*，这个值相对于其他不同聚类数所产生的惯性值来说极大（后面会解释），因此不是一个好的惯性值。这表明个别数据点分布较广，并没有紧密地聚集在一起。
- en: 'In most cases, we do not really know what the optimal numbers of clusters are,
    so we need to plot the inertia scores for different numbers of clusters. We can
    do this by using the following code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们并不确切知道最佳的聚类数量，因此我们需要为不同的聚类数绘制惯性得分。我们可以通过以下代码来实现：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following plot:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![](img/c094e7ab-092b-4c7f-8b34-2e0508722eaf.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c094e7ab-092b-4c7f-8b34-2e0508722eaf.png)'
- en: Inertia as a function of the number of clusters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性作为聚类数的函数
- en: In the preceding code, first, we create a list of clusters that have values
    from 1 to 10\. Each value denotes the number of clusters that will be used in
    the machine learning model. Next, we create an empty list that will store all
    of the inertia values that each model will produce.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，首先，我们创建了一个包含 1 到 10 值的聚类列表。每个值表示将用于机器学习模型中的聚类数量。接着，我们创建了一个空列表，用来存储每个模型所产生的惯性值。
- en: Next, we loop over the list of clusters and build and evaluate a k-means model
    for each cluster value in the list. Each model now produces an inertia, which
    is stored in the list that we initialized at the start of the code block. A simple
    line plot is then constructed by using the list of clusters along the xaxis and
    the corresponding inertia values along the yaxis, using `matplotlib`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历聚类列表，并为列表中每个聚类值构建并评估一个 k-means 模型。每个模型现在会产生一个惯性值，该值会存储在我们在代码块开始时初始化的列表中。然后，使用
    `matplotlib` 绘制一个简单的折线图，x 轴为聚类数，y 轴为相应的惯性值。
- en: The plot tells us that the inertia values are the lowest when the number of
    clusters is equal to 10\. However, having a large number of clusters is also something
    that we must aim at avoiding, as having too many groups does not help us to generalize
    well, and the characteristics about each group become very specific.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表告诉我们，当聚类数为 10 时，惯性值最低。然而，拥有过多的聚类也是我们需要避免的事情，因为过多的组别并不能帮助我们很好地进行泛化，而且每个组别的特征会变得非常具体。
- en: Therefore, the ideal way to choose the best number of clusters for a problem,
    given that we do not have prior information about the number of groups that we
    want beforehand, is to identify the **elbow point** of the plot.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择问题的最佳聚类数量的理想方式是，假设我们事先没有关于我们想要的组数的先验信息，找到图中的**肘部点**。
- en: 'The elbow point is the point at which the rate of decrease in inertia values
    slows down. The elbow point is illustrated in the following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部点是惯性值减少速率减缓的那个点。肘部点在下图中得到了说明：
- en: '![](img/fa54e78e-83a7-4135-80b2-98fa7b8ebe95.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa54e78e-83a7-4135-80b2-98fa7b8ebe95.png)'
- en: Elbow point of the graph
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的肘部点
- en: In the preceding plot, it is clear that the elbow point corresponds to four
    clusters. This could mean that there are four distinct types of fraudulent transactions,
    apart from the standard categorizations of fraud and not fraud. However, since
    we know beforehand that the dataset has a binary target feature with two categories,
    we will dig too deeply into why four is the ideal number of groups/clusters for
    this dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，很明显肘部点对应于四个聚类。这可能意味着除了标准的“欺诈”和“非欺诈”分类外，存在四种不同类型的欺诈交易。然而，由于我们事先知道数据集有一个二元目标特征，包含两个类别，我们不会深入探讨为什么四个聚类是该数据集的理想聚类数。
- en: Feature engineering for optimization
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化的特征工程
- en: 'Engineering the features in your dataset is a concept that is fundamentally
    used to improve the performance of your model. Fine-tuning the features to the
    algorithm''s design is beneficial, because it can lead to an improvement in accuracy,
    while reducing the generalization errors at the same time. The different kinds
    of feature engineering techniques for optimizing your dataset that you will learn
    are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的特征工程是一个基本概念，用于提高模型的性能。将特征调整到算法设计的最佳状态是有益的，因为它可以提高准确性，同时减少泛化误差。你将学习到的几种用于优化数据集的特征工程技术如下：
- en: Scaling
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放
- en: Principal component analysis
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Scaling
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放
- en: 'Scaling is the process of standardizing your data so that the values under
    every feature fall within a certain range, such as -1 to +1\. In order to scale
    the data, we subtract each value of a particular feature with the mean of that
    feature, and divide it by the variance of that feature. In order to scale the
    features in our fraud detection dataset, we use the following code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放是标准化数据的过程，使每个特征下的值落在某个特定范围内，如-1到+1\. 为了缩放数据，我们用某一特征的每个值减去该特征的均值，再除以该特征的方差。为了缩放我们欺诈检测数据集中的特征，我们使用以下代码：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding code, we use the `StandardScalar()`function to scale our dataframe,
    and then we build a k-means model with two clusters on the scaled data. After
    evaluating the inertia of the model, the value output is 295,000, which is substantially
    better than the value of *4.99 × 10^(17)*, produced by the model without scaling.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`StandardScalar()`函数来缩放我们的数据框，然后我们在缩放后的数据上构建了一个包含两个聚类的k-means模型。评估模型的惯性后，输出的值为295,000，明显优于没有缩放时模型输出的*4.99
    × 10^(17)*。
- en: 'We can then create a new plot of the number of clusters versus the inertia
    values, using the same code that we did earlier, with the only difference being
    replacing the original dataframe with the scaled dataframe:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用与之前相同的代码绘制聚类数与惯性值的图表，唯一的不同是将原始数据框替换为缩放后的数据框：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This produces the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/7945b2c1-962a-4a74-a2f6-0d301b06041c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7945b2c1-962a-4a74-a2f6-0d301b06041c.png)'
- en: Optimal number of clusters, post-scaling
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放后的聚类最佳数量
- en: We notice that the preceding plot does not have a very clear elbow point, where
    the rate of decrease in the inertia values is lower. However, if we look closely,
    we can find this point at 8 clusters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，前面的图表并没有一个非常明显的肘部点，在这个点上惯性值的下降速率较低。然而，如果我们仔细观察，可以在8个聚类处找到这个点。
- en: Principal component analysis
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: The **principal component analysis** (**PCA**) is a subset of dimensionality
    reduction. **Dimensionality reduction** is the process of reducing the number
    of features that provide no predictive value to a predictive model. We also optimize
    and improve the computational efficiency of processing the algorithms. This is
    because a dataset with a smaller number of features will make it easier for the
    algorithm to detect patterns more quickly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是降维的一个子集。**降维**是指减少对预测模型没有预测价值的特征的过程。我们还优化并提高了算法处理的计算效率。这是因为一个特征较少的数据集会让算法更容易更快地检测到模式。'
- en: 'The first step in PCA is called **decorrelation**. Features that are highly
    correlated with each other provide no value to the predictive model. Therefore,
    in the decorrelation step, the PCA takes two highly correlated features and spreads
    their data points such that it''s aligned across the axis, and is not correlated
    anymore. This process can be illustrated as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的第一步叫做**去相关化**。相互高度相关的特征对预测模型没有价值。因此，在去相关化步骤中，PCA将两个高度相关的特征的数据点展开，使其在轴上对齐，并且不再相关。这个过程可以如下图所示：
- en: '![](img/2ca670b5-80b4-42eb-9604-968e02b5a01e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ca670b5-80b4-42eb-9604-968e02b5a01e.png)'
- en: The process of decorrelation
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 去相关化过程
- en: Once the features are decorrelated, the principal components (or features) are
    extracted from the data. These features are the ones that have high variance,
    and, in turn, provide the most value to a predictive model. The features with
    low variance are discarded, and thus, the number of dimensions in the dataset
    is reduced.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦特征被去相关化，主成分（或特征）就会从数据中提取出来。这些特征具有较高的方差，并且提供了对预测模型最有价值的信息。方差较低的特征会被丢弃，因此数据集的维度数量减少。
- en: 'In order to perform dimensionality reduction using PCA, we use the following
    code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用PCA进行降维，我们使用以下代码：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, first, we import the `PCA`method from scikit-learn. Next,
    we initialize a PCAmodel with five components. Here, we are specifying that we
    want the PCA to reduce the dataset to only the five most important features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，首先，我们从scikit-learn导入`PCA`方法。接下来，我们初始化一个具有五个主成分的PCA模型。在这里，我们指定要将数据集减少到仅包含五个最重要的特征。
- en: 'We then fit the PCA model to the dataframe and transform it, in order to obtain
    the decorrelated features. Checking the shape of the final array of features,
    we can see that it only has five features. Finally, we create a new k-means model
    with only the principal component features, as shown in the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将PCA模型拟合到数据框并进行转换，以获得去相关的特征。检查最终特征数组的形状，我们可以看到它只有五个特征。最后，我们使用仅包含主成分特征的k-means模型，如下所示的代码所示：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Evaluating the inertia of the new model improved its performance. We obtained
    a lower value of inertia than in the case of the scaled model. Now, let''s evaluate
    the inertia scores for different numbers of principal components or features.
    In order to this, we use the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 评估新模型的惯性改善了其性能。我们得到了比缩放模型更低的惯性值。现在，让我们评估不同主成分或特征数量的惯性分数。为此，我们使用以下代码：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding code, the following applies:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，适用以下内容：
- en: First, we initialize a list to store the different principal component values
    that we want to build our models with. These values are from 1 to 10.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个列表，用于存储我们想用来构建模型的不同主成分值。这些值从1到10。
- en: Next, we initialize an empty list, in order to store the inertia values from
    each and every model.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个空的列表，用于存储每个模型的惯性值。
- en: Using each principal component value, we build a new k-means model and append
    the inertia value for that model into the empty list.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用每个主成分值，我们构建一个新的k-means模型，并将该模型的惯性值附加到空列表中。
- en: Finally, a plot is constructed between the inertia values and the different
    values of components.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制惯性值与不同主成分值之间的关系图。
- en: 'This plot is illustrated as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像如下所示：
- en: '![](img/eef35bb7-907b-437c-8a0a-5a282fe3ea82.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eef35bb7-907b-437c-8a0a-5a282fe3ea82.png)'
- en: Inertia values versus the numbers of principal components
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性值与主成分数量的关系
- en: In the preceding plot, it is clear that the inertia value is lowest for one
    component.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，可以清楚地看到，惯性值在一个主成分时最小。
- en: Cluster visualization
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类可视化
- en: 'Visualizing how your clusters are formed is no easy task when the number of
    variables/dimensions in your dataset is very large. There are two main methods
    that you can use in order to visualize how the clusters are distributed, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集中的变量/维度非常多时，直观展示你的聚类是一个不容易的任务。有两种主要方法可以用来可视化聚类的分布，如下所示：
- en: '**t-SNE**: This creates a map of the dataset in two-dimensional space'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t-SNE**：在二维空间中创建数据集的地图'
- en: '**Hierarchical clustering**: This uses a tree-based visualization, known as
    a **dendrogram**, in order to create hierarchies'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：使用基于树的可视化方式，称为**树状图**，来创建层次结构'
- en: In this section, you will learn how to implement these visualization techniques,
    in order to create compelling cluster visuals.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何实现这些可视化技术，以创建引人注目的集群可视化效果。
- en: t-SNE
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-SNE
- en: The **t-SNE** is an abbreviation that stands for **t-distributed stochastic
    neighbor embedding**. The fundamental concept behind the t-SNE is to map a higher
    dimension to a two-dimensional space. In simple terms, if your dataset has more
    than two features, the t-SNE does a great job at showing you how your entire dataset
    can be visualized on your computer screen!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-SNE** 是 **t-分布随机邻域嵌入** 的缩写。t-SNE 的基本概念是将高维度映射到二维空间。简单来说，如果您的数据集具有超过两个特征，t-SNE
    将非常适合显示您的整个数据集如何在计算机屏幕上可视化！'
- en: 'The first step is to implement the k-means algorithm and create a set of prediction
    labels that we can merge into the unlabeled dataset. We can do this by using the
    following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是实现 k-means 算法，并创建一组我们可以合并到未标记数据集中的预测标签。我们可以通过使用以下代码来实现这一点：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Don't worry about how the preceding segment of code works for the moment, as
    this will be explained in detail in a later section within this chapter, when
    we deal with converting an unsupervised machine learning problem into a supervised
    learning one.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时不要担心前面代码段的工作原理，因为在本章的后续部分中，我们将详细解释如何将无监督机器学习问题转换为监督学习问题。
- en: 'Next, we will create a t-SNE object and fit that into our array of data points
    that consists of only the features. We will then transform the features at the
    same time so that we can view all the features on a two-dimensional space. This
    is done in the following code segment:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 t-SNE 对象，并将其拟合到我们仅包含特征的数据点数组中。然后，我们同时转换这些特征，以便可以在二维空间中查看所有特征。这在以下代码段中完成：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding code, the following applies:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，以下内容适用：
- en: First, we initialize the t-SNE object by using the `TSNE()`function.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过使用 `TSNE()` 函数初始化 t-SNE 对象。
- en: Using the t-SNE object, we fit and transform the data in our features, using
    the `fit_transform()`method.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 t-SNE 对象，我们使用 `fit_transform()` 方法对我们的特征数据进行拟合和转换。
- en: 'Next, we create the t-SNE visualization by using the following code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码创建 t-SNE 可视化：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code, the following applies:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，以下内容适用：
- en: We extract the first and second features from the set of transformed features
    for the x axis and *y* axis, respectively.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从转换特征集中提取第一和第二个特征，分别作为 x 轴和 *y* 轴。
- en: 'We then plot a scatter plot and color it by the target labels, which were generated
    earlier, using the k-means algorithm. This generates the following plot:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们绘制散点图，并根据先前使用 k-means 算法生成的目标标签对其进行着色。这生成以下图表：
- en: '![](img/3e1e748d-08c5-4dce-9d61-07117f8f82af.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e1e748d-08c5-4dce-9d61-07117f8f82af.png)'
- en: t-SNE visualization
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 可视化
- en: In the preceding plot, the yellow color represents the transactions that have
    been assigned the fraud label, while the purple color represents the transactions
    that have been assigned the non-fraudulent label. (Please refer to the color version
    of the image.)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图中，黄色表示被分配欺诈标签的交易，而紫色表示被分配非欺诈标签的交易。（请参考图像的彩色版本。）
- en: Hierarchical clustering
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: As discussed initially, the hierarchical clustering technique uses the dendrogram
    to visualize clusters or groups. In order to explain how the dendrogram works,
    we will consider a dataset with four features.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如最初讨论的那样，层次聚类技术使用树状图来可视化集群或群组。为了解释树状图的工作原理，我们将考虑一个具有四个特征的数据集。
- en: Step 1 – Individual features as individual clusters
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步 – 将每个特征作为单独的集群
- en: 'In the first step, each feature in the dataset is considered to be its own
    cluster. This is illustrated in the following diagram:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，数据集中的每个特征被认为是其自己的集群。这在以下图表中有所说明：
- en: '![](img/5d56db01-84a4-4320-b24a-d491ae54b4f7.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d56db01-84a4-4320-b24a-d491ae54b4f7.png)'
- en: Each feature as a single cluster in the dendrogram
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征作为树状图中的单个集群
- en: Each feature in the preceding diagram is one single cluster, at this point in
    time. The algorithm now searches to find the two features that are closest to
    each other, and merges them into a single cluster.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 每个前面图表中的特征都是一个单独的集群，现阶段如此。此算法现在搜索找到彼此最接近的两个特征，并将它们合并成一个单独的集群。
- en: Step 2 – The merge
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步 – 合并
- en: 'In this step, the algorithm merges the data points in the two closest features
    together, into one single cluster. This is illustrated in the following diagram:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，算法将两个最接近的特征中的数据点合并到一个聚类中。这个过程在下面的图示中有所体现：
- en: '![](img/e0cffef7-2d65-4b61-b6ef-e0dd0683c8a7.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0cffef7-2d65-4b61-b6ef-e0dd0683c8a7.png)'
- en: The process in which features merge into a single cluster
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 特征合并为单一聚类的过程
- en: In the preceding diagram, it is clear that the algorithm has now chosen **Feature
    2** and **Feature 3**, and has decided that the data under these two features
    are the closest to each other.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，很明显算法已经选择了**特征2**和**特征3**，并决定这两个特征下的数据是彼此最接近的。
- en: Step 3 – Iteration
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步 – 迭代
- en: 'The algorithm now continues the process of merging features together iteratively,
    until no more clusters can be formed. The final dendrogram that is formed is as
    follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，算法继续迭代合并特征，直到无法再形成任何聚类。最终形成的树状图如下所示：
- en: In the preceding diagram, **Feature 2** and **Feature 3** were grouped into
    a single cluster. The algorithm then decided that **Feature 1** and the cluster
    of **Feature 2** and **3** were closest to each other. Therefore, these three
    features were clustered into one group. Finally, **Feature 4** was grouped together
    with **Feature 3**.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，**特征2**和**特征3**被归为一个单一的聚类。然后，算法决定**特征1**和**特征2**与**特征3**的聚类是最接近的。因此，这三个特征被归为一个组。最后，**特征4**与**特征3**聚为一组。
- en: Implementing hierarchical clustering
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现层次聚类
- en: 'Now that you have learned how hierarchical clustering works, we can implement
    this concept. In order to create a hierarchical cluster, we use the following
    code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学习了层次聚类是如何工作的，我们可以实现这个概念。为了创建一个层次聚类，我们使用以下代码：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code results in a dendrogram, as illustrated in the following
    diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将生成一个树状图，如下面的图示所示：
- en: '![](img/ab00c657-6b5d-4608-b2d6-a64f8123ed7f.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab00c657-6b5d-4608-b2d6-a64f8123ed7f.png)'
- en: Dendrogram
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图
- en: 'In the preceding code, the following applies:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，以下内容适用：
- en: First, we create an array with four columns.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个包含四列的数组。
- en: We then use the `linkage` function to create the clusters. Within the function,
    we specify the `method` argument as complete, in order to indicate that we want
    the entire dendrogram.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用`linkage`函数创建聚类。在函数中，我们将`method`参数设置为complete，以表示我们想要整个树状图。
- en: Finally, we use the `dendrogram`function to create the dendrogram with the clusters.
    We set the label names to the list of feature names that was created earlier in
    the code.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`dendrogram`函数创建带有聚类的树状图。我们将标签名称设置为之前在代码中创建的特征名称列表。
- en: Going from unsupervised to supervised learning
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从无监督学习到监督学习
- en: The eventual goal of unsupervised learning is to take a dataset with no labels
    and assign labels to each row of the dataset, so that we can run a supervised
    learning algorithm through it. This allows us to create predictions that make
    use of the labels.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的最终目标是获取一个没有标签的数据集，并为数据集的每一行分配标签，以便我们可以通过它运行监督学习算法。这使我们能够创建利用这些标签的预测。
- en: In this section, you will learn how to convert the labels generated by the unsupervised
    machine learning algorithm into a decision tree that makes use of those labels.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何将无监督机器学习算法生成的标签转换为一个使用这些标签的决策树。
- en: Creating a labeled dataset
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建标注数据集
- en: 'The first step is to convert the labels generated by an unsupervised machine
    learning algorithm, such as the k-means algorithm, and append it to the dataset.
    We can do this by using the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将无监督机器学习算法（如k-means算法）生成的标签转换，并将其附加到数据集中。我们可以使用以下代码来实现：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code, we read in the fraud detection dataset and drop the
    target and index columns:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们读取了欺诈检测数据集，并删除了目标列和索引列：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, in the preceding code we initialize and fit a k-means model with two
    clusters:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在前面的代码中，我们初始化并拟合了一个具有两个聚类的k-means模型：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Finally, we create the target labels by using the `predict()`method, and convert
    it into a `pandas` series. We then merge this series into the dataframe, in order
    to create our labeled dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们通过使用`predict()`方法创建目标标签，并将其转换为`pandas`系列。然后我们将这个系列合并到数据框中，以便创建我们的标注数据集。
- en: Building the decision tree
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建决策树
- en: Now that we have the labeled dataset, we can create a decision tree, in order
    to convert the unsupervised machine learning problem into a supervised machine
    learning one.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了标注的数据集，我们可以创建一个决策树，将无监督学习问题转化为有监督学习问题。
- en: 'In order to do this, we start with all of the necessary package imports, as
    shown in the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们从所有必要的包导入开始，如下所示的代码所示：
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we rename the target column to a name that is appropriate (when we merged
    the target labels created by the k-means algorithm, it produced `0` as the default
    name). We can do this by using the following code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将目标列重命名为适当的名称（当我们合并由 k-means 算法创建的目标标签时，默认生成了 `0` 作为名称）。我们可以通过以下代码来实现：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we build the decision tree classification algorithm, using the following
    code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码构建决策树分类算法：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code, first, we create the features and target variables and
    initialize a decision tree classifier. We then fit the classifier onto the features
    and target.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，首先，我们创建了特征和目标变量，并初始化了一个决策树分类器。然后，我们将分类器拟合到特征和目标上。
- en: 'Finally, we want to visualize the decision tree. We can do this by using the
    following code:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望可视化决策树。我们可以通过以下代码来实现：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This results in the decision tree shown in the following diagram:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了下图所示的决策树：
- en: '![](img/d761508f-2bf5-4d69-b782-b2a616cc3b09.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d761508f-2bf5-4d69-b782-b2a616cc3b09.png)'
- en: A part of the decision tree that was created
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的决策树的一部分
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about how the k-means algorithm works, in order
    to cluster unlabeled data points into clusters or groups. You then learned how
    to implement the same using scikit-learn, and we expanded upon the feature engineering
    aspect of the implementation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了 k-means 算法的工作原理，以便将未标记的数据点聚类到不同的群组中。接着，你学会了如何使用 scikit-learn 实现这一点，并扩展了实现中的特征工程部分。
- en: Having learned how to visualize clusters using hierarchical clustering and t-SNE,
    you then learned how to map a multi-dimensional dataset into a two-dimensional
    space. Finally, you learned how to convert an unsupervised machine learning problem
    into a supervised learning one, using decision trees.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习如何使用层次聚类和 t-SNE 可视化聚类之后，你又学会了如何将多维数据集映射到二维空间。最后，你学习了如何使用决策树将无监督学习问题转化为有监督学习问题。
- en: In the next (and final) chapter, you will learn how to formally evaluate the
    performance of all of the machine learning algorithms that you have built so far!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的（也是最后一章），你将学习如何正式评估到目前为止你所构建的所有机器学习算法的性能！
