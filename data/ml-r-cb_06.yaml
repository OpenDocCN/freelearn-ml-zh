- en: Chapter 6. Classification (II) – Neural Network and SVM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. 分类（II）- 神经网络和SVM
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Classifying data with a support vector machine
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持向量机对数据进行分类
- en: Choosing the cost of a support vector machine
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择支持向量机的成本
- en: Visualizing an SVM fit
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化支持向量机拟合
- en: Predicting labels based on a model trained by a support vector machine
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于由支持向量机训练的模型预测标签
- en: Tuning a support vector machine
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整支持向量机
- en: Training a neural network with neuralnet
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用neuralnet训练神经网络
- en: Visualizing a neural network trained by neuralnet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化由neuralnet训练的神经网络
- en: Predicting labels based on a model trained by neuralnet
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于由neuralnet训练的模型预测标签
- en: Training a neural network with nnet
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用nnet训练神经网络
- en: Predicting labels based on a model trained by nnet
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于由nnet训练的模型预测标签
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Most research has shown that **support vector machines** (**SVM**) and **neural
    networks** (**NN**) are powerful classification tools, which can be applied to
    several different areas. Unlike tree-based or probabilistic-based methods that
    were mentioned in the previous chapter, the process of how support vector machines
    and neural networks transform from input to output is less clear and can be hard
    to interpret. As a result, both support vector machines and neural networks are
    referred to as black box methods.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究表明，**支持向量机**（**SVM**）和**神经网络**（**NN**）是强大的分类工具，可以应用于多个不同领域。与上一章中提到的基于树或基于概率的方法不同，支持向量机和神经网络从输入到输出的转换过程不太清晰，可能难以解释。因此，支持向量机和神经网络都被称为黑盒方法。
- en: The development of a neural network is inspired by human brain activities. As
    such, this type of network is a computational model that mimics the pattern of
    the human mind. In contrast to this, support vector machines first map input data
    into a high dimension feature space defined by the kernel function, and find the
    optimum hyperplane that separates the training data by the maximum margin. In
    short, we can think of support vector machines as a linear algorithm in a high
    dimensional space.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的发展灵感来源于人类大脑活动。因此，这类网络是一种模仿人类心智模式的计算模型。相比之下，支持向量机首先将输入数据映射到由核函数定义的高维特征空间，并找到通过最大间隔分离训练数据的最佳超平面。简而言之，我们可以将支持向量机视为高维空间中的线性算法。
- en: 'Both these methods have advantages and disadvantages in solving classification
    problems. For example, support vector machine solutions are the global optimum,
    while neural networks may suffer from multiple local optimums. Thus, choosing
    between either depends on the characteristics of the dataset source. In this chapter,
    we will illustrate the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法在解决分类问题时都有优点和缺点。例如，支持向量机解决方案是全局最优解，而神经网络可能会遭受多个局部最优解。因此，选择哪种方法取决于数据源的特征。在本章中，我们将说明以下内容：
- en: How to train a support vector machine
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练支持向量机
- en: Observing how the choice of cost can affect the SVM classifier
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察成本选择如何影响支持向量机分类器
- en: Visualizing the SVM fit
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化支持向量机拟合
- en: Predicting the labels of a testing dataset based on the model trained by SVM
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于由SVM训练的模型预测测试数据集的标签
- en: Tuning the SVM
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整SVM
- en: 'In the neural network section, we will cover:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络部分，我们将涵盖以下内容：
- en: How to train a neural network
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练神经网络
- en: How to visualize a neural network model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何可视化神经网络模型
- en: Predicting the labels of a testing dataset based on a model trained by `neuralnet`
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于由`neuralnet`训练的模型预测测试数据集的标签
- en: Finally, we will show how to train a neural network with `nnet`, and how to
    use it to predict the labels of a testing dataset
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将展示如何使用`nnet`训练神经网络，以及如何使用它来预测测试数据集的标签
- en: Classifying data with a support vector machine
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机对数据进行分类
- en: The two most well known and popular support vector machine tools are `libsvm`
    and `SVMLite`. For R users, you can find the implementation of `libsvm` in the
    `e1071` package and `SVMLite` in the `klaR` package. Therefore, you can use the
    implemented function of these two packages to train support vector machines. In
    this recipe, we will focus on using the `svm` function (the `libsvm` implemented
    version) from the `e1071` package to train a support vector machine based on the
    telecom customer churn data training dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最知名且最受欢迎的支持向量机工具是`libsvm`和`SVMLite`。对于R语言用户，你可以在`e1071`包中找到`libsvm`的实现，在`klaR`包中找到`SVMLite`。因此，你可以使用这两个包中实现的功能来训练支持向量机。在本例中，我们将重点关注使用来自`e1071`包的`svm`函数（`libsvm`实现版本）来训练基于电信客户流失数据训练数据集的支持向量机。
- en: Getting ready
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will continue to use the telecom churn dataset as the input
    data source to train the support vector machine. For those who have not prepared
    the dataset, please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for details.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将继续使用电信客户流失数据集作为输入数据源来训练支持向量机。对于那些尚未准备数据集的用户，请参阅[第5章](part0060_split_000.html#page
    "第5章。分类（I）-树、懒惰和概率性")，*分类（I）-树、懒惰和概率性*，以获取详细信息。
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to train the SVM:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来训练SVM：
- en: 'Load the `e1071` package:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`e1071`包：
- en: '[PRE0]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Train the support vector machine using the `svm` function with `trainset` as
    the input dataset, and use `churn` as the classification category:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`svm`函数并通过`trainset`作为输入数据集来训练支持向量机，并使用`churn`作为分类类别：
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, you can obtain overall information about the built model with `summary`:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用`summary`函数获取关于构建的模型的整体信息：
- en: '[PRE2]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How it works...
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The support vector machine constructs a hyperplane (or set of hyperplanes)
    that maximize the margin width between two classes in a high dimensional space.
    In these, the cases that define the hyperplane are support vectors, as shown in
    the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机构建一个超平面（或一组超平面），在多维空间中最大化两个类别之间的边缘宽度。在这些情况下，定义超平面的点是支持向量，如图所示：
- en: '![How it works...](img/00112.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00112.jpeg)'
- en: 'Figure 1: Support Vector Machine'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：支持向量机
- en: Support vector machine starts from constructing a hyperplane that maximizes
    the margin width. Then, it extends the definition to a nonlinear separable problem.
    Lastly, it maps the data to a high dimensional space where the data can be more
    easily separated with a linear boundary.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机从构建一个最大化边缘宽度的超平面开始。然后，它将定义扩展到非线性可分问题。最后，它将数据映射到一个高维空间，在那里数据可以更容易地用线性边界分离。
- en: The advantage of using SVM is that it builds a highly accurate model through
    an engineering problem-oriented kernel. Also, it makes use of the regularization
    term to avoid over-fitting. It also does not suffer from local optimal and multicollinearity.
    The main limitation of SVM is its speed and size in the training and testing time.
    Therefore, it is not suitable or efficient enough to construct classification
    models for data that is large in size. Also, since it is hard to interpret SVM,
    how does the determination of the kernel take place? Regularization is another
    problem that we need tackle.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVM的优势在于它通过一个面向工程问题的核函数构建了一个高度准确模型。此外，它利用正则化项来避免过拟合。它也不受局部最优和多重共线性影响。SVM的主要局限性在于其训练和测试过程中的速度和大小。因此，它不适合或效率不足以构建大型数据集的分类模型。此外，由于SVM难以解释，核函数的确定是如何进行的？正则化是我们需要解决的问题之一。
- en: In this recipe, we continue to use the telecom `churn` dataset as our example
    data source. We begin training a support vector machine using `libsvm` provided
    in the `e1071` package. Within the training function, `svm`, one can specify the
    `kernel` function, cost, and the `gamma` function. For the `kernel` argument,
    the default value is radial, and one can specify the kernel to a linear, polynomial,
    radial basis, and sigmoid. As for the `gamma` argument, the default value is equal
    to (1/data dimension), and it controls the shape of the separating hyperplane.
    Increasing the `gamma` argument usually increases the number of support vectors.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们继续使用电信 `churn` 数据集作为我们的示例数据源。我们开始使用 `e1071` 软件包中提供的 `libsvm` 训练支持向量机。在训练函数
    `svm` 中，可以指定 `kernel` 函数、成本和 `gamma` 函数。对于 `kernel` 参数，默认值是径向的，可以将核指定为线性、多项式、径向基和
    sigmoid。至于 `gamma` 参数，默认值等于（1/数据维度），它控制分离超平面的形状。增加 `gamma` 参数通常会增加支持向量的数量。
- en: As for the cost, the default value is set to 1, which indicates that the regularization
    term is constant, and the larger the value, the smaller the margin is. We will
    discuss more on how the cost can affect the SVM classifier in the next recipe.
    Once the support vector machine is built, the `summary` function can be used to
    obtain information, such as calls, parameters, number of classes, and the types
    of label.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关于成本，默认值设置为 1，这表示正则化项是常数，值越大，边界越小。我们将在下一菜谱中进一步讨论成本如何影响 SVM 分类器。一旦构建了支持向量机，可以使用
    `summary` 函数获取信息，例如调用次数、参数、类别数量和标签类型。
- en: See also
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Another popular support vector machine tool is `SVMLight`. Unlike the `e1071`
    package, which provides the full implementation of `libsvm`, the `klaR` package
    simply provides an interface to `SVMLight` only. To use `SVMLight`, one can perform
    the following steps:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的支持向量机工具是 `SVMLight`。与提供 `libsvm` 完整实现的 `e1071` 软件包不同，`klaR` 软件包仅提供对 `SVMLight`
    的接口。要使用 `SVMLight`，可以执行以下步骤：
- en: 'Install the `klaR` package:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `klaR` 软件包：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Download the `SVMLight` source code and binary for your platform from [http://svmlight.joachims.org/](http://svmlight.joachims.org/).
    For example, if your guest OS is Windows 64-bit, you should download the file
    from [http://download.joachims.org/svm_light/current/svm_light_windows64.zip](http://download.joachims.org/svm_light/current/svm_light_windows64.zip).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [http://svmlight.joachims.org/](http://svmlight.joachims.org/) 下载您平台上的 `SVMLight`
    源代码和二进制文件。例如，如果您的虚拟操作系统是 Windows 64 位，您应该从 [http://download.joachims.org/svm_light/current/svm_light_windows64.zip](http://download.joachims.org/svm_light/current/svm_light_windows64.zip)
    下载文件。
- en: 'Then, you should unzip the file and put the workable binary in the working
    directory; you may check your working directory by using the `getwd` function:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您应该解压文件并将可工作的二进制文件放入工作目录；您可以使用 `getwd` 函数检查您的工作目录：
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Train the support vector machine using the `svmlight` function:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `svmlight` 函数训练支持向量机：
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Choosing the cost of a support vector machine
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择支持向量机的成本
- en: The support vector machines create an optimum hyperplane that separates the
    training data by the maximum margin. However, sometimes we would like to allow
    some misclassifications while separating categories. The SVM model has a cost
    function, which controls training errors and margins. For example, a small cost
    creates a large margin (a soft margin) and allows more misclassifications. On
    the other hand, a large cost creates a narrow margin (a hard margin) and permits
    fewer misclassifications. In this recipe, we will illustrate how the large and
    small cost will affect the SVM classifier.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机通过最大边界创建一个最优的超平面来分离训练数据。然而，有时我们希望在分离类别时允许一些误分类。SVM 模型有一个成本函数，它控制训练错误和边界。例如，小的成本创建大的边界（软边界）并允许更多的误分类。另一方面，大的成本创建窄的边界（硬边界）并允许较少的误分类。在本菜谱中，我们将说明大成本和小成本如何影响
    SVM 分类器。
- en: Getting ready
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use the `iris` dataset as our example data source.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将使用 `iris` 数据集作为我们的示例数据源。
- en: How to do it...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to generate two different classification examples
    with different costs:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以生成两个具有不同成本的不同的分类示例：
- en: 'Subset the `iris` dataset with columns named as `Sepal.Length`, `Sepal.Width`,
    `Species`, with species in `setosa` and `virginica`:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列名为 `Sepal.Length`、`Sepal.Width` 和 `Species` 的 `iris` 数据集子集，其中物种为 `setosa`
    和 `virginica`：
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, you can generate a scatter plot with `Sepal.Length` as the x-axis and
    the `Sepal.Width` as the y-axis:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以生成一个散点图，其中`Sepal.Length`作为x轴，`Sepal.Width`作为y轴：
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![How to do it...](img/00113.jpeg)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00113.jpeg)'
- en: 'Figure 2: Scatter plot of Sepal.Length and Sepal.Width with subset of iris
    dataset'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2：鸢尾花数据集子集的`Sepal.Length`和`Sepal.Width`的散点图
- en: 'Next, you can train SVM based on `iris.subset` with the cost equal to 1:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`iris.subset`训练成本为1的SVM：
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we can circle the support vector with blue circles:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以用蓝色圆圈圈出支持向量：
- en: '[PRE9]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![How to do it...](img/00114.jpeg)'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00114.jpeg)'
- en: 'Figure 3: Circling support vectors with blue ring'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3：用蓝色圆圈圈出支持向量
- en: 'Lastly, we can add a separation line on the plot:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以在图上添加分离线：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![How to do it...](img/00115.jpeg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00115.jpeg)'
- en: 'Figure 4: Add separation line to scatter plot'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4：在散点图上添加分离线
- en: 'In addition to this, we create another SVM classifier where `cost = 10,000`:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们创建另一个成本为`10,000`的SVM分类器：
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![How to do it...](img/00116.jpeg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00116.jpeg)'
- en: 'Figure 5: A classification example with large cost'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5：大成本分类示例
- en: How it works...
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we demonstrate how different costs can affect the SVM classifier.
    First, we create an iris subset with the columns, `Sepal.Length`, `Sepal.Width`,
    and `Species` containing the species, `setosa` and `virginica`. Then, in order
    to create a soft margin and allow some misclassification, we use an SVM with small
    cost (where `cost = 1`) to train the support of the vector machine. Next, we circle
    the support vectors with blue circles and add the separation line. As per *Figure
    5*, one of the green points (`virginica`) is misclassified (it is classified to
    `setosa`) to the other side of the separation line due to the choice of the small
    cost.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了不同的成本如何影响SVM分类器。首先，我们创建了一个包含物种`setosa`和`virginica`的`Sepal.Length`、`Sepal.Width`和`Species`列的鸢尾花子集。然后，为了创建软边界并允许一些误分类，我们使用成本较小的SVM（其中`cost
    = 1`）来训练支持向量机。接下来，我们用蓝色圆圈圈出支持向量并添加分离线。根据*图5*，一个绿色点（`virginica`）由于成本选择较小而被误分类（被分类为`setosa`）到分离线的另一侧。
- en: In addition to this, we would like to determine how a large cost can affect
    the SVM classifier. Therefore, we choose a large cost (where `cost = 10,000`).
    From Figure 5, we can see that the margin created is narrow (a hard margin) and
    no misclassification cases are present. As a result, the two examples show that
    the choice of different costs may affect the margin created and also affect the
    possibilities of misclassification.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想确定大成本如何影响SVM分类器。因此，我们选择了一个大成本（其中`cost = 10,000`）。从图5中，我们可以看到创建的边界很窄（硬边界）且没有误分类案例。因此，这两个示例表明，不同成本的选择可能会影响创建的边界，并影响误分类的可能性。
- en: See also
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'The idea of soft margin, which allows misclassified examples, was suggested
    by Corinna Cortes and Vladimir N. Vapnik in 1995 in the following paper: Cortes,
    C., and Vapnik, V. (1995). *Support-vector networks. Machine learning*, 20(3),
    273-297.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许误分类的软边界概念是由Corinna Cortes和Vladimir N. Vapnik在1995年以下论文中提出的：Cortes, C.，and
    Vapnik, V. (1995). *支持向量机. 机器学习*，20(3)，273-297。
- en: Visualizing an SVM fit
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化SVM拟合
- en: To visualize the built model, one can first use the plot function to generate
    a scatter plot of data input and the SVM fit. In this plot, support vectors and
    classes are highlighted through the color symbol. In addition to this, one can
    draw a contour filled plot of the class regions to easily identify misclassified
    samples from the plot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化构建的模型，可以使用绘图函数首先生成数据输入和SVM拟合的散点图。在这个图中，支持向量和类别通过颜色符号突出显示。此外，还可以绘制类区域的填充轮廓图，以便从图中轻松识别误分类样本。
- en: Getting ready
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will use two datasets: the `iris` dataset and the telecom
    `churn` dataset. For the telecom `churn` dataset, one needs to have completed
    the previous recipe by training a support vector machine with SVM, and to have
    saved the SVM fit model.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用两个数据集：`iris`数据集和电信`churn`数据集。对于电信`churn`数据集，需要完成之前的菜谱，通过训练SVM来训练支持向量机，并保存SVM拟合模型。
- en: How to do it...
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to visualize the SVM fit object:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以可视化SVM拟合对象：
- en: 'Use SVM to train the support vector machine based on the iris dataset, and
    use the `plot` function to visualize the fitted model:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基于鸢尾花数据集的SVM训练支持向量机，并使用`plot`函数可视化拟合的模型：
- en: '[PRE12]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![How to do it...](img/00117.jpeg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00117.jpeg)'
- en: 'Figure 6: The SVM classification plot of trained SVM fit based on iris dataset'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6：基于iris数据集训练的SVM分类图
- en: 'Visualize the SVM fit object, `model`, using the `plot` function with the dimensions
    of `total_day_minutes` and `total_intl_charge`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot`函数，以`total_day_minutes`和`total_intl_charge`的维度可视化SVM拟合对象`model`：
- en: '[PRE13]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![How to do it...](img/00118.jpeg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00118.jpeg)'
- en: 'Figure 7: The SVM classification plot of trained SVM fit based on churn dataset'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7：基于流失数据集训练的SVM分类图
- en: How it works...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we demonstrate how to use the `plot` function to visualize the
    SVM fit. In the first plot, we train a support vector machine using the `iris`
    dataset. Then, we use the `plot` function to visualize the fitted SVM.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们展示了如何使用`plot`函数来可视化SVM拟合。在第一个图中，我们使用`iris`数据集训练了一个支持向量机。然后，我们使用`plot`函数来可视化拟合的SVM。
- en: In the argument list, we specify the fitted model in the first argument and
    the dataset (this should be the same data used to build the model) as the second
    parameter. The third parameter indicates the dimension used to generate the classification
    plot. By default, the `plot` function can only generate a scatter plot based on
    two dimensions (for the x-axis and y-axis). Therefore, we select the variables,
    `Petal.Length` and `Petal.Width` as the two dimensions to generate the scatter
    plot.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数列表中，我们将拟合的模型指定为第一个参数，将数据集（这应该是用于构建模型的数据）作为第二个参数。第三个参数表示用于生成分类图的维度。默认情况下，`plot`函数只能根据两个维度（用于x轴和y轴）生成散点图。因此，我们选择变量`Petal.Length`和`Petal.Width`作为两个维度来生成散点图。
- en: From *Figure 6*, we find `Petal.Length` assigned to the x-axis, `Petal.Width`
    assigned to the y-axis, and data points with `X` and `O` symbols scattered on
    the plot. Within the scatter plot, the `X` symbol shows the support vector and
    the `O` symbol represents the data points. These two symbols can be altered through
    the configuration of the `svSymbol` and `dataSymbol` options. Both the support
    vectors and true classes are highlighted and colored depending on their label
    (green refers to viginica, red refers to versicolor, and black refers to setosa).
    The last argument, `slice`, is set when there are more than two variables. Therefore,
    in this example, we use the additional variables, `Sepal.width` and `Sepal.length`,
    by assigning a constant of `3` and `4`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图6*中，我们发现`Petal.Length`被分配到x轴，`Petal.Width`被分配到y轴，带有`X`和`O`符号的数据点散布在图上。在散点图中，`X`符号表示支持向量，`O`符号表示数据点。这两个符号可以通过配置`svSymbol`和`dataSymbol`选项来更改。支持向量和真实类别根据它们的标签（绿色代表virginica，红色代表versicolor，黑色代表setosa）突出显示并着色。最后一个参数`slice`在存在多于两个变量时设置。因此，在这个例子中，我们使用额外的变量`Sepal.width`和`Sepal.length`，通过分配常数`3`和`4`。
- en: Next, we take the same approach to draw the SVM fit based on customer churn
    data. In this example, we use `total_day_minutes` and `total_intl_charge` as the
    two dimensions used to plot the scatterplot. As per *Figure 7*, the support vectors
    and data points in red and black are scattered closely together in the central
    region of the plot, and there is no simple way to separate them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们采用相同的方法来绘制基于客户流失数据的SVM拟合。在这个例子中，我们使用`total_day_minutes`和`total_intl_charge`作为绘制散点图的两个维度。根据*图7*，支持向量和红色、黑色数据点在图的中央区域紧密散布，没有简单的方法可以将它们分开。
- en: See also
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'There are other parameters, such as `fill`, `grid`, `symbolPalette`, and so
    on, that can be configured to change the layout of the plot. You can use the `help`
    function to view the following document for further information:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有其他参数，如`fill`、`grid`、`symbolPalette`等，可以配置以改变图的布局。您可以使用`help`函数查看以下文档以获取更多信息：
- en: '[PRE14]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Predicting labels based on a model trained by a support vector machine
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于支持向量机训练的模型预测标签
- en: In the previous recipe, we trained an SVM based on the training dataset. The
    training process finds the optimum hyperplane that separates the training data
    by the maximum margin. We can then utilize the SVM fit to predict the label (category)
    of new observations. In this recipe, we will demonstrate how to use the `predict`
    function to predict values based on a model trained by SVM.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们基于训练数据集训练了一个SVM。训练过程通过最大间隔找到将训练数据分开的最优超平面。然后，我们可以利用SVM拟合来预测新观察值的标签（类别）。在本配方中，我们将演示如何使用`predict`函数根据SVM训练的模型预测值。
- en: Getting ready
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by generating a fitted SVM, and
    save the fitted model in model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要完成之前的菜谱，通过生成拟合SVM，并将拟合模型保存在model中。
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to predict the labels of the testing dataset:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以预测测试数据集的标签：
- en: 'Predict the label of the testing dataset based on the fitted SVM and attributes
    of the testing dataset:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据拟合的SVM和测试数据集的属性预测测试数据集的标签：
- en: '[PRE15]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, you can use the `table` function to generate a classification table with
    the prediction result and labels of the testing dataset:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`table`函数来生成一个包含测试数据集预测结果和标签的分类表：
- en: '[PRE16]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, you can use `classAgreement` to calculate coefficients compared to the
    classification agreement:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`classAgreement`来计算与分类一致性相比的系数：
- en: '[PRE17]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, you can use `confusionMatrix` to measure the prediction performance based
    on the classification table:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以使用`confusionMatrix`来根据分类表衡量预测性能：
- en: '[PRE18]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we first used the `predict` function to obtain the predicted
    labels of the testing dataset. Next, we used the `table` function to generate
    the classification table based on the predicted labels of the testing dataset.
    So far, the evaluation procedure is very similar to the evaluation process mentioned
    in the previous chapter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们首先使用`predict`函数获取测试数据集的预测标签。然后，我们使用`table`函数根据测试数据集的预测标签生成分类表。到目前为止，评估过程与上一章中提到的评估过程非常相似。
- en: We then introduced a new function, `classAgreement`, which computes several
    coefficients of agreement between the columns and rows of a two-way contingency
    table. The coefficients include diag, kappa, rand, and crand. The `diag` coefficient
    represents the percentage of data points in the main diagonal of the classification
    table, `kappa` refers to `diag`, which is corrected for an agreement by a change
    (the probability of random agreements), `rand` represents the Rand index, which
    measures the similarity between two data clusters, and `crand` indicates the Rand
    index, which is adjusted for the chance grouping of elements.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后引入了一个新的函数，`classAgreement`，它计算了二维列联表中行和列之间的一致性系数。这些系数包括diag、kappa、rand和crand。`diag`系数表示分类表主对角线上数据点的百分比，`kappa`指的是`diag`，它通过一个变化（随机一致性的概率）进行了校正，`rand`代表Rand指数，它衡量两个数据簇之间的相似性，而`crand`表示调整了元素随机分组机会的Rand指数。
- en: Finally, we used `confusionMatrix` from the `caret` package to measure the performance
    of the classification model. The accuracy of 0.9185 shows that the trained support
    vector machine can correctly classify most of the observations. However, accuracy
    alone is not a good measurement of a classification model. One should also reference
    sensitivity and specificity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用了`caret`包中的`confusionMatrix`来衡量分类模型的表现。准确率0.9185表明训练好的支持向量机可以正确分类大多数观测值。然而，准确率本身并不能很好地衡量一个分类模型。还应参考敏感性和特异性。
- en: There's more...
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Besides using SVM to predict the category of new observations, you can use SVM
    to predict continuous values. In other words, one can use SVM to perform regression
    analysis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用SVM来预测新观测值的类别外，你还可以使用SVM来预测连续值。换句话说，可以使用SVM进行回归分析。
- en: In the following example, we will show how to perform a simple regression prediction
    based on a fitted SVM with the type specified as `eps-regression`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将展示如何基于指定为`eps-regression`类型的拟合SVM执行简单的回归预测：
- en: 'Perform the following steps to train a regression model with SVM:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用SVM训练回归模型：
- en: 'Train a support vector machine based on a `Quartet` dataset:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于一个`Quartet`数据集训练支持向量机：
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Use the `predict` function to obtain prediction results:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`predict`函数获取预测结果：
- en: '[PRE20]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Plot the predicted points as squares and the training data points as circles
    on the same plot:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一张图上绘制预测点为正方形，训练数据点为圆圈：
- en: '[PRE21]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![There''s more...](img/00119.jpeg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![更多内容...](img/00119.jpeg)'
- en: 'Figure 8: The scatter plot contains predicted data points and training data
    points'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8：散点图包含预测数据点和训练数据点
- en: Tuning a support vector machine
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整支持向量机
- en: Besides using different feature sets and the `kernel` function in support vector
    machines, one trick that you can use to tune its performance is to adjust the
    gamma and cost configured in the argument. One possible approach to test the performance
    of different gamma and cost combination values is to write a `for` loop to generate
    all the combinations of gamma and cost as inputs to train different support vector
    machines. Fortunately, SVM provides a tuning function, `tune.svm`, which makes
    the tuning much easier. In this recipe, we will demonstrate how to tune a support
    vector machine through the use of `tune.svm`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用不同的特征集和支持向量机中的`kernel`函数外，你可以调整配置在参数中的gamma和成本来调整其性能。测试不同gamma和成本组合值性能的一个可能方法是编写一个`for`循环来生成所有gamma和成本的组合，作为训练不同支持向量机的输入。幸运的是，SVM提供了一个调整函数`tune.svm`，这使得调整变得容易得多。在这个配方中，我们将演示如何通过使用`tune.svm`来调整支持向量机。
- en: Getting ready
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by preparing a training dataset,
    `trainset`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要完成之前的配方，准备一个训练数据集`trainset`。
- en: How to do it...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to tune the support vector machine:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来调整支持向量机：
- en: 'First, tune the support vector machine using `tune.svm`:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`tune.svm`调整支持向量机：
- en: '[PRE22]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, you can use the `summary` function to obtain the tuning result:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`summary`函数获取调整结果：
- en: '[PRE23]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After retrieving the best performance parameter from tuning the result, you
    can retrain the support vector machine with the best performance parameter:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在调整结果中检索最佳性能参数后，你可以使用最佳性能参数重新训练支持向量机：
- en: '[PRE24]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, you can use the `predict` function to predict labels based on the fitted
    SVM:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`predict`函数根据拟合的支持向量机预测标签：
- en: '[PRE25]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, generate a classification table based on the predicted and original labels
    of the testing dataset:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，根据测试数据集的预测标签和原始标签生成一个分类表：
- en: '[PRE26]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Also, generate a class agreement to measure the performance:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，生成一个类一致性来衡量性能：
- en: '[PRE27]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, you can use a confusion matrix to measure the performance of the retrained
    model:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用混淆矩阵来衡量重新训练的模型性能：
- en: '[PRE28]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How it works...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: To tune the support vector machine, you can use a trial and error method to
    find the best gamma and cost parameters. In other words, one has to generate a
    variety of combinations of gamma and cost for the purpose of training different
    support vector machines.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整支持向量机，你可以使用试错法来找到最佳的gamma和成本参数。换句话说，必须生成各种gamma和成本的组合，以训练不同的支持向量机。
- en: In this example, we generate different gamma values from *10^-6* to *10^-1*,
    and cost with a value of either 10 or 100\. Therefore, you can use the tuning
    function, `svm.tune`, to generate 12 sets of parameters. The function then makes
    10 cross-validations and outputs the error dispersion of each combination. As
    a result, the combination with the least error dispersion is regarded as the best
    parameter set. From the summary table, we found that gamma with a value of 0.01
    and cost with a value of 100 are the best parameters for the SVM fit.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们生成从*10^-6*到*10^-1*的不同gamma值，以及值为10或100的成本。因此，你可以使用调整函数`svm.tune`生成12组参数。该函数然后进行10次交叉验证，并输出每个组合的错误分散度。结果，错误分散度最低的组合被认为是最佳参数集。从摘要表中，我们发现gamma值为0.01且成本值为100是SVM拟合的最佳参数。
- en: After obtaining the best parameters, we can then train a new support vector
    machine with gamma equal to 0.01 and cost equal to 100\. Additionally, we can
    obtain a classification table based on the predicted labels and labels of the
    testing dataset. We can also obtain a confusion matrix from the classification
    table. From the output of the confusion matrix, you can determine the accuracy
    of the newly trained model in comparison to the original model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得最佳参数后，我们可以使用gamma等于0.01且成本等于100的新支持向量机进行训练。此外，我们还可以根据预测标签和测试数据集的标签获得一个分类表。我们还可以从分类表中获得一个混淆矩阵。从混淆矩阵的输出中，你可以确定新训练的模型与原始模型相比的准确性。
- en: See also
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: 'For more information about how to tune SVM with `svm.tune`, you can use the
    `help` function to access this document:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于如何使用`svm.tune`调整SVM的更多信息，你可以使用`help`函数访问此文档：
- en: '[PRE29]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Training a neural network with neuralnet
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用neuralnet训练神经网络
- en: The neural network is constructed with an interconnected group of nodes, which
    involves the input, connected weights, processing element, and output. Neural
    networks can be applied to many areas, such as classification, clustering, and
    prediction. To train a neural network in R, you can use neuralnet, which is built
    to train multilayer perceptron in the context of regression analysis, and contains
    many flexible functions to train forward neural networks. In this recipe, we will
    introduce how to use neuralnet to train a neural network.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是由相互连接的节点组构建的，涉及输入、连接权重、处理元素和输出。神经网络可以应用于许多领域，如分类、聚类和预测。要在R中训练神经网络，您可以使用neuralnet，它是在回归分析背景下构建的，用于训练多层感知器，并包含许多灵活的函数来训练前向神经网络。在这个菜谱中，我们将介绍如何使用neuralnet来训练神经网络。
- en: Getting ready
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will use an `iris` dataset as our example dataset. We will
    first split the `iris` dataset into a training and testing datasets, respectively.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用`iris`数据集作为我们的示例数据集。我们首先将`iris`数据集分为训练集和测试集。
- en: How to do it...
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to train a neural network with neuralnet:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用neuralnet训练神经网络：
- en: 'First load the `iris` dataset and split the data into training and testing
    datasets:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先加载`iris`数据集并将数据分为训练集和测试集：
- en: '[PRE30]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, install and load the `neuralnet` package:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，安装并加载`neuralnet`包：
- en: '[PRE31]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Add the columns versicolor, setosa, and virginica based on the name matched
    value in the `Species` column:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据名称匹配值在`Species`列中添加versicolor、setosa和virginica列：
- en: '[PRE32]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, train the neural network with the `neuralnet` function with three hidden
    neurons in each layer. Notice that the results may vary with each training, so
    you might not get the same result. However, you can use set.seed at the beginning,
    so you can get the same result in every training process
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用具有每层三个隐藏神经元的`neuralnet`函数训练神经网络。请注意，每次训练的结果可能会有所不同，因此您可能不会得到相同的结果。然而，您可以在开始时使用set.seed，这样您可以在每次训练过程中得到相同的结果。
- en: '[PRE33]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, you can view the `summary` information by accessing the `result.matrix`
    attribute of the built neural network model:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以通过访问构建的神经网络模型的`result.matrix`属性来查看`summary`信息：
- en: '[PRE34]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Lastly, you can view the generalized weight by accessing it in the network:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以通过在网络上访问它来查看广义权重：
- en: '[PRE35]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How it works...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The neural network is a network made up of artificial neurons (or nodes). There
    are three types of neurons within the network: input neurons, hidden neurons,
    and output neurons. In the network, neurons are connected; the connection strength
    between neurons is called weights. If the weight is greater than zero, it is in
    an excitation status. Otherwise, it is in an inhibition status. Input neurons
    receive the input information; the higher the input value, the greater the activation.
    Then, the activation value is passed through the network in regard to weights
    and transfer functions in the graph. The hidden neurons (or output neurons) then
    sum up the activation values and modify the summed values with the transfer function.
    The activation value then flows through hidden neurons and stops when it reaches
    the output nodes. As a result, one can use the output value from the output neurons
    to classify the data.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是由人工神经元（或节点）组成的网络。网络中有三种类型的神经元：输入神经元、隐藏神经元和输出神经元。在网络中，神经元是相互连接的；神经元之间的连接强度称为权重。如果权重大于零，则处于兴奋状态。否则，处于抑制状态。输入神经元接收输入信息；输入值越高，激活度越大。然后，激活值根据图中的权重和传递函数在网络中传递。隐藏神经元（或输出神经元）随后将激活值相加，并使用传递函数修改总和值。激活值随后通过隐藏神经元流动，并在达到输出节点时停止。因此，可以使用输出神经元的输出值来分类数据。
- en: '![How it works...](img/00120.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00120.jpeg)'
- en: 'Figure 9: Artificial Neural Network'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：人工神经网络
- en: 'The advantages of a neural network are: first, it can detect nonlinear relationships
    between the dependent and independent variable. Second, one can efficiently train
    large datasets using the parallel architecture. Third, it is a nonparametric model
    so that one can eliminate errors in the estimation of parameters. The main disadvantages
    of a neural network are that it often converges to the local minimum rather than
    the global minimum. Also, it might over-fit when the training process goes on
    for too long.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的优点是：首先，它可以检测因变量和自变量之间的非线性关系。其次，可以使用并行架构高效地训练大型数据集。第三，它是一个非参数模型，因此可以消除参数估计中的误差。神经网络的主要缺点是它往往收敛到局部最小值而不是全局最小值。此外，当训练过程过长时，可能会出现过拟合。
- en: In this recipe, we demonstrate how to train a neural network. First, we split
    the `iris` dataset into training and testing datasets, and then install the `neuralnet`
    package and load the library into an R session. Next, we add the columns `versicolor`,
    `setosa`, and `virginica` based on the name matched value in the `Species` column,
    respectively. We then use the `neuralnet` function to train the network model.
    Besides specifying the label (the column where the name equals to versicolor,
    virginica, and setosa) and training attributes in the function, we also configure
    the number of hidden neurons (vertices) as three in each layer.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何训练一个神经网络。首先，我们将`iris`数据集分为训练集和测试集，然后安装`neuralnet`包并将库加载到R会话中。接下来，我们根据`Species`列中匹配的名称分别添加`versicolor`、`setosa`和`virginica`列。然后，我们使用`neuralnet`函数来训练网络模型。在函数中指定标签（名称等于versicolor、virginica和setosa的列）和训练属性之外，我们还配置了每层的隐藏神经元（顶点）数量为三个。
- en: Then, we examine the basic information about the training process and the trained
    network saved in the network. From the output message, it shows the training process
    needed 11,063 steps until all the absolute partial derivatives of the error function
    were lower than 0.01 (specified in the threshold). The error refers to the likelihood
    of calculating **Akaike Information Criterion** (**AIC**). To see detailed information
    on this, you can access the `result.matrix` of the built neural network to see
    the estimated weight. The output reveals that the estimated weight ranges from
    -18 to 24.40; the intercepts of the first hidden layer are 1.69, 1.41 and 24.40,
    and the two weights leading to the first hidden neuron are estimated as 0.95 (`Sepal.Length`),
    -7.22 (`Sepal.Width`), 1.79 (`Petal.Length`), and 9.94 (`Petal.Width`). We can
    lastly determine that the trained neural network information includes generalized
    weights, which express the effect of each covariate. In this recipe, the model
    generates 12 generalized weights, which are the combination of four covariates
    (`Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width`) to three responses
    (`setosa`, `virginica`, `versicolor`).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们检查训练过程和保存在网络中的训练网络的基本信息。从输出信息中，可以看出训练过程需要11,063步才能使所有误差函数的绝对偏导数低于0.01（指定在阈值中）。误差指的是计算**赤池信息量准则**（**AIC**）的可能性。要查看详细信息，您可以访问构建的神经网络的`result.matrix`以查看估计的权重。输出显示估计的权重范围从-18到24.40；第一隐藏层的截距为1.69、1.41和24.40，连接到第一隐藏神经元的两个权重估计为0.95（`Sepal.Length`）、-7.22（`Sepal.Width`）、1.79（`Petal.Length`）和9.94（`Petal.Width`）。我们最后可以确定训练的神经网络信息包括广义权重，这些权重表示每个协变量的影响。在这个菜谱中，模型生成了12个广义权重，这是四个协变量（`Sepal.Length`、`Sepal.Width`、`Petal.Length`、`Petal.Width`）与三个响应（`setosa`、`virginica`、`versicolor`）的组合。
- en: See also
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For a more detailed introduction on neuralnet, one can refer to the following
    paper: Günther, F., and Fritsch, S. (2010). *neuralnet: Training of neural networks*.
    *The R journal*, 2(1), 30-38.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于神经网络的更详细介绍，可以参考以下论文：Günther, F. 和 Fritsch, S. (2010). *neuralnet: 神经网络的训练*.
    *R期刊*, 2(1), 30-38。'
- en: Visualizing a neural network trained by neuralnet
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化由neuralnet训练的神经网络
- en: The package, `neuralnet`, provides the `plot` function to visualize a built
    neural network and the `gwplot` function to visualize generalized weights. In
    following recipe, we will cover how to use these two functions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet`包提供了`plot`函数来可视化构建的神经网络，以及`gwplot`函数来可视化广义权重。在接下来的菜谱中，我们将介绍如何使用这两个函数。'
- en: Getting ready
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by training a neural network
    and have all basic information saved in the network.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成之前的菜谱，通过训练神经网络并保存所有基本信息到网络中。
- en: How to do it...
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to visualize the neural network and the generalized
    weights:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以可视化神经网络和广义权重：
- en: 'You can visualize the trained neural network with the `plot` function:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`plot`函数可视化训练好的神经网络：
- en: '[PRE36]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![How to do it...](img/00121.jpeg)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00121.jpeg)'
- en: 'Figure 10: The plot of the trained neural network'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10：训练好的神经网络的图示
- en: 'Furthermore, you can use `gwplot` to visualize the generalized weights:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您还可以使用`gwplot`可视化广义权重：
- en: '[PRE37]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![How to do it...](img/00122.jpeg)'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00122.jpeg)'
- en: 'Figure 11: The plot of generalized weights'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11：广义权重的图示
- en: How it works...
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we demonstrate how to visualize the trained neural network and
    the generalized weights of each trained attribute. As per *Figure 10*, the plot
    displays the network topology of the trained neural network. Also, the plot includes
    the estimated weight, intercepts and basic information about the training process.
    At the bottom of the figure, one can find the overall error and number of steps
    required to converge.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们展示了如何可视化训练好的神经网络以及每个训练属性的广义权重。根据*图10*，该图显示了训练好的神经网络的网络拓扑结构。此外，该图还包括估计的权重、截距以及训练过程的基本信息。在图的下部，可以找到总体误差和收敛所需的步数。
- en: '*Figure 11* presents the generalized weight plot in regard to `network$generalized.weights`.
    The four plots in *Figure 11* display the four covariates: `Petal.Width`, `Sepal.Width`,
    `Petal.Length`, and `Petal.Width`, in regard to the versicolor response. If all
    the generalized weights are close to zero on the plot, it means the covariate
    has little effect. However, if the overall variance is greater than one, it means
    the covariate has a nonlinear effect.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11*展示了与`network$generalized.weights`相关的广义权重图。*图11*中的四个图显示了四个协变量：`Petal.Width`、`Sepal.Width`、`Petal.Length`和`Petal.Width`，相对于versicolor响应。如果图上的所有广义权重都接近于零，这意味着协变量几乎没有影响。然而，如果整体方差大于一，这意味着协变量有非线性影响。'
- en: See also
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information about `gwplot`, one can use the `help` function to access
    the following document:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`gwplot`的更多信息，可以使用`help`函数访问以下文档：
- en: '[PRE38]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Predicting labels based on a model trained by neuralnet
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于neuralnet模型训练预测标签
- en: Similar to other classification methods, we can predict the labels of new observations
    based on trained neural networks. Furthermore, we can validate the performance
    of these networks through the use of a confusion matrix. In the following recipe,
    we will introduce how to use the `compute` function in a neural network to obtain
    a probability matrix of the testing dataset labels, and use a table and confusion
    matrix to measure the prediction performance.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他分类方法类似，我们可以根据训练好的神经网络预测新观察值的标签。此外，我们可以通过使用混淆矩阵来验证这些网络的表现。在接下来的食谱中，我们将介绍如何使用神经网络中的`compute`函数获取测试数据集标签的概率矩阵，并使用表格和混淆矩阵来衡量预测性能。
- en: Getting ready
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by generating the training dataset,
    `trainset`, and the testing dataset, `testset`. The trained neural network needs
    to be saved in the network.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要通过生成训练数据集`trainset`和测试数据集`testset`来完成前面的食谱。训练好的神经网络需要保存在网络中。
- en: How to do it...
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to measure the prediction performance of the trained
    neural network:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以衡量训练好的神经网络的预测性能：
- en: 'First, generate a prediction probability matrix based on a trained neural network
    and the testing dataset, `testset`:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，基于训练好的神经网络和测试数据集`testset`生成一个预测概率矩阵：
- en: '[PRE39]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, obtain other possible labels by finding the column with the greatest
    probability:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过找到概率最大的列来获取其他可能的标签：
- en: '[PRE40]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Generate a classification table based on the predicted labels and the labels
    of the testing dataset:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据预测标签和测试数据集的标签生成一个分类表：
- en: '[PRE41]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, generate `classAgreement` from the classification table:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从分类表中生成`classAgreement`：
- en: '[PRE42]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, use `confusionMatrix` to measure the prediction performance:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`confusionMatrix`来衡量预测性能：
- en: '[PRE43]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we demonstrate how to predict labels based on a model trained
    by neuralnet. Initially, we use the `compute` function to create an output probability
    matrix based on the trained neural network and the testing dataset. Then, to convert
    the probability matrix to class labels, we use the `which.max` function to determine
    the class label by selecting the column with the maximum probability within the
    row. Next, we use a table to generate a classification matrix based on the labels
    of the testing dataset and the predicted labels. As we have created the classification
    table, we can employ a confusion matrix to measure the prediction performance
    of the built neural network.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们展示了如何根据由neuralnet训练的模型预测标签。最初，我们使用`compute`函数根据训练好的神经网络和测试数据集创建输出概率矩阵。然后，为了将概率矩阵转换为类别标签，我们使用`which.max`函数通过选择行内概率最大的列来确定类别标签。接下来，我们使用一个表格根据测试数据集的标签和预测标签生成分类矩阵。由于我们已经创建了分类表，我们可以使用混淆矩阵来衡量构建的神经网络的预测性能。
- en: See also
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'In this recipe, we use the `net.result` function, which is the overall result
    of the neural network, used to predict the labels of the testing dataset. Apart
    from examining the overall result by accessing `net.result`, the `compute` function
    also generates the output from neurons in each layer. You can examine the output
    of neurons to get a better understanding of how `compute` works:'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本食谱中，我们使用`net.result`函数，这是神经网络的总体结果，用于预测测试数据集的标签。除了通过访问`net.result`来检查总体结果之外，`compute`函数还生成了每一层的神经元输出。你可以检查神经元的输出，以更好地理解`compute`是如何工作的：
- en: '[PRE44]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Training a neural network with nnet
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用nnet训练神经网络
- en: The `nnet` package is another package that can deal with artificial neural networks.
    This package provides the functionality to train feed-forward neural networks
    with traditional back propagation. As you can find most of the neural network
    function implemented in the `neuralnet` package, in this recipe we provide a short
    overview of how to train neural networks with `nnet`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`nnet`包是另一个可以处理人工神经网络的包。此包提供了使用传统反向传播训练前馈神经网络的函数。正如你可以在`neuralnet`包中找到的大多数神经网络函数一样，在本食谱中，我们提供了一个简短的概述，说明如何使用`nnet`训练神经网络。'
- en: Getting ready
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we do not use the `trainset` and `trainset` generated from the
    previous step; please reload the `iris` dataset again.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们不使用从上一步生成的`trainset`和`trainset`；请重新加载`iris`数据集。
- en: How to do it...
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to train the neural network with `nnet`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用`nnet`训练神经网络：
- en: 'First, install and load the `nnet` package:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`nnet`包：
- en: '[PRE45]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, split the dataset into training and testing datasets:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将数据集分为训练集和测试集：
- en: '[PRE46]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, train the neural network with `nnet`:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用`nnet`训练神经网络：
- en: '[PRE47]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Use the `summary` to obtain information about the trained neural network:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary`获取已训练神经网络的详细信息：
- en: '[PRE48]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How it works...
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we demonstrate steps to train a neural network model with the
    `nnet` package. We first use `nnet` to train the neural network. With this function,
    we can set the classification formula, source of data, number of hidden units
    in the `size` parameter, initial random weight in the `rang` parameter, parameter
    for weight decay in the `decay` parameter, and the maximum iteration in the `maxit`
    parameter. As we set `maxit` to 200, the training process repeatedly runs till
    the value of the fitting criterion plus the decay term converge. Finally, we use
    the `summary` function to obtain information about the built neural network, which
    reveals that the model is built with 4-2-3 networks with 19 weights. Also, the
    model shows a list of weight transitions from one node to another at the bottom
    of the printed message.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们展示了使用`nnet`包训练神经网络模型的步骤。我们首先使用`nnet`来训练神经网络。通过这个函数，我们可以设置分类公式、数据源、`size`参数中的隐藏单元数量、`rang`参数中的初始随机权重、`decay`参数中的权重衰减参数以及`maxit`参数中的最大迭代次数。由于我们将`maxit`设置为200，训练过程会反复运行，直到拟合准则值加上衰减项收敛。最后，我们使用`summary`函数获取构建的神经网络的详细信息，这表明模型使用4-2-3网络构建，共有19个权重。此外，模型在打印消息的底部显示了一个从节点到另一个节点的权重转换列表。
- en: See also
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For those who are interested in the background theory of `nnet` and how it
    is made, please refer to the following articles:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对`nnet`的背景理论和其制作方式感兴趣的人，请参阅以下文章：
- en: Ripley, B. D. (1996) *Pattern Recognition and Neural Networks*. Cambridge
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ripley, B. D. (1996) *Pattern Recognition and Neural Networks*. Cambridge
- en: Venables, W. N., and Ripley, B. D. (2002). *Modern applied statistics with S.
    Fourth edition*. Springer
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venables, W. N., and Ripley, B. D. (2002). *Modern applied statistics with S.
    Fourth edition*. Springer
- en: Predicting labels based on a model trained by nnet
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于nnet训练的模型进行标签预测
- en: As we have trained a neural network with `nnet` in the previous recipe, we can
    now predict the labels of the testing dataset based on the trained neural network.
    Furthermore, we can assess the model with a confusion matrix adapted from the
    `caret` package.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个菜谱中，我们已经使用`nnet`训练了一个神经网络，现在我们可以根据训练好的神经网络预测测试数据集的标签。此外，我们可以使用来自`caret`包的混淆矩阵来评估模型。
- en: Getting ready
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by generating the training dataset,
    `trainset`, and the testing dataset, `testset`, from the `iris` dataset. The trained
    neural network also needs to be saved as `iris.nn`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成上一个菜谱，通过从`iris`数据集中生成训练数据集`trainset`和测试数据集`testset`。训练好的神经网络也需要保存为`iris.nn`。
- en: How to do it...
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to predict labels based on the trained neural network:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以基于训练好的神经网络进行标签预测：
- en: 'Generate the predictions of the testing dataset based on the model, `iris.nn`:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于模型`iris.nn`生成测试数据集的预测：
- en: '[PRE49]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Generate a classification table based on the predicted labels and labels of
    the testing dataset:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据预测标签和测试数据集的标签生成分类表：
- en: '[PRE50]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Lastly, generate a confusion matrix based on the classification table:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，根据分类表生成混淆矩阵：
- en: '[PRE51]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: How it works...
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Similar to other classification methods, one can also predict labels based on
    the neural networks trained by `nnet`. First, we use the `predict` function to
    generate the predicted labels based on a testing dataset, `testset`. Within the
    `predict` function, we specify the `type` argument to the class, so the output
    will be class labels instead of a probability matrix. Next, we use the `table`
    function to generate a classification table based on predicted labels and labels
    written in the testing dataset. Finally, as we have created the classification
    table, we can employ a confusion matrix from the `caret` package to measure the
    prediction performance of the trained neural network.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他分类方法类似，也可以基于由`nnet`训练的神经网络进行标签预测。首先，我们使用`predict`函数根据测试数据集`testset`生成预测标签。在`predict`函数中，我们指定类的`type`参数，以便输出将是类标签而不是概率矩阵。接下来，我们使用`table`函数根据预测标签和测试数据集中的标签生成分类表。最后，由于我们已经创建了分类表，我们可以使用`caret`包中的混淆矩阵来衡量训练好的神经网络的预测性能。
- en: See also
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For the `predict` function, if the `type` argument to `class` is not specified,
    by default, it will generate a probability matrix as a prediction result, which
    is very similar to `net.result` generated from the `compute` function within the
    `neuralnet` package:'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`predict`函数，如果未指定`class`的`type`参数，默认情况下，它将生成一个概率矩阵作为预测结果，这与`neuralnet`包内`compute`函数生成的`net.result`非常相似：
- en: '[PRE52]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
