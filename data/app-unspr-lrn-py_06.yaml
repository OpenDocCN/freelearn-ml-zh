- en: '*Chapter 6*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 6 章*'
- en: t-Distributed Stochastic Neighbor Embedding (t-SNE)
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: t-分布随机邻域嵌入（t-SNE）
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Describe and understand the motivation behind t-SNE
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述并理解 t-SNE 背后的动机
- en: Describe the derivation of SNE and t-SNE
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述 SNE 和 t-SNE 的推导过程
- en: Implement t-SNE models in scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现 t-SNE 模型
- en: Explain the limitations of t-SNE
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 t-SNE 的局限性
- en: In this chapter, we will discuss Stochastic Neighbor Embedding (SNE) and t-Distributed
    Stochastic Neighbor Embedding (t-SNE) as a means of visualizing high-dimensional
    datasets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论随机邻域嵌入（SNE）和 t-分布随机邻域嵌入（t-SNE）作为可视化高维数据集的一种手段。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter is the final instalment in the micro-series on dimensionality reduction
    techniques and transformations. Our previous chapters in this series have described
    a number of different methods for reducing the dimensionality of a dataset as
    a means of either cleaning the data, reducing its size for computational efficiency,
    or for extracting the most important information available within the dataset.
    While we have demonstrated many methods for reducing high-dimensional datasets,
    in many cases, we are unable to reduce the number of dimensions to a size that
    can be visualized, that is, two or three dimensions, without excessively degrading
    the quality of the data. Consider the MNIST dataset that we used in *Chapter 5*,
    *Autoencoders*, which is a collection of digitized handwritten digits of the numbers
    0 through 9\. Each image is 28 x 28 pixels in size, providing 784 individual dimensions
    or features. If we were to reduce these 784 dimensions down to 2 or 3 for visualization
    purposes, we would lose almost all the available information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是关于降维技术和变换的微型系列的最后一篇。我们在本系列的前几章中描述了多种不同的降维方法，用于清理数据、提高计算效率或提取数据集中最重要的信息。虽然我们已经展示了许多降维高维数据集的方法，但在许多情况下，我们无法将维度数量减少到可以可视化的规模，即二维或三维，而不严重降低数据的质量。考虑我们在*第
    5 章*《自编码器》中使用的 MNIST 数据集，它是 0 到 9 的手写数字的数字化集合。每个图像的大小为 28 x 28 像素，提供 784 个独立的维度或特征。如果我们将这
    784 个维度降到 2 或 3 个以进行可视化，我们几乎会失去所有可用的信息。
- en: In this chapter, we will discuss Stochastic Neighbor Embedding (SNE) and t-Distributed
    Stochastic Neighbor Embedding (t-SNE) as a means of visualizing high-dimensional
    datasets. These techniques are extremely helpful in unsupervised learning and
    the design of machine learning systems because the visualization of data is a
    powerful tool. Being able to visualize data allows relationships to be explored,
    groups to be identified, and results to be validated. t-SNE techniques have been
    used to visualize cancerous cell nuclei that have over 30 characteristics of interest,
    whereas data from documents can have over thousands of dimensions, sometimes even
    after applying techniques such as PCA.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论随机邻域嵌入（SNE）和 t-分布随机邻域嵌入（t-SNE）作为可视化高维数据集的一种手段。这些技术在无监督学习和机器学习系统设计中非常有用，因为数据的可视化是一种强大的工具。能够可视化数据可以帮助探索关系、识别群体并验证结果。t-SNE
    技术已被用于可视化癌细胞核，其中有超过 30 种感兴趣的特征，而来自文档的数据可能有成千上万个维度，有时即使在应用 PCA 等技术后也如此。
- en: '![Figure 6.1: MNIST data sample'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1：MNIST 数据样本'
- en: '](img/C12626_06_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_01.jpg)'
- en: 'Figure 6.1: MNIST data sample'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.1：MNIST 数据样本
- en: 'Throughout this chapter, we will explore SNE and t-SNE using the MNIST dataset
    provided with the accompanying source code as the basis of practical examples.
    Before we continue, we will quickly review MNIST and the data that is within it.
    The complete MNIST dataset is a collection of 60,000 training and 10,000 test
    examples of handwritten digits of the numbers 0 to 9, represented as black and
    white (or grayscale) images 28 x 28 pixels in size (giving 784 dimensions or features)
    with equal numbers of each type of digit (or class) in the dataset. Due to its
    size and the quality of the data, MNIST has become one of the quintessential datasets
    in machine learning, often being used as the reference dataset for many research
    papers in machine learning. One of the advantages of using MINST to explore SNE
    and t-SNE compared to other datasets is that while the samples contain a high
    number of dimensions, they can be visualized even after dimensionality reduction
    because they can be represented as an image. [*Figure 6.1*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor139)
    shows a sample of the MNIST dataset, and [*Figure 6.2*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor140)
    shows the same sample, reduced to 30 components using PCA:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用MNIST数据集，并结合附带的源代码，通过实际示例来探索SNE和t-SNE。在继续之前，我们将快速回顾MNIST及其包含的数据。完整的MNIST数据集包含60,000个训练样本和10,000个测试样本，所有样本为手写数字0到9，表示为28x28像素大小的黑白（或灰度）图像（总计784个维度或特征），每种数字（或类别）的数量相等。由于其数据量大且质量高，MNIST已成为机器学习中的经典数据集，通常作为许多研究论文中的参考数据集。与其他数据集相比，使用MNIST探索SNE和t-SNE的一个优势是，尽管样本具有较高的维度，但在降维后仍然可以可视化，因为它们可以表示为图像。[*图6.1*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor139)展示了一个MNIST数据集的样本，[*图6.2*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor140)展示了同一个样本，使用PCA降维到30个成分后的效果：
- en: '![Figure 6.2: MNST reduced using PCA to 30 components'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2：使用PCA将MNIST数据降维至30个成分'
- en: '](img/C12626_06_02.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_02.jpg)'
- en: 'Figure 6.2: MNST reduced using PCA to 30 components'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.2：使用PCA将MNIST数据降维至30个成分
- en: Stochastic Neighbor Embedding (SNE)
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机邻域嵌入（SNE）
- en: 'Stochastic Neighbor Embedding (SNE) is one of a number of different methods
    that fall within the category of **manifold learning**, which aims to describe
    high-dimensional spaces within low-dimensional manifolds or bounded areas. At
    first thought, this seems like an impossible task; how can we reasonably represent
    data in two dimensions if we have a dataset with at least 30 features? As we work
    through the derivation of SNE, it is hoped that you will see how it is possible.
    Don''t worry, we will not be covering the mathematical details of this process
    in great depth as it is outside of the scope of this chapter. Constructing an
    SNE can be divided into the following steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随机邻域嵌入（SNE）是属于**流形学习**类别的众多方法之一，旨在将高维空间描述为低维流形或有界区域。一开始看，这似乎是一项不可能完成的任务；如果我们有一个至少包含30个特征的数据集，如何合理地在二维中表示数据呢？在我们推导SNE的过程中，期望你能看到这是如何实现的。别担心，我们不会深入探讨这一过程的数学细节，因为这些内容超出了本章的范围。构建SNE可以分为以下步骤：
- en: Convert the distances between datapoints in the high-dimensional space to conditional
    probabilities. Say we had two points, ![A close up of a sign
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将高维空间中数据点之间的距离转换为条件概率。假设我们有两个点，![一张近距离的标志图片
- en: Description automatically generated](img/C12626_06_Formula_01.png) and ![A picture
    containing furniture, table, seat
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_06_Formula_01.png)和![一张包含家具、桌子、座位的图片
- en: Description automatically generated](img/C12626_06_Formula_02.png), in high-dimensional
    space, and we wanted to determine the probability (![](img/C12626_06_Formula_03.png))
    that ![A picture containing furniture, table, seat
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_06_Formula_02.png)，在高维空间中，我们想确定概率（![](img/C12626_06_Formula_03.png)），即![一张包含家具、桌子、座位的图片
- en: Description automatically generated](img/C12626_06_Formula_04.png) would be
    picked as a neighbor of ![A close up of a sign
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_06_Formula_04.png)将被选为![一张近距离的标志图片
- en: Description automatically generated](img/C12626_06_Formula_05.png). To define
    this probability, we use a Gaussian curve, and we see that the probability is
    high for nearby points, while it is very low for distant points.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_06_Formula_05.png)。为了定义这个概率，我们使用高斯曲线，可以看到对于邻近的点，概率较高，而对于远离的点，概率非常低。
- en: We need to determine the width of the Gaussian curve as this controls the rate
    of probability selection. A wide curve would suggest that many points are far
    away, while a narrow curve suggests that they are tightly compacted.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要确定高斯曲线的宽度，因为它控制概率选择的速率。宽曲线表明很多点距离较远，而窄曲线则表明点紧密集中。
- en: Once we project the data into the low-dimensional space, we can also determine
    the corresponding probability (![](img/C12626_06_Formula_06.png)) between the
    corresponding low-dimensional data, ![](img/C12626_06_Formula_07.png) and ![](img/C12626_06_Formula_08.png).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们将数据投影到低维空间，我们还可以确定相应的概率 (![](img/C12626_06_Formula_06.png))，这与对应的低维数据 ![](img/C12626_06_Formula_07.png)
    和 ![](img/C12626_06_Formula_08.png) 之间的关系有关。
- en: 'What SNE aims to do is position the data in the lower dimensions to minimize
    the differences between ![](img/C12626_06_Formula_09.png) and ![](img/C12626_06_Formula_06.png)
    over all the data points using a cost function (C) known as the Kullback-Leibler
    (KL) divergence:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SNE 的目标是将数据定位到低维空间，以通过使用名为 Kullback-Leibler (KL) 散度的代价函数 (C)，最小化 ![](img/C12626_06_Formula_09.png)
    和 ![](img/C12626_06_Formula_06.png) 之间的差异，覆盖所有数据点：
- en: '![Figure 6.3: Kullback-Leibler divergence.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.3：Kullback-Leibler 散度。'
- en: '](img/C12626_06_03.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_03.jpg)'
- en: 'Figure 6.3: Kullback-Leibler divergence.'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.3：Kullback-Leibler 散度。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For Python code to construct a Gaussian distribution, refer to the `GaussianDist.ipynb`
    Jupyter notebook at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/GaussianDist.ipynb](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/GaussianDist.ipynb).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建高斯分布的 Python 代码，请参阅 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/GaussianDist.ipynb](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/GaussianDist.ipynb)
    中的 `GaussianDist.ipynb` Jupyter 笔记本。
- en: Gaussian distribution maps the data into low-dimensional space. To do this,
    SNE uses a process of gradient descent to minimize C using the standard parameters
    of learning rate and epochs as we covered in the previous chapter, looking at
    neural networks and autoencoders. SNE implements an additional term in the training
    process—**perplexity**. Perplexity is a selection of the effective number of neighbors
    used in the comparison and is relatively stable for the values of perplexity between
    5 and 50\. In practice, a process of trial and error using perplexity values within
    this range is recommended.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布将数据映射到低维空间。为此，SNE 使用梯度下降过程来最小化 C，使用我们在上一章中讲解过的标准学习率和迭代次数参数，回顾了神经网络和自编码器的内容。SNE
    在训练过程中引入了一个额外的术语——**困惑度**。困惑度是用来选择有效邻居数量的参数，对于困惑度值在 5 到 50 之间时，效果相对稳定。实际上，建议使用该范围内的困惑度值进行反复试验。
- en: SNE provides an effective way of visualizing high-dimensional data in a low-dimensional
    space, though it still suffers from an issue known as **the crowding problem**.
    The crowding problem can occur if we have some points positioned approximately
    equidistantly within a region around a point *i*. When these points are visualized
    in the lower-dimensional space, they crowd around each other, making visualization
    difficult. The problem is exacerbated if we try to put some more space between
    these crowded points, because any other points that are further away will then
    be placed very far away within the low-dimensional space. Essentially, we are
    trying to balance being able to visualize close points while not losing information
    provided by points that are further away.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SNE 提供了一种有效的方式将高维数据可视化到低维空间中，尽管它仍然面临一个被称为 **拥挤问题** 的问题。如果我们有一些点在某个点 *i* 周围大致等距离地分布，就可能出现拥挤问题。当这些点被可视化到低维空间时，它们会紧密聚集在一起，导致可视化困难。如果我们试图让这些拥挤的点之间保持更大的间距，问题会加剧，因为任何远离这些点的其他点都会被置于低维空间的非常远的位置。实质上，我们在尝试平衡能够可视化接近的点，同时又不丢失远离点提供的信息。
- en: t-Distributed SNE
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: t-分布 SNE
- en: t-SNE aims to address the crowding problem using a modified version of the KL
    divergence cost function and by substituting the Gaussian distribution with the
    Student's t-distribution in the low-dimensional space. Student's t-distribution
    is a continuous distribution that is used when one has a small sample size and
    unknown population standard deviation. It is often used in the Student's t-test.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 通过修改 KL 散度代价函数，并用学生 t 分布代替低维空间中的高斯分布来解决拥挤问题。学生 t 分布是一种连续分布，通常在样本量较小且未知总体标准差时使用，广泛应用于学生
    t 检验中。
- en: The modified KL cost function considers the pairwise distances in the low-dimensional
    space equally, while the student's distribution employs a heavy tail in the low-dimensional
    space to avoid the crowding problem. In the higher-dimensional probability calculation,
    the Gaussian distribution is still used to ensure that a moderate distance in
    the higher dimensions is still represented as such in the lower dimensions. This
    combination of different distributions in the respective spaces allows the faithful
    representation of datapoints separated by small and moderate distances.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的 KL 代价函数将低维空间中的成对距离视为相等，而学生分布在低维空间中采用重尾分布来避免拥挤问题。在高维概率计算中，仍然使用高斯分布，确保高维中的适度距离在低维中也能得到相应的表示。不同分布在各自空间中的组合使得在小距离和中等距离分离的数据点能够得到真实的表示。
- en: Note
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For example code of how to reproduce the Student's t Distribution in Python
    refer to the Jupyter notebook at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/StudentTDist.ipynb](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/StudentTDist.ipynb).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何在 Python 中重现学生 t 分布的示例代码，请参考 Jupyter notebook：[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/StudentTDist.ipynb](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/blob/master/Lesson06/StudentTDist.ipynb)。
- en: Thankfully, we don't need to worry about implementing t-SNE by hand, because
    scikit-learn provides a very effective implementation in its straightforward API.
    What we need to remember is that both SNE and t-SNE determine the probability
    of two points being neighbors in both high- and low-dimensionality space and aim
    to minimize the difference in the probability between the two spaces.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要手动实现 t-SNE，因为 scikit-learn 提供了一个非常有效的实现，且 API 简单明了。我们需要记住的是，SNE 和
    t-SNE 都是通过计算两个点在高维空间和低维空间中作为邻居的概率，旨在最小化这两个空间之间的概率差异。
- en: 'Exercise 24: t-SNE MNIST'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 24：t-SNE MNIST
- en: 'In this exercise, we will use the MNIST dataset (provided in the accompanying
    source code) to explore the scikit-learn implementation to t-SNE. As described
    earlier, using MNIST allows us to visualize the high-dimensional space in a way
    that is not possible in other datasets such as the Boston Housing Price or Iris
    dataset:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 MNIST 数据集（随附源代码提供）来探索 scikit-learn 对 t-SNE 的实现。如前所述，使用 MNIST 使我们能够以其他数据集（如波士顿住房价格或鸢尾花数据集）无法实现的方式来可视化高维空间：
- en: 'For this exercise, import `pickle`, `numpy`, `PCA`, and `TSNE` from scikit-learn,
    as well as `matplotlib`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于此练习，导入 `pickle`、`numpy`、`PCA` 和 `TSNE` 来自 scikit-learn，以及 `matplotlib`：
- en: '[PRE0]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load and visualize the MNIST dataset that is provided with the accompanying
    source code:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并可视化随附源代码提供的 MNIST 数据集：
- en: Note
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE1]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.4: Output after loading the dataset'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.4：加载数据集后的输出'
- en: '](img/C12626_06_04.jpg)'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_04.jpg)'
- en: 'Figure 6.4: Output after loading the dataset'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.4：加载数据集后的输出
- en: This demonstrates that MNIST has been successfully loaded.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明 MNIST 数据集已成功加载。
- en: In this exercise, we will use PCA on the dataset to reduce extract only the
    first 30 components.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本练习中，我们将对数据集使用 PCA 降维，只提取前 30 个成分。
- en: Note
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Visualize the effect of reducing the dataset to 30 components. To do this,
    we must first transform the dataset into the lower-dimensional space and then
    use the `inverse_transform` method to return the data to its original size for
    plotting. We will, of book, need to reshape the data before and after the transform
    process:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化将数据集降至 30 个成分的效果。为此，我们必须首先将数据集转换到低维空间，然后使用 `inverse_transform` 方法将数据返回到原始大小，以便进行绘图。当然，我们还需要在转换过程前后调整数据的形状：
- en: '[PRE3]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.5: Visualizing the effect of reducing the dataset'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.5：可视化数据集降维的效果'
- en: '](img/C12626_06_05.jpg)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_05.jpg)'
- en: 'Figure 6.5: Visualizing the effect of reducing the dataset'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.5：可视化减少数据集的效果
- en: Note that while we have lost some clarity in the images, for the most part,
    the numbers are still pretty clearly visible due to the dimension reduction process.
    It is interesting to note, however, that the number four (4) seems to have been
    the most visually affected by this process. Perhaps much of the discarded information
    from the PCA process contained information specific to the samples of four (4).
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，虽然我们在图像中丧失了一些清晰度，但由于降维过程，大部分数字仍然相当清晰可见。有趣的是，数字四（4）似乎是受此过程影响最大的。也许 PCA 过程丢弃的许多信息包含了特定于数字四（4）样本的信息。
- en: 'Now, we will apply t-SNE to the PCA-transformed data to visualize the 30 components
    in two-dimensional space. We can construct a t-SNE model in scikit-learn using
    the standard model API interface. We will start off using the default values that
    specify that we are embedding the 30 dimensions into two for visualization, using
    a perplexity of 30, a learning rate of 200, and 1,000 iterations. We will specify
    a `random_state` value of 0 and set `verbose` to 1:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将 t-SNE 应用于 PCA 转换后的数据，以在二维空间中可视化 30 个组件。我们可以使用 scikit-learn 中的标准模型 API
    接口来构建 t-SNE 模型。我们将首先使用默认值，这些值指定我们将 30 维数据嵌入到二维空间中进行可视化，使用 30 的困惑度、200 的学习率和 1000
    次迭代。我们将指定 `random_state` 值为 0，并将 `verbose` 设置为 1：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.6: Applying t-SNE to PCA-transformed data'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.6：将 t-SNE 应用于 PCA 转换后的数据'
- en: '](img/C12626_06_06.jpg)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_06.jpg)'
- en: 'Figure 6.6: Applying t-SNE to PCA-transformed data'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.6：将 t-SNE 应用于 PCA 转换后的数据
- en: In the previous screenshot, we can see a number of configuration options available
    for the t-distributed stochastic neighbor embedding model with some more important
    than the others. We will focus on the values of `learning_rate`, `n_components`,
    `n_iter`, `perplexity`, `random_state`, and `verbose`. For `learning_rate`, as
    discussed previously, t-SNE uses stochastic gradient descent to project the high-dimensional
    data in low-dimensional space. The learning rate controls the speed at which the
    process is executed. If learning rate is too high, the model may fail to converge
    on a solution or if too slow may take a very long time to reach it (if at all).
    A good rule of thumb is to start with the default; if you find the model producing
    NaNs (not a number values), you may need to reduce the learning rate. Once you
    are happy with the model, it is also wise to reduce the learning rate and let
    it run for longer (`increase n_iter`) as; you may in fact get a slightly better
    result. `n_components` is the number of dimensions in the embedding (or visualization
    space). More often than not, you would like a two-dimensional plot of the data,
    hence you just need the default value of `2`. `n_iter` is the maximum number of
    iterations of gradient descent. `perplexity`, as discussed in the previous section,
    is the number of neighbors to use in the visualizing the data.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在之前的截图中，我们可以看到 t-SNE 模型的多个配置选项，其中一些比其他选项更为重要。我们将重点关注 `learning_rate`、`n_components`、`n_iter`、`perplexity`、`random_state`
    和 `verbose` 的值。对于 `learning_rate`，如前所述，t-SNE 使用随机梯度下降将高维数据投影到低维空间。学习率控制过程执行的速度。如果学习率过高，模型可能无法收敛到解决方案；如果学习率过低，可能需要很长时间才能收敛（如果能够收敛）。一个好的经验法则是从默认值开始；如果你发现模型产生了
    NaN（非数值），则可能需要降低学习率。一旦你对模型满意，也可以降低学习率并让其运行更长时间（`增加 n_iter`）；事实上，这样可能会得到略微更好的结果。`n_components`
    是嵌入（或可视化空间）的维度数量。通常，你希望获得数据的二维图，因此只需使用默认值 `2`。`n_iter` 是梯度下降的最大迭代次数。`perplexity`，如前一节所述，是在可视化数据时使用的邻居数量。
- en: Typically, a value between 5 and 50 will be appropriate, knowing that larger
    datasets typically require more perplexity than smaller ones. `random_state` is
    an important variable for any model or algorithm that initializes its values randomly
    at the start of training. The random number generators provided within computer
    hardware and software tools are not, in fact, truly random; they are actually
    pseudo-random number generators. They give a good approximation of randomness,
    but are not truly random. Random numbers within computers start with a value known
    as a seed and are then produced in a complicated manner after that. By providing
    the same seed at the start of the process, the same "random numbers" are produced
    each time the process is run. While this sounds counter-intuitive, it is great
    for reproducing machine learning experiments as you won't see any difference in
    performance solely due to the initialization of the parameters at the start of
    training. This can provide more confidence that a change in performance is due
    to the considered change to the model or training, for example, the architecture
    of the neural network.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，介于 5 到 50 之间的值是合适的，因为较大的数据集通常需要比较小的数据集更多的困惑度（perplexity）。`random_state` 是任何模型或算法中一个重要的变量，它在训练开始时会随机初始化其值。计算机硬件和软件工具提供的随机数生成器实际上并非真正的随机数生成器；它们实际上是伪随机数生成器。它们提供了一个良好的随机性近似值，但并不是真正的随机。计算机中的随机数是从一个称为种子的值开始的，之后通过复杂的方式生成。通过在过程开始时提供相同的种子，每次运行该过程时都会生成相同的“随机数”。虽然这听起来不直观，但对于复现机器学习实验来说，这非常有用，因为你不会看到仅仅由于参数初始化的不同而导致的性能差异。这可以提供更多的信心，认为性能变化是由于模型或训练的某些改变，例如神经网络的架构。
- en: Note
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Producing true random sequences is actually one of the hardest tasks to achieve
    with a computer. Computer software and hardware is designed such that the instructions
    provided are executed in exactly the same way each time it is run so that you
    get the same result. Random differences in execution, while being ideal for producing
    sequences of random numbers, would be a nightmare in terms of automating tasks
    and debugging problems.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 产生真正的随机序列实际上是计算机最难实现的任务之一。计算机的软件和硬件设计是为了每次执行时按完全相同的方式运行指令，从而得到相同的结果。执行中的随机差异，虽然对于生成随机数序列来说理想，但在自动化任务和调试问题时会造成噩梦。
- en: '`verbose` is the verbosity level of the model and describes the amount of information
    printed to the screen during the model fitting process. A value of 0 indicates
    no output, while 1 or greater indicates increasing levels of detail in the output.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`verbose` 是模型的详细程度，描述了在模型拟合过程中打印到屏幕上的信息量。值为 0 表示没有输出，而 1 或更大的值表示输出中详细信息的增加。'
- en: 'Use t-SNE to transform the decomposed dataset of MNIST:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 t-SNE 转换 MNIST 的分解数据集：
- en: '[PRE5]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.7: Transforming the decomposed dataset'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.7：转换分解数据集'
- en: '](img/C12626_06_07.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_07.jpg)'
- en: 'Figure 6.7: Transforming the decomposed dataset'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.7：转换分解数据集
- en: The output provided during the fitting process provides an insight into the
    calculations being completed by scikit-learn. We can see that it is indexing and
    computing neighbors for all the samples and is then determining the conditional
    probabilities of being neighbors for the data in batches of 10\. At the end of
    the process, it provides a mean standard deviation (variance) value of 304.9988
    with KL divergence after 250 and 1,000 iterations of gradient descent.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在拟合过程中提供的输出能帮助我们了解 scikit-learn 完成的计算。我们可以看到它正在为所有样本进行索引和计算邻居，然后再批量地计算数据作为邻居的条件概率，每次批次为
    10。过程结束时，它提供了一个标准差（方差）均值为 304.9988，且在梯度下降的 250 和 1,000 次迭代后得到了 KL 散度。
- en: 'Now, visualize the number of dimensions in the returned dataset:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，可视化返回数据集中的维度数量：
- en: '[PRE6]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, we have successfully reduced the 784 dimensions down to 2 for visualization,
    so what does it look like?
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所以，我们成功地将 784 个维度降到了 2 维进行可视化，那么它看起来是什么样的呢？
- en: 'Create a scatter plot of the two-dimensional data produced by the model:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型生成的二维数据的散点图：
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.8: 2D representation of MNIST (no labels).'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.8：MNIST 的二维表示（无标签）。'
- en: '](img/C12626_06_08.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_08.jpg)'
- en: 'Figure 6.8: 2D representation of MNIST (no labels).'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.8：MNIST 的二维表示（无标签）。
- en: In [*Figure 6*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor144)*.8*, we can
    see that we have represented the MNIST data in two dimensions, but we can also
    see that it seems to be grouped together. There are a number of different clusters
    or clumps of data congregated together and separated from other clusters by some
    white space. There also seem to be about nine different groups of data. All these
    observations suggest that there is some relationship within and between the individual
    clusters.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 [*图 6*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor144)*.8* 中，我们可以看到我们已经将
    MNIST 数据表示为二维，但我们也可以看到它似乎被分组在一起。这里有很多不同的数据聚类或团块，它们与其他聚类通过一些空白区域分开。似乎大约有九个不同的数据组。所有这些观察结果表明，个别聚类之间以及聚类内可能存在某种关系。
- en: 'Plot the two-dimensional data grouped by the corresponding image label, and
    use markers to separate the individual labels. Along with the data, add the image
    labels to the plot to investigate the structure of the embedded data:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制按图像标签分组的二维数据，并使用标记区分各个标签。结合数据，在图上添加图像标签，以研究嵌入数据的结构：
- en: '[PRE9]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.9: 2D representation of MNIST with labels.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.9：带标签的 MNIST 二维表示。'
- en: '](img/C12626_06_09.jpg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_09.jpg)'
- en: 'Figure 6.9: 2D representation of MNIST with labels.'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.9：带标签的 MNIST 二维表示。
- en: '[*Figure*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor145) *6.9* is very interesting!
    We can see here that the clusters correspond with each of the different image
    classes (zero through nine) within the dataset. In an unsupervised fashion, that
    is, without providing the labels in advance, a combination of PCA and t-SNE has
    been able to separate and group the individual classes within the MNIST dataset.
    What is particularly interesting is that there seems to be some confusion within
    the data regarding the number four images and the number nine images, as well
    as the five and three images; the two clusters somewhat overlap. This makes sense
    if we look at the number nine and number four PCA images extracted from *Step
    4*, *Exercise 24*, *t-SNE MNIST*:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[*图*](C12626_06_ePub_Final_SZ.xhtml#_idTextAnchor145) *6.9* 非常有趣！我们可以看到，数据集中的不同图像类别（从零到九）对应着不同的聚类。在无监督的情况下，即没有提前提供标签，PCA
    和 t-SNE 的结合成功地将 MNIST 数据集中的各个类别分开并进行分组。特别有趣的是，数据中似乎存在一些混淆，尤其是数字四和数字九的图像，以及数字五和数字三的图像；这两个聚类有些重叠。如果我们查看从
    *步骤 4*、*练习 24* 和 *t-SNE MNIST* 提取的数字九和数字四的 PCA 图像，这就可以理解：'
- en: '![Figure 6.10: PCA images of nine.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.10：数字九的 PCA 图像。'
- en: '](img/C12626_06_10.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_10.jpg)'
- en: 'Figure 6.10: PCA images of nine.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.10：数字九的 PCA 图像。
- en: 'They do, in fact, look quite similar; perhaps it is due to the uncertainty
    in the shape of the number four. Looking at the image that follows, we can see
    in the four on the left-hand side that the two vertical lines almost join, while
    the four on the right-hand side has the two lines parallel:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它们实际上看起来非常相似；也许是因为数字四的形状存在不确定性。看一下接下来的图像，我们可以看到左侧的数字四，两条垂直线几乎连接，而右侧的数字四则是两条平行线：
- en: '![Figure 6.11: Shape of number four'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.11：数字四的形状'
- en: '](img/C12626_06_11.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_11.jpg)'
- en: 'Figure 6.11: Shape of number four'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.11：数字四的形状
- en: The other interesting feature to note in *Figure 6.9* are the edge cases, better
    shown in color in the Jupyter notebooks. We can see around the edges of each cluster
    that some samples would be misclassified in the traditional supervised learning
    sense but represent samples that may have more in common with other clusters than
    their own. Let's take a look at an example; there are a number of samples of the
    number three that are quite far from the correct cluster.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 *图 6.9* 中需要注意的另一个有趣特征是边缘案例，这在 Jupyter 笔记本中通过颜色显示得更清楚。我们可以看到每个聚类的边缘附近，一些样本在传统的监督学习中会被误分类，但它们实际上可能与其他聚类更为相似。让我们看一个例子；有许多数字三的样本，它们离正确的聚类相当远。
- en: 'Get the index of all the number threes in the dataset:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据集中所有数字三的索引：
- en: '[PRE10]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.12: Index of threes in the dataset.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.12：数据集中三的索引。'
- en: '](img/C12626_06_12.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_12.jpg)'
- en: 'Figure 6.12: Index of threes in the dataset.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.12：数据集中三的索引。
- en: 'Find the threes that were plotted with an `x` value of less than 0:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找 `x` 值小于 0 的三类数据：
- en: '[PRE11]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.13: The threes with x value less than zero.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.13：x 值小于零的三类数据。'
- en: '](img/C12626_06_13.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_13.jpg)'
- en: 'Figure 6.13: The threes with x value less than zero.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.13：x值小于零的三类
- en: 'Display the coordinates to find one that is reasonably far from the three cluster:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示坐标以找到一个合理远离三的聚类的样本：
- en: '[PRE12]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.14: Coordinates away from the three cluster'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.14：远离三的聚类的坐标'
- en: '](img/C12626_06_14.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_06_14.jpg)'
- en: 'Figure 6.14: Coordinates away from the three cluster'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.14：远离三的聚类的坐标
- en: 'Chose a sample with a reasonably high negative value as an `x` coordinate.
    In this example, we will select the fourth sample, which is sample 10\. Display
    the image for the sample:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个具有较高负值的`x`坐标的样本。在这个示例中，我们将选择第四个样本，即样本10。显示该样本的图像：
- en: '[PRE13]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.15: Image of sample ten'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15：样本十的图像'
- en: '](img/C12626_06_15.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_15.jpg)'
- en: 'Figure 6.15: Image of sample ten'
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.15：样本十的图像
- en: Looking at this sample image and the corresponding t-SNE coordinates, approximately
    (-8, 47), it is not surprising that this sample lies near the cluster of eights
    and fives as there are quite a few features that are common to both of those numbers
    in this image. In this example, we applied a simplified SNE, demonstrating some
    of its efficiencies as well as possible sources of confusion and the output of
    unsupervised learning.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 看着这个示例图像及其对应的t-SNE坐标，大约是(-8, 47)，不难理解这个样本为何会靠近八和五的聚类，因为在这张图像中，八和五这两个数字有很多相似的特征。在这个示例中，我们应用了简化版的SNE，展示了它的一些高效性以及可能的混淆源和无监督学习的输出结果。
- en: Note
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Even with providing a random number seed, t-SNE does not guarantee identical
    outputs each time it is executed because it is based upon selection probabilities.
    As such, you may notice some differences in the specifics between the example
    provided in the content and your implementation. While the specifics may differ,
    the overall principals and techniques still apply. From a real-world application
    perspective, it is recommended that the process is repeated multiple times to
    discern the significant information from the data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 即使提供了随机数种子，t-SNE也不能保证每次执行时输出完全相同，因为它基于选择概率。因此，您可能会注意到，内容中提供的示例与您的实现之间在细节上有所不同。尽管具体细节可能有所不同，但整体原则和技术依然适用。从实际应用的角度来看，建议多次重复该过程，以从数据中辨别出重要信息。
- en: 'Activity 12: Wine t-SNE'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 12：葡萄酒 t-SNE
- en: In this activity, we will reinforce our knowledge of t-SNE using the Wine dataset.
    By completing this activity, you will be able to build-SNE models for your own
    custom applications. The Wine dataset ([https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine))
    is a collection of attributes regarding the chemical analysis of wine from Italy
    from three different producers, but the same type of wine for each producer. This
    information could be used as an example to verify the validity of a bottle of
    wine made from the grapes from a specific region in Italy. The 13 attributes are
    Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids,
    Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted
    wines, and Proline.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将通过使用葡萄酒数据集来巩固我们对t-SNE的理解。完成此活动后，您将能够为自己的自定义应用程序构建SNE模型。葡萄酒数据集 ([https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine))
    是关于来自意大利三家不同生产商的葡萄酒化学分析的属性集合，但每个生产商的葡萄酒类型相同。此信息可作为示例，用于验证瓶装葡萄酒是否来自意大利特定地区的葡萄。13个属性包括：酒精、苹果酸、灰分、灰的碱度、镁、总酚、类黄酮、非类黄酮酚、前花青素、颜色强度、色调、稀释酒的OD280/OD315比值，以及脯氨酸。
- en: Each sample contains a class identifier (1 – 3).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本包含一个类别标识符（1 – 3）。
- en: Note
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本数据集来源于 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)，可以从
    [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12)
    下载。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [http://archive.ics.uci.edu/ml]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: 'These steps will help you complete the activity:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助您完成活动：
- en: Import `pandas`, `numpy`, `matplotlib`, and the `t-SNE` and `PCA` models from
    scikit-learn.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`matplotlib`以及scikit-learn中的`t-SNE`和`PCA`模型。
- en: Load the Wine dataset using the `wine.data` file included in the accompanying
    source code and display the first five rows of data.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随附源代码中的`wine.data`文件加载Wine数据集，并显示前五行数据。
- en: Note
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can delete columns within Pandas DataFrames through use of the `del` keyword.
    Simply pass `del` the DataFrame and the selected column within square root.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以通过使用`del`关键字删除Pandas DataFrame中的列。只需将`del`和所选列放在方括号内。
- en: The first column contains the labels; extract this column and remove it from
    the dataset.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一列包含标签；提取该列并将其从数据集中移除。
- en: Execute PCA to reduce the dataset to the first six components.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行PCA，将数据集降至前六个主成分。
- en: Determine the amount of variance within the data described by these six components.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定这六个成分所描述的数据中的方差量。
- en: Create a t-SNE model using a specified random state and a `verbose` value of
    1.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的随机状态和`verbose`值为1创建t-SNE模型。
- en: Fit the PCA data to the t-SNE model.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将PCA数据拟合到t-SNE模型中。
- en: Confirm that the shape of the t-SNE fitted data is two dimensional.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认t-SNE拟合数据的形状是二维的。
- en: Create a scatter plot of the two-dimensional data.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建二维数据的散点图。
- en: Create a secondary scatter plot of the two-dimensional data with the class labels
    applied to visualize any clustering that may be present.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个二位数据的散点图，并应用类标签，以可视化可能存在的聚类。
- en: 'At the end of this activity, you will have constructed a t-SNE visualization
    of the Wine dataset described using its six components and identified some relationships
    in the location of the data within the plot. The final plot will look similar
    to the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动结束时，您将构建一个基于Wine数据集六个成分的t-SNE可视化，并识别图中数据位置的一些关系。最终的图将类似于以下内容：
- en: '![Figure 6.16: The expected plot'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.16：预期的图示'
- en: '](img/C12626_06_16.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_16.jpg)'
- en: 'Figure 6.16: The expected plot'
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.16：预期的图示
- en: Note
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 345.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第345页找到。
- en: In this section, we covered the basics of generating SNE plots. The ability
    to represent high-dimensional data in low-dimensional space is critical, especially
    for developing a thorough understanding of the data at hand. Occasionally, these
    plots can be tricky to interpret as the exact relationships are sometimes contradictory,
    leading at times to misleading structures.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了生成SNE图示的基础知识。在低维空间中表示高维数据的能力至关重要，特别是对于深入理解手头数据至关重要。有时，这些图示的解释可能会有些棘手，因为确切的关系有时会相互矛盾，导致误导性结构。
- en: Interpreting t-SNE Plots
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释t-SNE图示
- en: Now that we are able to use t-distributed SNE to visualize high-dimensional
    data, it is important to understand the limitations of such plots and what aspects
    are important in interpreting and generating them. In this section of the chapter,
    we will highlight some of the important features of t-SNE and demonstrate how
    care should be taken when using the visualization technique.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用t-分布SNE来可视化高维数据，重要的是理解此类图示的局限性，以及在解释和生成这些图示时需要关注的方面。在本章的这一部分，我们将突出t-SNE的一些重要特性，并演示在使用该可视化技术时应注意的事项。
- en: Perplexity
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 困惑度
- en: As described in the introduction to t-SNE, the perplexity values specify the
    number of nearest neighbors to be used in computing the conditional probability.
    The selection of this value can make a significant difference to the end result;
    with a low value of perplexity, local variations in the data dominate because
    a small number of samples are used in the calculation. Conversely, a large value
    of perplexity considers more global variations as many more samples are used in
    the calculation. Typically, it is worth trying a range of different values to
    investigate the effect of perplexity. Again, values between 5 and 50 tend to work
    quite well.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如t-SNE介绍中所述，困惑度值指定用于计算条件概率的最近邻数量。选择该值对最终结果有显著影响；当困惑度值较低时，数据中的局部变化占主导，因为计算中使用的样本数量较少。相反，较大的困惑度值会考虑更多的全局变化，因为使用了更多的样本进行计算。通常，尝试不同的值以调查困惑度的效果是值得的。通常，困惑度值在5到50之间的效果较好。
- en: 'Exercise 25: t-SNE MNIST and Perplexity'
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习25：t-SNE MNIST与困惑度
- en: 'In this exercise, we will try a range of different values for perplexity and
    look at the effect in the visualization plot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将尝试不同的困惑度值，并查看它在可视化图中的效果：
- en: 'Import `pickle`, `numpy`, `matplotlib`, and `PCA` and `t-SNE` from scikit-learn:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`、`matplotlib`，以及来自scikit-learn的`PCA`和`t-SNE`：
- en: '[PRE14]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the MNIST dataset:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集：
- en: Note
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE15]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Using PCA, select only the first 30 components of variance from the image data:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PCA，从图像数据中选择前30个方差成分：
- en: '[PRE16]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this exercise, we are investigating the effect of perplexity on the t-SNE
    manifold. Iterate through a model/plot loop with a perplexity of 3, 30, and 300:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们正在研究困惑度对t-SNE流形的影响。通过一个困惑度为3、30和300的模型/绘图循环进行迭代：
- en: '[PRE17]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.17: Iterating through a model'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.17：通过模型进行迭代'
- en: '](img/C12626_06_17.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_17.jpg)'
- en: 'Figure 6.17: Iterating through a model'
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.17：通过模型进行迭代
- en: Note the KL divergence in each of the three different perplexity values, along
    with the increase in the average standard deviation (variance). Looking at the
    three following t-SNE plots with class labels, we can see that with a low perplexity
    value, the clusters are nicely contained with relatively few overlaps. However,
    there is almost no space between the clusters. As we increase the perplexity,
    the space between the clusters improves with reasonably clear distinctions at
    a perplexity of 30\. As the perplexity increases to 300, we can see that the clusters
    of eight and five, along with nine, four, and seven, are starting to converge.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意三个不同困惑度值下的KL散度，以及平均标准差（方差）的增加。通过查看以下三个带有类标签的t-SNE图，我们可以看到，当困惑度值较低时，聚类被很好地分隔，重叠较少。然而，聚类之间几乎没有空间。当我们增加困惑度时，聚类之间的空间改善，在困惑度为30时有相对清晰的区分。随着困惑度增加到300，我们可以看到8和5的聚类，以及9、4和7的聚类，开始趋于融合。
- en: 'Start with a low perplexity value:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从低困惑度值开始：
- en: '![Figure 6.18: Plot of low perplexity value'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.18：低困惑度值的绘图'
- en: '](img/C12626_06_18.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_18.jpg)'
- en: 'Figure 6.18: Plot of low perplexity value'
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.18：低困惑度值的绘图
- en: 'Increasing the perplexity by a factor of 10 shows much clearer clusters:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将困惑度增加10倍后，聚类变得更加清晰：
- en: '![Figure 6.19: Plot after increasing perplexity by a factor of 10'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.19：困惑度增加10倍后的绘图'
- en: '](img/C12626_06_19.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_19.jpg)'
- en: 'Figure 6.19: Plot after increasing perplexity by a factor of 10'
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.19：困惑度增加10倍后的绘图
- en: 'By increasing the perplexity to 300, we start to merge more of the labels together:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 将困惑度增加到300后，我们开始将更多的标签合并在一起：
- en: '![Figure 6.20: Increasing the perplexity value to 300'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.20：将困惑度值增加到300'
- en: '](img/C12626_06_20.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_20.jpg)'
- en: 'Figure 6.20: Increasing the perplexity value to 300'
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.20：将困惑度值增加到300
- en: In this exercise, we developed our understanding of the effect of perplexity
    and the sensitivity of this value to the overall result. A small perplexity value
    can lead to a more homogenous mix of locations with very little space between
    them. Increasing the perplexity separates the clusters more effectively, but an
    excessive value leads to overlapping clusters.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们加深了对困惑度影响及其对整体结果敏感性的理解。较小的困惑度值可能导致位置之间的混合更加均匀，且它们之间的空间非常小。增加困惑度可以更有效地分离聚类，但过大的值会导致聚类重叠。
- en: 'Activity 13: t-SNE Wine and Perplexity'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动13：t-SNE葡萄酒和困惑度
- en: In this activity, we will use the Wine dataset to further reinforce the influence
    of perplexity on the t-SNE visualization process. In this activity, we are trying
    to determine whether we can identify the source of the wine based on its chemical
    composition. The t-SNE process provides an effective means of representing and
    possibly identifying the sources.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将使用葡萄酒数据集进一步强化困惑度对t-SNE可视化过程的影响。在这个活动中，我们尝试确定是否可以根据葡萄酒的化学成分识别其来源。t-SNE过程提供了一种有效的表示方法，可能帮助识别来源。
- en: Note
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity1](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12)3.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自于[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)，可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity1](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12)3下载。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [http://archive.ics.uci.edu/ml]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: Import `pandas`, `numpy`, `matplotlib`, and the `t-SNE` and `PCA` models from
    scikit-learn.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`matplotlib`以及来自scikit-learn的`t-SNE`和`PCA`模型。
- en: Load the Wine dataset and inspect the first five rows.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载Wine数据集并检查前五行数据。
- en: The first column provides the labels; extract these from the DataFrame and store
    them in a separate variable. Ensure that the column is removed from the DataFrame.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一列提供了标签；从DataFrame中提取这些标签并存储在单独的变量中。确保该列从DataFrame中移除。
- en: Execute PCA on the dataset and extract the first six components.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集执行PCA操作，并提取前六个主成分。
- en: Construct a loop that iterates through the perplexity values (1, 5, 20, 30,
    80, 160, 320). For each loop, generate a t-SNE model with the corresponding perplexity
    and print a scatter plot of the labeled wine classes. Note the effect of different
    perplexity values.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个循环，遍历不同的困惑度值（1、5、20、30、80、160、320）。对于每次循环，生成一个带有相应困惑度的t-SNE模型，并打印带标签的葡萄酒类别的散点图。注意不同困惑度值的效果。
- en: By the end of this activity, you will have generated a two-dimensional representation
    of the Wine dataset and inspected the resulting plot for clusters or groupings
    of data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动结束时，你将生成Wine数据集的二维表示，并检查生成的图表，寻找数据的聚类或分组。
- en: Note
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 348.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第348页找到。
- en: Iterations
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迭代次数
- en: The final parameter we will experimentally investigate is that of iterations,
    which, as per our investigation in autoencoders, is simply the number of training
    epochs to apply to gradient descent. Thankfully, the number of iterations is a
    reasonably simple parameter to adjust and often requires only a certain amount
    of patience as the position of the points in the low-dimensional space stabilize
    in their final locations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实验研究的最后一个参数是迭代次数，正如我们在自动编码器中的研究所示，这只是应用于梯度下降的训练轮次数量。幸运的是，迭代次数是一个相对简单的参数，通常只需要一定的耐心，因为低维空间中点的位置会在最终位置上稳定下来。
- en: 'Exercise 26: t-SNE MNIST and Iterations'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 26：t-SNE MNIST 和迭代次数
- en: 'In this exercise, we will look at the influence of a range of different iteration
    parameters applied to the t-SNE model and highlight some indicators that perhaps
    more training is required. Again, the value of these parameters is highly dependent
    on the dataset and the volume of data available for training. Again, we will use
    MNIST in this example:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将观察一系列不同的迭代参数对t-SNE模型的影响，并突出一些可能需要更多训练的指示符。再次强调，这些参数的值在很大程度上依赖于数据集以及可用于训练的数据量。在本示例中，我们仍然使用MNIST数据集：
- en: 'Import `pickle`, `numpy`, `matplotlib`, and `PCA` and `t-SNE` from scikit-learn:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`、`matplotlib`以及来自scikit-learn的PCA和t-SNE：
- en: '[PRE18]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load the MNIST dataset:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集：
- en: Note
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE19]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using PCA, select only the first 30 components of variance from the image data:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PCA，从图像数据中仅选择前30个方差成分：
- en: '[PRE20]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this exercise, we are investigating the effect of iterations on the t-SNE
    manifold. Iterate through a model/plot loop with iteration and iteration with
    progress values of `250`, `500`, and `1000`:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将研究迭代次数对t-SNE流形的影响。通过模型/绘图循环，进行迭代，迭代次数分别为`250`、`500`和`1000`：
- en: '[PRE21]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Plot the results:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE22]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A reduced number of iterations limits the extent to which the algorithm can
    find relevant neighbors, leading to ill-defined clusters:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迭代次数较少会限制算法找到相关邻居的程度，导致聚类不清晰：
- en: '![Figure 6.21: Plot after 250 iterations'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.21：250次迭代后的绘图]'
- en: '](img/C12626_06_21.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_21.jpg)'
- en: 'Figure 6.21: Plot after 250 iterations'
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.21：250次迭代后的绘图
- en: 'Increasing the number of iterations provides the algorithm with sufficient
    time to adequately project the data:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 增加迭代次数为算法提供了足够的时间来充分投影数据：
- en: '![Figure 6.22: Plot after increasing the iterations to 500'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.22：将迭代次数增加到500后的绘图'
- en: '](img/C12626_06_22.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_22.jpg)'
- en: 'Figure 6.22: Plot after increasing the iterations to 500'
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.22：将迭代次数增加到500后的绘图
- en: 'Once the clusters have settled, increased iterations have an extremely small
    effect and essentially lead to increased training time:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦簇群稳定，增加迭代次数的影响非常小，基本上只是增加了训练时间：
- en: '![Figure 6.23: Plot after 1,000 iterations'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.23：1000次迭代后的绘图'
- en: '](img/C12626_06_23.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_06_23.jpg)'
- en: 'Figure 6.23: Plot after 1,000 iterations'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.23：1000次迭代后的绘图
- en: Looking at the previous plots, we can see that the cluster positions with iteration
    values of 500 and 1,000 are stable and relatively unchanged between the plots.
    The most interesting plot is that of an iteration value of 250, where it seems
    as though the clusters are still in a process of motion, making their way to the
    final positions. As such, there is sufficient evidence to suggest an iteration
    value of 500 is sufficient.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的绘图来看，我们可以看到，迭代次数为500和1000时，簇群的位置稳定且在各个图之间几乎没有变化。最有趣的图是迭代次数为250的图，其中簇群似乎仍在移动过程中，正在向最终位置靠拢。因此，有充分的证据表明，500次迭代足以。
- en: 'Activity 14: t-SNE Wine and Iterations'
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动14：t-SNE葡萄酒与迭代次数
- en: In this activity, we will investigate the effect of the number of iterations
    on the visualization of the Wine dataset. This is a process that's commonly used
    during the exploration phase of data processing, cleaning, and understanding the
    relationships in the data. Depending on the dataset and the type of analysis,
    we may need to try a number of different iterations, such as those completed in
    this activity.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将研究迭代次数对葡萄酒数据集可视化的影响。这是数据处理、清洗和理解数据关系的探索阶段中常用的一个过程。根据数据集和分析类型，我们可能需要尝试多种不同的迭代次数，就像本次活动中所做的那样。
- en: Note
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/).
    It can be downloaded from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity1](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12)4.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集来自 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)。它可以从
    [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity1](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson06/Activity12)4
    下载。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [http://archive.ics.uci.edu/ml]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: 'These steps will help you complete the activity:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成活动：
- en: Import `pandas`, `numpy`, `matplotlib`, and the `t-SNE` and `PCA` models from
    scikit-learn.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`、`matplotlib`，以及从scikit-learn导入`t-SNE`和`PCA`模型。
- en: Load the Wine dataset and inspect the first five rows.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载葡萄酒数据集并检查前五行数据。
- en: The first column provides the labels; extract these from the DataFrame and store
    them in a separate variable. Ensure that the column is removed from the DataFrame.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一列提供了标签；从DataFrame中提取这些标签并存储到一个单独的变量中。确保将该列从DataFrame中删除。
- en: Execute PCA on the dataset and extract the first six components.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集执行PCA，并提取前六个主成分。
- en: Construct a loop that iterates through the iteration values (`250`, `500`, `1000`).
    For each loop, generate a t-SNE model with the corresponding number of iterations
    and an identical number of iterations without progress values.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个循环，遍历迭代值（`250`、`500`、`1000`）。对于每个循环，生成一个具有相应迭代次数的t-SNE模型，并生成一个没有进度值的相同迭代次数的模型。
- en: Construct a scatter plot of the labeled wine classes. Note the effect of different
    iteration values.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建标记葡萄酒类别的散点图。注意不同迭代值的影响。
- en: By completing this activity, we will have investigated the effect of modifying
    the iteration parameter of the model. This is an important parameter in ensuring
    that the data has settled into a somewhat final position in the low-dimensional
    space.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成本活动，我们将研究修改模型迭代参数的效果。这是确保数据在低维空间中稳定在某个最终位置的重要参数。
- en: Note
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 353.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第353页找到。
- en: Final Thoughts on Visualizations
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于可视化的最终思考
- en: As we conclude our chapter on t-Distributed Stochastic Neighbor Embeddings,
    there are a couple of important aspects to note regarding the visualizations.
    The first is that the size of the clusters or the relative space between clusters
    may not actually provide any real indication of proximity. As we discussed earlier
    in the chapter, a combination of Gaussian and Student's t-distributions is used
    to represent high-dimensional data in a low-dimensional space. As such, there
    is no guarantee of a linear relationship in distance, as t-SNE balances the positions
    of localized and global data structures. The actual distance between points in
    local structures may be visually very close within the representation, but still
    might be some distance away in high-dimensional space.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们总结关于t-分布随机邻域嵌入（t-SNE）这一章节时，有几个关于可视化的重要方面需要注意。首先，聚类的大小或聚类之间的相对空间可能并不能真正反映接近度。正如我们在本章前面讨论的，结合高斯分布和学生t分布被用来在低维空间中表示高维数据。因此，距离之间的线性关系并不能得到保证，因为t-SNE平衡了局部和全局数据结构的位置。局部结构中点之间的实际距离在可视化表示中可能看起来非常接近，但在高维空间中可能仍然存在一定的距离。
- en: This property also has additional consequences in that sometimes, random data
    can be present as if it had some structure, and that it is often required to generate
    multiple visualizations using differing values of perplexity, learning rate, number
    of iterations, and random seed values.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特性还有一个附带的后果，那就是有时随机数据看起来像是具有某种结构，并且通常需要生成多个可视化图像，使用不同的困惑度、学习率、迭代次数和随机种子值。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to t-Distributed Stochastic Neighbor Embeddings
    as a means of visualizing high-dimensional information that may have been produced
    from prior processes such as PCA or autoencoders. We discussed the means by which
    t-SNEs produce this representation and generated a number of them using the MNIST
    and Wine datasets and scikit-learn. In this chapter, we were able to see some
    of the power of unsupervised learning because PCA and t-SNE were able to cluster
    the classes of each image without knowing the ground truth result. In the next
    chapter, we will build on this practical experience as we look into the applications
    of unsupervised learning, including basket analysis and topic modeling.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了t-分布随机邻域嵌入（t-SNE）作为可视化高维信息的一种方法，这些信息可能来自先前的过程，如PCA或自编码器。我们讨论了t-SNE如何生成这种表示，并使用MNIST和Wine数据集以及scikit-learn生成了多个表示。在这一章中，我们能够看到无监督学习的一些强大之处，因为PCA和t-SNE能够在不知道真实标签的情况下对每张图片的类别进行聚类。在下一章中，我们将基于这次实践经验，探讨无监督学习的应用，包括篮子分析和主题建模。
