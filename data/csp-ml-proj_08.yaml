- en: Handwritten Digit Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手写数字识别
- en: We have looked at how to build recommendation models using multi-class classification
    models. In this chapter, we are going to expand our knowledge and experience of
    building multi-class classification models with an image dataset. Image recognition
    is a well-known **machine learning** (**ML**) problem and is one of the topics
    that are actively being researched. One image recognition problem that has high
    applicability to our lives is recognizing handwritten letters and digits. A good
    example of the application of a handwritten image recognition system is the address
    recognition system that is used at post offices. Using such a technology, post
    offices can now automatically and more quickly identify addresses that are written
    by hand, and expedite and improve overall mailing services.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何使用多类分类模型构建推荐模型。在本章中，我们将扩展我们构建多类分类模型的知识和经验，使用图像数据集。图像识别是一个众所周知的**机器学习**（**ML**）问题，也是当前积极研究的一个主题。一个与我们的生活高度相关的图像识别问题就是识别手写字母和数字。手写图像识别系统的应用实例之一就是邮局使用的地址识别系统。利用这项技术，邮局现在可以自动且更快地识别手写的地址，从而加速和提高整体邮寄服务。
- en: In this chapter, we are going to build machine learning models for handwritten
    digit recognition. We are going to start with a dataset that contains grayscale
    pixel-by-pixel information about over 40,000 handwritten digit images. We will
    look at the distributions of the values in each pixel and discuss how sparse this
    grayscale image dataset is. Then, we are going to discuss when and how to apply
    dimensionality reduction techniques, more specifically **Principal Component Analysis**
    (**PCA**), and how we can benefit from this technique for our image recognition
    project. We will be exploring different learning algorithms, such as logistic
    regression and Naive Bayes, and will also cover how to build an **Artificial Neural
    Network** (**ANN**), which forms the backbone of deep learning technologies, using
    the Accord.NET framework. Then, we will compare the prediction performances of
    these ML models by looking at various evaluation metrics, and discuss which model
    performed the best for the handwritten digit recognition project.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建用于手写数字识别的机器学习模型。我们将从一个包含超过40,000个手写数字图像的灰度像素信息的数据集开始。我们将查看每个像素中值的分布，并讨论这个灰度图像数据集的稀疏性。然后，我们将讨论何时以及如何应用降维技术，特别是**主成分分析**（**PCA**），以及我们如何从这项技术中受益于我们的图像识别项目。我们将探索不同的学习算法，例如逻辑回归和朴素贝叶斯，并也将介绍如何使用Accord.NET框架构建**人工神经网络**（**ANN**），这是深度学习技术的核心。然后，我们将通过查看各种评估指标来比较这些机器学习模型的预测性能，并讨论哪个模型在手写数字识别项目中表现最佳。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Problem definition for the handwritten digit recognition project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写数字识别项目的问题定义
- en: Data analysis for an image dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数据集的数据分析
- en: Feature engineering and dimensionality reduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程与降维
- en: ML models for handwritten digit recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于手写数字识别的机器学习模型
- en: Evaluating multi-class classification models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估多类分类模型
- en: Problem definition
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义
- en: Image recognition technology can be applied to, and can be found easily in,
    our daily lives. At post offices, image recognition systems are used to programmatically
    understand addresses that are written by hand. Social network services, such as
    Facebook, use image recognition technology for automatic people tag suggestions,
    for instance, when you want to tag people in your photos. Also, as briefly mentioned
    in the very first chapter of this book, Microsoft's Kinect uses image recognition
    technology for its motion-sensing games. Of these real-life applications, we are
    going to experiment with building a handwritten digit recognition system. As you
    can imagine, such digit image recognition models and systems can be used for automated
    handwritten address recognition at post offices. Before we had this ability to
    teach machines to identify and understand handwritten digits, people had to go
    through and look at each letter to find out the destination and the origin of
    individual letters. However, now that we can train machines to understand handwritten
    addresses, mailing processes have become much easier and faster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像识别技术可以应用于我们的日常生活中，并且很容易找到。在邮局，图像识别系统被用来程序化地理解手写的地址。社交网络服务，如Facebook，使用图像识别技术进行自动的人脸标签建议，例如，当你想在照片中标记人时。此外，正如本书第一章简要提到的，微软的Kinect使用图像识别技术进行动作感应游戏。在这些实际应用中，我们将尝试构建一个手写数字识别系统。正如你可以想象的，这样的数字图像识别模型和系统可以用于邮局自动的手写地址识别。在我们有能力教会机器识别和理解手写数字之前，人们必须逐个查看信件以找出每个信件的目的地和起源。然而，现在我们能够训练机器理解手写地址，邮寄过程变得更加容易和快捷。
- en: In order to build a handwritten digit recognition model, we are going to use
    the **MNIST** dataset, which has over 60,000 handwritten digit images. The **MNIST**
    dataset contains 28 x 28 images that are in grayscale. You can find more information
    at this link: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
    For this project, we will be using a cleaned and processed MNIST dataset that
    can be found at this link: [https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data).
    With this data, we will first look at how the digits are distributed across the
    dataset, and how sparse the feature set is. Then, we are going to use PCA for
    dimensionality reduction and to visualize the differences in the distributions
    of features among different classes. With this PCA-transformed data, we are going
    to train a few ML models to compare their prediction performances. On top of logistic
    regression and Naive Bayes classification algorithms, we are going to experiment
    with the ANN, as it is known to work well for image datasets. We will look at
    accuracy, precision versus recall, and **area under the curve** (**AUC**), to
    compare the prediction performances among different machine learning models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个手写数字识别模型，我们将使用**MNIST**数据集，该数据集包含超过60,000张手写数字图像。**MNIST**数据集包含28 x 28像素的灰度图像。您可以在以下链接中找到更多信息：[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。对于这个项目，我们将使用一个清洗和处理的MNIST数据集，您可以在以下链接中找到：[https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data)。有了这些数据，我们首先将查看数字在数据集中的分布情况，以及特征集的稀疏程度。然后，我们将使用PCA进行降维，并可视化不同类别之间特征分布的差异。使用这个PCA转换后的数据，我们将训练几个机器学习模型来比较它们的预测性能。除了逻辑回归和朴素贝叶斯分类算法之外，我们还将尝试使用人工神经网络（ANN），因为它已知在图像数据集上表现良好。我们将查看准确率、精确率与召回率的比较，以及**曲线下面积**（**AUC**），以比较不同机器学习模型的预测性能。
- en: 'To summarize our problem definition for the handwritten digit recognition project:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们对手写数字识别项目的定义问题：
- en: What is the problem? We need a handwritten digit recognition model that can
    classify each handwritten image into a corresponding digit class, so that it can
    be used for applications such as the address recognition system.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题是什么？我们需要一个手写数字识别模型，可以将每个手写图像分类到相应的数字类别，以便它可以用于地址识别系统等应用。
- en: Why is it a problem? Without such a model, it takes an enormous amount of human
    labor to identify and organize letters by addresses. If we have a technology that
    can recognize handwritten digits that are written on letters, it can significantly
    reduce the amount of human labor required for the same task.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么这是一个问题？没有这样的模型，识别和组织信件地址需要大量的人工劳动。如果我们有一种技术可以识别写在信件上的手写数字，它可以显著减少完成相同任务所需的人工劳动量。
- en: What are some of the approaches to solving this problem? We are going to use
    publicly available data that contains numerous examples of handwritten digit images.
    With this data, we are going to build machine learning models that can classify
    each image into one of 10 digits.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的方法有哪些？我们将使用包含大量手写数字图像示例的公开数据。利用这些数据，我们将构建能够将每张图像分类为10个数字之一的机器学习模型。
- en: What are the success criteria? We want a machine learning model that accurately
    classifies each image with the corresponding digit. Since this model will eventually
    be used for address recognition, we want high precision rates, even if we have
    to sacrifice recall rates.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功的标准是什么？我们希望有一个机器学习模型能够准确地将每张图像与相应的数字对应起来。由于这个模型最终将用于地址识别，我们希望有高精确率，即使我们必须牺牲召回率。
- en: Data analysis for the image dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像数据集的数据分析
- en: Let's start looking into this image dataset. As mentioned in the previous section,
    we will be using the data from this link: [https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data).
    You can download the `train.csv` data from the link and store it in a place from
    where you can load it into your C# environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从研究这个图像数据集开始。如前所述，我们将使用以下链接中的数据：[https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data)。您可以从链接下载`train.csv`数据，并将其存储在一个可以从中加载到您的C#环境中的位置。
- en: Target variable distribution
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标变量分布
- en: 'The first thing we are going to look at is the distribution of the target variables.
    Our target variable is encoded in the `label` column, which can take values between
    0 and 9, and represents the digit that the image belongs to. The following code
    snippet shows how we aggregated the data by the target variable and counted the
    number of examples for each digit:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要关注的是目标变量的分布。我们的目标变量编码在`label`列中，可以取0到9之间的值，代表图像所属的数字。以下代码片段展示了我们如何根据目标变量聚合数据，并计算每个数字的示例数量：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As in other chapters, we used the `AggregateRowsBy` method in Deedle''s data
    frame to aggregate data by the target variable, `label`, count the number of records
    in each label, and sort by the counts. Similar to previous chapters, we are using
    the `DataBarBox` class to display a bar plot of target variable distributions
    in the dataset. The following is the bar plot that you will see when you run this
    code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他章节一样，我们在Deedle的数据框中使用了`AggregateRowsBy`方法，按目标变量`label`聚合数据，计算每个标签的记录数，并按计数排序。与之前的章节类似，我们使用`DataBarBox`类来显示数据集中目标变量分布的条形图。以下是在运行此代码时您将看到的条形图：
- en: '![](img/00119.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: 'In the console output, you will see the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台输出中，您将看到以下内容：
- en: '![](img/00120.gif)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.gif)'
- en: As you can see from the bar plot and this console output, the digit `1`, occurs
    the most in the dataset, and the digit `5`, occurs the least. However, there is
    no one class that takes the majority of the examples in the dataset, and the target
    variables are pretty well balanced and spread across different classes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如从条形图和这个控制台输出中可以看出，数字`1`在数据集中出现得最多，而数字`5`出现得最少。然而，数据集中没有一类占大多数示例，目标变量在各个类别中分布得相当均衡。
- en: Handwritten digit images
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手写数字图像
- en: 'Before we start looking into the feature set, let''s first look at actual images
    of handwritten digits. In each record of our dataset, we have the grayscale values
    for 784 pixels for each of the 28 x 28 images. In order to build an image from
    this flattened dataset, we need to first convert each array of 784-pixel values
    into a two-dimensional array. The following code shows the helper function we
    wrote to create an image from a flattened array:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始研究特征集之前，让我们先看看手写数字的实际图像。在我们的数据集的每一条记录中，我们都有28 x 28图像中每个图像的784个像素的灰度值。为了从这个扁平化的数据集中构建图像，我们首先需要将每个784像素值的数组转换为二维数组。以下代码展示了我们编写的用于从扁平化数组创建图像的辅助函数：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see from this code, it first initializes a two-dimensional integer
    array, `pixelData`, which is going to store the pixel data. Since we know each
    image is a 28 x 28 image, we are going to take the first 28 pixels in the flattened
    data as the first row in the two-dimensional integer array, the second set of
    28 pixels as the second row, and so forth. Inside the `for` loop, we are converting
    the value of each pixel into a **Blue-Green-Red-Alpha** (**BGRA**) byte array,
    named `bgra`. As we know the images are in grayscale, we can use the same value
    for blue, green, and red components. Once we have converted the flattened pixel
    data into a 28 x 28 two-dimensional integer array, we can now build images of
    the handwritten digit images. We are using the `Bitmap` class to reconstruct these
    handwritten digit images. The following code shows how we used this helper function
    to build images for each digit:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从此代码中可以看出，它首先初始化一个二维整数数组`pixelData`，该数组将存储像素数据。由于我们知道每个图像是一个28 x 28的图像，我们将取展平数据中的前28个像素作为二维整数数组的第一行，第二组28个像素作为第二行，依此类推。在`for`循环内部，我们将每个像素的值转换为名为`bgra`的**蓝-绿-红-透明度**（**BGRA**）字节数组。由于我们知道图像是灰度的，我们可以使用相同的值作为蓝色、绿色和红色组件。一旦我们将展平的像素数据转换为28
    x 28的二维整数数组，我们现在就可以构建手写数字图像了。我们使用`Bitmap`类来重建这些手写数字图像。以下代码展示了我们如何使用此辅助函数为每个数字构建图像：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When you run this code, you will see the following images being stored on your
    local drive:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，您将在本地驱动器上看到以下图像被存储：
- en: '![](img/00121.gif)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00121.gif)'
- en: You can use the same code to generate more images, which will help you better
    understand what raw images of handwritten digits look like.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用相同的代码生成更多图像，这将帮助您更好地理解手写数字的原始图像是什么样的。
- en: Image features - pixels
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征 - 像素
- en: Let's now look at image features. In our dataset, we have integer values for
    each pixel in each image that represent a grayscale value. It will be helpful
    to understand the ranges of values each pixel can take, and whether we can find
    any noticeable differences in the distributions of that pixel data among different
    handwritten digit classes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看图像特征。在我们的数据集中，每个图像中的每个像素都有代表灰度值的整数。了解每个像素可以取的值范围，以及我们是否可以在不同手写数字类别的像素数据分布中找到任何明显的差异，将是有帮助的。
- en: 'We will first take a look at individual distributions of pixel data. The following
    code snippet shows how you can calculate the quartiles for each pixel in our dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看像素数据的单个分布。以下代码片段展示了您如何计算数据集中每个像素的四分位数：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similar to the case previous chapters, we used the `Quantiles` method in `Accord.Statistics.Measures`
    to get the quartiles for each pixel. As you might recall from previous chapters,
    quartiles are the values that separate the data into four sections. In other words,
    the first quartile (`Q1`) represents the 25% percentile that is the middle point
    between the minimum value and the median value. The second quartile (`Q2`) represents
    the median value, and the third quartile (`Q3`) represents the 75% percentile
    that is the middle point between the median and the maximum. In this code example,
    we are only computing quartiles for the first 20 pixels that have values other
    than 0, as you can see in lines 4-7, and in line 11\. When you run this code,
    you will get an output that looks like the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章的情况类似，我们在`Accord.Statistics.Measures`中使用了`Quantiles`方法来获取每个像素的四分位数。如您从前几章中回忆起来，四分位数是将数据分为四个部分的值。换句话说，第一四分位数（`Q1`）代表最小值和平均值之间的中间点，即25%的分位数。第二四分位数（`Q2`）代表平均值，第三四分位数（`Q3`）代表中位数和最大值之间的中间点，即75%的分位数。在这个代码示例中，我们只计算了具有非零值的第一个20个像素的四分位数，如您在4-7行和第11行中看到的那样。当您运行此代码时，您将得到如下所示的输出：
- en: '![](img/00122.gif)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00122.gif)'
- en: Here, we are only showing the first five distributions. As you can see from
    this output, the majority of the pixel values are 0\. If you look at the images
    that we reconstructed in the previous section, the majority of the pixels in the
    image are black and only a subset of the pixels are used to show digits. These
    pixels in black are encoded as `0` in our pixel data, and thus it is expected
    that many pixels have 0 values for the corresponding image.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只显示了前五个分布。如您从输出中看到的那样，大多数像素值都是0。如果您查看我们在上一节中重建的图像，图像中的大多数像素是黑色的，只有一小部分像素用于显示数字。这些黑色像素在我们的像素数据中被编码为`0`，因此许多像素具有0值是预期的。
- en: 'Let''s build some scatter plots, so that we can understand this data better
    visually. The following code builds scatter plots of distributions of the first
    20 non-zero pixel features for each handwritten digit:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一些散点图，以便我们能够更好地从视觉上理解这些数据。以下代码构建了每个手写数字的前20个非零像素特征的分布散点图：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you look closely at this code, we first build a `featureColumns` string array
    from the `featureCols`, `List` object. The `List` object, `featureCols`, is a
    list of the first 20 pixels that have values other than 0, and this was built
    from the previous code when we were computing quartiles. We are using the same
    helper function, `BuildXYPairs`, that we used in the previous chapter to transform
    the data frame into an array of x-y pairs, where the `x` values are the indexes
    of each pixel and the `y` values are the actual pixel value. Using this helper
    function, we use the `ScatterplotBox` class to display a scatter plot that shows
    the pixel distribution for each of the 20 sample pixels.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看这段代码，我们首先从`featureCols`列表对象中构建了一个`featureColumns`字符串数组。`List`对象`featureCols`是具有非零值的前20个像素的列表，这是在计算四分位数时从先前代码中构建的。我们使用了与上一章中相同的辅助函数`BuildXYPairs`，将数据框转换为x-y对的数组，其中`x`值是每个像素的索引，`y`值是实际的像素值。使用这个辅助函数，我们使用`ScatterplotBox`类来显示一个散点图，该图显示了每个20个样本像素的像素分布。
- en: 'The following is a scatter plot for the 0 digit:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个0位数的散点图：
- en: '![](img/00123.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: The majority of the first 20 pixels have 0 values for all the images in the
    0 digit class. Of those 20 pixels that we show in this scatter plot, there are
    only three pixels that have values other than 0\. Let's look at the distributions
    of these pixels for a different digit class.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在0位数类别中的所有图像中，前20个像素的大多数值都是0。在这20个我们在散点图中展示的像素中，只有三个像素的值不是0。让我们看看不同数字类别中这些像素的分布情况。
- en: 'The following scatter plot is for the 1 digit class:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下散点图适用于一位数类别：
- en: '![](img/00124.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00124.jpeg)'
- en: Similar to the case of the 0 digit class, of those 20 pixels that we show here,
    the majority have 0 values and only three pixels have values other than 0\. Compared
    to the previous scatter plot for of the 0 digit class, the distributions of the
    pixel data are slightly different for the 1 digit class.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与0位数类别的情况类似，在这20个我们在散点图中展示的像素中，大多数像素的值都是0，只有三个像素的值不是0。与0位数类别的先前散点图相比，像素数据的分布对于1位数类别略有不同。
- en: 'The following is for the 2 digit class:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容适用于两位数类别：
- en: '![](img/00125.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00125.jpeg)'
- en: This scatter plot shows quite different distributions for the 20 pixels that
    we show here. The majority of those 20 pixels have values ranging between 0 and
    255, and only a few have 0 values for all the images. This kind of difference
    in the distributions of the feature set will help our ML models learn how to correctly
    classify handwritten digits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个散点图显示了这里展示的20个像素的不同分布。其中大多数像素的值在0到255之间，只有少数像素在所有图像中都是0。这种特征集分布的差异将有助于我们的机器学习模型学习如何正确地分类手写数字。
- en: 'Lastly, we are going to look at one more scatter plot, where we will see how
    the target variables are distributed across two different pixels. We used the
    following code to generate a sample two-dimensional scatter plot:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将查看一个额外的散点图，我们将看到目标变量是如何分布在两个不同的像素上的。我们使用了以下代码来生成一个示例二维散点图：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For illustration purposes, we chose the fifteenth and sixteenth indexed features,
    which turn out to be `pixel43` and `pixel44`. When you run this code, you will
    see the following scatter plot:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，我们选择了第十五和第十六个索引的特征，它们实际上是`pixel43`和`pixel44`。当你运行这段代码时，你会看到以下散点图：
- en: '![](img/00126.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.jpeg)'
- en: We can see some distinctions among different classes, but since the majority
    of the pixel values for both `pixel43` and `pixel44` are 0, it is quite difficult
    to draw a clear distinction among different target classes by looking at this
    scatter plot. In the next section, we are going to look at how to use PCA and
    its principal components to create another version of this scatter plot that can
    help us identify a clearer distinction among different target classes when we
    visualize the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到不同类别之间的一些区别，但由于`pixel43`和`pixel44`的大多数像素值都是0，因此通过查看这个散点图很难在不同目标类别之间画出清晰的界限。在下一节中，我们将探讨如何使用PCA及其主成分来创建这个散点图的另一个版本，这有助于我们在可视化数据时识别不同目标类别之间的更清晰的界限。
- en: 'The full code for this data analysis step can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/DataAnalyzer.cs).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据分析步骤的完整代码可以在以下链接中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/DataAnalyzer.cs).
- en: Feature engineering and dimensionality reduction
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程和降维
- en: So far, we have looked at the distributions of the target variables and pixel
    data. In this section, we are going to start discussing building train and test
    sets for our ML modeling step, and then how we can use PCAfor dimensionality reduction
    and to visualize data using the principal components.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了解释变量和像素数据的分布。在本节中，我们将开始讨论为我们的机器学习步骤构建训练集和测试集，然后我们将讨论如何使用PCA进行降维以及使用主成分可视化数据。
- en: Splitting the sample set into train versus test sets
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将样本集分割为训练集和测试集
- en: 'The first task we are going to do in this step is to randomly split our dataset
    into train and test sets. Let''s first look at the code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将要做的第一个任务是随机将我们的数据集分割为训练集和测试集。让我们首先看看代码：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see from the preceding code, we are taking roughly about 70% of our
    data for training, and the rest for testing. Here, we are using the `Random` class
    to generate random numbers to split the sample set into train and test sets using
    the indexes of the records. Once we have built train and test sets, we are removing
    columns or pixels that have 0 values for all the images (line 12). This is because
    if a feature doesn't vary among different target classes, it doesn't have any
    information about those target classes for ML models to learn.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们将大约70%的数据用于训练，其余的用于测试。在这里，我们使用`Random`类生成随机数，使用记录的索引将样本集分割为训练集和测试集。一旦我们构建了训练集和测试集，我们就移除了所有图像中值为0的列或像素（第12行）。这是因为如果一个特征在不同目标类别之间没有变化，它对那些目标类别没有信息，这些信息是机器学习模型需要学习的。
- en: 'Now that we have train and test sets, let''s check on the distributions of
    target classes in both train and test sets. The following code can be used for
    the aggregation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练集和测试集，让我们检查训练集和测试集中目标类别的分布。以下代码可用于聚合：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'When you run this code, you will see the following plot for the target variable
    distribution in the train set:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，你将在训练集中看到目标变量分布的以下图表：
- en: '![](img/00127.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00127.jpeg)'
- en: 'And, the following is what we see for the test set:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，以下是测试集的显示结果：
- en: '![](img/00128.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00128.jpeg)'
- en: These distributions look similar to what we saw in the data analysis step, when
    we analyzed the target variable distribution in the overall dataset. Let's now
    start discussing how we can apply PCA to our train set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分布看起来与我们之前在数据分析步骤中分析整体数据集中的目标变量分布时看到的情况相似。现在让我们开始讨论如何将PCA应用于我们的训练集。
- en: Dimensionality reduction by PCA
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过PCA进行降维
- en: We saw that many of our feature or pixel values are 0, when we were analyzing
    our data. In such cases, applying PCA can be helpful for reducing the dimensions
    of the data, while minimizing the loss of information from the reduced dimensions.
    Simply put, PCA is used to explain a dataset and its structure through linear
    combinations of the original features. So, each principal component is a linear
    combination of the features. Let's start looking at how we can run PCA in C#,
    using the Accord.NET framework.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们分析数据时，我们注意到许多特征或像素值是0。在这种情况下，应用PCA可以帮助减少数据的维度，同时最大限度地减少从减少的维度中损失的信息。简单来说，PCA通过原始特征的线性组合来解释数据集及其结构。因此，每个主成分都是特征的线性组合。让我们开始看看如何使用Accord.NET框架在C#中运行PCA。
- en: 'The following is how you can initialize and train a PCA:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何初始化和训练PCA的方法：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Once a `PrincipalComponentAnalysis` is trained with the data, it contains all
    the information about the linear combinations for each principal component and
    can be applied to transform other data. We used `PrincipalComponentMethod.Standardize`
    to standardize our data before applying PCA. This is because PCA is sensitive
    to the scale of each feature. So, we want to standardize our dataset before applying
    PCA.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用数据训练了`PrincipalComponentAnalysis`，它就包含了每个主成分的线性组合的所有信息，并且可以应用于转换其他数据。我们在应用PCA之前使用`PrincipalComponentMethod.Standardize`来标准化我们的数据。这是因为PCA对每个特征的规模很敏感。因此，我们在应用PCA之前想要标准化我们的数据集。
- en: 'In order to PCA-transform other data, you can use the `Transform` method, as
    shown in the following code snippet:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将PCA转换其他数据，你可以使用`Transform`方法，如下面的代码片段所示：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that we have learned how we can apply PCA to our dataset, let''s look at
    the first two principal components and see if we can find any noticeable patterns
    in the target variable distributions. The following code shows how we can build
    a scatter plot of the first two components with target classes color-coded:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何将PCA应用于我们的数据集，让我们来看看前两个主成分，看看我们是否能在目标变量分布中找到任何明显的模式。以下代码展示了我们如何构建前两个成分的散点图，并使用目标类别进行颜色编码：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once you run this code, you will see the following scatter plot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行此代码，你将看到以下散点图：
- en: '![](img/00129.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpeg)'
- en: When you compare this chart with the one between `pixel43` and `pixel44` that
    we looked at during the data analysis step, this looks quite different. From this
    scatter plot of the first two principal components, we can see that the target
    classes are more discernible. Although it is not perfectly separable from these
    two components, we can see that if we combine more components into our analysis
    and modeling, it will get easier to separate one target class from another.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将此图表与我们在数据分析步骤中查看的`pixel43`和`pixel44`之间的图表进行比较时，这看起来相当不同。从第一个两个主成分的散点图中，我们可以看到目标类别更加明显。尽管这些两个成分并不能完美分离，但我们可以看出，如果我们将更多的成分结合到我们的分析和建模中，将更容易将一个目标类别从另一个中分离出来。
- en: 'Another important aspect of PCA that we should look at is the amount of variance
    explained by each principal component. Let''s take a look at the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的另一个重要方面是我们应该关注的，即每个主成分解释的方差量。让我们看看以下代码：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can retrieve the cumulative proportion of the variance in our data explained
    by each PCA component by using the `CumulativeProportion` property. In order to
    get the individual proportion explained by each PCA component, you can use the
    `Proportion` property of each PCA component. Then, we will use the `DataSeriesBox`
    class to plot a line chart to display the cumulative proportions of the variance
    explained by each component.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`CumulativeProportion`属性来检索由每个PCA成分解释的数据中方差的累积比例。为了获取每个PCA成分解释的个别比例，你可以使用每个PCA成分的`Proportion`属性。然后，我们将使用`DataSeriesBox`类来绘制折线图，以显示每个成分解释的方差累积比例。
- en: 'When you run this code, it will produce the following plot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，它将生成以下图表：
- en: '![](img/00130.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00130.jpeg)'
- en: As you can see from this plot, about 90% of the variance in the dataset can
    be explained by the first 200 components. With 600 components, we can explain
    almost 100% of the variance in our dataset. Compared to the total of 784 pixels
    we had as our features in the raw dataset, this is a big reduction in the dimension
    of our data. Depending on how much variance you want to capture for your ML models,
    you can use this chart to decide the number of components that is most suitable
    for your modeling process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从该图表中可以看到，数据集中的约90%的方差可以通过前200个成分来解释。使用600个成分，我们可以解释我们数据集中几乎100%的方差。与原始数据集中作为特征的784个像素总数相比，这是数据维度的大幅减少。根据你想要为你的机器学习模型捕获多少方差，你可以使用此图表来决定最适合你的建模过程的成分数量。
- en: 'Finally, we need to export the train and test sets, so that we can use them
    for the following model building step. You can use the following code to export
    the PCA-transformed train and test sets:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要导出训练集和测试集，以便我们可以使用它们进行以下模型构建步骤。你可以使用以下代码来导出PCA转换后的训练集和测试集：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The full code for this feature engineering and dimensionality reduction step
    can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/FeatureEngineering.cs).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此特征工程和降维步骤的完整代码可以在以下链接中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/FeatureEngineering.cs)。
- en: ML models for handwritten digit recognition
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习模型用于手写数字识别
- en: Now that we have everything ready for building ML models, let's start building
    those models. In this section, we will cover how to sub-select the features based
    on the PCA results and then discuss how we can build logistic regression and Naive
    Bayes classifiers for the handwritten digit recognition model. We are going to
    introduce a new learning model, the neural network, and explain how to build one
    for this project, using the Accord.NET framework.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为构建机器学习模型准备好了所有东西，让我们开始构建这些模型。在本节中，我们将介绍如何根据PCA结果子选择特征，然后讨论我们如何为手写数字识别模型构建逻辑回归和朴素贝叶斯分类器。我们将介绍一个新的学习模型——神经网络，并解释如何使用Accord.NET框架为这个项目构建一个神经网络。
- en: Loading data
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'The first step in building a ML model for handwritten digit recognition is
    to load the data that we built from the previous section. You can use the following
    code to load the train and test sets that we created previously:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 构建用于手写数字识别的机器学习模型的第一步是加载我们在上一节构建的数据。您可以使用以下代码来加载我们之前创建的训练集和测试集：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'For our experimentation with different models in this chapter, we will be using
    the principal components that cumulatively explain about 70% of the variance in
    our dataset. Take a look at the following code to see how we filtered for the
    components of our interest:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用累积解释我们数据集约70%方差的特征进行不同模型的实验。查看以下代码，了解我们是如何筛选出感兴趣的特征成分的：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see in the first line of this code, we are taking the first 91 components
    (up to the ninetieth index) as the features for our models. If you recall from
    the previous step or look at the plot for the cumulative variance proportion explained
    by the components, you will see that the first 91 components capture about 70%
    of the variance in our dataset. Then, we create a two-dimensional array of doubles
    that we will use for training and testing our ML models. The following code shows
    the helper function, `BuildJaggedArray`, that we wrote to convert a data frame
    into a two-dimensional array:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从代码的第一行所见，我们正在将前91个成分（直到第九十个索引）作为我们模型的特征。如果您还记得之前的步骤或查看成分解释的累积方差比例的图表，您会看到前91个成分捕捉了我们数据集约70%的方差。然后，我们创建一个二维的double数组，我们将用它来训练和测试我们的机器学习模型。以下代码展示了我们编写的辅助函数`BuildJaggedArray`，该函数将数据框转换为二维数组：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Logistic regression classifier
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归分类器
- en: 'The first learning algorithm we are going to experiment with for handwritten
    digit recognition is logistic regression. We wrote a method, named `BuildLogitModel`,
    which takes in the inputs and outputs to the model, trains a logistic regression
    classifier, and then evaluates the performance. The following code shows how this
    method is written:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实验的第一个用于手写数字识别的学习算法是逻辑回归。我们编写了一个名为`BuildLogitModel`的方法，它接受模型输入和输出，训练一个逻辑回归分类器，然后评估性能。以下代码展示了这个方法的编写方式：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Similar to the previous chapter, we are using the `MultinomialLogisticLearning`
    class to train a logistic regression classifier. Once this model is trained, we
    start evaluating by various evaluation metrics, which we will discuss in more
    detail in the following section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章类似，我们正在使用`MultinomialLogisticLearning`类来训练一个逻辑回归分类器。一旦这个模型被训练，我们就开始通过各种评估指标进行评估，我们将在下一节中更详细地讨论这些指标。
- en: Naive Bayes classifier
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: 'The second model we are going to experiment with is a Naive Bayes classifier.
    Similar to the previous case involving the logistic regression classifier, we
    wrote a helper function, `BuildNBModel`, that takes in the inputs and outputs,
    trains a Naive Bayes classifier, and then evaluates the trained model. The code
    looks as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要实验的第二种模型是一个朴素贝叶斯分类器。类似于之前涉及逻辑回归分类器的案例，我们编写了一个辅助函数`BuildNBModel`，它接受输入和输出，训练一个朴素贝叶斯分类器，然后评估训练好的模型。代码如下：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you might recall from the previous chapter, we are using the `NaiveBayesLearning`
    class to train a Naive Bayes classifier. We are using `NormalDistribution`, as
    all the features for our ML models are the principal components from the previous
    PCA step, and the values of these components are continuous values.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能从上一章回忆起来，我们正在使用`NaiveBayesLearning`类来训练一个朴素贝叶斯分类器。我们使用`NormalDistribution`，因为我们的机器学习模型的所有特征都是来自之前PCA步骤的主成分，而这些成分的值是连续的。
- en: Neural network classifier
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络分类器
- en: 'The last learning algorithm that we are going to experiment with is the ANN.
    As you might know already, the neural network model is the backbone of all of
    the deep learning technologies. The neural network model is known to perform well
    for image datasets, so we will compare the performance of this model against the
    other models to see how much performance gain we get by using the neural network
    over the other classification models. In order to build neural network models
    in C# using the Accord.NET framework, you will need to install the `Accord.Neuro`
    package first. You can install the `Accord.Neuro` package by using the following
    command in the **NuGet Package Manager Console**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实验的最后一种学习算法是人工神经网络（ANN）。正如你可能已经知道的，神经网络模型是所有深度学习技术的核心。神经网络模型在图像数据集上表现良好，因此我们将比较该模型与其他模型的性能，以查看通过使用神经网络相对于其他分类模型我们能获得多少性能提升。为了使用
    Accord.NET 框架在 C# 中构建神经网络模型，你首先需要安装 `Accord.Neuro` 包。你可以在 **NuGet 包管理器控制台**中使用以下命令安装
    `Accord.Neuro` 包：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s now take a look at how we can build a neural network model in C#, using
    the Accord.NET framework. The code looks like the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何使用 Accord.NET 框架在 C# 中构建神经网络模型。代码如下：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s take a closer look at this code. We first transform the training labels
    from a one-dimensional array into a two-dimensional array, where the columns are
    the target classes and the values are 1 if the given record belongs to the given
    target class, and 0 if it does not. We are using the `Accord.Math.Jagged.OneHot`
    method to perform one-hot encoding for the training labels. Then, we build a neural
    network by using the `ActivationNetwork` class. The `ActivationNetwork` class
    takes three parameters: the activation function, the input count, and the information
    about the layers. For the activation function, we are using a sigmoid function, `BipolarSigmoidFunction`.
    The input count is straightforward, as it is the number of features that we are
    going to use to train this model, which is 91\. For this model, we only used one
    hidden layer with 20 neurons. For a deeper neural network, you can use more than
    one hidden layer and can also experiment with different numbers of neurons in
    each hidden layer. Lastly, the last parameter of the `ActivationNetwork` constructor
    represents the output count. Since the target variable is the digit class, it
    can take values between 0 and 9, and thus the number of output neurons we need
    is 10. Once this network is built, we can use the `LevenbergMarquardtLearning`
    learning algorithm to train the network.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这段代码。我们首先将训练标签从一维数组转换为二维数组，其中列是目标类别，如果给定的记录属于给定的目标类别，则值为 1，如果不属于，则值为
    0。我们使用 `Accord.Math.Jagged.OneHot` 方法对训练标签进行独热编码。然后，我们使用 `ActivationNetwork` 类构建神经网络。`ActivationNetwork`
    类接受三个参数：激活函数、输入计数和层的信息。对于激活函数，我们使用 sigmoid 函数，`BipolarSigmoidFunction`。输入计数很简单，因为它是我们将要用于训练此模型的特征数量，即
    91。对于此模型，我们只使用了一个包含 20 个神经元的隐藏层。对于更深的神经网络，你可以使用多个隐藏层，并且也可以在每个隐藏层中实验不同的神经元数量。最后，`ActivationNetwork`
    构造函数的最后一个参数代表输出计数。由于目标变量是数字类别，它可以取 0 到 9 之间的值，因此我们需要 10 个输出神经元。一旦构建了这个网络，我们就可以使用
    `LevenbergMarquardtLearning` 学习算法来训练网络。
- en: 'Once we have set up the network and the learning algorithm, we can actually
    start training a neural network model. As you might know already, a neural network
    model needs to be run through the dataset multiple times (epochs) during its learning
    phase for better predictability. You can use the `RunEpoch` method to train and
    update the neural network model in each epoch. To save some time, we are only
    running 10 epochs to train our neural network model. However, we recommend you
    try increasing this value, as it can improve the performance of your neural network
    model. The following shows how the error measure decreases as we train and update
    the neural network model in each epoch:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了网络和学习算法，我们实际上就可以开始训练神经网络模型了。正如你可能已经知道的，神经网络模型在其学习阶段需要多次（epochs）通过数据集运行以获得更好的可预测性。你可以使用
    `RunEpoch` 方法在每个 epoch 中训练和更新神经网络模型。为了节省时间，我们只运行了 10 个 epoch 来训练我们的神经网络模型。然而，我们建议你尝试增加这个值，因为它可以提高你的神经网络模型的性能。以下展示了随着我们训练和更新神经网络模型在每个
    epoch 中，错误度量是如何降低的：
- en: '![](img/00131.gif)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00131.gif)'
- en: As you can see from this output, the error measure decreases significantly in
    each epoch. One thing to note here is that the amount of reduction in the error
    measure decreases in each additional epoch. When you are building a neural network
    model with large numbers of epochs, you can monitor the amount of gain in each
    run and decide to stop when there is no more significant performance gain.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如此输出所示，错误度量在每个epoch中显著降低。在此需要注意的是，错误度量减少的量在每个额外的epoch中减少。当你构建具有大量epochs的神经网络模型时，你可以监控每次运行的增益量，并在没有更多显著性能增益时决定停止。
- en: The full code that we used for the model building step can be found at this
    link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/Modeling.cs).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 用于模型构建步骤的完整代码可以在以下链接找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/Modeling.cs)。
- en: Evaluating multi-class classification models
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估多类分类模型
- en: In this section, we are going to evaluate the three models that we built in
    the previous section. We are going to revisit the validation metrics that we used
    previously for the classification models, and compare the performances of each
    model against the others.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将评估上一节中构建的三个模型。我们将回顾之前用于分类模型的验证指标，并比较每个模型之间的性能。
- en: Confusion matrices
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'First, let''s look at confusion matrices. The following code shows how you
    can build a confusion matrix with the predicted output and the actual output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看混淆矩阵。以下代码展示了如何使用预测输出和实际输出构建混淆矩阵：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This method is similar to the one we wrote in the previous chapter, except that
    it is returning a two-dimensional array, instead of a string array. We are going
    to use this two-dimensional array output for calculating precision and recall
    rates in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法与我们之前章节中编写的方法类似，不同之处在于它返回的是一个二维数组，而不是字符串数组。我们将在下一节中使用这个二维数组输出来计算精确率和召回率。
- en: 'The confusion matrix for the logistic regression classifier looks like the
    following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归分类器的混淆矩阵如下：
- en: '![](img/00132.gif)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00132.gif)'
- en: 'For the Naive Bayes classifier, you will get a confusion matrix that looks
    similar to the following table:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于朴素贝叶斯分类器，你将得到一个类似于以下表格的混淆矩阵：
- en: '![](img/00133.gif)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00133.gif)'
- en: 'Lastly, for the neural network model, the confusion matrix looks like the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于神经网络模型，混淆矩阵如下：
- en: '![](img/00134.gif)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00134.gif)'
- en: From these confusion matrices, the neural network model outperforms the other
    two models, and the logistic regression model seems to come in second.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些混淆矩阵中，神经网络模型优于其他两个模型，逻辑回归模型似乎位居第二。
- en: Accuracy and precision/recall
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率和精确率/召回率
- en: 'The second metric that we are going to look at is the accuracy measure. We
    are use `ZeroOneLoss` to compute the loss, and then subtract it from `1` to get
    the accuracy number. The code to compute the accuracy measure is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的第二项指标是准确度度量。我们使用`ZeroOneLoss`来计算损失，然后从`1`中减去以获得准确度数值。计算准确度度量的代码如下：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The third and fourth metrics that we are going to look at are the precision
    and recall rates. Unlike before, we have 10 classes for the target prediction.
    So, we are going to have to calculate precision and recall rates separately for
    each of the target classes. The code looks like the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的第三和第四项指标是精确率和召回率。与之前不同，目标预测有10个类别。因此，我们将为每个目标类别分别计算精确率和召回率。代码如下：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see from this code, the input to this `PrintPrecisionRecall` method
    is the confusion matrix that we built from the previous section. In this method,
    it iterates through each of the target classes and computes the precision and
    recall rates.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，此`PrintPrecisionRecall`方法的输入是我们从上一节构建的混淆矩阵。在此方法中，它遍历每个目标类别并计算精确率和召回率。
- en: 'The following is the output that we get when we compute accuracy, precision,
    and recall for the logistic regression model:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们计算逻辑回归模型的准确度、精确率和召回率时的输出：
- en: '![](img/00135.gif)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00135.gif)'
- en: 'For the Naive Bayes model, we get the following results for the metrics:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于朴素贝叶斯模型，我们得到以下指标结果：
- en: '![](img/00136.gif)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00136.gif)'
- en: 'Lastly, for the neural network model, the performance results look as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于神经网络模型，性能结果如下：
- en: '![](img/00137.gif)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00137.gif)'
- en: As you might notice from these results, the neural network model outperformed
    the other two models. Both the overall accuracy and the precision/recall rates
    are the highest for the neural network model, when compared to the logistic regression
    and Naive Bayes models. The logistic regression model seems to come in as the
    second best model among the three that we built.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，从这些结果来看，神经网络模型优于其他两个模型。与逻辑回归和朴素贝叶斯模型相比，神经网络模型的整体准确率和精确率/召回率都是最高的。逻辑回归模型似乎在我们构建的三个模型中位居第二。
- en: One versus Rest AUC
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对多AUC
- en: 'The last evaluation measure that we are going to look at is the **Receiver
    Operating Characteristic** (**ROC**) curve and the AUC. One thing we need to do
    differently in this chapter, when we are building a ROC curve and an AUC, is that
    we need to build one for each of the target classes. Let''s take a look at the
    code first:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一种评估指标是**接收者操作特征**（**ROC**）曲线和AUC。在本章中，当我们构建ROC曲线和AUC时，我们需要做的一件事是，为每个目标类别构建一个。让我们先看看代码：
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see from this `DrawROCCurve` method that we wrote, we iterate through
    each target class in a `for` loop, and reformat the predicted and actual labels
    by encoding `1` if each label matches with the target class, and 0 if it does
    not. After we have done this encoding, we can then use the `ReceiverOperatingCharacteristic`
    class to compute the AUC and build the ROC curve.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们编写的`DrawROCCurve`方法中可以看出，我们通过一个`for`循环遍历每个目标类别，并通过编码将每个标签与目标类别匹配时标记为`1`，不匹配时标记为`0`。完成编码后，我们可以使用`ReceiverOperatingCharacteristic`类来计算AUC并构建ROC曲线。
- en: 'The following is the ROC curve for the logistic regression model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表是逻辑回归模型的ROC曲线：
- en: '![](img/00138.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00138.jpeg)'
- en: 'For the Naive Bayes model, the ROC curve looks as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于朴素贝叶斯模型，ROC曲线如下所示：
- en: '![](img/00139.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00139.jpeg)'
- en: 'Lastly, the ROC curve for the neural network model looks like the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，神经网络模型的ROC曲线如下所示：
- en: '![](img/00140.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00140.jpeg)'
- en: As expected from the previous metrics that we have looked at, the results look
    the best for the neural network model, and the logistic regression model comes
    in as second-best. For the Naive Bayes model, there are some digits that it didn't
    compute well. For example, the Naive Bayes model struggles to classify the digits
    6 and 7 well. However, the AUC numbers for all of the target classes are close
    to 1 for the neural network, which suggests that the model is trained well to
    identify digits for handwritten images.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所预期的，从之前我们查看的指标来看，神经网络模型的结果最好，逻辑回归模型位居第二。对于朴素贝叶斯模型，有一些数字它没有很好地计算。例如，朴素贝叶斯模型在分类数字6和7时遇到了困难。然而，对于所有目标类别，神经网络模型的AUC数值都接近1，这表明模型已经很好地训练来识别手写图像中的数字。
- en: From looking at the confusion matrix, the accuracy, precision and recall rates,
    and the ROC curves, we can conclude that the neural network model works the best
    among the three classifiers that we trained in this chapter. This reaffirms the
    fact that neural networks work well on image datasets and image recognition problems.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看混淆矩阵、准确率、精确率和召回率以及ROC曲线，我们可以得出结论，在本章训练的三个分类器中，神经网络模型表现最好。这再次证实了神经网络在图像数据集和图像识别问题上的良好表现。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built our first image recognition model that can identify
    handwritten digits in grayscale images. We started this chapter by discussing
    how this type of model can be widely used in real-life applications, and how we
    are planning to build a handwritten digit recognition model. Then, we started
    looking into the dataset. We first looked at the distributions of target classes
    to see if the sample set is a well-balanced set. When we were analyzing the pixel
    data, we noticed that the majority of the pixel values were 0, and we could intuitively
    make sense of it by reconstructing the images from the pixel data. During the
    feature engineering step, we discussed how we can use PCA for dimensionality reduction.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了我们第一个图像识别模型，该模型可以识别灰度图像中的手写数字。我们本章开始时讨论了这种类型的模型如何在现实生活中的广泛应用，以及我们计划如何构建手写数字识别模型。然后，我们开始研究数据集。我们首先查看目标类别的分布，以查看样本集是否是一个平衡良好的集合。当我们分析像素数据时，我们注意到大多数像素值都是0，我们可以通过从像素数据重建图像来直观地理解这一点。在特征工程步骤中，我们讨论了如何使用PCA进行降维。
- en: With these PCA-transformed features, we then started building various machine
    learning models. On top of the logistic regression and Naive Bayes models that
    we are already familiar with, we introduced a new ML model, neural network. We
    learned how to initialize the `ActivationNetwork` model with `BipolarSigmoidFunction`
    as an activation function. We then started training the neural network with the
    `LevenbergMarquardtLearning` learning algorithm over 10 epochs. We saw how error
    measures decrease in each additional epoch, and discussed how the amount of gain
    in the error rate is in diminishing returns for additional epochs. In the model
    evaluation step, we combined multiple validation metrics for classification models.
    For the machine learning models we built in this chapter, we looked at the confusion
    matrix, prediction accuracy, precision and recall rates, and the ROC curves and
    AUC. We noticed how the neural network model outperformed the other two models,
    which reaffirmed that neural network models work well for image data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在对这些PCA转换后的特征进行处理后，我们开始构建各种机器学习模型。在已经熟悉的逻辑回归和朴素贝叶斯模型的基础上，我们引入了一个新的机器学习模型——神经网络。我们学习了如何使用`BipolarSigmoidFunction`作为激活函数初始化`ActivationNetwork`模型。然后，我们使用`LevenbergMarquardtLearning`学习算法在10个周期内开始训练神经网络。我们观察到每个额外周期中误差度量是如何减少的，并讨论了误差率增加的额外周期是如何逐渐减少的。在模型评估步骤中，我们结合了多个分类模型的验证指标。对于本章构建的机器学习模型，我们考虑了混淆矩阵、预测准确率、精确率和召回率，以及ROC曲线和AUC。我们注意到神经网络模型优于其他两个模型，这再次证实了神经网络模型非常适合图像数据。
- en: In the next chapter, we are going to switch gears and start building models
    for anomaly detection. We are going to work on a cyber attack detection project
    using PCA. With the Network Intrusion dataset, we will discuss how to use PCA
    to detect cyber attacks, and run multiple experiments to find the optimal threshold
    at which to notify us about potential cyber attacks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向，开始构建用于异常检测的模型。我们将使用PCA进行网络攻击检测项目。利用网络入侵数据集，我们将讨论如何使用PCA来检测网络攻击，并运行多个实验以找到通知我们潜在网络攻击的最佳阈值。
