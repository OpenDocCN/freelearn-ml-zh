- en: Chapter 2. Approaching Simple Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：简单线性回归的接近方法
- en: Having set up all your working tools (directly installing Python and IPython
    or using a scientific distribution), you are now ready to start using linear models
    to incorporate new abilities into the software you plan to build, especially predictive
    capabilities. Up to now, you have developed software solutions based on certain
    specifications you defined (or specifications that others have handed to you).
    Your approach has always been to tailor the response of the program to particular
    inputs, by writing code carefully mapping every single situation to a specific,
    predetermined response. Reflecting on it, by doing so you were just incorporating
    practices that you (or others) have learned from experience.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好所有工作工具（直接安装Python和IPython或使用科学发行版）之后，你现在准备好开始使用线性模型将新能力融入你计划构建的软件中，特别是预测能力。到目前为止，你开发的软件解决方案是基于你定义的某些规范（或其他人交给你的规范）。你的方法始终是通过编写代码，仔细地将每个情况映射到特定的、预先确定的响应，来调整程序的响应以适应特定的输入。回顾一下，通过这样做，你只是结合了从经验中学到的实践。
- en: However, the world is complex, and sometimes your experience is not enough to
    make your software smart enough to make a difference in a fairly competitive business
    or in challenging problems with many different and mutable facets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，世界是复杂的，有时你的经验并不足以让你的软件足够智能，在竞争激烈的企业或具有许多不同和可变面的挑战性问题中产生差异。
- en: 'In this chapter, we will start exploring an approach that is different from
    manual programming. We are going to present an approach that enables the software
    to self-learn the correct answers to particular inputs, provided you can define
    the problem in terms of data and target response and that you can incorporate
    in the processes some of your domain expertise—for instance, choosing the right
    features for prediction. Therefore, your experience will go on being critical
    when it comes to creating your software, though in the form of learning from data.
    In fact, your software will be learning from data accordingly to your specifications.
    We are also going to illustrate how it is possible to achieve this by resorting
    to one of the simplest methods for deriving knowledge from data: linear models.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始探索一种不同于手动编程的方法。我们将提出一种使软件能够自我学习特定输入的正确答案的方法，前提是你能够用数据和目标响应来定义问题，并且你可以在过程中结合一些你的领域专业知识——例如，选择正确的预测特征。因此，当涉及到创建你的软件时，你的经验将继续至关重要，尽管是以从数据中学习的形式。实际上，你的软件将根据你的规格从数据中学习。我们还将说明如何通过求助于从数据中推导知识的最简单方法之一：线性模型来实现这一点。
- en: 'Specifically, in this chapter, we are going to discuss the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将讨论以下主题：
- en: Understanding what problems machine learning can solve
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习可以解决的问题
- en: What problems a regression model can solve
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型可以解决哪些问题
- en: The strengths and weaknesses of correlation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性的优缺点
- en: How correlations extends to a simple regression model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将相关性扩展到简单的回归模型
- en: The when, what, and why of a regression model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型的何时、何事和为何
- en: The essential mathematics behind gradient descent
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降背后的基本数学
- en: In the process, we will be using some statistical terminology and concepts in
    order to provide you with the prospect of linear regression in the larger frame
    of statistics, though our approach will remain practical, offering you the tools
    and hints to start building linear models using Python and thus enrich your software
    development.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们将使用一些统计术语和概念，以便在统计学的更大框架中为您提供线性回归的前景，尽管我们的方法将保持实用，为您提供使用Python开始构建线性模型所需的工具和提示，从而丰富您的软件开发。
- en: Defining a regression problem
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义回归问题
- en: 'Thanks to machine learning algorithms, deriving knowledge from data is possible.
    Machine learning has solid roots in years of research: it has really been a long
    journey since the end of the fifties, when Arthur Samuel clarified machine learning
    as being a "field of study that gives computers the ability to learn without being
    explicitly programmed."'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了机器学习算法，从数据中推导知识成为可能。机器学习有着坚实的根基，源于多年的研究：它实际上是从五十年代末期以来的一段漫长旅程，当时亚瑟·塞缪尔将机器学习明确界定为“一个研究领域，它赋予计算机在没有明确编程的情况下学习的能力。”
- en: The data explosion (the availability of previously unrecorded amounts of data)
    has enabled the widespread usage of both recent and classic machine learning techniques
    and made them high-performance techniques. If nowadays you can talk by voice to
    your mobile phone and expect it to answer properly to you, acting as your secretary
    (such as Siri or Google Now), it is uniquely because of machine learning. The
    same holds true for every application based on machine learning such as face recognition,
    search engines, spam filters, recommender systems for books/music/movies, handwriting
    recognition, and automatic language translation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据爆炸（以前未记录的大量数据的可用性）使得最近和经典的机器学习技术得到广泛应用，并使它们成为高性能技术。如今，如果你可以通过语音与你的手机交谈并期望它能够正确地回应你，充当你的秘书（如Siri或Google
    Now），这完全是由于机器学习。同样，对于基于机器学习的每个应用也是如此，如人脸识别、搜索引擎、垃圾邮件过滤器、书籍/音乐/电影的推荐系统、手写识别和自动语言翻译。
- en: Some other actual usages of machine learning algorithms are somewhat less obvious,
    but nevertheless important and profitable, such as credit rating and fraud detection,
    algorithmic trading, advertising profiling on the Web, and health diagnostics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的一些其他实际用途可能不那么明显，但同样重要且有利可图，如信用评级和欺诈检测、算法交易、网络上的广告定位和健康诊断。
- en: 'Generally speaking, machine learning algorithms can learn in three ways:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，机器学习算法可以通过三种方式学习：
- en: '**Supervised learning**: This is when we present labeled examples to learn
    from. For instance, when we want to be able to predict the selling price of a
    house in advance in a real estate market, we can get the historical prices of
    houses and have a supervised learning algorithm successfully figure out how to
    associate the prices to the house characteristics.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：这是我们提供标记示例以从中学习的情况。例如，当我们想要在房地产市场中提前预测房屋的售价时，我们可以获取房屋的历史价格，并让监督学习算法成功地找出如何将价格与房屋特征相关联。'
- en: '**Unsupervised learning**: This is when we present examples without any hint,
    leaving it to the algorithm to create a label. For instance, when we need to figure
    out how the groups inside a customer database can be partitioned into similar
    segments based on their characteristics and behaviors.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：这是我们提供没有任何提示的示例，将其留给算法来创建标签的情况。例如，当我们需要根据客户数据库中各组的特征和行为将它们划分为相似段时。'
- en: '**Reinforcement learning**: This is when we present examples without labels,
    as in unsupervised learning, but get feedback from the environment as to whether
    label guessing is correct or not. For instance, when we need software to act successfully
    in a competitive setting, such as a videogame or the stock market, we can use
    reinforcement learning. In this case, the software will then start acting in the
    setting and it will learn directly from its errors until it finds a set of rules
    that ensure its success.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：这是当我们提供没有标签的示例时，就像无监督学习一样，但会从环境中获得反馈，以确定标签猜测是否正确。例如，当我们需要软件在竞争环境中成功行动时，如电子游戏或股市，我们可以使用强化学习。在这种情况下，软件将开始在环境中行动，并直接从其错误中学习，直到找到确保其成功的一组规则。'
- en: Linear models and supervised learning
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型和监督学习
- en: 'Unsupervised learning has important applications in robotic vision and automatic
    feature creation, and reinforcement learning is critical for developing autonomous
    AI (for instance, in robotics, but also in creating intelligent software agents);
    however, supervised learning is most important in data science because it allows
    us to accomplish something the human race has aspired to for ages: prediction.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在机器人视觉和自动特征创建中有着重要的应用，而强化学习对于开发自主人工智能（例如，在机器人技术中，但也在创建智能软件代理）至关重要；然而，监督学习在数据科学中最为重要，因为它使我们能够实现人类种族长期以来一直渴望实现的目标：预测。
- en: '**Prediction** has applications in business and for general usefulness, enabling
    us to take the best course of action since we know from predictions the likely
    outcome of a situation. Prediction can make us successful in our decisions and
    actions, and since ancient times has been associated with magic or great wisdom.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**在商业和一般用途中都有应用，它使我们能够采取最佳行动，因为我们从预测中得知了某种情况的可能结果。预测可以使我们在决策和行动中取得成功，并且自古以来就与魔法或大智慧联系在一起。'
- en: Supervised learning is no magic at all, though it may look like sorcery to some
    people, as Sir Arthur Charles Clarke stated, "any sufficiently advanced technology
    is indistinguishable from magic." Supervised learning, based on human achievements
    in mathematics and statistics, helps to leverage human experience and observations
    and turn them into precise predictions in a way that no human mind could. However,
    supervised learning can predict only in certain favorable conditions. It is paramount
    to have examples from the past at hand from which we can extract rules and hints
    that can support wrapping up a highly likely prediction given certain premises.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习根本不是魔法，尽管它可能对某些人来说看起来像是魔法，正如亚瑟·查尔斯·克拉克爵士所说，“任何足够先进的技术都与魔法无法区分。”基于人类在数学和统计学方面的成就，监督学习有助于利用人类经验和观察，并以一种任何人类心智都无法实现的方式将它们转化为精确的预测。然而，监督学习只能在某些有利条件下进行预测。拥有过去的事例至关重要，我们可以从中提取规则和线索，以支持在给定某些前提的情况下做出高度可能的预测。
- en: In one way or another, no matter the exact formulation of the machine learning
    algorithm, the idea is that you can tell the outcome because there have been certain
    premises in the observed past that led to particular conclusions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无论机器学习算法的确切公式如何，其想法是你可以预测结果，因为观察到的过去中存在某些前提，导致了特定的结论。
- en: In mathematical formalism, we call the outcome we want to predict the response
    or target variable and we usually label it using the lower case letter *y*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学形式化中，我们称我们想要预测的结果为响应变量或目标变量，我们通常使用小写字母 *y* 来标记它。
- en: The premises are instead called the **predictive** **variables**, or simply
    attributes or features, and they are labeled as a lowercase *x* if there is a
    single one and by an uppercase *X* if there are many. Using the uppercase letter
    *X* we intend to use matrix notation, since we can also treat the *y* as a response
    vector (technically a column vector) and the *X* as a matrix containing all values
    of the feature vectors, each arranged into a separate column of the matrix.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 前提被称为 **预测** 变量，或者简单地称为属性或特征，如果只有一个，则用小写 *x* 标记，如果有多个，则用大写 *X* 标记。使用大写字母 *X*，我们的意图是使用矩阵符号，因为我们也可以将
    *y* 视为一个响应向量（技术上是一个列向量），将 *X* 视为一个包含所有特征向量值的矩阵，每个特征向量都排列在矩阵的单独一列中。
- en: It is also important to always keep a note of the dimensions of *X* and *y*;
    thus, by convention, we can call *n* the number of observations and *p* the number
    of variables. Consequently our *X* will be a matrix of size (*n*, *p*), and our
    *y* will always be a vector of size *n*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，始终注意 *X* 和 *y* 的维度也很重要；因此，按照惯例，我们可以将 *n* 称为观测值的数量，将 *p* 称为变量的数量。因此，我们的 *X*
    将是一个大小为 (*n*, *p*) 的矩阵，而我们的 *y* 将始终是一个大小为 *n* 的向量。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Throughout the book, we will also have recourse to statistical notation, which
    is actually a bit more explicit and verbose. A statistical formula tries to give
    an idea of all the predictors involved in the formula (we will show an example
    of this later) whereas matrix notation is more implicit.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们还将使用统计符号，这实际上要明确和冗长得多。一个统计公式试图给出公式中所有预测变量的概念（我们将在稍后展示一个例子），而矩阵符号则更为隐晦。
- en: We can affirm that, when we are learning to predict from data in a supervised
    way, we are actually building a function that can answer the question about how
    *X* can imply *y*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以肯定，当我们以监督方式从数据中学习预测时，我们实际上是在构建一个可以回答关于 *X* 如何暗示 *y* 的函数的问题。
- en: 'Using these new matrix symbolic notations, we can define a function, a functional
    mapping that can translate *X* values into *y* without error or with an acceptable
    margin of error. We can affirm that all our work will be to determinate a function
    of the following kind:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些新的矩阵符号表示，我们可以定义一个函数，一个将 *X* 值转换为 *y* 的函数映射，可以无误差或在一个可接受的误差范围内进行转换。我们可以肯定，我们所有的努力都是为了确定以下类型的函数：
- en: '![Linear models and supervised learning](img/00005.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![线性模型和监督学习](img/00005.jpeg)'
- en: When the function is specified, and we have in mind a certain algorithm with
    certain parameters and an *X* matrix made up of certain data, conventionally we
    can refer to it as a hypothesis. The term is suitable because we can intend our
    function as a ready hypothesis, set with all its parameters, to be tested if working
    more or less well in predicting our target *y*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当函数被指定，并且我们心中有一个带有特定参数的算法以及由某些数据组成的 *X* 矩阵时，我们通常可以将其称为一个假设。这个术语是合适的，因为我们可以将我们的函数视为一个准备好的假设，设置好所有参数，以便测试它在预测目标
    *y* 时工作得是否好坏。
- en: Before talking about the function (the supervised algorithm that does all the
    magic), we should first spend some time reasoning about what feeds the algorithm
    itself. We have already introduced the matrix *X*, the predictive variables, and
    the vector *y*, the target answer variable; now it is time to explain how we can
    extract them from our data and what exactly their role is in a learning algorithm.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论函数（执行所有魔法的监督算法）之前，我们首先应该花些时间思考一下是什么在喂养这个算法本身。我们已经介绍了矩阵 *X*，预测变量，以及向量 *y*，目标答案变量；现在，我们需要解释如何从我们的数据中提取它们，以及它们在学习算法中的确切作用。
- en: Reflecting on predictive variables
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反思预测变量
- en: Reflecting on the role of your predictive variable in a supervised algorithm,
    there are a few caveats that you have to keep in mind throughout our illustrations
    in the book, and yes, they are very important and decisive.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督算法中反思你的预测变量的作用，在整个书籍的说明中，你必须牢记一些注意事项，是的，它们非常重要且具有决定性。
- en: 'To store the predictive variables, we use a matrix, usually called the *X*
    matrix:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储预测变量，我们使用一个矩阵，通常称为 *X* 矩阵：
- en: '![Reflecting on predictive variables](img/00006.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![反思预测变量](img/00006.jpeg)'
- en: In this example, our *X* is made up of only one variable and it contains *n*
    cases (or observations).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的 *X* 只包含一个变量，它包含 *n* 个案例（或观察）。
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you would like to know when to use a variable or feature, just consider that
    in machine learning *feature* and *attribute* are terms that are favored over
    *variable*, which has a definitively statistical flavor hinting at something that
    varies. Depending on the context and audience, you can effectively use one or
    the other.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道何时使用一个变量或特征，只需考虑在机器学习中，*特征* 和 *属性* 是比 *变量* 更受青睐的术语，*变量* 具有明确的统计味道，暗示着某种变化。根据上下文和受众，你可以有效地使用其中一个。
- en: 'In Python code, you can build a one-column matrix structure by typing:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 代码中，你可以通过输入以下代码来构建一个一列的矩阵结构：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the NumPy `array` we can quickly derive a vector and a matrix. If you
    start from a Python list, you will get a vector (which is neither a row nor a
    column vector, actually). By using the `reshape` method, you can transform it
    into a row or column vector, based on your specifications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NumPy 的 `array`，我们可以快速推导出一个向量和矩阵。如果你从一个 Python 列表开始，你会得到一个向量（实际上既不是行向量也不是列向量）。通过使用
    `reshape` 方法，你可以根据你的要求将其转换成一个行向量或列向量。
- en: 'Real-world data usually need matrices that are more complex, and real-world
    matrices comprise uncountable different data columns (the variety element of big
    data). Most likely, a standard *X* matrix will have more columns, so the notation
    we will be referring to is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的数据通常需要更复杂的矩阵，现实世界的矩阵包含无数不同的数据列（大数据的多样性元素）。很可能会出现，标准的 *X* 矩阵会有更多的列，因此我们将引用的符号是：
- en: '![Reflecting on predictive variables](img/00007.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![反思预测变量](img/00007.jpeg)'
- en: 'Now, our matrix has more variables, all *p* variables, so its size is *n* x
    *p*. In Python, there are two methods to make up such a data matrix:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的矩阵有更多的变量，所有 *p* 个变量，因此其大小是 *n* x *p*。在 Python 中，有两种方法可以构建这样的数据矩阵：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You just have to transform with the `array` function a list of lists, where
    each internal list is a row matrix; or you create a vector with your data and
    then reshape it in the shape of your desired matrix:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要使用 `array` 函数将一个列表的列表转换，其中每个内部列表都是一个行矩阵；或者你可以创建一个包含你的数据的向量，然后将其重塑成你想要的矩阵形状：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'In NumPy there are also special functions for rapidly creating matrices of
    ones and zeros. As an argument, just specify the intended (`x`, `y`) shape in
    a tuple:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，也有用于快速创建全一和全零矩阵的特殊函数。作为参数，只需在元组中指定预期的 (`x`, `y`) 形状即可：
- en: '`all_zeros = np.zeros((5,3))`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`all_zeros = np.zeros((5,3))`'
- en: '`all_ones = np.ones((5,3))`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`all_ones = np.ones((5,3))`'
- en: The information present in the set of observations from the past, that we are
    using as *X*, can deeply affect how we are going to build the link between our
    *X* and the *y*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的作为 *X* 的过去观察集的信息可以深刻地影响我们如何建立 *X* 和 *y* 之间的联系。
- en: 'In fact, usually it is the case that we do not know the full range of possible
    associations between *X* and *y* because:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，通常情况下，我们并不知道 *X* 和 *y* 之间可能存在的全部关联范围，因为：
- en: We have just observed a certain *X*, so our experience of *y* for a given *X*
    is biased, and this is a sampling bias because, as in a lottery, we have drawn
    only certain numbers in a game and not all the available ones
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们刚刚观察到某个 *X*，因此我们对给定 *X* 的 *y* 经验是有偏见的，这是一种抽样偏差，因为在彩票中，我们只抽取了游戏中的某些数字，而不是所有可用的数字
- en: We never observed certain *(X, y)* associations (please note the formulation
    in a tuple, indicating the interconnection between *X* and *y*), because they
    never happened before, but that does not exclude them from happening in the future
    (and incidentally we are striving to forecast the future)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从未观察到某些 *(X, y)* 关联（请注意括号中的表述，表示 *X* 和 *y* 之间的相互关系），因为它们以前从未发生过，但这并不意味着它们将来不会发生（顺便说一句，我们正在努力预测未来）
- en: There is little to do with the second problem, (we can extrapolate the future
    only through the directions pointed out by the past), but you can actually check
    how recent the data you are using is. If you are trying to forecast in a context
    that is very susceptible to changes and mutable from day to day, you have to keep
    in mind that your data could quickly become outdated and you may be unable to
    guess new trends. An example of a mutable context where we constantly need to
    update models is the advertising sector (where the competitive scenery is frail
    and continually changing). Consequently, you continually need to gather fresher
    data that could allow you to build a much more effective supervised algorithm.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与第二个问题关系不大，（我们只能通过过去指出的方向来预测未来），但你实际上可以检查你使用的数据有多新。如果你试图在一个非常容易变化且每天都会变化的背景下进行预测，你必须记住你的数据可能会很快过时，你可能无法预测新的趋势。一个需要不断更新模型的可变环境例子是广告行业（那里的竞争环境脆弱且不断变化）。因此，你需要不断地收集更新的数据，这可能会让你构建一个更有效的监督算法。
- en: 'As for the first problem, you can solve it using more and more cases from different
    sources. The more you sample, the more likely your drawn set of *X* will resemble
    a complete set of possible and true associations of *X* with *y*. This is understandable
    via an important idea in probability and statistics: the law of large numbers.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 至于第一个问题，你可以通过使用来自不同来源的越来越多的情况来解决它。你抽取的样本越多，你的 *X* 集合就越有可能类似于 *X* 与 *y* 可能和真实关联的完整集合。这可以通过概率和统计学中的一个重要思想来理解：大数定律。
- en: The law of large numbers suggests that, as the number of your experiments grows,
    so the likelihood that the average of their results will represent the true value
    (that the experiments themselves are trying to figure out) will increase.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 大数定律表明，随着你实验数量的增加，其结果平均值代表真实值的可能性（即实验本身试图弄清楚的价值）也会增加。
- en: Supervised algorithms learn from large samples of historical data, called **batches**,
    fetched all at once from large data repositories, such as databases or data lakes.
    Alternatively, they also could pick the examples that are most useful for their
    learning by themselves and ignore the bulk of the data (this is called **active
    learning** and it is a kind of semi-supervised learning that we won't discuss
    here).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法通过从大型数据存储库（如数据库或数据湖）一次性获取的历史数据的大量样本进行学习，称为**批次**。或者，它们也可以自行选择对学习最有用的示例，并忽略大量数据（这被称为**主动学习**，它是一种半监督学习，我们在这里不会讨论）。
- en: If our environment is fast-paced, they also could just stream data as it is
    available, continuously adapting to any new association between the predictive
    variables and the response (this is called online learning and we will discuss
    it in [Chapter 7](part0046_split_000.html#1BRPS2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 7. Online and Batch Learning"), *Online and Batch Learning*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的环境节奏快，它们也可以像数据可用时一样实时流数据，持续适应预测变量和响应之间的任何新关联（这被称为在线学习，我们将在[第7章](part0046_split_000.html#1BRPS2-a2faae6898414df7b4ff4c9a487a20c6
    "第7章。在线和批量学习")中讨论，*在线和批量学习*）。
- en: Another important aspect of the *X* matrix of predictors to be considered is
    that up to now we assumed that we could deterministically derive the response
    *y* using the information in the matrix *X*. Unfortunately this is not always
    so in the real world and it is not rare that you actually try to figure out your
    response *y* using a completely wrong set of predictive *X*. In such cases, you
    have to figure out that you are actually wasting your time in trying to fit something
    working between your *X* and *y* and that you should look for some different *X*
    (again more data in the sense of more variables).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 预测 *X* 矩阵的另一个重要方面是，到目前为止，我们假设我们可以确定性推导出响应 *y*，使用矩阵 *X* 中的信息。不幸的是，在现实世界中并不总是如此，而且你实际上试图使用一组完全错误的预测
    *X* 来确定你的响应 *y* 的情况并不少见。在这种情况下，你必须意识到你实际上在浪费时间试图在 *X* 和 *y* 之间拟合某种工作关系，你应该寻找一些不同的
    *X*（在更多变量的意义上，即更多的数据）。
- en: 'According to the model used, having more variables and cases is usually beneficial
    under different points of view. More cases reduces the possibility of learning
    from a biased and limited set of observations. Many algorithms can better estimate
    their internal parameters (and produce more accurate predictions) if trained using
    large sets of observations. Also, having more variables at hand can be beneficial,
    but in the sense that it increases the chance of having explicative features to
    be used for machine learning. Many algorithms are in fact sensitive to redundant
    information and noise present in features, consequently requiring some feature
    selection to reduce the predictors involved in the model. This is quite the case
    with linear regression, which can surely take advantage of more cases for its
    training, but it should also receive a parsimonious and efficient set of features
    to perform at its best. Another important aspect to know about the *X* matrix
    is that it should be made up solely of numbers. Therefore, it really matters what
    you are working with. You can work with the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所使用的模型，从不同角度来看，拥有更多的变量和案例通常是有益的。更多的案例减少了从有偏见和有限的观察集中学习的可能性。许多算法如果使用大量观察集进行训练，可以更好地估计其内部参数（并产生更准确的预测）。此外，拥有更多的变量在手也可能有益，但这种益处在于它增加了使用解释性特征进行机器学习的机会。实际上，许多算法对特征中存在的冗余信息和噪声很敏感，因此需要一些特征选择来减少模型中涉及的预测因子。这在线性回归中尤为如此，它当然可以利用更多的案例进行训练，但它也应该接收到一个简约且高效的特性集，以发挥其最佳性能。关于
    *X* 矩阵的另一个重要方面是，它应该仅由数字组成。因此，你正在处理的内容真的很重要。你可以处理以下内容：
- en: Physical measurements, which are always OK because they are naturally numbers
    (for example, height)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理测量值，因为它们自然是数字（例如，身高）
- en: Human measurements, which are a bit less OK, but are still fine when they have
    a certain order (that is, all numbers that we give as scores based on our judgment)
    and so they can be converted into rank numbers (such as 1, 2, and 3 for the first,
    second, and third values, and so on)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类测量值，虽然稍微差一些，但如果有一定的顺序（即，我们根据判断给出的所有数字），仍然是可以接受的，因此它们可以被转换为排名数字（例如，第一、第二和第三值分别为1、2和3，依此类推）
- en: We call such values quantitative measurements. We expect quantitative measurement
    to be continuous and that means a quantitative variable can take any real positive
    or negative number as a valid value. Human measurements are usually only positive,
    starting from zero or one, so it is just a fair approximation to consider them
    quantitative.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这样的值为定量测量。我们期望定量测量是连续的，这意味着定量变量可以取任何有效的正数或负数作为值。人类测量值通常只有正数，从零或一开始，因此将它们视为定量测量是合理的。
- en: For physical measurements, in statistics, we distinguish between interval and
    ratio variables. The difference is that ratio variables have a natural zero whereas
    in interval data the zero is an arbitrary one. A good example is temperature;
    in fact, unless you use the Kelvin scale, whose zero is an absolute one, both
    Fahrenheit and Celsius have arbitrary scales. The main implication is about the
    ratios (if the zero is arbitrary, the ratio is also arbitrary).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，对于物理测量值，我们区分区间变量和比例变量。区别在于比例变量有一个自然的零点，而在区间数据中，零点是任意的。一个很好的例子是温度；事实上，除非你使用开尔文温标，其零点是绝对的，否则华氏和摄氏都有任意的刻度。主要影响在于比率（如果零是任意的，比率也是任意的）。
- en: 'Human measurements that are numerical are called **ordinal variables**. Unlike
    interval data, ordinal data does not have a natural zero. Moreover, the interval
    between each value on an interval scale is equal and regular; however, in an ordinal
    scale, though the distance between the values is the same, their real distance
    could be very different. Let''s think of a scale made of three textual values:
    good, average, and bad. Next, let''s say that we arbitrarily decide that good
    is 3, average is 2, and bad is 1\. We call this arbitrary assignment of values
    ordinal encoding. Now, from a mathematical point of view, though the interval
    between 3 and 2 in respect of the interval from 2 to 1 is the same (that is, one
    point), are we really sure that the real distance between good and average is
    the same as that from average and bad? For instance, in terms of customer satisfaction,
    does it costs the same effort going from an evaluation of bad to one of average
    and from one of average to one of excellent?'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数值的人类测量被称为**有序变量**。与区间数据不同，有序数据没有自然零点。此外，区间尺度上每个值之间的间隔是相等且规则的；然而，在有序尺度上，尽管值之间的距离相同，但它们的实际距离可能非常不同。让我们考虑一个由三个文本值组成的尺度：好、平均和坏。接下来，让我们随意决定好是3，平均是2，坏是1。我们称这种随意的值赋为有序编码。现在，从数学的角度来看，尽管3和2之间的间隔与2和1之间的间隔相同（即一个点），但我们真的确定好和平均之间的实际距离与平均和坏之间的距离相同吗？例如，从客户满意度的角度来看，从差到平均再到优秀的努力是否花费相同的精力？
- en: Qualitative measurements (for example, a value judgment such as good, average,
    or bad, or an attribute such as being colored red, green, or blue) need some work
    to be done, some clever data manipulation, but they can still be part of our *X*
    matrix using the right transformation. Even more unstructured qualitative information
    (such as text, sound, or a drawing) can be transformed and reduced to a pool of
    numbers and can be ingested into an *X* matrix.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 定性测量（例如，如好、平均或坏这样的价值判断，或如红色、绿色或蓝色这样的属性）需要进行一些工作，一些巧妙的数据操作，但它们仍然可以通过适当的转换成为我们的
    *X* 矩阵的一部分。甚至更无结构的定性信息（如文本、声音或绘图）也可以转换并减少到数字池中，并可以摄入到 *X* 矩阵中。
- en: Qualitative variables can be stored as numbers into single-value vectors or
    they can have a vector for each class. In such a case, we are talking about binary
    variables (also called dummy variables in statistical language).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 定性变量可以存储为单值向量中的数字，或者每个类别都有一个向量。在这种情况下，我们谈论的是二元变量（在统计语言中也称为虚拟变量）。
- en: We are going to discuss in greater detail how to transform the data at hand,
    especially if its type is qualitative, into an input matrix suitable for supervised
    learning in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5章[数据准备](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "第5章。数据准备")中更详细地讨论如何将手头的数据（特别是如果其类型是定性数据）转换成适合监督学习的输入矩阵。
- en: 'As a starting point before working on the data itself, it is necessary to question
    the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际处理数据之前，有必要质疑以下问题：
- en: The quality of the data—that is, whether the data available can really represent
    the right information pool for extracting *X*-*y* rules
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的质量——也就是说，可用的数据是否真的能代表提取 *X*-*y* 规则的正确信息池
- en: The quantity of data—that is, checking how much data is available, keeping in
    mind that, for building robust machine learning solutions, it is safer to have
    a large variety of variables and cases (at least when you're dealing with thousands
    of examples)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的数量——也就是说，检查有多少数据可用，记住，为了构建稳健的机器学习解决方案，拥有大量不同变量和案例更安全（至少当你处理数千个示例时）
- en: The extension of data in time—that is, checking how much time the data spans
    in the past (since we are learning from the past)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据在时间上的扩展——也就是说，检查数据在过去覆盖了多少时间（因为我们是从过去学习的）
- en: Reflecting on response variables
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反思响应变量
- en: Reflecting on the role of the response variable, our attention should be first
    drawn to what type of variable we are going to predict, because that will distinguish
    the type of supervised problem to be solved.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 反思响应变量的作用，我们首先应该关注的是我们将要预测的变量类型，因为这将区分要解决的监督问题的类型。
- en: If our response variable is a quantitative one, a numeric value, our problem
    will be a regression one. Ordinal variables can be solved as a regression problem,
    especially if they take many different distinct values. The output of a regression
    supervised algorithm is a value that can be directly used and compared with other
    predicted values and with the real response values used for learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的响应变量是一个定量变量，即一个数值，我们的问题将是回归问题。有序变量可以作为回归问题来解决，特别是如果它们包含许多不同的不同值。回归监督算法的输出是一个可以直接使用并与其他预测值以及用于学习的真实响应值进行比较的值。
- en: For instance, as an example of a regression problem, in the real estate business
    a regression model could predict the value of a house just from some information
    about its location and its characteristics, allowing an immediate discovery of
    market prices that are too cheap or too expensive by using the model's predictions
    as an indicator of a fair fact-based estimation (if we can reconstruct the price
    by a model, it is surely well justified by the value of the measurable characteristics
    we used as our predictors).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作为一个回归问题的例子，在房地产行业中，一个回归模型可以仅根据一些关于其位置和特性的信息来预测房屋的价值，这使得我们可以通过使用模型的预测作为公平事实估计的指标来立即发现市场价格是否过于便宜或昂贵（如果我们可以通过模型重建价格，那么它肯定是由我们用作预测者的可测量特性的价值所充分证明的）。
- en: If our response variable is a qualitative one, our problem is one of classification.
    If we have to guess between just two classes, our problem is called **a binary
    classification**; otherwise, if more classes are involved, it is a called a multi-label
    classification problem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的响应变量是一个定性变量，我们的问题就是分类问题。如果我们只需要在两个类别之间进行猜测，那么我们的问题被称为**二元分类**；否则，如果涉及更多类别，则称为多标签分类问题。
- en: For instance, if we want to guess the winner in a game between two football
    teams, we have a binary classification problem because we just need to know if
    the first team will win or not (the two classes are *team wins*, *team loses*).
    A multi-label classification could instead be used to predict which football team
    among a certain number will win (so in our prediction, the classes to be guessed
    are the teams).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想要猜测两支足球队之间的比赛胜者，我们面临的是一个二元分类问题，因为我们只需要知道第一支队伍是否会获胜（两类是*队伍获胜*，*队伍失败*）。相反，我们可以使用多标签分类来预测在特定数量的足球队中哪支队伍会获胜（因此，在我们的预测中，要猜测的类别是球队）。
- en: 'Ordinal variables, if they do not take many distinct values, can be solved
    as a multi-label classification problem. For instance, if you have to guess the
    final ranking of a team in a football championship, you could try to predict its
    final position in the leader board as a class. Consequently, in this ordinal problem
    you have to guess many classes corresponding to different positions in the championship:
    class 1 could represent the first position, class 2 the second position, and so
    on. In conclusion, you could figure the final ranking of a team as the positional
    class whose likelihood of winning is the greatest.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有序变量不包含很多不同的值，它们可以被视为多标签分类问题。例如，如果你必须猜测一支足球队在足球锦标赛中的最终排名，你可以尝试预测它在排行榜上的最终位置作为一个类别。因此，在这个有序问题中，你必须猜测与锦标赛中不同位置相对应的多个类别：类别1可以代表第一位置，类别2代表第二位置，依此类推。总之，你可以将一支球队的最终排名视为可能性最大的获胜位置类别。
- en: As for the output, classification algorithms can provide both classification
    into a precise class and an estimate of the probability of being part of any of
    the classes at hand.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 至于输出，分类算法可以提供对精确类别的分类以及属于任何手头类别的概率估计。
- en: Continuing with examples from the real estate business, a classification model
    could predict if a house could be a bargain or if it could increase its value
    given its location and its characteristics, thus allowing a careful investment
    selection.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 继续以房地产行业为例，一个分类模型可以预测一栋房子是否可以成为一笔划算的交易，或者根据其位置和特性是否能够增值，从而允许进行谨慎的投资选择。
- en: The most noticeable problem with response variables is their exactness. Measurement
    errors in regression problems and misclassification in classification ones can
    damage the ability of your model to perform well on real data by providing inaccurate
    information to be learned. In addition, biased information (such as when you provide
    cases of a certain class and not from all those available) can hurt the capacity
    of your model to predict in real-life situations because it will lead the model
    to look at data from a non-realistic point of view. Inaccuracies in the response
    variable are more difficult and more dangerous for your model than problems with
    your features.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 响应变量最明显的问题是其精确性。回归问题中的测量误差和分类问题中的误分类可以通过提供不准确的信息来损害模型在真实数据上的表现能力。此外，偏颇的信息（例如，当你只提供某个类别的案例而不是所有可用的案例时）可能会损害模型在现实生活中的预测能力，因为它会导致模型从非现实的角度看待数据。响应变量的不准确比特征问题对模型更困难、更危险。
- en: 'For single predictors, the outcome variable *y* is also a vector. In NumPy,
    you just set it up as a generic vector or as a column vector:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个预测变量，结果变量 *y* 也是一个向量。在 NumPy 中，你只需将其设置为通用向量或列向量：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The family of linear models
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型家族
- en: The family of linear models is so named because the function that specifies
    the relationship between the *X*, the predictors, and the *y*, the target, is
    a linear combination of the *X* values. A linear combination is just a sum where
    each addendum value is modified by a weight. Therefore, a linear model is simply
    a smarter form of a summation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型家族之所以得名，是因为指定 *X*（预测变量）、*y*（目标变量）之间关系的函数是 *X* 值的线性组合。线性组合实际上就是一个求和，其中每个加数都经过一个权重的调整。因此，线性模型仅仅是求和的一种更智能的形式。
- en: Of course there is a trick in this summation that makes the predictors perform
    like they do while predicting the answer value. As we mentioned before, the predictors
    should tell us something, they should give us some hint about the answer variable;
    otherwise any machine learning algorithm won't work properly. We can predict our
    response because the information about the answer is already somewhere inside
    the features, maybe scattered, twisted, or transformed, but it is just there.
    Machine learning just gathers and reconstructs such information.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在这个求和中有一个技巧，使得预测变量在预测答案值时表现得像它们那样。正如我们之前提到的，预测变量应该告诉我们一些信息，它们应该给我们一些关于答案变量的提示；否则，任何机器学习算法都无法正常工作。我们可以预测我们的响应，因为关于答案的信息已经存在于特征中，可能分散、扭曲或转换，但它确实在那里。机器学习只是收集和重建这样的信息。
- en: In linear models, such inner information is rendered obvious and extracted by
    the weights used for the summation. If you actually manage to have some meaningful
    predictors, the weights will just do all the heavy work to extract it and transform
    it into a proper and exact answer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性模型中，这种内部信息通过用于求和的权重变得明显并被提取出来。如果你实际上有一些有意义的预测变量，权重将只做所有繁重的工作来提取和转换它，形成一个适当且精确的答案。
- en: Since the *X* matrix is a numeric one, the sum of its elements will result in
    a number itself. Linear models are consequently the right tool for solving any
    regression problem, but they are not limited to just guessing real numbers. By
    a transformation of the response variable, they can be enabled to predict counts
    (positive integer numbers) and probabilities relative to being part of a certain
    group or class (or not).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *X* 矩阵是一个数值矩阵，其元素之和将得到一个数值本身。因此，线性模型是解决任何回归问题的正确工具，但它们不仅限于猜测实数。通过响应变量的转换，它们可以预测计数（正整数）和相对于属于某个特定组或类的概率（或不）。
- en: 'In statistics, the linear model family is called the **generalized linear model**
    (**GLM**). By means of special link functions, proper transformation of the answer
    variable, proper constraints on the weights and different optimization procedures
    (the learning procedures), GLM can solve a very wide range of different problems.
    In this book, our treatise won''t extend beyond what is necessary to the statistical
    field. However, we will propose a couple of models of the larger family of the
    GLM, namely linear regression and logistic regression; both methods are appropriate
    to solve the two most basic problems in data science: regression and classification.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，线性模型家族被称为**广义线性模型**（**GLM**）。通过特殊的链接函数、对答案变量的适当转换、对权重适当的约束以及不同的优化过程（学习过程），GLM可以解决非常广泛的各类问题。在这本书中，我们的论述不会超出统计领域所必需的内容。然而，我们将提出GLM大家族中的一些模型，即线性回归和逻辑回归；这两种方法都适合解决数据科学中最基本的两个问题：回归和分类。
- en: Because linear regression does not require any particular transformation of
    the answer variable and because it is conceptually the real foundation of linear
    models, we will start by understanding how it works. To make things easier, we
    will start from the case of a linear model using just a single predictor variable,
    a so-called **simple linear regression**. The predictive power of a simple linear
    regression is very limited in comparison with its multiple form, where many predictors
    at once contribute to the model. However, it is much easier to understand and
    figure out its functioning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性回归不需要对答案变量进行任何特定的转换，并且它在概念上是线性模型的真实基础，因此我们将首先了解它是如何工作的。为了使事情更容易理解，我们将从一个仅使用单个预测变量的线性模型案例开始，即所谓的**简单线性回归**。与同时使用许多预测变量的多重形式相比，简单线性回归的预测能力非常有限。然而，它更容易理解和弄清楚其工作原理。
- en: Preparing to discover simple linear regression
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备发现简单的线性回归
- en: We provide some practical examples in Python throughout the book and do not
    leave explanations about the various regression models at a purely theoretical
    level. Instead, we will explore together some example datasets, and systematically
    illustrate to you the commands necessary to achieve a working regression model,
    interpret its structure, and deploy a predicting application.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整本书中提供了一些Python的实际示例，并且不会将各种回归模型仅停留在纯粹的理论层面上的解释。相反，我们将一起探索一些示例数据集，并系统地向您展示实现工作回归模型、解释其结构和部署预测应用的必要命令。
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: A dataset is a data structure containing predictive variables and sometimes
    response ones. For machine learning purposes, it can be structured or semi-structured
    into a matrix form, in the shape of a table with rows and columns.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个包含预测变量有时还包括响应变量的数据结构。在机器学习方面，它可以被结构化或半结构化为矩阵形式，呈现为具有行和列的表格。
- en: For the initial presentation of the linear regression in its simple version
    (using only one predictive variable to forecast the response variable), we have
    chosen a couple of datasets relative to real estate evaluation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归简单版本（仅使用一个预测变量来预测响应变量）的初始介绍，我们选择了一些与房地产评估相关的数据集。
- en: Real estate is quite an interesting topic for an automatic predictive model
    since there is quite a lot of freely available data from censuses and, being an
    open market, even more data can be scraped from websites monitoring the market
    and its offers. Moreover, because the renting or buying of a house is quite an
    important economic decision for many individuals, online services that help to
    gather and digest the large amounts of available information are indeed a good
    business model idea.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 房地产是一个非常适合自动预测模型的有趣话题，因为从人口普查中可以获取大量免费数据，而且作为一个开放的市场，还可以从监控市场和其提供的网站中抓取更多数据。此外，由于租房或买房对许多个人来说是一个相当重要的经济决策，因此帮助收集和整理大量可用信息的在线服务确实是一个很好的商业模式想法。
- en: The first dataset is quite a historical one. Taken from the paper by Harrison,
    D. and Rubinfeld, D.L. *Hedonic Housing Prices and the Demand for Clean Air* (J.
    Environ. Economics & Management, vol.5, 81-102, 1978), the dataset can be found
    in many analysis packages and is present at the UCI Machine Learning Repository
    ([https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集是一个非常历史性的数据集。它来自 Harrison, D. 和 Rubinfeld, D.L. 的论文 *Hedonic Housing Prices
    and the Demand for Clean Air*（J. Environ. Economics & Management，vol.5，81-102，1978），该数据集可以在许多分析包中找到，并存在于
    UCI 机器学习存储库中（[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)）。
- en: The dataset is made up of 506 census tracts of Boston from the 1970 census and
    it features 21 variables regarding various aspects that could impact real estate
    value. The target variable is the median monetary value of the houses, expressed
    in thousands of USD. Among the available features, there are some fairly obvious
    ones such as the number of rooms, the age of the buildings, and the crime levels
    in the neighborhood, and some others that are a bit less obvious, such as the
    pollution concentration, the availability of nearby schools, the access to highways,
    and the distance from employment centers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由 1970 年人口普查中的波士顿 506 个街区组成，并包含 21 个可能影响房地产价值的变量。目标变量是房屋的中位货币价值，以千美元为单位表示。在可用的特征中，有一些相当明显，例如房间数量、建筑物的年龄和社区的犯罪水平，还有一些不那么明显的特征，例如污染浓度、附近学校的可用性、通往高速公路的通道以及就业中心的距离。
- en: The second dataset from the Carnegie Mellon University Statlib repository ([https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing))
    contains 20,640 observations derived from the 1990 US Census. Each observation
    is a series of statistics (9 predictive variables) regarding a block group—that
    is, approximately 1,425 individuals living in a geographically compact area. The
    target variable is an indicator of the house value of that block (technically
    it is the natural logarithm of the median house value at the time of the census).
    The predictor variables are basically median income.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学 Statlib 存储库的第二个数据集（[https://archive.ics.uci.edu/ml/datasets/Housing](https://archive.ics.uci.edu/ml/datasets/Housing)）包含从
    1990 年美国人口普查中得出的 20,640 个观测值。每个观测值是一系列统计数据（9 个预测变量），涉及一个街区组——即大约 1,425 个居住在地理上紧凑区域的人。目标变量是该街区房屋价值的指标（技术上说是普查时中位房屋价值的自然对数）。预测变量基本上是中位收入。
- en: The dataset has been used in Pace and Barry (1997), *Sparse Spatial Autoregressions*,
    *Statistics and Probability Letters*, ([http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf](http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf)),
    a paper on regression analysis including spatial variables (information about
    places including their position or their nearness to other places in the analysis).
    The idea behind the dataset is that variations of house values can be explained
    by exogenous variables (that is, external to the house itself) representing population,
    the density of buildings, and the population's affluence aggregated by area.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已被 Pace 和 Barry (1997) 在 *Sparse Spatial Autoregressions*，*Statistics and
    Probability Letters* 中使用，[http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf](http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf)），这是一篇关于包括空间变量的回归分析的论文（分析中关于位置的信息，包括它们在分析中的位置或与其他地点的邻近程度）。该数据集背后的想法是，房屋价值的变异性可以通过代表人口、建筑密度和按区域汇总的人口富裕程度的内生变量（即，房屋本身之外）来解释。
- en: 'The code for downloading the data is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据的代码如下：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Starting from the basics
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从基础知识开始
- en: 'We will start exploring the first dataset, the Boston dataset, but before delving
    into numbers, we will upload a series of helpful packages that will be used during
    the rest of the chapter:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始探索第一个数据集，波士顿数据集，但在深入数字之前，我们将上传一系列将在本章剩余部分使用的有用包：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If you are working from an IPython Notebook, running the following command
    in a cell will instruct the Notebook to represent any graphic output in the Notebook
    itself (otherwise, if you are not working on IPython, just ignore the command
    because it won''t work in IDEs such as Python''s IDLE or Spyder):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 IPython Notebook，可以在单元格中运行以下命令来指示 Notebook 在其自身中显示任何图形输出（如果你不在 IPython
    中工作，只需忽略此命令，因为它在 Python 的 IDLE 或 Spyder 等IDE中无法工作）：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To immediately select the variables that we need, we just frame all the data
    available into a Pandas data structure, `DataFrame`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了立即选择我们需要的变量，我们只需将所有可用的数据框架成一个Pandas数据结构，即`DataFrame`。
- en: 'Inspired by a similar data structure present in the R statistical language,
    a `DataFrame` renders data vectors of different types easy to handle under the
    same dataset variable, offering at the same time much convenient functionality
    for handling missing values and manipulating data:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 受到R统计语言中存在的类似数据结构的启发，`DataFrame`使得在同一数据集变量下处理不同类型的数据向量变得容易，同时提供了处理缺失值和操作数据的许多便利功能：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, we are ready to build our first regression model, learning directly
    from the data present in our Pandas DataFrame.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经准备好构建我们的第一个回归模型，直接从我们的Pandas DataFrame中的数据学习。
- en: As we mentioned, linear regression is just a simple summation, but it is indeed
    not the simplest model possible. The simplest is the statistical mean. In fact,
    you can simply guess by always using the same constant number, and the mean very
    well absolves such a role because it is a powerful descriptive number for data
    summary.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，线性回归只是一个简单的求和，但它确实不是可能的最简单模型。最简单的是统计均值。实际上，你可以简单地通过始终使用相同的常数来猜测，而均值很好地承担了这样的角色，因为它是一个强大的数据总结描述数。
- en: The mean works very well with normally distributed data but often it is quite
    suitable even for different distributions. A normally distributed curve is a distribution
    of data that is symmetric and has certain characteristics regarding its shape
    (a certain height and spread).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 均值与正态分布数据配合得非常好，但通常对于不同的分布也非常适用。一个正态分布的曲线是数据分布，它是对称的，并且具有关于其形状的某些特性（一定的高度和分布）。
- en: The characteristics of a normal distribution are defined by formulas and there
    are appropriate statistical tests to find out if your variable is normal or not,
    since many other distributions resemble the bell shape of the normal one and many
    different normal distributions are generated by different mean and variance parameters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正态分布的特性由公式定义，并且有适当的统计测试来确定你的变量是否为正态分布，因为许多其他分布与正态分布的钟形相似，并且许多不同的正态分布是由不同的均值和方差参数生成的。
- en: The key to understanding if a distribution is normal is the **probability density
    function** (**PDF**), a function describing the probability of values in the distribution.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 理解一个分布是否为正态分布的关键是**概率密度函数**（**PDF**），这是一个描述分布中值概率的函数。
- en: 'In the case of a normal distribution, the PDF is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在正态分布的情况下，概率密度函数（PDF）如下：
- en: '![Starting from the basics](img/00008.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![从基础开始](img/00008.jpeg)'
- en: 'In such a formulation, the symbol *µ* represents the mean (which coincides
    with the median and the mode) and the symbol *σ* is the variance. Based on different
    means and variances, we can calculate different value distributions, as the following
    code demonstrates and visualizes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种表述中，符号*µ*代表均值（与中位数和众数相同）和符号*σ*代表方差。基于不同的均值和方差，我们可以计算不同的值分布，如下面的代码所示并进行可视化：
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Starting from the basics](img/00009.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![从基础开始](img/00009.jpeg)'
- en: Because of its properties, the normal distribution is a fundamental distribution
    in statistics since all statistical models involve working on normal variables.
    In particular, when the mean is zero and the variance is one (unit variance),
    the normal distribution, called a standard normal distribution under such conditions,
    has even more favorable characteristics for statistical models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其性质，正态分布是统计学中的一个基本分布，因为所有统计模型都涉及对正态变量的处理。特别是，当均值为零且方差为1（单位方差）时，在这种条件下称为标准正态分布的正态分布，对于统计模型具有更加有利的特性。
- en: Anyway, in the real world, normally distributed variables are instead rare.
    Consequently, it is important to verify that the actual distribution we are working
    on is not so far from an ideal normal one or it will pose problems in your expected
    results. Normally distributed variables are an important requirement for statistical
    models (such as mean and, in certain aspects, linear regression). On the contrary,
    machine learning models do not depend on any previous assumption about how your
    data should be distributed. But, as a matter of fact, even machine learning models
    work well if data has certain characteristics, so working with a normally distributed
    variable is preferable to other distributions. Throughout the book, we will provide
    warnings about what to look for and check when building and applying machine learning
    solutions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，在现实世界中，正态分布的变量是罕见的。因此，验证我们正在工作的实际分布是否与理想的正态分布相差不远是很重要的，否则它可能会在你的预期结果中引起问题。正态分布变量是统计模型（如平均值和，在某种程度上，线性回归）的重要要求。相反，机器学习模型不依赖于任何关于你的数据应该如何分布的先验假设。但事实上，即使机器学习模型在数据具有某些特征时也能很好地工作，因此，与正态分布变量一起工作比其他分布更可取。在整本书中，我们将提供有关在构建和应用机器学习解决方案时应该寻找和检查的内容的警告。
- en: 'For the calculation of a mean, relevant problems can arise if the distribution
    is not symmetric and there are extreme cases. In such an occurrence, the extreme
    cases will tend to draw the mean estimate towards them, which consequently won''t
    match with the bulk of the data. Let''s then calculate the mean of the value of
    the 506 tracts in Boston:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于平均值的计算，如果分布不是对称的，或者存在极端情况，可能会出现相关的问题。在这种情况下，极端情况往往会将平均值估计值拉向它们，从而导致与大量数据不匹配。那么，让我们计算一下波士顿506个地块的价值的平均值：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this case, we calculated the mean using a method available in the Pandas
    DataFrame; however, the NumPy function `mean` can be also called to calculate
    a mean from an array of data:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用Pandas DataFrame中可用的方法计算了平均值；然而，也可以调用NumPy函数`mean`从数据数组中计算平均值：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In terms of a mathematical formulation, we can express this simple solution
    as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学公式来说，我们可以将这个简单的解决方案表达如下：
- en: '![Starting from the basics](img/00010.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![从基础开始](img/00010.jpeg)'
- en: 'We can now evaluate the results by measuring the error produced in predicting
    the real *y* values by this rule. Statistics suggest that, to measure the difference
    between the prediction and the real value, we should square the differences and
    then sum them all. This is called **the squared sum of errors**:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过测量预测真实*y*值时产生的误差来评估结果。统计学表明，为了衡量预测值和真实值之间的差异，我们应该将差异平方，然后将它们全部相加。这被称为**误差平方和**：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now that we have calculated it, we can visualize it as a distribution of errors:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出来了，我们可以将其可视化为误差分布：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Starting from the basics](img/00011.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![从基础开始](img/00011.jpeg)'
- en: The plot shows how frequent certain errors are in respect of their values. Therefore,
    you will immediately notice that most errors are around zero (there is a high
    density around that value). Such a situation can be considered a good one, since
    in most cases the mean is a good approximation, but some errors are really very
    far from the zero and they can attain considerable values (don't forget that the
    errors are squared, anyway, so the effect is emphasized). When trying to figure
    out such values, your approach will surely lead to a relevant error and we should
    find a way to minimize it using a more sophisticated approach.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了某些错误值出现的频率。因此，你将立即注意到，大多数错误都围绕零（该值周围有很高的密度）。这种情况可以被认为是一个好情况，因为在大多数情况下，平均值是一个很好的近似值，但一些错误确实非常远离零，并且可以达到相当大的值（别忘了，错误是平方的，所以效果被强调）。当试图找出这样的值时，你的方法肯定会引导到一个相关的错误，我们应该找到一种更复杂的方法来最小化它。
- en: A measure of linear relationship
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性关系的度量
- en: Evidently, the mean is not a good representative of certain values, but it is
    certainly a good baseline to start from. Certainly, an important problem with
    the mean is its being fixed, whereas the target variable is changeable. However,
    if we assume that the target variable changes because of the effect of some other
    variable we are measuring, then we can adjust the mean with respect to the variations
    in cause.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，均值不是某些值的良好代表，但它确实是一个良好的起点。当然，均值的一个重要问题是它是固定的，而目标变量是可变的。然而，如果我们假设目标变量是由于我们正在测量的某些其他变量的影响而变化的，那么我们可以根据原因的变化调整均值。
- en: One improvement on our previous approach could be to build a mean conditional
    on certain values of another variable (or even more than one) actually related
    to our target, whose variation is somehow similar to the variation of the target
    one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们之前的方法的一个改进可能是构建一个基于另一个变量（甚至更多）的某些值（实际上与我们的目标相关）的均值，其变化在某种程度上类似于目标变量的变化。
- en: Intuitively, if we know the dynamics we want to predict with our model, we can
    try to look for variables that we know can impact the answer values.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，如果我们知道我们想要用我们的模型预测的动态，我们可以尝试寻找我们知道可以影响答案值的变量。
- en: In the real estate business, we actually know that usually the larger a house
    is, the more expensive it is; however, this rule is just part of the story and
    the price is affected by many other considerations. For the moment, we will keep
    it simple and just assume that an extension to a house is a factor that positively
    affects the price, and consequently, more space equals more costs when building
    the house (more land, more construction materials, more work, and consequently
    a higher price).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在房地产业务中，我们实际上知道通常房子越大，价格越贵；然而，这个规则只是故事的一部分，价格还受到许多其他因素的影响。目前，我们将保持简单，并假设房屋的扩建是一个正影响价格的因子，因此，更多的空间意味着更高的成本（更多的土地、更多的建筑材料、更多的工作，以及因此更高的价格）。
- en: Now, we have a variable that we know should change with our target and we just
    need to measure it and extend our initial formula based on constant values with
    something else.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个我们知道应该随着我们的目标变量变化的变量，我们只需要测量它，并基于常数值扩展我们的初始公式。
- en: 'In statistics, there is a measure that helps to measure how (in the sense of
    how much and in what direction) two variables relate to each other: **correlation**.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，有一个度量可以帮助衡量两个变量如何（在多大程度上以及朝哪个方向）相互关联：**相关性**。
- en: In correlation, a few steps are to be considered. First, your variables have
    to be standardized (or your result won't be a correlation but a covariation, a
    measure of association that is affected by the scale of the variables you are
    working with).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关性分析中，需要考虑几个步骤。首先，你的变量必须标准化（否则你的结果将不是相关性而是协变，这是一种受你正在处理的变量规模影响的关联度量的指标）。
- en: In statistical *Z* score standardization, you subtract from each variable its
    mean and then you divide the result by the standard deviation. The resulting transformed
    variable will have a mean of 0 and a standard deviation of 1 (or unit variance,
    since variance is the squared standard deviation).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计的 *Z* 分数标准化中，你需要从每个变量中减去其均值，然后将结果除以标准差。得到的转换后的变量将具有均值为 0 和标准差为 1（或单位方差，因为方差是标准差的平方）。
- en: 'The formula for standardizing a variable is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化一个变量的公式如下：
- en: '![A measure of linear relationship](img/00012.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![线性关系的度量](img/00012.jpeg)'
- en: 'This can be achieved in Python using a simple function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过使用 Python 中的一个简单函数来实现：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After standardizing, you compare the squared difference of each variable with
    its own mean. If the two differences agree in sign, their multiplication will
    become positive (evidence that they have the same directionality); however, if
    they differ, the multiplication will turn negative. By summing all the multiplications
    between the squared differences, and dividing them by the number of observations,
    you will finally get the correlation which will be a number ranging from -1 to
    1.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化后，你比较每个变量的平方差与其自身均值的平方差。如果这两个差值在符号上相同，它们的乘积将是正的（表明它们具有相同的方向性）；然而，如果它们不同，乘积将变为负的。通过将所有平方差之间的乘积相加，并将它们除以观察次数，你最终将得到一个介于
    -1 到 1 之间的相关系数。
- en: The absolute value of the correlation will provide you with the intensity of
    the relation between the two variables compared, 1 being a sign of a perfect match
    and zero a sign of complete independence between them (they have no relation between
    them). The sign instead will hint at the proportionality; positive is direct (when
    one grows the other does the same), negative is indirect (when one grows, the
    other shrinks).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系的绝对值将向您提供两个比较变量之间关系的强度，1表示完美匹配，而0表示它们之间完全独立（它们之间没有任何关系）。相反，符号将暗示比例；正号是直接的（当一个增长时，另一个也增长），负号是间接的（当一个增长时，另一个缩小）。
- en: 'Covariance can be expressed as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差可以表示如下：
- en: '![A measure of linear relationship](img/00013.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![线性关系的度量](img/00013.jpeg)'
- en: 'Whereas, Pearson''s correlation can be expressed as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 而皮尔逊相关系数可以表示如下：
- en: '![A measure of linear relationship](img/00014.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![线性关系的度量](img/00014.jpeg)'
- en: 'Let''s check these two formulations directly on Python. As you may have noticed,
    Pearson''s correlation is really covariance calculated on standardized variables,
    so we define the `correlation` function as a wrapper of both the `covariance`
    and `standardize` ones (you can find all these functions ready to be imported
    from `Scipy`; we are actually recreating them here just to help you understand
    how they work):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接在Python中检查这些公式。如您所注意到的，皮尔逊相关系数实际上是标准化的变量上的协方差，因此我们将`correlation`函数定义为`covariance`和`standardize`函数的包装器（您可以从`Scipy`导入所有这些函数；我们在这里实际上是在重新创建它们，只是为了帮助您理解它们是如何工作的）：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our correlation estimation for the relation between the value of the target
    variable and the average number of rooms in houses in the area is 0.695, which
    is positive and remarkably strong, since the maximum positive score of a correlation
    is 1.0.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对目标变量值与该区域房屋平均房间数之间关系的相关估计值为0.695，这是一个正的相关性，并且非常显著，因为相关性的最大正值是1.0。
- en: Tip
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a way to estimate if a correlation is relevant or not, just square it; the
    result will represent the percentage of the variance shared by the two variables.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计一个相关性是否相关，只需将其平方；结果将表示两个变量共享的方差百分比。
- en: 'Let''s graph what happens when we correlate two variables. Using a **scatterplot**,
    we can easily visualize the two involved variables. A scatterplot is a graph where
    the values of two variables are treated as Cartesian coordinates; thus, for every
    (*x*, *y*) value a point is represented in the graph:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制当我们对两个变量进行相关性分析时会发生什么。使用**散点图**，我们可以轻松地可视化两个涉及的变量。散点图是一种图表，其中两个变量的值被视为笛卡尔坐标；因此，对于每个(*x*,
    *y*)值，图上都有一个点表示：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![A measure of linear relationship](img/00015.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![线性关系的度量](img/00015.jpeg)'
- en: 'The scatterplot also plots the average value for both the target and the predictor
    variables as dashed lines. This divides the plot into four quadrants. If we compare
    it with the previous covariance and correlation formulas, we can understand why
    the correlation value was close to 1: in the bottom-right and in top-left quadrants,
    there are just a few mismatching points where one of variables is above its average
    and the other is below its own.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图还绘制了目标和预测变量的平均值作为虚线。这把图表分成四个象限。如果我们将其与先前的协方差和相关性公式进行比较，我们可以理解为什么相关性值接近1：在右下角和左上角象限中，只有少数不匹配的点，其中一个变量高于其平均值，而另一个变量低于其平均值。
- en: A perfect match (correlation values of 1 or -1) is possible only when the points
    are in a straight line (and all points are therefore concentrated in the right-uppermost
    and left-lowermost quadrants). Thus, correlation is a measure of linear association,
    of how close to a straight line your points are. Ideally, having all your points
    on a single line favors a perfect mapping of your predictor variable to your target.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 完美匹配（相关性值为1或-1）仅在点位于一条直线上（因此所有点都集中在右上角和左下角象限）时才可能。因此，相关性是线性关联的度量，是您的点接近直线的程度。理想情况下，所有点都在一条直线上有利于将预测变量完美映射到目标变量。
- en: Extending to linear regression
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展到线性回归
- en: Linear regression tries to fit a line through a given set of points, choosing
    the best fit. The best fit is the line that minimizes the summed squared difference
    between the value dictated by the line for a certain value of *x* and its corresponding
    *y* values. (It is optimizing the same squared error that we met before when checking
    how good a mean was as a predictor.)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归试图通过一组给定的点拟合一条线，选择最佳拟合。最佳拟合是使由线决定的 *x* 的值与其对应的 *y* 值之间的平方差之和最小的线。（这是在检查均值作为预测指标的好坏时遇到的相同平方误差的优化。）
- en: 'Since linear regression is a line; in bi-dimensional space (*x*, *y*), it takes
    the form of the classical formula of a line in a Cartesian plane: *y = mx + q*,
    where *m* is the angular coefficient (expressing the angle between the line and
    the *x* axis) and *q* is the intercept between the line and the *x* axis.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性回归是一条线；在二维空间（*x*，*y*）中，它采用笛卡尔平面上经典线的公式形式：*y = mx + q*，其中 *m* 是角度系数（表示线与
    *x* 轴之间的角度）和 *q* 是线与 *x* 轴之间的截距。
- en: 'Formally, machine learning indicates the correct expression for a linear regression
    as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，机器学习将线性回归的正确表达式表示如下：
- en: '![Extending to linear regression](img/00016.jpeg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![扩展到线性回归](img/00016.jpeg)'
- en: Here, again, *X* is a matrix of the predictors, *β* is a matrix of coefficients,
    and *β[0]* is a constant value called the **bias** (it is the same as the Cartesian
    formulation, only the notation is different).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里再次，*X* 是预测变量的矩阵，*β* 是系数矩阵，而 *β[0]* 是一个称为 **偏差** 的常数值（它与笛卡尔公式相同，只是符号不同）。
- en: We can better understand its functioning mechanism by seeing it in action with
    Python, first using the `StatsModels` package, then using the Scikit-learn one.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 Python 看到它的实际运行机制来更好地理解其工作原理，首先使用 `StatsModels` 包，然后使用 Scikit-learn
    包。
- en: Regressing with Statsmodels
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Statsmodels 进行回归
- en: Statsmodels is a package designed with statistical analysis in mind; therefore,
    its function offers quite a rich output of statistical checks and information.
    Scalability is not an issue for the package; therefore, it is really a good starting
    point for learning, but is certainly not the optimal solution if you have to crunch
    quite large datasets (or even big data) because of its optimization algorithm.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Statsmodels 是一个考虑到统计分析的包；因此，它的函数提供了相当丰富的统计检查和信息输出。可扩展性对该包来说不是问题；因此，它确实是学习的良好起点，但如果你必须处理相当大的数据集（甚至大数据）时，由于其优化算法，它肯定不是最佳解决方案。
- en: 'There are two different methods (two modules) to work out a linear regression
    with Statsmodels:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种不同的方法（两个模块）使用 Statsmodels 进行线性回归：
- en: '`statsmodels.api`: This works with distinct predictor and answer variables
    and requires you to define any transformation of the variables on the predictor
    variable, including adding the intercept'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsmodels.api`：这适用于不同的预测变量和答案变量，并要求你定义预测变量上的任何变量转换，包括添加截距'
- en: '`statsmodels.formula.api`: This works in a similar way to R, allowing you to
    specify a functional form (the formula of the summation of the predictors)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`statsmodels.formula.api`：这类似于 R，允许你指定一个函数形式（预测变量求和的公式）'
- en: We will illustrate our example using the `statsModels.api`; however, we will
    also show you an alternative method with `statsmodels.formula.api`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `statsModels.api` 展示我们的示例；然而，我们还将展示使用 `statsmodels.formula.api` 的另一种方法。
- en: 'As a first step, let''s upload both the modules of Statsmodels, naming them
    as conventionally indicated in the package documentation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们上传 Statsmodels 的两个模块，按照包文档中的惯例命名：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As a second step, it is necessary to define the *y* and *X* variables:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第二步，有必要定义 *y* 和 *X* 变量：
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The *X* variable needs to be extended by a constant value `()`; the bias will
    be calculated accordingly. In fact, as you remember, the formula of a linear regression
    is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* 变量需要通过一个常数值 `()` 进行扩展；偏差将相应地计算。实际上，正如你记得的那样，线性回归的公式如下：'
- en: '![Regressing with Statsmodels](img/00017.jpeg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Statsmodels 进行回归](img/00017.jpeg)'
- en: 'However, using `StatsModels.api`, the formula actually becomes the following:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用 `StatsModels.api`，公式实际上变成了以下形式：
- en: '![Regressing with Statsmodels](img/00018.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Statsmodels 进行回归](img/00018.jpeg)'
- en: This can be interpreted as a combination of the variables in *X*, multiplied
    by its corresponding *β* value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以解释为 *X* 中变量的组合，乘以其对应的 *β* 值。
- en: Consequently, the predictor *X* now contains both the predictive variable and
    a unit constant. Also, *β* is no longer a single coefficient, but a vector of
    coefficients.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，预测变量*X*现在既包含预测变量又包含一个单位常数。此外，*β*不再是一个单一的系数，而是一系列系数。
- en: 'Let''s have a visual confirmation of this by requiring the first values of
    the Pandas DataFrame using the `head` method:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`head`方法查看Pandas DataFrame的第一个值来验证这一点：
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Regressing with Statsmodels](img/00019.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![使用Statsmodels进行回归](img/00019.jpeg)'
- en: 'At this point, we just need to set the initialization of the linear regression
    calculation:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们只需要设置线性回归计算的初始化：
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Also, we need to ask for the estimation of the regression coefficients, the
    *β* vector:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要请求回归系数的估计，即*β*向量：
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we had wanted to manage the same result using the `StatsModels.formula.api`,
    we should have typed the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用`StatsModels.formula.api`来管理相同的结果，我们应该输入以下内容：
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The previous two code lines simultaneously comprise both steps seen together,
    without requiring any particular variable preparation since the bias is automatically
    incorporated. In fact, the specification about how the linear regression should
    work is incorporated into the string `target ~ RM`, where the variable name left
    of the tilde (`~`) indicates the answer variable, the variable name (or names,
    in the case of a multiple regression analysis) on the right being for the predictor.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 前两条代码行同时包含了两个步骤，无需进行任何特定的变量准备，因为偏差是自动包含的。实际上，关于线性回归应该如何工作的规范被包含在字符串`target ~
    RM`中，其中波浪号(`~`)左侧的变量名表示答案变量，右侧的变量名（或名称，在多重回归分析的情况下）表示预测变量。
- en: Actually, `smf.ols` expects quite a different input compared to `sm.OLS`, because
    it can accept our entire original dataset (it selects what variables are to be
    used by using the provided formula), whereas `sm.OLS` expects a matrix containing
    just the features to be used for prediction. Consequently, some caution has to
    be exercised when using two such different approaches.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`smf.ols`期望的输入与`sm.OLS`相当不同，因为它可以接受我们的整个原始数据集（它通过提供的公式选择要使用的变量），而`sm.OLS`期望一个仅包含用于预测的特征的矩阵。因此，在使用这两种截然不同的方法时必须谨慎行事。
- en: 'A summary (a method of the fitted model) can quickly tell you everything that
    you need to know about regression analysis. In case you have tried `statsmodesl.formula.api`,
    we also re-initialize the linear regression using the `StatsModels.api` since
    they are not working on the same *X* and our following code relies on `sm.OLS`
    specifications:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 一个总结（拟合模型的某种方法）可以快速告诉你关于回归分析所需了解的所有内容。如果你尝试过`statsmodesl.formula.api`，我们也重新使用`StatsModels.api`初始化线性回归，因为它们不是在相同的*X*上工作，而我们的后续代码依赖于`sm.OLS`规范：
- en: '[PRE23]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Regressing with Statsmodels](img/00020.jpeg)![Regressing with Statsmodels](img/00021.jpeg)![Regressing
    with Statsmodels](img/00022.jpeg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![使用Statsmodels进行回归](img/00020.jpeg)![使用Statsmodels进行回归](img/00021.jpeg)![使用Statsmodels进行回归](img/00022.jpeg)'
- en: You will receive quite a long series of tables containing many statistical tests
    and information. Though quite daunting at the beginning, you actually do not need
    all these outputs, unless the purpose of your analysis is a statistical one. Data
    science is mainly concerned with real models working on predicting real data,
    not on formally correct specifications of statistical problems. Nevertheless,
    some of these outputs are still useful for successful model building and we are
    going to provide you with an insight into the main figures.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你将收到一系列相当长的表格，包含许多统计测试和信息。虽然一开始可能相当令人畏惧，但实际上你不需要所有这些输出，除非你的分析目的是统计性的。数据科学主要关注的是实际模型在预测真实数据上的工作，而不是在统计问题的形式化规范上。尽管如此，其中一些输出对于成功构建模型仍然很有用，我们将为你提供对主要数字的洞察。
- en: 'Before explaining the outputs, we first need to extract two elements from the
    fitted model: the coefficients and the predictions calculated on the data on which
    we built the model. They both are going to come in very handy during the following
    explanations:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释输出之前，我们首先需要从拟合模型中提取两个元素：系数和基于我们构建模型的数据的预测。它们在接下来的解释中都将非常有用：
- en: '[PRE24]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The coefficient of determination
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定系数
- en: 'Let''s start from the first table of results. The first table is divided into
    two columns. The first one contains a description of the fitted model:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个结果表开始。第一个表分为两列。第一列包含拟合模型的描述：
- en: '**Dep. Variable**: It just reminds you what the target variable was'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因变量**: 这只是提醒你目标变量是什么'
- en: '**Model**: Another reminder of the model that you have fitted, the OLS is ordinary
    least squares, another way to refer to linear regression'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**: 这是您已拟合的模型的另一个提醒，OLS是普通最小二乘法，也是线性回归的另一种说法'
- en: '**Method**: The parameters fitting method (in this case least squares, the
    classical computation method)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法**: 参数拟合方法（在这种情况下是最小二乘法，经典计算方法）'
- en: '**No. Observations**: The number of observations that have been used'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观测数**: 已使用的观测值数量'
- en: '**DF Residuals**: The degrees of freedom of the residuals, which is the number
    of observations minus the number of parameters'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差DF**: 残差的自由度，即观测值数量减去参数数量'
- en: '**DF Model**: The number of estimated parameters in the model (excluding the
    constant term from the count)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型DF**: 模型中估计的参数数量（不包括常数项）'
- en: 'The second table gives a more interesting picture, focusing how good the fit
    of the linear regression model is and pointing out any possible problems with
    the model:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张表提供了一个更有趣的图景，重点关注线性回归模型的拟合程度，并指出模型可能存在的问题：
- en: '**R-squared**: This is the coefficient of determination, a measure of how well
    the regression does with respect to a simple mean.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R平方**: 这是决定系数，衡量回归相对于简单平均的效果。'
- en: '**Adj. R-squared**: This is the coefficient of determination adjusted based
    on the number of parameters in a model and the number of observations that helped
    build it.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整R平方**: 这是基于模型中的参数数量和帮助构建它的观测值数量的决定系数的调整值。'
- en: '**F-statistic**: This is a measure telling you if, from a statistical point
    of view, all your coefficients, apart from the bias and taken together, are different
    from zero. In simple words, it tells you if your regression is really better than
    a simple average.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F统计量**: 这是一个衡量指标，告诉你从统计学的角度来看，除了偏差之外，所有系数加在一起是否与零不同。简单来说，它告诉你你的回归是否真的比简单平均更好。'
- en: '**Prob (F-statistic)**: This is the probability that you got that F-statistic
    just by lucky chance due to the observations that you have used (such a probability
    is actually called the **p-value** of F-statistic). If it is low enough you can
    be confident that your regression is really better than a simple mean. Usually
    in statistics and science a test probability has to be equal or lower than 0.05
    (a conventional criterion of statistical significance) for having such a confidence.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F统计量的概率**: 这是你仅仅因为使用了观测值而得到那个F统计量的概率（这种概率实际上被称为F统计量的**p值**）。如果它足够低，你可以有信心你的回归确实比简单平均更好。通常在统计学和科学中，测试概率必须等于或低于0.05（统计显著性的传统标准）才能有这种信心。'
- en: '**AIC**: This is the **Akaike Information Criterion**. AIC is a score that
    evaluates the model based on the number of observations and the complexity of
    the model itself. The lesser the AIC score, the better. It is very useful for
    comparing different models and for statistical variable selection.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AIC**: 这是赤池信息准则。AIC是根据观测值数量和模型本身的复杂性来评估模型的分数。AIC分数越低，越好。它对于比较不同模型和进行统计变量选择非常有用。'
- en: '**BIC**: This is the **Bayesian Information Criterion**. It works as AIC, but
    it presents a higher penalty for models with more parameters.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BIC**: 这是贝叶斯信息准则。它像AIC一样工作，但它对参数更多的模型施加更高的惩罚。'
- en: Most of these statistics make sense when we are dealing with more than one predictor
    variable, so they will be discussed in the next chapter. Thus, for the moment,
    as we are working with a simple linear regression, the two measures that are worth
    examining closely are F-statistic and R-squared. F-statistic is actually a test
    that doesn't tell you too much if you have enough observations and you can count
    on a minimally correlated predictor variable. Usually it shouldn't be much of
    a concern in a data science project.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理多个预测变量时，大多数这些统计数据都是有意义的，因此它们将在下一章中进行讨论。因此，目前，由于我们正在处理简单的线性回归，值得仔细检查的两个度量标准是F统计量和R平方。实际上，F统计量是一个测试，如果你有足够的观测值并且可以依赖一个最小相关性的预测变量，它不会告诉你太多。通常，在数据科学项目中，这不应该是一个很大的问题。
- en: R-squared is instead much more interesting because it tells you how much better
    your regression model is in comparison to a single mean. It does so by providing
    you with a percentage of the unexplained variance of a mean as a predictor that
    actually your model was able to explain.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: R平方更有趣，因为它告诉你你的回归模型与单个均值相比有多好。它是通过提供一个百分比来做到这一点的，即均值的未解释方差，实际上你的模型能够解释。
- en: 'If you want to compute the measure yourself, you just have to calculate the
    sum of squared errors of the mean of the target variable. That''s your baseline
    of unexplained variance (the variability in house prices that in our example we
    want to explain by a model). If from that baseline you subtract the sum of squared
    errors of your regression model, you will get the residual sum of squared errors,
    which can be compared using a division with your baseline:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己计算这个度量，你只需要计算目标变量均值的平方误差和。这就是你的未解释方差的基础线（在我们例子中，我们希望通过模型来解释的房价的变异性）。如果你从这个基础线中减去回归模型的平方误差和，你将得到残差平方误差和，可以通过除以基础线来比较：
- en: '[PRE25]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Tip
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: When working with floats, rounding errors are possible, so don't be afraid if
    some of the lesser decimals don't match in your calculations; if they match the
    8th decimal, you can be quite confident that the result is the same.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理浮点数时，可能会出现舍入误差，所以如果你的计算中某些小数位不匹配，不要害怕；如果它们匹配到第8位小数，你可以相当自信地认为结果是相同的。
- en: Ideally, if you can reduce your sum of squared errors of the regression to zero,
    you will get the maximum percentage of explained variance—that is, a score of
    1.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，如果你能将回归的平方误差和减少到零，你将得到最大的解释方差百分比——即得分为1。
- en: The R-squared measure is also comparable with the percentage that you obtain
    squaring the correlation between your predictor and the target variable.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: R平方度量也可以与你的预测变量和目标变量之间相关性的平方所获得的百分比进行比较。
- en: 'In our example, it is 0.484, which actually is exactly our R-squared correlation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，它是0.484，这实际上正好是我们的R平方相关系数：
- en: '[PRE26]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As we have seen, R-squared is perfectly aligned with the squared errors that
    the linear regression is trying to minimize; thus, a better R-squared means a
    better model. However, there are some problems with the measure (and with linear
    regression itself, actually) when working with more predictors at once. Again,
    we have to wait until we model more predictors at once; therefore, just for a
    simple linear regression, a better R-squared should hint at a better model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，R平方与线性回归试图最小化的平方误差完全一致；因此，更好的R平方意味着更好的模型。然而，当同时处理多个预测变量时，这个度量（以及线性回归本身）存在一些问题。再次强调，我们必须等到我们同时建模多个预测变量；因此，对于简单的线性回归来说，更好的R平方应该暗示着更好的模型。
- en: Meaning and significance of coefficients
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系数的意义和重要性
- en: 'The second output table informs us about the coefficients and provides us with
    a series of tests. These tests can make us confident that we have not been fooled
    by a few extreme observations in the foundations of our analysis or by some other
    problem:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出表告诉我们关于系数的信息，并提供了一系列测试。这些测试可以让我们确信，我们没有因为分析基础中的几个极端观察值或某些其他问题而被误导：
- en: '**coef**: The estimated coefficient'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**coef**：估计的系数'
- en: '**std err**: The standard error of the estimate of the coefficient; the larger
    it is, the more uncertain the estimation of the coefficient'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**std err**：系数估计的标准误差；它越大，系数的估计就越不确定'
- en: '**t**: The t-statistic value, a measure indicating whether the coefficient
    true value is different from zero'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t**：t统计量值，一个衡量系数真实值是否与零不同的指标'
- en: '**P > |t|**: The p-value indicating the probability that the coefficient is
    different from zero just by chance'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P > |t|**：表示系数不同于零的概率的p值，仅通过偶然性'
- en: '**[95.0% Conf. Interval]**: The lower and upper values of the coefficient,
    considering 95% of all the chances of having different observations and so different
    estimated coefficients'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[95.0% 置信区间]**：考虑所有可能的不同观察值和因此不同的估计系数的95%的所有机会，系数的下限和上限值'
- en: From a data science viewpoint, t-tests and confidence bounds are not very useful
    because we are mostly interested in verifying whether our regression is working
    while predicting answer variables. Consequently, we will focus just on the `coef`
    value (the estimated coefficients) and on their standard error.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据科学的角度来看，t检验和置信区间并不是非常有用，因为我们主要感兴趣的是验证我们的回归在预测答案变量时是否有效。因此，我们将只关注`coef`值（估计系数）及其标准误差。
- en: The coefficients are the most important output that we can obtain from our regression
    model because they allow us to re-create the weighted summation that can predict
    our outcomes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 系数是我们从回归模型中可以获得的最重要输出，因为它们允许我们重新创建加权求和，从而预测我们的结果。
- en: 'In our example, our coefficients are *−34.6706* for the bias (also called the
    **intercept**, recalling the formula for a line in a Cartesian space) and *9.1021*
    for the `RM` variable. Recalling our formula, we can plug in the numbers we obtained:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们的系数对于偏差（也称为**截距**，回忆一下笛卡尔空间中线的公式）是*−34.6706*，对于`RM`变量是*9.1021*。回忆我们的公式，我们可以将我们获得的数字代入：
- en: '![Meaning and significance of coefficients](img/00023.jpeg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![系数的意义和重要性](img/00023.jpeg)'
- en: 'Now, if you replace the betas and *X* with the estimated coefficients, and
    the variables'' names with *−34.6706* and *9.1021*, everything becomes the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你用估计的系数替换贝塔和*x*，用变量的名称替换*−34.6706*和*9.1021*，一切就变成了以下内容：
- en: '![Meaning and significance of coefficients](img/00024.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![系数的意义和重要性](img/00024.jpeg)'
- en: 'Now, if you know the average number of rooms in an area of Boston, you can
    make a quick estimate of the expected value. For instance, *x[RM]* is `4.55`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你知道波士顿某个地区的平均房间数量，你可以快速估计期望值。例如，*x[RM]*是`4.55`：
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We have to notice two points here. First, in such a formulation, the beta of
    each variable becomes its *unit change* measure, which corresponds to the change
    the outcome will undergo if the variable increases by one unit. In our case, our
    average room space becomes `5.55`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须注意两个点。首先，在这种公式中，每个变量的贝塔成为其*单位变化*的度量，这对应于变量增加一个单位时结果将经历的变化。在我们的例子中，我们的平均房间空间变为`5.55`：
- en: '[PRE28]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The increase for a unit change in *x[RM]* corresponds to a change in the outcome
    equivalent to *β[RM]*. The other point to be noticed is that, if our average room
    space becomes 1 or 2, our estimated value will turn negative, which is completely
    unrealistic. This is because the mapping between predictor and the target variable
    happened in a delimited bound of values of the predictor:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*x[RM]*单位变化引起的增加对应于结果的变化，相当于*β[RM]*。另一个需要注意的点是我们平均房间空间变为1或2时，我们的估计值将变为负数，这是完全不现实的。这是因为预测变量和目标变量之间的映射发生在预测变量值的限定范围内：'
- en: '[PRE29]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Whenever we try to estimate our answer values using an *x* (or a set of *X*)
    that is outside the boundaries we used for fitting the model, we risk a response
    that has not been optimized at all by the linear regression calculations. Expressed
    in another way, linear regression can learn what it sees, and, unless there is
    a clear linear functional form between the predictor and the target (they can
    be truly expressed as a line), you risk weird estimations when your predictors
    have an unusual value. In other words, a linear regression can always work within
    the range of values it learned from (this is called **interpolation**) but can
    provide correct values for its learning boundaries (a different predictive activity
    called **extrapolation**) only in certain conditions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时我们尝试使用一个*x*（或一组*x*）来估计答案值，而这个*x*（或一组*x*）超出了我们用于拟合模型的边界，我们都有可能得到一个线性回归计算根本未进行优化的响应。换一种说法，线性回归可以学习它所看到的东西，除非预测变量和目标变量之间存在清晰的线性函数形式（它们可以真正表示为一条线），否则当预测变量具有不寻常的值时，你可能会得到奇怪的估计。换句话说，线性回归总是在它从中学到的值范围内工作（这被称为**插值**），但只有在某些条件下才能为其学习边界提供正确的值（这被称为**外推**）。
- en: Tip
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As we previously mentioned, the number of observations used for fitting the
    model is of paramount importance to obtain a robust and reliable linear regression
    model. The more observations, the less likely the model is to be surprised by
    unusual values when running in production.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，用于拟合模型的观测数对于获得一个稳健且可靠的线性回归模型至关重要。观测数越多，模型在生产运行中遇到异常值的可能性就越小。
- en: Standard errors instead are very important because they signal a weak or unclear
    relationship between the predictor and the answer. You can notice this by dividing
    the standard error by its beta. If the ratio is 0.5 or even larger, then it's
    a clear sign that the model has little confidence that it provided you with the
    right coefficient estimates. Having more cases is always the solution because
    it can reduce the standard errors of the coefficients and improve our estimates;
    however, there are also other methods to reduce errors, such as removing the redundant
    variance present among the features by a principal component analysis or selecting
    a parsimonious set of predictors by greedy selections. All these topics will be
    discussed when we work with multiple predictors; at this point in the book, we
    will illustrate the remedies to such a problem.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 标准误差则非常重要，因为它们表明预测变量与答案之间关系薄弱或不明确。你可以通过将标准误差除以其 beta 值来注意到这一点。如果比率是 0.5 或更大，那么这是一个明显的信号，表明模型对其提供的正确系数估计几乎没有信心。拥有更多案例总是解决方案，因为它可以减少系数的标准误差并提高我们的估计；然而，也有其他方法可以减少误差，例如通过主成分分析去除特征之间的冗余方差，或者通过贪婪选择选择一个简约的预测变量集。所有这些主题将在我们处理多个预测变量时讨论；在本书的这一部分，我们将说明解决此类问题的方法。
- en: Evaluating the fitted values
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估拟合值
- en: 'The last table deals with an analysis of the residuals of the regression. The
    residuals are the difference between the target values and the predicted fitted
    values:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一张表处理的是回归残差的分解。残差是目标值与预测拟合值之间的差异：
- en: '**Skewness**: This is a measure of the symmetry of the residuals around the
    mean. For symmetric distributed residuals, the value should be around zero. A
    positive value indicates a long tail to the right; a negative value a long tail
    to the left.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏度**：这是衡量残差围绕平均值对称性的指标。对于对称分布的残差，其值应接近零。正值表示右侧有长尾；负值表示左侧有长尾。'
- en: '**Kurtosis**: This is a measure of the shape of the distribution of the residuals.
    A bell-shaped distribution has a zero measure. A negative value points to a too
    flat distribution; a positive one has too great a peak.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**峰度**：这是衡量残差分布形状的指标。钟形分布的测量值为零。负值指向过于平坦的分布；正值表示峰度过高。'
- en: '**Omnibus D''Angostino''s test**: This is a combined statistical test for skewness
    and kurtosis.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Omnibus D''Angostino 测试**：这是一个针对偏度和峰度的组合统计测试。'
- en: '**Prob(Omnibus)**: This is the Omnibus statistic turned into a probability.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Omnibus 概率**：这是将 Omnibus 统计量转换为概率。'
- en: '**Jarque-Bera**: This is another test of skewness and kurtosis.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jarque-Bera 统计量**：这是对偏度和峰度的另一种测试。'
- en: '**Prob (JB)**: This is the JB statistic turned into a probability.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JB 概率**：这是 JB 统计量转换为概率。'
- en: '**Durbin-Watson**: This is a test for the presence of correlation among the
    residuals (relevant during analysis of time-based data).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Durbin-Watson 统计量**：这是对残差之间相关性的测试（在分析基于时间的数据时相关）。'
- en: '**Cond. No**: This is a test for multicollinearity (we will deal with the concept
    of multicollinearity when working with many predictors).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件数**：这是对多重共线性（当处理多个预测变量时，我们将讨论多重共线性的概念）的测试。'
- en: 'A close analysis of residuals is quite relevant in statistical practice since
    it can highlight the presence of serious problems with regression analysis. When
    working with a single variable it is interesting to visually check its residuals
    to figure out if there are strange cases or if the residuals don''t distribute
    randomly. In particular, it is important to keep an eye out for any of these three
    problems showing up:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计实践中，对残差进行密切分析相当重要，因为它可以突出回归分析中存在严重问题的迹象。当处理单个变量时，有趣的是通过视觉检查其残差，以确定是否存在异常案例或残差是否随机分布。特别是，重要的是要密切关注以下三个问题中的任何一个出现：
- en: Values too far from the average. Large standardized residuals hint at a serious
    difficulty when modeling such observations. Also, in the process of learning these
    values, the regression coefficients may have been distorted.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过于偏离平均值的值。大的标准化残差暗示在建模此类观察值时存在严重困难。此外，在学习这些值的过程中，回归系数可能已经被扭曲。
- en: Different variance in respect of the value of the predictor. If the linear regression
    is an average conditioned on the predictor, dishomogeneous variance points out
    that the regression is not working properly when the predictor has certain values.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测变量值的方差不同。如果线性回归是基于预测变量的平均条件，则异方差性表明当预测变量具有某些值时，回归工作不正常。
- en: Strange shapes in the cloud of residual points may indicate that you need a
    more complex model for the data you are analyzing.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在残差点的云层中出现的奇异形状可能表明，你需要为正在分析的数据使用一个更复杂的模型。
- en: 'In our case, we can easily compute the residuals by subtracting the fitted
    values from the answer variable and then plotting the resulting standardized residuals
    in a graph:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们可以通过从答案变量中减去拟合值来轻松计算残差，然后在一个图表中绘制结果的标准残差：
- en: '[PRE30]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Evaluating the fitted values](img/00025.jpeg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![评估拟合值](img/00025.jpeg)'
- en: The resulting scatterplot indicates that the residuals show some of the problems
    we previously indicated as a warning that something is not going well with your
    regression analysis. First, there are a few points lying outside the band delimited
    by the two dotted lines at normalized residual values −3 and +3 (a range that
    should hypothetically cover 99.7% of values if the residuals have a normal distribution).
    These are surely influential points with large errors and they can actually make
    the entire linear regression under-perform. We will talk about possible solutions
    to this problem when we discuss outliers in the next chapter.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的散点图表明，残差显示出我们之前作为警告指出的一些问题，即你的回归分析中可能存在问题。首先，有一些点位于由标准化残差值-3和+3之间的两条虚线所界定的带状区域之外（如果残差呈正态分布，理论上应该覆盖99.7%的值）。这些肯定是具有大误差的显著点，它们实际上可能会使整个线性回归表现不佳。我们将在下一章讨论异常值时讨论此问题的可能解决方案。
- en: Then, the cloud of points is not at all randomly scattered, showing different
    variances at different values of the predictor variable (the **abscissa axis**)
    and you can spot unexpected patterns (points in a straight line, or the core points
    placed in a kind of U shape).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点云并非完全随机分布，它在预测变量的不同值（**横轴**）上显示出不同的方差，你可以发现意外的模式（直线上的点或以某种U形排列的核心点）。
- en: We are not at all surprised; the average number of rooms is likely a good predictor
    but it is not the only cause, or it has to be rethought as a direct cause (the
    number of rooms indicates a larger house, but what if the rooms are smaller than
    average?). This leads us to discuss whether a strong correlation really makes
    a variable a good working candidate for a linear relationship.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不感到惊讶；平均房间数可能是一个很好的预测变量，但它不是唯一的原因，或者它必须被重新考虑为直接原因（房间数表示更大的房子，但如果是平均以下的房间呢？）。这使我们讨论一个强烈的关联是否真的使变量成为线性关系的良好候选者。
- en: Correlation is not causation
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性不等于因果关系
- en: Actually, seeing a correlation between your predictor and your target variable,
    and managing to model it successfully using a linear regression, doesn't really
    mean that there is a causal relation between the two (though your regression may
    work very well, and even optimally).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，看到你的预测变量和目标变量之间存在相关性，并且能够成功地使用线性回归对其进行建模，这并不意味着两者之间真的存在因果关系（尽管你的回归可能非常有效，甚至是最优的）。
- en: Though using a data science approach, instead of a statistical one, will guarantee
    a certain efficacy in your model, it is easy to fall into some mistakes when having
    no clue why your target variable is correlated with a predictor.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用数据科学方法而不是统计方法可以保证你的模型具有一定的有效性，但在不知道为什么目标变量与预测变量相关时，很容易陷入一些错误。
- en: 'We will tell you about six different reasons, and offer a cautionary word to
    help you handle such predictors without difficulty:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将告诉你六个不同的原因，并提供一个警示词，帮助你轻松处理这样的预测变量：
- en: '**Direct causation**: *x* causes *y*; for instance, in the real estate business
    the value is directly proportional to the size of the house in square meters.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接因果关系**：*x* 导致 *y*；例如，在房地产业务中，价值与房屋的平方米数成正比。'
- en: '**Reciprocal effects**: *x* causes *y* but it is also influenced by *y*. This
    is quite typical of many macro-economic dynamics where the effect of a policy
    augments or diminishes its effects. As an example in real estate, high crime rates
    in an area can lower its prices but lower prices mean that the area could quickly
    become even more degraded and dangerous.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相互影响**：*x* 导致 *y*，但它也受到 *y* 的影响。这在许多宏观经济动态中很典型，政策的效果会增强或减弱其效果。例如，在房地产领域，一个地区的犯罪率可能会降低其价格，但低价意味着该地区可能会迅速变得更加恶化且危险。'
- en: '**Spurious causation**: This happens when the real cause is actually *z*, which
    causes both *x* and *y*; consequently it is just a fallacious illusion that *x*
    implies *y* because it is *z* behind the scenes. For instance, the presence of
    expensive art shops and galleries may seem to correlate with house prices; in
    reality, both are determined by the presence of affluent residents.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚假因果关系**：这发生在真实原因实际上是*z*，它同时导致*x*和*y*；因此，它只是一个错误的错觉，即*x*意味着*y*，因为背后是*z*。例如，昂贵艺术品商店和画廊的存在似乎与房价相关；实际上，两者都是由富裕居民的存在决定的。'
- en: '**Indirect causation**: *x* in reality is not causing *y* but it is causing
    something else, which then causes *y*. A good municipality investing in infrastructures
    after higher taxes can indirectly affect house prices because the area becomes
    more comfortable to live in, thus attracting more demand. Higher taxes, and thus
    more investments, indirectly affect house prices.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**间接因果关系**：实际上*x*并没有导致*y*，而是导致其他事物，然后这些事物又导致*y*。一个优秀的市政府在提高税收后投资基础设施可以间接影响房价，因为该地区变得更加宜居，从而吸引更多需求。更高的税收，因此更多的投资，间接影响房价。'
- en: '**Conditional effect**: *x* causes *y* in respect of the values of another
    variable *z*; for instance, when *z* has certain values *x* is not influencing
    *y* but, when *z* takes particular values, the *x* starts impacting *y*. We also
    call this situation interaction. For instance the presence of schools in an area
    can become an attractor when the crime rate is low, so it affects house prices
    only when there is little criminality.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件效应**：在另一个变量*z*的值方面，*x*导致*y*；例如，当*z*具有某些值时，*x*不会影响*y*，但当*z*取特定值时，*x*开始影响*y*。我们也将这种情况称为交互。例如，当犯罪率低时，一个地区的学校存在可以成为一个吸引点，因此它只会在犯罪性很少时影响房价。'
- en: '**Random effect**: Any recorded correlation between *x* and *y* has been due
    to a lucky sampling selection; in reality there is no relationship with *y* at
    all.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机效应**：任何记录的*x*和*y*之间的相关性都是由于幸运的抽样选择；实际上，与*y*之间根本不存在任何关系。'
- en: The ideal case is when you have a direct causation; then, you will have a predictor
    in your model that will always provide you with the best values to derive your
    responses.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的情况是您有一个直接的因果关系；那么，您的模型中将有预测因子，它将始终为您提供最佳值以推导出您的响应。
- en: In the other cases, it is likely that the imperfect cause-effect relationship
    with the target variable will lead to more noisy estimates, especially in production
    when you will have to work with data not seen before by the model.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，目标变量与不完美因果关系可能导致更嘈杂的估计，尤其是在生产中，您将不得不处理模型之前未见过的数据。
- en: Reciprocal effects are more typical of econometric models. They require special
    types of regression analysis. Including them in your regression analysis may improve
    your model; however, their role may be underestimated.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 互为因果关系在计量经济学模型中更为典型。它们需要特殊类型的回归分析。将它们包含在您的回归分析中可能会改进您的模型；然而，它们的作用可能被低估。
- en: Spurious and indirect causes will add some noise to your *x* and *y* relationship;
    this could bring noisier estimates (larger standard errors). Often, the solution
    is to get more observations for your analysis.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 虚假和间接原因会给您的*x*和*y*关系添加一些噪声；这可能导致更嘈杂的估计（更大的标准误差）。通常，解决方案是为您的分析获取更多观测值。
- en: Conditional effects, if not caught, can limit your model's ability to produce
    accurate estimates. If you are not aware of any of them, given your domain knowledge
    of the problem, it is a good step to check for any of them using some automatic
    procedure to test possible interactions between the variables.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有被发现，条件效应可能会限制您模型产生准确估计的能力。如果您对任何这些都不了解，根据您对问题的领域知识，使用一些自动程序检查可能的变量间交互是一个好步骤。
- en: Random effects are the worst possible thing that could happen to your model,
    but they are easily avoided if you follow the data science procedure that we will
    be describing in [Chapter 6](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 6. Achieving Generalization"), *Achieving Generalization*, when we deal
    with all the actions necessary to validate your model's results.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 随机效应可能是您模型可能遇到的最糟糕的事情，但如果您遵循我们在[第6章](part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6
    "第6章。实现泛化")中将要描述的数据科学程序，即*实现泛化*，当我们处理验证您模型结果所需的所有必要行动时，它们是容易避免的。
- en: Predicting with a regression model
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用回归模型进行预测
- en: When we plug the coefficients into the regression formula, predicting is just
    a matter of applying new data to the vector of coefficients by a matrix multiplication.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将系数代入回归公式时，预测只是将新数据应用于系数向量进行矩阵乘法的问题。
- en: 'First, you can rely on the fitted model by providing it with an array containing
    new cases. In the following example, you can see how, given the `Xp` variable
    with a single new case, this is easily predicted using the `predict` method on
    the fitted model:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以通过提供一个包含新案例数组的数组来依赖拟合模型。在以下示例中，你可以看到，给定单个新案例的`Xp`变量，使用拟合模型的`predict`方法可以轻松预测：
- en: '[PRE31]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'A nice usage of the `predict` method is to project the fitted predictions on
    our previous scatterplot to allow us to visualize the price dynamics in respect
    of our predictor, the average number of rooms:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`方法的一个很好的用法是将拟合预测投影到我们之前的散点图上，以便我们可以可视化与我们的预测变量，平均房间数量相关的价格动态：'
- en: '[PRE32]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Predicting with a regression model](img/00026.jpeg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归模型进行预测](img/00026.jpeg)'
- en: '[PRE33]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Besides the `predict` method, generating the predictions is quite easy by just
    using the `dot` function in `NumPy`. After preparing an *X* matrix containing
    both the variable data and the bias (a column of ones) and the coefficient vectors,
    all you have to do is to multiply the matrix by the vector. The result will itself
    be a vector of length equal to the number of observations:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`predict`方法外，通过仅使用`NumPy`中的`dot`函数，生成预测相当简单。在准备一个包含变量数据和偏差（一列）的*X*矩阵以及系数向量后，你所要做的就是将矩阵乘以向量。结果本身将是一个长度等于观测数数量的向量：
- en: '[PRE34]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: A comparison of the results obtained by the `predict` method and this simple
    multiplication will reveal a perfect match. Because predicting from a linear regression
    is simple, if necessary you could even implement this multiplication on your application
    in a language different from Python. In such a case, you will just need to find
    a matrix calculation library or program a function by yourself. To our knowledge,
    you can easily write such a function even in the SQL script language.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`predict`方法和这种简单乘法获得的结果比较将揭示完美匹配。因为从线性回归进行预测很简单，如果需要，你甚至可以在不同于Python的语言中实现这种乘法。在这种情况下，你只需要找到一个矩阵计算库或自己编写一个函数。据我们所知，你甚至可以在SQL脚本语言中轻松编写这样的函数。
- en: Regressing with Scikit-learn
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Scikit-learn进行回归
- en: 'As we have seen while working with the `StatsModels` package, a linear model
    can be built using a more oriented machine learning package such as Scikit-learn.
    Using the `linear_model` module, we can set a linear regression model specifying
    that the predictors shouldn''t be normalized and that our model should have a
    bias:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在使用`StatsModels`包时所见，可以使用更面向机器学习的包，如Scikit-learn来构建线性模型。使用`linear_model`模块，我们可以设置一个线性回归模型，指定预测变量不应归一化，并且我们的模型应该有偏差：
- en: '[PRE35]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Data preparation, instead, requires counting the observations and carefully
    preparing the predictor array to specify its two dimensions (if left as a vector,
    the fitting procedure will raise an error):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备，相反，需要计算观测值并仔细准备预测数组以指定其两个维度（如果保留为向量，拟合过程将引发错误）：
- en: '[PRE36]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After completing all the previous steps, we can fit the model using the `fit`
    method:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有前面的步骤后，我们可以使用`fit`方法拟合模型：
- en: '[PRE37]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: A very convenient feature of the Scikit-learn package is that all the models,
    no matter their type of complexity, share the same methods. The `fit` method is
    always used for fitting and it expects an *X* and a *y* (when the model is a supervised
    one). Instead, the two common methods for making an exact prediction (always for
    regression) and its probability (when the model is probabilistic) are `predict`
    and `predict_proba`, respectively.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包的一个非常方便的特性是，所有模型，无论其复杂度类型如何，都共享相同的方法。`fit`方法始终用于拟合，并期望一个*X*和一个*y*（当模型是监督模型时）。相反，两个常见的用于精确预测（总是用于回归）及其概率（当模型是概率模型时）的方法分别是`predict`和`predict_proba`。
- en: 'After fitting the model, we can inspect the vector of the coefficients and
    the bias constant:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合模型后，我们可以检查系数向量和偏差常数：
- en: '[PRE38]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Using the `predict` method and slicing the first 10 elements of the resulting
    list, we output the first 10 predicted values:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`predict`方法和结果列表的前10个元素进行切片，我们输出前10个预测值：
- en: '[PRE39]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As previously seen, if we prepare a new matrix and we add a constant, we can
    calculate the results by ourselves using a simple matrix–vector multiplication:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果我们准备一个新的矩阵并添加一个常数，我们可以通过简单的矩阵-向量乘法自行计算结果：
- en: '[PRE40]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As expected, the result of the product provides us with the same estimates
    as the `predict` method:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，乘积的结果为我们提供了与`predict`方法相同的估计：
- en: '[PRE41]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: At this point, it would be natural to question the usage of such a `linear_model`
    module. Compared with the previous functions offered by Statsmodels, Scikit-learn
    seems to offer little statistical output, and one seemingly with many linear regression
    features stripped out. In reality, it offers exactly what is needed in data science
    and it is perfectly fast-performing when dealing with large datasets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，质疑这种`linear_model`模块的使用是很自然的。与Statsmodels之前提供的函数相比，Scikit-learn似乎提供的统计输出很少，而且看起来很多线性回归功能都被移除了。实际上，它提供了数据科学中所需的一切，并且在处理大型数据集时性能非常出色。
- en: 'If you are working on IPython, just try the following simple test to generate
    a large dataset and check the performance of the two versions of linear regression:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用IPython，只需尝试以下简单的测试来生成一个大型数据集并检查两种线性回归版本的性能：
- en: '[PRE42]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After generating ten million observations of a single variable, start by measuring
    using the `%%time` magic function for IPython. This magic function automatically
    computes how long it takes to complete the calculations in the IPython cell:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成一百万个单一变量的观测值之后，首先使用IPython的`%%time`魔术函数来测量。这个魔术函数会自动计算在IPython单元格中完成计算所需的时间：
- en: '[PRE43]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, it is the turn of the Statsmodels package:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，轮到Statsmodels包登场了：
- en: '[PRE44]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Though a single variable is involved in the model, Statsmodels's default algorithms
    prove to be three times slower than Scikit-learn. We will repeat this test in
    the next chapter, too, when using more predictive variables in one go and other
    different `fit` methods.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型中只涉及一个变量，但Statsmodels的默认算法证明比Scikit-learn慢三倍。我们将在下一章重复这个测试，当一次使用更多的预测变量和其他不同的`fit`方法时。
- en: Minimizing the cost function
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小化成本函数
- en: 'At the core of linear regression, there is the search for a line''s equation
    that it is able to minimize the sum of the squared errors of the difference between
    the line''s *y* values and the original ones. As a reminder, let''s say our regression
    function is called `h`, and its predictions `h(X)`, as in this formulation:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的核心是寻找一条直线的方程，该方程能够最小化直线*y*值与原始值之间差异的平方和。作为提醒，让我们假设我们的回归函数被称为`h`，其预测为`h(X)`，如下所示：
- en: '![Minimizing the cost function](img/00027.jpeg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![最小化成本函数](img/00027.jpeg)'
- en: 'Consequently, our cost function to be minimized is as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们要最小化的成本函数如下：
- en: '![Minimizing the cost function](img/00028.jpeg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![最小化成本函数](img/00028.jpeg)'
- en: There are quite a few methods to minimize it, some performing better than others
    in the presence of large quantities of data. Among the better performers, the
    most important ones are **Pseudoinverse** (you can find this in books on statistics),
    **QR factorization**, and **gradient descent**.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以最小化它，其中一些在大数据量下表现优于其他方法。在表现较好的方法中，最重要的是**伪逆**（你可以在统计学的书籍中找到它）、**QR分解**和**梯度下降**。
- en: Explaining the reason for using squared errors
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释使用平方误差的原因
- en: Looking under the hood of a linear regression analysis, at first it could be
    puzzling to realize that we are striving to minimize the squared differences between
    our estimates and the data from which we are building the model. Squared differences
    are not as intuitively explainable as absolute differences (the difference without
    a sign).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 查看线性回归分析的内部机制，一开始可能会令人困惑，因为我们正在努力最小化我们的估计值与构建模型的数据之间的平方差。平方差不如绝对差（不带符号的差）直观易懂。
- en: For instance, if you have to predict a monetary value, such as the price of
    a stock or the return from an advertising activity, you are more interested in
    knowing your absolute error, not your R-squared one, which could be perceived
    as misleading (since with squares larger losses are emphasized).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你需要预测一个货币价值，比如股票价格或广告活动的回报，你更感兴趣的是知道你的绝对误差，而不是你的R平方值，这可能会被误解（因为平方会放大更大的损失）。
- en: As we mentioned before, linear regression takes its steps from the statistical
    knowledge domain, and there are actually quite a few reasons in statistics that
    make minimizing a squared error preferable to minimizing an absolute one.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，线性回归从统计知识领域吸取了步骤，实际上在统计学中有很多原因使得最小化平方误差比最小化绝对误差更可取。
- en: 'Unfortunately, such reasons are quite complex and too technical and consequently
    beyond the real scope of this book; however, from a high-level point of view,
    a good and reasonable explanation is that squaring nicely achieves two very important
    objectives:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这样的理由相当复杂，过于技术化，因此超出了这本书的实际范围；然而，从高层次的角度来看，一个良好合理的解释是，平方很好地实现了两个非常重要的目标：
- en: It removes negative values; therefore opposite errors won't reciprocally cancel
    each other when summed
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它消除了负值；因此，当求和时，相反的误差不会相互抵消
- en: It emphasizes larger differences, because as they are squared they will proportionally
    increase the sum of the errors compared to a simple sum of absolute values
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它强调了更大的差异，因为当它们被平方时，与简单求和绝对值相比，它们将成比例地增加误差总和
- en: Minimizing the squared differences with an estimator leads us to use the mean
    (as we suggested before as a basic model, without providing any justification
    for it).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 使用估计值最小化平方差异使我们使用平均值（正如我们之前所建议的，作为一个基本模型，没有提供任何理由）。
- en: 'Let''s just check together using Python, without developing all the formulations.
    Let''s define an `x` vector of values:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起用Python来检查，而不需要开发所有公式。让我们定义一个包含值的`x`向量：
- en: '[PRE45]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s also define a function returning the cost function as squared differences:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也定义一个返回成本函数（平方差异）的函数：
- en: '[PRE46]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Using the `fmin` minimization procedure offered by the `scipy` package, we
    try to figure out, for a vector (which will be our x vector of values), the value
    that makes the least squared summation:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`scipy`包提供的`fmin`最小化过程，我们试图找出一个向量（这将是我们的`x`向量），其值使得平方和最小：
- en: '[PRE47]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We just output our best `e` value and verify if it actually is the mean of
    the `x` vector:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只输出我们最好的`e`值，并验证它是否确实是`x`向量的平均值：
- en: '[PRE48]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If instead we try to figure out what minimizes the sum of absolute errors:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试找出什么最小化了绝对误差的总和：
- en: '[PRE49]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We will find out that it is the median, not the mean. Unfortunately, the median
    does not have the same statistical properties as the mean.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会发现它是中位数，而不是平均值。不幸的是，中位数并不具备与平均值相同的统计特性。
- en: Pseudoinverse and other optimization methods
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伪逆和其他优化方法
- en: 'There is an analytical formula for solving a regression analysis and getting
    a vector of coefficients out of data, minimizing the cost function:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一种解析公式可以用于解决回归分析，并从数据中得到系数向量，最小化成本函数：
- en: '![Pseudoinverse and other optimization methods](img/00029.jpeg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![伪逆和其他优化方法](img/00029.jpeg)'
- en: Demonstrating this equation goes beyond the practical scope of this book, but
    we can test it using the power of Python coding.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 展示这个方程超出了这本书的实用范围，但我们可以利用Python编程的力量来测试它。
- en: 'We can therefore directly solve for this by using `np.linalg.inv` from `NumPy`
    to obtain the inverse of a matrix, or alternative methods such as solving for
    *w* in linear equations that are called normal equations:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以直接通过使用`NumPy`的`np.linalg.inv`来求解矩阵的逆，或者使用其他方法，如求解线性方程中的`w`，这些方程被称为正规方程：
- en: '![Pseudoinverse and other optimization methods](img/00030.jpeg)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![伪逆和其他优化方法](img/00030.jpeg)'
- en: 'Here the function `np.linalg.solve` can do all the calculations for us:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，函数`np.linalg.solve`可以为我们完成所有计算：
- en: '[PRE50]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The only problem in solving a linear regression using these approaches is complexity,
    possibly some loss in accuracy of the computation when directly calculating the
    inverse using `np.linalg.inv`, and, naturally, the fact that the *X^TX* multiplication
    has to be invertible (sometimes it isn't when using multiple variables that are
    strongly related to each other).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些方法解决线性回归的唯一问题是复杂性，可能是在直接使用`np.linalg.inv`计算逆时，计算精度可能有所损失，以及，当然，*X^TX*乘积必须是可逆的（有时当使用相互之间高度相关的多个变量时，它可能不是可逆的）。
- en: Even using another algorithm (QR factorization, a core algorithm in Statsmodels
    that can overcome some previously quoted numeric misbehaviors), the worst performance
    can be estimated to be *O(n³)*; that is, cubic complexity.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用另一个算法（QR分解，Statsmodels的核心算法，可以克服一些之前引用的数值问题），最坏的性能可以估计为*O(n³)*；即，立方复杂度。
- en: Using Pseudoinverse (in NumPy, `np.linalg.pinv`) can help achieve a *O(n^m)*
    complexity where *m* is estimated to be <2.37 (approximately quadratic, then).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 使用伪逆（在NumPy中，`np.linalg.pinv`）可以帮助实现 *O(n^m)* 的复杂度，其中 *m* 估计小于2.37（大约是二次的）。
- en: This can really be a great limitation in being able to quickly estimate linear
    regression analysis. In fact, if you are working with *10³* observations, a feasible
    number of observations in statistical analysis, it will take at worst *10⁹* computations;
    however, when working with data science projects, which easily reach *10⁶* observations,
    the number of computations required to find the solution to a regression problem
    may rocket to *10^(18)*.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上可能是一个很大的限制，限制了快速估计线性回归分析的能力。事实上，如果你正在处理 *10³* 个观测值，这在统计分析中是一个可行的观测值数量，最坏情况下需要
    *10⁹* 次计算；然而，当处理数据科学项目时，这些项目很容易达到 *10⁶* 个观测值，找到回归问题解决方案所需的计算次数可能会激增到 *10^(18)*。
- en: Gradient descent at work
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降在工作
- en: As an alternative to the usual classical optimization algorithms, the gradient
    descent technique is able to minimize the cost function of a linear regression
    analysis using far fewer computations. Gradient descent complexity ranks in the
    order *O(n*p)*, thus making learning regression coefficients feasible even in
    the occurrence of a large *n* (which stands for the number of observations) and
    a large *p* (number of variables).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 作为传统经典优化算法的替代方案，梯度下降技术能够通过远少的计算量来最小化线性回归分析的代价函数。梯度下降的复杂度以 *O(n*p)* 的顺序排列，因此即使在出现大量
    *n*（代表观测数）和大量 *p*（变量数）的情况下，学习回归系数也是可行的。
- en: The method works by leveraging a simple heuristic that gradually converges on
    the optimal solution starting from a random one. Explaining it simply, it resembles
    walking blind in the mountains. If you want to descend to the lowest valley, even
    if you don't know and can't see the path, you can proceed approximately by going
    downhill for a while, then stopping, then going downhill again and so on, always
    aiming at each stage for where the surface descends until you arrive at a point
    when you cannot descend anymore. Hopefully, at that point you will have reached
    your destination.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法通过利用一个简单的启发式方法，从随机点开始逐渐收敛到最优解。简单来说，它类似于在山中盲目行走。如果你想下降到最低的山谷，即使你不知道并且看不到路径，你也可以通过先下山一段时间，然后停下来，然后再下山，如此循环，始终在每个阶段都朝着地表下降的方向前进，直到你到达一个不能再下降的点。希望在那个点上，你将到达目的地。
- en: In such a situation, your only risk is happening on an intermediate valley (where
    there is a wood or a lake, for instance) and mistaking it for your desired arrival
    because the land stops descending there.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你唯一的风险是发生在中间的山谷（例如，那里有一片树林或一个湖泊）并错误地将其视为你期望的到达地，因为土地在那里停止下降。
- en: In an optimization process, such a situation is defined as a local minimum (whereas
    your target is a global minimum instead, the best minimum possible) and it is
    a possible outcome of your journey downhill depending on the function you are
    working on minimizing. The good news is, in any case, that the error function
    of the linear model family is a bowl-shaped one (technically our cost function
    is a concave one) and it is unlikely that you can get caught anywhere if you descend
    properly.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个优化过程中，这种情况被定义为局部最小值（而你的目标是全局最小值，即可能的最小值），这是你在最小化所工作的函数过程中可能出现的可能结果。好消息是，在任何情况下，线性模型家族的误差函数都是碗状的（技术上我们的代价函数是凹的），如果你正确下降，你不太可能被困在任何地方。
- en: 'The necessary steps to work out a gradient-descent-based solution are easily
    described, given our cost function for a set of coefficients (the vector *w*):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定一组系数（向量 *w*）的代价函数的情况下，描述基于梯度下降的解决方案的必要步骤是很容易的：
- en: '![Gradient descent at work](img/00031.jpeg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降在工作](img/00031.jpeg)'
- en: We first start by choosing a random initialization for *w*, by choosing some
    random numbers (taken from a standardized normal curve, for instance, having a
    zero mean and unit variance).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过选择一个随机的 *w* 的初始化值开始，通过选择一些随机数（例如，从标准正态曲线中取出的，具有零均值和单位方差）。
- en: Then we start reiterating an update of the values of *w* (opportunely using
    the gradient descent computations) until the marginal improvement from the previous
    *J(w)* is small enough to let us figure out that we have finally reached an optimum
    minimum.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始重复更新 *w* 的值（恰当地使用梯度下降计算），直到从上一个 *J(w)* 的边际改进足够小，以至于我们可以确定我们已经最终达到了一个最优的最小值。
- en: 'We can opportunely update our coefficients, one by one, by subtracting from
    each of them a portion alpha (*α*, the learning rate) of the partial derivative
    of the cost function:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从每个系数中减去成本函数的偏导数的一部分alpha（*α*，学习率）来适时地更新我们的系数，一个接一个：
- en: '![Gradient descent at work](img/00032.jpeg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降工作原理](img/00032.jpeg)'
- en: 'Here, in our formula, *w[j]* is to be intended as a single coefficient (we
    are iterating over them). After resolving the partial derivative, the final resolution
    form is as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的公式中，*w[j]*应被视为一个单独的系数（我们正在迭代它们）。在解决偏导数之后，最终的解形式如下：
- en: '![Gradient descent at work](img/00033.jpeg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降工作原理](img/00033.jpeg)'
- en: Simplifying everything, our gradient for the coefficient of *x* is just the
    average of our predicted values multiplied by their respective *x* value.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 简化一切，我们的*x*系数的梯度只是我们的预测值的平均值乘以它们各自的*x*值。
- en: Alpha, called the **learning rate**, is very important in the process, because,
    if it is too large, it may cause the optimization to detour and fail. You have
    to think of each gradient as a jump or as a run in a direction. If you fully take
    it, you may happen to pass over the optimum minimum and end up in another rising
    slope. Too many consecutive long steps may even force you to climb up the cost
    slope, worsening your initial position (given by a cost function that is its summed
    square, the loss of an overall score of fitness).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: Alpha，被称为**学习率**，在过程中非常重要，因为，如果它太大，可能会导致优化偏离并失败。你必须把每个梯度看作是一次跳跃或是在一个方向上的奔跑。如果你完全接受它，你可能会错过最优的最小值，最终落在另一个上升的斜坡上。过多的连续长步骤甚至可能迫使你爬上成本斜坡，使你的初始位置（由成本函数给出，其总和的平方是整体适应度分数的损失）变得更糟。
- en: Using a small alpha, gradient descent won't jump beyond the solution but it
    may take a much longer time to reach the desired minimum. How to choose the right
    alpha is a matter of trial and error; anyway, starting from an alpha such as 0.01
    is never a bad choice, based on our experience in many optimization problems.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较小的alpha值，梯度下降不会跳过解，但它可能需要更长的时间才能达到期望的最小值。如何选择合适的alpha是一个试错的问题；无论如何，根据我们在许多优化问题中的经验，从一个如0.01的alpha值开始永远不是一个坏的选择。
- en: Naturally, the gradient, given the same alpha, will in any case produce shorter
    steps as you approach the solution. Visualizing the steps in a graph can really
    give you a hint about whether gradient descent is working out a solution or not.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，给定相同的alpha，随着你接近解，梯度在任何情况下都会产生更短的步骤。在图表中可视化这些步骤可以真正给你一个关于梯度下降是否在找到解的提示。
- en: Though quite conceptually simple (it is based on an intuition that we have surely
    applied ourselves to move step-by-step, directing where we can optimize our result),
    gradient descent is very effective and indeed scalable when working with real
    data. Such interesting characteristics have elevated it to the core optimization
    algorithm in machine learning; it is not limited to just the linear model family,
    but it can also be extended, for instance, to neural networks for the process
    of back propagation, which updates all the weights of the neural net in order
    to minimize training errors. Surprisingly, gradient descent is also at the core
    of another complex machine learning algorithm, gradient boosting tree ensembles,
    where we have an iterative process minimizing the errors using a simpler learning
    algorithm (a so-called **weak learner** because it is limited by a high bias)
    to progress towards optimization.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在概念上相当简单（它基于一种直觉，我们肯定已经应用于逐步移动，指导我们如何优化结果），但在处理真实数据时，梯度下降非常有效，并且确实具有可扩展性。这些有趣的特征使其成为机器学习中的核心优化算法；它不仅限于线性模型家族，还可以扩展，例如，用于反向传播过程的神经网络，以最小化训练错误。令人惊讶的是，梯度下降也是另一个复杂机器学习算法的核心，即梯度提升树集成，其中我们有一个迭代过程，使用一个更简单的学习算法（所谓的**弱学习器**，因为它受到高偏差的限制）来进步优化。
- en: 'Here is a first implementation in Python. We will slightly modify it in the
    next chapter to make it work efficiently with more predictors than one:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Python的一个初步实现。我们将在下一章中对其进行轻微修改，使其能够有效地处理多于一个预测因子：
- en: '[PRE51]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, after defining the response variable, selecting our predictor (the `RM`
    feature, the average number of rooms per dwelling), and adding a bias (the constant
    number `1`), we are ready in the following code to define all the functions in
    our optimization process:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在定义了响应变量，选择了我们的预测变量（每栋住宅的平均房间数 `RM` 特征），并添加了偏差（常数 `1`）之后，我们就可以在下面的代码中定义我们优化过程中的所有函数了：
- en: '[PRE52]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'After finally defining all the functions necessary for gradient descent to
    work, we can start optimizing it for a solution to our single regression problem:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终定义了所有必要的函数以使梯度下降工作之后，我们可以开始优化它以解决我们的单个回归问题：
- en: '[PRE53]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Scikit-learn `linear_regression` (and other linear models present in the linear
    methods module) are actually powered by gradient descent, making Scikit-learn
    our favorite choice when working in data science projects with large and big data.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的 `linear_regression`（以及线性方法模块中存在的其他线性模型）实际上是由梯度下降驱动的，这使得 Scikit-learn
    成为我们在数据科学项目中处理大型和大数据时的首选选择。
- en: Summary
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced linear regression as a supervised machine learning
    algorithm. We explained its functional form, its relationship with the statistical
    measures of mean and correlation, and we tried to build a simple linear regression
    model on the Boston house prices data. After doing that we finally glanced at
    how regression works under the hood by proposing its key mathematical formulations
    and their translation into Python code.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了线性回归作为一种监督机器学习算法。我们解释了它的函数形式，它与均值和相关性统计量之间的关系，并尝试在波士顿房价数据上构建一个简单的线性回归模型。在完成这些之后，我们最终通过提出其关键数学公式及其转换为
    Python 代码来简要介绍了回归是如何在底层工作的。
- en: In the next chapter, we will continue our discourse about linear regression,
    extending our predictors to multiple variables and carrying on our explanation
    where we left it suspended during our initial illustration with a single variable.
    We will also point out the most useful transformations you can apply to data to
    make it suitable for processing by a linear regression algorithm.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讨论线性回归，将我们的预测变量扩展到多个变量，并继续我们在使用单个变量进行初步说明时暂停的解释。我们还将指出你可以应用到的最有用的数据转换，使数据适合由线性回归算法处理。
