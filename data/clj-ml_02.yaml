- en: Chapter 2. Understanding Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章 理解线性回归
- en: In this chapter, we begin our exploration of machine learning models and techniques.
    The ultimate objective of machine learning is to *generalize* the facts from some
    empirical sample data. This is called **generalization**, and is essentially the
    ability to use these inferred facts to accurately perform at an accurate rate
    on new, unseen data. The two broad categories of machine learning are **supervised**
    learning and **unsupervised** learning. The term **supervised learning** is used
    to describe the task of machine learning in which an understanding or a model
    is formulated from some labeled data. By labeled, we mean that the sample data
    is associated with some observed value. In a basic sense, the model is a statistical
    description of the data and how the data varies over different parameters. The
    initial data used by supervised machine learning techniques to create the model
    is called the **training data** of the model. On the other hand, unsupervised
    learning techniques estimate models by finding patterns in unlabeled data. As
    the data used by unsupervised learning techniques is unlabeled, there is often
    no definite yes-or-no-based reward system to determine if an estimated model is
    accurate and correct.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始探索机器学习模型和技术。机器学习的最终目标是*泛化*从某些经验样本数据中得出的事实。这被称为**泛化**，本质上是指使用这些推断事实以准确率在新的、未见过的数据上准确执行的能力。机器学习的两大类别是**监督**学习和**无监督**学习。**监督学习**这个术语用来描述机器学习中的任务，即从某些标记数据中制定理解或模型。通过标记，我们指的是样本数据与某些观察到的值相关联。在基本意义上，模型是数据的统计描述以及数据如何在不同参数上变化。监督机器学习技术用来创建模型的初始数据被称为模型的**训练数据**。另一方面，无监督学习技术通过在未标记数据中寻找模式来估计模型。由于无监督学习技术使用的数据是未标记的，通常没有基于是或否的明确奖励系统来确定估计的模型是否准确和正确。
- en: We will now examine *linear regression*, which is an interesting model that
    can be used for prediction. As a type of supervised learning, regression models
    are created from some data in which a number of parameters are somehow combined
    to produce several target values. The model actually describes the relation between
    the target value and the model's parameters, and can be used to predict a target
    value when supplied with the values for the parameters of the model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将考察*线性回归*，这是一个有趣的预测模型。作为一种监督学习，回归模型是从某些数据中创建的，其中一些参数以某种方式组合产生几个目标值。该模型实际上描述了目标值与模型参数之间的关系，并在提供模型参数的值时可以用来预测目标值。
- en: We will first study linear regression with single as well as multiple variables,
    and then describe the algorithms that can be used to formulate machine learning
    models from some given data. We will study the reasoning behind these models and
    simultaneously demonstrate how we can implement the algorithms to create these
    models in Clojure.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将研究单变量和多变量线性回归，然后描述可以用来从一些给定数据中制定机器学习模型的算法。我们将研究这些模型背后的推理，并同时展示如何在Clojure中实现这些算法来创建这些模型。
- en: Understanding single-variable linear regression
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解单变量线性回归
- en: We often come across situations where we would need to create an approximate
    model from some sample data. This model can then be used to predict more such
    data when its required parameters are supplied. For example, we might want to
    study the frequency of rainfall on a given day in a particular city, which we
    will assume varies depending on the humidity on that day. A formulated model could
    be useful in predicting the possibility of rainfall on a given day if we know
    the humidity on that day. We start formulating a model from some data by first
    fitting a straight line (that is, an equation) with some parameters and coefficients
    over this data. This type of model is called a **linear regression** model. We
    can think of linear regression as a way of fitting a straight line, ![Understanding
    single-variable linear regression](img/4351OS_02_01.jpg), over the sample data,
    if we assume that the sample data has only a single dimension.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常遇到需要从一些样本数据中创建近似模型的情况。然后，可以使用该模型在提供所需参数时预测更多此类数据。例如，我们可能想研究特定城市某一天降雨的频率，我们假设这取决于那一天的湿度。一个制定好的模型可以用来预测某一天降雨的可能性，如果我们知道那一天的湿度。我们从一些数据开始制定模型，首先通过一些参数和系数在这个数据上拟合一条直线（即方程）。这种类型的模型被称为**线性回归**模型。如果我们假设样本数据只有一个维度，我们可以将线性回归视为在样本数据上拟合一条直线，![理解单变量线性回归](img/4351OS_02_01.jpg)。
- en: The linear regression model is simply described as a linear equation that represents
    the **regressand** or **dependent variable** of the model. The formulated regression
    model can have one to several parameters depending on the available data, and
    these parameters of the model are also termed as **regressors**, **features**,
    or **independent variables** of the model. We will first explore linear regression
    models with a single independent variable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型简单地说是一个线性方程，它表示模型的**回归量**或**因变量**。根据可用数据，建立的回归模型可以有一个或多个参数，这些模型的参数也被称为**回归变量**、**特征**或**独立变量**。我们将首先探讨具有单个独立变量的线性回归模型。
- en: 'An example problem for using linear regression with a single variable would
    be to predict the probability of rainfall on a particular day, which depends on
    the humidity on that day. This training data can be represented in the following
    tabular form:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单变量线性回归的一个示例问题可能是预测特定一天降雨的概率，这取决于那一天的湿度。这些训练数据可以用以下表格形式表示：
- en: '![Understanding single-variable linear regression](img/4351OS_02_85.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_85.jpg)'
- en: 'For a single-variable linear model, the dependent variable must vary with respect
    to a single parameter. Thus, our sample data essentially consists of two vectors,
    that is, one for the values of the dependent parameter *Y* and the other for the
    values of the independent variable *X*. Both vectors have the same length. This
    data can be formally represented as two vectors, or single column matrices, as
    follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单变量线性模型，因变量必须相对于单个参数变化。因此，我们的样本数据本质上由两个向量组成，即一个用于依赖参数 *Y* 的值，另一个用于独立变量 *X*
    的值。这两个向量长度相同。这些数据可以用以下两种形式正式表示为两个向量，或单列矩阵：
- en: '![Understanding single-variable linear regression](img/4351OS_02_03.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_03.jpg)'
- en: 'Let''s quickly define the following two matrices in Clojure, *X* and *Y*, to
    represent some sample data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速定义以下两个矩阵，Clojure 中的 *X* 和 *Y*，以表示一些样本数据：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we define 10 points of data; these points can be easily plotted on a
    scatter graph using the following Incanter `scatter-plot` function:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了10个数据点；这些点可以用以下Incanter `scatter-plot`函数轻松地绘制在散点图上：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code displays the following scatter plot of our data:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了以下数据点的散点图：
- en: '![Understanding single-variable linear regression](img/4351OS_02_04.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_04.jpg)'
- en: The previous scatter plot is a simple representation of the 10 data points that
    we defined in `X` and `Y`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的散点图是我们定义在 `X` 和 `Y` 中的10个数据点的简单表示。
- en: Note
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `scatter-plot` function can be found in the `charts` namespace of the Incanter
    library. The namespace declaration of a file using this function should look similar
    to the following declaration:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`scatter-plot` 函数可以在Incanter库的 `charts` 命名空间中找到。使用此函数的文件命名空间声明应类似于以下声明：'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have a visualization of our data, let''s estimate a linear model
    over the given data points. We can generate a linear model of any data using the
    `linear-model` function from the Incanter library. This function returns a map
    that describes the formulated model and also a lot of useful data about this model.
    For starters, we can plot the linear model over our previous scatter plot by using
    the `:fitted` key-value pair from this map. We first get the value of the `:fitted`
    key from the returned map and add it to the scatter plot using the `add-lines`
    function; this is shown in the following code:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对我们的数据有了可视化，让我们在给定的数据点上估计一个线性模型。我们可以使用Incanter库中的`linear-model`函数生成任何数据的线性模型。这个函数返回一个映射，描述了构建的模型，以及关于这个模型的大量有用数据。首先，我们可以使用这个映射中的`:fitted`键值对在我们的先前散点图上绘制线性模型。我们首先从返回的映射中获取`:fitted`键的值，并使用`add-lines`函数将其添加到散点图中；这在上面的代码中有所展示：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code produces the following self-explanatory plot of the linear model
    over the scatter plot we defined previously:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了以下自解释的线性模型图，该图覆盖了我们之前定义的散点图：
- en: '![Understanding single-variable linear regression](img/4351OS_02_06.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_06.jpg)'
- en: The previous plot depicts the linear model `samp-linear-model` as a straight
    line drawn over the 10 data points that we defined in `X` and `Y`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的图表描绘了线性模型`samp-linear-model`，作为在`X`和`Y`中定义的10个数据点上绘制的直线。
- en: Note
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The `linear-model` function can be found in the `stats` namespace of the Incanter
    library. The namespace declaration of a file using `linear-model` should look
    similar to the following declaration:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`linear-model`函数可以在Incanter库的`stats`命名空间中找到。使用`linear-model`的文件命名空间声明应类似于以下声明：'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Well, it looks like Incanter's `linear-model` function did most of the work
    for us. Essentially, this function creates a linear model of our data by using
    the **ordinary-least squares** (**OLS**) curve-fitting algorithm. We will soon
    dive into the details of this algorithm, but let's first understand how exactly
    a curve is fit onto some given data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看起来Incanter的`linear-model`函数为我们做了大部分工作。本质上，这个函数通过使用**普通最小二乘法**（**OLS**）曲线拟合算法来创建我们数据的线性模型。我们很快就会深入了解这个算法的细节，但首先让我们了解曲线是如何精确地拟合到一些给定数据上的。
- en: Let's first define how a straight line can be represented. In coordinate geometry,
    a line is simply a function of an independent variable, *x*, which has a given
    slope, *m*, and an intercept, *c*. The function of the line *y* can be formally
    written as ![Understanding single-variable linear regression](img/4351OS_02_01.jpg).
    The slope of the line represents how much the value of *y* changes when the value
    of *x* varies. The intercept of this equation is just where the line meets the
    *y* axis of the plot. Note that the equation *y* is not the same as *Y*, which
    actually represents the values of the equation that we have been provided with.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义一条直线如何表示。在坐标几何学中，一条直线简单地是一个独立变量*x*的函数，它有一个给定的斜率*m*和截距*c*。直线的函数*y*可以正式写成![理解单变量线性回归](img/4351OS_02_01.jpg)。直线的斜率表示当*x*的值变化时，*y*的值变化了多少。这个方程的截距就是直线与图表的*y*轴相交的地方。请注意，方程*y*与*Y*不同，*Y*实际上代表了我们提供的方程的值。
- en: 'Analogous to this definition of a straight line from coordinate geometry, we
    formally define the linear regression model with a single variable using our definition
    of the matrices *X* and *Y*, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于从坐标几何学中定义的直线，我们使用我们对矩阵*X*和*Y*的定义，正式地使用单变量线性回归模型进行定义，如下所示：
- en: '![Understanding single-variable linear regression](img/4351OS_02_09.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_09.jpg)'
- en: This definition of the linear model with a single variable is actually quite
    versatile since we can use the same equation to define a linear model with multiple
    variables; we will see this later in the chapter. In the preceding definition,
    the term ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    is a coefficient that represents the linear scale of *y* with respect to *x*.
    In terms of geometry, it's simply the slope of a line that fits the given data
    in matrices *X* and *Y*. Since *X* is a matrix or vector, ![Understanding single-variable
    linear regression](img/4351OS_02_10.jpg) can also be thought of as a scaling factor
    for the matrix *X*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以使用相同的方程来定义具有多个变量的线性模型，因此这个单变量线性模型的定义实际上非常灵活；我们将在本章后面看到这一点。在前面的定义中，术语![理解单变量线性回归](img/4351OS_02_10.jpg)是一个系数，它表示*y*相对于*x*的线性尺度。从几何学的角度来看，它就是拟合给定数据矩阵*X*和*Y*的直线的斜率。由于*X*是一个矩阵或向量，![理解单变量线性回归](img/4351OS_02_10.jpg)也可以被视为矩阵*X*的缩放因子。
- en: Also, the term ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    is another coefficient that explains the value of *y* when *x* is zero. In other
    words, it's the *y* intercept of the equation. The coefficient ![Understanding
    single-variable linear regression](img/4351OS_02_10.jpg) of the formulated model
    is termed as the **regression coefficient** or **effect** of the linear model,
    and the coefficient ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    is termed as the **error term** or **bias** of the model. A model may even have
    several regression coefficients, as we will see later in this chapter. It turns
    out that the error ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    is actually just another regression coefficient and can be conventionally mentioned
    along with the other effects of the model. Interestingly, this error determines
    the scatter or variance of the data in general.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，术语![理解单变量线性回归](img/4351OS_02_11.jpg)是另一个系数，它解释了当*x*为零时*y*的值。换句话说，它是方程的*y*截距。所构建模型的系数![理解单变量线性回归](img/4351OS_02_10.jpg)被称为线性模型的**回归系数**或**效应**，而系数![理解单变量线性回归](img/4351OS_02_11.jpg)被称为模型的**误差项**或**偏差**。一个模型甚至可能有多个回归系数，正如我们将在本章后面看到的。结果证明，误差![理解单变量线性回归](img/4351OS_02_11.jpg)实际上只是另一个回归系数，并且可以传统地与其他模型的效应一起提及。有趣的是，这个误差决定了数据的一般散点或方差。
- en: 'Using the map returned by the `linear-model` function from our earlier example,
    we can easily inspect the coefficients of the generated model. The returned map
    has a `:coefs` key that maps to a vector containing the coefficients of the model.
    By convention, the error term is also included in this vector, simply as another
    coefficient:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前示例中`linear-model`函数返回的映射，我们可以轻松地检查生成的模型的系数。返回的映射有一个`:coefs`键，它映射到一个包含模型系数的向量。按照惯例，误差项也包含在这个向量中，简单地作为另一个系数：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we've defined a linear model over our data. It's obvious that not all the
    points will be on a line that is plotted to represent the formulated model. Each
    data point has some deviation from the linear model's plot over the *y* axis,
    and this deviation can be either positive or negative. To represent the overall
    deviation of the model from the given data, we use the *residual sum of squares*,
    *mean-squared error*, and *root mean-squared error* functions. The values of these
    three functions represent a scalar measure of the amount of error in the formulated
    model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了数据上的线性模型。很明显，并非所有点都会落在表示所构建模型的线条上。每个数据点在*y*轴上与线性模型的绘图都有一定的偏差，这种偏差可以是正的也可以是负的。为了表示模型与给定数据之间的整体偏差，我们使用*残差平方和*、*均方误差*和*均方根误差*函数。这三个函数的值代表了对所构建模型中误差量的标量度量。
- en: The difference between the terms *error* and *residual* is that an error is
    a measure of the amount by which an observed value differs from its expected value,
    while a residual is an estimate of the unobservable statistical error, which is
    simply not modeled or understood by the statistical model that we are using. We
    can say that, in a set of observed values, the difference between an observed
    value and the mean of all values is a residual. The number of residuals in a formulated
    model must be equal to the number of observed values of the dependent variable
    in the sample data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *误差* 和 *残差* 之间的区别在于，误差是衡量观察值与其预期值差异的量度，而残差是对不可观察的统计误差的估计，这简单地没有被我们使用的统计模型所建模或理解。我们可以这样说，在观察值集中，一个观察值与所有值的平均值之间的差异是一个残差。构建模型中的残差数量必须等于样本数据中依赖变量的观察值数量。
- en: 'We can use the `:residuals` keyword to fetch the residuals from the linear
    model generated by the `linear-model` function, as shown in the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `:residuals` 关键字从由 `linear-model` 函数生成的线性模型中获取残差，如下面的代码所示：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The **sum of squared errors of prediction** (**SSE**) is simply the sum of errors
    in a formulated model. Note that in the following equation, the sign of the error
    term ![Understanding single-variable linear regression](img/4351OS_02_12.jpg)
    isn't significant since we square this difference value; thus, it will always
    produce a positive value. The SSE is also termed as the **residual sum of squares**
    (**RSS**).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测均方误差**（**SSE**）简单地是构建模型中误差的总和。注意，在以下方程中，误差项 ![理解单变量线性回归](img/4351OS_02_12.jpg)
    的符号并不重要，因为我们平方了这个差异值；因此，它总是产生一个正值。SSE 也被称为 **残差平方和**（**RSS**）。'
- en: '![Understanding single-variable linear regression](img/4351OS_02_13.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_13.jpg)'
- en: 'The `linear-model` function also calculates the SSE of the formulated model,
    and this value can be retrieved using the `:sse` keyword; this is illustrated
    in the following lines of code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`linear-model` 函数还计算构建模型的 SSE，并且可以使用 `:sse` 关键字检索此值；以下代码行展示了这一点：'
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The **mean-squared error** (**MSE**) measures the average magnitude of errors
    in a formulated model without considering the direction of the errors. We can
    calculate this value by squaring the differences of all the given values of the
    dependent variable and their corresponding predicted values on the formulated
    linear model, and calculating the mean of these squared errors. The MSE is also
    termed as the **mean-squared prediction error** of a model. If the MSE of a formulated
    model is zero, then we can say that the model fits the given data perfectly. Of
    course, this is practically impossible for real data, although we could find a
    set of values that produce an MSE of zero in theory.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）衡量了在构建的模型中误差的平均幅度，不考虑误差的方向。我们可以通过平方所有给定依赖变量的值与其在构建的线性模型中的对应预测值的差异，并计算这些平方误差的平均值来计算这个值。MSE
    也被称为模型的 **均方预测误差**。如果一个构建的模型的 MSE 为零，那么我们可以说该模型完美地拟合了给定的数据。当然，这在实际数据中是几乎不可能的，尽管在理论上我们可以找到一组产生零
    MSE 的值。'
- en: 'For a given set of *N* values of the dependent variable ![Understanding single-variable
    linear regression](img/4351OS_02_14.jpg) and an estimated set of values ![Understanding
    single-variable linear regression](img/4351OS_02_15.jpg) calculated from a formulated
    model, we can formally represent the MSE function of the formulated model ![Understanding
    single-variable linear regression](img/4351OS_02_16.jpg) as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖变量的给定的一组 *N* 个值 ![理解单变量线性回归](img/4351OS_02_14.jpg) 和从构建的模型中计算出的估计值 ![理解单变量线性回归](img/4351OS_02_15.jpg)，我们可以正式表示构建模型的
    MSE 函数 ![理解单变量线性回归](img/4351OS_02_16.jpg) 如下：
- en: '![Understanding single-variable linear regression](img/4351OS_02_17.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_17.jpg)'
- en: The **root mean-squared error** (**RMSE**) or **root-mean squared deviation**
    is simply the square root of the MSE and is often used to measure the deviation
    of a formulated linear model. The RMSE is partial to larger errors, and is hence
    scale-dependent. This means that the RMSE is particularly useful when large errors
    are undesirable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）或 **均方根偏差**简单地是 MSE 的平方根，常用于衡量构建的线性模型的偏差。RMSE 对较大误差有偏，因此是尺度相关的。这意味着当不希望有较大误差时，RMSE
    特别有用。'
- en: 'We can formally define the RMSE of a formulated model as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下正式定义一个公式的模型的均方根误差（RMSE）：
- en: '![Understanding single-variable linear regression](img/4351OS_02_18.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_18.jpg)'
- en: Another measure of the accuracy of a formulated linear model is the **coefficient
    of determination**, which is written as ![Understanding single-variable linear
    regression](img/4351OS_02_19.jpg). The coefficient of determination indicates
    how well the formulated model fits the given sample data, and is defined as follows.
    This coefficient is defined in terms of the mean of observed values in the sample
    data ![Understanding single-variable linear regression](img/4351OS_02_20.jpg),
    the SSE, and the total sum of errors ![Understanding single-variable linear regression](img/4351OS_02_21.jpg).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个衡量公式的线性模型准确度的指标是**确定系数**，表示为![理解单变量线性回归](img/4351OS_02_19.jpg)。确定系数表示公式的模型与给定样本数据拟合得有多好，其定义如下。该系数是根据样本数据中观察值的平均值![理解单变量线性回归](img/4351OS_02_20.jpg)、SSE和总误差和![理解单变量线性回归](img/4351OS_02_21.jpg)来定义的。
- en: '![Understanding single-variable linear regression](img/4351OS_02_22.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_22.jpg)'
- en: 'We can retrieve the calculated value of ![Understanding single-variable linear
    regression](img/4351OS_02_19.jpg) from the model generated by the `linear-model`
    function by using the `:r-square` keyword as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`linear-model`函数生成的模型，并使用`:r-square`关键字来检索![理解单变量线性回归](img/4351OS_02_19.jpg)的计算值，如下所示：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In order to formulate a model that best fits the sample data, we should strive
    to minimize the previously described values. For some given data, we can formulate
    several models and calculate the total error for each model. This calculated error
    can then be used to determine which formulated model is the best fit for the data,
    thus selecting the optimal linear model for the given data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了制定一个最适合样本数据的模型，我们应该努力最小化之前描述的值。对于某些给定数据，我们可以制定几个模型，并计算每个模型的总体误差。然后，可以使用这个计算出的误差来确定哪个公式的模型最适合数据，从而选择给定数据的最佳线性模型。
- en: 'Based on the MSE of a formulated model, the model is said to have a **cost
    function**. The problem of fitting a linear model over some data is equivalent
    to the problem of minimizing the cost function of a formulated linear model. The
    cost function, which is represented as ![Understanding single-variable linear
    regression](img/4351OS_02_23.jpg), can be simply thought of as a function of the
    parameters of a formulated model. Generally, this cost function translates to
    the MSE of a model. Since the RMSE varies with the formulated parameters of the
    model, the following cost function of the model is a function of these parameters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式的均方误差（MSE），模型被认为有一个**成本函数**。在数据上拟合线性模型的问题等价于最小化公式的线性模型成本函数的问题。成本函数，表示为![理解单变量线性回归](img/4351OS_02_23.jpg)，可以简单地看作是公式模型参数的函数。通常，这个成本函数转化为模型的均方误差。由于RMSE随模型的公式参数变化，以下模型的成本函数是这些参数的函数：
- en: '![Understanding single-variable linear regression](img/4351OS_02_24.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_24.jpg)'
- en: 'This brings us to the following formal definition of the problem of fitting
    a linear regression model over some data for the estimated effects ![Understanding
    single-variable linear regression](img/4351OS_02_10.jpg) and ![Understanding single-variable
    linear regression](img/4351OS_02_11.jpg) of a linear model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们引向以下关于在数据上拟合线性回归模型问题的正式定义，对于线性模型的估计效应![理解单变量线性回归](img/4351OS_02_10.jpg)和![理解单变量线性回归](img/4351OS_02_11.jpg)：
- en: '![Understanding single-variable linear regression](img/4351OS_02_25.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_25.jpg)'
- en: This definition states that we can estimate a linear model, represented by the
    parameters ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    and ![Understanding single-variable linear regression](img/4351OS_02_11.jpg),
    by determining the values of these parameters, for which the cost function ![Understanding
    single-variable linear regression](img/4351OS_02_23.jpg) takes on the least possible
    value, ideally zero.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该定义指出，我们可以通过确定这些参数的值来估计一个线性模型，这些参数由![理解单变量线性回归](img/4351OS_02_10.jpg)和![理解单变量线性回归](img/4351OS_02_11.jpg)表示，在这些参数下，成本函数![理解单变量线性回归](img/4351OS_02_23.jpg)取最小可能值，理想情况下为零。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In the preceding equation, the ![Understanding single-variable linear regression](img/4351OS_02_26.jpg)
    expression represents the standard norm *N*-dimensional Euclidian space of the
    cost function. By the term *norm*, we mean a function that has only positive values
    in the *N*-dimensional space.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，![理解单变量线性回归](img/4351OS_02_26.jpg)表达式代表成本函数的N维欧几里得空间的标准范数。通过“范数”一词，我们指的是在N维空间中只有正值的功能。
- en: 'Let''s visualize how the Euclidian space of the cost function of a formulated
    model varies with respect to the parameters of the model. For this, let''s assume
    that the ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    parameter that represents the constant error is zero. A plot of the cost function
    ![Understanding single-variable linear regression](img/4351OS_02_27.jpg) of the
    linear model over the parameter ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    will ideally appear as a parabolic curve, similar to the following plot:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化构建的模型成本函数的欧几里得空间如何随模型参数的变化而变化。为此，我们假设![理解单变量线性回归](img/4351OS_02_11.jpg)参数，它代表常数误差为零。线性模型在参数![理解单变量线性回归](img/4351OS_02_10.jpg)上的成本函数![理解单变量线性回归](img/4351OS_02_27.jpg)的图将理想地呈现为抛物线形状，类似于以下图表：
- en: '![Understanding single-variable linear regression](img/4351OS_02_28.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_28.jpg)'
- en: For a single parameter, ![Understanding single-variable linear regression](img/4351OS_02_10.jpg),
    we can plot the preceding chart, which has two dimensions. Similarly, for two
    parameters, ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    and ![Understanding single-variable linear regression](img/4351OS_02_11.jpg),
    of the formulated model, a plot of three dimensions is produced. This plot appears
    bowl-shaped or having a convex surface, as illustrated in the following diagram.
    Also, we can generalize this for *N* parameters of the formulated model and produce
    a plot of ![Understanding single-variable linear regression](img/4351OS_02_30.jpg)
    dimensions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个参数![理解单变量线性回归](img/4351OS_02_10.jpg)，我们可以绘制前面的二维图表。同样，对于两个参数![理解单变量线性回归](img/4351OS_02_10.jpg)和![理解单变量线性回归](img/4351OS_02_11.jpg)，构建的模型，将产生三维图表。此图表呈现为碗形或具有凸表面的形状，如图所示。此外，我们可以将此推广到构建模型的N个参数，并生成![理解单变量线性回归](img/4351OS_02_30.jpg)维度的图表。
- en: '![Understanding single-variable linear regression](img/4351OS_02_31.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![理解单变量线性回归](img/4351OS_02_31.jpg)'
- en: Understanding gradient descent
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度下降
- en: The gradient descent algorithm is one of the simplest, although not the most
    efficient techniques to formulate a linear model that has the least possible value
    for the cost function or error of the model. This algorithm essentially finds
    the local minimum of the cost function for a formulated linear model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法是构建线性模型的最简单技术之一，尽管它不是最有效的方法，可以使得成本函数或模型误差尽可能小。该算法本质上寻找构建的线性模型成本函数的局部最小值。
- en: As we previously described, a three-dimensional plot of the cost function for
    a single-variable linear regression model would appear as a convex or bowl-shaped
    surface with a *global minimum*. By minimum, we mean that the cost function has
    the least possible value at this point on the surface of the plot. The gradient
    descent algorithm essentially starts from any point on the surface and performs
    a sequence of steps to approach the local minimum of the surface.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，单变量线性回归模型成本函数的三维图将呈现为凸或碗形的表面，具有*全局最小值*。通过“最小值”，我们指的是成本函数在图表表面的这一点上具有可能的最小值。梯度下降算法本质上从表面的任何一点开始，执行一系列步骤来接近表面的局部最小值。
- en: This process can be imagined as dropping a ball into a valley or between two
    adjacent hills, as a result of which the ball slowly rolls towards the point that
    has the least elevation above sea level. The algorithm is repeated until the value
    of the apparent cost function from the current point on the surface converges
    to zero, which figuratively means that the ball rolling down the hill comes to
    a stop, as we described earlier.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以想象成把一个球扔进山谷或两个相邻的山丘之间，结果球会慢慢滚向海平面以上最低的点。算法会重复进行，直到从表面当前点的明显成本函数值收敛到零，这比喻地意味着滚下山的球停止了，正如我们之前描述的那样。
- en: Of course, gradient descent may not really work if there are multiple local
    minimums on the surface of the plot. However, for an appropriately scaled single-variable
    linear regression model, the surface of the plot always has a single global minimum,
    as we illustrated earlier. Thus, we can still use the gradient descent algorithm
    in such situations to find the global minimum of the surface of the plot.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果图表的表面上存在多个局部最小值，梯度下降可能根本不起作用。然而，对于适当缩放的单一变量线性回归模型，图表的表面总是有一个唯一的全局最小值，正如我们之前所展示的。因此，在这种情况下，我们仍然可以使用梯度下降算法来找到图表表面的全局最小值。
- en: 'The gist of this algorithm is that we start from some point on the surface
    and then take several steps towards the lowest point. We can formally represent
    this with the following equality:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的精髓是从表面上某个点开始，然后朝着最低点迈出几步。我们可以用以下等式正式表示这一点：
- en: '![Understanding gradient descent](img/4351OS_02_32.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![理解梯度下降](img/4351OS_02_32.jpg)'
- en: Here, we start from the point represented by ![Understanding gradient descent](img/4351OS_02_33.jpg)
    on the plot of the cost function *J*, and incrementally subtract the product of
    the first-order partial derivative of the cost function ![Understanding gradient
    descent](img/4351OS_02_34.jpg), which is derived with respect to the parameters
    of the formulated model. This means that we slowly step downwards on the surface
    towards the local minimum, until we cannot find a lower point on the surface.
    The term ![Understanding gradient descent](img/4351OS_02_35.jpg) determines how
    large our steps towards the local minimum are, and is called the *step* of the
    gradient descent algorithm. We repeat this iteration until the difference between
    ![Understanding gradient descent](img/4351OS_02_36.jpg) and ![Understanding gradient
    descent](img/4351OS_02_33.jpg) converges to zero, or at least reduces to a threshold
    value close to zero.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从成本函数 *J* 的图表上表示为 ![理解梯度下降](img/4351OS_02_33.jpg) 的点开始，并逐步减去成本函数一阶偏导数
    ![理解梯度下降](img/4351OS_02_34.jpg) 的乘积，该偏导数是根据公式的模型参数导出的。这意味着我们缓慢地向表面下方移动，朝着局部最小值前进，直到我们无法在表面上找到更低的点。术语
    ![理解梯度下降](img/4351OS_02_35.jpg) 决定了我们朝着局部最小值迈出的步子有多大，被称为梯度下降算法的 *步长*。我们重复这个迭代过程，直到
    ![理解梯度下降](img/4351OS_02_36.jpg) 和 ![理解梯度下降](img/4351OS_02_33.jpg) 之间的差异收敛到零，或者至少减少到接近零的阈值值。
- en: 'The process of stepping down towards the local minimum of the surface of the
    cost functions plot is illustrated in the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了向成本函数图表表面局部最小值下降的过程：
- en: '![Understanding gradient descent](img/4351OS_02_37.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![理解梯度下降](img/4351OS_02_37.jpg)'
- en: The preceding illustration is a contour diagram of the surface of the plot,
    in which the circular lines connect the points with an equal height. We start
    from the point ![Understanding gradient descent](img/4351OS_02_39.jpg) and perform
    a single iteration of the gradient descent algorithm to step down the surface
    to point ![Understanding gradient descent](img/4351OS_02_40.jpg). We repeat this
    process until we reach the local minimum of the surface with respect to the initial
    starting point ![Understanding gradient descent](img/4351OS_02_39.jpg). Note that,
    through each iteration, the size of the step reduces since the slope of a tangent
    to this surface also tends to zero as we approach the local minimum.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的插图是图表表面的等高线图，其中圆形线条连接了等高点的位置。我们从点 ![理解梯度下降](img/4351OS_02_39.jpg) 开始，执行一次梯度下降算法的迭代，将表面下降到点
    ![理解梯度下降](img/4351OS_02_40.jpg)。我们重复这个过程，直到达到相对于初始起始点 ![理解梯度下降](img/4351OS_02_39.jpg)
    的表面局部最小值。请注意，通过每次迭代，步长的大小都会减小，因为当接近局部最小值时，该表面的切线斜率也趋于零。
- en: 'For a single-variable linear regression model with an error constant ![Understanding
    gradient descent](img/4351OS_02_11.jpg) that is equal to zero, we can simplify
    the partial derivative component ![Understanding gradient descent](img/4351OS_02_34.jpg)
    of the gradient descent algorithm. When there is only one parameter of the model,
    ![Understanding gradient descent](img/4351OS_02_10.jpg), the first order partial
    derivate is simply the slope of a tangent at that point on the surface of the
    plot. Thus, we calculate the slope of this tangent and take a step in the direction
    of this slope such that we arrive at a point of elevation above the *y* axis.
    This is shown in the following formula:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于误差常数为零的单变量线性回归模型![理解梯度下降](img/4351OS_02_11.jpg)，我们可以简化梯度下降算法的偏导数组件![理解梯度下降](img/4351OS_02_34.jpg)。当模型只有一个参数![理解梯度下降](img/4351OS_02_10.jpg)时，一阶偏导数简单地是该点在图表表面切线的斜率。因此，我们计算这条切线的斜率，并沿着这个斜率方向迈出一步，以便到达一个高于*y*轴的点。这如下公式所示：
- en: '![Understanding gradient descent](img/4351OS_02_41.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![理解梯度下降](img/4351OS_02_41.jpg)'
- en: 'We can implement this simplified version of the gradient descent algorithm
    as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个简化的梯度下降算法实现如下：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding function, we begin from the point `x-start` and recursively
    apply the gradient descent algorithm until the value `x-new` converges. Note that
    this process is implemented as a tail recursive function using the `loop` form.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们从`x-start`点开始，递归地应用梯度下降算法，直到`x-new`值收敛。请注意，这个过程是通过使用`loop`形式实现的尾递归函数。
- en: 'Using partial differentiation, we can formally express how both the parameters
    ![Understanding gradient descent](img/4351OS_02_10.jpg) and ![Understanding gradient
    descent](img/4351OS_02_11.jpg) can be calculated using the gradient descent algorithm
    as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用偏导数，我们可以正式表达如何使用梯度下降算法计算参数![理解梯度下降](img/4351OS_02_10.jpg)和![理解梯度下降](img/4351OS_02_11.jpg)：
- en: '![Understanding gradient descent](img/4351OS_02_42.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![理解梯度下降](img/4351OS_02_42.jpg)'
- en: Understanding multivariable linear regression
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解多元线性回归
- en: A multivariable linear regression model can have multiple variables or features,
    as opposed to the linear regression model with a single variable that we previously
    studied. Interestingly, the definition of a linear model with a single variable
    can itself be extended via matrices to be applied to multiple variables.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归模型可以包含多个变量或特征，这与我们之前研究过的单变量线性回归模型不同。有趣的是，单变量线性模型的定义本身可以通过矩阵扩展来应用于多个变量。
- en: 'We can extend our previous example for predicting the probability of rainfall
    on a particular day to a model with multiple variables by including more independent
    variables, such as the minimum and maximum temperatures, in the sample data. Thus,
    the training data for a multivariable linear regression model will look similar
    to the following illustration:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们之前用于预测特定一天降雨概率的例子扩展到包含更多独立变量的多元变量模型中，例如最小和最大温度。因此，多元线性回归模型的训练数据将类似于以下插图：
- en: '![Understanding multivariable linear regression](img/4351OS_02_43.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_43.jpg)'
- en: 'For a multivariable linear regression model, the training data is defined by
    two matrices, *X* and *Y*. Here, *X* is an ![Understanding multivariable linear
    regression](img/4351OS_02_44.jpg) matrix, where *P* is the number of independent
    variables in the model. The matrix *Y* is a vector of length *N*, just like in
    a linear model with a single variable. This model is illustrated as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多元线性回归模型，训练数据由两个矩阵定义，*X*和*Y*。在这里，*X*是一个![理解多元线性回归](img/4351OS_02_44.jpg)矩阵，其中*P*是模型中的独立变量数量。矩阵*Y*是一个长度为*N*的向量，就像在单变量线性模型中一样。该模型如下所示：
- en: '![Understanding multivariable linear regression](img/4351OS_02_45.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_45.jpg)'
- en: For the following example of multivariable linear regression in Clojure, we
    will not generate the sample data through code but use the sample data from the
    Incanter library. We can fetch any dataset using the Incanter library's `get-dataset`
    function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下Clojure中的多元线性回归示例，我们不会通过代码生成样本数据，而是使用Incanter库中的样本数据。我们可以使用Incanter库的`get-dataset`函数获取任何数据集。
- en: Note
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the upcoming example, the `sel`, `to-matrix`, and `get-dataset` functions
    from the Incanter library can be imported into our namespace as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的示例中，可以从Incanter库中导入`sel`、`to-matrix`和`get-dataset`函数到我们的命名空间中，如下所示：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can fetch the **Iris** dataset by calling the `get-dataset` function with
    the `:iris` keyword argument; this is shown as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用带有`:iris`关键字参数的`get-dataset`函数来获取**Iris**数据集；如下所示：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We first define the variable `iris` as a matrix using the `to-matrix` and `get-dataset`
    functions, and then define two matrices `X` and `Y`. Here, `Y` is actually a vector
    of 150 values, or a matrix of size ![Understanding multivariable linear regression](img/4351OS_02_46.jpg),
    while `X` is a matrix of size ![Understanding multivariable linear regression](img/4351OS_02_47.jpg).
    Hence, `X` can be used to represent the values of four independent variables,
    and `Y` represents the values of the dependent variable. Note that the `sel` function
    is used to select a set of columns from the `iris` matrix. In fact, we could select
    many more such columns from the `iris` data matrix, but we will use only four
    in the following example for the sake of simplicity.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`to-matrix`和`get-dataset`函数将变量`iris`定义为矩阵，然后定义两个矩阵`X`和`Y`。在这里，`Y`实际上是一个包含150个值的向量，或者是一个大小为![理解多元线性回归](img/4351OS_02_46.jpg)的矩阵，而`X`是一个大小为![理解多元线性回归](img/4351OS_02_47.jpg)的矩阵。因此，`X`可以用来表示四个独立变量的值，而`Y`表示因变量的值。请注意，`sel`函数用于从`iris`矩阵中选择一组列。实际上，我们可以从`iris`数据矩阵中选择更多的此类列，但为了简化起见，在下面的例子中我们只使用四个。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset that we used in the previous code example is the *Iris* dataset,
    which is available in the Incanter library. This dataset has quite a bit of historical
    significance, as it was used by Sir Ronald Fisher to first develop the **linear
    discriminant analysis** (**LDA**) method for classification (for more information,
    refer to "The Species Problem in Iris"). This dataset contains 50 samples of three
    distinct species of the Iris plant, namely *Setosa*, *Versicolor*, and *Virginica*.
    Four features of the flowers of these species are measured in each sample, namely
    the petal width, petal length, sepal width, and sepal length. Note that we will
    encounter this dataset several times over the course of this book.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用的数据集是*Iris*数据集，该数据集可在Incanter库中找到。这个数据集具有相当大的历史意义，因为它被罗纳德·费舍尔爵士首次用于开发**线性判别分析**（**LDA**）方法进行分类（更多信息请参阅“Iris中的物种问题”）。该数据集包含三种不同的Iris植物物种的50个样本，即*Setosa*、*Versicolor*和*Virginica*。在每个样本中测量这些物种的花朵的四个特征，即花瓣宽度、花瓣长度、萼片宽度和萼片长度。请注意，在本书的后续内容中，我们将多次遇到这个数据集。
- en: 'Interestingly, the `linear-model` function accepts a matrix with multiple columns,
    so we can use this function to fit a linear regression model over both single
    variable and multivariable data as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，`linear-model`函数接受一个多列矩阵，因此我们可以使用此函数来拟合单变量和多变量数据的线性回归模型，如下所示：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code example, we plot the linear model using the `xy-plot`
    function while providing optional parameters to specify the labels of the axes
    in the defined plot. Also, we specify the range of the *x* axis by generating
    a vector using the `range` function. The `plot-iris-linear-model` function generates
    the following plot:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们使用`xy-plot`函数绘制线性模型，同时提供可选参数来指定定义的图中轴的标签。我们还通过使用`range`函数生成一个向量来指定*x*轴的范围。`plot-iris-linear-model`函数生成了以下图形：
- en: '![Understanding multivariable linear regression](img/4351OS_02_48.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_48.jpg)'
- en: Although the curve in the plot produced from the previous example doesn't appear
    to have any definitive shape, we can still use this generated model to estimate
    or predict the value of the dependent variable by supplying values for the independent
    variables to the formulated model. In order to do this, we must first define the
    relationship between the dependent and independent variables of a linear regression
    model with multiple features.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从前面示例生成的曲线看起来没有明确的形状，我们仍然可以使用这个生成的模型通过为模型提供独立变量的值来估计或预测因变量的值。为了做到这一点，我们必须首先定义具有多个特征的线性回归模型中因变量和自变量之间的关系。
- en: A linear regression model of *P* independent variables produces ![Understanding
    multivariable linear regression](img/4351OS_02_50.jpg) regression coefficients,
    since we include the error constant along with the other coefficients of the model
    and also define an extra variable ![Understanding multivariable linear regression](img/4351OS_02_39.jpg),
    which is always *1*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 具有P个独立变量的线性回归模型产生![理解多元线性回归](img/4351OS_02_50.jpg)个回归系数，因为我们除了包括模型的其他系数外，还定义了一个额外的变量![理解多元线性回归](img/4351OS_02_39.jpg)，该变量总是*1*。
- en: 'The `linear-model` function agrees with the proposition that the number of
    coefficients *P* in the formulated model is always one more than the total number
    of independent variables in the sample data *N*; this is shown in the following
    code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`linear-model`函数与命题一致，即所构建模型中的系数数量*P*总是比样本数据中的自变量总数*N*多一个；这在下述代码中显示：'
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We formally express the relationship between a multivariable regression model''s
    dependent and independent variables as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正式表达多元回归模型中因变量和自变量之间的关系如下：
- en: '![Understanding multivariable linear regression](img/4351OS_02_52.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_52.jpg)'
- en: Since the variable ![Understanding multivariable linear regression](img/4351OS_02_39.jpg)
    is always *1* in the preceding equation, the value ![Understanding multivariable
    linear regression](img/4351OS_02_53.jpg) is analogous to the error constant ![Understanding
    multivariable linear regression](img/4351OS_02_11.jpg) from the definition of
    a linear model with a single variable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变量![理解多元线性回归](img/4351OS_02_39.jpg)在先前的方程中总是*1*，因此值![理解多元线性回归](img/4351OS_02_53.jpg)与单变量线性模型定义中的误差常数![理解多元线性回归](img/4351OS_02_11.jpg)类似。
- en: 'We can define a single vector to represent all the coefficients of the previous
    equation as ![Understanding multivariable linear regression](img/4351OS_02_10.jpg).
    This vector is termed as the **parameter vector** of the formulated regression
    model. Also, the independent variables of the model can be represented by a vector.
    Thus, we can define the regression variable *Y* as the product of the transpose
    of the parameter vector and the vector of independent variables of the model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个向量来表示前述方程中所有的系数，即![理解多元线性回归](img/4351OS_02_10.jpg)。这个向量被称为所构建回归模型的**参数向量**。此外，模型的独立变量也可以用一个向量表示。因此，我们可以定义回归变量*Y*为参数向量的转置与模型独立变量向量的乘积：
- en: '![Understanding multivariable linear regression](img/4351OS_02_54.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_54.jpg)'
- en: 'Polynomial functions can also be reduced to the standard form by substituting
    a single variable for every higher-order variable in the polynomial equation.
    For example, consider the following polynomial equation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式函数也可以通过将多项式方程中的每个高阶变量替换为一个单一变量来简化为标准形式。例如，考虑以下多项式方程：
- en: '![Understanding multivariable linear regression](img/4351OS_02_55.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_55.jpg)'
- en: We can substitute the variables ![Understanding multivariable linear regression](img/4351OS_02_56.jpg)
    for ![Understanding multivariable linear regression](img/4351OS_02_57.jpg) to
    reduce the equation to the standard form of a multivariable linear regression
    model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用![理解多元线性回归](img/4351OS_02_56.jpg)替换![理解多元线性回归](img/4351OS_02_57.jpg)，将方程简化为多元线性回归模型的标准形式。
- en: 'This brings us to the following formal definition of the cost function for
    a linear model with multiple variables, which is simply an extension of the definition
    of the cost function for a linear model with a single variable:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们引向以下具有多个变量的线性模型的成本函数的正式定义，这仅仅是单个变量线性模型成本函数定义的扩展：
- en: '![Understanding multivariable linear regression](img/4351OS_02_58.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![理解多元线性回归](img/4351OS_02_58.jpg)'
- en: Note that in the preceding definition, we can use the individual coefficients
    of the model interchangeably with the parameter vector ![Understanding multivariable
    linear regression](img/4351OS_02_10.jpg).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在先前的定义中，我们可以将模型的各个系数与参数向量![理解多元线性回归](img/4351OS_02_10.jpg)互换使用。
- en: 'Analogous to our problem definition of fitting a model with a single variable
    over some given data, we can define the problem of formulating a multivariable
    linear model as the problem of minimizing the preceding cost function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们定义的问题，即对给定数据拟合单变量模型，我们可以将构建多变量线性模型的问题定义为最小化先前成本函数的问题：
- en: '![Understanding multivariable linear regression](img/4351OS_02_59.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![理解多变量线性回归](img/4351OS_02_59.jpg)'
- en: Gradient descent with multiple variables
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多变量梯度下降
- en: We can apply the gradient descent algorithm to find the local minimum of a model
    with multiple variables. Of course, since we have multiple coefficients in the
    model, we have to apply the algorithm for all these coefficients as opposed to
    just two coefficients in a regression model with a single variable.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将梯度下降算法应用于寻找具有多个变量的模型局部最小值。当然，由于模型中有多个系数，我们必须对所有的这些系数应用算法，而不是像单变量回归模型中只对两个系数应用算法。
- en: 'The gradient descent algorithm can thus be used to find the values of all the
    coefficients in the parameter vector ![Gradient descent with multiple variables](img/4351OS_02_10.jpg)
    of a multivariable linear regression model, and is formally defined as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度下降算法可以用来找到多变量线性回归模型参数向量![多变量梯度下降](img/4351OS_02_10.jpg)中所有系数的值，并且形式上定义为以下内容：
- en: '![Gradient descent with multiple variables](img/4351OS_02_60.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![多变量梯度下降](img/4351OS_02_60.jpg)'
- en: In the preceding definition, the term ![Gradient descent with multiple variables](img/4351OS_02_61.jpg)
    simply refers to the sample values for the ![Gradient descent with multiple variables](img/4351OS_02_62.jpg)
    independent variable in the formulated model. Also, the variable ![Gradient descent
    with multiple variables](img/4351OS_02_63.jpg) is always *1*. Thus, this definition
    can be applied to just the two coefficients that correspond to our previous definition
    of the gradient descent algorithm for a linear regression model with a single
    variable.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的定义中，术语![多变量梯度下降](img/4351OS_02_61.jpg)简单地指的是构建模型中独立变量的样本值![多变量梯度下降](img/4351OS_02_62.jpg)。此外，变量![多变量梯度下降](img/4351OS_02_63.jpg)始终为*1*。因此，这个定义可以应用于与之前定义的单变量线性回归模型的梯度下降算法相对应的两个系数。
- en: 'As we''ve seen earlier, the gradient descent algorithm can be applied to a
    linear regression model with both single and multivariables. For some models,
    however, the gradient descent algorithm can actually take a lot of iterations,
    or rather time, to converge the estimated values of the model''s coefficients.
    Sometimes, the algorithm can also diverge, and thus we will be unable to calculate
    the model''s coefficients in such circumstances. Let''s examine some of the factors
    that affect the behavior and performance of this algorithm:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，梯度下降算法可以应用于具有单变量和多变量的线性回归模型。然而，对于某些模型，梯度下降算法实际上可能需要很多迭代，或者说很多时间，才能收敛到模型系数的估计值。有时，算法也可能发散，因此在这种情况下我们无法计算出模型的系数。让我们来考察一些影响该算法行为和性能的因素：
- en: All the features of the sample data must be scaled with respect to each other.
    By scaling, we mean that all the values for the independent variables in the sample
    data take on a similar range of values. Ideally, all independent variables must
    have observed values between *-1* and *1*. This can be formally expressed as follows:![Gradient
    descent with multiple variables](img/4351OS_02_64.jpg)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有样本数据的特征都必须相互缩放。通过缩放，我们指的是样本数据中独立变量的所有值都取相似的范围。理想情况下，所有独立变量必须观察到介于*-1*和*1*之间的值。这可以形式上表达如下：![多变量梯度下降](img/4351OS_02_64.jpg)
- en: We can normalize the observed values for the independent variables about the
    mean of these values. We can further normalize this data by using the standard
    deviation of the observed values. In summary, we substitute the values with those
    produced by subtracting the mean of these values, ![Gradient descent with multiple
    variables](img/4351OS_02_65.jpg),and dividing the resulting expression by the
    standard deviation ![Gradient descent with multiple variables](img/4351OS_02_66.jpg).
    This is shown in the following formula:![Gradient descent with multiple variables](img/4351OS_02_67.jpg)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将独立变量的观测值相对于这些值的平均值进行归一化。我们还可以通过使用观测值的标准差进一步归一化这些数据。总之，我们用减去这些值的平均值，![多个变量的梯度下降](img/4351OS_02_65.jpg)，并将结果表达式除以标准差![多个变量的梯度下降](img/4351OS_02_66.jpg)得到的值来替换这些值。这可以通过以下公式表示：![多个变量的梯度下降](img/4351OS_02_67.jpg)
- en: The stepping or learning rate, ![Gradient descent with multiple variables](img/4351OS_02_35.jpg),
    is another important factor that determines how fast the algorithm converges towards
    the values of the parameters of the formulated model. Ideally, the stepping rate
    should be selected so that the differences between the old and new iterated values
    of the parameters of the model have an optimal amount of change in every iteration.
    On one hand, if this value is too large, the algorithm could even produce diverging
    values for the parameters of the model after each iteration. Thus, the algorithm
    will never find a global minimum in this case. On the other hand, a small value
    for this rate could result in slowing down the algorithm through an unnecessarily
    large number of iterations.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长或学习率，![多个变量的梯度下降](img/4351OS_02_35.jpg)，是决定算法收敛到模型参数值速度的另一个重要因素。理想情况下，步长率应该选择得使得模型参数的旧迭代值和新迭代值之间的差异在每次迭代中都有最佳的变化量。一方面，如果这个值太大，算法在每次迭代后甚至可能产生模型参数的发散值。因此，在这种情况下，算法将永远找不到全局最小值。另一方面，这个率太小可能会导致算法通过不必要的迭代次数减慢。
- en: Understanding Ordinary Least Squares
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解普通最小二乘法
- en: Another technique to estimate the parameter vector of a linear regression model
    is the **Ordinary Least Squares** (**OLS**) method. The OLS method essentially
    works by minimizing the sum of squared errors in a linear regression model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 估计线性回归模型参数向量的另一种技术是**普通最小二乘法**（**OLS**）。OLS方法本质上是通过最小化线性回归模型中的平方误差和来工作的。
- en: 'The sum of squared errors of prediction, or SSE, of a linear regression model
    can be defined in terms of the model''s actual and expected values as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型的预测平方误差和（SSE）可以用模型的实际值和期望值来定义如下：
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_68.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![理解普通最小二乘法](img/4351OS_02_68.jpg)'
- en: 'The preceding definition of the SSE can be factorized using matrix products
    as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的SSE定义可以用矩阵乘法进行因式分解如下：
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_69.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![理解普通最小二乘法](img/4351OS_02_69.jpg)'
- en: 'We can solve the preceding equation for the estimated parameter vector ![Understanding
    Ordinary Least Squares](img/4351OS_02_10.jpg) by using the definition of a global
    minimum. Since this equation is a form of quadratic equation and the term ![Understanding
    Ordinary Least Squares](img/4351OS_02_70.jpg) is always greater than zero, the
    global minimum of the surface of the cost function can be defined as the point
    at which the rate of change of the slope of a tangent to the surface at that point
    is zero. Also, the plot is a function of the parameters of the linear model, and
    so the equation of the surface plot should be differentiated by the estimated
    parameter vector ![Understanding Ordinary Least Squares](img/4351OS_02_10.jpg).
    We can thus solve this equation for the optimal parameter vector ![Understanding
    Ordinary Least Squares](img/4351OS_02_10.jpg) of the formulated model as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用全局最小值的定义来解前面的方程，以求解估计的参数向量![理解普通最小二乘法](img/4351OS_02_10.jpg)。由于这个方程是二次方程的形式，并且项![理解普通最小二乘法](img/4351OS_02_70.jpg)总是大于零，因此成本函数表面的全局最小值可以定义为在该点切线斜率变化率为零的点。此外，该图是线性模型参数的函数，因此表面图的方程应该对估计的参数向量![理解普通最小二乘法](img/4351OS_02_10.jpg)进行微分。因此，我们可以如下求解所构建模型的最佳参数向量![理解普通最小二乘法](img/4351OS_02_10.jpg)：
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_71.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![理解普通最小二乘法](img/4351OS_02_71.jpg)'
- en: 'The last equation in the preceding derivation gives us the definition of the
    optimal parameter vector ![Understanding Ordinary Least Squares](img/4351OS_02_10.jpg),
    which is formally expressed as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 前面推导中的最后一个等式给出了最优参数向量![理解普通最小二乘法](img/4351OS_02_10.jpg)的定义，它正式表达如下：
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_72.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![理解普通最小二乘法](img/4351OS_02_72.jpg)'
- en: 'We can implement the preceding definition of the parameter vector through the
    OLS method using the core.matrix library''s `transpose` and `inverse` functions
    and the Incanter library''s `bind-columns` function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用core.matrix库的`transpose`和`inverse`函数以及Incanter库的`bind-columns`函数来实现先前的参数向量定义的OLS方法：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, we first add a column in which each element is `1`, as the first column
    of the matrix `MX` uses the `bind-columns` function. The extra column that we
    add represents the independent variable ![Understanding Ordinary Least Squares](img/4351OS_02_39.jpg),
    whose value is always `1`. We then use the `transpose` and `inverse` functions
    to calculate the estimated coefficients of the linear regression model for the
    data in matrices `MX` and `MY`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先添加一个列，其中每个元素都是`1`，因为矩阵`MX`的第一列使用`bind-columns`函数。我们添加的额外列代表独立变量![理解普通最小二乘法](img/4351OS_02_39.jpg)，其值始终为`1`。然后我们使用`transpose`和`inverse`函数计算矩阵`MX`和`MY`中数据的线性回归模型的估计系数。
- en: Note
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the current example, the `bind-columns` function from the Incanter library
    can be imported into our namespace as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前示例，可以将Incanter库中的`bind-columns`函数导入我们的命名空间，如下所示：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The previously defined function can be applied to the matrices that we have
    previously defined (*X* and *Y*) as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将先前定义的函数应用于我们先前定义的矩阵（*X*和*Y*），如下所示：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, `ols-linear-model-coefs` is simply the variable and `ols-linear-model`
    is a matrix with a single column, which is represented as a vector. We perform
    this conversion using the `as-vec` function from the clatrix library.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`ols-linear-model-coefs`只是一个变量，而`ols-linear-model`是一个单列矩阵，它被表示为一个向量。我们使用clatrix库中的`as-vec`函数执行此转换。
- en: 'We can actually verify that the coefficients estimated by the `ols-linear-model`
    function are practically equal to the ones generated by the Incanter library''s
    `linear-model` function, which is illustrated as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以验证由`ols-linear-model`函数估计的系数实际上与Incanter库的`linear-model`函数生成的系数相等，如下所示：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the last expression in the preceding code example, we find the difference
    between the coefficients produced by the `ols-linear-model` function, the difference
    produced by the `linear-model` function, and check whether each of these differences
    is less than `0.0001`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码示例的最后表达式中，我们找到了由`ols-linear-model`函数产生的系数之间的差异，由`linear-model`函数产生的差异，并检查这些差异中的每一个是否小于`0.0001`。
- en: Using linear regression for prediction
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归进行预测
- en: Once we've determined the coefficients of a linear regression model, we can
    use these coefficients to predict the value of the dependent variable of the model.
    The predicted value is defined by the linear regression model as the sum of the
    products of each coefficient and the value of its corresponding independent variable.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了线性回归模型的系数，我们可以使用这些系数来预测模型因变量的值。预测值由线性回归模型定义为每个系数与其对应自变量值的乘积之和。
- en: 'We can easily define the following generic function, which when supplied with
    the coefficients and values of independent variables, predicts the value of the
    dependent variable for a given formulated linear regression model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松定义以下通用函数，当提供系数和自变量的值时，它预测给定公式的线性回归模型中因变量的值：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding function, we use a precondition to assert the number of coefficients
    and the values of independent variables. This function expects that the number
    of values of the independent variables is one less than the number of coefficients
    of the model, as we add an extra parameter to represent an independent variable
    whose value is always *1*. The function then calculates the product of the corresponding
    coefficients and the values of the independent variables using the `map` function,
    and then calculates the sum of these product terms using the `reduce` function.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们使用一个先决条件来断言系数的数量和自变量的值。这个函数期望自变量的值数量比模型的系数数量少一个，因为我们添加了一个额外的参数来表示一个值始终为*1*的自变量。然后，该函数使用`map`函数计算相应的系数和自变量值的乘积，然后使用`reduce`函数计算这些乘积项的总和。
- en: Understanding regularization
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解正则化
- en: Linear regression estimates some given training data using a linear equation;
    this solution may not always be the best fit for the given data. Of course, it
    depends largely on the problem that we are trying to model. **Regularization**
    is a commonly used technique to provide a better fit for the data. Generally,
    a given model is regularized by reducing the effect of some of the independent
    variables of the model. Alternatively, we could model it as a higher-order polynomial.
    Regularization isn't exclusive to linear regression, and most machine learning
    algorithms use some form of regularization in order to create a more accurate
    model from the given training data.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归使用线性方程估计一些给定的训练数据；这种解决方案可能并不总是给定数据的最佳拟合。当然，这很大程度上取决于我们试图建模的问题。**正则化**是一种常用的技术，用于提供更好的数据拟合。通常，一个给定的模型通过减少模型中一些自变量的影响来进行正则化。或者，我们也可以将其建模为更高阶的多项式。正则化并不局限于线性回归，大多数机器学习算法都使用某种形式的正则化，以便从给定的训练数据中创建更精确的模型。
- en: A model is said to be **underfit** or **high bias** when it doesn't estimate
    the dependent variable to a value that is close to the observed values of the
    dependent variable in the training data. On the other hand, a model can also be
    called **overfit**, or said to have **high variance**, when the estimated model
    fits the data perfectly, but isn't general enough to be useful for prediction.
    Overfit models often describe random errors or noise in the training data instead
    of the underlying relationship between the dependent and independent variables
    of the model. The best fit regression model generally lies in between the models
    created by underfitting and overfitting models and can be obtained through the
    process of regularization.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型未能估计出与训练数据中依赖变量的观察值接近的值时，我们称其为**欠拟合**或**高偏差**。另一方面，当一个估计模型完美地拟合数据，但不够通用以至于不能用于预测时，我们也可以称之为**过拟合**或**高方差**。过拟合模型通常描述的是训练数据中的随机误差或噪声，而不是模型中依赖变量和自变量之间的基本关系。最佳拟合回归模型通常位于欠拟合和过拟合模型之间，可以通过正则化过程获得。
- en: 'A commonly used method for the regularization of an underfit or overfit model
    is **Tikhnov regularization**. In statistics, this method is also called **ridge
    regression**. We can describe the general form of Tikhnov regularization as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于欠拟合或过拟合模型的正则化，常用的方法是**Tikhonov正则化**。在统计学中，这种方法也称为**岭回归**。我们可以将Tikhonov正则化的通用形式描述如下：
- en: '![Understanding regularization](img/4351OS_02_73.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_02_73.jpg)'
- en: Suppose *A* represents a mapping from the vector of independent variables *x*
    to the dependent variable *y*. The value *A* is analogous to the parameter vector
    of a regression model. The relationship between the vector *x* and the observed
    values of the dependent variable, written as *b*, can be expressed as follows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*A*代表从自变量向量*x*到依赖变量*y*的映射。值*A*类似于回归模型的参数向量。向量*x*与依赖变量的观察值之间的关系，用*b*表示，可以表达如下。
- en: 'An underfit model has a significant error, or rather deviation, with respect
    to the actual data. We should strive to minimize this error. This can be formally
    expressed as follows and is based on the sum of residues of the estimated model:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个欠拟合模型与实际数据存在显著的误差，或者说偏差。我们应该努力最小化这个误差。这可以形式化地表达如下，并且基于估计模型的残差之和：
- en: '![Understanding regularization](img/4351OS_02_74.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_02_74.jpg)'
- en: 'Tikhnov regularization adds a penalized least squares term to the preceding
    equation to prevent overfitting and is formally defined as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Tikhonov正则化向先前的方程添加了一个惩罚最小二乘项，以防止过拟合，其形式如下：
- en: '![Understanding regularization](img/4351OS_02_75.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_02_75.jpg)'
- en: 'The term ![Understanding regularization](img/4351OS_02_76.jpg) in the preceding
    equation is called the regularization matrix. In the simplest form of Tikhnov
    regularization, this matrix takes the value ![Understanding regularization](img/4351OS_02_77.jpg),
    where ![Understanding regularization](img/4351OS_02_78.jpg) is a constant. Although
    applying this equation to a regression model is beyond the scope of this book,
    we can use Tikhnov regularization to produce a linear regression model with the
    following cost function:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的方程中的术语![理解正则化](img/4351OS_02_76.jpg)被称为正则化矩阵。在Tikhonov正则化的最简单形式中，这个矩阵取值为![理解正则化](img/4351OS_02_77.jpg)，其中![理解正则化](img/4351OS_02_78.jpg)是一个常数。尽管将此方程应用于回归模型超出了本书的范围，但我们可以使用Tikhonov正则化来生成具有以下成本函数的线性回归模型：
- en: '![Understanding regularization](img/4351OS_02_79.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_02_79.jpg)'
- en: In the preceding equation, the term ![Understanding regularization](img/4351OS_02_80.jpg)
    is called the regularization parameter of the model. This value must be chosen
    appropriately as larger values for this parameter could produce an underfit model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的方程中，术语![理解正则化](img/4351OS_02_80.jpg)被称为模型的正则化参数。此值必须选择适当，因为此参数的较大值可能会产生欠拟合模型。
- en: 'Using the previously defined cost function, we can apply a gradient descent
    to determine the parameter vector as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用先前定义的成本函数，我们可以应用梯度下降来确定参数向量，如下所示：
- en: '![Understanding regularization](img/4351OS_02_81.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_02_81.jpg)'
- en: 'We can also apply regularization to the OLS method of determining the parameter
    vector as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将正则化应用于确定参数向量的OLS方法，如下所示：
- en: '![Understanding regularization](img/4351OS_02_82.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_02_82.jpg)'
- en: In the preceding equation, *L* is called the smoothing matrix, and can take
    on the following forms. Note that we've used the latter form of the definition
    of *L* in [Chapter 1](ch01.html "Chapter 1. Working with Matrices"), *Working
    with Matrices*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的方程中，*L*被称为平滑矩阵，可以采用以下形式。请注意，我们在[第1章](ch01.html "第1章。矩阵操作")中使用了*L*定义的后一种形式，即*矩阵操作*。
- en: '![Understanding regularization](img/4351OS_01_0100.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![理解正则化](img/4351OS_01_0100.jpg)'
- en: Interestingly, when the regularization parameter ![Understanding regularization](img/4351OS_02_80.jpg)
    in the preceding equation is *0*, the regularized solution reduces to the original
    solution using the OLS method.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当先前的方程中的正则化参数![理解正则化](img/4351OS_02_80.jpg)为*0*时，正则化解简化为使用OLS方法得到的原始解。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, we''ve studied linear regression and a couple of algorithms
    that can be used to formulate an optimal linear regression model from some sample
    data. The following are some of the other points that we covered:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了线性回归以及一些可以用来从样本数据中构建最优线性回归模型的算法。以下是我们所涵盖的一些其他要点：
- en: We discussed linear regression with single and multiple variables
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了单变量和多变量的线性回归
- en: We implemented the gradient descent algorithm to formulate a linear regression
    model with one variable
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实现了梯度下降算法来构建一个单变量线性回归模型
- en: We implemented the **Ordinary Least Squares** (**OLS**) method to find the coefficients
    of an optimal linear regression model
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实现了**普通最小二乘法**（**OLS**）来找到最优线性回归模型的系数
- en: We introduced regularization and how it could be applied to linear regression
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了正则化及其在线性回归中的应用
- en: In the following chapter, we will study a different area of machine learning,
    that is, classification. Classification is also a form of regression and is used
    to categorize data into different classes or groups.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究机器学习的另一个领域，即分类。分类也是一种回归形式，用于将数据分类到不同的类别或组中。
