- en: Scala for Tree-Based Ensemble Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala用于树集成技术
- en: In the previous chapter, we solved both classification and regression problems
    using linear models. We also used logistic regression, support vector machine,
    and Naive Bayes. However, in both cases, we haven't experienced good accuracy
    because our models showed low confidence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用线性模型解决了分类和回归问题。我们还使用了逻辑回归、支持向量机和朴素贝叶斯。然而，在这两种情况下，我们没有获得很好的准确度，因为我们的模型表现出低置信度。
- en: On the other hand, tree-based and tree ensemble classifiers are really useful,
    robust, and widely used for both classification and regression tasks. This chapter
    will provide a quick glimpse at developing these classifiers and regressors using
    tree-based and ensemble techniques, such as **decision trees** (**DTs**), **random
    forests** (**RF**), and **gradient boosted trees** (**GBT**), for both classification
    and regression. More specifically, we will revisit and solve both the regression
    (from [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala for Regression
    Analysis*) and classification (from [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*) problems we discussed previously.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于树和树集成分类器对于分类和回归任务来说非常有用、稳健且广泛使用。本章将简要介绍使用基于树和集成技术（如**决策树**（**DTs**）、**随机森林**（**RF**）和**梯度提升树**（**GBT**））开发这些分类器和回归器的方法，用于分类和回归。更具体地说，我们将重新审视并解决之前讨论过的回归（来自[第2章](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml)，*Scala用于回归分析*）和分类（来自[第3章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)，*Scala用于分类学习*）问题。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Decision trees and tree ensembles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树和树集成
- en: Decision trees for supervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于监督学习的决策树
- en: Gradient boosted trees for supervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于监督学习的梯度提升树
- en: Random forest for supervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于监督学习的随机森林
- en: What's next?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Scala 2.11.x和Java 1.8.x已安装并配置在您的机器上。
- en: 'The code files of this chapters can be found on GitHub:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter04)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter04](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter04)'
- en: 'Check out the following playlist to see the Code in Action video for this chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下播放列表，以查看本章的代码实战视频：
- en: '[http://bit.ly/2WhQf2i](http://bit.ly/2WhQf2i)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2WhQf2i](http://bit.ly/2WhQf2i)'
- en: Decision trees and tree ensembles
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和树集成
- en: 'DTs normally fall under supervised learning techniques, which are used to identify
    and solve problems related to classification and regression. As the name indicates,
    DTs have various branches—where each branch indicates a possible decision, appearance,
    or reaction in terms of statistical probability. In terms of features, DTs are
    split into two main types: the training set and the test set, which helps produce
    a good update on the predicted labels or classes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DTs通常属于监督学习技术，用于识别和解决与分类和回归相关的问题。正如其名所示，DTs有各种分支——每个分支表示基于统计概率的可能决策、外观或反应。就特征而言，DTs分为两大类：训练集和测试集，这有助于对预测标签或类别的更新产生良好的效果。
- en: 'Both binary and multiclass classification problems can be handled by DT algorithms,
    which is one of the reasons it is used across problems. For instance, for the
    admission example we introduced in [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala
    for Learning Classification*, DTs learn from the admission data to approximate
    a sine curve with a set of `if...else` decision rules, as shown in the following
    diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DT算法可以处理二元和多类分类问题，这也是它在各种问题中广泛应用的原因之一。例如，对于我们在[第3章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)中介绍的招生示例，“Scala用于分类学习”，DTs通过一组`if...else`决策规则从招生数据中学习，以近似正弦曲线，如图所示：
- en: '![](img/b07f5aa1-744a-4782-8fc6-4ac4bdab1cc4.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b07f5aa1-744a-4782-8fc6-4ac4bdab1cc4.png)'
- en: Generating decision rules using DTs based on university admission data
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大学招生数据使用DTs生成决策规则
- en: 'In general, the bigger the tree, the more complex the decision rules and the
    more fitted the model is. Another exciting power of DTs is that they can be used
    to solve both classification and regression problems. Now let''s see some pros
    and cons of DTs. The two widely-used tree-based ensemble techniques are RF and
    GBT. The main difference between these two techniques is the way and order in
    which trees are trained:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，树越大，决策规则越复杂，模型拟合度越高。决策树（DT）的另一个令人兴奋的功能是它们可以用来解决分类和回归问题。现在让我们看看DT的一些优缺点。两种广泛使用的基于树的集成技术是RF和GBT。这两种技术之间的主要区别在于训练树的方式和顺序：
- en: RFs train each tree independently but based on a random sample of the data.
    These random samples help to make the model more robust than a single DT, and
    hence it is less likely to have an overload on the training data.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RF独立地训练每棵树，但基于数据的随机样本。这些随机样本有助于使模型比单个DT更稳健，因此它不太可能对训练数据产生过载。
- en: GBTs train one tree at a time. The errors created by the trees trained previously
    will be rectified by every new tree that is trained. As more trees are added,
    the model becomes more expressive.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GBT一次训练一棵树。先前训练的树产生的错误将被每棵新训练的树纠正。随着树的增加，模型的表达能力更强。
- en: RFs take a subset of observations and a subset of variables to build, which
    is an ensemble of DTs. These trees are actually trained on different parts of
    the same training set, but individual trees grow very deep tends to learn from
    highly unpredictable patterns.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林（RF）从观测值和变量子集中选取一部分来构建，这是一个决策树的集成。这些树实际上是在同一训练集的不同部分上训练的，但单个树生长得很深，往往能从高度不可预测的模式中学习。
- en: Sometimes very deep trees are responsible for overfitting problems in DT models.
    In addition, these biases can make the classifier a low performer even if the
    quality of the features represented is good with respect to the dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有时非常深的树是DT模型中过拟合问题的原因。此外，这些偏差可能会使分类器表现不佳，即使就数据集而言，表示的特征质量很好。
- en: 'When DTs are built, RFs integrate them together to get a more accurate and
    stable prediction. RFs helps to average multiple DTs together, with the goal of
    reducing the variance to ensure consistency by computing proximities between pairs
    of cases. This is a direct consequence on RF too. By maximum voting from a panel
    of independent jurors, we get the final prediction that is better than the best
    jury. The following figure shows how the decisions from two forests are ensembled
    together to get the final prediction:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建决策树时，随机森林将它们整合在一起以获得更准确和稳定的预测。RF通过计算成对案例之间的邻近度来平均多个决策树，目的是减少方差以确保一致性。这是对RF的直接后果。通过一组独立陪审团的多数投票，我们得到比最佳陪审团更好的最终预测。以下图显示了两个森林的决策如何集成以获得最终预测：
- en: '![](img/debf3299-9124-459b-92b1-c5682ebce220.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/debf3299-9124-459b-92b1-c5682ebce220.png)'
- en: Tree-based ensemble and its assembling technique
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的集成及其组装技术
- en: 'In the end, both RF and GBT produce a weighted collection of DT, which is followed
    by predicting the combining results from the individual trees of each ensemble
    model. When using these approaches (as a classifier or regressor), the parameter
    settings are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，RF和GBT都产生一个决策树的加权集合，随后从每个集成模型的单个树中进行预测组合结果。当使用这些方法（作为分类器或回归器）时，参数设置如下：
- en: If the number of trees is 1, no bootstrapping is applied. If the number of trees
    is greater than 1, bootstrapping is applied, with `auto`, `all`, `sqrt`, `log2`,
    and one-third being the supported values.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果树木的数量为1，则不应用自助法。如果树木的数量大于1，则应用自助法，支持的值有`auto`、`all`、`sqrt`、`log2`和三分之一。
- en: The supported numerical values are [0.0-1.0] and [1-n]. If `numTrees` is `1`,
    `featureSubsetStrategy` is set to be `all`. If `numTrees > 1` (for RF), `featureSubsetStrategy`
    is set to be `sqrt` for classification. If `featureSubsetStrategy` is chosen as
    `auto`, the algorithm infers the best feature subset strategy automatically.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持的数值范围是[0.0-1.0]和[1-n]。如果`numTrees`为`1`，则`featureSubsetStrategy`设置为`all`。如果`numTrees`大于1（对于RF），则`featureSubsetStrategy`设置为分类的`sqrt`。如果选择`auto`作为`featureSubsetStrategy`，则算法会自动推断最佳特征子集策略。
- en: The impurity criterion is used only for the information-gain calculation, with
    gini and variance as the supported values for classification and regression, respectively.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杂质标准仅用于信息增益的计算，分类时支持的值是基尼指数，回归时支持的值是方差。
- en: '`maxDepth` is the maximum depth of the tree (for example, depth 0 means 1 leaf
    node, depth 1 means 1 internal node and 2 leaf nodes, and so on).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxDepth`是树的最大深度（例如，深度0表示1个叶子节点，深度1表示1个内部节点和2个叶子节点，依此类推）。'
- en: '`maxBins` signifies the maximum number of bins used to split the features,
    where the suggested value is 100 to get better results.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxBins`表示用于分割特征的bin的最大数量，建议值为100以获得更好的结果。'
- en: Now that we've already dealt with both regression analysis and classification
    problems, let's see how to use DT, RF, and GBT more comfortably to solve these
    problems. Let's get started with DT.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经处理了回归分析和分类问题，让我们看看如何更舒适地使用DT、RF和GBT来解决这些问题。让我们从DT开始。
- en: Decision trees for supervised learning
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于监督学习的决策树
- en: In this section, we'll see how to use DTs to solve both regression and classification
    problems. In the previous two chapters, [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml),
    *Scala for Regression Analysis*, and [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*, we solved customer churn and insurance-severity
    claim problems. Those were classification and regression problems, respectively.
    In both approaches, we used other classic models. However, we'll see how we can
    solve them with tree-based and ensemble techniques. We'll use the DT implementation
    from the Apache Spark ML package in Scala.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用DT来解决回归和分类问题。在前面的两章中，[第2章](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml)，“Scala
    for Regression Analysis”和[第3章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)，“Scala
    for Learning Classification”，我们解决了客户流失和保险严重索赔问题。这些分别是分类和回归问题。在这两种方法中，我们都使用了其他经典模型。然而，我们将看到如何使用基于树和集成技术来解决它们。我们将使用Scala中的Apache
    Spark ML包中的DT实现。
- en: Decision trees for classification
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于分类的决策树
- en: 'First of all, we know the customer churn prediction problem in [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala
    for Learning Classification*, and we know the data as well. We also know the working
    principle of DTs. So we can directly move to the coding part using the Spark based
    implementation of DTs. First we create a `DecisionTreeClassifier` estimator by
    instantiating the `DecisionTreeClassifier` interface. Additionally, we need to
    specify the label and feature vector columns:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在[第3章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)中了解了客户流失预测问题，“Scala for
    Learning Classification”，我们也了解了相关数据。我们还知道了决策树（DT）的工作原理。因此，我们可以直接使用基于Spark的决策树实现来进入编码部分。首先，我们通过实例化`DecisionTreeClassifier`接口创建一个`DecisionTreeClassifier`估计器。此外，我们还需要指定标签和特征向量列：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As discussed in the previous chapters, we have three transformers (`ipindexer`,
    `labelindexer`, and `assembler`) and an estimator (`dTree`). We can now chain
    them together in a single pipeline so that each of them will act as a stage:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，我们有三个转换器（`ipindexer`、`labelindexer`和`assembler`）和一个估计器（`dTree`）。我们现在可以将它们链接在一起形成一个单一的管道，这样每个转换器都将作为一个阶段：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since we would like to perform hyperparameter tuning and cross-validation,
    we will have to create a `paramGrid` variable, which will be used for grid search
    over the hyperparameter space during the K-fold cross-validation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望执行超参数调整和交叉验证，我们需要创建一个`paramGrid`变量，该变量将在K折交叉验证期间用于超参数空间的网格搜索：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: More specifically, this will search for the DT's `impurity`, `maxBins`, and
    `maxDepth` for the best model. The maximum number of bins is used for separating
    continuous features and for choosing how to split on features at each node. Combined,
    the algorithm searches through the DT's `maxDepth` and `maxBins` parameters for
    the best model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，这将搜索DT的`impurity`、`maxBins`和`maxDepth`以找到最佳模型。最大bin数用于分离连续特征，并选择在每个节点上如何分割特征。算法结合搜索DT的`maxDepth`和`maxBins`参数以找到最佳模型。
- en: 'In the preceding code segment, we''re creating a progressive `paramGrid` variable,
    where we specify the combination as a list of string or integer values. That means
    we are creating the grid space with different hyperparameter combinations. This
    will help us to provide the best model, consisting of optimal hyperparameters.
    However, for that, we need to have a `BinaryClassificationEvaluator` evaluator
    to evaluate each model and pick the best one during the cross-validation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码段中，我们创建了一个渐进的`paramGrid`变量，其中我们指定组合为字符串或整数值的列表。这意味着我们正在创建具有不同超参数组合的网格空间。这将帮助我们提供最佳模型，包括最优超参数。然而，为了做到这一点，我们需要一个`BinaryClassificationEvaluator`评估器来评估每个模型，并在交叉验证期间选择最佳模型：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We use `CrossValidator` to perform 10-fold cross validation to select the best
    model:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`CrossValidator`进行10折交叉验证以选择最佳模型：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s now call the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and the DT classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们调用`fit`方法，以便执行完整的预定义管道，包括所有特征预处理和DT分类器，多次执行——每次使用不同的超参数向量：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now it''s time to evaluate the predictive power of the DT model on the test
    dataset:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估DT模型在测试数据集上的预测能力了：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will lead us to the following DataFrame showing the predicted labels against
    the actual labels. Additionally, it shows the raw probabilities:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下DataFrame显示预测标签与实际标签的对比。此外，它还显示了原始概率：
- en: '![](img/d2d26de4-9bd0-4238-a36d-ddf1cba4948d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d2d26de4-9bd0-4238-a36d-ddf1cba4948d.png)'
- en: 'However, based on the preceding prediction DataFrame, it is really difficult
    to guess the classification''s accuracy. But in the second step, the evaluation
    is done using `BinaryClassificationEvaluator` as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，根据前面的预测DataFrame，很难猜测分类的准确率。但在第二步，评估是使用`BinaryClassificationEvaluator`进行的，如下所示：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will provide an output with an accuracy value:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供一个包含准确率值的输出：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So, we get about 84% classification accuracy from our binary classification
    model. Just like with SVM and LR, we will observe the area under the precision-recall
    curve and the area under the **receiver operating characteristic** (**ROC**) curve
    based on the following RDD, which contains the raw scores on the test set:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从我们的二元分类模型中获得了大约84%的分类准确率。就像SVM和LR一样，我们将观察基于以下RDD的精确率-召回率曲线下面积和接收器操作特征（ROC）曲线下面积，该RDD包含测试集上的原始分数：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding RDD can be used for computing the previously mentioned two performance
    metrics:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的RDD可以用来计算前面提到的两个性能指标：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this case, the evaluation returns 84% accuracy but only 67% precision, which
    is much better than that of SVM and LR:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估返回了84%的准确率，但只有67%的精确度，这比SVM和LR要好得多：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we calculate some more metrics, for example, false and true positive,
    and false and true negative, as these predictions are also useful to evaluate
    the model''s performance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算一些更多的指标，例如，假阳性和真阳性，以及假阴性和真阴性，因为这些预测也有助于评估模型的表现：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Additionally, we compute the Matthews correlation coefficient:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们计算马修斯相关系数：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s observe how high the model confidence is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察模型置信度有多高：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Fantastic! We achieved only 70% accuracy, which is probably why we had a low
    number of trees, but for what factors?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们只达到了70%的准确率，这可能是我们树的数量较少的原因，但具体是哪些因素呢？
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now let''s see at what level we achieved the best model after the cross-validation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看交叉验证后我们达到了最佳模型的水平：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'According to the following output, we achieved the best tree model at `depth
    5` with `53 nodes`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下输出，我们在`深度5`和`53个节点`达到了最佳的树模型：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s extract those moves taken (that is, decisions) during tree construction
    by showing the tree. This tree helps us to find the most valuable features in
    our dataset:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过显示树来提取在树构建过程中采取的移动（即决策）。这棵树帮助我们找到数据集中最有价值的特征：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the following output, the `toDebugString()` method prints the tree''s decision
    nodes and final the prediction outcomes at the end leaves:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下输出中，`toDebugString()`方法打印了树的决策节点和最终预测结果在叶子节点：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can also see that certain features (`3` and `11` in our case) are used for
    decision making—that is, the two most important reasons customers are likely to
    churn. But what are those two features? Let''s see them:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到某些特征（在我们的例子中是`3`和`11`）被用于决策，即客户可能流失的两个最重要的原因。但这两个特征是什么？让我们看看：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'According to the following output, feature 3 and 11 were most important predictors:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以下输出，特征3和11是最重要的预测因子：
- en: '[PRE21]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The customer service calls and total day minutes are selected by DTs, since
    they provide an automated mechanism for determining the most important features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 客户服务电话和总日分钟数由DTs选择，因为它们提供了一个自动化的机制来确定最重要的特征。
- en: Decision trees for regression
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归决策树
- en: In [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala for Learning
    Classification*, we learned how to predict the problem regarding slowness in traffic.
    We applied **linear regression** (**LR**) and generalized linear regression to
    solve this problem. Also, we knew the data very well.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)，“用于学习的Scala分类”，我们学习了如何预测关于交通缓慢的问题。我们应用了**线性回归**（**LR**）和广义线性回归来解决此问题。我们也非常了解数据。
- en: 'As stated earlier, DT also can provide very powerful responses and performance
    in the case of a regression problem. Similar to `DecisionTreeClassifier`, a `DecisionTreeRegressor`
    estimator can be instantiated with the `DecisionTreeRegressor()` method. Additionally,
    we need to explicitly specify the label and feature columns:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，决策树（DT）在回归问题中也能提供非常强大响应和性能。类似于`DecisionTreeClassifier`，可以使用`DecisionTreeRegressor()`方法实例化`DecisionTreeRegressor`估计器。此外，我们需要明确指定标签和特征列：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can set the max bins, number of trees, max depth, and impurity while instantiating
    the preceding estimator.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化前面的估计器时，我们可以设置最大分箱数、树的数量、最大深度和纯度：
- en: 'However, since we''ll perform k-fold cross-validation, we can set those parameters
    while creating `paramGrid`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们将执行k折交叉验证，我们可以在创建`paramGrid`时设置这些参数：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For a better and more stable performance, let''s prepare the k-fold cross-validation
    and grid search as a part of model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust number of folds based on your settings
    and dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好和更稳定的性能，让我们准备k折交叉验证和网格搜索作为模型调优的一部分。正如你所猜到的，我将执行10折交叉验证。请根据你的设置和数据集自由调整折数：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Fantastic! We have created the cross-validation estimator. Now it''s time to
    train DT regression model with cross-validation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经创建了交叉验证估计器。现在是时候使用交叉验证训练DT回归模型了：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now that we have the fitted model, we can make predictions. So let''s start
    evaluating the model on the train and validation set and calculate RMSE, MSE,
    MAE, R squared, and so on:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了拟合好的模型，我们可以进行预测。所以让我们开始评估模型在训练集和验证集上的表现，并计算RMSE、MSE、MAE、R平方等：
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once we have the best-fitted and cross-validated model, we can expect a good
    prediction accuracy. Let''s observe the result on the train and the validation
    set:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳拟合和交叉验证模型，我们可以期待良好的预测精度。让我们观察训练集和验证集上的结果：
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following output shows the MSE, RMSE, R-squared, MAE and explained variance
    on the test set:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了测试集上的MSE、RMSE、R平方、MAE和解释方差：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Great! We have managed to compute the raw prediction on the train set and the
    test set, and we can see the improvements compared to LR regression model. Let''s
    hunt for the model that will help us to achieve better accuracy:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经成功计算了训练集和测试集上的原始预测，并且我们可以看到与LR回归模型相比的改进。让我们寻找能帮助我们实现更好精度的模型：
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Additionally, we can see how the decisions were made by observing DTs in the
    forest:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过观察森林中的决策树来了解决策是如何做出的：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following is the output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为输出结果：
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With DTs, it is possible to measure the feature importance, so that in a later
    stage we can decide which features to use and which ones to drop from the DataFrame.
    Let''s find the feature importance out of the best model we just created before
    for all the features that are arranged in an ascending order as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树（DTs），我们可以测量特征重要性，这样在后续阶段我们可以决定使用哪些特征以及从DataFrame中删除哪些特征。让我们找出我们刚刚创建的最佳模型的所有特征的重要性，这些特征按以下升序排列：
- en: '[PRE32]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Following is the feature importance generated by the model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由模型生成的特征重要性：
- en: '[PRE33]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The last result is important to understand the feature importance. As you can
    see, RF has ranked some features to be more important. For example, the last few
    features are the most important ones, while eight of them are less important.
    We can drop those unimportant columns and train the DT model again to observe
    whether there is any greater reduction of MAE and increase in R-squared on the
    test set.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的结果对于理解特征重要性很重要。正如你所见，随机森林（RF）将一些特征排名为更重要。例如，最后几个特征是最重要的，而其中八个则相对不那么重要。我们可以删除这些不重要的列，并再次训练DT模型，以观察测试集上MAE的减少和R平方的增加是否有所改善。
- en: Gradient boosted trees for supervised learning
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习的梯度提升树
- en: In this section, we'll see how to use GBT to solve both regression and classification
    problems. In the previous two chapters, [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala
    for Regression Analysis*, and [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml), *Scala
    for Learning Classification*, we solved the customer churn and insurance severity
    claim problems, which were classification and regression problem, respectively.
    In both approaches, we used other classic models. However, we'll see how we can
    solve them with tree-based and ensemble techniques. We'll use the GBT implementation
    from the Spark ML package in Scala.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用GBT来解决回归和分类问题。在前两章中，[第二章](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml)，*Scala回归分析*，和[第三章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)，*Scala学习分类*，我们解决了客户流失和保险严重索赔问题，分别是分类和回归问题。在这两种方法中，我们使用了其他经典模型。然而，我们将看到如何使用基于树和集成技术来解决它们。我们将使用Scala中的Spark
    ML包中的GBT实现。
- en: Gradient boosted trees for classification
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于分类的梯度提升树
- en: 'We know the customer churn prediction problem from [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*, and we know the data well. We already know
    the working principles of RF, so let''s start using the Spark-based implementation
    of RF:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[第三章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)，*Scala学习分类*中了解到客户流失预测问题，并且我们对数据很熟悉。我们已经知道RF的工作原理，所以让我们开始使用基于Spark的RF实现：
- en: 'Instantiate a `GBTClassifier` estimator by invoking the `GBTClassifier()` interface:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`GBTClassifier()`接口实例化一个`GBTClassifier`估计器：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have three transformers and an estimator ready. Chain in a single pipeline,
    that is, each of them acts a stage:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经有了三个转换器和一个估计器就绪。将它们链式连接成一个单一管道，即它们各自作为一个阶段：
- en: '[PRE35]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the `paramGrid` variable to perform such a grid search over the hyperparameter
    space:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`paramGrid`变量以在超参数空间中进行网格搜索：
- en: '[PRE36]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`BinaryClassificationEvaluator`评估器来评估模型：
- en: '[PRE37]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We use a `CrossValidator` for performing 10-fold cross validation for best
    model selection:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`CrossValidator`进行10折交叉验证以选择最佳模型：
- en: '[PRE38]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s call now the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and DT classifier, is executed multiple times—each
    time with a different hyperparameter vector:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们调用`fit`方法，这样完整的预定义管道，包括所有特征预处理和DT分类器，就会执行多次——每次使用不同的超参数向量：
- en: '[PRE39]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now it''s time to evaluate the predictive power of DT model on the test dataset:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估DT模型在测试数据集上的预测能力了：
- en: 'Transform the test set with the model pipeline, which will update the features
    as per the same mechanism we described in the preceding feature engineering step:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型管道转换测试集，这将根据我们在前面的特征工程步骤中描述的相同机制更新特征：
- en: '[PRE40]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This will lead us to the following DataFrame showing the predicted labels against
    the actual labels. Additionally, it shows the raw probabilities:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下DataFrame，显示预测标签与实际标签的对比。此外，它还显示了原始概率：
- en: '![](img/5764c41b-e8ca-46e1-8139-1361a875ed98.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5764c41b-e8ca-46e1-8139-1361a875ed98.png)'
- en: However, after seeing the preceding prediction DataFrame, it is really difficult
    to guess the classification accuracy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在看到前面的预测DataFrame后，很难猜测分类准确率。
- en: 'But in the second step, in the evaluation is done using `BinaryClassificationEvaluator`
    as follows:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但在第二步中，评估是使用`BinaryClassificationEvaluator`进行的，如下所示：
- en: '[PRE41]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This will give us the classification accuracy:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们提供分类准确率：
- en: '[PRE42]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'So we get about 87% classification accuracy from our binary classification
    model. Just like with SVM and LR, we will observe the area under the precision-recall
    curve and the area under the ROC curve based on the following RDD, which contains
    the raw scores on the test set:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从我们的二元分类模型中获得大约87%的分类准确率。就像SVM和LR一样，我们将根据以下RDD观察精确率-召回率曲线下的面积和ROC曲线下的面积，该RDD包含测试集上的原始分数：
- en: '[PRE43]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The preceding RDD can be used for computing the previously mentioned performance
    metrics:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的RDD可用于计算之前提到的性能指标：
- en: '[PRE44]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will share the value in terms of accuracy and prediction:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将分享准确性和预测的价值：
- en: '[PRE45]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In this case, the evaluation returns 87% accuracy but only 73% precision, which
    is much better than SVM and LR. Then we calculate some more false and true metrics.
    Positive and negative predictions can also be useful to evaluate the model''s
    performance:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估返回 87% 的准确率，但只有 73% 的精确率，这比 SVM 和 LR 好得多。然后我们计算更多的错误和真实指标。正负预测也可以用来评估模型性能：
- en: '[PRE46]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Additionally, we compute the Matthews correlation coefficient:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们计算马修斯相关系数：
- en: '[PRE47]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s observe how high the model confidence is:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察模型置信度有多高：
- en: '[PRE48]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now let''s take a look at the true positive, false positive, true negative,
    and false negative rates. Additionally, we see the MCC:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看真正的阳性、假阳性、真正性和假阴性率。此外，我们看到了 MCC：
- en: '[PRE49]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: These rates looks promising as we experienced positive MCC that shows mostly
    positive correlation indicating a robust classifier. Now, similar to DTs, RFs
    can be debugged during the classification. For the tree to be printed and to select
    the most important features, run the last few lines of code in the DT. Note that
    we still confine the hyperparameter space with `numTrees`, `maxBins`, and `maxDepth`
    by limiting them to `7`. Remember that bigger trees will most likely perform better.
    Therefore, feel free to play around with this code and add features, and also
    use a bigger hyperparameter space, for instance, bigger trees.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些比率看起来很有希望，因为我们经历了正 MCC，这表明大多数情况下有正相关，表明这是一个稳健的分类器。现在，类似于决策树，随机森林在分类过程中也可以进行调试。为了打印树并选择最重要的特征，请运行决策树中的最后几行代码。请注意，我们仍然通过将
    `numTrees`、`maxBins` 和 `maxDepth` 限制为 `7` 来限制超参数空间。记住，更大的树更有可能表现更好。因此，请随意尝试这段代码，添加特征，并使用更大的超参数空间，例如更大的树。
- en: GBTs for regression
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GBT 回归
- en: To reduce the size of a loss function, GBTs will train many DTs. For each instance,
    the algorithm will use the ensemble that is currently available to predict the
    label of each training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少损失函数的大小，GBT 将训练许多决策树。对于每个实例，算法将使用当前可用的集成来预测每个训练实例的标签。
- en: 'Similar to decision trees, GBTs can do the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与决策树类似，GBT 可以执行以下操作：
- en: Handle both categorical and numerical features
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理分类和数值特征
- en: Be used for both binary classification and regression (multiclass classification
    is not yet supported)
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于二元分类和回归（多类分类尚不支持）
- en: Do not require feature scaling
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要特征缩放
- en: Capture non-linearity and feature interactions from very high-dimensional datasets
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从非常高维的数据集中捕获非线性特征和特征交互
- en: 'Suppose we have *N* data instances (being *x[i]* = features of instance *i*)
    and *y* is the label (being *y[i]* = label of instance *i*), then *f(x[i])* is
    GBT model''s predicted label for instance *i*, which tries to minimize any of
    the following losses:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有 *N* 个数据实例（其中 *x[i]* 表示实例 *i* 的特征）和 *y* 是标签（其中 *y[i]* 表示实例 *i* 的标签），那么
    *f(x[i])* 是 GBT 模型对实例 *i* 的预测标签，它试图最小化以下损失之一：
- en: '![](img/65f6f4b8-e90a-47da-a3c0-030f4ff1d493.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/65f6f4b8-e90a-47da-a3c0-030f4ff1d493.png)'
- en: '![](img/880a15a7-d9c3-4c57-ac07-1527f8b1fb83.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/880a15a7-d9c3-4c57-ac07-1527f8b1fb83.png)'
- en: '![](img/a742e469-420d-40f4-aa94-c9bbef6a0223.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a742e469-420d-40f4-aa94-c9bbef6a0223.png)'
- en: The first equation is called the *log* loss, which is twice the binomial negative
    *log* likelihood. The second one called squared error is commonly referred to
    as *L2* loss and the default loss for GBT-based regression task. Finally, the
    third, called absolute error, is commonly referred to as *L1* loss and is recommended
    if the data points have many outliers and robust than squared error.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程称为 *log* 损失，是二项式负 *log* 似然的两倍。第二个称为平方误差，通常被称为 *L2* 损失，是 GBT 基于回归任务的默认损失。最后，第三个称为绝对误差，通常称为
    *L1* 损失，如果数据点有许多异常值，则比平方误差更稳健。
- en: 'Now that we know the minimum working principle of the GBT regression algorithm,
    we can get started. Let''s instantiate a `GBTRegressor` estimator by invoking
    the `GBTRegressor()` interface:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 GBT 回归算法的最小工作原理，我们可以开始。让我们通过调用 `GBTRegressor()` 接口来实例化 `GBTRegressor`
    估计器：
- en: '[PRE50]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We can set the max bins, number of trees, max depth, and impurity when instantiating
    the preceding estimator. However, since we''ll perform k-fold cross-validation,
    we can set those parameters while creating the `paramGrid` variable too:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化前面的估计器时，我们可以设置最大箱数、树的数量、最大深度和纯度。然而，由于我们将执行 k 折交叉验证，我们也可以在创建 `paramGrid`
    变量时设置这些参数：
- en: '[PRE51]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**Validation while training**: Gradient boosting can overfit, especially when
    you train your model with more trees. In order to prevent this issue, it is useful
    to validate (for example, using cross-validation) while carrying out the training.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练过程中的验证**：梯度提升可能会过拟合，尤其是在你用更多树训练模型时。为了防止这个问题，在训练过程中进行验证（例如，使用交叉验证）是有用的。'
- en: 'For a better and more stable performance, let''s prepare the k-fold cross-validation
    and grid search as part of the model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust the number of folds based on your
    settings and dataset:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的性能和更稳定的性能，让我们准备k折交叉验证和网格搜索作为模型调优的一部分。正如你所猜到的，我将执行10折交叉验证。请根据你的设置和数据集自由调整折数：
- en: '[PRE52]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Fantastic! We have created the cross-validation estimator. Now it''s time to
    train the `RandomForestRegression` model with cross-validation:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经创建了交叉验证估计器。现在，是时候用交叉验证来训练`RandomForestRegression`模型了：
- en: '[PRE53]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now that we have the fitted model, we can make predictions. Let''s start evaluating
    the model on the train and validation sets and calculate RMSE, MSE, MAE, and R
    squared error:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了拟合的模型，我们可以进行预测。让我们开始评估模型在训练集和验证集上的表现，并计算RMSE、MSE、MAE和R平方误差：
- en: '[PRE54]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Once we have the best-fitted and cross-validated model, we can expect a high
    prediction accuracy. Now let''s observe the result on the train and the validation
    sets:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳拟合和交叉验证的模型，我们可以期待高预测准确率。现在让我们观察训练集和验证集上的结果：
- en: '[PRE55]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following output shows the MSE, RMSE, R-squared, MAE and explained variance
    on the test set:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了测试集上的均方误差（MSE）、均方根误差（RMSE）、R-squared、MAE和解释方差：
- en: '[PRE56]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    set, and we can see the improvements compared to the LR, DT, and GBT regression
    models. Let''s hunt for the model that helps us to achieve better accuracy:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经成功计算了训练集和测试集上的原始预测，并且我们可以看到与LR、DT和GBT回归模型相比的改进。让我们寻找帮助我们实现更高准确率的模型：
- en: '[PRE57]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Additionally, we can see how the decisions were made by observing the DTs in
    the forest:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过观察森林中的决策树（DTs）来了解决策是如何做出的：
- en: '[PRE58]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'In the following output, the toDebugString() method prints the tree''s decision
    nodes and final prediction outcomes at the end leaves:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下输出中，toDebugString()方法打印了树的决策节点和最终预测结果在最终叶子节点：
- en: '[PRE59]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'With random forest, it is possible to measure the feature importance so that
    in a later stage, we can decide which features to use and which ones to drop from
    the DataFrame. Let''s find the feature importance out of the best model we just
    created before for all the features that are arranged in an ascending order as
    follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林，我们可以测量特征重要性，这样在后续阶段，我们可以决定使用哪些特征以及从DataFrame中删除哪些特征。让我们找出我们刚刚创建的最佳模型中所有特征的排序，如下所示：
- en: '[PRE60]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Following is the feature importance generated by the model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型生成的特征重要性：
- en: '[PRE61]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The last result is important to understand the feature importance. As you can
    see, the RF has ranked some features that looks to be more important. For example,
    the last two features are the most important and the first two are less important.
    We can drop some unimportant columns and train the RF model to observe whether
    there is any reduction in the R-squared and MAE values on the test set.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的结果对于理解特征重要性非常重要。正如你所见，随机森林（RF）将一些看起来更重要的特征进行了排名。例如，最后两个特征是最重要的，而前两个则不那么重要。我们可以删除一些不重要的列，并训练RF模型来观察测试集上的R-squared和MAE值是否有任何减少。
- en: Random forest for supervised learning
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习的随机森林
- en: 'In this section, we''ll see how to use RF to solve both regression and classification
    problems. We''ll use DT implementation from the Spark ML package in Scala. Although
    both GBT and RF are ensembles of trees, the training processes are different.
    For instance, RF uses the bagging technique to perform the example, while GBT
    uses boosting. Nevertheless, there are several practical trade-offs between both
    the ensembles that can pose a dilemma about what to choose. However, RF would
    be the winner in most of the cases. Here are some justifications:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用RF来解决回归和分类问题。我们将使用Scala中的Spark ML包中的DT实现。尽管GBT和RF都是树的集成，但它们的训练过程是不同的。例如，RF使用bagging技术进行示例，而GBT使用boosting。尽管如此，两者之间有几个实际的权衡，可能会造成选择的困境。然而，在大多数情况下，RF将是赢家。以下是一些理由：
- en: GBTs train one tree at a time, but RF can train multiple trees in parallel.
    So the training time is lower with RF. However, in some special cases, training
    and using a smaller number of trees with GBTs is faster and more convenient.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GBTs一次训练一棵树，但RF可以并行训练多棵树。因此，RF的训练时间更低。然而，在某些特殊情况下，使用GBTs训练和较少的树数量更快、更方便。
- en: RFs are less prone to overfitting. In other words, RFs reduces variance with
    more trees, but GBTs reduce bias with more trees.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFs不太容易过拟合。换句话说，RFs通过增加树的数量来减少方差，而GBTs通过增加树的数量来减少偏差。
- en: RFs can be easier to tune since performance improves monotonically with the
    number of trees, but GBTs perform badly with an increased number of trees.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFs更容易调整，因为性能随着树的数量单调增加，但GBTs随着树的数量增加表现不佳。
- en: Random forest for classification
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于分类的随机森林
- en: We're familiar with the customer churn prediction problem from [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*, and we also know the data well. We also know
    the working principle of RF. So, we can directly jump into coding using the Spark-based
    implementation of RFs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们熟悉客户流失预测问题，来自[第3章](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml)，《Scala用于学习分类》，并且我们也对数据很了解。我们还了解随机森林的工作原理。因此，我们可以直接跳入使用基于Spark的RF实现进行编码。
- en: 'We get started by instantiating a `RandomForestClassifier` estimator by invoking
    the `RandomForestClassifier()` interface:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用`RandomForestClassifier()`接口来实例化一个`RandomForestClassifier`估计器：
- en: '[PRE62]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now that we have three transformers and an estimator ready, the next task is
    to chain in a single pipeline, that is, each of them acts as a stage:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了三个转换器和估计器，下一个任务是将它们链式连接成一个单一的管道，即每个都作为阶段：
- en: '[PRE63]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s define `paramGrid` to perform a grid search over the hyperparameter
    space:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义`paramGrid`以在超参数空间中进行网格搜索：
- en: '[PRE64]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`BinaryClassificationEvaluator`评估器来评估模型：
- en: '[PRE65]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We use a `CrossValidator` to perform 10-fold cross validation to select the
    best model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`CrossValidator`进行10折交叉验证以选择最佳模型：
- en: '[PRE66]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let''s call now the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and the DT classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称这个`fit`方法为执行完整预定义管道，包括所有特征预处理和DT分类器，多次执行——每次使用不同的超参数向量：
- en: '[PRE67]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Now it's time to evaluate the predictive power of the DT model on the test dataset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估DT模型在测试数据集上的预测能力了。
- en: 'As a first step, we need to transform the test set with the model pipeline,
    which will map the features according to the same mechanism we described in the
    feature engineering step:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们需要使用模型管道转换测试集，这将根据我们在特征工程步骤中描述的相同机制映射特征：
- en: '[PRE68]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This will lead us to the following DataFrame showing the predicted labels against
    the actual labels. Additionally, it shows the raw probabilities:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下DataFrame，显示预测标签与实际标签的对比。此外，它还显示了原始概率：
- en: '![](img/779f9ae9-174a-49f5-a1af-6bc666d4286d.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/779f9ae9-174a-49f5-a1af-6bc666d4286d.png)'
- en: However, based on the preceding prediction DataFrame, it is really difficult
    to guess the classification accuracy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于先前的预测DataFrame，很难猜测分类准确率。
- en: 'But in the second step, the evaluation is done using `BinaryClassificationEvaluator`
    as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 但在第二步中，评估是使用`BinaryClassificationEvaluator`进行的，如下所示：
- en: '[PRE69]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The following is the output:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是：
- en: '[PRE70]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'So we get about 87% classification accuracy from our binary classification
    model. Now, similar to SVM and LR, we will observe the area under the precision-recall
    curve and the area under the ROC curve based on the following RDD, which contains
    the raw scores on the test set:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从我们的二分类模型中获得大约87%的分类准确率。现在，类似于SVM和LR，我们将观察基于以下RDD的精确度-召回曲线下的面积和ROC曲线下的面积，该RDD包含测试集上的原始分数：
- en: '[PRE71]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The preceding RDD can be used to compute the previously mentioned performance
    metrics:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的RDD可以用来计算之前提到的性能指标：
- en: '[PRE72]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'In this case, the evaluation returns 88% accuracy but only 73% precision, which
    is much better than SVM and LR:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估返回了88%的准确率，但只有73%的精确度，这比SVM和LR要好得多：
- en: '[PRE73]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Then we calculate some more metrics, for example, false and true positive and
    negative predictions, which will be useful to evaluate the model''s performance:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算一些更多的指标，例如，假正例和真正例以及真负例和假负例预测，这些将有助于评估模型性能：
- en: '[PRE74]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Additionally, we compute the Matthews correlation coefficient:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们计算马修斯相关系数：
- en: '[PRE75]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Let''s observe how high the model confidence is:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们观察模型置信度有多高：
- en: '[PRE76]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now let''s take a look at the true positive, false positive, true negative,
    and false negative rates. Additionally, we see the MCC:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下真正例（true positive）、假正例（false positive）、真负例（true negative）和假负例（false
    negative）的比率。此外，我们还可以看到MCC：
- en: '[PRE77]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Just like DT and GBT, RF not only shows robust performance but also a slightly
    improved performance. And like DT and GBT, RF can be debugged to get the DT that
    was constructed during the classification. For the tree to be printed and the
    most important features selected, try the last few lines of code in the DT, and
    you're done.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 就像DT和GBT一样，RF不仅表现出稳健的性能，而且略有改进。并且像DT和GBT一样，RF可以被调试以获取分类过程中构建的DT。为了打印树和选择最重要的特征，尝试DT的最后几行代码，然后完成。
- en: Can you guess how many different models were trained? Well, we have 10-folds
    on cross-validation and 5-dimensional hyperparameter space cardinalities between
    2 and 7\. Now let's do some simple math: *10 * 7 * 5 * 2 * 3 * 6 = 12,600* models!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜到训练了多少个不同的模型吗？嗯，我们在交叉验证中有10折，并且在2到7之间的5维超参数空间中。现在让我们做一些简单的数学计算：*10 * 7 *
    5 * 2 * 3 * 6 = 12,600*个模型！
- en: Now that we have seen how to use RF in a classification setting, let's see another
    example of regression analysis.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何在分类设置中使用RF，让我们看看回归分析的另一个例子。
- en: Random forest for regression
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: Since RF is fast and scalable enough for a large-scale dataset, Spark-based
    implementations of RF help you achieve massive scalability. Fortunately, we already
    know the working principles of RF.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RF足够快且可扩展，适用于大规模数据集，基于Spark的RF实现可以帮助你实现大规模可扩展性。幸运的是，我们已经知道了RF的工作原理。
- en: If the proximities are calculated in RF, the storage requirements also grow
    exponentially.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在RF中计算邻近度，存储需求也会呈指数增长。
- en: 'We can jump directly into coding using the Spark-based implementation of RF
    for regression. We get started by instantiating a `RandomForestClassifier` estimator
    by invoking the `RandomForestClassifier()` interface:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接使用基于Spark的RF回归实现进行编码。我们通过调用`RandomForestClassifier()`接口来实例化`RandomForestClassifier`估计器：
- en: '[PRE78]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Now let''s create a grid space by specifying some hyperparameters, such as
    the max number of bins, max depth of the trees, number of trees, and impurity
    types:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过指定一些超参数，如最大箱数、树的最大深度、树的数量和纯度类型来创建一个网格空间：
- en: '[PRE79]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'For a better and more stable performance, let''s prepare the k-fold cross-validation
    and grid search as part of the model tuning. As you can guess, I am going to perform
    10-fold cross-validation. Feel free to adjust the number of folds based on your
    settings and dataset:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好、更稳定的性能，让我们准备k折交叉验证和网格搜索作为模型调优的一部分。正如你所猜到的，我将执行10折交叉验证。请根据你的设置和数据集自由调整折数：
- en: '[PRE80]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Fantastic! We have created the cross-validation estimator. Now it''s time to
    train the random forest regression model with cross-validation:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经创建了交叉验证估计器。现在是时候使用交叉验证来训练随机森林回归模型了：
- en: '[PRE81]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now that we have the fitted model, we can make predictions. Let''s start evaluating
    the model on the train and validation sets and calculate RMSE, MSE, MAE, and R
    squared:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了拟合的模型，我们可以进行预测。让我们开始评估模型在训练集和验证集上的表现，并计算RMSE、MSE、MAE和R平方：
- en: '[PRE82]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Once we have the best-fitted and cross-validated model, we can expect a good
    prediction accuracy. Now let''s observe the result on the train and validation
    sets:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳拟合和交叉验证的模型，我们可以期待良好的预测准确率。现在让我们在训练集和验证集上观察结果：
- en: '[PRE83]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The following output shows the MSE, RMSE, R-squared, MAE and explained variance
    on the test set:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出显示了测试集上的均方误差（MSE）、均方根误差（RMSE）、决定系数（R-squared）、平均绝对误差（MAE）和解释方差：
- en: '[PRE84]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Great! We have managed to compute the raw prediction on the train and the test
    sets, and we can see the improvements compared to the LR, DT, and GBT regression
    models. Let''s hunt for the model that helps us to achieve better accuracy:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经成功计算了训练集和测试集上的原始预测值，并且我们可以看到与LR、DT和GBT回归模型相比的改进。让我们寻找帮助我们实现更高准确率的模型：
- en: '[PRE85]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Additionally, we can see how the decisions were made by seeing DTs in the forest:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过查看森林中的决策树来了解决策是如何做出的：
- en: '[PRE86]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'In the following output, the `toDebugString()` method prints the tree''s decision
    nodes and final prediction outcomes at the end leaves:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的输出中，`toDebugString()`方法打印了树的决策节点和最终预测结果在叶子节点：
- en: '[PRE87]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'With RF, it is possible to measure the feature importance so that at a later
    stage, we can decide which features to use and which ones to drop from the DataFrame.
    Let''s find the feature importance out of the best model we just created before
    we arrange all the feature in an ascending order as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RF，我们可以测量特征重要性，这样在以后阶段，我们可以决定使用哪些特征以及从DataFrame中删除哪些特征。在我们将所有特征按升序排列之前，让我们先找出我们刚刚创建的最佳模型中的特征重要性：
- en: '[PRE88]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Following is the feature importance generated by the model:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型生成的特征重要性：
- en: '[PRE89]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: The last result is important for understanding the feature importance. As seen,
    some features have higher weights than others. Even some of these have zero weights.
    Higher the weights the higher the importance of a feature. For example, the last
    two features are the most important, and the first two are less important. We
    can drop some unimportant columns and train the RF model to observe whether there
    is any reduction in the R-squared and MAE values on the test set.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的结果对于理解特征重要性非常重要。正如所见，一些特征的权重高于其他特征。甚至其中一些特征的权重为零。权重越高，特征的相对重要性就越高。例如，最后两个特征是最重要的，而前两个则相对不那么重要。我们可以删除一些不重要的列，并训练RF模型来观察测试集上的R-squared和MAE值是否有任何减少。
- en: What's next?
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: So far, we have mostly covered classic and tree-based algorithms for both regression
    and classification. We saw that the ensemble technique showed the best performance
    compared to classic algorithms. However, there are other algorithms, such as one-vs-rest
    algorithm, which work for solving classification problems using other classifiers,
    such as logistic regression.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要介绍了回归和分类的经典和基于树的算法。我们看到了与经典算法相比，集成技术表现出最佳性能。然而，还有其他算法，例如一对一算法，它适用于使用其他分类器（如逻辑回归）解决分类问题。
- en: Apart from this, neural-network-based approaches, such as **multilayer perceptron**
    (**MLP**), **convolutional neural network** (**CNN**), and **recurrent neural
    network** (**RNN**), can also be used to solve supervised learning problems. However,
    as expected, these algorithms require a large number of training samples and a
    large computing infrastructure. The datasets we used so far throughout the examples
    had a few samples. Moreover, those were not so high dimensional. This doesn't
    mean that we cannot use them to solve these two problems; we can, but this results
    in huge overfitting due to a lack of training samples.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，基于神经网络的算法，如**多层感知器**（**MLP**）、**卷积神经网络**（**CNN**）和**循环神经网络**（**RNN**），也可以用来解决监督学习问题。然而，正如预期的那样，这些算法需要大量的训练样本和强大的计算基础设施。到目前为止，我们在示例中使用的数据集样本数量很少。此外，这些数据集的维度也不是很高。这并不意味着我们不能用它们来解决这两个问题；我们可以，但这会导致由于训练样本不足而导致的巨大过拟合。
- en: How do we fix this issue? Well, we can either search for other datasets or generate
    training data randomly. We'll discuss and show how we can train neural-network-based
    deep learning models to solve other problems.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决这个问题？嗯，我们可以寻找其他数据集或随机生成训练数据。我们将讨论并展示如何训练基于神经网络的深度学习模型来解决其他问题。
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we had a brief introduction to powerful tree-based algorithms,
    such as DTs, GBT, and RF, for solving both classification and regression tasks.
    We saw how to develop these classifiers and regressors using tree-based and ensemble
    techniques. Through two real-world classification and regression problems, we
    saw how tree ensemble techniques outperform DT-based classifiers or regressors.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了用于解决分类和回归任务的强大基于树的算法，如决策树（DTs）、梯度提升树（GBT）和随机森林（RF）。我们看到了如何使用基于树和集成技术来开发这些分类器和回归器。通过两个现实世界的分类和回归问题，我们看到了如何基于树的集成技术优于基于决策树的分类器或回归器。
- en: We covered supervised learning for both classification and regression on structured
    and labeled data. However, with the rise of cloud computing, IoT, and social media,
    unstructured data is growing unprecedentedly, giving more than 80% data, most
    of which is unlabeled.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了结构化和标记数据的监督学习，包括分类和回归。然而，随着云计算、物联网和社交媒体的兴起，非结构化数据正以前所未有的速度增长，超过80%的数据，其中大部分是无标签的。
- en: Unsupervised learning techniques, such as clustering analysis and dimensionality
    reduction, are key applications in data-driven research and industry settings
    to find hidden structures from unstructured datasets automatically. There are
    many clustering algorithms, such as k-means and bisecting k-means. However, these
    algorithms cannot perform well with high-dimensional input datasets and often
    suffer from the *curse of dimensionality*. Reducing the dimensionality using algorithms
    such as **principal component analysis** (**PCA**) and feeding the latent data
    is useful for clustering billions of data points.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术，例如聚类分析和降维，是数据驱动研究和工业环境中从非结构化数据集中自动发现隐藏结构的关键应用。有许多聚类算法，如k-means和二分k-means。然而，这些算法在高维输入数据集上表现不佳，并且经常遭受**维度灾难**的困扰。使用**主成分分析**（PCA）等算法降低维度并将潜在数据输入是有助于聚类数十亿数据点的。
- en: In the next chapter, we will use one kind of genomic data to cluster a population
    according to their predominant ancestry, also called geographic ethnicity. We
    will also learn how to evaluate the clustering analysis result and about the dimensionality
    reduction technique to avoid the curse of dimensionality.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用一种基因组数据来根据人群的主要祖先聚类，也称为地理种族。我们还将学习如何评估聚类分析结果，以及关于降维技术以避免维度灾难的内容。
