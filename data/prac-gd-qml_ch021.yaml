- en: Chapter 12
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章
- en: Quantum Generative Adversarial Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子生成对抗网络
- en: '*Fake it ‘till you make it*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*假装做到你就能做到*'
- en: — Someone, somewhere
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: —— 某人，某地
- en: So far, we have only dealt with quantum machine learning models in the context
    of supervised learning. In this final chapter of our QML journey, we will discuss
    the wonders and mysteries of a QML model that will lead us into the domain of
    unsupervised learning. We will discuss quantum versions of the famous **Generative
    Adversarial Networks** (often abbreviated as **GANs**) that are called **Quantum
    Generative Adversarial Networks**, **quantum GANs**, or **QGANs**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了在监督学习背景下量子机器学习模型。在我们量子机器学习（QML）之旅的最后一章中，我们将讨论一个QML模型的奇妙和神秘之处，这将引领我们进入无监督学习的领域。我们将讨论著名的**生成对抗网络**（通常缩写为**GANs**）的量子版本，称为**量子生成对抗网络**、**量子GANs**或**QGANs**。
- en: In this chapter, you will learn what classical and quantum GANs are, what they
    are useful for, and how they can be used. We will begin from the basics, exploring
    the intuitive ideas that lead to the concept of a GAN. Then, we will get into
    some of the details and discuss QGANs. In particular, we will talk about the different
    types of QGANs out there and their (possible) advantages. You will also learn
    how to work with them using PennyLane (with its TensorFlow interface) and Qiskit.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解什么是经典和量子生成对抗网络（GANs），它们有什么用途，以及如何使用它们。我们将从基础知识开始，探讨导致GAN概念直观想法的原理。然后，我们将深入一些细节，并讨论量子生成对抗网络（QGANs）。特别是，我们将讨论现有的不同类型的QGANs及其（可能的）优势。你还将学习如何使用PennyLane（及其TensorFlow接口）和Qiskit与它们一起工作。
- en: 'We’ll cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: GANs and their quantum counterparts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs及其量子对应物
- en: Quantum GANs in PennyLane
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLane中的量子GANs
- en: Quantum GANs in Qiskit
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskit中的量子GANs
- en: Excited about this last chapter? Let’s begin by understanding what these GANs
    are all about.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对最后一章感到兴奋吗？让我们先了解这些GANs究竟是什么。
- en: 12.1 GANs and their quantum counterparts
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.1 GANs及其量子对应物
- en: Quantum GANs are **generative models** that can be trained in a perfectly unsupervised
    manner. By the fact that they are generative models we mean that quantum GANs
    will be useful for generating data that can mimic a training dataset; for instance,
    if you had a large dataset with pictures of people, a good generative model would
    be able to generate new pictures of people that would be indiscernible from those
    coming from the original distribution. The fact that QGANs can be trained in an
    unsupervised fashion simply means that our datasets will not have to be labeled;
    we won’t have to tell the generator whether its output is good or bad, the model
    will figure that out on its own. How exactly? Stay tuned!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 量子GANs是**生成模型**，可以以完全无监督的方式训练。我们所说的生成模型是指量子GANs将能够生成可以模仿训练数据集的数据；例如，如果你有一个包含大量人物图片的大数据集，一个好的生成模型将能够生成新的人物图片，这些图片与原始分布中的图片难以区分。QGANs可以以无监督方式训练的事实仅仅意味着我们的数据集将不需要标记；我们不需要告诉生成器其输出是好是坏，模型将自行解决这个问题。具体如何？请耐心等待！
- en: That’s the big picture of GANs, but, before we can explore all their details,
    there’s something we need to talk about. Let’s talk about how to counterfeit money.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是GANs的大致情况，但在我们探索所有细节之前，我们需要讨论一些事情。让我们谈谈如何伪造货币。
- en: 12.1.1 A seemingly unrelated story about money
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1.1 关于钱的一个看似无关的故事
- en: 'Of course, all of us reading these lines are law-abiding citizens — no need
    to call the police just now — but, for the purposes of intellectual illustration,
    let’s put ourselves in the place of the bad guys for one day. In the process of
    counterfeiting money, there are two main actors involved:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，所有阅读这些文字的人都是守法公民——现在不需要报警——但为了知识上的说明，让我们假设我们自己是坏蛋一天。在伪造货币的过程中，涉及两个主要角色：
- en: We, the bad guys who create (generate) counterfeit money, trying to make it
    as close to the real thing as possible
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们，那些试图制作尽可能接近真币的假币的坏蛋
- en: Some authority, usually a central bank, which is in charge of designing tools
    and techniques to discern real money from counterfeit money
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些权威机构，通常是中央银行，负责设计工具和技术来辨别真币和假币
- en: This is shown in *Figure* [*12.1*](#Figure12.1). By the way, we have drawn the
    fake dollar ourselves. Graphic design is our passion.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这在*图* [*12.1*](#Figure12.1) 中有所展示。顺便说一句，我们自己绘制了假美元。图形设计是我们的热情所在。
- en: '![Figure 12.1: Schematic representation of the agents involved in the generation
    of counterfeit money](img/file1438.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1：参与伪造货币生成的代理的示意图](img/file1438.png)'
- en: '**Figure 12.1**: Schematic representation of the agents involved in the generation
    of counterfeit money'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.1**：参与伪造货币生成的代理的示意图'
- en: 'Now that we are all set, we can imagine what our counterfeiting career could
    look like. Since none of us has any experience in this field, our first attempts
    at faking banknotes would be extremely disastrous: any of our generated banknotes
    would be very easily identified as fake by the central bank. However, that would
    just be the beginning of the story. Along the process — and assuming we didn’t
    get arrested — we could always try to study how the central bank is discerning
    real notes from counterfeit ones and use it to our advantage by trying to fool
    its detection mechanisms. Naturally, however, that would only be a temporary solution,
    for it wouldn’t take long for the central bank to notice our improved fake notes
    and design better detection systems, which would take us back to the drawing board,
    starting the process all over again.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了，我们可以想象我们的伪造生涯可能是什么样子。由于我们中没有人有这方面的经验，我们伪造纸币的第一次尝试可能会非常灾难性：我们生成的任何纸币都很容易被央行识别为假币。然而，这仅仅是故事的开头。在这个过程中——假设我们没有被捕——我们总是可以尝试研究央行如何区分真币和假币，并利用它来欺骗其检测机制。然而，自然地，这只是一个临时解决方案，因为央行很快就会注意到我们改进的假币并设计出更好的检测系统，这将使我们回到起点，重新开始这个过程。
- en: Banknotes have a finite amount of defining features, so, after a large enough
    number of iterations of this process, at some point, we would likely end up producing
    banknotes that would be identical to the real ones. And, thus, a beautiful equilibrium
    would be reached in which the central bank would no longer be able to detect our
    fake notes. Sadly for us, this adventure would most surely end with the central
    bank changing the notes completely and sending us before a judge. But let’s ignore
    those tiny details!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 纸币有有限数量的定义特征，因此，经过足够多的迭代后，在某个时刻，我们可能会最终生产出与真实纸币完全相同的纸币。因此，将达到一个美丽的平衡点，其中央行将无法再检测到我们的假币。遗憾的是，对我们来说，这次冒险很可能会以央行完全更换纸币并将我们送交法官为结局。但让我们忽略那些小细节！
- en: Important note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Just in case it wasn’t obvious, we are joking when we talk about imagining ourselves
    doing counterfeiting. Counterfeiting money is, as you hopefully know, a serious
    criminal offense that we, of course, don’t encourage or endorse in any way. Please,
    don’t do illegal stuff. The editorial team thought — with good reason! — that
    this was worth a disclaimer; so here it is!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就算不明显，我们谈论想象自己从事伪造时是在开玩笑。伪造货币，正如你希望知道的，是一种严重的刑事犯罪，我们当然不会以任何方式鼓励或支持这种行为。请，不要做违法的事情。编辑团队认为——有充分的理由！——这值得一个免责声明；所以，这就是它！
- en: Now, you may wonder why we have discussed this. Well, because, as it turns out,
    the process of training a GAN is just like that of counterfeiting money — minus
    the risk of ending up in prison. Let’s see how it works!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道我们为什么讨论这个。好吧，因为，正如事实所证明的，训练GAN的过程就像伪造货币一样——只是没有入狱的风险。让我们看看它是如何工作的！
- en: 12.1.2 What actually is a GAN?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1.2 什么是GAN？
- en: 'GANs were introduced in 2014 in a very influential paper [[46](ch030.xhtml#Xgoodfellow2014generative)]
    by Goodfellow et al. As we mentioned in the introduction, a GAN is a machine learning
    model that can be trained to generate data closely reassembling the patterns and
    properties of a given dataset. In order to accomplish this, a GAN has two main
    components:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是在2014年由Goodfellow等人发表的一篇非常有影响力的论文[[46](ch030.xhtml#Xgoodfellow2014generative)]中引入的。正如我们在引言中提到的，GAN是一种机器学习模型，可以训练生成与给定数据集的模式和属性非常相似的数据。为了实现这一点，GAN有两个主要组件：
- en: A ”generative” neural network (generator), which will be nothing more than a
    neural network taking arbitrary seeds as input and returning outputs that match
    the datatype of the elements in the original dataset. The goal of this neural
    network will be, by the end of the training, to generate new data that be indistinguishable
    from the data in the original dataset.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个“生成”型神经网络（生成器），它将只是一个以任意种子作为输入并返回与原始数据集中元素数据类型匹配的输出的神经网络。这个神经网络的目的是，在训练结束时，生成新的数据，这些数据与原始数据集中的数据无法区分。
- en: A discriminator neural network, which will be a binary-classifier neural network
    taking as input the original data in the dataset and the output of the generative
    network. This discriminator network will be tasked with trying to discern the
    generated data from the original data.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个判别器神经网络，它将是一个二元分类神经网络，输入为数据集中的原始数据和生成网络的输出。这个判别器网络将负责尝试区分生成数据和原始数据。
- en: These components are depicted in *Figure* [*12.2*](#Figure12.2). By the way,
    we have drawn the fake tree ourselves. Graphic design is our passion.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件在*图* [*12.2*](#Figure12.2) 中进行了展示。顺便说一下，我们自己是画了这些假树。图形设计是我们的激情。
- en: '![Figure 12.2: Schematic representation of the agents involved in a generative
    adversarial network](img/file1439.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2：生成对抗网络中涉及的代理的示意图](img/file1439.png)'
- en: '**Figure 12.2**: Schematic representation of the agents involved in a generative
    adversarial network'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.2**：生成对抗网络中涉及的代理的示意图'
- en: To learn more…
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多……
- en: GANs have been very successfully used in practical generative tasks. For instance,
    StyleGANs are GANs introduced by NVIDIA researchers [[57](ch030.xhtml#Xkarras2019style)]
    that are able to generate extremely realistic human faces. Their code is open
    source (you can find it at [https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan))
    and they power the mesmerizing website ”This Person Does Not Exist” ([https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: GANs已经在实际的生成任务中取得了非常成功的应用。例如，NVIDIA研究人员引入的StyleGANs是一种GAN，能够生成极其逼真的人类面孔。他们的代码是开源的（你可以在[https://github.com/NVlabs/stylegan](https://github.com/NVlabs/stylegan)找到），并且为迷人的网站”This
    Person Does Not Exist”([https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/))提供动力。
- en: 'This description settles the question of what a GAN is, but now we need to
    understand how these GANs are actually trained. In essence, this is how the whole
    training process works:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个描述解决了GAN是什么的问题，但现在我们需要了解这些GAN是如何实际训练的。本质上，整个训练过程是这样的：
- en: You initialize the generator and the discriminator to some random configuration.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将生成器和判别器初始化为某种随机配置。
- en: You train the discriminator to discern the real data from the output of the
    generator. At this initial stage, this should be a very easy task for the discriminator.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你训练判别器来区分生成器的输出和真实数据。在这个初始阶段，这对判别器来说应该是一个非常简单的任务。
- en: 'You then train the generator to fool the discriminator: you train it in a way
    that the discriminator — as trained in the previous step — will classify as many
    of the generated outputs as real. Once trained, you use it to generate a bunch
    of fake data.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你训练生成器来欺骗判别器：你以这种方式训练它，使得判别器——在之前步骤中训练的——将尽可能多的生成输出分类为真实。一旦训练完成，你就用它来生成一大堆假数据。
- en: And here is where the fun begins. You re-train the discriminator on the new
    generated dataset, and then you re-train the generator to fool the new discriminator.
    And you repeat this process in as many iterations as you want. Ideally, in each
    iteration, it will be harder for the discriminator to tell the generated data
    from the real data. And, eventually, an equilibrium will be reached in which the
    generated data will be indiscernible from the original data. Just like in our
    previous counterfeiting adventure — and with no legal troubles on the horizon!
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 而这里就是乐趣开始的地方。你使用新的生成数据集重新训练判别器，然后重新训练生成器来欺骗新的判别器。你可以重复这个过程，直到你想要的次数。理想情况下，在每次迭代中，判别器将更难区分生成数据和真实数据。最终，将达到一个平衡点，其中生成数据将无法与原始数据区分开来。就像我们之前的伪造冒险一样——而且没有法律问题在眼前！
- en: This process is exemplified schematically in *Figure* [*12.3*](#Figure12.3),
    where we present a schematic illustration of the training process of a GAN meant
    to generate pictures of cute cats. When the GAN is initialized, the generator
    just produces random noise. After subsequent training iterations, the output of
    the generator will more closely resemble the images in the original dataset —
    which, in this example, should be a dataset with pictures of cats. By the way,
    we have drawn the fake cats ourselves. Have we mentioned that graphic design is
    our passion?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在*图* [*12.3*](#Figure12.3) 中以示意图的形式进行了展示，其中我们展示了旨在生成可爱猫咪图片的生成对抗网络（GAN）的训练过程。当GAN初始化时，生成器仅产生随机噪声。经过后续的训练迭代后，生成器的输出将更接近原始数据集中的图像——在这个例子中，应该是一个包含猫咪图片的数据集。顺便说一下，我们自己是画了这些假猫。我们提到过图形设计是我们的激情吗？
- en: We should highlight that this scheme is very oversimplified. In truth, you usually
    don’t ”fully” train the discriminator and the generator alternately, but you optimize
    them in an alternate fashion. For example, if you were using gradient descent
    with a given batch size, then, on each epoch and on each batch, you would optimize
    the weights of the discriminator in a single optimizer step, and then you would
    do the same for the weights of the generator.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该强调，这个方案非常简化。实际上，你通常不会“完全”交替训练判别器和生成器，而是交替优化它们。例如，如果你使用给定批量大小的梯度下降法，那么在每个epoch和每个batch中，你会在单个优化器步骤中优化判别器的权重，然后你会对生成器的权重做同样的操作。
- en: '![Figure 12.3: Schematic illustration of the training process of a GAN meant
    to generate pictures of cute cats ](img/file1440.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3：旨在生成可爱猫咪图片的GAN训练过程的示意图](img/file1440.png)'
- en: '**Figure 12.3**: Schematic illustration of the training process of a GAN meant
    to generate pictures of cute cats'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.3**：旨在生成可爱猫咪图片的GAN训练过程的示意图'
- en: With this description of the training process, we can now make sense of the
    term GAN. These models are ”generative” because they are aimed at generating data.
    They are ”networks” because, well, they use neural networks. And they are ”adversarial”
    because the whole training process consists in a competition between a generator
    network and a discriminator network. These networks engage in a fierce competition
    in which we, their programmers and creators, shall be the only true winners.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对训练过程的描述，我们现在可以理解GAN这个术语。这些模型是“生成”的，因为它们旨在生成数据。它们是“网络”，因为，嗯，它们使用了神经网络。而且它们是“对抗”的，因为整个训练过程都包含生成器网络和判别器网络之间的竞争。这些网络在激烈的竞争中展开，而我们，它们的程序员和创造者，将是唯一的真正赢家。
- en: To learn more…
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: All this time, we have been talking about how GANs use neural networks in both
    the discriminator and the generator. However, these neural networks are not always
    like the ones we have discussed in this book.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些时间，我们一直在谈论GAN如何在判别器和生成器中使用神经网络。然而，这些神经网络并不总是像我们在本书中讨论的那样。
- en: 'The neural networks that we have studied are known as ”dense” neural networks.
    In these networks, all the layers are dense, which means that neurons in subsequent
    layers are fully connected. However, when neural networks are designed to handle
    images — whether it be generating them, classifying them, or manipulating them
    — a different kind of layer is often employed: convolutional layers. We won’t
    get into the details of how these layers work (check Chapter 14 in Gerón’s book
    [[104](ch030.xhtml#Xhandsonml)] for a thorough explanation), but you should at
    least know that they exist.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所研究的神经网络被称为“密集”神经网络。在这些网络中，所有层都是密集的，这意味着后续层的神经元是完全连接的。然而，当神经网络被设计来处理图像——无论是生成、分类还是操作图像时——通常会使用不同类型的层：卷积层。我们不会深入探讨这些层的工作原理（详细解释请参考Gerón的书中第14章
    [[104](ch030.xhtml#Xhandsonml)]），但你至少应该知道它们的存在。
- en: GANs are often used in image generation tasks, so, should you ever decide to
    study classical GANs, be aware that you will surely have to deal with these layers
    at some point. And, yes, there are quantum versions of convolutional layers and
    convolutional networks [[114](ch030.xhtml#Xquantum-conv), [52](ch030.xhtml#Xhavlivcek2019supervised)]
    that, sadly, we do not have the time to cover in this book.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GAN通常用于图像生成任务，所以，如果你决定研究经典GAN，请注意你肯定会在某个时候处理这些层。是的，还有卷积层和卷积网络的量子版本 [[114](ch030.xhtml#Xquantum-conv),
    [52](ch030.xhtml#Xhavlivcek2019supervised)]，遗憾的是，我们没有时间在这本书中涵盖。
- en: There are a few details that we should highlight about the training process
    of a GAN. The first and most important one is the fact that at no point in the
    training is the generator network ”exposed” to or fed the original data. The only
    way the generator network can learn about the data it has to replicate is through
    the discriminator. In this way, instead of us having to tell the generator network
    what its output should look like, the discriminator takes up our role as teachers
    and enables us to train the whole network in a fully unsupervised manner.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GAN的训练过程，有几个细节我们应该强调。首先也是最重要的一点是，在整个训练过程中，生成器网络“暴露”或提供原始数据的情况从未发生。生成器网络了解它必须复制的唯一方式是通过判别器。这样，我们不必告诉生成器网络其输出应该是什么样子，判别器就承担了我们的角色，使我们能够以完全无监督的方式训练整个网络。
- en: Another issue to which we should pay attention is that GANs, like any other
    machine learning model, are vulnerable to problems in training. For instance,
    how could we have any guarantee that the generated outputs are not just slightly
    distorted copies of the original data, rather than new data elements that match
    the patterns in the original dataset? For instance, in the cat GAN that we considered
    in *Figure* [*12.3*](#Figure12.3), how could we have guarantees that the generated
    images are new cat pictures rather than, say, blurred copies of our original images
    that have lost any resemblance to cats but that were nevertheless able to fool
    the discriminator network? This could happen, for example, if our discriminator
    weren’t powerful enough compared to the generator.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们应该注意的问题是，GANs，像任何其他机器学习模型一样，容易在训练过程中出现问题。例如，我们如何保证生成的输出不是原始数据的略微扭曲的副本，而不是与原始数据集中的模式相匹配的新数据元素？例如，在我们考虑的
    *图* [*12.3*](#Figure12.3) 中的猫 GAN，我们如何保证生成的图像是新的猫图片，而不是，比如说，我们原始图像的模糊副本，这些图像已经失去了与猫的相似性，但仍然能够欺骗判别器网络？这可能发生，例如，如果我们的判别器与生成器相比不够强大。
- en: To learn more…
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: The training of a GAN can also fail if the resulting GAN is unable to generate
    all the possible variations (or **modes**) of data that can be found in the dataset.
    For instance, in the example that we have been considering, we would find this
    problem if our GAN were only able to generate pictures of a small selection of
    cats, maybe even only one! This occurrence is known as **mode collapse**. To try
    to avoid it, several modified GANs have been proposed, including **Wasserstein
    GANs** (**WGANs**) [[7](ch030.xhtml#Xarjovsky2017wasserstein)], which derive their
    loss function from a distance called the Wasserstein metric.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成的 GAN 无法生成数据集中所有可能的变化（或**模式**），GAN 的训练也可能失败。例如，在我们考虑的例子中，如果我们的 GAN 只能生成一小部分猫的图片，甚至可能只有一张！这种情况被称为**模式坍塌**。为了尝试避免这种情况，已经提出了几种修改后的
    GAN，包括**Wasserstein GANs**（**WGANs**）[[7](ch030.xhtml#Xarjovsky2017wasserstein)]，它们从称为
    Wasserstein 距离的度量中推导出其损失函数。
- en: In the models that we considered in previous chapters, there was always a simple,
    straightforward way to effectively assess their performance — namely evaluating
    loss functions on test datasets. When working with GANs, things can be more subtle.
    In general, you should always take a look at the generated data and check if the
    results are satisfactory.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前章节中考虑的模型中，始终有一个简单直接的方法来有效地评估它们的性能——即在测试数据集上评估损失函数。当与 GAN 一起工作时，事情可能会更加微妙。一般来说，你应该始终查看生成的数据，并检查结果是否令人满意。
- en: Important note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'A GAN consists of two neural networks: a generator and a discriminator. They
    compete against each other in an iterative training process. The discriminator
    is tasked with discerning a dataset of real data from the output of the generator
    network, while the generator network is tasked with generating data that the discriminator
    will mistakenly identify as real.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 由两个神经网络组成：一个生成器和判别器。它们在迭代训练过程中相互竞争。判别器的任务是区分真实数据集和生成器网络输出的数据，而生成器网络的任务是生成判别器会错误地识别为真实的数据。
- en: Just to conclude this overview of classical GANs, let’s discuss a few technicalities
    about the training of the generator and discriminator networks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 仅为了总结一下关于经典 GANs 的概述，让我们讨论一下关于生成器和判别器网络训练的一些技术细节。
- en: 12.1.3 Some technicalities about GANs
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1.3 关于 GANs 的技术细节
- en: We have already mentioned how the generator and discriminator networks are ordinary
    neural network models — even if they may be different from the ones that we’ve
    discussed so far — that are constantly re-trained in an iterative process. We
    will now briefly talk about how this training is carried out.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到，生成器和判别器网络是普通的神经网络模型——即使它们可能与我们之前讨论的不同——它们在迭代过程中不断重新训练。现在我们将简要谈谈这种训练是如何进行的。
- en: Let ![X](img/file9.png "X") be a set of real data and let ![S](img/file73.png
    "S") be a set of ”seeds” that we give to the generator. In the case of the discriminator
    neural network, we are just training a binary classifier and, as is standard,
    this classifier will return an output bounded between ![0](img/file12.png "0")
    and ![1](img/file13.png "1"). Without loss of generality, we will assume that
    values closer to ![1](img/file13.png "1") are meant to represent inputs from the
    real dataset while values closer to ![0](img/file12.png "0") are labeled as generated
    inputs — that’s an arbitrary choice; it could perfectly be the other way around.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 设![X](img/file9.png "X")为一个真实数据集，设![S](img/file73.png "S")为我们提供给生成器的“种子”集合。在判别器神经网络的情况下，我们只是在训练一个二元分类器，并且按照惯例，这个分类器将返回一个介于![0](img/file12.png
    "0")和![1](img/file13.png "1")之间的输出。不失一般性，我们将假设接近![1](img/file13.png "1")的值表示来自真实数据集的输入，而接近![0](img/file12.png
    "0")的值被标记为生成的输入——这是一个任意的选择；它完全可以相反。
- en: 'As with any other binary classifier, the most natural loss function to use
    will be the binary cross-entropy loss, and hence this classifier will be trained
    as it would in supervised learning: assigning the ”true label” ![1](img/file13.png
    "1") to any input from the real dataset and the ”true label” ![0](img/file12.png
    "0") to any generated input. In this way, if we let ![G](img/file1441.png "G")
    and ![D](img/file1101.png "D") denote the actions of the generator and the discriminator,
    the discriminator training loss, ![L_{D}](img/file1442.png "L_{D}"), would be
    computed as'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他二元分类器一样，最自然的损失函数将是二元交叉熵损失，因此这个分类器将像监督学习一样进行训练：将“真实标签”![1](img/file13.png
    "1")分配给来自真实数据集的任何输入，将“真实标签”![0](img/file12.png "0")分配给任何生成的输入。这样，如果我们让![G](img/file1441.png
    "G")和![D](img/file1101.png "D")表示生成器和判别器的动作，判别器训练损失![L_{D}](img/file1442.png "L_{D}")将被计算为
- en: '| ![L_{D} = - \frac{1}{&#124;X&#124; + &#124;S&#124;}\left( {\sum\limits_{x
    \in X}\log D(x) + \sum\limits_{s \in S}\log\left( {1 - D(G(s)} \right)} \right),](img/file1443.png
    "L_{D} = - \frac{1}{&#124;X&#124; + &#124;S&#124;}\left( {\sum\limits_{x \in X}\log
    D(x) + \sum\limits_{s \in S}\log\left( {1 - D(G(s)} \right)} \right),") |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ![L_{D} = - \frac{1}{|X| + |S|}\left( {\sum\limits_{x \in X}\log D(x) + \sum\limits_{s
    \in S}\log\left( {1 - D(G(s)} \right)} \right),](img/file1443.png "L_{D} = - \frac{1}{|X|
    + |S|}\left( {\sum\limits_{x \in X}\log D(x) + \sum\limits_{s \in S}\log\left(
    {1 - D(G(s)} \right)} \right),") |'
- en: where we are using ![|X|](img/file1444.png "|X|") and ![|S|](img/file1445.png
    "|S|") to denote the sizes of the sets ![X](img/file9.png "X") and ![S](img/file73.png
    "S"), respectively. The job of the discriminator would be to minimize this loss.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用![|X|](img/file1444.png "|X|")和![|S|](img/file1445.png "|S|")分别表示集合![X](img/file9.png
    "X")和![S](img/file73.png "S")的大小。判别器的工作是使这个损失最小化。
- en: Now, what about the generator network? What could be a good choice for the loss
    function that we would like to minimize in its training process? Our goal when
    training the generator is to fool the discriminator trying to get it to classify
    our generated data as real data. Hence, the goal in the training of the generator
    is to maximize the loss function of the discriminator, that is, to minimize
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于生成器网络呢？我们希望在它的训练过程中最小化的损失函数应该是什么？当我们训练生成器时，我们的目标是欺骗判别器，让它将我们的生成数据分类为真实数据。因此，生成器训练中的目标是最大化判别器的损失函数，即最小化
- en: '| ![- L_{D} = \frac{1}{&#124;X&#124; + &#124;S&#124;}\left( {\sum\limits_{x
    \in X}\log D(x) + \sum\limits_{s \in S}\log\left( {1 - D(G(s)} \right)} \right).](img/file1446.png
    "- L_{D} = \frac{1}{&#124;X&#124; + &#124;S&#124;}\left( {\sum\limits_{x \in X}\log
    D(x) + \sum\limits_{s \in S}\log\left( {1 - D(G(s)} \right)} \right).") |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![- L_{D} = \frac{1}{|X| + |S|}\left( {\sum\limits_{x \in X}\log D(x) + \sum\limits_{s
    \in S}\log\left( {1 - D(G(s)} \right)} \right).](img/file1446.png "- L_{D} = \frac{1}{|X|
    + |S|}\left( {\sum\limits_{x \in X}\log D(x) + \sum\limits_{s \in S}\log\left(
    {1 - D(G(s)} \right)} \right).") |'
- en: Nevertheless, the contribution of the first term in the sum is necessarily constant
    in the generator training since it does not depend on the generator in any way.
    Thus, equivalently, we may consider the goal of the generator training to be the
    minimization of the generator loss function
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于第一个项在生成器训练中必然是常数，因为它以任何方式都不依赖于生成器。因此，等价地，我们可以考虑生成器训练的目标是最小化生成器损失函数
- en: '| ![L_{G}^{\prime} = \frac{1}{&#124;S&#124;}\sum\limits_{s \in S}\log\left(
    {1 - D(G(s)} \right).](img/file1447.png "L_{G}^{\prime} = \frac{1}{&#124;S&#124;}\sum\limits_{s
    \in S}\log\left( {1 - D(G(s)} \right).") |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![L_{G}^{\prime} = \frac{1}{|S|}\sum\limits_{s \in S}\log\left( {1 - D(G(s))}
    \right).](img/file1447.png "L_{G}^{\prime} = \frac{1}{|S|}\sum\limits_{s \in S}\log\left(
    {1 - D(G(s))} \right).") |'
- en: That is how things are in theory. However, in practice, it has been shown [[46](ch030.xhtml#Xgoodfellow2014generative)]
    that it is usually more stable to take the goal of the generator training to be
    the minimization of the loss
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上事情就是这样。然而，在实践中，已经证明 [[46](ch030.xhtml#Xgoodfellow2014generative)]，通常将生成器训练的目标设定为损失最小化会更稳定。
- en: '| ![L_{G} = - \frac{1}{&#124;S&#124;}\sum\limits_{s \in S}\log\left( {D(G(s)}
    \right).](img/file1448.png "L_{G} = - \frac{1}{&#124;S&#124;}\sum\limits_{s \in
    S}\log\left( {D(G(s)} \right).") |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![L_{G} = - \frac{1}{|S|}\sum\limits_{s \in S}\log\left( {D(G(s))} \right).](img/file1448.png
    "L_{G} = - \frac{1}{|S|}\sum\limits_{s \in S}\log\left( {D(G(s))} \right).") |'
- en: The crucial thing here is that, with both definitions, if these generator loss
    functions decrease while training the generator, it will be more likely for our
    generated data to be (mistakenly) classified as real data by our classifier. That
    will mean, in turn, that our data should be gradually getting more and more similar
    to the data in the original dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的关键是，使用这两种定义，如果在训练生成器时这些生成器损失函数下降，那么我们的生成数据被我们的分类器（错误地）分类为真实数据的可能性会更大。这反过来意味着，我们的数据应该逐渐变得更加类似于原始数据集中的数据。
- en: It has also been shown that, in the optimal equilibrium between the generator
    and the discriminator, the discriminator assigns values ![D(x)](img/file1449.png
    "D(x)") and ![G(D(s))](img/file1450.png "G(D(s))") equal to ![\left. 1\slash 2
    \right.](img/file136.png "\left. 1\slash 2 \right.") (because it cannot distinguish
    between real and generated data), and, hence, when ![\left. L_{D} = L_{G} = -
    \log 1\slash 2 = \log 2 \approx 0.6931 \right.](img/file1451.png "\left. L_{D}
    = L_{G} = - \log 1\slash 2 = \log 2 \approx 0.6931 \right."). You can find the
    proof (with slightly different but equivalent loss functions) in the original
    GANs paper [[46](ch030.xhtml#Xgoodfellow2014generative)].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 还已经证明，在生成器和判别器之间的最佳均衡状态下，判别器分配的值 ![D(x)](img/file1449.png "D(x)") 和 ![G(D(s))](img/file1450.png
    "G(D(s))") 等于 ![\left. \frac{1}{2} \right.](img/file136.png "\left. \frac{1}{2}
    \right.")（因为它无法区分真实和生成数据），因此，当 ![\left. L_{D} = L_{G} = - \log \frac{1}{2} = \log
    2 \approx 0.6931 \right.](img/file1451.png "\left. L_{D} = L_{G} = - \log \frac{1}{2}
    = \log 2 \approx 0.6931 \right.")。你可以在原始 GANs 论文中找到证明（使用略有不同但等效的损失函数）[[46](ch030.xhtml#Xgoodfellow2014generative)]。
- en: To learn more…
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: It can be shown that the optimal configuration of a GAN is a Nash equilibrium
    of an adversarial game between the generator and the discriminator (see, for instance,
    the helpful tutorial given by Goodfellow at NIPS [[47](ch030.xhtml#Xgoodfellow2016nips)]).
    In this equilibrium, the configuration of the GAN is a (local) minimizer of both
    the generator and discriminator losses.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，GAN 的最佳配置是生成器和判别器之间对抗游戏的纳什均衡（例如，参见 Goodfellow 在 NIPS 上提供的有用教程 [[47](ch030.xhtml#Xgoodfellow2016nips)]）。在这个均衡中，GAN
    的配置是生成器和判别器损失的（局部）最小化者。
- en: That should be enough of an introduction to classical GANs. Let’s now see what
    quantum GANs are and what they have to offer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是对经典 GAN 的足够介绍。现在让我们看看量子 GAN 是什么，以及它们能提供什么。
- en: 12.1.4 Quantum GANs
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1.4 量子 GAN
- en: What is a quantum GAN? It’s just a GAN, with its competing discriminator and
    generator, where a part of the model is implemented by a quantum model (usually
    some form of a quantum neural network), and it is trained just like a classical
    GAN. In other words, training a quantum GAN is just like counterfeiting money
    — but you don’t risk going to prison and you get to play with quantum stuff.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是量子 GAN？它只是一个 GAN，具有其竞争的判别器和生成器，其中模型的一部分由量子模型（通常是某种形式的量子神经网络）实现，并且它就像经典 GAN
    一样进行训练。换句话说，训练量子 GAN 就像伪造货币——但你不会冒坐牢的风险，而且你可以玩量子东西。
- en: To learn more…
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: By the way, did you know that there are proposals of quantum money that cannot
    be counterfeited at all? The original idea was proposed by Stephen Wiesner [[97](ch030.xhtml#Xwiesner1983conjugate)]
    and it became the inspiration for unbreakable quantum cryptographical protocols
    such as the famous BB84 proposed by Bennett and Brassard [[13](ch030.xhtml#Xbennett84quantum)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，你知道有无法伪造的量子货币的提案吗？最初的想法是由斯蒂芬·威斯纳[[97](ch030.xhtml#Xwiesner1983conjugate)]提出的，并成为著名的BB84量子密码协议的灵感，该协议由本内特和布拉萨德[[13](ch030.xhtml#Xbennett84quantum)]提出。
- en: In truth, that is as close to a precise definition as we can get, because the
    range of models that can fit into the category of QGAN is vast. Depending on the
    kind of problem that you want to tackle, you may want to use quantum GANs with
    completely different architectures which, still, will share the same core elements
    of a competing discriminator and generator. The examples that we will consider
    in the following sections will help us exemplify this.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这已经是我们能得到的接近精确定义的程度，因为可以适应QGAN类别模型范围非常广泛。根据你想要解决的问题类型，你可能希望使用具有完全不同架构的量子GANs，这些架构仍然会共享竞争判别器和生成器的相同核心元素。以下章节中的例子将帮助我们说明这一点。
- en: 'Broadly speaking, any quantum GAN could fit into one of the following categories:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，任何量子GAN都可以适应以下类别之一：
- en: '**Uses quantum data and both the generator and the** **discriminator are quantum:**
    This quantum data will just be some quantum states, and the generator and discriminator
    will be implemented by quantum circuits.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用量子数据和生成器以及** **判别器都是量子**：这种量子数据将只是某些量子状态，生成器和判别器将通过量子电路实现。'
- en: This situation allows for a very special QGAN architecture, with a fully quantum
    model. Since we are dealing with quantum data (states), and all the components
    of the GAN are quantum circuits, they can be perfectly joined together without
    having to resort to feature maps or measurement operations in the middle of the
    model.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种情况允许一个非常特殊的QGAN架构，具有完全量子模型。由于我们处理的是量子数据（状态），并且GAN的所有组件都是量子电路，它们可以完美地连接在一起，而无需在模型中间使用特征图或测量操作。
- en: Later in the chapter, we will study an example of this purely quantum architecture
    on PennyLane.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将研究PennyLane上纯量子架构的例子。
- en: '**Uses quantum data and a quantum generator with a classical** **discriminator:**
    If the discriminator is classical, the architecture of our QGANs will be more
    similar to that of classical GANs. The generator will produce quantum states but,
    ultimately, they will be transformed into classical data by some measurement operation
    in order to feed them into the classifier. Of course, the original quantum data
    will also have to be measured.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用量子数据和经典** **判别器**：如果判别器是经典的，我们的QGANs架构将与经典GANs的架构更加相似。生成器将产生量子状态，但最终，它们将通过某种测量操作转换成经典数据，以便输入到分类器中。当然，原始的量子数据也必须进行测量。'
- en: '**Uses classical data with a quantum generator** **or discriminator:** This
    is the scenario in which QGANs can best match their classical counterparts. The
    use of QGANs in these cases essentially mounts up to replacing the generator or
    the discriminator (or both) with a quantum model with classical inputs and outputs.
    In the case of a quantum discriminator, for example, we would have to use a feature
    map to load classical data into a quantum state.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用经典数据和量子** **生成器或判别器**：这是QGANs可以最好地匹配其经典对应物的场景。在这些情况下使用QGANs本质上相当于用具有经典输入和输出的量子模型替换生成器或判别器（或两者）。例如，在量子判别器的情况下，我们不得不使用特征图将经典数据加载到量子状态中。'
- en: Because the availability of classical data is much bigger than that of quantum
    data, this is the type of architecture that has been studied more widely by the
    quantum computing community.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于经典数据的可用性远大于量子数据，这种架构类型已经被量子计算社区更广泛地研究。
- en: Later in the chapter, we will consider a QGAN with classical data and a classical
    classifier, but with a quantum generator. That will be in our Qiskit section.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将考虑一个具有经典数据和经典分类器，但具有量子生成器的QGAN。那将在我们的Qiskit部分。
- en: To learn more…
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: In the literature, there are many different proposals of quantum versions of
    GANs. Some of the earliest ones include works by Lloyd and Weedbrook [[64](ch030.xhtml#Xlloyd2018quantum)],
    by Dallaire-Demers and Killoran [[28](ch030.xhtml#Xdallaire2018quantum)], and
    by Zoufal, Lucchi, and Woerner [[101](ch030.xhtml#Xzoufal2019quantum)].
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，有许多关于GAN量子版本的不同的提议。其中一些最早的包括Lloyd和Weedbrook的工作 [[64](ch030.xhtml#Xlloyd2018quantum)]，Dallaire-Demers和Killoran的工作
    [[28](ch030.xhtml#Xdallaire2018quantum)]，以及Zoufal、Lucchi和Woerner的工作 [[101](ch030.xhtml#Xzoufal2019quantum)]。
- en: Exercise 12.1
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 练习12.1
- en: 'In this book, we have discussed four different QML models: quantum support
    vector machines, quantum neural networks, hybrid QNNs and quantum GANs. Decide
    which of these models would be suitable for the following tasks:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们讨论了四种不同的QML模型：量子支持向量机、量子神经网络、混合QNN和量子GAN。决定以下任务中哪个模型是合适的：
- en: Distinguishing cat pictures from dog pictures.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区分猫的图片和狗的图片。
- en: Generating pictures of dogs.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成狗的图片。
- en: Deciding whether a financial transaction is fraudulent based on its metadata.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据其元数据判断一笔金融交易是否欺诈。
- en: Assessing the risk of heart failure from a patients’ medical records and data
    from an electrocardiogram.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估患者病历和心电图数据中心脏衰竭的风险。
- en: Creating a dataset of random images of electrocardiograms in order to train
    future doctors.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建随机心电图图像数据集，以训练未来的医生。
- en: We should give you a word of caution. GANs aren’t the easiest models to train.
    As we mentioned previously, when you train a GAN, you don’t have a single and
    straightforward loss function that can measure how successful your training is.
    Training a GAN is not a simple optimization problem, but a more intricate process.
    Using quantum models, of course, only makes matters more difficult, and training
    quantum GANs can be…complicated.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该给你一个警告。GAN不是最容易训练的模型。正如我们之前提到的，当你训练一个GAN时，你没有一个单一且直接的损失函数来衡量你的训练是否成功。训练GAN不是一个简单的优化问题，而是一个更复杂的过程。当然，使用量子模型只会使问题更加困难，训练量子GAN可能会很…复杂。
- en: 'We will now consider a couple of interesting QGAN examples, in both PennyLane
    and Qiskit. Naturally, since we’ve picked them, our quantum GANs will learn smoothly.
    But you have been warned: quantum GANs are usually wild creatures.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将考虑几个有趣的QGAN示例，包括PennyLane和Qiskit。自然地，因为我们选择了它们，所以我们的量子GAN将顺利学习。但是，你已经被告知了：量子GAN通常是野性的生物。
- en: 12.2 Quantum GANs in PennyLane
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.2 PennyLane中的量子GAN
- en: In this section, we are going to train a purely quantum GAN that will learn
    a one-qubit state. In our previous counterfeiting example, we imagined ourselves
    as behaving like a GAN in order to replicate some training data (a banknote) to
    produce fake banknotes that, ideally, would get closer and closer to the real
    thing in each iteration. In this case, our training data will be a one-qubit state,
    characterized by some amplitudes, and the job of our QGAN will be to replicate
    that state without the generator having direct access to it. Our dataset, then,
    will consist of multiple copies of a one-qubit state, and our goal will be to
    train a generator able to prepare that state (or something very close to it).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练一个纯量子GAN，它将学习一个单量子比特状态。在我们之前的伪造示例中，我们想象自己像GAN一样行为，以复制一些训练数据（纸币）来生产假钞，理想情况下，每次迭代都会越来越接近真实物品。在这种情况下，我们的训练数据将是一个单量子比特状态，由一些振幅特征，我们的QGAN的任务将是复制该状态，而生成器没有直接访问它。因此，我们的数据集将包括多个单量子比特状态的副本，我们的目标将是训练一个能够准备该状态（或非常接近该状态）的生成器。
- en: To learn more…
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: Notice that this setting does not violate the no-cloning theorem that we proved
    in *Section* *[*1.4.5*](ch008.xhtml#x1-320001.4.5). We will have multiple copies
    of the same quantum state and we will perform operations on them, including measuring
    them (and, hence, collapsing their states). From that, we will learn some properties
    of the state that we will use to reproduce it with the generator. But we won’t
    be having a unitary operation (a quantum gate) that creates an additional, independent
    copy of a given state. In fact, we will destroy the original copies in the process!*
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种设置并没有违反我们在*第1.4.5节* [(*1.4.5*](ch008.xhtml#x1-320001.4.5)] 中证明的不克隆定理。我们将有多个相同的量子状态的副本，并且将对它们执行操作，包括测量它们（因此，它们的态会坍缩）。从这些操作中，我们将学习一些关于状态的属性，我们将使用这些属性来使用生成器重现它。但我们将不会有一个单位操作（量子门）来创建给定状态的额外、独立的副本。事实上，我们在过程中会销毁原始副本！*
- en: '*What we will be doing here is more similar to **quantum state** **tomography**
    (see, for instance, the review by Altepeter, James, and Kwiat [[6](ch030.xhtml#Xaltepeter2005photonic)]),
    which can be defined as the process of applying quantum operations and measurements
    to multiple copies of a state and, from the results, learning to reconstruct the
    original state.*  *For this example, we will use the PyTorch machine learning
    package. Please, have a look at *Subsection* [*11.3.1*](ch020.xhtml#x1-20500011.3.1)
    if you haven’t already.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们在这里要做的事情更类似于**量子态****全息术**（例如，参见Altepeter、James和Kwiat的综述[[6](ch030.xhtml#Xaltepeter2005photonic)]），这可以被定义为对状态的多份副本应用量子操作和测量，并从结果中学习重建原始状态。*
    *对于这个例子，我们将使用PyTorch机器学习包。如果您还没有看过，请参阅*子节*[*11.3.1*](ch020.xhtml#x1-20500011.3.1)。'
- en: The reason behind our choice to use PyTorch is simple. As much as we have used
    TensorFlow so far, we only know how to use it at a basic level, relying heavily
    on the Keras interface. On the other hand, we have studied PyTorch extensively
    in the previous chapter, which makes it a better tool for us when it comes to
    dealing with more complex architectures. In other words, this choice isn’t grounded
    in any technical superiority of any package over the other, but solely on what’s
    most practical given the content that we’ve covered in this book. In fact, virtually
    any model that can be built and trained on PyTorch can also be dealt with on TensorFlow
    and vice versa.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用PyTorch的原因很简单。尽管我们到目前为止已经使用了TensorFlow，但我们只知道如何使用它的基础级别，严重依赖于Keras界面。另一方面，我们在上一章中广泛研究了PyTorch，这使得它在我们处理更复杂的架构时成为一个更好的工具。换句话说，这个选择并不是基于任何包在技术上的优越性，而是基于我们在这本书中涵盖的内容中最实用的选择。事实上，几乎任何可以在PyTorch上构建和训练的模型也可以在TensorFlow上处理，反之亦然。
- en: With those preliminaries aside, let’s get to our model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些预备知识之外，让我们进入我们的模型。
- en: 12.2.1 Preparing a QGAN model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2.1 准备QGAN模型
- en: 'The purely quantum GAN that we seek to implement and train will run on a device
    with two qubits, and it will be made up of the following components:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求实现和训练的纯量子生成对抗网络（GAN）将在具有两个量子比特的设备上运行，并且它将由以下组件组成：
- en: A quantum circuit that will be able to prepare the one-qubit state ![\left|
    \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle") that
    we want our QGAN to learn. This circuit will run on the first qubit of the device.
    We should regard it as a black box, the inner working of which is fully opaque
    to our model.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个量子电路，它将能够准备我们想要我们的QGAN学习的单量子比特状态![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle")。这个电路将在设备的第一个量子比特上运行。我们应该将其视为一个黑盒，其内部工作对我们模型是完全透明的。
- en: 'The state ![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1}
    \right\rangle"), which we will refer to as the ”true state,” is the quantum training
    data that we will use in our QGAN. This circuit will just provide us with a way
    of accessing the training data: as many copies of the ![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle") state as we may need in the training process.
    This emulates, for instance, a physics experiment that produces some quantum state
    that we want to learn.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")，我们将称之为“真实状态”，是我们将在QGAN中使用的量子训练数据。这个电路将仅仅为我们提供访问训练数据的方法：在训练过程中可能需要的任何数量的![\left|
    \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")状态副本。这模拟了例如一个产生我们想要学习的某些量子状态的物理实验。
- en: A quantum generator, which will also run on the first qubit of the device and
    which aims to prepare a state similar to ![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle") on the first qubit. The quantum generator will
    be implemented by a variational form dependent on some trainable parameters.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个量子生成器，它也将运行在设备的第一个量子比特上，并且旨在在第一个量子比特上准备一个类似于![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle")的状态。量子生成器将通过依赖于一些可训练参数的变分形式来实现。
- en: A quantum discriminator, which will run on the first and second qubits of the
    device. Its ”input” will be the state on the first qubit, which can either be
    the state we want our QGAN to learn or the state prepared by the generator. Of
    course, the job of the discriminator will be to try to distinguish these two states.
    We implement it with two qubits (instead of just one) to be sure that it has enough
    discriminative power.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个量子判别器，它将在设备的第一个和第二个量子比特上运行。它的“输入”将是第一个量子比特上的状态，这可以是我们要我们的QGAN学习的状态或生成器准备的状态。当然，判别器的任务将是尝试区分这两种状态。我们用两个量子比特（而不是一个）来实现它，以确保它有足够的判别能力。
- en: Since this discriminator already takes a quantum input, it only needs to consist
    of a variational form followed by a measurement operation — there will be no need
    to use feature maps, as we had to do when working with classical data. As usual,
    we will place the measurement operation on the first qubit.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这个判别器已经接受量子输入，它只需要由一个变分形式后跟一个测量操作组成——将不需要使用特征图，正如我们在处理经典数据时必须做的那样。像往常一样，我们将测量操作放在第一个量子比特上。
- en: All the components that we have just described are depicted in *Figure* [*12.4*](#Figure12.4).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚描述的所有组件都在 *图* [*12.4*](#Figure12.4) 中展示。
- en: '![(a) Circuit preparing the state  that we want our QGAN to learn.](img/file1453.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![(a) 准备我们想要我们的QGAN学习的状态的电路。](img/file1453.jpg)'
- en: '**(a)** Circuit preparing the state ![img](img/file1452.png) that we want our
    QGAN to learn.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**(a)** 准备我们想要我们的QGAN学习的状态 ![img](img/file1452.png) 的电路。'
- en: '![(b) Generator circuit that outputs a state . We aim for  to be similar to  .](img/file1455.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![(b) 生成器电路输出状态 。我们的目标是让 与 相似。](img/file1455.jpg)'
- en: '**(b)** Generator circuit that outputs a state ![img](img/file1454.png). We
    aim for ![img](img/file1454.png) to be similar to ![img](img/file1452.png) .'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**(b)** 生成器电路输出状态 ![img](img/file1454.png)。我们的目标是让 ![img](img/file1454.png)
    与 ![img](img/file1452.png) 相似。'
- en: '![(c) Discriminator circuit, tasked with deciding whether the state  is the
    state  or the output of the generator.](img/file1457.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![(c) 判别器电路，负责判断状态是否是状态 或生成器的输出。](img/file1457.jpg)'
- en: '**(c)** Discriminator circuit, tasked with deciding whether the state ![img](img/file1456.png)
    is the state ![img](img/file1452.png) or the output of the generator.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**(c)** 判别器电路，负责判断状态 ![img](img/file1456.png) 是否是状态 ![img](img/file1452.png)
    或生成器的输出。'
- en: '**Figure 12.4**: Components of the quantum GAN that we will train to generate
    ![img](img/file1452.png)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.4**：我们将训练以生成 ![img](img/file1452.png) 的量子GAN的组件'
- en: 'Now that we have a sense of where we are heading, let’s get ready to write
    some code. First of all, we will do our usual imports and set some seeds to ensure
    the reproducibility of our results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了我们的方向，让我们准备编写一些代码。首先，我们将进行我们通常的导入，并设置一些种子以确保结果的再现性：
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will construct the state ![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle") using the universal one-qubit gate ![U_{3}(\varphi,\theta,\delta)](img/file1458.png
    "U_{3}(\varphi,\theta,\delta)") since, as we learned back in *Chapter* [*1*](ch008.xhtml#x1-180001),
    *Foundations of Quantum Computing*, it allows us to create any one-qubit state.
    In particular, we will feed it the values ![\left. \varphi = \pi\slash 3 \right.](img/file1459.png
    "\left. \varphi = \pi\slash 3 \right."), ![\left. \theta = \pi\slash 4 \right.](img/file1460.png
    "\left. \theta = \pi\slash 4 \right."), and ![\left. \delta = \pi\slash 5 \right.](img/file1461.png
    "\left. \delta = \pi\slash 5 \right."):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将使用通用单量子比特门 ![U_{3}(\varphi,\theta,\delta)](img/file1458.png "U_{3}(\varphi,\theta,\delta)")
    来构建状态 ![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")，因为，正如我们在
    *第* [*1*](ch008.xhtml#x1-180001) *章* *量子计算基础* 中所学到的，它允许我们创建任何单量子比特状态。特别是，我们将输入值
    ![\left. \varphi = \pi\slash 3 \right.](img/file1459.png "\left. \varphi = \pi\slash
    3 \right.")、![\left. \theta = \pi\slash 4 \right.](img/file1460.png "\left. \theta
    = \pi\slash 4 \right.") 和 ![\left. \delta = \pi\slash 5 \right.](img/file1461.png
    "\left. \delta = \pi\slash 5 \right."):'
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With these values set, we can define a function that will construct the circuit
    that will prepare ![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1}
    \right\rangle"):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 设置这些值后，我们可以定义一个函数来构建准备 ![\left| \psi_{1} \right\rangle](img/file177.png "\left|
    \psi_{1} \right\rangle") 的电路：
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that we have defined this as a function and not as a quantum node. That’s
    because, for the purposes of the training, we are not interested in running any
    of the components of the quantum GAN individually. We will instead have to run
    them in composition. For instance, we will have to run this circuit that we’ve
    just defined composed with the discriminator.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将其定义为函数而不是量子节点。这是因为，为了训练的目的，我们并不感兴趣于单独运行量子 GAN 的任何组件。我们反而需要将它们组合起来运行。例如，我们必须运行我们刚刚定义的电路与判别器组合后的电路。
- en: 'Now that we have a circuit that can prepare ![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle"), it’s time for us to think about the two core
    components of our QGAN: the generator and the discriminator. Specifically, we
    will have to find some suitable variational forms for them.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个可以制备![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1}
    \right\rangle")的电路，是我们考虑我们 QGAN 的两个核心组件的时候了：生成器和判别器。具体来说，我们必须为它们找到一些合适的可变形式。
- en: 'For the generator, we will simply use a parametrized U3 gate, whereas, for
    the discriminator, we will use a variation of the two-local variational form.
    These can be implemented as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成器，我们将简单地使用一个参数化的 U3 门，而对于判别器，我们将使用两种局部可变形式的变体。这些可以按照以下方式实现：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can see a graphical representation of the discriminator variational form
    in *Figure* [*12.5*](#Figure12.5); its implementation is mostly analogous to that
    of the two-local variational form with just a few small differences. On a very
    minor note, we have renamed the vector of optimizable parameters to `weights`
    (instead of `theta`) to avoid any sort of confusion with the angle `theta` that
    defines ![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle").
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图* [*12.5*](#Figure12.5) 中看到判别器可变形式的图形表示；其实现方式与两种局部可变形式类似，只是有一些细微的差别。在非常小的细节上，我们将可优化参数的向量重命名为
    `weights`（而不是 `theta`），以避免与定义![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle")的角度 `theta` 产生混淆。
- en: '![Figure 12.5: Discriminator variational form on two qubits and two repetitions](img/file1462.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5：两个量子比特和两次重复的判别器可变形式](img/file1462.jpg)'
- en: '**Figure 12.5**: Discriminator variational form on two qubits and two repetitions'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.5**：两个量子比特和两次重复的判别器可变形式'
- en: 'Taking advantage of these newly-defined variational forms, we will define the
    circuits of the generator and the discriminator as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些新定义的可变形式，我们将定义生成器和判别器的电路如下：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now ready to define the quantum nodes that we will use in the training.
    In the classifier, we shall take the measurement operation to be the computation
    of the expectation value of ![M = \left| 0 \right\rangle\left\langle 0 \right|](img/file1388.png
    "M = \left| 0 \right\rangle\left\langle 0 \right|") on the first qubit. For this
    purpose, we may construct the matrix ![M](img/file704.png "M") as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义我们将在训练中使用的量子节点。在分类器中，我们将测量操作定义为在第一个量子比特上计算![M = \left| 0 \right\rangle\left\langle
    0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|")的期望值。为此，我们可能构造矩阵![M](img/file704.png
    "M")如下：
- en: '[PRE5]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And we can now define two quantum nodes: one concatenating the generation of
    the state ![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")
    with the discriminator, and one concatenating the generator with the discriminator.
    We can achieve this with the following piece of code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义两个量子节点：一个将状态![\left| \psi_{1} \right\rangle](img/file177.png "\left|
    \psi_{1} \right\rangle")的生成与判别器连接起来，另一个将生成器与判别器连接起来。我们可以通过以下代码片段实现这一点：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The measurement operation is the computation of the expectation value of ![M](img/file704.png
    "M") on the first qubit in both nodes; since this operation is the output of the
    discriminator, these measurement operations need be identical. Notice, by the
    way, that, since the discriminator works on the two qubits of our device, we could
    have also used the expectation value of ![M](img/file704.png "M") on the second
    qubit.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 测量操作是在两个节点中第一个量子比特上计算![M](img/file704.png "M")的期望值；由于这个操作是判别器的输出，这些测量操作需要是相同的。顺便说一下，由于判别器作用于我们设备上的两个量子比特，我们也可以使用第二个量子比特上![M](img/file704.png
    "M")的期望值。
- en: The training process
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练过程
- en: 'Now we have fully set up our model, and we have defined all the nodes that
    we will use in its training. But there’s something essential that we haven’t yet
    defined: the loss functions of the discriminator and the generator.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完全设置了我们的模型，并定义了我们在其训练中将要使用的所有节点。但还有一件重要的事情我们还没有定义：判别器和生成器的损失函数。
- en: 'As we discussed before, a reasonable choice for the loss function of the discriminator
    of a GAN is the binary cross-entropy. In our case, our discriminator only has
    to classify two data points: the true state ![\left| \psi_{1} \right\rangle](img/file177.png
    "\left| \psi_{1} \right\rangle") with intended label ![1](img/file13.png "1"),
    and the generated state ![\left| \psi_{g} \right\rangle](img/file1463.png "\left|
    \psi_{g} \right\rangle") with intended label ![0](img/file12.png "0"). Therefore,
    if we let ![D](img/file1101.png "D") denote the action of the discriminator under
    a certain configuration, the binary cross-entropy loss would be'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，对于GAN的判别器的损失函数，一个合理的选择是二元交叉熵。在我们的情况下，我们的判别器只需要分类两个数据点：具有预期标签![1](img/file13.png
    "1")的真实状态![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")，以及具有预期标签![0](img/file12.png
    "0")的生成状态![\left| \psi_{g} \right\rangle](img/file1463.png "\left| \psi_{g} \right\rangle")。因此，如果我们让![D](img/file1101.png
    "D")表示判别器在某种配置下的作用，二元交叉熵损失将是
- en: '| ![L_{D} = - \frac{1}{2}\left( {\log\left( {1 - D(\left&#124; \psi_{g} \right\rangle)}
    \right) + \log\left( {D(\left&#124; \psi_{1} \right\rangle} \right)} \right).](img/file1464.png
    "L_{D} = - \frac{1}{2}\left( {\log\left( {1 - D(\left&#124; \psi_{g} \right\rangle)}
    \right) + \log\left( {D(\left&#124; \psi_{1} \right\rangle} \right)} \right).")
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ![L_{D} = - \frac{1}{2}\left( {\log\left( {1 - D(\left&#124; \psi_{g} \right\rangle)}
    \right) + \log\left( {D(\left&#124; \psi_{1} \right\rangle)} \right)} \right).](img/file1464.png
    "L_{D} = - \frac{1}{2}\left( {\log\left( {1 - D(\left&#124; \psi_{g} \right\rangle)}
    \right) + \log\left( {D(\left&#124; \psi_{1} \right\rangle)} \right)} \right).")
    |'
- en: 'This loss function can be implemented with our previously-defined nodes as
    follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数可以使用我们之前定义的节点如下实现：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, what about the loss of the generator? We already know that the goal of
    the generator is to fool the discriminator into misclassifying the generated state
    as real. Moreover, we have also mentioned a reasonable generator loss function
    is
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于生成器的损失呢？我们已经知道生成器的目标是欺骗判别器将生成的状态错误地分类为真实状态。此外，我们还提到了一个合理的生成器损失函数是
- en: '| ![L_{G} = - \log\left( {D(\left&#124; \psi_{g} \right\rangle)} \right).](img/file1465.png
    "L_{G} = - \log\left( {D(\left&#124; \psi_{g} \right\rangle)} \right).") |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![L_{G} = - \log\left( {D(\left&#124; \psi_{g} \right\rangle)} \right).](img/file1465.png
    "L_{G} = - \log\left( {D(\left&#124; \psi_{g} \right\rangle)} \right).") |'
- en: This would be the binary cross-entropy loss of the discriminator if it were
    tasked with classifying the generated state as the true state.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果判别器被要求将生成的状态分类为真实状态，这将是对判别器的二元交叉熵损失。
- en: 'We can easily implement this loss as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地实现这个损失如下：
- en: '[PRE8]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And that defines all our losses. Let’s now prepare ourselves for the training
    process. First and foremost, let’s initialize the weights of the generator and
    the discriminator to a tensor with random values:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这就定义了我们所有的损失。现在让我们为训练过程做好准备。首先，让我们初始化生成器和判别器的权重为一个具有随机值的张量：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dimensions of these arrays are justified from the fact that the generator
    uses ![3](img/file472.png "3") weights and the variational form of the discriminator
    has ![3 + 1](img/file1466.png "3 + 1") groups of parametrized gates, with ![3](img/file472.png
    "3") parameters being used on each of the ![2](img/file302.png "2") qubits on
    which the form acts. Also, remember that we need to set `requires_grad` `=` `True`
    in order for PyTorch to be able to compute gradients on these weights later on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组的维度可以从以下事实中得到证明：生成器使用了![3](img/file472.png "3")个权重，判别器的变分形式有![3 + 1](img/file1466.png
    "3 + 1")组参数化的门，其中![3](img/file472.png "3")个参数被用于每个形式作用的![2](img/file302.png "2")个量子比特上。另外，记住我们需要将`requires_grad`设置为`True`，以便PyTorch能够在稍后对这些权重计算梯度。
- en: 'Now we can define the optimizers that we will use in the training. For this
    problem, we will rely on the stochastic gradient descent algorithm, which is a
    more simple version of the Adam optimizer that we used in previous chapters (see
    *Section* [*8.2.3*](ch017.xhtml#x1-1520008.2.3) for a refresher). When invoking
    the optimizers, we have to provide an array or dictionary with the parameters
    that we want our optimizer to look after. Back when we defined PyTorch models
    as subclasses of `nn``.``Module`, we could just get this with the `parameters`
    method, but in this case, we will create the list ourselves. This can be done
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义在训练中使用的优化器。对于这个问题，我们将依赖于随机梯度下降算法，这是我们在前几章中使用的 Adam 优化器的一个更简单的版本（见 *第8.2.3节*
    [ch017.xhtml#x1-1520008.2.3] 以获取复习）。在调用优化器时，我们必须提供一个数组或字典，其中包含我们希望优化器关注的参数。当我们以前将
    PyTorch 模型定义为 `nn.Module` 的子类时，我们可以通过 `parameters` 方法直接获取这些参数，但在这个情况下，我们将自己创建这个列表。这可以按照以下方式完成：
- en: '[PRE10]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this call to the optimizers, we have set their learning rate to ![0.5](img/file1166.png
    "0.5").
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次调用优化器时，我们将它们的学习率设置为![0.5](img/file1166.png "0.5")。
- en: 'And those are all the ingredients needed to train our model. We can execute
    the following piece of code in order to do so:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是训练我们模型所需的所有成分。我们可以执行以下代码片段来完成这项工作：
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There is quite a lot to digest here. In the first few lines of code, we are
    simply defining some arrays in which we will store data as the training progresses.
    The arrays `dis_losses` and `gen_losses` will save the discriminator and generator
    losses in each training cycle, and the array `log_weights` will store the generator
    weights obtained at the end of each training cycle. We will later use this information
    in order to assess the effectiveness of the training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有很多东西需要消化。在代码的前几行中，我们只是在定义一些数组，我们将随着训练的进行在这些数组中存储数据。数组 `dis_losses` 和 `gen_losses`
    将在每个训练周期中保存判别器和生成器的损失，而数组 `log_weights` 将存储在每个训练周期结束时获得的生成器权重。我们将在以后使用这些信息来评估训练的有效性。
- en: 'We have fixed the training to run for ![150](img/file1467.png "150") optimization
    cycles. In each of them, we will optimize the values of the discriminator, then
    optimize those of the generator, and, finally, log all the results. Let’s go through
    it step by step:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将训练设置为运行![150](img/file1467.png "150")次优化周期。在每个周期中，我们将优化判别器的值，然后优化生成器的值，最后记录所有结果。让我们一步一步来：
- en: When we optimize the discriminator, we reset its optimizer (`optd`) and then
    compute the discriminator loss function and store it in `lossd`. Observe that,
    when we send the generator weights, we pass them through the `detach` method.
    This method removes the need to compute gradients for these weights. The discriminator
    optimizer is not going to touch those weights either way, so this will save us
    some computation time. Once we have the loss, we just compute its gradients with
    the `backward` method and run a step of the discriminator optimizer.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们优化判别器时，我们重置其优化器（`optd`），然后计算判别器损失函数并将其存储在 `lossd` 中。注意，当我们发送生成器权重时，我们通过 `detach`
    方法传递它们。这个方法消除了对这些权重计算梯度的需要。无论怎样，判别器优化器都不会触及这些权重，这将为我们节省一些计算时间。一旦我们有了损失，我们就使用 `backward`
    方法计算其梯度，并运行判别器优化器的一步。
- en: The optimization of the generator is fully analogous. We simply use the generator
    optimizer `optg` on the gradients obtained from the generator loss `lossg`. Of
    course, we detach the discriminator weights in the call to the generator loss
    function instead of the generator weights.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器的优化是完全相似的。我们只需在从生成器损失 `lossg` 获得的梯度上使用生成器优化器 `optg`。当然，在调用生成器损失函数时，我们移除了判别器权重，而不是生成器权重。
- en: Finally, we log the values of the losses. For this purpose, we simply store
    the values of the losses that we computed in the training cycle. These will probably
    be different from the ones at the end of the cycle, but they will still be informative
    enough.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们记录损失值。为此，我们只需存储在训练周期中计算的损失值。这些值可能不同于周期末的值，但它们仍然足够有信息量。
- en: After this, we store the generator weights. Please observe the call to the `clone`
    method. This call ensures that we are getting a copy of the weights and not a
    reference to the weights tensor. If we didn’t call this method, all the weight
    arrays in `log_weights` would reference the same tensor and their values would
    all be the same and would change (simultaneously) as the training progresses!
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之后，我们存储生成器的权重。请注意对`clone`方法的调用。这个调用确保我们得到权重的一个副本，而不是权重张量的引用。如果我们没有调用这个方法，`log_weights`中的所有权重数组都会引用同一个张量，它们的值都会相同，并且会（同时）随着训练的进行而改变！
- en: Finally, we print some information about the training. Since we are going to
    execute this loop for ![150](img/file1467.png "150") training cycles and the training
    will be fast, we shall only print information every ![15](img/file599.png "15")
    cycles.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们打印一些关于训练的信息。由于我们将执行这个循环![150](img/file1467.png "150")个训练周期，并且训练将会很快，所以我们每![15](img/file599.png
    "15")个周期只打印一次信息。
- en: Notice how, instead of fully training the discriminator and the generator in
    an alternating fashion, we are optimizing them in an alternating fashion in every
    training cycle.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不是交替地完全训练判别器和生成器，而是在每个训练周期中交替优化它们。
- en: 'The output that we get upon running the preceding code is the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后得到的输出如下：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Just by looking at this raw output, we can see that there is a chance that
    our training may have been successful: the discriminator loss and the generator
    loss are both approaching ![\left. - \log 1\slash 2 \right.](img/file1468.png
    "\left. - \log 1\slash 2 \right."), just as they should do at the optimal point.
    This is a good sign!'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 只需看一下这个原始输出，我们就可以看出，我们的训练可能已经成功：判别器损失和生成器损失都在接近![\left. - \log 1\slash 2 \right.](img/file1468.png
    "\left. - \log 1\slash 2 \right.")，正如它们应该在最佳点所做的那样。这是一个好兆头！
- en: 'In order to have a better insight on the evolution of these losses, we may
    use the `gen_losses` and `dis_losses` array in order to plot their evolution.
    This can be done as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解这些损失的演变，我们可以使用`gen_losses`和`dis_losses`数组来绘制它们的演变。这可以按以下方式完成：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The resulting graph can be found in *Figure* [*12.6*](#Figure12.6) and, indeed,
    we can see a nice trend from which to draw some optimism.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图可以在*图* [*12.6*](#Figure12.6) 中找到，实际上，我们可以看到一个很好的趋势，从中我们可以得出一些乐观的结论。
- en: '![Figure 12.6: Evolution of the losses of the discriminator and the generator
    along the training process ](img/file1469.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.6：判别器和生成器在训练过程中的损失演变](img/file1469.png)'
- en: '**Figure 12.6**: Evolution of the losses of the discriminator and the generator
    along the training process'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12.6**：判别器和生成器在训练过程中的损失演变'
- en: But now comes the moment of truth. Let’s see if, indeed, our model has learned
    as we wanted it to. We mentioned in the previous section that, when training generative
    adversarial networks, the best criteria for determining whether a training process
    was successful or not depends on the problem at hand. In our case, our training
    will be successful if the state returned by the generator is close to ![\left|
    \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle").
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在到了检验真伪的时刻。让我们看看我们的模型是否真的像我们希望的那样学习了。在上一节中，我们提到，在训练生成对抗网络时，确定训练过程是否成功的最佳标准取决于具体问题。在我们的情况下，如果生成器返回的状态接近![\left|
    \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")，则我们的训练将成功。
- en: Now, how do we determine the state vector of a qubit? It turns out that the
    state of a qubit is fully characterized (up to an unimportant global phase, as
    we saw back in *Section* *[*1.3.4*](ch008.xhtml#x1-250001.3.4)) by its Bloch sphere
    coordinates. And now that we’ve come across these coordinates, let’s learn how
    to compute them with an exercise that we hope you will find interesting — although,
    admittedly, is slightly orthogonal to this chapter.*
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何确定一个量子比特的状态向量？事实证明，量子比特的状态（除了一个不重要的全局相位之外，正如我们在*第 1.3.4 节*中看到的那样）完全由其布洛赫球坐标来表征。既然我们已经遇到了这些坐标，让我们通过一个希望你会觉得有趣的练习来学习如何计算它们——尽管，诚实地讲，这个练习与本章内容略有不相关。*
- en: '*Exercise 12.2'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*练习 12.2'
- en: Prove that the Bloch sphere coordinates of a one-qubit state are the expectation
    values of the observables given by the three Pauli matrices ![X](img/file9.png
    "X"), ![Y](img/file11.png "Y"), and ![Z](img/file8.png "Z").
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 证明一个单比特态的布洛赫球坐标是三个泡利矩阵![X](img/file9.png "X")、![Y](img/file11.png "Y")和![Z](img/file8.png
    "Z")给出的可观测量的期望值。
- en: 'We can prepare two quantum nodes that return these expectation values for both
    ![\left| \psi_{1} \right\rangle](img/file177.png "\left| \psi_{1} \right\rangle")
    and the state returned by the generator after the training. This can be done as
    follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以准备两个量子节点，这些节点返回这两个期望值：![\left| \psi_{1} \right\rangle](img/file177.png "\left|
    \psi_{1} \right\rangle")和训练后生成器返回的状态。这可以按以下方式完成：
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And the output that we get is the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出如下：
- en: '[PRE15]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The outputs are identical, so we can safely say that our training has been a
    huge success!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是相同的，因此我们可以安全地说，我们的训练取得了巨大的成功！
- en: In order to bring this section to an end, we will visually explore how the state
    created by the generator has evolved throughout the training. We can do this using
    the array of weights `log_weights` and the `generated_coordinates` function that
    we have just defined. This function takes the weights of the generator as input,
    so we can get the Bloch coordinates of the generated states at any point in the
    training using the saved weights.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本节，我们将通过视觉方式探索生成器在整个训练过程中创建的状态是如何演变的。我们可以使用我们刚刚定义的`log_weights`权重数组和`generated_coordinates`函数来完成这项工作。这个函数将生成器的权重作为输入，因此我们可以使用保存的权重在任何训练阶段获取生成的状态的Bloch坐标。
- en: 'We can accomplish this as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样完成：
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function will plot, for any training cycle, a representation of the Bloch
    coordinates of the generated states superposed to the coordinates of the state
    that we want our QGAN to learn. In *Figure* [*12.7*](#Figure12.7) you can see
    the plots corresponding to a wide range of cycles.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将为任何训练周期绘制一个表示生成的状态的Bloch坐标的图，并将其叠加到我们希望我们的QGAN学习的状态的坐标上。在*图* [*12.7*](#Figure12.7)中，你可以看到对应于广泛周期的绘图。
- en: '![Figure 12.7: Evolution of the Bloch coordinates of the generated state as
    the training progresses](img/file1470.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7：随着训练的进行，生成的状态的Bloch坐标的演变](img/file1470.png)'
- en: '**Figure 12.7**: Evolution of the Bloch coordinates of the generated state
    as the training progresses'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.7**：随着训练的进行，生成的状态的Bloch坐标的演变'
- en: Exercise 12.3
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 练习12.3
- en: Try to replicate this example on a different state (you may need to increase
    the number of training cycles to reach convergence in some cases).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在不同的状态上复制这个例子（在某些情况下，你可能需要增加训练周期数以达到收敛）。
- en: That brings this example to an end. Let’s now consider a different QGAN, this
    time implemented in Qiskit.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了这个例子。现在让我们考虑一个不同的QGAN，这次是在Qiskit中实现的。
- en: 12.3 Quantum GANs in Qiskit
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12.3 Qiskit中的量子GAN
- en: An early proposal of a QGAN was introduced by IBM researchers Zoufal, Lucchi,
    and Woerner [[101](ch030.xhtml#Xzoufal2019quantum)] to learn a probability distribution
    using a QGAN with a quantum generator and a classical discriminator. In this section,
    we will discuss how to implement this kind of QGAN with Qiskit, so let’s put everything
    in more precise terms.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 量子GAN的一个早期提议是由IBM研究人员Zoufal、Lucchi和Woerner提出的 [[101](ch030.xhtml#Xzoufal2019quantum)]，他们使用具有量子生成器和经典判别器的QGAN来学习概率分布。在本节中，我们将讨论如何使用Qiskit实现这种类型的QGAN，因此让我们更精确地定义所有这些。
- en: This type of quantum GAN is given a dataset of real numbers that follow a certain
    probability distribution. This distribution may potentially be continuous, but
    it could be discretized to take some values ![m,m + 1,m + 2,\ldots,M - 1,M](img/file1471.png
    "m,m + 1,m + 2,\ldots,M - 1,M") with ![m < M](img/file1472.png "m < M"); this
    will usually be done by fixing the values ![m](img/file259.png "m") and ![M](img/file704.png
    "M"), rounding the samples and ignoring those that are smaller than ![m](img/file259.png
    "m") or bigger than ![M](img/file704.png "M"). Each of the resulting labels ![j
    = m,\ldots,M](img/file1473.png "j = m,\ldots,M") will have a certain probability
    ![p_{j}](img/file1474.png "p_{j}") of appearing in the dataset. That is the distribution
    that we want the generator in our QGAN to learn.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的量子GAN被给定了遵循某种概率分布的实数数据集。这种分布可能是连续的，但也可能被离散化以取一些值 ![m,m + 1,m + 2,\ldots,M
    - 1,M](img/file1471.png "m,m + 1,m + 2,\ldots,M - 1,M")，其中 ![m < M](img/file1472.png
    "m < M")；这通常是通过固定值 ![m](img/file259.png "m") 和 ![M](img/file704.png "M")，四舍五入样本并忽略小于
    ![m](img/file259.png "m") 或大于 ![M](img/file704.png "M") 的样本来完成的。每个结果标签 ![j = m,\ldots,M](img/file1473.png
    "j = m,\ldots,M") 都将有一定的概率 ![p_{j}](img/file1474.png "p_{j}") 出现在数据集中。这就是我们希望我们的QGAN中的生成器学习到的分布。
- en: And what does the generator of these QGANs look like? It is a quantum generator
    that is dependent on some classical parameters. It needs to be designed to have
    ![n](img/file244.png "n") qubits in such a way that ![M - m < 2^{n}](img/file1475.png
    "M - m < 2^{n}"), so that we may assign, to each possible outcome ![r](img/file1337.png
    "r") after a measurement in the computational basis of the generator, a label
    ![\alpha(r)](img/file1476.png "\alpha(r)") in ![m,\ldots M](img/file1477.png "m,\ldots
    M"). Thus, the goal of the training will be for the state returned by the generator
    to be as close as possible to
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些QGAN的生成器看起来是什么样子呢？它是一个依赖于一些经典参数的量子生成器。它需要设计成具有![n](img/file244.png "n")个量子位，这样![M
    - m < 2^{n}](img/file1475.png "M - m < 2^{n}")，以便我们可以在生成器的计算基中测量后，为每个可能的输出![r](img/file1337.png
    "r")分配一个![m,\ldots M](img/file1477.png "m,\ldots M")中的标签![\alpha(r)](img/file1476.png
    "\alpha(r)")。因此，训练的目标将是使生成器返回的状态尽可能接近
- en: '| ![\sum\limits_{r}\sqrt{p_{\alpha(r)}}\left&#124; r \right\rangle.](img/file1478.png
    "\sum\limits_{r}\sqrt{p_{\alpha(r)}}\left&#124; r \right\rangle.") |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| ![\sum\limits_{r}\sqrt{p_{\alpha(r)}}\left&#124; r \right\rangle.](img/file1478.png
    "\sum\limits_{r}\sqrt{p_{\alpha(r)}}\left&#124; r \right\rangle.") |'
- en: In this way, measuring samples from the trained generator should be equivalent
    to extracting more data samples from the original distribution, because the probability
    of measuring ![\left| r \right\rangle](img/file1479.png "\left| r \right\rangle")
    (which is associated to label ![\alpha(r)](img/file1476.png "\alpha(r)")) is exactly
    ![\left| \sqrt{p_{\alpha(r)}} \right|^{2} = p_{\alpha(r)}](img/file1480.png "\left|
    \sqrt{p_{\alpha(r)}} \right|^{2} = p_{\alpha(r)}").
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，从训练好的生成器中测量的样本应该等同于从原始分布中提取更多的数据样本，因为测量![\left| r \right\rangle](img/file1479.png
    "\left| r \right\rangle")（与标签![\alpha(r)](img/file1476.png "\alpha(r)"))相关联）的概率正好是![\left|
    \sqrt{p_{\alpha(r)}} \right|^{2} = p_{\alpha(r)}](img/file1480.png "\left| \sqrt{p_{\alpha(r)}}
    \right|^{2} = p_{\alpha(r)}").
- en: The discriminator that enables the training of this QGAN is a classical neural
    network tasked with distinguishing whether an input datum belongs to the original
    dataset or has been generated by the discriminator.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使QGAN训练成为可能的判别器是一个经典的神经网络，其任务是区分输入数据是否属于原始数据集或是由判别器生成的。
- en: 'So that’s the QGAN that we are going to work with: a hybrid QGAN in which the
    generator is quantum and the discriminator is classic. Sounds interesting? Let’s
    see how we can implement it and train it using Qiskit.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们将要工作的QGAN：一个混合QGAN，其中生成器是量子化的，而判别器是经典的。听起来很有趣？让我们看看我们如何使用Qiskit实现和训练它。
- en: 'In order to get started, let’s import NumPy and Qiskit while setting some seeds
    to ensure the reproducibility of our results:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始，让我们导入NumPy和Qiskit，同时设置一些种子以确保我们结果的复现性：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will consider a particular example of the general problem that we outlined
    previously. We will take a dataset with ![1000](img/file790.png "1000") samples
    generated from the binomial distribution with ![n = 3](img/file1481.png "n = 3")
    trials and probability ![\left. p = 1\slash 2 \right.](img/file1482.png "\left.
    p = 1\slash 2 \right."). These distributions can only take ![4 = 2^{2}](img/file1483.png
    "4 = 2^{2}") possible values (![0,1,2,3](img/file1484.png "0,1,2,3")), so will
    have to use ![2](img/file302.png "2") qubits in our generator. We may generate
    the samples of our dataset using NumPy as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑之前概述的通用问题的特定例子。我们将使用一个包含![1000](img/file790.png "1000")个样本的数据集，这些样本是从![n
    = 3](img/file1481.png "n = 3")次试验和概率![\left. p = 1\slash 2 \right.](img/file1482.png
    "\left. p = 1\slash 2 \right.")的二项分布中生成的。这些分布只能取![4 = 2^{2}](img/file1483.png
    "4 = 2^{2}")个可能值 (![0,1,2,3](img/file1484.png "0,1,2,3"))，因此我们的生成器将需要使用![2](img/file302.png
    "2")个量子位。我们可以使用以下方式使用NumPy生成数据集的样本：
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Qiskit framework already incorporates a `QGAN` class that can create and
    train the QGAN architecture that we discussed previously — it’s almost tailor-made
    for this problem! We may import the class from the `qiskit_machine_learning``.``algorithms`
    module and define our QGAN as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Qiskit框架已经集成了`QGAN`类，可以创建和训练我们之前讨论过的QGAN架构——它几乎是为这个问题量身定制的！我们可以从`qiskit_machine_learning``.``algorithms`模块导入该类，并定义我们的QGAN如下：
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the call to the `QGAN` initializer, we had to specify the dataset whose distribution
    we want to learn, the bounds at which we want to ”cut” the dataset (in this case,
    we just specified the actual bounds of our distribution), an array containing
    the number of qubits of the generator circuit, the batch size, the number of training
    cycles that we want our QGAN to run for, the quantum instance on which the QGAN
    will run and, lastly, an optional seed.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`QGAN`初始化器时，我们必须指定我们想要学习的数据集的分布，我们想要“切割”数据集的界限（在这种情况下，我们只指定了我们分布的实际界限），包含生成器电路量子比特数的数组，批处理大小，我们希望QGAN运行的训练周期数，QGAN将运行的量子实例，最后是一个可选的种子。
- en: You may be confused by the fact that we’ve had to send the number of qubits
    of the quantum generator in an array. That’s because this `QGAN` class could support
    generating samples of any dimension ![d](img/file1485.png "d") (using ![d](img/file1485.png
    "d") generators); in our case, we have ![d = 1](img/file1486.png "d = 1"), hence
    we only need to pass an array with a single element.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对我们不得不以数组形式发送量子生成器的量子比特数感到困惑。这是因为这个`QGAN`类可以支持生成任何维度![d](img/file1485.png
    "d")（使用![d](img/file1485.png "d")个生成器）的样本；在我们的情况下，我们有![d = 1](img/file1486.png
    "d = 1")，因此我们只需要传递一个包含单个元素的数组。
- en: This QGAN object already comes with a default implementation for the generator
    and the discriminator, and we will rely on them.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个QGAN对象已经包含了生成器和判别器的默认实现，我们将依赖它们。
- en: To learn more…
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息...
- en: In this default implementation, the discriminator is a dense neural network
    having two consecutive intermediate layers with ![50](img/file1390.png "50") and
    ![20](img/file588.png "20") neurons each; the activation function in these intermediate
    layers is the leaky ReLU function and that of the output layer is the sigmoid
    function. The generator uses a variational form consisting of a layer of Hadamard
    gates applied on each qubit followed by the two-local variational form with one
    repetition and circular entanglement.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在此默认实现中，判别器是一个具有两个连续中间层的密集神经网络，每个中间层有![50](img/file1390.png "50")和![20](img/file588.png
    "20")个神经元；这些中间层的激活函数是漏ReLU函数，输出层的激活函数是sigmoid函数。生成器使用一个变分形式，包括在每个量子比特上应用一个Hadamard门，然后是具有一次重复和圆周纠缠的两个局部变分形式。
- en: These details are not specified in the documentation, but they can be found
    in the source code.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些细节在文档中未指定，但可以在源代码中找到。
- en: 'In order to train the QGAN, we may run the following instruction:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练QGAN，我们可以运行以下指令：
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The training will take a few minutes to complete, depending on the hardware
    configuration of your computer. In order to plot the evolution of the generator
    and discriminator losses throughout the training process, we may run the following
    code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成可能需要几分钟，具体取决于你电脑的硬件配置。为了在整个训练过程中绘制生成器和判别器损失的演变，我们可以运行以下代码：
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This yields the plot shown in *Figure* [*12.8*](#Figure12.8). We can see how
    both losses are approaching ![\left. - \log 1\slash 2 \right.](img/file1468.png
    "\left. - \log 1\slash 2 \right."), which can give us hope for the success of
    our training.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了*图* [*12.8*](#Figure12.8)中所示的图表。我们可以看到，两个损失都接近![\left. - \log 1\slash 2
    \right.](img/file1468.png "\left. - \log 1\slash 2 \right.")，这可以给我们训练成功的希望。
- en: '![Figure 12.8: Evolution of the generator and discriminator losses during the
    QGAN training, learning a distribution ](img/file1487.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8：QGAN训练过程中生成器和判别器损失的演变，学习分布](img/file1487.png)'
- en: '**Figure 12.8**: Evolution of the generator and discriminator losses during
    the QGAN training, learning a distribution'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.8**：QGAN训练过程中生成器和判别器损失的演变，学习分布'
- en: 'In order to check if our training has been successful, we will plot the distribution
    of the measurement outcomes of our generator against the original distribution.
    We may generate the data for this plot as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的训练是否成功，我们将绘制生成器测量结果的分布与原始分布的对比图。我们可以按照以下方式生成此图的所需数据：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In this piece of code, we have first asked our QGAN to generate a sample with
    the distribution it has learned. Then, we have created an array `real_distr` with
    the relative frequencies of the values in the distribution (entry `j` corresponds
    to the relative frequency of the value ![j](img/file258.png "j")). Lastly, we
    have plotted the real distribution against our generated distribution. The output
    can be found in *Figure* [*12.9*](#Figure12.9).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先让我们的QGAN生成一个具有它所学习分布的样本。然后，我们创建了一个包含分布中值相对频率的数组 `real_distr`（条目 `j`
    对应于值 ![j](img/file258.png "j") 的相对频率）。最后，我们将真实分布与我们的生成分布进行了对比绘图。输出可以在 *图* [*12.9*](#Figure12.9)
    中找到。
- en: '![Figure 12.9: Histogram comparing the real distribution (thicker bar) with
    the one generated by the QGAN (thinner bar) ](img/file1488.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图12.9：比较真实分布（较粗的柱状图）与QGAN生成的分布（较细的柱状图）的直方图](img/file1488.png)'
- en: '**Figure 12.9**: Histogram comparing the real distribution (thicker bar) with
    the one generated by the QGAN (thinner bar)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.9**：比较真实分布（较粗的柱状图）与QGAN生成的分布（较细的柱状图）'
- en: Of course, for the purposes of this example, this visualization is more than
    enough to convince us that the training, indeed, has been effective. In more sophisticated
    examples, one may instead want to rely on more quantitative metrics of success.
    One such metric is the **relative entropy** or **Kullback–Leibler** **divergence**
    from one distribution to another. In layman’s terms, this entropy measures how
    ”different” two distributions are in a way that if two distributions ![P_{0}](img/file1489.png
    "P_{0}") and ![P_{1}](img/file1490.png "P_{1}") are identical, the relative entropy
    from ![P_{0}](img/file1489.png "P_{0}") to ![P_{1}](img/file1490.png "P_{1}")
    is ![0](img/file12.png "0"). As ![P_{1}](img/file1490.png "P_{1}") becomes more
    different from ![P_{0}](img/file1489.png "P_{0}"), the relative entropy increases.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，就这个例子而言，这种可视化已经足够让我们相信训练确实有效。在更复杂的例子中，人们可能更希望依赖更多量化的成功指标。其中一个指标是从一个分布到另一个分布的**相对熵**或**库尔巴克-莱布勒****散度**。用通俗的话说，这个熵度衡量了两个分布“不同”的程度，如果两个分布
    ![P_{0}](img/file1489.png "P_{0}") 和 ![P_{1}](img/file1490.png "P_{1}") 是相同的，那么从
    ![P_{0}](img/file1489.png "P_{0}") 到 ![P_{1}](img/file1490.png "P_{1}") 的相对熵是
    ![0](img/file12.png "0")。当 ![P_{1}](img/file1490.png "P_{1}") 与 ![P_{0}](img/file1489.png
    "P_{0}") 的差异越来越大时，相对熵也会增加。
- en: To learn more…
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息…
- en: When you are given two discrete probability distributions ![P_{0}](img/file1489.png
    "P_{0}") and ![P_{1}](img/file1490.png "P_{1}") over a space ![X](img/file9.png
    "X"), the relative entropy from ![P_{0}](img/file1489.png "P_{0}") to ![P_{1}](img/file1490.png
    "P_{1}") can be defined as
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当你被给出两个在空间 ![X](img/file9.png "X") 上的离散概率分布 ![P_{0}](img/file1489.png "P_{0}")
    和 ![P_{1}](img/file1490.png "P_{1}") 时，从 ![P_{0}](img/file1489.png "P_{0}") 到
    ![P_{1}](img/file1490.png "P_{1}") 的相对熵可以定义为
- en: '| ![D(P_{1} \parallel P)0) = \sum\limits_{x \in X}P_{1}(x)\log\left( \frac{P_{1}(x)}{P_{0}(x)}
    \right).](img/file1491.png "D(P_{1} \parallel P)0) = \sum\limits_{x \in X}P_{1}(x)\log\left(
    \frac{P_{1}(x)}{P_{0}(x)} \right).") |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| ![D(P_{1} \parallel P)0) = \sum\limits_{x \in X}P_{1}(x)\log\left( \frac{P_{1}(x)}{P_{0}(x)}
    \right).](img/file1491.png "D(P_{1} \parallel P)0) = \sum\limits_{x \in X}P_{1}(x)\log\left(
    \frac{P_{1}(x)}{P_{0}(x)} \right).") |'
- en: 'Qiskit’s QGAN implementation logs the values of the relative entropy throughout
    the QGAN training. In this way, we may plot the evolution of the relative entropy
    over the training process of our QGAN with the following instructions:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Qiskit的QGAN实现记录了QGAN训练过程中相对熵的值。这样，我们可以使用以下指令绘制QGAN训练过程中相对熵的变化：
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is shown in *Figure* [*12.10*](#Figure12.10).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在 *图* [*12.10*](#Figure12.10) 中。
- en: '![Figure 12.10: Evolution of the relative entropy over the training of our
    QGAN, learning a distribution](img/file1492.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图12.10：QGAN训练过程中相对熵的变化](img/file1492.png)'
- en: '**Figure 12.10**: Evolution of the relative entropy over the training of our
    QGAN, learning a distribution'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12.10**：QGAN训练过程中学习分布的相对熵变化'
- en: Here it can be clearly shown that the relative entropy approaches ![0](img/file12.png
    "0") as the training progresses, just as we expected. This concludes our example.
    It’s time to wrap up!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 可以清楚地看到，随着训练的进行，相对熵趋近于 ![0](img/file12.png "0")，正如我们所期望的那样。这标志着我们例子的结束。现在是时候总结一下了！
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, we have explored a whole new kind of quantum machine learning
    models: quantum GANs. Unlike the models we had considered before, these are used
    primarily for generation tasks. And, unlike our previous models, they are trained
    in a fully unsupervised manner.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了一种全新的量子机器学习模型：量子 GANs。与之前考虑的模型不同，这些模型主要用于生成任务。而且，与之前的模型不同，它们是以完全无监督的方式进行训练的。
- en: After understanding what GANs are in general, we introduced the general notion
    of a QGAN, and then we learned how to implement a couple of QGAN models using
    PennyLane and Qiskit.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解 GANs 的一般概念之后，我们介绍了 QGAN 的一般概念，然后我们学习了如何使用 PennyLane 和 Qiskit 实现几个 QGAN 模型。
- en: With this, we also conclude our study of quantum machine learning for this book.
    We hope that you have had a good time learning about all these ways of making
    quantum computers learn! But your quantum journey does not need to end here. Please,
    keep on reading for a sneak peek of what you can expect in the near future in
    the quantum computing field.**
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们也完成了这本书对量子机器学习的探讨。我们希望你在学习所有这些使量子计算机学习的方法时过得愉快！但你的量子之旅并不需要在这里结束。请继续阅读，提前一瞥你可以在不久的将来在量子计算领域期待的内容。**
