- en: Emotion Detection with CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行情感检测
- en: Up until recently, interacting with a computer was not too dissimilar from interacting
    with, say, a power tool; we pick it up, turn it on, manually control it, and then
    put it down until the next time we require it for that specific task. But recently,
    we are seeing signs that this is about to change; computers allow natural forms
    of interaction and are becoming more ubiquitous, more capable, and more ingrained
    in our daily lives. They are becoming less like heartless dumb tools and more
    like friends, able to entertain us, look out for us, and assist us with our work.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，与计算机的交互与与，比如说，电动工具的交互并没有太大的区别；我们拿起它，打开它，手动控制它，然后放下，直到下一次我们需要它来完成那个特定任务。但最近，我们看到了这种状况即将改变的迹象；计算机允许自然形式的交互，并且正在变得更加普遍、更强大、更深入到我们的日常生活中。它们正在变得越来越不像无情的笨拙工具，更像朋友，能够娱乐我们，照顾我们，并帮助我们完成工作。
- en: With this shift comes a need for computers to be able to understand our emotional
    state. For example, you don't want your social robot cracking a joke after you
    arrive back from work having lost your job (to an AI bot!). This is a field of
    computer science known as **affective computing** (also referred to as **artificial
    emotional intelligence** or **emotional AI**), a field that studies systems that
    can recognize, interpret, process, and simulate human emotions. The first stage
    of this is being able to recognize emotional state, which is the topic of this
    chapter. We will first introduce the data and model we will be using, and then
    walk through how we approach the problem of expression recognition on the iPhone
    and how to appropriately preprocess the data for inference.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这种转变，计算机需要能够理解我们的情绪状态。例如，你不想在你失业（被一个AI机器人取代）下班回家后，你的社交机器人还开玩笑（！）。这是一个被称为**情感计算**（也称为**人工情感智能**或**情感AI**）的计算机科学领域，该领域研究能够识别、解释、处理和模拟人类情绪的系统。这一阶段的第一步是能够识别情绪状态，这是本章的主题。我们将首先介绍我们将使用的数据和模型，然后介绍我们如何处理iPhone上的表情识别问题以及如何适当地预处理数据以进行推理。
- en: 'By the end of of this chapter, you will have achieved the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将实现以下目标：
- en: Built a simple application that will infer your mood in real time using the
    front camera feed
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建了一个简单的应用程序，该程序将使用前置摄像头实时推断您的情绪
- en: Gained hands-on experience using the `Vision` framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用`Vision`框架获得了实践经验
- en: Developed a deeper understanding and intuition of how **convolutional neural
    networks** (**CNNs**) work and how they can be applied at the edge
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对**卷积神经网络**（**CNNs**）的工作原理以及它们如何在边缘应用有了更深入的理解和直觉
- en: Let's start by introducing the data and model we will be using.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先介绍我们将使用的数据和模型。
- en: Facial expressions
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部表情
- en: Our face is one of the strongest indicators of emotions; as we laugh or cry,
    we put our emotions on display, allowing others to glimpse into our minds. It's
    a form of nonverbal communication that, apparently, accounts for over 50% of our
    communication with others. Forty independently controlled muscles make the face
    one of the most complex systems we possess, which could be the reason we use it
    as a medium for communicating something so important as our current emotional
    state. But can we classify it?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的面部是我们情绪的最强指示器之一；当我们笑或哭时，我们将我们的情绪展示出来，让他人窥视我们的内心。这是一种非言语交流形式，据称，它占我们与他人交流的50%以上。四十个独立控制的肌肉使面部成为我们拥有的最复杂的系统之一，这也可能是我们将其用作传达我们当前情绪状态这样重要信息媒介的原因。但我们能对其进行分类吗？
- en: 'In 2013, the **International Conference on Machine Learning** (**ICML**) ran
    a competition inviting contestants to build a facial expression classifier using
    a training dataset of over 28,000 grayscale images. They were labeled as either
    anger, disgust, fear, happiness, sadness, surprise, or neutral. The following
    are a few samples of this training data (available at [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，**国际机器学习会议**（**ICML**）举办了一场竞赛，邀请参赛者使用超过28,000张灰度图像的训练数据集构建面部表情分类器。它们被标记为愤怒、厌恶、恐惧、快乐、悲伤、惊讶或中性。以下是一些训练数据的样本（可在[https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge)找到）：
- en: '![](img/b20a9c86-91b5-48b3-bde6-93ab67ba7fed.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b20a9c86-91b5-48b3-bde6-93ab67ba7fed.png)'
- en: 'As previously mentioned, the training dataset consists of 28,709 grayscale
    images of faces in 48 x 48 pixels, where each face is centered and associated
    with a label defining the assigned emotion. This emotion can be one of the following
    labels (textual description was added for legibility):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练数据集包含28,709张48 x 48像素的灰度人脸图像，其中每个面部居中，并关联一个定义分配情感的标签。这种情感可以是以下标签之一（添加了文本描述以提高可读性）：
- en: '![](img/a4ac4f1b-3cd4-480c-890e-24af9138d668.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a4ac4f1b-3cd4-480c-890e-24af9138d668.png)'
- en: Neural networks (or any other machine learning algorithm) can't really do anything by
    themselves. All a neural network does is find a direct or indirect correlation
    between two datasets (inputs and their corresponding outputs). In order for a
    neural network to learn, we need to present it with two meaningful datasets where
    some true correlation exists between the inputs and outputs. A good practice when
    tackling any new data problem is to come up with a predictive theory of how you
    might approach it or search for correlation using techniques such as data visualization
    or some other explorational data analysis technique. In doing so, we also better
    understand how we need to prepare our data to align it with the training data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（或任何其他机器学习算法）实际上不能自己做什么。神经网络所做的只是找到两个数据集（输入及其相应的输出）之间的直接或间接相关性。为了使神经网络学习，我们需要向它展示两个有意义的、输入和输出之间存在真实相关性的数据集。在处理任何新的数据问题时，一个好的做法是提出一个预测理论，说明你如何接近它或使用数据可视化或其他探索性数据分析技术来寻找相关性。这样做，我们也能更好地理解我们需要如何准备我们的数据，以便与训练数据对齐。
- en: 'Let''s look at the results of a data visualization technique that can be performed
    on the training data; here, it''s our assumption that some pattern exists between
    each expression (happy, sad, angry, and so on). One way of visually inspecting
    this is by averaging each expression and the associated variance. This can be
    achieved simply by finding the mean and standard deviation across all images for
    their respective class (expression example, happy, angry, and so on). The results
    of some of the expressions can be seen in the following image:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看可以应用于训练数据的数据可视化技术的结果；在这里，我们假设每个表情（快乐、悲伤、愤怒等）之间都存在某种模式。一种视觉检查这种方法的方式是通过平均每个表情及其相关的方差。这可以通过找到所有图像的相应类（表情示例、快乐、愤怒等）的平均值和标准差来实现。以下图像显示了某些表情的结果：
- en: '![](img/34542a2e-b0a6-41d2-8f6c-f909ea917e36.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34542a2e-b0a6-41d2-8f6c-f909ea917e36.png)'
- en: After you get over the creepiness of the images, you get a sense that a pattern
    does exist, and you understand what our model needs to learn to be able to recognize
    facial expressions. Some other notable, and fairly visible, takeaways from this
    exercise include the amount of variance with the disgust expression; this hints
    that our model might find it difficult to effectively learn to recognize this
    expression. The other observation - and the one more applicable to our task in
    this chapter - is that the training data consists of forward-facing faces with
    little padding beyond the face, therefore highlighting what the model expects
    for its input. Now that we have a better sense of our data; let's move on and
    introduce the model we will be using in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在你克服了图像的诡异感之后，你会感觉到确实存在某种模式，并理解我们的模型需要学习什么才能识别面部表情。从这个练习中得出的其他一些值得注意的、相当明显的结果包括厌恶表情的方差量；这暗示我们的模型可能难以有效地学习识别这种表情。另一个观察结果——并且更适用于我们本章的任务——是训练数据由面向前的面部图像组成，面部周围几乎没有填充，因此突出了模型期望的输入。现在我们对我们数据有了更好的了解；让我们继续介绍本章我们将使用的模型。
- en: 'In [chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml), *Recognising Objects
    in the World,* we presented the intuition behind CNNs or ConvNets. So, given that
    we won''t be introducing any new concepts in this chapter, we will omit any discussion
    on the details of the model and just present it here for reference, with some
    commentary about its architecture and the format of the data it is expecting for
    its input:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml)，“世界中的物体识别”，我们介绍了CNN或卷积神经网络背后的直觉。因此，鉴于我们本章不会介绍任何新概念，我们将省略对模型细节的任何讨论，只是在这里提供参考，并对其架构和期望输入数据格式进行一些评论：
- en: '![](img/3ca0c020-6418-4e82-9f76-bce205f70ae9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3ca0c020-6418-4e82-9f76-bce205f70ae9.png)'
- en: The preceding figure is a visualization of the architecture of the model; it's
    your typical CNN, with a stack of convolutional and pooling layers before being
    flattened and fed into a series of fully connected layers. Finally, it is fed
    into a softmax activation layer for multi-class classification. As mentioned earlier,
    the model is expecting a 3D tensor with the dimensions 48 x 48 x 1 (width, height,
    channels). To avoid feeding our model with large numbers (0 - 255), the input
    has been normalized (dividing each pixel by 255, which gives us a range of 0.0
    - 1.0). The model outputs the probability of a given input with respect to each
    class, that is, seven outputs with each class representing the probability of
    how likely it is correlated for the given input. To make a prediction, we simply
    take the class with the largest probability.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图是模型架构的可视化；它是一个典型的CNN，在扁平化并输入一系列全连接层之前，有一堆卷积和池化层。最后，它被输入到一个softmax激活层，用于多类分类。如前所述，模型期望一个维度为48
    x 48 x 1的3D张量（宽度、高度、通道）。为了避免将大数值（0 - 255）输入到我们的模型中，输入已经被归一化（每个像素除以255，得到0.0 -
    1.0的范围）。模型输出给定输入相对于每个类的概率，即每个类代表给定输入相关性的概率。为了做出预测，我们只需选择概率最大的类。
- en: 'This model was trained on 22,967 samples, reversing the other 5,742 samples
    for validation. After 15 epochs, the model achieved approximately 59% accuracy
    on the validation set, managing to squeeze into the 13^(th) place of the Kaggle
    competition (at the time of writing this chapter). The following graphs show the
    training accuracy and loss during training:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在22,967个样本上进行了训练，其余5,742个样本用于验证。经过15个epoch后，模型在验证集上达到了大约59%的准确率，成功挤进了Kaggle竞赛的第13名（在撰写本章时）。以下图表显示了训练过程中的准确率和损失：
- en: '![](img/4cf6c2bf-9e4e-43e8-8474-86c3fd69e450.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4cf6c2bf-9e4e-43e8-8474-86c3fd69e450.png)'
- en: This concludes our brief introduction of the data and model we will be using
    for this chapter. The two main takeaways are an appreciation of what data the
    model has been fed during training, and the fact that our model achieved just
    59% accuracy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对本章将使用的数据和模型的简要介绍。两个主要的收获是对模型在训练期间所喂食的数据的欣赏，以及我们的模型仅达到了59%的准确率。
- en: The former dictates how we approach obtaining and process the data before feeding
    it into the model. The latter poses an opportunity for further investigation to
    better understand what is pulling the accuracy down and how to improve it; it
    also can be seen as a design challenge—a design to be made around this constraint.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 前者决定了我们在将数据输入模型之前如何获取和处理数据。后者提供了一个进一步研究的机会，以更好地理解是什么因素拉低了准确率以及如何提高它；它也可以被视为一个设计挑战——一个围绕这个限制条件的设计。
- en: In this chapter, we are mainly concerned with the former so, in the next section,
    we will explore how to obtain and preprocess the data before feeding it to the
    model. Let's get started.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要关注前者，因此，在下一节中，我们将探讨如何在将数据输入模型之前获取和预处理数据。让我们开始吧。
- en: Input data and preprocessing
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入数据和预处理
- en: In this section, we will implement the preprocessing functionality required
    to transform images into something the model is expecting. We will build up this
    functionality in a playground project before migrating it across to our project
    in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现预处理功能，以将图像转换为模型所期望的形式。我们将在Playground项目中逐步构建这个功能，然后在下一节将其迁移到我们的项目中。
- en: 'If you haven''t done so already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter4/Start/` and open the Playground
    project `ExploringExpressionRecognition.playground`. Once loaded, you will see
    the playground for this chapter, as shown in the following screenshot:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请从配套仓库中拉取最新代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载完成后，导航到目录`Chapter4/Start/`并打开Playground项目`ExploringExpressionRecognition.playground`。加载完成后，您将看到本章的Playground，如下面的截图所示：
- en: '![](img/bdcff650-0176-4b84-910f-8f1e8913e55a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bdcff650-0176-4b84-910f-8f1e8913e55a.png)'
- en: Before starting, to avoid looking at images of me, please replace the test images
    with either personal photos of your own or royalty free images from the internet,
    ideally a set expressing a range of emotions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，为了避免看到我的照片，请将测试图像替换为你的个人照片或来自互联网的免费照片，理想情况下是一组表达各种情绪的照片。
- en: Along with the test images, this playground includes a compiled Core ML model
    (we introduced it in the previous image) with its generated set of wrappers for
    inputs, outputs, and the model itself. Also included are some extensions for `UIImage`,
    `UIImageView`, `CGImagePropertyOrientation`, and an empty `CIImage` extension,
    to which we will return later in the chapter. The others provide utility functions
    to help us visualize the images as we work through this playground.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了测试图像外，这个游乐场还包括一个编译好的Core ML模型（我们在上一张图片中介绍了它）及其生成的输入、输出和模型本身的包装器。还包括一些对`UIImage`、`UIImageView`、`CGImagePropertyOrientation`和空`CIImage`扩展的扩展，我们将在本章后面返回。其他扩展提供了帮助我们在游乐场中可视化图像的实用函数。
- en: Before jumping into the code, let's quickly discuss the approach we will take
    in order to determine what we actually need to implement.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入代码之前，让我们快速讨论我们将采取的方法，以确定我们实际上需要实现的内容。
- en: Up to this point, our process of performing machine learning has been fairly
    straightforward; apart from some formatting of input data, our model didn't require
    too much work. This is not the case here. A typical photo of someone doesn't normally
    have just a face, nor is their face nicely aligned to the frame unless you're
    processing passport photos. When developing machine learning applications, you
    have two broad paths.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们执行机器学习的过程相当直接；除了对输入数据进行一些格式化外，我们的模型不需要太多工作。这里的情况并非如此。典型的人的照片通常不仅仅是一个面部，除非你正在处理护照照片，否则他们的面部通常不会很好地对齐到框架中。在开发机器学习应用时，你有两条主要路径。
- en: 'The first, which is becoming increasingly popular, is to use an end-to-end
    machine learning model capable of just being fed the raw input and producing adequate
    results. One particular field that has had great success with end-to-end models
    is speech recognition. Prior to end-to-end deep learning, speech recognition systems
    were made up of many smaller modules, each one focusing on extracting specific
    pieces of data to feed into the next module, which was typically manually engineered.
    Modern speech recognition systems use end-to-end models that take the raw input
    and output the result. Both of the described approaches can been seen in the following
    diagram:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法，正变得越来越流行，是使用一种端到端机器学习模型，它只需接收原始输入并产生足够的结果。在端到端模型中取得巨大成功的特定领域之一是语音识别。在端到端深度学习之前，语音识别系统由许多较小的模块组成，每个模块都专注于提取特定的数据片段以供下一个模块使用，而下一个模块通常是手动设计的。现代语音识别系统使用端到端模型，接收原始输入并输出结果。以下图中可以见到这两种描述的方法：
- en: '![](img/11db396a-0e52-4e50-85c4-0f1d067eb6e1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/11db396a-0e52-4e50-85c4-0f1d067eb6e1.png)'
- en: 'Obviously, this approach is not constrained to speech recognition and we have
    seen it applied to image recognition tasks, too, along with many others. But there
    are two things that make this particular case different; the first is that we
    can simplify the problem by first extracting the face. This means our model has
    less features to learn and offers a smaller, more specialized model that we can
    tune. The second thing, which is no doubt obvious, is that our training data consisted
    of only faces and not natural images. So, we have no other choice but to run our
    data through two models, the first to extract faces and the second to perform
    expression recognition on the extracted faces, as shown in this diagram:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种方法不仅限于语音识别，我们还看到它被应用于图像识别任务以及其他许多任务。但是，有两件事使这个特定案例与众不同；第一是我们可以通过首先提取面部来简化问题。这意味着我们的模型需要学习的特征更少，提供了一个更小、更专业的模型，我们可以对其进行调整。第二件事，无疑很明显，是我们的训练数据仅由面部组成，而不是自然图像。因此，我们别无选择，只能将我们的数据通过两个模型运行，第一个用于提取面部，第二个用于对提取的面部进行表情识别，如图所示：
- en: '![](img/c2d9524b-d2e3-40d2-a2e0-323d9fca879f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2d9524b-d2e3-40d2-a2e0-323d9fca879f.png)'
- en: Luckily for us, Apple has mostly taken care of our first task of detecting faces
    through the `Vision` framework it released with iOS 11\. The `Vision` framework
    provides performant image analysis and computer vision tools, exposing them through
    a simple API. This allows for face detection, feature detection and tracking,
    and classification of scenes in images and video. The latter (expression recognition)
    is something we will take care of using the Core ML model introduced earlier.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于我们的第一个任务——通过iOS 11发布的`Vision`框架来检测面部——苹果公司已经为我们处理得差不多了。《Vision》框架提供了高性能的图像分析和计算机视觉工具，通过简单的API暴露出来。这使得面部检测、特征检测和跟踪，以及图像和视频中的场景分类成为可能。后者（表情识别）是我们将使用之前介绍过的Core
    ML模型来处理的。
- en: Prior to the introduction of the `Vision` framework, face detection would typically
    be performed using the Core Image filter. Going back further, you had to use something
    like OpenCV. You can learn more about Core Image here: [https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html](https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入`Vision`框架之前，面部检测通常是通过Core Image过滤器来完成的。更早之前，你需要使用类似OpenCV的工具。你可以在这里了解更多关于Core
    Image的信息： [https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html](https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html).
- en: 'Now that we have got a bird''s-eye view of the work that needs to be done,
    let''s turn our attention to the editor and start putting all of this together.
    Start by loading the images; add the following snippet to your playground:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对需要完成的工作有了鸟瞰图，让我们将注意力转向编辑器，开始将这些内容组合起来。首先加载图片；将以下代码片段添加到你的playground中：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding snippet, we are simply loading each of the images we have
    included in our resources'' `Images` folder and adding them to an array we can
    access conveniently throughout the playground. Once all the images are loaded,
    we set the constant `faceIdx`, which will ensure that we access the same images
    throughout our experiments. Finally, we create an `ImageView` to easily preview
    it. Once it has finished running, click on the eye icon in the right-hand panel
    to preview the loaded image, as shown in the following screenshot:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们只是简单地加载我们资源文件夹`Images`中包含的每一张图片，并将它们添加到一个我们可以方便地在整个playground中访问的数组中。一旦所有图片都加载完毕，我们设置一个常量`faceIdx`，这将确保我们在整个实验中访问相同的图片。最后，我们创建一个`ImageView`来轻松预览它。一旦运行完成，点击右侧面板中的眼睛图标来预览加载的图片，如图所示：
- en: '![](img/b30cea31-9392-46f5-a018-527fa1fdb3b5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b30cea31-9392-46f5-a018-527fa1fdb3b5.png)'
- en: 'Next, we will take advantage of the functionality available in the `Vision`
    framework to detect faces. The typical flow when working with the `Vision` framework
    is **defining a request**, which determines what analysis you want to perform,
    and **defining the handler**, which will be responsible for executing the request
    and providing means of obtaining the results (either through delegation or explicitly
    queried). The result of the analysis is a collection of observations that you
    need to cast into the appropriate observation type; concrete examples of each
    of these can be seen here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将利用`Vision`框架中提供的功能来检测面部。使用`Vision`框架时的典型流程是**定义一个请求**，这决定了你想要执行哪种分析，以及**定义处理程序**，它将负责执行请求并提供获取结果的方式（通过代理或显式查询）。分析的结果是一系列观察结果，你需要将它们转换为适当的观察类型；每个这些的具体示例可以在以下内容中看到：
- en: As illustrated in the preceding diagram, the request determines what type of
    image analysis will be performed; the handler, using a request or multiple requests
    and an image, performs the actual analysis and generates the results (also known
    as **observations**). These are accessible via a property or delegate if one has
    been assigned. The type of observation is dependent on the request performed;
    it's worth highlighting that the `Vision` framework is tightly integrated into
    Core ML and provides another layer of abstraction and uniformity between you and
    the data and process. For example, using a classification Core ML model would
    return an observation of type `VNClassificationObservation`. This layer of abstraction
    not only simplifies things but also provides a consistent way of working with
    machine learning models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述图表所示，请求确定将执行哪种类型的图像分析；处理器使用请求或多个请求以及图像执行实际分析并生成结果（也称为**观察结果**）。这些结果可以通过属性或代理访问，如果已经分配的话。观察的类型取决于执行的请求；值得注意的是，`Vision`框架紧密集成到Core
    ML中，并在您与数据和过程之间提供另一层抽象和一致性。例如，使用分类Core ML模型将返回类型为`VNClassificationObservation`的观察结果。这层抽象不仅简化了事情，还提供了一种与机器学习模型一致工作的方法。
- en: 'In the previous figure, we showed a request handler specifically for static
    images. `Vision` also provides a specialized request handler for handling sequences
    of images, which is more appropriate when dealing with requests such as tracking.
    The following diagram illustrates some concrete examples of the types of requests
    and observations applicable to this use case:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们展示了专门用于静态图像的请求处理器。`Vision`还提供了一个专门用于处理图像序列的请求处理器，这在处理如跟踪等请求时更为合适。以下图表展示了适用于此用例的一些具体请求和观察类型示例：
- en: '![](img/baf94581-ad26-4ebd-9bf2-0c2575426b4a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/baf94581-ad26-4ebd-9bf2-0c2575426b4a.png)'
- en: So, when do you use `VNImageRequestHandler` and `VNSequenceRequestHandler`?
    Though the names provide clues as to when one should be used over the other, it's
    worth outlining some differences.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，何时使用`VNImageRequestHandler`和`VNSequenceRequestHandler`？尽管名称提供了何时使用一个而不是另一个的线索，但概述一些差异是值得的。
- en: The image request handler is for interactive exploration of an image; it holds
    a reference to the image for its life cycle and allows optimizations of various
    request types. The sequence request handler is more appropriate for performing
    tasks such as tracking and does not optimize for multiple requests on an image.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图像请求处理器用于交互式探索图像；它在其生命周期内保持对图像的引用，并允许优化各种请求类型。序列请求处理器更适合执行如跟踪等任务，并且不对图像上的多个请求进行优化。
- en: 'Let''s see how this all looks in code; add the following snippet to your playground:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这一切在代码中的样子；将以下片段添加到您的游乐场中：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we are simply creating the request and handler; as discussed in the preceding
    code, the request encapsulates the type of image analysis while the handler is
    responsible for executing the request. Next, we will get `faceDetectionRequestHandler`
    to run `faceDetectionRequest`; add the following code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是创建请求和处理程序；如前述代码所述，请求封装了图像分析的类型，而处理器负责执行请求。接下来，我们将`faceDetectionRequestHandler`用于运行`faceDetectionRequest`；添加以下代码：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `perform` function of the handler can throw an error if it fails; for this
    reason, we wrap the call with `try?` at the beginning of the statement and can
    interrogate the `error` property of the handler to identify the reason for failing.
    We pass the handler a list of requests (in this case, only our `faceDetectionRequest`),
    the image we want to perform the analysis on, and, finally, the orientation of
    the image that can be used by the request during analysis.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器的`perform`函数在失败时可能会抛出错误；因此，我们在语句的开头用`try?`包裹调用，并且可以查询处理器的`error`属性来识别失败的原因。我们向处理器传递一个请求列表（在本例中，只有我们的`faceDetectionRequest`），我们想要进行分析的图像，以及，最后，图像在分析期间可以使用的方向。
- en: 'Once the analysis is done, we can inspect the observation obtained through
    the `results` property of the request itself, as shown in the following code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分析完成，我们就可以通过请求本身的`results`属性来检查获得的观察结果，如下所示：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The type of observation is dependent on the analysis; in this case, we're expecting
    a `VNFaceObservation`. Hence, we cast it to the appropriate type and then iterate
    through all the observations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 观察的类型取决于分析；在这种情况下，我们期待一个`VNFaceObservation`。因此，我们将其转换为适当的类型，然后遍历所有观察结果。
- en: 'Next, we will take each recognized face and extract the bounding box. Then,
    we''ll proceed to draw it in the image (using an extension method of `UIImageView`
    found within the `UIImageViewExtension.swift` file). Add the following block within
    the `for` loop shown in the preceding code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对每个识别到的脸部提取边界框。然后，我们将继续在图像中绘制它（使用在`UIImageViewExtension.swift`文件中找到的`UIImageView`扩展方法）。在前面代码中显示的`for`循环内添加以下代码块：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can obtain the bounding box of each face via the let `boundingBox` property;
    the result is normalized, so we then need to scale this based on the dimensions
    of the image. For example, you can obtain the width by multiplying `boundingBox`
    with the width of the image: `bbox.width * imageSize.width`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`boundingBox`属性获取每个脸部的边界框；结果是归一化的，因此我们需要根据图像的尺寸进行缩放。例如，你可以通过将`boundingBox`与图像的宽度相乘来获取宽度：`bbox.width
    * imageSize.width`。
- en: 'Next, we invert the *y *axis as the coordinate system of Quartz 2D is inverted
    with respect to that of UIKit''s coordinate system, as shown in this diagram:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将反转y轴，因为Quartz 2D的坐标系与UIKit的坐标系相反，如图所示：
- en: '![](img/0d810f17-d915-44c5-98f3-557955090048.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0d810f17-d915-44c5-98f3-557955090048.png)'
- en: 'We invert our coordinates by subtracting the bounding box''s origin and height
    from height of the image and then passing this to our `UIImageView` to render
    the rectangle. Click on the eye icon in the right-hand panel in line with the
    statement `imageView.drawRect(rect: invertedFaceRect)` to preview the results;
    if successful, you should see something like the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过从图像的高度中减去边界框的起点和高度来反转坐标，然后将这个值传递给我们的`UIImageView`以渲染矩形。点击右面板中与`imageView.drawRect(rect:
    invertedFaceRect)`语句对齐的“眼睛”图标以预览结果；如果成功，你应该看到以下类似的内容：'
- en: '![](img/4e9221cd-d50f-44f4-89d7-1b6bfad5379b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4e9221cd-d50f-44f4-89d7-1b6bfad5379b.png)'
- en: 'An alternative to inverting the face rectangle would be to use an `AfflineTransform`,
    such as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 反转脸部矩形的另一种方法是使用`AfflineTransform`，例如：
- en: '`var transform = CGAffineTransform(scaleX: 1, y: -1)`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`var transform = CGAffineTransform(scaleX: 1, y: -1)`'
- en: '`transform = transform.translatedBy(x: 0, y: -imageSize.height)`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform = transform.translatedBy(x: 0, y: -imageSize.height)`'
- en: '`let invertedFaceRect = faceRect.apply(transform)`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`let invertedFaceRect = faceRect.apply(transform)`'
- en: This approach leads to less code and therefore less chances of errors. So, it
    is the recommended approach. The long approach was taken previously to help illuminate
    the details.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法代码更少，因此出错的机会也更少。所以，这是推荐的方法。之前采用长方法是为了帮助阐明细节。
- en: 'Let''s now take a quick detour and experiment with another type of request;
    this time, we will analyze our image using `VNDetectFaceLandmarksRequest`. It
    is similar to `VNDetectFaceRectanglesRequest` in that this request will detect
    faces and expose their bounding boxes; but, unlike `VNDetectFaceRectanglesRequest`,
    `VNDetectFaceLandmarksRequest` also provides detected facial landmarks. A landmark
    is a prominent facial feature such as your eyes, nose, eyebrow, face contour,
    or any other feature that can be detected and describes a significant attribute
    of a face. Each detected facial landmark consists of a set of points that describe
    its contour (outline). Let''s see how this looks; add a new request as shown in
    the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们快速转换一下，尝试另一种类型的请求；这次，我们将使用`VNDetectFaceLandmarksRequest`来分析我们的图像。它与`VNDetectFaceRectanglesRequest`类似，因为这个请求将检测脸部并暴露其边界框；但是，与`VNDetectFaceRectanglesRequest`不同，`VNDetectFaceLandmarksRequest`还提供了检测到的面部特征点。一个特征点是一个显著的面部特征，如你的眼睛、鼻子、眉毛、面部轮廓或任何其他可检测的特征，它描述了面部的一个重要属性。每个检测到的面部特征点由一组描述其轮廓（轮廓线）的点组成。让我们看看这看起来怎么样；在下面的代码中添加一个新的请求：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding snippet should look familiar to you; it''s almost the same as
    what we did previously, but this time replacing `VNDetectFaceRectanglesRequest`
    with `VNDetectFaceLandmarksRequets`. We have also refreshed the image in our image
    view with the statement `imageView.image = images[faceIdx]`. As we did before,
    let''s iterate through each of the detected observations and extract some of the
    common landmarks. Start off by creating the outer loop, as shown in this code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段应该对你来说很熟悉；它几乎与之前我们做的相同，但这次我们将`VNDetectFaceRectanglesRequest`替换为`VNDetectFaceLandmarksRequets`。我们还使用`imageView.image
    = images[faceIdx]`语句刷新了图像视图。像之前一样，让我们遍历每个检测到的观察结果并提取一些常见的特征点。首先创建外循环，如下面的代码所示：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Up to this point, the code will look familiar; next, we will look at each of
    the landmarks. But first, let''s create a function to handle the transformation
    of our points from the Quartz 2D coordinate system to UIKit''s coordinate system.
    We add the following function but within the same block as our `faceRect` declaration:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，代码看起来很熟悉；接下来，我们将查看每个里程碑。但首先，让我们创建一个函数来处理将我们的点从Quartz 2D坐标系转换为UIKit坐标系的转换。我们在`faceRect`声明相同的块中添加以下函数：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As mentioned before, each landmark consists of a set of points that describe
    the contour of that particular landmark, and, like our previous feature, the points
    are normalized between 0.0 - 1.0\. Therefore, we need to scale them based on the
    associated face rectangle, which is exactly what we did in the preceding example.
    For each point, we are scaling and transforming it into the appropriate coordinate
    space, and then returning the mapped array to the caller.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个里程碑由一组描述该特定里程碑轮廓的点组成，并且，像我们之前的特征一样，这些点在0.0 - 1.0之间归一化。因此，我们需要根据相关的面部矩形对其进行缩放，这正是我们在前面的例子中所做的。对于每个点，我们对其进行缩放和转换，将其转换到适当的坐标系中，然后将映射后的数组返回给调用者。
- en: 'Let''s now define some constants that we will use to visualize each landmark;
    we add the following two constants in the function we implemented just now, `getTransformedPoints`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一些我们将用于可视化每个里程碑的常量；我们在刚刚实现的函数`getTransformedPoints`中添加以下两个常量：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will now step through a few of the landmarks, showing how we extract the
    features and occasionally showing the result. Let''s start with the left eye and
    right eye; continue adding the following code just after the constants you just
    defined:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将逐步展示一些重要里程碑，展示我们如何提取特征，偶尔也会展示结果。让我们从左眼和右眼开始；在您刚刚定义的常量之后立即添加以下代码：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Hopefully, as is apparent from the preceding code snippet, we get a reference
    to each of the landmarks by interrogating the face observations landmark property,
    which itself references the appropriate landmark. In the preceding code, we get reference
    to the landmarks `leftEye` and `rightEye`. And for each, we first render the contour
    of the eye, as shown in this screenshot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 希望从前面的代码片段中可以看出，我们通过查询面部观察的`landmark`属性来获取每个里程碑的引用，该属性本身引用了适当的里程碑。在前面的代码中，我们获取了`leftEye`和`rightEye`里程碑的引用。对于每个里程碑，我们首先绘制眼睛的轮廓，如图所示：
- en: '![](img/11105948-31bf-4caa-8a12-e6371e1fbeac.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11105948-31bf-4caa-8a12-e6371e1fbeac.png)'
- en: 'Next, we iterate through each of the points to find the center of the eye and
    render a circle using the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历每个点以找到眼睛的中心，并使用以下代码绘制一个圆：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is slightly unnecessary as one of the landmarks available is `leftPupil`,
    but I wanted to use this instance to highlight the importance of inspecting the
    available landmarks. The next half of the block is concerned with performing the
    same tasks for the right eye; by the end of it, you should have an image resembling
    something like the following, with both eyes drawn:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这稍微有些不必要，因为可用的里程碑之一是`leftPupil`，但我想要使用这个实例来强调检查可用里程碑的重要性。块的下一半关注于对右眼执行相同的任务；到结束时，您应该得到一个类似以下内容的图像，其中画有双眼：
- en: '![](img/626871f2-e4cf-4623-844e-a85119d87c41.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/626871f2-e4cf-4623-844e-a85119d87c41.png)'
- en: 'Let''s continue highlighting some of the landmarks available. Next, we will
    inspect the face contour and nose; add the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续突出显示一些可用的里程碑。接下来，我们将检查面部轮廓和鼻子；添加以下代码：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The patterns should be obvious now; here we can draw the landmarks `faceContour`,
    `nose`, and `noseCrest`; with that done, your image should look something like
    the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模式现在应该很明显了；在这里，我们可以绘制`faceContour`、`nose`和`noseCrest`这些里程碑；完成这些后，您的图像应该看起来像以下这样：
- en: 'As an exercise, draw the lips (and any other facial landmark) using the landmarks
    `innerLips` and `outerLips`. With that implemented, you should end up with something
    like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，使用`innerLips`和`outerLips`里程碑绘制嘴唇（以及任何其他面部里程碑）。实现这一点后，您应该得到类似以下的内容：
- en: '![](img/7c95fd6d-891b-4152-a42d-3de1698b3c12.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c95fd6d-891b-4152-a42d-3de1698b3c12.png)'
- en: Before returning to our task of classifying facial expressions, let's quickly
    finish our detour with some practical uses for landmark detection (other than
    drawing or placing glasses on a face).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在返回到我们的任务——对面部表情进行分类之前，让我们快速完成关于里程碑检测的一些实际用途的旁白（除了在脸上绘制或放置眼镜之外）。
- en: As highlighted earlier, our training set consists of images that are predominantly
    forward-facing and orientated fairly straight. With this in mind, one practical
    use of knowing the position of each eye is being able to qualify an image; that
    is, is the face sufficiently in view and orientated correctly? Another use would
    be to slightly realign the face so that it fits in better with your training set
    (keeping in mind that our images are reduced to 28 x 28, so some detriment to
    quality can be ignored).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的训练集主要由正面朝前且方向相当直的图像组成。考虑到这一点，了解每个眼睛的位置的一个实际用途是能够对图像进行评估；也就是说，人脸是否足够在视野中并且方向正确？另一个用途是将人脸稍微重新定位，以便更好地与您的训练集匹配（考虑到我们的图像被缩小到28
    x 28，因此可以忽略一些质量损失）。
- en: For now, I'll leave the implementation of these to you but, by using the angle
    between the two eyes, you can apply an affine transformation to correct the orientation,
    that is, rotate the image.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我将这些实现的细节留给您，但通过使用两个眼睛之间的角度，您可以应用仿射变换来纠正方向，即旋转图像。
- en: 'Let''s now return to our main task of classification; as we did before, we
    will create a `VNDetectFaceRectanglesRequest` request to handle identifying each
    face within a given image and, for each face, we will perform some preprocessing
    before feeding it into our model. If you recall our discussion on the model, our
    model is expecting a single-channel (grayscale) image of a face with the size
    48 x 48 and its values normalized between 0.0 and 1.0\. Let''s walk through each
    part of the task piece by piece, starting with creating the request, as we did
    previously:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到主要任务——分类；像之前一样，我们将创建一个`VNDetectFaceRectanglesRequest`请求来处理识别给定图像中的每个面部，并且对于每个面部，我们将在将其输入到我们的模型之前进行一些预处理。如果您还记得我们关于模型的讨论，我们的模型期望一个单通道（灰度）的人脸图像，大小为48
    x 48，其值在0.0和1.0之间归一化。让我们一步一步地通过这个任务的每个部分，从创建请求开始，就像我们之前做的那样：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code should look familiar to you now, with the only difference being the
    instantiation of our model (the bold statement): `let model = ExpressionRecognitionModelRaw()`.
    Next, we want to crop out the face from the image; in order to do this, we will
    need to write a utility function that will implement this. Since we want to carry
    this over to our application, let''s write it as an extension of the `CIImage`
    class. Click on the `CIImageExtension.swift` file within the `Sources` folder
    in the left-hand panel to open up the relevant file; currently, this file is just
    an empty extension body, as shown in the following code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该对前面的代码很熟悉了，唯一的区别在于我们模型的实例化（粗体语句）：`let model = ExpressionRecognitionModelRaw()`。接下来，我们想要从图像中裁剪出人脸；为了做到这一点，我们需要编写一个实用函数来实现这一功能。由于我们希望将其应用到我们的应用程序中，让我们将其编写为`CIImage`类的一个扩展。在左侧面板中点击`Sources`文件夹内的`CIImageExtension.swift`文件以打开相关文件；目前，此文件只是一个空的扩展体，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Go ahead and add the following snippet of code within the body of `CIImage` to
    implement the functionality of cropping:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在`CIImage`的体内添加以下代码片段以实现裁剪功能：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code, we are simply creating a new image of itself constrained
    to the region passed in; this method, `context.createCGImage`, returns a `CGImage`,
    which we then wrap in a `CIImage` before returning to the caller. With our crop
    method taken care of, we return to our main playground source and add the following
    snippet after the face rectangle declared previously to crop a face from our image:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们只是创建了一个新的图像，该图像被限制为传入的区域；这个方法`context.createCGImage`返回一个`CGImage`，然后我们将其包装在`CIImage`中，然后再返回给调用者。在我们的裁剪方法处理完毕后，我们回到主playground源代码，并在之前声明的面部矩形之后添加以下代码片段以从我们的图像中裁剪人脸：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We first create an instance of `CIImage` from `CGImage` (referenced by the `UIImage`
    instance); we then pad out our face rectangle. The reason for doing this is to
    better match it with our training data; if you refer to our previous experiments,
    the detected bounds fit tightly around the eyes and chin while our training data
    encompasses a more holistic view of the face. The numbers selected were through
    trial and error, but I imagine there is some statistically relevant ratio between
    the distance between the eyes and height of the face—maybe. We finally crop our
    image using the `crop` method we implemented earlier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从`CGImage`（由`UIImage`实例引用）创建一个`CIImage`实例；然后我们填充我们的面部矩形。这样做的原因是为了更好地匹配我们的训练数据；如果你参考我们的先前实验，检测到的边界紧密地围绕着眼睛和下巴，而我们的训练数据则包含了一个更全面的视图。选定的数字是通过试错得到的，但我猜想眼睛之间的距离和面部高度之间可能存在某种统计相关的比率——也许吧。我们最后使用之前实现的`crop`方法裁剪我们的图像。
- en: 'Next, we will resize the image (to the size the model is expecting) but, once
    again, this functionality is not yet available. So, our next task! Jump back into
    the `CIImageExtension.swift` file and add the following method to handle resizing:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将图像（调整到模型期望的大小）进行缩放，但，同样，这个功能目前还没有可用。所以，我们的下一个任务！回到`CIImageExtension.swift`文件，并添加以下方法来处理缩放：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You may notice that we are not inverting the face rectangle here as we did before;
    the reason is that we were only required to do this to transform from the Quartz
    2D coordinate system to UIKit's coordinate system, which we are not doing here.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，我们在这里没有像之前那样反转面部矩形；原因是我们之前只要求这样做，以便从Quartz 2D坐标系转换到UIKit坐标系，而我们在这里没有这样做。
- en: 'Despite the number of lines, the majority of the code is concerned with calculating
    the scale and translation required to center it. Once we have calculated these,
    we simply pass in a `CGAffineTransform`, with our scale, to the `transformed`
    method and then our centrally aligned rectangle to the `clamped` method. With
    this now implemented, let''s return to our main playground code and make use of
    it by resizing our cropped image, as shown in the following lines:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码行数很多，但大部分代码都是关于计算使图像居中所需的缩放和转换。一旦我们计算出这些值，我们只需将一个包含我们的缩放值的`CGAffineTransform`传递给`transformed`方法，然后将居中对齐的矩形传递给`clamped`方法。现在这个功能已经实现，让我们回到主playground代码，通过以下行来使用它，对裁剪后的图像进行缩放：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Three more steps are required before we can pass our data to our model for
    inference. The first is to convert it to a single channel, the second is to rescale
    the pixels so that they are between the values of 0.0 and 1.0, and finally we
    wrap it in a `MLMultiArray`, which we can then feed into our model''s `predict`
    method. To achieve the previous, we will add another extension to our `CIImage`
    class. It will render out the image using a single channel, along with extracting
    the pixel data and returning it in an array, which we can then easily access for
    rescaling. Jump back into the `CIImageExtension.swift` file and add the following
    method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以将数据传递给模型进行推理之前，还需要三个步骤。第一步是将它转换为单通道，第二步是将像素重新缩放，使它们的值在0.0和1.0之间，最后我们将它包裹在一个`MLMultiArray`中，然后我们可以将其喂给模型的`predict`方法。为了实现这一点，我们将向我们的`CIImage`类添加另一个扩展。它将使用单通道渲染图像，同时提取像素数据，并以数组的形式返回，这样我们就可以轻松地访问它进行缩放。回到`CIImageExtension.swift`文件，并添加以下方法：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once again, don''t be intimidated by the amount of code; there are two main
    tasks this method does. The first is rendering out the image to a `CVPixelBuffer`
    using a single channel, grayscale. To highlight this, the code responsible is
    shown in the following block:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，不要被代码的数量吓倒；这个方法主要完成两个任务。第一个任务是将图像渲染到使用单通道灰度的`CVPixelBuffer`中。为了突出这一点，负责的代码如下所示：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We render the image to a `CVPixelBuffer` to provide a convenient way for us
    to access the raw pixels that we can then use to populate our array. We then return
    this to the caller. The main chunk of code that is responsible for this is shown
    here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将图像渲染到`CVPixelBuffer`中，为我们提供了一个方便的方式来访问原始像素，然后我们可以使用这些像素来填充我们的数组。然后我们将这个结果返回给调用者。负责这一过程的代码主要部分如下所示：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we first determine the dimensions by obtaining the width and height of
    our image, using `CVPixelBufferGetWidth` and `CVPixelBufferGetHeight` respectively.
    Then we use these to create an appropriately sized array to hold the pixel data.
    We then obtain the base address of our `CVPixelBuffer` and call its `assumingMemoryBound`
    method to give us a typed pointer. We can use this to access each pixel, which
    we do to populate our `pixelData` array before returning it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先通过使用`CVPixelBufferGetWidth`和`CVPixelBufferGetHeight`分别获取图像的宽度和高度来确定维度。然后我们使用这些信息创建一个适当大小的数组来存储像素数据。然后我们获取`CVPixelBuffer`的基址并调用它的`assumingMemoryBound`方法来给我们一个类型指针。我们可以使用这个指针来访问每个像素，我们这样做是为了在返回之前填充`pixelData`数组。
- en: 'With your `getGrayscalePixelData` method now implemented, return to the main
    source of the playground and resume where you left off by adding the following
    code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经实现了`getGrayscalePixelData`方法，回到沙盒的主源代码，继续你之前中断的地方，添加以下代码：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding snippet, we are obtaining the raw pixels of our cropped image
    using our `getGrayscalePixelData` method, before rescaling them by dividing each
    pixel by 255.0 (the maximum value). Our final task of preparation is putting our
    data into a data structure that our model will accept, a `MLMultiArray`. Add the
    following code to do just this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用`getGrayscalePixelData`方法获取我们裁剪图像的原始像素，在将每个像素除以255.0（最大值）之前进行缩放。我们准备工作的最后任务是把我们数据放入模型可以接受的数据结构中，即`MLMultiArray`。添加以下代码来完成这个任务：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We start by creating an instance of `MLMultiArray` with the shape of our input
    data and then proceed to copy across our standardized pixel data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个具有输入数据形状的`MLMultiArray`实例，然后继续复制我们的标准化像素数据。
- en: 'With our model instantiated and data prepared, we can now perform inference
    using the following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型实例化和数据准备就绪后，我们现在可以使用以下代码进行推理：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Previously, we dispatched inference on a background thread then printed out
    all probabilities of each class to the console. With that now complete, run your
    playground, and if everything is working fine, you should get something like the
    following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此前，我们在后台线程上进行了推理，然后将每个类别的所有概率打印到控制台。现在，完成这个任务后，运行你的沙盒，如果一切正常，你应该会得到以下类似的结果：
- en: '| Angry  | 0.0341557003557682 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 生气  | 0.0341557003557682 |'
- en: '| Happy | 0.594196200370789 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 快乐 | 0.594196200370789 |'
- en: '| Disgust | 2.19011440094619e-06 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 厌恶 | 2.19011440094619e-06 |'
- en: '| Sad | 0.260873317718506 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 悲伤 | 0.260873317718506 |'
- en: '| Fear | 0.013140731491148 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 恐惧 | 0.013140731491148 |'
- en: '| Surprise | 0.000694742717314512 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 惊讶 | 0.000694742717314512 |'
- en: '| Neutral | 0.0969370529055595 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 中立 | 0.0969370529055595 |'
- en: 'As a designer and builder of intelligent systems, it is your task to interpret
    these results and present them to the user. Some questions you''ll want to ask
    yourself are as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 作为智能系统的设计师和构建者，你的任务是解释这些结果并将它们呈现给用户。以下是一些你可能想要问自己的问题：
- en: What is an acceptable threshold of a probability before setting the class as
    true?
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将类别设置为真之前，一个可接受的概率阈值是多少？
- en: Can this threshold be dependent on probabilities of other classes to remove
    ambiguity? That is, if **Sad** and **Happy** have a probability of 0.3, you can
    infer that the prediction is inaccurate, or at least not useful.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个阈值是否可以依赖于其他类的概率来消除歧义？也就是说，如果**悲伤**和**快乐**的概率为0.3，你可以推断出预测是不准确的，或者至少不是很有用。
- en: Is there a way to accept multiple probabilities?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有接受多个概率的方法？
- en: Is it useful to expose the threshold to the user and have it manually set and/or
    tune it?
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否应该向用户公开阈值，并允许他们手动设置和/或调整它？
- en: These are only a few questions you should ask. The specific questions, and their
    answers, will depend on your use case and users. At this point, we have everything
    we need to preprocess and perform inference; let's now turn our attention to the
    application for this chapter.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是你应该问的几个问题。具体问题及其答案将取决于你的用例和用户。到目前为止，我们已经拥有了预处理和进行推理所需的一切；现在让我们将注意力转向本章的应用。
- en: 'If you find that you are not getting any output, it could be that you need
    to flag the playground as running indefinitely so that it doesn''t exit before
    running the background thread. You can do this by adding the following statement
    in your playground: `PlaygroundPage.current.needsIndefiniteExecution = true`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现没有任何输出，可能是因为你需要将沙盒标记为无限运行，这样它就不会在运行后台线程之前退出。你可以在你的沙盒中添加以下语句来实现这一点：`PlaygroundPage.current.needsIndefiniteExecution
    = true`
- en: When this is set to `true`, you will need to explicitly stop the playground.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个设置为`true`时，你需要明确停止沙盒。
- en: Bringing it all together
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: If you haven't done already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter4/Start/FacialEmotionDetection`
    and open the project `FacialEmotionDetection.xcodeproj`. Once loaded, you will
    hopefully recognize the project structure as it closely resembles our first example.
    For this reason, we will just concentrate on the main components that are unique
    for this project, and I suggest reviewing previous chapters for clarification
    on anything that is unclear.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有做的话，请从附带的存储库中拉取最新的代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载后，导航到目录`Chapter4/Start/FacialEmotionDetection`并打开项目`FacialEmotionDetection.xcodeproj`。一旦加载，您可能会希望识别出项目结构，因为它与我们第一个例子非常相似。因此，我们将只关注这个项目特有的主要组件，我建议您回顾前面的章节，以澄清任何不清楚的地方。
- en: 'Let''s start by reviewing our project and its main components; your project
    should look similar to what is shown in the following screenshot:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先回顾一下我们的项目和其主要组件；您的项目应该看起来与以下截图所示相似：
- en: '![](img/d3332a21-5276-4a59-9ba0-9780f0ca27b5.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d3332a21-5276-4a59-9ba0-9780f0ca27b5.png)'
- en: As shown in the preceding screenshot, the project looks a lot like our previous
    projects. I am going to make an assumption that the classes `VideoCapture`, `CaptureVideoPreviewView`,
    and `UIColorExtension` look familiar and you are comfortable with their contents. `CIImageExtension`
    is what we just implemented in the previous section, and therefore we won't be
    covering it here. The `EmotionVisualizerView` class is a custom view that visualizes
    the outputs from our model. And, finally, we have the bundled `ExpressionRecognitionModelRaw.mlmodel`.
    Our main focus in this section will be on wrapping the functionality we implemented
    in the previous section to handle preprocessing and hooking it up within the `ViewController`
    class. Before we start, let's quickly review what we are doing and consider some
    real-life applications for expression/emotion recognition.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个截图所示，项目看起来与我们的前几个项目非常相似。我将假设类`VideoCapture`、`CaptureVideoPreviewView`和`UIColorExtension`看起来很熟悉，并且您对它们的内
    容感到舒适。`CIImageExtension`是我们之前章节中刚刚实现的，因此我们在这里不会涉及它。`EmotionVisualizerView`类是一个自定义视图，用于可视化我们模型的输出。最后，我们有捆绑的`ExpressionRecognitionModelRaw.mlmodel`。在本节中，我们的主要焦点将是将之前章节中实现的功能进行封装，以处理预处理，并将其在`ViewController`类中连接起来。在我们开始之前，让我们快速回顾一下我们正在做什么，并考虑一些表情/情感识别的实际应用。
- en: In this section, we are building a simple visualization of the detected faces;
    we will pass in our camera feed to our preprocessor, then hand it over to our
    model to perform inference, and finally feed the results to our `EmotionVisualizerView`
    to render the output as an overlay on the screen. It's a simple example but sufficiently
    implements the mechanics required to embed in your own creations. So, what are
    some of its practical uses?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正在构建一个简单的检测到的面部可视化；我们将把我们的摄像头输入传递给我们的预处理程序，然后将其交给我们的模型进行推理，最后将结果传递给我们的`EmotionVisualizerView`以在屏幕上渲染输出。这是一个简单的例子，但足以实现嵌入到您自己的创作中所需的所有机制。那么，它的一些实际用途有哪些呢？
- en: In a broad sense, there are three main uses: **analytical**, **reactive**, and **anticipatory**.
    Analytical is generally what you are likely to hear. These applications typically
    observe reactions by the user in relation to the content being presented; for
    example, you might measure arousal from content observed by the user, which is
    then used to drive future decisions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，有三个主要用途：**分析**、**反应**和**预测**。分析通常是您可能会听到的。这些应用通常观察用户对所呈现内容的反应；例如，您可能会测量用户观察到的唤醒程度，然后将其用于驱动未来的决策。
- en: While analytical experiences remain mostly passive, reactive applications proactively
    adjust the experience based on live feedback. One example that illustrates this
    well is **DragonBot**, a research project from the *Social Robotics Group* at
    MIT exploring intelligent tutoring systems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分析体验主要保持被动，但反应性应用会根据实时反馈主动调整体验。一个很好地说明这一点的例子是来自麻省理工学院*社会机器人小组*的研究项目**DragonBot**，该项目探索智能辅导系统。
- en: DragonBot uses emotional awareness to adapt to the student; for example, one
    of its applications is a reading game that adapts the words based on the recognized
    emotion. That is, the system can adjust the difficulty of the task (words in this
    case) based on the user's ability, determined by the recognized emotion.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: DragonBot使用情感意识来适应学生；例如，其应用之一是一个根据识别到的情感调整单词的阅读游戏。也就是说，系统可以根据用户的能力调整任务的难度（在这种情况下是单词），这种能力是通过识别到的情感确定的。
- en: Finally, we have anticipatory applications. Anticipatory applications are semi-autonomous.
    They proactively try to infer the user's context and predict a likely action,
    therefore adjusting their state or triggering an action. An fictional example
    could be an email client that delays sending messages if the user had composed
    the message when angry.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有预测性应用。预测性应用是半自主的。它们主动尝试推断用户的环境并预测一个可能的行为，因此调整它们的状态或触发一个动作。一个虚构的例子可能是一个电子邮件客户端，如果用户在愤怒时编写了消息，它会延迟发送消息。
- en: 'Hopefully, this highlights some of the opportunities, but for now, let''s return
    to our example and start building out the class that will be responsible for handling
    the preprocessing. Start off by creating a new swift file called `ImageProcess.swift`;
    and, within the file, add the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能突出一些机会，但就目前而言，让我们回到我们的例子，开始构建负责处理预处理的类。首先创建一个新的Swift文件，命名为`ImageProcess.swift`；在文件中，添加以下代码：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we have defined the protocol for the delegate to handle the result once
    the preprocessing has completed, as well as the main class that exposes the method
    for initiating the task. Most of the code we will be using is what we have written
    in the playground; start off by declaring the request and request handler at the
    class level:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为代理定义了处理预处理完成后结果的协议，以及暴露启动任务方法的主体类。我们将使用的绝大部分代码是我们已经在playground中编写的；首先在类级别声明请求和请求处理器：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s now make use of the request by having our handler execute it within
    the body of the `getFaces` method''s background queue dispatch block:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们利用请求，让我们的处理器在`getFaces`方法背景队列调度块的体内执行它：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This should all look familiar to you. We pass in our request and image to the
    image handler. Then, we instantiate an array to hold the data for each face detected
    in the image. Finally, we obtain the observations and start iterating through
    each of them. It''s within this block that we will perform the preprocessing and
    populate our `facesData` array as we had done in the playground. Add the following
    code within the loop:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都应该对您来说都很熟悉。我们将请求和图像传递给图像处理器。然后，我们实例化一个数组来保存图像中检测到的每个面部的数据。最后，我们获得观察结果并开始遍历它们。在这个块中，我们将执行预处理并填充我们的`facesData`数组，就像我们在playground中所做的那样。在循环中添加以下代码：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding block, we obtain the detected face''s bounding box and create
    the cropping bounds, including padding. Our next task will be to crop the face
    from the image, resize it to our target size of 48 x 48, extract the raw pixel
    data along with normalizing the values, and finally populate an `MLMultiArray`.
    This is then added to our `facesData` array to be returned to the delegate; appending
    the following code to your script does just that:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的块中，我们获得了检测到的面部边界框并创建了包括填充在内的裁剪边界。我们的下一个任务将从图像中裁剪面部，将其调整到我们的目标大小48 x 48，提取原始像素数据并对其进行归一化，最后填充一个`MLMultiArray`。然后，这个数组被添加到我们的`facesData`数组中，以便返回给代理；将以下代码添加到您的脚本中即可实现这一点：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Nothing new has been introduced here apart from chaining the methods to make
    it more legible (at least for me). Our final task is to notify the delegate once
    we have finished; add the following just outside the observations loop block:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将方法链接起来以使其更易于阅读（至少对我来说是这样）之外，这里没有引入任何新内容。我们的最终任务是完成通知代理；在观察循环块外部添加以下代码：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, with that complete, our `ImageProcessor` is ready to be used. Let''s hook
    everything up. Jump into the `ViewController` class, where we will hook our `ImageProcessor`.
    We will pass its results to our model and finally pass the output from our model
    to `EmotionVisualizerView` to present the results to the user. Let''s start by
    reviewing what currently exists:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着这一切的完成，我们的`ImageProcessor`已经准备好使用。让我们将其连接起来。跳转到`ViewController`类，我们将在这里连接我们的`ImageProcessor`。我们将将其结果传递给我们的模型，最后将模型的输出传递给`EmotionVisualizerView`以向用户展示结果。让我们首先回顾一下目前存在的内容：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our `ViewController` has references to its IB counterpart, most notably the `previewView`
    and `viewVisualizer`. The former will render the captured camera frames and `viewVisualizer`
    will be responsible for visualizing the output of our model. We then have `videoCapture`,
    which is a utility class that encapsulates setting up, capturing, and tearing
    down the camera. We get access to the captured frames by assigning ourselves as
    the delegate and implement the appropriate protocol as we have done as an extension
    at the bottom.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`ViewController`引用了其IB对应物，最值得注意的是`previewView`和`viewVisualizer`。前者将渲染捕获的相机帧，而`viewVisualizer`将负责可视化模型的输出。然后我们有`videoCapture`，这是一个封装设置、捕获和拆除相机的实用类。我们通过指定自己为代理并实现适当的协议来获取捕获的帧，就像我们在底部作为扩展所做的那样。
- en: 'Let''s begin by declaring the model and `ImageProcessor` variables required
    for our task; add the following at the class level of your `ViewController`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先声明完成任务所需的模型和`ImageProcessor`变量；在`ViewController`的类级别添加以下内容：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we need to assign ourselves as the delegate of `ImageProcessor` in order
    to receive the results once the processing has completed. Add the following statement
    to the bottom of your `viewDidLoad` method:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将自己指定为`ImageProcessor`的代理，以便在处理完成后接收结果。将以下语句添加到`viewDidLoad`方法的底部：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We will return shortly to implement the required protocol; for now, let''s
    make use of our `ImageProcessor` by passing in the frame we receive from the camera.
    Within the `onFrameCaptured` method, we add the following statement, which will
    pass each frame to our `ImageProcessor` instance. It''s shown in bold in the following
    code block:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快返回以实现所需的协议；现在，让我们通过传递从相机接收到的帧来利用我们的`ImageProcessor`。在`onFrameCaptured`方法中，我们添加以下语句，该语句将每个帧传递给我们的`ImageProcessor`实例。以下代码块中已用粗体显示：
- en: '[PRE33]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Our final task will be to implement the `ImageProcessorDelegate` protocol;
    this will be called when our `ImageProcessor` has completed identifying and extracting
    each face for a given camera frame along with performing the preprocessing necessary
    for our model. Once completed, we will pass the data to our model to perform inference,
    and finally pass these onto our `EmotionVisualizerView`. Because nothing new is
    being introduced here, let''s go ahead and add the block in its entirety:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的任务将是实现`ImageProcessorDelegate`协议；当我们的`ImageProcessor`完成对给定相机帧的每个面部识别和提取，以及执行模型所需的预处理时，将调用此协议。一旦完成，我们将数据传递给我们的模型以进行推理，最后将这些数据传递到我们的`EmotionVisualizerView`。因为这里没有引入新的内容，所以让我们直接添加整个块：
- en: '[PRE34]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The only notable thing to point out is that our model needs to perform inference
    in the background thread and our `ImageProcessor` calls its delegate on the main
    thread. For this reason, we dispatch inference to the background and then return
    the results on the main thread—this is necessary whenever you want to update the
    user interface.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们的模型需要在后台线程上进行推理，而`ImageProcessor`在其代理上调用主线程。因此，我们将推理调度到后台，然后在主线程上返回结果——这在你想要更新用户界面时是必要的。
- en: 'With that complete, we are now in a good place to build and deploy to test;
    if all goes well, you should see something like the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们现在处于构建和部署以进行测试的良好位置；如果一切顺利，你应该会看到以下内容：
- en: '![](img/c5daed44-e9db-4000-b067-b433927216f6.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5daed44-e9db-4000-b067-b433927216f6.png)'
- en: Let's wrap up the chapter by reviewing what we have covered and point out some
    interesting areas to explore before moving on to the next chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾我们已经涵盖的内容并指出一些在进入下一章之前值得探索的有趣领域来结束本章。
- en: In this chapter, we have taken a naive approach with respect to processing the
    captured frames; in a commercial application you would want to optimize this process
    such as utilizing **object tracking** from the `Vision` framework to replace explicit
    face detection, which is computationally cheaper.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们采取了天真方法来处理捕获的帧；在商业应用中，您可能希望优化此过程，例如利用`Vision`框架中的**对象跟踪**来替换显式的面部检测，这计算成本更低。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we applied a CNN for the task of recognizing facial expressions.
    Using this, we could infer the emotional state of a given face. As usual, we again
    spent the majority of our time understanding the required input for the model
    and implementing the functionality to facilitate this. But, in doing so, we uncovered
    some important considerations when developing intelligent applications; the first
    is the explicit awareness of using either an end-to-end solution or a multi-step
    approach, with the multi-step approach being the most common one you will use.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们应用了卷积神经网络（CNN）来识别面部表情的任务。利用这一点，我们可以推断出给定面部的情绪状态。像往常一样，我们花费了大部分时间来理解模型所需的输入并实现促进这一功能的特性。但是，在这个过程中，我们发现了开发智能应用时的一些重要考虑因素；第一个是明确意识到使用端到端解决方案或多步骤方法，其中多步骤方法是您最常用的方法。
- en: This essentially means you, the designer and builder of intelligent applications,
    will be building data pipelines consisting of many models, each transforming the
    data in preparation for the next. This is similar to how deep networks work but
    provides greater flexibility. The second consideration is highlighting the availability
    of complementary frameworks available on iOS, in particular the `Vision` framework.
    It was used as one of the steps in our pipeline but offers a lot of convenience
    for common tasks, as well as a consistent workflow.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上意味着你，作为智能应用的设计师和构建者，将构建由许多模型组成的数据管道，每个模型都在为下一个步骤转换数据。这与深度网络的工作方式类似，但提供了更大的灵活性。第二个考虑因素是强调iOS上可用的互补框架的可用性，特别是`Vision`框架。它被用作我们流程中的一步，但为常见任务提供了很多便利，以及一致的流程。
- en: 'In this example, our pipeline consisted of only two steps, face detection and
    then emotion recognition. But we also briefly played with a feature of the `Vision`
    framework that can be used to identify facial landmarks. So, it is plausible to
    consider facial landmarks to train the emotional classifier rather than the raw
    pixels, in which case our pipeline would consist of three steps: face detection,
    landmark detection, and, finally, emotion recognition.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的流程只包含两个步骤：面部检测和情绪识别。但我们还简要地尝试了`Vision`框架的一个功能，该功能可以用来识别面部特征点。因此，考虑使用面部特征点来训练情绪分类器而不是原始像素是合理的。在这种情况下，我们的流程将包括三个步骤：面部检测、特征点检测，最后是情绪识别。
- en: Finally, we briefly explored some use cases showing how emotion recognition
    could be applied; as our computers shift away from being pure tools towards being
    companions, being able to detect and react to the emotional state of the user
    will become increasingly more important. So, it's an area well worth further exploring.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要探讨了几个用例，展示了情绪识别如何被应用；随着我们的计算机从纯工具向伴侣转变，能够检测和反应用户的情绪状态将变得越来越重要。因此，这是一个值得进一步探索的领域。
- en: In the next chapter, we will introduce the concept of transfer learning and
    how we can use it to transfer styles from one image onto another.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍迁移学习的概念以及我们如何利用它将一种图像的风格转移到另一种图像上。
