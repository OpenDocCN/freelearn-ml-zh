- en: Ensembles – When One Model Is Not Enough
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法——当一个模型不足以应对时
- en: In the previous three chapters, we saw how **neural networks** help directly
    and indirectly in solving natural language understanding and image processing
    problems. This is because neural networks are proven to work well with **homogeneous
    data**; that is, if all the input features are of the same breed—pixels, words,
    characters, and so on. On the other hand, when it comes to **heterogeneous****data**,
    it is the **ensemble****methods** that are known to shine. They are well suited
    to deal with heterogeneous data—for example, where one column contains users'
    ages, the other has their incomes, and a third has their city of residence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的三章中，我们看到**神经网络**如何直接或间接地帮助解决自然语言理解和图像处理问题。这是因为神经网络已被证明能够很好地处理**同质数据**；即，如果所有输入特征属于同一类——像素、单词、字符等。另一方面，当涉及到**异质数据**时，**集成方法**被认为能够发挥优势。它们非常适合处理异质数据——例如，一列包含用户的年龄，另一列包含他们的收入，第三列包含他们的居住城市。
- en: You can view ensemble estimators as meta-estimators; they are made up of multiple
    instances of other estimators. The way they combine their underlying estimators
    is what differentiates between the different ensemble methods—for example, the
    **bagging** versus the **boosting** methods. In this chapter, we are going to
    look at these methods in detail and understand their underlying theory. We will
    also learn how to diagnose our own models and understand why they make certain
    decisions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将集成估计器视为元估计器；它们由多个其他估计器的实例组成。它们组合底层估计器的方式决定了不同集成方法之间的差异——例如，**袋装法**与**提升法**。在本章中，我们将详细探讨这些方法，并理解它们的理论基础。我们还将学习如何诊断自己的模型，理解它们为何做出某些决策。
- en: As always, I would also like to seize the opportunity to shed light on general
    machine learning concepts while dissecting each individual algorithm. In this
    chapter, we will see how to handle the estimators' uncertainties using the classifiers'
    probabilities and the regression ranges.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，我还希望借此机会在剖析每个单独的算法时，顺便阐明一些常见的机器学习概念。在本章中，我们将看到如何利用分类器的概率和回归范围来处理估计器的不确定性。
- en: 'The following topics will be discussed in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下内容：
- en: The motivation behind ensembles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法的动机
- en: Averaging/bagging ensembles
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均法/袋装集成方法
- en: Boosting ensembles
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升集成方法
- en: Regression ranges
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归范围
- en: The ROC curve
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC 曲线
- en: Area under the curve
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线下的面积
- en: Voting and stacking ensembles
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票法与堆叠集成方法
- en: Answering the question why ensembles?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回答为什么选择集成方法的问题？
- en: The main idea behind ensembles is to combine multiple estimators so that they
    make better predictions than a single estimator. However, you should not expect
    the mere combination of multiple estimators to just lead to better results. The
    combined predictions of multiple estimators who make the exact same mistakes will
    be as wrong as each individual estimator in the group. Therefore, it is helpful
    to think of the possible ways to mitigate the mistakes that individual estimators
    make. To do so, we have to revisit our old friend the bias and variance dichotomy.
    We will meet few machine learning teachers better than this pair.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法背后的主要思想是将多个估计器结合起来，使它们的预测效果比单一估计器更好。然而，你不应该仅仅期待多个估计器的简单结合就能带来更好的结果。多个估计器的预测组合如果犯了完全相同的错误，其结果会与每个单独的估计器一样错误。因此，考虑如何减轻单个估计器所犯的错误是非常有帮助的。为此，我们需要回顾一下我们熟悉的偏差-方差二分法。很少有机器学习的老师比这对概念更能帮助我们了。
- en: If you recall from [Chapter 2](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml),
    *Making Decisions with Trees*, when we allowed our decision trees to grow as much
    as they can, they tended to fit the training data like a glove but failed to generalize
    to newer data points. We referred to this as overfitting, and we have seen the
    same behavior with unregularized linear models and with a small number of nearest
    neighbors. Conversely, aggressively restricting the growth of trees, limiting
    the number of the features in linear models, and asking too many neighbors to
    vote caused the models to become biased and underfit the data at hand. So, we
    had to tread a thin line between trying to find the optimum balance between the
    bias-variance and the underfitting-overfitting dichotomies.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得[第2章](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml)《用树做决策》，当我们允许决策树尽可能生长时，它们往往会像手套一样拟合训练数据，但无法很好地推广到新的数据点。我们称之为过拟合，在线性模型和少量最近邻的情况下也看到了相同的行为。相反，严格限制树的生长，限制线性模型中的特征数量，或者要求太多邻居投票，都会导致模型偏向并且拟合不足。因此，我们必须在偏差-方差和拟合不足-过拟合的对立之间找到最佳平衡。
- en: In the following sections, we are going to follow a different approach. We will
    deal with the bias-variance dichotomy as a continuous scale, starting from one
    side of this scale and using the concept of *ensemble* to move toward the other
    side. In the next section, we are going to start by looking at high-variance estimators
    and averaging their results to reduce their variance. Later on, we will start
    from the other side and use the concept of boosting to reduce the estimators'
    biases.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将采取一种不同的方法。我们将把偏差-方差的对立看作一个连续的尺度，从这个尺度的一端开始，并利用*集成*的概念向另一端推进。在下一节中，我们将从高方差估计器开始，通过平均它们的结果来减少它们的方差。随后，我们将从另一端开始，利用提升的概念来减少估计器的偏差。
- en: Combining multiple estimators via averaging
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过平均结合多个估计器
- en: '"To derive the most useful information from multiple sources of evidence, you
    should always try to make these sources independent of each other."'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '"为了从多个证据源中提取最有用的信息，你应该始终尝试使这些来源彼此独立。"'
- en: – Daniel Kahneman
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: –丹尼尔·卡尼曼
- en: If a single fully grown decision tree overfits, and if having many voters in
    the nearest neighbors algorithm has an opposite effect, then why not combine the
    two concepts? Rather than having a single tree, let's have a forest that combines
    the predictions of each tree in it. Nevertheless, we do not want all the trees
    in our forest to be identical; we would love them to be as diverse as possible.
    The **bagging** and random forest meta-estimators are the most common examples
    here. To achieve diversity, they make sure that each one of the individual estimators
    they use is trained on a random subset of the training data—hence the *random*
    prefix in random forest. Each time a random sample is drawn, it can be done with
    replacement (**bootstrapping**) or without replacement (**pasting**). The term
    bagging stands for **bootstrap aggregation** as the estimators draw their samples
    with replacement. Furthermore, for even more diversity, the ensembles can assure
    that each tree sees a random subset of the training features.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果单棵完全生长的决策树发生过拟合，并且在最近邻算法中，增加投票者数量产生相反的效果，那么为什么不将这两个概念结合起来呢？与其拥有一棵树，不如拥有一片森林，将其中每棵树的预测结果结合起来。然而，我们并不希望森林中的所有树都是相同的；我们希望它们尽可能多样化。**袋装**和随机森林元估计器就是最常见的例子。为了实现多样性，它们确保每个单独的估计器都在训练数据的随机子集上进行训练——因此在随机森林中有了*随机*这个前缀。每次抽取随机样本时，可以进行有放回抽样（**自助法**）或无放回抽样（**粘贴法**）。术语袋装代表**自助法聚合**，因为估计器在抽样时是有放回的。此外，为了实现更多的多样性，集成方法还可以确保每棵树看到的训练特征是随机选择的子集。
- en: Both ensembles use decision tree estimators by default, but the **bagging**
    ensemble can be reconfigured to use any other estimator. Ideally, we would like
    to use high-variance estimators. The decisions made by the individual estimators
    are combined via voting or averaging.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种集成方法默认使用决策树估计器，但**袋装**集成方法可以重新配置为使用其他任何估计器。理想情况下，我们希望使用高方差估计器。各个估计器做出的决策通过投票或平均来结合。
- en: Boosting multiple biased estimators
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升多个有偏估计器
- en: '"If I have seen further than others, it is by standing upon the shoulders of
    giants."'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '"如果我看得比别人更远，那是因为我站在巨人的肩膀上。"'
- en: –Isaac Newton
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: –艾萨克·牛顿
- en: In contrast to fully grown trees, a shallow tree tends to be biased. Boosting
    a biased estimator is commonly performed via **AdaBoost** or **gradient boosting**.
    The AdaBoost meta-estimator starts with a weak or biased estimator, then each
    consequent estimator learns from the mistakes made by its predecessors. We saw
    in [Chapter 2](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml), *Making Decisions
    with Trees*, that we can give each individual training sample a different weight
    so that the estimators can give more emphasis to some samples versus others. In
    **AdaBoost**, erroneous predictions made by the preceding estimators are given
    more weight for their successors to pay more attention to.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与完全生长的树相比，浅层树往往会产生偏差。提升一个偏差估计器通常通过**AdaBoost**或**梯度提升**来实现。AdaBoost 元估计器从一个弱估计器或偏差估计器开始，然后每一个后续估计器都从前一个估计器的错误中学习。我们在[第
    2 章](66742a94-deba-4899-9f6b-1c17d0f6bf7e.xhtml)《使用决策树做决策》中看到过，我们可以给每个训练样本分配不同的权重，从而让估计器对某些样本给予更多关注。
    在**AdaBoost**中，前一个估计器所做的错误预测会赋予更多的权重，以便后续的估计器能够更加关注这些错误。
- en: The **gradient boosting** meta-estimator follows a slightly different approach.
    It starts with a biased estimator, computes its loss function, then builds each
    consequent estimator to minimize the loss function of its predecessors. As we
    saw earlier, gradient descent always comes in handy when iteratively minimizing
    loss functions, hence the *gradient* prefix in the name of the gradient boosting
    algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升**元估计器采用了稍微不同的方法。它从一个偏差估计器开始，计算其损失函数，然后构建每个后续估计器以最小化前一个估计器的损失函数。正如我们之前所看到的，梯度下降法在迭代最小化损失函数时非常有用，这也是**梯度提升**算法名称中“梯度”前缀的由来。'
- en: Due to the iterative nature of the two ensembles, they both have a learning
    rate to control their learning speed and to make sure they don't miss the local
    minima when converging. Like the **bagging** algorithm, **AdaBoost** is not limited
    to decision trees as its base estimator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两种集成方法都是迭代性质的，它们都有一个学习率来控制学习速度，并确保在收敛时不会错过局部最小值。像**自助法**算法一样，**AdaBoost**
    并不局限于使用决策树作为基本估计器。
- en: Now that we have a good idea about the different ensemble methods, we can use
    real-life data to demonstrate how they work in practice. Each of the ensemble
    methods described here can be used for classification and regression. The classifier
    and regressor hyperparameters are almost identical for each ensemble. Therefore,
    I will pick a regression problem to demonstrate each algorithm and briefly show
    the classification capabilities of the random forest and gradient boosting algorithms
    since they are the most commonly used ensembles.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对不同的集成方法有了一个大致了解，接下来可以使用真实的数据来演示它们在实际中的应用。这里描述的每个集成方法都可以用于分类和回归。分类器和回归器的超参数对于每个集成都几乎是相同的。因此，我将选择一个回归问题来演示每个算法，并简要展示随机森林和梯度提升算法的分类能力，因为它们是最常用的集成方法。
- en: In the next section, we are going to download a dataset prepared by the**University
    of California, Irvine** (**UCI**). It contains 201 samples for different cars,
    along with their prices. We will be using this dataset in a later section to predict
    the car prices via regression.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将下载由**加利福尼亚大学欧文分校**（**UCI**）准备的数据集。它包含了 201 个不同汽车的样本以及它们的价格。我们将在后面的章节中使用该数据集通过回归预测汽车价格。
- en: Downloading the UCI Automobile dataset
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载 UCI 汽车数据集
- en: 'The Automobile dataset was created by Jeffrey C. Schlimmer and published in
    UCI''s machine learning repository. It contains information about 201 automobiles,
    along with their prices. The names of the features are missing. Nevertheless,
    I could get them from the dataset''s description ([http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names](http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names)).
    So, we can start by seeing the URL and the feature names, as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车数据集由 Jeffrey C. Schlimmer 创建并发布在 UCI 的机器学习库中。它包含了 201 辆汽车的信息以及它们的价格。特征名称缺失，不过我可以从数据集的描述中找到它们（[http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names](http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names)）。因此，我们可以先查看
    URL 和特征名称，如下所示：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we use the following code to download our data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码来下载我们的数据。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is mentioned in the dataset's description that missing values are replaced
    with a question mark. To make things more Pythonic, we set `na_values` to `'?'`
    to replace these question marks with NumPy's **Not a Number**(**NaN**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集描述中提到，缺失值被替换为问号。为了让代码更符合 Python 风格，我们将`na_values`设置为`'?'`，用 NumPy 的**不是数字**（**NaN**）替换这些问号。
- en: Next, we can perform our **Exploratory Data Analysis** (**EDA**), check the
    percentages of the missing values, and see how to deal with them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以进行**探索性数据分析**（**EDA**），检查缺失值的百分比，并查看如何处理它们。
- en: Dealing with missing values
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'Now, we can check which columns have the most missing values:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查哪些列缺失值最多：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This gives us the following list:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下列表：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since the price is our target value, we can just ignore the four records where
    the prices are unknown:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于价格是我们的目标值，我们可以忽略那些价格未知的四条记录：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As for the remaining features, I''d say let''s drop the`normalized-losses`
    column since 41 of its values are missing. Later on, we will use the data imputation
    techniques to deal with the other columns with fewer missing values. You can drop
    the `normalized-losses` column using the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剩余的特征，我认为我们可以删除`normalized-losses`列，因为其中有41个值是缺失的。稍后，我们将使用数据插补技术处理其他缺失值较少的列。你可以使用以下代码删除`normalized-losses`列：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: At this point, we have a data frame with all the required features and their
    names. Next, we want to split the data into training and test sets, and then prepare
    our features. The different feature types require different preparations. You
    may need to separately scale the numerical features and encode the categorical
    ones. So, it is good practice to be able to differentiate between the numerical
    and the categorical features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经有了一个包含所有必要特征及其名称的数据框。接下来，我们想将数据拆分为训练集和测试集，然后准备特征。不同的特征类型需要不同的准备工作。你可能需要分别缩放数值特征并编码类别特征。因此，能够区分数值型特征和类别型特征是一个很好的实践。
- en: Differentiating between numerical features and categorical ones
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分数值特征和类别特征
- en: 'Here, we are going to create a dictionary to separately list the numerical
    and categorical features. We will also make a combined list of the two, and provide
    the name of the target column, as in the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将创建一个字典，分别列出数值型和类别型特征。我们还将这两者合并为一个列表，并提供目标列的名称，如以下代码所示：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By doing so, you can deal with the columns differently. Furthermore, just for
    my own sanity and to notprint too many zeros in the future, I rescaled the prices
    to be in thousands, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，你可以以不同的方式处理列。此外，为了保持理智并避免将来打印过多的零，我将价格重新缩放为千元，如下所示：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can also display certain features separately. Here, we print a random sample,
    where just the categorical features are shown:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以单独显示某些特征。在这里，我们打印一个随机样本，仅显示类别特征：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here are the resulting rows. I set `random_state` to `42` to make sure we all
    get the same random rows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的行。我将`random_state`设置为`42`，确保我们得到相同的随机行：
- en: '![](img/806e2ee8-cfdf-4819-b53d-a58a4a38dd61.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/806e2ee8-cfdf-4819-b53d-a58a4a38dd61.png)'
- en: All other transformations, such as scaling, imputing, and encoding, should be
    done after splitting the data into training and test sets. That way, we can ensure
    that no information is leaked from the test set into the training samples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他转换，如缩放、插补和编码，都应该在拆分数据集为训练集和测试集之后进行。这样，我们可以确保没有信息从测试集泄漏到训练样本中。
- en: Splitting the data into training and test sets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集
- en: 'Here, we keep 25% of the data for testing and use the rest for training:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们保留25%的数据用于测试，其余的用于训练：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we can use the information from the previous section to create our `x`
    and `y` values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用前面部分的信息创建我们的`x`和`y`值：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As usual, with regression tasks, it is handy to understand the distribution
    of the target values:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，对于回归任务，了解目标值的分布是很有用的：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A histogram is usually a good choice for understanding distributions, as seen
    in the following graph:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图通常是理解分布的一个好选择，如下图所示：
- en: '![](img/7e1e4eaa-47ca-4d98-a27d-299eb324a237.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e1e4eaa-47ca-4d98-a27d-299eb324a237.png)'
- en: We may come back to this distribution later to put our regressor's mean error
    in perspective. Additionally, you can use this range for sanity checks. For example,
    if you know that all the prices you have seen fall in the range of 5,000 to 45,000,
    you may decide when to put your model in production to fire an alert any time
    it returns prices far from this range.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会稍后回到这个分布，来将回归模型的平均误差放到合理的范围内。此外，你还可以使用这个范围进行合理性检查。例如，如果你知道所有看到的价格都在5,000到45,000之间，你可能会决定在将模型投入生产时，如果模型返回的价格远离这个范围，就触发警报。
- en: Imputing the missing values and encoding the categorical features
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充缺失值并编码类别特征
- en: 'Before bringing our ensembles to action, we need to make sure we do not have
    null values in our data. We will replace the missing values with the most frequent
    value in each column using the `SimpleImputer` function from[Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml),
    *Preparing Your Data*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用我们的集成方法之前，我们需要确保数据中没有空值。我们将使用来自[第4章](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)《准备数据》的`SimpleImputer`函数，用每列中最常见的值来替换缺失值：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You may have already seen me complain many times about the scikit-learn transformers,
    which do not respect the column names and insist on converting the input data
    frames into NumPy arrays. To stop myself from complaining again, let me solve
    my itch by using the following `ColumnNamesKeeper` class. Whenever I wrap it around
    a transformer, it will make sure all the data frames are kept unharmed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经看到我多次抱怨scikit-learn的转换器，它们不尊重列名，并且坚持将输入数据框转换为NumPy数组。为了不再抱怨，我通过使用以下`ColumnNamesKeeper`类来解决我的痛点。每当我将它包装在转换器周围时，它会确保所有的数据框都保持不变：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, it mainly saves the column name when the `fit` method is called.
    Then, we can use the saved names to recreate the data frames after the transformation
    steps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它主要在调用`fit`方法时保存列名。然后，我们可以使用保存的列名在变换步骤后重新创建数据框。
- en: The code for `ColumnNamesKeeper` can be simplified further by inheriting from
    `sklearn.base.BaseEstimator` and `sklearn.base.TransformerMixin`. You can check
    the source code of any of the library's built-in transformers if you are willing
    to write more scikit-learn-friendly transformers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`ColumnNamesKeeper`的代码可以通过继承`sklearn.base.BaseEstimator`和`sklearn.base.TransformerMixin`来进一步简化。如果你愿意编写更符合scikit-learn风格的转换器，可以查看该库内置转换器的源代码。'
- en: 'Now, I can call `SimpleImputer` again while preserving `x_train` and `x_test`
    as data frames:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以再次调用`SimpleImputer`，同时保持`x_train`和`x_test`作为数据框：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We learned in [Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml), *Preparing
    Your Data*, that `OrdinalEncoder`**is recommended for tree-based algorithms, in
    addition to any other non-linear algorithms. The `category_encoders` library doesn't
    mess with the column names, and so we can use `OrdinalEncoder` without the need
    for`ColumnNamesKeeper` this time. In the following code snippet, we also specify
    which columns to encode (the categorical columns) and which to keep unchanged
    (the remaining ones):**
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第4章](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)《准备数据》中学到，`OrdinalEncoder`**推荐用于基于树的算法，此外还适用于任何其他非线性算法。`category_encoders`库不会改变列名，因此我们这次可以直接使用`OrdinalEncoder`，无需使用`ColumnNamesKeeper`。在以下代码片段中，我们还指定了要编码的列（类别列）和保持不变的列（其余列）：**
- en: '**[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE15]'
- en: In addition to `OrdinalEncoder`, you can also test the encoders mentioned in
    the target encoding in [Chapter 4](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)*,
    Preparing Your Data*. They, too, are meant to be used with the algorithms explained
    in this chapter. In the next section, we are going to use the random forest algorithm
    with the data we have just prepared.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`OrdinalEncoder`，你还可以测试[第4章](f97d5d65-e61e-4f65-9b83-1ac4d6a54a77.xhtml)《准备数据》中提到的目标编码器*。它们同样适用于本章中解释的算法。在接下来的部分，我们将使用随机森林算法来处理我们刚准备好的数据。
- en: Using random forest for regression
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行回归
- en: 'The random forest algorithm is going to be the first ensemble to deal with
    here. It''s an easy-to-grasp algorithm with straightforward hyperparameters. Nevertheless,
    as we usually do, we will start by training the algorithm using its default values,
    as follows, then explain its hyperparameters after that:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法将是我们首先要处理的集成方法。它是一个容易理解的算法，具有直接的超参数设置。尽管如此，我们通常的做法是先使用默认值训练算法，如下所示，然后再解释其超参数：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since each tree is independent of the others, I set `n_jobs` to `-1` to use
    my multiple processors to train the trees in parallel. Once they are trained and
    the predictions are obtained, we can print the following accuracy metrics:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每棵树相互独立，我将`n_jobs`设置为`-1`，以利用多个处理器并行训练树木。一旦它们训练完成并获得预测结果，我们可以打印出以下的准确度指标：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will print the following scores:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下分数：
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The average car price is 13,400\. So, a **Mean Absolute Error** (**MAE***)*
    of `1.35` seems reasonable. As for the **Mean Squared Error** (**MSE**), it makes
    sense to use its square root to keep it in the same units as the MAE. In brief,
    given the high R² score and the low errors, the algorithm seems to perform well
    with its default values. Furthermore, you can plot the errors to get a better
    understanding of the model''s performance:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 平均汽车价格为13,400。因此，**平均绝对误差**（**MAE**）为`1.35`是合理的。至于**均方误差**（**MSE**），将其平方根作为度量单位比较更为合适，以与MAE保持一致。简而言之，鉴于高R²分数和较低的误差，算法在默认值下表现良好。此外，你可以绘制误差图，进一步了解模型的表现：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'I''ve excluded some of the formatting lines to keep the code concise. In the
    end, we get the following graphs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持代码简洁，我省略了一些格式化行。最后，我们得到以下图表：
- en: '![](img/8d5c005a-a4d0-47be-b420-cdd3984f4ac7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d5c005a-a4d0-47be-b420-cdd3984f4ac7.png)'
- en: By plotting the predictions versus the actuals, we can make sure that the models
    don't systematically overestimate or underestimate. This is shown via the 45^o
    slope of the scattered points on the left. A lower slope for the scattered points
    would have systematically reflected an underestimation. Having the scattered points
    aligned on a straight line assures us that there aren't non-linearities that the
    model couldn't capture. The histogram to the right shows that most of the errors
    are below 2,000\. It is good to understand what mean and maximum errors you can
    expect to get in the future.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制预测值与实际值的对比图，我们可以确保模型不会系统性地高估或低估。这一点通过左侧散点的45度斜率得到了体现。散点斜率较低会系统性地反映低估。如果散点在一条直线上的分布，意味着模型没有遗漏任何非线性因素。右侧的直方图显示大多数误差低于2,000。了解未来可以预期的平均误差和最大误差是很有帮助的。
- en: Checking the effect of the number of trees
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查树木数量的影响
- en: By default, each tree is trained on a random sample from the training data.
    This is achieved by setting the `bootstrap` hyperparameter to `True`. In bootstrap
    sampling, a sample may be used during training more than once, while another sample
    may not be used at all.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每棵树都会在训练数据的随机样本上进行训练。这是通过将`bootstrap`超参数设置为`True`实现的。在自助采样中，某些样本可能会在训练中被使用多次，而其他样本可能根本没有被使用。
- en: When `max_samples` is kept as `None`, each tree is trained on a random sample
    of a size that is equal to the entire training data size. You can set `max_samples`
    to a fraction that is less than 1, then each tree is trained on a smaller random
    sub-sample. Similarly, we can set `max_features` to a fraction that is less than
    1 to make sure each tree uses a random subset of the available features. These
    parameters help each tree to have its own personality and to ensure the diversity
    of the forest. To put it more formally, these parameters increase the variance
    of each individual tree. So, it is advised to have as many trees as possible to
    reduce the variance we have just introduced.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当`max_samples`设置为`None`时，每棵树都会在一个随机样本上训练，样本的大小等于整个训练数据的大小。你可以将`max_samples`设置为小于1的比例，这样每棵树就会在一个更小的随机子样本上训练。同样，我们可以将`max_features`设置为小于1的比例，以确保每棵树使用可用特征的随机子集。这些参数有助于让每棵树具有自己的“个性”，并确保森林的多样性。更正式地说，这些参数增加了每棵树的方差。因此，建议尽可能增加树木的数量，以减少我们刚刚引入的方差。
- en: 'Here, we compare three forests, with a different number of trees in each:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们比较了三片森林，每片森林中树木的数量不同：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we can plot the MAE for each forest to see the merits of having more
    trees:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制每个森林的MAE，以查看增加树木数量的优点：
- en: '![](img/476a677f-6df2-4dea-8fa5-d6a7545117d0.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/476a677f-6df2-4dea-8fa5-d6a7545117d0.png)'
- en: Clearly, we have just encountered a new set of hyperparameters to tune `bootstrap`,
    `max_features`, and `max_samples`. So, it makes sense to apply cross-validation
    for hyperparameter tuning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们刚刚遇到了需要调优的新的超参数集`bootstrap`、`max_features`和`max_samples`。因此，进行交叉验证来调整这些超参数是有意义的。
- en: Understanding the effect of each training feature
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解每个训练特征的影响
- en: 'Once a random forest is trained, we can list the training features, along with
    their importance. As usual, we put the outcome in a data frame by using the column
    names and the `feature_importances_` attribute, as shown:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦随机森林训练完成，我们可以列出训练特征及其重要性。通常情况下，我们通过使用列名和`feature_importances_`属性将结果放入数据框中，如下所示：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the resulting data frame:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的数据框：
- en: '![](img/5c526a3c-1ceb-4c52-87a5-4a83419a4181.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c526a3c-1ceb-4c52-87a5-4a83419a4181.png)'
- en: 'Unlike with linear models, all the values here are positive. This is because
    these values only show the importance of each feature, regardless of whether it
    is positively or negatively correlated with the target. This is common for decision
    trees, as well as for tree-based ensembles. Thus, we can use **Partial Dependence
    Plots**(**PDPs**)to show the relationship between the target and the different
    features. Here, we only plot it for the top six features according to their importance:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '与线性模型不同，这里的所有值都是正的。这是因为这些值仅显示每个特征的重要性，无论它们与目标的正负相关性如何。这对于决策树以及基于树的集成模型来说是常见的。因此，我们可以使用**部分依赖图**（**PDPs**）来展示目标与不同特征之间的关系。在这里，我们仅针对按重要性排名前六的特征绘制图表： '
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting graphs are easier to read, especially when the relationship between
    the target and the features is non-linear:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表更易于阅读，特别是当目标与特征之间的关系是非线性时：
- en: '![](img/b3eadf45-3663-4c33-8c33-b1c380f5ddf6.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3eadf45-3663-4c33-8c33-b1c380f5ddf6.png)'
- en: We can now tell that cars with bigger engines, more horsepower, and less mileage
    per gallon tend to be more expensive.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看出，具有更大引擎、更多马力和每加仑油耗更少的汽车往往更昂贵。
- en: PDPs are not just useful for ensemble methods, but also for any other complex
    non-linear model. Despite the fact the neural networks have coefficients for each
    layer, the PDP is essential in understanding the network as a whole. Furthermore,
    you can also understand the interaction between the different feature pairs by
    passing the list of features as a list tuples, with a pair of features in each
    tuple.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: PDP不仅对集成方法有用，对于任何其他复杂的非线性模型也很有用。尽管神经网络对每一层都有系数，但PDP对于理解整个网络至关重要。此外，您还可以通过将特征列表作为元组列表传递，每个元组中有一对特征，来理解不同特征对之间的相互作用。
- en: Using random forest for classification
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行分类
- en: 'To demonstrate the random forest classifier, we are going to use a synthetic
    dataset. We first create the dataset using the built-in `make_hastie_10_2` class:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示随机森林分类器，我们将使用一个合成数据集。我们首先使用内置的`make_hastie_10_2`类创建数据集：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This previous code snippet creates a random dataset. I set `random_state` to
    a fixed number to make sure we both get the same random data. Now, we can split
    the resulting data into training and test sets:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段创建了一个随机数据集。我将`random_state`设置为一个固定的数，以确保我们获得相同的随机数据。现在，我们可以将生成的数据分为训练集和测试集：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, to evaluate the classifier, we are going to introduce a new concept called
    the **Receiver Operating Characteristic** (**ROC**) curve in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了评估分类器，我们将在下一节介绍一个名为**接收者操作特征曲线**（**ROC**）的新概念。
- en: The ROC curve
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC曲线
- en: '"Probability is expectation founded upon partial knowledge. A perfect acquaintance
    with all the circumstances affecting the occurrence of an event would change expectation
    into certainty, and leave neither room nor demand for a theory of probabilities."'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '"概率是建立在部分知识上的期望。对事件发生影响的所有情况的完全了解会把期望转变为确定性，并且不会留下概率理论的需求或空间。"'
- en: – George Boole (Boolean data types are named after him)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: – 乔治·布尔（布尔数据类型以他命名）
- en: In a classification problem, the classifier assigns probabilities to each sample
    to reflect how likely it is that each sample belongs to a certain class. We get
    these probabilities via the classifier's `predict_proba()` method. The `predict()`
    method is typically a wrapper on top of the `predict_proba()` method. In a binary-classification
    problem, it assigns each sample to a specific class if the probability of it belonging
    to the class is above 50%. In practice, we may not always want to stick to this
    50% threshold, especially as different thresholds usually change the **T****rue
    Positive Rates** (**TPRs**) and **False Positive Rates** (**FPRs**) for each class.
    So, you can choose a different threshold to optimize for a desired TPR.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，分类器会为每个样本分配一个概率值，以反映该样本属于某一类别的可能性。我们通过分类器的`predict_proba()`方法获得这些概率值。`predict()`方法通常是`predict_proba()`方法的封装。在二分类问题中，如果样本属于某个类别的概率超过50%，则将其分配到该类别。实际上，我们可能并不总是希望遵循这个50%的阈值，尤其是因为不同的阈值通常会改变**真正阳性率**(**TPRs**)和**假阳性率**(**FPRs**)在每个类别中的表现。因此，你可以选择不同的阈值来优化所需的TPR。
- en: 'The best way to decide which threshold suits your needs is to use a ROC curve.
    This helps us see the TPR and FPR for each threshold. To create this curve, we
    will train our random forest classifier on the synthetic dataset we have just
    created, but get the classifier''s probabilities this time:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的方法来决定哪个阈值适合你的需求是使用ROC曲线。这有助于我们看到每个阈值下的TPR和FPR。为了创建这个曲线，我们将使用我们刚创建的合成数据集来训练我们的随机森林分类器，但这次我们会获取分类器的概率值：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we can calculate the TPR and FPR for each threshold, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按以下方式计算每个阈值的TPR和FPR：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s stop for a moment to explain what TPR and FPR mean:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们停下来稍作解释，看看TPR和FPR是什么意思：
- en: The**TPR**, also known as **recall** or **sensitivity**, is calculated as the
    number of **True Positive** (**TP**) cases divided by all the positive cases;
    that is, ![](img/d14315f8-a807-4550-9467-be22f09a947b.png), where *FN* is the
    positive cases falsely classified as negative (false negatives).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TPR**，也叫做**召回率**或**敏感度**，计算方法是**真正阳性**(**TP**)案例的数量除以所有正类案例的数量；即，！[](img/d14315f8-a807-4550-9467-be22f09a947b.png)，其中*FN*是被错误分类为负类的正类案例（假阴性）。'
- en: The **True Negative Rates** (**TNR**), also known as **specificity**, is calculated
    as the number of **True Negative** (**TN**) cases divided by all the negative
    cases; that is, ![](img/edcaf90b-1764-47ed-84bc-e42e16bfc779.png), where *FP*
    is the negative cases falsely classified as positive (false positives).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正阴性率**(**TNR**)，也叫做**特异度**，计算方法是**真正阴性**(**TN**)案例的数量除以所有负类案例的数量；即，！[](img/edcaf90b-1764-47ed-84bc-e42e16bfc779.png)，其中*FP*是被错误分类为正类的负类案例（假阳性）。'
- en: The **FPR** is defined as 1 minus TNR; that is, ![](img/a73fab77-1996-4d38-88fc-669a544b5249.png).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FPR**定义为1减去TNR，也就是！[](img/a73fab77-1996-4d38-88fc-669a544b5249.png)。'
- en: The **False Negative Rate** (**FNR**) is defined as 1 minus TPR; that is, ![](img/1d8f94e7-78b4-4197-8feb-678071889616.png).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性率**(**FNR**)定义为1减去TPR；即，！[](img/1d8f94e7-78b4-4197-8feb-678071889616.png)。'
- en: 'Now, we can put the calculated TPR and FPR for our dataset into the following
    table:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们计算出的TPR和FPR放入以下表格中：
- en: '![](img/0abc76af-774e-4e2d-99b5-294fb69ab66c.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0abc76af-774e-4e2d-99b5-294fb69ab66c.png)'
- en: 'Even better than a table, we can plot them into a graph using the following
    code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 比表格更好的是，我们可以使用以下代码将其绘制成图表：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'I''ve omitted the graph''s styling code for the sake of brevity. I also added
    a 45^o line and the **Area Under the Curve** (**AUC**), which I will explain in
    a bit:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我省略了图表的样式代码。我还添加了一个45°的线条和**曲线下面积**(**AUC**)，稍后我会解释这个概念：
- en: '![](img/c989db9d-5d8a-4d47-a9a1-8d3b031676ed.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c989db9d-5d8a-4d47-a9a1-8d3b031676ed.png)'
- en: 'A classifier that randomly assigns each sample to a certain class will have
    a ROC curve that looks like the dashed 45^o line. Any improvement over this will
    make the curve more convex upward. Obviously, random forest''s ROC curve is better
    than chance. An optimum classifier will touch the upper-left corner of the graph.
    Therefore, the AUC can be used to reflect how good the classifier is. An area
    above `0.5` is better than chance, and an area of `1.0` is the best possible value.
    We typically expect values between `0.5` and `1.0`. Here, we got an AUC of `0.94`.
    The AUC can be calculated using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机将每个样本分配到某个类别的分类器，其ROC曲线将像虚线的45度线一样。任何在此基础上的改进都会使曲线更向上凸起。显然，随机森林的ROC曲线优于随机猜测。一个最佳分类器将触及图表的左上角。因此，AUC可以用来反映分类器的好坏。`0.5`以上的区域比随机猜测好，`1.0`是最佳值。我们通常期待的AUC值在`0.5`到`1.0`之间。在这里，我们得到了`0.94`的AUC。AUC可以使用以下代码来计算：
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can also use the ROC and AUC to compare two classifiers. Here, I trained
    the random forest classifier with the `bootstrap` hyperparameter set to `True`
    and compared it to the same classifier when `bootstrap` was set to `False`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用ROC和AUC来比较两个分类器。在这里，我训练了`bootstrap`超参数设置为`True`的随机森林分类器，并将其与`bootstrap`设置为`False`时的相同分类器进行了比较：
- en: '![](img/0457044f-36f5-4ba0-a1ab-4ae6c68f4a9b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0457044f-36f5-4ba0-a1ab-4ae6c68f4a9b.png)'
- en: 'No wonder the `bootstrap` hyperparameter is set to `True` by default—it gives
    better results. Now, you have seen how to use random forest algorithms to solve
    classification and regression problems. In the next section, we are going to explain
    a similar ensemble: the bagging ensemble.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 难怪`bootstrap`超参数默认设置为`True`——它能提供更好的结果。现在，你已经看到如何使用随机森林算法来解决分类和回归问题。在下一节中，我们将解释一个类似的集成方法：包外集成方法。
- en: Using bagging regressors
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用包外回归器
- en: We will go back to the Automobile dataset as we are going to use the **bagging
    regressor** this time. The bagging meta-estimator is very similar to random forest.
    It is built of multiple estimators, each one trained on a random subset of the
    data using a bootstrap sampling method. The key difference here is that although
    decision trees are used as the base estimators by default, any other estimator
    can be used as well. Out of curiosity, let's use the **K-Nearest Neighbors** (**KNN**)
    regressor as our base estimator this time. However, we need to prepare the data
    to suit the new regressor's needs.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回到汽车数据集，因为这次我们将使用**包外回归器**。包外元估计器与随机森林非常相似。它由多个估计器构成，每个估计器都在数据的随机子集上进行训练，使用自助采样方法。这里的关键区别是，虽然默认情况下使用决策树作为基本估计器，但也可以使用任何其他估计器。出于好奇，这次我们将**K-最近邻**（**KNN**）回归器作为我们的基本估计器。然而，我们需要准备数据，以适应新回归器的需求。
- en: Preparing a mixture of numerical and categorical features
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数值和类别特征的混合
- en: It is recommended to put all features on the same scale when using distance-based
    algorithms such as KNN*.* Otherwise, the effect of the features with higher magnitudes
    on the distance metric will overshadow the other features. As we have a mixture
    of numerical and categorical features here, we need to create two parallel pipelines
    to prepare each feature set separately.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于距离的算法（如KNN）时，建议将所有特征放在相同的尺度上。否则，具有更大量级的特征对距离度量的影响将会掩盖其他特征的影响。由于我们这里有数值和类别特征的混合，因此我们需要创建两个并行的管道，分别准备每个特征集。
- en: 'Here is a top-level view of our pipeline:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们管道的顶层视图：
- en: '![](img/306f62b0-7a03-40b8-bbd2-30a85bba31f4.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/306f62b0-7a03-40b8-bbd2-30a85bba31f4.png)'
- en: 'Here, we start by building the four transformers in our pipelines: `Imputer`,
    `Scaler`**,** and `OneHotEncoder`. We also wrap them in `ColumnNamesKeeper`, which
    we created earlier in this chapter:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先构建管道中的四个变换器：`Imputer`、`Scaler`**、**和`OneHotEncoder`。我们还将它们包装在`ColumnNamesKeeper`中，这是我们在本章前面创建的：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we put them into two parallel pipelines:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将它们放入两个并行的管道中：
- en: '[PRE30]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we concatenate the outputs of the pipelines for both the training
    and the test sets:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练集和测试集的管道输出连接起来：
- en: '[PRE31]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: At this point, we are ready to build our bagged KNNs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们准备好构建我们的包外KNN。
- en: Combining KNN estimators using a bagging meta-estimator
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用包外元估计器结合KNN估计器
- en: '`BaggingRegressor` has a `base_estimator` hyperparameter, where you can set
    the estimators you want to use. Here, `KNeighborsRegressor` is used with a single
    neighbor. Since we are aggregating multiple estimators to reduce their variance,
    it makes sense to have a high variance estimator in the first place, hence the
    small number of neighbors here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaggingRegressor` 有一个 `base_estimator` 超参数，你可以在其中设置你想要使用的估算器。这里，`KNeighborsRegressor`
    与一个单一邻居一起使用。由于我们是通过聚合多个估算器来减少它们的方差，因此一开始就使用高方差的估算器是合理的，因此这里邻居的数量较少：'
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This new setup gives us an MAE of `1.8`. We can stop here, or we may decide
    to improve the ensemble's performance by tuning its big array of hyperparameters.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新设置给我们带来了 `1.8` 的 MAE。我们可以在这里停下来，或者我们可以决定通过调整一系列超参数来改进集成的性能。
- en: First of all, we can try different estimators other than KNN, each with its
    own hyperparameters. Then, the bagging ensemble also has its own hyperparameters.
    We can change the number of estimators via `n_estimators`. Then, we can decide
    whether to use the entire training set or a random subset of it for each estimator
    via `max_samples`. Similarly, we can also pick a random subset of the columns
    to use for each estimator to use via `max_features`. The choice of whether to
    use bootstrapping for the rows and the columns can be made via the `bootstrap`
    and `bootstrap_features`hyperparameters, respectively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以尝试不同的估算器，而不是 KNN，每个估算器都有自己的超参数。然后，Bagging 集成也有自己的超参数。我们可以通过 `n_estimators`
    来更改估算器的数量。然后，我们可以通过 `max_samples` 来决定是否对每个估算器使用整个训练集或其随机子集。同样，我们也可以通过 `max_features`
    来选择每个估算器使用的列的随机子集。是否对行和列使用自助抽样，可以通过 `bootstrap` 和 `bootstrap_features` 超参数分别来决定。
- en: Finally, since each estimator is trained separately, we can use a machine with
    a high number of CPUs and parallelize the training process by setting `n_jobs`
    to `-1`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于每个估算器都是单独训练的，我们可以使用具有大量 CPU 的机器，并通过将 `n_jobs` 设置为 `-1` 来并行化训练过程。
- en: Now that we have experienced two versions of the averaging ensembles, it is
    time to check their boosting counterparts. We will start with the gradient boosting
    ensemble, then move to the AdaBoost ensemble.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经体验了两种平均集成方法，是时候检查它们的提升法对应方法了。我们将从梯度提升集成开始，然后转到 AdaBoost 集成。
- en: Using gradient boosting to predict automobile prices
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度提升法预测汽车价格
- en: If I were ever stranded on a desert island and had to pick one algorithm to
    take with me, I'd definitely chose the gradient boosting ensemble! It has proven
    to work very well on many classification and regression problems. We are going
    to use it with the same automobile data from the previous sections. The classifier
    and the regressor versions of this ensemble share the exact same hyperparameters,
    except for the loss functions they use. This means that everything we are going
    to learn here will be useful to us whenever we decide to use gradient boosting
    ensembles for classification.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我被困在一个荒岛上，只能带一个算法，我一定会选择梯度提升集成！它已经在许多分类和回归问题中证明了非常有效。我们将使用与之前章节相同的汽车数据。该集成的分类器和回归器版本共享完全相同的超参数，唯一不同的是它们使用的损失函数。这意味着我们在这里学到的所有知识都将在我们决定使用梯度提升集成进行分类时派上用场。
- en: 'Unlike the averaging ensembles we have seen so far, the boosting ensembles
    build their estimators iteratively. The knowledge learned from the initial ensemble
    is used to build its successors. This is the main downside of boosting ensembles,
    where parallelism is unfeasible. Putting parallelism aside, this iterative nature
    of the ensemble calls for a learning rate to be set. This helps the gradient descent
    algorithm reach the loss function''s minima easily. Here, we use 500 trees, each
    with a maximum of 3 nodes, and a learning rate of `0.01`. Furthermore, the **Least
    Squares** (**LS**) loss is used here; think MSE. More on the available loss functions
    in a moment:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今看到的平均集成方法不同，提升法集成是迭代地构建估算器的。从初始集成中学到的知识被用于构建后继的估算器。这是提升法集成的主要缺点，无法实现并行化。将并行化放在一边，这种迭代的特性要求设置一个学习率。这有助于梯度下降算法更容易地达到损失函数的最小值。这里，我们使用
    500 棵树，每棵树最多 3 个节点，并且学习率为 `0.01`。此外，这里使用的是**最小二乘法**（**LS**）损失，类似于均方误差（MSE）。稍后会详细介绍可用的损失函数：
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This new algorithm gives us the following performance on the test set:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新算法在测试集上的表现如下：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, this setting gave a lower MSE compared to random forest, while
    random forest had a better MAE. Another loss function that the gradient boosting**regressor
    can use is **Least Absolute Deviation** (**LAD**); think MAE, this time. LAD may
    help when dealing with outliers, and it can sometimes reduce the model's MAE performance
    on the test set. Nevertheless, it did not improve the MAE for the dataset at hand.
    We also have a percentile (quantile) loss, but before going deeper into the supported
    loss functions, we need to learn how to diagnose the learning process.**
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个设置相比随机森林给出了更低的MSE，而随机森林则有更好的MAE。梯度提升回归器还可以使用另一种损失函数——**最小绝对偏差**（**LAD**），这类似于MAE。LAD在处理异常值时可能会有所帮助，并且有时能减少模型在测试集上的MAE表现。然而，它并没有改善当前数据集的MAE表现。我们还有一个百分位数（分位数）损失函数，但在深入了解支持的损失函数之前，我们需要先学会如何诊断学习过程。**
- en: '**The main hyperparameters to set here are the number of trees, the depth of
    the trees, the learning rate, and the loss function. As a rule of thumb, you should
    aim for a higher number of trees and a low learning rate. As we will see in a
    bit, these two hyperparameters are inversely proportional to each other. Controlling
    the depth of your trees is purely dependent on your data. In general, we need
    to have shallow trees and let boosting empower them. Nevertheless, the depth of
    the tree controls the number of feature interactions we want to capture. In a
    stub (a tree with a single split), only one feature can be learned at a time.
    A deeper tree resembles a nested `if` condition where a few more features are
    at play each time. I usually start with`max_depth` set to around `3` and `5` and
    tune it along the way.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**这里需要设置的主要超参数包括树的数量、树的深度、学习率和损失函数。根据经验法则，应该设定更高的树的数量和较低的学习率。正如我们稍后会看到的，这两个超参数是相互反比的。控制树的深度完全取决于你的数据。一般来说，我们需要使用较浅的树，并通过提升法增强它们的效果。然而，树的深度控制着我们希望捕捉到的特征交互的数量。在一个树桩（只有一个分裂的树）中，每次只能学习一个特征。较深的树则类似于嵌套的`if`条件，每次有更多的特征参与其中。我通常会从`max_depth`设定为大约`3`和`5`开始，并在后续调整。**'
- en: Plotting the learning deviance
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制学习偏差图
- en: With each additional estimator, we expect the algorithm to learn more and the
    loss to decrease. Yet, at some point, the additional estimators will keep overfitting
    on the training data while not offering much improvement for the test data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每次添加估计器，我们预计算法会学习得更多，损失也会减少。然而，在某个时刻，额外的估计器将继续对训练数据进行过拟合，而对测试数据的改进不大。
- en: To have a clear picture, we need to plot the calculated loss with each additional
    estimator for both the training and test sets. As for the training loss, it is
    saved by the gradient boosting meta-estimator into its `loss_` attribute. For
    the test loss, we can use the meta-estimator's `staged_predict()` methods. This
    method can be used for a given dataset to make predictions for each intermediate
    iteration.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地了解情况，我们需要将每次添加的估计器计算出的损失绘制出来，分别针对训练集和测试集。对于训练损失，梯度提升元估计器会将其保存在`loss_`属性中。对于测试损失，我们可以使用元估计器的`staged_predict()`方法。该方法可以用于给定数据集，在每次中间迭代时进行预测。
- en: 'Since we have multiple loss functions to choose from, gradient boosting also
    provides a`loss_()` method, which calculates the loss for us based on the loss
    function used. Here, we create a new function to calculate the training and test
    errors for each iteration and put them into a data frame:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有多种损失函数可以选择，梯度提升还提供了一个`loss_()`方法，根据所用的损失函数计算损失。在这里，我们创建了一个新函数，用于计算每次迭代的训练和测试误差，并将它们放入数据框中：
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Since we are going to use an LS loss here, you can simply replace the `estimator.loss_()`
    method with `mean_squared_error()` and get the exact same results. But let's keep
    the `estimator.loss_()` function for a more versatile and reusable code.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们这里将使用最小二乘损失（LS loss），你可以简单地用`mean_squared_error()`方法替代`estimator.loss_()`，得到完全相同的结果。但为了代码的更高灵活性和可重用性，我们保留`estimator.loss_()`函数。
- en: 'Next, we train our gradient boosting regressor, as usual:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们像往常一样训练我们的梯度提升回归模型：
- en: '[PRE36]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we use the trained model, along with the test set, to plot the training
    and test learning deviance:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用训练好的模型和测试集，绘制训练和测试的学习偏差：
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Running the code gives us the following graph:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码会得到如下图表：
- en: '![](img/b3779f19-026c-41c1-b736-d59e91380aa9.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3779f19-026c-41c1-b736-d59e91380aa9.png)'
- en: The beauty of this graph is that it tells us that the improvements on the test
    set stopped after `120` estimators or so, despite the continuous improvement in
    the training set; that is, it started to overfit. Furthermore, we can use this
    graph to understand the effect of a chosen learning rate, as we did in *[Chapter
    7](7559b34f-080c-485d-b4bc-5f22580fc1d1.xhtml)*, *Neural Networks - Here comes
    the Deep Learning*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图的美妙之处在于，它告诉我们测试集上的改进在大约`120`个估计器后停止了，尽管训练集上的改进持续不断；也就是说，它开始过拟合了。此外，我们可以通过这张图理解所选学习率的效果，就像我们在*[第7章](7559b34f-080c-485d-b4bc-5f22580fc1d1.xhtml)*，《神经网络
    - 深度学习来临》中所做的那样。
- en: Comparing the learning rate settings
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较学习率设置
- en: 'Rather than training one model, we will train three gradient boosting regressors
    this time, each with a different learning rate. Then, we will plot the deviance
    graph for each one side by side, as shown:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不会只训练一个模型，而是训练三个梯度提升回归模型，每个模型使用不同的学习率。然后，我们将并排绘制每个模型的偏差图，如下所示：
- en: '![](img/6e63c903-27fd-44ab-a4d1-315f8b478d9a.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e63c903-27fd-44ab-a4d1-315f8b478d9a.png)'
- en: As with other gradient descent-based models, a high learning rate causes the
    estimator to overshoot and miss the local minima. We can see this in the first
    graph where no improvements are seen despite the consecutive iterations. The learning
    rates in the second and third graphs seem reasonable. In comparison, the learning
    rate in the third graph seems to be too slow for the model to converge in 500
    iterations. You may then decide to increase the number of estimators for the third
    model to allow it to converge.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他基于梯度下降的模型一样，高学习率会导致估计器过度调整，错过局部最小值。我们可以在第一张图中看到这一点，尽管有连续的迭代，但没有看到改进。第二和第三张图中的学习率看起来合理。相比之下，第三张图中的学习率似乎对于模型在500次迭代内收敛来说太慢了。你可以决定增加第三个模型的估计器数量，让它能够收敛。
- en: We have learned from the bagging ensembles that using a random training sample
    with each estimator may help with overfitting. In the next section, we are going
    to see whether the same approach can also help the boosting ensembles.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从袋装集成方法中学到，通过为每个估计器使用一个随机训练样本，可以帮助减少过拟合。在下一节中，我们将看看相同的方法是否也能帮助提升集成方法。
- en: Using different sample sizes
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用不同的样本大小
- en: We have been using the entire training set for each iteration. This time, we
    are going to train three gradient boosting regressors, each with a different subsample
    size, and plot their deviance graphs as before. We will use a fixed learning rate
    of `0.01` and the LAD**as our loss function, as shown:**
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在为每次迭代使用整个训练集。这一次，我们将训练三个梯度提升回归模型，每个模型使用不同的子样本大小，并像之前一样绘制它们的偏差图。我们将使用固定的学习率`0.01`，并使用LAD**作为我们的损失函数，如下所示：**
- en: '**![](img/0b4eacd5-f8e0-41a7-9a38-895ae96e13a6.png)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/0b4eacd5-f8e0-41a7-9a38-895ae96e13a6.png)**'
- en: In the first graph, the entire training sample is used for each iteration. So,
    the training loss did not fluctuate as much as in the other two graphs. Nevertheless,
    the sampling used in the second model allowed it to reach a better test score,
    despite its noisy loss graph. This was similarly the case for the third model,
    but with a slightly larger final error.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一张图中，每次迭代都会使用整个训练样本。因此，训练损失不像其他两张图那样波动。然而，第二个模型中使用的采样方法使其尽管损失图较为噪声，仍然达到了更好的测试得分。第三个模型的情况也类似，但最终误差略大。
- en: Stopping earlier and adapting the learning rate
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止并调整学习率
- en: The `n_iter_no_change` hyperparameter is used to stop the training process after
    a certain number of iterations if the validation score is not improving enough.
    The subset set aside for validation, `validation_fraction`, is used to calculate
    the validation score. The `tol`**hyperparameter is used to decide how much improvement
    we must consider as being enough.**
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_iter_no_change`超参数用于在一定数量的迭代后停止训练过程，前提是验证得分没有得到足够的改进。用于验证的子集，`validation_fraction`，用于计算验证得分。`tol`**超参数用于决定我们认为多少改进才算足够。**'
- en: '**The `fit` method in the gradient boosting algorithm accepts a callback function
    that is called after each iteration. It can also be used to set a custom condition
    to stop the training process based on it. Furthermore, It can be used for monitoring
    or for any other customizations you need. This callback function is called with
    three parameters: the order of the current iteration (`n`), an instance of gradient
    boosting (`estimator`), and its settings (`params`). To demonstrate how this callback
    function works, let''s build a function to change the learning rate to `0.01`
    for one iteration at every `10` iterations, and keep it at `0.1` for the remaining
    iterations, as shown:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升算法中的 `fit` 方法接受一个回调函数，该函数会在每次迭代后被调用。它还可以用于设置基于自定义条件的训练停止条件。此外，它还可以用于监控或进行其他自定义设置。该回调函数接受三个参数：当前迭代的顺序（`n`）、梯度提升实例（`estimator`）以及它的设置（`params`）。为了演示这个回调函数是如何工作的，我们构建了一个函数，在每10次迭代时将学习率更改为`0.01`，其余迭代保持为`0.1`，如下所示：'
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we use our `lr_changer` function, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`lr_changer`函数，如下所示：
- en: '[PRE39]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, if we print the deviance as we usually do, we will see how after every
    10^(th) iteration, the calculated loss jumps due to the learning rate changes:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果像我们通常做的那样打印偏差，我们会看到每隔第10^(th) 次迭代后，由于学习率的变化，计算的损失值会跳跃：
- en: '![](img/b94a9c30-3e42-4298-8ec8-63bdeb95e55e.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b94a9c30-3e42-4298-8ec8-63bdeb95e55e.png)'
- en: What I've just done is pretty useless, but it demonstrates the possibilities
    you have at hand. For example, you can borrow ideas such as the adaptive learning
    rate and the momentum from the solvers used in the neural networks and incorporate
    them here using this callback function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才做的事情几乎没有什么实际用途，但它展示了你手头的可能性。例如，你可以借鉴神经网络中求解器的自适应学习率和动量等思想，并通过此回调函数将其融入到这里。
- en: Regression ranges
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归范围
- en: '"I try to be a realist and not a pessimist or an optimist."'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: “我尽量做一个现实主义者，而不是悲观主义者或乐观主义者。”
- en: –Yuval Noah Harari
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: –尤瓦尔·诺亚·哈拉里
- en: One last gem that gradient boosting regression offers to us is regression ranges.
    These are very useful in quantifying the uncertainty of your predictions.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升回归为我们提供的最后一个宝贵资源是回归范围。这对于量化预测的不确定性非常有用。
- en: 'We try our best to make our predictions exactly the same as the actual data.
    Nevertheless, our data can still be noisy, or the features used may not capture
    the whole truth. Take the following example:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尽力让我们的预测与实际数据完全一致。然而，我们的数据可能仍然是嘈杂的，或者使用的特征可能并未捕捉到完整的真相。请看下面的例子：
- en: '| **x[1]** | **x[2]** | **y** |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| **x[1]** | **x[2]** | **y** |'
- en: '| 0 | 0 | 10 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 10 |'
- en: '| 1 | 1 | 50 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 50 |'
- en: '| 0 | 0 | 20 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 20 |'
- en: '| 0 | 0 | 22 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 22 |'
- en: Consider a new sample with *x[1]* = 0 and *x[2]* = 0\. We already have three
    training examples with the exact same features, so what would the predicted *y*
    value be for this new sample? If a squared loss function is used during the training,
    then the predicted target will be close to `17.3`, which is the mean of the three
    corresponding targets*(`10`, `20`, and `22`). Now, if MAE is used, then the predicted
    target will be closer to `22`, which is the median (50^(th) percentile) of the
    three corresponding targets. Rather than the 50^(th) percentile, we can use any
    other percentiles when a **quantile** loss function is used. So, to achieve regression
    ranges, we can use two regressors with two different quantiles as the upper and
    lower bounds of our range.*
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个新的样本，*x[1]* = 0 且 *x[2]* = 0。我们已经有三个具有相同特征的训练样本，那么这个新样本的预测 *y* 值是多少呢？如果在训练过程中使用平方损失函数，则预测的目标将接近`17.3`，即三个相应目标（`10`，`20`，`22`）的均值。现在，如果使用
    MAE（平均绝对误差）的话，预测的目标会更接近`22`，即三个相应目标的中位数（50^(th) 百分位）。而不是50^(th) 百分位，我们可以在使用**分位数**损失函数时使用其他任何百分位数。因此，为了实现回归范围，我们可以使用两个回归器，分别用两个不同的分位数作为我们范围的上下限。
- en: '*Although the regression ranges work regardless of the dimensionality of the
    data at hand, the format of the page has forced us to come up with a two-dimensional
    example for more clarity. The following code creates 400samples to play with:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管回归范围在数据维度无关的情况下有效，但页面格式迫使我们用一个二维示例来提供更清晰的展示。以下代码创建了400个样本以供使用：'
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here is a scatter plot of the generated *y* versus *x* values:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的 *y* 与 *x* 值的散点图：
- en: '![](img/f3c2d69c-05d6-413b-af26-4bef741ba82b.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3c2d69c-05d6-413b-af26-4bef741ba82b.png)'
- en: 'Now, we can train two regressors with the 10^(th) and 90^(th) percentiles as
    our range boundaries and plot those regression boundaries, along with our scattered
    data points:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练两个回归模型，使用第10百分位数和第90百分位数作为我们的范围边界，并绘制这些回归边界，以及我们的散点数据点：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can see that the majority of the points fall within the range. Ideally,
    we would expect 80% of the points to fall in the **90**-**100** range:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大部分数据点落在了范围内。理想情况下，我们希望80%的数据点落在**90**-**100**的范围内：
- en: '![](img/6f7f891c-eae2-48f9-a5cd-33ab625fe6ae.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f7f891c-eae2-48f9-a5cd-33ab625fe6ae.png)'
- en: 'We can now use the same strategy to predict automobile prices:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用相同的策略来预测汽车价格：
- en: '[PRE42]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, we can check what percentage of our test set falls within the regression
    range:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以检查测试集中的多少百分比数据点落在回归范围内：
- en: '[PRE43]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Calculating the mean of `df_pred_range['Actuals in Range?']` gives us `0.49`,
    which is very close to the `0.5` value we expected. Obviously, we can use wider
    or narrower ranges, depending on our use case. If your model is going to be used
    to help car owners sell their cars, you may need to give them reasonable ranges,
    since telling someone that they can sell their car for any price between $5 and
    $30,000 is pretty accurate yet useless advice. Sometimes, a less accurate yet
    useful model is better than an accurate and useless one.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 计算`df_pred_range['Actuals in Range?']`的平均值为`0.49`，这个值非常接近我们预期的`0.5`。显然，根据我们的使用场景，我们可以使用更宽或更窄的范围。如果你的模型将用于帮助车主出售汽车，你可能需要给出合理的范围，因为告诉某人他们可以以$5到$30,000之间的任何价格出售汽车，虽然很准确，但并没有多大帮助。有时候，一个不那么精确但有用的模型，比一个准确却无用的模型要好。
- en: Another boosting algorithm that is not used as much nowadays is the AdaBoost
    algorithm. We will briefly explore it in the next section for the sake of completeness.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个如今使用较少的提升算法是AdaBoost算法。为了完整性，我们将在下一节简要探讨它。
- en: Using AdaBoost ensembles
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AdaBoost集成方法
- en: 'In an AdaBoost ensemble, the mistakes made in each iteration are used to alter
    the weights of the training samples for the following iterations. As in the boosting
    meta-estimator, this method can also use any other estimators instead of the decision
    trees used by default. Here, we have used it with its default estimators on the
    Automobile dataset:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在AdaBoost集成中，每次迭代中所犯的错误被用来调整训练样本的权重，以便用于后续迭代。与提升元估计器一样，这种方法也可以使用其他任何估计器，而不仅限于默认使用的决策树。这里，我们使用默认的估计器在汽车数据集上进行训练：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The AdaBoost meta-estimator also has a `staged_predict` method, which allows
    us to plot the improvement in the training or test loss after each iteration.
    Here is the code for plotting the test error:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost元估计器也有一个`staged_predict`方法，允许我们在每次迭代后绘制训练或测试损失的改善情况。以下是绘制测试误差的代码：
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here is a plot for the calculated loss after each iteration:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每次迭代后计算损失的图表：
- en: '![](img/3b048dd9-613c-4fe3-94e2-649a975e5d87.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b048dd9-613c-4fe3-94e2-649a975e5d87.png)'
- en: As in the other ensembles, the more estimators we add, the more accurate it
    becomes. Once we start to overfit, we should be able to stop. That's why having
    a validation sample is essential in knowing when to stop. I used the test set
    for demonstration here, but in practice, the test sample should be kept aside
    and a validation set should be used instead.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他集成方法一样，我们添加的估计器越多，模型的准确度就越高。一旦我们开始过拟合，就应该停止。因此，拥有一个验证样本对于确定何时停止非常重要。这里我使用了测试集进行演示，但在实际应用中，测试样本应该保持单独，并使用验证集来代替。
- en: Exploring more ensembles
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索更多的集成方法
- en: The main ensemble techniques are the ones we have seen so far. The following
    ones are also good to know about and can be useful for some peculiar cases.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，我们已经看过的主要集成技术就是这些。接下来的一些技术也值得了解，并且在一些特殊情况下可能会有用。
- en: Voting ensembles
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投票集成方法
- en: Sometimes, we have a number of good estimators, each with its own mistakes.
    Our objective is not to mitigate their bias or variance, but to combine their
    predictions in the hope that they don't all make the same mistakes. In these cases,
    `VotingClassifier` and `VotingRegressor` could be used. You can give a higher
    preference to some estimators versus the others by adjusting the `weights` hyperparameter.
    `VotingClassifier` has different voting strategies, depending on whether the predicted
    class labels are to be used or whether the predicted probabilities should be used
    instead.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们有多个优秀的估计器，每个估计器都有自己的错误。我们的目标不是减小它们的偏差或方差，而是结合它们的预测，希望它们不会犯同样的错误。在这种情况下，可以使用`VotingClassifier`和`VotingRegressor`。你可以通过调整`weights`超参数，给某些估计器更高的优先权。`VotingClassifier`有不同的投票策略，取决于是否使用预测的类别标签，或者是否应该使用预测的概率。
- en: Stacking ensembles
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠集成
- en: Rather than voting, you can combine the predictions of multiple estimators by
    adding an extra one that uses their predictions as input. This strategy is known
    as **stacking**. The inputs of the final estimator can be limited to the predictions
    of the previous estimators, or it can be a combination of their predictions and
    the original training data. To avoid overfitting, the final estimators are usually
    trained using cross-validation.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 与其投票，你可以通过增加一个额外的估计器，将多个估计器的预测结果结合起来，作为其输入。这个策略被称为**堆叠**。最终估计器的输入可以仅限于先前估计器的预测，或者可以是它们的预测与原始训练数据的结合。为了避免过拟合，最终估计器通常使用交叉验证进行训练。
- en: Random tree embedding
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机树嵌入
- en: We have seen how the trees are capable of capturing the non-linearities in the
    data. So, if we still want to use a simpler algorithm, we can just use the trees
    to transform the data and leave the prediction for the simple algorithm to do.
    When building a tree, each data point falls into one of its leaves. Therefore,
    the IDs of the leaves can be used to represent the different data points. If we
    build multiple trees, then each data point can be represented by the ID of the
    leaf it fell on in each tree. These leaves, IDs can be used as our new features
    and can be fed into a simpler estimator. This kind of embedding is useful for
    feature compression and allows linear models to capture the non-linearities in
    the data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到树能够捕捉数据中的非线性特征。因此，如果我们仍然希望使用更简单的算法，我们可以仅使用树来转换数据，并将预测交给简单的算法来完成。在构建树时，每个数据点都会落入其中一个叶节点。因此，叶节点的ID可以用来表示不同的数据点。如果我们构建多个树，那么每个数据点就可以通过它在每棵树中所落叶节点的ID来表示。这些叶节点ID可以作为新的特征，输入到更简单的估计器中。这种嵌入方法对于特征压缩非常有用，并且允许线性模型捕捉数据中的非线性特征。
- en: 'Here, we use an unsupervised `RandomTreesEmbedding` method to transform our
    automobile features, and then use the transformed features in a `Ridge` regression:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用无监督的`RandomTreesEmbedding`方法来转换我们的汽车特征，然后在`Ridge`回归中使用转换后的特征：
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'From the preceding block of code, we can observe the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码块中，我们可以观察到以下几点：
- en: This approach is not limited to `RandomTreesEmbedding`.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法不限于`RandomTreesEmbedding`。
- en: Gradient-boosted trees can also be used to transform the data for a downstream
    estimator to use.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树也可以用于转换数据，供下游估计器使用。
- en: Both `GradientBoostingRegressor` and `GradientBoostingClassifier` have an `apply`
    function, which can be used for feature transformation.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GradientBoostingRegressor`和`GradientBoostingClassifier`都有一个`apply`函数，可用于特征转换。'
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how algorithms benefit from being assembled in the form
    of ensembles. We learned how these ensembles can mitigate the bias versus variance
    trade-off.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了算法如何从以集成的形式组装中受益。我们学习了这些集成如何缓解偏差与方差的权衡。
- en: When dealing with heterogeneous data, the gradient boosting and random forest
    algorithms are my first two choices for classification and regression. They do
    not require any sophisticated data preparation, thanks to their dependence on
    trees. They are able to deal with non-linear data and capture feature interactions.
    Above all, the tuning of their hyperparameters is straightforward.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理异构数据时，梯度提升和随机森林算法是我进行分类和回归时的首选。由于它们依赖于树结构，它们不需要任何复杂的数据预处理。它们能够处理非线性数据并捕捉特征之间的交互。最重要的是，它们的超参数调整非常简单。
- en: The more estimators in each method, the better, and you should not worry so
    much about them overfitting. As for gradient boosting, you can pick a lower learning
    rate if you can afford to have more trees. In addition to these hyperparameters,
    the depth of the trees in each of the two algorithms should be tuned via trail
    and error and cross-validation. Since the two algorithms come from different sides
    of the bias-variance spectrum, you may initially aim for forests with big trees
    that you can prune later on. Conversely, you can start with shallow trees and
    rely on your gradient-boosting meta-estimator to boost them.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法中的估计器越多越好，你不需要过于担心它们会过拟合。至于梯度提升方法，如果你能承受更多的树木，可以选择较低的学习率。除了这些超参数外，每个算法中树的深度应该通过反复试验和交叉验证来调优。由于这两种算法来自偏差-方差谱的不同端点，你可以最初选择拥有大树的森林，并在之后进行修剪。相反，你也可以从浅层树开始，并依赖你的梯度提升元估计器来增强它们。
- en: So far in this book, we have predicted a single target at a time. Here, we predicted
    the prices of automobiles and that's it. In the next chapter, we will see how
    to predict multiple targets at a time. Furthermore, when our aim is to use the
    probabilities given by a classifier, having a calibrated classifier is paramount.
    We can have a better estimation of our risks if we have probabilities that we
    trust. Thus, calibrating a classifier is going to be another topic covered in
    the next chapter.*********
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中我们每次只预测一个目标。比如说，我们预测了汽车的价格，仅此而已。在下一章，我们将看到如何一次性预测多个目标。此外，当我们的目标是使用分类器给出的概率时，拥有一个校准过的分类器至关重要。如果我们能得到可信的概率，我们就能更好地评估我们的风险。因此，校准分类器将是下一章将要讨论的另一个话题。
