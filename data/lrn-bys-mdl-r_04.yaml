- en: Chapter 4. Machine Learning Using Bayesian Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：使用贝叶斯推理的机器学习
- en: Now that we have learned about Bayesian inference and R, it is time to use both
    for machine learning. In this chapter, we will give an overview of different machine
    learning techniques and discuss each of them in detail in subsequent chapters.
    Machine learning is a field at the intersection of computer science and statistics,
    and a subbranch of artificial intelligence or AI. The name essentially comes from
    the early works in AI where researchers were trying to develop learning machines
    that automatically learned the relationship between input and output variables
    from data alone. Once a machine is trained on a dataset for a given problem, it
    can be used as a black box to predict values of output variables for new values
    of input variables.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了贝叶斯推理和R，现在是时候将两者都用于机器学习了。在本章中，我们将概述不同的机器学习技术，并在随后的章节中详细讨论每一个。机器学习是计算机科学和统计学交叉的领域，也是人工智能或AI的一个子分支。这个名字本质上来源于AI早期的工作，当时研究人员试图开发学习机器，这些机器能够仅从数据中自动学习输入和输出变量之间的关系。一旦机器在一个给定问题的数据集上进行了训练，它就可以作为一个黑盒来预测新输入变量的输出变量的值。
- en: 'It is useful to set this learning process of a machine in a mathematical framework.
    Let *X* and *Y* be two random variables such that we seek a learning machine that
    learns the relationship between these two variables from data and predicts the
    value of *Y*, given the value of *X*. The system is fully characterized by a joint
    probability distribution *P(X,Y)*, however, the form of this distribution is unknown.
    The goal of learning is to find a function *f(X)*, which maps from *X* to *Y*,
    such that the predictions ![Machine Learning Using Bayesian Inference](img/image00379.jpeg)
    contain as small error as possible. To achieve this, one chooses a loss function
    *L(Y, f(X))* and finds an *f(X)* that minimizes the expected or average loss over
    the joint distribution of *X* and *Y* given by:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器的这个学习过程设置在数学框架中是有用的。设*X*和*Y*是两个随机变量，我们寻求一个学习机器，它能从数据中学习这两个变量之间的关系，并预测给定*X*值的*Y*值。系统完全由联合概率分布*P(X,Y)*来表征，然而，这个分布的形式是未知的。学习的目标是找到一个函数*f(X)*，它将*X*映射到*Y*，使得预测![使用贝叶斯推理的机器学习](img/image00379.jpeg)包含尽可能小的误差。为了实现这一点，选择一个损失函数*L(Y,
    f(X))*，并找到一个*f(X)*，它最小化给定*X*和*Y*的联合分布的期望或平均损失，如下所示：
- en: '![Machine Learning Using Bayesian Inference](img/image00380.jpeg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯推理的机器学习](img/image00380.jpeg)'
- en: In Statistical Decision Theory, this is called **empirical risk minimization**.
    The typical loss function used is **square loss function** (![Machine Learning
    Using Bayesian Inference](img/image00381.jpeg)), if *Y* is a continuous variable,
    and **Hinge loss function** (![Machine Learning Using Bayesian Inference](img/image00382.jpeg)),
    if *Y* is a binary discrete variable with values ![Machine Learning Using Bayesian
    Inference](img/image00383.jpeg). The first case is typically called **regression**
    and second case is called **binary classification**, as we will see later in this
    chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计决策理论中，这被称为**经验风险最小化**。典型的损失函数是**平方损失函数**([使用贝叶斯推理的机器学习](img/image00381.jpeg))，如果*Y*是一个连续变量，以及**Hinge损失函数**([使用贝叶斯推理的机器学习](img/image00382.jpeg))，如果*Y*是一个具有![使用贝叶斯推理的机器学习](img/image00383.jpeg)值的二进制离散变量。第一种情况通常被称为**回归**，第二种情况称为**二元分类**，正如我们将在本章后面看到的。
- en: 'The mathematical framework described here is called **supervised learning**,
    where the machine is presented with a training dataset containing ground truth
    values corresponding to pairs *(Y, X)*. Let us consider the case of square loss
    function again. Here, the learning task is to find an *f(X)* that minimizes the
    following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的数学框架被称为**监督学习**，其中机器被呈现一个包含对应于对(Y, X)对的地面真实值的训练数据集。让我们再次考虑平方损失函数的情况。在这里，学习任务是要找到一个*f(X)*，它最小化以下内容：
- en: '![Machine Learning Using Bayesian Inference](img/image00384.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯推理的机器学习](img/image00384.jpeg)'
- en: 'Since the objective is to predict values of *Y* for the given values of *X*,
    we have used the conditional distribution *P(Y|X)* inside the integral using factorization
    of *P(X, Y)*. It can be shown that the minimization of the preceding loss function
    leads to the following solution:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标是预测给定*X*值的*Y*值，我们在积分内使用了*P(X, Y)*的分解来使用条件分布*P(Y|X)*。可以证明，先前损失函数的最小化导致以下解：
- en: '![Machine Learning Using Bayesian Inference](img/image00385.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯推理进行机器学习](img/image00385.jpeg)'
- en: The meaning of the preceding equation is that the best prediction of *Y* for
    any input value *X* is the mean or expectation denoted by *E*, of the conditional
    probability distribution *P(Y|X)* conditioned at *X*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程的意义是，对于任何输入值*X*，*Y*的最佳预测是条件概率分布*P(Y|X)*在*X*处的均值或期望，用*E*表示。
- en: In [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing Bayesian Inference"),
    *Introducing Bayesian Inference*, we mentioned **maximum likelihood estimation**
    (**MLE**) as a method for learning the parameters ![Machine Learning Using Bayesian
    Inference](img/image00386.jpeg) of any distribution *P(X)*. In general, MLE is
    the same as the minimization of a square loss function if the underlying distribution
    is a normal distribution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0030.xhtml#aid-SJGS2 "第3章。介绍贝叶斯推理")《介绍贝叶斯推理》中，我们提到了**最大似然估计**（**MLE**）作为学习任何分布*P(X)*参数的方法![使用贝叶斯推理进行机器学习](img/image00386.jpeg)。一般来说，如果基础分布是正态分布，MLE与平方损失函数的最小化相同。
- en: Note that, in empirical risk minimization, we are learning the parameter, *E[(Y|X)]*,
    the mean of the conditional distribution, for a given value of *X*. We will use
    one particular machine learning task, linear regression, to explain the advantage
    of Bayesian inference over the classical method of learning. However, before this,
    we will briefly explain some more general aspects of machine learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在经验风险最小化中，我们正在学习参数，*E[(Y|X)]*，给定*X*值的条件分布的均值。我们将使用一个特定的机器学习任务，线性回归，来解释贝叶斯推理相对于经典学习方法的优势。然而，在这之前，我们将简要解释一些机器学习的更一般方面。
- en: There are two types of supervised machine learning models, namely generative
    models and discriminative models. In the case of generative models, the algorithm
    tries to learn the joint probability of *X* and *Y*, which is *P(X,Y)*, from data
    and uses it to estimate mean *P(Y|X)*. In the case of discriminative models, the
    algorithm tries to directly learn the desired function, which is the mean of *P(Y|X)*,
    and no modeling of the *X* variable is attempted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习模型有两种类型，即生成模型和判别模型。在生成模型的情况下，算法试图从数据中学习*X*和*Y*的联合概率，即*P(X,Y)*，并使用它来估计均值*P(Y|X)*。在判别模型的情况下，算法试图直接学习期望的函数，即*P(Y|X)*的均值，并且不尝试对*X*变量进行建模。
- en: Labeling values of the target variable in the training data is done manually.
    This makes supervised learning very expensive when one needs to use very large
    datasets as in the case of text analytics. However, very often, supervised learning
    methods produce the most accurate results.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据中对目标变量的值进行标记是手动完成的。这使得当需要使用非常大的数据集，如文本分析中的情况时，监督学习变得非常昂贵。然而，非常常见的是，监督学习方法产生最准确的结果。
- en: If there is not enough training data available for learning, one can still use
    machine learning through **unsupervised learning**. Here, the learning is mainly
    through the discovery of patterns of associations between variables in the dataset.
    Clustering data points that have similar features is a classic example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有足够的训练数据可供学习，一个人仍然可以通过**无监督学习**来使用机器学习。在这里，学习主要是通过发现数据集中变量之间关联模式的过程。聚类具有相似特征的点是一个经典的例子。
- en: '**Reinforcement learning** is the third type of machine learning, where the
    learning takes place in a dynamic environment where the machine needs to perform
    certain actions based on its current state. Associated with each action is a reward.
    The machine needs to learn what action needs to be taken at each state so that
    the total reward is maximized. This is typically how a robot learns to perform
    tasks, such as driving a vehicle, in a real-life environment.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**是机器学习的第三种类型，学习发生在动态环境中，机器需要根据其当前状态执行某些动作。与每个动作相关联的是奖励。机器需要学习在每个状态下需要采取什么动作，以便最大化总奖励。这通常是机器人学习在现实生活环境中执行任务，如驾驶车辆的方式。'
- en: Why Bayesian inference for machine learning?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么机器学习要使用贝叶斯推理？
- en: We have already discussed the advantages of Bayesian statistics over classical
    statistics in the last chapter. In this chapter, we will see in more detail how
    some of the concepts of Bayesian inference that we learned in the last chapter
    are useful in the context of machine learning. For this purpose, we take one simple
    machine learning task, namely linear regression. Let us consider a learning task
    where we have a dataset *D* containing *N* pair of points ![Why Bayesian inference
    for machine learning?](img/image00387.jpeg) and the goal is to build a machine
    learning model using linear regression that it can be used to predict values of
    ![Why Bayesian inference for machine learning?](img/image00388.jpeg), given new
    values of ![Why Bayesian inference for machine learning?](img/image00389.jpeg).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一章讨论了贝叶斯统计相对于经典统计的优势。在本章中，我们将更详细地探讨我们上一章学到的贝叶斯推理的一些概念在机器学习中的应用。为此，我们选取一个简单的机器学习任务，即线性回归。让我们考虑一个学习任务，其中我们有一个包含
    *N* 对点的数据集 *D* ![Why Bayesian inference for machine learning?](img/image00387.jpeg)，目标是使用线性回归构建一个机器学习模型，该模型可以用来预测
    ![Why Bayesian inference for machine learning?](img/image00388.jpeg) 的值，给定新的 ![Why
    Bayesian inference for machine learning?](img/image00389.jpeg) 值。
- en: 'In linear regression, first, we assume that *Y* is of the following form:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，首先，我们假设 *Y* 的形式如下：
- en: '![Why Bayesian inference for machine learning?](img/image00390.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Why Bayesian inference for machine learning?](img/image00390.jpeg)'
- en: Here, *F(X)* is a function that captures the true relationship between *X* and
    *Y*, and ![Why Bayesian inference for machine learning?](img/image00391.jpeg)
    is an error term that captures the inherent noise in the data. It is assumed that
    this noise is characterized by a normal distribution with mean 0 and variance
    ![Why Bayesian inference for machine learning?](img/image00392.jpeg). What this
    implies is that if we have an infinite training dataset, we can learn the form
    of *F(X)* from data and, even then, we can only predict *Y* up to an additive
    noise term ![Why Bayesian inference for machine learning?](img/image00391.jpeg).
    In practice, we will have only a finite training dataset *D*; hence, we will be
    able to learn only an approximation for *F(X)* denoted by ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*F(X)* 是一个函数，它捕捉了 *X* 和 *Y* 之间真实关系的函数，![Why Bayesian inference for machine
    learning?](img/image00391.jpeg) 是一个误差项，它捕捉了数据中固有的噪声。假设这种噪声由均值为 0、方差为 ![Why Bayesian
    inference for machine learning?](img/image00392.jpeg) 的正态分布来表征。这意味着，如果我们有一个无限大的训练数据集，我们可以从数据中学习到
    *F(X)* 的形式，即使在这种情况下，我们也只能预测到加上一个噪声项 ![Why Bayesian inference for machine learning?](img/image00391.jpeg)
    的 *Y*。在实践中，我们只有有限大小的训练数据集 *D*；因此，我们只能学习到 *F(X)* 的一个近似形式，用 ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg) 表示。
- en: Note that we are discussing two types of errors here. One is an error term ![Why
    Bayesian inference for machine learning?](img/image00391.jpeg) that is due to
    the inherent noise in the data that we cannot do much about. The second error
    is in learning *F(X)*, approximately through the function ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg) from the dataset *D*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里讨论了两种类型的误差。一种是误差项 ![Why Bayesian inference for machine learning?](img/image00391.jpeg)，这是由于数据中固有的噪声造成的，我们对此无能为力。第二种误差是在学习
    *F(X)* 时，通过函数 ![Why Bayesian inference for machine learning?](img/image00393.jpeg)
    从数据集 *D* 中近似地学习。
- en: 'In general, ![Why Bayesian inference for machine learning?](img/image00393.jpeg),
    which the approximate mapping between input variable *X* and output variable *Y*,
    is a function of *X* with a set of parameters ![Why Bayesian inference for machine
    learning?](img/image00386.jpeg). When ![Why Bayesian inference for machine learning?](img/image00393.jpeg)
    is a linear function of the parameters ![Why Bayesian inference for machine learning?](img/image00386.jpeg),
    we say the learning model is linear. It is a general misconception that linear
    regression corresponds to the case only if ![Why Bayesian inference for machine
    learning?](img/image00393.jpeg) is a linear function of *X*. The reason for linearity
    in the parameter and not in *X* is that, during the minimization of the loss function,
    one actually minimizes over the parameter values to find the best ![Why Bayesian
    inference for machine learning?](img/image00393.jpeg). Hence, a function that
    is linear in ![Why Bayesian inference for machine learning?](img/image00386.jpeg)
    will lead to a linear optimization problem that can be tackled analytically and
    numerically more easily. Therefore, linear regression corresponds to the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，![为什么机器学习要使用贝叶斯推理？](img/image00393.jpeg)，它是输入变量*X*和输出变量*Y*之间近似映射的函数，是一个包含一组参数![为什么机器学习要使用贝叶斯推理？](img/image00386.jpeg)的*X*的函数。当![为什么机器学习要使用贝叶斯推理？](img/image00393.jpeg)是参数![为什么机器学习要使用贝叶斯推理？](img/image00386.jpeg)的线性函数时，我们说学习模型是线性的。一个普遍的误解是，只有当![为什么机器学习要使用贝叶斯推理？](img/image00393.jpeg)是*X*的线性函数时，线性回归才对应这种情况。参数线性而不是*X*的原因是，在最小化损失函数的过程中，实际上是通过最小化参数值来寻找最佳的![为什么机器学习要使用贝叶斯推理？](img/image00393.jpeg)。因此，在![为什么机器学习要使用贝叶斯推理？](img/image00386.jpeg)上是线性的函数会导致一个可以更容易地解析和数值解决的线性优化问题。因此，线性回归对应以下：
- en: '![Why Bayesian inference for machine learning?](img/image00394.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![为什么机器学习要使用贝叶斯推理？](img/image00394.jpeg)'
- en: 'This is an expansion over a set of *M* basis functions ![Why Bayesian inference
    for machine learning?](img/image00395.jpeg). Here, each basis function ![Why Bayesian
    inference for machine learning?](img/image00396.jpeg) is a function of *X* without
    any unknown parameters. In machine learning, these are called feature functions
    or model features. For the linear regression problem, the loss function, therefore,
    can be written as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在*M*个基函数![为什么机器学习要使用贝叶斯推理？](img/image00395.jpeg)上的扩展。在这里，每个基函数![为什么机器学习要使用贝叶斯推理？](img/image00396.jpeg)是*X*的函数，没有任何未知参数。在机器学习中，这些被称为特征函数或模型特征。对于线性回归问题，因此损失函数可以写成以下形式：
- en: '![Why Bayesian inference for machine learning?](img/image00397.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![为什么机器学习要使用贝叶斯推理？](img/image00397.jpeg)'
- en: Here, ![Why Bayesian inference for machine learning?](img/image00398.jpeg) is
    the transpose of the parameter vector ![Why Bayesian inference for machine learning?](img/image00399.jpeg)
    and *B(X)* is the vector composed of the basis functions ![Why Bayesian inference
    for machine learning?](img/image00400.jpeg). Learning ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg) from a dataset implies estimating
    the values of ![Why Bayesian inference for machine learning?](img/image00386.jpeg)
    by minimizing the loss function through some optimization schemes such as gradient
    descent.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![为什么机器学习要使用贝叶斯推理？](img/image00398.jpeg)是参数向量![为什么机器学习要使用贝叶斯推理？](img/image00399.jpeg)的转置，*B(X)*是由基函数![为什么机器学习要使用贝叶斯推理？](img/image00400.jpeg)组成的向量。从数据集中学习![为什么机器学习要使用贝叶斯推理？](img/image00393.jpeg)意味着通过一些优化方案（如梯度下降）最小化损失函数来估计![为什么机器学习要使用贝叶斯推理？](img/image00386.jpeg)的值。
- en: It is important to choose as many basis functions as possible to capture interesting
    patterns in the data. However, choosing more numbers of basis functions or features
    will overfit the model in the sense that it will even start fitting the noise
    contained in the data. Overfit will lead to poor predictions on new input data.
    Therefore, it is important to choose an optimum number of best features to maximize
    the predictive accuracy of any machine learning model. In machine learning based
    on classical statistics, this is achieved through what is called **bias-variance
    tradeoff** and **model regularization**. Whereas, in machine learning through
    Bayesian inference, accuracy of a predictive model can be maximized through Bayesian
    model averaging, and there is no need to impose model regularization or bias-variance
    tradeoff. We will learn each of these concepts in the following sections.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 选择尽可能多的基函数来捕捉数据中的有趣模式是很重要的。然而，选择更多的基函数或特征会导致模型过拟合，即它甚至开始拟合数据中包含的噪声。过拟合会导致对新输入数据的预测效果变差。因此，选择最佳特征的最佳数量以最大化任何机器学习模型的预测准确性是很重要的。在基于经典统计学的机器学习中，这通过所谓的**偏差-方差权衡**和**模型正则化**来实现。而在基于贝叶斯推理的机器学习中，可以通过贝叶斯模型平均来最大化预测模型的准确性，无需施加模型正则化或偏差-方差权衡。我们将在以下各节中学习这些概念。
- en: Model overfitting and bias-variance tradeoff
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型过拟合与偏差-方差权衡
- en: 'The expected loss mentioned in the previous section can be written as a sum
    of three terms in the case of linear regression using squared loss function, as
    follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中提到的预期损失，在采用平方损失函数的线性回归情况下，可以写成三个项的和，如下所示：
- en: '![Model overfitting and bias-variance tradeoff](img/image00401.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![模型过拟合与偏差-方差权衡](img/image00401.jpeg)'
- en: 'Here, *Bias* is the difference ![Model overfitting and bias-variance tradeoff](img/image00402.jpeg)
    between the true model *F(X)* and average value of ![Model overfitting and bias-variance
    tradeoff](img/image00393.jpeg) taken over an ensemble of datasets. *Bias* is a
    measure of how much the average prediction over all datasets in the ensemble differs
    from the true regression function *F(X)*. *Variance* is given by ![Model overfitting
    and bias-variance tradeoff](img/image00403.jpeg). It is a measure of extent to
    which the solution for a given dataset varies around the mean over all datasets.
    Hence, *Variance* is a measure of how much the function ![Model overfitting and
    bias-variance tradeoff](img/image00393.jpeg) is sensitive to the particular choice
    of dataset *D*. The third term *Noise*, as mentioned earlier, is the expectation
    of difference ![Model overfitting and bias-variance tradeoff](img/image00404.jpeg)
    between observation and the true regression function, over all the values of *X*
    and *Y*. Putting all these together, we can write the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*偏差*是真实模型 *F(X)* 与对数据集集合取平均值的差异 ![模型过拟合与偏差-方差权衡](img/image00402.jpeg)。*偏差*是衡量集合中所有数据集的平均预测与真实回归函数
    *F(X)* 差异的程度。*方差*由 ![模型过拟合与偏差-方差权衡](img/image00403.jpeg) 给出。它是衡量给定数据集的解在所有数据集的平均值周围变化的程度的度量。因此，*方差*是衡量函数
    ![模型过拟合与偏差-方差权衡](img/image00393.jpeg) 对特定数据集 *D* 选择敏感性的度量。第三项 *噪声*，如前所述，是观察值与真实回归函数之间的差异
    ![模型过拟合与偏差-方差权衡](img/image00404.jpeg) 的期望，在所有 *X* 和 *Y* 的值上。将这些放在一起，我们可以写出以下公式：
- en: '![Model overfitting and bias-variance tradeoff](img/image00405.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![模型过拟合与偏差-方差权衡](img/image00405.jpeg)'
- en: The objective of machine learning is to learn the function ![Model overfitting
    and bias-variance tradeoff](img/image00393.jpeg) from data that minimizes the
    expected loss *E[L]*. One can keep minimizing the bias by keeping more and more
    basis functions in the model and thereby increasing the model's complexity. However,
    since each of the model parameters ![Model overfitting and bias-variance tradeoff](img/image00406.jpeg)
    are learned from a given dataset, the more complex the model becomes, the more
    sensitive its parameter estimation would be to the dataset used. This results
    in increased variance for more complex models. Hence, in any supervised machine
    learning task, there is a tradeoff between model bias and model complexity. One
    has to choose a model of optimum complexity to minimize the error of prediction
    on an unseen dataset. In the classical or frequentist approach, this is done by
    partitioning the labeled data into three sets. One is the training set, the second
    is the validation set, and the third is the test set. Models of different complexity
    that are trained using the training set are evaluated using the validation dataset
    to choose the model with optimum complexity. It is then, finally, evaluated against
    the test set to estimate the prediction error.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的目标是学习从数据中学习函数![模型过拟合与偏差-方差权衡](img/image00393.jpeg)，以最小化预期的损失 *E[L]*。可以通过在模型中保持更多的基函数来不断减少偏差，从而增加模型复杂度。然而，由于每个模型参数![模型过拟合与偏差-方差权衡](img/image00406.jpeg)都是从给定的数据集中学习得到的，因此模型越复杂，其参数估计对数据集的敏感性就越高。这导致复杂模型具有更大的方差。因此，在任何监督机器学习任务中，模型偏差和模型复杂度之间存在权衡。必须选择一个最优复杂度的模型，以最小化未见数据集上的预测误差。在经典或频率论方法中，这是通过将标记数据分为三组来实现的。第一组是训练集，第二组是验证集，第三组是测试集。使用训练集训练的不同复杂度的模型将使用验证数据集进行评估，以选择最优复杂度的模型。然后，最终使用测试集来估计预测误差。
- en: Selecting models of optimum complexity
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最优复杂度的模型
- en: There are different ways of selecting models with the right complexity so that
    the prediction error on unseen data is less. Let's discuss each of these approaches
    in the context of the linear regression model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来选择具有适当复杂性的模型，以便在未见数据上的预测误差更小。让我们在线性回归模型的背景下讨论这些方法中的每一个。
- en: Subset selection
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子集选择
- en: 'In the subset selection approach, one selects only a subset of the whole set
    of variables, which are significant, for the model. This not only increases the
    prediction accuracy of the model by decreasing model variance, but it is also
    useful from the interpretation point of view. There are different ways of doing
    subset selection, but the following two are the most commonly used approaches:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在子集选择方法中，只选择整个变量集中对模型显著的那部分变量。这不仅通过减少模型方差来提高模型的预测精度，而且从解释的角度来看也是有益的。有几种不同的子集选择方法，但以下两种是最常用的方法：
- en: '**Forward selection**: In forward selection, one starts with no variables (intercept
    alone), and by using a greedy algorithm, adds other variables one by one. For
    each step, the variable that most improves the fit is chosen to add to the model.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向前选择法**：在向前选择法中，从一个没有变量（仅包含截距）的模型开始，通过使用贪婪算法，逐个添加其他变量。在每一步中，选择最能改善拟合的变量添加到模型中。'
- en: '**Backward selection**: In backward selection, one starts with the full model
    and sequentially deletes the variable that has the least impact on the fit. At
    each step, the variable with the least Z-score is selected for elimination. In
    statistics, the Z-score of a random variable is a measure of the standard deviation
    between an element and its mean. A small value of Z-score (typically < 2) indicates
    that the effect of the variable is more likely by chance and is not statistically
    significant.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向后选择法**：在向后选择法中，从一个完整的模型开始，依次删除对拟合影响最小的变量。在每一步中，选择Z分数最小的变量进行删除。在统计学中，随机变量的Z分数是衡量一个元素与其均值之间标准差的一个度量。Z分数的值较小（通常小于2）表明变量的影响更有可能是偶然的，并且不具有统计学意义。'
- en: Model regularization
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型正则化
- en: 'In this approach, one adds a penalty term to the loss function that does not
    allow the size of the parameter to become very large during minimization. There
    are two main ways of doing this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，向损失函数中添加一个惩罚项，该惩罚项不允许参数的大小在最小化过程中变得非常大。主要有两种实现方式：
- en: '**Ridge regression**: This simple type of regularization is where the additional
    term is proportional to the magnitude of the parameter vector given by ![Model
    regularization](img/image00407.jpeg). The loss function for linear regression
    with the regularization term can be written as follows:![Model regularization](img/image00408.jpeg)Parameters
    ![Model regularization](img/image00409.jpeg) having a large magnitude will contribute
    more to the loss. Hence, minimization of the preceding loss function will typically
    produce parameters having small values and reduce the overfit. The optimum value
    of ![Model regularization](img/image00410.jpeg) is found from the validation set.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**岭回归**：这种简单的正则化类型是，额外的项与由![模型正则化](img/image00407.jpeg)给出的参数向量的幅度成正比。具有正则化项的线性回归的损失函数可以写成以下形式：![模型正则化](img/image00408.jpeg)具有较大幅度的参数![模型正则化](img/image00409.jpeg)将对损失做出更大的贡献。因此，前面损失函数的最小化通常会产生具有较小值的参数并减少过拟合。![模型正则化](img/image00410.jpeg)的最优值是从验证集中找到的。'
- en: '**Lasso**: In Lasso also, one adds a penalty term similar to ridge regression,
    but the term is proportional to the sum of modulus of each parameter and not its
    square:![Model regularization](img/image00411.jpeg)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lasso**：在Lasso中，也添加了一个类似于岭回归的惩罚项，但这个项是每个参数模的加权和，而不是其平方：![模型正则化](img/image00411.jpeg)'
- en: Though this looks like a simple change, Lasso has some very important differences
    with respect to ridge regression. First of all, the presence of the ![Model regularization](img/image00412.jpeg)
    term makes the loss function nonlinear in parameters ![Model regularization](img/image00386.jpeg).
    The corresponding minimization problem is called the quadratic programming problem
    compared to the linear programming problem in ridge regression, for which a closed
    form solution is available. Due to the particular form ![Model regularization](img/image00412.jpeg)
    of the penalty, when the coefficients shrink as a result of minimization, some
    of them eventually become zero. So, Lasso is also in some sense a subset selection
    problem.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然这看起来是一个简单的变化，但Lasso与岭回归有一些非常重要的区别。首先，![模型正则化](img/image00412.jpeg)项的存在使得损失函数在参数![模型正则化](img/image00386.jpeg)上是非线性的。与岭回归中的线性规划问题相比，相应的最小化问题称为二次规划问题，对于后者，存在闭式解。由于惩罚项![模型正则化](img/image00412.jpeg)的特殊形式，当系数在最小化过程中缩小，其中一些最终会变成零。因此，从某种意义上说，Lasso也是一个子集选择问题。
- en: A detailed discussion of various subset selection and model regularization approaches
    can be found in the book by Trevor Hastie et.al (reference 1 in the *References*
    section of this chapter).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 关于各种子集选择和模型正则化方法的详细讨论，可以在Trevor Hastie等人所著的书籍中找到（本章“参考文献”部分的第1条参考文献）。
- en: Bayesian averaging
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯平均
- en: So far, we have learned that simply minimizing the loss function (or equivalently
    maximizing the log likelihood function in the case of normal distribution) is
    not enough to develop a machine learning model for a given problem. One has to
    worry about models overfitting the training data, which will result in larger
    prediction errors on new datasets. The main advantage of Bayesian methods is that
    one can, in principle, get away from this problem, without using explicit regularization
    and different datasets for training and validation. This is called Bayesian model
    averaging and will be discussed here. This is one of the answers to our main question
    of the chapter, *why Bayesian inference for machine learning?*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到，仅仅最小化损失函数（或等价地，在正态分布的情况下最大化对数似然函数）对于开发给定问题的机器学习模型是不够的。我们必须担心模型过度拟合训练数据，这将在新的数据集上导致更大的预测误差。贝叶斯方法的主要优势在于，原则上可以摆脱这个问题，而无需使用显式的正则化和为训练和验证使用不同的数据集。这被称为贝叶斯模型平均，将在下面进行讨论。这是本章主要问题之一，“为什么机器学习的贝叶斯推理？”的答案之一。
- en: For this, let's do a full Bayesian treatment of the linear regression problem.
    Since we only want to explain how Bayesian inference avoids the overfitting problem,
    we will skip all the mathematical derivations and state only the important results
    here. For more details, interested readers can refer to the book by Christopher
    M. Bishop (reference 2 in the *References* section of this chapter).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，我们将对线性回归问题进行完整的贝叶斯处理。由于我们只想解释贝叶斯推理如何避免过拟合问题，我们将跳过所有的数学推导，只在此处陈述重要的结果。更多细节，感兴趣的读者可以参考Christopher
    M. Bishop的书籍（本章“参考文献”部分的第2条参考文献）。
- en: 'The linear regression equation ![Bayesian averaging](img/image00413.jpeg),
    with ![Bayesian averaging](img/image00391.jpeg) having a normal distribution with
    zero mean and variance ![Bayesian averaging](img/image00414.jpeg) (equivalently,
    precision ![Bayesian averaging](img/image00415.jpeg)), can be cast in a probability
    distribution form with *Y* having a normal distribution with mean *f(X)* and precision
    ![Bayesian averaging](img/image00416.jpeg). Therefore, linear regression is equivalent
    to estimating the mean of the normal distribution:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归方程![贝叶斯平均](img/image00413.jpeg)，其中![贝叶斯平均](img/image00391.jpeg)具有零均值和方差![贝叶斯平均](img/image00414.jpeg)（等价于精度![贝叶斯平均](img/image00415.jpeg)），可以转换为具有*Y*具有均值*f(X)*和精度![贝叶斯平均](img/image00416.jpeg)的概率分布形式。因此，线性回归等价于估计正态分布的均值：
- en: '![Bayesian averaging](img/image00417.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯平均](img/image00417.jpeg)'
- en: Since ![Bayesian averaging](img/image00418.jpeg), where the set of basis functions
    *B(X)* is known and we are assuming here that the noise parameter ![Bayesian averaging](img/image00416.jpeg)
    is also a known constant, only ![Bayesian averaging](img/image00386.jpeg) needs
    to be taken as an uncertain variable for a fully Bayesian treatment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于![贝叶斯平均](img/image00418.jpeg)，其中基础函数集*B(X)*是已知的，并且我们假设这里的噪声参数![贝叶斯平均](img/image00416.jpeg)也是一个已知的常数，因此只需要将![贝叶斯平均](img/image00386.jpeg)作为一个不确定变量来完全贝叶斯处理。
- en: 'The first step in Bayesian inference is to compute a posterior distribution
    of parameter vector ![Bayesian averaging](img/image00386.jpeg). For this, we assume
    that the prior distribution of ![Bayesian averaging](img/image00386.jpeg) is an
    *M* dimensional normal distribution (since there are *M* components) with mean
    ![Bayesian averaging](img/image00419.jpeg) and covariance matrix ![Bayesian averaging](img/image00420.jpeg).
    As we have seen in [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing
    Bayesian Inference"), *Introducing Bayesian Inference*, this corresponds to taking
    a conjugate distribution for the prior:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯推理的第一步是计算参数向量![贝叶斯平均](img/image00386.jpeg)的后验分布。为此，我们假设![贝叶斯平均](img/image00386.jpeg)的先验分布是一个*M*维正态分布（因为有*M*个分量），均值为![贝叶斯平均](img/image00419.jpeg)，协方差矩阵为![贝叶斯平均](img/image00420.jpeg)。正如我们在[第3章](part0030.xhtml#aid-SJGS2
    "第3章。介绍贝叶斯推理")中看到的，*介绍贝叶斯推理*，这对应于对先验取共轭分布：
- en: '![Bayesian averaging](img/image00421.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯平均](img/image00421.jpeg)'
- en: 'The corresponding posterior distribution is given by:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的后验分布由以下给出：
- en: '![Bayesian averaging](img/image00422.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯平均](img/image00422.jpeg)'
- en: Here, ![Bayesian averaging](img/image00423.jpeg) and ![Bayesian averaging](img/image00424.jpeg).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![贝叶斯平均](img/image00423.jpeg)和![贝叶斯平均](img/image00424.jpeg)。
- en: 'Here, *B* is an *N x M* matrix formed by stacking basis vectors *B*, at different
    values of *X*, on top of each other as shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*B*是一个由基础向量*B*在不同*X*值上堆叠而成的*N x M*矩阵，如图所示：
- en: '![Bayesian averaging](img/image00425.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯平均](img/image00425.jpeg)'
- en: 'Now that we have the posterior distribution for ![Bayesian averaging](img/image00386.jpeg)
    as a closed-form analytical expression, we can use it to predict new values of
    *Y*. To get an analytical closed-form expression for the predictive distribution
    of *Y*, we make an assumption that ![Bayesian averaging](img/image00426.jpeg)
    and ![Bayesian averaging](img/image00427.jpeg). This corresponds to a prior with
    zero mean and isotropic covariance matrix characterized by one precision parameter
    ![Bayesian averaging](img/image00428.jpeg). The predictive distribution or the
    probability that the prediction for a new value of *X = x* is *y*, is given by:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了![贝叶斯平均](img/image00386.jpeg)的后验分布作为一个封闭形式的解析表达式，我们可以用它来预测*Y*的新值。为了得到*Y*预测分布的解析封闭形式，我们假设![贝叶斯平均](img/image00426.jpeg)和![贝叶斯平均](img/image00427.jpeg)。这对应于一个具有零均值和各向同性协方差矩阵的先验，其特征是一个精度参数![贝叶斯平均](img/image00428.jpeg)。预测分布或预测新值*X
    = x*为*y*的概率由以下给出：
- en: '![Bayesian averaging](img/image00429.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯平均](img/image00429.jpeg)'
- en: 'This equation is the central theme of this section. In the classical or frequentist
    approach, one estimates a particular value ![Bayesian averaging](img/image00430.jpeg)
    for the parameter ![Bayesian averaging](img/image00386.jpeg) from the training
    dataset and finds the probability of predicting *y* by simply using ![Bayesian
    averaging](img/image00431.jpeg). This does not address the overfitting of the
    model unless regularization is used. In Bayesian inference, we are integrating
    out the parameter variable ![Bayesian averaging](img/image00386.jpeg) by using
    its posterior probability distribution ![Bayesian averaging](img/image00432.jpeg)
    learned from the data. This averaging will remove the necessity of using regularization
    or keeping the parameters to an optimal level through bias-variance tradeoff.
    This can be seen from the closed-form expression for *P(y|x)*, after we substitute
    the expressions for ![Bayesian averaging](img/image00433.jpeg) and ![Bayesian
    averaging](img/image00434.jpeg) for the linear regression problem and do the integration.
    Since both are normal distributions, the integration can be done analytically
    that results in the following simple expression for *P(y|x)*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个等式是本节的核心主题。在经典或频率派方法中，一个人从训练数据集中估计参数![Bayesian averaging](img/image00386.jpeg)的特定值![Bayesian
    averaging](img/image00430.jpeg)，并通过简单地使用![Bayesian averaging](img/image00431.jpeg)来预测
    *y* 的概率。除非使用正则化，否则这不会解决模型的过拟合问题。在贝叶斯推理中，我们通过使用从数据中学习到的参数变量![Bayesian averaging](img/image00386.jpeg)的后验概率分布![Bayesian
    averaging](img/image00432.jpeg)来积分出参数变量。这种平均将消除使用正则化或通过偏差-方差权衡将参数保持在最佳水平的必要性。这可以从线性回归问题的封闭形式表达式
    *P(y|x)* 中看出，在我们将![Bayesian averaging](img/image00433.jpeg)和![Bayesian averaging](img/image00434.jpeg)的表达式代入并进行积分后。由于两者都是正态分布，积分可以解析地进行，从而得到以下简单的
    *P(y|x)* 表达式：
- en: '![Bayesian averaging](img/image00435.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![Bayesian averaging](img/image00435.jpeg)'
- en: Here, ![Bayesian averaging](img/image00436.jpeg).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![Bayesian averaging](img/image00436.jpeg)。
- en: This equation implies that the variance of the predictive distribution consists
    of two terms. One term, 1/![Bayesian averaging](img/image00416.jpeg), coming from
    the inherent noise in the data and the second term coming from the uncertainty
    associated with the estimation of model parameter ![Bayesian averaging](img/image00386.jpeg)
    from data. One can show that as the size of training data *N* becomes very large,
    the second term decreases and in the limit ![Bayesian averaging](img/image00437.jpeg)
    it becomes zero.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个等式表明预测分布的方差由两个项组成。一个项是来自数据固有的噪声和第二个项来自与从数据估计模型参数![Bayesian averaging](img/image00386.jpeg)的不确定性。可以证明，随着训练数据大小
    *N* 变得非常大，第二个项会减小，并在极限![Bayesian averaging](img/image00437.jpeg)时变为零。
- en: The example shown here illustrates the power of Bayesian inference. Since one
    can take care of uncertainty in the parameter estimation through Bayesian averaging,
    one doesn't need to keep separate validation data and all the data can be used
    for training. So, a full Bayesian treatment of a problem avoids the overfitting
    issue. Another major advantage of Bayesian inference, which we will not go into
    in this section, is treating latent variables in a machine learning model. In
    the next section, we will give a high-level overview of the various common machine
    learning tasks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的例子说明了贝叶斯推理的力量。由于可以通过贝叶斯平均处理参数估计中的不确定性，因此不需要保留单独的验证数据，所有数据都可以用于训练。因此，对问题的全面贝叶斯处理可以避免过拟合问题。贝叶斯推理的另一个主要优点，我们将在本节中不深入探讨，是处理机器学习模型中的潜在变量。在下一节中，我们将对各种常见的机器学习任务进行高层次概述。
- en: An overview of common machine learning tasks
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见机器学习任务的概述
- en: This section is a prequel to the following chapters, where we will discuss different
    machine learning techniques in detail. At a high level, there are only a handful
    of tasks that machine learning tries to address. However, for each of such tasks,
    there are several approaches and algorithms in place.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本节是以下章节的序言，我们将详细讨论不同的机器学习技术。从高层次来看，机器学习试图解决的任务只有少数几个。然而，对于每个这样的任务，都有几种方法和算法可供选择。
- en: 'The typical tasks in any machine learning are one of the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习中的典型任务通常是以下之一：
- en: Classification
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Regression
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Clustering
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Association rules
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联规则
- en: Forecasting
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Dimensional reduction
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低
- en: Density estimation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密度估计
- en: In classification, the objective is to assign a new data point to one of the
    predetermined classes. Typically, this is either a supervised or semi-supervised
    learning problem. The well-known machine learning algorithms used for classification
    are logistic regression, **support vector machines** (**SVM**), decision trees,
    Naïve Bayes, neural networks, Adaboost, and random forests. Here, Naïve Bayes
    is a Bayesian inference-based method. Other algorithms, such as logistic regression
    and neural networks, have also been implemented in the Bayesian framework.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类中，目标是把新的数据点分配到预定的类别之一。通常，这是一个监督学习或半监督学习问题。用于分类的著名机器学习算法包括逻辑回归、**支持向量机**（**SVM**）、决策树、朴素贝叶斯、神经网络、Adaboost和随机森林。在这里，朴素贝叶斯是基于贝叶斯推理的方法。其他算法，如逻辑回归和神经网络，也已在贝叶斯框架中实现。
- en: Regression is probably the most common machine learning problem. It is used
    to determine the relation between a set of input variables (typically, continuous
    variables) and an output (dependent) variable that is continuous. We discussed
    the simplest example of linear regression in some detail in the previous section.
    More complex examples of regression are generalized linear regression, spline
    regression, nonlinear regression using neural networks, support vector regression,
    and Bayesian network. Bayesian formulations of regression include the Bayesian
    network and Bayesian linear regression.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回归可能是最常见的机器学习问题。它用于确定一组输入变量（通常是连续变量）与一个连续的输出（因变量）之间的关系。我们在上一节中详细讨论了线性回归的最简单例子。更复杂的回归例子包括广义线性回归、样条回归、使用神经网络的非线性回归、支持向量回归和贝叶斯网络。回归的贝叶斯表述包括贝叶斯网络和贝叶斯线性回归。
- en: Clustering is a classic example of unsupervised learning. Here, the objective
    is to group together similar items in a dataset based on certain features of the
    data. The number of clusters is not known in advance. Hence, clustering is more
    of a pattern detection problem. The well-known clustering algorithms are K-means
    clustering, hierarchical clustering, and **Latent Dirichlet allocation** (**LDA**).
    In this, LDA is formulated as a Bayesian inference problem. Other clustering methods
    using Bayesian inference include the Bayesian mixture model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是无监督学习的经典例子。在这里，目标是根据数据的某些特征将数据集中的相似项分组。簇的数量事先并不知道。因此，聚类更像是模式检测问题。著名的聚类算法包括K均值聚类、层次聚类和**潜在狄利克雷分配**（**LDA**）。在这里，LDA被表述为一个贝叶斯推理问题。其他使用贝叶斯推理的聚类方法包括贝叶斯混合模型。
- en: Association rule mining is an unsupervised method that finds items that are
    co-occurring in large transactions of data. The market basket analysis, which
    finds the items that are sold together in a supermarket, is based on association
    rule mining. The Apriori algorithm and frequent pattern matching algorithm are
    two main methods used for association rule mining.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则挖掘是一种无监督方法，它在大规模数据交易中寻找共同出现的项。基于关联规则挖掘的市场篮子分析，是寻找在超市中一起销售的物品。Apriori算法和频繁模式匹配算法是用于关联规则挖掘的两种主要方法。
- en: Forecasting is similar to regression, except that the data is a time series
    where there are observations with different values of time stamp and the objective
    is to predict future values based on the current and past values. For this purpose,
    one can use methods such as ARIMA, neural networks, and dynamic Bayesian networks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 预测与回归类似，不同之处在于数据是时间序列，其中存在具有不同时间戳值的观测值，目标是基于当前和过去值预测未来的值。为此，可以使用ARIMA、神经网络和动态贝叶斯网络等方法。
- en: One of the fundamental issues in machine learning is called *the* *curse of
    dimensionality*. Since there can be a large number of features in a machine learning
    model, the typical minimization of error that one has to do to estimate model
    parameters will involve search and optimization in a large dimensional space.
    Most often, data will be very sparse in this higher dimensional space. This can
    make the search for optimal parameters very inefficient. To avoid this problem,
    one tries to project this higher dimensional space into a lower dimensional space
    containing a few important variables. One can then use these lower dimensional
    variables as features. The two well-known examples of dimensional reduction are
    principal component analysis and self-organized maps.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个基本问题被称为**维度诅咒**。由于机器学习模型中可能存在大量特征，为了估计模型参数，通常需要进行大量维度的搜索和优化。在更高维度的空间中，数据通常非常稀疏。这可能会使寻找最佳参数变得非常低效。为了避免这个问题，人们试图将这个高维空间投影到一个包含少数重要变量的低维空间中。然后可以使用这些低维变量作为特征。降维的两个著名例子是主成分分析和自组织映射。
- en: Often, the probability distribution of a population is directly estimated, without
    any parametric models, from a small amount of observed data for making inferences.
    This is called **density estimation**. The simplest form of density estimation
    is histograms, though it is not adequate for many practical applications. The
    more sophisticated density estimations are **kernel density estimation** (**KDE**)
    and vector quantization.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，从少量观察数据中直接估计总体概率分布，而不使用任何参数模型，以进行推断。这被称为**密度估计**。密度估计的最简单形式是直方图，尽管它对于许多实际应用来说并不充分。更复杂的密度估计包括**核密度估计（KDE**）和矢量量化。
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Friedman J., Hastie T., and Tibshirani R. *The Elements of Statistical Learning
    – Data Mining, Inference, and Prediction*. Springer Series in Statistics. 2009
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Friedman J., Hastie T., and Tibshirani R. *《统计学习的要素 – 数据挖掘、推理和预测》*. Springer
    Series in Statistics. 2009
- en: 'Bishop C.M. *Pattern Recognition and Machine Learning (Information Science
    and Statistics)*. Springer. 2006\. ISBN-10: 0387310738'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bishop C.M. *《模式识别与机器学习（信息科学和统计学）》*. Springer. 2006\. ISBN-10: 0387310738'
- en: Summary
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we got an overview of what machine learning is and what some
    of its high-level tasks are. We also discussed the importance of Bayesian inference
    in machine learning, particularly in the context of how it can help to avoid important
    issues, such as model overfit and how to select optimum models. In the coming
    chapters, we will learn some of the Bayesian machine learning methods in detail.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了机器学习是什么以及它的一些高级任务。我们还讨论了贝叶斯推理在机器学习中的重要性，特别是在它如何帮助避免重要问题，例如模型过拟合以及如何选择最佳模型。在接下来的章节中，我们将详细了解一些贝叶斯机器学习方法。
