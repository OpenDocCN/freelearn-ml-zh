- en: Learning to Detect and Track Objects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习检测和跟踪对象
- en: In the previous chapter, you got your hands on deep convolutional neural networks
    and built deep classification and localization networks using transfer learning.
    You have started your deep learning journey and have familiarized yourself with
    a range of deep learning concepts. You now understand how deep models are trained
    and you are ready to learn about more advanced deep learning concepts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你接触了深度卷积神经网络，并使用迁移学习构建了深度分类和定位网络。你已经开始了深度学习之旅，并熟悉了一系列深度学习概念。你现在理解了深度模型的训练过程，并准备好学习更多高级的深度学习概念。
- en: In this chapter, you will continue your deep learning journey, first using object
    detection models to detect multiple objects of different types in a video of a relevant
    scene such as a street view with cars and people. After that, you will learn how
    such models are built and trained.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将继续你的深度学习之旅，首先使用目标检测模型在相关场景的视频中检测不同类型的多个对象，例如带有汽车和人的街景视频。之后，你将学习如何构建和训练这样的模型。
- en: In general, robust object detection models have a wide range of applications
    nowadays. Those areas include but are not limited to medicine, robotics, surveillance,
    and many others. Understanding how they work will allow you to use them for building
    your own real-life applications, as well as elaborating on new models on top of
    them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，鲁棒的目标检测模型在当今有着广泛的应用。这些领域包括但不限于医学、机器人技术、监控以及许多其他领域。了解它们的工作原理将使你能够使用它们来构建自己的实际应用，并在其基础上开发新的模型。
- en: After we cover object detection, we will implement the** Simple Online and Realtime
    Tracking** (**Sort**)algorithm, which is able to robustly track detected objects
    throughout frames. During the implementation of the Sort algorithm, you will also
    get acquainted with the **Kalman filter**, which in general is an important algorithm
    when working with time series.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍目标检测之后，我们将实现**简单在线实时跟踪**（**Sort**）算法，该算法能够鲁棒地在帧之间跟踪检测到的对象。在Sort算法的实现过程中，你还将熟悉**卡尔曼滤波器**，它通常是在处理时间序列时的重要算法。
- en: A combination of a good detector and tracker finds multiple applications in
    industrial problems. In this chapter, we'll limit the applications by counting
    the total objects by their type as they appear throughout the video of the relevant
    scene. Once you understand how this specific task is achieved, you will probably have
    your own usage ideas that will end up in your own applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的检测器和跟踪器的组合在工业问题中有着多种应用。在本章中，我们将通过计算相关场景视频中出现的不同类型的总对象数量来限制应用范围。一旦你理解了如何完成这个特定任务，你可能会产生自己的使用想法，这些想法最终会体现在你的应用中。
- en: For example, having a good object tracker allows you to answer statistical questions
    such as which part of the scene appears more condensed? And, where do objects
    move more slowly or quickly during the observation time? In some scenarios, you
    might be interested in monitoring the trajectories of specific objects, estimating
    their speed or the time that they spend in different areas of the scene. Having
    a good tracker is the solution for all of these things.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，拥有一个好的对象跟踪器可以使你回答诸如场景的哪个部分看起来更密集？以及，在观察时间内，物体移动得更快或更慢的地方在哪里？在某些情况下，你可能对监控特定物体的轨迹、估计它们的速度或它们在场景不同区域停留的时间感兴趣。一个好的跟踪器是解决所有这些问题的方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Preparing the app
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备应用
- en: Preparing the main script
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备主脚本
- en: Detecting objects with SSD
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SSD检测对象
- en: Understanding object detectors
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解对象检测器
- en: Tracking detected objects
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪检测到的对象
- en: Implementing a Sort tracker
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Sort跟踪器
- en: Understanding the Kalman filter
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卡尔曼滤波器
- en: Seeing the app in action
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观看应用的实际运行
- en: Let's start the chapter by pointing out the technical requirements and planning
    the app.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从指出技术要求和规划应用开始本章。
- en: Getting started
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: As mentioned in all of the chapters of the book, you need an appropriate installation
    of **OpenCV**, **SciPy**, and **NumPY**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书的所有章节所提到的，你需要安装合适的**OpenCV**、**SciPy**和**NumPY**。
- en: You can find the code that we present in this chapter at the GitHub repository
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter10](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter10).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 存储库中找到我们本章中展示的代码，网址为[https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter10](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter10)。
- en: When running the app with Docker, the Docker container should have appropriate
    access to the **X11 server**. This app cannot run in **headless mode**. The best
    environment to run the app with Docker is a **Linux** desktop environment. On
    **macOS**, you can use **xQuartz **(refer, to [https://www.xquartz.org/](https://www.xquartz.org/))
    in order to create an accessible X11 server.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Docker 运行应用程序时，Docker 容器应能够访问适当的 **X11 服务器**。此应用程序不能在 **无头模式** 下运行。使用 Docker
    运行应用程序的最佳环境是 **Linux** 桌面环境。在 **macOS** 上，你可以使用 **xQuartz**（参考[https://www.xquartz.org/](https://www.xquartz.org/)）来创建可访问的
    X11 服务器。
- en: You can also use one of the available Docker files in the repository in order
    to run the app.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用存储库中可用的 Docker 文件来运行应用程序。
- en: Planning the app
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划应用程序
- en: 'As mentioned previously, the final app will be able to detect, track, and count
    objects in a scene. This will require the following components:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，最终的应用程序将能够检测、跟踪和计数场景中的对象。这需要以下组件：
- en: '`main.py`: This is the main script for detecting, tracking, and counting objects
    in real time.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main.py`: 这是用于实时检测、跟踪和计数对象的脚本。'
- en: '`sort.py`: This is the module that implements the tracking algorithm.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sort.py`: 这是一个实现跟踪算法的模块。'
- en: We will first prepare the main script. During the preparation, you will learn
    how to use detection networks, as well as how they work and how they are trained.
    In the same script, we will use the tracker to track and count objects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先准备主脚本。在准备过程中，你将学习如何使用检测网络，以及它们的工作原理和训练方法。在同一脚本中，我们将使用跟踪器来跟踪和计数对象。
- en: After preparing the main script, we will prepare the tracking algorithm and
    will be able to run the app. Let's now start with the preparation of the main
    script.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 准备完主脚本后，我们将准备跟踪算法，并能够运行应用程序。现在让我们开始准备主脚本。
- en: Preparing the main script
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备主脚本
- en: The main script will be responsible for the complete logic of the app. It will
    process a video stream and use an object-detection deep convolutional neural network
    combined with the tracking algorithm that we will prepare later in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 主脚本将负责应用程序的完整逻辑。它将处理视频流并使用我们将在本章后面准备的深度卷积神经网络进行对象检测，并结合跟踪算法。
- en: 'The algorithm is used to track objects from frame to frame. It will also be
    responsible for illustrating results. The script will accept arguments and have
    some intrinsic constants, which are defined in the following initialization steps
    of the script:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法用于从帧到帧跟踪对象。它还将负责展示结果。脚本将接受参数并具有一些内在常量，这些常量在脚本以下初始化步骤中定义：
- en: 'As with any other script, we start by importing all the required modules:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与任何其他脚本一样，我们首先导入所有必需的模块：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will use `argparse` as we want our script to accept arguments. We store the
    object classes in a separate file in order not to contaminate our script. Finally,
    we import our `Sort` tracker, which we will build later in the chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `argparse`，因为我们希望我们的脚本接受参数。我们将对象类别存储在单独的文件中，以避免污染脚本。最后，我们导入我们将在本章后面构建的
    `Sort` 跟踪器。
- en: 'Next, we create and parse arguments:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建并解析参数：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our first argument is the input, which can be a path to a video, the ID of a
    camera (`0` for the default camera), or a video stream **Universal Resource Identifier**
    (**URI**). For example, you will be able to connect the app to a remote IP camera
    using the **Real-time Transport Control Protocol** (**RTCP**).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个参数是输入，可以是视频的路径、摄像头的 ID（默认摄像头为 `0`），或视频流的 **统一资源标识符**（**URI**）。例如，你将能够使用
    **实时传输控制协议**（**RTCP**）将应用程序连接到远程 IP 摄像头。
- en: The networks that we will use will predict the bounding boxes of objects. Each
    bounding box will have a score, which will specify how probable it is that the
    bounding box contains an object of a certain type.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的网络将预测对象的边界框。每个边界框都将有一个分数，该分数将指定边界框包含特定类型对象的概率有多高。
- en: The next parameter is `threshold`, which specifies the minimal value of the
    score. If the score is below `threshold`, then we will not consider the detection.
    The last parameter is `mode`, in which we want to run the script. If we run it
    in `detection` mode, the flow of the algorithm will stop after detecting objects and
    will not proceed further with tracking. The results of object detections will
    be illustrated in the frame.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个参数是 `threshold`，它指定了分数的最小值。如果分数低于 `threshold`，则我们不考虑该检测。最后一个参数是 `mode`，我们想要以该模式运行脚本。如果我们以
    `检测` 模式运行，算法的流程将在检测到对象后停止，不会进一步进行跟踪。对象检测的结果将在帧中展示。
- en: 'OpenCV accepts the ID of a camera as an integer. If we specify the ID of a
    camera, the input argument will be a string instead of an integer. Hence, we need
    to convert it to an integer if required:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenCV 接受摄像头的 ID 作为整数。如果我们指定摄像头的 ID，输入参数将是一个字符串而不是整数。因此，如果需要，我们需要将其转换为整数：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we define the required constants:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义所需的常量：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this app, we will track cars and people. We will illustrate bounding boxes
    in a yellowish color and write text in white. We'll also define the standard input
    size of the **Single Shot Detector** (**SSD**) model that we are going to use
    for detection.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用中，我们将追踪汽车和行人。我们将用黄色调的色调展示边界框，并用白色书写文字。我们还将定义我们将要用于检测的 **单次检测器**（**SSD**）模型的标准化输入大小。
- en: Detecting objects with SSD
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SSD 检测对象
- en: 'OpenCV has methods for importing models built with deep learning frameworks.
    We load the TensorFlow SSD model as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 有导入使用深度学习框架构建的模型的方法。我们如下加载 TensorFlow SSD 模型：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first parameter of the `readNetFromTensorflow` method accepts a path to
    a file that contains a TensorFlow model in binary **Protobuf** (**Protocol Buffers**)
    format. The second parameter is optional. It is a path to a text file that contains
    a graph definition of the model, again in Protobuf format.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`readNetFromTensorflow` 方法的第一个参数接受一个包含二进制 **Protobuf**（**协议缓冲区**）格式 TensorFlow
    模型的文件路径。第二个参数是可选的。它是包含模型图定义的文本文件路径，同样也是以 Protobuf 格式。'
- en: Surely, the model file itself might contain the graph definition and OpenCV
    can read that definition from the model file. But, with many networks, it might
    be required to create a separate definition, as OpenCV cannot interpret all operations
    available in TensorFlow and those operations should be replaced with operations
    that OpenCV can interpret.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，模型文件本身可能包含图定义，OpenCV 可以从模型文件中读取该定义。但是，对于许多网络，可能需要创建一个单独的定义，因为 OpenCV 无法解释
    TensorFlow 中所有可用的操作，这些操作应该被 OpenCV 可以解释的操作所替代。
- en: 'Let''s now define functions that will be useful for illustrating detections.
    The first function is for illustrating a single bounding box:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在定义一些有用的函数来展示检测。第一个函数用于展示单个边界框：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'From the previous code, the `illustrate_box` function accepts an image, a normalized
    bounding box as an array of four coordinates specifying two opposite corners of
    the box. It also accepts a caption for the box. Then, the following steps are
    covered in the function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，`illustrate_box` 函数接受一个图像，一个归一化的边界框，作为包含框两个对角顶点的四个坐标的数组。它还接受一个框的标题。然后，函数中涵盖了以下步骤：
- en: 'It first extracts the size of the image:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先提取图像的大小：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It then extracts the two points, scales them by the size of the image, and
    converts them into integers:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它提取两个点，按图像大小进行缩放，并将它们转换为整数：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that, we draw the corresponding `rectangle` using the two points:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们使用两个点绘制相应的 `矩形`：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we put the caption near the first point:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在第一个点附近放置标题：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The second function will illustrate all `detections`, given as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数将展示所有 `检测`，如下所示：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: From the preceding code snippet, the second function accepts detections as a
    two-dimensional `numpy` array and a frame on which it illustrates the detections.
    Each detection consists of the class ID of the detected object, a score specifying
    the probability that the bounding box contains an object of the specified class,
    and the bounding box of the detection itself.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码片段中，第二个函数接受检测作为二维 `numpy` 数组和用于展示检测的帧。每个检测包括被检测对象的类别ID、一个分数，表示边界框包含指定类别对象的概率，以及检测本身的边界框。
- en: The function first extracts the previously stated values for all detections,
    then illustrates each bounding box of the detection using the `illustrate_box`
    methods. The class name and `score` are added as the caption for the box.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先提取所有检测的先前所述值，然后使用`illustrate_box`方法展示每个检测的边界框。类别名称和`score`作为框的标题添加。
- en: 'Let''s now connect to the camera:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们连接到摄像头：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We pass the `input` argument to `VideoCapture`, which, as mentioned previously,
    can be a video file, stream, or camera ID.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`input`参数传递给`VideoCapture`，正如之前提到的，它可以是一个视频文件、流或摄像头ID。
- en: 'Now that we have loaded the network, defined the required functions for illustration,
    and opened the video capture, we are ready to iterate over frames, detect objects,
    and illustrate the results. We use a `for` loop for this purpose:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了网络，定义了所需的说明函数，并打开了视频捕获，我们准备遍历帧、检测对象并展示结果。我们使用`for`循环来完成这个目的：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The body of the loop contains the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 循环体包含以下步骤：
- en: 'It sets the frame as the input of the `detector` network:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将帧设置为`detector`网络的输入：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`blobFromImage` creates a four-dimensional input for the network from the provided
    image. It also resizes the image to the input size and swaps the red and blue
    channels of the image as the network is trained on RGB images, whereas OpenCV
    reads frames in BGR.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`blobFromImage`从提供的图像创建一个四维输入网络。它还将图像调整到输入大小，并将图像的红蓝通道交换，因为网络是在RGB图像上训练的，而OpenCV读取帧为BGR。'
- en: 'Then it makes a prediction with the network and gets the output in the desired
    format:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它使用网络进行预测，并得到所需格式的输出：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From the previous code, `forward` stands for forward propagation. The result
    is a two-dimensional `numpy` array. The first index of the array specifies the
    detection number, and the second index represents a specific detection, which
    is expressed by the object class, score, and four values specifying two corner
    coordinates of the bounding box.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，`forward`代表前向传播。结果是二维的`numpy`数组。数组的第一个索引指定检测编号，第二个索引表示一个特定的检测，它通过对象类别、得分和四个值来表示边界框的两个角坐标。
- en: 'After that, it extracts `scores` from `detections`, and filters out the ones
    that have a very low score:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，它从`detections`中提取`scores`，并过滤掉得分非常低的那些：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the cases when the script is running in `detection` mode, illustrate `detections`
    right away:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当脚本以`detection`模式运行时，立即展示`detections`：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we have to set termination criteria:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们必须设置终止条件：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we have everything ready to run our script in detection mode. A sample
    result is shown in the image that follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好以检测模式运行我们的脚本。接下来的图像显示了样本结果：
- en: '![](img/c5b10d54-39ae-4182-bbc9-b48ba3457076.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5b10d54-39ae-4182-bbc9-b48ba3457076.png)'
- en: You can note in the frame from the preceding image that the SSD model has successfully
    detected all the cars and the single individual (person) visible in the scene.
    Let's now look at how we can use other object detectors.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在前一张图像的帧中注意到，SSD模型已经成功检测到场景中所有的汽车和单个个体（人）。现在让我们看看如何使用其他目标检测器。
- en: Using other detectors
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他检测器
- en: In this chapter, we are using an object detector to get bounding boxes with
    their object types, which will be further processed by the Sort algorithm for
    tracking. In general, it does not matter by what exact means the boxes are obtained.
    In our case, we have used an SSD pre-trained model. Let's now understand how to
    replace it with a different model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用一个目标检测器来获取带有其对象类型的边界框，这些边界框将被Sort算法进一步处理以进行跟踪。一般来说，获取这些边界框的确切方式并不重要。在我们的案例中，我们使用了SSD预训练模型。现在让我们了解如何用不同的模型替换它。
- en: Let's first understand how we can use YOLO for this purpose. YOLO is also a
    single-stage detector and stands for **You Only Look Once** (**YOLO**). The original
    YOLO models are based on **Darknet**, which is another open-source neural network
    framework and is written in C++ and CUDA. OpenCV has the ability to load networks
    based on Darknet, similarly to how it loads TensorFlow models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解如何使用YOLO来完成这个目的。YOLO也是一个单阶段检测器，代表**You Only Look Once**（**YOLO**）。原始YOLO模型基于**Darknet**，这是一个另一个开源神经网络框架，用C++和CUDA编写。OpenCV能够加载基于Darknet的网络，类似于它加载TensorFlow模型的方式。
- en: In order to load a YOLO model, you should first download the files containing
    the network configuration and the network weights.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载YOLO模型，您首先需要下载包含网络配置和网络权重的文件。
- en: The latter can be done by visiting [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
    In our case, as an example, we will use **YOLOv3-tiny**, which is the most lightweight
    one at the time of writing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过访问[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)来完成。在我们的例子中，我们将使用**YOLOv3-tiny**，这是当时最轻量级的版本。
- en: 'Once you have downloaded the network configuration and weights, you can load
    them similarly to how you loaded the SSD model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下载了网络配置和权重后，你可以像加载SSD模型一样加载它们：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The difference is that the `readNetFromDarknet` function is used instead of `readNetFromTensorflow`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于，使用`readNetFromDarknet`函数而不是`readNetFromTensorflow`。
- en: 'In order to use this detector instead of the SSD, we have several things to
    do:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这个检测器代替SSD，我们有一些事情要做：
- en: 'We have to change the size of the input:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须更改输入的大小：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The network is originally trained in with the specified size. If you have a
    high-resolution input video stream and you want the network to detect small objects
    in the scene, you can set the input to a different size, which is a multiplier
    of 160, for example, size (640, 480). The larger the input size, the more small
    objects will be detected, but the network will make predictions slower.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 网络最初是在指定的尺寸下训练的。如果你有一个高分辨率的输入视频流，并且希望网络检测场景中的小物体，你可以将输入设置为不同的尺寸，例如160的倍数，例如尺寸（640，480）。输入尺寸越大，检测到的物体越小，但网络预测会变慢。
- en: 'We have to change class names:'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须更改类名：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Although the YOLO network is trained on the **COCO** dataset, the IDs of the
    objects are different. You can still run with the previous class names, but you
    will have the wrong names of the classes in that case.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管YOLO网络是在**COCO**数据集上训练的，但对象的ID是不同的。在这种情况下，你仍然可以使用之前的类名运行，但那样你会得到错误的类名。
- en: You can download the file from the darknet repository [https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从darknet仓库[https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet)下载文件。
- en: 'We have to slightly change the input:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须稍微更改输入：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In comparison with the input for SSD, we add `scalefactor`, which normalizes
    the input.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与SSD的输入相比，我们添加了`scalefactor`，它对输入进行归一化。
- en: Now we are ready to successfully make predictions. Although, we are not completely
    ready to display the results with this detector. The problem is that the predictions
    of the YOLO model have a different format.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好成功地进行预测。尽管如此，我们还没有完全准备好使用这个检测器显示结果。问题是YOLO模型的预测格式不同。
- en: 'Each detection consists of the coordinates of the center of the bounding box:
    the width, the height of the bounding box, and a one-hot vector representing the
    probabilities of each type of object in the bounding box. In order to finalize
    the integration, we have to bring the detections in the format that we use in
    the app. The latter can be accomplished with the following steps:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每个检测都包括边界框中心的坐标：边界框的宽度和高度，以及一个表示边界框中每种类型对象概率的one-hot向量。为了最终完成集成，我们必须将检测转换为我们在应用中使用的格式。这可以通过以下步骤完成：
- en: 'We extract the center coordinates of the bounding boxes:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提取边界框的中心坐标：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then also extract the width and height of the bounding boxes:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们也提取边界框的宽度和高度：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we extract `scores_one_hot`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们提取`scores_one_hot`：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we find the `class_ids` of the maximum scores:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们找到最大分数的`class_ids`：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After that, we extract the maximum scores:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们提取最大分数：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we construct `detections` in the format consumed by the rest of the app
    using the results obtained in the previous steps:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用前一步骤获得的结果，以应用其他部分所需格式构建`detections`：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now we can successfully run the app with the new detector. Depending on your
    needs, the available resources, and the required accuracy, you might want to use
    other detection models, such as other versions of SSD or **Mask-RCNN**, which
    is one of the most accurate object detection networks at the time of writing,
    although it is much slower than the SSD models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以成功运行带有新检测器的应用。根据你的需求、可用资源和所需的精度，你可能希望使用其他检测模型，例如SSD的其他版本或**Mask-RCNN**，这是当时最准确的对象检测网络之一，尽管它的速度比SSD模型慢得多。
- en: You can try to load your model of choice with OpenCV, as we have done for both
    YOLO and SSD in this chapter. With this approach, you might encounter difficulties
    loading the model. For example, you might have to adapt the network configuration
    such that all the operations in the network can be processed by OpenCV.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试使用OpenCV加载你选择的模型，就像我们在本章中为YOLO和SSD所做的那样。使用这种方法，你可能会遇到加载模型的困难。例如，你可能需要调整网络配置，以便网络中的所有操作都可以由OpenCV处理。
- en: The latter is particularly due to the fact that modern deep learning frameworks
    develop quite fast and OpenCV at least needs time to catch up to include all new
    operations. Another approach that you might prefer is to run a model using the
    original framework, similarly to what we did in [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml),
    *Learning to Classify and Localize Objects*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 后者尤其是因为现代深度学习框架发展非常快，而OpenCV至少需要时间来赶上，以便包括所有新的操作。你可能更喜欢的一种方法是使用原始框架运行模型，就像我们在[第9章](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml)，“学习分类和定位物体”中所做的那样。
- en: So now that we understand how to use detectors, let's look at how they work
    in the next section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然我们已经了解了如何使用检测器，那么让我们在下一节中看看它们是如何工作的。
- en: Understanding object detectors
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解物体检测器
- en: In [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml),* Learning to Classify
    and Localize Objects*, we learned how to use the feature maps of a certain layer
    of a convolutional neural network to predict the bounding box of an object in
    the scene, which in our case was a head.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml)，“学习分类和定位物体”中，我们学习了如何使用卷积神经网络某一层的特征图来预测场景中物体的边界框，在我们的例子中是头部。
- en: You might note that the difference between the localization network that we
    composed and the detection networks (that we used in this chapter) is that the
    detection networks predict multiple bounding boxes instead of a single one, as
    well as assigning a class to each of the bounding boxes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，我们组成的定位网络与检测网络（我们在本章中使用）之间的区别在于，检测网络预测多个边界框而不是一个，并为每个边界框分配一个类别。
- en: Let's now make a smooth transition between the two architectures so that you
    can understand how object detection networks like YOLO and SSD work.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在这两种架构之间进行平滑过渡，以便你可以理解像YOLO和SSD这样的物体检测网络是如何工作的。
- en: The single-object detector
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单物体检测器
- en: First of all, let's look at how to predict the class in parallel with the box.
    In [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml),* Learning to Classify
    and Localize Objects*, you also learned how to make a classifier. Nothing limits
    us to combining classification with localization in a single network. That is
    done by connecting the classification and localization blocks to the same feature
    map of the base network and training it all together with a loss function, which
    is a sum of localization and classification losses. You can create and train such
    a network as an exercise.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何与边界框并行预测类别。在[第9章](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml)，“学习分类和定位物体”中，你也学习了如何制作一个分类器。没有什么限制我们只能在单个网络中将分类与定位结合起来。这是通过将分类和定位块连接到基础网络的相同特征图，并使用损失函数（定位和分类损失的加和）一起训练来实现的。你可以作为一个练习创建并训练这样的网络。
- en: The question remains, *what if there is no object in the scene?* To resolve
    this, we can simply add one more class that corresponds to the background and
    assign zero to the loss of the bounding box predictor when training. As a result,
    you will have a detector that detects multiple classes of objects but can only
    detect one object in the scene. Let's now look at how we can predict multiple
    boxes instead of one, and hence, arrive at a complete architecture of an object
    detector.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 问题仍然存在，*如果场景中没有物体怎么办？* 为了解决这个问题，我们可以在训练时简单地添加一个与背景相对应的额外类别，并将边界框预测器的损失设置为0。结果，你将拥有一个可以检测多个类别的物体检测器，但只能检测场景中的一个物体。现在让我们看看我们如何预测多个边界框而不是一个，从而得出物体检测器的完整架构。
- en: The sliding-window approach
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滑动窗口方法
- en: One of the earliest approaches to create an architecture that can detect multiple
    objects in the scene was the **sliding-window** approach. With this approach,
    you first build a classifier for objects of interest. Then, you pick a rectangle
    (a window) of a size that is several or many times smaller than the image where
    you want to detect an object. After that, you slide it across all possible locations
    in the image and classify whether there is an object of the chosen type in each
    position of the rectangle.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 最早用于创建能够检测场景中多个对象的架构的方法之一是**滑动窗口**方法。使用这种方法，你首先为感兴趣的对象构建一个分类器。然后，你选择一个矩形（窗口）的大小，这个大小是图像大小的几倍或许多倍，你希望在图像中检测对象。之后，你将其滑动到图像的所有可能位置，并判断矩形中的每个位置是否包含所选类型的对象。
- en: During sliding, a sliding size of between a fraction of the box size and the
    complete box size is used. The procedure is repeated with different sizes of the
    sliding window. Finally, you pick the window positions that have a class score
    above some threshold and you report that those window positions with their sizes
    are the bounding boxes of the chosen object classes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在滑动过程中，使用介于框大小的一部分和完整框大小之间的滑动窗口大小。使用不同大小的滑动窗口重复此过程。最后，你选择那些具有高于某个阈值的类别分数的窗口位置，并报告这些窗口位置及其大小是所选对象类别的边界框。
- en: The problem with this approach is, first of all, that a lot of classifications
    should be done on a single image, and hence the architecture of the detector will
    be quite heavy. Another problem is that the objects are localized only with the
    precision of the sliding size. Also, the sizes of the detection bounding boxes
    have to be equal to the sizes of the sliding windows. Surely, the detection could
    be improved if the slide size was reduced and the number of window sizes was increased,
    but this would result in an even greater computational cost.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，首先，需要在单张图像上执行大量的分类，因此检测器的架构将会相当庞大。另一个问题是，对象仅以滑动窗口大小的精度进行定位。此外，检测边界框的大小必须等于滑动窗口的大小。当然，如果减小滑动窗口的大小并增加窗口大小的数量，检测效果可能会得到改善，但这将导致更高的计算成本。
- en: One of the ideas you already might have come up with is to combine the single-object
    detector with the sliding-window approach and take advantage of both.  For example,
    you could split the image into regions. For example, we could take a 5 x 5 grid
    and run the single-object detector in each cell of the grid.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经想到的一个想法是将单目标检测器与滑动窗口方法结合起来，并利用两者的优势。例如，你可以将图像分割成区域。例如，我们可以取一个5 x 5的网格，并在网格的每个单元格中运行单目标检测器。
- en: You could go even further by creating more grids with a larger or smaller size,
    or by making the grid cells overlap. As a mini-project to get a deep understanding
    of the ideas covered, you might like to implement them and play with the results.
    Still, with these approaches, we make the architectures heavier, that is, once
    we enlarge the grid size or the number of grids in order to improve the accuracy.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以更进一步，通过创建更大或更小的网格，或者使网格单元格重叠。作为一个迷你项目，为了深入理解所涵盖的想法，你可能喜欢实现它们并玩弄结果。然而，使用这些方法，我们使架构更重，也就是说，一旦我们扩大网格大小或网格数量以改进准确性。
- en: Single-pass detectors
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单次遍历检测器
- en: In the previously stated ideas, we have used single-object classification or
    detection networks to achieve multiple-object detection. In all scenarios, for
    each predefined region, we feed the network with the complete image or part of
    it multiple times. In other words, we have multiple passes that result in heavy
    architecture.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前提到的想法中，我们使用了单目标分类或检测网络来实现多目标检测。在所有场景中，对于每个预定义的区域，我们多次将整个图像或其部分输入到网络中。换句话说，我们有多重遍历，导致架构庞大。
- en: '*Wouldn''t it be nice to have a network that, once fed with an image, detects
    all the objects in the scene in a single pass?* An idea that you could try is
    to make more outputs for our single-object detector so that it predicts multiple
    boxes instead of one. This is a good idea, but there is a problem. Suppose we
    have multiple dogs in the scene that could appear in different locations and in
    different numbers.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*拥有一个网络，一旦输入图像，就能在单次遍历中检测场景中的所有对象，这不是很好吗？* 你可以尝试的一个想法是为我们的单目标检测器创建更多的输出，使其预测多个框而不是一个。这是一个好主意，但存在问题。假设场景中有多个狗，它们可能出现在不同的位置和不同的数量。'
- en: '*How should we make an invariant correspondence between the dogs and the outputs?*
    If we make an attempt to train such a network by assigning boxes to the outputs,
    for example, from left to right, we will simply end up with predictions that are
    close to the average value of all positions.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们应该如何使狗和输出之间建立一种不变对应关系呢？* 如果我们尝试通过将盒子分配给输出，例如从左到右，来训练这样一个网络，那么我们最终得到的预测结果将接近所有位置的平均值。'
- en: 'Networks such as SSD and YOLO tackle the issues and implement multiscale and
    multibox detection in a single pass. We can sum up their architecture with the
    following three components:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: SSD和YOLO等网络处理这些问题，并在单次遍历中实现多尺度和多框检测。我们可以用以下三个组件来总结它们的架构：
- en: First of all, they have a position-aware multibox detector connected to a feature
    map. We have discussed the training problem that arises when connecting several
    box predictors to the complete feature map. The problem with SSD and YOLO is solved
    by having a predictor that is connected to a small region of the feature map instead
    of a complete feature map.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它们有一个位置感知的多框检测器连接到特征图。我们已经讨论了将多个框预测器连接到完整特征图时出现的训练问题。SSD和YOLO的问题通过有一个连接到特征图小区域的预测器来解决，而不是连接到完整特征图。
- en: It predicts boxes in the region of the image that corresponds to just that exact
    region of the feature map. Then, the same predictor predicts across all possible
    locations of the feature map. This operation is implemented using convolutional
    layers. There are convolutional kernels, with their activations, that slide across
    the feature map and have coordinates and classes as their output feature maps.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它预测图像中与特征图对应区域的框。然后，相同的预测器在整个特征图的所有可能位置上进行预测。这个操作是通过卷积层实现的。存在一些卷积核，它们的激活在特征图上滑动，并具有坐标和类别作为它们的输出特征图。
- en: For example, you can obtain a similar operation if you go back to the code of
    the localization model and replace the last two layers, which flatten the output
    and create four fully connected neurons for predicting box coordinates with a
    convolutional layer with four kernels. Also, since the predictors act in a certain
    region and are aware only about that region, they predict coordinates that are
    relative to that region, instead of predicting coordinates that are relative to
    the complete image.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你回到定位模型的代码，并用具有四个核的卷积层替换最后两层，这些层将输出展平并创建四个全连接神经元来预测框坐标，你就可以获得类似的操作。此外，由于预测器在特定区域工作，并且只对该区域有意识，它们预测的坐标是相对于该区域的，而不是相对于完整图像的坐标。
- en: Both YOLO and SSD predict several boxes in each location instead of a single
    one. They predict offset coordinates from several **default boxes**, which are
    also called **anchor boxes**. These boxes are chosen sizes and shapes that are
    close to the objects in the dataset or the natural scene, so that relative coordinates
    have small values and even the default boxes match the object bounding boxes pretty
    well.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO和SSD在每个位置都预测多个框，而不是一个。它们从几个**默认框**预测偏移坐标，这些默认框也称为**锚框**。这些框的大小和形状与数据集中或自然场景中的对象非常接近，因此相对坐标值较小，甚至默认框与对象边界框也非常吻合。
- en: For example, a car usually appears as a wide box and a person usually appears
    as a tall box. Multiple boxes allow you to achieve better accuracy as well as
    to have multiple predictions in the same area. For example, if a person is sitting
    on a bike somewhere in the image and we have a single box, then we would omit
    one of the objects. With multiple anchor boxes, the objects will correspond to
    different anchor boxes.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一辆车通常表现为一个宽框，而一个人通常表现为一个高框。多个框允许你达到更高的精度，并在同一区域进行多个预测。例如，如果图像中某处有一个人坐在自行车上，而我们只有一个框，那么我们就会忽略其中一个对象。使用多个锚框，对象将对应于不同的锚框。
- en: Besides having multisize anchor boxes, they use several feature maps with different
    sizes to accomplish multiscale prediction. If the prediction module is connected
    to the top feature maps of the network with a small size, it is responsible for
    large objects.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了具有多尺寸锚框外，它们还使用不同尺寸的多个特征图来完成多尺度预测。如果预测模块连接到网络的小尺寸顶部特征图，它负责大对象。
- en: If it is connected to one of the bottom feature maps, it is responsible for
    small objects. Once all the multibox predictions in the chosen feature maps are
    made, the results are translated to the absolute coordinates of the image and
    concatenated. As a result, we obtain the predictions in the form that we used
    in this chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果它与底部的某个特征图相连，它负责小物体。一旦在所选特征图中做出所有多框预测，结果将被转换为图像的绝对坐标并连接起来。因此，我们获得了本章中使用的预测形式。
- en: If you are interested in more implementation details, we advise you to read
    corresponding papers, as well as to analyze the corresponding implementation code.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣更多实现细节，我们建议你阅读相应的论文，以及分析相应的实现代码。
- en: So now that you understand how the detectors work, you are probably also interested
    in the principles of their training. However, before we understand those principles,
    let's understand the metric called **Intersection over Union**, which is heavily
    used when training and evaluating these networks as well as filtering their predictions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你现在已经了解了探测器的工作原理，你可能也对它们的训练原理感兴趣。然而，在我们理解这些原理之前，让我们先了解一个称为**交并比**的度量，它在训练和评估这些网络以及过滤它们的预测时被广泛使用。
- en: We will also implement a function to compute this metric, which we will use
    when building the Sort algorithm for tracking. Hence, you should note that understanding
    this metric is important not only for object detection but also for tracking.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将实现一个计算此度量的函数，我们将在构建跟踪的排序算法时使用它。因此，你应该注意，理解这个度量不仅对目标检测很重要，对跟踪也很重要。
- en: Learning about Intersection over Union
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习交并比
- en: '**Intersection over U****nion** (**IoU**), which is also called the **Jaccard
    index**, is defined as the size of the intersection divided by the size of the
    union and has the following formula:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**交并比**（**IoU**），也称为**Jaccard指数**，定义为交集大小除以并集大小，其公式如下：'
- en: '![](img/9e471002-bc57-445c-9aec-b4281d4824a6.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9e471002-bc57-445c-9aec-b4281d4824a6.png)'
- en: 'That formula is equivalent to the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式等同于以下公式：
- en: '![](img/889c18e0-6464-4bdf-9561-0af7113b054e.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/889c18e0-6464-4bdf-9561-0af7113b054e.png)'
- en: 'In the following diagram, we illustrate IoU for two boxes:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们展示了两个框的交并比（IoU）：
- en: '![](img/b405602e-a935-4138-b019-46269ae54bc7.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b405602e-a935-4138-b019-46269ae54bc7.png)'
- en: In the previous diagram, the union is the total area of the complete figure
    and the intersection is the part where the boxes overlap. The IoU can have a value
    in the range of (0,1) and reaches the maximal value only when the boxes match
    exactly. Once the boxes are separated, it becomes zero.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，并集是整个图形的总面积，交集是框重叠的部分。交并比（IoU）的值在（0,1）范围内，只有当框完全匹配时才达到最大值。一旦框分离，它变为零。
- en: 'Let''s define a function that accepts two bounding boxes and returns their
    `iou` value:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，它接受两个边界框并返回它们的`iou`值：
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In order to calculate the `iou` value, the following steps are necessary:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算`iou`值，以下步骤是必要的：
- en: 'We first extract the top-left and bottom-right coordinates of both bounding
    boxes:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先提取两个边界框的左上角和右下角坐标：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we get the element-wise `maximum` of the two top-left corners:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们得到两个左上角元素级的`最大值`：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The two arrays are compared element-wise and the result will be a new array
    containing the larger values of the corresponding indexes in the array. In our
    case, maximum *x* and *y* coordinates are obtained and stored in `int_tl`. If
    the boxes intersect, this is the top-left corner of the intersection.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数组进行元素级比较，结果将是一个新数组，包含数组中相应索引的较大值。在我们的情况下，获得并存储在`int_tl`中的最大*x*和*y*坐标。如果框相交，这是交集的左上角。
- en: 'Then, we get the element-wise `minimum` of the bottom-right corners:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们得到右下角元素级的`最小值`：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Similar to the previous case, this is the bottom-right corner of the intersection
    if the boxes intersect.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一种情况类似，如果框相交，这是交集的右下角。
- en: 'Then, we calculate areas of the bounding boxes:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算边界框的面积：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The difference between the bottom-right and the top-left corner coordinates
    of a box is the width and height of the box, hence the product of the elements
    of the resulting array is the area of the bounding box.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 框的右下角和左上角的坐标差是框的宽度和高度，因此结果数组的元素乘积是边界框的面积。
- en: 'After that, we calculate the intersection area:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算交集区域：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: If the boxes do not overlap, at least one element of the resulting array will
    be negative. Negative values are replaced with zeros. Hence, in such cases, the
    area is zero, as expected.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果框没有重叠，结果数组中至少有一个元素将是负数。负值被替换为零。因此，在这种情况下，面积为零，正如预期的那样。
- en: 'And at last, we calculate IoU and `return` the result:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算IoU并`返回`结果：
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: So, now that you have understood what IoU is and have built a function to compute
    it, you are ready to learn how the detection networks used are trained.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在你已经理解了什么是IoU，并且已经构建了一个计算它的函数，你就可以学习如何训练所使用的检测网络了。
- en: Training SSD- and YOLO-like networks
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练SSD和YOLO类似的网络
- en: You are already aware that networks such as YOLO and SSD predict objects with
    predefined anchor boxes. Out of all available boxes, only one box is chosen, which
    corresponds to the object. During prediction time, the box is assigned with the
    class of the object and the offsets are predicted.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道，YOLO和SSD等网络使用预定义的锚框来预测对象。在所有可用的框中，只有一个框被选中，对应于对象。在预测时间，该框被分配给对象的类别，并预测偏移量。
- en: '*So, the question is, how do we choose that single box?* You might already
    have guessed that IoU is used for that purpose. The correspondence between the
    ground truth boxes and anchor boxes can be made as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*那么，问题来了，我们如何选择那个单独的框呢？* 你可能已经猜到了，IoU就是用来这个目的的。真实框和锚框之间的对应关系可以如下建立：'
- en: Create a matrix that contains all IoU values of all possible ground truth and
    anchor box pairs. Say, the row corresponds to the ground truth box and the column
    corresponds to anchor box.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个矩阵，包含所有可能的真实框和锚框对的IoU值。比如说，行对应于真实框，列对应于锚框。
- en: Find the maximal element in the matrix and assign the corresponding boxes to
    each other. Remove the row and column of the maximal element from the matrix.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在矩阵中找到最大元素，并将相应的框分配给对方。从矩阵中移除最大元素的行和列。
- en: Repeat *step 2* until there are no ground truth boxes available, or in other
    words, until all the rows of the matrix are removed.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*，直到没有可用的真实框，换句话说，直到矩阵的所有行都被移除。
- en: Once the assignment is done, all that is left to do is to define a loss function
    for each box, sum the results as the total loss and train the network. The loss
    for the box offsets bounding boxes which contain objects can be simply defined
    as IoU—the greater the IoU, the closer the bounding box is to the ground truth,
    hence, it's negated value should be reduced.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成分配的任务，剩下的就是为每个框定义一个损失函数，将结果相加作为总损失，并训练网络。对于包含对象的框的偏移量损失可以简单地定义为IoU——IoU值越大，边界框就越接近真实值，因此其负值应该减少。
- en: The anchor boxes that do not contain objects do not contribute to the loss.
    The loss of object classes is also straightforward—the anchor boxes that do not
    have assignments are trained with the background class and the ones that do have
    assignments are trained with their corresponding classes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不包含对象的锚框不会对损失做出贡献。对象类别的损失也很直接——没有分配的锚框用背景类别训练，而有分配的锚框则用它们对应的类别训练。
- en: Each of the considered networks has some modifications to the described loss
    so that it achieves better performance on the specific network. You can pick a
    network and define the loss described here on your own, which will be a good exercise
    for you. If you are building your own app and you need the corresponding trained
    network with relatively high accuracy in a limited amount of time, you might consider
    using the training methods that come with the code base of the corresponding network.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到的每个网络都对描述的损失进行了一些修改，以便在特定网络上实现更好的性能。你可以选择一个网络，并自己定义这里描述的损失，这将是一个很好的练习。如果你正在构建自己的应用程序，并且需要在有限的时间内获得相对高精度的相应训练网络，你可能会考虑使用相应网络代码库中附带的训练方法。
- en: So, now that you have understood how to train these networks, let's continue
    the `main` script of the app and integrate it with the Sort tracker in the next
    section.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在你已经了解了如何训练这些网络，让我们继续应用程序的`main`脚本，并在下一节中将其与Sort跟踪器集成。
- en: Tracking detected objects
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪检测到的对象
- en: Once we can successfully detect objects in each frame, we can track them by
    associating detections between frames. As mentioned previously, in this chapter,
    we are using the Sort algorithm for multiple-object tracking, which stands for
    **Simple Online and Realtime Tracking**.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们能够在每一帧中成功检测到物体，我们就可以通过关联帧之间的检测来跟踪它们。如前所述，在本章中，我们使用Sort算法进行多目标跟踪，该算法代表**简单在线实时跟踪**。
- en: Given sequences of multiple bounding boxes, this algorithm associates the boxes
    of sequence elements and fine-tunes the bounding box coordinates based on physical
    principles. One of the principles is that a physical object cannot rapidly change
    its speed or direction of movement. For example, under normal conditions, a moving
    car cannot reverse its movement direction between two consequent frames.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 给定多个边界框的序列，此算法关联序列元素的边界框，并根据物理原理微调边界框坐标。其中一个原则是物理对象不能迅速改变其速度或运动方向。例如，在正常条件下，一辆行驶的汽车不能在连续两帧之间改变其运动方向。
- en: 'We suppose that the detector annotates the objects correctly and we instantiate
    one **Multiple Object Trackers** (`mots`) for each class of objects that we want
    to track:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设检测器正确标注了物体，并且为我们要跟踪的每个物体类别实例化一个**多目标跟踪器**（`mots`）：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We store the instances in a  dictionary. The keys in the dictionary are set
    to the corresponding class IDs. We will track the detected objects using the following function:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实例存储在字典中。字典的键设置为相应的类ID。我们将使用以下函数跟踪检测到的物体：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The function accepts detections and an optional illustration frame. The main
    loop of the function iterates over the multi-object trackers that we have instantiated.
    Then, for each multi-object tracker, the following steps are covered:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受检测和可选的插图帧。函数的主循环遍历我们所实例化的多目标跟踪器。然后，对于每个多目标跟踪器，以下步骤被覆盖：
- en: 'We first extract detections of the object type of the current multi-object
    tracker from all the passed detections:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从所有传递的检测中提取当前多目标跟踪器类型的物体检测。
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we update the tracker by passing the bounding boxes of the current object
    type to the `update` method of the tracker:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过将当前物体类型的边界框传递给跟踪器的`update`方法来更新跟踪器：
- en: '[PRE38]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `update` method returns the bounding box coordinates of the tracked objects
    associated with the IDs of the object.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`update`方法返回与物体ID关联的跟踪物体的边界框坐标。'
- en: 'If the illustration frame is provided, illustrate the boxes in the frame:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提供了插图帧，则在帧中描绘边界框：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: For each returned result, the corresponding bounding box will be drawn using
    our previously defined `illustrate_box` function. Each box will be annotated with
    the class name and the ID of the box.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个返回的结果，将使用我们之前定义的`illustrate_box`函数绘制相应的边界框。每个框将标注类别名称和框的ID。
- en: 'We also want to define a function that will print general information about
    tracking on the frame:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想要定义一个函数，该函数将打印关于跟踪在帧上的通用信息：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For each class of tracked objects, the function will write the total number
    of tracked objects and the number of currently tracked objects.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个跟踪物体的类别，该函数将写入跟踪物体的总数和当前跟踪物体的数量。
- en: 'Now that we have defined the functions for tracking and illustration, we are
    ready to modify the main loop, which iterates over frames, so that we can run
    our app in tracking mode:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了跟踪和插图函数，我们准备修改主循环，该循环遍历帧，以便我们可以以跟踪模式运行我们的应用程序：
- en: '[PRE41]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: From the previous snippet, if the app runs in tracking mode, the detected objects
    of the chosen classes will be tracked throughout frames using our `track` function
    and tracking information will be shown on the frame.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码片段中，如果应用程序以跟踪模式运行，所选类别的检测到的物体将使用我们的`track`函数在整个帧中进行跟踪，并且跟踪信息将显示在帧上。
- en: What's left to do is to elaborate on the tracking algorithm in order to finalize
    the complete app. We will do that in the next section with the help of the Sort
    tracker.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作是对跟踪算法进行详细阐述，以便最终完成整个应用程序。我们将借助Sort跟踪器在下一节中完成这项工作。
- en: Implementing a Sort tracker
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Sort跟踪器
- en: The Sort algorithm is a simple yet robust real-time tracking algorithm for the
    multiple-object tracking of detected objects in video sequences. The algorithm
    has a mechanism to associate detections and trackers that results in a maximum
    of one detection box for each tracked object.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 排序算法是一种简单而健壮的实时跟踪算法，用于视频序列中检测到的对象的多个对象跟踪。该算法具有一种关联检测和跟踪器的机制，结果是在每个跟踪对象上最多有一个检测框。
- en: For each tracked object, the algorithm creates an instance of a single object-tracking
    class. Based on physical principles such as an object cannot rapidly change size
    or speed, the class instance can predict the feature location of the object and
    maintain tracking from frame to frame. The latter is achieved with the help of
    the **Kalman** filter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个跟踪对象，算法创建一个单一对象跟踪类的实例。基于物体不能快速改变大小或速度的物理原理，该类实例可以预测物体的特征位置，并从一帧到另一帧保持跟踪。后者是通过
    **卡尔曼** 滤波器实现的。
- en: 'We import the modules that we will use in the implementation of the algorithm
    as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下方式导入在算法实现中将要使用的模块：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As usual, the main dependencies are `numpy` and OpenCV. The unfamiliar `linear_sum_assignment` method
    will be used when associating detected objects with tracked ones.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，主要依赖项是 `numpy` 和 OpenCV。在将检测到的对象与跟踪对象关联时，将使用不熟悉的 `linear_sum_assignment`
    方法。
- en: Let's now dive into the algorithm by first understanding what the Kalman Filter
    is, which is used in the implementation of a single box tracker in the next section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入算法，首先了解卡尔曼滤波器是什么，这是在下一节实现单个框跟踪器时使用的。
- en: Understanding the Kalman filter
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卡尔曼滤波器
- en: The Kalman filter is a statistical model that has a wide range of applications
    in signal processing, control theory, and statistics. The Kalman filter is a complex
    model, but it could be thought of as an algorithm to **de-noise** the observations
    of an object that contain a lot of noise over time when we know the dynamics of
    the system with certain accuracy.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 卡尔曼滤波器是一个在信号处理、控制理论和统计学中有广泛应用的统计模型。卡尔曼滤波器是一个复杂的模型，但可以将其视为一个算法，当我们对系统的动力学有一定准确性的了解时，它可以**去噪**包含大量噪声的物体的观察结果。
- en: Let's look at an example, to illustrate how the Kalman filter works. Imagine
    we want to find the location of a train that moves on rails. The train will have
    a velocity, but unfortunately, the only measurements we have are from radar, which
    only shows the location of the train.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，以说明卡尔曼滤波器是如何工作的。想象一下，我们想要找到在铁轨上移动的火车的位置。火车将有一个速度，但不幸的是，我们唯一拥有的测量数据来自雷达，它只显示火车的位置。
- en: 'We would like to accurately measure the location of the train. If we were to
    look at each radar measurement, we could learn the location of the train from
    it, but what if the radar is not very reliable and has high measurement noise.
    For example, the locations that radar reported are as shown in the following diagram:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望准确地测量火车的位置。如果我们查看每个雷达测量值，我们可以从中学习到火车的位置，但如果雷达不太可靠并且有高测量噪声怎么办。例如，雷达报告的位置如下所示：
- en: '![](img/ebcecf52-a143-4b3b-8b77-b4f520b2e9e7.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](img/ebcecf52-a143-4b3b-8b77-b4f520b2e9e7.png)'
- en: '*What can we tell about the real location of the train at 3 p.m.?* Well, there
    is a possibility that the train was at position 5, but since we know that trains
    are heavy and change their speed very slowly, it would be very hard for the train
    to reverse its direction of travel twice in quick succession, to go to position
    5 and back. So, we can use some knowledge of how things work, and the previous
    observations, to make more reliable predictions about the location of the train.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们如何了解火车在下午3点的实际位置？* 好吧，有一种可能性是火车在位置5，但因为我们知道火车很重，速度变化很慢，所以火车很难在短时间内两次改变行驶方向，到达位置5然后再返回。因此，我们可以利用对事物工作原理的一些了解以及之前的观察，来对火车的位置做出更可靠的预测。'
- en: 'For example, if we assumed that we could describe the train by its location
    and velocity, we would define the state to be the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们假设我们可以用位置和速度来描述火车，我们会定义状态如下：
- en: '![](img/750b462b-f85a-4033-950e-5dad31c8ac07.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](img/750b462b-f85a-4033-950e-5dad31c8ac07.png)'
- en: Here, *x* is the location of the train and *v* is the velocity of the train.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是火车的位置，*v* 是火车的速度。
- en: 'Now we need a way to describe our model of the world, which is called the **state-transition
    model**—for a train, it is simple:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一种方式来描述我们的世界模型，这被称为 **状态转换模型**——对于火车来说，它是简单的：
- en: '![](img/1ad60428-2633-4c3e-b254-98e617c14c06.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ad60428-2633-4c3e-b254-98e617c14c06.png)'
- en: 'We could write this in a matrix form using the state variable, *s*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用状态变量 *s* 将其写成矩阵形式：
- en: '![](img/d3421676-135c-46cd-b1ad-eaa96ee347ba.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d3421676-135c-46cd-b1ad-eaa96ee347ba.png)'
- en: The matrix, *F*, is called the **state-transition matrix**.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *F* 被称为**状态转移矩阵**。
- en: 'As such, we believe that the train doesn''t change its velocity and moves at
    a constant speed. This means that there should be a straight line on the graph
    of observations, but that''s too restrictive and we know that no real system behaves
    that way, so we allow for some noise being present in the system, that is, **process
    noise**:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们相信火车不会改变其速度，并以恒定速度移动。这意味着在观测值的图表上应该有一条直线，但这太严格了，我们知道没有真实的系统会这样表现，所以我们允许系统中存在一些噪声，即**过程噪声**：
- en: '![](img/e0482ee7-1982-4c30-95b1-56dc7dc3c2d9.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e0482ee7-1982-4c30-95b1-56dc7dc3c2d9.png)'
- en: 'Once we make statistical assumptions about the nature of the process noise,
    this will become a statistical framework, which is usually what happens. But,
    this way, if we are uncertain about our state transition model, but certain about
    observations, surely the best solution would still be what the instruments reported.
    So, we need to tie our state to our observations. Notice that we are observing
    *x*, so the observation could be recovered by multiplying the state by a simple
    row matrix:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对过程噪声的性质做出统计假设，这将成为一个统计框架，这通常是发生的情况。但是，这样，如果我们对我们的状态转移模型不确定，但对观测值确定，那么最好的解决方案仍然是仪器报告的内容。因此，我们需要将我们的状态与我们的观测值联系起来。注意，我们正在观测
    *x*，因此观测值可以通过将状态乘以一个简单的行矩阵来恢复：
- en: '![](img/4e49b45c-88d5-43ae-b353-8ab4cb95ff8b.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4e49b45c-88d5-43ae-b353-8ab4cb95ff8b.png)'
- en: 'But, as we said, we have to allow for the observations being imperfect (maybe
    our radar is very old, and sometimes has erroneous readings), that is, we need
    to allow for **observation noise**; thus, the final observation is the following:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如我们所说的，我们必须允许观测值不完美（也许我们的雷达非常旧，有时会有错误的读数），也就是说，我们需要允许**观测噪声**；因此，最终的观测值如下：
- en: '![](img/d9a16bcf-3bfc-4d3e-b41c-3c5fee17584d.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d9a16bcf-3bfc-4d3e-b41c-3c5fee17584d.png)'
- en: 'Now, if we can characterize process noise and observation noise, the Kalman
    filter will be able to give us good predictions for the locations of the train
    at each point, using only the observations *before* that time. The best way to
    parametrize noise is with a covariance matrix:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们能够描述过程噪声和观测噪声，卡尔曼滤波器将能够仅使用该时间之前的观测值，为我们提供关于火车在每个位置的好预测。最佳参数化噪声的方式是使用协方差矩阵：
- en: '![](img/0db437b2-e554-45dd-897c-93bbbe583257.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0db437b2-e554-45dd-897c-93bbbe583257.png)'
- en: 'The Kalman filter has a recursive **state-transition model**, so we have to
    supply the initial value of the state. If we pick it to be `(0, 0)`, and if we
    assume that **process** **noise** and **measurement noise** are equally probable
    (this is a terrible assumption in real life), the Kalman filter will give us the
    following predictions for each point in time:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 卡尔曼滤波器有一个递归的**状态转移模型**，因此我们必须提供状态初始值。如果我们选择它为 `(0, 0)`，并且如果我们假设**过程噪声**和**测量噪声**是等可能的（这在现实生活中是一个糟糕的假设），卡尔曼滤波器将为每个时间点提供以下预测：
- en: '![](img/48a67f29-b7f5-4978-9136-1d11bfd56edb.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/48a67f29-b7f5-4978-9136-1d11bfd56edb.png)'
- en: Since we believe our observations as much as our assumption that the velocity
    doesn't change, we got a smoothed curve (blue) that is not as extreme, but it
    is still not that convincing. So, we have to make sure that we encode our intuition
    in the variables that we pick.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们相信我们的观测值和我们的假设一样，即速度不会改变，我们得到了一条（蓝色）不那么极端的平滑曲线，但它仍然不够令人信服。因此，我们必须确保我们在选择的变量中编码我们的直觉。
- en: 'Now, if we say that the **signal-to-noise ratio**, that is, the square root
    of the ratio of covariances, is 10, we will get the following results:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们说**信噪比**，即协方差比率的平方根是 10，我们将得到以下结果：
- en: '![](img/d8fea792-6cd0-4bf0-b92b-c1036e39613e.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d8fea792-6cd0-4bf0-b92b-c1036e39613e.png)'
- en: As you can see, the velocity does indeed move very slowly, but we seem to have
    underestimated how far the train has gone. *Or have we?*
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，速度确实移动得很慢，但我们似乎低估了火车行驶的距离。*或者，是我们高估了吗？*
- en: It's a really hard task to tune the Kalman filter, and there are many algorithms
    for doing that, but unfortunately, none are perfect. For this chapter, we will
    not cover those; we will try to pick parameters that make sense, and we will see
    that those parameters give decent results.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 调整卡尔曼滤波器是一个非常困难的任务，有许多算法可以做到这一点，但不幸的是，没有一个完美无缺。对于本章，我们不会介绍这些；我们将尝试选择有意义的参数，并且我们将看到这些参数给出了相当好的结果。
- en: Now let's revisit our single car tracking model, and see how we should model
    our system dynamics.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下我们的单辆车跟踪模型，看看我们应该如何建模系统动力学。
- en: Using a box tracker with the Kalman filter
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用带有卡尔曼滤波器的框跟踪器
- en: First, we have to figure out how to model each car's state. It might be better
    to start with the observation model; that is, *what can we measure about each
    car?*
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须弄清楚如何建模每辆车的状态。可能从观测模型开始会更好；也就是说，*我们可以测量每辆车的哪些内容？*
- en: 'Well, the object detectors give us boxes, but the way they are presented is
    not the best physical interpretation; similar to the train example given previously,
    we want variables we can reason about and that are closer to the underlying dynamics
    of the traffic. So, we use the following observation model:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，目标检测器给我们提供了框，但它们呈现的方式并不是最好的物理解释；类似于之前给出的火车示例，我们想要可以推理的变量，并且更接近交通的潜在动力学。因此，我们使用以下观测模型：
- en: '![](img/79301632-1b42-4f7e-a72e-bad89cc97138.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79301632-1b42-4f7e-a72e-bad89cc97138.png)'
- en: Here, *u* and *v* are the horizontal and vertical pixel locations of the center
    of the target, and *s* and *r* represent the scale (area) and the aspect ratio
    of the target’s bounding box respectively. Since our cars are moving around the
    screen and are moving further away or coming closer, both coordinates and the
    size of the bounding boxes will change over time.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*u* 和 *v* 是目标中心的水平和垂直像素位置，而 *s* 和 *r* 分别代表目标边界框的尺寸（面积）和宽高比。由于我们的汽车在屏幕周围移动，并且越来越远或越来越近，因此坐标和边界框的大小会随时间变化。
- en: 'Assuming that nobody is driving like a lunatic, the velocities of the cars
    in the image should stay more or less constant; that''s why we can limit our model
    to the location and velocities of the objects. So, the state we will take is the
    following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 假设没有人在像疯子一样开车，图像中汽车的速率应该保持大致恒定；这就是为什么我们可以将我们的模型限制在物体的位置和速率上。因此，我们将采取的状态如下：
- en: '![](img/b31bf4a5-3f32-4b71-ba2c-ba7e570f2441.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b31bf4a5-3f32-4b71-ba2c-ba7e570f2441.png)'
- en: We have used a notation where the dot on top of a variable means the rate of
    change of that variable.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一种表示法，其中变量上方的点表示该变量的变化率。
- en: 'The **state transition model** will be that the velocities and the aspect ratio
    stay constant over time (with some **process noise**). In the following screenshot,
    we have visualized all the boundary boxes, and their corresponding states (the
    location of the center and the velocity vector):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态转换模型**将是速度和宽高比随时间保持恒定（带有一些**过程噪声**）。在下面的屏幕截图中，我们可视化了所有的边界框及其对应的状态（中心位置和速度矢量）：'
- en: '![](img/b9b6ae4a-1f4e-4ab9-ac0f-0ed8e474079f.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9b6ae4a-1f4e-4ab9-ac0f-0ed8e474079f.png)'
- en: As you can see, we have set up the model so that what it observes is slightly
    different from what we receive from our tracker․ So, in the next section, we'll
    go over the transformation functions we need to go from a boundary box to and
    from the state space of the Kalman Filter.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们已经设置了模型，使其观察到的与从我们的跟踪器接收到的略有不同。因此，在下一节中，我们将介绍从边界框到卡尔曼滤波器状态空间的转换函数。
- en: Converting boundary boxes to observations
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将边界框转换为观测值
- en: In order to pass the boundary boxes to the Kalman filter, we will have to define
    a transformation function from each boundary box to the observation model, and,
    in order to use the predicted boundary boxes for object tracking, we need to define
    a function from a state to a boundary box.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将边界框传递给卡尔曼滤波器，我们必须定义一个从每个边界框到观测模型的转换函数，并且，为了使用预测的边界框进行目标跟踪，我们需要定义一个从状态到边界框的函数。
- en: 'Let''s start with a transformation function from a boundary box to an observation:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从边界框到观测值的转换函数开始：
- en: 'First, we calculate the center coordinates of the boundary box:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们计算边界框的中心坐标：
- en: '[PRE43]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we calculate the width and height of the box, which we will use to calculate
    the size (that is, the area) and the scale:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算框的宽度和高度，我们将使用这些值来计算尺寸（即面积）和比例：
- en: '[PRE44]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we calculate the size of `bbox`, that is, the area:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算 `bbox` 的大小，即面积：
- en: '[PRE45]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'After that, we calculate the aspect ratio, which is done just by dividing the
    width by the height:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们计算宽高比，这仅仅是通过将宽度除以高度来完成的：
- en: '[PRE46]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then `return` the result as a 4 x 1 matrix:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后`返回`结果作为一个4 x 1矩阵：
- en: '[PRE47]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, since we know that we have to define the inverse transformation as well,
    let''s define `state_to_bbox`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们知道我们必须定义逆变换，那么让我们定义`state_to_bbox`：
- en: 'It takes a 7 x 1 matrix as an argument and unpacks all the components that
    we need to construct a boundary box:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接受一个7 x 1矩阵作为参数，并解包我们构建边界框所需的所有组件：
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then, it calculates the width and the height of the boundary box, from the
    aspect ratio and scale:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它根据宽高比和比例计算边界框的宽度和高度：
- en: '[PRE49]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'After that, it calculates the coordinates of the center:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，它计算中心坐标：
- en: '[PRE50]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, it calculates the half size of the box as a `numpy` tuple, and uses it
    to calculate the coordinates of the opposite corners of the box:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它计算盒子的半尺寸作为一个`numpy`元组，并使用它来计算盒子的对角线顶点坐标：
- en: '[PRE51]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we return the boundary box as a one-dimensional `numpy` array:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将边界框作为一维`numpy`数组返回：
- en: '[PRE52]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Geared with the transformation functions, let's see how we can use OpenCV to
    build a Kalman filter.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了转换函数，让我们看看如何使用OpenCV构建卡尔曼滤波器。
- en: Implementing a Kalman filter
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现卡尔曼滤波器
- en: Now, geared with our model, let's get our hands dirty and write a class that
    handles all this magic. We are going to write a custom class that will use `cv2.KalmanFilter`
    as a Kalman filter, but we will add some helper attributes to be able to keep
    track of each object.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了我们的模型，让我们动手编写一个处理所有这些魔法的类。我们将编写一个自定义类，该类将使用`cv2.KalmanFilter`作为卡尔曼滤波器，但我们将添加一些辅助属性来跟踪每个对象。
- en: 'First, let''s take a look at the initialization of the class, where we will
    set up our Kalman filter by passing the state model, transition matrix, and initial
    parameters:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看类的初始化，我们将通过传递状态模型、转换矩阵和初始参数来设置我们的卡尔曼滤波器：
- en: 'We first start by initializing the class with the boundary box—`bbox`—and the
    label for the `label` object:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过初始化类，包括边界框`bbox`和`label`对象的标签：
- en: '[PRE53]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Then we set up some helper variables that will let us filter boxes as they
    appear and disappear in the tracker:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们设置一些辅助变量，这将使我们能够过滤在跟踪器中出现和消失的框：
- en: '[PRE54]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, we initialize `cv2.KalmanFilter` with the correct dimensionality and
    data type:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用正确的维数和数据类型初始化`cv2.KalmanFilter`：
- en: '[PRE55]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We set the transition matrix and the corresponding process'' **noise covariance
    matrix**. The covariance matrix is a simple model that involves the movement of
    each object with the current constant velocity in the horizontal and vertical
    directions, and becomes bigger or smaller using a constant rate:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了转换矩阵和相应的过程**噪声协方差矩阵**。协方差矩阵是一个简单的模型，涉及每个对象在水平和垂直方向上的当前恒定速度运动，并使用恒定速率变大或变小：
- en: '[PRE56]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We also set how certain we are about the constant speed process. We choose
    a **diagonal covariance matrix**; that is, our state variable is not correlated,
    and we set the variance for location variables as `10`, and as 10,000 for velocity
    variables. We believe that location changes are more predictable than velocity
    changes:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还设置了我们对恒定速度过程的确定性。我们选择一个**对角协方差矩阵**；也就是说，我们的状态变量不相关，我们将位置变量的方差设置为`10`，速度变量的方差设置为10,000。我们相信位置变化比速度变化更可预测：
- en: '[PRE57]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then, we set the **Observation model** to be the following matrix, which implies
    that we are just measuring the first four variables in the state, that is, all
    the location variables:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将**观测模型**设置为以下矩阵，这意味着我们只是在测量状态中的前四个变量，即所有位置变量：
- en: '[PRE58]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now that we have set the measurement of the noise covariance, we believe that
    the horizontal and vertical locations are greater than the aspect ratio and the
    zoom, so we give smaller values to those two measurement variances:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经设置了噪声协方差的测量，我们相信水平和垂直位置大于宽高比和缩放，因此我们给这两个测量方差赋予较小的值：
- en: '[PRE59]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, we set the initial position and the uncertainty associated with the
    Kalman filter:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们设置卡尔曼滤波器的初始位置和与之相关的不确定性：
- en: '[PRE60]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'After we are done setting up the Kalman filter, we need to be able to actually
    predict the new position of the object when it moves. We will do that by defining
    two more methods—`update` and `predict`. The `update` method will update the Kalman
    filter based on a new observation, and the `predict` method will predict a new
    position based on previous evidence. Now let''s take a look at the `update` method:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设置好卡尔曼滤波器之后，我们需要能够预测物体移动时的新位置。我们将通过定义另外两个方法——`update` 和 `predict` 来实现这一点。`update`
    方法将根据新的观测值更新卡尔曼滤波器，而 `predict` 方法将根据先前证据预测新位置。现在让我们看看 `update` 方法：
- en: '[PRE61]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: As you can see, the `update` method takes a boundary box of the new location, `bbox`,
    converts it to an observation, and calls the `correct` method on the OpenCV implementation.
    We have only added some variables to keep track of how long it has been since
    we have updated the object that we are tracking.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`update` 方法接受新位置的边界框 `bbox`，将其转换为观测值，并在 OpenCV 实现上调用 `correct` 方法。我们只添加了一些变量来跟踪我们更新正在跟踪的对象有多长时间了。
- en: 'Now let''s take a look at the `predict` function; its procedure is explained
    in the following steps:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 `predict` 函数；其过程将在以下步骤中解释：
- en: 'It first checks whether we have called `predict` twice in a row; if we have called
    it twice in a row, then it sets `self.hit_streak` to `0`:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它首先检查我们是否连续两次调用了 `predict`；如果我们连续两次调用了它，那么它将 `self.hit_streak` 设置为 `0`：
- en: '[PRE62]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then it increments `self.time_since_update` by `1`, so we keep track of how
    long we have been tracking this object:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它将 `self.time_since_update` 增加 `1`，这样我们就可以跟踪我们跟踪这个对象有多长时间了：
- en: '[PRE63]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then we call the `predict` method of the OpenCV implementation and return a
    boundary box that corresponds with the prediction:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们调用 OpenCV 实现的 `predict` 方法，并返回与预测相对应的边界框：
- en: '[PRE64]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: So, now that we have implemented a single-object tracker, the next step is to
    create a mechanism that can associate a detection box with a tracker, which we
    will do in the next section.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经实现了一个单目标跟踪器，下一步是创建一个可以将检测框与跟踪器关联的机制，我们将在下一节中完成。
- en: Associating detections with trackers
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将检测与跟踪器关联
- en: 'In the Sort algorithm, decisions about whether two bounding boxes should be
    considered to be of the same object are made based onIntersection over Union.
    Previously in this chapter, you learned about this metric and implemented a function
    to compute it. Here, we''ll define a function that will associate detection and
    tracking boxes based on their IoU value:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sort 算法中，是否将两个边界框视为同一对象的决策是基于交并比（IoU）。在本章之前，您已经学习了这个指标并实现了一个计算它的函数。在这里，我们将定义一个函数，它将根据它们的
    IoU 值将检测框和跟踪框关联起来：
- en: '[PRE65]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The function accepts the bounding boxes of detections and the predicted boxes
    of trackers, as well as an IoU threshold. It returns matches as an array of pairs
    of corresponding indexes in the corresponding arrays, indexes of unmatched boxes
    of detections, and indexes of unmatched boxes of trackers. In order to achieve
    this, it takes the following steps:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受检测的边界框和跟踪器的预测框，以及一个 IoU 阈值。它返回匹配项，作为对应数组中对应索引的数组，未匹配检测框的索引和未匹配跟踪器框的索引。为了实现这一点，它采取以下步骤：
- en: 'First, it initializes a matrix in which the IoU values of each possible pair
    of boxes will be stored:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它初始化一个矩阵，其中将存储每个可能框对之间的 IoU 值：
- en: '[PRE66]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Then, we iterate overdetection and tracker boxes, calculate IoU for each pair,
    and store the resulting values in the matrix:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们遍历检测和跟踪框，计算每一对的 IoU，并将结果值存储在矩阵中：
- en: '[PRE67]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Using `iou_matrix`, we will find matching pairs such that the sum of the values
    of the IoUs of these pairs gets the maximal possible value:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `iou_matrix`，我们将找到匹配的对，使得这些对中 IoU 值的总和达到最大可能值：
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: For this purpose, we have used the **Hungarian algorithm**, which is implemented
    as the `linear_sum_assignment` function. It is a combinatorial optimization algorithm
    that solves the **assignment problem**.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们使用了 **匈牙利算法**，它作为 `linear_sum_assignment` 函数实现。它是一个组合优化算法，用于解决 **分配问题**。
- en: In order to use this algorithm, we have passed the opposite values of `iou_matrix`.
    The algorithm associates indexes such that the total sum is minimal. Hence, we
    find the maximal value when we negate the matrix. The straightforward way to find
    these associations would be to iterate over all possible combinations and pick
    the one that has the maximal value.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用此算法，我们已传递`iou_matrix`的相反值。该算法将索引关联起来，使得总和最小。因此，当我们取矩阵的负值时，我们找到最大值。找到这些关联的直接方法是对所有可能的组合进行迭代，并选择具有最大值的那个。
- en: The problem with the latter approach is that the time complexity of it will
    be exponential and hence it will be too slow once we have multiple detections
    and trackers. Meanwhile, the Hungarian algorithm has a time complexity of ***O(n³)***.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 后者方法的缺点是，其时间复杂度将是指数级的，因此一旦我们有多个检测和跟踪器，它将会非常慢。同时，匈牙利算法的时间复杂度为***O(n³)***。
- en: 'Then we change the format of the result of the algorithm so that it appears
    as pairs of matched indexes in a `numpy` array:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们更改算法结果的格式，使其以`numpy`数组中匹配索引对的格式出现：
- en: '[PRE69]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Then get the intersection over union values of the matches from `iou_matrix`:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从`iou_matrix`中获取匹配项的交并比值：
- en: '[PRE70]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Filter out matches that have an IoU value that is too low:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉具有过低IoU值的匹配项：
- en: '[PRE71]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Then, find the indexes of the detection boxes that were not matched:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，找到未匹配的检测框的索引：
- en: '[PRE72]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'After that, find the indexes of the tracker boxes that were not matched:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，找到未匹配的跟踪器框的索引：
- en: '[PRE73]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'At last, it returns the matches as well as the indexes of the unmatched detection
    and tracker boxes:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它返回匹配项以及未匹配检测和跟踪框的索引：
- en: '[PRE74]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: So, now that we have mechanisms to track a single object and to associate detections
    with single-object trackers, what's left to do is to create a class that will
    use these mechanisms to track multiple objects throughout frames. We will do this
    in the next section and then the algorithm will be complete.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经有了跟踪单个对象和将检测与单个对象跟踪器关联起来的机制，接下来要做的就是创建一个类，该类将使用这些机制在帧之间跟踪多个对象。我们将在下一节中这样做，然后算法将完成。
- en: Defining the main class of the tracker
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义跟踪器的主要类
- en: 'The constructor of the class is given as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数如下所示：
- en: '[PRE75]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'It stores two parameters:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 它存储了两个参数：
- en: The first parameter is `max_age`, which specifies how many consecutive times
    a tracker of a certain object can remain without an associated box before we consider
    the object to have gone from the scene and delete the tracker.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数是`max_age`，它指定某个对象的跟踪器在没有关联框的情况下可以连续多少次存在，我们才认为该对象已从场景中消失并删除跟踪器。
- en: The second parameter is `min_hits`, which specifies how many consecutive times
    a tracker should be associated with a box so that we consider it to be a certain
    object. It also creates properties for storing the trackers and counting the total
    number of trackers during the instance lifetime.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数是`min_hits`，它指定跟踪器应连续多少次与一个框关联，我们才将其视为某个对象。它还创建属性以存储跟踪器并在实例生命周期内计算跟踪器的总数。
- en: 'We also define a method for creating an ID of a tracker:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个用于创建跟踪器ID的方法：
- en: '[PRE76]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The method increments the count of the trackers by one and returns the number
    as the ID.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将跟踪器的计数增加一个，并返回该数字作为ID。
- en: 'Now we are ready to define the `update` method, which will do the heavy lifting:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好定义`update`方法，它将执行繁重的工作：
- en: '[PRE77]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The `update` method accepts detection boxes and covers the following steps:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '`update`方法接受检测框，并包括以下步骤：'
- en: 'For all available `trackers`, it predicts their new locations and removes `trackers`
    with failed predictions right away:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有可用的`trackers`，它预测它们的新位置，并立即删除预测失败的`trackers`：
- en: '[PRE78]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We then get the predicted boxes of the `trackers`:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们得到跟踪器的预测框：
- en: '[PRE79]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Then, we associate the boxes predicted by the trackers with the detection boxes:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将跟踪器预测的框与检测框关联起来：
- en: '[PRE80]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We then update the matched `trackers` with the associated detections:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们更新匹配的`trackers`与关联的检测：
- en: '[PRE81]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'For all unmatched detections, we create new `trackers` that are initialized
    with the corresponding bounding box:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有未匹配的检测，我们创建新的`trackers`，并使用相应的边界框进行初始化：
- en: '[PRE82]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We then compose the `return` value as an `array` of the tracker box and tracker
    ID concatenations of the relevant trackers:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将`return`值组合成一个包含相关跟踪器框和跟踪器ID连接的`array`：
- en: '[PRE83]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: In the previous codes snippet, we consider only those `trackers` that were updated
    with a detection box in the current frame and that have at least a `hit_streak` consecutive
    association with detection boxes. Depending on the particular application of the
    algorithm, you might want to change this behavior to make it a better fit for
    your needs.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们只考虑了那些在当前帧中更新了检测框并且至少有一个`hit_streak`连续关联检测框的`trackers`。根据算法的特定应用，您可能想要更改此行为，使其更适合您的需求。
- en: 'We then clean up the `trackers` by removing the ones that have not been updated
    with a new bounding box for a while:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过移除一段时间内没有更新新边界框的`trackers`来清理跟踪器：
- en: '[PRE84]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'At last, we `return` the results:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们`返回`结果：
- en: '[PRE85]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: So, now that we have completed the implementation of the algorithm, we have
    everything ready to run the app and see it in action.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经完成了算法的实现，我们一切准备就绪，可以运行应用程序并看到它的实际效果。
- en: Seeing the app in action
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观看应用程序的实际运行
- en: 'Once we run our app, it will use a passed video or another video stream, then
    process it and illustrate the results:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行我们的应用程序，它将使用传递的视频或另一个视频流，然后处理它并展示结果：
- en: '![](img/fbd2cba6-34ff-4bb2-a4f1-0a88c752c84e.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fbd2cba6-34ff-4bb2-a4f1-0a88c752c84e.png)'
- en: On each processed frame, it will display the object type, a bounding box, and
    the number of each tracked object. It will also display general information about
    tracking in the top-left corner of the frame. This general information consists
    of the total number of tracked video objects throughout for each type of tracked
    object, as well as the tracked objects currently available in the scene.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个处理的帧中，它将显示物体类型、边界框以及每个跟踪物体的数量。它还会在帧的左上角显示有关跟踪的一般信息。这些一般信息包括每种跟踪物体类型在整个过程中跟踪的视频物体的总数，以及场景中当前可用的跟踪物体。
- en: Summary
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Throughout this chapter, we have used an object detection network and combined
    it with a tracker to track and count objects over time. After reading through
    the chapter, you should now understand how detection networks work and understand
    their training mechanisms.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了一个物体检测网络，并将其与跟踪器结合使用，以跟踪和计数随时间推移的物体。阅读完本章后，您现在应该理解检测网络是如何工作的，以及它们的训练机制。
- en: You have learned how you can import models built with other frameworks into
    OpenCV and bind them into an application that processes a video or uses other
    video streams such as your camera or a remote IP camera. You have implemented
    a simple, yet robust, algorithm for tracking, which, in combination with a robust
    detector network, allows the answering of multiple statistical questions related
    to video data.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经学会了如何将使用其他框架构建的模型导入OpenCV，并将其绑定到处理视频或使用其他视频流（如您的相机或远程IP相机）的应用程序中。您实现了一个简单但稳健的跟踪算法，它与稳健的检测网络结合使用，可以回答与视频数据相关的多个统计问题。
- en: You can now use and train object detection networks of your choice in order
    to create your own highly accurate applications that implement their functionality
    around object detection and tracking.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用并训练您选择的物体检测网络，以便创建您自己的高度精确的应用程序，这些应用程序的功能围绕物体检测和跟踪实现。
- en: Throughout the course of the book, you have made yourself familiar with a background
    in one of the main branches of machine learning, called **computer vision**. You started
    by using simple approaches such as image filters and shape analysis techniques.
    Then, you proceeded with classical feature extraction approaches and built several
    practical apps based on these approaches. After that, you learned about the statistical
    properties of a natural scene and you were able to use these properties to track
    unknown objects.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，您已经熟悉了机器学习主要分支之一，即**计算机视觉**的背景。您从使用简单的图像滤波器和形状分析技术开始。然后，您继续使用经典的特征提取方法，并基于这些方法构建了几个实际的应用程序。之后，您学习了自然场景的统计特性，并能够使用这些特性来跟踪未知物体。
- en: Next, you started to learn about, use, and train supervised models such as **Support
    Vector Machines** (**SVMs**) and **cascading classifiers**. Having all this theoretical
    and practical knowledge about classical computer vision approaches, you dived
    into deep learning models, which nowadays give state-of-the-art results for many
    machine learning problems, especially in the field of computer vision.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你开始学习、使用和训练监督模型，例如**支持向量机**（**SVMs**）和**级联分类器**。在掌握了所有关于经典计算机视觉方法的理论和实践知识后，你深入研究了深度学习模型，这些模型如今在许多机器学习问题中提供了最先进的结果，尤其是在计算机视觉领域。
- en: You now understand how **convolutional networks** work and how deep learning
    models are trained, and you have built and trained your own networks on top of
    other pre-trained models. Having all this knowledge and practice, you can analyze,
    understand, and apply other computer vision models as well as elaborating on new
    models once you come up with new ideas. You are ready to work on your own **c****omputer
    vision** projects, which might change the world!
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经理解了**卷积网络**的工作原理以及深度学习模型的训练方式，你还在其他预训练模型的基础上构建并训练了自己的网络。拥有所有这些知识和实践经验，你就可以分析、理解和应用其他计算机视觉模型，一旦你有了新的想法，你还可以详细阐述新的模型。你现在可以开始着手自己的**计算机视觉**项目了，这可能会改变世界！
