- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: In machine learning, the task of classification is that of dividing a set of
    observations (objects) into groups called **classes**, based on an analysis of
    their formal description. In **classification**, each observation (object) is
    assigned to a group or nominal category based on specific qualitative properties.
    Classification is a supervised task because it requires known classes for training
    samples. The labeling of a training set is usually done manually, with the involvement
    of specialists in the given field of study. It’s also notable that if classes
    are not initially defined, then there will be a problem with clustering. Furthermore,
    in the classification task, there may be more than two classes (multi-class),
    and each of the objects may belong to more than one class (intersecting).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，分类的任务是将一组观察（对象）根据其形式描述的分析划分为称为**类别**的组。在**分类**中，每个观察（对象）根据特定的定性属性被分配到一组或命名类别。分类是一个监督任务，因为它需要已知的类别来训练样本。训练集的标记通常是通过手动完成，并涉及该研究领域的专家。值得注意的是，如果类别最初没有定义，那么在聚类中将会出现问题。此外，在分类任务中，可能存在超过两个类别（多类别），并且每个对象可能属于多个类别（相交）。
- en: In this chapter, we will discuss various approaches to solving a classification
    task with machine learning. We are going to look at some of the most well-known
    and widespread algorithms, which are logistic regression, **support vector machine**
    (**SVM**), and **k-nearest neighbors** (**kNN**). Logistic regression is one of
    the most straightforward algorithms based on linear regression and a special loss
    function. SVM is based on a concept of support vectors that helps to build a decision
    boundary to separate data. This approach can be effectively used with high-dimensional
    data. kNN has a simple implementation algorithm that uses the idea of data compactness.
    Also, we will show how the multi-class classification problem can be solved with
    the algorithms mentioned previously. We will implement program examples to see
    how to use these algorithms to solve the classification task with different C++
    libraries.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论使用机器学习解决分类任务的多种方法。我们将查看一些最知名和最广泛使用的算法，包括逻辑回归、**支持向量机**（**SVM**）和**k最近邻**（**kNN**）。逻辑回归是基于线性回归和特殊损失函数的最直接算法之一。SVM基于支持向量的概念，有助于构建决策边界来分离数据。这种方法可以有效地用于高维数据。kNN具有简单的实现算法，它使用数据紧凑性的想法。此外，我们还将展示如何使用前面提到的算法解决多类别分类问题。我们将实现程序示例，以了解如何使用这些算法通过不同的C++库来解决分类任务。
- en: 'The following topics are covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: An overview of classification methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类方法的概述
- en: Exploring various classification methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索各种分类方法
- en: Examples of using C++ libraries for dealing with the classification task
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用C++库处理分类任务的示例
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The required technologies and installations for this chapter include the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的技术和安装包括以下内容：
- en: The `mlpack` library
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlpack`库'
- en: The `Dlib` library
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib`库'
- en: The Flashlight library
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flashlight库
- en: A modern C++ compiler with C++20 support
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持C++20的现代C++编译器
- en: CMake build system version >= 3.10
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake构建系统版本 >= 3.10
- en: 'The code files for this chapter can be found at the following GitHub repo:
    [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下GitHub仓库中找到：[https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-Edition/tree/main/Chapter07).
- en: An overview of classification methods
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类方法的概述
- en: Classification is a fundamental task in **applied statistics**, **machine learning**,
    and **artificial intelligence** (**AI**). This is because classification is one
    of the most understandable and easy-to-interpret data analysis technologies, and
    classification rules can be formulated in a natural language. In machine learning,
    a classification task is solved using supervised algorithms because the classes
    are defined in advance, and the objects in the training set have class labels.
    Analytical models that solve a classification task are called **classifiers**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是**应用统计学**、**机器学习**和**人工智能**（**AI**）中的一个基本任务。这是因为分类是最容易理解和解释的数据分析技术之一，分类规则可以用自然语言表述。在机器学习中，分类任务是通过监督算法解决的，因为类别是预先定义的，训练集中的对象有类别标签。解决分类任务的解析模型被称为**分类器**。
- en: Classification is the process of moving an object to a predetermined class based
    on its formalized features. Each object in this problem is usually represented
    as a vector in *N*-dimensional space. Each dimension in that space is a description
    of one of the features of the object.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是将对象根据其形式化的特征移动到预定的类别的过程。在这个问题中，每个对象通常被表示为 *N*-维空间中的一个向量。该空间中的每个维度都是对象的一个特征的描述。
- en: We can formulate the classification task with mathematical notation. Let *X*
    denote the set of descriptions of objects, and *Y* be a finite set of names or
    class labels. There is an unknown objective function—namely, the mapping ![](img/B19849_Formula_0011.png),
    whose values are known only on the objects of the final training sample, ![](img/B19849_Formula_0021.png).
    So, we have to construct an ![](img/B19849_Formula_0031.png) algorithm, capable
    of classifying an ![](img/B19849_Formula_0041.png) arbitrary object. In mathematical
    statistics, classification problems are also called discriminant analysis problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用数学符号来表述分类任务。让 *X* 表示对象的描述集合，*Y* 是一个有限的名字或类别标签集合。存在一个未知的客观函数——即映射 ![](img/B19849_Formula_0011.png)，其值只在最终训练样本的对象上已知，![](img/B19849_Formula_0021.png)。因此，我们必须构建一个
    ![](img/B19849_Formula_0031.png) 算法，能够对 ![](img/B19849_Formula_0041.png) 任意对象进行分类。在数学统计学中，分类问题也被称为判别分析问题。
- en: 'The classification task is applicable to many areas, including the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务适用于许多领域，包括以下内容：
- en: '**Trade**: The classification of customers and products allows a business to
    optimize marketing strategies, stimulate sales, and reduce costs'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贸易**：对客户和产品的分类使企业能够优化营销策略，刺激销售，并降低成本'
- en: '**Telecommunications**: The classification of subscribers allows a business
    to appraise customer loyalty, and therefore develop loyalty programs'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电信**：对订阅者的分类使企业能够评估客户忠诚度，因此开发忠诚度计划'
- en: '**Medicine and health care**: Assisting the diagnosis of disease by classifying
    the population into risk groups'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健**：通过将人群分类到风险组中来辅助疾病诊断'
- en: '**Banking**: The classification of customers is used for credit-scoring procedures'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**银行**：对客户的分类用于信用评分程序'
- en: 'Classification can be solved by using the following methods:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分类可以通过以下方法来解决：
- en: Logistic regression
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The kNN method
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kNN 方法
- en: SVM
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM
- en: Discriminant analysis
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别分析
- en: Decision trees
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Neural networks
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'We looked into discriminant analysis in [*Chapter 6*](B19849_06.xhtml#_idTextAnchor301),
    *Dimensionality Reduction*, as an algorithm for dimensionality reduction, but
    most libraries provide an **application programming interface** (**API**) for
    working with the discriminant analysis algorithm as a classifier, too. We will
    discuss decision trees in [*Chapter 9*](B19849_09.xhtml#_idTextAnchor496), *Ensemble
    Learning*, focusing on algorithm ensembles. We will also discuss neural networks
    in the chapter that follows this: [*Chapter 10*](B19849_10.xhtml#_idTextAnchor539),
    *Neural Networks for* *Image Classification*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第6章*](B19849_06.xhtml#_idTextAnchor301)，*降维*中探讨了判别分析，将其作为一个降维算法，但大多数库也提供了用于判别分析算法作为分类器的**应用程序编程接口**（**API**）。我们将在[*第9章*](B19849_09.xhtml#_idTextAnchor496)，*集成学习*中讨论决策树，重点关注算法集成。我们还将讨论在接下来的章节[*第10章*](B19849_10.xhtml#_idTextAnchor539)，*用于图像分类的神经网络*中的神经网络。
- en: Now we’ve discussed what the classification task is, let’s look at various classification
    methods.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了分类任务是什么，让我们来看看各种分类方法。
- en: Exploring various classification methods
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索各种分类方法
- en: Nowadays, **deep learning** has become increasingly popular for classification
    tasks too, especially when dealing with complex and high-dimensional data such
    as images, audio, and text. Deep neural networks can learn hierarchical representations
    of data that allow them to perform accurate classification. In this chapter, we
    concentrate on more classical classification approaches because they are still
    applicable and usually require fewer computational resources. Specifically, we
    will discuss some of the classification methods such as logistic regression, **kernel
    ridge regression** (**KRR**), the kNN method, and SVM approaches.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，**深度学习**在分类任务中也变得越来越流行，尤其是在处理复杂和高维数据（如图像、音频和文本）时。深度神经网络可以学习数据的层次表示，从而能够进行准确的分类。在本章中，我们专注于更经典的分类方法，因为它们仍然适用，并且通常需要较少的计算资源。具体来说，我们将讨论一些分类方法，如Logistic回归、**核岭回归**（**KRR**）、kNN方法和SVM方法。
- en: Logistic regression
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Logistic回归
- en: 'Logistic regression determines the degree of dependence between the categorical
    dependent and one or more independent variables by using the logistic function.
    It aims to find the values of the coefficients for the input variables, as with
    linear regression. The difference, in the case of logistic regression, is that
    the output value is converted by using a non-linear (logistic) function. The logistic
    function has an S-shaped curve and converts any value to a number between `0`
    and `1`. This property is useful because we can apply the rule to the output of
    the logistic function to bind `0` and `1` to a class prediction. The following
    screenshot shows a logistic function graph:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Logistic回归通过使用Logistic函数来确定分类因变量与一个或多个自变量之间的依赖程度。它旨在找到输入变量的系数值，就像线性回归一样。在Logistic回归的情况下，区别在于输出值是通过使用非线性（Logistic）函数转换的。Logistic函数具有S形曲线，并将任何值转换为`0`到`1`之间的数字。这一特性很有用，因为我们可以将规则应用于Logistic函数的输出，将`0`和`1`绑定到类预测。以下截图显示了Logistic函数的图形：
- en: '![Figure 7.1 – Logistic function](img/B19849_07_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – Logistic函数](img/B19849_07_01.jpg)'
- en: Figure 7.1 – Logistic function
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – Logistic函数
- en: For example, if the result of the function is less than `0.5`, then the output
    is `0`. Prediction is not just a simple answer (`+1` or `-1`) either, and we can
    interpret it as a probability of being classified as `+1`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果函数的结果小于`0.5`，则输出为`0`。预测不仅仅是简单的答案（`+1`或`-1`），我们还可以将其解释为被分类为`+1`的概率。
- en: In many tasks, this interpretation is an essential business requirement. For
    example, in the task of credit-scoring, where logistic regression is traditionally
    used, the probability of a loan being defaulted on is a common prediction. As
    with the case of linear regression, logistic regression performs the task better
    if outliers and correlating variables are removed. The logistic regression model
    can be quickly trained and is well-suited for binary classification problems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多任务中，这种解释是一个基本的企业需求。例如，在信用评分任务中，Logistic回归传统上被使用，贷款违约的概率是一个常见的预测。与线性回归的情况一样，如果移除了异常值和相关性变量，Logistic回归的性能会更好。Logistic回归模型可以快速训练，非常适合二元分类问题。
- en: 'The basic idea of a linear classifier is that the feature space can be divided
    by a hyperplane into two half-spaces, in each of which one of the two values of
    the target class is predicted. If we can divide a feature space without errors,
    then the training set is called **linearly separable**. Logistic regression is
    a unique type of linear classifier, but it is able to predict the probability
    of ![](img/B19849_Formula_0051.png), attributing the example of ![](img/B19849_Formula_006.png)
    to the class *+*, as illustrated here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器的基本思想是特征空间可以通过一个超平面分为两个半空间，在每个半空间中预测目标类的两个值之一。如果我们能够无错误地划分特征空间，那么训练集被称为**线性可分**。Logistic回归是一种独特的线性分类器，但它能够预测![](img/B19849_Formula_0051.png)的概率，将![](img/B19849_Formula_006.png)的例子归为类*+*，如图所示：
- en: '![](img/B19849_Formula_007.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_007.jpg)'
- en: 'Consider the task of binary classification, with labels of the target class
    denoted by `+1` (positive examples) and `-1` (negative examples). We want to predict
    the probability of ![](img/B19849_Formula_0081.png); so, for now, we can build
    a linear forecast using the following optimization technique: ![](img/B19849_Formula_0091.png).
    So, how do we convert the resulting value into a probability whose limits are
    `[0, 1]`? This approach requires a specific function. In the logistic regression
    model, the specific function ![](img/B19849_Formula_010.png) is used for this.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑二元分类任务，目标类的标签用 `+1`（正例）和 `-1`（反例）表示。我们想要预测 ![公式图片](img/B19849_Formula_0081.png)
    的概率；因此，现在我们可以使用以下优化技术构建线性预测：![公式图片](img/B19849_Formula_0091.png)。那么，我们如何将得到的值转换为概率，其极限为
    `[0, 1]`？这种方法需要一个特定的函数。在逻辑回归模型中，用于此的特定函数 ![公式图片](img/B19849_Formula_010.png) 被使用。
- en: 'Let’s denote *P(X)* by the probability of the occurring event *X*. The probability
    odds ratio *OR(X)* is determined from ![](img/B19849_Formula_0111.png). This is
    the ratio of the probabilities of whether the event will occur or not. We can
    see that the probability and the odds ratio both contain the same information.
    However, while *P(X)* is in the range of `0` to `1`, *OR(X)* is in the range of
    `0` to ![](img/B19849_Formula_0121.png). If you calculate the logarithm of *OR(X)*
    (known as the **logarithm of the odds**, or the **logarithm of the probability
    ratio**), it is easy to see that the following applies: ![](img/B19849_Formula_0131.png).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 *P(X)* 表示事件 *X* 发生的概率。概率优势比 *OR(X)* 由 ![公式图片](img/B19849_Formula_0111.png)
    确定。这是事件发生与否的概率比。我们可以看到，概率和优势比都包含相同的信息。然而，虽然 *P(X)* 的范围是 `0` 到 `1`，*OR(X)* 的范围是
    `0` 到 ![公式图片](img/B19849_Formula_0121.png)。如果你计算 *OR(X)* 的对数（称为 **优势对数** 或 **概率比的对数**），很容易看出以下适用：![公式图片](img/B19849_Formula_0131.png)。
- en: 'Using the logistic function to predict the probability of ![](img/B19849_Formula_0051.png),
    it can be obtained from the probability ratio (for the time being, let’s assume
    we have the weights too) as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用逻辑函数预测 ![公式图片](img/B19849_Formula_0051.png) 的概率，可以从概率比（暂时假设我们也有权重）中得到如下：
- en: '![](img/B19849_Formula_015.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_015.jpg)'
- en: 'So, the logistic regression predicts the probability of classifying a sample
    to the "*+*" class as a sigmoid transformation of a linear combination of the
    model weights vector, as well as the sample’s features vector, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，逻辑回归预测将样本分类到 "*+*" 类的概率，作为模型权重向量与样本特征向量的线性组合的sigmoid变换，如下所示：
- en: '![](img/B19849_Formula_016.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_016.jpg)'
- en: 'From the maximum likelihood principle, we can obtain an optimization problem
    that the logistic regression solves—namely, the minimization of the logistic loss
    function. For the "`-`" class, the probability is determined by a similar formula,
    as illustrated here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从最大似然原理中，我们可以得到逻辑回归解决的优化问题——即最小化逻辑损失函数。对于 "`-`" 类，概率由一个类似的公式确定，如图所示：
- en: '![](img/B19849_Formula_017.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_017.jpg)'
- en: 'The expressions for both classes can be combined into one, as illustrated here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 两个类的表达式可以合并为一个，如图所示：
- en: '![](img/B19849_Formula_018.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_018.jpg)'
- en: 'Here, the expression ![](img/B19849_Formula_019.png) is called the margin of
    classification of the ![](img/B19849_Formula_006.png) object. The classification
    margin can be understood as a model’s *confidence* in the object’s classification.
    An interpretation of this margin is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，表达式 ![公式图片](img/B19849_Formula_019.png) 被称为 ![公式图片](img/B19849_Formula_006.png)
    对象的分类边缘。分类边缘可以理解为模型对对象分类的 *置信度*。这个边缘的解释如下：
- en: If the margin vector’s absolute value is large and positive, the class label
    is set correctly, and the object is far from the separating hyperplane. Such an
    object is therefore classified confidently.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果边缘向量的绝对值很大且为正，则类标签设置正确，对象远离分离超平面。因此，这样的对象被自信地分类。
- en: If the margin is large (by modulo) but negative, then the class label is set
    incorrectly. The object is far from the separating hyperplane. Such an object
    is most likely an anomaly.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果边缘很大（通过模运算）但为负，则类标签设置错误。对象远离分离超平面。这样的对象很可能是异常值。
- en: If the margin is small (by modulo), then the object is close to the separating
    hyperplane. In this case, the margin sign determines whether the object is correctly
    classified.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果边缘很小（通过模运算），则对象接近分离超平面。在这种情况下，边缘符号决定了对象是否被正确分类。
- en: 'In the discrete case, the likelihood function ![](img/B19849_Formula_0212.png)
    can be interpreted as the probability that the sample *X*1 *, . . . , X*n is equal
    to *x*1 *, . . . , x*nin the given set of experiments. Furthermore, this probability
    depends on *θ*, as illustrated here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散情况下，似然函数 ![](img/B19849_Formula_0212.png) 可以解释为样本 *X*1 *, . . . , X*n 等于
    *x*1 *, . . . , x*n 在给定实验集中的概率。此外，这个概率依赖于 *θ*，如图所示：
- en: '![](img/B19849_Formula_022.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_022.jpg)'
- en: 'The maximum likelihood estimate ![](img/B19849_Formula_0231.png) for the unknown
    parameter ![](img/B19849_Formula_0241.png) is called the value of ![](img/B19849_Formula_0241.png),
    for which the function ![](img/B19849_Formula_0261.png) reaches its maximum (as
    a function of *θ*, with fixed ![](img/B19849_Formula_0271.png)), as illustrated
    here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未知参数 ![](img/B19849_Formula_0241.png) 的最大似然估计 ![](img/B19849_Formula_0231.png)
    被称为 ![](img/B19849_Formula_0241.png) 的值，此时函数 ![](img/B19849_Formula_0261.png)
    达到其最大值（*θ* 固定时作为 *θ* 的函数），如图所示：
- en: '![](img/B19849_Formula_028.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_028.jpg)'
- en: 'Now, we can write out the likelihood of the sample—namely, the probability
    of observing the given vector ![](img/B19849_Formula_0291.png) in the sample ![](img/B19849_Formula_0301.png).
    We make one assumption—objects arise independently from a single distribution,
    as illustrated here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以写出样本的似然函数——即在样本 ![](img/B19849_Formula_0301.png) 中观察到给定向量 ![](img/B19849_Formula_0291.png)
    的概率。我们做出一个假设——对象独立地从单个分布中出现，如图所示：
- en: '![](img/B19849_Formula_0311.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0311.jpg)'
- en: 'Let’s take the logarithm of this expression since the sum is much easier to
    optimize than the product, as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们取这个表达式的对数，因为求和比求积更容易优化，如下所示：
- en: '![](img/B19849_Formula_032.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_032.jpg)'
- en: 'In this case, the principle of maximizing the likelihood leads to a minimization
    of the expression, as illustrated here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最大化似然的原则导致表达式的最小化，如图所示：
- en: '![](img/B19849_Formula_033.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_033.jpg)'
- en: 'This formula is a logistic loss function, summed over all objects of the training
    sample. Usually, it is a good idea to add some regularization to a model to deal
    with overfitting. **L2 regularization** of logistic regression is arranged in
    much the same way as for the ridge regression (**linear regression** with regularization).
    However, it is common to use the controlled variable decay parameter *C* that
    is used in SVM models, where it denotes soft margin parameter denotation. So,
    for logistic regression, *C* is equal to the inverse regularization coefficient
    ![](img/B19849_Formula_0341.png). The relationship between *C* and ![](img/B19849_Formula_0351.png)
    would be the following: lowering *C* would strengthen the regularization effect.
    Therefore, instead of the functional ![](img/B19849_Formula_0361.png), the following
    function should be minimized:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式是逻辑损失函数，对所有训练样本中的对象进行求和。通常，给模型添加一些正则化来处理过拟合是一个好主意。逻辑回归的**L2正则化**与岭回归（带有正则化的**线性回归**）的安排方式大致相同。然而，在SVM模型中，通常使用控制变量衰减参数
    *C*，其中它表示软边界参数的表示。因此，对于逻辑回归，*C* 等于逆正则化系数 ![](img/B19849_Formula_0341.png)。*C*
    和 ![](img/B19849_Formula_0351.png) 之间的关系如下：降低 *C* 会增强正则化效果。因此，而不是最小化函数 ![](img/B19849_Formula_0361.png)，应该最小化以下函数：
- en: '![](img/B19849_Formula_037.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_037.jpg)'
- en: .
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: For this function minimization, we can apply different methods—for example,
    the method of least squares, or the **gradient descent** method. The vital issue
    with logistic regression is that it is generally a linear classifier, in order
    to deal with non-linear decision boundaries, which typically use polynomial features
    with original features as a basis for them. This approach was discussed in [*Chapter
    3*](B19849_03.xhtml#_idTextAnchor152) when we discussed polynomial regression.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个函数最小化问题，我们可以应用不同的方法——例如，最小二乘法，或者**梯度下降**方法。逻辑回归的关键问题是它通常是一个线性分类器，为了处理非线性决策边界，通常使用以原始特征为基础的多项式特征。这种方法在我们讨论多项式回归时在[*第三章*](B19849_03.xhtml#_idTextAnchor152)中进行了讨论。
- en: KRR
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KRR
- en: KRR combines linear ridge regression (linear regression and L2 norm regularization)
    with the kernel trick and can be used for classification problems. It learns a
    linear function in the higher-dimensional space produced by the chosen kernel
    and training data. For non-linear kernels, it learns a non-linear function in
    the original space.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: KRR结合了线性岭回归（线性回归和L2范数正则化）与核技巧，可用于分类问题。它学习在由所选核和训练数据产生的更高维空间中的线性函数。对于非线性核，它在原始空间中学习非线性函数。
- en: 'The model learned by KRR is identical to the SVM model, but these approaches
    have the following differences:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: KRR学习到的模型与SVM模型相同，但这些方法有以下不同之处：
- en: The KRR method uses squared error loss, while the SVM model uses insensitive
    loss or hinge loss for classification
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KRR方法使用平方误差损失，而SVM模型在分类时使用不可分损失或铰链损失。
- en: In contrast to the SVM method, the KRR training can be completed in closed form
    so that it can be trained faster for medium-sized datasets
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与SVM方法相比，KRR的训练可以以闭式形式完成，因此对于中等大小的数据集可以更快地训练。
- en: The learned KRR model is non-sparse and can be slower than the SVM model when
    it comes to prediction times
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习到的KRR模型是非稀疏的，在预测时间上可能比SVM模型慢。
- en: Despite these differences, both approaches usually use L2 regularization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些不同之处，但两种方法通常都使用L2正则化。
- en: SVM
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVM
- en: The SVM method is a set of algorithms used for classification and regression
    analysis tasks. Considering that in an *N*-dimensional space, each object belongs
    to one of two classes, SVM generates an (*N-1*)-dimensional hyperplane to divide
    these points into two groups. It’s similar to an on-paper depiction of points
    of two different types that can be linearly divided. Furthermore, the SVM selects
    the hyperplane, which is characterized by the maximum distance from the nearest
    group elements.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SVM方法是一组用于分类和回归分析任务的算法。考虑到在*N*维空间中，每个对象属于两个类别之一，SVM生成一个(*N-1*)维超平面来将这些点分为两组。它类似于纸上描绘的两个不同类型点可以线性划分的图示。此外，SVM选择具有最大距离于最近组元素的超平面。
- en: The input data can be separated using various hyperplanes. The best hyperplane
    is a hyperplane with the maximum resulting separation and the maximum resulting
    difference between the two classes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据可以使用各种超平面进行分离。最佳超平面是具有最大分离结果和两个类别之间最大差异的超平面。
- en: 'Imagine the data points on the plane. In the following case, the separator
    is just a straight line:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 想象平面上数据点。在以下情况下，分离器只是一条直线：
- en: '![Figure 7.2 – Point separation line](img/B19849_07_02.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 点分离线](img/B19849_07_02.jpg)'
- en: Figure 7.2 – Point separation line
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 点分离线
- en: Let’s draw distinct straight lines that divide the points into two sets. Then,
    choose a straight line as far as possible from the points, maximizing the distance
    from it to the nearest point on each side. If such a line exists, then it is called
    the maximum margin hyperplane. Intuitively, a good separation is achieved due
    to the hyperplane itself (which has the longest distance to the nearest point
    of the training sample of any class), since, in general, the bigger the distance,
    the smaller the classifier error.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们画出将点分为两集的明显直线。然后，选择尽可能远离点的直线，最大化它到每一边最近点的距离。如果存在这样的线，则称为最大间隔超平面。直观上，由于超平面本身（它具有最长距离到任何类别的训练样本的最近点），因此，一般来说，距离越大，分类器误差越小。
- en: 'Consider learning samples given with the set ![](img/B19849_Formula_0391.png)
    consisting of ![](img/B19849_Formula_0401.png) objects with ![](img/B19849_Formula_119.png)
    parameters, where ![](img/B19849_Formula_0421.png) takes the values -1 or 1, thus
    defining the point’s classes. Each point ![](img/B19849_Formula_0431.png) is a
    vector of the dimension ![](img/B19849_Formula_119.png). Our task is to find the
    maximum margin hyperplane that separates the observations. We can use analytic
    geometry to define any hyperplane as a set of points ![](img/B19849_Formula_0431.png)
    that satisfy the condition, as illustrated here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑给定的学习样本集合 ![](img/B19849_Formula_0391.png)，其中包含 ![](img/B19849_Formula_0401.png)
    个对象，每个对象有 ![](img/B19849_Formula_119.png) 个参数，其中 ![](img/B19849_Formula_0421.png)
    取值为-1或1，从而定义了点的类别。每个点 ![](img/B19849_Formula_0431.png) 是一个维度为 ![](img/B19849_Formula_119.png)
    的向量。我们的任务是找到分离观察值的最大间隔超平面。我们可以使用解析几何来定义任何超平面为满足条件的点集 ![](img/B19849_Formula_0431.png)，如图所示：
- en: '![](img/B19849_Formula_046.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 点分离线](img/B19849_Formula_046.jpg)'
- en: Here, ![](img/B19849_Formula_0471.png) and ![](img/B19849_Formula_0481.png).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_0471.png) 和 ![](img/B19849_Formula_0481.png)。
- en: 'Thus, the linear separating (discriminant) function is described by the equation
    *g(x)=0*. The distance from the point to the separating function *g(x)=0* (the
    distance from the point to the plane) is equal to the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，线性分离（判别）函数由方程 *g(x)=0* 描述。点到分离函数 *g(x)=0*（点到平面的距离）等于以下：
- en: '![](img/B19849_Formula_049.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_049.jpg)'
- en: .
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '![](img/B19849_Formula_312.png) lies in the closure of the boundary that is
    ![](img/B19849_Formula_0512.png). The border, which is the width of the dividing
    strip, needs to be as large as possible. Considering that the closure of the boundary
    satisfies the condition ![](img/B19849_Formula_0521.png), then the distance from
    ![](img/B19849_Formula_0531.png) is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_312.png) 位于边界 ![](img/B19849_Formula_0512.png) 的闭集中。边界，即分隔带的宽度，需要尽可能大。考虑到边界的闭集满足条件
    ![](img/B19849_Formula_0521.png)，那么 ![](img/B19849_Formula_0531.png) 的距离如下所示：'
- en: '![](img/B19849_Formula_054.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_054.jpg)'
- en: .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'Thus, the width of the dividing strip is ![](img/B19849_Formula_0551.png).
    To exclude points from the dividing strip, we can write out the following conditions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分隔带的宽度是 ![](img/B19849_Formula_0551.png)。为了排除分隔带中的点，我们可以写出以下条件：
- en: '![](img/B19849_Formula_056.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_056.jpg)'
- en: 'Let’s also introduce the index function ![](img/B19849_Formula_057.png) that
    shows to which class ![](img/B19849_Formula_058.png) belongs, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再介绍一个索引函数 ![](img/B19849_Formula_057.png)，它显示了 ![](img/B19849_Formula_058.png)
    属于哪个类别，如下所示：
- en: '![](img/B19849_Formula_0591.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0591.jpg)'
- en: 'Thus, the task of choosing a separating function that generates a corridor
    of the greatest width can be written as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择一个生成最大宽度走廊的分离函数的任务可以写成如下：
- en: '![](img/B19849_Formula_060.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_060.jpg)'
- en: The *J(w)* function was introduced with the assumption that ![](img/B19849_Formula_0612.png)
    for all ![](img/B19849_Formula_194.png). Since the objective function is quadratic,
    this problem has a unique solution.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*J(w)* 函数是在假设 ![](img/B19849_Formula_0612.png) 对所有 ![](img/B19849_Formula_194.png)
    成立的情况下引入的。由于目标函数是二次的，这个问题有一个唯一解。'
- en: 'According to the Kuhn-Tucker theorem, this condition is equivalent to the following
    problem:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据库恩-塔克定理，这个条件等价于以下问题：
- en: '![](img/B19849_Formula_063.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_063.jpg)'
- en: .
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'This is provided that ![](img/B19849_Formula_0641.png) and ![](img/B19849_Formula_0651.png),
    where ![](img/B19849_Formula_0661.png), are new variables. We can rewrite ![](img/B19849_Formula_0671.png)
    in the matrix form, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在假设 ![](img/B19849_Formula_0641.png) 和 ![](img/B19849_Formula_0651.png)，其中
    ![](img/B19849_Formula_0661.png)，是新的变量的情况下提供的。我们可以将 ![](img/B19849_Formula_0671.png)
    以矩阵形式重写，如下所示：
- en: '![](img/B19849_Formula_068.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_068.jpg)'
- en: 'The *H* coefficients of the matrix can be calculated as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的 *H* 系数可以按照以下方式计算：
- en: '![](img/B19849_Formula_069.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_069.jpg)'
- en: Quadratic programming methods can solve the task ![](img/B19849_Formula_0701.png).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 二次规划方法可以解决 ![](img/B19849_Formula_0701.png) 任务。
- en: 'After finding the optimal ![](img/B19849_Formula_0712.png) for every ![](img/B19849_Formula_0721.png),
    one of the two following conditions is fulfilled:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在找到每个 ![](img/B19849_Formula_0721.png) 的最优 ![](img/B19849_Formula_0712.png)
    之后，满足以下两个条件之一：
- en: '![](img/B19849_Formula_0731.png) (*i* corresponds to a non-support vector)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0731.png) (*i* 对应于非支持向量)'
- en: '![](img/B19849_Formula_0741.png) (*i* corresponds to a support vector)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0741.png) (*i* 对应于支持向量)'
- en: 'Then, ![](img/B19849_Formula_0751.png) can be found from the relation ![](img/B19849_Formula_0761.png),
    and the value of ![](img/B19849_Formula_077.png) can be determined, considering
    that for any ![](img/B19849_Formula_0781.png) and ![](img/B19849_Formula_0791.png),
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从关系 ![](img/B19849_Formula_0761.png) 中找到 ![](img/B19849_Formula_0751.png)，并确定
    ![](img/B19849_Formula_077.png) 的值，考虑到对于任何 ![](img/B19849_Formula_0781.png) 和
    ![](img/B19849_Formula_0791.png)，如下所示：
- en: '![](img/B19849_Formula_080.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_080.jpg)'
- en: So, we can calculate w0 with the provided formula considering the provided conditions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以根据提供的条件使用提供的公式来计算 w0。
- en: 'Finally, we can obtain the discriminant function, illustrated here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以获得判别函数，如图所示：
- en: '![](img/B19849_Formula_0812.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0812.jpg)'
- en: Note that the summation is not carried out over all vectors, but only over the
    set *S*, which is the set of support vectors ![](img/B19849_Formula_0822.png).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，求和不是对所有向量进行的，而只是对集合 *S* 进行的，*S* 是支持向量 ![](img/B19849_Formula_0822.png) 的集合。
- en: Unfortunately, the described algorithm is realized only for linearly separable
    datasets, which, in itself, occurs rather infrequently. There are two approaches
    for working with linearly non-separable data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，所描述的算法仅适用于线性可分的数据集，而这种数据集本身出现的频率相当低。处理线性不可分数据有两种方法。
- en: 'One of them is called a soft margin, which chooses a hyperplane that divides
    the training sample as purely (with minimal error) as possible, while at the same
    time maximizing the distance to the nearest point on the training dataset. For
    this, we have to introduce additional variables, ![](img/B19849_Formula_0831.png),
    which characterize the magnitude of the error on each object *x*i. Furthermore,
    we can introduce the penalty for the total error into the goal functional, like
    this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种被称为软边界，它选择一个尽可能纯粹（最小化错误）地分割训练样本的超平面，同时最大化到训练数据集中最近点的距离。为此，我们必须引入额外的变量，![](img/B19849_Formula_0831.png)，这些变量表征了每个对象
    *x*i 上的错误大小。此外，我们还可以将总误差的惩罚引入目标泛函中，如下所示：
- en: '![](img/B19849_Formula_084.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_084.jpg)'
- en: 'Here, ![](img/B19849_Formula_0851.png) is a method tuning parameter that allows
    you to adjust the relationship between maximizing the width of the dividing strip
    and minimizing the total error. The value of the penalty ![](img/B19849_Formula_086.png)
    for the corresponding object *x*i depends on the location of the object *x*i relative
    to the dividing line. So, if *x*i lies on the opposite side of the discriminant
    function, then we can assume the value of the penalty ![](img/B19849_Formula_0871.png),
    if *x*i lies in the dividing strip, and comes from its class. The corresponding
    weight is, therefore, ![](img/B19849_Formula_0881.png). In the ideal case, we
    assume that ![](img/B19849_Formula_089.png). The resulting problem can then be
    rewritten as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_0851.png) 是一个方法调整参数，允许你调整最大化分割条带宽度和最小化总误差之间的关系。对于相应的对象
    *x*i，惩罚 ![](img/B19849_Formula_086.png) 的值取决于对象 *x*i 相对于分割线的位置。因此，如果 *x*i 位于判别函数的对面，则我们可以假设惩罚
    ![](img/B19849_Formula_0871.png) 的值，如果 *x*i 位于分割条带中，并且来自其类别。因此，相应的权重是 ![](img/B19849_Formula_0881.png)。在理想情况下，我们假设
    ![](img/B19849_Formula_089.png)。然后，可以将该问题重新表述如下：
- en: '![](img/B19849_Formula_090.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_090.jpg)'
- en: 'Notice that elements that are not an ideal case are involved in the minimization
    process too, as illustrated here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，非理想情况下的元素也参与了最小化过程，如下所示：
- en: '![](img/B19849_Formula_0911.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0911.jpg)'
- en: 'Here, the constant *β* is the weight that takes into account the width of the
    strip. If *β* is small, then we can allow the algorithm to locate a relatively
    high number of elements in a non-ideal position (in the dividing strip). If *β*
    is vast, then we require the presence of a small number of elements in a non-ideal
    position (in the dividing strip). Unfortunately, the minimization problem is rather
    complicated due to the ![](img/B19849_Formula_0921.png) discontinuity. Instead,
    we can use the minimization of the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，常数 *β* 是考虑条带宽度的权重。如果 *β* 很小，那么我们可以允许算法在非理想位置（分割条带中）定位相对较多的元素。如果 *β* 很大，那么我们要求在非理想位置（分割条带中）的元素数量很少。不幸的是，由于
    ![](img/B19849_Formula_0921.png) 的不连续性，最小化问题相当复杂。相反，我们可以使用以下内容的优化：
- en: '![](img/B19849_Formula_093.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_093.jpg)'
- en: 'This occurs under restrictions ![](img/B19849_Formula_0941.png), as illustrated
    here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在以下限制 ![](img/B19849_Formula_0941.png) 下发生的，如下所示：
- en: '![](img/B19849_Formula_095.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_095.jpg)'
- en: Another idea of the SVM method in the case of the impossibility of a linear
    separation of classes is the transition to a space of higher dimension, in which
    such a separation is possible. While the original problem can be formulated in
    a finite-dimensional space, it often happens that the samples for discrimination
    are not linearly separable in this space. Therefore, it is suggested to map the
    original finite-dimensional space into a larger dimension space, which makes the
    separation much easier. To keep the computational load reasonable, the mappings
    used in support vector algorithms provide ease of calculating points in terms
    of variables in the original space, specifically in terms of the kernel function.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在类别的线性分离不可能的情况下，SVM方法的一个想法是过渡到更高维度的空间，在那里这种分离是可能的。虽然原始问题可以在有限维空间中表述，但往往发生的情况是，用于判别的样本在这个空间中不是线性可分的。因此，建议将原始的有限维空间映射到更大的维度空间，这使得分离变得容易得多。为了保持计算负载合理，支持向量算法中使用的映射提供了在原始空间变量（特别是核函数）方面的计算便利。
- en: 'First, the function of the mapping ![](img/B19849_Formula_096.png) is selected
    to map the data of ![](img/B19849_Formula_0971.png) into a space of a higher dimension.
    Then, a non-linear discriminant function can be written in the form ![](img/B19849_Formula_0981.png).
    The idea of the method is to find the kernel function ![](img/B19849_Formula_0991.png)
    and maximize the objective function, as illustrated here:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择映射函数 ![](img/B19849_Formula_096.png) 将 ![](img/B19849_Formula_0971.png)
    的数据映射到更高维度的空间。然后，可以写出形式为 ![](img/B19849_Formula_0981.png) 的非线性判别函数。该方法的思想是找到核函数
    ![](img/B19849_Formula_0991.png) 并最大化目标函数，如图所示：
- en: '![](img/B19849_Formula_100.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_100.jpg)'
- en: Here, to minimize computations, the direct mapping of data into a space of a
    higher dimension is not used. Instead, an approach called the kernel trick is
    used—that is, *K(x, y)*, which is a kernel matrix. The kernel trick is a method
    used in machine learning algorithms to transform non-linear data into a higher-dimensional
    space where it becomes linearly separable. This allows to use of linear algorithms
    to solve non-linear problems because they are often simpler and more computationally
    efficient (see [*Chapter 6*](B19849_06.xhtml#_idTextAnchor301) for the detailed
    explanation of the kernel trick).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，为了最小化计算，没有使用将数据直接映射到更高维度的空间。相反，使用了一种称为核技巧的方法——即 *K(x, y)*，这是一个核矩阵。核技巧是机器学习算法中使用的一种方法，用于将非线性数据转换到更高维度的空间，使其成为线性可分的。这允许使用线性算法来解决非线性问题，因为它们通常更简单且计算效率更高（参见[*第6章*](B19849_06.xhtml#_idTextAnchor301)中对核技巧的详细解释）。
- en: In general, the more support vectors the method chooses, the better it generalizes.
    Any training example that does not constitute a support vector is correctly classified
    if it appears in the test set because the border between positive and negative
    examples is still in the same place. Therefore, the expected error rate of the
    support vector method is, as a rule, equal to the proportion of examples that
    are support vectors. As the number of measurements grows, this proportion also
    grows, so the method is not immune from the curse of dimensionality but it is
    more resistant to it than most algorithms.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，方法选择的支撑向量越多，其泛化能力越好。任何不构成支撑向量的训练示例，如果出现在测试集中，都会被正确分类，因为正负示例之间的边界仍然在同一位置。因此，支持向量方法预期的错误率通常等于支撑向量示例的比例。随着测量数量的增加，这个比例也会增加，因此该方法并非不受维度的诅咒，但它比大多数算法更能抵抗这种诅咒。
- en: It is also worth noting that the support vector method is sensitive to noise
    and data standardization.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，支持向量方法对噪声和数据标准化很敏感。
- en: Also, the method of SVMs is not only limited to the classification task but
    can also be adapted for solving regression tasks. So, you can usually use the
    same SVM software implementation for solving classification and regression tasks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SVM方法不仅限于分类任务，还可以适应解决回归任务。因此，通常可以使用相同的SVM软件实现来解决分类和回归任务。
- en: kNN method
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kNN方法
- en: 'The kNN is a popular classification method that is sometimes used in regression
    problems. It is one of the most natural approaches to classification. The essence
    of the method is to classify the current item by the most prevailing class of
    its neighbors. Formally, the basis of the method is the hypothesis of compactness:
    if the metric of the distance between the examples is clarified successfully,
    then similar examples are more likely to be in the same class. For example, if
    you don’t know what type of product to specify in the ad for a Bluetooth headset,
    you can find five similar headset ads. If four of them are categorized as *Accessories*
    and only one as *Hardware*, common sense will tell you that your ad should probably
    be in the *Accessories* category.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 是一种流行的分类方法，有时也用于回归问题。它是分类的最自然方法之一。该方法的核心是通过其邻居中最普遍的类别来对当前项进行分类。形式上，该方法的基础是紧致性假设：如果成功阐明示例之间的距离度量，则相似的示例更有可能属于同一类别。例如，如果你不知道在蓝牙耳机广告中指定什么类型的产品，你可以找到五个类似的头盔广告。如果其中四个被分类为
    *配件*，只有一个被分类为 *硬件*，常识会告诉你你的广告可能应该放在 *配件* 类别中。
- en: 'In general, to classify an object, you must perform the following operations
    sequentially:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了对对象进行分类，必须按顺序执行以下操作：
- en: Calculate the distance from the object to other objects in the training dataset.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算对象到训练数据集中其他对象的距离。
- en: Select the *k* of training objects, with the minimal distance to the object
    that is classified.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与被分类对象距离最小的 *k* 个训练对象。
- en: Set the classifying object class to the class most often found among the nearest
    *k* neighbors.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类对象的类别设置为在最近的 *k* 邻居中最常出现的类别。
- en: If we take the number of nearest neighbors *k = 1*, then the algorithm loses
    the ability to generalize (that is, to produce a correct result for data not previously
    encountered in the algorithm) because the new item is assigned to the closest
    class. If we set too high a value, then the algorithm may not reveal many local
    features.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将最近邻的数量 *k = 1*，那么算法就会失去泛化能力（即，无法对算法中未遇到过的数据进行正确的结果），因为新项目被分配到最近的类别。如果我们设置得太高，那么算法可能不会揭示许多局部特征。
- en: 'The function for calculating the distance must meet the following rules:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 计算距离的函数必须满足以下规则：
- en: '![](img/B19849_Formula_1012.png)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_1012.png)'
- en: '![](img/B19849_Formula_1021.png) only when *x = y*'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_1021.png) 仅当 *x = y* 时'
- en: '![](img/B19849_Formula_1031.png)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_1031.png)'
- en: '![](img/B19849_Formula_1041.png) in the case when the *x*, *y*, and *z* points
    don’t lie on one straight line'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B19849_Formula_1041.png) 当 *x*、*y* 和 *z* 点不在一条直线上时'
- en: 'In this case, *x*, *y*, and *z* are feature vectors of compared objects. For
    ordered attribute values, Euclidean distance can be applied, as illustrated here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*x*、*y* 和 *z* 是比较对象的特征向量。对于有序属性值，可以应用欧几里得距离，如下所示：
- en: '![](img/B19849_Formula_1051.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_1051.jpg)'
- en: In this case, *n* is the number of attributes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*n* 是属性的数量。
- en: 'For string variables that cannot be ordered, the difference function can be
    applied, which is set as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无法排序的字符串变量，可以使用差分函数，其设置如下：
- en: '![](img/B19849_Formula_1061.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_1061.jpg)'
- en: 'When finding the distance, the importance of the attributes is sometimes taken
    into account. Usually, attribute relevance can be determined subjectively by an
    expert or analyst, and is based on their own experience, expertise, and problem
    interpretation. In this case, each *i*th square of the difference in the sum is
    multiplied by the coefficient *Z*i. For example, if the attribute *A* is three
    times more important than the attribute ![](img/B19849_Formula_1221.png) (![](img/B19849_Formula_1081.png),
    ![](img/B19849_Formula_109.png)), then the distance is calculated as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找距离时，有时会考虑属性的重要性。通常，属性的相关性可以通过专家或分析师的主观判断来确定，这基于他们的经验、专业知识和问题解释。在这种情况下，总和差异的每个
    *i* 次方乘以系数 *Z*i。例如，如果属性 *A* 比属性 ![](img/B19849_Formula_1221.png) (![](img/B19849_Formula_1081.png),
    ![](img/B19849_Formula_109.png)) 重要三倍，那么距离的计算如下：
- en: '![](img/B19849_Formula_110.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_110.jpg)'
- en: This technique is called **stretching the axes**, which reduces the classification
    error.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被称为**拉伸坐标轴**，这可以减少分类误差。
- en: 'The choice of class for the object of classification can also be different,
    and there are two main approaches to making this choice: *unweighted voting* and
    *weighted voting*.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类对象的选择也可以不同，并且有两种主要方法来做出这种选择：**无权投票**和**加权投票**。
- en: For unweighted voting, we determine how many objects have the right to vote
    in the classification task by specifying the *k* number. We identify such objects
    by their minimal distance to the new object. The individual distance to each object
    is no longer critical for voting. All have equal rights in a class definition.
    Each existing object votes for the class to which it belongs. We assign a class
    with the most votes to a new object. However, there may be a problem if several
    classes score an equal number of votes. Weighted voting removes this problem.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无权投票，我们通过指定**k**的数量来确定在分类任务中有多少对象有投票权。我们通过它们到新对象的最近距离来识别这些对象。对每个对象的单独距离对于投票不再至关重要。在类定义中，所有对象都有平等的权利。每个现有对象都会为它所属的类投票。我们将获得最多投票的类分配给新对象。然而，如果几个类获得相同数量的投票，可能会出现问题。加权投票解决了这个问题。
- en: 'During the weighted vote, we also take into account the distance to the new
    object. The smaller the distance, the more significant the contribution of the
    vote. The votes for the class formula is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在加权投票过程中，我们也会考虑新对象的距离。距离越小，投票的贡献就越大。对类的投票公式如下：
- en: '![](img/B19849_Formula_111.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_111.jpg)'
- en: In this case, ![](img/B19849_Formula_1122.png) is the square of the distance
    from the known object ![](img/B19849_Formula_1132.png) to the new object ![](img/B19849_Formula_1141.png),
    while ![](img/B19849_Formula_026.png) is the number of known objects of the class
    for which votes are calculated. `class` is the name of the class. The new object
    corresponds to the class with the most votes. In this case, the probability that
    several classes gain the same number of votes is much lower. When ![](img/B19849_Formula_116.png),
    the new object is assigned to the class of the nearest neighbor.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，![](img/B19849_Formula_1122.png)是已知对象![](img/B19849_Formula_1132.png)到新对象![](img/B19849_Formula_1141.png)的距离的平方，而![](img/B19849_Formula_026.png)是计算投票的已知对象的数量。`class`是类的名称。新对象对应于获得最多投票的类。在这种情况下，几个类获得相同数量投票的概率要低得多。当![](img/B19849_Formula_116.png)时，新对象被分配到最近邻的类。
- en: A notable feature of the kNN approach is its laziness. Laziness means that the
    calculations begin only at the moment of the classification. When using training
    samples with the kNN method, we don’t simply build the model but also do sample
    classification simultaneously. Note that the method of nearest neighbors is a
    well-studied approach (in machine learning, econometrics, and statistics, only
    linear regression is more well-known). For the method of nearest neighbors, there
    are quite a few crucial theorems that state that on *infinite* samples, kNN is
    the optimal classification method. The authors of the classic book *The Elements
    of Statistical Learning* consider kNN to be a theoretically ideal algorithm, the
    applicability of which is limited only by computational capabilities and the curse
    of dimensionality.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: kNN方法的一个显著特点是它的惰性。惰性意味着计算仅在分类的瞬间开始。当使用kNN方法进行训练样本时，我们不仅构建模型，同时也会进行样本分类。请注意，最近邻方法是一个研究得很好的方法（在机器学习、计量经济学和统计学中，只有线性回归更为人所知）。对于最近邻方法，有相当多的关键定理表明，在**无限**样本上，kNN是最佳分类方法。经典书籍《统计学习基础》的作者认为kNN是一个理论上理想的算法，其适用性仅受计算能力和维度的诅咒所限制。
- en: 'kNN is one of the simplest classification algorithms, so it is often ineffective
    in real-world tasks. The KNN algorithm has several disadvantages. Besides a low
    classification accuracy when we don’t have enough samples, the kNN classifier’s
    problem is the speed of classification: if there are *N* objects in the training
    set and the dimension of the space is *K*, then the number of operations for classifying
    a test sample can be estimated as ![](img/B19849_Formula_1171.png). The dataset
    used for the algorithm must be representative. The model cannot be *separated*
    from the data: to classify a new example, you need to use all the examples.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 是最简单的分类算法之一，因此在现实世界的任务中往往效果不佳。KNN 算法有几个缺点。除了在没有足够样本的情况下分类精度低之外，kNN 分类器的另一个问题是分类速度：如果训练集中有
    *N* 个对象，空间维度是 *K*，那么对测试样本进行分类的操作次数可以估计为 ![](img/B19849_Formula_1171.png)。用于算法的数据集必须是具有代表性的。模型不能与数据分离：要分类一个新示例，你需要使用所有示例。
- en: The positive features include the fact that the algorithm is resistant to abnormal
    outliers since the probability of such a record falling into the number of kNN
    is small. If this happens, then the impact on the vote (uniquely weighted) with
    ![](img/B19849_Formula_1181.png) is also likely to be insignificant, and therefore,
    the impact on the classification result is also small. The program implementation
    of the algorithm is relatively simple, and the algorithm result is easily interpreted.
    Experts in applicable fields, therefore, understand the logic of the algorithm,
    based on finding similar objects. The ability to modify the algorithm by using
    the most appropriate combination of functions and metrics allows you to adjust
    the algorithm for a specific task.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正面特征包括算法对异常离群值的抵抗力，因为这种记录落入 kNN 数量中的概率很小。如果发生这种情况，那么对投票（唯一加权）的影响 ![](img/B19849_Formula_1181.png)
    也可能是不显著的，因此对分类结果的影响也较小。算法的程序实现相对简单，算法结果易于解释。因此，应用领域的专家可以根据找到相似对象来理解算法的逻辑。通过使用最合适的函数和度量组合来修改算法的能力，你可以调整算法以适应特定任务。
- en: Multi-class classification
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类分类
- en: Most of the existing methods of multi-class classification are either based
    on binary classifiers or are reduced to them. The general idea of such an approach
    is to use a set of binary classifiers trained to separate different groups of
    objects from each other. With such a multi-class classification, various voting
    schemes for a set of binary classifiers are used.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的多类分类方法要么基于二元分类器，要么被简化为它们。这种方法的总体思路是使用一组训练有素的二元分类器，以将不同组对象彼此分离。在这种多类分类中，使用各种投票方案对一组二元分类器进行投票。
- en: In the **one-against-all** strategy for *N* classes, *N* classifiers are trained,
    each of which separates its class from all other classes. At the recognition stage,
    the unknown vector *X* is fed to all *N* classifiers. The membership of the vector
    *X* is determined by the classifier that gave the highest estimate. This approach
    can meet the problem of class imbalances when they arise. Even if the task of
    a multi-class classification is initially balanced (that is, it has the same number
    of training samples in each class), when training a binary classifier, the ratio
    of the number of samples in each binary problem increases with an increase in
    the number of classes, which, therefore significantly affects tasks with a notable
    number of classes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在针对 *N* 个类的 **一对一** 策略中，训练 *N* 个分类器，每个分类器将其类别与其他所有类别分离。在识别阶段，未知向量 *X* 被输入到所有
    *N* 个分类器中。向量 *X* 的成员资格由给出最高估计的分类器确定。这种方法可以解决当出现类别不平衡问题时的问题。即使多类分类的任务最初是平衡的（即，每个类别的训练样本数量相同），当训练二元分类器时，每个二元问题的样本数量比例会随着类别数量的增加而增加，这因此显著影响了具有显著数量类别的任务。
- en: The **each-against-each** strategy allocates ![](img/B19849_Formula_1191.png)
    classifiers. These classifiers are trained to distinguish all possible pairs of
    classes of each other. For the input vector, each classifier gives an estimate
    of ![](img/B19849_Formula_120.png), reflecting membership in the classes ![](img/B19849_Formula_194.png)
    and ![](img/B19849_Formula_1222.png). The result is a class with a maximum sum
    ![](img/B19849_Formula_1232.png), where *g* is a monotonically non-decreasing
    function—for example, identical or logistic.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**每个对抗每个**策略分配 ![](img/B19849_Formula_1191.png) 个分类器。这些分类器被训练来区分所有可能的类别对。对于输入向量，每个分类器给出一个关于
    ![](img/B19849_Formula_120.png) 的估计，反映其在 ![](img/B19849_Formula_194.png) 和 ![](img/B19849_Formula_1222.png)
    类别中的成员资格。结果是具有最大和 ![](img/B19849_Formula_1232.png) 的类别，其中 *g* 是单调不减函数——例如，相同或逻辑函数。'
- en: The **shooting tournament** strategy also involves training ![](img/B19849_Formula_1242.png)
    classifiers that distinguish all possible pairs of classes. Unlike the previous
    strategy, at the stage of classification of the vector *X*, we arrange a tournament
    between classes. We create a tournament tree, where each class has one opponent
    and only a winner can go to the next tournament stage. So, at each step, only
    one classifier determines the vector *X* class, then the *winning* class is used
    to determine the next classifier with the next pair of classes. The process is
    carried out until there is only one winning class left, which should be considered
    the result.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**射击锦标赛**策略也涉及训练 ![](img/B19849_Formula_1242.png) 个分类器，这些分类器区分所有可能的类别对。与之前的策略不同，在向量
    *X* 的分类阶段，我们安排了类别之间的锦标赛。我们创建一个锦标赛树，其中每个类别都有一个对手，只有获胜者才能进入下一轮锦标赛。因此，在每一步，只有一个分类器确定向量
    *X* 的类别，然后使用获胜的类别来确定下一对类别的下一个分类器。这个过程一直进行到只剩下一个获胜的类别，这应该被视为结果。'
- en: Some methods can produce multi-class classification immediately, without additional
    configuration and combinations. The kNN algorithms or neural networks can be considered
    examples of such methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法可以立即产生多类分类，无需额外的配置和组合。kNN 算法或神经网络可以被认为是此类方法的例子。
- en: 'Also, the logistic regression can be generalized for the multi-class case by
    using the softmax function. The softmax function is used to determine the probability
    that a sample belongs to a particular class, it looks as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，逻辑回归可以通过使用 softmax 函数来推广到多类情况。softmax 函数用于确定一个样本属于特定类别的概率，其形式如下：
- en: '![](img/B19849_Formula_125.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![公式 125](img/B19849_Formula_125.jpg)'
- en: 'Here, *K* is the number of possible classes and the theta is a vector of learnable
    parameters. For the case when *K=2*, this expression reduces to the logistic regression:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*K* 是可能的类别数量，而 theta 是一个可学习参数的向量。当 *K=2* 时，这个表达式简化为逻辑回归：
- en: '![](img/B19849_Formula_126.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![公式 126](img/B19849_Formula_126.jpg)'
- en: 'Replacing the vector difference:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 用单个参数向量替换向量差：
- en: '![](img/B19849_Formula_127.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![公式 127](img/B19849_Formula_127.jpg)'
- en: 'with a single parameter vector, we can see that the probability for the one
    class will be predicted as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于一类，其概率将按以下方式预测：
- en: '![](img/B19849_Formula_128.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![公式 128](img/B19849_Formula_128.jpg)'
- en: 'For the second class, it will be the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二类，它将是以下内容：
- en: '![](img/B19849_Formula_129.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![公式 129](img/B19849_Formula_129.jpg)'
- en: As you can see, these expressions are equal to the logistic regression we already
    saw.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这些表达式等同于我们之前看到的逻辑回归。
- en: Now we have become familiar with some of the most widespread classification
    algorithms, let’s look at how to use them in different C++ libraries.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了一些最广泛使用的分类算法，让我们看看如何在不同的 C++ 库中使用它们。
- en: Examples of using C++ libraries for dealing with the classification task
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 C++ 库处理分类任务的示例
- en: 'Let’s now see how to use the methods we’ve described for solving a classification
    task on artificial datasets, which we can see in the following screenshot:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何使用我们描述的方法来解决人工数据集上的分类任务，这些数据集可以在下面的屏幕截图中看到：
- en: '![Figure 7.3 – Artificial datasets](img/B19849_07_03.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 人工数据集](img/B19849_07_03.jpg)'
- en: Figure 7.3 – Artificial datasets
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 人工数据集
- en: As we can see, these datasets contain two and three different classes of objects,
    so it makes sense to use methods for multi-class classification because such tasks
    appear more often in real life; they can be easily reduced to binary classification.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这些数据集包含两种和三种不同的对象类别，因此使用多类分类方法是有意义的，因为这类任务在现实生活中出现得更频繁；它们可以很容易地简化为二分类。
- en: Classification is a supervised technique, so we usually have a training dataset,
    as well as new data for classification. To model this situation, we will use two
    datasets in our examples, one for training and one for testing. They come from
    the same distribution in one large dataset. However, the test set won’t be used
    for training; therefore, we can evaluate the accuracy metric and see how well
    models perform and generalize.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是一种监督技术，因此我们通常有一个训练数据集，以及用于分类的新数据。为了模拟这种情况，在我们的示例中我们将使用两个数据集，一个用于训练，一个用于测试。它们来自一个大型数据集中的相同分布。然而，测试集不会用于训练；因此，我们可以评估准确度指标，并查看模型的表现和泛化能力如何。
- en: Using the mlpack library
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用mlpack库
- en: 'In this section, we show how to use the `mlpack` library for solving the classification
    task. This library provides the implementation for the three main types of classification
    algorithms: logistic regression, softmax regression, and SVM.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示如何使用`mlpack`库来解决分类任务。这个库为三种主要的分类算法提供了实现：逻辑回归、softmax回归和SVM。
- en: With softmax regression
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用softmax回归
- en: 'The `mlpack` library implements multi-class logistic regression in the `SoftmaxRegression`
    class. Using this class is very simple. We have to initialize an object with the
    number of samples that will be used for training and the number of classes. Let’s
    say we have the following objects as training data and labels:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库在`SoftmaxRegression`类中实现了多类逻辑回归。使用这个类非常简单。我们必须使用用于训练的样本数量和类的数量初始化一个对象。假设我们有以下对象作为训练数据和标签：'
- en: '[PRE0]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we can initialize an object of `SoftmaxRegression` as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以如下初始化一个`SoftmaxRegression`对象：
- en: '[PRE1]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After we have the classifier object, we can train it and apply the classification
    function for some new data. The following code snippet shows how it can be done:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有了分类器对象之后，我们可以对其进行训练，并对一些新数据应用分类函数。以下代码片段显示了如何实现：
- en: '[PRE2]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Having the prediction vector we can visualize it with the technique we are
    using in this book that is based on the `plotcpp` library. Notice that the `Train`
    and `Classify` method requires the `size_t` type for the class labels. The following
    screenshot shows the results of applying the `mlpack` implementation of the softmax
    regression algorithm to our datasets:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 获得预测向量后，我们可以使用本书中基于`plotcpp`库的技术将其可视化。请注意，`Train`和`Classify`方法需要`size_t`类型的类标签。以下截图显示了将`mlpack`实现的softmax回归算法应用于我们的数据集的结果：
- en: '![Figure 7.4 – Softmax classification with mlpack](img/B19849_07_04.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 使用mlpack的softmax分类](img/B19849_07_04.jpg)'
- en: Figure 7.4 – Softmax classification with mlpack
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 使用mlpack的softmax分类
- en: Notice that we have classification errors in the **Dataset 0**, **Dataset 1**,
    and **Dataset 2** datasets, and other datasets were classified almost correctly.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们在**Dataset 0**、**Dataset 1**和**Dataset 2**数据集中存在分类错误，而其他数据集的分类几乎正确。
- en: With SVMs
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SVMs
- en: 'The `mlpack` library also has an implementation of the multi-class SVM algorithm
    in the `LinearSVM` class. The library provides mostly the same API for all classification
    algorithms so the initialization of the classifier object is mostly the same as
    in the previous example. The main difference is that you can use the constructor
    without parameters. So, the object initialization will be the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack`库在`LinearSVM`类中也有多类SVM算法的实现。该库为所有分类算法提供了几乎相同的API，因此分类器对象的初始化与上一个示例大致相同。主要区别在于您可以使用不带参数的构造函数。因此，对象初始化如下：'
- en: '[PRE3]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we train the classificator with the `Train` method and apply the `Classify`
    method for new data samples, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`Train`方法训练分类器，并使用`Classify`方法对新数据样本进行应用，如下所示：
- en: '[PRE4]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following screenshot shows the results of applying the `mlpack` implementation
    of the SVM algorithm to our datasets:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将`mlpack`实现的SVM算法应用于我们的数据集的结果：
- en: '![Figure 7.5 – SVM classification with mlpack](img/B19849_07_05.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 使用mlpack的SVM分类](img/B19849_07_05.jpg)'
- en: Figure 7.5 – SVM classification with mlpack
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 使用mlpack的SVM分类
- en: You can see that the results we got from the SVM method are pretty much the
    same as we got with the softmax regression.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们从SVM方法得到的结果与使用softmax回归得到的结果几乎相同。
- en: With the linear regression algorithm
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用线性回归算法
- en: 'The `mlpack` library also implements the classic logistic regression algorithm
    in the `LogisticRegression` class. An object of this class can be applied to classify
    samples only into two classes. The usage API is the same as in the previous examples
    for the `mlpack` library. The typical application of this class will be the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlpack` 库还在 `LogisticRegression` 类中实现了经典的逻辑回归算法。这个类的对象只能用于将样本分类为两个类别。使用 API
    与 `mlpack` 库之前的示例相同。这个类的典型应用如下：'
- en: '[PRE5]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following screenshot shows the results of applying the two-class logistic
    regression to our datasets:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将两类逻辑回归应用于我们的数据集的结果：
- en: '![Figure 7.6 – Logistic regression classification with mlpack](img/B19849_07_06.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 使用 mlpack 的逻辑回归分类](img/B19849_07_06.jpg)'
- en: Figure 7.6 – Logistic regression classification with mlpack
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 使用 mlpack 的逻辑回归分类
- en: You can see that we only got reasonable classifications for **Dataset 3** and
    **Dataset 4** as they can be separated with a straight line. However, due to the
    two-class limitation, we were not able to get the correct results.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，我们只为 **Dataset 3** 和 **Dataset 4** 获得了合理的分类，因为它们可以用直线分开。然而，由于两类的限制，我们无法获得正确的结果。
- en: Using the Dlib library
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dlib 库
- en: 'The `Dlib` library doesn’t have many classification algorithms. There are two
    that are most applicable: *KRR* and *SVM*. These methods are implemented as binary
    classifiers, but for multi-class classification, this library provides the `one_vs_one_trainer`
    class, which implements the voting strategy. Note that this class can use classifiers
    of different types so that you can combine the KRR and the SVM for one classification
    task. We can also specify which classifiers should be used for which distinct
    classes.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库没有很多分类算法。其中有两个是最适用的：*KRR* 和 *SVM*。这些方法被实现为二元分类器，但对于多类分类，这个库提供了 `one_vs_one_trainer`
    类，它实现了投票策略。请注意，这个类可以使用不同类型的分类器，这样您可以将 KRR 和 SVM 结合起来进行一个分类任务。我们还可以指定哪些分类器应该用于哪些不同的类别。'
- en: With KRR
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 KRR
- en: 'The following code sample shows how to use the `Dlib` KRR algorithm implementation
    for the multi-class classification:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了如何使用 `Dlib` 的 KRR 算法实现进行多类分类：
- en: '[PRE6]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Firstly, we initialized the object of the `krr_trainer` class, and then we
    configured it with the instance of a kernel object. In this example, we used the
    `radial_basis_kernel` type for the kernel object, in order to deal with samples
    that can’t be linearly separated. After we obtained the binary classifier object,
    we initialized the instance of the `one_vs_one_trainer` class and added this classifier
    to its stack with the `set_trainer()` method. Then, we used the `train()` method
    for training our multi-class classifier. As with most of the algorithms in the
    `Dlib` library, this one assumes that the training samples and labels have the
    `std::vector` type, whereby each element has a `matrix` type. The `train()` method
    returns a decision function—namely, the object that behaves as a functor, which
    then takes a single sample and returns a classification label for it. This decision
    function is an object of the `one_vs_one_decision_function` type. The following
    piece of code demonstrates how we can use it:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化了 `krr_trainer` 类的对象，然后使用核对象的实例配置它。在这个例子中，我们使用了 `radial_basis_kernel`
    类型作为核对象，以处理无法线性分离的样本。在获得二元分类器对象后，我们初始化了 `one_vs_one_trainer` 类的实例，并使用 `set_trainer()`
    方法将这个分类器添加到其堆栈中。然后，我们使用 `train()` 方法来训练我们的多类分类器。与 `Dlib` 库中的大多数算法一样，这个算法假设训练样本和标签具有
    `std::vector` 类型，其中每个元素具有 `matrix` 类型。`train()` 方法返回一个决策函数——即作为函数对象的行为的对象，它接受一个单独的样本并返回其分类标签。这个决策函数是
    `one_vs_one_decision_function` 类型的对象。以下代码片段展示了我们如何使用它：
- en: '[PRE7]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There is no explicit implementation for the accuracy metric in the `Dlib` library;
    so, in this example, accuracy is calculated directly as a ratio of correctly classified
    test samples against the total number of test samples.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中没有对准确度指标进行明确的实现；因此，在这个例子中，准确度直接计算为正确分类的测试样本数与总测试样本数的比率。'
- en: 'The following screenshot shows the results of applying the `Dlib` implementation
    of the KRR algorithm to our datasets:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将 `Dlib` 的 KRR 算法实现应用于我们的数据集的结果：
- en: '![Figure 7.7 – KRR classification with DLib](img/B19849_07_07.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 使用 DLib 的 KRR 分类](img/B19849_07_07.jpg)'
- en: Figure 7.7 – KRR classification with Dlib
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 使用 Dlib 的 KRR 分类
- en: Notice that the KRR algorithm performed a correct classification on all datasets.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 KRR 算法在所有数据集上进行了正确的分类。
- en: With SVM
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 SVM
- en: 'The following code sample shows how to use the `Dlib` SVM algorithm implementation
    for multi-class classification:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了如何使用 `Dlib` SVM 算法实现进行多类分类：
- en: '[PRE8]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This sample shows that the `Dlib` library also has a unified API for using different
    algorithms, and the main difference from the previous example is the object of
    the binary classifier. For the SVM classification, we used an object of the `svm_nu_trainer`
    type, which was also configured with the kernel object of the `radial_basis_kernel`
    type.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例表明 `Dlib` 库也有一个统一的 API 用于使用不同的算法，与前一个示例的主要区别是二分类器的对象。对于 SVM 分类，我们使用了 `svm_nu_trainer`
    类型的对象，该对象还配置了 `radial_basis_kernel` 类型的核对象。
- en: 'The following screenshot shows the results of applying the `Dlib` implementation
    of the SVM algorithm to our datasets:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了将 `Dlib` SVM 算法的实现应用于我们的数据集的结果：
- en: '![Figure 7.8 – SVM classification with DLib](img/B19849_07_08.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 使用 DLib 的 SVM 分类](img/B19849_07_08.jpg)'
- en: Figure 7.8 – SVM classification with Dlib
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 使用 Dlib 的 SVM 分类
- en: You can see the `Dlib` implementation of the SVM algorithm also did correct
    classification on all datasets because of the `mlpack` implementation of the same
    algorithm made incorrect classification in some cases due to its linearity.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到 `Dlib` SVM 算法的实现也在所有数据集上进行了正确的分类，因为 `mlpack` 对同一算法的实现由于线性特性在某些情况下进行了错误的分类。
- en: Using the Flashlight library
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Flashlight 库
- en: The Flashlight library doesn’t have any special classes for the classification
    algorithms. But using the linear algebra primitives and auto-gradient facilities
    of the library, we can implement the logistic regression algorithm from scratch.
    Also, to handle the datasets that are non-linearly separable, the kernel trick
    approach will be implemented.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Flashlight 库没有为分类算法提供任何特殊的类。但通过使用库的线性代数原语和自动微分功能，我们可以从头实现逻辑回归算法。此外，为了处理非线性可分的数据集，我们将实现核技巧方法。
- en: With logistic regression
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用逻辑回归
- en: 'The following example shows how to implement the two-class classification with
    the Flashlight library. Let’s define a function for training a linear classifier;
    it will have the following signature:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用 Flashlight 库实现二类分类。让我们定义一个用于训练线性分类器的函数；它将具有以下签名：
- en: '[PRE9]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`train_x` and `train_y` are the training samples and their labels correspondingly.
    The function’s result is the learned parameters vector—in our case, defined with
    the `fl::Tensor` class. We are going to use the batched gradient descent algorithm
    to learn the parameters vector. So, we can use the Flashlight dataset types to
    simplify work with training data batches. The following code snippet shows how
    we can make a dataset object that will allow us to iterate over batches:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_x` 和 `train_y` 是训练样本及其相应的标签。函数的结果是学习到的参数向量——在我们的例子中，使用 `fl::Tensor`
    类定义。我们将使用批梯度下降算法来学习参数向量。因此，我们可以使用 Flashlight 数据集类型来简化对训练数据批次的处理。以下代码片段展示了我们如何创建一个数据集对象，它将允许我们遍历批次：'
- en: '[PRE10]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: At first, we defined the regular dataset object with the `fl::TensorDataset`
    type and then we used it to create the object of the `fl::BatchDataset` type that
    was initialized with the batch size value also.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `fl::TensorDataset` 类型定义了常规数据集对象，然后我们使用它创建了 `fl::BatchDataset` 类型的对象，该对象还初始化了批次大小值。
- en: 'Next, we need to initialize the parameters vector that we will learn with the
    gradient descent, as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要初始化我们将使用梯度下降法学习的参数向量，如下所示：
- en: '[PRE11]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Notice that we explicitly passed the `true` value as the last argument to enable
    the gradient calculation by the Flashlight autograd mechanism. Now, we are ready
    to define the training cycle with the predefined number of epochs. In each of
    the epochs, we will iterate over all batches in the dataset. So, such a cycle
    can be defined as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们明确地将 `true` 值作为最后一个参数传递，以启用 Flashlight autograd 机制进行梯度计算。现在，我们准备好使用预定义的周期数定义训练周期。在每个周期中，我们将遍历数据集中的所有批次。因此，这样的周期可以定义为以下：
- en: '[PRE12]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can see two nested loops: the outer one for epochs and the inner one for
    batches. The `batch_dataset` object used in the `for` loop is compatible with
    C++ range based for loop construction, so it’s easily used to access the batches.
    Also, notice that we defined two variables, `x` and `y`, with the `fl::Variable`
    type as we did for weights. Usage of this type makes it possible to pass tensor
    values into the autograd mechanism. And for these variables, we didn’t configure
    the gradients calculation because they are not trainable parameters. Another important
    issue is that we used `fl::reshape` to make all tensor shapes compatible with
    the matrix multiplication that will be applied in the loss function calculation.
    The logistic regression loss function looks as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到两个嵌套循环：外层循环用于epoch，内层循环用于批次。在`for`循环中使用的`batch_dataset`对象与C++基于范围的for循环构造兼容，因此它很容易用来访问批次。此外，请注意，我们定义了两个变量`x`和`y`，其类型与权重相同，为`fl::Variable`类型。使用这种类型使得将张量值传递到自动微分机制成为可能。对于这些变量，我们没有配置梯度计算，因为它们不是可训练的参数。另一个重要问题是，我们使用了`fl::reshape`来使所有张量形状与损失函数计算中将要应用的矩阵乘法兼容。逻辑回归损失函数如下所示：
- en: '![](img/B19849_Formula_130.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_130.jpg)'
- en: 'In the code, we can implement it with the following lines:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以用以下几行来实现它：
- en: '[PRE13]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After we get the loss value, we can apply the gradient descent algorithm to
    correct the weights (parameters vector) according to the influence of the current
    training samples batch. The following code snippet shows how to do it:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到损失值后，我们可以应用梯度下降算法，根据当前训练样本批次的影响来纠正权重（参数向量）。以下代码片段展示了如何实现它：
- en: '[PRE14]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Notice that the last step in the gradient zeroing was done to make it possible
    to learn something new from the next training sample and not mix gradients. At
    the end of the training cycle, the resulting parameters vector can be returned
    from the function as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到梯度归零的最后一步是为了使从下一个训练样本中学习新内容成为可能，而不是混合梯度。在训练周期结束时，可以从函数中返回结果参数向量，如下所示：
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And the following sample shows how our training function can be used :'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用我们的训练函数：
- en: '[PRE16]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Having the learned parameter vector, we can use it to classify a new data sample
    as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得学习参数向量后，我们可以用它来分类新的数据样本，如下所示：
- en: '[PRE17]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can see that we implemented the logistic function call that returns a result
    into the `p` variable. The value of this variable can be interpreted as the probability
    of the event that the sample belongs to a particular class. We introduced the
    `threshold` variable to check the probability. If it is greater than this threshold,
    then we classify the sample as it has a class of `1`; otherwise, it has a class
    of `0`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们将返回结果的逻辑函数调用实现了到`p`变量中。这个变量的值可以解释为样本属于特定类的事件的概率。我们引入了`threshold`变量来检查概率。如果它大于这个阈值，那么我们将样本分类为类`1`；否则，它属于类`0`。
- en: 'The following screenshot shows the results of applying Flashlight implementation
    of the logistic regression algorithm to our datasets with two classes:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将Flashlight实现的逻辑回归算法应用于我们的具有两个类别的数据集的结果：
- en: '![Figure 7.9 – Logistic regression classification with Flashlight](img/B19849_07_09.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 使用Flashlight进行逻辑回归分类](img/B19849_07_09.jpg)'
- en: Figure 7.9 – Logistic regression classification with Flashlight
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 使用Flashlight进行逻辑回归分类
- en: You can see that it fails to correctly classify **Dataset 0** and **Dataset
    1** with a non-linear class boundary but successfully classified **Dataset 4**
    as it is linearly separable.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，它未能正确分类**数据集0**和**数据集1**（具有非线性类边界），但成功地将**数据集4**分类为线性可分。
- en: With logistic regression and kernel trick
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用逻辑回归和核技巧
- en: 'To address the problem with non-linear class boundaries, we can apply the kernel
    trick. Let’s see how we can implement it in the Flashlight library with the Gaussian
    kernel. The idea is to move our data samples into higher dimension space where
    they can be linearly separable. The Gaussian kernel function looks like as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决非线性类边界的问题，我们可以应用核技巧。让我们看看我们如何在Flashlight库中使用高斯核来实现它。想法是将我们的数据样本移动到更高维的空间中，在那里它们可以线性可分。高斯核函数如下所示：
- en: '![](img/B19849_Formula_1312.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_1312.png)'
- en: 'To make our calculations more computationally effective, we can rewrite them
    in the following way:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的计算更加高效，我们可以按以下方式重写它们：
- en: '![](img/B19849_Formula_1322.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_1322.png)'
- en: 'The following code sample shows this formula implementation:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了该公式的实现：
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `make_kernel_matrix` function takes two matrices and applies the Gaussian
    kernel returning the single matrix. Let’s see how we can apply it to our problem.
    At first, we apply it to our training dataset as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_kernel_matrix` 函数接受两个矩阵，并应用高斯核，返回单个矩阵。让我们看看我们如何将其应用于我们的问题。首先，我们将其应用于我们的训练数据集，如下所示：'
- en: '[PRE19]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Notice that the function was called with the same `train_x` value for the two
    arguments. So, we moved our training dataset into the higher dimension space based
    on this training dataset. The gamma is a scaling hyperparameter that was configured
    manually in this example. Having this transformed dataset, we can train a classifier
    with the function that we created in the previous example as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到函数对两个参数使用了相同的 `train_x` 值。因此，我们根据这个训练数据集将我们的训练数据集移动到更高维的空间。在这个例子中，gamma 是一个手动配置的缩放超参数。有了这个转换后的数据集，我们可以使用我们在上一个例子中创建的函数来训练一个分类器，如下所示：
- en: '[PRE20]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, to use these weights (parameters vector), we should apply the kernel
    to the new data samples in the following way:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了使用这些权重（参数向量），我们应该以下述方式将核函数应用于新的数据样本：
- en: '[PRE21]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can see that we used the reshaped new sample as the first argument and
    the training set tensor as the second argument. So, we transformed the new sample
    into the higher dimension space based on the original training data to preserve
    the same space properties. Then, we can apply the same classification procedure
    with a threshold as in the previous example, as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们使用了重塑的新样本作为第一个参数，训练集张量作为第二个参数。因此，我们根据原始训练数据将新样本转换到更高维的空间，以保持相同的空间属性。然后，我们可以像上一个例子一样应用相同的分类程序，如下所示：
- en: '[PRE22]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can see that we just used the transformed weights tensor and transformed
    sample.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们只是使用了转换后的权重张量和转换后的样本。
- en: 'The following screenshot shows the results of applying the logistic regression
    with the kernel trick implementation to our two-class datasets:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将带有核技巧的逻辑回归应用于我们的双类数据集的结果：
- en: '![Figure 7.10 – Logistic regression with kernel trick classification with Flashlight](img/B19849_07_10.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 使用Flashlight的带有核技巧的逻辑回归分类](img/B19849_07_10.jpg)'
- en: Figure 7.10 – Logistic regression with kernel trick classification with Flashlight
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 使用Flashlight的带有核技巧的逻辑回归分类
- en: You can see that with the kernel trick logistic regression successfully classified
    data with non-linear class boundaries.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用核技巧的逻辑回归成功地对具有非线性类别边界的数据进行分类。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed supervised machine learning approaches to solving
    classification tasks. These approaches use trained models to determine the class
    of an object according to its characteristics. We considered two methods of binary
    classification: logistic regression and SVMs. We looked at the approaches for
    the implementation of multi-class classification.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了用于解决分类任务的监督机器学习方法。这些方法使用训练好的模型根据其特征确定对象的类别。我们考虑了两种二分类方法：逻辑回归和SVMs。我们探讨了多类分类的实现方法。
- en: We saw that working with non-linear data requires additional improvements in
    the algorithms and their tuning. Implementations of classification algorithms
    differ in terms of performance, as well as the amount of required memory and the
    amount of time required for learning. Therefore, the classification algorithm’s
    choice should be guided by a specific task and business requirements. Furthermore,
    their implementations in different libraries can produce different results, even
    for the same algorithm. Therefore, it makes sense to have several libraries for
    your software.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到处理非线性数据需要算法和它们的调整的额外改进。分类算法的实现因性能、所需内存量和学习所需时间而异。因此，分类算法的选择应受特定任务和业务需求指导。此外，它们在不同库中的实现可能产生不同的结果，即使是相同的算法。因此，为你的软件拥有几个库是有意义的。
- en: In the next chapter, we will discuss recommender systems. We will see how they
    work, which algorithms exist for their implementation, and how to train and evaluate
    them. In the simplest sense, recommender systems are used to predict which objects
    (goods or services) are of interest to a user. Examples of such systems can be
    seen in many online stores such as Amazon or on streaming sites such as Netflix,
    which recommend new content based on your previous consumption.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论推荐系统。我们将了解它们是如何工作的，有哪些算法用于实现它们，以及如何训练和评估它们。在最简单的意义上，推荐系统用于预测用户可能感兴趣的对象（商品或服务）。这样的系统在许多在线商店，如亚马逊，或流媒体网站，如Netflix上都可以看到，它们根据你的先前消费推荐新的内容。
- en: Further reading
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Logistic Regression—Detailed* *Overview*: [https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*逻辑回归——详细概述*：[https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)'
- en: 'Understanding SVM algorithm from examples (along with code): [https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过示例理解SVM算法（附带代码）：[https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)
- en: 'Understanding Support Vector Machines: A Primer: [https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/](https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/)'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解支持向量机：入门指南：[https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/](https://appliedmachinelearning.wordpress.com/2017/03/09/understanding-support-vector-machines-a-primer/)
- en: '*Support Vector Machine: Kernel Trick; Mercer’s* *Theorem*: [https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d](https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*支持向量机：核技巧；梅尔策的定理*：[https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d](https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d)'
- en: 'SVMs with Kernel Trick (lecture): [https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有核技巧的SVM（讲座）：[https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf](https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf)
- en: '*Support Vector Machines—Kernels and the Kernel* *Trick*: [https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf](https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*支持向量机——核和核技巧*：[https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf](https://cogsys.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf)'
- en: '*A Complete Guide to K-Nearest-Neighbors with Applications in Python and* *R*:
    [https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K-Nearest-Neighbors的完整指南及其在Python和R中的应用*：[https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)'
