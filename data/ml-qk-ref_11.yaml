- en: Advanced Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级方法
- en: We have made it to the final chapter of this book. However, this doesn't mean
    that we can ignore the topics that we are going to discuss in the upcoming sections.
    These topics are state of the art and will separate you from the rest.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达了这本书的最后一章。然而，这并不意味着我们可以忽视即将讨论的主题。这些主题是当前最前沿的，并将使你与其他人区分开来。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Kernel principal component analysis
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核主成分分析
- en: Independent component analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: Compressed sensing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩感知
- en: Bayesian multiple imputations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯多重插补
- en: Self-organizing maps
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自组织映射
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we understood what **principal component analysis**
    (**PCA**) is, how it works, and when we should be deploying it. However, as a
    dimensionality reduction technique, do you think that you can put this to use
    in every scenario? Can you recall the roadblock or the underlying assumption behind
    it that we discussed?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了**主成分分析**（PCA）是什么，它是如何工作的，以及我们应该在何时部署它。然而，作为一种降维技术，你认为你可以在每个场景中都使用它吗？你能回忆起我们讨论过的它背后的障碍或潜在假设吗？
- en: Yes, the most important assumption behind PCA is that it works for datasets
    that are linearly separable. However, in the real world, you don't get this kind
    of dataset very often. We need a method to capture non-linear data patterns.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，PCA背后最重要的假设是它适用于线性可分的数据集。然而，在现实世界中，你很少遇到这种类型的数据集。我们需要一种方法来捕捉非线性数据模式。
- en: 'On the left-hand side, we have got a dataset in which there are two classes.
    We can see that once we arrive at the projections and establish the components,
    PCA doesn''t have an effect on it and that it is not able to separate it by a
    line in a 2D dimension. That is, PCA can only function well when we have got low-level
    dimensions and linearly separable data. The following plot shows the dataset of
    two classes:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们有一个包含两个类别的数据集。我们可以看到，一旦我们到达投影并建立组件，PCA对它没有影响，并且它不能在二维维度上通过一条线将其分开。也就是说，PCA只有在我们有低维度和线性可分数据时才能很好地工作。以下图显示了两个类别的数据集：
- en: '![](img/a232d621-ee4e-41be-8803-acf75c70efd6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a232d621-ee4e-41be-8803-acf75c70efd6.png)'
- en: 'This is why we bring in the kernel method: so that we can merge it with PCA
    to achieve it.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正是因为这个原因，我们引入了核方法：这样我们就可以将其与PCA结合以实现它。
- en: 'Just to recap what you learned about the kernel method, we will discuss it
    and its importance in brief:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 只为了回顾一下你关于核方法学到的知识，我们将简要讨论它及其重要性：
- en: We have got data in a low dimensional space. However, at times, it's difficult
    to achieve classification (green and red) when we have got non-linear data (as
    shown in the following diagram). This being said, we do have a clear understanding
    that having a tool that can map the data from a lower to a higher dimension will
    result in a proper classification. This tool is called the **kernel method**.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在低维空间中有数据。然而，当我们有非线性数据（如下面的图所示）时，有时很难进行分类（绿色和红色）。话虽如此，我们确实清楚地理解，拥有一种可以将数据从低维映射到高维的工具将导致适当的分类。这个工具被称为**核方法**。
- en: The same dataset turns out to be linearly separable in the new feature space.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的数据集在新特征空间中表现出线性可分性。
- en: 'The following diagram shows data in low and high dimensional spaces:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了低维和高维空间中的数据：
- en: '![](img/c8e0b260-2472-4be2-a5ab-c9dbd95c0e2b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c8e0b260-2472-4be2-a5ab-c9dbd95c0e2b.png)'
- en: 'To classify the green and red points in the preceding diagram, the feature
    mapping function has to take the data and change is from being 2D to 3D, that
    is, *Φ = R² → R³*. The equation for this is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将前图中绿色和红色点进行分类，特征映射函数必须将数据从二维转换为三维，即*Φ = R² → R³*。这个方程如下：
- en: '![](img/a66ab011-d5a2-4bcd-9ac0-a5942e1c9c6d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a66ab011-d5a2-4bcd-9ac0-a5942e1c9c6d.png)'
- en: 'The goal of the kernel method is to figure out and choose kernel function *K*.
    This is so that we can find the geometry feature in the new dimension and classify
    data patterns. Let''s see how this is done:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 核方法的目标是找出并选择核函数*K*。这样我们就可以在新维度中找到几何特征并对数据模式进行分类。让我们看看这是如何完成的：
- en: '![](img/ce27e77c-769b-44f4-bef5-13c8c45a8758.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ce27e77c-769b-44f4-bef5-13c8c45a8758.png)'
- en: '![](img/b741c251-2a86-48a1-9ca2-1d36852fe146.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b741c251-2a86-48a1-9ca2-1d36852fe146.png)'
- en: '![](img/ab6c39e4-d612-4194-a8fc-d36429518393.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ab6c39e4-d612-4194-a8fc-d36429518393.png)'
- en: '![](img/21979279-0315-447f-b2fc-e8a34ca7c792.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/21979279-0315-447f-b2fc-e8a34ca7c792.png)'
- en: '![](img/a28c212f-cb59-4bd0-a194-ec0a992eb0cc.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a28c212f-cb59-4bd0-a194-ec0a992eb0cc.png)'
- en: Here, Phi is a feature mapping function. But do we always need to know the feature
    mapping function? Not really. Kernel function *K* does the trick. With a given
    kernel function, *K*, we can come up with a feature space, *H*. Two of the popular
    kernel functions are Gaussian and polynomial kernel functions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Phi是一个特征映射函数。但我们是否总是需要知道特征映射函数？实际上并不需要。核函数*K*完成了这项工作。给定核函数*K*，我们可以得到一个特征空间*H*。两种流行的核函数是高斯核函数和多项式核函数。
- en: Picking an apt kernel function will enable us to figure out the characteristics
    of the data in the new feature space quite well.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个合适的核函数将使我们能够很好地了解新特征空间中数据的特征。
- en: Now that we have made ourselves familiar with the kernel trick, let's move on
    to the Kernel PCA.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了核技巧，让我们继续学习核PCA。
- en: Kernel PCA
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核PCA
- en: 'The Kernel PCA is an algorithm that not only keeps the main spirit of PCA as
    it is, but goes a step further to make use of the kernel trick so that it is operational
    for non-linear data:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 核PCA是一种算法，它不仅保留了PCA的主要精神，而且更进一步，利用核技巧使其能够处理非线性数据：
- en: 'Let''s define the covariance matrix of the data in the feature space, which
    is the product of the mapping function and the transpose of the mapping function:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义特征空间中数据的协方差矩阵，它是映射函数与映射函数转置的乘积：
- en: '![](img/390c4461-854c-493d-8ffd-a0e4e8c7a67c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/390c4461-854c-493d-8ffd-a0e4e8c7a67c.png)'
- en: It is similar to the one we used for PCA.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 它与我们用于PCA的类似。
- en: 'The next step is to solve the following equation so that we can compute principal
    components:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是解下面的方程，以便我们能够计算主成分：
- en: '![](img/5c3b4a63-2c96-4b9e-bec7-23d242aab163.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5c3b4a63-2c96-4b9e-bec7-23d242aab163.png)'
- en: Here, *C[F]* is the covariance matrix of the data in feature space, *v* is the
    eigenvector, and *λ* (lambda) is the eigenvalues.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*C[F]*是特征空间中数据的协方差矩阵，*v*是特征向量，*λ*（lambda）是特征值。
- en: 'Let''s put the value of *step 1* into *step 2 *– that is, the value of *C[F]* in
    the equation of *step 2.* The eigenvector will be as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将*步骤1*的值放入*步骤2*中——即在*步骤2*的方程中的*C[F]*的值。特征向量将如下所示：
- en: '![](img/703773c2-f899-49f5-b428-f29b89542f81.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/703773c2-f899-49f5-b428-f29b89542f81.png)'
- en: Here, ![](img/4607835d-fe85-40be-ba82-34c117ed45a4.png) is a scalar number.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![图片](img/4607835d-fe85-40be-ba82-34c117ed45a4.png)是一个标量数。
- en: 'Now, let''s add the kernel function into the equation. Let''s multiply *Φ(x[k]) *on
    both sides of the formula, ![](img/dcda92c5-f517-4edb-838c-8a34df198137.png):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将核函数添加到方程中。让我们将*Φ(x[k])*乘以公式的两边，![图片](img/dcda92c5-f517-4edb-838c-8a34df198137.png)：
- en: '![](img/350e63a8-acbb-4abc-9650-2c1a679c3a25.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/350e63a8-acbb-4abc-9650-2c1a679c3a25.png)'
- en: 'Let''s put the value of *v* from the equation in *step 3* into the equation
    of *step 4*, as follows:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将*步骤3*中的方程中的*v*值放入*步骤4*的方程中，如下所示：
- en: '![](img/d1c66f74-0346-4b50-bfa4-e479e9618888.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1c66f74-0346-4b50-bfa4-e479e9618888.png)'
- en: 'Now, we call *K *![](img/4f568c8c-7d71-4ba3-98f9-3af69361f697.png). Upon simplifying
    the equation from *step 5* by keying in the value of *K*, we get the following:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们调用*K*![图片](img/4f568c8c-7d71-4ba3-98f9-3af69361f697.png)。通过输入*K*的值简化*步骤5*的方程，我们得到以下结果：
- en: '![](img/cf4a358d-3282-40ed-89e8-5d402a80317c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf4a358d-3282-40ed-89e8-5d402a80317c.png)'
- en: 'On doing eigen decomposition, we get the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行特征值分解后，我们得到以下结果：
- en: '![](img/ae3ae0fb-89df-4a34-8075-56d9f82e8481.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ae3ae0fb-89df-4a34-8075-56d9f82e8481.png)'
- en: 'On normalizing the feature space for centering, we get the following result:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在对特征空间进行归一化以进行中心化后，我们得到以下结果：
- en: '![](img/2cf064cb-9d15-463d-9965-6c1ffbd533ad.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2cf064cb-9d15-463d-9965-6c1ffbd533ad.png)'
- en: 'Now, let''s execute the Kernel PCA in Python. We will keep this simple and
    work on the Iris dataset. We will also see how we can utilize the new compressed
    dimension in the model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Python中执行核PCA。我们将保持简单，并使用Iris数据集进行操作。我们还将看看我们如何利用模型中的新压缩维度：
- en: 'Let''s load the libraries:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载库：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, load the data and create separate objects for the explanatory and target
    variables:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，加载数据并创建解释变量和目标变量的单独对象：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s have a look at the explanatory data:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看解释数据：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/350741b5-b4ee-47cd-b509-36a0b61ff7a0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/350741b5-b4ee-47cd-b509-36a0b61ff7a0.png)'
- en: 'Let''s split the data into train and test sets, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将数据分为训练集和测试集，如下所示：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can standardize the data:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以标准化数据：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s have a look at `X_train`:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看`X_train`：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/6b2e88a9-d310-4d32-a791-ccd5fcec18fa.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6b2e88a9-d310-4d32-a791-ccd5fcec18fa.png)'
- en: 'Now, let''s apply the kernel PCA on this. Here, we are trying to condense the
    data into just two components. The kernel that''s been chosen here is the radial
    basis function:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将核 PCA 应用到此。这里，我们试图将数据压缩成仅两个成分。这里选择的核心是径向基函数：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We have got the new train and test data with the help of the kernel PCA.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们借助核 PCA 获得了新的训练和测试数据。
- en: 'Let''s see what the data looks like:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看数据看起来像什么：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the following as output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/f108b9c3-cb99-4b75-9b9e-d87476ddfae8.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f108b9c3-cb99-4b75-9b9e-d87476ddfae8.png)'
- en: Now, we've got two components here. Earlier, `X_train` showed us four variables.
    Now, the data has been shrunk into two fields.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们这里有两个成分。之前，`X_train` 展示了四个变量。现在，数据已经被缩减成两个字段。
- en: Independent component analysis
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: '**Independent component analysis** (**ICA**) is similar to PCA in terms of
    dimensionality reduction. However, it originated from the signal processing world
    wherein they had this problem that multiple signals were being transmitted from
    a number of sources, and there were a number of devices set up to capture it.
    However, the problem was that the captured signal by the device was not very clear
    as it happened to be a mix of a number of sources. They needed to have clear and
    independent reception for the different devices that gave birth to ICA. Heralt
    and Jutten came up with this in.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**独立成分分析** (**ICA**) 在降维方面与 PCA 类似。然而，它起源于信号处理领域，在那里他们面临这样一个问题，即多个信号从多个来源传输，并设置了多个设备来捕获它。然而，问题是设备捕获的信号并不清晰，因为它碰巧是多个来源的混合。他们需要为产生
    ICA 的不同设备提供清晰和独立的接收。Heralt 和 Jutten 提出了这个概念。'
- en: The difference between PCA and ICA is that PCA focuses upon finding uncorrelated
    factors, whereas ICA is all about deriving independent factors. Confused? Let
    me help you. Uncorrelated factors imply that there is no linear relationship between
    them, whereas independence means that two factors have got no bearing on each
    other. For example, scoring good marks in mathematics is independent of which
    state you live in.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 和 ICA 的区别在于，PCA 侧重于寻找不相关的因子，而 ICA 则是关于推导独立因子。困惑吗？让我来帮你。不相关的因子意味着它们之间没有线性关系，而独立性意味着两个因子之间没有任何影响。例如，数学得高分与你在哪个州生活无关。
- en: An underlying assumption for this algorithm is that the variables are linear
    mixtures of unknown latent and independent variables.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的一个基本假设是变量是未知潜在变量和独立变量的线性组合。
- en: 'The data *x[i ](t)* is modeled using hidden variables *s[i](t)*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据 *x[i ](t)* 使用隐藏变量 *s[i](t)* 进行建模：
- en: '![](img/78e5eed1-8082-4bd9-b049-53969f9be9df.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78e5eed1-8082-4bd9-b049-53969f9be9df.png)'
- en: Here, *i= 1,2,3..........n.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， *i= 1,2,3..........n.*
- en: 'It can also be written in the form of matrix decomposition as **x=As**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 它也可以写成矩阵分解的形式作为 **x=As**：
- en: '![](img/285cf751-6a6b-4b1d-854b-6a4aa1d20554.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/285cf751-6a6b-4b1d-854b-6a4aa1d20554.png)'
- en: 'Here, we have the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '**A**: Constant mixing matrix'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A**：常数混合矩阵'
- en: '**s**: Latent factor matrices, which are independent of each other'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**s**：相互独立的潜在因子矩阵'
- en: We have to estimate the values of both **A** and **s** while we have got **X**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有 **X** 时，我们必须估计 **A** 和 **s** 的值。
- en: In other words, our goal is to find *W*, which is *W= A*^(*-1*), which is an
    unmixing matrix.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们的目标是找到 *W*，它是 *W= A*^(*-1*)，这是一个去混矩阵。
- en: Here, *s[ij] *has to be statistically independent of and non-Gaussian (not following
    normal distribution).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， *s[ij] *必须与统计独立且非高斯（不遵循正态分布）。
- en: Preprocessing for ICA
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ICA 预处理
- en: 'The preprocessing of ICA can be done as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 的预处理可以按以下方式进行：
- en: '**Centering**: The first step is to center *x*. That is, we need to subtract
    its mean vector from *x* so as to make *x* a zero-mean variable.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中心化**：第一步是使 *x* 中心化。也就是说，我们需要从 *x* 中减去其均值向量，以便使 *x* 成为零均值变量。'
- en: '**Whitening**: Before putting the data through ICA, we are supposed to whiten
    the data. This means that the data has to be uncorrelated. Geometrically speaking,
    it tends to restore the initial shape of the data and only the resultant matrix
    needs to be rotated.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**白化**：在将数据通过 ICA 之前，我们应当对数据进行白化。这意味着数据必须是相互独立的。从几何上讲，它倾向于恢复数据的初始形状，并且只需要旋转结果矩阵。'
- en: Approach
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: To find out what unmixing matrices are independent, we have to bank upon non-Gaussianity.
    Let's see how we can do this.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出哪些去混矩阵是独立的，我们必须依赖于非高斯性。让我们看看我们如何做到这一点。
- en: 'Here, we will need to maximize the kurtosis, which will turn the distribution
    into a non-Gaussian. This will result in independent components. The following
    diagram shows an image of fast ICA:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要最大化峰度，这将使分布变成非高斯分布。这将导致独立成分。以下图显示了快速 ICA 的图像：
- en: '![](img/490fddd1-61b4-48b2-92fd-f42080453314.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/490fddd1-61b4-48b2-92fd-f42080453314.png)'
- en: For this, we have the `FastICA` library in Python.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一点，Python 中有 `FastICA` 库。
- en: 'Let''s look at how we can execute this in Python. We will work with the same
    Iris data. This might not be an ideal dataset for executing ICA, but this is being
    done for directional purposes. To execute the code in Python, we will need to
    perform the following steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Python 中执行这个操作。我们将使用相同的数据集 Iris。这可能不是一个执行 ICA 的理想数据集，但这是为了方向性的目的。要在
    Python 中执行代码，我们需要执行以下步骤：
- en: 'First, we need to load the library:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要加载库：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we need to load the data:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要加载数据：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s partition the data into train and test sets:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们把数据分成训练集和测试集：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s make the data a standard scalar:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们把数据转换成标准标量：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we need to load in the ICA library:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要加载 ICA 库：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We carry out ICA as follows. We will stick to three components here:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将按照以下方式进行 ICA。在这里我们将坚持三个成分：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will then plot the results, as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将绘制结果，如下所示：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output for this is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出如下：
- en: '![](img/f973e18d-de80-4e93-9c42-6f8e91373c7f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f973e18d-de80-4e93-9c42-6f8e91373c7f.png)'
- en: We can see the three different components here (by color).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这里有三个不同的成分（通过颜色）。
- en: Compressed sensing
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩感知
- en: 'Compressed sensing is one of the easiest problems to solve in the area of information
    theory and signal processing. It is a signal acquisition and reconstruction technique
    where the signal is compressible. The signal must be sparse. Compressed sensing
    tries to fit samples of a signal to functions, and it has a preference to use
    as few basic functions as possible to match the samples. This is described in
    the following diagram:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩感知是信息理论和信号处理领域中容易解决的问题之一。它是一种信号采集和重建技术，其中信号是可压缩的。信号必须是稀疏的。压缩感知试图将信号的样本拟合到函数中，并且它倾向于使用尽可能少的基本函数来匹配样本。这将在以下图中描述：
- en: '![](img/e2564922-b0cf-4d3d-8b76-7214e7f225be.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2564922-b0cf-4d3d-8b76-7214e7f225be.png)'
- en: This is one of the prime equations that we see in linear algebra, where **y**
    is a **M x 1** matrix, phi is a **M x N** matrix that has got a number of columns
    that is higher than the number of rows, and **x** is a **N x 1** matrix comprising
    **k** non-zero entries. There are so many unknowns, which is expressed as an **N**
    length vector and **M** measurements, wherein **M << N**. In this type of equation,
    we know that many solutions are possible since the null space of this matrix is
    non-trivial. Hence, this equation can accommodate many solutions.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在线性代数中看到的主要方程之一，其中 **y** 是一个 **M x 1** 矩阵，phi 是一个 **M x N** 矩阵，其列数多于行数，而
    **x** 是一个由 **k** 个非零项组成的 **N x 1** 矩阵。未知数非常多，这可以用一个 **N** 长度的向量表示，以及 **M** 个测量值，其中
    **M << N**。在这种类型的方程中，我们知道由于这个矩阵的零空间非平凡，所以可能存在许多解。因此，这个方程可以容纳许多解。
- en: Our goal
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的目标
- en: 'Our goal is to find out the solution with a least possible non-zero entry of
    all of the solutions. That is, the solution should give us as few non-zeros as
    possible. Are you wondering where this can be applied? There are plenty of applications
    for it. The areas where it can be applied are as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到所有解中非零项最少的解。也就是说，解应该给我们尽可能少的非零项。你在想这能应用在哪里吗？有很多应用场景。它可以应用在以下领域：
- en: Signal representation
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号表示
- en: Medical imaging
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医学成像
- en: Sparse channel estimation
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏信道估计
- en: Let's say that we have got a time signal. This signal is highly sparse, but
    we know a little bit about it as it has a few frequencies. Can you sense what
    it is from the earlier equation? Yes, it can be deemed as *X*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个时间信号。这个信号非常稀疏，但我们对其有一些了解，因为它有几个频率。你能从之前的方程中感受到它是什么吗？是的，它可以被认为是 *X*。
- en: 'Let''s call this **unknown** signal *X*. Now, even though we don''t know the
    whole signal, we can still make observations about it, or samples, as shown in
    the following code:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称这个**未知**信号为 *X*。现在，尽管我们不知道整个信号，我们仍然可以对其做出观察，或者说是样本，如下面的代码所示：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will form a random equation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将形成一个随机方程：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we need to fit the `l1` norm. We get the following output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要拟合 `l1` 范数。我们得到以下输出：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we need to fit the `l2` norm. We get the following output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要拟合 `l2` 范数。我们得到以下输出：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'By summing up the two sinusoids, we get the following output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将两个正弦波相加，我们得到以下输出：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let''s take the sample out of `n`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从`n`中取出一个样本：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s create the `idct` matrix operator:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`idct`矩阵运算符：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output for this is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出的结果如下：
- en: '![](img/ba0f4f4d-2003-4acb-b05f-88f6d596e91a.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba0f4f4d-2003-4acb-b05f-88f6d596e91a.png)'
- en: 'To reconstruct the signal, we must do the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重构信号，我们必须执行以下操作：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: That is how we reconstruct the signal.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的信号重构方式。
- en: Self-organizing maps
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自组织映射
- en: '**Self-organizing maps** (**SOM**) were invented by Teuvo Kohonen in the 1980s.
    Sometimes, they are known as **Kohonen maps**. So, why do they exist? The prime
    motive for these kind of maps is to reduce dimensionality through a neural network.
    The following diagram shows the different 2D patterns from the input layers:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**自组织映射**（**SOM**）是由Teuvo Kohonen在20世纪80年代发明的。有时，它们也被称为**Kohonen映射**。那么，它们为什么存在呢？这类映射的主要动机是通过神经网络来降低维度。以下图显示了输入层的不同2D模式：'
- en: '![](img/45326737-dbab-470b-8d50-9d6a959d22bb.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/45326737-dbab-470b-8d50-9d6a959d22bb.png)'
- en: They take the number of columns as input. As we can see from the 2D output,
    it transforms and reduces the amount of columns in the dataset into 2D.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 它以列数作为输入。正如我们从2D输出中可以看到的，它将数据集中的列数转换并减少到2D。
- en: The following link leads to the the 2D output: [https://www.cs.hmc.edu/~kpang/nn/som.html](https://www.cs.hmc.edu/~kpang/nn/som.html)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接指向2D输出：[https://www.cs.hmc.edu/~kpang/nn/som.html](https://www.cs.hmc.edu/~kpang/nn/som.html)
- en: The depiction of the preceding diagram in 2D talks about a health of the country
    based on various factors. That is, it shows whether they are rich or poor. Some
    other factors that are taken into account are education, quality of life, sanitation,
    inflation, and health. Therefore, it forms a huge set of columns or dimensions.
    Countries such as Belgium and Sweden seem to show similar traits, depicting that
    they have got a good score on the health indicator.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图的2D表示是根据各种因素来描述一个国家的健康状况。也就是说，它显示了它们是富裕还是贫穷。考虑的其他因素包括教育、生活质量、卫生、通货膨胀和健康。因此，它形成了一个巨大的列集或维度集。比利时和瑞典等国家似乎显示出相似的特性，表明它们在健康指标上得分很高。
- en: Since this is an unsupervised learning technique, the data wasn't labeled. Based
    on patterns alone, the neural network is able to understand which country should
    be placed where.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一种无监督学习技术，数据没有被标记。仅基于模式，神经网络能够理解哪个国家应该放在哪里。
- en: Similar to the situation we just covered, opportunities are aplenty where self-organizing
    maps can be utilized. It can be thought as being similar in nature to K-means
    clustering.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们刚才讨论的情况类似，自组织映射有很多可以利用的机会。它可以被认为是与K-means聚类在本质上相似。
- en: SOM
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SOM
- en: 'Let''s go through process of how SOMs learn:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看SOM（自组织映射）是如何学习的：
- en: Each node's weights are initialized by small standardized random values. These
    act like coordinates for different output nodes.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个节点的权重都通过小的标准化随机值初始化。这些值类似于不同输出节点的坐标。
- en: The first row's input (taking the first row from all of the variables) is fed
    into the first node.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一行的输入（从所有变量中取第一行）被输入到第一个节点。
- en: 'Now, we have got two vectors. If *V* is the current input vector and *W* is
    the node''s weight vector, then we calculate the Euclidean distance, like so:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有两个向量。如果*V*是当前输入向量，而*W*是节点的权重向量，那么我们计算欧几里得距离，如下所示：
- en: '![](img/14175655-4cfa-4bfe-bfa3-fb1b6980397d.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14175655-4cfa-4bfe-bfa3-fb1b6980397d.png)'
- en: The node that has a weight vector closest to the input vector is tagged as the
    **best-matching unit** (**BMU**).
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重向量与输入向量最接近的节点被标记为**最佳匹配单元**（**BMU**）。
- en: A similar operation is carried out for all the rows of input and weight vectors.
    BMUs are found for all.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入和权重向量的所有行都执行类似的操作。为所有找到BMU。
- en: 'Once the BMU has been determined for every iteration, the other nodes within
    the BMU''s neighborhood are computed. Nodes within the same radius will have their
    weights updated. A green arrow indicates the radius. Slowly, the neighborhood will
    shrink to the size of just one node, as shown in the following diagram:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦确定了每个迭代的BMU（最佳匹配单元），BMU邻域内的其他节点将被计算。同一半径内的节点将更新其权重。绿色箭头表示半径。随着时间的推移，邻域将逐渐缩小到仅剩一个节点的大小，如下面的图所示：
- en: '![](img/c16e2611-8dcc-4f93-85bd-f848c0c03343.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c16e2611-8dcc-4f93-85bd-f848c0c03343.png)'
- en: 'The most interesting part of the Kohonen algorithm is that the radius of the
    neighborhood keeps on shrinking. It takes place through the exponential decay
    function. The value of lambda is dependent on sigma. The number of iterations
    that have been chosen for the algorithm to run is given by the following equation:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kohonen算法最有趣的部分是邻域的半径不断缩小。这是通过指数衰减函数实现的。lambda的值依赖于sigma。算法运行所选择的迭代次数由以下方程给出：
- en: '![](img/521300be-39b7-464b-b6cf-54de2bbe33e7.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/521300be-39b7-464b-b6cf-54de2bbe33e7.png)'
- en: '![](img/19c08a6b-1031-49ce-8524-5cc78e877c20.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19c08a6b-1031-49ce-8524-5cc78e877c20.png)'
- en: 'The weights get updated via the following equation:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重通过以下方程更新：
- en: '![](img/a0e055c8-766e-4e64-8f0a-63c0da661b15.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a0e055c8-766e-4e64-8f0a-63c0da661b15.png)'
- en: 'Here, this is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，这是如下：
- en: '![](img/97390ac7-2127-4b75-abf3-b28943ae5e8e.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/97390ac7-2127-4b75-abf3-b28943ae5e8e.png)'
- en: '*t= 1, 2...* can be explained as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*t= 1, 2...* 可以解释如下：'
- en: '*L(t)*: Learning rate'
  id: totrans-172
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L(t)*：学习率'
- en: '*D*: Distance of a node from BMU'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D*：节点到BMU的距离'
- en: '*σ*: Width of the function'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ*：函数的宽度'
- en: 'Now, let''s carry out one use case of this in Python. We will try to detect
    fraud in a credit card dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Python中演示这个用例。我们将尝试检测信用卡数据集中的欺诈行为：
- en: 'Let''s load the libraries:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载库：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, it''s time to load the data:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候加载数据了：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we will standardize the data:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将标准化数据：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s import the `minisom` library and key in the hyperparameters, that is,
    learning rate, sigma, length, and number of iterations:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入`minisom`库并输入超参数，即学习率、sigma、长度和迭代次数：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s visualize the results:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化结果：
- en: '[PRE27]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following output will be generated from the preceding code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出将由前面的代码生成：
- en: '![](img/c7c81ab6-292a-4327-94f0-8311d8093be0.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c7c81ab6-292a-4327-94f0-8311d8093be0.png)'
- en: 'We can see that the nodes that have a propensity toward fraud have got white
    backgrounds. This means that we can track down those customers with the help of
    those nodes:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，有欺诈倾向的节点具有白色背景。这意味着我们可以通过这些节点追踪到那些客户：
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This will give you the pattern of frauds.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出欺诈的模式。
- en: Bayesian multiple imputation
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯多重插补
- en: Bayesian multiple imputation has got the spirit of the Bayesian framework. It
    is required to specify a parametric model for the complete data and a prior distribution
    over unknown model parameters, *θ*. Subsequently, *m* independent trials are drawn
    from the missing data, as given by the observed data using Bayes' Theorem. Markov
    Chain Monte Carlo can be used to simulate the entire joint posterior distribution
    of the missing data. BMI follows a normal distribution while generating imputations
    for the missing values.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯多重插补具有贝叶斯框架的精神。需要指定完整数据的参数模型以及未知模型参数*θ*的先验分布。随后，从缺失数据中抽取*m*个独立的试验，如使用贝叶斯定理根据观察数据给出。可以使用马尔可夫链蒙特卡洛模拟缺失数据的整个联合后验分布。BMI在生成缺失值的插补时遵循正态分布。
- en: 'Let''s say that the data is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据如下：
- en: '*Y = (Yobs, Ymiss),*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y = (Yobs, Ymiss),*'
- en: Here, *Yobs* is the observed *Y* and *Ymiss* is the missing *Y.*
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Yobs*是观察到的*Y*，而*Ymiss*是缺失的*Y*。
- en: 'If *P(Y|θ)* is the parametric model, the parameter *θ* is the mean and the
    covariance matrix that parameterizes a normal distribution. If this is the case,
    let *P(θ)* be the prior:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*P(Y|θ)*是参数模型，参数*θ*是正态分布的均值和协方差矩阵。如果是这种情况，让*P(θ)*为先验：
- en: '![](img/432ba803-d702-4821-9978-219b8a355fa9.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/432ba803-d702-4821-9978-219b8a355fa9.png)'
- en: 'Let''s make use of the `Amelia` package in R and execute this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用R中的`Amelia`包并执行以下操作：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, let''s make the imputation:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行插补：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have studied the Kernel PCA, along with ICA. We also studied
    compressed sensing, the goals of compressed sensing, and self-organizing maps
    and how they work. Finally, we concluded with Bayesian multiple imputations.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了核主成分分析（Kernel PCA），以及独立成分分析（ICA）。我们还研究了压缩感知、压缩感知的目标以及自组织映射及其工作原理。最后，我们以贝叶斯多重插补作为结论。
