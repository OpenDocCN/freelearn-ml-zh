- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Distributed Training of Machine Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式训练机器学习模型
- en: When it comes to **Machine Learning** (**ML**) model training, the primary goal
    for a data scientist or ML practitioner is to train the optimal model based on
    the relevant data to address the business use case. While this goal is of primary
    importance, the panacea is to perform this task as quickly and effectively as
    possible. So, *how do we speed up model training?* Moreover, sometimes, the data
    or the model might be too big to fit into a single GPU memory. *So how do we prevent
    out-of-memory (**OOM) errors?*
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到 **机器学习**（**ML**）模型训练时，数据科学家或 ML 实践者的主要目标是根据相关数据训练出最优模型，以解决业务用例。虽然这个目标是首要的，但最佳方案是以尽可能快和有效的方式完成这项任务。那么，*我们如何加快模型训练速度呢？*
    此外，有时数据或模型可能太大，无法适应单个 GPU 内存。*那么我们如何防止内存不足（**OOM**）错误呢？
- en: The simplest answer to this question is to basically throw more compute resources,
    in other words, more CPUs and GPUs, at the problem. This is essentially using
    larger compute hardware and is commonly referred to as a **scale-up** strategy.
    However, there is only a finite number of CPUs and GPUs that can be squeezed into
    a server. So, sometimes a **scale-out** strategy is required, whereby we add more
    servers into the mix, essentially distributing the workload across multiple physical
    compute resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的最简单答案就是向问题投入更多的计算资源，换句话说，更多的 CPU 和 GPU。这本质上是在使用更大的计算硬件，通常被称为 **扩展** 策略。然而，服务器中可以挤入的
    CPU 和 GPU 数量是有限的。因此，有时需要 **扩展** 策略，即向混合中添加更多服务器，本质上是在多个物理计算资源之间分配工作负载。
- en: 'Nonetheless, spreading the model training workload across more CPUs or GPUs,
    and even across more compute servers, will definitely speed up the overall training
    process. Making use of either a scale-up, scale-out, or a combination of the two
    strategies also adds further complexity to the overall orchestration and configuration
    of the model training activity. Therefore, this chapter will help navigate these
    challenges to help overcome the additional complexities imposed by the **distributed
    training** process by covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，将模型训练工作负载分散到更多的 CPU 或 GPU，甚至分散到更多的计算服务器上，肯定会加快整体训练过程。利用扩展、扩展或两种策略的组合也会给模型训练活动的整体编排和配置增加更多的复杂性。因此，本章将帮助您应对这些挑战，通过涵盖以下主题来帮助克服分布式训练过程带来的额外复杂性：
- en: Building ML systems using AWS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS 构建 ML 系统
- en: Introducing the fundamentals of distributed training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍分布式训练的基本原理
- en: Executing a distributed training workload on AWS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS 上执行分布式训练工作负载
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following prerequisites before getting started with this
    chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，您应该具备以下先决条件：
- en: A web browser (for the best experience, it is recommended that you use a Chrome
    or Firefox browser)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络浏览器（为了获得最佳体验，建议您使用 Chrome 或 Firefox 浏览器）
- en: Access to the AWS account that you used in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问您在 [*第 5 章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析* 中使用的 AWS 账户
- en: An AWS account (if you are unfamiliar with how to get started with an AWS account,
    you can go to this link [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/))
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 账户（如果您不熟悉如何开始使用 AWS 账户，您可以访问此链接 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)）
- en: Access to the SageMaker Studio development environment that we created in [*Chapter
    5*](B18493_05.xhtml#_idTextAnchor095), *Data Analysis*
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问我们在 [*第 5 章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析* 中创建的 SageMaker Studio
    开发环境
- en: Example Jupyter notebooks for this chapter are provided in the companion GitHub
    repository ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的示例 Jupyter 笔记本提供在配套 GitHub 仓库中 ([https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06](https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06)))
- en: Building ML systems using AWS
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS 构建 ML 系统
- en: Before we can explore the fundamentals of how to implement the distributed training
    strategies highlighted at the outset, we first need to level set and understand
    just how the ML model training exercise can be performed on the AWS platform.
    Once we understand how AWS handles model training, we can further expand on this
    concept to address the concept of distributed training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索如何实现一开始就强调的分布式训练策略的基本原理之前，我们首先需要统一认识，了解如何在AWS平台上执行ML模型训练练习。一旦我们了解了AWS如何处理模型训练，我们就可以进一步扩展这个概念来讨论分布式训练的概念。
- en: 'To assist ML practitioners in building ML systems, AWS provides the SageMaker
    ([https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/)) service.
    While SageMaker is a single AWS service, it comprises multiple modules that map
    specifically to an ML task. For example, SageMaker provides the Training job component
    that is purpose-built to take care of the heavy lifting and scaling of the model
    training task. ML practitioners can use SageMaker Training jobs to essentially
    provision ephemeral compute environments or clusters to handle the model training
    task. Essentially, all the ML practitioner needs to do is specify a few configuration
    parameters, and SageMaker Training jobs takes care of the rest. For example, we
    need to supply the following four basic parameters:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助ML从业者构建ML系统，AWS提供了SageMaker ([https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/))
    服务。虽然SageMaker是一个单一的AWS服务，但它由多个模块组成，这些模块专门对应于ML任务。例如，SageMaker提供了专门用于处理模型训练任务的重负载和扩展的训练作业组件。ML从业者可以使用SageMaker训练作业来基本提供临时的计算环境或集群来处理模型训练任务。本质上，ML从业者需要做的只是指定一些配置参数，SageMaker训练作业就会处理其余的工作。例如，我们需要提供以下四个基本参数：
- en: The URL for the S3 bucket, which contains the model training, testing, and optionally,
    the validation data
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含模型训练、测试，以及可选的验证数据的S3存储桶的URL。
- en: The type and quantity of ML compute instances required to perform the model
    training task
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行模型训练任务所需的ML计算实例的类型和数量。
- en: The location of the S3 bucket to store the trained model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储训练好的模型的S3存储桶的位置。
- en: The location, either locally or on S3, where the model training code is stored
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储模型训练代码的位置，无论是本地还是S3。
- en: 'The following code snippet shows just how easy it can be to formalize these
    four basic requirements into a SageMaker Training job request:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何将这些四个基本要求正式化成一个SageMaker训练作业请求是多么简单：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using this code snippet, we basically tell SageMaker that we want to use the
    built-in PyTorch estimator by declaring the `estimator` variable to use the PyTorch
    framework. We then supply the necessary requirements, such as the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个代码片段，我们基本上告诉SageMaker我们想要使用内置的PyTorch估计器，通过声明使用PyTorch框架的`estimator`变量。然后，我们提供必要的要求，例如以下内容：
- en: '`entry_point`: This is the location of the training script.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entry_point`：这是训练脚本的存储位置。'
- en: '`instance_count`: This is the number of compute servers to be provisioned in
    the cluster.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_count`：这是集群中要配置的计算服务器数量。'
- en: '`instance_type`: This is the type of compute resources required in the cluster.
    In this example, we are specifying the `ml.p3.16xlarge` instances.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`instance_type`：这是集群中所需的计算资源类型。在这个例子中，我们指定了`ml.p3.16xlarge`实例。'
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the SageMaker PyTorch estimator, as well as how to leverage
    the SageMaker SDK to instantiate the estimator, see the AWS documentation on how
    to use PyTorch on SageMaker ([https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html](https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html))
    and the SageMaker SDK documentation (https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html#pytorch-estimator).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SageMaker PyTorch估计器的更多信息，以及如何利用SageMaker SDK实例化估计器，请参阅AWS关于如何在SageMaker上使用PyTorch的文档（[https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html](https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html)）和SageMaker
    SDK文档（https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html#pytorch-estimator）。
- en: 'Once we have declared the estimator, we specify the location of the training
    and validation datasets on S3, as shown in the following code snippet:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们声明了估计器，我们就在S3上指定训练和验证数据集的位置，如下代码片段所示：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then call the `fit()` method of the PyTorch estimator to tell SageMaker
    to execute the Training job on the datasets, as shown in the following code snippet:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用PyTorch估计器的`fit()`方法，告诉SageMaker在数据集上执行训练作业，如下代码片段所示：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Behind the scenes, SageMaker creates an ephemeral compute cluster, executes
    the training task on these resources, and then produces the resultant optimized
    model, which is then stored on Amazon S3\. After this task has been performed,
    SageMaker tears down the ephemeral cluster with users only paying for the resources
    consumed during the training time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，SageMaker创建了一个短暂的计算集群，在这些资源上执行训练任务，然后生成最终的优化模型，该模型随后存储在Amazon S3上。在此任务完成后，SageMaker将拆除短暂的集群，用户只需为训练时间消耗的资源付费。
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more detailed information as to how SageMaker Training jobs work behind
    the scenes, see the AWS documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SageMaker训练作业在幕后如何工作的更详细信息，请参阅AWS文档（[https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)）。
- en: So now that we have a basic idea of how a model training exercise can be performed
    using Amazon SageMaker, *how can we improve on model training time and essentially
    speed up the process by leveraging more* *compute resources?*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经对如何使用Amazon SageMaker执行模型训练练习有了基本的了解，*那么我们如何改进模型训练时间，并通过利用更多计算资源来加快这个过程呢？*
- en: To answer this question, we can very easily implement a scale-up strategy with
    the SageMaker Training job. All we have to do is change the `instance_type` parameter
    for the `estimator` variable from `ml.p3.2xlarge` to `ml.p3.16xlarge`. By doing
    this, we are increasing the size of, or scaling up the compute resource from,
    an instance with 8 vCPUs, 61 GB of RAM, and a single GPU to an instance with 64
    vCPUs, 488 GB of RAM, and 8 GPUs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们可以非常容易地通过SageMaker训练作业实现一个扩展策略。我们只需要将`estimator`变量的`instance_type`参数从`ml.p3.2xlarge`更改为`ml.p3.16xlarge`。通过这样做，我们正在增加或扩展计算资源的大小，从一个具有8个vCPU、61GB
    RAM和单个GPU的实例扩展到一个具有64个vCPU、488GB RAM和8个GPU的实例。
- en: 'The resultant code now looks as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的代码现在如下所示：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, as you can see, implementing a scale-up strategy is very straightforward
    when using SageMaker. However, *what if we need to go beyond the maximum capacity
    of an accelerated* *computing instance?*
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如您所见，使用SageMaker实施扩展策略非常简单。然而，*如果我们需要超出加速计算实例的最大容量怎么办？*
- en: Well, then, we would need to implement a scale-out strategy and distribute the
    training process across multiple compute nodes. In the next section, we will explore
    how to apply a scale-out strategy using distributed training for SageMaker Training
    jobs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们就需要实施一个扩展策略，并将训练过程分散到多个计算节点上。在下一节中，我们将探讨如何使用分布式训练来应用扩展策略于SageMaker训练作业。
- en: Introducing the fundamentals of distributed training
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍分布式训练的基本原理
- en: 'In the previous section, we highlighted how to apply a scale-up strategy to
    SageMaker Training jobs by simply specifying a large compute resource or large
    instance type. Implementing a scale-out strategy for the training process is just
    as straightforward. For example, we can increase the `instance_count` parameter
    for the Training job from `1` to `2` and thereby instruct SageMaker to instantiate
    an ephemeral cluster consisting of 2 compute resources as opposed to 1 node. Thus,
    the following code snippet highlights what the `estimator` variable configuration
    will look like:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们强调了如何通过指定大型计算资源或大型实例类型来应用扩展策略于SageMaker训练作业。实施训练过程的扩展策略同样简单。例如，我们可以将训练作业的`instance_count`参数从`1`增加到`2`，从而指示SageMaker实例化一个由2个计算资源组成的短暂集群，而不是1个节点。因此，以下代码片段突出了`estimator`变量配置将如何看起来：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Unfortunately, simply changing the number of compute instances doesn’t completely
    solve the problem. As already stated at the outset of this chapter, applying a
    scale-out strategy to distribute the SageMaker Training job adds further complexity
    to the overall orchestration and configuration of the model training activity.
    For example, when distributing the training activity, we need to also take into
    consideration the following aspects:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，仅仅更改计算实例的数量并不能完全解决问题。正如在本章开头所述，将扩展策略应用于分散SageMaker训练作业会增加模型训练活动的整体编排和配置的复杂性。例如，在分配训练活动时，我们还需要考虑以下方面：
- en: '*How do the various compute resources get access to and share* *the data?*'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何让各种计算资源访问和共享数据？*'
- en: '*How do the compute resources communicate and coordinate their training tasks
    with* *each other?*'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算资源之间是如何相互通信和协调它们的训练任务的？*'
- en: So, while simply specifying the number of compute resources for the Training
    job will create an appropriately sized training cluster, we also need to inform
    SageMaker about our **model placement strategy**. A model placement strategy directs
    SageMaker on exactly how the model is allocated or assigned to each of the compute
    resources within each node of the cluster. In turn, SageMaker uses the placement
    strategy to coordinate how each node interacts with the associated training data
    and, subsequently, how each node coordinates and communicates its portion of the
    model training task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虽然简单地指定训练作业的计算资源数量将创建一个适当规模的训练集群，但我们还需要通知SageMaker我们的**模型放置策略**。模型放置策略指导SageMaker如何将模型分配或分配给集群中每个节点的每个计算资源。反过来，SageMaker使用放置策略来协调每个节点如何与相关的训练数据互动，以及每个节点如何协调和沟通其模型训练任务的相应部分。
- en: '*So how do we determine an effective model placement strategy* *for SageMaker?*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*那么我们如何确定SageMaker的有效模型放置策略呢？*'
- en: The best way to answer this question is to understand what placement strategies
    are available and dissect how each of these strategies works. There are numerous
    placement strategies that are specific to each of the different training frameworks,
    as well as many open source frameworks. Nonetheless, all these different mechanisms
    can be grouped into two specific categories of placement strategies, namely **data
    parallel** and **model parallel**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这个问题的最佳方式是了解可用的放置策略，并剖析每种策略是如何工作的。针对不同的训练框架，以及许多开源框架，存在众多特定的放置策略。尽管如此，所有这些不同的机制都可以归纳为两种特定的放置策略类别，即**数据并行**和**模型并行**。
- en: Let’s explore SageMaker’s data parallel strategy first.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来探讨SageMaker的数据并行策略。
- en: Reviewing the SageMaker distributed data parallel strategy
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查SageMaker分布式数据并行策略
- en: As the name implies, a data parallel strategy focuses on the placement of model’s
    training data. So, in order to fully understand just how this placement strategy
    is applied to the data, we should start by understanding just how a training activity
    interacts with the training data to optimize an ML model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，数据并行策略侧重于模型训练数据的放置。因此，为了全面理解这种放置策略是如何应用于数据的，我们应该首先了解训练活动是如何与训练数据互动以优化机器学习模型的。
- en: When we train an ML model, we basically create a training loop that applies
    the specific ML algorithm to the data. Typically, as is the case with deep learning
    algorithms, we break the data into smaller groups of records or batches of data.
    These batches are referred to as **mini batches**. We then pass each of these
    mini batches forward through the neural network layers, and then backward to optimize
    or train the model parameters. After completing one mini batch, we then apply
    the same procedure to the next mini batch, and so on, until we’ve run through
    the entirety of the data. A full execution of this process on the entire dataset
    is referred to as an **epoch**. Depending on the type of algorithm and, of course,
    the use case, we may have to run the algorithm to train the model for multiple
    epochs. It’s this task that invariably takes the most time, and it’s this task
    that we essentially want to improve on to reduce the overall time it takes to
    train the model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个机器学习模型时，我们基本上创建了一个训练循环，该循环将特定的机器学习算法应用于数据。通常情况下，与深度学习算法一样，我们将数据分成更小的记录组或数据批次。这些批次被称为**小批量**。然后我们将每个小批量通过神经网络层向前传递，然后向后传递以优化或训练模型参数。完成一个小批量后，我们再将相同的程序应用于下一个小批量，依此类推，直到我们处理完所有数据。在整个数据集上完整执行此过程被称为一个**epoch**。根据算法类型和当然，用例，我们可能需要运行算法多次epoch来训练模型。这项任务无疑花费了最多的时间，这也是我们本质上想要改进以减少模型训练所需总时间的任务。
- en: So, when using a data parallel placement strategy, we are basically converting
    the training task from a sequential process to a parallel process. Instead of
    running the algorithm through a mini batch, then the next mini batch, then the
    next mini batch sequentially, we are now giving each individual mini batch to
    a separate compute resource, with each compute resource, in turn, running the
    model training process on its individual mini batch. Therefore, with each compute
    resource running its own mini batch at the same time, we are effectively distributing
    the epoch across multiple compute resources in parallel, therefore, improving
    the overall model training time. Consequently, using the data parallel technique
    does, however, introduce an additional complication, namely parallel optimization
    of all the weighted parameters for the model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当使用数据并行放置策略时，我们基本上是将训练任务从顺序过程转换为并行过程。我们不再按顺序运行算法通过一个迷你批次，然后是下一个迷你批次，然后是下一个迷你批次，我们现在将每个单独的迷你批次分配给一个单独的计算资源，每个计算资源依次在其单独的迷你批次上运行模型训练过程。因此，当每个计算资源同时运行其自己的迷你批次时，我们实际上是在并行地将纪元分布到多个计算资源中，从而提高了整体模型训练时间。因此，使用数据并行技术确实引入了一个额外的复杂性，即对所有模型加权参数的并行优化。
- en: 'To further elaborate on this problem, we’ll use the example depicted in *Figure
    6**.1*, detailing the individual node parameters for the model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步阐述这个问题，我们将使用 *图 6**.1* 中描述的示例，详细说明模型的单个节点参数：
- en: '![Figure 6.1 – Individual node parameters](img/B18493_06_001.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 单个节点参数](img/B18493_06_001.jpg)'
- en: Figure 6.1 – Individual node parameters
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 单个节点参数
- en: As you can see from the example in *Figure 6**.1*, we have four individual compute
    resources or nodes. Using the data parallel placement strategy, we have effectively
    placed a copy of the model algorithm onto each of these nodes and distributed
    the mini batch data across these resources. Now, each node computes the gradient
    reduction operation, in this case, the sum of weighted parameters of the model,
    on its individual mini batch of the data, in essence, producing four unique gradient
    calculation results. Since our goal is not to produce four separate representations
    of an optimized model but rather a single optimized model, *how do we combine
    the results across all* *four nodes?*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从 *图 6**.1* 中的示例中可以看到，我们有四个单独的计算资源或节点。使用数据并行放置策略，我们实际上在每个这些节点上放置了一个模型算法的副本，并将迷你批次数据分布到这些资源中。现在，每个节点在其数据单独的迷你批次上计算梯度减少操作，在这种情况下，是模型加权参数的总和，本质上产生了四个独特的梯度计算结果。由于我们的目标不是产生四个独立的优化模型表示，而是一个单一的优化模型，*那么我们如何将所有四个节点的结果组合起来呢？*
- en: 'To solve this problem, SageMaker provides an additional optimization operation
    to the distributed training process and uses the **AllReduce** algorithm to share
    and communicate the results across the cluster. By including this additional step
    in the process, we can see the outcome in *Figure 6**.2*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，SageMaker 在分布式训练过程中提供了一个额外的优化操作，并使用 **AllReduce** 算法在集群间共享和通信结果。通过在过程中包含这个额外的步骤，我们可以在
    *图 6**.2* 中看到结果：
- en: '![Figure 6.2 – Shared node parameters](img/B18493_06_002.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 共享节点参数](img/B18493_06_002.jpg)'
- en: Figure 6.2 – Shared node parameters
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 共享节点参数
- en: From *Figure 6**.2*, we can see that the AllReduce step takes the results from
    each node’s gradient reduction operation and shares the results with every other
    node, ensuring that each node’s representation of the model includes the optimizations
    from all the other nodes. This, therefore, guarantees that a single, consistent
    model is produced as the final output from the distributed training process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 6**.2* 中，我们可以看到 AllReduce 步骤从每个节点的梯度减少操作中获取结果，并将结果与每个其他节点共享，确保每个节点对模型的表示包括所有其他节点的优化。因此，这保证了从分布式训练过程中产生一个单一、一致的模式作为最终输出。
- en: Note
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While the initial concept of using the AllReduce step for distributed deep learning
    was initially introduced in a blog post by Baidu Research, the original post has
    since been removed. So, for more background information on the intricacies of
    how it works, you can review the open source implementation called **Horovod**
    ([https://eng.uber.com/horovod/](https://eng.uber.com/horovod/)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AllReduce 步骤用于分布式深度学习的初始概念最初是在百度研究的一个博客文章中提出的，但原始帖子已经被删除。因此，为了了解更多关于其工作细节的背景信息，你可以查看名为
    **Horovod** 的开源实现（[https://eng.uber.com/horovod/](https://eng.uber.com/horovod/))）。
- en: Up until this point in the chapter, we have used the broad term *compute resources*
    to denote CPUs, GPUs, and physical compute instances. However, when implementing
    a successful data parallel placement strategy, it’s important to fully understand
    just how SageMaker uses these compute resources to execute the distributed training
    workload.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章的这一部分，我们使用广泛的术语**计算资源**来表示CPU、GPU和物理计算实例。然而，在实施一个成功的数据并行放置策略时，完全理解SageMaker如何使用这些计算资源来执行分布式训练工作负载是非常重要的。
- en: 'Succinctly, when we instruct SageMaker to implement a data parallel placement
    strategy for the Training job, we are instructing SageMaker to distribute or shard
    the mini batches across all of the compute resources. SageMaker, in turn, shards
    the training data into all the GPUs and, from time to time, the CPUs on all of
    the instance types specified in the `estimator` object. So, in order to make this
    concept easier to understand, *Figure 6**.3* illustrates an example of just how
    SageMaker handles the task:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，当我们指示SageMaker为训练作业实施数据并行放置策略时，我们实际上是在指示SageMaker将微型批次分布或分片到所有的计算资源中。SageMaker反过来将训练数据分片到所有指定的`estimator`对象中的GPU上，以及偶尔的CPU上。为了使这个概念更容易理解，**图6.3**.3展示了SageMaker处理任务的示例：
- en: '![Figure 6.3 – Data parallel training task on SageMaker](img/B18493_06_003.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – SageMaker上的数据并行训练任务](img/B18493_06_003.jpg)'
- en: Figure 6.3 – Data parallel training task on SageMaker
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – SageMaker上的数据并行训练任务
- en: As you can see from the example shown in *Figure 6**.3* when calling the `fit()`
    method for the SageMaker `estimator` object, specifying two GPU instances (each
    with eight GPUs), SageMaker creates a copy of the model training routine, or **training
    script**, on both of the instances in the ephemeral cluster. Accordingly, each
    training script is further copied onto each GPU on each of the instances. Once
    every GPU has a copy of the training script, each GPU, in turn, executes the training
    script on its individually sharded mini batch of the training data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从**图6.3**.3中所示，当为SageMaker的`estimator`对象调用`fit()`方法时，指定两个GPU实例（每个实例有八个GPU），SageMaker在临时集群中的两个实例上创建模型训练例程或**训练脚本**的副本。相应地，每个训练脚本被进一步复制到每个实例上的每个GPU上。一旦每个GPU都有一个训练脚本的副本，每个GPU随后在其各自分片的训练数据微型批次上执行训练脚本。
- en: The GPU worker then trains the model copy to produce a set of optimal parameters,
    which then shares with the other GPU workers, both within the same instance, as
    well as the other GPUs in the second instance, using the `fit()` operation is
    reported as being successful. The result is a single optimized model, stored on
    S3, and a reduction of the overall model training time, by a factor of 16, which
    is the total number of GPUs allocated to the task.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GPU工作进程随后训练模型副本以生成一组最佳参数，然后与其他GPU工作进程共享，无论是同一实例内的，还是第二个实例中的其他GPU，使用`fit()`操作报告成功。结果是单个优化模型，存储在S3上，并且整体模型训练时间减少了16倍，这是分配给任务的GPU总数。
- en: So, as you can see, we have effectively reduced the overall training time by
    implementing a data parallel placement strategy. While this strategy is effective
    for large training datasets and is a good start at reducing the time it takes
    to train a model, this strategy, however, doesn’t always work when we have large
    models to train.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如您所见，我们通过实施数据并行放置策略有效地减少了整体训练时间。虽然这种策略对于大型训练数据集有效，并且是减少模型训练时间的良好开端，但这种策略在训练大型模型时并不总是有效。
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since the model parallel placement strategy is essentially distributing the
    model’s computational graph or model pipeline across multiple nodes, this placement
    strategy is often referred to as a **pipeline** **parallel** strategy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型并行放置策略本质上是在多个节点上分布模型的计算图或模型流水线，这种放置策略通常被称为**管道** **并行**策略。
- en: To address the challenge of reducing the overall training time when we have
    large ML models with millions or even billions of trainable parameters, we can
    review how to implement a model parallel placement strategy.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决当我们有数百万甚至数十亿可训练参数的大型ML模型时，如何减少整体训练时间的问题，我们可以回顾如何实施模型并行放置策略。
- en: Reviewing the SageMaker model data parallel strategy
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查SageMaker模型数据并行策略
- en: The data parallel strategy was largely conceived as a method of reducing the
    overall model training time, where at the time of its induction, training on large
    quantities of data imposed the biggest challenge. However, with the invention
    of large-scale **natural language processing** (**NLP**) models, such as **Generative
    Pre-trained Transformers** (**GPT**) from OpenAI ([https://openai.com/blog/gpt-3-apps/](https://openai.com/blog/gpt-3-apps/)),
    training a large ML model will billions of parameters now imposes the biggest
    challenge. Basically, these models are far too large to fit into the GPU’s onboard
    memory.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行策略主要被构想为一种减少整体模型训练时间的方法，在引入该策略时，在大数据量上进行训练提出了最大的挑战。然而，随着大规模**自然语言处理**（**NLP**）模型，如OpenAI的**生成预训练转换器**（**GPT**）[https://openai.com/blog/gpt-3-apps/](https://openai.com/blog/gpt-3-apps/)的发明，现在训练具有数十亿参数的大型ML模型提出了最大的挑战。基本上，这些模型太大，无法适应GPU的板载内存。
- en: Now that we have a rudimentary idea of just how SageMaker implements a data
    parallel placement strategy, it’s relatively easy to translate the concept to
    a model parallel placement strategy. The key difference is that while a data parallel
    strategy breaks the large quantity of training data into smaller shards, the model
    parallel placement strategy performs a similar trick to a large ML model, allowing
    these smaller pieces of the model to fit into GPU memory. This also means that
    we don’t have to degrade the model’s capabilities by having to prune or compress
    it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对SageMaker如何实现数据并行放置策略有了基本的了解，将其概念转换为模型并行放置策略相对容易。关键区别在于，虽然数据并行策略将大量训练数据分割成更小的数据块，但模型并行放置策略对大型ML模型执行类似的技巧，使得这些模型的小部分能够适应GPU内存。这也意味着我们不需要通过修剪或压缩模型来降低模型的能力。
- en: '*Figure 6**.4* highlights just how similar the model parallel execution is
    to a data parallel execution when SageMaker executes a Training job, using a model
    parallel placement strategy:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.4*突出了当SageMaker使用模型并行放置策略执行训练作业时，模型并行执行与数据并行执行是多么相似：'
- en: '![Figure 6.4 – Model parallel training task on SageMaker](img/B18493_06_004.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – SageMaker上的模型并行训练任务](img/B18493_06_004.jpg)'
- en: Figure 6.4 – Model parallel training task on SageMaker
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – SageMaker上的模型并行训练任务
- en: You can see from *Figure 6**.4* that when calling the `fit()` method for the
    SageMaker `estimator` object, just like the data parallel example in *Figure 6**.3*,
    SageMaker allocates a copy of the model training script to each GPU on each of
    the two instances. However, instead of distributing the same copy of the model
    to each GPU worker, as was the case with the data parallel placement strategy,
    SageMaker splits the model into smaller pieces, or model partitions, assigning
    each model partition to a GPU worker.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从*图6**.4*中看到，当调用SageMaker `estimator`对象的`fit()`方法时，就像*图6**.3*中的数据并行示例一样，SageMaker将模型训练脚本的一个副本分配给两个实例中的每个GPU。然而，与数据并行放置策略不同，SageMaker将模型分割成更小的部分，或者模型分区，并将每个模型分区分配给一个GPU工作器。
- en: To coordinate how the training of each model partition, SageMaker implements
    a **pipeline scheduler**. In the same way that the AllReduce optimization algorithm
    coordinates parameter optimizations across different GPU workers, the pipeline
    scheduler ensures that as each batch of data is fed into the model and computation
    for each partition of the model is correctly coordinated and scheduled between
    all the GPU workers. This ensures that the matrix calculations for each layer,
    during both the forward and backward passes over these network partitions, happen
    in accordance with the overall structure of the model architecture. For example,
    the scheduler would ensure that the mathematical calculations for layer two are
    executed before layer three on the forward pass and that layer three’s gradient
    calculation occurs before layer two. Once all the desired epochs have been executed,
    essentially both the forward and backward passes through the model architecture
    over the entirety of the training data, SageMaker dismantles the ephemeral cluster
    and stores the optimized model on S3.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了协调每个模型分区的训练，SageMaker 实现了一个**管道调度器**。与 AllReduce 优化算法协调不同 GPU 工作者的参数优化方式相同，管道调度器确保每个批次的数据被输入到模型中，并且每个模型分区的计算被正确协调和调度在所有
    GPU 工作者之间。这确保了在正向和反向遍历这些网络分区时，每一层的矩阵计算都符合模型架构的整体结构。例如，调度器会确保在正向遍历中，层二的数学计算在层三之前执行，并且层三的梯度计算在层二之前发生。一旦执行了所有所需的周期，本质上就是通过整个训练数据集的正向和反向遍历模型架构，SageMaker
    解散临时集群并将优化后的模型存储在 S3 上。
- en: In summation, the data parallel placement strategy was originally conceived
    to reduce the overall training time of a model by sharing the data and parallelizing
    the execution across multiple compute resources. The primary motivation behind
    a model parallel placement strategy is to address large models that don’t fit
    into the compute resource’s memory. This then begs the question as to whether
    it’s possible to combine both the data parallel and model parallel placement strategies
    to reduce the overall training time for both large models, as well as large datasets
    in a distributed fashion.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据并行放置策略最初是为了通过共享数据和并行化跨多个计算资源执行来减少模型的总体训练时间。模型并行放置策略背后的主要动机是解决不适合计算资源内存的大型模型。这引出了一个问题，即是否可以将数据并行和模型并行放置策略结合起来，以分布式方式减少大型模型和大型数据集的总体训练时间。
- en: Let’s review this hybrid methodology next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们回顾这种混合方法。
- en: Reviewing a hybrid data parallel and model parallel strategy
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合数据并行和模型并行策略的回顾
- en: The fact that both the data parallel and model parallel placement strategies
    were created to address specific challenges, it is fundamentally impossible to
    combine both strategies into a unified, hybrid strategy. Essentially, both strategies
    only solve their specific issues by either sharding the data or sharding the model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据并行和模型并行放置策略都是为了解决特定挑战而创建的，因此将两种策略结合成一个统一的混合策略在本质上是不可能的。本质上，两种策略通过数据分片或模型分片来解决它们各自的具体问题。
- en: 'Fortunately, because the entirety of the Training job is orchestrated and managed
    by SageMaker, the ability to combine both strategies into a hybrid strategy is
    now possible. For example, if we review *Figure 6**.5*, we can visualize just
    how SageMaker allows us to execute a Training job that implements both the data
    parallel and model parallel placement strategies independently:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于整个训练作业由 SageMaker 协调和管理，现在可以将两种策略结合成一个混合策略成为可能。例如，如果我们回顾*图 6.5*，我们可以直观地看到
    SageMaker 如何使我们能够独立执行实现数据并行和模型并行放置策略的训练作业：
- en: '![Figure 6.5 – Independent data parallel and model parallel strategies on SageMaker](img/B18493_06_005.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – SageMaker 上的独立数据并行和模型并行策略](img/B18493_06_005.jpg)'
- en: Figure 6.5 – Independent data parallel and model parallel strategies on SageMaker
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – SageMaker 上的独立数据并行和模型并行策略
- en: '*Figure 6**.5* illustrates taking the same number of compute instances, and
    essentially implementing a two-node data parallel placement strategy, along with
    a four-way model parallel placement strategy. This translates to creating two
    copies of the training script and assigning each copy to one of the two compute
    instances. We then execute the distributed training task using the data parallel
    placement strategy. While the training task is being executed, we further partition
    the specific copy of the model architecture across each of the GPUs in the individual
    compute instance, using the model parallel placement strategy.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.5* 展示了使用相同数量的计算实例，并基本上实现了一个两节点数据并行放置策略，以及一个四路模型并行放置策略。这意味着创建两个训练脚本的副本，并将每个副本分配给两个计算实例中的一个。然后我们使用数据并行放置策略执行分布式训练任务。在训练任务执行的同时，我们使用模型并行放置策略将特定模型架构的副本分配给每个计算实例中的各个
    GPU。'
- en: So, while each of these placement strategies is unique in its approach, by using
    SageMaker, we can reap the benefits of both approaches to reduce the overall training
    time on large datasets, as well as large ML models. In the next section, we will
    review examples of how to practically implement each of the placement strategies
    on SageMaker, including an example of this hybrid approach.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管每种放置策略在其方法上都是独特的，但通过使用 SageMaker，我们可以获得两种方法的好处，以减少在大数据集和大型机器学习模型上的整体训练时间。在下一节中，我们将回顾如何在
    SageMaker 上实际实施每种放置策略的示例，包括这个混合方法的示例。
- en: Executing a distributed training workload on AWS
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 上执行分布式训练工作负载
- en: Now that we’ve been introduced to some of the fundamentals of distributed training
    and what happens behind the scenes when we leverage SageMaker to launch a distributed
    Training job, let’s explore how we can execute such a workload on AWS. Since we’ve
    reviewed two placement techniques, namely data parallel and model parallel, we
    will start by reviewing how to execute distributed data parallel training. After
    which, we will then review how to execute distributed model parallel training,
    but also include the hybrid methodology and include an independent data parallel
    placement strategy alongside the model parallel example.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了分布式训练的一些基本原理，以及当我们利用 SageMaker 启动分布式训练作业时幕后发生的事情，让我们来探讨如何在 AWS 上执行这样的工作负载。由于我们已经审查了两种放置技术，即数据并行和模型并行，我们将首先回顾如何执行分布式数据并行训练。之后，我们将回顾如何执行分布式模型并行训练，同时包括混合方法和在模型并行示例中包含独立的数据并行放置策略。
- en: Note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we leverage a **Vision Transformer** (**ViT**) model to address
    an image classification use case. Since the objective of this section is to showcase
    how to practically implement both the data parallel and model parallel placement
    strategies, we will not be diving into the particulars of the model itself but
    rather using it within the context of transfer learning. To learn more about the
    ViT model, please review the *Transformers for Image Recognition at Scale* paper
    ([https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们利用一个**视觉Transformer**（**ViT**）模型来解决图像分类用例。由于本节的目标是展示如何实际实施数据并行和模型并行放置策略，我们不会深入探讨模型本身的细节，而是将其用于迁移学习的上下文中。要了解更多关于
    ViT 模型的信息，请参阅*大规模图像识别的 Transformer*论文（[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)）。
- en: Let’s get started with the data parallel workload.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据并行工作负载开始。
- en: Executing distributed data parallel training on Amazon SageMaker
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上执行分布式数据并行训练
- en: 'There are two crucial elements to executing a distributed Training job using
    the data parallel placement strategy on SageMaker:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SageMaker 通过数据并行放置策略执行分布式训练作业有两个关键要素：
- en: Configuring the backend cluster
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置后端集群
- en: Configuring the model training script
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置模型训练脚本
- en: In the next section, we will start by walking through an example of how to configure
    the backend ephemeral SageMaker cluster.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将首先通过一个示例来讲解如何配置后端暂态 SageMaker 集群。
- en: Configuring the backend cluster
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置后端集群
- en: To get started with setting up the SageMaker cluster, we will be leveraging
    the same SageMaker Studio environment, along with the sample code from the companion
    GitHub repository, that we introduced in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始设置SageMaker集群，我们将利用与[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中介绍的相同的SageMaker
    Studio环境，以及来自配套GitHub存储库的示例代码。
- en: Note
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you haven’t provisioned the SageMaker Studio environment, please refer back
    to the *Setting up EMR and SageMaker Studio* section in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未配置SageMaker Studio环境，请参阅[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中的*设置EMR和SageMaker
    Studio*部分。
- en: 'The following steps will walk you through setting up the example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将指导您设置示例：
- en: Log into the AWS account that was used for [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095),
    *Data Analysis,* examples, and open the SageMaker management console ([https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home)).
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录用于[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*示例的AWS账户，并打开SageMaker管理控制台（[https://console.aws.amazon.com/sagemaker/home](https://console.aws.amazon.com/sagemaker/home)）。
- en: With the SageMaker management console open, use the left-hand navigation panel
    to click on the **SageMaker Domain** link. Under the **Users** section, you will
    see **Name** of the user, and the **Launch app** drop-down box.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SageMaker管理控制台打开的情况下，使用左侧导航面板点击**SageMaker域**链接。在**用户**部分，您将看到用户的**名称**和**启动应用**下拉框。
- en: Click the **Launch app** drop-down and select the **Studio** option to launch
    the Studio IDE.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**启动应用**下拉菜单并选择**Studio**选项以启动Studio IDE。
- en: Once the Studio environment is open, double-click on the **Applied-Machine-Learning-and-High-Performance-Computing-on-AWS**
    folder that we cloned in [*Chapter 5*](B18493_05.xhtml#_idTextAnchor095), *Data
    Analysis*.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦Studio环境打开，双击我们在[*第5章*](B18493_05.xhtml#_idTextAnchor095)，*数据分析*中克隆的**Applied-Machine-Learning-and-High-Performance-Computing-on-AWS**文件夹。
- en: Now, double-click on the `Chapter06` folder to get access to the example Jupyter
    notebooks.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，双击`Chapter06`文件夹以访问示例Jupyter笔记本。
- en: Double-click on the `1_distributed_data_parallel_training.ipynb` file to launch
    the notebook.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`1_distributed_data_parallel_training.ipynb`文件以启动笔记本。
- en: Note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The notebook will initialize an **ml.m5.xlarge** compute instance with **4 vCPUs**,
    and **16 GB** of RAM to run a pre-configured **PyTorch 1.8** kernel. This instance
    type exceeds the free resource type allowed by the AWS Free Tier ([https://aws.amazon.com/free](https://aws.amazon.com/free))
    and will therefore incur AWS usage costs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本将初始化一个**ml.m5.xlarge**计算实例，配备**4 vCPUs**和**16 GB**的RAM来运行预配置的**PyTorch 1.8**内核。此实例类型超过了AWS免费层允许的免费资源类型（[https://aws.amazon.com/free](https://aws.amazon.com/free)），因此将产生AWS使用费用。
- en: Once the example notebook has been launched and the kernel has been started,
    click on the **Kernel** menu option, and select the **Restart Kernel and Run All
    Cells…** option to execute the notebook code cells.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦示例笔记本已启动且内核已启动，点击**内核**菜单选项，并选择**重启内核并运行所有单元格…**选项来执行笔记本代码单元格。
- en: While the notebook is running, let’s review the code to understand exactly what’s
    happening. In the first two code cells, we download both the training and validation
    horses or humans datasets. These datasets have been provided by Laurence Moroney
    ([https://laurencemoroney.com/datasets.html](https://laurencemoroney.com/datasets.html))
    and contain 500 rendered images of various species of horse, as well as 527 rendered
    images of humans. We will be using this dataset to generate higher resolution
    versions, thereby creating much larger image file sizes to simulate having a large
    training and validation dataset. By increasing the size of the data, we are therefore
    creating a scenario where training a model on these large image files would, in
    effect, introduce a delay in the overall time it takes to train an image classification
    model. Consequently, we are setting up the requirement to leverage a data parallel
    placement strategy that will, in effect, reduce the overall time taken to train
    our image classification model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当笔记本正在运行时，让我们回顾一下代码，以了解到底发生了什么。在前两个代码单元中，我们下载了训练集和验证集的马或人数据集。这些数据集由Laurence
    Moroney提供（[https://laurencemoroney.com/datasets.html](https://laurencemoroney.com/datasets.html)），包含500张不同品种的马的渲染图像，以及527张人的渲染图像。我们将使用这个数据集来生成更高分辨率的版本，从而创建更大的图像文件大小，以模拟拥有大量训练和验证数据集的情况。因此，通过增加数据的大小，我们实际上创建了一个场景，在这个场景中，在大型图像文件上训练模型将导致图像分类模型的整体训练时间延迟。因此，我们正在设置一个要求利用数据并行放置策略，这将实际上减少训练我们的图像分类模型所需的总时间。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: These datasets are licensed under the Creative Commons 2.0 Attribution 2.0 Unported
    License.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集受Creative Commons 2.0 Attribution 2.0 Unported License许可。
- en: 'In the third code cell, as shown in the following code snippet, we programmatically
    extract both the downloaded `train.zip` and `validation.zip` files and save them
    locally to a `data` folder:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三个代码单元中，如以下代码片段所示，我们以编程方式提取下载的`train.zip`和`validation.zip`文件，并将它们保存在本地的`data`文件夹中：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that data has been downloaded and extracted, we should have two folders
    within the `data` directory called `train` and `validation`. Each of these folders
    contains images of both horses and humans. However, as already mentioned, these
    images are pretty small in size. For example, if we examine the `horse01-0.png`
    file in the `./data/train/horses` folder, you will note that the file is only
    151.7 KB in size. Since we only have 500 of these tiny files representing horses,
    we need to somehow come up with a way to make these files bigger. Therefore, we
    will use an ML model called **Enhanced Deep Residual Networks for Single Image
    Super-Resolution** (**EDSR**) to increase the resolution of these files and, in
    effect, increase the size of the files to simulate a real-world use case where
    images are in MB, as opposed to KB.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经下载并提取，我们应该在`data`目录下有两个文件夹，分别命名为`train`和`validation`。这两个文件夹都包含马和人的图片。然而，正如之前提到的，这些图片的尺寸相当小。例如，如果我们检查`./data/train/horses`文件夹中的`horse01-0.png`文件，你会注意到这个文件的大小仅为151.7
    KB。由于我们只有500个这样的小文件代表马，我们需要想出一种方法来使这些文件变大。因此，我们将使用一个名为**Enhanced Deep Residual
    Networks for Single Image Super-Resolution**（**EDSR**）的ML模型来提高这些文件的分辨率，从而实际上增加文件的大小，以模拟一个真实世界的使用案例，其中图片大小为MB，而不是KB。
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While it’s not within the scope of this chapter to detail the EDSR model, we
    are simply using it to enhance the resolution of the images, thereby making the
    file size bigger. You can learn more about the pre-trained model from Hugging
    Face by referencing their model repository ([https://huggingface.co/eugenesiow/edsr-base](https://huggingface.co/eugenesiow/edsr-base)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章的范围不包括详细说明EDSR模型，我们只是用它来提高图像的分辨率，从而使文件大小变大。你可以通过参考Hugging Face的模型存储库了解更多关于预训练模型的信息（[https://huggingface.co/eugenesiow/edsr-base](https://huggingface.co/eugenesiow/edsr-base)）。
- en: 'So, in the next set of code cells, as shown in the following code snippet,
    we run the pre-trained EDSR model on our image dataset to increase the image resolution
    and, as a byproduct, increase the image file size:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一组代码单元中，如以下代码片段所示，我们运行预训练的EDSR模型在我们的图像数据集上以提高图像分辨率，并作为副产品增加图像文件大小：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see from the example output from the code cell, we have increased
    the file size of each image from approximately 178 KB to just under 2 MB. So,
    with the datasets ready for training, we can upload them to S3 so that the ephemeral
    SageMaker cluster can access them. The following code snippet shows how we initialize
    the SageMaker permissions to S3 and use the `upload()` method from the SageMaker
    Python SDK’s `S3Upload` class to store the data on S3:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从代码单元格的示例输出中看到的那样，我们已将每个图像的文件大小从大约178 KB增加到略低于2 MB。因此，当数据集准备好用于训练时，我们可以将它们上传到S3，以便临时的SageMaker集群可以访问它们。以下代码片段展示了我们如何初始化SageMaker对S3的权限，并使用SageMaker
    Python SDK的`S3Upload`类的`upload()`方法将数据存储在S3上：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we are ready to define the SageMaker estimator. You will recall from the
    code snippet shown in the *Building ML systems using AWS* section that all we
    had to do was define a `PyTorch` estimator and provide the basic configuration
    parameters, such as `instance_coun`t and `instance_type`, and SageMaker took care
    of the rest of the heavy lifting to orchestrate the Training job. However, in
    order to configure a data parallel placement strategy, we need to provide an additional
    configuration parameter, called `distribution`, to the estimator. As you can see
    from the following code snippet, we declared the same instance of the estimator,
    but now we’ve added the `distribution` parameter to inform SageMaker that we wish
    to enable a `dataparallel` placement strategy:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好定义SageMaker估计器。您可能还记得在*使用AWS构建ML系统*部分中展示的代码片段，我们只需要定义一个`PyTorch`估计器并提供基本的配置参数，例如`instance_count`和`instance_type`，SageMaker就会负责剩下的繁重工作来编排训练作业。然而，为了配置数据并行放置策略，我们需要向估计器提供一个额外的配置参数，称为`distribution`。正如您可以从以下代码片段中看到的那样，我们声明了相同的估计器实例，但现在我们添加了`distribution`参数来通知SageMaker我们希望启用`dataparallel`放置策略：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, all that’s left to do is initiate the Training job by calling the `fit()`
    method of our `estimator` object. The following code snippet shows how to initialize
    the distributed Training job using the training and validation data that we’ve
    already uploaded to S3:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们剩下的工作就是通过调用我们的`estimator`对象的`fit()`方法来启动训练作业。以下代码片段展示了如何使用我们已上传到S3的训练和验证数据初始化分布式训练作业：
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once the Training job has been initialized, SageMaker will redirect the logs
    so that we can see what’s happening inside the PyTorch training container, and
    we can match the log output to what we learned about how the distributed data
    parallel placement strategy functions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了训练作业，SageMaker将重定向日志，以便我们可以看到PyTorch训练容器内部发生了什么，并且我们可以将日志输出与我们关于分布式数据并行放置策略如何工作的了解相匹配。
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you receive a `estimator.fit()` method, you can follow the resolution steps
    from the *How do I resolve the ResourceLimitExceeded error in Amazon SageMaker?*
    knowledge article ([https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/](https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您收到`estimator.fit()`方法的错误，您可以遵循*如何解决Amazon SageMaker中的ResourceLimitExceeded错误*知识文章中的解决步骤（[https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/](https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/))。
- en: You will recall from the *Reviewing the SageMaker distributed data parallel
    strategy* section that the training script, as well as the model algorithm, are
    copied to each GPU within the compute instance. Since each GPU is essentially
    executing its own copy of the training script to optimize its own unique set of
    model parameters and then share them with the GPU works using AllReduce, we also
    need to ensure that the training script itself is configured in such a way that
    it gets executed as a part of a larger distributed training process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得在*审查SageMaker分布式数据并行策略*部分中，训练脚本以及模型算法被复制到计算实例中的每个GPU。由于每个GPU本质上都在执行其自己的训练脚本副本以优化其独特的模型参数集，然后通过AllReduce与GPU工作共享，我们还需要确保训练脚本本身被配置为作为更大分布式训练过程的一部分执行。
- en: Basically, what this means is that when we specify the `distribution` parameter
    for the SageMaker `estimator` object, we are instructing SageMaker to configure
    the appropriate backend resources for a distributed Training job. But we also
    need to configure the training script to properly use this distributed backend
    cluster. So, to extend the training script’s ability to correctly leverage the
    backend cluster’s distributed capabilities, AWS provides the `smdistributed`,
    for the specified deep learning framework, which is PyTorch in this case.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这意味着当我们为 SageMaker 的 `estimator` 对象指定 `distribution` 参数时，我们是在指示 SageMaker
    为分布式训练作业配置适当的后端资源。但我们也需要配置训练脚本来正确使用这个分布式后端集群。因此，为了扩展训练脚本利用后端集群分布式功能的能力，AWS 为指定的深度学习框架（在本例中为
    PyTorch）提供了 `smdistributed`。
- en: Note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: AWS also provides the `smdistributed` library for the TensorFlow 2.x deep learning
    framework. For more information on how to leverage a distributed data parallel
    placement strategy for a TensorFlow training script, you can review the TensorFlow
    Guide ([https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html](https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 还为 TensorFlow 2.x 深度学习框架提供了 `smdistributed` 库。有关如何利用分布式数据并行放置策略为 TensorFlow
    训练脚本提供支持的信息，您可以查看 TensorFlow 指南 ([https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html](https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html))。
- en: In the next section, we will review how to configure the model training script
    using the `smdistributed` Python library.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾如何使用 `smdistributed` Python 库配置模型训练脚本。
- en: Configuring the model training script
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置模型训练脚本
- en: 'There are five basic specific steps for incorporating the `smdistributed` library
    into a PyTorch training script. To review these steps, we can open the `./src/train.py`
    file within the Studio IDE and walk through the important code as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `smdistributed` 库集成到 PyTorch 训练脚本中有五个基本具体步骤。要查看这些步骤，我们可以在 Studio IDE 中打开 `./src/train.py`
    文件，并按以下方式遍历重要的代码：
- en: 'The first step is to import the `smdistributed` libraries for the PyTorch framework.
    As you can see from the following code snippet, by importing and then initializing
    these modules, we are essentially wrapping PyTorch’s ability to execute parallel
    training methods into the data parallel placement strategy:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是导入 PyTorch 框架的 `smdistributed` 库。正如您可以从以下代码片段中看到的那样，通过导入并初始化这些模块，我们实际上是将
    PyTorch 执行并行训练方法的能力封装到数据并行放置策略中：
- en: '[PRE10]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The next step is to integrate the data into PyTorch’s data loading mechanism
    so that PyTorch can iterate through the chunks of data assigned to the GPU worker.
    In the following code snippet, we specify `num_replicas` as the number of GPU
    workers participating in this distributed training exercise. We also supply the
    GPU worker’s local rank or its membership ranking within the current exercise,
    by specifying the `rank` parameter:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将数据集成到 PyTorch 的数据加载机制中，以便 PyTorch 可以迭代分配给 GPU 工作者的数据块。在下面的代码片段中，我们将 `num_replicas`
    指定为参与此分布式训练练习的 GPU 工作者数量。我们还通过指定 `rank` 参数提供了 GPU 工作者的本地排名或其在当前练习中的成员排名：
- en: '[PRE15]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'From the previous code snippet, we used the `world_size` and `rank` variables.
    The `world_size` variable was used to denote the total number of GPU workers over
    which the data parallel task is being distributed. So, as you can see from the
    next code snippet, to get the total amount of GPUs, we call the `get_world_size()`
    method from the `smdistributed.dataparallel.torch.distributed` module. Similarly,
    we also use the `get_rank()` method from this library to get the current GPU’s
    membership ranking:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前面的代码片段中，我们使用了 `world_size` 和 `rank` 变量。`world_size` 变量用于表示数据并行任务正在分布的总 GPU
    工作者数量。因此，正如您可以从下一个代码片段中看到的那样，为了获取总的 GPU 数量，我们调用 `smdistributed.dataparallel.torch.distributed`
    模块中的 `get_world_size()` 方法。同样，我们也使用这个库中的 `get_rank()` 方法来获取当前 GPU 的成员排名：
- en: '[PRE20]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Lastly, we configured the mini batch size for the PyTorch `DataLoader()` method
    to sample, declared as the `batch_size` variable. This is the global batch size
    of the training job, divided by the number of GPU workers, represented by the
    `world_size` variable described in *step 3*:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们为 PyTorch 的 `DataLoader()` 方法配置了小批量大小，声明为 `batch_size` 变量。这是训练作业的全局批量大小，除以
    GPU 工作者的数量，由 *步骤 3* 中描述的 `world_size` 变量表示：
- en: '[PRE24]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: So, with these minimal code additions applied to the model training routine,
    we have effectively implemented an example of a data parallel placement strategy.
    Next, let’s look at how to use the same example but apply a model parallel placement
    strategy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将这些最小代码添加应用到模型训练流程中，我们已经有效地实现了一个数据并行放置策略的示例。接下来，让我们看看如何使用相同的示例，但应用模型并行放置策略。
- en: Executing distributed model parallel training on Amazon SageMaker
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上执行分布式模型并行训练
- en: Since we are using the same image classification model, shown in the previous
    example, to illustrate an example of model parallel training, you can follow the
    same steps to open and execute the notebook. However, instead of opening the `1_distributed_data_parallel_training.ipynb`
    file, in this example, we are going to open the `2_distributed_model_parallel_training.ipynb`
    file and run all of the code cells.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用与前面示例中相同的图像分类模型来展示模型并行训练的示例，您可以使用相同的步骤打开并执行笔记本。然而，在本例中，我们将打开 `2_distributed_model_parallel_training.ipynb`
    文件并运行所有的代码单元，而不是打开 `1_distributed_data_parallel_training.ipynb` 文件。
- en: So, just as with the data parallel placement strategy, there are two crucial
    components to successfully implementing the model parallel placement strategy
    on SageMaker, namely configuring the backend cluster and configuring the model
    training script. Let’s start by exploring all the changes that need to be made
    to the `estimator` configuration.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，就像数据并行放置策略一样，在 SageMaker 上成功实施模型并行放置策略有两个关键组件，即配置后端集群和配置模型训练脚本。让我们首先探讨需要对
    `estimator` 配置进行的所有更改。
- en: Configuring the backend cluster
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置后端集群
- en: 'When reviewing the estimator configuration, note that the options provided
    to the `distribution` parameter have changed. As you can see from the following
    code snippet, we now specify a `modelparallel` option instead of enabling the
    `dataparallel` setting for `smdistributed`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当审查估计器配置时，请注意提供给 `distribution` 参数的选项已更改。如您从以下代码片段中看到的，我们现在指定了一个 `modelparallel`
    选项，而不是为 `smdistributed` 启用 `dataparallel` 设置：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Additionally, as shown in the following code snippet, we declare a variable
    called `smp_options`, whereby we specify a dictionary of the configuration options
    specific to the `modelparallel` strategy:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如以下代码片段所示，我们声明了一个名为 `smp_options` 的变量，通过该变量我们指定了针对 `modelparallel` 策略的配置选项字典：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see from the previous code snippet, where the most important configuration
    options have been highlighted, we set the `placement_strategy` parameter as `spread`.
    In effect, we are configuring SageMaker to evenly spread the model partitions
    across all GPU devices within the compute instance. Since we are using a single
    **ml.p3.16xlarge** instance with eight GPUs, and not multiple compute instances,
    we are spreading the model partitions evenly within the instance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的代码片段中看到的，其中最重要的配置选项已被突出显示，我们将 `placement_strategy` 参数设置为 `spread`。实际上，我们正在配置
    SageMaker 在计算实例内的所有 GPU 设备上均匀地分配模型分区。由于我们使用的是单个 **ml.p3.16xlarge** 实例，该实例具有八个
    GPU，而不是多个计算实例，因此我们在实例内部均匀分配模型分区。
- en: Additionally, we are setting the pipeline scheduling mechanism, the `pipeline`
    parameter, to `interleaved`. This setting improves the overall performance of
    the backend cluster by prioritizing the backward execution model exaction tasks
    to free up GPU memory.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将管道调度机制，即 `pipeline` 参数，设置为 `interleaved`。此设置通过优先执行反向执行模型提取任务以释放 GPU 内存，从而提高了后端集群的整体性能。
- en: Lastly, to enable both a model parallel, as well as a hybrid implementation
    of the data parallel placement strategies, we set the distributed data parallel,
    or `ddp` parameter, to `True`. As we saw in the section entitled, *Reviewing a
    hybrid data parallel and model parallel strategy*, both the data parallel and
    model parallel strategies can be used at the same time to further reduce the overall
    time it takes to train the model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了启用模型并行以及数据并行放置策略的混合实现，我们将分布式数据并行或 `ddp` 参数设置为 `True`。正如我们在标题为“**混合数据并行和模型并行策略回顾**”的部分中看到的，数据并行和模型并行策略可以同时使用，以进一步减少训练模型所需的总时间。
- en: So, since we are using both strategies concurrently for this example, we must
    also supply a `mpi` parameter, to instruct SageMaker as to how each GPU worker
    communicates what it’s doing with the other GPU workers. For example, in the previous
    code snippet, after enabling the `mpi_options` setting, we have also set `processes_per_host`
    to `8`. This setting, in effect, configures the ephemeral SageMaker cluster architecture
    to match *Figure 6**.5*, where we set the GPU workers on the single **ml.p3.16xlarge**
    compute instance to use a four-way model parallel strategy to essentially partition
    the model across four GPU workers. Additionally, we also configure a two-way data
    parallel strategy to partition the training data into two shards and execute the
    model partitions in parallel across the shards. Therefore, two-way x four-way
    equates to eight processes per single host.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于我们在这个例子中同时使用了这两种策略，我们必须也提供一个`mpi`参数，以指导SageMaker每个GPU工作器如何与其他GPU工作器通信其正在执行的操作。例如，在之前的代码片段中，在启用`mpi_options`设置后，我们还设置了`processes_per_host`为`8`。这个设置实际上配置了临时的SageMaker集群架构以匹配*图6**.5*，其中我们将单个**ml.p3.16xlarge**计算实例上的GPU工作器设置为使用四路模型并行策略，从而将模型基本分割成四个GPU工作器。此外，我们还配置了双向数据并行策略，将训练数据分割成两个数据块，并在数据块上并行执行模型分割。因此，双向乘以四路等于每个主机八个进程。
- en: As you can see, adding these minimal configuration changes implements a data
    parallel and model parallel capable SageMaker cluster. Yet, just as with the previous
    example, there are also changes that need to be made to the training script. Let’s
    review these next.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，通过添加这些最小配置更改，可以实现一个支持数据并行和模型并行的SageMaker集群。然而，就像之前的例子一样，训练脚本也需要进行一些更改。接下来让我们来回顾这些更改。
- en: Configuring the model training script
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置模型训练脚本
- en: 'Since implementing a model parallel placement strategy is more intricate than
    a data parallel strategy, there are a few extra requirements that need to be added
    to the training script. Let’s now open the `./src/train_smp.py` file to review
    the most important requirements. As you might immediately notice, there are 11
    specific script changes required to execute a model parallel placement strategy
    for a PyTorch model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现模型并行放置策略比数据并行策略更为复杂，训练脚本中需要添加一些额外的要求。现在让我们打开`./src/train_smp.py`文件来回顾最重要的要求。你可能立即会注意到，为了执行PyTorch模型的模型并行放置策略，需要11个特定的脚本更改：
- en: 'Once again, and as you can see from the following code snippet, the first step
    is to import the `modelparallel` modules from the `smdistributed` library and
    initialize these modules as a wrapper for PyTorch:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，正如你从下面的代码片段中看到的，第一步是从`smdistributed`库中导入`modelparallel`模块，并将这些模块初始化为PyTorch的包装器：
- en: '[PRE30]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Once the module has been initialized, we then extend our image classification
    model, defined using the `model` variable, and wrap it into the `DistributedModel()`
    class, as shown in the following code block. This signals that our model is now
    being distributed:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模块初始化完成，我们就扩展使用`model`变量定义的图像分类模型，并将其包装进`DistributedModel()`类中，如下面的代码块所示。这表明我们的模型现在正在被分布式处理：
- en: '[PRE35]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Since the model is now being distributed, we also need to distribute the optimizer.
    So, as you can see from the following code snippet, we optimize the model parameters
    using PyTorch’s implementation of the `Adam()` algorithm and subsequently distribute
    the optimization task across GPUs by wrapping it into the `DistributedOptimizer()`
    class:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于模型现在正在被分布式处理，我们还需要分布式优化器。所以，正如你从下面的代码片段中看到的，我们使用PyTorch的`Adam()`算法实现来优化模型参数，并通过将优化任务包装进`DistributedOptimizer()`类来跨GPU分布式优化任务：
- en: '[PRE38]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Alongside distributing the model itself, as well as the optimizer, we also
    need to define exactly how the forward and backward pass through the model’s computational
    graph, or the model pipeline, are executed. Accordingly, we extend the computation
    results from both the forward and backward passes of the model by wrapping them
    within a `step()` decorator function. The following code snippet shows the `step()`
    decorator that extends `train_step()` for the forward pass, and `test_step()`
    for the backward pass:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了分布式处理模型本身以及优化器之外，我们还需要精确定义正向和反向传递通过模型计算图或模型管道的执行方式。相应地，我们通过将正向和反向传递的计算结果包装在`step()`装饰函数中来扩展模型的计算结果。下面的代码片段显示了扩展`train_step()`用于正向传递和`test_step()`用于反向传递的`step()`装饰器：
- en: '[PRE42]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Lastly, once the model has been trained using the model parallel strategy,
    and as you can see from the following code snippet, we only save the final model
    on the highest-ranking GPU worker of the cluster:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，一旦使用模型并行策略训练了模型，正如您可以从以下代码片段中看到的那样，我们只在集群中排名最高的GPU工作节点上保存最终模型：
- en: '[PRE58]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: While these are only a few of the most important parameters for configuring
    the training script using the `smdistributed.modelprallel` module, you can see
    that with a minimal amount of code, we can fully provision our training script
    to use an automatically configured SageMaker ephemeral cluster for both a data
    parallel and model parallel placement strategy, thus reducing the overall training
    time using this hybrid implementation.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些只是使用`smdistributed.modelprallel`模块配置训练脚本时最重要的几个参数，但您可以看到，通过极少的代码，我们可以完全配置我们的训练脚本，以使用自动配置的SageMaker临时集群，同时实现数据并行和模型并行放置策略，从而减少使用这种混合实现的整体训练时间。
- en: Summary
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we drew your attention to two potential challenges that ML
    practitioners may face when training ML models: firstly, the challenge of reducing
    the overall model training time, especially when there is a large amount of training
    data; and secondly, the challenge of reducing the overall model training time
    when there are large models with millions and billions of trainable parameters.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们关注了机器学习实践者在训练机器学习模型时可能面临的两个潜在挑战：首先，减少整体模型训练时间，尤其是在有大量训练数据的情况下；其次，当存在具有数百万甚至数十亿可训练参数的大型模型时，减少整体模型训练时间的挑战。
- en: We reviewed three specific strategies that can be used to address these challenges,
    namely the data parallel placement strategy, which distributes a large amount
    of training data across multiple worker resources to execute the model training
    process in parallel. Additionally, we also reviewed the model parallel placement
    strategy, which distributes a very large ML model across multiple GPU resources
    to offset trying to squeeze these large models into the available memory resources.
    Lastly, we also explored how both these strategies can be combined, using a hybrid
    methodology, to further reap the benefits that both offer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了三种可以用来解决这些挑战的具体策略，即数据并行放置策略，它将大量训练数据分布到多个工作资源中，以并行执行模型训练过程。此外，我们还回顾了模型并行放置策略，它将一个非常大的机器学习模型分布到多个GPU资源中，以避免试图将这些大型模型压缩到可用的内存资源中。最后，我们还探讨了如何通过混合方法将这两种策略结合起来，以进一步获得两者带来的好处。
- en: Furthermore, we also reviewed how Amazon SageMaker can be used to solve these
    challenges, specifically focusing on how SageMaker takes care of the heavy lifting
    of building a distributed training compute and storage infrastructure specifically
    configured to handle any of these three placement strategies. SageMaker not only
    provisions the ephemeral compute resources but also provides Python libraries
    that can be integrated into the model training script to fully make use of the
    cluster.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还回顾了如何使用Amazon SageMaker来解决这些挑战，特别是关注SageMaker如何处理构建专门配置以处理任何这三种放置策略的分布式训练计算和存储基础设施的重任。SageMaker不仅提供临时的计算资源，还提供了可以集成到模型训练脚本中的Python库，以充分利用集群。
- en: Now that we’ve seen how to carry out ML model training using distributed training,
    in the next chapter, we will review how to deploy the trained ML models at scale.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用分布式训练进行机器学习模型训练，在下一章中，我们将回顾如何大规模部署训练好的机器学习模型。
