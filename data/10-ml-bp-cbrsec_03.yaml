- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Detecting Suspicious Activity
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测可疑活动
- en: Many problems in cybersecurity are constructed as anomaly detection tasks, as
    attacker behavior is generally deviant from good actor behavior. An anomaly is
    anything that is out of the ordinary—an event that doesn’t fit in with normal
    behavior and hence is considered suspicious. For example, if a person has been
    consistently using their credit card in Bangalore, a transaction using the same
    card in Paris might be an anomaly. If a website receives roughly 10,000 visits
    every day, a day when it receives 2 million visits might be anomalous.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络安全中，许多问题都构建为异常检测任务，因为攻击者的行为通常与良好行为者的行为不同。异常是任何不寻常的事物——一个不符合正常行为的事件，因此被认为是可疑的。例如，如果一个人一直在班加罗尔使用他们的信用卡，那么在巴黎使用同一张卡进行交易可能就是一个异常。如果一个网站每天大约有10,000次访问，那么一天内它收到2000万次访问可能就是异常的。
- en: Anomalies are few and rare and indicate behavior that is strange and suspicious.
    Anomaly detection algorithms are *unsupervised*; we do not have labeled data to
    train a model. We learn what the normal expected behavior is and flag anything
    that deviates from it as abnormal. Because labeled data is very rarely available
    in security-related areas, anomaly detection methods are crucial in identifying
    attacks, fraud, and intrusions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 异常情况很少见，表明行为异常且可疑。异常检测算法是**无监督的**；我们没有标记的数据来训练模型。我们学习正常预期的行为，并将任何偏离正常的行为标记为异常。由于在安全相关领域，标记数据非常罕见，因此异常检测方法对于识别攻击、欺诈和入侵至关重要。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要主题：
- en: Basics of anomaly detection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测的基础
- en: Statistical algorithms for intrusion detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于入侵检测的统计算法
- en: '**Machine learning** (**ML**) algorithms for intrusion detection'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）算法用于入侵检测'
- en: By the end of this chapter, you will know how to detect outliers and anomalies
    using statistical and ML methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何使用统计和机器学习方法检测异常值和异常。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All of the implementation in this chapter (and the book too) is using the Python
    programming language. Most standard computers will allow you to run all of the
    code without any memory or runtime issues.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章（以及本书）的所有实现都使用Python编程语言。大多数标准计算机都将允许您在没有内存或运行时问题的情况下运行所有代码。
- en: 'There are two options for you to run the code in this book, as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中运行代码有两种选择，如下所示：
- en: '**Jupyter Notebook**: An interactive code and text notebook with a GUI that
    will allow you to run code locally. *Real Python* has a very good introductory
    tutorial on getting started at *Jupyter Notebook: An* *Introduction* ([https://realpython.com/jupyter-notebook-introduction/](https://realpython.com/jupyter-notebook-introduction/)).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**：一个带有GUI的交互式代码和文本笔记本，允许您在本地运行代码。"Real Python"有一个非常好的入门教程，介绍如何开始使用[Jupyter
    Notebook：一个介绍](https://realpython.com/jupyter-notebook-introduction/)。'
- en: '**Google Colab**: This is simply the online version of Jupyter Notebook. You
    can use the free tier as this is sufficient. Be sure to download any data or files
    that you create, as they disappear after the runtime is cleaned up.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab**：这是Jupyter Notebook的在线版本。您可以使用免费层，因为这是足够的。确保下载您创建的任何数据或文件，因为它们在运行时清理后会消失。'
- en: 'You will need to install a few libraries that we need for our experiments and
    analysis. A list of libraries is provided as a text file, and all the libraries
    can be installed using the `pip` utility with the following command:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装一些我们实验和分析所需的库。库的列表以文本文件的形式提供，并且可以使用以下命令使用`pip`实用程序安装所有库：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In case you get an error for a particular library not found or installed, you
    can simply install it with `pip` `install <library_name>`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到特定库未找到或未安装的错误，您可以使用`pip` `install <library_name>`命令简单地安装它。
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%202](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%202).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本章的代码文件，链接为[https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%202](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%202)。
- en: Basics of anomaly detection
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常检测的基础
- en: In this section, we will look at anomaly detection, which forms the foundation
    for detecting intrusions and suspicious activity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨异常检测，它是检测入侵和可疑活动的基础。
- en: What is anomaly detection?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是异常检测？
- en: The word *anomaly* means *something that deviates from what is standard, normal,
    or expected*. Anomalies are events or data points that do not fit in with the
    rest of the data. They represent deviations from the expected trend in data. Anomalies
    are rare occurrences and, therefore, few in number.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: “异常”一词的意思是“偏离标准、正常或预期的事物”。异常是不符合其他数据的事件或数据点。它们代表了数据中预期趋势的偏差。异常是罕见事件，因此数量很少。
- en: 'For example, consider a bot or fraud detection model used in a social media
    website such as Twitter. If we examine the number of follow requests sent to a
    user per day, we can get a general sense of the trend and plot this data. Let’s
    say that we plotted this data for a month, and ended up with the following trend:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个用于社交媒体网站如Twitter的机器人或欺诈检测模型。如果我们检查每天发送给用户的关注请求数量，我们可以得到趋势的一般感觉并绘制这些数据。假设我们为一个月的数据绘制了图表，并得到了以下趋势：
- en: '![Figure 2.1 – Trend for the number of follow requests over a month](img/B19327_02_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 一个月内关注请求数量的趋势](img/B19327_02_01.jpg)'
- en: Figure 2.1 – Trend for the number of follow requests over a month
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 一个月内关注请求数量的趋势
- en: What do you notice? The user seems to have roughly 30-40 follow requests per
    day. On the 8th and 18th days, however, we see a spike that clearly stands out
    from the daily trend. These two days are anomalies.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到了什么？用户似乎每天大约有30-40个关注请求。然而，在第8天和第18天，我们看到一个明显偏离日常趋势的峰值。这两天是异常值。
- en: 'Anomalies can also be visually observed in a two-dimensional space. If we plot
    all the points in the dataset, the anomalies should stand out as being different
    from the others. For instance, continuing with the same example, let us say we
    have a number of features such as the number of messages sent, likes, retweets,
    and so on by a user. Using all of the features together, we can construct an *n*-dimensional
    feature vector for a user. By applying a dimensionality reduction algorithm such
    as **principal component analysis** (**PCA**) (at a high level, this algorithm
    can convert data to lower dimensions and still retain the properties), we can
    reduce it to two dimensions and plot the data. Say we get a plot as follows, where
    each point represents a user, and the dimensions represent principal components
    of the original data. The points colored in red clearly stand out from the rest
    of the data—these are outliers:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 异常也可以在二维空间中直观地观察到。如果我们绘制数据集中的所有点，异常值应该会明显地与其他数据不同。例如，继续使用相同的例子，假设我们有一个用户发送的消息数量、点赞、转发等特征。使用所有这些特征，我们可以为用户构建一个*n*-维特征向量。通过应用降维算法，如**主成分分析**（PCA）（在较高层次上，该算法可以将数据转换为较低维度并保留其属性），我们可以将其降低到二维并绘制数据。假设我们得到以下图表，其中每个点代表一个用户，维度代表原始数据的主成分。用红色着色的点明显与其他数据不同——这些是异常值：
- en: '![Figure 2.2 – A 2D representation of data with anomalies in red](img/B19327_02_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 数据的二维表示，异常值用红色表示](img/B19327_02_02.jpg)'
- en: Figure 2.2 – A 2D representation of data with anomalies in red
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 数据的二维表示，异常值用红色表示
- en: Note that anomalies do not necessarily represent a malicious event—they simply
    indicate that the trend deviates from what was normally expected. For example,
    a user suddenly receiving increased amounts of friend requests is anomalous, but
    this may have been because they posted some very engaging content. Anomalies,
    when flagged, must be investigated to determine whether they are malicious or
    benign.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，异常并不一定代表恶意事件——它们只是表明趋势偏离了通常预期的模式。例如，一个用户突然收到大量好友请求是异常的，但这可能是因为他们发布了一些非常吸引人的内容。当异常被标记时，必须进行调查以确定它们是恶意的还是良性的。
- en: Anomaly detection is considered an important problem in the field of cybersecurity.
    Unusual or abnormal events can often indicate security breaches or attacks. Furthermore,
    anomaly detection does not need labeled data, which is hard to come by in security
    problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测被认为是网络安全领域的一个重要问题。不寻常或异常事件通常可以表明安全漏洞或攻击。此外，异常检测不需要标记数据，这在安全问题上很难获得。
- en: Introducing the NSL-KDD dataset
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍NSL-KDD数据集
- en: Now that we have introduced what anomaly detection is in sufficient detail,
    we will look at a real-world dataset that will help us observe and detect anomalies
    in action.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经足够详细地介绍了什么是异常检测，我们将查看一个现实世界的数据集，这将帮助我们观察和检测异常的实际操作。
- en: The data
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据
- en: Before we jump into any algorithms for anomaly detection, let us talk about
    the dataset we will be using in this chapter. The dataset that is popularly used
    for anomaly and intrusion detection tasks is the **Network Security Laboratory-Knowledge
    Discovery in Databases** (**NSL-KDD**) dataset. This was originally created in
    1999 for use in a competition at the *5*th *International Conference on Knowledge
    Discovery and Data Mining (KDD)*. The task in the competition was to develop a
    network intrusion detector, which is a predictive model that can distinguish between
    bad connections, called intrusions or attacks, and benign normal connections.
    This database contains a standard set of data to be audited, which includes a
    wide variety of intrusions simulated in a military network environment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入任何异常检测算法之前，让我们谈谈本章中将使用的数据集。在异常和入侵检测任务中广泛使用的数据集是**网络安全实验室-数据库知识发现**（**NSL-KDD**）数据集。这个数据集最初是在1999年为了在*第5届国际知识发现和数据挖掘会议（KDD）*上的比赛而创建的。比赛的任务是开发一个网络入侵检测器，这是一个预测模型，可以区分恶意连接，称为入侵或攻击，和良性的正常连接。这个数据库包含了一组标准数据，用于审计，其中包括在军事网络环境中模拟的广泛多样的入侵。
- en: Exploratory data analysis (EDA)
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: This activity consists of a few steps, which we will look at in the next subsections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动包括几个步骤，我们将在下一小节中探讨。
- en: Downloading the data
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 下载数据
- en: The actual NSL-KDD dataset is fairly large (nearly 4 million records). We will
    be using a smaller version of the data that is a 10% subset randomly sampled from
    the whole data. This will make our analysis feasible. You can, of course, experiment
    by downloading the full data and rerunning our experiments.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的NSL-KDD数据集相当大（近400万条记录）。我们将使用数据的一个较小版本，它是从整个数据中随机抽取的10%的子集。这将使我们的分析变得可行。当然，您可以通过下载完整数据并重新运行我们的实验来实验。
- en: 'First, we import the necessary Python libraries:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的Python库：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we set the paths for the locations of training and test data, as well
    as the paths to a label file that holds a header (names of features) for the data:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们设置了训练数据和测试数据的位置路径，以及包含数据特征（名称）标题的标签文件的路径：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we download the data and column names using the `wget` command through
    Python. As these files are zipped (compressed), we have to first extract the contents
    using the `gunzip` command. The following Python code snippet does that for us:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用Python中的`wget`命令下载数据和列名。由于这些文件是压缩的，我们必须首先使用`gunzip`命令提取内容。以下Python代码片段为我们完成了这项工作：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we construct a DataFrame from the downloaded streams:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们从下载的数据流中构建一个DataFrame：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This completes our step of downloading the data and creating a DataFrame from
    it. A DataFrame is a tabular data structure that will allow us to manipulate,
    slice and dice, and filter the data as needed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们下载数据并从中创建DataFrame的步骤。DataFrame是一种表格数据结构，它将允许我们按需操纵、切片、切块和过滤数据。
- en: Understanding the data
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解数据
- en: 'Once the data is downloaded, you can have a look at the DataFrame simply by
    printing the top five rows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据下载完成，您可以通过简单地打印前五行来查看DataFrame：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This should give you an output just like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给你一个类似以下输出的结果：
- en: '![Figure 2.3 – Top five rows from the NSL-KDD dataset](img/B19327_02_03.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3 – NSL-KDD数据集的前五行](img/B19327_02_03.jpg)'
- en: Figure 2.3 – Top five rows from the NSL-KDD dataset
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – NSL-KDD数据集的前五行
- en: 'As you can see, the top five rows of the data are displayed. The dataset has
    42 columns. The last column, named `target`, identifies the kind of network attack
    for every row in the data. To examine the distribution of network attacks (that
    is, how many examples of each kind of attack are present), we can run the following
    statement:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，数据显示了前五行。该数据集有42列。最后一列，命名为`target`，标识了数据中每一行的网络攻击类型。为了检查网络攻击的分布（即每种攻击类型的示例数量），我们可以运行以下语句：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will list all network attacks and the count (number of rows) for each
    attack, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将列出所有网络攻击及其计数（每条攻击的行数），如下所示：
- en: '![Figure 2.4 – Distribution of data by label (attack type)](img/B19327_02_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 标签（攻击类型）的数据分布](img/B19327_02_04.jpg)'
- en: Figure 2.4 – Distribution of data by label (attack type)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 标签（攻击类型）的数据分布
- en: We can see that there are a variety of attack types present in the data, with
    the `smurf` and `neptune` types accounting for the largest part. Next, we will
    look at how to model this data using statistical algorithms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据中存在多种攻击类型，其中`smurf`和`neptune`类型占据了最大部分。接下来，我们将探讨如何使用统计算法来建模这些数据。
- en: Statistical algorithms for intrusion detection
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入侵检测的统计算法
- en: Now that we have taken a look at the data, let us look at basic statistical
    algorithms that can help us isolate anomalies and thus identify intrusions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看过数据，让我们看看可以帮助我们隔离异常并因此识别入侵的基本统计算法。
- en: Univariate outlier detection
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单变量异常检测
- en: In the most basic form of anomaly detection, known as *univariate anomaly detection*,
    we build a model that considers the trends and detects anomalies based on a single
    feature at a time. We can build multiple such models, each operating on a single
    feature of the data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的异常检测形式，称为*单变量异常检测*中，我们构建一个模型，该模型考虑趋势，并基于每次一个特征来检测异常。我们可以构建多个这样的模型，每个模型操作数据的一个单独特征。
- en: z-score
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Z值
- en: 'This is the most fundamental method to detect outliers and a cornerstone of
    statistical anomaly detection. It is based on the **central limit theorem** (**CLT**),
    which says that in most observed distributions, data is clustered around the mean.
    For every data point, we calculate a *z-score* that indicates how far it is from
    the mean. Because absolute distances would depend on the scale and nature of data,
    we measure how many standard deviations away from the mean the point falls. If
    the mean and standard deviation of a feature are μ and σ respectively, the *z-score*
    for a point *x* is calculated as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是检测异常的最基本方法，也是统计异常检测的基石。它基于**中心极限定理**（**CLT**），该定理表明，在大多数观察到的分布中，数据都围绕着平均值聚集。对于每个数据点，我们计算一个*Z值*，以指示它距离平均值的远近。由于绝对距离会依赖于数据的规模和性质，我们测量点距离平均值的多少个标准差。如果一个特征的均值和标准差分别是μ和σ，那么点*x*的*Z值*计算如下：
- en: z =  x− μ _ σ
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: z = x− μ / σ
- en: The value of *z* is the number of standard deviations away from the mean that
    *x* falls. The CLT says that most data (99%) falls within two standard deviations
    (on either side) of the mean. Thus, the higher the value of *z*, the higher the
    chances of the point being an anomaly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z*的值是*x*相对于平均值的标准差数。中心极限定理（CLT）表明，大多数数据（99%）都位于平均值两侧两个标准差（任意一侧）的范围内。因此，*Z*的值越高，该点为异常点的可能性就越高。'
- en: 'Recall our defining characteristic of anomalies: they are rare and few in number.
    To simulate this setup, we sample from our dataset. We choose only those rows
    for which the target is either `normal` or `teardrop`. In this new dataset, the
    examples labeled `teardrop` are anomalies. We assign a label of `0` to the normal
    data points and `1` to the anomalous ones:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们定义异常的特征：它们是罕见的，数量很少。为了模拟这种设置，我们从数据集中采样。我们只选择那些目标为`正常`或`泪滴`的行。在这个新的数据集中，标记为`泪滴`的示例是异常。我们将`0`标签分配给正常数据点，将`1`标签分配给异常点：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As univariate outlier detection operates on only one feature at a time, let
    us choose `wrong_fragment` as the feature for demonstration. To calculate the
    *z-score* for every data point, we first calculate the mean and standard deviation
    of `wrong_fragment`. We then subtract the mean of the entire group from the `wrong_fragment`
    value in each row and divide it by the standard deviation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于单变量异常检测一次只操作一个特征，让我们选择`wrong_fragment`作为演示的特征。为了计算每个数据点的*Z值*，我们首先计算`wrong_fragment`的均值和标准差。然后，我们从每行的`wrong_fragment`值中减去整个组的均值，并除以标准差：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can plot the distribution of the *z-score* to visually discern the nature
    of the distribution. The following line of code can generate a density plot:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制*Z值*的分布，以直观地识别分布的性质。以下代码行可以生成密度图：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It should give you something like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该给出类似这样的结果：
- en: '![Figure 2.5 – Density plot of z-scores](img/B19327_02_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5 – Z值的密度图](img/B19327_02_05.jpg)'
- en: Figure 2.5 – Density plot of z-scores
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – Z值的密度图
- en: We see a sharp spike in the density around `0`, which indicates that most of
    the data points have a *z-score* around 0\. Also, notice the very small blip around
    `10`; these are the small number of outlier points that have high *z-scores*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`0`附近观察到密度的一个尖锐峰值，这表明大部分数据点都有一个大约为0的*Z值*。此外，注意`10`附近的非常小的波动；这些是具有高*Z值*的少量异常点。
- en: 'Now, all we have to do is filter out those rows that have a *z-score* of more
    than `2` or less than `-2`. We want to assign a label of `1` (predicted anomalies)
    to these rows, and `0` (predicted normal) to the others:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要过滤掉那些具有超过`2`或小于`-2`的*Z 分数*的行。我们希望将这些行的标签分配为`1`（预测异常），而将其他行的标签分配为`0`（预测正常）：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we have the actual and predicted labels for each row, we can evaluate the
    performance of our model using the confusion matrix (described earlier in [*Chapter
    1*](B19327_01.xhtml#_idTextAnchor013), *On Cybersecurity and Machine Learning*).
    Fortunately, the `scikit-learn` package in Python provides a very convenient built-in
    method that allows us to compute the matrix, and another package called `seaborn`
    allows us to quickly plot it. The code is illustrated in the following snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了每行的实际标签和预测标签，我们可以使用混淆矩阵（如[*第 1 章*](B19327_01.xhtml#_idTextAnchor013)“关于网络安全和机器学习”中所述）来评估我们模型的性能。幸运的是，Python
    中的 `scikit-learn` 包提供了一个非常方便的内置方法，允许我们计算矩阵，另一个名为 `seaborn` 的包允许我们快速绘制它。以下代码片段展示了代码：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will produce a confusion matrix as shown:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个如所示的混淆矩阵：
- en: '![Figure 2.6 – Confusion matrix](img/B19327_02_06.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 混淆矩阵](img/B19327_02_06.jpg)'
- en: Figure 2.6 – Confusion matrix
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 混淆矩阵
- en: Observe the confusion matrix carefully, and compare it with the skeleton confusion
    matrix from [*Chapter 1*](B19327_01.xhtml#_idTextAnchor013), *On Cybersecurity
    and Machine Learning*. We can see that our model is able to perform very well;
    all of the data points have been classified correctly. We have only true positives
    and negatives, and no false positives or false negatives.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察混淆矩阵，并将其与[*第 1 章*](B19327_01.xhtml#_idTextAnchor013)“关于网络安全和机器学习”中的骨架混淆矩阵进行比较。我们可以看到，我们的模型能够表现出色；所有数据点都被正确分类。我们只有真正的正例和负例，没有假正例或假负例。
- en: Elliptic envelope
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 椭圆包
- en: Elliptic envelope is an algorithm to detect anomalies in Gaussian data. At a
    high level, the algorithm models the data as a high-dimensional Gaussian distribution.
    The goal is to construct an ellipse covering most of the data—points falling outside
    the ellipse are anomalies or outliers. Statistical methods such as covariance
    matrices of features are used to estimate the size and shape of the ellipse.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 椭圆包是一种检测高斯数据中异常的算法。从高层次来看，该算法将数据建模为高维高斯分布。目标是构建一个覆盖大部分数据的椭圆——落在椭圆外的点是异常或离群值。使用特征协方差矩阵等统计方法来估计椭圆的大小和形状。
- en: 'The concept of elliptic envelope is easier to visualize in a two-dimensional
    space. A very idealized representation is shown in *Figure 2**.7*. The points
    colored blue are within the boundary of the ellipse and hence considered normal
    or benign. Points in red fall outside, and hence are anomalies:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 椭圆包的概念在二维空间中更容易可视化。*图 2.7*展示了非常理想化的表示。蓝色着色的点位于椭圆边界内，因此被认为是正常或良性的。红色点位于边界外，因此是异常：
- en: '![Figure 2.7 – How elliptic envelope detects anomalies](img/B19327_02_07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 椭圆包如何检测异常](img/B19327_02_07.jpg)'
- en: Figure 2.7 – How elliptic envelope detects anomalies
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 椭圆包如何检测异常
- en: Note that the axes are labeled **Dimension 1** and **Dimension 2**. These dimensions
    can be features you have extracted from your data; or, in the case of high-dimensional
    data, they might represent principal component features.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，坐标轴被标记为**维度 1**和**维度 2**。这些维度可以是您从数据中提取的特征；或者在高维数据的情况下，它们可能代表主成分特征。
- en: 'Implementing elliptic envelope as an anomaly detector in Python is straightforward.
    We will use the resampled data (consisting of only `normal` and `teardrop` data
    points) and drop the categorical features, as before:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中实现椭圆包作为异常检测器非常简单。我们将使用之前使用的重采样数据（仅包含`normal`和`teardrop`数据点）并丢弃分类特征：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The implementation of the algorithm is such that it produces `-1` if a point
    is outside the ellipse, and `1` if it is within. To be consistent with our ground
    truth labeling, we will reassign `-1` to `1` and `1` to `0`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的实现方式是，如果点位于椭圆外，则输出`-1`，如果位于椭圆内，则输出`1`。为了与我们的真实标签一致，我们将`-1`重新分配为`1`，将`1`重新分配为`0`：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Plot the confusion matrix as described previously. The result should be something
    like this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前面描述的方式绘制混淆矩阵。结果应该类似于以下内容：
- en: '![Figure 2.8 – Confusion matrix for elliptic envelope](img/B19327_02_08.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.8 – 椭圆包的混淆矩阵](img/B19327_02_08.jpg)'
- en: Figure 2.8 – Confusion matrix for elliptic envelope
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 椭圆包的混淆矩阵
- en: Note that this model has significant false positives and negatives. While the
    number may appear small, recall that our data had very few examples of the positive
    class (labeled 1) to begin with. The confusion matrix tells us that there were
    931 false negatives and only 48 true positives. This indicates that the model
    has extremely low precision and is unable to isolate anomalies properly.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个模型有显著的假阳性和假阴性。虽然数字可能看起来很小，但请记住，我们的数据最初正类（标记为1）的例子非常少。混淆矩阵告诉我们有931个假阴性，只有48个真阳性。这表明该模型具有极低的精确度，并且无法正确地隔离异常。
- en: Local outlier factor
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部离群因子
- en: '**Local outlier factor** (also known as **LOF**) is a density-based anomaly
    detection algorithm. It examines points in the local neighborhood of a point to
    detect whether that point is anomalous. While other algorithms consider a point
    with respect to the global data distribution, LOF considers only the local neighborhood
    and determines whether the point fits in. This is particularly useful to identify
    hidden outliers, which may be part of a cluster of points that is not an anomaly
    globally. Look at *Figure 2**.9*, for instance:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部离群因子**（也称为**LOF**）是一种基于密度的异常检测算法。它检查一个点的局部邻域中的点，以检测该点是否异常。虽然其他算法考虑的是点相对于全局数据分布的情况，但LOF只考虑局部邻域，并确定该点是否合适。这对于识别可能不是全局异常的点的簇中的隐藏异常特别有用。例如，看看*图
    2**.9*：'
- en: '![Figure 2.9 – Local outliers](img/B19327_02_09.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – 本地异常值](img/B19327_02_09.jpg)'
- en: Figure 2.9 – Local outliers
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 本地异常值
- en: In this figure, the yellow point is clearly an outlier. The points marked in
    red are not really outliers if you consider the entirety of the data. However,
    observe the neighborhood of the points; they are far away and stand apart from
    the local clusters they are in. Therefore, they are anomalies local to the area.
    LOF can detect such local anomalies in addition to global ones.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，黄色的点显然是一个异常值。用红色标记的点，如果你考虑整个数据集，它们并不是真正的异常值。然而，观察这些点的邻域；它们离得很远，并且与它们所在的局部簇相分离。因此，它们是该区域局部的异常。LOF可以检测这样的局部异常，除了全局异常。
- en: 'In brief, the algorithm works as follows. For every point *P*, we do the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，算法的工作原理如下。对于每个点*P*，我们执行以下操作：
- en: Compute the distances from *P* to every other point in the data. This distance
    can be computed using a metric called *Manhattan distance*. If (x1, y1) and (x2,
    y2) are two distinct points, the Manhattan distance between them is |x1 – x2|
    + |y1 – y2|. This can be generalized to multiple dimensions.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从*P*到数据中每个其他点的距离。这个距离可以使用称为*曼哈顿距离*的度量来计算。如果(x1, y1)和(x2, y2)是两个不同的点，它们之间的曼哈顿距离是|x1
    – x2| + |y1 – y2|。这可以推广到多个维度。
- en: Based on the distance, calculate the *K* closest points. This is the neighborhood
    of point *P*.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据距离，计算*K*个最近点。这是点*P*的邻域。
- en: Calculate the local reachability density, which is nothing but the inverse of
    the average distance between *P* and the *K* closest points. The local reachability
    density measures how close the neighborhood points are to *P*. A smaller value
    of density indicates that *P* is far away from its neighbors.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算局部可达密度，这实际上就是*P*与*K*个最近点之间的平均距离的倒数。局部可达密度衡量邻域点与*P*的接近程度。密度值越小，表示*P*离其邻居越远。
- en: Finally, calculate the LOF. This is the sum of distances from *P* to the neighboring
    points weighted by the sum of densities of the neighborhood points.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算LOF。这是从*P*到邻近点的距离之和，并按邻域点密度之和加权。
- en: Based on the LOF, we can determine whether *P* represents an anomaly in the
    data or not.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于LOF（局部离群因子），我们可以确定*P*是否代表数据中的异常。
- en: A high LOF value indicates that *P* is far from its neighbors and the neighbors
    have high densities (that is, they are close to their neighbors). This means that
    *P* is a local outlier in its neighborhood.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 高LOF值表示*P*远离其邻居，并且邻居的密度很高（即，它们靠近它们的邻居）。这意味着*P*是其邻域中的局部异常值。
- en: A low LOF value indicates that either *P* is far from the neighbors or that
    neighbors possibly have low densities themselves. This means that it is not an
    outlier in the neighborhood.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 低LOF值表示*P*远离邻居，或者邻居本身的密度可能很低。这意味着它在该邻域中不是异常值。
- en: Note that the performance of our model here depends on the selection of *K*,
    the number of neighbors to form the neighborhood. If we set *K* to be too high,
    we would basically be looking for outliers at the global dataset level. This would
    lead to false positives because points that are in a cluster (so not locally anomalous)
    far away from the high-density regions would also be classified as anomalies.
    On the other hand, if we set *K* to be very small, our neighborhood would be very
    sparse and we would be looking for anomalies with respect to very small regions
    of points, which would also lead to misclassification.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们模型在这里的性能取决于邻居数量*K*的选择，这是形成邻域的数量。如果我们把*K*设置得过高，我们基本上会在全局数据集级别上寻找异常值。这会导致假阳性，因为远离高密度区域的簇内点（所以不是局部异常）也会被分类为异常。另一方面，如果我们把*K*设置得非常小，我们的邻域会非常稀疏，我们就会在非常小的点区域中寻找异常，这也会导致误分类。
- en: 'We can try this out in Python using the built-in off-the-shelf implementation.
    We will use the same data and features that we used before:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Python中使用内置的现成实现尝试这个。我们将使用之前使用过的相同数据和特征：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After rescoring the predicted labels for consistency as described before, we
    can plot the confusion matrix. You should see something like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在如前所述重新评分预测标签以保持一致性之后，我们可以绘制混淆矩阵。你应该会看到类似这样的：
- en: '![Figure 2.10 – Confusion matrix for LOF with K = 5](img/B19327_02_10.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10 – 使用K = 5的LOF混淆矩阵](img/B19327_02_10.jpg)'
- en: Figure 2.10 – Confusion matrix for LOF with K = 5
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 使用K = 5的LOF混淆矩阵
- en: 'Clearly, the model has an extremely high number of false negatives. We can
    examine how the performance changes by changing the value of *K*. Note that we
    first picked a very small value. If we rerun the same code with *K* = 250, we
    get the following confusion matrix:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，模型有极多的假阴性。我们可以通过改变*K*的值来检查性能如何变化。注意，我们首先选择了一个非常小的值。如果我们用*K* = 250重新运行相同的代码，我们得到以下混淆矩阵：
- en: '![Figure 2.11 – Confusion matrix for LOF with K = 250](img/B19327_02_11.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图2.11 – 使用K = 250的LOF混淆矩阵](img/B19327_02_11.jpg)'
- en: Figure 2.11 – Confusion matrix for LOF with K = 250
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – 使用K = 250的LOF混淆矩阵
- en: This second model is slightly better than the first. To find the best *K* value,
    we can try doing this over all possible values of *K*, and observe how our metrics
    change. We will vary *K* from 100 to 10,000, and for each iteration, we will calculate
    the accuracy, precision, and recall. We can then plot the trends in metrics with
    increasing *K* to check which one shows the best performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模型略优于第一个。为了找到最佳的*K*值，我们可以尝试对所有可能的*K*值进行测试，并观察我们的指标如何变化。我们将从100到10,000变化*K*，并对每个迭代计算准确率、精确率和召回率。然后我们可以绘制随着*K*增加的指标趋势图，以检查哪个表现最好。
- en: The complete code listing for this is shown next. First, we define empty lists
    that will hold our measurements (accuracy, precision, and recall) for each value
    of *K* that we test. We then fit an LOF model and compute the confusion matrix.
    Recall the definition of a confusion matrix from [*Chapter 1*](B19327_01.xhtml#_idTextAnchor013),
    *On Cybersecurity and Machine Learning*, and note which entries of the matrix
    define the true positives, false positives, and false negatives.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码列表展示了完整的代码。首先，我们定义空列表，将保存我们对每个测试的*K*值的测量（准确率、精确率和召回率）。然后我们拟合一个LOF模型并计算混淆矩阵。回想一下[*第一章*](B19327_01.xhtml#_idTextAnchor013)中关于网络安全和机器学习的混淆矩阵的定义，并注意矩阵中的哪些条目定义了真正的正例、假正例和假负例。
- en: 'Using the matrix, we compute accuracy, precision, and recall, and record them
    in the arrays. Note the calculation of the precision and recall; we deviate from
    the formula slightly by adding `1` to the denominator. Why do we do this? In extreme
    cases, we will have zero true or false positives, and we do not want the denominator
    to be `0` in order to avoid a division-by-zero error:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该矩阵，我们计算准确率、精确率和召回率，并将它们记录在数组中。注意精确率和召回率的计算；我们通过在分母中添加`1`稍微偏离了公式。我们为什么要这样做？在极端情况下，我们可能会有零真正的或错误的正例，我们不希望分母为`0`以避免除以零错误：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once complete, we can plot the three series to show how the value of *K* affects
    the metrics. We can do this using `matplotlib`, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们可以绘制三个序列来展示*K*的值如何影响指标。我们可以使用`matplotlib`这样做，如下所示：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is what the output looks like:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输出看起来像什么：
- en: '![Figure 2.12 – Accuracy, precision, and recall trends with K](img/B19327_02_12.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图2.12 – 随K变化的准确率、精确率和召回率趋势](img/B19327_02_12.jpg)'
- en: Figure 2.12 – Accuracy, precision, and recall trends with K
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 随K变化的准确率、精确率和召回率趋势
- en: We see that while accuracy and recall have remained more or less similar, the
    value of precision shows a declining trend as we increase *K*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，虽然准确率和召回率大致相似，但随着*K*的增加，精确率的值显示出下降趋势。
- en: This completes our discussion of statistical measures and methods for anomaly
    detection and their application to intrusion detection. In the next section, we
    will look at a few advanced unsupervised methods for doing the same.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对异常检测的统计量和方法的讨论，以及它们在入侵检测中的应用。在下一节中，我们将探讨一些用于执行相同任务的先进无监督方法。
- en: Machine learning algorithms for intrusion detection
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入侵检测的机器学习算法
- en: This section will cover ML methods such as clustering, autoencoders, SVM, and
    isolation forests, which can be used for anomaly detection.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍如聚类、自编码器、SVM和隔离森林等ML方法，这些方法可用于异常检测。
- en: Density-based scan (DBSCAN)
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的扫描（DBSCAN）
- en: In the previous chapter where we introduced **unsupervised ML** (**UML**), we
    studied the concept of clustering via the K-Means clustering algorithm. However,
    recall that *K* is a hyperparameter that has to be set manually; there is no good
    way to know the ideal number of clusters in advance. **DBSCAN** is a density-based
    clustering algorithm that does not need a pre-specified number of clusters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了**无监督机器学习**（**UML**），我们通过K-Means聚类算法研究了聚类的概念。然而，请记住*K*是一个超参数，必须手动设置；没有好的方法可以事先知道理想簇的数量。**DBSCAN**是一种基于密度的聚类算法，不需要预先指定的簇数量。
- en: 'DBSCAN hinges on two parameters: the minimum number of points required to call
    a group of points a cluster and ξ (which specifies the minimum distance between
    two points to call them neighbors). Internally, the algorithm classifies every
    data point as being from one of the following three categories:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN依赖于两个参数：将一组点称为簇所需的最小点数和ξ（它指定了两个点之间的最小距离，以将它们称为邻居）。在内部，算法将每个数据点分类为以下三个类别之一：
- en: '**Core points** are those that have at least the minimum number of points in
    the neighborhood defined by a circle of radius ξ'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核心点**是指在半径为ξ的圆内至少有最小数量的点的那些点'
- en: '**Border points**, which are not core points, but in the neighborhood area
    or cluster of a core point described previously'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界点**，不是核心点，但位于之前描述的核心点的邻域区域或簇中'
- en: An **anomaly point**, which is neither a core point nor reachable from one (that
    is, not a border point either)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**异常点**，既不是核心点，也不能从其中一个点到达（也就是说，也不是边界点）
- en: 'In a nutshell, the process works as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个过程如下所示：
- en: Set the values for the parameters, the minimum number of points, and ξ.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置参数值、最小点数和ξ。
- en: Choose a starting point (say, *A*) at random, and find all points at a distance
    of ξ or less from this point.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个起点（比如说，*A*），并找到距离该点ξ或更近的所有点。
- en: If the number of points meets the threshold for the minimum number of points,
    then *A* is a core point, and a cluster can form around it. All the points at
    a distance of ξ or less are border points and are added to the cluster centered
    on *A*.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果点的数量达到最小点数阈值，则*A*是一个核心点，并且可以围绕它形成一个簇。距离ξ或更近的所有点都是边界点，并添加到以*A*为中心的簇中。
- en: Repeat these steps for each point. If a point (say, *B*) added to a cluster
    also turns out to be a core point, we first form its own cluster and then merge
    it with the original cluster around *A*.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个点重复这些步骤。如果一个点（比如说，*B*）被添加到簇中，结果也证明它是一个核心点，我们首先形成它自己的簇，然后将其与以*A*为中心的原始簇合并。
- en: 'Thus, DBSCAN is able to identify high-density regions in the feature space
    and group points into clusters. A point that does not fall within a cluster is
    determined to be anomalous or an outlier. DBSCAN offers two powerful advantages
    over the K-Means clustering discussed earlier:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DBSCAN能够识别特征空间中的高密度区域并将点分组到簇中。不在簇中的点被确定为异常或离群点。DBSCAN比之前讨论的K-Means聚类具有两个强大的优势：
- en: We do not need to specify the number of clusters. Oftentimes, when analyzing
    data, and especially in unsupervised settings, we may not be aware of the number
    of classes in the data. We, therefore, do not know what the number of clusters
    should be for anomaly detection. DBSCAN eliminates this problem and forms clusters
    appropriately based on density.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要指定簇的数量。在分析数据时，尤其是在无监督设置中，我们可能不知道数据中的类别数量。因此，我们不知道异常检测应该有多少个簇。DBSCAN消除了这个问题，并基于密度适当地形成簇。
- en: DBSCAN is able to find clusters that have arbitrary shapes. Because it is based
    on the density of regions around the core points, the shape of the clusters need
    not be circular. K-Means clustering, on the other hand, cannot detect overlapping
    or arbitrary-shaped clusters.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN能够找到具有任意形状的簇。因为它基于核心点周围的区域密度，所以簇的形状不必是圆形的。另一方面，K-Means聚类无法检测重叠或任意形状的簇。
- en: 'As an example of the arbitrary shape clusters that DBSCAN forms, consider the
    following figure. The color of the points represents the ground-truth labels of
    the classes they belong to. If K-Means is used, circularly shaped clusters are
    formed that do not separate the two classes. On the other hand, if we use DBSCAN,
    it is able to properly identify clusters based on density and separate the two
    classes in a much better manner:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作为DBSCAN形成任意形状簇的例子，考虑以下图。点的颜色代表它们所属类别的真实标签。如果使用K-Means，则会形成圆形簇，这并不能将两个类别分开。另一方面，如果我们使用DBSCAN，它能够根据密度正确地识别簇，并且以更好的方式将两个类别分开：
- en: '![Figure 2.13 – How K-Means (left) and DBSCAN (right) would perform for irregularly
    shaped clusters](img/B19327_02_13.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13 – K-Means（左）和DBSCAN（右）在处理不规则形状簇时的表现](img/B19327_02_13.jpg)'
- en: Figure 2.13 – How K-Means (left) and DBSCAN (right) would perform for irregularly
    shaped clusters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – K-Means（左）和DBSCAN（右）在处理不规则形状簇时的表现
- en: 'The Python implementation of DBSCAN is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN的Python实现如下：
- en: '[PRE17]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After rescoring the labels, we can plot the confusion matrix. You should see
    something like this:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新评分标签后，我们可以绘制混淆矩阵。你应该会看到类似这样的：
- en: '![Figure 2.14 – Confusion matrix for DBSCAN](img/B19327_02_14.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图2.14 – DBSCAN的混淆矩阵](img/B19327_02_14.jpg)'
- en: Figure 2.14 – Confusion matrix for DBSCAN
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – DBSCAN的混淆矩阵
- en: This presents an interesting confusion matrix. We see that all of the outliers
    are being detected—but along with it, a large number of inliers are also being
    wrongly classified as outliers. This is a classic case of low precision and high
    recall.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这呈现了一个有趣的混淆矩阵。我们看到所有异常值都被检测到了——但随之而来的是，大量内点也被错误地分类为异常值。这是一个低精确度和高召回率的经典案例。
- en: 'We can experiment with how this model performs as we vary its two parameters.
    For example, if we run the same block of code with `minimum_samples = 1000` and
    `epsilon = 0.8`, we will get the following confusion matrix:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试如何通过改变其两个参数来测试这个模型的性能。例如，如果我们用`minimum_samples = 1000`和`epsilon = 0.8`运行相同的代码块，我们将得到以下混淆矩阵：
- en: '![Figure 2.15 – Confusion matrix for DBSCAN](img/B19327_02_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图2.15 – DBSCAN的混淆矩阵](img/B19327_02_15.jpg)'
- en: Figure 2.15 – Confusion matrix for DBSCAN
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – DBSCAN的混淆矩阵
- en: This model is worse than the previous one and is an extreme case of low precision
    and high recall. Everything is predicted to be an outlier.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型比之前的模型更差，是低精确度和高召回率的极端案例。所有预测都被认为是异常值。
- en: 'What happens if you set `epsilon` to a high value—say, `35`? You end up with
    the following confusion matrix:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将`epsilon`设置为高值——比如说，`35`？你最终会得到以下混淆矩阵：
- en: '![Figure 2.16 – Confusion matrix for improved DBSCAN](img/B19327_02_16.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16 – 改进后的DBSCAN的混淆矩阵](img/B19327_02_16.jpg)'
- en: Figure 2.16 – Confusion matrix for improved DBSCAN
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 改进后的DBSCAN的混淆矩阵
- en: Somewhat better than before! You can experiment with other values of the parameters
    to find out what works best.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 比之前有所改善！你可以尝试其他参数值来找出什么效果最好。
- en: One-class SVM
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单类支持向量机
- en: '**Support vector machine** (**SVM**) is an algorithm widely used for classification
    tasks. **One-class SVM** (**OC-SVM**) is the version used for anomaly detection.
    However, before we turn to OC-SVM, it would be helpful to have a primer into what
    SVM is and how it actually works.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）是一种广泛用于分类任务的算法。**单类支持向量机**（**OC-SVM**）是用于异常检测的版本。然而，在我们转向OC-SVM之前，了解SVM是什么以及它是如何实际工作的会有所帮助。'
- en: Support vector machines
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: The fundamental goal of SVM is to calculate the optimal decision boundary between
    two classes, also known as the separating hyperplane. Data points are classified
    into a category depending on which side of the hyperplane or decision boundary
    they fall on.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的基本目标是计算两个类别之间的最优决策边界，也称为分离超平面。数据点根据它们落在超平面或决策边界的哪一侧被分类到某个类别。
- en: 'For example, if we consider points in two dimensions, the orange and yellow
    lines as shown in the following figure are possible hyperplanes. Note that the
    visualization here is straightforward because we have only two dimensions. For
    3D data, the decision boundary will be a plane. For *n*-dimensional data, the
    decision boundary will be an *n-1*-dimensional hyperplane:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们考虑二维空间中的点，如图中所示，橙色和黄色线是可能超平面。请注意，这里的可视化很简单，因为我们只有两个维度。对于三维数据，决策边界将是一个平面。对于*n*-维数据，决策边界将是一个*n-1*-维超平面：
- en: '![Figure 2.17 – OC-SVM](img/B19327_02_17.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17 – OC-SVM](img/B19327_02_17.jpg)'
- en: Figure 2.17 – OC-SVM
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 – OC-SVM
- en: 'We can see that in *Figure 2**.17*, both the orange and yellow lines are possible
    hyperplanes. But how does the SVM choose the best hyperplane? It evaluates all
    possible hyperplanes for the following criteria:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图2**.17*中看到这一点。橙色和黄色线都是可能的超平面。但SVM是如何选择最佳超平面的呢？它会根据以下标准评估所有可能超平面：
- en: How well does this hyperplane separate the two classes?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个超平面将两个类别分得多好？
- en: What is the smallest distance between data points (on either side) and the hyperplane?
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点（在任一侧）与超平面之间的最小距离是多少？
- en: Ideally, we want a hyperplane that best separates the two classes and has the
    largest distance to the data points in either class.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望找到一个超平面，它最好地分离两个类别，并且与两个类别中的数据点都有最大的距离。
- en: 'In some cases, points may not be linearly separable. SVM employs the *kernel
    trick*; using a kernel function, the points are projected to a higher-dimension
    space. Because of the complex transformations, points that are not linearly separable
    in their original low-dimensional space may become separable in a higher-dimensional
    space. *Figure 2**.18* (drawn from [https://sebastianraschka.com/faq/docs/select_svm_kernels.html](https://sebastianraschka.com/faq/docs/select_svm_kernels.html))
    shows an example of a kernel transformation. In the original two-dimensional space,
    the classes are not linearly separable by a hyperplane. However, when projected
    into three dimensions, they become linearly separable. The hyperplane calculated
    in three dimensions is mapped back to the two-dimensional space in order to make
    predictions:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，点可能不是线性可分的。SVM使用*核技巧*；通过使用核函数，点被投影到更高维的空间。由于复杂的变换，原始低维空间中不可线性分离的点可能在更高维空间中变得可分离。*图2**.18*（来自[https://sebastianraschka.com/faq/docs/select_svm_kernels.html](https://sebastianraschka.com/faq/docs/select_svm_kernels.html)）展示了核变换的一个例子。在原始二维空间中，类别不能通过超平面线性分离。然而，当投影到三维空间时，它们变得线性可分。在三维空间中计算的超平面被映射回二维空间以进行预测：
- en: '![Figure 2.18 – Kernel transformations](img/B19327_02_18.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图2.18 – 核变换](img/B19327_02_18.jpg)'
- en: Figure 2.18 – Kernel transformations
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 – 核变换
- en: SVMs are effective in high-dimensional feature spaces and are also memory efficient
    since they use a small number of points for training.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: SVM在高维特征空间中非常有效，并且由于它们使用少量点进行训练，因此内存效率也很高。
- en: The OC-SVM algorithm
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OC-SVM算法
- en: Now that we have a sufficient background into what SVM is and how it works,
    let us discuss OC-SVM and how it can be used for anomaly detection.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对SVM是什么以及它是如何工作的有了足够的背景知识，让我们来讨论OC-SVM以及它是如何用于异常检测的。
- en: OC-SVM has its foundations in the concepts of **Support Vector Data Description**
    (also known as **SVDD**). While SVM takes a planar approach, the goal in SVDD
    is to build a *hypersphere* enclosing the data points. Note that as this is anomaly
    detection, we have no labels. We construct the hypersphere and optimize it to
    be as small as possible. The hypothesis behind this is that outliers will be removed
    from the regular points, and will hence fall outside the spherical boundary.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: OC-SVM基于**支持向量数据描述**（也称为**SVDD**）的概念。虽然SVM采用平面方法，但SVDD的目标是构建一个包围数据点的*超球体*。请注意，由于这是异常检测，我们没有标签。我们构建超球体并优化它以尽可能小。背后的假设是异常值将从常规点中移除，因此将落在球面边界之外。
- en: For every point, we calculate the distance to the center of the sphere. If the
    distance is less than the radius, the point falls inside the sphere and is benign.
    If it is greater, the point falls outside the sphere and is hence classified as
    an anomaly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个点，我们计算其到球心距离。如果距离小于半径，该点位于球内，因此是良性的。如果距离大于半径，该点位于球外，因此被分类为异常。
- en: 'The OC-SVM algorithm can be implemented in Python as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: OC-SVM 算法可以用以下方式在 Python 中实现：
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Plotting the confusion matrix, this is what we see:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制混淆矩阵，我们看到如下：
- en: '![Figure 2.19 – Confusion matrix for OC-SVM](img/B19327_02_19.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.19 – OC-SVM 的混淆矩阵](img/B19327_02_19.jpg)'
- en: Figure 2.19 – Confusion matrix for OC-SVM
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19 – OC-SVM 的混淆矩阵
- en: Okay—this seems to be an improvement. While our true positives have decreased,
    so have our false positives—and by a lot.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧——这似乎是一个改进。虽然我们的真正阳性减少了，但我们的假阳性也减少了——而且减少了很多。
- en: Isolation forest
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隔离森林
- en: In order to understand what an isolation forest is, it is necessary to have
    an overview of decision trees and random forests.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解隔离森林是什么，有必要对决策树和随机森林有一个概述。
- en: Decision trees
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree is an ML algorithm that creates a hierarchy of rules in order
    to classify a data point. The leaf nodes of a tree represent the labels for classification.
    All internal nodes (non-leaf) represent rules. For every possible result of the
    rule, a different child is defined. Rules are such that outputs are generally
    binary in nature. For continuous features, the rule compares the feature with
    some value. For example, in our fraud detection decision tree, `Amount > 10,000`
    is a rule that has outputs as `1` (yes) or `0` (no). In the case of categorical
    variables, there is no ordering, and so a greater-than or less-than comparison
    is meaningless. Rules for categorical features check for membership in sets. For
    example, if there is a rule involving the day of the week, it can check whether
    the transaction fell on a weekend using the *Day € {Sat,* *Sun}* rule.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种机器学习算法，它创建一个规则层次结构以对数据点进行分类。树的叶子节点代表分类的标签。所有内部节点（非叶子节点）代表规则。对于规则的每一个可能结果，定义一个不同的子节点。规则通常是二进制输出的。对于连续特征，规则将特征与某个值进行比较。例如，在我们的欺诈检测决策树中，`Amount
    > 10,000` 是一个输出为 `1`（是）或 `0`（否）的规则。在分类变量的情况下，没有顺序，因此大于或小于比较是没有意义的。分类特征的规则检查集合中的成员资格。例如，如果有一个涉及一周中某天的规则，它可以检查交易是否发生在周末，使用
    *Day € {Sat, Sun}* 规则。
- en: The tree defines the set of rules to be used for classification; we start at
    the root node and traverse the tree depending on the output of our rules.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 树定义了用于分类的规则集；我们从根节点开始，根据规则的输出遍历树。
- en: A random forest is a collection of multiple decision trees, each trained on
    a different subset of data and hence having a different structure and different
    rules. While making a prediction for a data point, we run the rules in each tree
    independently and choose the predicted label by a majority vote.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一组多个决策树，每个决策树在不同的数据子集上训练，因此具有不同的结构和不同的规则。在为数据点做出预测时，我们独立地在每棵树上运行规则，并通过多数投票选择预测标签。
- en: The isolation forest algorithm
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隔离森林算法
- en: Now that we have set a fair bit of background on random forests, let us turn
    to isolation forests. An isolation forest is an unsupervised anomaly detection
    algorithm. It is based on the underlying definition of anomalies; anomalies are
    rare occurrences and deviate significantly from normal data points. Because of
    this, if we process the data in a tree-like structure (similar to what we do for
    decision trees), the non-anomalous points will require more and more rules (which
    means traversing more deeply into the tree) to be classified, as they are all
    similar to each other. Anomalies can be detected based on the path length a data
    point takes from the root of the tree.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对随机森林的背景知识做了很多介绍，让我们转向隔离森林。隔离森林是一种无监督的异常检测算法。它基于异常的潜在定义；异常是罕见发生且与正常数据点显著偏离的事件。因此，如果我们以树状结构（类似于我们对决策树所做的那样）处理数据，非异常点需要越来越多的规则（这意味着在树中深入遍历）来分类，因为它们彼此之间都很相似。异常可以根据数据点从树根到路径的长度来检测。
- en: 'First, we construct a set of *isolation trees* similar to decision trees, as
    follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们构建一组类似于决策树的 *隔离树*，如下所示：
- en: Perform a random sampling over the data to obtain a subset to be used for training
    an isolation tree.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行随机抽样以获得用于训练隔离树的子集。
- en: Select a feature from the set of features available.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从可用的特征集中选择一个特征。
- en: Select a random threshold for the feature (if it is continuous), or a random
    membership test (if it is categorical).
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为特征选择一个随机阈值（如果它是连续的），或一个随机成员测试（如果它是分类的）。
- en: Based on the rule created in *step 3*, data points will be assigned to either
    the left branch or the right branch of the tree.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 *步骤 3* 中创建的规则，数据点将被分配到树的左侧分支或右侧分支。
- en: Repeat *steps 2-4* recursively until each data point is isolated (in a leaf
    node by itself).
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归重复 *步骤 2-4*，直到每个数据点被孤立（单独位于叶节点）。
- en: 'After the isolation trees have been constructed, we have a trained isolation
    forest. In order to run inferencing, we use an ensemble approach to examine the
    path length required to isolate a particular point. Points that can be isolated
    with the fewest number of rules (that is, closer to the root node) are more likely
    to be anomalies. If we have *n* training data points, the anomaly score for a
    point *x* is calculated as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在隔离树构建完成后，我们有一个训练好的隔离森林。为了进行推理，我们使用集成方法来检查隔离特定点所需的路径长度。可以用最少规则（即接近根节点）隔离的点更有可能是异常点。如果我们有
    *n* 个训练数据点，点 *x* 的异常分数计算如下：
- en: s(x, n) = 2 − E(h(x)) _ c(n)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: s(x, n) = 2 − E(h(x)) _ c(n)
- en: 'Here, *h(x)* represents the path length (number of edges of the tree traversed
    until the point *x* is isolated). *E(h(x))*, therefore, represents the expected
    value or mean of all the path lengths across multiple trees in the isolation forest.
    The constant *c(n)* is the average path length of an unsuccessful search in a
    binary search tree; we use it to normalize the expected value of *h(x)*. It is
    dependent on the number of training examples and can be calculated using the harmonic
    number *H(n)* as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h(x)* 代表路径长度（在到达点 *x* 被孤立之前树遍历的边数）。因此，*E(h(x))* 代表隔离森林中多个树中所有路径长度的期望值或平均值。常数
    *c(n)* 是在二叉搜索树中未成功搜索的平均路径长度；我们用它来标准化 *h(x)* 的期望值。它依赖于训练样本的数量，可以使用调和数 *H(n)* 计算如下：
- en: c(n) = 2H(n − 1) − 2(n − 1) / n
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: c(n) = 2H(n − 1) − 2(n − 1) / n
- en: The value of s(x, n) is used to determine whether the point *x* is anomalous
    or not. Higher values closer to `1` indicate that the points are anomalies, and
    smaller values indicate that they are normal.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: s(x, n) 的值用于确定点 *x* 是否是异常点。接近 `1` 的较高值表示点是异常点，较小的值表示它们是正常的。
- en: The `scikit-learn` package provides us with an efficient implementation for
    an isolation forest so that we don’t have to do any of the hard work ourselves.
    We simply fit a model and use it to make predictions. For simplicity of our analysis,
    let us use only the continuous-valued variables, and ignore the categorical string
    variables for now.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 包为我们提供了一个隔离森林的高效实现，这样我们就不必自己进行任何困难的工作。我们只需拟合一个模型并使用它进行预测。为了分析简单，让我们只使用连续值变量，并暂时忽略分类字符串变量。'
- en: 'First, we will use the original DataFrame we constructed and select the columns
    of interest to us. Note that we must record the labels in a list beforehand since
    labels cannot be used during model training. We then fit an isolation forest model
    on our features and use it to make predictions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用我们构建的原始 DataFrame 并选择我们感兴趣的列。请注意，我们必须事先记录标签，因为在模型训练期间不能使用标签。然后我们在我们的特征上拟合一个隔离森林模型，并使用它进行预测：
- en: '[PRE19]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `predict` function here calculates the anomaly score and returns a prediction
    based on that score. A prediction of `-1` indicates that the example is determined
    to be anomalous, and that of `1` indicates that it is not. Recall that our actual
    labels are in the form of `0` and `1`, not `-1` and `1`. For an apples-to-apples
    comparison, we will recode the predicted labels, replacing `1` with `0` and `-1`
    with `1`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `predict` 函数计算异常分数，并根据该分数返回预测。预测值为 `-1` 表示示例被确定为异常，预测值为 `1` 表示不是。回想一下，我们的实际标签是
    `0` 和 `1` 的形式，而不是 `-1` 和 `1`。为了进行苹果对苹果的比较，我们将重新编码预测标签，将 `1` 替换为 `0`，将 `-1` 替换为
    `1`：
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we can plot a confusion matrix using the actual and predicted labels,
    in the same way that we did before. On doing so, you should end up with the following
    plot:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用实际和预测标签绘制混淆矩阵，就像我们之前做的那样。这样做后，你应该得到以下图表：
- en: '![Figure 2.20 – Confusion matrix for isolation forest](img/B19327_02_20.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.20 – 隔离森林的混淆矩阵](img/B19327_02_20.jpg)'
- en: Figure 2.20 – Confusion matrix for isolation forest
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20 – 隔离森林的混淆矩阵
- en: We see that while this model predicts the majority of benign classes correctly,
    it also has a significant chunk of false positives and negatives.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，虽然这个模型正确预测了大多数良性类别，但它也有一大部分的误报和漏报。
- en: Autoencoders
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器
- en: '**Autoencoders** (**AEs**) are **deep neural networks** (**DNNs**) that can
    be used for anomaly detection. As this is not an introductory book, we expect
    you to have some background and a preliminary understanding of **deep learning**
    (**DL**) and how neural networks work. As a refresher, we will present some basic
    concepts here. This is not meant to be an exhaustive tutorial on neural networks.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**（**AEs**）是**深度神经网络**（**DNNs**），可用于异常检测。由于这不是一本入门书籍，我们假设您有一些背景知识和对**深度学习**（**DL**）以及神经网络如何工作的初步了解。作为复习，我们将在下面介绍一些基本概念。这并不是一个关于神经网络的全面教程。'
- en: A primer on neural networks
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络入门
- en: Let us now go through a few basics of neural networks, how they are formed,
    and how they work.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在来探讨一些神经网络的基本知识，它们是如何形成的，以及它们是如何工作的。
- en: Neural networks – structure
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络 – 结构
- en: The fundamental building block of a neural network is a *neuron*. This is a
    computational unit that takes in multiple numeric inputs and applies a mathematical
    transformation on it to produce an output. Each input to a neuron has a *weight*
    associated with it. The neuron first calculates a weighted sum of the inputs and
    then applies an *activation function* that transforms this sum into an output.
    Weights represent the parameters of our neural network; training a model means
    essentially finding the optimum values of the weights such that the classification
    error is reduced.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的基本构建块是*神经元*。这是一个计算单元，它接收多个数值输入，并对它们应用数学变换以产生输出。每个神经元的输入都有一个与之相关的*权重*。神经元首先计算输入的加权和，然后应用一个*激活函数*，将这个和转换成输出。权重代表我们神经网络的参数；训练模型本质上意味着找到权重的最优值，以减少分类误差。
- en: 'A sample neuron is depicted in *Figure 2**.21*. It can be generalized to a
    neuron with any number of inputs, each having its own weight. Here, σ represents
    the activation function. This determines how the weighted sum of inputs is transformed
    into an output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图2.21*中描述了一个示例神经元。它可以推广到具有任意数量输入的神经元，每个输入都有自己的权重。在这里，σ代表激活函数。这决定了如何将输入的加权和转换成输出：
- en: '![Figure 2.21 – Basic structure of a neuron](img/B19327_02_21.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图2.21 – 神经元的基本结构](img/B19327_02_21.jpg)'
- en: Figure 2.21 – Basic structure of a neuron
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 – 神经元的基本结构
- en: A group of neurons together form a *layer* of neurons, and multiple such layers
    connected together form a neural network. The more the number of layers, the *deeper*
    the network is said to be. Input data (in the form of features) is fed to the
    first layer. In the simplest form of neural networks, every neuron in a layer
    is connected to every neuron in the next layer; this is known as a **fully connected
    neural** **network** (**FCNN**).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一组神经元共同构成一个*层*，多个这样的层连接在一起形成一个神经网络。层的数量越多，网络就被说成是越*深*。输入数据（以特征的形式）被馈送到第一层。在神经网络的最简单形式中，一个层中的每个神经元都与下一层的每个神经元相连；这被称为**全连接神经网络**（**FCNN**）。
- en: The final layer is the output layer. In the case of binary classification, the
    output layer has only one neuron with a *sigmoid* activation function. The output
    of this neuron indicates the probability of the data point belonging to the positive
    class. If it is a multiclass classification problem, then the final layer contains
    as many neurons as the number of classes, each with a *softmax* activation. The
    outputs are normalized such that each one represents the probability of the input
    belonging to a particular class, and they all add up to 1\. All of the layers
    other than the input and output are not visible from the outside; they are known
    as *hidden layers*.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层是输出层。在二分类的情况下，输出层只有一个具有*sigmoid*激活函数的神经元。这个神经元的输出表示数据点属于正类的概率。如果是一个多分类问题，那么最终层包含与类别数量相等的神经元，每个神经元都有一个*softmax*激活。输出被归一化，使得每个输出代表输入属于特定类别的概率，并且它们的总和为1。除了输入层和输出层之外的所有层从外部是不可见的；它们被称为*隐藏层*。
- en: 'Putting all of this together, *Figure 2**.22* shows the structure of a neural
    network:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，*图2.22*显示了神经网络的架构：
- en: '![Figure 2.22 – A neural network](img/B19327_02_22.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图2.22 – 一个神经网络](img/B19327_02_22.jpg)'
- en: Figure 2.22 – A neural network
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 – 一个神经网络
- en: 'Let us take a quick look at four commonly used activation functions: sigmoid,
    tanh, **rectified linear unit** (**ReLU**), and softmax.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览四种常用的激活函数：sigmoid、tanh、**修正线性单元**（**ReLU**）和softmax。
- en: Neural networks – activation functions
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络 – 激活函数
- en: 'The *sigmoid* function normalizes a real-valued input into a value between
    0 and 1\. It is defined as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*sigmoid* 函数将实值输入归一化到0到1之间的值。其定义如下：'
- en: f(z) =  1 _ 1+ e −z
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: f(z) = 1 / (1 + e^(-z))
- en: 'If we plot the function, we end up with a graph as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制这个函数，我们最终得到如下图形：
- en: '![Figure 2.23 – sigmoid activation](img/B19327_02_23.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图2.23 – sigmoid激活](img/B19327_02_23.jpg)'
- en: Figure 2.23 – sigmoid activation
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 – sigmoid激活
- en: As we can see, any number is squashed into a range from 0 to 1, which makes
    the sigmoid function an ideal candidate for outputting probabilities.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，任何数字都被压缩到0到1的范围内，这使得sigmoid函数成为输出概率的理想候选者。
- en: 'The *hyperbolic tangent* function, also known as the *tanh* function, is defined
    as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数，也称为*tanh*函数，定义为以下：
- en: tanh(z) =  2 _ 1+ e −2z  − 1
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: tanh(z) = 2 / (1 + e^(-2z)) - 1
- en: 'Plotting this function, we see a graph as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制这个函数，我们看到如下图形：
- en: '![Figure 2.24 – tanh activation](img/B19327_02_24.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图2.24 – tanh激活](img/B19327_02_24.jpg)'
- en: Figure 2.24 – tanh activation
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24 – tanh激活
- en: 'This looks very similar to the sigmoid graph, but note the subtle differences:
    here, the output of the function ranges from `-1` to `1` instead of `0` to `1`.
    Negative numbers get mapped to negative values, and positive numbers get mapped
    to positive values. As the function is centered on `0` (as opposed to sigmoid
    centered on `0.5`), it provides better mathematical conveniences. Generally, we
    use *tanh* as an activation function for hidden layers and *sigmoid* for the output
    layer.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常类似于sigmoid图形，但请注意细微的差异：在这里，函数的输出范围从`-1`到`1`，而不是从`0`到`1`。负数被映射到负值，正数被映射到正值。由于函数以`0`为中心（与sigmoid以`0.5`为中心相反），它提供了更好的数学便利性。通常，我们使用*tanh*作为隐藏层的激活函数，而使用sigmoid作为输出层的激活函数。
- en: 'The *ReLU* activation function is defined as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU* 激活函数的定义如下：'
- en: ReLU (z) = {max(0, z); z ≥ 0 0 ; z < 0
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU(z) = {max(0, z); z ≥ 0 0 ; z < 0
- en: 'Basically, the output is the same as the input if it is positive; else, it
    is 0\. The graph looks like this:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，如果输出是正数，则输出与输入相同；否则，输出为0。图形看起来是这样的：
- en: '![Figure 2.25 – ReLU activation](img/B19327_02_25.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图2.25 – ReLU激活](img/B19327_02_25.jpg)'
- en: Figure 2.25 – ReLU activation
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 – ReLU激活
- en: 'The *softmax* function can be considered to be a normalized version of sigmoid.
    While sigmoid operates on a single input, softmax will operate on a vector of
    inputs and produce a vector as output. Each value in the output vector will be
    between 0 and 1\. If the vector `Z` is defined as [z1, z2, z3 ….. zK], softmax
    is defined as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*softmax* 函数可以被认为是sigmoid的归一化版本。虽然sigmoid作用于单个输入，但softmax将作用于输入向量并产生输出向量。输出向量中的每个值将在0到1之间。如果向量`Z`定义为[z1,
    z2, z3 …… zK]，则softmax定义为以下：'
- en: σ (z i) =  e z i _ ∑ j=1 j=K e z j
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: σ(z_i) = e^(z_i) / ∑(j=1 to K) e^(z_j)
- en: Basically, we first calculate the sigmoid values for each element in the vector
    and form a sigmoid vector. Then, we normalize this vector by the total so that
    each element is between `0` and `1` and they add up to `1` (representing probabilities).
    This can be better illustrated with a concrete example.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们首先计算向量中每个元素的sigmoid值，形成一个sigmoid向量。然后，我们通过总和来归一化这个向量，使得每个元素都在`0`到`1`之间，并且它们的总和为`1`（代表概率）。这可以通过一个具体的例子更好地说明。
- en: 'Say that our output vector `Z` has three elements: `Z` `=` `[2,` `0.9,` `0.1]`.
    Then, we have *z*1= 2, *z*2 = 0.9, and *z*3 = 0.1; we therefore have e z 1 = e 2
    = 7.3890, e z 2 = e 0.9 = 2.4596, and e z 3 = e 0.1 = 1.1051\. Applying the previous
    equation, the denominator is the sum of these three, which is 10.9537\. Now, the
    output vector is simply the ratio of each element to this summed-up value—that
    is, [ 7.3890 _ 10.9537,  2.4596 _ 10.9537,  1.1051 _ 10.9537], which comes out
    to be [0.6745, 0.2245, 0.1008]. These values represent probabilities of the input
    belonging to each class respectively (they do not add up to 1 because of rounding
    errors).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的输出向量`Z`有三个元素：`Z` = [2, 0.9, 0.1]。那么，我们有*z*1= 2, *z*2 = 0.9, 和*z*3 = 0.1；因此，我们有e^z_1
    = e^2 = 7.3890, e^z_2 = e^0.9 = 2.4596, 和e^z_3 = e^0.1 = 1.1051。应用前面的方程，分母是这三个数的总和，即10.9537。现在，输出向量简单地是每个元素与这个总和的比值——即[7.3890
    / 10.9537, 2.4596 / 10.9537, 1.1051 / 10.9537]，这等于[0.6745, 0.2245, 0.1008]。这些值分别代表输入属于每个类的概率（由于舍入误差，它们的总和不为1）。
- en: Now that we have an understanding of activation functions, let us discuss how
    a neural network actually functions from end to end.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了激活函数，让我们讨论一下神经网络从端到端是如何实际工作的。
- en: Neural networks – operation
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络 – 操作
- en: When a data point is passed through a neural network, it undergoes a series
    of transformations through the neurons in each layer. This phase is known as the
    forward pass. As the weights are assigned randomly at the beginning, the output
    at each neuron is different. The final layer will give us the probability of the
    point belonging to a particular class; we compare this with our ground-truth labels
    (`0` or `1`) and calculate a loss. Just as **mean squared error** (**MSE**) in
    linear regression is a loss function indicating the error, classification in neural
    networks uses *binary cross-entropy* and *categorical cross-entropy* as the loss
    for binary and multiclass classification respectively.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个数据点通过神经网络时，它会在每一层的神经元中经历一系列的转换。这个阶段被称为正向传递。由于初始时权重是随机分配的，每个神经元的输出都不同。最后一层将给出该点属于特定类别的概率；我们将其与我们的真实标签（`0`或`1`）进行比较，并计算损失。正如线性回归中的**均方误差**（**MSE**）是一个表示误差的损失函数一样，神经网络中的分类使用*二元交叉熵*和*分类交叉熵*作为二元和多类分类的损失函数。
- en: Once the loss is calculated, it is passed back through the network in a process
    called *backpropagation*. Every weight parameter is adjusted based on how it contributes
    to the loss. This phase is known as the backward pass. The same gradient descent
    that we learned before applies here too! Once all the data points have been passed
    through the network once, we say that a training *epoch* has completed. We continue
    this process multiple times, with the weights changing and the loss (hopefully)
    decreasing in each iteration. The training can be stopped either after a fixed
    number of epochs or if we reach a point where the loss changes only minimally.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了损失，它就会通过一个称为*反向传播*的过程反向通过网络。每个权重参数都会根据其对损失的贡献进行调整。这个阶段被称为反向传递。我们之前学过的相同梯度下降法也适用于这里！一旦所有数据点都通过网络一次，我们就说一个训练*周期*已经完成。我们继续这个过程多次，权重会改变，损失（希望）在每次迭代中都会减少。训练可以在固定数量的周期后停止，或者如果我们达到一个损失变化仅略微的点。
- en: Autoencoders – a special class of neural networks
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自编码器 – 神经网络的一个特殊类别
- en: While a standard neural network aims to learn a decision function that will
    predict the class of an input data point (that is, classification), the goal of
    autoencoders is to simply *reconstruct* a data point. While training autoencoders,
    both the input and output that we provide to the autoencoder are the same, which
    is the feature vector of the data point. The rationale behind it is that because
    we train only on normal (non-anomalous) data, the neural network will learn how
    to reconstruct it. On the anomalous data, however, we expect it to fail; remember—the
    model was never exposed to these data points, and we expect them to be significantly
    different than the normal ones. As a result, the error in reconstruction is expected
    to be high for anomalous data points, and we use this error as a metric to determine
    whether a point is an anomaly.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然标准神经网络旨在学习一个决策函数，该函数将预测输入数据点的类别（即分类），但自编码器的目标仅仅是*重建*一个数据点。在训练自编码器时，我们提供给自编码器的输入和输出是相同的，即数据点的特征向量。其背后的原理是，因为我们只在对正常（非异常）数据进行训练，所以神经网络将学习如何重建它。然而，对于异常数据，我们期望它失败；记住——模型从未接触过这些数据点，我们期望它们与正常数据有显著差异。因此，重建误差预计对于异常数据点会很高，我们使用这个误差作为指标来确定一个点是否是异常。
- en: 'An autoencoder has two components: an encoder and a decoder. The encoder takes
    in input data and reduces it to a lower-dimensional space. The output of the encoder
    can be considered to be a dimensionally reduced version of the input. This output
    is fed to the decoder, which then projects it into a higher-dimensional subspace
    (similar to that of the input).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器有两个组件：编码器和解码器。编码器接收输入数据并将其降低到低维空间。编码器的输出可以被认为是输入的降维版本。这个输出被送入解码器，解码器然后将它投影到一个更高维的子空间（类似于输入）。
- en: 'Here is what an autoencoder looks like:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个自编码器的样子：
- en: '![Figure 2.26 – Autoencoder](img/B19327_02_26.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图2.26 – 自编码器](img/B19327_02_26.jpg)'
- en: Figure 2.26 – Autoencoder
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.26 – 自编码器
- en: We will implement this in Python using a framework known as `keras`. This is
    a library built on top of the TensorFlow framework that allows us to easily and
    intuitively design, build, and customize neural network models.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用名为 `keras` 的框架在 Python 中实现这一点。这是一个建立在 TensorFlow 框架之上的库，它允许我们轻松直观地设计、构建和定制神经网络模型。
- en: 'First, we import the necessary libraries and divide our data into training
    and test sets. Note that while we train the autoencoder only on normal or non-anomalous
    data, in the real world it is impossible to have a dataset that is 100% clean;
    there will be some contamination involved. To simulate this, we train on both
    the normal and abnormal examples, with the abnormal ones being very small in proportion.
    The `stratify` parameter ensures that the training and testing data has a similar
    distribution of labels so as to avoid an imbalanced dataset. The code is illustrated
    in the following snippet:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库并将数据分为训练集和测试集。请注意，虽然我们只在对正常或非异常数据上训练自动编码器，但在现实世界中，不可能有一个100%干净的数据库集；总会有些污染。为了模拟这种情况，我们在正常和异常示例上训练，其中异常示例的比例非常小。`stratify`
    参数确保训练和测试数据具有相似的标签分布，以避免数据集不平衡。以下代码片段展示了这一点：
- en: '[PRE21]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We then build our neural network using `keras`. For each layer, we specify the
    number of neurons. If our feature vector has *N* dimensions, the input layer would
    have to have *N* layers. Similarly, because this is an autoencoder, the output
    layer would also have *N* layers.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `keras` 构建我们的神经网络。对于每一层，我们指定神经元的数量。如果我们的特征向量有 *N* 维度，输入层将需要 *N* 个神经元。同样，因为这是一个自动编码器，输出层也会有
    *N* 个神经元。
- en: 'We first build the encoder part. We start with an input layer, followed by
    three fully connected layers of decreasing dimensions. To each layer, we feed
    the output of the previous layer as input:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建编码器部分。我们从输入层开始，然后是三个维度递减的完全连接层。我们将前一层输出作为输入输入到每一层：
- en: '[PRE22]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we work our way back to higher dimensions and build the decoder part:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们逐步回到更高的维度并构建解码器部分：
- en: '[PRE23]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we put it all together to form the autoencoder:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将所有这些组合起来形成自动编码器：
- en: '[PRE24]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We also define the encoder and decoder models separately, to make predictions
    later:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分别定义了编码器和解码器模型，以便稍后进行预测：
- en: '[PRE25]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we want to examine the structure of the autoencoder, we can compile it and
    print out a summary of the model:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想检查自动编码器的结构，我们可以编译它并打印出模型的摘要：
- en: '[PRE26]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You should see which layers are in the network and what their dimensions are
    as well:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到网络中的哪些层以及它们的维度：
- en: '![Figure 2.27 – Autoencoder model summary](img/B19327_02_27.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图2.27 – 自动编码器模型摘要](img/B19327_02_27.jpg)'
- en: Figure 2.27 – Autoencoder model summary
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.27 – 自动编码器模型摘要
- en: 'Finally, we actually train the model. Normally, we provide features and the
    ground truth for training, but in this case, our input and output are the same:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实际上开始训练模型。通常，我们提供用于训练的特征和真实值，但在这个案例中，我们的输入和输出是相同的：
- en: '[PRE27]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s see the result:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果：
- en: '![Figure 2.28 – Training phase](img/B19327_02_28.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图2.28 – 训练阶段](img/B19327_02_28.jpg)'
- en: Figure 2.28 – Training phase
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.28 – 训练阶段
- en: 'Now that the model is fit, we can use it to make predictions. To evaluate this
    model, we will predict the output for each input data point, and calculate the
    reconstruction error:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经拟合，我们可以用它来做出预测。为了评估这个模型，我们将对每个输入数据点进行预测，并计算重建误差：
- en: '[PRE28]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we need to set a threshold for the reconstruction error, after which we
    will call data points as anomalies. The easiest way to do this is to leverage
    the distribution of the reconstruction error. Here, we say that when the error
    is above the 99th percentile, it represents an anomaly:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要为重建误差设置一个阈值，超过这个阈值后，我们将数据点称为异常。最简单的方法是利用重建误差的分布。在这里，我们说当误差超过99百分位数时，它表示一个异常：
- en: '[PRE29]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'When we generate the confusion matrix, we see something like this:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们生成混淆矩阵时，我们会看到类似这样的结果：
- en: '![Figure 2.29 – Confusion matrix for Autoencoder](img/B19327_02_29.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图2.29 – 自动编码器的混淆矩阵](img/B19327_02_29.jpg)'
- en: Figure 2.29 – Confusion matrix for Autoencoder
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.29 – 自动编码器的混淆矩阵
- en: As we can see, this model is a very poor one; there are absolutely no true positives
    here.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个模型非常糟糕；这里完全没有真正的正例。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter delved into the details of anomaly detection. We began by learning
    what anomalies are and what their occurrence can indicate. Using NSL-KDD, a benchmark
    dataset, we first explored statistical methods used to detect anomalies, such
    as the z-score, elliptical envelope, LOF, and DBSCAN. Then, we examined ML methods
    for the same task, including isolation forests, OC-SVM, and deep autoencoders.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了异常检测的细节。我们首先学习了什么是异常以及它们的出现可能表明什么。使用NSL-KDD这个基准数据集，我们首先探索了用于检测异常的统计方法，例如z分数、椭圆包络、LOF和DBSCAN。然后，我们考察了用于同一任务的机器学习方法，包括隔离森林、OC-SVM和深度自编码器。
- en: Using the techniques introduced in this chapter, you will be able to examine
    a dataset and detect anomalous data points. Identifying anomalies is key in many
    security problems such as intrusion and fraud detection.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章介绍的技术，你将能够检查数据集并检测异常数据点。在许多安全问题上，如入侵和欺诈检测，识别异常是关键。
- en: In the next chapter, we will learn about malware, and how to detect it using
    state-of-the-art models known as transformers.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于恶意软件的内容，以及如何使用称为变换器的最先进模型来检测恶意软件。
