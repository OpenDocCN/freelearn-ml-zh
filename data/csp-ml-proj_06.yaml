- en: Customer Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户细分
- en: In this chapter, we are going to learn about unsupervised learning models and
    how they can be used to extract insights from the data. Up until now, we have
    been focusing on supervised learning, where our **machine learning** (**ML**)
    models have known target variables that they try to predict. We have built classification
    models for spam email filtering and Twitter sentiment analysis. We have also built
    regression models for foreign exchange rate forecasting and predicting the fair
    value of house prices. All of these ML models that we have built so far are supervised
    learning algorithms, where the models learn to map the given input to expected
    outcomes. However, there are cases where we are more interested in finding hidden
    insights and drawing inferences from datasets, and we can use unsupervised learning
    algorithms for such tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习无监督学习模型及其如何从数据中提取洞察。到目前为止，我们一直专注于监督学习，其中我们的机器学习（ML）模型有已知的目标变量，它们试图预测这些变量。我们已经为垃圾邮件过滤和Twitter情感分析构建了分类模型。我们还为外汇汇率预测和预测房价公允价值构建了回归模型。我们迄今为止构建的所有这些机器学习模型都是监督学习算法，其中模型学习将给定的输入映射到预期的结果。然而，在某些情况下，我们更感兴趣的是从数据集中发现隐藏的洞察和推断，我们可以使用无监督学习算法来完成这些任务。
- en: In this chapter, we are going to use an online retail dataset that contains
    information about the prices and quantities of items that customers bought. We
    will explore the data by looking at how the distributions of item prices and quantities
    for purchase orders differ from those of cancel orders. We will also look at how
    online store activities are spread across different countries. Then, we are going
    to take this transaction-level data and transform and aggregate it into customer-level
    data. As we transform this data to have a customer-centric view, we are going
    to discuss ways to build scale-independent features for unsupervised learning
    algorithms. With this feature set, we are going to use a k-means clustering algorithm
    to build customer segments and extract insights on the customer behaviors within
    each segment. We will introduce a new validation metric, Silhouette Coefficient,
    to evaluate the clustering results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个包含客户购买的商品价格和数量信息的在线零售数据集。我们将通过观察购买订单和取消订单中商品价格和数量的分布差异来探索这些数据。我们还将研究在线商店活动在不同国家之间的分布情况。然后，我们将把这一级交易数据转换和汇总成客户级数据。在我们将数据转换为以客户为中心的视角时，我们将讨论为无监督学习算法构建规模无关特征的方法。有了这个特征集，我们将使用k-means聚类算法来构建客户细分市场，并从每个细分市场内提取客户行为洞察。我们将介绍一个新的验证指标，即轮廓系数，以评估聚类结果。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Problem definition for a customer segmentation project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户细分项目的问题定义
- en: Data analysis for an online retail dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线零售数据集的数据分析
- en: Feature engineering and aggregation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程和汇总
- en: Unsupervised learning using a k-means clustering algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k-means聚类算法进行无监督学习
- en: Clustering model validations using the Silhouette Coefficient
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用轮廓系数进行聚类模型验证
- en: Problem definition
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义
- en: Let's discuss in more detail what problems we are going to solve and build clustering
    models for. Whether you are trying to send marketing emails to your customers
    or you simply want to better understand your customers and their behaviors on
    your online store, you will want to analyze and identify different types and segments
    of your customers. Some customers might buy lots of items at once (bulk buyers),
    some might primarily buy expensive or luxury items (luxury product buyers), or
    some might have bought one or two items and never come back (unengaged customers).
    Depending on these behavioral patterns, your marketing campaigns should vary.
    For example, sending out emails with promotions on luxury items is likely to provoke
    luxury product buyers to log in to the online store and purchase certain items,
    but such an email campaign is not going to work well for bulk buyers. On the other
    hand, sending out emails with promotions on items that are frequently bought in
    bulk, such as pens and notepads for office supplies, is likely to make bulk buyers
    log in to the online store and place purchase orders, but it might not be attractive
    for luxury product buyers. By identifying customer segments based on their behavioral
    patterns and using customized marketing campaigns, you can optimize your marketing
    channels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论我们将要解决的问题，并构建聚类模型。无论你是试图向客户发送营销邮件，还是仅仅想更好地了解你的客户及其在线商店的行为，你都将想要分析和识别不同类型和细分市场的客户。一些客户可能会一次性购买大量商品（批量购买者），一些可能会主要购买昂贵或奢侈品（奢侈品购买者），或者一些可能只购买了一两件商品就再也没有回来（不活跃客户）。根据这些行为模式，你的营销活动应该有所不同。例如，发送关于奢侈品促销的邮件可能会激发奢侈品购买者登录在线商店并购买某些商品，但这种营销活动可能对批量购买者效果不佳。另一方面，发送关于经常批量购买的物品（如办公用品的笔和便签本）的促销邮件可能会使批量购买者登录在线商店并下订单，但这可能对奢侈品购买者没有吸引力。通过根据客户的行为模式识别客户细分，并使用定制化的营销活动，你可以优化你的营销渠道。
- en: In order to build models for customer segmentation, we are going to use an online
    retail dataset that contains all the transactions that occurred between Jan. 12th
    2010 and Sep. 12th 2011 for a UK-based online retail store. This dataset is available
    in the UCI Machine Learning Repository and can be downloaded from this link: [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#).
    With this data, we are going to build features that contain information about
    the net revenue, average item price, and average purchase quantity per customer.
    Using these features, we are going to build a clustering model using a **k-means
    clustering algorithm** that clusters the customer base into different segments.
    We will be using **Silhouette Coefficient** metrics to evaluate the quality of
    the clusters and deduce the optimal number of customer segments to build.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建客户细分模型，我们将使用一个包含2010年1月12日至2011年9月12日之间发生的所有交易的在线零售数据集，该数据集属于一家英国在线零售店。这个数据集可在UCI机器学习仓库中找到，并可通过以下链接下载：[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)。利用这些数据，我们将构建包含关于净收入、平均商品价格和每位客户平均购买数量的特征。使用这些特征，我们将使用**k-means聚类算法**构建一个聚类模型，将客户基础划分为不同的细分市场。我们将使用**轮廓系数**指标来评估聚类质量，并推断出构建客户细分的最优数量。
- en: 'To summarize our problem definition for the customer segmentation project:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结客户细分项目的定义问题：
- en: What is the problem? We need a clustering model that segments customers into
    different clusters, so that we can understand and draw insights about the behavioral
    patterns of the customers better.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题是什么？我们需要一个聚类模型，将客户划分为不同的集群，以便我们更好地理解和提取关于客户行为模式的见解。
- en: Why is it a problem? There is no one-fits-all marketing campaign that works
    for all different types of customers. We will need to build custom-tailored marketing
    campaigns for bulk buyers and luxury product buyers separately. Also, we will
    have to target unengaged customers differently from the other customer types to
    have them re-engage with the products. The more customized the marketing messages
    are, the more likely customers will engage. It will be a big advantage if we have
    an ML model that clusters our customer base into different segments based on their
    behavioral patterns on the online store.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么这是一个问题？没有一种适合所有不同类型客户的营销活动都能奏效。我们需要为批量购买者和奢侈品购买者分别构建定制化的营销活动。此外，我们还需要将未参与活动的客户与其他客户类型区分开来，以便让他们重新参与产品。营销信息越定制化，客户参与的可能性就越大。如果我们有一个基于在线商店中客户行为模式将客户基础聚类到不同段落的机器学习模型，这将是一个巨大的优势。
- en: What are some of the approaches to solving this problem?We are going to use
    the online retail dataset that contains all transactions from 2010 to mid-2011
    to aggregate the key features, such as net revenue, average unit price, and average
    purchase quantity for each customer. Then, we will use a k-means clustering algorithm
    to build a clustering model and use the Silhouette Coefficient to evaluate the
    quality of clusters and choose the optimal number of clusters.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的方法有哪些？我们将使用包含2010年至2011年中期所有交易的在线零售数据集来聚合关键特征，例如每个客户的净收入、平均单价和平均购买数量。然后，我们将使用k-means聚类算法构建聚类模型，并使用轮廓系数来评估聚类的质量并选择最佳聚类数量。
- en: What are the success criteria? We do not want too many clusters, as this would
    make it more difficult to explain and understand different patterns of customers.
    We will use the Silhouette Coefficient score to tell us the best number of clusters
    to use for customer segmentation.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功的标准是什么？我们不希望有太多的聚类，因为这会使解释和理解不同客户模式变得更加困难。我们将使用轮廓系数得分来告诉我们用于客户分段的最佳聚类数量。
- en: Data analysis for the online retail dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线零售数据集的数据分析
- en: It is now time to look into the dataset. You can follow this link: [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#),
    click on the `Data Folder` link in the top left corner, and download the `Online
    Retail.xlsx` file. You can save the file as a CSV format and load it into a Deedle
    data frame.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是查看数据集的时候了。您可以点击以下链接：[http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#)，点击左上角的`Data
    Folder`链接，并下载`Online Retail.xlsx`文件。您可以将文件保存为CSV格式，并将其加载到Deedle数据框中。
- en: Handling missing values
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'Since we will be aggregating the transaction data for each customer, we need
    to check whether there are any missing values in the `CustomerID` column. The
    following screenshot shows a few records with no `CustomerID`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将对每个客户的交易数据进行聚合，我们需要检查`CustomerID`列中是否有任何缺失值。以下截图显示了一些没有`CustomerID`的记录：
- en: '![](img/00082.gif)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00082.gif)'
- en: 'We are going to drop those records with missing values from the `CustomerID`,
    `Description`, `Quantity`, `UnitPrice`, and `Country` columns. The following code
    snippet shows how we can drop records with missing values for those columns:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将删除`CustomerID`、`Description`、`Quantity`、`UnitPrice`和`Country`列中包含缺失值的记录。以下代码片段显示了如何删除这些列的缺失值记录：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We use the `DropSparseRows` method of the Deedle data frame to drop all the
    records with missing values in the columns of our interest. Then, we append the
    data frame with an additional column `Amount`, which is the total price for the
    given transaction. We can calculate this value by multiplying the unit price with
    the quantity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Deedle数据框的`DropSparseRows`方法来删除我们感兴趣列中所有包含缺失值的记录。然后，我们添加一个额外的列`Amount`到数据框中，这是给定交易的总额。我们可以通过将单价乘以数量来计算这个值。
- en: As you can see from the previous image, we had 541,909 records before we dropped
    the missing values. After dropping the records with missing values from the columns
    of our interest, the number of records in the data frame ends up being 406,829\.
    Now, we have a data frame that contains the information about `CustomerID`, `Description`,
    `Quantity`, `UnitPrice`, and `Country` for all the transactions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面的图像中可以看到，我们在删除缺失值之前有541,909条记录。在从我们感兴趣的列中删除包含缺失值的记录后，数据框中的记录数最终变为406,829条。现在，我们有一个包含所有交易信息的`CustomerID`、`Description`、`Quantity`、`UnitPrice`和`Country`的数据框。
- en: Variable distributions
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量分布
- en: 'Let''s start looking at the distributions in our dataset. First, we will take
    a look at the top five countries by the volume of transactions. The code we used
    to aggregate the records by the countries and count the number of transactions
    that occurred in each country is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始查看数据集中的分布。首先，我们将查看交易量最大的前五个国家。我们用来按国家聚合记录并计算每个国家发生的交易数量的代码如下：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/00083.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00083.jpeg)'
- en: 'The number of transactions for each of the top five countries looks as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 交易量最大的前五个国家的交易数量如下：
- en: '![](img/00084.gif)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00084.gif)'
- en: As expected, the largest number of transactions occurred in the United Kingdom.
    Germany and France come in as the countries with the second and third most transactions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，交易数量最多的是英国。德国和法国分别排在第二和第三位，交易数量最多。
- en: Let's start looking at the distributions of the features that we will be using
    for our clustering model—purchase quantity, unit price, and net amount. We will
    be looking at these distributions in three ways. First, we will get the overall
    distribution of each feature, regardless of whether the transaction was for purchase
    or cancellation. Second, we will take a look at the purchase orders only, excluding
    the cancel orders. Third, we will look at the distributions for cancel orders
    only.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始查看我们将用于聚类模型的特征分布——购买数量、单价和净额。我们将以三种方式查看这些分布。首先，我们将获取每个特征的总体分布，无论交易是购买还是取消。其次，我们将仅查看购买订单，排除取消订单。第三，我们将查看仅取消订单的分布。
- en: 'The code to get distributions of transaction quantity is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 获取交易数量分布的代码如下：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As in the previous chapter, we are using the `Quantiles` method to compute
    `quartiles`—min, 25% percentile, median, 75% percentile, and max. Once we get
    the overall distribution of order quantities per transaction, we then look at
    the distribution for purchase orders and cancel orders. In our dataset, cancel
    orders are encoded with negative numbers in the `Quantity` column. In order to
    separate cancel orders from purchase orders, we can simply filter out positive
    and negative quantities from our data fame as in the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，我们使用`分位数`方法来计算四分位数——最小值、25%分位数、中位数、75%分位数和最大值。一旦我们得到每笔订单数量的总体分布，我们再查看购买订单和取消订单的分布。在我们的数据集中，取消订单在`数量`列中用负数表示。为了将取消订单与购买订单分开，我们可以简单地从我们的数据表中过滤出正数和负数，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In order to get the `quartiles` of per-transaction unit prices, we use the
    following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取每笔交易单位价格的四分位数，我们使用以下代码：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Similarly, we can compute the `quartiles` of the per-transaction total amount
    using the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用以下代码计算每笔交易的总额的四分位数：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When you run the code, you will see the following output for the distributions
    of per-transaction order quantity, unit price, and total amount:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行代码时，你会看到以下输出，显示了每笔交易的订单数量、单价和总额的分布：
- en: '![](img/00085.gif)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00085.gif)'
- en: If you look at the distribution of the overall order quantities in this output,
    you will notice that from the first quartile (25% percentile), the quantities
    are positive. This suggests that there are far less cancel orders than purchase
    orders, which is actually a good thing for an online retail store. Let's look
    at how the purchase orders and cancel orders are divided in our dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看输出中的总体订单数量分布，你会注意到从第一个四分位数（25%分位数）开始，数量是正的。这表明取消订单的数量远少于购买订单，这对在线零售店来说实际上是个好事。让我们看看在我们的数据集中购买订单和取消订单是如何划分的。
- en: 'Using the following code, you can draw a bar chart to compare the number of
    purchase orders against cancel orders:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，你可以绘制条形图来比较购买订单与取消订单的数量：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When you run this code, you will see the following bar chart:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，你会看到以下条形图：
- en: '![](img/00086.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00086.jpeg)'
- en: As expected and shown in the previous distribution output, the number of cancel
    orders is much less than the number of purchase orders. With these analysis results,
    we are going to start building features for our clustering model for customer
    segmentation in the next section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期并在之前的分布输出中所示，取消订单的数量远少于购买订单的数量。有了这些分析结果，我们将在下一节开始构建用于客户细分聚类模型的特征。
- en: The full code for this data analysis step can be found by following this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/DataAnalyzer.cs).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据分析步骤的完整代码可以通过以下链接找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/DataAnalyzer.cs)。
- en: Feature engineering and data aggregation
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程和数据聚合
- en: 'The records in the dataset we have now represent individual transactions. However,
    we want to build a clustering model that clusters customers into different segments.
    In order to do that, we need to transform and aggregate our data by customer.
    In other words, we will need to group our data by `CustomerID` and `aggregate`
    all the transactions that belong to each customer by summing, counting, or taking
    averages of the values. Let''s look at an example first. The following code groups
    the transaction-level data by `CustomerID` and computes the net revenue, total
    number of transactions, total number of cancel orders, average unit price, and
    average order quantity:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在数据集中的记录代表单个交易。然而，我们想要构建一个聚类模型，将客户聚类到不同的细分市场。为了做到这一点，我们需要按客户对数据进行转换和聚合。换句话说，我们需要按
    `CustomerID` 对数据进行分组，并通过求和、计数或取值平均值来聚合每个客户所属的交易。让我们先看一个例子。以下代码按 `CustomerID` 对交易级别数据进行分组，并计算净收入、交易总数、取消订单总数、平均单价和平均订单数量：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you may see from this code, we are using the `AggregateRowsBy` method in
    the Deedle data frame and passing a custom `aggFunc` for each aggregation. In
    the first example, where we compute the net revenue per customer, we sum all the
    purchase amounts for each customer. For the second feature, we count the number
    of transactions to compute the total number of orders for each customer. In order
    to compute the average order quantity for each customer, we sum up all the order
    quantities and divide it by the number of transactions. As you can see from this
    case, the `AggregateRowsBy` method comes in handy when you need to transform and
    aggregate a data frame with a custom `aggregation` function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中看到，我们在 Deedle 数据框中使用了 `AggregateRowsBy` 方法，并为每个聚合传递了一个自定义的 `aggFunc`。在第一个例子中，我们计算每个客户的净收入时，我们汇总了每个客户的购买金额。对于第二个特征，我们计算交易数量以确定每个客户的订单总数。为了计算每个客户的平均订单数量，我们将所有订单数量相加，然后除以交易数量。如您所看到的这个案例，当您需要使用自定义
    `aggregation` 函数转换和聚合数据框时，`AggregateRowsBy` 方法非常有用。
- en: 'Once we have computed all these features, we need to combine all the data into
    one place. We created a new empty data frame and added each of these aggregated
    features as separate columns to the new data frame. The following code shows how
    we created a features data frame:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了所有这些特征，我们需要将这些数据合并到一个地方。我们创建了一个新的空数据框，并将这些聚合特征作为单独的列添加到新的数据框中。以下代码展示了我们如何创建特征数据框：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To take a closer look at the distributions of these features, we wrote a helper
    function that computes the `quartiles` of a given feature and prints out the results.
    The code for this helper function is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地查看这些特征的分布，我们编写了一个辅助函数，该函数计算给定特征的 `四分位数` 并打印出结果。此辅助函数的代码如下：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of this code looks like the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码的输出如下所示：
- en: '![](img/00087.gif)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00087.gif)'
- en: 'If you look closely, there is one thing that is concerning. There is a small
    number of customers that have negative net revenue and negative average quantity.
    This suggests some customers have more cancel orders than purchase orders. However,
    this is odd. To cancel an order, there needs to be a purchase order first. This
    suggests that our dataset is not complete and there are some orphan cancel orders
    that do not have matching previous purchase orders. Since we cannot go back in
    time and pull out more data for those customers with orphan cancel orders, the
    simplest way to handle this problem is to drop those customers with orphan cancel
    orders. The following code shows some criteria we can use to drop such customers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细观察，会发现一个问题令人担忧。有少数客户具有负净收入和负平均数量。这表明一些客户的取消订单数量可能超过购买订单数量。然而，这是奇怪的。要取消订单，首先需要有一个购买订单。这表明我们的数据集可能不完整，存在一些没有匹配先前购买订单的孤儿取消订单。由于我们无法回到过去为那些有孤儿取消订单的客户提取更多数据，处理这个问题的最简单方法就是删除那些有孤儿取消订单的客户。以下代码展示了我们可以用来删除此类客户的某些标准：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see from this code snippet, we drop any customer who has a negative
    net revenue, negative average quantity, and percentage of cancel orders more than
    50%. After dropping these customers, the resulting distributions look like the
    following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码片段中可以看到，我们删除了任何具有负净收入、负平均数量和取消订单百分比超过50%的客户。在删除这些客户后，结果分布看起来如下：
- en: '![](img/00088.gif)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00088.gif)'
- en: 'As you can see from these distributions, the scales for each feature are very
    different. `NetRevenue` rages from 0 to 279,489.02, while `PercentageCancelled`
    ranges from 0 to 0.45\. We are going to transform these features into percentiles,
    so that we can have all of our features on the same scale of 0 to 1\. The following
    code shows how to compute percentiles for each feature:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这些分布中可以看到，每个特征的尺度都非常不同。`NetRevenue`的范围从0到279,489.02，而`PercentageCancelled`的范围从0到0.45。我们将把这些特征转换成百分位数，这样我们就可以让所有特征都在0到1的同一尺度上。以下代码显示了如何计算每个特征的百分位数：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using the `StatsFunctions.PercentileRank` method, we can compute the percentile
    for each record. The following output shows the results for the `NetRevenue` and
    `NumTransactions` features:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`StatsFunctions.PercentileRank`方法，我们可以计算每条记录的百分位数。以下输出显示了`NetRevenue`和`NumTransactions`特征的计算结果：
- en: '![](img/00089.gif)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00089.gif)'
- en: As you can see from this output, instead of a wide range, the values for both
    features now range between 0 and 1\. We will use these percentile features when
    we build our clustering model in the following section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从输出中可以看到，这两个特征的值现在都在0到1之间，而不是一个广泛的范围。在下一节构建聚类模型时，我们将使用这些百分位数特征。
- en: The full code for this feature engineering step can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/FeatureEngineering.cs).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特征工程步骤的完整代码可以在以下链接中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/FeatureEngineering.cs)。
- en: Unsupervised learning – k-means clustering
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习 - k-means聚类
- en: 'It is now time to start building our clustering models. In this project, we
    are going to try clustering customers into different segments based on the following
    three features: `NetRevenuePercentile`, `AvgUnitPricePercentile`, and `AvgQuantityPercentile`,
    so that we can analyze the item selections based on the spending habits of the
    customers. Before we start fitting a k-means clustering algorithm to our feature
    set, there is an important step we need to take. We need to normalize our features,
    so that our clustering model does not put more weight on certain features over
    the others. If variances of features are different, then a clustering algorithm
    can put more weight on those with small variances and can tend to cluster them
    together. The following code shows how you can normalize each feature:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始构建我们的聚类模型了。在这个项目中，我们将尝试根据以下三个特征将客户聚类到不同的细分市场：`NetRevenuePercentile`、`AvgUnitPricePercentile`和`AvgQuantityPercentile`，这样我们就可以根据客户的消费习惯来分析商品选择。在我们开始将k-means聚类算法拟合到特征集之前，有一个重要的步骤我们需要采取。我们需要对特征进行归一化，这样我们的聚类模型就不会对某些特征赋予比其他特征更多的权重。如果特征的方差不同，那么聚类算法可能会对那些方差小的特征赋予更多的权重，并倾向于将它们聚在一起。以下代码显示了如何归一化每个特征：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that we have normalized our variables, let''s start building clustering
    models. In order to build a k-means clustering model, we need to know the number
    of clusters we want in advance. Since we do not know what the best number of clusters
    is, we are going to try a few different numbers of clusters and rely on the validation
    metrics, the Silhouette Score, to tell us what the optimal number of clusters
    is. The following code shows how to build clustering models that use a k-means
    clustering algorithm:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经归一化了变量，让我们开始构建聚类模型。为了构建一个k-means聚类模型，我们需要提前知道我们想要的聚类数量。由于我们不知道最佳聚类数量是多少，我们将尝试几个不同的聚类数量，并依靠验证指标，即轮廓分数（Silhouette
    Score），来告诉我们最佳的聚类数量是多少。以下代码显示了如何构建使用k-means聚类算法的聚类模型：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When you run this code, it will output the centroids for each cluster. The
    following is an output of cluster centroids from a 4-cluster clustering model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，它将输出每个聚类的中心点。以下是一个4聚类聚类模型的聚类中心点输出：
- en: '![](img/00090.gif)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00090.gif)'
- en: 'As you can see from this output, the cluster with label 3 is a cluster of customers
    who have high net revenue, middle-high average unit price, and middle-high average
    quantity. So, these customers are high value customers who bring in the most revenue
    and buy items with prices above average in above-average quantities. In contrast,
    the cluster labeled as 1 is a cluster of customers who have low net revenue, high
    average unit price and middle-low average quantity. So, these customers buy expensive
    items in average quantities and do not bring in that much revenue for the online
    store. As you may notice from this example, you can already see some patterns
    among different clusters. Let''s now look at which customers in each segment buy
    the most. The following is the top 10 items bought for each segment of the 4-cluster
    clustering model:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从输出结果中可以看到，标签为3的簇是由那些具有高净收入、中等偏高的平均单价和中等偏高的平均数量的客户组成的。因此，这些客户是高价值客户，他们带来了最多的收入，并且以高于平均的价格购买数量也高于平均水平的商品。相比之下，标签为1的簇是由那些具有低净收入、高平均单价和中等偏低平均数量的客户组成的。因此，这些客户以平均数量购买昂贵的商品，并且为在线商店带来的收入并不多。您可能已经注意到这个例子，您已经可以看到不同簇之间的一些模式。现在让我们看看每个细分市场中的哪些客户购买最多。以下是为4簇聚类模型每个细分市场购买的前10个商品：
- en: '![](img/00091.gif)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00091.gif)'
- en: 'This top 10 item list for each segment gives you a rough idea of what kinds
    of items the customers in each segment buy the most. This is out of scope for
    this chapter, but you can take a step further and analyze individual words in
    the item description and use word frequency analysis, such as we did in [Chapter
    2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam Email Filtering*
    and [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter
    Sentiment Analysis*. Another way to visualize the clustering results is to draw
    scatter plots for the segments. The following chart shows a scatter plot of `NetRevenuePercentile`
    versus `AvgQuantityPercentile` for the 4-cluster clustering model:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个细分市场的这个前10个商品列表为您提供了一个大致的概念，了解每个细分市场的客户购买最多的商品类型。这超出了本章的范围，但您可以进一步分析商品描述中的单个单词，并使用词频分析，例如我们在[第2章](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470)“垃圾邮件过滤”和[第3章](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470)“Twitter情感分析”中所做的那样。可视化聚类结果的另一种方法是绘制细分市场的散点图。以下图表显示了4簇聚类模型中`NetRevenuePercentile`与`AvgQuantityPercentile`的散点图：
- en: '![](img/00092.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00092.jpeg)'
- en: 'The following chart shows a scatter plot of `AvgUnitPricePercentile ` versus 
    `AvgQuantityPercentile` for the 4-cluster clustering model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了4簇聚类模型中`AvgUnitPricePercentile`与`AvgQuantityPercentile`的散点图：
- en: '![](img/00093.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00093.jpeg)'
- en: 'The following chart shows a scatter plot of `NetRevenuePercentile` versus `AvgUnitPricePercentile` for
    the 4-cluster clustering model:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了4簇聚类模型中`NetRevenuePercentile`与`AvgUnitPricePercentile`的散点图：
- en: '![](img/00094.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00094.jpeg)'
- en: As you can see from these plots, a scatter plot is a good way to visualize how
    each cluster is formed and what the boundaries look like for each cluster. For
    example, if you look at the scatter plot of `NetRevenuePercentile` versus `AvgUnitPricePercentile`,
    cluster 1 has high average unit price and low net revenue. This corresponds to
    the findings we have from looking at the cluster centroids. For higher dimensions
    and larger number of clusters, it gets more difficult to visualize using scatter
    plots. However, very often, visualizing in charts helps draw insights more easily
    from these clustering analyses. Let's start looking at how we can evaluate the
    cluster quality and choose the optimal number of clusters using the Silhouette
    Coefficient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这些图表中可以看到，散点图是可视化每个簇形成方式和每个簇边界的好方法。例如，如果您查看`NetRevenuePercentile`与`AvgUnitPricePercentile`的散点图，簇1具有高平均单价和低净收入。这与我们从查看簇中心点得出的发现相对应。对于更高维度和更多簇的情况，使用散点图进行可视化变得更加困难。然而，在图表中进行可视化通常有助于更容易地从这些聚类分析中得出见解。让我们开始探讨如何使用轮廓系数评估簇的质量和选择最佳簇数量。
- en: The full code that was used in this k-means clustering step can be found at
    this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/Clustering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/Clustering.cs).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个k-means聚类步骤中使用的完整代码可以在以下链接中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/Clustering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/Clustering.cs)。
- en: Clustering model validations using the Silhouette Coefficient
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用轮廓系数进行聚类模型验证
- en: 'The** Silhouette Coefficient** or **Silhouette Score** provides an easy way
    to evaluate the quality of clusters. The Silhouette Coefficient measures how closely
    related an object is to its own cluster against the other clusters. The way to
    compute the Silhouette Coefficient is as follows; for each record, `i`, calculate
    the average distance between the record and all the other records in the same
    cluster and call this number, `a[i]`. Then, calculate the average distances between
    the record and all the records in each other cluster for all the other clusters,
    take the lowest average distance, and call this number, `b[i]`. Once you have
    these two numbers, subtract `a[i]` from `b[i]` and divide it by the maximum number
    between `a[i]` and `b[i]`. You iterate this process for all the records in the
    dataset and calculate the average value to get the Silhouette Coefficient. The
    following is a formula to calculate the Silhouette Coefficient for a single data
    point:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**Silhouette 系数**或**Silhouette 分数**提供了一种简单的方法来评估簇的质量。Silhouette 系数衡量一个对象与其自身簇的紧密程度相对于其他簇。计算Silhouette系数的方法如下；对于每个记录，`i`，计算该记录与同一簇中所有其他记录的平均距离，并称这个数字为`a[i]`。然后，计算该记录与每个其他簇中所有其他记录的平均距离，对于所有其他簇，取最低的平均距离，并称这个数字为`b[i]`。一旦你有了这两个数字，从`b[i]`中减去`a[i]`，然后除以`a[i]`和`b[i]`之间的最大值。你迭代这个过程，为数据集中的所有记录计算平均值以获得Silhouette系数。以下是一个用于计算单个数据点Silhouette系数的公式：'
- en: '![](img/00095.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00095.jpeg)'
- en: 'In order to get the final Silhouette value, you will need to iterate through
    the data points and take the average of Silhouette values. The Silhouette Coefficient
    ranges between -1 and 1\. The closer to 1, the better the cluster qualities are.
    The following code shows how we implemented this formula:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最终的Silhouette值，你需要遍历数据点，并取Silhouette值的平均值。Silhouette系数介于-1和1之间。越接近1，簇的质量越好。以下代码展示了我们如何实现这个公式：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A helper function to calculate the average distance between a data point and
    all the points in a cluster is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个计算数据点与簇中所有点之间平均距离的辅助函数：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see from the code, we iterate through each data point and start calculating
    the average distances between the given data point and all the other records in
    different clusters. Then, we take the difference between the lowest average distance
    to different clusters and the average distance within the same cluster and divide
    it by the maximum of those two numbers. Once we have iterated through all the
    data points, we take the average of this Silhouette value and return it as the
    Silhouette Coefficient for the clustering model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中可以看出，我们遍历每个数据点，并开始计算给定数据点与不同簇中所有其他记录的平均距离。然后，我们取不同簇中最低平均距离与同一簇中平均距离之间的差值，并除以这两个数字中的最大值。一旦我们遍历了所有数据点，我们取这个Silhouette值的平均值，并将其作为聚类模型的Silhouette系数返回。
- en: 'When you run this code for the clustering models with different numbers of
    clusters, you will see an output similar to the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行具有不同簇数量的聚类模型代码时，你会看到以下类似的输出：
- en: '![](img/00096.gif)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00096.gif)'
- en: As you can see from this output, the Silhouette Score increased as we increased
    the number of clusters to a certain point and then it dropped. In our case, a
    k-means clustering model with six clusters performed the best and six clusters
    seem to be the best choice for our dataset.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中可以看出，随着我们将簇的数量增加到一定点，Silhouette Score 会增加，然后又下降。在我们的案例中，具有六个簇的k-means聚类模型表现最佳，六个簇似乎是我们数据集的最佳选择。
- en: Oftentimes, just looking at the Silhouette Coefficient is not enough to make
    a decision on the best number of clusters. For example, a clustering model with
    a really large number of clusters can have a great Silhouette Score, but it would
    not help us draw any insights from such a clustering model. As clustering analysis
    is primarily used for explanatory analysis to draw insights and identify hidden
    patterns from the data, it is important that the clustering results can be explained.
    Pairing the Silhouette Score with two-dimensional or three-dimensional scatter
    plots will help you come up with the best number of clusters to choose and decide
    what makes the most sense to your dataset and project.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 经常情况下，仅仅查看轮廓系数并不足以决定最佳的聚类数量。例如，一个具有大量聚类的聚类模型可以拥有很高的轮廓分数，但这并不能帮助我们从这个聚类模型中得出任何见解。由于聚类分析主要用于解释性分析，以从数据中提取见解和识别隐藏的模式，因此聚类结果的可解释性非常重要。将轮廓分数与二维或三维散点图相结合，将有助于你确定最佳聚类数量，并决定什么对你的数据集和项目最有意义。
- en: Summary
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored unsupervised learning and how it can be used to
    draw insights and identify hidden patterns in the data. Unlike other projects
    we have worked on so far, we did not have specific target variables that our ML
    models can learn from. We just had a raw online retail dataset, in which we had
    information about the items, quantities, and unit prices that customers bought
    on the online store. With this given dataset, we transformed transaction-level
    data into customer-level data and created numerous aggregate features. We learned
    how we can utilize the `AggregateRowsBy` method in Deedle's data frame to create
    aggregate features and transform the dataset to have a customer-centric view.
    We then briefly discussed a new library, `CenterSpace.NMath.Stats`, which we can
    use for various statistical computations. More specifically, we used the `StatsFunctions.PercentileRank`
    method to compute the percentiles of each record for a given feature.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了无监督学习及其如何用于从数据中提取见解和识别隐藏的模式。与迄今为止我们工作的其他项目不同，我们没有特定的目标变量，我们的机器学习模型可以从这些变量中学习。我们只有一个原始的在线零售数据集，其中包含了客户在在线商店购买的商品、数量和单价信息。使用这个给定的数据集，我们将交易级数据转换为客户级数据，并创建了大量的聚合特征。我们学习了如何利用Deedle的数据框中的`AggregateRowsBy`方法来创建聚合特征并将数据集转换为以客户为中心的视图。然后我们简要讨论了一个新的库，`CenterSpace.NMath.Stats`，我们可以用它来进行各种统计计算。更具体地说，我们使用了`StatsFunctions.PercentileRank`方法来计算给定特征的每个记录的百分位数。
- en: We covered how we can fit a k-means clustering algorithm using the `Accord.NET`
    framework. Using the k-means clustering algorithm, we were able to build a few
    clustering models with different numbers of clusters. We discussed how we can
    draw insights using the 4-cluster clustering model as an example and how we can
    cluster customers into different customer segments, where one segment's customer
    characteristics were high net revenue, above-average unit price, and above-average
    quantity, the other segment's customer characteristics were low net revenue, high
    average unit price, and below-average quantity, and so forth. We then looked at
    the top 10 items each customer segment purchased the most frequently and created
    scatter plots of different segments on our feature space.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了如何使用`Accord.NET`框架来拟合k-means聚类算法。使用k-means聚类算法，我们能够构建具有不同聚类数量的几个聚类模型。我们以4聚类聚类模型为例，讨论了如何通过它来提取见解，以及如何将客户聚类到不同的客户细分市场，其中某一细分市场的客户特征是高净收入、平均单价高于平均水平以及平均数量高于平均水平，而另一细分市场的客户特征是低净收入、高平均单价以及平均数量低于平均水平，等等。然后我们查看每个客户细分市场购买频率最高的前10个商品，并在我们的特征空间上创建了不同细分市场的散点图。
- en: Lastly, we used the **S****ilhouette Coefficient** to evaluate the cluster qualities,
    and learned how we can use this as one of the criteria for choosing the optimal
    number of clusters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用了**S**轮廓系数来评估聚类质量，并学习了如何将其作为选择最佳聚类数量的标准之一。
- en: From the next chapter, we are going to start building models for audio and image datasets.
    In the next chapter, we are going to discuss how to build a music genre recommendation
    model using a music audio dataset. We will learn how to build a ranking system
    where the output is the ranks of likelihood of individual categories. We will
    also learn what types of metrics to use to evaluate such a ranking model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从下一章开始，我们将开始构建音频和图像数据集的模型。在下一章中，我们将讨论如何使用音乐音频数据集构建音乐流派推荐模型。我们将学习如何构建一个输出为各个类别可能性排名的排名系统。我们还将学习使用哪些类型的指标来评估这样的排名模型。
