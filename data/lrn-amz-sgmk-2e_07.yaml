- en: 'Chapter 5: Training CV Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：训练计算机视觉模型
- en: In the previous chapter, you learned how to use SageMaker's built-in algorithms
    for traditional machine learning problems, including classification, regression,
    and anomaly detection. We saw that these algorithms work well on tabular data,
    such as CSV files. However, they are not well suited for image datasets, and they
    generally perform very poorly on **CV** (**CV**) tasks.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何使用 SageMaker 内置的算法来解决传统机器学习问题，包括分类、回归和异常检测。我们看到这些算法在处理表格数据（如 CSV
    文件）时效果良好。然而，它们不太适合图像数据集，通常在 **CV**（**计算机视觉**）任务上表现非常差。
- en: For a few years now, CV has taken the world by storm, and not a month goes by
    without a new breakthrough in extracting patterns from images and videos. In this
    chapter, you will learn about three built-in algorithms designed specifically
    for CV tasks. We'll discuss the types of problems that you can solve with them.
    We'll also spend a lot of time explaining how to prepare image datasets, as this
    crucial topic is often inexplicably overlooked. Of course, we'll train and deploy
    models too.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉（CV）已经席卷全球，几乎每个月都有新的突破，能够从图像和视频中提取模式。本章中，你将学习三种专为计算机视觉任务设计的内置算法。我们将讨论你可以用它们解决的各种问题，并详细解释如何准备图像数据集，因为这一关键主题经常被莫名其妙地忽视。当然，我们还会训练并部署模型。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括以下主题：
- en: Discovering the CV built-in algorithms in Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现 Amazon SageMaker 中的计算机视觉内置算法
- en: Preparing image datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备图像数据集
- en: 'Using the built-in CV algorithms: **image classification**, **object detection**,
    and **semantic segmentation**'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的计算机视觉算法：**图像分类**、**目标检测**和**语义分割**
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 AWS 账户来运行本章中的示例。如果你还没有账户，请访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建一个。你还应当熟悉 AWS 免费套餐（[https://aws.amazon.com/free/](https://aws.amazon.com/free/)），它允许你在一定的使用限制内免费使用许多
    AWS 服务。
- en: You will need to install and configure the AWS **Command Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置 AWS **命令行界面**（**CLI**）（[https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)）。
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory, but
    strongly encouraged, as it includes many projects that we will need (Jupyter,
    `pandas`, `numpy`, and more).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个可用的 Python 3.x 环境。安装 Anaconda 发行版（[https://www.anaconda.com/](https://www.anaconda.com/)）不是强制性的，但强烈建议安装，因为它包含了我们将需要的许多项目（Jupyter、`pandas`、`numpy`
    等）。
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可以在 GitHub 上找到：[https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装一个
    Git 客户端来访问这些示例（[https://git-scm.com/](https://git-scm.com/)）。
- en: Discovering the CV built-in algorithms in Amazon SageMaker
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现 Amazon SageMaker 中的计算机视觉内置算法
- en: 'SageMaker includes three CV algorithms, based on proven deep learning networks.
    In this section, you''ll learn about these algorithms, what kind of problem they
    can help you solve, and what their training scenarios are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 包括三种基于深度学习网络的计算机视觉算法。在本节中，你将了解这些算法，它们能帮助你解决什么样的问题，以及它们的训练场景：
- en: '**Image classification** assigns one or more labels to an image.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类**将一个或多个标签分配给图像。'
- en: '**Object detection** detects and classifies objects in an image.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测**是在图像中检测并分类物体。'
- en: '**Semantic segmentation** assigns every pixel of an image to a specific class.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**将图像的每一个像素分配到一个特定的类别。'
- en: Discovering the image classification algorithm
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现图像分类算法
- en: Starting from an input image, the **image classification** algorithm predicts
    a probability for each class present in the training dataset. This algorithm is
    based on the **ResNet** convolutional neural network ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)).
    Published in 2015, **ResNet** won the ILSVRC classification task that same year
    ([http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/)).
    Since then, it has become a popular and versatile choice for image classification.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入图像开始，**图像分类**算法为训练数据集中每个类别预测一个概率。该算法基于**ResNet**卷积神经网络（[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)）。**ResNet**于2015年发布，并在同年获得ILSVRC分类任务的冠军（[http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/)）。从那时起，它成为了一个流行且多用途的图像分类选择。
- en: Many hyperparameters can be set, including the depth of the network, which can
    range from 18 to 200 layers. In general, the more layers the network has, the
    better it will learn, at the expense of increased training times.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以设置许多超参数，包括网络的深度，范围从18层到200层。通常，网络层数越多，学习效果越好，但训练时间也会增加。
- en: Please note that the **image classification** algorithm supports both **single-label**
    and **multi-label** classification. We will focus on single-label classification
    in this chapter. Working with several labels is very similar, and you'll find
    a complete example at [https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**图像分类**算法支持**单标签**和**多标签**分类。在本章中，我们将重点讨论单标签分类。处理多个标签非常类似，您可以在[https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_mscoco_multi_label/)找到一个完整的示例。
- en: Discovering the object detection algorithm
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现物体检测算法
- en: Starting from an input image, the **object detection** algorithm predicts both
    the class and the location of each object in the image. Of course, the algorithm
    can only detect object classes present in the training dataset. The location of
    each object is defined by a set of four coordinates, called a **bounding box**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入图像开始，**物体检测**算法预测图像中每个物体的类别和位置。当然，该算法只能检测训练数据集中存在的物体类别。每个物体的位置由一组四个坐标定义，称为**边界框**。
- en: 'This algorithm is based on the **Single Shot MultiBox Detector** (**SSD**)
    architecture ([https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)).
    For classification, you can pick from two base networks: **VGG-16** ([https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556))
    or **ResNet-50**.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法基于**单次多框检测器**（**SSD**）架构（[https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)）。对于分类，您可以选择两个基础网络：**VGG-16**（[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)）或**ResNet-50**。
- en: 'The following output shows an example of object detection (source: [https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg](https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg)):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出展示了一个物体检测示例（来源：[https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg](https://www.dressagechien.net/wp-content/uploads/2017/11/chien-et-velo.jpg)）：
- en: '![Figure 5.1 – Test image'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 测试图像'
- en: '](img/B17705_05_1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_05_1.jpg)'
- en: Figure 5.1 – Test image
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 测试图像
- en: Discovering the semantic segmentation algorithm
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现语义分割算法
- en: Starting from an input image, the **semantic segmentation** algorithm predicts
    the class of every pixel of the image. This is a much harder problem than image
    classification (which only considers the full image) or object detection (which
    only focuses on specific parts of the image). Using the probabilities contained
    in a prediction, it's possible to build **segmentation masks** that cover specific
    objects in the picture.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入图像开始，**语义分割**算法预测图像中每个像素的类别。这比图像分类（仅考虑整幅图像）或物体检测（仅关注图像的特定部分）要困难得多。通过使用预测中包含的概率，可以构建**分割掩码**，覆盖图像中的特定物体。
- en: 'Three neural networks may be used for segmentation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 三种神经网络可以用于分割：
- en: '**Fully Convolutional Networks** (**FCNs**): [https://arxiv.org/abs/1411.4038](https://arxiv.org/abs/1411.4038)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全卷积网络**（**FCNs**）：[https://arxiv.org/abs/1411.4038](https://arxiv.org/abs/1411.4038)'
- en: '**Pyramid Scene Parsing** (**PSP**): [https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金字塔场景解析**（**PSP**）：[https://arxiv.org/abs/1612.01105](https://arxiv.org/abs/1612.01105)'
- en: '**DeepLab** v3: [https://arxiv.org/abs/1706.05587](https://arxiv.org/abs/1706.05587)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepLab** v3: [https://arxiv.org/abs/1706.05587](https://arxiv.org/abs/1706.05587)'
- en: The encoder network is **ResNet**, with either 50 or 101 layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器网络是**ResNet**，有 50 层或 101 层。
- en: 'The following output shows the result of segmenting the previous image. We
    see the segmentation masks, and each class is assigned a unique color; the background
    is black, and so on:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出显示了对前一张图像进行分割后的结果。我们可以看到分割掩膜，每个类别都分配了一个独特的颜色；背景为黑色，等等：
- en: '![Figure 5.2 – Segmented test image'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2 – 分割后的测试图像'
- en: '](img/B17705_05_2.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_05_2.jpg)'
- en: Figure 5.2 – Segmented test image
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 分割后的测试图像
- en: Now let's see how we can train these algorithms on our own data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在我们自己的数据上训练这些算法。
- en: Training with CV algorithms
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用计算机视觉（CV）算法进行训练
- en: 'All three algorithms are based on **supervised learning**, so our starting
    point will be a labeled dataset. Of course, the nature of these labels will be
    different for each algorithm:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种算法都是基于**监督学习**的，因此我们的起点将是一个带标签的数据集。当然，这些标签的性质对于每个算法会有所不同：
- en: Class labels for **image classification**
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类**的类别标签'
- en: Bounding boxes and class labels for **object detection**
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测**的边界框和类别标签'
- en: Segmentation masks and class labels for **semantic segmentation**
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**的分割掩膜和类别标签'
- en: Annotating image datasets is a lot of work. If you need to build your own dataset,
    **Amazon SageMaker Ground Truth** can definitely help, and we studied it in [*Chapter
    2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030), *Handling Data Preparation
    Techniques*. Later in this chapter, we'll show you how to use image datasets labeled
    with Ground Truth.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像数据集进行标注是一项繁重的工作。如果你需要构建自己的数据集，**Amazon SageMaker Ground Truth**绝对能帮助你，我们在[*第二章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)中研究了它，*处理数据准备技术*。在本章稍后部分，我们将展示如何使用标注过的图像数据集。
- en: When it comes to packaging datasets, the use of **RecordIO** files is strongly
    recommended ([https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio)).
    Packaging images in a small number of record-structured files makes it much easier
    to move datasets around and to split them for distributed training. Having said
    that, you can also train on individual image files if you prefer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在打包数据集时，强烈建议使用**RecordIO**文件（[https://mxnet.apache.org/api/faq/recordio](https://mxnet.apache.org/api/faq/recordio)）。将图像打包成少量的记录结构化文件，可以使数据集的移动和分割变得更容易，便于分布式训练。话虽如此，如果你愿意，也可以在单独的图像文件上进行训练。
- en: Once your dataset is ready in S3, you need to decide whether you'd like to train
    from scratch, or whether you'd like to start from a pretrained network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的数据集准备好并存储在 S3 上，你需要决定是从头开始训练，还是从一个预训练的网络开始。
- en: Training from scratch is fine if you have plenty of data, and if you're convinced
    that there's value in building a specific model with it. However, this will take
    a lot of time, possibly hundreds of epochs, and hyperparameter selection will
    be absolutely critical in getting good results.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有大量数据，而且确信从这些数据中构建特定模型是有价值的，那么从头开始训练是没问题的。然而，这将耗费大量时间，可能需要数百次的迭代，并且超参数选择在获得好结果方面至关重要。
- en: Using a pretrained network is generally a better option, even if you have lots
    of data. Thanks to **transfer learning**, you can start from a model trained on
    a huge collection of images (think millions) and fine-tune it on your data and
    classes. Training will be much shorter, and you will get models with higher accuracy
    rates quicker.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练网络通常是更好的选择，即使你有大量数据。得益于**迁移学习**，你可以从一个在大量图像（假设是数百万张）上训练的模型开始，然后根据你的数据和类别进行微调。这样训练的时间会大大缩短，并且你会更快地获得更高准确率的模型。
- en: Given the complexity of the models and the size of datasets, training with CPU
    instances is simply not an option. We'll use GPU instances for all examples.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于模型的复杂性和数据集的大小，使用 CPU 实例进行训练显然不可行。我们将为所有示例使用 GPU 实例。
- en: Last but not least, all three algorithms are based on **Apache MXNet**. This
    lets you export their models outside of SageMaker and deploy them anywhere you
    like.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，所有这三种算法都是基于**Apache MXNet**的。这使得你可以将它们的模型导出到 SageMaker 之外，并在任何地方进行部署。
- en: In the next sections, we're going to zoom in on image datasets, and how to prepare
    them for training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨图像数据集，以及如何为训练准备它们。
- en: Preparing image datasets
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备图像数据集
- en: 'Input formats are more complex for image datasets than for tabular datasets,
    and we need to get them exactly right. The CV algorithms in SageMaker support
    three input formats:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据集的输入格式比表格数据集更为复杂，我们需要确保格式完全正确。SageMaker中的计算机视觉算法支持三种输入格式：
- en: Image files
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像文件
- en: '**RecordIO** files'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RecordIO**文件'
- en: Augmented manifests built by **SageMaker Ground Truth**
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由**SageMaker Ground Truth**构建的增强清单
- en: In this section, you'll learn how to prepare datasets in these different formats.
    To the best of my knowledge, this topic has rarely been addressed in such detail.
    Get ready to learn a lot!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何准备这些不同格式的数据集。据我所知，这个话题很少被如此详细地讨论。准备好学习很多内容吧！
- en: Working with image files
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理图像文件
- en: This is the simplest format, and it's supported by all three algorithms. Let's
    see how to use it with the **image classification** algorithm.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的格式，所有三种算法都支持它。让我们看看如何将其与**图像分类**算法一起使用。
- en: Converting an image classification dataset to image format
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将图像分类数据集转换为图像格式
- en: A dataset in image format has to be stored in S3\. Image files don't need to
    be sorted in any way, and you simply could store all of them in the same bucket.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图像格式的数据集必须存储在S3中。图像文件不需要按任何特定方式排序，你可以将它们全部存储在同一个存储桶中。
- en: 'Images are described in a **list file**, a text file containing a line per
    image. For **image classification**, three columns are present: the unique identifier
    of the image, its class label, and its path. Here''s an example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述存储在**列表文件**中，列表文件是一个文本文件，每行代表一张图像。对于**图像分类**，文件中有三列：图像的唯一标识符、类标签以及路径。以下是一个示例：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first line tells us that `image2753.jpg` belongs to class 5 and has been
    assigned ID 1023.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行告诉我们，`image2753.jpg`属于第5类，且已分配ID 1023。
- en: You need a list file for each channel, so you would need one for the training
    dataset, one for the validation dataset, and so on. You can either write bespoke
    code to generate them, or you can use a simple program that is part of `im2rec`,
    and it's available in Python and C++. We'll use the Python version.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 每个通道都需要一个列表文件，因此你需要为训练数据集、验证数据集等创建一个。你可以编写定制的代码来生成它们，或者使用`im2rec`中的一个简单程序，该程序在Python和C++中都有。我们将使用Python版本。
- en: 'Let''s use the Dogs vs. Cats dataset available on **Kaggle** ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)).
    This dataset is 812 MB. Unsurprisingly, it contains two classes: dogs and cats.
    It''s already split for training and testing (25,000 and 12,500 images, respectively).
    Here''s how:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用**Kaggle**上的Dogs vs. Cats数据集（[https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)）。该数据集为812
    MB。不出所料，包含两类：狗和猫。它已经分为训练集和测试集（分别为25,000张和12,500张图片）。具体操作如下：
- en: We create a `kaggle` CLI ([https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`kaggle`命令行工具（[https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api)）。
- en: 'On our local machine, we download and extract the training dataset (you can
    ignore the test set, which is only needed for the competition):'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的本地机器上，我们下载并提取训练数据集（你可以忽略测试集，它只用于竞赛）：
- en: '[PRE1]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Dog and cat images are mixed up in the same folder. We create a subfolder for
    each class, and move the appropriate images there:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 狗和猫的图像混合在同一个文件夹中。我们为每个类别创建一个子文件夹，并将相应的图像移动到这些子文件夹中：
- en: '[PRE2]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We''ll need validation images, so let''s move 1,250 random dog images and 1,250
    random cat images to specific directories. I''m using `bash` scripting here, but
    feel free to use any tool you like:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要验证图像，因此让我们将1,250张随机狗图像和1,250张随机猫图像移动到指定目录。我这里使用`bash`脚本，但你可以根据需要使用任何工具：
- en: '[PRE3]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We move the remaining 22,500 images to the training folder:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将剩余的22,500张图片移动到训练文件夹：
- en: '[PRE4]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Our dataset now looks like this:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们的数据集如下所示：
- en: '[PRE5]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We download the `im2rec` tool from GitHub ([https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py)).
    It requires dependencies, which we need to install (you may have to adapt the
    command to your own environment and flavor of Linux):'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从GitHub下载`im2rec`工具（[https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py)）。它需要依赖项，我们需要安装这些依赖（你可能需要根据自己的环境和Linux版本调整命令）：
- en: '[PRE6]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We run `im2rec` to build two list files, one for training data and one for
    validation data:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行`im2rec`以生成两个列表文件，一个用于训练数据，另一个用于验证数据：
- en: '[PRE7]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 3197  0.000000  cat/cat.1625.jpg
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3197  0.000000  cat/cat.1625.jpg
- en: 15084 1.000000  dog/dog.12322.jpg
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 15084  1.000000  dog/dog.12322.jpg
- en: 1479  0.000000  cat/cat.11328.jpg
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1479  0.000000  cat/cat.11328.jpg
- en: 5262  0.000000  cat/cat.3484.jpg
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 5262  0.000000  cat/cat.3484.jpg
- en: 20714 1.000000  dog/dog.6140.jpg
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 20714 1.000000  dog/dog.6140.jpg
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We move the list files to specific directories. This is required because they
    will be passed to the `Estimator` as two new channels, `train_lst` and `validation_lst`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将列表文件移动到特定目录。这是必要的，因为它们将作为两个新通道`train_lst`和`validation_lst`传递给`Estimator`：
- en: '[PRE9]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The dataset now looks like this:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集现在看起来是这样的：
- en: '[PRE10]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we sync this folder to the SageMaker default bucket for future use.
    Please make sure to only sync the four folders, and nothing else:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将这个文件夹同步到SageMaker默认存储桶中以备后用。请确保只同步这四个文件夹，其他的不要同步：
- en: '[PRE11]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, let's move on to using the image format with the object detection algorithms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续在目标检测算法中使用图像格式。
- en: Converting detection datasets to image format
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将检测数据集转换为图像格式
- en: 'The general principle is identical. We need to build a file tree representing
    the four channels: `train`, `validation`, `train_annotation`, and `validation_annotation`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一般原则是相同的。我们需要构建一个文件树，表示四个通道：`train`、`validation`、`train_annotation`和`validation_annotation`。
- en: The main difference lies in how labeling information is stored. Instead of list
    files, we need to build JSON files.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于标签信息的存储方式。我们需要构建JSON文件，而不是列表文件。
- en: 'Here''s an example of a fictitious picture in an object detection dataset.
    For each object in the picture, we define the coordinates of the top-left corner
    of its bounding box, its height, and its width. We also define the class identifier,
    which points to a category array that also stores class names:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个虚构图像的示例，来自一个目标检测数据集。对于图像中的每个物体，我们定义其边界框的左上角坐标、其高度和宽度。我们还定义了类标识符，它指向一个类别数组，该数组还存储类名称：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We would need to do this for every picture in the dataset, building a JSON file
    for the training set and one for the validation set.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对数据集中的每张图片执行此操作，为训练集构建一个JSON文件，为验证集构建一个JSON文件。
- en: Finally, let's see how to use the image format with the semantic segmentation
    algorithm.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何在语义分割算法中使用图像格式。
- en: Converting segmentation datasets to image format
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将分割数据集转换为图像格式
- en: Image format is the only format supported by the image segmentation algorithm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图像格式是图像分割算法唯一支持的格式。
- en: 'This time, we need to build a file tree representing the four channels: `train`,
    `validation`, `train_annotation`, and `validation_annotation`. The first two channels
    contain the source images, and the last two contain the segmentation mask images.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们需要构建一个文件树，表示四个通道：`train`、`validation`、`train_annotation`和`validation_annotation`。前两个通道包含源图像，最后两个通道包含分割掩膜图像。
- en: 'File naming is critical in matching an image to its mask: the source image
    and the mask image must have the same name in their respective channels. Here''s
    an example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文件命名在将图像与其掩膜匹配时至关重要：源图像和掩膜图像必须在各自的通道中具有相同的名称。以下是一个示例：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can see sample pictures in the following figure. The source image on the
    left would go to the `train` folder and the mask picture on the right would go
    to the `train_annotation` folder. They would have to have exactly the same name
    so that the algorithm could match them:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下图中看到示例图像。左侧的源图像应放入`train`文件夹，右侧的掩膜图像应放入`train_annotation`文件夹。它们必须具有完全相同的名称，以便算法可以匹配它们：
- en: '![Figure 5.3 – Sample image from the Pascal VOC dataset'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3 – Pascal VOC 数据集中的示例图像'
- en: '](img/B17705_05_3.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_05_3.jpg)'
- en: Figure 5.3 – Sample image from the Pascal VOC dataset
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – Pascal VOC 数据集中的示例图像
- en: One clever feature of this format is how it matches class identifiers to mask
    colors. Mask images are PNG files with a 256-color palette. Each class in the
    dataset is assigned a specific entry in the color palette. These colors are the
    ones you see in masks for objects belonging to that class.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种格式的一个巧妙特点是它如何将类标识符与掩膜颜色匹配。掩膜图像是具有256色调色板的PNG文件。数据集中的每个类都分配了调色板中的一个特定条目。这些颜色就是你在该类物体的掩膜中看到的颜色。
- en: 'If your labeling tool or your existing dataset doesn''t support this PNG feature,
    you can add your own color mapping file. Please refer to the AWS documentation
    for details: [https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的标签工具或现有数据集不支持这种PNG特性，你可以添加自己的颜色映射文件。详情请参考AWS文档：[https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html](https://docs.aws.amazon.com/sagemaker/latest/dg/semantic-segmentation.html)。
- en: 'Now, let''s prepare the **Pascal VOC** dataset. This dataset is frequently
    used to benchmark object detection and semantic segmentation model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们准备 **Pascal VOC** 数据集。这个数据集常用于对目标检测和语义分割模型进行基准测试：
- en: 'We first download and extract the 2012 version of the dataset. Again, I recommend
    using an AWS-hosted instance to speed up network transfers:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先下载并提取数据集的 2012 版本。我建议使用 AWS 托管的实例来加速网络传输：
- en: '[PRE14]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create a work directory where we''ll build the four channels:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个工作目录，在这里构建四个通道：
- en: '[PRE15]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Using the list of training files defined in the dataset, we copy the corresponding
    images to the `train` folder. I''m using `bash` scripting here; feel free to use
    your tool of choice:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据集中定义的训练文件列表，我们将相应的图像复制到 `train` 文件夹。我在这里使用的是 `bash` 脚本，当然你也可以使用你喜欢的工具：
- en: '[PRE16]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then do the same for validation images, training masks, and validation masks:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们对验证图像、训练掩码和验证掩码做相同的处理：
- en: '[PRE17]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We check that we have the same number of images in the two training channels,
    and in the two validation channels:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查两个训练通道和两个验证通道中的图像数量是否一致：
- en: '[PRE18]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We see 1,464 training files and masks, and 1,449 validation files and masks.
    We''re all set:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到了 1,464 个训练文件和掩码，以及 1,449 个验证文件和掩码。我们准备好了：
- en: '[PRE19]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The last step is to sync the file tree to S3 for later use. Again, please make
    sure to sync only the four folders:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是将文件树同步到 S3 以备后用。再次提醒，请确保只同步四个文件夹：
- en: '[PRE20]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We know how to prepare classification, detection, and segmentation datasets
    in image format. This is a critical step, and you have to get things exactly right.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何准备图像格式的分类、检测和分割数据集。这是一个关键步骤，你必须确保一切都准确无误。
- en: Still, I'm sure that you found the steps in this section a little painful. So
    did I! Now imagine doing the same with millions of images. That doesn't sound
    very exciting, does it?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我敢肯定你会觉得本节中的步骤有些麻烦，我也是！现在想象一下，面对数百万张图像做同样的事情。这听起来可不太刺激，对吧？
- en: We need an easier way to prepare image datasets. Let's see how we can simplify
    dataset preparation with **RecordIO** files.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种更简便的方法来准备图像数据集。让我们看看如何使用 **RecordIO** 文件简化数据集准备。
- en: Working with RecordIO files
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RecordIO 文件
- en: RecordIO files are easier to move around. It's much more efficient for an algorithm
    to read a large sequential file than to read lots of tiny files stored at random
    disk locations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: RecordIO 文件更容易搬动。对于算法来说，读取一个大的顺序文件比读取很多存储在随机磁盘位置的小文件更高效。
- en: Converting an image classification dataset to RecordIO
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将图像分类数据集转换为 RecordIO
- en: 'Let''s convert the Dogs vs. Cats dataset to RecordIO:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 Dogs vs. Cats 数据集转换为 RecordIO：
- en: 'Starting from a freshly extracted copy of the dataset, we move the images to
    the appropriate class folder:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从新提取的数据集副本开始，我们将图像移到相应的类别文件夹中：
- en: '[PRE21]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We run `im2rec` to generate list files for the training dataset (90%) and the
    validation dataset (10%). There's no need to split the dataset ourselves!
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行 `im2rec` 来为训练数据集（90%）和验证数据集（10%）生成列表文件。我们无需自己分割数据集！
- en: '[PRE22]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We run `im2rec` once more to generate the RecordIO files:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次运行 `im2rec` 来生成 RecordIO 文件：
- en: '[PRE23]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: $ ls dogscats*
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $ ls dogscats*
- en: dogscats_train.idx dogscats_train.lst dogscats_train.rec
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dogscats_train.idx dogscats_train.lst dogscats_train.rec
- en: dogscats_val.idx dogscats_val.lst dogscats_val.rec
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dogscats_val.idx dogscats_val.lst dogscats_val.rec
- en: '[PRE24]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s store the RecordIO files in S3, as we''ll use them later:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 RecordIO 文件存储在 S3 上，因为我们稍后会用到它们：
- en: '[PRE25]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This was much simpler, wasn't it? `im2rec` has additional options to resize
    images and more. It can also break the dataset into several chunks, a useful technique
    for **Pipe Mode** and **Distributed Training**. We'll study them in [*Chapter
    9*](B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168), *Scaling Your Training Jobs*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这要简单得多，不是吗？`im2rec` 还提供了额外的选项来调整图像大小等等。它还可以将数据集分割成几个块，这是 **管道模式** 和 **分布式训练**
    中一个有用的技术。我们将在[*第 9 章*](B17705_09_Final_JM_ePub.xhtml#_idTextAnchor168)，“*扩展你的训练任务*”中学习这些内容。
- en: Now, let's move on to using RecordIO files for object detection.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续使用 RecordIO 文件进行目标检测。
- en: Converting an object detection dataset to RecordIO
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将目标检测数据集转换为 RecordIO
- en: The process is very similar. A major difference is the format of list files.
    Instead of dealing only with class labels, we also need to store bounding boxes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 过程非常相似。一个主要的区别是列表文件的格式。我们不仅要处理类标签，还需要存储边界框。
- en: 'Let''s see what this means for the Pascal VOC dataset. The following image
    is taken from the dataset:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这对 Pascal VOC 数据集意味着什么。以下图像取自该数据集：
- en: '![Figure 5.4 – Sample image from the Pascal VOC dataset'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4 – 来自 Pascal VOC 数据集的示例图像'
- en: '](img/B17705_05_4.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_05_4.jpg)'
- en: Figure 5.4 – Sample image from the Pascal VOC dataset
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 来自 Pascal VOC 数据集的示例图像
- en: 'It contains three chairs. The labeling information is stored in an individual
    **XML** file, shown in slightly abbreviated form:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含三个椅子。标注信息存储在一个单独的 **XML** 文件中，文件内容略有简化，如下所示：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Converting this to a list file entry should look like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将此转换为列表文件条目的格式应如下所示：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s decode each column:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解码每一列：
- en: '`9404` is a unique image identifier.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`9404` 是唯一的图像标识符。'
- en: '`2` is the number of columns containing header information, including this
    one.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2` 是包含头信息的列数，包括这一列。'
- en: '`6` is the number of columns for labeling information. These six columns are
    the class identifier, the four bounding-box coordinates, and a flag telling us
    whether the object is difficult to see (we won''t use it).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`6` 是标注信息的列数。这六列分别是类别标识符、四个边界框坐标，以及一个标志，告诉我们该对象是否难以看到（我们不会使用它）。'
- en: 'The following is for the first object:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是第一个对象的信息：
- en: a) `8` is the class identifier. Here, `8` is the `chair` class.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `8` 是类别标识符。这里，`8` 代表 `chair` 类别。
- en: b) `0.0022 0.6607 0.2612 1.0000` are the relative coordinates of the `0` means
    that the object is not difficult.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `0.0022 0.6607 0.2612 1.0000` 是 `0` 表示该对象不困难的相对坐标。
- en: 'For the second object, we have the following:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第二个对象，我们有如下信息：
- en: a) `8` is the class identifier.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `8` 是类别标识符。
- en: b) `0.9576 0.6429 1.0000 0.8839` are the coordinates of the second object.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `0.9576 0.6429 1.0000 0.8839` 是第二个对象的坐标。
- en: c) `1` means that the object is difficult.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `1` 表示该对象是困难的。
- en: 'The third object has the following:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个对象具有以下信息：
- en: a) `8` is the class identifier.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `8` 是类别标识符。
- en: b) `0.6272 0.4435 0.7076 0.628` are the coordinates of the third object.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `0.6272 0.4435 0.7076 0.628` 是第三个对象的坐标。
- en: c) `1` means that the object is difficult.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `1` 表示该对象是困难的。
- en: '`VOC2007/JPEGImages/003988.jpg` is the path to the image.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VOC2007/JPEGImages/003988.jpg` 是图片的路径。'
- en: So, how do we convert thousands of XML files into a couple of list files? Unless
    you enjoy writing parsers, this isn't a very exciting task.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何将成千上万的 XML 文件转换为几个列表文件呢？除非你喜欢写解析器，否则这并不是一项非常有趣的任务。
- en: 'Fortunately, our work has been cut out for us. Apache MXNet includes a Python
    script, `prepare_dataset.py`, that will handle this task. Let''s see how it works:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们的工作已经简化了。Apache MXNet 包含一个 Python 脚本 `prepare_dataset.py`，可以处理这项任务。让我们看看它是如何工作的：
- en: 'For the next steps, I recommend using an Amazon Linux 2 EC2 instance with at
    least 10 GB of storage. Here are the setup steps:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于接下来的步骤，我建议使用至少有 10 GB 存储的 Amazon Linux 2 EC2 实例。以下是设置步骤：
- en: '[PRE28]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Download the 2007 and 2012 Pascal VOC datasets with `wget`, and extract them
    in the same directory:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `wget` 下载 2007 和 2012 版本的 Pascal VOC 数据集，并将它们提取到同一个目录中：
- en: '[PRE29]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Clone the Apache MXNet repository ([https://github.com/apache/incubator-mxnet/](https://github.com/apache/incubator-mxnet/)):'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆 Apache MXNet 仓库 ([https://github.com/apache/incubator-mxnet/](https://github.com/apache/incubator-mxnet/))：
- en: '[PRE30]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Run the `prepare_dataset.py` script to build our training dataset, merging
    the training and validation sets of the 2007 and 2012 versions:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `prepare_dataset.py` 脚本来构建我们的训练数据集，将 2007 和 2012 版本的训练集和验证集合并：
- en: '[PRE31]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s follow similar steps to generate our validation dataset, using the test
    set of the 2007 version:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们遵循类似的步骤来生成验证数据集，使用 2007 版本的测试集：
- en: '[PRE32]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the top-level directory, we see the files generated by the script. Feel
    free to take a look at the list files; they should have the format presented previously:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在顶级目录中，我们看到脚本生成的文件。可以随意查看文件列表，它们应该与之前展示的格式一致：
- en: '[PRE33]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let''s store the RecordIO files in S3 as we''ll use them later:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将 RecordIO 文件存储到 S3，因为我们稍后会使用它们：
- en: '[PRE34]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `prepare_dataset.py` script has really made things simple here. It also
    supports the **COCO** dataset ([http://cocodataset.org](http://cocodataset.org)),
    and the workflow is extremely similar.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`prepare_dataset.py` 脚本确实简化了很多事情。它还支持 **COCO** 数据集 ([http://cocodataset.org](http://cocodataset.org))，并且工作流程非常相似。'
- en: What about converting other public datasets? Well, your mileage may vary. You'll
    find more examples at [https://gluon-cv.mxnet.io/build/examples_datasets/index.html](https://gluon-cv.mxnet.io/build/examples_datasets/index.html).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何转换其他公共数据集呢？嗯，结果可能因情况而异。你可以在 [https://gluon-cv.mxnet.io/build/examples_datasets/index.html](https://gluon-cv.mxnet.io/build/examples_datasets/index.html)
    上找到更多示例。
- en: RecordIO is definitely a step forward. Still, when working with custom datasets,
    it's very likely that you'll have to write your own list file generator. That's
    not a huge deal, but it's extra work.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: RecordIO 绝对是一个进步。然而，在处理自定义数据集时，很可能你需要编写自己的列表文件生成器。这虽然不是一件大事，但也是额外的工作。
- en: Datasets labeled with **Amazon SageMaker Ground Truth** solve these problems
    altogether. Let's see how this works!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**Amazon SageMaker Ground Truth**标注的数据集解决了这些问题。让我们看看它是如何工作的！
- en: Working with SageMaker Ground Truth files
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与SageMaker Ground Truth文件合作
- en: 'In [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030), *Handling
    Data Preparation Techniques*, you learned about SageMaker Ground Truth workflows
    and their outcome, an **augmented manifest** file. This file is in **JSON Lines**
    format: each JSON object describes a specific annotation.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)《数据准备技巧处理》中，您了解了SageMaker
    Ground Truth工作流及其结果，一个**增强的清单**文件。该文件采用**JSON Lines**格式：每个JSON对象描述了一个特定的标注。
- en: 'Here''s an example from the semantic segmentation job we ran in [*Chapter 2*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030),
    *Handling Data Preparation Techniques* (the story is the same for other task types).
    We see the paths to the source image and the segmentation mask, as well as color
    map information telling us how to match mask colors to classes:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在[*第2章*](B17705_02_Final_JM_ePub.xhtml#_idTextAnchor030)《数据准备技巧处理》中运行的语义分割任务的一个示例（对于其他任务类型，流程相同）。我们看到原始图像和分割掩模的路径，以及颜色映射信息，告诉我们如何将掩模颜色与类别匹配：
- en: '[PRE35]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following images are the ones referenced in the preceding JSON document:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面JSON文档中引用的图像：
- en: '![Figure 5.5 – Source image and segmented image'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5 – 原始图像和分割图像'
- en: '](img/B17705_05_5.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_05_5.jpg)'
- en: Figure 5.5 – Source image and segmented image
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 原始图像和分割图像
- en: This is exactly what we would need to train our model. In fact, we can pass
    the augmented manifest to the SageMaker `Estimator` as is. No data processing
    is required whatsoever.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们训练模型所需要的。事实上，我们可以将增强的清单直接传递给SageMaker的`Estimator`，完全不需要任何数据处理。
- en: 'To use an **augmented manifest** pointing at labeled images in S3, we would
    simply pass its location and the name of the JSON attributes (highlighted in the
    previous example):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用指向S3中标注图像的**增强清单**，我们只需传递其位置和JSON属性的名称（在前面的示例中已突出显示）：
- en: '[PRE36]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: That's it! This is much simpler than anything we've seen before.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这比我们之前看到的任何内容都要简单。
- en: You can find more examples of using SageMaker Ground Truth at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/ground_truth_labeling_jobs)找到更多使用SageMaker
    Ground Truth的示例。
- en: Now that we know how to prepare image datasets for training, let's put the CV
    algorithms to work.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何为训练准备图像数据集，让我们开始使用计算机视觉（CV）算法。
- en: Using the built-in CV algorithms
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内置的计算机视觉（CV）算法
- en: In this section, we're going to train and deploy models with all three algorithms
    using public image datasets. We will cover both training from scratch and transfer
    learning.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用公共图像数据集，通过所有三种算法训练并部署模型。我们将涵盖从头开始训练和迁移学习两种方法。
- en: Training an image classification model
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练图像分类模型
- en: In this first example, let's use the image classification algorithm to build
    a model classifying the Dogs vs. Cats dataset that we prepared in a previous section.
    We'll first train using image format, and then using RecordIO format.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个示例中，我们将使用图像分类算法来构建一个模型，分类我们在前一节准备的狗与猫数据集。我们将首先使用图像格式进行训练，然后使用RecordIO格式。
- en: Training in image format
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用图像格式进行训练
- en: 'We will begin training using the following steps:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤开始训练：
- en: 'In a Jupyter notebook, we define the appropriate data paths:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Jupyter笔记本中，我们定义了适当的数据路径：
- en: '[PRE37]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We configure the `Estimator` for the image classification algorithm:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为图像分类算法配置`Estimator`：
- en: '[PRE38]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We use a GPU instance called `ml.p3.2xlarge`, which packs more than enough punch
    for this dataset ($4.131/hour in eu-west-1).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用名为`ml.p3.2xlarge`的GPU实例，这个实例对这个数据集提供了足够的计算力（在eu-west-1区域的价格为每小时$4.131）。
- en: 'What about hyperparameters ([https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html))?
    We set the number of classes (2) and the number of training samples (22,500).
    Since we''re working with the image format, we need to resize images explicitly,
    setting the smallest dimension to 224 pixels. As we have enough data, we decide
    to train from scratch. In order to keep the training time low, we settle for an
    18-layer **ResNet** model, and we train only for 10 epochs:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超参数怎么样（[https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)）？我们设置类别数（2）和训练样本数（22,500）。由于我们使用图像格式，我们需要显式调整图像大小，将最小尺寸设置为224像素。由于数据充足，我们决定从头开始训练。为了保持训练时间短，我们选择一个18层的**ResNet**模型，只训练10个epochs：
- en: '[PRE39]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We define the four channels, setting their content type to `application/x-image`:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义四个通道，设置它们的内容类型为`application/x-image`：
- en: '[PRE40]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We launch the training job as follows:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按以下方式启动训练作业：
- en: '[PRE41]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In the training log, we see that data download takes about 3 minutes. Surprise,
    surprise: we also see that the algorithm builds RecordIO files before training.
    This step lasts about 1 minute:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练日志中，我们看到数据下载大约需要3分钟。令人惊讶的是，我们还看到算法在训练之前构建RecordIO文件。这一步骤大约持续1分钟：
- en: '[PRE42]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As training starts, we see that an epoch takes approximately 22 seconds:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练开始时，我们看到一个epoch大约需要22秒：
- en: '[PRE43]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The job lasts 506 seconds in total (about 8 minutes), costing us (506/3600)*$4.131=$0.58\.
    It reaches a validation accuracy of **91.2%** (hopefully, you see something similar).
    This is pretty good considering that we haven't even tweaked the hyperparameters
    yet.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该作业总共持续了506秒（约8分钟），花费了我们（506/3600）*$4.131=$0.58。它达到了**91.2%**的验证准确率（希望您看到类似的结果）。考虑到我们甚至还没有调整超参数，这是相当不错的结果。
- en: 'We then deploy the model on a small CPU instance as follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在小型CPU实例上部署模型如下：
- en: '[PRE44]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We download the following test image and send it for prediction in `application/x-image`
    format.![Figure 5.6 – Test picture
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载以下测试图像，并以`application/x-image`格式发送进行预测。![图5.6 – 测试图片
- en: '](img/B17705_05_6.jpg)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_05_6.jpg)'
- en: '[PRE45]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Printing out the probability and the class, our model indicates that this image
    is a dog with 99.997% confidence and that the image belongs to class 1:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印出概率和类别后，我们的模型指示这是一只狗，置信度为99.997%，并且图像属于类别1：
- en: '[PRE46]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'When we''re done, we delete the endpoint as follows:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，我们按以下方式删除端点：
- en: '[PRE47]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now let's run the same training job with the dataset in RecordIO format.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用RecordIO格式的数据集运行相同的训练作业。
- en: Training in RecordIO format
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练RecordIO格式
- en: 'The only difference is how we define the input channels. We only need two channels
    this time in order to serve the RecordIO files we uploaded to S3\. Accordingly,
    the content type is set to `application/x-recordio`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于我们如何定义输入通道。这次我们只需要两个通道，以便提供我们上传到S3的RecordIO文件。因此，内容类型设置为`application/x-recordio`：
- en: '[PRE48]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Training again, we see that data download takes 1 minute and that the file generation
    step has disappeared. Although it's difficult to draw any conclusion from a single
    run, using RecordIO datasets will generally save you time and money, even when
    training on a single instance.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 再次训练时，我们看到数据下载需要1分钟，文件生成步骤已消失。虽然从单次运行中很难得出任何结论，但使用RecordIO数据集通常会节省您时间和金钱，即使在单个实例上进行训练。
- en: The Dogs vs. Cats dataset has over 10,000 samples per class, which is more than
    enough to train from scratch. Now, let's try a dataset where that's not the case.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 狗与猫数据集每类样本超过10,000个，足以从头开始训练。现在，让我们尝试一个数据集，情况不同。
- en: Fine-tuning an image classification model
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调图像分类模型
- en: 'Please consider the **Caltech-256** dataset, a popular public dataset of 15,240
    images in 256 classes, plus a clutter class ([http://www.vision.caltech.edu/Image_Datasets/Caltech256/](http://www.vision.caltech.edu/Image_Datasets/Caltech256/)).
    Browsing image categories, we see that all classes have a small number of samples.
    For instance, the "duck" class only has 60 images: it''s doubtful that a deep
    learning algorithm, no matter how sophisticated, could extract the unique visual
    features of ducks with that little data.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑**Caltech-256**数据集，这是一个受欢迎的公共数据集，包含256类共15,240张图像，还有一个混乱的类别（[http://www.vision.caltech.edu/Image_Datasets/Caltech256/](http://www.vision.caltech.edu/Image_Datasets/Caltech256/)）。浏览图像类别，我们发现所有类别都只有少量样本。例如，“鸭子”类仅有60张图像：用这么少的数据，无论多么复杂的深度学习算法都很难提取出鸭子的独特视觉特征。
- en: In such cases, training from scratch is simply not an option. Instead, we will
    use a technique called **transfer learning**, where we start from a network that
    has already been trained on a very large and diverse image dataset. **ImageNet**
    ([http://www.image-net.org/](http://www.image-net.org/)) is probably the most
    popular choice for pretraining, with 1,000 classes and millions of images.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，从头开始训练根本不可行。相反，我们将使用一种叫做**迁移学习**的技术，在一个已经在非常大且多样化的图像数据集上训练过的网络基础上开始训练。**ImageNet**（[http://www.image-net.org/](http://www.image-net.org/)）可能是最流行的预训练选择，包含1,000个类别和数百万张图像。
- en: The pretrained network has already learned how to extract patterns from complex
    images. Assuming that the images in our dataset are similar enough to those in
    the pretraining dataset, our model should be able to inherit that knowledge. Training
    for only a few more epochs on our dataset, we should be able to **fine-tune**
    the pretrained model on our data and classes.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的网络已经学会了如何从复杂图像中提取特征。假设我们数据集中的图像与预训练数据集中的图像足够相似，我们的模型应该能够继承这些知识。只需在我们的数据集上训练几个周期，我们应该能够**微调**预训练模型，以适应我们的数据和类别。
- en: 'Let''s see how we can easily do this with SageMaker. In fact, we''ll reuse
    the code for the previous example with minimal changes. Let''s get into it:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过SageMaker轻松实现这一点。实际上，我们将重用前一个示例中的代码，只做最小的修改。我们开始吧：
- en: 'We download the Caltech-256 in RecordIO format (if you''d like, you could download
    it in image format, and convert it to RecordIO: practice makes perfect!):'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载Caltech-256数据集的RecordIO格式版本（如果你愿意，可以下载图像格式，并转换成RecordIO格式：实践出真知！）：
- en: '[PRE49]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We upload the dataset to S3:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据集上传到S3：
- en: '[PRE50]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We configure the `Estimator` function for the image classification algorithm.
    The code is strictly identical to *step 2* in the previous example.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置`Estimator`函数来进行图像分类算法。代码与前一个示例中的*步骤2*完全相同。
- en: We use `use_pretrained_network` to 1\. The final fully connected layer of the
    pretrained network will be resized to the number of classes present in our dataset,
    and its weights will be assigned random values.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`use_pretrained_network`来1\. 预训练网络的最终全连接层将根据我们数据集中的类别数量进行调整，并将其权重赋予随机值。
- en: 'We set the correct number of classes (256+1) and training samples as follows:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设置正确的类别数量（256+1）和训练样本数，如下所示：
- en: '[PRE51]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Since we're fine-tuning, we only train for 5 epochs, with a smaller learning
    rate of 0.001.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们在进行微调，所以我们只训练5个周期，并使用较小的学习率0.001。
- en: We configure channels and we launch the training job. The code is strictly identical
    to *step 4* in the previous example.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置通道，并启动训练任务。代码与前一个示例中的*步骤4*完全相同。
- en: 'After 5 epochs and 272 seconds, we see the following metric in the training
    log:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在经过5个周期和272秒后，我们在训练日志中看到以下指标：
- en: '[PRE52]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This is quite good for just a few minutes of training. Even with enough data,
    it would have taken much longer to get that result from scratch.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对于仅仅几分钟的训练来说已经相当不错。即使数据量充足，从头开始也需要更长的时间才能获得这样的结果。
- en: To deploy and test the model, we would reuse *steps 7-9* in the previous example.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了部署和测试模型，我们将重用前一个示例中的*步骤7-9*。
- en: As you can see, transfer learning is a very powerful technique. It can deliver
    excellent results, even when you have little data. You will also train for fewer
    epochs, saving time and money in the process.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，迁移学习是一种非常强大的技术。即使数据量很少，它也能提供优秀的结果。你还会训练更少的周期，从而节省时间和资金。
- en: Now, let's move on to the next algorithm, **object detection**.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入下一个算法——**物体检测**。
- en: Training an object detection model
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个物体检测模型
- en: 'In this example, we''ll use the object detection algorithm to build a model
    on the Pascal VOC dataset that we prepared in a previous section:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用物体检测算法，在前一部分准备好的Pascal VOC数据集上构建一个模型：
- en: 'We start by defining data paths:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从定义数据路径开始：
- en: '[PRE53]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We select the object detection algorithm:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择物体检测算法：
- en: '[PRE54]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We configure the `Estimator`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置`Estimator`：
- en: '[PRE55]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We set the required hyperparameters. We select a pretrained ResNet-50 network
    for the base network. We set the number of classes and training samples. We settle
    on 30 epochs, which should be enough to start seeing results:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置所需的超参数。我们选择一个预训练的ResNet-50网络作为基础网络。我们设置类别数量和训练样本数。我们决定训练30个周期，应该足够开始看到结果：
- en: '[PRE56]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We then configure the two channels, and we launch the training job:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们配置两个通道，并启动训练任务：
- en: '[PRE57]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Selecting our job in **SageMaker components and registries** | **Experiments
    and trials**, we can see near-real-time metrics and charts. The next image shows
    the validation **mean average precision metric (mAP)**, a key metric for object
    detection models.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在**SageMaker组件与注册表** | **实验与试验**中选择我们的任务，我们可以看到接近实时的指标和图表。下一张图显示了验证集的**平均精度指标（mAP）**，这是目标检测模型的一个关键指标。
- en: '![Figure 5.7 – Validation mAP'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.7 – 验证集 mAP'
- en: '](img/B17705_05_7.jpg)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_05_7.jpg)'
- en: Figure 5.7 – Validation mAP
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.7 – 验证集 mAP
- en: Please explore the other tabs (**Metrics**, **Parameters**, **Artifacts**, and
    so on). They contain everything you need to know about a particular job. Please
    note the **Stop training job** button in the top-right corner, which you can use
    to terminate a job at any time.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请浏览其他标签页（**指标**、**参数**、**工件**等）。它们包含了关于特定任务的所有信息。请注意右上角的**停止训练任务**按钮，您可以随时使用它来终止任务。
- en: Training lasts for 1 hour and 40 minutes. This is a pretty heavy model! We get
    a **mean average precision metric** (**mAP**) of 0.5151\. Production use would
    require more training, but we should be able to test the model already.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练持续了1小时40分钟。这个模型相当庞大！我们得到了**平均精度指标**（**mAP**）为0.5151。生产环境中使用需要更多的训练，但我们现在应该可以测试模型了。
- en: 'Given its complexity, we deploy the model to a larger CPU instance:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于模型的复杂性，我们将其部署到一个更大的CPU实例上：
- en: '[PRE58]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We download a test image from Wikipedia and predict it with our model:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从维基百科下载一张测试图像，并用我们的模型进行预测：
- en: '[PRE59]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The response contains a list of predictions. Each individual prediction contains
    a class identifier, the confidence score, and the relative coordinates of the
    bounding box. Here are the first predictions in the response:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 响应包含一个预测列表。每个单独的预测包含一个类别标识符、置信度得分以及边界框的相对坐标。以下是响应中的前几个预测：
- en: '[PRE60]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Using this information, we could plot the bounding boxes on the source image.
    For the sake of brevity, I will not include the code here, but you''ll find it
    in the GitHub repository for this book. The following output shows the result:'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用这些信息，我们可以在源图像上绘制边界框。为了简洁起见，我不会在此处包含代码，但您可以在本书的GitHub仓库中找到。以下输出展示了结果：
- en: '![Figure 5.8 – Test image'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.8 – 测试图像'
- en: '](img/B17705_05_8.jpg)'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_05_8.jpg)'
- en: Figure 5.8 – Test image
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.8 – 测试图像
- en: 'When we''re done, we delete the endpoint as follows:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们完成后，我们删除端点，如下所示：
- en: '[PRE61]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This concludes our exploration of object detection. We have one more algorithm
    to go: **Semantic Segmentation**.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们的目标检测探索。我们还有一个算法要介绍：**语义分割**。
- en: Training a semantic segmentation model
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练语义分割模型
- en: 'In this example, we''ll use the semantic segmentation algorithm to build a
    model on the Pascal VOC dataset that we prepared in a previous section:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用语义分割算法，在之前章节中准备的Pascal VOC数据集上构建模型：
- en: 'As usual, we define the data paths, as follows:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如常，我们定义数据路径，如下所示：
- en: '[PRE62]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We select the semantic segmentation algorithm, and we configure the `Estimator`
    function:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择语义分割算法，并配置`Estimator`函数：
- en: '[PRE63]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We define the required hyperparameters. We select a pretrained ResNet-50 network
    for the base network and a pretrained **FCN** for detection. We set the number
    of classes and training samples. Again, we settle on 30 epochs, which should be
    enough to start seeing results:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了所需的超参数。我们选择了一个预训练的ResNet-50网络作为基础网络，并选择一个预训练的**FCN**进行检测。我们设置了类别数和训练样本数。再次，我们选择了30个训练周期，这应该足以开始看到结果：
- en: '[PRE64]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We configure the four channels, setting the content type to `image/jpeg` for
    source images, and `image/png` for mask images. Then, we launch the training job:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置四个通道，设置源图像的内容类型为`image/jpeg`，掩膜图像的内容类型为`image/png`。然后，我们启动训练任务：
- en: '[PRE65]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Training lasts about 32 minutes. We get a **mean intersection-over-union metric**
    (**mIOU**) of 0.4874, as shown in the following plot:![Figure 5.9 – Validation
    mIOU
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练大约持续了32分钟。我们得到了**平均交并比指标**（**mIOU**）为0.4874，如下图所示：![图 5.9 – 验证集 mIOU
- en: '](img/B17705_05_9.jpg)'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_05_9.jpg)'
- en: Figure 5.9 – Validation mIOU
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.9 – 验证集 mIOU
- en: 'We deploy the model to a CPU instance:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将模型部署到一个CPU实例上：
- en: '[PRE66]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Once the endpoint is in service, we grab a test image, and we send it for prediction
    as a byte array with the appropriate content type:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点开始服务，我们获取一张测试图像，并将其作为字节数组发送进行预测，附上正确的内容类型：
- en: '[PRE67]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Using the **Python Imaging Library** (**PIL**), we process the response mask
    and display it:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**Python图像库**（**PIL**），我们处理响应掩膜并显示它：
- en: '[PRE68]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The following images show the source image and the predicted mask. This result
    is promising, and would improve with more training:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图像展示了源图像和预测的掩码。这个结果很有前景，随着更多训练，它会得到改进：
- en: '![Figure 5.10 – Test image and segmented test image'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.10 – 测试图像和分割后的测试图像'
- en: '](img/B17705_05_10.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_05_10.jpg)'
- en: Figure 5.10 – Test image and segmented test image
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.10 – 测试图像和分割后的测试图像
- en: 'Predicting again with the `application/x-protobuf` accept type, we receive
    class probabilities for all pixels in the source image. The response is a protobuf
    buffer, which we save to a binary file:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次使用`application/x-protobuf`接收类型进行预测时，我们会收到源图像中所有像素的类别概率。响应是一个protobuf缓冲区，我们将其保存到一个二进制文件中：
- en: '[PRE69]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The buffer contains two tensors: one with the shape of the probability tensor,
    and one with the actual probabilities. We load them using `values` tensor describes
    one image of size 289x337, where each pixel is assigned 21 probabilities, one
    for each of the Pascal VOC classes. You can check that 289*337*21=2,045,253.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该缓冲区包含两个张量：一个是概率张量的形状，另一个是实际的概率值。我们使用`values`张量加载它们，`values`张量描述的是一个289x337大小的图像，其中每个像素都被分配了21个概率值，分别对应Pascal
    VOC的每个类别。您可以验证289*337*21=2,045,253。
- en: 'Knowing that, we can now reshape the `values` tensor, retrieve the 21 probabilities
    for the (0,0) pixel, and print the class identifier with the highest probability:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 知道这些之后，我们现在可以重新塑形`values`张量，获取(0,0)像素的21个概率，并打印出概率最高的类别标识符：
- en: '[PRE70]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Here is the output:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE71]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The highest probability is at index 0: the predicted class for pixel (0,0)
    is class 0, the background class.'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最高概率位于索引0：像素(0,0)的预测类别为类别0，即背景类别。
- en: 'When we''re done, we delete the endpoint as follows:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，我们可以按如下方式删除端点：
- en: '[PRE72]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As you can see, these three algorithms make it easy to train CV models. Even
    with default hyperparameters, we get good results pretty quickly. Still, we start
    feeling the need to scale our training jobs. Don't worry once the relevant features
    have been covered in future chapters, we'll revisit some of our CV examples and
    we'll scale them radically!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这三种算法使得训练计算机视觉（CV）模型变得简单。即使使用默认的超参数，我们也能很快得到不错的结果。不过，我们开始感觉到需要扩展训练工作。别担心，一旦相关内容在后续章节中讲解完毕，我们会再次回顾一些CV示例，并大幅扩展它们！
- en: In this chapter, you learned about image classification, object detection, and
    semantic segmentation algorithms. You also learned how to prepare datasets in
    Image, RecordIO, and SageMaker Ground Truth formats. Labeling and preparing data
    is a critical step that takes a lot of work, and we covered it in great detail.
    Finally, you learned how to use the SageMaker SDK to train and deploy models with
    the three algorithms, as well as how to interpret results.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 本章您学习了图像分类、目标检测和语义分割算法。您还学习了如何准备图像、RecordIO和SageMaker Ground Truth格式的数据集。标注和准备数据是一个非常关键的步骤，需要大量的工作，我们在这一部分进行了详细的讲解。最后，您学习了如何使用SageMaker
    SDK来训练和部署这三种算法的模型，并了解如何解读结果。
- en: In the next chapter, you will learn how to use built-in algorithms for natural
    language processing.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何使用内置算法进行自然语言处理。
