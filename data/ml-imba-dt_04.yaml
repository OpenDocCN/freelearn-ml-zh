- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Ensemble Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é›†æˆæ–¹æ³•
- en: Think of a top executive at a major company. They donâ€™t make decisions on their
    own. Throughout the day, they need to make numerous critical decisions. How do
    they make those choices? Not alone, but by consulting their advisors.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹ä¸€å®¶å¤§å‹å…¬å¸çš„é¡¶çº§é«˜ç®¡ã€‚ä»–ä»¬ä¸ä¼šç‹¬è‡ªåšå‡ºå†³ç­–ã€‚åœ¨ä¸€å¤©ä¸­ï¼Œä»–ä»¬éœ€è¦åšå‡ºè®¸å¤šå…³é”®å†³ç­–ã€‚ä»–ä»¬æ˜¯å¦‚ä½•åšå‡ºè¿™äº›é€‰æ‹©çš„ï¼Ÿä¸æ˜¯ç‹¬è‡ªä¸€äººï¼Œè€Œæ˜¯é€šè¿‡å’¨è¯¢ä»–ä»¬çš„é¡¾é—®ã€‚
- en: Letâ€™s say that an executive consults five different advisors from different
    departments, each proposing a slightly different solution based on their expertise,
    skills, and domain knowledge. To make the most effective decision, the executive
    combines the insights and opinions of all five advisors to create a hybrid solution
    that incorporates the best parts of each proposal. This scenario illustrates the
    concept of **ensemble methods**, where multiple weak classifiers are combined
    to create a stronger and more accurate classifier. By combining different approaches,
    ensemble methods can often achieve better performance than relying on a single
    classifier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä¸€ä½é«˜ç®¡å’¨è¯¢äº†æ¥è‡ªä¸åŒéƒ¨é—¨çš„äº”ä½ä¸åŒé¡¾é—®ï¼Œæ¯ä½é¡¾é—®æ ¹æ®ä»–ä»¬çš„ä¸“ä¸šçŸ¥è¯†ã€æŠ€èƒ½å’Œé¢†åŸŸçŸ¥è¯†æå‡ºç•¥å¾®ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†åšå‡ºæœ€æœ‰æ•ˆçš„å†³ç­–ï¼Œé«˜ç®¡ç»“åˆäº†äº”ä½é¡¾é—®çš„è§è§£å’Œæ„è§ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ··åˆè§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­åŒ…å«äº†æ¯ä¸ªææ¡ˆçš„æœ€ä½³éƒ¨åˆ†ã€‚è¿™ä¸ªåœºæ™¯è¯´æ˜äº†**é›†æˆæ–¹æ³•**çš„æ¦‚å¿µï¼Œå…¶ä¸­å¤šä¸ªå¼±åˆ†ç±»å™¨è¢«ç»„åˆèµ·æ¥åˆ›å»ºä¸€ä¸ªæ›´å¼ºã€æ›´å‡†ç¡®çš„åˆ†ç±»å™¨ã€‚é€šè¿‡ç»“åˆä¸åŒçš„æ–¹æ³•ï¼Œé›†æˆæ–¹æ³•é€šå¸¸å¯ä»¥å®ç°æ¯”ä¾èµ–å•ä¸ªåˆ†ç±»å™¨æ›´å¥½çš„æ€§èƒ½ã€‚
- en: 'We can create a strong model through ensemble methods by combining the results
    from multiple weak classifiers. These weak classifiers, such as simplified decision
    trees, neural networks, or support vector machines, perform slightly better than
    random guessing. In contrast, a strong model, created by ensembling these weak
    classifiers, performs significantly better than random guessing. The weak classifiers
    can be fed different sources of information. There are two general approaches
    for building ensembles of models: bagging and boosting.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»“åˆå¤šä¸ªå¼±åˆ†ç±»å™¨çš„ç»“æœæ¥åˆ›å»ºä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹ã€‚è¿™äº›å¼±åˆ†ç±»å™¨ï¼Œå¦‚ç®€åŒ–çš„å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œæˆ–æ”¯æŒå‘é‡æœºï¼Œå…¶è¡¨ç°ç•¥å¥½äºéšæœºçŒœæµ‹ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡é›†æˆè¿™äº›å¼±åˆ†ç±»å™¨åˆ›å»ºçš„å¼ºå¤§æ¨¡å‹ï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºéšæœºçŒœæµ‹ã€‚å¼±åˆ†ç±»å™¨å¯ä»¥æ¥å—ä¸åŒæ¥æºçš„ä¿¡æ¯ã€‚æ„å»ºæ¨¡å‹é›†æˆçš„ä¸¤ç§é€šç”¨æ–¹æ³•ï¼šbaggingå’Œboostingã€‚
- en: The problem with traditional ensemble methods is that they use classifiers that
    assume balanced data. Thus, they may not work very well with imbalanced datasets.
    So, we combine the popular machine learning ensembling methods with the techniques
    for dealing with imbalanced data that we studied in previous chapters. We are
    going to discuss those combinations in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿé›†æˆæ–¹æ³•çš„é—®é¢˜åœ¨äºå®ƒä»¬ä½¿ç”¨å‡è®¾æ•°æ®å¹³è¡¡çš„åˆ†ç±»å™¨ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯èƒ½ä¸é€‚åˆå¤„ç†ä¸å¹³è¡¡çš„æ•°æ®é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æµè¡Œçš„æœºå™¨å­¦ä¹ é›†æˆæ–¹æ³•ä¸æˆ‘ä»¬ä¹‹å‰ç« èŠ‚ä¸­ç ”ç©¶çš„ä¸å¹³è¡¡æ•°æ®å¤„ç†æŠ€æœ¯ç›¸ç»“åˆã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« è®¨è®ºè¿™äº›ç»„åˆã€‚
- en: 'Here are the topics that will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: Bagging techniques for imbalanced data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„BaggingæŠ€æœ¯
- en: Boosting techniques for imbalanced data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„BoostingæŠ€æœ¯
- en: Ensemble of ensembles
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆé›†æˆ
- en: Model performance comparison
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
- en: 'In *Figure 4**.1*, we have categorized the various ensembling techniques that
    we will cover in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾4*.1ä¸­ï¼Œæˆ‘ä»¬å°†æœ¬ç« å°†è¦æ¶µç›–çš„å„ç§é›†æˆæŠ€æœ¯è¿›è¡Œäº†åˆ†ç±»ï¼š
- en: '![](img/B17259_04_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_04_01.jpg)'
- en: Figure 4.1 â€“ Overview of ensembling techniques
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.1 â€“ é›†æˆæŠ€æœ¯æ¦‚è¿°
- en: By the end of this chapter, you will understand how to adapt ensemble models
    such as bagging and boosting to account for class imbalances in datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†äº†è§£å¦‚ä½•å°†è¯¸å¦‚baggingå’Œboostingä¹‹ç±»çš„é›†æˆæ¨¡å‹åº”ç”¨äºæ•°æ®é›†ä¸­ç±»ä¸å¹³è¡¡çš„é—®é¢˜ã€‚
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: The Python notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04).
    As usual, you can open the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapterâ€™s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„Pythonç¬”è®°æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04)ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œä½ å¯ä»¥é€šè¿‡ç‚¹å‡»æœ¬ç« ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡æˆ–é€šè¿‡ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLåœ¨[https://colab.research.google.com](https://colab.research.google.com)å¯åŠ¨å®ƒæ¥æ‰“å¼€GitHubç¬”è®°æœ¬ã€‚
- en: 'In this chapter, we will continue to use a synthetic dataset generated using
    the `make_classification` API, just as we did in the previous chapters. Toward
    the end of this chapter, we will test the methods we learned in this chapter on
    some real datasets. Our full dataset contains 90,000 examples with a 1:99 imbalance
    ratio. Here is what the training dataset looks like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨ä½¿ç”¨`make_classification` APIç”Ÿæˆçš„åˆæˆæ•°æ®é›†ï¼Œå°±åƒæˆ‘ä»¬åœ¨å‰å‡ ç« ä¸­æ‰€åšçš„é‚£æ ·ã€‚åœ¨æœ¬ç« çš„æœ«å°¾ï¼Œæˆ‘ä»¬å°†å¯¹æˆ‘ä»¬åœ¨æœ¬ç« ä¸­å­¦åˆ°çš„æ–¹æ³•åœ¨å‡ ä¸ªçœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬çš„å®Œæ•´æ•°æ®é›†åŒ…å«90,000ä¸ªç¤ºä¾‹ï¼Œä¸å¹³è¡¡æ¯”ç‡ä¸º1:99ã€‚ä»¥ä¸‹æ˜¯è®­ç»ƒæ•°æ®é›†çš„å¤–è§‚ï¼š
- en: '![](img/B17259_04_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_04_02.jpg)'
- en: Figure 4.2 â€“ Plot of a dataset with a 1:99 imbalance ratio
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.2 â€“ ä¸å¹³è¡¡æ¯”ç‡ä¸º1:99çš„æ•°æ®é›†çš„ç»˜å›¾
- en: With our imbalanced dataset ready to use, letâ€™s look at the first ensembling
    method, called bagging.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¤±è¡¡æ•°æ®é›†å·²ç»å‡†å¤‡å¥½ä½¿ç”¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ç¬¬ä¸€ç§é›†æˆæ–¹æ³•ï¼Œç§°ä¸ºBaggingã€‚
- en: Bagging techniques for imbalanced data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„BaggingæŠ€æœ¯
- en: Imagine a business executive with thousands of confidential files regarding
    an important merger or acquisition. The analysts assigned to the case donâ€™t have
    enough time to review all the files. Each can randomly select some files from
    the set and start reviewing them. Later, they can combine their insights in a
    meeting to draw conclusions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªå•†ä¸šé«˜ç®¡ï¼Œä»–æ‹¥æœ‰æ•°åƒä»½å…³äºé‡è¦å¹¶è´­çš„æœºå¯†æ–‡ä»¶ã€‚åˆ†é…ç»™è¿™ä¸ªæ¡ˆä¾‹çš„åˆ†æå¸ˆæ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´æ¥å®¡æŸ¥æ‰€æœ‰æ–‡ä»¶ã€‚æ¯ä¸ªäººéƒ½å¯ä»¥ä»é›†åˆä¸­éšæœºé€‰æ‹©ä¸€äº›æ–‡ä»¶å¹¶å¼€å§‹å®¡æŸ¥ã€‚ç¨åï¼Œä»–ä»¬å¯ä»¥åœ¨ä¼šè®®ä¸Šç»“åˆä»–ä»¬çš„è§è§£æ¥å¾—å‡ºç»“è®ºã€‚
- en: This scenario is a metaphor for a process in machine learning called bagging
    [1], which is short for **bootstrap aggregating**. In bagging, much like the analysts
    in the previous scenario, we create several subsets of the original dataset, train
    a weak learner on each subset, and then aggregate their predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åœºæ™¯æ˜¯æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªç§°ä¸ºBaggingçš„è¿‡ç¨‹çš„éšå–»[1]ï¼Œå®ƒæ˜¯**è‡ªåŠ©èšåˆ**çš„ç¼©å†™ã€‚åœ¨Baggingä¸­ï¼Œå°±åƒå‰ä¸€ä¸ªåœºæ™¯ä¸­çš„åˆ†æå¸ˆä¸€æ ·ï¼Œæˆ‘ä»¬åˆ›å»ºåŸå§‹æ•°æ®é›†çš„å‡ ä¸ªå­é›†ï¼Œåœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒä¸€ä¸ªå¼±å­¦ä¹ å™¨ï¼Œç„¶åèšåˆå®ƒä»¬çš„é¢„æµ‹ã€‚
- en: 'Why use weak learners instead of strong learners? The rationale applies to
    both bagging and boosting methods (discussed later in this chapter). There are
    several reasons:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä½¿ç”¨å¼±å­¦ä¹ å™¨è€Œä¸æ˜¯å¼ºå­¦ä¹ å™¨ï¼Ÿè¿™ä¸ªç†ç”±é€‚ç”¨äºBaggingå’ŒBoostingæ–¹æ³•ï¼ˆæœ¬ç« åé¢å°†è®¨è®ºï¼‰ã€‚æœ‰å‡ ä¸ªåŸå› ï¼š
- en: '**Speed**: Weak learners are computationally efficient and inexpensive to execute.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€Ÿåº¦**ï¼šå¼±å­¦ä¹ å™¨åœ¨è®¡ç®—ä¸Šæ•ˆç‡é«˜ä¸”æˆæœ¬ä½ã€‚'
- en: '**Diversity**: Weak learners are more likely to make different types of errors,
    which is advantageous when combining their predictions. Using strong learners
    could result in them all making the same type of error, leading to less effective
    ensembles.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šæ ·æ€§**ï¼šå¼±å­¦ä¹ å™¨æ›´æœ‰å¯èƒ½çŠ¯ä¸åŒç±»å‹çš„é”™è¯¯ï¼Œè¿™åœ¨ç»“åˆå®ƒä»¬çš„é¢„æµ‹æ—¶æ˜¯æœ‰åˆ©çš„ã€‚ä½¿ç”¨å¼ºå­¦ä¹ å™¨å¯èƒ½å¯¼è‡´å®ƒä»¬éƒ½çŠ¯åŒä¸€ç§ç±»å‹çš„é”™è¯¯ï¼Œä»è€Œå¯¼è‡´é›†æˆæ•ˆæœè¾ƒå·®ã€‚'
- en: '**Overfitting**: As a corollary to the previous point, the diversity in errors
    helps reduce the risk of overfitting in the ensemble.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿‡æ‹Ÿåˆ**ï¼šä½œä¸ºå‰ä¸€ç‚¹çš„æ¨è®ºï¼Œé”™è¯¯çš„å¤šæ ·æ€§æœ‰åŠ©äºé™ä½é›†æˆä¸­è¿‡æ‹Ÿåˆçš„é£é™©ã€‚'
- en: '**Interpretability**: While the ensemble as a whole may not be easily interpretable,
    its individual components â€“ often simpler models â€“ are easier to understand and
    interpret.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯è§£é‡Šæ€§**ï¼šè™½ç„¶æ•´ä¸ªé›†æˆå¯èƒ½ä¸å®¹æ˜“è§£é‡Šï¼Œä½†å…¶ä¸ªåˆ«ç»„ä»¶â€”â€”é€šå¸¸æ˜¯æ›´ç®€å•çš„æ¨¡å‹â€”â€”æ›´å®¹æ˜“ç†è§£å’Œè§£é‡Šã€‚'
- en: Now, back to bagging. The first step of the algorithm is called **bootstrapping**.
    In this step, we make several subsets or smaller groups of data by randomly picking
    items from the main data. The data is picked with the possibility of picking the
    same item more than once (this process is called â€œrandom sampling with replacementâ€),
    so these smaller groups may have some items in common. Then, we train our classifiers
    on each of these smaller groups.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå›åˆ°Baggingã€‚ç®—æ³•çš„ç¬¬ä¸€æ­¥ç§°ä¸º**è‡ªåŠ©æŠ½æ ·**ã€‚åœ¨è¿™ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»ä¸»æ•°æ®ä¸­éšæœºé€‰æ‹©é¡¹ç›®æ¥åˆ¶ä½œå‡ ä¸ªå­é›†æˆ–è¾ƒå°çš„æ•°æ®ç»„ã€‚æ•°æ®çš„é€‰æ‹©æœ‰å¯èƒ½ä¼šé€‰æ‹©åŒä¸€ä¸ªé¡¹ç›®å¤šæ¬¡ï¼ˆè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºâ€œæœ‰æ”¾å›çš„éšæœºæŠ½æ ·â€ï¼‰ï¼Œæ‰€ä»¥è¿™äº›è¾ƒå°çš„ç»„å¯èƒ½æœ‰ä¸€äº›å…±åŒçš„é¡¹ç›®ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨è¿™äº›è¾ƒå°çš„ç»„ä¸Šè®­ç»ƒæˆ‘ä»¬çš„åˆ†ç±»å™¨ã€‚
- en: The second step is called **aggregating**. The test sample is passed to each
    classifier at the time of prediction. After this, we take the average or majority
    prediction as the real answer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ç§°ä¸º**èšåˆ**ã€‚åœ¨é¢„æµ‹æ—¶ï¼Œå°†æµ‹è¯•æ ·æœ¬ä¼ é€’ç»™æ¯ä¸ªåˆ†ç±»å™¨ã€‚ä¹‹åï¼Œæˆ‘ä»¬å–å¹³å‡æˆ–å¤šæ•°é¢„æµ‹ä½œä¸ºçœŸå®ç­”æ¡ˆã€‚
- en: 'As shown in *Figure 4**.3*, the dataset is first sampled with replacement into
    three subsets. Then, separate classifiers are trained on each of the subsets.
    Finally, the results of the classifiers are combined at the time of prediction:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚**å›¾4**.3æ‰€ç¤ºï¼Œæ•°æ®é›†é¦–å…ˆé€šè¿‡æœ‰æ”¾å›æŠ½æ ·åˆ†æˆä¸‰ä¸ªå­é›†ã€‚ç„¶åï¼Œåœ¨æ¯ä¸ªå­é›†ä¸Šåˆ†åˆ«è®­ç»ƒå•ç‹¬çš„åˆ†ç±»å™¨ã€‚æœ€åï¼Œåœ¨é¢„æµ‹æ—¶å°†åˆ†ç±»å™¨çš„ç»“æœåˆå¹¶ï¼š
- en: '![](img/B17259_04_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_04_03.jpg)'
- en: Figure 4.3 â€“ Demonstrating how bagging works
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.3 â€“ å±•ç¤º bagging çš„å·¥ä½œåŸç†
- en: '*Figure 4**.4* summarizes the bagging algorithm in a pseudocode format:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 4**.4* ä»¥ä¼ªä»£ç æ ¼å¼æ€»ç»“äº† bagging ç®—æ³•ï¼š'
- en: '![](img/B17259_04_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_04.jpg)'
- en: Figure 4.4 â€“ Bagging pseudocode
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.4 â€“ Bagging çš„ä¼ªä»£ç 
- en: 'Weâ€™ll train a bagging classifier model from `sklearn` on the dataset we created
    previously. Since itâ€™s possible to provide a base estimator to `BaggingClassifier`,
    weâ€™ll use `DecisionTreeClassifier` with the maximum depth of the trees being `6`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨ä¹‹å‰åˆ›å»ºçš„æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ¥è‡ª `sklearn` çš„ bagging åˆ†ç±»å™¨æ¨¡å‹ã€‚ç”±äºå¯ä»¥å‘ `BaggingClassifier` æä¾›ä¸€ä¸ªåŸºä¼°è®¡å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æœ€å¤§æ ‘æ·±ä¸º
    `6` çš„ `DecisionTreeClassifier`ï¼š
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Letâ€™s plot the decision boundary:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼š
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You may refer to the definition of `plot_decision_boundary()` in the corresponding
    notebook on GitHub. We use the `DecisionBoundaryDisplay` API from the `sklearn.inspection`
    module to plot the decision boundary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥å‚è€ƒ GitHub ä¸Šç›¸åº”ç¬”è®°æœ¬ä¸­ `plot_decision_boundary()` çš„å®šä¹‰ã€‚æˆ‘ä»¬ä½¿ç”¨ `sklearn.inspection`
    æ¨¡å—ä¸­çš„ `DecisionBoundaryDisplay` API æ¥ç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚
- en: '*Figure 4**.5* shows the learned decision boundary on the training data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 4**.5* å±•ç¤ºäº†åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å­¦ä¹ å†³ç­–è¾¹ç•Œï¼š'
- en: '![](img/B17259_04_05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_05.jpg)'
- en: Figure 4.5 â€“ The decision boundary of BaggingClassifier on the training data
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.5 â€“ BaggingClassifier åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å†³ç­–è¾¹ç•Œ
- en: 'Letâ€™s also note the baseline metric of average precision when using this model
    on our test set:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¹Ÿæ³¨æ„ä½¿ç”¨æ­¤æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„åŸºçº¿æŒ‡æ ‡å¹³å‡ç²¾åº¦ï¼š
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Figure 4**.6* shows the resulting PR curve:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 4**.6* å±•ç¤ºäº†å¾—åˆ°çš„ PR æ›²çº¿ï¼š'
- en: '![](img/B17259_04_06.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_06.jpg)'
- en: Figure 4.6 â€“ Precision-recall curve of BaggingClassifier on the test data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.6 â€“ BaggingClassifier åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„ç²¾ç¡®åº¦-å¬å›ç‡æ›²çº¿
- en: 'Here are some other metrics:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›å…¶ä»–æŒ‡æ ‡ï¼š
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this chapter, we will also consider the **F2 score** (Fbeta-score with beta=2.0),
    which proportionally combines precision and recall, giving more weight to recall
    and less weight to precision.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬è¿˜å°†è€ƒè™‘ **F2 åˆ†æ•°**ï¼ˆFbeta åˆ†æ•°ï¼Œbeta=2.0ï¼‰ï¼Œå®ƒæŒ‰æ¯”ä¾‹ç»“åˆç²¾ç¡®åº¦å’Œå¬å›ç‡ï¼Œç»™äºˆå¬å›ç‡æ›´é«˜çš„æƒé‡ï¼Œè€Œç»™äºˆç²¾ç¡®åº¦è¾ƒä½çš„æƒé‡ã€‚
- en: So, what problems may we face when using `BaggingClassifier` on an imbalanced
    dataset? An obvious thing could be that when bootstrapping, some subsets on which
    base classifiers get trained may have very few minority class examples or none
    at all. This would mean that each of the individual base classifiers is going
    to perform poorly on the minority class, and combining their performance would
    still be poor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå½“åœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šä½¿ç”¨ `BaggingClassifier` æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°ä»€ä¹ˆé—®é¢˜ï¼Ÿä¸€ä¸ªæ˜æ˜¾çš„äº‹æƒ…å¯èƒ½æ˜¯ï¼Œåœ¨å¼•å¯¼è¿‡ç¨‹ä¸­ï¼Œä¸€äº›ç”¨äºè®­ç»ƒåŸºåˆ†ç±»å™¨çš„å­é›†å¯èƒ½åªæœ‰å¾ˆå°‘çš„å°‘æ•°ç±»ç¤ºä¾‹ï¼Œç”šè‡³æ²¡æœ‰ã€‚è¿™æ„å‘³ç€æ¯ä¸ªåŸºåˆ†ç±»å™¨åœ¨å°‘æ•°ç±»ä¸Šçš„è¡¨ç°éƒ½ä¼šå¾ˆå·®ï¼Œè€Œå®ƒä»¬çš„ç»„åˆè¡¨ç°ä»ç„¶ä¼šå¾ˆå·®ã€‚
- en: We can combine undersampling techniques with bagging (one such method is UnderBagging)
    or oversampling techniques with bagging (one such method is OverBagging) to get
    better results. We will discuss such techniques next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†ä¸‹é‡‡æ ·æŠ€æœ¯ä¸ baggingï¼ˆä¾‹å¦‚ UnderBaggingï¼‰æˆ–è¿‡é‡‡æ ·æŠ€æœ¯ä¸ baggingï¼ˆä¾‹å¦‚ OverBaggingï¼‰ç»“åˆèµ·æ¥ï¼Œä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºè¿™äº›æŠ€æœ¯ã€‚
- en: UnderBagging
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnderBagging
- en: The UnderBagging [2] technique uses random undersampling at the time of bootstrapping
    (or selection of subsets). We choose the whole set of the minority class examples
    for each classifier and bootstrap with replacement as many examples from the majority
    class as there are minority class examples. The aggregation step remains the same
    as in bagging. We can choose any classifier, say a decision tree, for training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: UnderBagging [2] æŠ€æœ¯åœ¨å¼•å¯¼ï¼ˆæˆ–å­é›†é€‰æ‹©ï¼‰æ—¶ä½¿ç”¨éšæœºä¸‹é‡‡æ ·ã€‚æˆ‘ä»¬ä¸ºæ¯ä¸ªåˆ†ç±»å™¨é€‰æ‹©å°‘æ•°ç±»ç¤ºä¾‹çš„æ•´ä¸ªé›†åˆï¼Œå¹¶ç”¨ä¸å°‘æ•°ç±»ç¤ºä¾‹æ•°é‡ç›¸åŒçš„å¤šæ•°ç±»ç¤ºä¾‹è¿›è¡Œå¸¦æ›¿æ¢çš„å¼•å¯¼ã€‚èšåˆæ­¥éª¤ä¸
    bagging ç›¸åŒã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä»»ä½•åˆ†ç±»å™¨ï¼Œä¾‹å¦‚å†³ç­–æ ‘ï¼Œè¿›è¡Œè®­ç»ƒã€‚
- en: There are variants of UnderBagging where resampling with replacement of the
    minority class can also be applied to obtain more diverse ensembles.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: UnderBagging æœ‰å˜ä½“ï¼Œå…¶ä¸­å¯ä»¥å¯¹å°‘æ•°ç±»è¿›è¡Œå¸¦æ›¿æ¢çš„é‡é‡‡æ ·ï¼Œä»¥è·å¾—æ›´å¤šæ ·åŒ–çš„é›†æˆã€‚
- en: 'The flowchart in *Figure 4**.7* represents the main steps in the UnderBagging
    algorithm with three subsets of data. It involves creating multiple subsets of
    data, performing random undersampling for the majority class in each subset, training
    classifiers on each subset, and finally combining the predictions of the classifiers:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 4**.7* ä¸­çš„æµç¨‹å›¾ä»£è¡¨äº† UnderBagging ç®—æ³•åœ¨ä¸‰ä¸ªæ•°æ®å­é›†ä¸­çš„ä¸»è¦æ­¥éª¤ã€‚å®ƒåŒ…æ‹¬åˆ›å»ºå¤šä¸ªæ•°æ®å­é›†ï¼Œå¯¹æ¯ä¸ªå­é›†ä¸­çš„å¤šæ•°ç±»è¿›è¡Œéšæœºä¸‹é‡‡æ ·ï¼Œåœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒåˆ†ç±»å™¨ï¼Œæœ€åç»“åˆåˆ†ç±»å™¨çš„é¢„æµ‹ï¼š'
- en: '![](img/B17259_04_07.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_07.jpg)'
- en: Figure 4.7 â€“ Demonstrating how the UnderBagging algorithm works
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.7 â€“ å±•ç¤º UnderBagging ç®—æ³•çš„å·¥ä½œåŸç†
- en: 'The `imbalanced-learn` library provides an implementation for `BalancedBaggingClassifier`.
    By default, this classifier uses a decision tree as the base classifier and `RandomUnderSampler`
    as the sampler via the `sampler` parameter. *Figure 4**.8* shows the decision
    boundary of the trained UnderBagging model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn` åº“ä¸º `BalancedBaggingClassifier` æä¾›äº†å®ç°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ­¤åˆ†ç±»å™¨ä½¿ç”¨å†³ç­–æ ‘ä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œå¹¶é€šè¿‡
    `sampler` å‚æ•°ä½¿ç”¨ `RandomUnderSampler` ä½œä¸ºé‡‡æ ·å™¨ã€‚*å›¾4.8* å±•ç¤ºäº†è®­ç»ƒå¥½çš„UnderBaggingæ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼š'
- en: '![](img/B17259_04_08.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_08.jpg)'
- en: Figure 4.8 â€“ The decision boundary of the UnderBagging classifier on the training
    data
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.8 â€“ åœ¨è®­ç»ƒæ•°æ®ä¸ŠUnderBaggingåˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œ
- en: ğŸš€ Bagging classifier in production at Microsoft
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ å¾®è½¯ç”Ÿäº§ç¯å¢ƒä¸­çš„Baggingåˆ†ç±»å™¨
- en: In a real-world application at Microsoft [3], the team faced a significant challenge
    in forecasting Live Site Incident escalations (previously mentioned in [*Chapter
    2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling Methods*). The dataset was
    highly imbalanced, making it difficult for standard classifiers to perform well.
    To tackle this issue, Microsoft employed ensemble methods, specifically `BalancedBaggingClassifier`
    from the `imbalanced-learn` library. They used UnderBagging, where each bootstrap
    sample is randomly undersampled to get a balanced class distribution. As we have
    just discussed, UnderBagging uses all minority class samples and a random selection
    of majority class samples to train the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®è½¯çš„ä¸€ä¸ªå®é™…åº”ç”¨ä¸­ [3]ï¼Œå›¢é˜Ÿåœ¨é¢„æµ‹Live Site Incidentå‡çº§ï¼ˆå¦‚[*ç¬¬2ç« *](B17259_02.xhtml#_idTextAnchor042)ï¼Œ*è¿‡é‡‡æ ·æ–¹æ³•*ï¼‰æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚æ•°æ®é›†é«˜åº¦ä¸å¹³è¡¡ï¼Œä½¿å¾—æ ‡å‡†åˆ†ç±»å™¨éš¾ä»¥è¡¨ç°è‰¯å¥½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¾®è½¯é‡‡ç”¨äº†é›†æˆæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ¥è‡ª
    `imbalanced-learn` åº“çš„ `BalancedBaggingClassifier`ã€‚ä»–ä»¬ä½¿ç”¨äº†UnderBaggingï¼Œå…¶ä¸­æ¯ä¸ªå¼•å¯¼æ ·æœ¬éƒ½æ˜¯éšæœºæ¬ é‡‡æ ·çš„ï¼Œä»¥è·å¾—å¹³è¡¡çš„ç±»åˆ«åˆ†å¸ƒã€‚æ­£å¦‚æˆ‘ä»¬åˆšæ‰è®¨è®ºçš„ï¼ŒUnderBaggingä½¿ç”¨æ‰€æœ‰å°‘æ•°ç±»æ ·æœ¬å’Œå¤šæ•°ç±»æ ·æœ¬çš„éšæœºé€‰æ‹©æ¥è®­ç»ƒæ¨¡å‹ã€‚
- en: Bagged classification delivered the best results during their evaluation and
    also proved to be more consistent after they tracked it over a few months. They
    were able to significantly improve their forecasting accuracy for incident escalations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»–ä»¬çš„è¯„ä¼°ä¸­ï¼ŒBaggedåˆ†ç±»æä¾›äº†æœ€ä½³ç»“æœï¼Œå¹¶ä¸”åœ¨ç»è¿‡å‡ ä¸ªæœˆçš„è·Ÿè¸ªåä¹Ÿè¯æ˜æ›´åŠ ä¸€è‡´ã€‚ä»–ä»¬èƒ½å¤Ÿæ˜¾è‘—æé«˜å¯¹äº‹ä»¶å‡çº§çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚
- en: OverBagging
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OverBagging
- en: Instead of random undersampling of the majority class samples, the minority
    class is oversampled (with replacement) at the time of bootstrapping. This method
    is called OverBagging [2]. As a variant, both minority and majority class examples
    can be resampled with replacements to achieve an equal number of majority and
    minority class examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼•å¯¼è¿‡ç¨‹ä¸­ï¼Œä¸æ˜¯å¯¹å¤šæ•°ç±»æ ·æœ¬è¿›è¡Œéšæœºæ¬ é‡‡æ ·ï¼Œè€Œæ˜¯å¯¹å°‘æ•°ç±»è¿›è¡Œè¿‡é‡‡æ ·ï¼ˆå¸¦æ›¿æ¢ï¼‰ã€‚è¿™ç§æ–¹æ³•ç§°ä¸ºOverBagging [2]ã€‚ä½œä¸ºä¸€ç§å˜ä½“ï¼Œå¤šæ•°ç±»å’Œå°‘æ•°ç±»çš„æ ·æœ¬éƒ½å¯ä»¥è¿›è¡Œå¸¦æ›¿æ¢çš„é‡é‡‡æ ·ï¼Œä»¥è¾¾åˆ°å¤šæ•°ç±»å’Œå°‘æ•°ç±»æ ·æœ¬æ•°é‡ç›¸ç­‰ã€‚
- en: 'The flowchart in *Figure 4**.9* represents the main steps in the OverBagging
    algorithm with three subsets of data. It involves creating multiple subsets of
    data, performing random oversampling for the minority class in each subset, training
    classifiers on each subset, and finally combining the predictions of the classifiers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾4.9* ä¸­çš„æµç¨‹å›¾å±•ç¤ºäº†OverBaggingç®—æ³•åœ¨ä¸‰ä¸ªæ•°æ®å­é›†ä¸Šçš„ä¸»è¦æ­¥éª¤ã€‚å®ƒåŒ…æ‹¬åˆ›å»ºå¤šä¸ªæ•°æ®å­é›†ï¼Œå¯¹æ¯ä¸ªå­é›†ä¸­çš„å°‘æ•°ç±»è¿›è¡Œéšæœºè¿‡é‡‡æ ·ï¼Œåœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒåˆ†ç±»å™¨ï¼Œå¹¶æœ€ç»ˆç»“åˆåˆ†ç±»å™¨çš„é¢„æµ‹ï¼š'
- en: '![](img/B17259_04_09.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_09.jpg)'
- en: Figure 4.9 â€“ Demonstrating how the OverBagging algorithm works
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.9 â€“ å±•ç¤ºOverBaggingç®—æ³•çš„å·¥ä½œåŸç†
- en: For OverBagging, we can use the same `BalancedBaggingClassifier` with `RandomOverSampler`
    in the `sampler` parameter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºOverBaggingï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ `sampler` å‚æ•°ä¸­ä½¿ç”¨ä¸ `BalancedBaggingClassifier` ç›¸åŒçš„ `RandomOverSampler`ã€‚
- en: 'We will see the following decision boundary:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çœ‹åˆ°ä»¥ä¸‹å†³ç­–è¾¹ç•Œï¼š
- en: '![](img/B17259_04_10.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_10.jpg)'
- en: Figure 4.10 â€“ The decision boundary of the OverBagging classifier on the training
    data
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.10 â€“ åœ¨è®­ç»ƒæ•°æ®ä¸ŠOverBaggingåˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œ
- en: We will compare the performance metrics of these techniques after discussing
    the various bagging techniques.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¨è®ºäº†å„ç§BaggingæŠ€æœ¯ä¹‹åï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒè¿™äº›æŠ€æœ¯çš„æ€§èƒ½æŒ‡æ ‡ã€‚
- en: SMOTEBagging
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTEBagging
- en: Can we use SMOTE at the time of bootstrapping instead of random oversampling
    of minority class examples? The answer is yes. The majority class will be bootstrapped
    with replacement, and the minority class will be sampled using SMOTE until a balancing
    ratio is reached.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦åœ¨å¼•å¯¼è¿‡ç¨‹ä¸­ä½¿ç”¨SMOTEä»£æ›¿å¯¹å°‘æ•°ç±»æ ·æœ¬çš„éšæœºè¿‡é‡‡æ ·ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ã€‚å¤šæ•°ç±»å°†è¿›è¡Œå¸¦æ›¿æ¢çš„å¼•å¯¼ï¼Œè€Œå°‘æ•°ç±»å°†ä½¿ç”¨SMOTEè¿›è¡Œé‡‡æ ·ï¼Œç›´åˆ°è¾¾åˆ°å¹³è¡¡æ¯”ç‡ã€‚
- en: The pseudocode for SMOTEBagging [2] is very similar to that for OverBagging,
    with the key difference being the use of the SMOTE algorithm instead of random
    oversampling to augment the minority class data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTEBagging [2] çš„ä¼ªä»£ç ä¸ OverBagging éå¸¸ç›¸ä¼¼ï¼Œå…³é”®åŒºåˆ«åœ¨äºä½¿ç”¨ SMOTE ç®—æ³•è€Œä¸æ˜¯éšæœºè¿‡é‡‡æ ·æ¥å¢å¼ºå°‘æ•°ç±»æ•°æ®ã€‚
- en: 'Similar to OverBagging, we can implement `SMOTEBagging` using the `BalancedBagging``â€¨    Classifier` API with SMOTE as the `sampler` parameter.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡è¢‹è£…ç±»ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¸¦æœ‰ SMOTE ä½œä¸º `sampler` å‚æ•°çš„ `BalancedBagging Classifier` API å®ç° `SMOTEBagging`ã€‚
- en: 'The decision boundary is not very different from OverBagging:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–è¾¹ç•Œä¸è¿‡è¢‹è£…æ²¡æœ‰å¾ˆå¤§åŒºåˆ«ï¼š
- en: '![](img/B17259_04_11.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_11.jpg)'
- en: Figure 4.11 â€“ The decision boundary of the SMOTEBagging classifier on the training
    data
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4.11 â€“ SMOTEBagging åˆ†ç±»å™¨åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å†³ç­–è¾¹ç•Œ
- en: A note about random forest and how it is related to bagging
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºéšæœºæ£®æ—åŠå…¶ä¸è¢‹è£…çš„å…³ç³»çš„è¯´æ˜
- en: Random forest [4] is another model that is based on the concept of bagging.
    The way the `RandomForestClassifier` and `BaggingClassifier` models from `sklearn`
    differ from each other is the fact that `RandomForestClassifier` considers a random
    subset of features while trying to decide the feature on which to split the nodes
    in the decision tree, while `BaggingClassifier` takes all the features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ— [4] æ˜¯å¦ä¸€ä¸ªåŸºäºè¢‹è£…æ¦‚å¿µçš„æ¨¡å‹ã€‚`sklearn` ä¸­çš„ `RandomForestClassifier` å’Œ `BaggingClassifier`
    æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«åœ¨äºï¼Œ`RandomForestClassifier` åœ¨å°è¯•å†³å®šå†³ç­–æ ‘ä¸­èŠ‚ç‚¹åˆ†è£‚çš„ç‰¹å¾æ—¶ï¼Œè€ƒè™‘äº†ç‰¹å¾çš„ä¸€ä¸ªéšæœºå­é›†ï¼Œè€Œ `BaggingClassifier`
    åˆ™é‡‡ç”¨æ‰€æœ‰ç‰¹å¾ã€‚
- en: '*Table 4.1* highlights the difference between random forest and bagging classifiers:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¡¨ 4.1* çªå‡ºäº†éšæœºæ£®æ—å’Œè¢‹è£…åˆ†ç±»å™¨ä¹‹é—´çš„å·®å¼‚ï¼š'
- en: '|  | **RandomForestClassifier** | **BaggingClassifier** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | **RandomForestClassifier** | **BaggingClassifier** |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Base classifier | Decision trees | Any classifier. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| åŸºæœ¬åˆ†ç±»å™¨ | å†³ç­–æ ‘ | ä»»ä½•åˆ†ç±»å™¨ã€‚ |'
- en: '| Bootstrap sampling | Yes | Yes. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| è‡ªä¸¾é‡‡æ · | æ˜¯ | æ˜¯ã€‚ |'
- en: '| Take a subset of features | Yes (at each node) | No, by default. We can use
    the `max_features` hyperparameter to take subsets of features. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| é€‰å–ç‰¹å¾å­é›† | æ˜¯ï¼ˆåœ¨æ¯ä¸ªèŠ‚ç‚¹ï¼‰ | å¦ï¼Œé»˜è®¤æƒ…å†µä¸‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `max_features` è¶…å‚æ•°æ¥é€‰å–ç‰¹å¾å­é›†ã€‚ |'
- en: '| Works best with? | Any tabular data, but it shines with large feature sets
    | Any tabular data, but itâ€™s best when the base classifier is carefully chosen.
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| æœ€é€‚åˆçš„æ•°æ®ç±»å‹ | ä»»ä½•è¡¨æ ¼æ•°æ®ï¼Œä½†åœ¨å¤§å‹ç‰¹å¾é›†ä¸Šè¡¨ç°æœ€ä½³ | ä»»ä½•è¡¨æ ¼æ•°æ®ï¼Œä½†æœ€å¥½åœ¨ä»”ç»†é€‰æ‹©åŸºæœ¬åˆ†ç±»å™¨æ—¶ä½¿ç”¨ã€‚ |'
- en: '| Handles missing values and outliers | Yes, inherently | Depends on the base
    classifier. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ | æ˜¯ï¼Œå›ºæœ‰ | å–å†³äºåŸºæœ¬åˆ†ç±»å™¨ã€‚ |'
- en: Table 4.1 â€“ RandomForestClassifier versus BaggingClassifier
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 4.1 â€“ RandomForestClassifier ä¸ BaggingClassifier å¯¹æ¯”
- en: The `imbalanced-learn` library provides the `BalancedRandomForestClassifier`
    class to tackle the imbalanced datasets where each of the bootstraps is undersampled
    before the individual decision trees are trained. As an exercise, we encourage
    you to learn about `BalancedRandomForestClassifier`. See how it relates to the
    other techniques we just discussed. Also, try out the various sampling strategies
    and explore the parameters this class offers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn` åº“æä¾›äº† `BalancedRandomForestClassifier` ç±»æ¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ï¼Œå…¶ä¸­åœ¨è®­ç»ƒå•ä¸ªå†³ç­–æ ‘ä¹‹å‰ï¼Œæ¯ä¸ªè‡ªä¸¾æ ·æœ¬éƒ½æ˜¯æ¬ é‡‡æ ·çš„ã€‚ä½œä¸ºç»ƒä¹ ï¼Œæˆ‘ä»¬é¼“åŠ±ä½ äº†è§£
    `BalancedRandomForestClassifier`ã€‚çœ‹çœ‹å®ƒå¦‚ä½•ä¸æˆ‘ä»¬åˆšæ‰è®¨è®ºçš„å…¶ä»–æŠ€æœ¯ç›¸å…³ã€‚è¿˜å¯ä»¥å°è¯•å„ç§é‡‡æ ·ç­–ç•¥ï¼Œå¹¶æ¢ç´¢æ­¤ç±»æä¾›çš„å‚æ•°ã€‚'
- en: Comparative performance of bagging methods
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¢‹è£…æ–¹æ³•çš„æ¯”è¾ƒæ€§èƒ½
- en: 'Letâ€™s compare the performance of various bagging methods using the same dataset
    weâ€™ve employed so far. Weâ€™ll use the decision tree as a baseline and evaluate
    different techniques across several performance metrics. The highest values for
    each metric across all techniques are highlighted in bold:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨è¿„ä»Šä¸ºæ­¢ä½¿ç”¨çš„ç›¸åŒæ•°æ®é›†æ¯”è¾ƒå„ç§è¢‹è£…æ–¹æ³•çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å†³ç­–æ ‘ä½œä¸ºåŸºçº¿ï¼Œå¹¶åœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šè¯„ä¼°ä¸åŒçš„æŠ€æœ¯ã€‚æ‰€æœ‰æŠ€æœ¯ä¸­æ¯ä¸ªæŒ‡æ ‡çš„æœ€é«˜å€¼éƒ½ä»¥ç²—ä½“çªå‡ºæ˜¾ç¤ºï¼š
- en: '| **TECHNIQUE** | **F2** | **PRECISION** | **RECALL** | **AVERAGE PRECISION**
    | **AUC-ROC** |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **æŠ€æœ¯** | **F2** | **ç²¾ç¡®åº¦** | **å¬å›ç‡** | **å¹³å‡ç²¾ç¡®åº¦** | **AUC-ROC** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SMOTEBagging | **0.928** | 0.754 | 0.985 | 0.977 | **1.000** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SMOTEBagging | **0.928** | 0.754 | 0.985 | 0.977 | **1.000** |'
- en: '| OverBagging | 0.888 | 0.612 | **1.000** | 0.976 | **1.000** |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| è¿‡è¢‹è£… | 0.888 | 0.612 | **1.000** | 0.976 | **1.000** |'
- en: '| UnderBagging | 0.875 | 0.609 | 0.981 | 0.885 | 0.999 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ä¸‹è¢‹è£… | 0.875 | 0.609 | 0.981 | 0.885 | 0.999 |'
- en: '| Bagging | 0.891 | 0.967 | 0.874 | 0.969 | **1.000** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| è¢‹è£… | 0.891 | 0.967 | 0.874 | 0.969 | **1.000** |'
- en: '| Balanced random forest | 0.756 | 0.387 | 0.993 | 0.909 | 0.999 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| å¹³è¡¡éšæœºæ£®æ— | 0.756 | 0.387 | 0.993 | 0.909 | 0.999 |'
- en: '| Random forest | 0.889 | **0.975** | 0.870 | **0.979** | **1.000** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| éšæœºæ£®æ— | 0.889 | **0.975** | 0.870 | **0.979** | **1.000** |'
- en: '| Decision tree | 0.893 | 0.960 | 0.878 | 0.930 | 0.981 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| å†³ç­–æ ‘ | 0.893 | 0.960 | 0.878 | 0.930 | 0.981 |'
- en: Table 4.2 â€“ Performance comparison of various bagging techniques
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.2 â€“ å„ç§è¢‹è£…æŠ€æœ¯çš„æ€§èƒ½æ¯”è¾ƒ
- en: 'Here are some conclusions we can draw from *Table 4.2*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¯ä»¥ä»*è¡¨4.2*ä¸­å¾—å‡ºçš„ç»“è®ºï¼š
- en: For maximizing the F2 score, **SMOTEBagging** did the best
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºäº†æœ€å¤§åŒ–F2åˆ†æ•°ï¼Œ**SMOTEBagging**è¡¨ç°æœ€ä½³
- en: For high precision, **bagging** and **random forest** performed exceptionally
    well
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºé«˜ç²¾ç¡®åº¦ï¼Œ**è¢‹è£…æ³•**å’Œ**éšæœºæ£®æ—**è¡¨ç°å¼‚å¸¸å‡ºè‰²
- en: For high recall, **OverBagging** and **balanced random forest** are strong choices
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºé«˜å¬å›ç‡ï¼Œ**OverBagging**å’Œ**å¹³è¡¡éšæœºæ£®æ—**æ˜¯å¼ºæœ‰åŠ›çš„é€‰æ‹©
- en: For general performance across all metrics, **SMOTEBagging** and **bagging**
    proved to be solid options
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰æŒ‡æ ‡çš„ç»¼åˆæ€§èƒ½ï¼Œ**SMOTEBagging**å’Œ**è¢‹è£…æ³•**è¢«è¯æ˜æ˜¯ç¨³å›ºçš„é€‰é¡¹
- en: In conclusion, although ensemble approaches such as bagging and random forest
    establish robust benchmarks that are challenging to outperform, incorporating
    imbalanced learning strategies such as SMOTEBagging can lead to notable gains.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œå°½ç®¡å¦‚è¢‹è£…æ³•å’Œéšæœºæ£®æ—è¿™æ ·çš„é›†æˆæ–¹æ³•å»ºç«‹äº†éš¾ä»¥è¶…è¶Šçš„ç¨³å¥åŸºå‡†ï¼Œä½†ç»“åˆå¦‚SMOTEBaggingè¿™æ ·çš„ä¸å¹³è¡¡å­¦ä¹ ç­–ç•¥å¯ä»¥å¸¦æ¥æ˜¾è‘—çš„æ”¶ç›Šã€‚
- en: This concludes our discussion of bagging techniques. If bagging is the wisdom
    of the crowd, boosting is the master sculptor, refining the previous art with
    each stroke. Weâ€™ll try to understand how boosting works in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±ç»“æŸäº†æˆ‘ä»¬å¯¹è¢‹è£…æŠ€æœ¯çš„è®¨è®ºã€‚å¦‚æœè¢‹è£…æ³•æ˜¯ç¾¤ä¼—çš„æ™ºæ…§ï¼Œé‚£ä¹ˆæå‡æ³•å°±æ˜¯å¤§å¸ˆçº§çš„é›•å¡‘å®¶ï¼Œé€šè¿‡æ¯ä¸€ç¬”éƒ½ç²¾ç‚¼ç€å…ˆå‰çš„è‰ºæœ¯ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚å°è¯•ç†è§£æå‡æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: Boosting techniques for imbalanced data
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤„ç†ä¸å¹³è¡¡æ•°æ®çš„æå‡æŠ€æœ¯
- en: 'Imagine two friends doing group study to solve their mathematics assignment.
    The first student is strong in most topics but weak in two topics: complex numbers
    and triangles. So, the first student asks the second student to spend more time
    on these two topics. Then, while solving the assignments, they combine their answers.
    Since the first student knows most of the topics well, they decided to give more
    weight to his answers to the assignment questions. What these two students are
    doing is the key idea behind boosting.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸¤ä¸ªæœ‹å‹ä¸€èµ·åšå°ç»„å­¦ä¹ æ¥è§£å†³ä»–ä»¬çš„æ•°å­¦ä½œä¸šã€‚ç¬¬ä¸€ä¸ªå­¦ç”Ÿåœ¨å¤§å¤šæ•°ä¸»é¢˜ä¸Šéƒ½å¾ˆå¼ºï¼Œä½†åœ¨ä¸¤ä¸ªä¸»é¢˜ä¸Šè¾ƒå¼±ï¼šå¤æ•°å’Œä¸‰è§’å½¢ã€‚å› æ­¤ï¼Œç¬¬ä¸€ä¸ªå­¦ç”Ÿè¦æ±‚ç¬¬äºŒä¸ªå­¦ç”Ÿåœ¨è¿™ä¸¤ä¸ªä¸»é¢˜ä¸ŠèŠ±æ›´å¤šçš„æ—¶é—´ã€‚ç„¶åï¼Œåœ¨è§£å†³ä½œä¸šçš„è¿‡ç¨‹ä¸­ï¼Œä»–ä»¬ç»“åˆäº†ä»–ä»¬çš„ç­”æ¡ˆã€‚ç”±äºç¬¬ä¸€ä¸ªå­¦ç”Ÿå¤§å¤šæ•°ä¸»é¢˜éƒ½å¾ˆç†Ÿæ‚‰ï¼Œä»–ä»¬å†³å®šåœ¨ä½œä¸šé—®é¢˜çš„ç­”æ¡ˆä¸­ç»™äºˆä»–çš„ç­”æ¡ˆæ›´å¤šçš„æƒé‡ã€‚è¿™ä¸¤ä¸ªå­¦ç”Ÿæ‰€åšçš„æ˜¯æå‡æ³•çš„æ ¸å¿ƒæ€æƒ³ã€‚
- en: In bagging, we noticed that we could train all the classifiers in parallel.
    These classifiers are trained on a subset of the data, and all of them have an
    equal say at the time of prediction.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¢‹è£…æ³•ä¸­ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°æˆ‘ä»¬å¯ä»¥å¹¶è¡Œè®­ç»ƒæ‰€æœ‰åˆ†ç±»å™¨ã€‚è¿™äº›åˆ†ç±»å™¨æ˜¯åœ¨æ•°æ®çš„ä¸€ä¸ªå­é›†ä¸Šè®­ç»ƒçš„ï¼Œå¹¶ä¸”åœ¨é¢„æµ‹æ—¶å®ƒä»¬éƒ½æœ‰å¹³ç­‰çš„å‘è¨€æƒã€‚
- en: In boosting, the classifiers are trained one after the other. While every classifier
    learns from the whole data, points in the dataset are assigned different weights
    based on their difficulty of classification. Classifiers are also assigned weights
    that tell us about their predictive power. While predicting new data, the weighted
    sum of the classifiers is used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æå‡æ³•ä¸­ï¼Œåˆ†ç±»å™¨æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°è®­ç»ƒçš„ã€‚è™½ç„¶æ¯ä¸ªåˆ†ç±»å™¨éƒ½ä»æ•´ä¸ªæ•°æ®ä¸­å­¦ä¹ ï¼Œä½†æ ¹æ®æ•°æ®é›†ä¸­ç‚¹çš„åˆ†ç±»éš¾åº¦ï¼Œè¿™äº›ç‚¹è¢«åˆ†é…äº†ä¸åŒçš„æƒé‡ã€‚åˆ†ç±»å™¨ä¹Ÿè¢«åˆ†é…äº†æƒé‡ï¼Œè¿™äº›æƒé‡å‘Šè¯‰æˆ‘ä»¬å®ƒä»¬çš„é¢„æµ‹èƒ½åŠ›ã€‚åœ¨é¢„æµ‹æ–°æ•°æ®æ—¶ï¼Œä½¿ç”¨åˆ†ç±»å™¨çš„åŠ æƒæ€»å’Œã€‚
- en: 'Boosting begins by training the first classifier on the whole dataset, with
    each data point assigned the same weight. In the second iteration, the data points
    that were misclassified in the first iteration are given more weight, and a second
    classifier is trained with these new weights. A weight is also assigned to the
    classifiers themselves based on their overall performance. This process continues
    through multiple iterations with different classifiers. *Figure 4**.12* illustrates
    this concept for a two-class dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡æ³•é¦–å…ˆåœ¨å…¨éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒç¬¬ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œæ¯ä¸ªæ•°æ®ç‚¹åˆ†é…ç›¸åŒçš„æƒé‡ã€‚åœ¨ç¬¬äºŒæ¬¡è¿­ä»£ä¸­ï¼Œç¬¬ä¸€æ¬¡è¿­ä»£ä¸­è¢«è¯¯åˆ†ç±»çš„æ•°æ®ç‚¹è¢«èµ‹äºˆæ›´å¤šçš„æƒé‡ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ–°æƒé‡è®­ç»ƒç¬¬äºŒä¸ªåˆ†ç±»å™¨ã€‚åŒæ—¶ï¼Œä¹Ÿä¼šæ ¹æ®åˆ†ç±»å™¨çš„æ•´ä½“æ€§èƒ½ç»™åˆ†ç±»å™¨æœ¬èº«åˆ†é…æƒé‡ã€‚è¿™ä¸ªè¿‡ç¨‹é€šè¿‡å¤šä¸ªè¿­ä»£å’Œä¸åŒçš„åˆ†ç±»å™¨ç»§ç»­è¿›è¡Œã€‚*å›¾4.12*å±•ç¤ºäº†å¯¹äºåŒåˆ†ç±»æ•°æ®é›†çš„æ­¤æ¦‚å¿µï¼š
- en: '![](img/B17259_04_12.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_04_12.jpg)'
- en: 'Figure 4.12 â€“ Boosting idea: (left) the decision boundary from the first classifier;
    (middle) the weights of misclassified data points are bumped up for the second
    classifier; (right) the decision boundary from the second classifier'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.12 â€“ æå‡æ³•æ€æƒ³ï¼šï¼ˆå·¦ï¼‰ç¬¬ä¸€ä¸ªåˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œï¼›ï¼ˆä¸­ï¼‰ç¬¬äºŒåˆ†ç±»å™¨å¯¹è¯¯åˆ†ç±»æ•°æ®ç‚¹çš„æƒé‡æå‡ï¼›ï¼ˆå³ï¼‰ç¬¬äºŒä¸ªåˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œ
- en: The kind of boosting we just described is called **AdaBoost**. There is another
    category of boosting algorithms called gradient boosting, where the main focus
    is on minimizing the residuals (the difference between the actual value and predicted
    output value) of the previous model, trying to correct the previous modelâ€™s mistakes.
    There are several popular gradient boosting implementations, such as **XGBoost**,
    **LightGBM**, and **CatBoost**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšæ‰æè¿°çš„è¿™ç§æå‡æ–¹æ³•ç§°ä¸º**AdaBoost**ã€‚è¿˜æœ‰ä¸€ç§æå‡ç®—æ³•çš„ç±»åˆ«ç§°ä¸ºæ¢¯åº¦æå‡ï¼Œå…¶ä¸»è¦é‡ç‚¹æ˜¯æœ€å°åŒ–å…ˆå‰æ¨¡å‹çš„æ®‹å·®ï¼ˆå®é™…å€¼ä¸é¢„æµ‹è¾“å‡ºå€¼ä¹‹é—´çš„å·®å¼‚ï¼‰ï¼Œè¯•å›¾çº æ­£å…ˆå‰æ¨¡å‹çš„é”™è¯¯ã€‚æœ‰å‡ ä¸ªæµè¡Œçš„æ¢¯åº¦æå‡å®ç°ï¼Œå¦‚**XGBoost**ã€**LightGBM**å’Œ**CatBoost**ã€‚
- en: In this chapter, we will mostly focus on AdaBoost and modify it to account for
    data imbalance. However, swapping AdaBoost with XGBoost, for example, shouldnâ€™t
    be too difficult.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸»è¦å…³æ³¨AdaBoostå¹¶å°†å…¶ä¿®æ”¹ä¸ºè€ƒè™‘æ•°æ®ä¸å¹³è¡¡ã€‚ç„¶è€Œï¼Œå°†AdaBoostä¸XGBoostç­‰æ›¿æ¢ä¸åº”å¤ªå›°éš¾ã€‚
- en: AdaBoost
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost
- en: 'AdaBoost, short for adaptive boosting, is one of the earliest boosting methods
    based on decision trees. Decision trees are classifiers that are easy to ensemble
    together:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoostï¼Œå³è‡ªé€‚åº”æå‡ï¼Œæ˜¯åŸºäºå†³ç­–æ ‘çš„æœ€æ—©æå‡æ–¹æ³•ä¹‹ä¸€ã€‚å†³ç­–æ ‘æ˜¯æ˜“äºç»„åˆåœ¨ä¸€èµ·çš„åˆ†ç±»å™¨ï¼š
- en: '![](img/B17259_04_13.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_13.jpg)'
- en: Figure 4.13 â€“ AdaBoost pseudocode
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.13 â€“ AdaBoostä¼ªä»£ç 
- en: 'The following code shows how to import the classifier from the `sklearn` library
    and train it on the data:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç æ˜¾ç¤ºäº†å¦‚ä½•ä»`sklearn`åº“ä¸­å¯¼å…¥åˆ†ç±»å™¨å¹¶åœ¨æ•°æ®ä¸Šè®­ç»ƒå®ƒï¼š
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Letâ€™s plot what the decision boundary looks like after the model gets trained
    on the data:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶æ¨¡å‹åœ¨æ•°æ®ä¸Šè®­ç»ƒåçš„å†³ç­–è¾¹ç•Œï¼š
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 4**.14* shows the decision boundary of the model on the training data:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾4**.14*æ˜¾ç¤ºäº†æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å†³ç­–è¾¹ç•Œï¼š'
- en: '![](img/B17259_04_14.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_14.jpg)'
- en: Figure 4.14 â€“ The decision boundary of AdaBoostClassifier on the training data
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.14 â€“ AdaBoostClassifieråœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å†³ç­–è¾¹ç•Œ
- en: We can make oversampling and undersampling an integral part of the boosting
    algorithm, similar to how we did for the bagging algorithm. We will discuss that
    next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·ä½œä¸ºæå‡ç®—æ³•çš„ç»„æˆéƒ¨åˆ†ï¼Œç±»ä¼¼äºæˆ‘ä»¬å¯¹baggingç®—æ³•æ‰€åšçš„é‚£æ ·ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºè¿™ä¸€ç‚¹ã€‚
- en: RUSBoost, SMOTEBoost, and RAMOBoost
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RUSBoostã€SMOTEBoostå’ŒRAMOBoost
- en: 'As you might have guessed, we can combine AdaBoost with resampling techniques.
    Here is the main idea: at each boosting iteration, before training a classifier
    on the incorrect examples from the previous iteration, we sample the data (via
    some undersampling or oversampling variant). Hereâ€™s the general pseudocode:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€çŒœæµ‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†AdaBoostä¸é‡é‡‡æ ·æŠ€æœ¯ç›¸ç»“åˆã€‚ä»¥ä¸‹æ˜¯ä¸»è¦æ€è·¯ï¼šåœ¨æ¯æ¬¡æå‡è¿­ä»£ä¸­ï¼Œåœ¨è®­ç»ƒåˆ†ç±»å™¨ä¹‹å‰ï¼Œå¯¹å‰ä¸€æ¬¡è¿­ä»£ä¸­çš„é”™è¯¯ç¤ºä¾‹è¿›è¡Œæ•°æ®é‡‡æ ·ï¼ˆé€šè¿‡æŸäº›æ¬ é‡‡æ ·æˆ–è¿‡é‡‡æ ·å˜ä½“ï¼‰ã€‚ä»¥ä¸‹æ˜¯é€šç”¨ä¼ªä»£ç ï¼š
- en: '**Input**: The training data and some decision tree classifiers.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥**ï¼šè®­ç»ƒæ•°æ®å’Œä¸€äº›å†³ç­–æ ‘åˆ†ç±»å™¨ã€‚'
- en: '**Output**: An aggregated classifier.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¾“å‡º**ï¼šä¸€ä¸ªèšåˆåˆ†ç±»å™¨ã€‚'
- en: Initialize the equal weights for all the samples of the training data.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–è®­ç»ƒæ•°æ®æ‰€æœ‰æ ·æœ¬çš„ç›¸ç­‰æƒé‡ã€‚
- en: 'Repeat this for each decision tree classifier:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æ¯ä¸ªå†³ç­–æ ‘åˆ†ç±»å™¨é‡å¤æ­¤æ“ä½œï¼š
- en: 'Resample the data using a data sampling method:'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ•°æ®é‡‡æ ·æ–¹æ³•é‡æ–°é‡‡æ ·æ•°æ®ï¼š
- en: If the sampling method used is **Random UnderSampling** (**RUS**), the method
    is called **RUSBoost** [5].
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨çš„é‡‡æ ·æ–¹æ³•æ˜¯**éšæœºæ¬ é‡‡æ ·**ï¼ˆ**RUS**ï¼‰ï¼Œåˆ™è¯¥æ–¹æ³•ç§°ä¸º**RUSBoost** [5]ã€‚
- en: If the sampling method used is SMOTE, the method is called **SMOTEBoost** [6].
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœä½¿ç”¨çš„é‡‡æ ·æ–¹æ³•æ˜¯SMOTEï¼Œåˆ™è¯¥æ–¹æ³•ç§°ä¸º**SMOTEBoost** [6]ã€‚
- en: In **RAMOBoost** (short for **Ranked Minority Oversampling in Boosting** [7]),
    oversampling of the minority class is done based on the weight of the minority
    class examples. If the weight of an example is more (because the model didnâ€™t
    do well on that example in the previous iteration), then itâ€™s oversampled more,
    and vice versa.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨**RAMOBoost**ï¼ˆå³**Boostingä¸­çš„æ’åºå°‘æ•°è¿‡é‡‡æ ·** [7]ï¼‰ä¸­ï¼Œå°‘æ•°ç±»çš„è¿‡é‡‡æ ·æ˜¯åŸºäºå°‘æ•°ç±»ç¤ºä¾‹çš„æƒé‡è¿›è¡Œçš„ã€‚å¦‚æœä¸€ä¸ªç¤ºä¾‹çš„æƒé‡æ›´é«˜ï¼ˆå› ä¸ºæ¨¡å‹åœ¨å‰ä¸€æ¬¡è¿­ä»£ä¸­å¯¹è¯¥ç¤ºä¾‹çš„è¡¨ç°ä¸ä½³ï¼‰ï¼Œåˆ™å¯¹å…¶è¿›è¡Œæ›´å¤šçš„è¿‡é‡‡æ ·ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: Train a classifier on the resampled data, giving more importance to samples
    with higher weights based on previous iterations.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨é‡é‡‡æ ·æ•°æ®ä¸Šè®­ç»ƒåˆ†ç±»å™¨ï¼Œæ ¹æ®å…ˆå‰è¿­ä»£ä¸­æ ·æœ¬çš„æƒé‡ç»™äºˆæ›´é«˜çš„é‡è§†ã€‚
- en: Compute the error for the classifier on the given data by comparing its predictions
    with the actual outputs.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¯”è¾ƒå…¶é¢„æµ‹ä¸å®é™…è¾“å‡ºï¼Œè®¡ç®—ç»™å®šæ•°æ®ä¸Šåˆ†ç±»å™¨çš„é”™è¯¯ã€‚
- en: Consider all the wrongly classified examples for the next iteration. Increase
    the weights of such wrongly classified examples.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸‹ä¸€æ¬¡è¿­ä»£ä¸­æ‰€æœ‰è¢«é”™è¯¯åˆ†ç±»çš„ç¤ºä¾‹ã€‚å¢åŠ è¿™äº›é”™è¯¯åˆ†ç±»ç¤ºä¾‹çš„æƒé‡ã€‚
- en: Combine all the decision tree classifiers into a final classifier, where the
    classifiers with smaller error values on the training data have a larger say in
    the final prediction.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†³ç­–æ ‘åˆ†ç±»å™¨ç»„åˆæˆä¸€ä¸ªæœ€ç»ˆåˆ†ç±»å™¨ï¼Œå…¶ä¸­åœ¨è®­ç»ƒæ•°æ®ä¸Šå…·æœ‰è¾ƒå°é”™è¯¯å€¼çš„åˆ†ç±»å™¨åœ¨æœ€ç»ˆé¢„æµ‹ä¸­å…·æœ‰æ›´å¤§çš„å‘è¨€æƒã€‚
- en: 'In this pseudocode, *Step 4 (I)* is the only extra step we have added compared
    to the AdaBoost algorithm. Letâ€™s discuss the pros and cons of these techniques:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¼ªä»£ç ä¸­ï¼Œ*æ­¥éª¤4ï¼ˆIï¼‰*æ˜¯æˆ‘ä»¬ç›¸å¯¹äºAdaBoostç®—æ³•æ‰€æ·»åŠ çš„å”¯ä¸€é¢å¤–æ­¥éª¤ã€‚è®©æˆ‘ä»¬è®¨è®ºè¿™äº›æŠ€æœ¯çš„ä¼˜ç¼ºç‚¹ï¼š
- en: In RUSBoost, as the data is reduced, we tend to have a faster training time.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨RUSBoostä¸­ï¼Œéšç€æ•°æ®çš„å‡å°‘ï¼Œæˆ‘ä»¬å€¾å‘äºæœ‰æ›´å¿«çš„è®­ç»ƒæ—¶é—´ã€‚
- en: SMOTEBoost produces synthetic samples from the minority class. Thus, it adds
    diversity to the data and may improve the classifierâ€™s accuracy. However, it would
    increase the time to train and may not be scalable to very large datasets.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTEBoostä»å°‘æ•°ç±»ç”Ÿæˆåˆæˆæ ·æœ¬ã€‚å› æ­¤ï¼Œå®ƒå¢åŠ äº†æ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶å¯èƒ½æé«˜åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œå®ƒä¼šå¢åŠ è®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•æ‰©å±•åˆ°éå¸¸å¤§çš„æ•°æ®é›†ã€‚
- en: RAMOBoost gives preference to the samples near the class boundaries. This can
    improve performance in some cases. However, like SMOTEBoost, this method may increase
    the training time and cost and may cause overfitting of the final model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAMOBoostä¼˜å…ˆè€ƒè™‘é è¿‘ç±»åˆ«è¾¹ç•Œçš„æ ·æœ¬ã€‚è¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œç±»ä¼¼äºSMOTEBoostï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å’Œæˆæœ¬ï¼Œå¹¶å¯èƒ½å¯¼è‡´æœ€ç»ˆæ¨¡å‹çš„è¿‡æ‹Ÿåˆã€‚
- en: 'The `imbalanced-learn` library provides the implementation for `RUSBoostClassifier`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`åº“æä¾›äº†`RUSBoostClassifier`çš„å®ç°ï¼š'
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Letâ€™s examine the decision boundary of the trained model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥è®­ç»ƒæ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼š
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting plot is shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå›¾å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/B17259_04_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_15.jpg)'
- en: Figure 4.15 â€“ The decision boundary of RUSBoostClassifier on training data
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.15 â€“ RUSBoostClassifieråœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å†³ç­–è¾¹ç•Œ
- en: The `imbalanced-learn` library doesnâ€™t have the implementations of RAMOBoost
    and SMOTEBoost yet (as of version 0.11.0). You can check the open source repository
    at [https://github.com/dialnd/imbalanced-algorithms](https://github.com/dialnd/imbalanced-algorithms)
    for reference implementations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`åº“å°šæœªå®ç°RAMOBoostå’ŒSMOTEBoostï¼ˆæˆªè‡³ç‰ˆæœ¬0.11.0ï¼‰ã€‚æ‚¨å¯ä»¥åœ¨[https://github.com/dialnd/imbalanced-algorithms](https://github.com/dialnd/imbalanced-algorithms)çš„å¼€æ”¾æºä»£ç å­˜å‚¨åº“ä¸­æŸ¥çœ‹å‚è€ƒå®ç°ã€‚'
- en: Can we create multiple subsets of the majority class, train an ensemble from
    each of these subsets, and combine all weak classifiers in these ensembles into
    a final output? This approach will be explored in the next section, where we will
    utilize the ensemble of ensembles technique.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦ä¸ºå¤šæ•°ç±»åˆ›å»ºå¤šä¸ªå­é›†ï¼Œä»æ¯ä¸ªå­é›†ä¸­è®­ç»ƒä¸€ä¸ªé›†æˆï¼Œå¹¶å°†è¿™äº›é›†æˆä¸­çš„æ‰€æœ‰å¼±åˆ†ç±»å™¨ç»„åˆæˆä¸€ä¸ªæœ€ç»ˆè¾“å‡ºï¼Ÿè¿™ç§æ–¹æ³•å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­æ¢è®¨ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨é›†æˆé›†æˆæŠ€æœ¯ã€‚
- en: Ensemble of ensembles
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é›†æˆé›†æˆ
- en: 'Can we combine boosting and bagging? As we saw earlier, in bagging, we create
    multiple subsets of data and then train classifiers on those datasets. We can
    treat AdaBoost as a classifier while doing bagging. The process is simple: first,
    we create the bags and then train different AdaBoost classifiers on each bag.
    Here, AdaBoost is an ensemble in itself. Thus, these models are called an **ensemble**
    **of ensembles**.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦ç»“åˆæå‡å’Œbaggingï¼Ÿå¦‚æˆ‘ä»¬ä¹‹å‰æ‰€çœ‹åˆ°çš„ï¼Œåœ¨baggingä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºå¤šä¸ªæ•°æ®å­é›†ï¼Œç„¶ååœ¨é‚£äº›æ•°æ®é›†ä¸Šè®­ç»ƒåˆ†ç±»å™¨ã€‚æˆ‘ä»¬å¯ä»¥å°†AdaBoostè§†ä¸ºåœ¨baggingè¿‡ç¨‹ä¸­ä½¿ç”¨çš„åˆ†ç±»å™¨ã€‚è¿‡ç¨‹å¾ˆç®€å•ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºè¢‹å­ï¼Œç„¶ååœ¨æ¯ä¸ªè¢‹å­ä¸Šè®­ç»ƒä¸åŒçš„AdaBooståˆ†ç±»å™¨ã€‚åœ¨è¿™é‡Œï¼ŒAdaBoostæœ¬èº«å°±æ˜¯ä¸€ç§é›†æˆã€‚å› æ­¤ï¼Œè¿™äº›æ¨¡å‹è¢«ç§°ä¸º**é›†æˆ**
    **çš„é›†æˆ**ã€‚
- en: On top of having an ensemble of ensembles, we can also do undersampling (or
    oversampling) at the time of bagging. This gives us the **benefits of bagging**,
    **boosting**, and **random undersampling** (or oversampling) in a single model.
    We will discuss one such algorithm in this section, called **EasyEnsemble**. Since
    random undersampling doesnâ€™t have significant overhead, both algorithms have training
    times similar to any other algorithm with the same number of weak classifiers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ‹¥æœ‰é›†æˆé›†æˆä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åœ¨baggingæ—¶è¿›è¡Œä¸‹é‡‡æ ·ï¼ˆæˆ–è¿‡é‡‡æ ·ï¼‰ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹ä¸­è·å¾—ä¸‹é‡‡æ ·ï¼ˆæˆ–è¿‡é‡‡æ ·ï¼‰çš„**è¢‹è£…**ã€**æå‡**å’Œ**éšæœºä¸‹é‡‡æ ·**ï¼ˆæˆ–è¿‡é‡‡æ ·ï¼‰çš„**å¥½å¤„**ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ä¸ªåä¸º**EasyEnsemble**çš„ç®—æ³•ï¼Œå®ƒå…·æœ‰ä¸å…·æœ‰ç›¸åŒæ•°é‡å¼±åˆ†ç±»å™¨çš„ä»»ä½•å…¶ä»–ç®—æ³•ç›¸ä¼¼çš„è®­ç»ƒæ—¶é—´ã€‚ç”±äºéšæœºä¸‹é‡‡æ ·æ²¡æœ‰æ˜¾è‘—çš„å¼€é”€ï¼Œè¿™ä¸¤ä¸ªç®—æ³•çš„è®­ç»ƒæ—¶é—´ä¸å…¶ä»–ç®—æ³•ç›¸ä¼¼ã€‚
- en: EasyEnsemble
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EasyEnsemble
- en: 'The EasyEnsemble algorithm [8] generates balanced datasets from the original
    dataset and trains a different AdaBoost classifier on each of the balanced datasets.
    Later, it creates an aggregate classifier that makes predictions based on the
    majority votes of the AdaBoost classifiers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: EasyEnsembleç®—æ³•[8]ä»åŸå§‹æ•°æ®é›†ä¸­ç”Ÿæˆå¹³è¡¡æ•°æ®é›†ï¼Œå¹¶åœ¨æ¯ä¸ªå¹³è¡¡æ•°æ®é›†ä¸Šè®­ç»ƒä¸åŒçš„AdaBooståˆ†ç±»å™¨ã€‚éšåï¼Œå®ƒåˆ›å»ºä¸€ä¸ªèšåˆåˆ†ç±»å™¨ï¼Œè¯¥åˆ†ç±»å™¨åŸºäºAdaBooståˆ†ç±»å™¨çš„å¤šæ•°æŠ•ç¥¨è¿›è¡Œé¢„æµ‹ï¼š
- en: '![](img/B17259_04_16.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_16.jpg)'
- en: Figure 4.16 â€“ EasyEnsemble pseudocode
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.16 â€“ EasyEnsembleä¼ªä»£ç 
- en: '*Figure 4**.17* summarizes the EasyEnsemble algorithm using three subsets of
    the training data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾4**.17*æ€»ç»“äº†ä½¿ç”¨è®­ç»ƒæ•°æ®ä¸‰ä¸ªå­é›†çš„EasyEnsembleç®—æ³•ï¼š'
- en: '![](img/B17259_04_17.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_17.jpg)'
- en: Figure 4.17 â€“ EasyEnsemble algorithm explained
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.17 â€“ EasyEnsembleç®—æ³•è§£é‡Š
- en: Instead of randomly undersampling the majority class examples, we can randomly
    oversample the minority class examples too.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä»…å¯ä»¥éšæœºä¸‹é‡‡æ ·å¤šæ•°ç±»ç¤ºä¾‹ï¼Œè¿˜å¯ä»¥éšæœºä¸Šé‡‡æ ·å°‘æ•°ç±»ç¤ºä¾‹ã€‚
- en: 'The `imbalanced-learn` library provides the API for EasyEnsemble using `EasyEnsembleClassifier`.
    The `EasyEnsembleClassifier` API provides a `base_estimator` argument that can
    be used to set any classifier, with the default being `AdaBoostClassifier`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`åº“æä¾›äº†ä½¿ç”¨`EasyEnsembleClassifier`çš„APIã€‚`EasyEnsembleClassifier`
    APIæä¾›äº†ä¸€ä¸ª`base_estimator`å‚æ•°ï¼Œå¯ä»¥ç”¨æ¥è®¾ç½®ä»»ä½•åˆ†ç±»å™¨ï¼Œé»˜è®¤ä¸º`AdaBoostClassifier`ï¼š'
- en: '[PRE8]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Letâ€™s plot the decision boundary:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶å†³ç­–è¾¹ç•Œï¼š
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B17259_04_18.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_18.jpg)'
- en: Figure 4.18 â€“ The decision boundary of EasyEnsembleClassifier on the training
    data
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.18 â€“ EasyEnsembleClassifieråœ¨è®­ç»ƒæ•°æ®ä¸Šçš„å†³ç­–è¾¹ç•Œ
- en: By default, EasyEnsemble uses `AdaBoostClassifier` as the base estimator. However,
    we can use any other estimator as well, such as `XGBoostClassifier`, or tune it
    in other ways, say by passing another `sampling_strategy`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼ŒEasyEnsembleä½¿ç”¨`AdaBoostClassifier`ä½œä¸ºåŸºçº¿ä¼°è®¡å™¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•å…¶ä»–ä¼°è®¡å™¨ï¼Œä¾‹å¦‚`XGBoostClassifier`ï¼Œæˆ–è€…ä»¥å…¶ä»–æ–¹å¼è°ƒæ•´å®ƒï¼Œä¾‹å¦‚é€šè¿‡ä¼ é€’å¦ä¸€ä¸ª`sampling_strategy`ã€‚
- en: This concludes our discussion of EasyEnsemble. Next, we will compare the various
    boosting methods that weâ€™ve studied.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±ç»“æŸäº†æˆ‘ä»¬å¯¹EasyEnsembleçš„è®¨è®ºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒæˆ‘ä»¬ç ”ç©¶è¿‡çš„å„ç§æå‡æ–¹æ³•ã€‚
- en: Comparative performance of boosting methods
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æå‡æ–¹æ³•çš„æ¯”è¾ƒæ€§èƒ½
- en: 'Letâ€™s compare the performance of the various boosting methods weâ€™ve discussed.
    We use a decision tree as a baseline and RUSBoost, AdaBoost, XGBoost, and EasyEnsemble,
    along with two variants. By default, `EasyEnsembleClassifier` uses `AdaBoostClassifier`
    as a baseline estimator. We use XGBoost instead as the estimator in the second
    variant of `EasyEnsembleClassifier`; in the third variant, we use `not majority`
    for our `sampling_strategy`, along with the XGBoost estimator:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¯”è¾ƒæˆ‘ä»¬è®¨è®ºè¿‡çš„å„ç§æå‡æ–¹æ³•çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä»¥å†³ç­–æ ‘ä½œä¸ºåŸºçº¿ï¼Œå¹¶ä½¿ç”¨RUSBoostã€AdaBoostã€XGBoostå’ŒEasyEnsembleï¼Œä»¥åŠä¸¤ç§å˜ä½“ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`EasyEnsembleClassifier`ä½¿ç”¨`AdaBoostClassifier`ä½œä¸ºåŸºçº¿ä¼°è®¡å™¨ã€‚æˆ‘ä»¬åœ¨`EasyEnsembleClassifier`çš„ç¬¬äºŒç§å˜ä½“ä¸­ä½¿ç”¨XGBoostä½œä¸ºä¼°è®¡å™¨ï¼›åœ¨ç¬¬ä¸‰ç§å˜ä½“ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`not
    majority`ä½œä¸ºæˆ‘ä»¬çš„`sampling_strategy`ï¼Œå¹¶é…åˆXGBoostä¼°è®¡å™¨ï¼š
- en: '| **Technique** | **F2 Score** | **Precision** | **Recall** | **Average Precision**
    | **AUC-ROC** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **æŠ€æœ¯** | **F2åˆ†æ•°** | **ç²¾ç¡®åº¦** | **å¬å›ç‡** | **å¹³å‡ç²¾ç¡®åº¦** | **AUC-ROC** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| EasyEnsemble (`estimator=XGBoost` and `sampling_strategy =` `not_majority`)
    | 0.885 | 0.933 | 0.874 | **0.978** | **1.000** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| EasyEnsembleï¼ˆä¼°è®¡å™¨=XGBoostå’Œ`sampling_strategy =` `not_majority`ï¼‰ | 0.885 |
    0.933 | 0.874 | **0.978** | **1.000** |'
- en: '| EasyEnsemble (`estimator=XGBoost`) | 0.844 | 0.520 | **1.000** | **0.978**
    | 0.999 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| EasyEnsembleï¼ˆä¼°è®¡å™¨=XGBoostï¼‰ | 0.844 | 0.520 | **1.000** | **0.978** | 0.999
    |'
- en: '| EasyEnsemble | 0.844 | 0.519 | **1.000** | 0.940 | 0.999 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| EasyEnsemble | 0.844 | 0.519 | **1.000** | 0.940 | 0.999 |'
- en: '| RUSBoost | 0.836 | 0.517 | 0.989 | 0.948 | **1.000** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| RUSBoost | 0.836 | 0.517 | 0.989 | 0.948 | **1.000** |'
- en: '| AdaBoost | **0.907** | 0.938 | 0.900 | **0.978** | **1.000** |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost | **0.907** | 0.938 | 0.900 | **0.978** | **1.000** |'
- en: '| XGBoost | 0.885 | 0.933 | 0.874 | 0.968 | **1.000** |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost | 0.885 | 0.933 | 0.874 | 0.968 | **1.000** |'
- en: '| Decision Tree | 0.893 | **0.960** | 0.878 | 0.930 | 0.981 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| å†³ç­–æ ‘ | 0.893 | **0.960** | 0.878 | 0.930 | 0.981 |'
- en: Table 4.3 â€“ Performance comparison of various boosting techniques
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨4.3 â€“ å„ç§æå‡æŠ€æœ¯çš„æ€§èƒ½æ¯”è¾ƒ
- en: 'Here are some conclusions from *Table 4.3*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä»*è¡¨4.3*ä¸­å¾—å‡ºçš„ç»“è®ºï¼š
- en: For the highest F2 score, AdaBoost is the best choice
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæœ€é«˜çš„F2åˆ†æ•°ï¼ŒAdaBoostæ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚
- en: For high precision, the plain decision tree beats all other techniques
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºé«˜ç²¾ç¡®åº¦ï¼Œçº¯å†³ç­–æ ‘å‡»è´¥äº†æ‰€æœ‰å…¶ä»–æŠ€æœ¯ã€‚
- en: For perfect recall, EasyEnsemble (`estimator=XGBoost`) and EasyEnsemble perform
    perfectly
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºå®Œç¾çš„å¬å›ç‡ï¼ŒEasyEnsembleï¼ˆä¼°è®¡å™¨ä¸º`XGBoost`ï¼‰å’ŒEasyEnsembleè¡¨ç°å®Œç¾ã€‚
- en: For overall balanced performance, AdaBoost and EasyEnsemble (`estimator=XGBoost`
    and `sampling_strategy=not_majority`) are strong contenders
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºæ•´ä½“å¹³è¡¡æ€§èƒ½ï¼ŒAdaBoostå’ŒEasyEnsembleï¼ˆä¼°è®¡å™¨ä¸º`XGBoost`å’Œ`sampling_strategy=not_majority`ï¼‰æ˜¯å¼ºæœ‰åŠ›çš„ç«äº‰è€…ã€‚
- en: The ensembling techniques such as RUSBoost and EasyEnsemble are specifically
    designed for handling data imbalance and improving recall compared to a baseline
    model such as the decision tree or even AdaBoost
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆæŠ€æœ¯ï¼Œå¦‚RUSBoostå’ŒEasyEnsembleï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†æ•°æ®ä¸å¹³è¡¡å¹¶æé«˜å¬å›ç‡ï¼Œä¸åŸºçº¿æ¨¡å‹ï¼ˆå¦‚å†³ç­–æ ‘æˆ–ç”šè‡³AdaBoostï¼‰ç›¸æ¯”ã€‚
- en: Overall, the results indicate that while ensemble methods such as AdaBoost and
    XGBoost provide robust baselines that are hard to beat, leveraging imbalanced
    learning techniques can indeed modify the decision boundaries of the resulting
    classifiers, which can potentially help with improving the recall. The efficacy
    of these techniques, however, largely depends on the dataset and performance metric
    under consideration.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œç»“æœè¡¨æ˜ï¼Œå°½ç®¡åƒAdaBoostå’ŒXGBoostè¿™æ ·çš„é›†æˆæ–¹æ³•æä¾›äº†éš¾ä»¥è¶…è¶Šçš„ç¨³å¥åŸºå‡†ï¼Œä½†åˆ©ç”¨ä¸å¹³è¡¡å­¦ä¹ æŠ€æœ¯ç¡®å®å¯ä»¥ä¿®æ”¹ç»“æœåˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œï¼Œè¿™æœ‰å¯èƒ½æœ‰åŠ©äºæé«˜å¬å›ç‡ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯çš„æœ‰æ•ˆæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæ‰€è€ƒè™‘çš„æ•°æ®é›†å’Œæ€§èƒ½æŒ‡æ ‡ã€‚
- en: By wrapping up our journey through the ensemble of ensembles, weâ€™ve added yet
    another powerful and dynamic tool to our machine learning arsenal.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç»“æŸæˆ‘ä»¬é€šè¿‡é›†æˆé›†æˆä¹‹æ—…ï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ å·¥å…·ç®±æ·»åŠ äº†å¦ä¸€ä¸ªå¼ºå¤§ä¸”åŠ¨æ€çš„å·¥å…·ã€‚
- en: Model performance comparison
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
- en: The effectiveness of the techniques weâ€™ve discussed so far can be highly dependent
    on the dataset they are applied to. In this section, we will conduct a comprehensive
    comparative analysis that compares the various techniques we have discussed so
    far while using the logistic regression model as a baseline. For a comprehensive
    review of the complete implementation, please consult the accompanying notebook
    available on GitHub.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€è®¨è®ºçš„æŠ€æœ¯æœ‰æ•ˆæ€§é«˜åº¦ä¾èµ–äºå®ƒä»¬åº”ç”¨åˆ°çš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è¿›è¡Œå…¨é¢çš„æ¯”è¾ƒåˆ†æï¼Œæ¯”è¾ƒæˆ‘ä»¬è¿„ä»Šä¸ºæ­¢æ‰€è®¨è®ºçš„å„ç§æŠ€æœ¯ï¼ŒåŒæ—¶ä»¥é€»è¾‘å›å½’æ¨¡å‹ä½œä¸ºåŸºå‡†ã€‚æœ‰å…³å®Œæ•´å®ç°çš„å…¨é¢å®¡æŸ¥ï¼Œè¯·å‚é˜…GitHubä¸Šæä¾›çš„é…å¥—ç¬”è®°æœ¬ã€‚
- en: 'The analysis spans four distinct datasets, each with its own characteristics
    and challenges:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ææ¶µç›–äº†å››ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹æ€§å’ŒæŒ‘æˆ˜ï¼š
- en: '**Synthetic data with Sep: 0.5**: A simulated dataset with moderate separation
    between classes, serving as a baseline to understand algorithm performance in
    simplified conditions.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep: 0.5çš„åˆæˆæ•°æ®**ï¼šä¸€ä¸ªç±»åˆ«ä¹‹é—´å…·æœ‰é€‚åº¦åˆ†ç¦»çš„æ¨¡æ‹Ÿæ•°æ®é›†ï¼Œä½œä¸ºåœ¨ç®€åŒ–æ¡ä»¶ä¸‹ç†è§£ç®—æ³•æ€§èƒ½çš„åŸºå‡†ã€‚'
- en: '**Synthetic data with Sep: 0.9**: Another synthetic dataset, but with a higher
    degree of separation, allowing us to examine how algorithms perform as class distinguishability
    improves.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep: 0.9çš„åˆæˆæ•°æ®**ï¼šå¦ä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œä½†å…·æœ‰æ›´é«˜çš„åˆ†ç¦»åº¦ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ£€æŸ¥éšç€ç±»åˆ«åŒºåˆ†æ€§çš„æé«˜ï¼Œç®—æ³•çš„è¡¨ç°å¦‚ä½•ã€‚'
- en: '`imblearn`) related to healthcare, chosen for its practical importance and
    the natural class imbalance often seen in medical datasets.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸åŒ»ç–—ä¿å¥ç›¸å…³çš„`imblearn`)ï¼Œå› å…¶å®é™…é‡è¦æ€§ä»¥åŠåŒ»ç–—æ•°æ®é›†ä¸­å¸¸è§çš„è‡ªç„¶ç±»åˆ«ä¸å¹³è¡¡è€Œè¢«é€‰ä¸­ã€‚
- en: '`imblearn` as well.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imblearn`ä¹Ÿæ˜¯å¦‚æ­¤ã€‚'
- en: Our primary metric for evaluation is average precision, a summary measure that
    combines both precision and recall, thereby providing a balanced view of algorithm
    performance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ä¸»è¦è¯„ä¼°æŒ‡æ ‡æ˜¯å¹³å‡ç²¾ç¡®åº¦ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„ç»¼åˆåº¦é‡ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªå¹³è¡¡çš„ç®—æ³•æ€§èƒ½è§†å›¾ã€‚
- en: Weâ€™d like to emphasize that we are using the vanilla versions of the various
    ensemble models for comparison. With some additional effort in tuning the hyperparameters
    of these models, we could certainly enhance the performance of these implementations.
    We leave that as an exercise for you.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³å¼ºè°ƒï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨å„ç§é›†æˆæ¨¡å‹çš„åŸå§‹ç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒã€‚é€šè¿‡ä¸€äº›é¢å¤–çš„åŠªåŠ›æ¥è°ƒæ•´è¿™äº›æ¨¡å‹çš„è¶…å‚æ•°ï¼Œæˆ‘ä»¬å½“ç„¶å¯ä»¥å¢å¼ºè¿™äº›å®ç°çš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ç•™ç»™ä½ ä½œä¸ºç»ƒä¹ ã€‚
- en: 'By comparing these diverse algorithms across a variety of datasets, this analysis
    aims to provide some valuable insights into the following aspects:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨å¤šç§æ•°æ®é›†ä¸Šæ¯”è¾ƒè¿™äº›ä¸åŒçš„ç®—æ³•ï¼Œè¿™é¡¹åˆ†ææ—¨åœ¨ä»¥ä¸‹æ–¹é¢æä¾›ä¸€äº›æœ‰ä»·å€¼çš„è§è§£ï¼š
- en: How conventional and specialized techniques stack up against each other
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼ ç»ŸæŠ€æœ¯ä¸ä¸“é—¨æŠ€æœ¯çš„å¯¹æ¯”
- en: The dependency of algorithm effectiveness on dataset characteristics
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç®—æ³•æœ‰æ•ˆæ€§å¯¹æ•°æ®é›†ç‰¹å¾çš„ä¾èµ–æ€§
- en: The practical implications of choosing one algorithm over another in different
    scenarios
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ä¸åŒåœºæ™¯ä¸‹é€‰æ‹©ä¸€ç§ç®—æ³•è€Œéå¦ä¸€ç§ç®—æ³•çš„å®é™…å½±å“
- en: '*Figure 4**.19* compares the performance of various bagging and boosting techniques
    using the average precision score, while using the logistic regression model as
    a baseline, over two synthetic datasets:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾4.19*æ¯”è¾ƒäº†ä½¿ç”¨å¹³å‡ç²¾ç¡®åº¦å¾—åˆ†ï¼Œä»¥é€»è¾‘å›å½’æ¨¡å‹ä¸ºåŸºå‡†ï¼Œåœ¨ä¸¤ä¸ªåˆæˆæ•°æ®é›†ä¸Šå„ç§baggingå’ŒboostingæŠ€æœ¯çš„æ€§èƒ½ï¼š'
- en: '![](img/B17259_04_19.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_04_19.jpg)'
- en: Figure 4.19 â€“ Average precision scores on synthetic datasets
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.19 â€“ åˆæˆæ•°æ®é›†ä¸Šçš„å¹³å‡ç²¾ç¡®åº¦å¾—åˆ†
- en: '*Figure 4**.20* shows similar plots across two real-world datasets:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾4.20*æ˜¾ç¤ºäº†ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„ç›¸ä¼¼å›¾ï¼š'
- en: '![](img/B17259_04_20.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_04_20.jpg)'
- en: Figure 4.20 â€“ Average precision scores on the thyroid_sick and abalone_19 datasets
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4.20 â€“ thyroid_sickå’Œabalone_19æ•°æ®é›†ä¸Šçš„å¹³å‡ç²¾ç¡®åº¦å¾—åˆ†
- en: 'Letâ€™s analyze these results for each of the datasets:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ†ææ¯ä¸ªæ•°æ®é›†çš„ç»“æœï¼š
- en: '**Synthetic data with Sep 0.5** (*Figure 4**.19*, left): XGBoost and logistic
    regression performed the best in terms of average precision, scoring 0.30 and
    0.27, respectively. Interestingly, ensemble methods designed specifically for
    imbalanced data, such as SMOTEBagging and OverBagging, perform comparably or even
    worse than conventional methods such as bagging. This suggests that specialized
    methods do not always guarantee an advantage in simpler synthetic settings.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep 0.5çš„åˆæˆæ•°æ®** (*å›¾4.19*ï¼Œå·¦ä¾§)ï¼šåœ¨å¹³å‡ç²¾ç¡®åº¦æ–¹é¢ï¼ŒXGBoostå’Œé€»è¾‘å›å½’è¡¨ç°æœ€ä½³ï¼Œåˆ†åˆ«å¾—åˆ†ä¸º0.30å’Œ0.27ã€‚æœ‰è¶£çš„æ˜¯ï¼Œä¸“é—¨ä¸ºä¸å¹³è¡¡æ•°æ®è®¾è®¡çš„é›†æˆæ–¹æ³•ï¼Œå¦‚SMOTEBaggingå’ŒOverBaggingï¼Œè¡¨ç°ä¸å¸¸è§„æ–¹æ³•å¦‚Baggingç›¸å½“ï¼Œç”šè‡³æ›´å·®ã€‚è¿™è¡¨æ˜åœ¨æ›´ç®€å•çš„åˆæˆè®¾ç½®ä¸­ï¼Œä¸“é—¨çš„æ–¹æ³•å¹¶ä¸æ€»æ˜¯èƒ½ä¿è¯ä¼˜åŠ¿ã€‚'
- en: '**Synthetic data with Sep 0.9** (*Figure 4**.19*, right): EasyEnsemble takes
    the lead on this dataset with an average precision score of 0.64, closely followed
    by logistic regression and XGBoost. This higher separation seems to allow EasyEnsemble
    to capitalize on its focus on balancing, leading to better performance. Other
    ensemble methods such as UnderBagging and OverBagging perform reasonably but do
    not surpass the leaders.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep 0.9çš„åˆæˆæ•°æ®** (*å›¾4.19*ï¼Œå³ä¾§)ï¼šEasyEnsembleåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šä»¥å¹³å‡ç²¾ç¡®åº¦0.64é¢†å…ˆï¼Œç´§éšå…¶åçš„æ˜¯é€»è¾‘å›å½’å’ŒXGBoostã€‚è¿™ç§æ›´é«˜çš„åˆ†ç¦»ä¼¼ä¹ä½¿EasyEnsembleèƒ½å¤Ÿåˆ©ç”¨å…¶å¯¹å¹³è¡¡çš„å…³æ³¨ï¼Œä»è€Œè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚å…¶ä»–é›†æˆæ–¹æ³•å¦‚UnderBaggingå’ŒOverBaggingè¡¨ç°å°šå¯ï¼Œä½†æœªè¶…è¶Šé¢†å…ˆè€…ã€‚'
- en: '**Thyroid sick dataset** (*Figure 4**.20*, left): In a real-world dataset focusing
    on thyroid sickness, XGBoost far outperforms all other methods with an average
    precision of 0.97\. Other ensemble methods such as bagging, OverBagging, and SMOTEBagging
    also score high, suggesting that ensembles are particularly effective for this
    dataset. Interestingly, boosting and RUSBoost do not keep pace, indicating that
    not all boosting variants are universally effective.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”²çŠ¶è…ºç–¾ç—…æ•°æ®é›†** (*å›¾4.20*ï¼Œå·¦ä¾§)ï¼šåœ¨ä¸€ä¸ªå…³æ³¨ç”²çŠ¶è…ºç–¾ç—…çš„çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸­ï¼ŒXGBoostçš„è¡¨ç°è¿œè¶…å…¶ä»–æ‰€æœ‰æ–¹æ³•ï¼Œå¹³å‡ç²¾ç¡®åº¦ä¸º0.97ã€‚å…¶ä»–é›†æˆæ–¹æ³•å¦‚Baggingã€OverBaggingå’ŒSMOTEBaggingä¹Ÿå¾—åˆ†å¾ˆé«˜ï¼Œè¡¨æ˜é›†æˆæ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºè¿™ä¸ªæ•°æ®é›†ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒBoostingå’ŒRUSBoostæ²¡æœ‰è·Ÿä¸Šæ­¥ä¼ï¼Œè¿™è¡¨æ˜å¹¶éæ‰€æœ‰Boostingå˜ä½“éƒ½æ™®éæœ‰æ•ˆã€‚'
- en: '**Abalone 19 dataset** (*Figure 4**.20*, right): For the Abalone 19 dataset,
    all methods perform relatively poorly, with XGBoost standing out with an average
    precision of 0.13\. EasyEnsemble comes in second with a score of 0.09, while traditional
    methods such as logistic regression and bagging lag behind. This could indicate
    that the dataset is particularly challenging for most methods, and specialized
    imbalanced techniques can only make marginal improvements.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Abalone 19æ•°æ®é›†** (*å›¾4.20*ï¼Œå³ä¾§)ï¼šå¯¹äºAbalone 19æ•°æ®é›†ï¼Œæ‰€æœ‰æ–¹æ³•çš„è¡¨ç°ç›¸å¯¹è¾ƒå·®ï¼ŒXGBoostä»¥å¹³å‡ç²¾ç¡®åº¦0.13è„±é¢–è€Œå‡ºã€‚EasyEnsembleä»¥0.09çš„å¾—åˆ†ä½å±…ç¬¬äºŒï¼Œè€Œä¼ ç»Ÿæ–¹æ³•å¦‚é€»è¾‘å›å½’å’ŒBaggingåˆ™è½åã€‚è¿™å¯èƒ½è¡¨æ˜è¯¥æ•°æ®é›†å¯¹å¤§å¤šæ•°æ–¹æ³•æ¥è¯´ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œä¸“é—¨çš„ä¸å¹³è¡¡æŠ€æœ¯åªèƒ½å¸¦æ¥å¾®å°çš„æ”¹è¿›ã€‚'
- en: 'Here are some overall insights:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€äº›æ€»ä½“è§è§£ï¼š
- en: Conventional methods such as XGBoost and logistic regression often provide strong
    baselines that are difficult to beat
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸¸è§„æ–¹æ³•å¦‚XGBoostå’Œé€»è¾‘å›å½’é€šå¸¸æä¾›å¼ºå¤§çš„åŸºçº¿ï¼Œéš¾ä»¥è¶…è¶Šã€‚
- en: The efficacy of specialized imbalanced learning techniques can vary significantly,
    depending on the dataset and its inherent complexities
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸“é—¨çš„ä¸å¹³è¡¡å­¦ä¹ æŠ€æœ¯çš„æœ‰æ•ˆæ€§å¯èƒ½å› æ•°æ®é›†åŠå…¶å›ºæœ‰çš„å¤æ‚æ€§è€Œæ˜¾è‘—ä¸åŒã€‚
- en: Ensemble methods generally perform well across various datasets, but their effectiveness
    can be context-dependent
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆæ–¹æ³•é€šå¸¸åœ¨å„ç§æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬çš„æœ‰æ•ˆæ€§å¯èƒ½å–å†³äºå…·ä½“æƒ…å¢ƒã€‚
- en: The choice of performance metric â€“ in this case, average precision â€“ can significantly
    influence the evaluation, making it crucial to consider multiple metrics for a
    comprehensive understanding
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ€§èƒ½æŒ‡æ ‡çš„é€‰æ‹©â€”â€”åœ¨æœ¬ä¾‹ä¸­ä¸ºå¹³å‡ç²¾ç¡®åº¦â€”â€”å¯ä»¥æ˜¾è‘—å½±å“è¯„ä¼°ï¼Œå› æ­¤è€ƒè™‘å¤šä¸ªæŒ‡æ ‡ä»¥è·å¾—å…¨é¢ç†è§£è‡³å…³é‡è¦ã€‚
- en: We hope that this chapter has shown how you can incorporate sampling techniques
    with ensemble methods to achieve improved results, especially when dealing with
    imbalanced data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›è¿™ä¸€ç« å·²ç»å±•ç¤ºäº†æ‚¨å¦‚ä½•å°†é‡‡æ ·æŠ€æœ¯ä¸é›†æˆæ–¹æ³•ç›¸ç»“åˆä»¥å®ç°æ›´å¥½çš„ç»“æœï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®æ—¶ã€‚
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: Ensemble methods in machine learning create strong classifiers by combining
    results from multiple weak classifiers using approaches such as bagging and boosting.
    However, these methods assume balanced data and may struggle with imbalanced datasets.
    Combining ensemble methods with sampling methods such as oversampling and undersampling
    leads to techniques such as UnderBagging, OverBagging, and SMOTEBagging, all of
    which can help address imbalanced data issues.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ ä¸­çš„é›†æˆæ–¹æ³•é€šè¿‡ç»“åˆå¤šä¸ªå¼±åˆ†ç±»å™¨çš„ç»“æœæ¥åˆ›å»ºå¼ºå¤§çš„åˆ†ç±»å™¨ï¼Œä¾‹å¦‚ä½¿ç”¨è¢‹è£…å’Œæå‡æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å‡è®¾æ•°æ®å¹³è¡¡ï¼Œå¯èƒ½éš¾ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†ã€‚å°†é›†æˆæ–¹æ³•ä¸è¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·ç­‰é‡‡æ ·æ–¹æ³•ç›¸ç»“åˆï¼Œå¯¼è‡´UnderBaggingã€OverBaggingå’ŒSMOTEBaggingç­‰æŠ€æœ¯ï¼Œæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥å¸®åŠ©è§£å†³ä¸å¹³è¡¡æ•°æ®é—®é¢˜ã€‚
- en: Ensembles of ensembles, such as EasyEnsemble, combine boosting and bagging techniques
    to create powerful classifiers for imbalanced datasets.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: EasyEnsembleç­‰é›†æˆé›†æˆï¼Œç»“åˆæå‡å’Œè¢‹è£…æŠ€æœ¯ï¼Œä¸ºä¸å¹³è¡¡æ•°æ®é›†åˆ›å»ºå¼ºå¤§çš„åˆ†ç±»å™¨ã€‚
- en: Ensemble-based imbalance learning techniques can be an excellent addition to
    your toolkit. The ones based on KNN, viz., SMOTEBoost, and RAMOBoost can be slow.
    However, the ensembles based on random undersampling and random oversampling are
    less costly. Also, boosting methods are found to sometimes work better than bagging
    methods in the case of imbalanced data. We can combine random sampling techniques
    with boosting to get better overall performance. As we emphasized previously,
    itâ€™s empirical, and we have to try to know what would work best for our data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºé›†æˆçš„ä¸å¹³è¡¡å­¦ä¹ æŠ€æœ¯å¯ä»¥æˆä¸ºæ‚¨å·¥å…·ç®±ä¸­çš„ä¼˜ç§€è¡¥å……ã€‚åŸºäºKNNçš„ï¼Œå¦‚SMOTEBoostå’ŒRAMOBoostå¯èƒ½è¾ƒæ…¢ã€‚ç„¶è€Œï¼ŒåŸºäºéšæœºæ¬ é‡‡æ ·å’Œéšæœºè¿‡é‡‡æ ·çš„é›†æˆæˆæœ¬è¾ƒä½ã€‚æ­¤å¤–ï¼Œå‘ç°æå‡æ–¹æ³•åœ¨å¤„ç†ä¸å¹³è¡¡æ•°æ®æ—¶æœ‰æ—¶æ¯”è¢‹è£…æ–¹æ³•æ›´æœ‰æ•ˆã€‚æˆ‘ä»¬å¯ä»¥å°†éšæœºé‡‡æ ·æŠ€æœ¯ä¸æå‡æ–¹æ³•ç›¸ç»“åˆï¼Œä»¥è·å¾—æ›´å¥½çš„æ•´ä½“æ€§èƒ½ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰å¼ºè°ƒçš„ï¼Œè¿™æ˜¯ç»éªŒæ€§çš„ï¼Œæˆ‘ä»¬å¿…é¡»å°è¯•äº†è§£ä»€ä¹ˆæœ€é€‚åˆæˆ‘ä»¬çš„æ•°æ®ã€‚
- en: In the next chapter, we will learn how to change the model to account for the
    imbalance in data and the various costs incurred by the model because of misclassifying
    the minority class examples.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•è°ƒæ•´æ¨¡å‹ä»¥è€ƒè™‘æ•°æ®çš„ä¸å¹³è¡¡ä»¥åŠæ¨¡å‹ç”±äºå¯¹å°‘æ•°ç±»æ ·æœ¬çš„é”™è¯¯åˆ†ç±»è€Œå¼•èµ·çš„å„ç§æˆæœ¬ã€‚
- en: Questions
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜
- en: Try using `RUSBoostClassifier` on the `abalone_19` dataset and compare the performance
    with other techniques from the previous chapters.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°è¯•åœ¨`abalone_19`æ•°æ®é›†ä¸Šä½¿ç”¨`RUSBoostClassifier`ï¼Œå¹¶å°†æ€§èƒ½ä¸å…¶ä»–ç« èŠ‚ä¸­çš„æŠ€æœ¯è¿›è¡Œæ¯”è¾ƒã€‚
- en: What is the difference between the `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
    classes in the `imbalanced-learn` library?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`åº“ä¸­çš„`BalancedRandomForestClassifier`å’Œ`BalancedBaggingClassifier`ç±»ä¹‹é—´çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ'
- en: References
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: 'L. Breiman, *Bagging predictors*, Mach Learn, vol. 24, no. 2, pp. 123â€“140,
    Aug. 1996, doi: 10.1007/BF00058655, [https://link.springer.com/content/pdf/10.1007/BF00058655.pdf](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf).'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'L. Breiman, *Bagging predictors*, Mach Learn, vol. 24, no. 2, pp. 123â€“140,
    Aug. 1996, doi: 10.1007/BF00058655, [https://link.springer.com/content/pdf/10.1007/BF00058655.pdf](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf).'
- en: '(The paper that introduced OverBagging, UnderBagging, and SMOTEBagging) S.
    Wang and X. Yao, *Diversity analysis on imbalanced data sets by using ensemble
    models*, in 2009 IEEE Symposium on Computational Intelligence and Data Mining,
    Nashville, TN, USA: IEEE, Mar. 2009, pp. 324â€“331\. doi: 10.1109/CIDM.2009.4938667,
    [https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf](https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf).'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ï¼ˆä»‹ç»äº†OverBaggingã€UnderBaggingå’ŒSMOTEBaggingçš„è®ºæ–‡ï¼‰S. Wangå’ŒX. Yao, *Diversity analysis
    on imbalanced data sets by using ensemble models*, in 2009 IEEE Symposium on Computational
    Intelligence and Data Mining, Nashville, TN, USA: IEEE, Mar. 2009, pp. 324â€“331\.
    doi: 10.1109/CIDM.2009.4938667, [https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf](https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf).'
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*å®æ—¶ç½‘ç«™äº‹ä»¶å‡çº§é¢„æµ‹* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)'
- en: 'L. Breiman, *Random Forests*, Machine Learning, vol. 45, no. 1, pp. 5â€“32, 2001,
    doi: 10.1023/A:1010933404324, [https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf).'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'L. Breiman, *Random Forests*, Machine Learning, vol. 45, no. 1, pp. 5â€“32, 2001,
    doi: 10.1023/A:1010933404324, [https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf).'
- en: '(The paper that introduced the RUSBoost algorithm) C. Seiffert, T. M. Khoshgoftaar,
    J. Van Hulse, and A. Napolitano, *RUSBoost: A Hybrid Approach to Alleviating Class
    Imbalance*, IEEE Trans. Syst., Man, Cybern. A, vol. 40, no. 1, pp. 185â€“197, Jan.
    2010, doi: 10.1109/TSMCA.2009.2029559, [https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf](https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf).'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ï¼ˆä»‹ç»äº†RUSBoostç®—æ³•çš„è®ºæ–‡ï¼‰C. Seiffert, T. M. Khoshgoftaar, J. Van Hulse, å’Œ A. Napolitano,
    *RUSBoost: ä¸€ç§ç¼“è§£ç±»åˆ«ä¸å¹³è¡¡çš„æ··åˆæ–¹æ³•*ï¼ŒIEEE Trans. Syst., Man, Cybern. A, ç¬¬40å·ï¼Œç¬¬1æœŸï¼Œç¬¬185â€“197é¡µï¼Œ2010å¹´1æœˆï¼Œdoi:
    10.1109/TSMCA.2009.2029559ï¼Œ[https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf](https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf).'
- en: '(The paper that introduced the SMOTEBoost algorithm) N. V. Chawla, A. Lazarevic,
    L. O. Hall, and K. W. Bowyer, *SMOTEBoost: Improving Prediction of the Minority
    Class in Boosting*, in Knowledge Discovery in Databases: PKDD 2003, N. LavraÄ,
    D. Gamberger, L. Todorovski, and H. Blockeel, Eds., in Lecture Notes in Computer
    Science, vol. 2838\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2003, pp.
    107â€“119\. doi: 10.1007/978-3-540-39804-2_12, [https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf](https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf).'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ï¼ˆä»‹ç»äº†SMOTEBoostç®—æ³•çš„è®ºæ–‡ï¼‰N. V. Chawla, A. Lazarevic, L. O. Hall, å’Œ K. W. Bowyer,
    *SMOTEBoost: åœ¨æå‡ä¸­æ”¹è¿›å°‘æ•°ç±»çš„é¢„æµ‹*ï¼Œåœ¨æ•°æ®åº“ä¸­çš„çŸ¥è¯†å‘ç°ï¼šPKDD 2003ï¼ŒN. LavraÄ, D. Gamberger, L. Todorovski,
    å’Œ H. Blockeel ç¼–è‘—ï¼Œè®¡ç®—æœºç§‘å­¦è®²åº§ç¬”è®°ï¼Œç¬¬2838å·ã€‚æŸæ—ï¼Œæµ·å¾·å ¡ï¼šSpringer Berlin Heidelbergï¼Œ2003å¹´ï¼Œç¬¬107â€“119é¡µã€‚doi:
    10.1007/978-3-540-39804-2_12ï¼Œ[https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf](https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf).'
- en: '(The paper that introduced RAMOBoost algorithm) Sheng Chen, Haibo He, and E.
    A. Garcia, *RAMOBoost: Ranked Minority Oversampling in Boosting*, IEEE Trans.
    Neural Netw., vol. 21, no. 10, pp. 1624â€“1642, Oct. 2010, doi: 10.1109/TNN.2010.2066988,
    [https://ieeexplore.ieee.org/abstract/document/5559472](https://ieeexplore.ieee.org/abstract/document/5559472).'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ï¼ˆä»‹ç»äº†RAMOBoostç®—æ³•çš„è®ºæ–‡ï¼‰Sheng Chen, Haibo He, å’Œ E. A. Garcia, *RAMOBoost: åœ¨æå‡ä¸­çš„æ’åºå°‘æ•°ç±»è¿‡é‡‡æ ·*ï¼ŒIEEE
    Trans. Neural Netw.ï¼Œç¬¬21å·ï¼Œç¬¬10æœŸï¼Œç¬¬1624â€“1642é¡µï¼Œ2010å¹´10æœˆï¼Œdoi: 10.1109/TNN.2010.2066988ï¼Œ[https://ieeexplore.ieee.org/abstract/document/5559472](https://ieeexplore.ieee.org/abstract/document/5559472).'
- en: '(The paper that introduced EasyEnsemble) Xu-Ying Liu, Jianxin Wu, and Zhi-Hua
    Zhou, *Exploratory Undersampling for Class-Imbalance Learning*, IEEE Trans. Syst.,
    Man, Cybern. B, vol. 39, no. 2, pp. 539â€“550, Apr. 2009, doi: 10.1109/TSMCB.2008.2007853,
    [http://129.211.169.156/publication/tsmcb09.pdf](http://129.211.169.156/publication/tsmcb09.pdf).'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ï¼ˆä»‹ç»äº†EasyEnsembleçš„è®ºæ–‡ï¼‰Xu-Ying Liu, Jianxin Wu, å’Œ Zhi-Hua Zhou, *ç”¨äºç±»åˆ«ä¸å¹³è¡¡å­¦ä¹ çš„æ¢ç´¢æ€§æ¬ é‡‡æ ·*ï¼ŒIEEE
    Trans. Syst., Man, Cybern. Bï¼Œç¬¬39å·ï¼Œç¬¬2æœŸï¼Œç¬¬539â€“550é¡µï¼Œ2009å¹´4æœˆï¼Œdoi: 10.1109/TSMCB.2008.2007853ï¼Œ[http://129.211.169.156/publication/tsmcb09.pdf](http://129.211.169.156/publication/tsmcb09.pdf).'
