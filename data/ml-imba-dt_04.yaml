- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Ensemble Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法
- en: Think of a top executive at a major company. They don’t make decisions on their
    own. Throughout the day, they need to make numerous critical decisions. How do
    they make those choices? Not alone, but by consulting their advisors.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下一家大型公司的顶级高管。他们不会独自做出决策。在一天中，他们需要做出许多关键决策。他们是如何做出这些选择的？不是独自一人，而是通过咨询他们的顾问。
- en: Let’s say that an executive consults five different advisors from different
    departments, each proposing a slightly different solution based on their expertise,
    skills, and domain knowledge. To make the most effective decision, the executive
    combines the insights and opinions of all five advisors to create a hybrid solution
    that incorporates the best parts of each proposal. This scenario illustrates the
    concept of **ensemble methods**, where multiple weak classifiers are combined
    to create a stronger and more accurate classifier. By combining different approaches,
    ensemble methods can often achieve better performance than relying on a single
    classifier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一位高管咨询了来自不同部门的五位不同顾问，每位顾问根据他们的专业知识、技能和领域知识提出略微不同的解决方案。为了做出最有效的决策，高管结合了五位顾问的见解和意见，创建了一个混合解决方案，其中包含了每个提案的最佳部分。这个场景说明了**集成方法**的概念，其中多个弱分类器被组合起来创建一个更强、更准确的分类器。通过结合不同的方法，集成方法通常可以实现比依赖单个分类器更好的性能。
- en: 'We can create a strong model through ensemble methods by combining the results
    from multiple weak classifiers. These weak classifiers, such as simplified decision
    trees, neural networks, or support vector machines, perform slightly better than
    random guessing. In contrast, a strong model, created by ensembling these weak
    classifiers, performs significantly better than random guessing. The weak classifiers
    can be fed different sources of information. There are two general approaches
    for building ensembles of models: bagging and boosting.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过结合多个弱分类器的结果来创建一个强大的模型。这些弱分类器，如简化的决策树、神经网络或支持向量机，其表现略好于随机猜测。相比之下，通过集成这些弱分类器创建的强大模型，其表现显著优于随机猜测。弱分类器可以接受不同来源的信息。构建模型集成的两种通用方法：bagging和boosting。
- en: The problem with traditional ensemble methods is that they use classifiers that
    assume balanced data. Thus, they may not work very well with imbalanced datasets.
    So, we combine the popular machine learning ensembling methods with the techniques
    for dealing with imbalanced data that we studied in previous chapters. We are
    going to discuss those combinations in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 传统集成方法的问题在于它们使用假设数据平衡的分类器。因此，它们可能不适合处理不平衡的数据集。因此，我们将流行的机器学习集成方法与我们之前章节中研究的不平衡数据处理技术相结合。我们将在本章讨论这些组合。
- en: 'Here are the topics that will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Bagging techniques for imbalanced data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据的Bagging技术
- en: Boosting techniques for imbalanced data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据的Boosting技术
- en: Ensemble of ensembles
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成集成
- en: Model performance comparison
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型性能比较
- en: 'In *Figure 4**.1*, we have categorized the various ensembling techniques that
    we will cover in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4*.1中，我们将本章将要涵盖的各种集成技术进行了分类：
- en: '![](img/B17259_04_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_04_01.jpg)'
- en: Figure 4.1 – Overview of ensembling techniques
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 集成技术概述
- en: By the end of this chapter, you will understand how to adapt ensemble models
    such as bagging and boosting to account for class imbalances in datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解如何将诸如bagging和boosting之类的集成模型应用于数据集中类不平衡的问题。
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The Python notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04).
    As usual, you can open the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapter’s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的Python笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter04)。像往常一样，你可以通过点击本章笔记本顶部的**在Colab中打开**图标或通过使用笔记本的GitHub
    URL在[https://colab.research.google.com](https://colab.research.google.com)启动它来打开GitHub笔记本。
- en: 'In this chapter, we will continue to use a synthetic dataset generated using
    the `make_classification` API, just as we did in the previous chapters. Toward
    the end of this chapter, we will test the methods we learned in this chapter on
    some real datasets. Our full dataset contains 90,000 examples with a 1:99 imbalance
    ratio. Here is what the training dataset looks like:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续使用使用`make_classification` API生成的合成数据集，就像我们在前几章中所做的那样。在本章的末尾，我们将对我们在本章中学到的方法在几个真实数据集上进行测试。我们的完整数据集包含90,000个示例，不平衡比率为1:99。以下是训练数据集的外观：
- en: '![](img/B17259_04_02.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_04_02.jpg)'
- en: Figure 4.2 – Plot of a dataset with a 1:99 imbalance ratio
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 不平衡比率为1:99的数据集的绘图
- en: With our imbalanced dataset ready to use, let’s look at the first ensembling
    method, called bagging.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的失衡数据集已经准备好使用，让我们看看第一种集成方法，称为Bagging。
- en: Bagging techniques for imbalanced data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据的Bagging技术
- en: Imagine a business executive with thousands of confidential files regarding
    an important merger or acquisition. The analysts assigned to the case don’t have
    enough time to review all the files. Each can randomly select some files from
    the set and start reviewing them. Later, they can combine their insights in a
    meeting to draw conclusions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个商业高管，他拥有数千份关于重要并购的机密文件。分配给这个案例的分析师没有足够的时间来审查所有文件。每个人都可以从集合中随机选择一些文件并开始审查。稍后，他们可以在会议上结合他们的见解来得出结论。
- en: This scenario is a metaphor for a process in machine learning called bagging
    [1], which is short for **bootstrap aggregating**. In bagging, much like the analysts
    in the previous scenario, we create several subsets of the original dataset, train
    a weak learner on each subset, and then aggregate their predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种场景是机器学习中一个称为Bagging的过程的隐喻[1]，它是**自助聚合**的缩写。在Bagging中，就像前一个场景中的分析师一样，我们创建原始数据集的几个子集，在每个子集上训练一个弱学习器，然后聚合它们的预测。
- en: 'Why use weak learners instead of strong learners? The rationale applies to
    both bagging and boosting methods (discussed later in this chapter). There are
    several reasons:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用弱学习器而不是强学习器？这个理由适用于Bagging和Boosting方法（本章后面将讨论）。有几个原因：
- en: '**Speed**: Weak learners are computationally efficient and inexpensive to execute.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：弱学习器在计算上效率高且成本低。'
- en: '**Diversity**: Weak learners are more likely to make different types of errors,
    which is advantageous when combining their predictions. Using strong learners
    could result in them all making the same type of error, leading to less effective
    ensembles.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：弱学习器更有可能犯不同类型的错误，这在结合它们的预测时是有利的。使用强学习器可能导致它们都犯同一种类型的错误，从而导致集成效果较差。'
- en: '**Overfitting**: As a corollary to the previous point, the diversity in errors
    helps reduce the risk of overfitting in the ensemble.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：作为前一点的推论，错误的多样性有助于降低集成中过拟合的风险。'
- en: '**Interpretability**: While the ensemble as a whole may not be easily interpretable,
    its individual components – often simpler models – are easier to understand and
    interpret.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：虽然整个集成可能不容易解释，但其个别组件——通常是更简单的模型——更容易理解和解释。'
- en: Now, back to bagging. The first step of the algorithm is called **bootstrapping**.
    In this step, we make several subsets or smaller groups of data by randomly picking
    items from the main data. The data is picked with the possibility of picking the
    same item more than once (this process is called “random sampling with replacement”),
    so these smaller groups may have some items in common. Then, we train our classifiers
    on each of these smaller groups.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到Bagging。算法的第一步称为**自助抽样**。在这个步骤中，我们通过从主数据中随机选择项目来制作几个子集或较小的数据组。数据的选择有可能会选择同一个项目多次（这个过程称为“有放回的随机抽样”），所以这些较小的组可能有一些共同的项目。然后，我们在这些较小的组上训练我们的分类器。
- en: The second step is called **aggregating**. The test sample is passed to each
    classifier at the time of prediction. After this, we take the average or majority
    prediction as the real answer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步称为**聚合**。在预测时，将测试样本传递给每个分类器。之后，我们取平均或多数预测作为真实答案。
- en: 'As shown in *Figure 4**.3*, the dataset is first sampled with replacement into
    three subsets. Then, separate classifiers are trained on each of the subsets.
    Finally, the results of the classifiers are combined at the time of prediction:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图4**.3所示，数据集首先通过有放回抽样分成三个子集。然后，在每个子集上分别训练单独的分类器。最后，在预测时将分类器的结果合并：
- en: '![](img/B17259_04_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_04_03.jpg)'
- en: Figure 4.3 – Demonstrating how bagging works
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 展示 bagging 的工作原理
- en: '*Figure 4**.4* summarizes the bagging algorithm in a pseudocode format:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.4* 以伪代码格式总结了 bagging 算法：'
- en: '![](img/B17259_04_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_04.jpg)'
- en: Figure 4.4 – Bagging pseudocode
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – Bagging 的伪代码
- en: 'We’ll train a bagging classifier model from `sklearn` on the dataset we created
    previously. Since it’s possible to provide a base estimator to `BaggingClassifier`,
    we’ll use `DecisionTreeClassifier` with the maximum depth of the trees being `6`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在之前创建的数据集上训练一个来自 `sklearn` 的 bagging 分类器模型。由于可以向 `BaggingClassifier` 提供一个基估计器，我们将使用最大树深为
    `6` 的 `DecisionTreeClassifier`：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s plot the decision boundary:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制决策边界：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You may refer to the definition of `plot_decision_boundary()` in the corresponding
    notebook on GitHub. We use the `DecisionBoundaryDisplay` API from the `sklearn.inspection`
    module to plot the decision boundary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考 GitHub 上相应笔记本中 `plot_decision_boundary()` 的定义。我们使用 `sklearn.inspection`
    模块中的 `DecisionBoundaryDisplay` API 来绘制决策边界。
- en: '*Figure 4**.5* shows the learned decision boundary on the training data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.5* 展示了在训练数据上的学习决策边界：'
- en: '![](img/B17259_04_05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_05.jpg)'
- en: Figure 4.5 – The decision boundary of BaggingClassifier on the training data
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – BaggingClassifier 在训练数据上的决策边界
- en: 'Let’s also note the baseline metric of average precision when using this model
    on our test set:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也注意使用此模型在测试集上的基线指标平均精度：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Figure 4**.6* shows the resulting PR curve:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.6* 展示了得到的 PR 曲线：'
- en: '![](img/B17259_04_06.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_06.jpg)'
- en: Figure 4.6 – Precision-recall curve of BaggingClassifier on the test data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – BaggingClassifier 在测试数据上的精确度-召回率曲线
- en: 'Here are some other metrics:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些其他指标：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this chapter, we will also consider the **F2 score** (Fbeta-score with beta=2.0),
    which proportionally combines precision and recall, giving more weight to recall
    and less weight to precision.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还将考虑 **F2 分数**（Fbeta 分数，beta=2.0），它按比例结合精确度和召回率，给予召回率更高的权重，而给予精确度较低的权重。
- en: So, what problems may we face when using `BaggingClassifier` on an imbalanced
    dataset? An obvious thing could be that when bootstrapping, some subsets on which
    base classifiers get trained may have very few minority class examples or none
    at all. This would mean that each of the individual base classifiers is going
    to perform poorly on the minority class, and combining their performance would
    still be poor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当在不平衡数据集上使用 `BaggingClassifier` 时，我们可能会遇到什么问题？一个明显的事情可能是，在引导过程中，一些用于训练基分类器的子集可能只有很少的少数类示例，甚至没有。这意味着每个基分类器在少数类上的表现都会很差，而它们的组合表现仍然会很差。
- en: We can combine undersampling techniques with bagging (one such method is UnderBagging)
    or oversampling techniques with bagging (one such method is OverBagging) to get
    better results. We will discuss such techniques next.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将下采样技术与 bagging（例如 UnderBagging）或过采样技术与 bagging（例如 OverBagging）结合起来，以获得更好的结果。我们将在下一节讨论这些技术。
- en: UnderBagging
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UnderBagging
- en: The UnderBagging [2] technique uses random undersampling at the time of bootstrapping
    (or selection of subsets). We choose the whole set of the minority class examples
    for each classifier and bootstrap with replacement as many examples from the majority
    class as there are minority class examples. The aggregation step remains the same
    as in bagging. We can choose any classifier, say a decision tree, for training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: UnderBagging [2] 技术在引导（或子集选择）时使用随机下采样。我们为每个分类器选择少数类示例的整个集合，并用与少数类示例数量相同的多数类示例进行带替换的引导。聚合步骤与
    bagging 相同。我们可以选择任何分类器，例如决策树，进行训练。
- en: There are variants of UnderBagging where resampling with replacement of the
    minority class can also be applied to obtain more diverse ensembles.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: UnderBagging 有变体，其中可以对少数类进行带替换的重采样，以获得更多样化的集成。
- en: 'The flowchart in *Figure 4**.7* represents the main steps in the UnderBagging
    algorithm with three subsets of data. It involves creating multiple subsets of
    data, performing random undersampling for the majority class in each subset, training
    classifiers on each subset, and finally combining the predictions of the classifiers:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.7* 中的流程图代表了 UnderBagging 算法在三个数据子集中的主要步骤。它包括创建多个数据子集，对每个子集中的多数类进行随机下采样，在每个子集上训练分类器，最后结合分类器的预测：'
- en: '![](img/B17259_04_07.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_07.jpg)'
- en: Figure 4.7 – Demonstrating how the UnderBagging algorithm works
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 展示 UnderBagging 算法的工作原理
- en: 'The `imbalanced-learn` library provides an implementation for `BalancedBaggingClassifier`.
    By default, this classifier uses a decision tree as the base classifier and `RandomUnderSampler`
    as the sampler via the `sampler` parameter. *Figure 4**.8* shows the decision
    boundary of the trained UnderBagging model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn` 库为 `BalancedBaggingClassifier` 提供了实现。默认情况下，此分类器使用决策树作为基分类器，并通过
    `sampler` 参数使用 `RandomUnderSampler` 作为采样器。*图4.8* 展示了训练好的UnderBagging模型的决策边界：'
- en: '![](img/B17259_04_08.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_08.jpg)'
- en: Figure 4.8 – The decision boundary of the UnderBagging classifier on the training
    data
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 在训练数据上UnderBagging分类器的决策边界
- en: 🚀 Bagging classifier in production at Microsoft
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 微软生产环境中的Bagging分类器
- en: In a real-world application at Microsoft [3], the team faced a significant challenge
    in forecasting Live Site Incident escalations (previously mentioned in [*Chapter
    2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling Methods*). The dataset was
    highly imbalanced, making it difficult for standard classifiers to perform well.
    To tackle this issue, Microsoft employed ensemble methods, specifically `BalancedBaggingClassifier`
    from the `imbalanced-learn` library. They used UnderBagging, where each bootstrap
    sample is randomly undersampled to get a balanced class distribution. As we have
    just discussed, UnderBagging uses all minority class samples and a random selection
    of majority class samples to train the model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在微软的一个实际应用中 [3]，团队在预测Live Site Incident升级（如[*第2章*](B17259_02.xhtml#_idTextAnchor042)，*过采样方法*）时面临重大挑战。数据集高度不平衡，使得标准分类器难以表现良好。为了解决这个问题，微软采用了集成方法，特别是来自
    `imbalanced-learn` 库的 `BalancedBaggingClassifier`。他们使用了UnderBagging，其中每个引导样本都是随机欠采样的，以获得平衡的类别分布。正如我们刚才讨论的，UnderBagging使用所有少数类样本和多数类样本的随机选择来训练模型。
- en: Bagged classification delivered the best results during their evaluation and
    also proved to be more consistent after they tracked it over a few months. They
    were able to significantly improve their forecasting accuracy for incident escalations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的评估中，Bagged分类提供了最佳结果，并且在经过几个月的跟踪后也证明更加一致。他们能够显著提高对事件升级的预测准确性。
- en: OverBagging
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OverBagging
- en: Instead of random undersampling of the majority class samples, the minority
    class is oversampled (with replacement) at the time of bootstrapping. This method
    is called OverBagging [2]. As a variant, both minority and majority class examples
    can be resampled with replacements to achieve an equal number of majority and
    minority class examples.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在引导过程中，不是对多数类样本进行随机欠采样，而是对少数类进行过采样（带替换）。这种方法称为OverBagging [2]。作为一种变体，多数类和少数类的样本都可以进行带替换的重采样，以达到多数类和少数类样本数量相等。
- en: 'The flowchart in *Figure 4**.9* represents the main steps in the OverBagging
    algorithm with three subsets of data. It involves creating multiple subsets of
    data, performing random oversampling for the minority class in each subset, training
    classifiers on each subset, and finally combining the predictions of the classifiers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.9* 中的流程图展示了OverBagging算法在三个数据子集上的主要步骤。它包括创建多个数据子集，对每个子集中的少数类进行随机过采样，在每个子集上训练分类器，并最终结合分类器的预测：'
- en: '![](img/B17259_04_09.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_09.jpg)'
- en: Figure 4.9 – Demonstrating how the OverBagging algorithm works
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 展示OverBagging算法的工作原理
- en: For OverBagging, we can use the same `BalancedBaggingClassifier` with `RandomOverSampler`
    in the `sampler` parameter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于OverBagging，我们可以在 `sampler` 参数中使用与 `BalancedBaggingClassifier` 相同的 `RandomOverSampler`。
- en: 'We will see the following decision boundary:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下决策边界：
- en: '![](img/B17259_04_10.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_10.jpg)'
- en: Figure 4.10 – The decision boundary of the OverBagging classifier on the training
    data
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 在训练数据上OverBagging分类器的决策边界
- en: We will compare the performance metrics of these techniques after discussing
    the various bagging techniques.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了各种Bagging技术之后，我们将比较这些技术的性能指标。
- en: SMOTEBagging
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTEBagging
- en: Can we use SMOTE at the time of bootstrapping instead of random oversampling
    of minority class examples? The answer is yes. The majority class will be bootstrapped
    with replacement, and the minority class will be sampled using SMOTE until a balancing
    ratio is reached.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否在引导过程中使用SMOTE代替对少数类样本的随机过采样？答案是肯定的。多数类将进行带替换的引导，而少数类将使用SMOTE进行采样，直到达到平衡比率。
- en: The pseudocode for SMOTEBagging [2] is very similar to that for OverBagging,
    with the key difference being the use of the SMOTE algorithm instead of random
    oversampling to augment the minority class data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTEBagging [2] 的伪代码与 OverBagging 非常相似，关键区别在于使用 SMOTE 算法而不是随机过采样来增强少数类数据。
- en: 'Similar to OverBagging, we can implement `SMOTEBagging` using the `BalancedBagging``     Classifier` API with SMOTE as the `sampler` parameter.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与过袋装类似，我们可以使用带有 SMOTE 作为 `sampler` 参数的 `BalancedBagging Classifier` API 实现 `SMOTEBagging`。
- en: 'The decision boundary is not very different from OverBagging:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界与过袋装没有很大区别：
- en: '![](img/B17259_04_11.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_11.jpg)'
- en: Figure 4.11 – The decision boundary of the SMOTEBagging classifier on the training
    data
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – SMOTEBagging 分类器在训练数据上的决策边界
- en: A note about random forest and how it is related to bagging
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机森林及其与袋装的关系的说明
- en: Random forest [4] is another model that is based on the concept of bagging.
    The way the `RandomForestClassifier` and `BaggingClassifier` models from `sklearn`
    differ from each other is the fact that `RandomForestClassifier` considers a random
    subset of features while trying to decide the feature on which to split the nodes
    in the decision tree, while `BaggingClassifier` takes all the features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林 [4] 是另一个基于袋装概念的模型。`sklearn` 中的 `RandomForestClassifier` 和 `BaggingClassifier`
    模型之间的区别在于，`RandomForestClassifier` 在尝试决定决策树中节点分裂的特征时，考虑了特征的一个随机子集，而 `BaggingClassifier`
    则采用所有特征。
- en: '*Table 4.1* highlights the difference between random forest and bagging classifiers:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 4.1* 突出了随机森林和袋装分类器之间的差异：'
- en: '|  | **RandomForestClassifier** | **BaggingClassifier** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | **RandomForestClassifier** | **BaggingClassifier** |'
- en: '| --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Base classifier | Decision trees | Any classifier. |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 基本分类器 | 决策树 | 任何分类器。 |'
- en: '| Bootstrap sampling | Yes | Yes. |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 自举采样 | 是 | 是。 |'
- en: '| Take a subset of features | Yes (at each node) | No, by default. We can use
    the `max_features` hyperparameter to take subsets of features. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 选取特征子集 | 是（在每个节点） | 否，默认情况下。我们可以使用 `max_features` 超参数来选取特征子集。 |'
- en: '| Works best with? | Any tabular data, but it shines with large feature sets
    | Any tabular data, but it’s best when the base classifier is carefully chosen.
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 最适合的数据类型 | 任何表格数据，但在大型特征集上表现最佳 | 任何表格数据，但最好在仔细选择基本分类器时使用。 |'
- en: '| Handles missing values and outliers | Yes, inherently | Depends on the base
    classifier. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 处理缺失值和异常值 | 是，固有 | 取决于基本分类器。 |'
- en: Table 4.1 – RandomForestClassifier versus BaggingClassifier
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 – RandomForestClassifier 与 BaggingClassifier 对比
- en: The `imbalanced-learn` library provides the `BalancedRandomForestClassifier`
    class to tackle the imbalanced datasets where each of the bootstraps is undersampled
    before the individual decision trees are trained. As an exercise, we encourage
    you to learn about `BalancedRandomForestClassifier`. See how it relates to the
    other techniques we just discussed. Also, try out the various sampling strategies
    and explore the parameters this class offers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn` 库提供了 `BalancedRandomForestClassifier` 类来处理不平衡数据集，其中在训练单个决策树之前，每个自举样本都是欠采样的。作为练习，我们鼓励你了解
    `BalancedRandomForestClassifier`。看看它如何与我们刚才讨论的其他技术相关。还可以尝试各种采样策略，并探索此类提供的参数。'
- en: Comparative performance of bagging methods
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 袋装方法的比较性能
- en: 'Let’s compare the performance of various bagging methods using the same dataset
    we’ve employed so far. We’ll use the decision tree as a baseline and evaluate
    different techniques across several performance metrics. The highest values for
    each metric across all techniques are highlighted in bold:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用迄今为止使用的相同数据集比较各种袋装方法的性能。我们将使用决策树作为基线，并在多个性能指标上评估不同的技术。所有技术中每个指标的最高值都以粗体突出显示：
- en: '| **TECHNIQUE** | **F2** | **PRECISION** | **RECALL** | **AVERAGE PRECISION**
    | **AUC-ROC** |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **F2** | **精确度** | **召回率** | **平均精确度** | **AUC-ROC** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SMOTEBagging | **0.928** | 0.754 | 0.985 | 0.977 | **1.000** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SMOTEBagging | **0.928** | 0.754 | 0.985 | 0.977 | **1.000** |'
- en: '| OverBagging | 0.888 | 0.612 | **1.000** | 0.976 | **1.000** |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 过袋装 | 0.888 | 0.612 | **1.000** | 0.976 | **1.000** |'
- en: '| UnderBagging | 0.875 | 0.609 | 0.981 | 0.885 | 0.999 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 下袋装 | 0.875 | 0.609 | 0.981 | 0.885 | 0.999 |'
- en: '| Bagging | 0.891 | 0.967 | 0.874 | 0.969 | **1.000** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 袋装 | 0.891 | 0.967 | 0.874 | 0.969 | **1.000** |'
- en: '| Balanced random forest | 0.756 | 0.387 | 0.993 | 0.909 | 0.999 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 平衡随机森林 | 0.756 | 0.387 | 0.993 | 0.909 | 0.999 |'
- en: '| Random forest | 0.889 | **0.975** | 0.870 | **0.979** | **1.000** |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 0.889 | **0.975** | 0.870 | **0.979** | **1.000** |'
- en: '| Decision tree | 0.893 | 0.960 | 0.878 | 0.930 | 0.981 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.893 | 0.960 | 0.878 | 0.930 | 0.981 |'
- en: Table 4.2 – Performance comparison of various bagging techniques
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2 – 各种袋装技术的性能比较
- en: 'Here are some conclusions we can draw from *Table 4.2*:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们可以从*表4.2*中得出的结论：
- en: For maximizing the F2 score, **SMOTEBagging** did the best
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了最大化F2分数，**SMOTEBagging**表现最佳
- en: For high precision, **bagging** and **random forest** performed exceptionally
    well
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高精确度，**袋装法**和**随机森林**表现异常出色
- en: For high recall, **OverBagging** and **balanced random forest** are strong choices
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高召回率，**OverBagging**和**平衡随机森林**是强有力的选择
- en: For general performance across all metrics, **SMOTEBagging** and **bagging**
    proved to be solid options
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有指标的综合性能，**SMOTEBagging**和**袋装法**被证明是稳固的选项
- en: In conclusion, although ensemble approaches such as bagging and random forest
    establish robust benchmarks that are challenging to outperform, incorporating
    imbalanced learning strategies such as SMOTEBagging can lead to notable gains.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，尽管如袋装法和随机森林这样的集成方法建立了难以超越的稳健基准，但结合如SMOTEBagging这样的不平衡学习策略可以带来显著的收益。
- en: This concludes our discussion of bagging techniques. If bagging is the wisdom
    of the crowd, boosting is the master sculptor, refining the previous art with
    each stroke. We’ll try to understand how boosting works in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对袋装技术的讨论。如果袋装法是群众的智慧，那么提升法就是大师级的雕塑家，通过每一笔都精炼着先前的艺术。我们将在下一节尝试理解提升法是如何工作的。
- en: Boosting techniques for imbalanced data
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据的提升技术
- en: 'Imagine two friends doing group study to solve their mathematics assignment.
    The first student is strong in most topics but weak in two topics: complex numbers
    and triangles. So, the first student asks the second student to spend more time
    on these two topics. Then, while solving the assignments, they combine their answers.
    Since the first student knows most of the topics well, they decided to give more
    weight to his answers to the assignment questions. What these two students are
    doing is the key idea behind boosting.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 想象两个朋友一起做小组学习来解决他们的数学作业。第一个学生在大多数主题上都很强，但在两个主题上较弱：复数和三角形。因此，第一个学生要求第二个学生在这两个主题上花更多的时间。然后，在解决作业的过程中，他们结合了他们的答案。由于第一个学生大多数主题都很熟悉，他们决定在作业问题的答案中给予他的答案更多的权重。这两个学生所做的是提升法的核心思想。
- en: In bagging, we noticed that we could train all the classifiers in parallel.
    These classifiers are trained on a subset of the data, and all of them have an
    equal say at the time of prediction.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装法中，我们注意到我们可以并行训练所有分类器。这些分类器是在数据的一个子集上训练的，并且在预测时它们都有平等的发言权。
- en: In boosting, the classifiers are trained one after the other. While every classifier
    learns from the whole data, points in the dataset are assigned different weights
    based on their difficulty of classification. Classifiers are also assigned weights
    that tell us about their predictive power. While predicting new data, the weighted
    sum of the classifiers is used.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升法中，分类器是一个接一个地训练的。虽然每个分类器都从整个数据中学习，但根据数据集中点的分类难度，这些点被分配了不同的权重。分类器也被分配了权重，这些权重告诉我们它们的预测能力。在预测新数据时，使用分类器的加权总和。
- en: 'Boosting begins by training the first classifier on the whole dataset, with
    each data point assigned the same weight. In the second iteration, the data points
    that were misclassified in the first iteration are given more weight, and a second
    classifier is trained with these new weights. A weight is also assigned to the
    classifiers themselves based on their overall performance. This process continues
    through multiple iterations with different classifiers. *Figure 4**.12* illustrates
    this concept for a two-class dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法首先在全部数据集上训练第一个分类器，每个数据点分配相同的权重。在第二次迭代中，第一次迭代中被误分类的数据点被赋予更多的权重，并使用这些新权重训练第二个分类器。同时，也会根据分类器的整体性能给分类器本身分配权重。这个过程通过多个迭代和不同的分类器继续进行。*图4.12*展示了对于双分类数据集的此概念：
- en: '![](img/B17259_04_12.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_04_12.jpg)'
- en: 'Figure 4.12 – Boosting idea: (left) the decision boundary from the first classifier;
    (middle) the weights of misclassified data points are bumped up for the second
    classifier; (right) the decision boundary from the second classifier'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – 提升法思想：（左）第一个分类器的决策边界；（中）第二分类器对误分类数据点的权重提升；（右）第二个分类器的决策边界
- en: The kind of boosting we just described is called **AdaBoost**. There is another
    category of boosting algorithms called gradient boosting, where the main focus
    is on minimizing the residuals (the difference between the actual value and predicted
    output value) of the previous model, trying to correct the previous model’s mistakes.
    There are several popular gradient boosting implementations, such as **XGBoost**,
    **LightGBM**, and **CatBoost**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才描述的这种提升方法称为**AdaBoost**。还有一种提升算法的类别称为梯度提升，其主要重点是最小化先前模型的残差（实际值与预测输出值之间的差异），试图纠正先前模型的错误。有几个流行的梯度提升实现，如**XGBoost**、**LightGBM**和**CatBoost**。
- en: In this chapter, we will mostly focus on AdaBoost and modify it to account for
    data imbalance. However, swapping AdaBoost with XGBoost, for example, shouldn’t
    be too difficult.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要关注AdaBoost并将其修改为考虑数据不平衡。然而，将AdaBoost与XGBoost等替换不应太困难。
- en: AdaBoost
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost
- en: 'AdaBoost, short for adaptive boosting, is one of the earliest boosting methods
    based on decision trees. Decision trees are classifiers that are easy to ensemble
    together:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost，即自适应提升，是基于决策树的最早提升方法之一。决策树是易于组合在一起的分类器：
- en: '![](img/B17259_04_13.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_13.jpg)'
- en: Figure 4.13 – AdaBoost pseudocode
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – AdaBoost伪代码
- en: 'The following code shows how to import the classifier from the `sklearn` library
    and train it on the data:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何从`sklearn`库中导入分类器并在数据上训练它：
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s plot what the decision boundary looks like after the model gets trained
    on the data:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制模型在数据上训练后的决策边界：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 4**.14* shows the decision boundary of the model on the training data:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4**.14*显示了模型在训练数据上的决策边界：'
- en: '![](img/B17259_04_14.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_14.jpg)'
- en: Figure 4.14 – The decision boundary of AdaBoostClassifier on the training data
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – AdaBoostClassifier在训练数据上的决策边界
- en: We can make oversampling and undersampling an integral part of the boosting
    algorithm, similar to how we did for the bagging algorithm. We will discuss that
    next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将过采样和欠采样作为提升算法的组成部分，类似于我们对bagging算法所做的那样。我们将在下一节讨论这一点。
- en: RUSBoost, SMOTEBoost, and RAMOBoost
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RUSBoost、SMOTEBoost和RAMOBoost
- en: 'As you might have guessed, we can combine AdaBoost with resampling techniques.
    Here is the main idea: at each boosting iteration, before training a classifier
    on the incorrect examples from the previous iteration, we sample the data (via
    some undersampling or oversampling variant). Here’s the general pseudocode:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所猜测，我们可以将AdaBoost与重采样技术相结合。以下是主要思路：在每次提升迭代中，在训练分类器之前，对前一次迭代中的错误示例进行数据采样（通过某些欠采样或过采样变体）。以下是通用伪代码：
- en: '**Input**: The training data and some decision tree classifiers.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入**：训练数据和一些决策树分类器。'
- en: '**Output**: An aggregated classifier.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出**：一个聚合分类器。'
- en: Initialize the equal weights for all the samples of the training data.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化训练数据所有样本的相等权重。
- en: 'Repeat this for each decision tree classifier:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个决策树分类器重复此操作：
- en: 'Resample the data using a data sampling method:'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据采样方法重新采样数据：
- en: If the sampling method used is **Random UnderSampling** (**RUS**), the method
    is called **RUSBoost** [5].
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用的采样方法是**随机欠采样**（**RUS**），则该方法称为**RUSBoost** [5]。
- en: If the sampling method used is SMOTE, the method is called **SMOTEBoost** [6].
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用的采样方法是SMOTE，则该方法称为**SMOTEBoost** [6]。
- en: In **RAMOBoost** (short for **Ranked Minority Oversampling in Boosting** [7]),
    oversampling of the minority class is done based on the weight of the minority
    class examples. If the weight of an example is more (because the model didn’t
    do well on that example in the previous iteration), then it’s oversampled more,
    and vice versa.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**RAMOBoost**（即**Boosting中的排序少数过采样** [7]）中，少数类的过采样是基于少数类示例的权重进行的。如果一个示例的权重更高（因为模型在前一次迭代中对该示例的表现不佳），则对其进行更多的过采样，反之亦然。
- en: Train a classifier on the resampled data, giving more importance to samples
    with higher weights based on previous iterations.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在重采样数据上训练分类器，根据先前迭代中样本的权重给予更高的重视。
- en: Compute the error for the classifier on the given data by comparing its predictions
    with the actual outputs.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过比较其预测与实际输出，计算给定数据上分类器的错误。
- en: Consider all the wrongly classified examples for the next iteration. Increase
    the weights of such wrongly classified examples.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑下一次迭代中所有被错误分类的示例。增加这些错误分类示例的权重。
- en: Combine all the decision tree classifiers into a final classifier, where the
    classifiers with smaller error values on the training data have a larger say in
    the final prediction.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有决策树分类器组合成一个最终分类器，其中在训练数据上具有较小错误值的分类器在最终预测中具有更大的发言权。
- en: 'In this pseudocode, *Step 4 (I)* is the only extra step we have added compared
    to the AdaBoost algorithm. Let’s discuss the pros and cons of these techniques:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个伪代码中，*步骤4（I）*是我们相对于AdaBoost算法所添加的唯一额外步骤。让我们讨论这些技术的优缺点：
- en: In RUSBoost, as the data is reduced, we tend to have a faster training time.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在RUSBoost中，随着数据的减少，我们倾向于有更快的训练时间。
- en: SMOTEBoost produces synthetic samples from the minority class. Thus, it adds
    diversity to the data and may improve the classifier’s accuracy. However, it would
    increase the time to train and may not be scalable to very large datasets.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTEBoost从少数类生成合成样本。因此，它增加了数据的多样性，并可能提高分类器的准确性。然而，它会增加训练时间，并且可能无法扩展到非常大的数据集。
- en: RAMOBoost gives preference to the samples near the class boundaries. This can
    improve performance in some cases. However, like SMOTEBoost, this method may increase
    the training time and cost and may cause overfitting of the final model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAMOBoost优先考虑靠近类别边界的样本。这在某些情况下可以提高性能。然而，类似于SMOTEBoost，这种方法可能会增加训练时间和成本，并可能导致最终模型的过拟合。
- en: 'The `imbalanced-learn` library provides the implementation for `RUSBoostClassifier`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`库提供了`RUSBoostClassifier`的实现：'
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s examine the decision boundary of the trained model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查训练模型的决策边界：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting plot is shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下所示：
- en: '![](img/B17259_04_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_15.jpg)'
- en: Figure 4.15 – The decision boundary of RUSBoostClassifier on training data
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – RUSBoostClassifier在训练数据上的决策边界
- en: The `imbalanced-learn` library doesn’t have the implementations of RAMOBoost
    and SMOTEBoost yet (as of version 0.11.0). You can check the open source repository
    at [https://github.com/dialnd/imbalanced-algorithms](https://github.com/dialnd/imbalanced-algorithms)
    for reference implementations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`库尚未实现RAMOBoost和SMOTEBoost（截至版本0.11.0）。您可以在[https://github.com/dialnd/imbalanced-algorithms](https://github.com/dialnd/imbalanced-algorithms)的开放源代码存储库中查看参考实现。'
- en: Can we create multiple subsets of the majority class, train an ensemble from
    each of these subsets, and combine all weak classifiers in these ensembles into
    a final output? This approach will be explored in the next section, where we will
    utilize the ensemble of ensembles technique.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否为多数类创建多个子集，从每个子集中训练一个集成，并将这些集成中的所有弱分类器组合成一个最终输出？这种方法将在下一节中探讨，我们将利用集成集成技术。
- en: Ensemble of ensembles
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成集成
- en: 'Can we combine boosting and bagging? As we saw earlier, in bagging, we create
    multiple subsets of data and then train classifiers on those datasets. We can
    treat AdaBoost as a classifier while doing bagging. The process is simple: first,
    we create the bags and then train different AdaBoost classifiers on each bag.
    Here, AdaBoost is an ensemble in itself. Thus, these models are called an **ensemble**
    **of ensembles**.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否结合提升和bagging？如我们之前所看到的，在bagging中，我们创建多个数据子集，然后在那些数据集上训练分类器。我们可以将AdaBoost视为在bagging过程中使用的分类器。过程很简单：首先，我们创建袋子，然后在每个袋子上训练不同的AdaBoost分类器。在这里，AdaBoost本身就是一种集成。因此，这些模型被称为**集成**
    **的集成**。
- en: On top of having an ensemble of ensembles, we can also do undersampling (or
    oversampling) at the time of bagging. This gives us the **benefits of bagging**,
    **boosting**, and **random undersampling** (or oversampling) in a single model.
    We will discuss one such algorithm in this section, called **EasyEnsemble**. Since
    random undersampling doesn’t have significant overhead, both algorithms have training
    times similar to any other algorithm with the same number of weak classifiers.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拥有集成集成之外，我们还可以在bagging时进行下采样（或过采样）。这使我们能够在单个模型中获得下采样（或过采样）的**袋装**、**提升**和**随机下采样**（或过采样）的**好处**。在本节中，我们将讨论一个名为**EasyEnsemble**的算法，它具有与具有相同数量弱分类器的任何其他算法相似的训练时间。由于随机下采样没有显著的开销，这两个算法的训练时间与其他算法相似。
- en: EasyEnsemble
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EasyEnsemble
- en: 'The EasyEnsemble algorithm [8] generates balanced datasets from the original
    dataset and trains a different AdaBoost classifier on each of the balanced datasets.
    Later, it creates an aggregate classifier that makes predictions based on the
    majority votes of the AdaBoost classifiers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: EasyEnsemble算法[8]从原始数据集中生成平衡数据集，并在每个平衡数据集上训练不同的AdaBoost分类器。随后，它创建一个聚合分类器，该分类器基于AdaBoost分类器的多数投票进行预测：
- en: '![](img/B17259_04_16.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_16.jpg)'
- en: Figure 4.16 – EasyEnsemble pseudocode
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – EasyEnsemble伪代码
- en: '*Figure 4**.17* summarizes the EasyEnsemble algorithm using three subsets of
    the training data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4**.17*总结了使用训练数据三个子集的EasyEnsemble算法：'
- en: '![](img/B17259_04_17.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_17.jpg)'
- en: Figure 4.17 – EasyEnsemble algorithm explained
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – EasyEnsemble算法解释
- en: Instead of randomly undersampling the majority class examples, we can randomly
    oversample the minority class examples too.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以随机下采样多数类示例，还可以随机上采样少数类示例。
- en: 'The `imbalanced-learn` library provides the API for EasyEnsemble using `EasyEnsembleClassifier`.
    The `EasyEnsembleClassifier` API provides a `base_estimator` argument that can
    be used to set any classifier, with the default being `AdaBoostClassifier`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`库提供了使用`EasyEnsembleClassifier`的API。`EasyEnsembleClassifier`
    API提供了一个`base_estimator`参数，可以用来设置任何分类器，默认为`AdaBoostClassifier`：'
- en: '[PRE8]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s plot the decision boundary:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制决策边界：
- en: '[PRE9]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/B17259_04_18.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_04_18.jpg)'
- en: Figure 4.18 – The decision boundary of EasyEnsembleClassifier on the training
    data
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 – EasyEnsembleClassifier在训练数据上的决策边界
- en: By default, EasyEnsemble uses `AdaBoostClassifier` as the base estimator. However,
    we can use any other estimator as well, such as `XGBoostClassifier`, or tune it
    in other ways, say by passing another `sampling_strategy`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，EasyEnsemble使用`AdaBoostClassifier`作为基线估计器。然而，我们也可以使用任何其他估计器，例如`XGBoostClassifier`，或者以其他方式调整它，例如通过传递另一个`sampling_strategy`。
- en: This concludes our discussion of EasyEnsemble. Next, we will compare the various
    boosting methods that we’ve studied.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对EasyEnsemble的讨论。接下来，我们将比较我们研究过的各种提升方法。
- en: Comparative performance of boosting methods
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升方法的比较性能
- en: 'Let’s compare the performance of the various boosting methods we’ve discussed.
    We use a decision tree as a baseline and RUSBoost, AdaBoost, XGBoost, and EasyEnsemble,
    along with two variants. By default, `EasyEnsembleClassifier` uses `AdaBoostClassifier`
    as a baseline estimator. We use XGBoost instead as the estimator in the second
    variant of `EasyEnsembleClassifier`; in the third variant, we use `not majority`
    for our `sampling_strategy`, along with the XGBoost estimator:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较我们讨论过的各种提升方法的性能。我们以决策树作为基线，并使用RUSBoost、AdaBoost、XGBoost和EasyEnsemble，以及两种变体。默认情况下，`EasyEnsembleClassifier`使用`AdaBoostClassifier`作为基线估计器。我们在`EasyEnsembleClassifier`的第二种变体中使用XGBoost作为估计器；在第三种变体中，我们使用`not
    majority`作为我们的`sampling_strategy`，并配合XGBoost估计器：
- en: '| **Technique** | **F2 Score** | **Precision** | **Recall** | **Average Precision**
    | **AUC-ROC** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **F2分数** | **精确度** | **召回率** | **平均精确度** | **AUC-ROC** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| EasyEnsemble (`estimator=XGBoost` and `sampling_strategy =` `not_majority`)
    | 0.885 | 0.933 | 0.874 | **0.978** | **1.000** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| EasyEnsemble（估计器=XGBoost和`sampling_strategy =` `not_majority`） | 0.885 |
    0.933 | 0.874 | **0.978** | **1.000** |'
- en: '| EasyEnsemble (`estimator=XGBoost`) | 0.844 | 0.520 | **1.000** | **0.978**
    | 0.999 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| EasyEnsemble（估计器=XGBoost） | 0.844 | 0.520 | **1.000** | **0.978** | 0.999
    |'
- en: '| EasyEnsemble | 0.844 | 0.519 | **1.000** | 0.940 | 0.999 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| EasyEnsemble | 0.844 | 0.519 | **1.000** | 0.940 | 0.999 |'
- en: '| RUSBoost | 0.836 | 0.517 | 0.989 | 0.948 | **1.000** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| RUSBoost | 0.836 | 0.517 | 0.989 | 0.948 | **1.000** |'
- en: '| AdaBoost | **0.907** | 0.938 | 0.900 | **0.978** | **1.000** |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| AdaBoost | **0.907** | 0.938 | 0.900 | **0.978** | **1.000** |'
- en: '| XGBoost | 0.885 | 0.933 | 0.874 | 0.968 | **1.000** |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost | 0.885 | 0.933 | 0.874 | 0.968 | **1.000** |'
- en: '| Decision Tree | 0.893 | **0.960** | 0.878 | 0.930 | 0.981 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 0.893 | **0.960** | 0.878 | 0.930 | 0.981 |'
- en: Table 4.3 – Performance comparison of various boosting techniques
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.3 – 各种提升技术的性能比较
- en: 'Here are some conclusions from *Table 4.3*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从*表4.3*中得出的结论：
- en: For the highest F2 score, AdaBoost is the best choice
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于最高的F2分数，AdaBoost是最好的选择。
- en: For high precision, the plain decision tree beats all other techniques
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高精确度，纯决策树击败了所有其他技术。
- en: For perfect recall, EasyEnsemble (`estimator=XGBoost`) and EasyEnsemble perform
    perfectly
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于完美的召回率，EasyEnsemble（估计器为`XGBoost`）和EasyEnsemble表现完美。
- en: For overall balanced performance, AdaBoost and EasyEnsemble (`estimator=XGBoost`
    and `sampling_strategy=not_majority`) are strong contenders
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于整体平衡性能，AdaBoost和EasyEnsemble（估计器为`XGBoost`和`sampling_strategy=not_majority`）是强有力的竞争者。
- en: The ensembling techniques such as RUSBoost and EasyEnsemble are specifically
    designed for handling data imbalance and improving recall compared to a baseline
    model such as the decision tree or even AdaBoost
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成技术，如RUSBoost和EasyEnsemble，专门设计用于处理数据不平衡并提高召回率，与基线模型（如决策树或甚至AdaBoost）相比。
- en: Overall, the results indicate that while ensemble methods such as AdaBoost and
    XGBoost provide robust baselines that are hard to beat, leveraging imbalanced
    learning techniques can indeed modify the decision boundaries of the resulting
    classifiers, which can potentially help with improving the recall. The efficacy
    of these techniques, however, largely depends on the dataset and performance metric
    under consideration.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果表明，尽管像AdaBoost和XGBoost这样的集成方法提供了难以超越的稳健基准，但利用不平衡学习技术确实可以修改结果分类器的决策边界，这有可能有助于提高召回率。然而，这些技术的有效性在很大程度上取决于所考虑的数据集和性能指标。
- en: By wrapping up our journey through the ensemble of ensembles, we’ve added yet
    another powerful and dynamic tool to our machine learning arsenal.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结束我们通过集成集成之旅，我们为我们的机器学习工具箱添加了另一个强大且动态的工具。
- en: Model performance comparison
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能比较
- en: The effectiveness of the techniques we’ve discussed so far can be highly dependent
    on the dataset they are applied to. In this section, we will conduct a comprehensive
    comparative analysis that compares the various techniques we have discussed so
    far while using the logistic regression model as a baseline. For a comprehensive
    review of the complete implementation, please consult the accompanying notebook
    available on GitHub.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所讨论的技术有效性高度依赖于它们应用到的数据集。在本节中，我们将进行全面的比较分析，比较我们迄今为止所讨论的各种技术，同时以逻辑回归模型作为基准。有关完整实现的全面审查，请参阅GitHub上提供的配套笔记本。
- en: 'The analysis spans four distinct datasets, each with its own characteristics
    and challenges:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 分析涵盖了四个不同的数据集，每个数据集都有其独特的特性和挑战：
- en: '**Synthetic data with Sep: 0.5**: A simulated dataset with moderate separation
    between classes, serving as a baseline to understand algorithm performance in
    simplified conditions.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep: 0.5的合成数据**：一个类别之间具有适度分离的模拟数据集，作为在简化条件下理解算法性能的基准。'
- en: '**Synthetic data with Sep: 0.9**: Another synthetic dataset, but with a higher
    degree of separation, allowing us to examine how algorithms perform as class distinguishability
    improves.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep: 0.9的合成数据**：另一个合成数据集，但具有更高的分离度，使我们能够检查随着类别区分性的提高，算法的表现如何。'
- en: '`imblearn`) related to healthcare, chosen for its practical importance and
    the natural class imbalance often seen in medical datasets.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与医疗保健相关的`imblearn`)，因其实际重要性以及医疗数据集中常见的自然类别不平衡而被选中。
- en: '`imblearn` as well.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imblearn`也是如此。'
- en: Our primary metric for evaluation is average precision, a summary measure that
    combines both precision and recall, thereby providing a balanced view of algorithm
    performance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要评估指标是平均精确度，这是一个结合了精确度和召回率的综合度量，从而提供了一个平衡的算法性能视图。
- en: We’d like to emphasize that we are using the vanilla versions of the various
    ensemble models for comparison. With some additional effort in tuning the hyperparameters
    of these models, we could certainly enhance the performance of these implementations.
    We leave that as an exercise for you.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调，我们正在使用各种集成模型的原始版本进行比较。通过一些额外的努力来调整这些模型的超参数，我们当然可以增强这些实现的性能。我们将这留给你作为练习。
- en: 'By comparing these diverse algorithms across a variety of datasets, this analysis
    aims to provide some valuable insights into the following aspects:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在多种数据集上比较这些不同的算法，这项分析旨在以下方面提供一些有价值的见解：
- en: How conventional and specialized techniques stack up against each other
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统技术与专门技术的对比
- en: The dependency of algorithm effectiveness on dataset characteristics
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法有效性对数据集特征的依赖性
- en: The practical implications of choosing one algorithm over another in different
    scenarios
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同场景下选择一种算法而非另一种算法的实际影响
- en: '*Figure 4**.19* compares the performance of various bagging and boosting techniques
    using the average precision score, while using the logistic regression model as
    a baseline, over two synthetic datasets:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.19*比较了使用平均精确度得分，以逻辑回归模型为基准，在两个合成数据集上各种bagging和boosting技术的性能：'
- en: '![](img/B17259_04_19.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_04_19.jpg)'
- en: Figure 4.19 – Average precision scores on synthetic datasets
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 – 合成数据集上的平均精确度得分
- en: '*Figure 4**.20* shows similar plots across two real-world datasets:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.20*显示了两个真实世界数据集上的相似图：'
- en: '![](img/B17259_04_20.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_04_20.jpg)'
- en: Figure 4.20 – Average precision scores on the thyroid_sick and abalone_19 datasets
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 – thyroid_sick和abalone_19数据集上的平均精确度得分
- en: 'Let’s analyze these results for each of the datasets:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析每个数据集的结果：
- en: '**Synthetic data with Sep 0.5** (*Figure 4**.19*, left): XGBoost and logistic
    regression performed the best in terms of average precision, scoring 0.30 and
    0.27, respectively. Interestingly, ensemble methods designed specifically for
    imbalanced data, such as SMOTEBagging and OverBagging, perform comparably or even
    worse than conventional methods such as bagging. This suggests that specialized
    methods do not always guarantee an advantage in simpler synthetic settings.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep 0.5的合成数据** (*图4.19*，左侧)：在平均精确度方面，XGBoost和逻辑回归表现最佳，分别得分为0.30和0.27。有趣的是，专门为不平衡数据设计的集成方法，如SMOTEBagging和OverBagging，表现与常规方法如Bagging相当，甚至更差。这表明在更简单的合成设置中，专门的方法并不总是能保证优势。'
- en: '**Synthetic data with Sep 0.9** (*Figure 4**.19*, right): EasyEnsemble takes
    the lead on this dataset with an average precision score of 0.64, closely followed
    by logistic regression and XGBoost. This higher separation seems to allow EasyEnsemble
    to capitalize on its focus on balancing, leading to better performance. Other
    ensemble methods such as UnderBagging and OverBagging perform reasonably but do
    not surpass the leaders.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sep 0.9的合成数据** (*图4.19*，右侧)：EasyEnsemble在这个数据集上以平均精确度0.64领先，紧随其后的是逻辑回归和XGBoost。这种更高的分离似乎使EasyEnsemble能够利用其对平衡的关注，从而获得更好的性能。其他集成方法如UnderBagging和OverBagging表现尚可，但未超越领先者。'
- en: '**Thyroid sick dataset** (*Figure 4**.20*, left): In a real-world dataset focusing
    on thyroid sickness, XGBoost far outperforms all other methods with an average
    precision of 0.97\. Other ensemble methods such as bagging, OverBagging, and SMOTEBagging
    also score high, suggesting that ensembles are particularly effective for this
    dataset. Interestingly, boosting and RUSBoost do not keep pace, indicating that
    not all boosting variants are universally effective.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**甲状腺疾病数据集** (*图4.20*，左侧)：在一个关注甲状腺疾病的真实世界数据集中，XGBoost的表现远超其他所有方法，平均精确度为0.97。其他集成方法如Bagging、OverBagging和SMOTEBagging也得分很高，表明集成方法特别适用于这个数据集。有趣的是，Boosting和RUSBoost没有跟上步伐，这表明并非所有Boosting变体都普遍有效。'
- en: '**Abalone 19 dataset** (*Figure 4**.20*, right): For the Abalone 19 dataset,
    all methods perform relatively poorly, with XGBoost standing out with an average
    precision of 0.13\. EasyEnsemble comes in second with a score of 0.09, while traditional
    methods such as logistic regression and bagging lag behind. This could indicate
    that the dataset is particularly challenging for most methods, and specialized
    imbalanced techniques can only make marginal improvements.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Abalone 19数据集** (*图4.20*，右侧)：对于Abalone 19数据集，所有方法的表现相对较差，XGBoost以平均精确度0.13脱颖而出。EasyEnsemble以0.09的得分位居第二，而传统方法如逻辑回归和Bagging则落后。这可能表明该数据集对大多数方法来说特别具有挑战性，而专门的不平衡技术只能带来微小的改进。'
- en: 'Here are some overall insights:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些总体见解：
- en: Conventional methods such as XGBoost and logistic regression often provide strong
    baselines that are difficult to beat
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规方法如XGBoost和逻辑回归通常提供强大的基线，难以超越。
- en: The efficacy of specialized imbalanced learning techniques can vary significantly,
    depending on the dataset and its inherent complexities
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专门的不平衡学习技术的有效性可能因数据集及其固有的复杂性而显著不同。
- en: Ensemble methods generally perform well across various datasets, but their effectiveness
    can be context-dependent
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法通常在各种数据集上表现良好，但它们的有效性可能取决于具体情境。
- en: The choice of performance metric – in this case, average precision – can significantly
    influence the evaluation, making it crucial to consider multiple metrics for a
    comprehensive understanding
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能指标的选择——在本例中为平均精确度——可以显著影响评估，因此考虑多个指标以获得全面理解至关重要。
- en: We hope that this chapter has shown how you can incorporate sampling techniques
    with ensemble methods to achieve improved results, especially when dealing with
    imbalanced data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这一章已经展示了您如何将采样技术与集成方法相结合以实现更好的结果，尤其是在处理不平衡数据时。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Ensemble methods in machine learning create strong classifiers by combining
    results from multiple weak classifiers using approaches such as bagging and boosting.
    However, these methods assume balanced data and may struggle with imbalanced datasets.
    Combining ensemble methods with sampling methods such as oversampling and undersampling
    leads to techniques such as UnderBagging, OverBagging, and SMOTEBagging, all of
    which can help address imbalanced data issues.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的集成方法通过结合多个弱分类器的结果来创建强大的分类器，例如使用袋装和提升方法。然而，这些方法假设数据平衡，可能难以处理不平衡数据集。将集成方法与过采样和欠采样等采样方法相结合，导致UnderBagging、OverBagging和SMOTEBagging等技术，所有这些都可以帮助解决不平衡数据问题。
- en: Ensembles of ensembles, such as EasyEnsemble, combine boosting and bagging techniques
    to create powerful classifiers for imbalanced datasets.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: EasyEnsemble等集成集成，结合提升和袋装技术，为不平衡数据集创建强大的分类器。
- en: Ensemble-based imbalance learning techniques can be an excellent addition to
    your toolkit. The ones based on KNN, viz., SMOTEBoost, and RAMOBoost can be slow.
    However, the ensembles based on random undersampling and random oversampling are
    less costly. Also, boosting methods are found to sometimes work better than bagging
    methods in the case of imbalanced data. We can combine random sampling techniques
    with boosting to get better overall performance. As we emphasized previously,
    it’s empirical, and we have to try to know what would work best for our data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 基于集成的不平衡学习技术可以成为您工具箱中的优秀补充。基于KNN的，如SMOTEBoost和RAMOBoost可能较慢。然而，基于随机欠采样和随机过采样的集成成本较低。此外，发现提升方法在处理不平衡数据时有时比袋装方法更有效。我们可以将随机采样技术与提升方法相结合，以获得更好的整体性能。正如我们之前强调的，这是经验性的，我们必须尝试了解什么最适合我们的数据。
- en: In the next chapter, we will learn how to change the model to account for the
    imbalance in data and the various costs incurred by the model because of misclassifying
    the minority class examples.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何调整模型以考虑数据的不平衡以及模型由于对少数类样本的错误分类而引起的各种成本。
- en: Questions
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Try using `RUSBoostClassifier` on the `abalone_19` dataset and compare the performance
    with other techniques from the previous chapters.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在`abalone_19`数据集上使用`RUSBoostClassifier`，并将性能与其他章节中的技术进行比较。
- en: What is the difference between the `BalancedRandomForestClassifier` and `BalancedBaggingClassifier`
    classes in the `imbalanced-learn` library?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`库中的`BalancedRandomForestClassifier`和`BalancedBaggingClassifier`类之间的区别是什么？'
- en: References
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'L. Breiman, *Bagging predictors*, Mach Learn, vol. 24, no. 2, pp. 123–140,
    Aug. 1996, doi: 10.1007/BF00058655, [https://link.springer.com/content/pdf/10.1007/BF00058655.pdf](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf).'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'L. Breiman, *Bagging predictors*, Mach Learn, vol. 24, no. 2, pp. 123–140,
    Aug. 1996, doi: 10.1007/BF00058655, [https://link.springer.com/content/pdf/10.1007/BF00058655.pdf](https://link.springer.com/content/pdf/10.1007/BF00058655.pdf).'
- en: '(The paper that introduced OverBagging, UnderBagging, and SMOTEBagging) S.
    Wang and X. Yao, *Diversity analysis on imbalanced data sets by using ensemble
    models*, in 2009 IEEE Symposium on Computational Intelligence and Data Mining,
    Nashville, TN, USA: IEEE, Mar. 2009, pp. 324–331\. doi: 10.1109/CIDM.2009.4938667,
    [https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf](https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf).'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '（介绍了OverBagging、UnderBagging和SMOTEBagging的论文）S. Wang和X. Yao, *Diversity analysis
    on imbalanced data sets by using ensemble models*, in 2009 IEEE Symposium on Computational
    Intelligence and Data Mining, Nashville, TN, USA: IEEE, Mar. 2009, pp. 324–331\.
    doi: 10.1109/CIDM.2009.4938667, [https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf](https://www.cs.bham.ac.uk/~wangsu/documents/papers/CIDMShuo.pdf).'
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*实时网站事件升级预测* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)'
- en: 'L. Breiman, *Random Forests*, Machine Learning, vol. 45, no. 1, pp. 5–32, 2001,
    doi: 10.1023/A:1010933404324, [https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf).'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'L. Breiman, *Random Forests*, Machine Learning, vol. 45, no. 1, pp. 5–32, 2001,
    doi: 10.1023/A:1010933404324, [https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf](https://link.springer.com/content/pdf/10.1023/A:1010933404324.pdf).'
- en: '(The paper that introduced the RUSBoost algorithm) C. Seiffert, T. M. Khoshgoftaar,
    J. Van Hulse, and A. Napolitano, *RUSBoost: A Hybrid Approach to Alleviating Class
    Imbalance*, IEEE Trans. Syst., Man, Cybern. A, vol. 40, no. 1, pp. 185–197, Jan.
    2010, doi: 10.1109/TSMCA.2009.2029559, [https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf](https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf).'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '（介绍了RUSBoost算法的论文）C. Seiffert, T. M. Khoshgoftaar, J. Van Hulse, 和 A. Napolitano,
    *RUSBoost: 一种缓解类别不平衡的混合方法*，IEEE Trans. Syst., Man, Cybern. A, 第40卷，第1期，第185–197页，2010年1月，doi:
    10.1109/TSMCA.2009.2029559，[https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf](https://www.researchgate.net/profile/Jason-Van-Hulse/publication/224608502_RUSBoost_A_Hybrid_Approach_to_Alleviating_Class_Imbalance/links/0912f50f4bec299a8c000000/RUSBoost-A-Hybrid-Approach-to-Alleviating-Class-Imbalance.pdf).'
- en: '(The paper that introduced the SMOTEBoost algorithm) N. V. Chawla, A. Lazarevic,
    L. O. Hall, and K. W. Bowyer, *SMOTEBoost: Improving Prediction of the Minority
    Class in Boosting*, in Knowledge Discovery in Databases: PKDD 2003, N. Lavrač,
    D. Gamberger, L. Todorovski, and H. Blockeel, Eds., in Lecture Notes in Computer
    Science, vol. 2838\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2003, pp.
    107–119\. doi: 10.1007/978-3-540-39804-2_12, [https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf](https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf).'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '（介绍了SMOTEBoost算法的论文）N. V. Chawla, A. Lazarevic, L. O. Hall, 和 K. W. Bowyer,
    *SMOTEBoost: 在提升中改进少数类的预测*，在数据库中的知识发现：PKDD 2003，N. Lavrač, D. Gamberger, L. Todorovski,
    和 H. Blockeel 编著，计算机科学讲座笔记，第2838卷。柏林，海德堡：Springer Berlin Heidelberg，2003年，第107–119页。doi:
    10.1007/978-3-540-39804-2_12，[https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf](https://www3.nd.edu/~dial/publications/chawla2003smoteboost.pdf).'
- en: '(The paper that introduced RAMOBoost algorithm) Sheng Chen, Haibo He, and E.
    A. Garcia, *RAMOBoost: Ranked Minority Oversampling in Boosting*, IEEE Trans.
    Neural Netw., vol. 21, no. 10, pp. 1624–1642, Oct. 2010, doi: 10.1109/TNN.2010.2066988,
    [https://ieeexplore.ieee.org/abstract/document/5559472](https://ieeexplore.ieee.org/abstract/document/5559472).'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '（介绍了RAMOBoost算法的论文）Sheng Chen, Haibo He, 和 E. A. Garcia, *RAMOBoost: 在提升中的排序少数类过采样*，IEEE
    Trans. Neural Netw.，第21卷，第10期，第1624–1642页，2010年10月，doi: 10.1109/TNN.2010.2066988，[https://ieeexplore.ieee.org/abstract/document/5559472](https://ieeexplore.ieee.org/abstract/document/5559472).'
- en: '(The paper that introduced EasyEnsemble) Xu-Ying Liu, Jianxin Wu, and Zhi-Hua
    Zhou, *Exploratory Undersampling for Class-Imbalance Learning*, IEEE Trans. Syst.,
    Man, Cybern. B, vol. 39, no. 2, pp. 539–550, Apr. 2009, doi: 10.1109/TSMCB.2008.2007853,
    [http://129.211.169.156/publication/tsmcb09.pdf](http://129.211.169.156/publication/tsmcb09.pdf).'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '（介绍了EasyEnsemble的论文）Xu-Ying Liu, Jianxin Wu, 和 Zhi-Hua Zhou, *用于类别不平衡学习的探索性欠采样*，IEEE
    Trans. Syst., Man, Cybern. B，第39卷，第2期，第539–550页，2009年4月，doi: 10.1109/TSMCB.2008.2007853，[http://129.211.169.156/publication/tsmcb09.pdf](http://129.211.169.156/publication/tsmcb09.pdf).'
