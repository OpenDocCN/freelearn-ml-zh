- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Distributed and GPU-Based Learning with LightGBM
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 LightGBM 的分布式和 GPU 学习
- en: This chapter looks at training LightGBM models on distributed computing clusters
    and GPUs. Distributed computing can significantly speed up training workloads
    and enable the training of much larger datasets than the memory available on a
    single machine. We’ll look at leveraging Dask for distributed computing and LightGBM’s
    support for GPU-based training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了在分布式计算集群和 GPU 上训练 LightGBM 模型。分布式计算可以显著加快训练工作负载，并允许训练比单台机器上可用的内存大得多的数据集。我们将探讨利用
    Dask 进行分布式计算以及 LightGBM 对基于 GPU 的训练的支持。
- en: 'The topics covered in the chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Distributed learning with LightGBM and Dask
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LightGBM 和 Dask 进行分布式学习
- en: GPU training for LightGBM
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM 的 GPU 训练
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The chapter includes examples of training and running LightGBM models on distributed
    computing clusters and GPUs. A Dask environment and GPUs are required to run the
    examples. Complete code examples are available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括在分布式计算集群和 GPU 上训练和运行 LightGBM 模型的示例。运行这些示例需要 Dask 环境和 GPU。完整的代码示例可在 [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11)
    找到。
- en: Distributed learning with LightGBM and Dask
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LightGBM 和 Dask 进行分布式学习
- en: Dask is an open-source Python library for distributed computing. It’s designed
    to integrate seamlessly with existing Python libraries and tools, including scikit-learn
    and LightGBM. This section looks at running distributed training workloads for
    LightGBM using Dask.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 是一个开源的 Python 分布式计算库。它旨在无缝集成到现有的 Python 库和工具中，包括 scikit-learn 和 LightGBM。本节探讨了使用
    Dask 运行 LightGBM 的分布式训练工作负载。
- en: Dask ([https://www.dask.org/](https://www.dask.org/)) allows you to set up clusters
    on both a single machine and across many machines. Running Dask on a single machine
    is the default and requires no setup. However, workloads that run on a single-machine
    cluster (or scheduler) can readily be run with a distributed scheduler.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Dask ([https://www.dask.org/](https://www.dask.org/)) 允许您在单台机器和多台机器上设置集群。在单台机器上运行
    Dask 是默认设置，无需设置。然而，在单机集群（或调度器）上运行的工作负载可以轻松地使用分布式调度器运行。
- en: Dask offers many ways to run a distributed cluster, including integrating Kubernetes,
    MPI, or automatic provisioning into a hyperscalar such as AWS or Google Cloud
    Platform.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了许多运行分布式集群的方法，包括与 Kubernetes、MPI 集成，或自动配置到超大规模平台（如 AWS 或 Google Cloud
    Platform）中。
- en: When running on a single machine, Dask still distributes the workload across
    multiple threads, which can significantly speed up workloads.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当在单台机器上运行时，Dask 仍然将工作负载分布在多个线程上，这可以显著加快工作负载。
- en: 'Dask provides cluster management utility classes to set up a cluster easily.
    A local cluster can be run as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了集群管理实用类，可以轻松设置集群。可以按以下方式运行本地集群：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding code creates a local cluster with four workers, configuring each
    worker to run two threads. The cluster runs on localhost, with the scheduler running
    on port `8786` by default. The host IP and port can be configured using parameters.
    In addition to running the scheduler, Dask also starts a diagnostic dashboard
    that’s implemented using Bokeh ([https://docs.bokeh.org/en/latest/](https://docs.bokeh.org/en/latest/)).
    By default, the dashboard runs on port `8787`. We can check the **Workers** page
    to see the status of our running cluster, as shown in *Figure 11**.1.*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个包含四个工作者的本地集群，每个工作者配置为运行两个线程。集群在 localhost 上运行，默认情况下调度器在端口 `8786` 上运行。可以通过参数配置主机
    IP 和端口。除了运行调度器外，Dask 还启动了一个使用 Bokeh ([https://docs.bokeh.org/en/latest/](https://docs.bokeh.org/en/latest/))
    实现的诊断仪表板。默认情况下，仪表板在端口 `8787` 上运行。我们可以检查 **工作者** 页面以查看运行集群的状态，如图 *图 11*.1* 所示。
- en: '![Figure 11.1 – Dask diagnostics dashboard showing four running workers with
    some technical statistics for each](img/B16690_11_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 显示四个运行工作者的 Dask 诊断仪表板，每个工作者都有一些技术统计数据](img/B16690_11_01.jpg)'
- en: Figure 11.1 – Dask diagnostics dashboard showing four running workers with some
    technical statistics for each
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 显示四个运行工作者的 Dask 诊断仪表板，每个工作者都有一些技术统计数据
- en: With a cluster up and running, we can prepare our data for use on the distributed
    cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集群启动并运行后，我们可以准备数据以在分布式集群上使用。
- en: Dask offers its own implementation of a data frame called the Dask DataFrame.
    A Dask DataFrame comprises many smaller pandas DataFrames, which are split based
    on the index. Each part can be stored on disk or distributed across a network,
    which allows working with much larger datasets than can fit into a single machine’s
    memory. Operations performed on the Dask DataFrame are automatically distributed
    to the pandas DataFrames.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Dask 提供了自己实现的数据帧，称为 Dask DataFrame。Dask DataFrame 由许多较小的 pandas DataFrame 组成，这些
    DataFrame 根据索引分割。每个部分可以存储在磁盘上或通过网络分布式，这使得可以处理比单个机器内存能容纳的更大的数据集。在 Dask DataFrame
    上执行的操作会自动分布到 pandas DataFrame 上。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When your dataset fits into RAM, using standard pandas DataFrames instead of
    a Dask DataFrame is recommended.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据集适合 RAM 时，建议使用标准的 pandas DataFrame 而不是 Dask DataFrame。
- en: 'We can create a Dask DataFrame by loading a CSV file. Note that the CSV file
    may be located on S3 or HDFS and can be very large. The following code creates
    a Dask DataFrame from a CSV file:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过加载 CSV 文件来创建一个 Dask DataFrame。请注意，CSV 文件可能位于 S3 或 HDFS 上，并且可能非常大。以下代码从一个
    CSV 文件创建一个 Dask DataFrame：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we also specify the block size for loading the CSV file. The block size
    sets the chunks the dataset is divided into and gives us granular control over
    the memory of individual DataFrame parts. When calling `df.shape`, we get an interesting
    result:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们也指定了加载 CSV 文件时的块大小。块大小设置了数据集被分割成的块的大小，并使我们能够对单个 DataFrame 部分的内存进行细粒度控制。当调用
    `df.shape` 时，我们得到一个有趣的结果：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The number of columns is returned as a number. However, looking at the number
    of rows, we got a wrapper class called `Delayed`. This illustrates that even though
    we’ve created the DataFrame, the data is not loaded into memory. Instead, Dask
    loads the data as needed on the workers that use the data. We can force Dask to
    compute the row count as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列数以数字的形式返回。然而，当我们查看行数时，我们得到了一个名为 `Delayed` 的包装类。这表明尽管我们已经创建了 DataFrame，但数据并未加载到内存中。相反，Dask
    在使用数据的工人上按需加载数据。我们可以强制 Dask 计算行数如下：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With the data available in a DataFrame, we can prepare it for training. We
    split our data into a training and test set using the `train_test_split` function
    from `dask_ml`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame 中有了可用数据后，我们可以为训练准备它。我们使用 `dask_ml` 中的 `train_test_split` 函数将我们的数据分割成训练集和测试集：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Although the function from `dask_ml` mirrors the functionality of scikit-learn’s
    `train_test_split`, the Dask version maintains the distributed nature of the underlying
    Dask DataFrame.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管来自 `dask_ml` 的函数与 scikit-learn 的 `train_test_split` 功能相似，但 Dask 版本保持了底层 Dask
    DataFrame 的分布式特性。
- en: Our cluster is now set up, and the data is prepared for training. We can now
    look toward training our LightGBM model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已设置好集群，并准备好了训练数据。我们现在可以开始训练我们的 LightGBM 模型了。
- en: 'The LightGBM library team offers and maintains Dask versions of each available
    learning algorithm: `DaskLGBMRegressor`, `DaskLGBMClassifier`, and `DaskLGBMRanker`.
    These are wrappers around the standard LightGBM scikit-learn interface with additional
    functionality to specify the Dask cluster client to use.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 库团队提供了并维护了每个可用的学习算法的 Dask 版本：`DaskLGBMRegressor`、`DaskLGBMClassifier`
    和 `DaskLGBMRanker`。这些是围绕标准 LightGBM scikit-learn 接口包装的，并增加了指定要使用的 Dask 集群客户端的功能。
- en: When LightGBM runs on a Dask cluster, training occurs with one LightGBM worker
    per Dask worker. LightGBM concatenates all data partitions on a single worker
    into a single dataset, and each LightGBM worker uses the local dataset independently.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当 LightGBM 在 Dask 集群上运行时，每个 Dask 工作器对应一个 LightGBM 工作器。LightGBM 将单个工作器上的所有数据分区连接成一个单一的数据集，并且每个
    LightGBM 工作器独立使用本地数据集。
- en: 'Each LightGBM worker then works in concert to train a single LightGBM model,
    using the Dask cluster to communicate. When data parallel training is performed
    (as is the case with Dask), LightGBM uses a **Reduce-Scatter** strategy:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，每个 LightGBM 工作器协同工作以训练单个 LightGBM 模型，使用 Dask 集群进行通信。当执行数据并行训练（如 Dask 的情况）时，LightGBM
    使用 **Reduce-Scatter** 策略：
- en: 'During the histogram-building phase, each worker builds histograms for different
    non-overlapping features. Then, a **Reduce-Scatter** operation is performed: each
    worker shares a part of its histogram with each other worker.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在直方图构建阶段，每个工作器为不同的非重叠特征构建直方图。然后，执行一个 **Reduce-Scatter** 操作：每个工作器将其直方图的一部分与每个其他工作器共享。
- en: After the **Reduce-Scatter**, each worker has a complete histogram for a subset
    of features and then finds the best split for these features.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **Reduce-Scatter** 之后，每个工作器都有一个特征子集的完整直方图，然后为这些特征找到最佳分割。
- en: 'Finally, a gathering operation is performed: each worker shares its best split
    with all other workers, so all workers have all the best splits.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，执行一个收集操作：每个工作器将其最佳分割与所有其他工作器共享，因此所有工作器都拥有所有最佳分割。
- en: The best feature split is chosen, and the data is partitioned accordingly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳特征分割，并根据此对数据进行分区。
- en: 'Fortunately, the complexity of the distributed algorithm is hidden from us,
    and the training code is identical to the scikit-learn training code we’re used
    to:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，分布式算法的复杂性对我们来说是隐藏的，训练代码与我们所熟悉的 scikit-learn 训练代码相同：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Running the preceding code trains the LightGBM model, and we can see the progress
    by checking the **Status** page of the Dask dashboard, as shown in *Figure 11**.2*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码训练 LightGBM 模型，我们可以通过检查 Dask 仪表板的 **状态** 页面来查看进度，如图 *图 11**.2* 所示。
- en: '![Figure 11.2 – Status page of the Dask dashboard showing the task stream while
    a LightGBM model is training](img/B16690_11_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 当 LightGBM 模型正在训练时的 Dask 仪表板状态页面](img/B16690_11_02.jpg)'
- en: Figure 11.2 – Status page of the Dask dashboard showing the task stream while
    a LightGBM model is training
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 当 LightGBM 模型正在训练时的 Dask 仪表板状态页面
- en: 'Dask LightGBM models can be fully serialized using **Pickle** or **joblib**,
    and we can save the model to disk as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Dask LightGBM 模型可以使用 **Pickle** 或 **joblib** 完全序列化，并且我们可以按以下方式将模型保存到磁盘：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Predictions can be made by calling the `predict` method of the model. Note
    that the Dask model expects a Dask DataFrame or array:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过调用模型的 `predict` 方法来进行预测。请注意，Dask 模型期望一个 Dask DataFrame 或数组：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Similar to getting the shape of a Dask DataFrame, the prediction operation
    is also delayed and only calculated when needed. We can use `compute` to get the
    prediction values:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与获取 Dask DataFrame 的形状类似，预测操作也是延迟的，仅在需要时计算。我们可以使用 `compute` 来获取预测值：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This concludes our look at leveraging Dask for distributed training with LightGBM.
    With Dask, LightGBM can train models on massive datasets well beyond the computing
    power of a single server. Dask scales alongside your needs, so you can start with
    your local laptop and move to a high-performance computing environment or cloud
    infrastructure as your data grows. Further, as shown previously, Dask is designed
    to work harmoniously with established Python libraries such as pandas, NumPy,
    and scikit-learn, providing a familiar environment for data scientists while extending
    the capabilities of these tools.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对利用 Dask 进行 LightGBM 分布式训练的探讨。有了 Dask，LightGBM 可以在单个服务器的计算能力之外的巨大数据集上训练模型。Dask
    随着您的需求扩展，因此您可以从您的本地笔记本电脑开始，随着数据量的增长，转移到高性能计算环境或云基础设施。此外，如前所述，Dask 设计得可以与现有的 Python
    库（如 pandas、NumPy 和 scikit-learn）和谐工作，为数据科学家提供了一个熟悉的环境，同时扩展了这些工具的功能。
- en: Next, we’ll look at speeding up LightGBM training when large models need to
    be trained using the GPU.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨在需要使用 GPU 训练大型模型时如何加快 LightGBM 的训练速度。
- en: GPU training for LightGBM
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LightGBM 的 GPU 训练
- en: 'The LightGBM library has native support for training the model on a GPU [*1*].
    Two GPU platforms are supported: GPU via OpenCL and CUDA. Leveraging the GPU via
    OpenCL offers support for the broadest range of GPUs (including AMD GPUs) and
    is significantly faster than running the model on a CPU. However, the CUDA platform
    offers the fastest runtime if you have an NVIDIA GPU available.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 库原生支持在 GPU 上训练模型 [*1*]。支持两种 GPU 平台：通过 OpenCL 和 CUDA 的 GPU。利用 OpenCL
    通过 GPU 提供了对最广泛的 GPU 范围的支持（包括 AMD GPU），并且比在 CPU 上运行模型要快得多。然而，如果您有可用的 NVIDIA GPU，CUDA
    平台提供了最快的运行时间。
- en: Setting up LightGBM for the GPU
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 LightGBM 以支持 GPU
- en: Setting up your environment to use the GPU can be a bit tricky, but we'll review
    the core steps here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 设置您的环境以使用 GPU 可能会有些棘手，但我们将在此回顾核心步骤。
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The GPU setup steps discussed here are offered as a guide and overview of the
    process of setting up your environment. The exact version number of libraries
    and drivers listed here may be outdated, and it’s recommended that you review
    the official documentation for up-to-date versions: [https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml](https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的 GPU 设置步骤提供的是设置您环境的指南和概述。这里列出的库和驱动程序的精确版本号可能已过时，建议您查阅官方文档以获取最新版本：[https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml](https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml)。
- en: In order to use the GPU, we have to *compile and build the LightGBM library
    from the source code*. The following instructions assume an Ubuntu Linux build
    environment; steps for other platforms are similar. Before we can build the library,
    we must install a few dependencies.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用GPU，我们必须从源代码*编译和构建LightGBM库*。以下说明假设是在Ubuntu Linux构建环境中；其他平台的步骤类似。在我们能够构建库之前，我们必须安装一些依赖项。
- en: 'Importantly, first, install the GPU drivers for your environment. If you have
    an NVIDIA GPU, also install CUDA. Instructions for doing so are available from
    the respective vendor sites:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，首先，为你的环境安装GPU驱动程序。如果你有NVIDIA GPU，也要安装CUDA。有关安装说明，可从相应供应商网站获取：
- en: '[https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.nvidia.com/cuda/](https://docs.nvidia.com/cuda/)'
- en: https://www.amd.com/en/support
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: https://www.amd.com/en/support
- en: 'Next, we need to install the OpenCL headers:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要安装OpenCL头文件：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, install the library build dependencies:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，安装库的构建依赖项：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We are now ready to compile the LightGBM library with GPU support. Clone the
    repository and build the library, setting the `USE_GPU` flag:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好编译具有GPU支持的LightGBM库。克隆存储库并构建库，设置`USE_GPU`标志：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As mentioned in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036), *Ensemble
    Learning – Bagging and Boosting,* LightGBM is a C++ library with a Python interface.
    With the preceding instructions, we have built the library with GPU support, but
    we must build and install the Python package to use the library from Python (including
    the scikit-learn API):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第2章*](B16690_02.xhtml#_idTextAnchor036)中所述，*集成学习 – Bagging和Boosting，* LightGBM是一个具有Python接口的C++库。根据前面的说明，我们已经构建了具有GPU支持的库，但我们必须构建和安装Python包，以便从Python（包括scikit-learn
    API）使用该库：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Running LightGBM on the GPU
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在GPU上运行LightGBM
- en: 'Running training code on the GPU is straightforward. We set the device parameter
    to either `gpu` or `cuda`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上运行训练代码很简单。我们将设备参数设置为`gpu`或`cuda`：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As shown in the preceding code, we turn off LightGBM’s sparse matrix optimization
    by setting `is_enable_sparse` to `False`. LightGBM’s sparse features are not supported
    on GPU devices. Further, depending on your dataset, you might get the following
    warning stating that `multi_logloss` is not implemented:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们通过将`is_enable_sparse`设置为`False`来关闭LightGBM的稀疏矩阵优化。LightGBM的稀疏特性在GPU设备上不受支持。此外，根据你的数据集，你可能会收到以下警告，指出`multi_logloss`未实现：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Notably, the fallback performed is only for evaluation and not training; training
    is still performed on the GPU. We can validate that the GPU is used by checking
    `nvidia-smi` (for NVIDIA GPUs):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这里执行的回退操作仅用于评估，而不是训练；训练仍然在GPU上执行。我们可以通过检查`nvidia-smi`（针对NVIDIA GPU）来验证是否使用了GPU：
- en: '![Figure 11.3 – nvidia-smi output while LightGBM training is running (as we
    can see, the GPU utilization is at 40%)](img/B16690_11_03.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – 当LightGBM正在训练时nvidia-smi的输出（如图所示，GPU利用率达到40%）](img/B16690_11_03.jpg)'
- en: Figure 11.3 – nvidia-smi output while LightGBM training is running (as we can
    see, the GPU utilization is at 40%)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 当LightGBM正在训练时nvidia-smi的输出（如图所示，GPU利用率达到40%）
- en: The speed-up achieved depends on your GPU. The training time was reduced from
    171 s to 11 s (a 15-times speed-up) for 150 iterations on the Forest Cover dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的速度提升取决于你的GPU。在Forest Cover数据集上，训练时间从171秒减少到11秒（速度提升了15倍），进行了150次迭代。
- en: 'The immense performance gain stemming from using a GPU is especially useful
    when performing parameter tuning. We can use GPU-based training with, for instance,
    Optuna to significantly accelerate the search for optimal parameters. All that’s
    needed is to move the model training in the `objective` function to the GPU device.
    When defining the objective function, we specify our Optuna parameter ranges as
    per usual:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU带来的巨大性能提升在执行参数调整时特别有用。我们可以使用Optuna等基于GPU的训练来显著加速对最优参数的搜索。所需做的只是将模型训练移动到`objective`函数中的GPU设备。在定义目标函数时，我们按照常规指定我们的Optuna参数范围：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We then create the model with the Optuna parameters and make sure to specify
    the device as `cuda` (or `gpu`):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用Optuna参数创建模型，并确保指定设备为`cuda`（或`gpu`）：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The last part of the objective function is to return the cross-validated scores:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数的最后部分是返回交叉验证的分数：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can then run a parameter study as we would normally:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以像平常一样运行参数研究：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Importantly, we also set the `n_jobs` parameter to `1` here, as running parallel
    jobs leveraging the GPU could cause unnecessary contention and overhead.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们还在这里将`n_jobs`参数设置为`1`，因为利用GPU运行并行作业可能会引起不必要的竞争和开销。
- en: 'There are a few noteworthy best practices for getting the best performance
    when training on a GPU:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GPU进行训练以获得最佳性能时，有一些值得注意的最佳实践：
- en: Always verify that the GPU is being used. LightGBM returns to CPU training if
    the GPU is unavailable despite setting `device=gpu`. A good way of checking is
    with a tool such as `nvidia-smi`, as shown previously, or comparing training times
    to reference benchmarks.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是验证GPU是否正在使用。即使设置了`device=gpu`，如果GPU不可用，LightGBM会回退到CPU训练。一个检查的好方法是使用如`nvidia-smi`之类的工具，如之前所示，或者将训练时间与参考基准进行比较。
- en: Use a much smaller `max_bin` size. Large datasets reduce the impact of a smaller
    `max_bin` size, and the smaller number of bins benefits training on the GPU. Similarly,
    use single-precision floats for increased performance if your GPU supports it.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更小的`max_bin`大小。大型数据集减少了较小`max_bin`大小的影响，而更少的bins有助于GPU上的训练。同样，如果你的GPU支持，可以使用单精度浮点数以增加性能。
- en: GPU training works best for large, dense datasets. Data needs to be moved to
    the GPU’s VRAM for training, and if the dataset is too small, the overhead involved
    with moving the data is too significant.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU训练最适合大型密集数据集。数据需要移动到GPU的VRAM中进行训练，如果数据集太小，移动数据涉及的开销就太显著了。
- en: Avoid one-hot encoding of feature columns, as this leads to sparse feature matrices,
    which do not work well on the GPU.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免对特征列进行one-hot编码，因为这会导致稀疏特征矩阵，这在GPU上表现不佳。
- en: This concludes our section on how to use the GPU with LightGBM. Although the
    setup might be more complex, GPUs offer a significant boost in training speed
    due to their ability to handle thousands of threads simultaneously, allowing for
    efficient parallel processing, especially with large datasets. The massive parallelism
    of GPUs is particularly beneficial for histogram-based algorithms in LightGBM,
    making operations such as building histograms more efficient and effective.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容总结了如何使用LightGBM与GPU结合。尽管设置可能更复杂，但由于GPU能够同时处理数千个线程，因此GPU提供了显著的训练速度提升，尤其是在处理大型数据集时，能够实现高效的并行处理。GPU的巨大并行性对于LightGBM中的基于直方图的算法尤其有益，使得如构建直方图等操作更加高效和有效。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed two ways of accelerating computing with LightGBM.
    The first is large-scale distributed training across many machines using the Python
    library Dask. We showed how to set up a Dask cluster, how data can be distributed
    to the cluster using the Dask DataFrame, and how to run LightGBM on the cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了两种使用LightGBM加速计算的方法。第一种是使用Python库Dask在多台机器上进行大规模分布式训练。我们展示了如何设置Dask集群，如何使用Dask
    DataFrame将数据分发到集群，以及如何在集群上运行LightGBM。
- en: Second, we also looked at how to leverage the GPU with LightGBM. Notably, the
    GPU setup is complex, but significant speed-up can be achieved when it’s available.
    We also discussed some best practices for training LightGBM models on the GPU.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种，我们还探讨了如何利用GPU与LightGBM结合。值得注意的是，GPU设置很复杂，但一旦可用，就能实现显著的加速。我们还讨论了在GPU上训练LightGBM模型的一些最佳实践。
- en: References
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| *[**1]* | *H. Zhang, S. Si, and C.-J. Hsieh, GPU-acceleration for Large-scale
    Tree* *Boosting, 2017.* |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| *[**1**] | *张华，史思，和蔡家驹，大型树提升的GPU加速，2017.* |'
