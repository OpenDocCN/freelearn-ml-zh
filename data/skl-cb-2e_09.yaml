- en: Tree Algorithms and Ensembles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树算法与集成方法
- en: 'In this chapter we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Doing basic classifications with decision trees
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行基本分类
- en: Visualizing a decision tree with pydot
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pydot 可视化决策树
- en: Tuning a decision tree
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整决策树
- en: Using decision trees for regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树进行回归
- en: Reducing overfitting with cross-validation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证减少过拟合
- en: Implementing random forest regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现随机森林回归
- en: Bagging regression with nearest neighbor
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最近邻法进行包外回归
- en: Tuning gradient boosting trees
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整梯度提升树
- en: Tuning an AdaBoost regressor
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整 AdaBoost 回归器
- en: Writing a stacking aggregator with scikit-learn
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 编写堆叠聚合器
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we focus on decision trees and ensemble algorithms. Decision
    algorithms are easy to interpret and visualize as they are outlines of the decision
    making process we are familiar with. Ensembles can be partially interpreted and
    visualized, but they have many parts (base estimators), so we cannot always read
    them easily.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论决策树和集成算法。决策算法容易解释和可视化，因为它们是我们熟悉的决策过程的概述。集成方法可以部分解释和可视化，但它们包含许多部分（基础估计器），因此我们不能总是轻松地读取它们。
- en: 'The goal of ensemble learning is that several estimators can work better than
    a single one. There are two families of ensemble methods implemented in scikit-learn:
    averaging methods and boosting methods. Averaging methods (random forest, bagging,
    extra trees) reduce variance by averaging the predictions of several estimators.
    Boosting methods (gradient boost and AdaBoost) reduce bias by sequential building
    base estimators with the goal of reducing the bias of the whole ensemble.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的目标是多个估计器比单个估计器表现更好。scikit-learn 中实现了两种集成方法：平均方法和提升方法。平均方法（如随机森林、包外法、额外树）通过平均多个估计器的预测来减少方差。提升方法（如梯度提升和
    AdaBoost）通过依次构建基础估计器来减少偏差，从而减少整个集成方法的偏差。
- en: A common characteristic of many ensemble constructions is using randomness to
    build predictors. Random forest, for example, uses randomness (as its name implies),
    and we will use a search through many model parameters using randomness. Use the
    ideas of randomness in this chapter to build on them at work, reduce the computational
    cost, and produce better-scoring algorithms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多集成方法的共同特点是使用随机性来构建预测器。例如，随机森林使用随机性（正如其名字所示），我们也将通过许多模型参数的搜索来利用随机性。本章中的随机性思路可以帮助你在工作中降低计算成本，并生成更高分的算法。
- en: We finish the chapter with a stacking aggregator, which is an ensemble of potentially
    very different models. Part of the data analysis in stacking is taking predictions
    of several machine learning algorithms as input.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后介绍了一个堆叠聚合器，它是一个可能非常不同模型的集成方法。堆叠中的数据分析部分是将多个机器学习算法的预测作为输入。
- en: A lot of data science is computationally intensive. If possible, use a multi-core
    computer. Throughout, there is a parameter called `n_jobs` set to `-1`, which
    utilizes all of your computer's cores.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很多数据科学任务计算量很大。如果可能的话，使用多核计算机。在整个过程中，有一个名为 `n_jobs` 的参数设置为 `-1`，它会利用计算机的所有核心。
- en: Doing basic classifications with decision trees
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行基本分类
- en: Here, we perform basic classification with decision trees. Decision trees for
    classification are sequences of decisions that determine a classification, or
    a categorical outcome. Additionally, the decision tree can be examined in SQL
    by other individuals within the same company looking at the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用决策树进行基本分类。决策树用于分类时，是一系列决策，用于确定分类结果或类别结果。此外，决策树可以通过 SQL 由同一公司内的其他人员查看数据来进行检查。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Start by loading the iris dataset once again and dividing the data into training
    and testing sets:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 重新加载鸢尾花数据集并将数据分为训练集和测试集：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Import the decision tree classifier and train it on the training set:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入决策树分类器并在训练集上训练它：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then measure the accuracy on the test set:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后在测试集上测量准确度：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The decision tree appears to be accurate. Let's examine it further.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树看起来很准确。让我们进一步检查它。
- en: Visualizing a decision tree with pydot
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pydot 可视化决策树
- en: If you would like to produce graphs, install the `pydot` library. Unfortunately,
    for Windows this installation could be non-trivial. Please focus on looking at
    the graphs rather than reproducing them if you struggle to install `pydot`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想生成图表，请安装 `pydot` 库。不幸的是，对于 Windows 来说，这个安装可能会比较复杂。如果您在安装 `pydot` 时遇到困难，请专注于查看图表，而不是重现它们。
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Within an IPython Notebook, perform several imports and type the following
    script:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 IPython Notebook 中，执行多个导入并键入以下脚本：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/7a0cd685-1f72-4a14-a1d9-540000655e0e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a0cd685-1f72-4a14-a1d9-540000655e0e.png)'
- en: How it works...
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This is the decision tree that was produced with the training; calling the
    `fit` method on `X_train` and `y_train`. Look at it closely, starting at the top
    of the tree. You have 105 samples in the training set. The training set is split
    into three sets of 35 each: *value = [35, 35, 35]*. Explicitly, these are 35 Setosa,
    35 Versicolor, and 35 Virginica flowers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过训练生成的决策树；通过在 `X_train` 和 `y_train` 上调用 `fit` 方法来得到的。仔细查看它，从树的顶部开始。您在训练集中有
    105 个样本。训练集被分成三组，每组 35 个：*value = [35, 35, 35]*。具体来说，这些是 35 个 Setosa、35 个 Versicolor
    和 35 个 Virginica 花：
- en: '![](img/7757355c-4e94-45fc-b93c-339e725fbeee.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7757355c-4e94-45fc-b93c-339e725fbeee.png)'
- en: The first decision is whether the petal length of the flower is less than or
    equal to 2.45\. If the answer is true, or yes, the flower is classified as being
    in the first category, *value = [35, 0, 0]*. The flower is classified as being
    a Setosa flower. In several examples of the iris dataset classification, this
    one was the easiest to classify.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个决策是花瓣长度是否小于或等于 2.45。如果答案为真，或者是，花朵将被分类为第一类，*value = [35, 0, 0]*。该花被分类为 Setosa
    类别。在鸢尾花数据集分类的多个示例中，这是最容易分类的一个。
- en: Otherwise, if the petal length is greater than 2.45, the first decision leads
    to a smaller decision tree. The smaller decision tree only has flowers of the
    last two types, Versicolour and Virginica, and the *value = [0, 35, 35]*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果花瓣长度大于 2.45，第一个决策将导致一个较小的决策树。这个较小的决策树仅包含最后两类花：Versicolour 和 Virginica，*value
    = [0, 35, 35]*。
- en: The algorithm proceeds to produce a complete tree of four levels, depth 4 (note
    that the top node is not included in counting the levels). With formal language,
    the three nodes characterizing a decision in the picture are called a **split**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 算法继续生成一个四层的完整树，深度为 4（注意，最上层节点不算在层数之内）。用正式语言来说，图中表现决策的三个节点被称为**分裂**。
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You might wonder what the gini reference is within the visualization of the
    decision tree. Gini refers to the gini function, which measures the quality of
    a split, with three nodes representing a decision. When the algorithm runs, a
    few splits that optimize the gini function are considered. The split that produces
    the best gini impurity measure is chosen.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道在决策树的可视化中，gini 参考是什么。Gini 指的是 gini 函数，它衡量分裂的质量，三个节点表示一个决策。当算法运行时，考虑了优化
    gini 函数的几个分裂。选择产生最佳 gini 不纯度度量的分裂。
- en: 'Another option is to measure entropy to determine how to split the tree. You
    can try both options and determine which is best using cross-validation. Change
    the criterion in the decision tree as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是测量熵来确定如何分割树。您可以尝试这两种选择，并通过交叉验证确定哪种效果最好。按如下方式更改决策树中的标准：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This leads to the following diagram of the tree:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下决策树图：
- en: '![](img/e4ace225-9883-4ac8-8f24-e5ebc5898b95.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4ace225-9883-4ac8-8f24-e5ebc5898b95.png)'
- en: You can examine how this criterion performs under cross-validation using `GridSearchCV`
    and vary the criterion parameter in the parameter grid. We will do this in the
    next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `GridSearchCV` 检查此标准在交叉验证下的表现，并在参数网格中更改标准参数。我们将在下一节中进行此操作。
- en: Tuning a decision tree
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整决策树
- en: We will continue to explore the iris dataset further by focusing on the first
    two features (sepal length and sepal width), optimizing the decision tree, and
    creating some visualizations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续深入探索鸢尾花数据集，重点关注前两个特征（花萼长度和花萼宽度），优化决策树并创建一些可视化图表。
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Load the iris dataset, focusing on the first two features. Additionally, split
    the data into training and testing sets:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载鸢尾花数据集，专注于前两个特征。并将数据分为训练集和测试集：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'View the data with pandas:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 查看数据：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/9ced7bac-4256-4fee-a792-e3b1bb470662.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ced7bac-4256-4fee-a792-e3b1bb470662.png)'
- en: 'Before optimizing the decision tree, let''s try a single decision tree with
    default parameters. Instantiate and train a decision tree:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在优化决策树之前，我们先试试一个默认参数的单一决策树。实例化并训练一个决策树：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Measure the accuracy score:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量准确度分数：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Visualizing the tree with `graphviz` reveals a very complex tree with many
    nodes and levels (the image is for representational purposes only: it is OK if
    you cannot read it! It is a very deep tree with lots of overfitting!):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`graphviz`可视化树，揭示了一棵非常复杂的树，包含许多节点和层级（这张图片仅供参考：如果你看不懂也没关系！它是一棵非常深的树，存在很多过拟合！）：
- en: '![](img/0efa05a9-d506-4952-a84f-518c4df1f233.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0efa05a9-d506-4952-a84f-518c4df1f233.png)'
- en: This is a case of overfitting. The decision tree is very elaborate. The whole
    iris dataset consists of 150 samples, and a very complex tree is undesirable.
    Recall that in previous chapters we have used linear SVMs, which split space in
    a simple way with a few straight lines.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是过拟合的一个例子。决策树非常复杂。整个鸢尾数据集由150个样本组成，而一个非常复杂的树是不可取的。回想一下，在之前的章节中，我们使用了线性SVM，它通过几条直线简单地划分空间。
- en: 'Before continuing, visualize the training data points using matplotlib:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，使用matplotlib可视化训练数据点：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/4ad4b828-df97-4c91-a8d9-74837a60dacc.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ad4b828-df97-4c91-a8d9-74837a60dacc.png)'
- en: How to do it...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'To optimize the decision tree''s performance, use `GridSearchCV`. Start by
    instantiating a decision tree:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了优化决策树的性能，使用`GridSearchCV`。首先实例化一个决策树：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, instantiate and train `GridSearchCV`:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，实例化并训练`GridSearchCV`：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note how in the parameter grid, `param_grid`, we vary the split scoring criterion
    between `gini` and `entropy` and vary the `max_depth` of a tree.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在参数网格`param_grid`中，我们将分割评分标准在`gini`和`entropy`之间变化，并调整树的`max_depth`。
- en: 'Now try to score the accuracy on the test set:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在尝试在测试集上评分准确度：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The accuracy improved slightly. Let's look at `GridSearchCV` more closely.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度略有提高。让我们更详细地看看`GridSearchCV`。
- en: 'View the scores of all the decision trees tried in the grid search:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看网格搜索中尝试的所有决策树的分数：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that this method will be unavailable in future versions of scikit-learn.
    Feel free to use `zip(gs_inst.cv_results_['mean_test_score'],gs_inst.cv_results_['params'])`
    to produce similar results.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种方法将在未来版本的scikit-learn中不可用。你可以使用`zip(gs_inst.cv_results_['mean_test_score'],gs_inst.cv_results_['params'])`来产生类似的结果。
- en: 'From this list of scores, you can see that deeper trees perform worse than
    shallow trees. In detail, the data in the training set is split into five parts.
    Training occurs in four parts while testing happens in one of the five parts.
    Very deep trees overfit: they perform well on the training sets, but on the five
    testing sets of the cross-validation, they perform badly.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个分数列表中，你可以看到较深的树表现得比浅层的树差。详细来说，训练集中的数据被分为五部分，训练发生在四部分中，而测试发生在五部分中的一部分。非常深的树会过拟合：它们在训练集上表现很好，但在交叉验证的五个测试集上表现很差。
- en: 'Select the best performing tree with the `best_estimator_` attribute:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`best_estimator_`属性选择表现最好的树：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Visualize the tree with `graphviz`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`graphviz`可视化树：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/159231ba-c3cb-4edc-9af6-7bf024922a4f.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/159231ba-c3cb-4edc-9af6-7bf024922a4f.png)'
- en: There's more...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'For additional insight, we will create an additional visualization. Start by
    creating a NumPy mesh grid as follows:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了获得更多的洞察，我们将创建一个额外的可视化。首先按如下方式创建一个NumPy网格：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Using the `best_estimator_` attribute in the grid search, predict the scenarios
    on the NumPy grid that was just created:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格搜索中的`best_estimator_`属性，预测刚刚创建的NumPy网格上的场景：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Look at the visualization:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下可视化结果：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/f77dc2dd-a3ae-4675-96f4-90964fdb18b9.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f77dc2dd-a3ae-4675-96f4-90964fdb18b9.png)'
- en: 'Using this type of visualization, you can see that decision trees try to construct
    rectangles to classify the type of iris flower. Every split creates a line perpendicular
    to one of the features. In the following graph there is a vertical line depicting
    the first decision, whether sepal length is greater (right of the line) or less
    than (left of the line) the number 5.45\. Typing `plt.axvline(x = 5.45, color=''black'')`
    with the preceding code yields the following result:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这种类型的可视化，你可以看到决策树尝试构建矩形来分类鸢尾花的种类。每个分割都会创建一条与某个特征垂直的线。在下面的图中，有一条垂直线表示第一个决策，是否花萼长度大于（线的右边）或小于（线的左边）5.45。键入`plt.axvline(x
    = 5.45, color='black')`与前面的代码一起，会得到如下结果：
- en: '![](img/9c2c6061-57fb-45ab-8eb1-af1d5a8e6848.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c2c6061-57fb-45ab-8eb1-af1d5a8e6848.png)'
- en: 'Visualize the first three lines:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化前三行：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/b9258f26-c92f-4f73-9efa-ae817d71e8f1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9258f26-c92f-4f73-9efa-ae817d71e8f1.png)'
- en: The horizontal line, `sepal_width = 2.8`, is shorter and ends at `x = 5.45`
    because it does not apply to the case of *sepal_length >= 5.45*. In the end, several
    rectangular regions are created.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 水平线 `sepal_width = 2.8` 比较短，并且结束于 `x = 5.45`，因为它不适用于 *sepal_length >= 5.45*
    的情况。最终，多个矩形区域被创建。
- en: 'The following graph shows the same type of visualization applied to the very
    large decision tree that overfits. The decision tree classifier attempts to place
    a rectangle around many specific samples of the iris dataset, which shows how
    it generalizes poorly with new samples:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下图展示了应用于过拟合的非常大的决策树的相同类型的可视化。决策树分类器试图将一个矩形框住鸢尾花数据集中许多特定样本，这显示了它在面对新样本时的泛化能力差：
- en: '![](img/a71c1714-82ac-4921-aa81-2175af88ac99.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a71c1714-82ac-4921-aa81-2175af88ac99.png)'
- en: 'Finally, you could also plot how max depth influences the cross-validation
    score. Script a grid search with a max depth range from 2 to 51:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你还可以绘制最大深度如何影响交叉验证得分。编写一个网格搜索脚本，设置最大深度范围从 2 到 51：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/893f5599-9588-48ed-b239-2366de550aac.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/893f5599-9588-48ed-b239-2366de550aac.png)'
- en: The plot shows, from a different perspective, that a higher max depth tends
    to decrease the cross-validation score.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该图从另一个角度展示了，较高的最大深度倾向于降低交叉验证得分。
- en: Using decision trees for regression
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行回归
- en: 'Decision trees for regression are very similar to decision trees for classification.
    The procedure for developing a regression model consists of four parts:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树和分类树非常相似。开发回归模型的过程包括四个部分：
- en: Load the dataset
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Split the set into training/testing subsets
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集/测试集
- en: Instantiate a decision tree regressor and train it
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化一个决策树回归器并训练它
- en: Score the model on the test subset
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试子集上评分模型
- en: Getting ready
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this example, load scikit-learn''s diabetes dataset:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，加载 scikit-learn 的糖尿病数据集：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that we have loaded the dataset, we must split the data into training and
    testing subsets. Before doing that, however, visualize the target variable using
    pandas:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了数据集，必须将数据分为训练集和测试集。在此之前，使用 pandas 可视化目标变量：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/3bc25907-7251-40db-9078-684918673037.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bc25907-7251-40db-9078-684918673037.png)'
- en: 'This is a regression example, and we cannot use `stratify=y` when splitting
    the dataset. Instead, we will bin the target variable: we will keep track of whether
    the target variable is less than 50, or between 50 and 100, and so on.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个回归示例，我们在拆分数据集时不能使用 `stratify=y`。相反，我们将对目标变量进行分箱：我们将记录目标变量是否小于50，或在50到100之间，等等。
- en: 'Create bins of width 50:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 创建宽度为50的区间：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Using `np.digitize`, bin the target variable:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `np.digitize` 对目标变量进行分箱：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Visualize the `binned_y` variable with pandas:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 可视化 `binned_y` 变量：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/bd0bf5d3-85e4-41ab-b633-2d7d0a022fcc.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd0bf5d3-85e4-41ab-b633-2d7d0a022fcc.png)'
- en: 'The NumPy array `binned_y` keeps track of which bin each element of `y` belongs
    to. Now, split the set into training and testing sets and stratify the `binned_y`
    array:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组 `binned_y` 记录了每个 `y` 元素所属的区间。现在，将数据集拆分为训练集和测试集，并对 `binned_y` 数组进行分层：
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How to do it...
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To create a decision tree regressor, instantiate the decision tree and train
    it:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了创建一个决策树回归器，实例化决策树并训练它：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To measure the model''s accuracy, make predictions for the target variable
    using the test set:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了衡量模型的准确性，使用测试集对目标变量进行预测：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Use an error metric to compare `y_test` (ground truth) and `y_pred` (model
    predictions). Here, use the `mean_absolute_error`, which is the average of the
    absolute value of the differences between the elements of `y_test` and `y_pred`:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用误差度量比较 `y_test`（真实值）和 `y_pred`（模型预测）。这里使用 `mean_absolute_error`，它是 `y_test`
    和 `y_pred` 之间差异的绝对值的平均值：
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As an alternative, measure the mean absolute percentage error, which is the
    average of the absolute value of the differences divided by the size of elements
    of the ground truth. This measures the magnitude of the error relative to the
    size of the element of the ground truth:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为替代方案，衡量平均绝对百分比误差，它是绝对值差异的平均值，差异被除以真实值元素的大小。这衡量了相对于真实值元素大小的误差幅度：
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Thus, we have established a baseline of performance with regard to the diabetes
    dataset. Every change to the model will possibly affect the error measurements.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经建立了关于糖尿病数据集的性能基准。模型的任何变化都可能影响误差度量。
- en: There's more...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'With pandas, you can quickly visualize the distribution of the errors. Turn
    the difference between the ground truth, `y_test`, and the predictions, `y_pred`,
    into a histogram:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas，你可以快速可视化误差的分布。将真实值`y_test`和预测值`y_pred`之间的差异转换为直方图：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/9e7ef642-acbc-4a03-823e-aad3a466bbc8.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e7ef642-acbc-4a03-823e-aad3a466bbc8.png)'
- en: 'You can do the same for the percentage error:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以对百分比误差进行同样的操作：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](img/3ff52f44-dd66-481d-a32a-d5fcbfa8effa.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ff52f44-dd66-481d-a32a-d5fcbfa8effa.png)'
- en: 'Finally, using code from previous sections, look at the tree of decisions itself.
    Note that we did not optimize for max depth:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用前面部分的代码，查看决策树本身。注意，我们并未对最大深度进行优化：
- en: '![](img/8050a13b-249b-4d5d-9498-f20fa1f80cdd.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8050a13b-249b-4d5d-9498-f20fa1f80cdd.png)'
- en: The tree is very elaborate and very likely to overfit.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树非常复杂，很可能会发生过拟合。
- en: Reducing overfitting with cross-validation
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证减少过拟合
- en: 'Here, we will use cross-validation on the diabetes dataset from the previous
    recipe to improve performance. Start by loading the dataset, as in the previous
    recipe:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用前面食谱中的糖尿病数据集进行交叉验证，以提高性能。首先加载数据集，如前面的食谱中所示：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How to do it...
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Use grid search to reduce overfitting. Import a decision tree and instantiate
    it:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格搜索来减少过拟合。导入决策树并实例化它：
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, import `GridSearchCV` and instantiate this class:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，导入`GridSearchCV`并实例化该类：
- en: '[PRE35]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'View the best estimator with the `best_estimator_` attribute:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看使用`best_estimator_`属性的最佳估计器：
- en: '[PRE36]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The best estimator has `max_depth` of `3`. Now check the error metrics:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最佳估计器的`max_depth`为`3`。现在检查误差指标：
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Check the mean percentage error:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查平均百分比误差：
- en: '[PRE38]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There's more...
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Finally, visualize the best regression tree with `graphviz`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`graphviz`可视化最佳回归树：
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/fabe4e5a-52da-425e-93e4-d28b1f202181.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fabe4e5a-52da-425e-93e4-d28b1f202181.png)'
- en: The tree has a better accuracy metrics and has been cross-validated to minimize
    overfitting.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 该树具有更好的准确度指标，并且已经通过交叉验证以最小化过拟合。
- en: Implementing random forest regression
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现随机森林回归
- en: Random forests is an ensemble algorithm. Ensemble algorithms use several algorithms
    together to improve predictions. Scikit-learn has several ensemble algorithms,
    most of which use trees to predict. Let's start by expanding on decision tree
    regression with several decision trees working together in a random forest.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成算法。集成算法将多种算法结合使用以提高预测准确性。Scikit-learn有几种集成算法，大多数都使用树来预测。让我们从扩展决策树回归开始，使用多棵决策树在随机森林中共同工作。
- en: A random forest is a mixture of several decision trees, where each tree provides
    a single vote toward the final prediction. The final random forest calculates
    a final output by averaging the results of all the trees it is composed of.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是由多个决策树组成的混合体，每棵树都为最终预测提供一个投票。最终的随机森林通过平均所有树的结果来计算最终输出。
- en: Getting ready
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Load the diabetes regression dataset as we did with decision trees. Split all
    of the data into training and testing sets:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们之前使用决策树一样加载糖尿病回归数据集。将所有数据分成训练集和测试集：
- en: '[PRE40]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s dive in and import and instantiate a random forest. Train the random
    forest:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们深入了解并导入并实例化一个随机森林。训练这个随机森林：
- en: '[PRE41]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Measure prediction error. Try the random forest on the test set:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量预测误差。尝试在测试集上运行随机森林：
- en: '[PRE42]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The errors have gone down slightly compared to a single decision tree.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与单棵决策树相比，误差略有下降。
- en: 'To access any of the trees that make up the random forest, use the `estimators_`
    attribute:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要访问构成随机森林的任何一棵树，请使用`estimators_`属性：
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To view the first tree on the list in `graphviz`, refer to the first element
    in the list, `rft.estimators_[0]`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在`graphviz`中查看列表中的第一棵树，请参考列表中的第一个元素`rft.estimators_[0]`：
- en: '[PRE44]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/78e9e1e5-64ef-446d-9643-e599d5302e4f.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78e9e1e5-64ef-446d-9643-e599d5302e4f.png)'
- en: To view the second tree, use `best_rft.estimators_[1]`. To view the last tree,
    use `best_rft.estimators_[9]` because there are, by default, 10 trees, indexed
    0 to 9, that make up the random forest.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看第二棵树，请使用`best_rft.estimators_[1]`。要查看最后一棵树，请使用`best_rft.estimators_[9]`，因为默认情况下，有10棵树，索引从0到9，这些树构成了随机森林。
- en: 'An additional feature of the random forest is determining feature importance
    through the `feature_importances_` attribute:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机森林的一个附加特性是通过`feature_importances_`属性来确定特征的重要性：
- en: '[PRE45]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You can visualize feature importance as well:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以可视化特征的重要性：
- en: '[PRE46]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![](img/8d795ad3-455b-4d75-9b22-dc44156e6c5d.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d795ad3-455b-4d75-9b22-dc44156e6c5d.png)'
- en: The most influential features are **body mass index** (**BMI**), followed by
    `bl_4` (the fourth of six blood serum measurements), and then average blood pressure.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最具影响力的特征是**体重指数**（**BMI**），其次是`bl_4`（六项血清测量中的第四项），然后是平均血压。
- en: Bagging regression with nearest neighbors
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近邻的Bagging回归
- en: Bagging is an additional ensemble type that, interestingly, does not necessarily
    involve trees. It builds several instances of a base estimator acting on random
    subsets of the first training set. In this section, we try **k-nearest neighbors**
    (**KNN**) as the base estimator.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging是一种附加的集成方法，有趣的是，它不一定涉及决策树。它在第一个训练集的随机子集上构建多个基础估计器实例。在本节中，我们尝试将**k-最近邻**（**KNN**）作为基础估计器。
- en: Pragmatically, bagging estimators are great for reducing the variance of a complex
    base estimator, for example, a decision tree with many levels. On the other hand,
    boosting reduces the bias of weak models, such as decision trees of very few levels,
    or linear models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，bagging估计器对于减少复杂基础估计器（例如，具有多个层次的决策树）的方差非常有效。另一方面，boosting通过减少弱模型的偏差（例如，层次较少的决策树或线性模型）来提升性能。
- en: 'To try out bagging, we will find the best parameters, a hyperparameter search,
    using scikit-learn''s random grid search. As we have done previously, we will
    go through the following process:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尝试bagging，我们将使用scikit-learn的随机网格搜索来寻找最佳参数，即超参数搜索。像之前一样，我们将进行以下流程：
- en: Figure out which parameters to optimize in the algorithm (these are the parameters
    researchers view as the best to optimize in the literature).
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定在算法中需要优化的参数（这些是研究人员在文献中认为最值得优化的参数）。
- en: Create a parameter distribution where the most important parameters are varied.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个参数分布，其中最重要的参数会发生变化。
- en: Perform a random grid search. If you're using an ensemble, keep the number of
    estimators low at first.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行随机网格搜索。如果你使用的是集成方法，开始时保持较低的估计器数量。
- en: Use the best parameters from the previous step with many estimators.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一阶段的最佳参数和多个估计器。
- en: Getting ready
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Once more, load the diabetes dataset used in the last section:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 再次加载上一节中使用的糖尿病数据集：
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'First, import `BaggingRegressor` and `KNeighborsRegressor`. Additionally, also
    import `RandomizedSearchCV`:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`BaggingRegressor`和`KNeighborsRegressor`。另外，还需要导入`RandomizedSearchCV`：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Then, set up a parameter distribution for the grid search. For a bagging meta-estimator,
    some parameters to vary include `max_samples`, `max_features`, `oob_score`, and
    the number of estimators, `n_estimators`. The number of estimators is set to a
    low number, 100, to optimize the other parameters before trying a large number
    of estimators.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，为网格搜索设置一个参数分布。对于bagging元估计器，一些要变化的参数包括`max_samples`、`max_features`、`oob_score`和估计器数量`n_estimators`。估计器的数量初始设置为较低的100，以便在尝试大量估计器之前优化其他参数。
- en: 'Additionally, there is one list of parameters for the KNN algorithm. It is
    named `base_estimator__n_neighbors`, where `n_neighbors` is the internal name
    within the KNN class. The `base_estimator` name is the name of the base estimator
    within the `BaggingRegressor` class. The `base_estimator__n_neighbors` list has
    the numbers `3` and `5`, which refer to the number of neighbors in the nearest
    neighbors algorithm:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，还有一个KNN算法的参数列表。它被命名为`base_estimator__n_neighbors`，其中`n_neighbors`是KNN类中的内部名称。`base_estimator`是`BaggingRegressor`类中基础估计器的名称。`base_estimator__n_neighbors`列表包含数字`3`和`5`，它们表示最近邻算法中的邻居数量：
- en: '[PRE49]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Instantiate the `KNeighboursRegressor` class and pass it as the `base_estimator`
    within `BaggingRegressor`:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`KNeighboursRegressor`类，并将其作为`BaggingRegressor`中的`base_estimator`：
- en: '[PRE50]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, instantiate and run a randomized search. Do a few iterations, `n_iter
    = 5`, as this could be time consuming:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，实例化并运行一个随机搜索。进行几次迭代，`n_iter = 5`，因为这可能会耗时较长：
- en: '[PRE51]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Look at the best parameters in the random search run:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看随机搜索运行中的最佳参数：
- en: '[PRE52]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Train a `BaggingRegressor` using the best parameters, except for `n_estimators`,
    which you can increase. We increase the number of estimators to 1,000 in this
    case:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳参数训练`BaggingRegressor`，除了`n_estimators`，你可以增加它。在这种情况下，我们将估计器的数量增加到1,000：
- en: '[PRE53]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Finally, measure the performance on a test set. The algorithm does not perform
    as well as others, but we can possibly use it as part of a stacking aggregator
    later:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在测试集上评估算法的表现。虽然该算法的表现不如其他算法，但我们可以在后续的堆叠聚合器中使用它：
- en: '[PRE54]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: If you look carefully, bagging regression performed slightly better than the
    random forest in the previous section as both mean absolute error and mean absolute
    percentage error are better. Always remember that you do not have to limit your
    ensemble learning to trees—here, you build an ensemble regressor with the KNN
    algorithm.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细观察，你会发现，在上一节中，袋装回归的表现稍微优于随机森林，因为无论是平均绝对误差还是平均绝对百分误差都更好。始终记住，你不必将集成学习仅限于树形结构——在这里，你可以使用KNN算法构建一个集成回归器。
- en: Tuning gradient boosting trees
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整梯度提升树
- en: 'We will examine the California housing dataset with gradient boosting trees.
    Our overall approach will be the same as before:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用梯度提升树来分析加利福尼亚住房数据集。我们整体的方法与之前相同：
- en: 'Focus on important parameters in the gradient boosting algorithm:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关注梯度提升算法中的重要参数：
- en: '`max_features`'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`'
- en: '`max_depth`'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`'
- en: '`min_samples_leaf`'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`'
- en: '`learning_rate`'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`'
- en: '`loss`'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`'
- en: Create a parameter distribution where the most important parameters are varied.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个参数分布，其中最重要的参数会有所变化。
- en: Perform a random grid search. If using an ensemble, keep the number of estimators
    low at first.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行随机网格搜索。如果使用集成方法，开始时保持估计器的数量较低。
- en: Use the best parameters from the previous step with many estimators.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一阶段的最佳参数和更多估计器进行训练。
- en: Getting ready
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Load the California housing dataset and split the loaded dataset into training
    and testing sets:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 加载加利福尼亚住房数据集，并将加载的数据集分为训练集和测试集：
- en: '[PRE55]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: How to do it...
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Load the gradient boosting algorithm and random grid search:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载梯度提升算法和随机网格搜索：
- en: '[PRE56]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Create a parameter distribution for the gradient boosting trees:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为梯度提升树创建一个参数分布：
- en: '[PRE57]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Run the grid search to find the best parameters. Perform a randomized search
    with 30 iterations:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行网格搜索以找到最佳参数。进行30次迭代的随机搜索：
- en: '[PRE58]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now look at the report in dataframe form. The functions to view the report
    have been wrapped so that they can be used more times:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在以数据框形式查看报告。查看报告的函数已经封装，可以多次使用：
- en: '[PRE59]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'View the dataframe that shows how gradient boosting trees performed with various
    parameter settings:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看展示梯度提升树在不同参数设置下表现的数据框：
- en: '[PRE60]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![](img/9bef6af7-a648-455f-b522-bc4791f310b1.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bef6af7-a648-455f-b522-bc4791f310b1.png)'
- en: 'From this dataframe; `ls` outperforms `huber` significantly as a loss function,
    `3` is the best `min_samples_leaf` (but `4` could perform well), `3` is the best
    `max_depth` (although `1` or `2` could work as well), `0.3` works well as a learning
    rate (so could `0.2` or `0.4` though), and a `max_features` of `1.0` works well,
    but so could some other number (such as half of the features: `0.5`).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个数据框来看；`ls`作为损失函数明显优于`huber`，`3`是最佳的`min_samples_leaf`（但`4`也可能表现不错），`3`是最佳的`max_depth`（尽管`1`或`2`也能有效），`0.3`作为学习率效果很好（`0.2`或`0.4`也可以），`max_features`为`1.0`效果良好，但也可以是其他数字（如特征的一半：`0.5`）。
- en: 'With this information, try another randomized search:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些信息，尝试另一次随机搜索：
- en: '[PRE61]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'View the new report that is generated:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看生成的新报告：
- en: '[PRE62]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '![](img/5f3155e2-5fdb-4ac0-804d-311617170495.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f3155e2-5fdb-4ac0-804d-311617170495.png)'
- en: 'With this information, you can run one more randomized search with the following
    parameter distributions:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些信息，你可以使用以下参数分布再进行一次随机搜索：
- en: '[PRE63]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Storing the result under `rs_gbt`, perform training one last time with 4,000
    estimators:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果存储在`rs_gbt`中，并使用4,000个估计器最后一次进行训练：
- en: '[PRE64]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Use scikit-learn''s `metrics` module to describe the errors on the test set:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn的`metrics`模块描述测试集上的误差：
- en: '[PRE65]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: If you recall, the R-squared for the random forest was slightly lower at 0.8252\.
    This algorithm was slightly better. For both, we performed randomized searches.
    Note that if you perform hyperparameter optimization with trees frequently, you
    can automate the multiple randomized parameter searches.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，随机森林的R平方值略低，为0.8252。这种算法稍微好一些。对于两者，我们都进行了随机化搜索。请注意，如果你频繁地进行树的超参数优化，可以自动化多个随机化参数搜索。
- en: There's more...
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Now, we will optimize a gradient boosting classifier instead of a regressor.
    The procedure is very similar.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将优化一个梯度提升分类器，而不是回归器。过程非常相似。
- en: Finding the best parameters of a gradient boosting classifier
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找梯度提升分类器的最佳参数
- en: 'Classifying using gradient boosting trees is very similar to the regression
    we have been doing. Again, we will do the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度提升树进行分类与我们之前做的回归非常相似。我们将再次执行以下操作：
- en: 'Find the best parameters of the gradient boosting classifier. These are the
    same as the gradient boosting regressor, with the exception that the loss function
    options are different. The parameters have the same names and are as follows:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找梯度提升分类器的最佳参数。这些参数与梯度提升回归器相同，不同之处在于损失函数选项有所不同。参数名称相同，具体如下：
- en: '`max_features`'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`'
- en: '`max_depth`'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`'
- en: '`min_samples_leaf`'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`'
- en: '`learning_rate`'
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`'
- en: '`loss`'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`'
- en: 'Run an estimator with the best parameter but more trees in the estimator. In
    the following code, note the change in the loss function called deviance. To do
    the classification, we will use a binary variable. Recall the visualization of
    the target set, `y`:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳参数运行估计器，但在估计器中使用更多的树。在下面的代码中，请注意损失函数（称为 deviance）的变化。为了进行分类，我们将使用一个二元变量。回顾目标集
    `y` 的可视化：
- en: '[PRE66]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '![](img/3acb7f7a-af30-4270-b9cf-b08d20cf0f52.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3acb7f7a-af30-4270-b9cf-b08d20cf0f52.png)'
- en: 'On the far right, there seems to be an anomaly: a lot of values in the distribution
    are equal to five. Perhaps we would like to separate that set and analyze it separately.
    As part of that process, we might want to be able to predetermine whether a point
    should belong to the anomaly set or not. We will build a classifier to separate
    points where `y` is equal to or greater than five:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在最右侧，似乎存在异常：分布中的许多值等于五。也许我们想将该数据集分开并单独分析。作为这个过程的一部分，我们可能希望能够预先确定一个点是否应该属于异常集。我们将构建一个分类器，将
    `y` 大于或等于五的点分离出来：
- en: 'First, split the set into training and testing. Stratify the binned variable,
    `binned_y`:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将数据集分割为训练集和测试集。对分箱变量 `binned_y` 进行分层：
- en: '[PRE67]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Create a binary variable that has the value `1` if the target variable `y`
    is `5` or greater and `0` if it is less than `5`. Note that if the binary variable
    is `1`, it belongs to the anomalous set:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个二元变量，当目标变量 `y` 大于或等于 5 时其值为 `1`，小于 5 时为 `0`。注意，如果二元变量为 `1`，则表示它属于异常集：
- en: '[PRE68]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, use the shape of `X_train` to split a binary variable into `y_train_binned`
    and `y_test_binned`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用 `X_train` 的形状将二元变量分割为 `y_train_binned` 和 `y_test_binned`：
- en: '[PRE69]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Perform a randomized grid search:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行随机网格搜索：
- en: '[PRE70]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'View the best parameters:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看最佳参数：
- en: '[PRE71]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Increase the number of estimators and train the final estimator:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加估计器数量并训练最终的估计器：
- en: '[PRE72]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'View the performance of the algorithm:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看算法的性能：
- en: '[PRE73]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The algorithm, a binary classifier, is about 94% accurate at determining whether
    the house belongs to the anomalous set. The hyperparameter optimization of the
    gradient boosting classifier was very similar, with the same important parameters
    as gradient boosting regression.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法是一个二分类器，准确率大约为 94%，能够判断房屋是否属于异常集。梯度提升分类器的超参数优化非常相似，具有与梯度提升回归相同的重要参数。
- en: Tuning an AdaBoost regressor
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整 AdaBoost 回归器
- en: The important parameters to vary in an AdaBoost regressor are `learning_rate`
    and `loss`. As with the previous algorithms, we will perform a randomized parameter
    search to find the best scores that the algorithm can do.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AdaBoost 回归器中，重要的调节参数是 `learning_rate` 和 `loss`。与之前的算法一样，我们将执行随机参数搜索，以找到该算法能达到的最佳分数。
- en: How to do it...
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Import the algorithm and randomized grid search. Try a randomized parameter
    distribution:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入算法和随机网格搜索。尝试一个随机的参数分布：
- en: '[PRE74]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'View the best parameters:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看最佳参数：
- en: '[PRE75]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'These suggest another randomized search with parameter distribution:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些表明进行另一次带有参数分布的随机搜索：
- en: '[PRE76]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Copy the dictionary that holds the best parameters. Increase the number of
    estimators in the copy to 3,000:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制包含最佳参数的字典。将副本中的估计器数量增加到 3,000：
- en: '[PRE77]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Train the final AdaBoost model:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练最终的 AdaBoost 模型：
- en: '[PRE78]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Measure the model performance on the test set:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上衡量模型性能：
- en: '[PRE79]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Unfortunately, this model clearly underperforms relative to the other tree models.
    We will set it aside without optimizing it any further because it would take more
    training time and Python development time.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个模型明显不如其他树模型表现得好。我们将暂时搁置它，不再进一步优化，因为这样做会增加更多的训练时间和 Python 开发时间。
- en: There's more...
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We have found the best parameters for a few algorithms. Here is a table summarizing
    the parameters to optimize for each algorithm under cross-validation. It is suggested
    you start optimizing these parameters:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了几个算法的最佳参数。下面是一个表格，总结了每个算法在交叉验证下需要优化的参数。建议您从优化这些参数开始：
- en: '![](img/01baf86e-5bbd-4420-8088-94e40924376c.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01baf86e-5bbd-4420-8088-94e40924376c.png)'
- en: Writing a stacking aggregator with scikit-learn
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 编写堆叠聚合器
- en: In this section, we will write a stacking aggregator with scikit-learn. A stacking
    aggregator mixes models of potentially very different types. Many of the ensemble
    algorithms we have seen mix models of the same type, usually decision trees.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 scikit-learn 编写一个堆叠聚合器。堆叠聚合器将可能非常不同类型的模型进行混合。我们看到的许多集成算法混合的是同类型的模型，通常是决策树。
- en: The fundamental process in the stacking aggregator is that we use the predictions
    of several machine learning algorithms as input for the training of another machine
    learning algorithm.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠聚合器中的基本过程是，我们使用几个机器学习算法的预测作为训练另一个机器学习算法的输入。
- en: In more detail, we train two or more machine learning algorithms using a pair
    of `X` and `y` sets (`X_1`, `y_1`). Then we make predictions on a second `X` set
    (`X_stack`), `y_pred_1`, `y_pred_2`, and so on.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，我们使用一对 `X` 和 `y` 集合（`X_1`，`y_1`）训练两个或多个机器学习算法。然后我们在第二个 `X` 集（`X_stack`）上做出预测，得到
    `y_pred_1`、`y_pred_2` 等。
- en: These predictions, `y_pred_1` and `y_pred_2`, become inputs to a machine learning
    algorithm with the training output `y_stack`. Finally, the error can be measured
    on a third input set, `X_3`, and a ground truth set, `y_3`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测，`y_pred_1` 和 `y_pred_2`，成为一个机器学习算法的输入，训练输出为 `y_stack`。最后，可以在第三个输入集 `X_3`
    和真实标签集 `y_3` 上衡量错误。
- en: It will be easier to see in an example.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个示例中会更容易理解。
- en: How to do it...
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Load the data from the California housing dataset once again. Observe how we
    create bins once more to stratify a continuous variable:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次从加利福尼亚住房数据集加载数据。观察我们如何再次创建分箱，以便对一个连续变量进行分层：
- en: '[PRE80]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now split the pair, `X` and `y`, into three `X` and `y` pairs, input and output,
    by using `train_test_split` twice. Note how we stratify the continuous variable
    at each stage:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在通过使用 `train_test_split` 两次，将一对 `X` 和 `y` 拆分为三个 `X` 和 `y` 对，输入和输出。注意我们在每个阶段如何对连续变量进行分层：
- en: '[PRE81]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Using `RandomizedSearchCV`, find the best parameters for the first of the algorithms
    in the stacking aggregator, in this case a bagging algorithm of several nearest
    neighbor models:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `RandomizedSearchCV` 查找堆叠聚合器中第一个算法的最佳参数，在此例中是多个最近邻模型的袋装算法：
- en: '[PRE82]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Using the best parameters, train the bagging regressor using many estimators,
    in this case, 3,000:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳参数，训练一个使用多个估算器的袋装回归器，在此例中为 3,000 个估算器：
- en: '[PRE83]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Do the same process for the gradient boost algorithm on the `X_1`, `y_1` pair
    of sets:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 `X_1` 和 `y_1` 集合进行梯度提升算法的相同处理：
- en: '[PRE84]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Train the best parameter set with more estimators:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更多估算器训练最佳参数集：
- en: '[PRE85]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Predict the target using `X_stack` using both algorithms:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `X_stack` 和两个算法预测目标：
- en: '[PRE86]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'View the metrics (error rates) that each algorithm produces. View the metrics
    for the bagging regressor:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看每个算法产生的指标（错误率）。查看袋装回归器的指标：
- en: '[PRE87]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'View the metrics for gradient boost:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看梯度提升的指标：
- en: '[PRE88]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Create a dataframe of the predictions from both algorithms. Alternatively,
    you could also create a NumPy array of the data:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含两个算法预测的 DataFrame。或者，你也可以创建一个 NumPy 数组来存储数据：
- en: '[PRE89]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'View the new dataframe of predictions:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看新的预测数据框：
- en: '[PRE90]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '>![](img/e48f9f14-cee9-4ac8-8199-782a1444fc27.png)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '>![](img/e48f9f14-cee9-4ac8-8199-782a1444fc27.png)'
- en: 'Look at the correlation between the prediction columns. The columns are correlated,
    but not perfectly. The ideal situation is that the algorithms are not perfectly
    correlated and both perform well. In this case, the bagging regressor does not
    perform nearly as well as gradient boost:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看预测列之间的相关性。这些列是相关的，但不是完全相关。理想的情况是算法之间没有完全相关，并且两个算法都表现良好。在这种情况下，袋装回归器的表现远不如梯度提升：
- en: '[PRE91]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '![](img/e1501268-e7c9-4955-a014-8489c424be18.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1501268-e7c9-4955-a014-8489c424be18.png)'
- en: 'Now do a randomized search with a third algorithm. This algorithm takes as
    input the predictions of the first two. We will use an extra trees regressor to
    make predictions on the predictions of the other two algorithms:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在使用第三个算法进行随机搜索。这个算法将前两个算法的预测作为输入。我们将使用额外的树回归器对其他两个算法的预测进行预测：
- en: '[PRE92]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Copy the parameter dictionary and increase the number of estimators within
    that copied dictionary. View the final dictionary, if you want to:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制参数字典，并在复制的字典中增加估算器的数量。如果你想查看最终的字典，可以查看：
- en: '[PRE93]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Train the extra trees regressor on the predictions dataframe using `y_stack` as
    a target:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预测数据框上训练额外的树回归器，使用 `y_stack` 作为目标：
- en: '[PRE94]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'To examine the overall performance of the stacking aggregator, you need a function
    that takes an `X` set as input, predicts creating a dataframe using the bagging
    regressor and gradient boost, and finally predicts on those predictions:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了检查堆叠聚合器的整体性能，你需要一个函数，该函数以`X`集合为输入，通过袋装回归器和梯度提升创建一个数据框，并最终在这些预测上进行预测：
- en: '[PRE95]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Predict using `X_test_prin`, the `X` set that was left out, using the useful
    `predict_from_X_set` function we just created:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`X_test_prin`进行预测，这是我们刚刚创建的有用的`predict_from_X_set`函数，在留出的`X`集合上进行预测：
- en: '[PRE96]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Measure the performance of the model:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量模型的性能：
- en: '[PRE97]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: What now? The R-squared metric improved slightly, and we worked very hard for
    that slight improvement. What we could do next is write more robust, production-like
    code for the stacker that makes it easy to place a lot of estimators that are
    not correlated within the stacker.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来怎么办？R平方指标略有改善，我们为这一小小的提升付出了很多努力。接下来我们可以编写更健壮、更像生产环境的代码，为堆叠器添加许多不相关的估计器，使其更易于操作。
- en: Additionally, we could do feature engineering—improving the columns of the data
    using math and/or domain knowledge of the California housing industry. You can
    also try different algorithms for different inputs. Two columns, latitude and
    longitude, are well suited for random forests and other inputs could be well-modeled
    with a linear algorithm.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以进行特征工程——通过数学和/或加利福尼亚房地产行业的领域知识来改进数据列。你还可以尝试对不同的输入使用不同的算法。两个列：纬度和经度，非常适合随机森林，而其他输入可以通过线性算法来很好地建模。
- en: Thirdly, we could explore different algorithms on the dataset. For this dataset
    we focused on complex, high-variance algorithms. We could try simpler high-bias
    algorithms. These alternative algorithms could help the stacking aggregator we
    used at the end.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们可以在数据集上探索不同的算法。对于这个数据集，我们专注于复杂的、高方差的算法。我们也可以尝试一些简单的高偏差算法。这些替代算法可能有助于我们最终使用的堆叠聚合器。
- en: Finally, in regards to the stacker, you could rotate the `X_stacker` set through
    cross-validation to make the most of the training set.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于堆叠器，你可以通过交叉验证旋转`X_stacker`集合，以最大化训练集的利用率。
