- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Building ML Solutions with AWS AI Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS AI服务构建机器学习解决方案
- en: Up to this point, we have mainly focused on the skills and technologies required
    to build and deploy ML models using open-source technologies and managed ML platforms.
    To solve business problems with ML, however, you don’t always have to build, train,
    and deploy your ML models from scratch. An alternative option is to use fully
    managed AI services. AI services are fully managed APIs or applications with pre-trained
    models that perform specific ML tasks, such as object detection or sentiment analysis.
    Some AI services also allow you to train custom models with your data for a defined
    ML task, such as document classification. AI services promise to enable organizations
    to build ML-enabled solutions without requiring strong ML competencies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注构建和部署机器学习模型所需的技能和技术，这些技能和技术使用开源技术和托管机器学习平台。然而，要使用机器学习解决业务问题，你并不总是需要从头开始构建、训练和部署你的机器学习模型。另一种选择是使用完全管理的AI服务。AI服务是完全管理的API或应用程序，具有预训练的模型，可以执行特定的机器学习任务，例如目标检测或情感分析。一些AI服务还允许你使用你的数据训练自定义模型，以完成定义的机器学习任务，例如文档分类。AI服务承诺使组织能够构建不需要强大机器学习能力的机器学习解决方案。
- en: 'In this chapter, we are going to switch gears and talk about several AWS AI
    services and where they can be used in business applications. Please note that
    the focus of this chapter will not be to deep dive into individual AI services,
    as that warrants dedicated books. Instead, we will focus on ML use cases that
    can be powered by AI services, and the architecture patterns that you can use
    to deploy these AI services. After reading this chapter, you should be able to
    identify some use cases where AI services can be a good fit and know where to
    find additional resources to get a deeper understanding of these services. Specifically,
    we are going to cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将转换方向，讨论几个AWS AI服务及其在商业应用中的使用情况。请注意，本章的重点不会深入探讨单个AI服务，因为这需要专门的书籍。相反，我们将关注可以由AI服务驱动的机器学习用例，以及你可以用来部署这些AI服务的架构模式。阅读本章后，你应该能够识别一些AI服务可以很好地适用的情况，并知道在哪里找到额外的资源来更深入地了解这些服务。具体来说，我们将涵盖以下主题：
- en: What are AI services?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是AI服务？
- en: Overview of AWS AI services
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS AI服务概述
- en: Building intelligent solutions using AI services
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AI服务构建智能解决方案
- en: Designing an MLOps architecture for AI services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计AI服务的MLOps架构
- en: Hands-on lab – running ML tasks with AI services
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践实验室 - 使用AI服务运行机器学习任务
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术需求
- en: You will continue to use our AWS environment for the hands-on portion of this
    book. The associated code samples can be found at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter11)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你将继续使用我们的AWS环境来完成这本书的动手部分。相关的代码示例可以在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter11)找到。
- en: What are AI services?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是AI服务？
- en: AI services are pre-built fully managed services that perform a particular set
    of ML tasks out of the box, such as facial analysis or text analysis. The primary
    target users for AI services are application developers who want to build AI applications
    without the need to build ML models from scratch. In contrast, the target audiences
    for ML platforms are data scientists and ML engineers, who need to go through
    the full ML lifecycle to build and deploy ML models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AI服务是预先构建的完全管理的服务，可以直接执行特定的一组机器学习任务，例如面部分析或文本分析。AI服务的主要目标用户是希望构建AI应用而不需要从头开始构建机器学习模型的应用开发者。相比之下，机器学习平台的目标受众是数据科学家和机器学习工程师，他们需要经历完整的机器学习生命周期来构建和部署机器学习模型。
- en: 'For an organization, AI services mainly solve the following key challenges:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个组织来说，AI服务主要解决以下关键挑战：
- en: '**Lack of high-quality training data for ML model development**: To train high-quality
    models, you need a large amount of high-quality curated data. For many organizations,
    data poses many challenges in data sourcing, data engineering, and data labeling.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏高质量的机器学习模型开发训练数据**：要训练高质量的模型，你需要大量的高质量精选数据。对于许多组织来说，数据在数据来源、数据工程和数据标注方面都面临着许多挑战。'
- en: '**Lack of data science skills for building and deploying custom ML models**:
    Data science and ML engineering skills are scarce in the market and expensive
    to acquire.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏构建和部署定制机器学习模型的数据科学技能**：数据科学和机器学习工程技能在市场上稀缺，且获取成本高昂。'
- en: '**Slow product time-to-market**: Building and deploying custom models and engineering
    infrastructure is time-consuming. This can be a hurdle for a quick time-to-market
    product delivery goal.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品上市时间慢**：构建和部署定制模型和工程基础设施耗时较长。这可能是一个快速上市产品的障碍。'
- en: '**Undifferentiated ML capabilities**: Many ML problems can be solved using
    commodity ML capabilities that do not provide unique competitive advantages. Spending
    resources on building undifferentiated ML capabilities can be a waste of scarce
    resources.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未区分的机器学习能力**：许多机器学习问题可以使用不提供独特竞争优势的通用机器学习能力来解决。在构建未区分的机器学习能力上投入资源可能会浪费宝贵的资源。'
- en: '**System scalability challenge**: Managing scalable infrastructure to meet
    dynamic market demands and growth is an engineering challenge.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统可扩展性挑战**：管理可扩展的基础设施以满足动态市场需求和增长是一个工程挑战。'
- en: While AI services can provide a cost-effective way of building ML-enabled products
    quickly, they do come with limitations. The main limitation is the lack of customization
    flexibility for specific functional and technical requirements. AI services usually
    focus on specific ML tasks with a predefined set of algorithms, so you usually
    don’t have the flexibility to alter the functionality of AI services. With AI
    services, you normally do not have access to the underlying models, thus limiting
    your ability to deploy the model elsewhere.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AI服务可以提供一种快速构建机器学习产品的经济高效的方式，但它们确实存在局限性。主要局限性是缺乏针对特定功能和技术的定制灵活性。AI服务通常专注于具有预定义算法集的特定机器学习任务，因此您通常没有灵活性来改变AI服务的功能。使用AI服务，您通常无法访问底层模型，这限制了您在其他地方部署模型的能力。
- en: The number of offerings in AI services has grown extensively in recent years,
    and we expect this trend to continue at an accelerated pace. Let’s shift our focus
    and talk about several AWS AI services.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，AI服务的产品种类大幅增长，我们预计这一趋势将以更快的速度持续发展。让我们转变我们的关注点，谈谈几个AWS AI服务。
- en: Overview of AWS AI services
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS AI服务概述
- en: AWS provides AI services in multiple ML domains, such as text and vision, as
    well as AI services for industrial use cases such as manufacturing anomaly detection
    and predictive maintenance. In this section, we will cover a subset of AWS AI
    services. The objective of this section will not be to deep dive into individual
    services but rather to make you aware of the fundamental capabilities offered
    by these AI services. This will let you know where and how these services can
    be integrated into your applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 在多个机器学习领域提供AI服务，如文本和视觉，以及针对工业用例的AI服务，例如制造异常检测和预测性维护。在本节中，我们将介绍AWS AI服务的一部分。本节的目标不是深入探讨单个服务，而是让您了解这些AI服务提供的基本功能。这将让您了解这些服务如何以及在哪里集成到您的应用程序中。
- en: Amazon Comprehend
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Comprehend
- en: 'NLP has gained significant interest across different industries in solving
    a range of business problems, such as automatic document processing, text summarization,
    document understanding, and document management and retrieval. **Amazon Comprehend**
    is an AI service that can perform NLP analysis on unstructured text documents.
    At its core, Amazon Comprehend provides the following main capabilities:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）在解决各种商业问题方面引起了不同行业的广泛关注，例如自动文档处理、文本摘要、文档理解和文档管理及检索。**Amazon Comprehend**
    是一种能够对非结构化文本文档进行自然语言处理的AI服务。在其核心，Amazon Comprehend 提供以下主要功能：
- en: '**Entity recognition**: Entities are the who, what, where, and when of text
    analytics. Entities can be the most important parts of a sentence as they identify
    the key components in a text. Examples of entities are proper nouns such as a
    person, place, or product. Entities can be used to create document search indexes
    and identify key information or relationships across documents. Comprehend provides
    APIs (for example, `DetectEntities`) for detecting entities with its built-in
    entity recognition models. It can detect entities such as people, places, organizations,
    and dates from the input text. You can also use Comprehend to train a custom entity
    recognizer for your custom entities if the built-in models do not meet your requirements.
    To train a custom entity recognizer, you can use the `CreateEntityRecognizer`
    API with your training data in the following two formats:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实体识别**：实体是文本分析中的谁、什么、哪里和何时。实体可以是句子中最重要的部分，因为它们确定了文本中的关键组成部分。实体的例子包括人名、地名或产品等专有名词。实体可用于创建文档搜索索引并识别跨文档的关键信息或关系。Comprehend提供API（例如，`DetectEntities`），使用其内置的实体识别模型检测实体。它可以检测来自输入文本的人、地点、组织机构和日期等实体。如果内置模型不符合您的需求，您还可以使用Comprehend训练自定义实体识别器以识别您的自定义实体。要训练自定义实体识别器，您可以使用`CreateEntityRecognizer`
    API，并使用以下两种格式之一提供您的训练数据：'
- en: '**Annotation**: You provide the locations of entities (beginning and end offsets
    of target characters) in documents, along with the entity type for each pair of
    offsets. This helps Comprehend train on both the entities and the context they
    are in.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标注**：您提供文档中实体的位置（目标字符的开始和结束偏移量），以及每对偏移量对应的实体类型。这有助于Comprehend在实体及其所处的上下文中进行训练。'
- en: '**Entity list**: You provide a list of entities and their entity types in plaintext
    and Comprehend will train to detect these specific entities. You can evaluate
    the custom model using the metrics emitted by a Comprehend custom model training
    job. Example evaluation metrics include precision, recall, and F1 scores.'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**实体列表**：您提供一份包含实体及其实体类型的文本列表，Comprehend将训练以检测这些特定实体。您可以使用Comprehend自定义模型训练作业发出的指标来评估自定义模型。示例评估指标包括精确率、召回率和F1分数。'
- en: Additional details on the evaluation metrics for Comprehend can be found at
    [https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html).
    Once the model has been trained, you have the option to deploy the model behind
    a private prediction endpoint to serve predictions.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于Comprehend评估指标的更多详细信息，请参阅[https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html)。一旦模型经过训练，您可以选择将模型部署到私有预测端点后面以提供预测。
- en: '**Sentiment analysis**: Comprehend can detect sentiment in text with its `DetectSentiment`
    API. Sentiment analysis is widely used in many business use cases, such as analyzing
    customers’ sentiments in customer support calls or understanding customers’ perceptions
    of products and services in reviews.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：Comprehend可以通过其`DetectSentiment` API检测文本中的情感。情感分析在许多商业用例中得到了广泛应用，例如分析客户支持电话中的客户情感或理解客户对产品和服务在评论中的看法。'
- en: '**Topic modeling**: Topic modeling has a wide range of uses, including document
    understanding, document categorization and organization, information retrieval,
    and content recommendation. Comprehend can discover common topics among documents
    with its `StartTopicsDetectionJob` API.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题建模**：主题建模有广泛的应用，包括文档理解、文档分类和组织、信息检索和内容推荐。Comprehend可以通过其`StartTopicsDetectionJob`
    API发现文档中的共同主题。'
- en: '**Language detection**: Comprehend can detect the dominant language that’s
    used in the text with its `DetectDominantLanguage` API. This feature can help
    with use cases such as routing incoming customer support calls to the right channel
    based on the language or classifying documents by different languages.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言检测**：Comprehend可以通过其`DetectDominantLanguage` API检测文本中使用的占主导地位的语言。此功能可以帮助处理诸如根据语言将
    incoming 客户支持电话路由到正确的渠道或根据不同语言对文档进行分类等用例。'
- en: '**Syntax analysis**: Comprehend can perform **part-of-speech** (**POS**) analysis
    of sentences using its `DetectSyntax` API. Example POSes include nouns, pronouns,
    verbs, adverbs, conjunctions, and adjectives in a sentence. POS analysis can help
    with use cases such as checking for the correctness of syntax and grammar in written
    text.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法分析**：Comprehend 可以使用其 `DetectSyntax` API 对句子进行词性（**POS**）分析。示例词性包括句子中的名词、代词、动词、副词、连词和形容词。词性分析可以帮助处理诸如检查书面文本中语法和句法的正确性等用例。'
- en: '**Event detection**: Comprehend can detect a predefined list of financial events
    such as IPO, stock split, and bankruptcy. It also detects augments associated
    with events such as a person or company filing for bankruptcy. This relationship
    helps build a knowledge graph to help us understand the who-did-what of the different
    events. You can find a full list of event and augment types at [https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件检测**：Comprehend 可以检测预定义的金融事件，如首次公开募股（IPO）、股票分割和破产。它还可以检测与事件相关联的增强，例如个人或公司申请破产。这种关系有助于构建知识图谱，帮助我们理解不同事件中的谁做了什么。您可以在
    [https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html)
    找到事件和增强类型的完整列表。'
- en: '**Text classification**: You can train a custom text classifier using your
    training data with Comprehend. Comprehend lets you train multi-class and multi-label
    classifiers through its `CreateDocumentClassifier` API. Multi-class assigns a
    single label to a text, whereas multi-label assigns multiple labels to a text.
    To evaluate the performance of the custom classifier, Comprehend provides a list
    of metrics that include accuracy, recall, and F1 score. You can find the full
    list of metrics at [https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：您可以使用 Comprehend 使用您的训练数据训练自定义文本分类器。Comprehend 允许您通过其 `CreateDocumentClassifier`
    API 训练多类和多标签分类器。多类将单个标签分配给文本，而多标签则将多个标签分配给文本。为了评估自定义分类器的性能，Comprehend 提供了一系列指标，包括准确率、召回率和
    F1 分数。您可以在 [https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html)
    找到完整的指标列表。'
- en: '**Personally identifiable information (PII) detection**: You can detect PII
    within English or Spanish text documents. The PII detection process offers the
    option to either locate or redact PII entities within the text. For locating PII
    entities, real-time analysis or asynchronous batch jobs can be employed. On the
    other hand, redacting PII entities specifically requires the use of an asynchronous
    batch job.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人身份信息（PII）检测**：您可以在英语或西班牙语文档中检测 PII。PII 检测过程提供了在文本中定位或擦除 PII 实体的选项。对于定位
    PII 实体，可以使用实时分析或异步批量作业。另一方面，擦除 PII 实体特别需要使用异步批量作业。'
- en: '**Key phrase detection**: The Key Phrases functionality in Amazon Comprehend
    uses ML models to analyze the input text and extract the most significant phrases
    or topics. These key phrases can provide a concise summary or highlight the main
    ideas and concepts present in the text.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键短语检测**：Amazon Comprehend 中的关键短语功能使用机器学习模型分析输入文本并提取最显著的短语或主题。这些关键短语可以提供简洁的摘要或突出文本中存在的主题和概念。'
- en: Flywheels are a feature of Comprehend that provides a managed workflow for continuously
    improving a custom natural language processing model. A flywheel stores all model
    data and versions in an AWS-managed data lake. As new labeled datasets become
    available, you create flywheel iterations to retrain and evaluate new model versions
    using the latest data. Based on performance metrics, the best new version can
    be promoted to become the active model serving inferences. This iterative process
    allows the model accuracy to improve over time as regular model retraining incorporates
    fresh data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Flywheels 是 Comprehend 的一个功能，它提供了一个管理化的工作流程，用于持续改进自定义自然语言处理模型。飞轮存储所有模型数据和版本在一个
    AWS 管理的数据湖中。随着新的标记数据集变得可用，您创建飞轮迭代以使用最新数据重新训练和评估新的模型版本。根据性能指标，最佳的新版本可以被提升为成为活跃模型，为推理提供服务。这个迭代过程允许模型精度随着时间的推移而提高，因为常规模型重新训练结合了新鲜数据。
- en: 'Comprehend APIs can be invoked using the `boto3` library and AWS **command-line
    interface** (**CLI**). You can find a full list of supported `boto3` methods for
    Comprehend at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html).
    The following shows the Python syntax for invoking Comprehend’s entity detection
    functionality:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `boto3` 库和 AWS **命令行界面**（**CLI**）调用 Comprehend API。您可以在 [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html)
    找到支持 Comprehend 的完整 `boto3` 方法列表。以下展示了调用 Comprehend 实体检测功能的 Python 语法：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Amazon Comprehend can be a good fit for building intelligent document processing
    solutions and other NLP products. It can also serve as a good baseline tool that
    can be compared with custom NLP models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Comprehend 可以很好地用于构建智能文档处理解决方案和其他 NLP 产品。它还可以作为一个良好的基线工具，可以与自定义 NLP 模型进行比较。
- en: Amazon Textract
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Textract
- en: Many business processes, such as loan application processing, expense processing,
    and medical claim processing, require extracting text and numbers from images
    and documents. Currently, many organizations largely handle these processes manually
    and the processes can be highly time-consuming and slow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 许多业务流程，如贷款申请处理、费用处理和医疗索赔处理，需要从图像和文档中提取文本和数字。目前，许多组织主要手动处理这些流程，这些流程可能非常耗时且缓慢。
- en: '**Amazon Textract** is an **optical character recognition** (**OCR**) AI service
    that’s primarily used for extracting printed text, handwritten text, and numbers
    from images and PDF documents. Textract is normally used as a processing step
    for downstream tasks such as document analysis and data entries. The core Textract
    functionalities are as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Textract** 是一个主要用于从图像和 PDF 文档中提取打印文本、手写文本和数字的 **光学字符识别**（**OCR**）AI
    服务。Textract 通常用作下游任务（如文档分析和数据录入）的处理步骤。Textract 的核心功能如下：'
- en: '**OCR**: OCR is a computer vision task that detects and extracts text data
    from PDF documents and images. The OCR component in Textract extracts raw text
    from the input documents and provides additional structural information about
    the documents. For example, the Textract output contains hierarchical structural
    relationships for the different objects in a document such as pages, paragraphs,
    sentences, and words. Textract also captures the positional information of the
    different objects in the input document. The hierarchical structural information
    and object positional data are useful when you’re extracting specific information
    from different locations in the documents. The OCR APIs are `DetectDocumentText`
    for detecting text synchronously and `StartDocumentTextDetection` for detecting
    text asynchronously.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OCR**: OCR 是一种计算机视觉任务，用于从 PDF 文档和图像中检测和提取文本数据。Textract 中的 OCR 组件从输入文档中提取原始文本，并提供有关文档的附加结构信息。例如，Textract
    的输出包含文档中不同对象（如页面、段落、句子和单词）的分层结构关系。Textract 还捕获输入文档中不同对象的位置信息。当您从文档的不同位置提取特定信息时，分层结构信息和对象位置数据非常有用。OCR
    API 包括 `DetectDocumentText` 用于同步检测文本和 `StartDocumentTextDetection` 用于异步检测文本。'
- en: '**Table extraction**: Many documents contain tabular data structures and need
    to be processed as a table. For example, you might have an insurance claim document
    that contains a list of claim items and their details in different columns, and
    you may want to enter these claim items into a system. The table extraction component
    in Textract can extract tables and cells in the tables from a document. To use
    the table extraction feature, you can use the `AnalyzeDocument` API for synchronous
    operations and `StartDocumentAnalysis` for asynchronous operations.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格提取**: 许多文档包含表格数据结构，需要作为表格进行处理。例如，您可能有一个包含索赔项目及其详细信息列表的保险索赔文档，并且您可能希望将这些索赔项目输入到系统中。Textract
    中的表格提取组件可以从文档中提取表格和表格中的单元格。要使用表格提取功能，您可以使用 `AnalyzeDocument` API 进行同步操作，或使用 `StartDocumentAnalysis`
    进行异步操作。'
- en: '**Form extraction**: Documents such as paystubs and loan application forms
    contain many name-value pairs whose relationships need to be preserved when they’re
    processed automatically. The form extraction component in Textract can detect
    these name-value pairs and their relationships for downstream processing, such
    as entering the names in those documents into a system. The form extraction component
    shares the same `AnalyzeDocument` and `StartDocumentAnalysis` APIs as the table
    extraction component.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表单提取**：例如工资单和贷款申请表等文档包含许多需要保留其关系的名称-值对，当它们自动处理时。Textract中的表单提取组件可以检测这些名称-值对及其关系，以便进行下游处理，例如将这些文档中的名称输入到系统中。表单提取组件与表格提取组件共享相同的`AnalyzeDocument`和`StartDocumentAnalysis`
    API。'
- en: '**Signature in document analysis**: Textract can detect signature locations
    in documents, returning bounding boxes specifying signature positions and confidence
    scores. Signature detection can run independently or alongside other features
    like forms, tables, and custom queries. When combined with forms or tables, Textract
    relates detected signatures to corresponding cells or key-value pairs.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分析中的签名**：Textract可以检测文档中的签名位置，返回指定签名位置和置信度分数的边界框。签名检测可以独立运行或与其他功能（如表单、表格和自定义查询）同时运行。当与表单或表格结合使用时，Textract将检测到的签名与相应的单元格或键值对相关联。'
- en: '**Queries in document analysis:** Textract allows adding custom queries to
    extract specific information from documents. A query like “What is the applicant’s
    address?” will return just that data point from the analyzed document in a separate
    response structure.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分析中的查询**：Textract允许添加自定义查询以从文档中提取特定信息。例如，“申请人的地址是什么？”这样的查询将仅从分析文档中返回该数据点，并在单独的响应结构中返回。'
- en: Textract also has features for analyzing a specific kind of documents such as
    invoices and identity documents. For example, Textract can extract relevant data
    such as vendor and receiver contact information from an invoice or receipt without
    the need for any templates or configuration. Similarly, it can extract relevant
    information from passports, driver’s licenses, and other identity documentation
    issued by the US Government.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Textract还具备分析特定类型文档的功能，例如发票和身份证明文件。例如，Textract可以从发票或收据中提取相关数据，如供应商和收件人联系信息，而无需任何模板或配置。同样，它可以从护照、驾照和美国政府签发的其他身份证明文件中提取相关信息。
- en: The Textract APIs are supported in the `boto3` library. The following code sample
    shows how to detect text using the `boto3` library. The full list of Textract
    APIs for `boto3` can be found at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Textract API在`boto3`库中得到支持。以下代码示例展示了如何使用`boto3`库检测文本。关于`boto3`的完整Textract API列表可以在[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html)找到。
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Textract also integrates with the **Amazon Augmented AI** (**A2I**) service
    to enable human-in-the-loop workflow integration for reviewing low-confidence
    prediction results from Textract. You can find more information about the A2I
    service at [https://aws.amazon.com/augmented-ai](https://aws.amazon.com/augmented-ai).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Textract还与**Amazon Augmented AI**（**A2I**）服务集成，以实现人工审核Textract的低置信度预测结果的闭环工作流程集成。有关A2I服务的更多信息，请访问[https://aws.amazon.com/augmented-ai](https://aws.amazon.com/augmented-ai)。
- en: Amazon Rekognition
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Rekognition
- en: '**Amazon Rekognition** is a video and image analysis AI service. It supports
    a range of use cases, such as metadata extraction from images and videos, content
    moderation, and security and surveillance. The core capabilities of Rekognition
    are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Rekognition**是一个视频和图像分析AI服务。它支持一系列用例，如从图像和视频中提取元数据、内容审核以及安全和监控。Rekognition的核心功能如下：'
- en: '**Label or object detection**: Label detection can be applied to use cases
    such as media metadata extraction for search and discovery, item identification
    and counting for insurance claim processing, and brand and logo detection. Rekognition
    can detect different objects, scenes, and activities in images and videos, and
    assign labels to them such as `soccer`, `outdoor`, and `playing soccer`. For the
    common objects that are detected, it also provides bounding boxes for the objects
    to indicate their specific positions in the image or videos. To use Rekognition
    for label detection, you can call the `DetectLabels` API. If Rekognition cannot
    detect specific objects in your images, you can also train a custom label detector
    with your training data using the `CreateProject` API. Once the model has been
    trained, you have the option to deploy a private prediction endpoint using the
    `StartProjectVersion` API.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签或对象检测**：标签检测可用于媒体元数据提取以进行搜索和发现、保险索赔处理中的物品识别和计数、以及品牌和标志检测等用例。Rekognition
    可以在图像和视频中检测不同的对象、场景和活动，并为它们分配标签，如 `soccer`、`outdoor` 和 `playing soccer`。对于检测到的常见对象，它还提供了对象的边界框，以指示它们在图像或视频中的具体位置。要使用
    Rekognition 进行标签检测，您可以调用 `DetectLabels` API。如果 Rekognition 无法检测您图像中的特定对象，您还可以使用
    `CreateProject` API 使用您的训练数据训练一个自定义标签检测器。一旦模型被训练，您可以选择使用 `StartProjectVersion`
    API 部署一个私有预测端点。'
- en: '**Facial analysis and recognition**: Facial analysis and recognition are useful
    for use cases such as video surveillance and security, automatic people labeling
    in images and video for content search, and understanding demographics. Rekognition
    can identify and analyze faces in images and videos. For example, you can perform
    analysis on faces to detect gender, age, and sentiment. You can also build an
    index of faces and assign names to them. Rekognition can map a detected face to
    a face in the index if a match is found. The main APIs for facial analysis and
    recognition are `DetectFaces`, `SearchFaces`, `IndexFaces`, and `CompareFaces`.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人脸分析和识别**：人脸分析和识别在视频监控和安全、图像和视频内容搜索中的自动人物标签、以及人口统计理解等用例中非常有用。Rekognition
    可以在图像和视频中识别和分析人脸。例如，您可以对人脸进行分析以检测性别、年龄和情感。您还可以构建人脸索引并为它们分配名称。如果找到匹配项，Rekognition
    可以将检测到的人脸映射到索引中的人脸。人脸分析和识别的主要 API 包括 `DetectFaces`、`SearchFaces`、`IndexFaces`
    和 `CompareFaces`。'
- en: '**Content moderation**: Rekognition has APIs (`StartContentModeration`) for
    detecting images and videos with explicit content and scenes, such as violence.
    Organizations can use this feature to filter out inappropriate and offensive content
    before making the content available to consumers.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容审核**：Rekognition 提供用于检测包含露骨内容或场景（如暴力）的图像和视频的 API (`StartContentModeration`)。组织可以使用此功能在将内容提供给消费者之前过滤掉不适当和冒犯性的内容。'
- en: '**Short text detection**: Rekognition can detect short text in images and provide
    bounding boxes around the detected text using its `DetectText` and `StartTextDetection`
    APIs. This feature can be used to detect street names, the names of stores, and
    license plate numbers.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**短文本检测**：Rekognition 可以使用其 `DetectText` 和 `StartTextDetection` API 在图像中检测短文本，并在检测到的文本周围提供边界框。此功能可用于检测街道名称、商店名称和车牌号码。'
- en: '**Personal protection equipment (PPE) detection**: Rekognition provides a built-in
    feature for detecting PPE in images and videos using the `DetectProtectiveEquipment`
    API. This feature can be used for automated PPE compliance monitoring.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个人防护装备 (PPE) 检测**：Rekognition 提供了一个内置功能，使用 `DetectProtectiveEquipment` API
    在图像和视频中检测 PPE。此功能可用于自动 PPE 合规性监控。'
- en: '**Celebrity identification**: Rekognition also maintains a celebrity database
    that can be used for identifying known celebrities in images and videos. It has
    a list of APIs for this feature, including `RecognizeCelebrities` and `GetCelebrityInfo`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名人识别**：Rekognition 还维护一个名人数据库，可用于在图像和视频中识别知名名人。它为此功能提供了一系列 API，包括 `RecognizeCelebrities`
    和 `GetCelebrityInfo`。'
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Rekognition also has native integration with Amazon Kinesis Video, a video stream
    service from AWS. You can build solutions to detect faces in real-time video streams.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Rekognition 还与 AWS 的视频流服务 Amazon Kinesis Video 原生集成。您可以使用它构建检测实时视频流中人脸的解决方案。
- en: Amazon Transcribe
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Transcribe
- en: '**Amazon Transcribe** (**Transcribe**) is a speech-to-text AI service. It can
    be used to transcribe video and audio files and streams to text for a range of
    use cases, such as media content and meeting subtitling, call analytics, and converting
    medical conversations into electronic health records.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Transcribe**（**转录**）是一种语音转文字的AI服务。它可以用于将视频和音频文件及流转换为文本，适用于多种用例，例如媒体内容字幕、会议字幕、通话分析以及将医疗对话转换为电子健康记录。'
- en: 'Amazon Transcribe supports both real-time transcription and batch transcription
    and has the following key capabilities:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Transcribe支持实时转录和批量转录，并具有以下关键功能：
- en: '**Media transcription**: Transcribe has pre-trained models for converting media
    files or streams into text in different languages, such as English, Chinese, and
    Spanish. It also adds punctuation and capitalization to make the transcribed text
    more readable. To kick off transcription, you can use the `StartTranscriptionJob`
    and `StartMedicalTranscriptionJob` APIs for batch transcription, the `StartStreamingTranscription`
    API for streaming transcription, and the `StartMedicalStreamTranscription` API
    for streaming medical input.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**媒体转录**：Transcribe具有预训练的模型，可以将媒体文件或流转换为不同语言的文本，例如英语、中文和西班牙语。它还添加标点符号和大写字母，使转录文本更易于阅读。要启动转录，可以使用`StartTranscriptionJob`和`StartMedicalTranscriptionJob`
    API进行批量转录，使用`StartStreamingTranscription` API进行流式转录，以及使用`StartMedicalStreamTranscription`
    API进行流式医疗输入。'
- en: '**Custom models**: You can provide your training data to train custom language
    models to increase the accuracy of the transcription for industry-specific terms
    or acronyms. The API for creating custom models is `CreateLanguageModel`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义模型**：您可以提供您的训练数据来训练自定义语言模型，以提高特定行业术语或缩写的转录准确性。创建自定义模型的API是`CreateLanguageModel`。'
- en: '**Call analytics**: Transcribe provides built-in analytics capabilities for
    calls. The transcripts for calls are displayed in a turn-by-turn format. Some
    examples of supported analytics are sentiment analysis, call categorization, issue
    detection (the reason behind the call), and call characteristics (talk time, non-talk
    time, loudness, interruption). The API for starting a call analytics job is `StartCallAnalyticsJob`.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通话分析**：Transcribe为通话提供了内置的分析功能。通话的转录内容以逐段格式显示。支持的某些分析示例包括情感分析、通话分类、问题检测（通话原因）和通话特征（谈话时间、非谈话时间、音量、中断）。启动通话分析作业的API是`StartCallAnalyticsJob`。'
- en: '**Redaction**: Transcribe can automatically mask or remove sensitive **personally
    identifiable information** (**PII**) data from transcripts to preserve privacy.
    When transcribing with redaction, Transcribe replaces PII information with **[PII]**
    in the transcript. To enable redaction, you can configure the `ContentRedaction`
    parameter in the batch transcription jobs.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑**：Transcribe可以自动屏蔽或移除转录中的敏感**个人身份信息**（**PII**）数据，以保护隐私。在带有编辑的转录中，Transcribe将PII信息替换为转录中的**[PII]**。要启用编辑，可以在批量转录作业中配置`ContentRedaction`参数。'
- en: '**Subtitle**: Transcribe can generate out-of-the-box subtitle files in WebVTT
    and SubRip format to use as video subtitles. To enable subtitle file generation,
    you can configure the `Subtitles` parameter for the transcription job.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字幕**：Transcribe可以生成WebVTT和SubRip格式的字幕文件，用作视频字幕。要启用字幕文件生成，可以在转录作业中配置`Subtitles`参数。'
- en: '**Detecting toxic speech**: Transcribe Toxicity Detection leverages both audio
    and text-based cues to identify and classify voice-based toxic content across
    seven categories including sexual harassment, hate speech, threat, abuse, profanity,
    insult, and graphic.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测有害言论**：Transcribe毒性检测利用音频和基于文本的线索，识别和分类包括性骚扰、仇恨言论、威胁、滥用、粗话、侮辱和暴力在内的七个类别的基于声音的有害内容。'
- en: '**Redacting transcripts**: Redaction helps securely remove sensitive data like
    names, addresses, and account details from speech-to-text outputs before further
    processing or analytics. This preserves privacy while still allowing leveraging
    transcripts for other downstream applications.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编辑转录内容**：编辑功能有助于在进一步处理或分析之前，安全地移除语音转文字输出中的敏感数据，如姓名、地址和账户详情。这既保护了隐私，又允许利用转录内容进行其他下游应用。'
- en: '**Partitioning speakers**: In your transcription output, Amazon Transcribe
    allows you to distinguish between various speakers. The system can identify up
    to 10 distinct speakers and assigns a unique label to the text associated with
    each speaker.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区说话人**：在您的转录输出中，Amazon Transcribe 允许您区分不同的说话人。系统可以识别多达 10 个不同的说话人，并为每个说话人关联的文本分配一个唯一的标签。'
- en: '**Multi-channel transcription**: When dealing with audio containing two channels,
    you can employ channel identification to transcribe the speech from each channel
    independently.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多通道转录**：当处理包含两个通道的音频时，你可以使用通道识别来独立转录每个通道的语音。'
- en: There is also a medical-related transcription service called Amazon Transcribe
    Medical, a HIPAA-eligible **automatic speech recognition** (**ASR**) service that
    is designed specifically for transcribing medical speech. The service can automatically
    identify and transcribe medical terminology, anatomical references, medications,
    procedures, and other clinically relevant information with high accuracy. Transcribe
    Medical also supports multiple input sources, including audio files, streaming
    data, and real-time transcription for live sessions, making it a versatile solution
    for healthcare providers, medical researchers, and life sciences organizations
    to efficiently convert medical speech into text for documentation, analysis, and
    downstream applications.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个名为 Amazon Transcribe Medical 的与医疗相关的转录服务，这是一个符合 HIPAA 标准的**自动语音识别**（**ASR**）服务，专门设计用于转录医疗语音。该服务可以自动识别和转录医疗术语、解剖学参考、药物、程序和其他临床相关信息，具有高精度。Transcribe
    Medical 还支持多个输入源，包括音频文件、流数据以及实时转录用于现场会议，使其成为医疗保健提供者、医学研究人员和生命科学组织将医疗语音高效转换为文本以进行文档、分析和下游应用的灵活解决方案。
- en: 'Transcribe has a set of APIs for these different operations. The following
    code sample shows how to use the `boto3` library to kick off a transcription job:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Transcribe 提供了一套用于这些不同操作的 API。以下代码示例展示了如何使用 `boto3` 库启动一个转录作业：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can find the full list of `boto3` APIs for Transcribe at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/transcribe.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/transcribe.html).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/transcribe.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/transcribe.html)
    找到 Transcribe 的完整 `boto3` API 列表。
- en: Amazon Personalize
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Personalize
- en: 'Personalized recommendations can help you optimize user engagement and revenues
    for many businesses such as e-commerce, financial product recommendation, and
    media content delivery. **Amazon Personalize** allows you to build personalized
    recommendation models using your data. You can use Personalize as the recommendation
    engine to power product and content recommendations based on individual tastes
    and behaviors. At a high level, the Personalize service provides the following
    three core functionalities:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化推荐可以帮助您优化许多业务（如电子商务、金融产品推荐和媒体内容交付）的用户参与度和收入。**Amazon Personalize** 允许您使用自己的数据构建个性化推荐模型。您可以使用
    Personalize 作为推荐引擎，根据个人口味和行为提供产品和内容推荐。从高层次来看，Personalize 服务提供了以下三个核心功能：
- en: '**User personalization**: Predicts the items a user will interact with or explore'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户个性化**：预测用户将交互或探索的项'
- en: '**Similar items**: Computes similar items based on the co-occurrence of items
    and item metadata'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似项**：根据项的共现和项元数据计算相似项'
- en: '**Personalized re-ranking**: Re-ranks the input list of items for a given user'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化重新排序**：重新排序给定用户的输入项列表'
- en: 'Amazon Personalize does not provide pre-trained models for recommendations.
    Instead, you need to train custom models using your data with the built-in algorithms
    provided by Personalize. To train a personalized model, you need to provide three
    datasets:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Personalize 不提供用于推荐的预训练模型。相反，您需要使用 Personalize 提供的内置算法使用您的数据进行自定义模型训练。要训练一个个性化模型，您需要提供三个数据集：
- en: '**Item dataset**: The item dataset contains the attributes of the items you
    want to recommend. This dataset helps Personalize learn about the contextual information
    about the items for better recommendations. This dataset is optional.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项数据集**：项数据集包含您想要推荐的项的属性。此数据集帮助 Personalize 了解有关项的上下文信息，以提供更好的推荐。此数据集是可选的。'
- en: '**User dataset**: The user dataset contains attributes of the users. This allows
    Personalize to have a better representation of each user to provide highly personalized
    recommendations. This dataset is also optional.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户数据集**：用户数据集包含用户的属性。这允许Personalize更好地表示每个用户，以提供高度个性化的推荐。此数据集也是可选的。'
- en: '**User-item interaction dataset**: This is a required dataset, and it provides
    the historical interaction between users and items, such as viewing a movie or
    purchasing a product. Personalize uses this data to learn the behaviors of individual
    users toward different items to generate highly personalized recommendations.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户-物品交互数据集**：这是一个必需的数据集，它提供了用户和物品之间的历史交互，例如观看电影或购买产品。Personalize使用这些数据来学习单个用户对不同物品的行为，以生成高度个性化的推荐。'
- en: 'To help understand how Personalize works, let’s review some of the main Personalize
    concepts:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解Personalize是如何工作的，让我们回顾一下Personalize的一些主要概念：
- en: '**Dataset group**: A dataset group contains related datasets (item, user, and
    interaction dataset) for model training.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集组**：数据集组包含用于模型训练的相关数据集（物品、用户和交互数据集）。'
- en: '**Recipe**: A recipe is the ML algorithm that’s used for model training. Personalize
    provides multiple recipes for the three main functionalities.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配方**：配方是用于模型训练的机器学习算法。Personalize为三个主要功能提供了多个配方。'
- en: '**Solution**: A solution represents a trained Personalize model.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决方案**：解决方案代表一个训练好的Personalize模型。'
- en: '**Campaign**: A Personalize campaign is a hosted endpoint for a trained Personalize
    model to handle recommendation and ranking requests.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动**：Personalize活动是一个托管端点，用于处理训练好的Personalize模型的推荐和排名请求。'
- en: 'To train and deploy a custom model using Personalize, you must follow these
    steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Personalize训练和部署自定义模型，您必须遵循以下步骤：
- en: '**Prepare and ingest the dataset**: In this step, you prepare the dataset in
    the required format, store it in S3, and then load the dataset into Personalize.
    There are three main API actions involved in this step – `CreateDatasetGroup`,
    `CreateDataset`, and `CreateDatasetImportJob`. `CreateDatasetGroup` creates an
    empty dataset group. `CreateDataset` adds datasets (for example, item dataset,
    user dataset, and interaction dataset) to a dataset group, and `CreateDatasetImportJob`
    kicks off a data ingestion job to load data from S3 to the Personalize data repository
    for subsequent model training.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备和摄取数据集**：在这一步中，您以所需的格式准备数据集，将其存储在S3中，然后将其加载到Personalize中。这一步涉及三个主要的API操作
    - `CreateDatasetGroup`、`CreateDataset`和`CreateDatasetImportJob`。`CreateDatasetGroup`创建一个空的数据集组。`CreateDataset`将数据集（例如，物品数据集、用户数据集和交互数据集）添加到数据集组中，而`CreateDatasetImportJob`启动一个数据摄取作业，将数据从S3加载到Personalize数据存储库，以便进行后续模型训练。'
- en: '**Pick a recipe for model training**: In this step, you choose a recipe (ML
    algorithm) to use for the different model training processes. There are multiple
    recipe options available for user personalization, related items, and personalized
    ranking. You can use the `ListRecipes` API to get the full list of recipes. The
    recipes are designed for specific use cases such as next best action, trending
    and popular items, or similar items. Pick the appropriate recipes based on the
    use cases.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择模型训练的配方**：在这一步中，您选择一个配方（机器学习算法）用于不同的模型训练过程。Personalize提供了多个配方选项，用于用户个性化、相关物品和个性化排名。您可以使用`ListRecipes`
    API获取完整的配方列表。这些配方是为特定用例设计的，例如下一步操作、趋势和热门物品或相似物品。根据用例选择合适的配方。'
- en: '**Create a solution**: In this step, you configure a solution with the dataset
    group and recipe for the model training job using the `CreateSolution` API. Then,
    you use the `CreateSolutionVersion` API to kick off the training job.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建解决方案**：在这一步中，您使用`CreateSolution` API配置一个解决方案，该解决方案包含用于模型训练作业的数据集组和配方。然后，您使用`CreateSolutionVersion`
    API启动训练作业。'
- en: '**Evaluate the model**: In this step, you evaluate the model metrics and determine
    if they meet the performance target. If they do not, then consider retraining
    the model using higher-quality and/or more data. Personalize outputs several evaluation
    metrics for the trained models, such as coverage, mean reciprocal rank, precision,
    and normalized discounted accumulative gain. You can find more details about these
    metrics at [https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html).
    The performance metrics are available in the Personalize management console. You
    can also get the metrics programmatically using the `GetSolutionMetrics` API.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：在这一步中，您评估模型指标并确定它们是否达到性能目标。如果没有达到，那么考虑使用更高质量和/或更多数据重新训练模型。个性化为训练模型输出多个评估指标，例如覆盖率、平均倒数排名、精确度和归一化折现累积增益。您可以在[https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html)找到有关这些指标的更多详细信息。性能指标可在个性化管理控制台中查看。您还可以使用`GetSolutionMetrics`
    API以编程方式获取这些指标。'
- en: '**Create a campaign**: In this final step, you deploy a solution (trained model)
    into the prediction endpoint so that you can use it in your applications. To do
    this, you can use the `CreateCampaign` API. You can provide additional configurations
    such as the **minimum provisioned transaction per second** (**minProvisionedTPS**)
    throughput, as well as item exploration configuration. Item exploration configuration
    allows Personalize to show a percentage of random items to users that are not
    based on user personalization. The idea is to let users explore items that they
    have not interacted with before to gauge interest. The item exploration configuration
    is only applicable for user personalization.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建活动**：在这一最终步骤中，您将解决方案（训练模型）部署到预测端点，以便您可以在应用程序中使用它。为此，您可以使用`CreateCampaign`
    API。您可以提供额外的配置，例如**每秒最小配置事务数**（minProvisionedTPS）吞吐量，以及项目探索配置。项目探索配置允许个性化向用户展示一定比例的随机项目，这些项目不是基于用户个性化。其理念是让用户探索他们之前未互动过的项目，以衡量兴趣。项目探索配置仅适用于用户个性化。'
- en: 'You can use the Personalize management console to build Personalize solutions
    and campaigns. Alternatively, you can use `boto3` to access the `personalize`
    API. The following code sample shows the Python syntax for creating a campaign.
    You can find the full list of `boto3` APIs for Personalize at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用个性化管理控制台来构建个性化解决方案和活动。或者，您可以使用`boto3`来访问`personalize` API。以下代码示例展示了创建活动的Python语法。您可以在[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html)找到Personalize的完整`boto3`
    API列表：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Personalize also provides several advanced functionalities, such as filters,
    which allow you to remove items from your list of items based on rules. You can
    also optimize the model training using a business objective such as customer loyalty.
    This feature allows you to give recommendations that optimize a certain business
    outcome.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化还提供了一些高级功能，例如过滤器，允许您根据规则从您的项目列表中删除项目。您还可以使用业务目标（如客户忠诚度）来优化模型训练。此功能允许您提供优化特定业务结果的推荐。
- en: Amazon Lex V2
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Lex V2
- en: Conversational agents have been broadly adopted across many different industries
    to improve the user engagement experience, such as self-service customer support
    and automating IT functions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对话式代理已被广泛应用于多个不同行业，以提高用户体验，例如自助客户支持和自动化IT功能。
- en: 'Amazon Lex V2 facilitates the creation of conversational interfaces using voice
    and text for applications. It offers functionalities like **natural language understanding**
    (**NLU**) and **automatic speech recognition** (**ASR**), allowing developers
    to build user-friendly interactions. Amazon Lex V2 has the following key concepts:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Lex V2简化了使用语音和文本创建对话界面的应用开发。它提供了诸如**自然语言理解**（NLU）和**自动语音识别**（ASR）等功能，允许开发者构建用户友好的交互。Amazon
    Lex V2具有以下关键概念：
- en: '**Bot**: An automated tool designed for tasks like placing orders, hotel bookings,
    or flower orders.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人**：一种用于执行如下订单、酒店预订或花束订单等任务的自动化工具。'
- en: '**Language**: Amazon Lex V2 bots can handle multiple languages independently.
    Configurable to engage users with native expressions, the platform supports a
    variety of languages and locales.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言**：Amazon Lex V2机器人可以独立处理多种语言。可配置以使用本地表达方式与用户互动，该平台支持多种语言和地区。'
- en: '**Intent**: Representing user actions, intents are created to support related
    functionalities. For instance, an intent for ordering pizzas may include details
    like the intent name (e.g., `PlaceOrder`), sample utterances, and fulfillment
    instructions, typically executed through a Lambda function.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**意图**：代表用户操作，意图被创建以支持相关功能。例如，用于订购披萨的意图可能包括意图名称（例如，`PlaceOrder`）、示例语句和执行说明，通常通过Lambda函数执行。'
- en: '**Slot**: Intents may require parameters known as slots, such as destination
    or date, with slot types defining the expected values. It prompts users for these
    values and ensures all required information is provided before fulfilling the
    intent.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**槽位**：意图可能需要称为槽位的参数，例如目的地或日期，槽位类型定义了预期的值。它提示用户输入这些值，并在满足意图之前确保提供所有必要的信息。'
- en: '**Slot Type**: Each slot is associated with a slot type, which can be custom
    or built-in. For instance, sizes may have an enumeration of `Small`, `Medium`,
    and `Large`, while built-in types like `AMAZON.Number` handle numeric inputs.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**槽位类型**：每个槽位都与一个槽位类型相关联，可以是自定义的或内置的。例如，尺寸可能有一个`Small`、`Medium`和`Large`的枚举，而内置类型如`AMAZON.Number`则处理数值输入。'
- en: '**Version**: A version in Amazon Lex V2 represents a snapshot of the bot’s
    configuration. It allows for different versions to be used in various workflow
    stages, like development, beta deployment, or production.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本**：在Amazon Lex V2中，版本代表机器人配置的快照。它允许在不同的工作流程阶段使用不同的版本，如开发、beta部署或生产。'
- en: '**Alias**: An alias serves as a pointer to a specific version of a bot, enabling
    seamless updates for client applications. By changing the alias to point to a
    new version, all clients receive the updated functionality without requiring individual
    updates.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**别名**：别名作为指向特定机器人版本的指针，使客户端应用程序的更新无缝。通过更改别名以指向新版本，所有客户端都将接收更新的功能，而无需进行单独更新。'
- en: To build a bot, you outline the conversation flow in the Amazon Lex V2 console,
    which dynamically manages the dialog and responses. The console supports the building,
    testing, and publishing of text or voice chatbots for integration into platforms
    like mobile devices and web applications. It also integrates with AWS Lambda and
    other AWS services, enhancing connectivity to data in various applications. In
    addition to the console, you can also use bot templates and automated bot designers
    to create bots.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个机器人，您在Amazon Lex V2控制台中概述对话流程，该控制台动态管理对话和响应。控制台支持构建、测试和发布文本或语音聊天机器人，以便集成到移动设备和Web应用程序等平台。它还与AWS
    Lambda和其他AWS服务集成，增强了对各种应用程序中数据的连接性。除了控制台之外，您还可以使用机器人模板和自动机器人设计器来创建机器人。
- en: Amazon Lex V2 also utilizes the generative AI features of Amazon Bedrock to
    facilitate the development of bots. With Amazon Bedrock, you can create new bots,
    incorporating relevant intents and slot types through natural language descriptions.
    The tool automates the generation of sample utterances tailored to your bot’s
    intents. Additionally, Amazon Bedrock facilitates the creation of specialized
    intents designed to address customer inquiries.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Lex V2还利用Amazon Bedrock的生成式AI功能来促进机器人的开发。使用Amazon Bedrock，您可以通过自然语言描述创建新的机器人，包括相关的意图和槽位类型。该工具自动生成针对您的机器人意图定制的示例语句。此外，Amazon
    Bedrock还促进创建专门针对客户咨询的意图。
- en: Amazon Kendra
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Kendra
- en: '**Amazon Kendra** is a fully managed intelligent search service. It uses ML
    to understand your natural language requests and perform NLU on the target data
    sources to return the relevant information. Instead of searching for answers using
    keywords such as `IT desk location` and getting a list of documents containing
    these keywords, you can ask natural language questions such as *Where is the IT
    desk?* and get the location of the IT desk, such as *3rd floor, room 301*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Kendra**是一个完全托管的智能搜索服务。它使用机器学习来理解您的自然语言请求，并在目标数据源上执行NLU以返回相关信息。您不必使用诸如`IT
    桌位位置`之类的关键词来搜索答案并获取包含这些关键词的文档列表，而是可以提出诸如*IT 桌位在哪里？*之类的自然语言问题，并得到IT桌位的地点，例如*3楼，301室*。'
- en: 'You can use Amazon Kendra to solve several use cases. For example, you can
    use it as part of a contact center workflow where customer agents can quickly
    find the most relevant information for customer requests. You can also use it
    within an enterprise for information discovery across different data sources to
    improve productivity. At a high level, Kendra has the following key functionalities:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Amazon Kendra来解决多个用例。例如，您可以使用它作为客户服务中心工作流程的一部分，其中客户代表可以快速找到客户请求的最相关信息。您还可以在企业的不同数据源中进行信息发现，以提高生产力。在较高层次上，Kendra具有以下关键功能：
- en: '**Document reading understanding**: Kendra performs reading comprehension on
    the source document and returns the specific information requested by the user
    in their questions.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档阅读理解**：Kendra对源文档进行阅读理解，并返回用户在问题中请求的特定信息。'
- en: '**Frequently asked question (FAQ) matching**: If you provide a list of FAQs,
    Kendra can automatically match the questions to the answers in the list.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常见问题解答（FAQ）匹配**：如果您提供FAQ列表，Kendra可以自动将问题与列表中的答案相匹配。'
- en: '**Document ranking**: Kendra can return a list of documents that contain the
    relevant information for the questions asked. To return the list in the order
    of semantic relevancies, Kendra uses ML to understand the semantic meaning of
    the documents.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档排名**：Kendra可以返回包含所提问题相关信息的文档列表。为了按语义相关性顺序返回列表，Kendra使用机器学习（ML）来理解文档的语义含义。'
- en: 'To understand how Kendra works, let’s review some of the key technical Amazon
    Kendra concepts:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解Kendra是如何工作的，让我们回顾一些关键的技术Amazon Kendra概念：
- en: '**Index**: An index provides search results for the documents and FAQ lists
    that it has indexed. Kendra generates indexes for documents and FAQ lists that
    allow them to be searched.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引**：索引为已索引的文档和FAQ列表提供搜索结果。Kendra为文档和FAQ列表生成索引，以便它们可以被搜索。'
- en: '**Documents**: Documents can be structured (FAQs) and unstructured (HTML, PDFs)
    and can be indexed by the Kendra index engine.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档**：文档可以是结构化（FAQs）的，也可以是无结构化（HTML、PDFs）的，并且可以通过Kendra索引引擎进行索引。'
- en: '**Data sources**: Data sources are locations where the documents are located.
    These can be S3 locations, Amazon RDS databases, and Google Workspace drives,
    among others. Kendra has a list of built-in connectors for connecting to different
    data sources.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据源**：数据源是文档所在的位置。这些可以是S3位置、Amazon RDS数据库和Google Workspace驱动器等。Kendra有一系列内置连接器，用于连接到不同的数据源。'
- en: '**Queries**: Queries are used for getting results from indexes. Queries can
    be natural language containing criteria and filters.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**：查询用于从索引中获取结果。查询可以是包含标准和过滤器的自然语言。'
- en: '**Tags**: Tags are metadata that can be assigned to indexes, data sources,
    and FAQs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：标签是可以分配给索引、数据源和常见问题解答（FAQs）的元数据。'
- en: 'There are two main steps in setting up Kendra to perform an intelligent search
    against your documents:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 设置Kendra以对您的文档执行智能搜索主要有两个步骤：
- en: '**Generate index**: The first step is to set up an index for your documents.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成索引**：第一步是为您的文档设置索引。'
- en: '**Add documents to index**: Once the index has been created, you can add document
    sources to the index to be indexed.'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将文档添加到索引**：一旦创建了索引，您可以将文档源添加到索引中以便进行索引。'
- en: 'Once the index has been created, you use the Kendra `query()` API to get responses
    for your index with queries. The following code snippet shows the Python syntax
    for querying an index:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了索引，您就使用Kendra `query()` API通过查询来获取索引的响应。以下代码片段显示了查询索引的Python语法：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Kendra has built-in connectors for a range of data sources, so you don’t have
    to build custom code to extract data from those sources. It also has native application
    integration with Amazon Lex, which allows Lex to send user queries directly to
    a Kendra index for fulfillment.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kendra为各种数据源内置了连接器，因此您无需构建自定义代码来从这些源提取数据。它还与Amazon Lex具有原生应用程序集成，允许Lex直接将用户查询发送到Kendra索引以进行满足。
- en: Kendra is being increasingly used along with large language models to provide
    a better user experience and accuracy for intelligent enterprise search solutions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kendra正越来越多地与大型语言模型结合使用，以提供更好的用户体验和准确性，用于智能企业搜索解决方案。
- en: Amazon Q
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Q
- en: Amazon Q is a generative AI-powered service designed to be an assistant tailored
    for various business needs and developer tasks. There are multiple sub-Q assistants
    for the different domains and services including Q for business, Q for builder,
    Q for QuickSight (an AWS business intelligence tool), and Q for Connect (a contact
    center solution). In this section, we will briefly cover Q for business as it
    is designed to help businesses connect to their own data. Business users, such
    as marketers, project and program managers, and sales representatives, can engage
    in customized conversations, address issues, create content, and execute various
    actions through Amazon Q for business. This platform is cognizant of the specific
    systems these users can access, enabling them to pose intricate and detailed queries.
    The responses they receive are tailored, ensuring that the results incorporate
    only information for which they have authorized access. To learn how Amazon Q
    for business works, check out the documentation at [https://docs.aws.amazon.com/amazonq/latest/business-use-dg/getting-started.html](https://docs.aws.amazon.com/amazonq/latest/business-use-dg/getting-started.html).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Q 是一个由生成人工智能驱动的服务，旨在为各种商业需求和开发任务提供定制助手。有多个子 Q 助手针对不同的领域和服务，包括针对商业的 Q、针对构建者的
    Q、针对 QuickSight（一个 AWS 商业智能工具）的 Q 和针对 Connect（一个联系中心解决方案）的 Q。在本节中，我们将简要介绍 Q for
    business，因为它旨在帮助商业用户连接到他们自己的数据。商业用户，如营销人员、项目和程序经理以及销售代表，可以通过 Amazon Q for business
    进行定制对话、解决问题、创建内容并执行各种操作。该平台了解这些用户可以访问的特定系统，使他们能够提出复杂和详细的查询。他们收到的回复都是定制的，确保结果仅包含他们已授权访问的信息。要了解
    Amazon Q for business 的工作原理，请查看[https://docs.aws.amazon.com/amazonq/latest/business-use-dg/getting-started.html](https://docs.aws.amazon.com/amazonq/latest/business-use-dg/getting-started.html)上的文档。
- en: Evaluating AWS AI services for ML use cases
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 AWS 人工智能服务用于机器学习用例
- en: 'To determine if an AI service is a good fit for your use cases, you need to
    evaluate it across multiple dimensions:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定人工智能服务是否适合您的用例，您需要从多个维度对其进行评估：
- en: '**Functional requirements**: Identify the functional requirements for your
    ML use cases and test whether the target AI services provide the features you
    are looking for. For example, Rekognition is a computer vision service, but it
    does not support all computer vision tasks. If you have an instance segmentation
    computer vision use case, you will have to build a model using an algorithm that
    supports it, such as Mask-RCNN.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功能需求**：确定您机器学习用例的功能需求，并测试目标人工智能服务是否提供您所需的特性。例如，Rekognition 是一种计算机视觉服务，但它并不支持所有计算机视觉任务。如果您有一个实例分割的计算机视觉用例，您将不得不使用支持该功能的算法来构建模型，例如
    Mask-RCNN。'
- en: '**Model performance against your data**: AWS AI services are trained with data
    sources to solve common use cases. To ensure the models perform well against your
    data, use your test dataset to evaluate the model metrics for your specific needs.
    If the pre-built models do not meet your performance target, then try the custom
    model building options if the services support it. If neither option works, then
    consider building custom models with your data.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能与您的数据对比**：AWS 人工智能服务使用数据源进行训练以解决常见用例。为确保模型在您的数据上表现良好，请使用您的测试数据集来评估模型指标以满足您的特定需求。如果预构建的模型未达到您的性能目标，那么如果服务支持，请尝试自定义模型构建选项。如果这两种选项都不起作用，那么考虑使用您自己的数据构建自定义模型。'
- en: '**API latency and throughput requirements**: Determine your latency and throughput
    requirements for your application and test the target AI service’s API against
    your requirements. In general, AWS AI services are designed for low latency and
    high throughput. However, you might have use cases that require extremely low
    latency, such as computer vision tasks at the edge. If the AI services cannot
    meet your requirements, then consider building models and hosting them in dedicated
    hosting infrastructure.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 延迟和吞吐量需求**：确定您应用程序的延迟和吞吐量需求，并测试目标人工智能服务的 API 是否符合您的需求。通常，AWS 人工智能服务是为低延迟和高吞吐量设计的。但是，您可能有需要极低延迟的用例，例如边缘的计算机视觉任务。如果人工智能服务无法满足您的需求，那么考虑在专用托管基础设施中构建模型并托管它们。'
- en: '**Security and integration requirements**: Determine your security and integration
    requirements and validate whether the AI services meet your requirements. For
    example, you might have custom requirements around authentication and might need
    to develop a custom integration architecture to enable support.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和集成要求**：确定您的安全和集成要求，并验证AI服务是否符合您的需求。例如，您可能对身份验证有定制要求，可能需要开发定制的集成架构以支持这些要求。'
- en: '**Model reproducibility requirements**: Since AI services manage the pre-trained
    models and ML algorithms for custom models, those models and algorithms can change
    over time. If you have strict reproducibility requirements, such as training a
    custom model using an old version of an algorithm for compliance reasons, then
    verify if the AI service provides such support before using it.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可复现性要求**：由于AI服务管理预训练模型和定制模型的ML算法，这些模型和算法可能会随时间变化。如果您有严格的可复现性要求，例如出于合规原因使用旧版本的算法来训练定制模型，那么在使用AI服务之前，请验证AI服务是否提供此类支持。'
- en: '**Cost**: Understand your usage pattern requirements and evaluate the cost
    of using the AI services. If the cost of developing and hosting a custom model
    is more cost-effective, and the operational overhead does not outweigh the cost
    benefits of a custom model, then consider the build-your-own option.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：了解您的使用模式需求并评估使用AI服务的成本。如果开发和使用定制模型的成本更有效，且运营成本没有超过定制模型带来的成本效益，那么可以考虑自行构建的选项。'
- en: There are other considerations when it comes to adopting AI services, such as
    monitoring metrics, versioning the control of APIs for audit requirements, and
    data types and volume requirements.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在采用AI服务时，还有其他考虑因素，例如监控指标、为审计要求对API版本进行控制以及数据类型和体积要求。
- en: Building intelligent solutions with AI services
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AI服务构建智能解决方案
- en: AI services can be used for building different intelligent solutions. To determine
    if you can use an AI service for your use case, you must identify the business
    and ML requirements and then evaluate if an AI service offers the functional and
    non-functional capabilities you are looking for. In this section, we will present
    several business use cases and architecture patterns that incorporate AI services.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: AI服务可用于构建不同的智能解决方案。为了确定您是否可以使用AI服务来满足您的用例，您必须确定业务和ML需求，然后评估AI服务是否提供您所需的职能和非职能能力。在本节中，我们将介绍几个包含AI服务的业务用例和架构模式。
- en: Automating loan document verification and data extraction
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化贷款文件验证和数据提取
- en: When we apply for a loan from a bank, we need to provide the bank with physical
    copies of documentation such as tax returns, pay stubs, bank statements, and photo
    ID. Upon receiving those documents, the bank needs to verify them and enter the
    information from the documents into loan application systems for further processing.
    At the time of writing, many banks still perform this verification and data extraction
    process manually, which is time-consuming and error-prone.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从银行申请贷款时，我们需要向银行提供税务申报表、工资条、银行对账单和照片身份证等文件的物理副本。在收到这些文件后，银行需要验证它们并将文件中的信息输入贷款申请系统以进行进一步处理。截至写作时，许多银行仍在手动执行此验证和数据提取过程，这既耗时又容易出错。
- en: 'To determine if you can use any AI services to solve your problem, you need
    to identify the ML problems to be solved. In this particular business workflow,
    we can identify the following ML problems:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定您是否可以使用任何AI服务来解决问题，您需要确定要解决的ML问题。在这个特定的业务流程中，我们可以确定以下ML问题：
- en: '**Document classification**: Documentation classification is an ML task where
    the documents are classified into different types, such as driver’s license, pay
    stubs, and bank statements. This process identifies the document types and ensures
    the required documents are received and can be further processed based on their
    types.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分类**：文档分类是ML任务，其中文档被分类到不同的类型，如驾照、工资条和银行对账单。此过程识别文档类型，并确保收到的所需文档可以根据其类型进行进一步处理。'
- en: '**Data extraction**: Data extraction is the task of identifying the relevant
    information from the documents and extracting it. Examples of such information
    include customer names and addresses, income information, data of birth details,
    and bank balances.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据提取**：数据提取是从文档中识别相关信息并将其提取出来的任务。此类信息的例子包括客户姓名和地址、收入信息、出生日期细节和银行余额。'
- en: 'As we have learned, these two tasks can be performed by the Comprehend and
    Textract AI services. The following diagram shows the architecture flow that incorporates
    these two services:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所学的，这两个任务可以通过Comprehend和Textract AI服务执行。以下图表显示了包含这两个服务的架构流程：
- en: '![Diagram  Description automatically generated](img/B20836_11_01.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B20836_11_01.png)'
- en: 'Figure 11.1: The loan document verification and data extraction process'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：贷款文档验证和数据提取过程
- en: In this architecture, we use a combination of Textract, Comprehend, and Amazon
    Augmented AI services to support loan document classification and the loan data
    processing flow.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在此架构中，我们使用Textract、Comprehend和Amazon Augmented AI服务的组合来支持贷款文档分类和贷款数据处理流程。
- en: Loan document classification workflow
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贷款文档分类工作流程
- en: First, we need to train a custom text classification model for classifying the
    text that appears in each type of document. Here, we will train a custom classification
    model using Comprehend. The training data for Comprehend’s custom classifier consists
    of the necessary input text and labels. Note that Comprehend has limits on the
    input text size and the maximum number of classes, and this limit can change.
    Check out the official documentation for the latest limitation details. Once the
    model has been trained, you get a private API endpoint for the classifier.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为每种文档中出现的文本训练一个自定义文本分类模型。在这里，我们将使用Comprehend训练一个自定义分类模型。Comprehend的自定义分类器的训练数据包括必要的输入文本和标签。请注意，Comprehend对输入文本大小和最大类别的数量有限制，并且这个限制可能会改变。请查看官方文档以获取最新的限制详情。一旦模型训练完成，您将获得一个用于分类器的私有API端点。
- en: 'Once the custom model has been trained and deployed, the main flow of the architecture
    is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦自定义模型训练并部署，架构的主要流程如下：
- en: '**Data extraction**: Once the documents have been received and digitized as
    images or PDFs, Textract can be used to extract text, tabular data, and form data
    from the documents. The output will be in JSON format and stored as files in S3.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据提取**：一旦文档收到并数字化为图像或PDF，可以使用Textract从文档中提取文本、表格数据和表单数据。输出将以JSON格式存储，并作为文件存储在S3中。'
- en: '**Human review**: To ensure the high accuracy of the extracted data by Textract,
    a human-in-the-loop process can be implemented to verify low-confidence predictions
    and manually correct them. This human-in-the-loop workflow can be implemented
    using the Amazon Augmented AI service.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人工审核**：为确保Textract提取的数据的高准确性，可以实施人工审核流程来验证低置信度预测并手动更正它们。此人工审核工作流程可以使用Amazon
    Augmented AI服务实现。'
- en: '**Document classification**: The JSON outputs are processed to generate classification
    prediction using the custom Comprehend model that has been trained.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文档分类**：使用已训练的自定义Comprehend模型对JSON输出进行处理，以生成分类预测。'
- en: '**Update downstream systems**: The prediction outputs are passed to downstream
    systems for further processing.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新下游系统**：将预测输出传递给下游系统进行进一步处理。'
- en: There are alternative architecture options available. For example, you can also
    treat documents as images and perform image classification using the Rekognition
    service. Another option is to train a custom model using your algorithms, such
    as LayoutLM, and prepare a training dataset with the output of Textract. It is
    prudent to validate multiple options to achieve the optimal price/performance
    trade-off when deciding on the right technology.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有可用的替代架构选项。例如，您还可以将文档视为图像，并使用Rekognition服务进行图像分类。另一个选项是使用您的算法（如LayoutLM）训练一个自定义模型，并使用Textract的输出准备一个训练数据集。在决定正确的技术时，验证多个选项以实现最佳的价格/性能权衡是明智的。
- en: Loan data processing flow
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贷款数据处理流程
- en: The loan data processing flow is concerned with processing the JSON outputs
    from the data extraction process. The JSON document contains raw text and structure
    details for the entire document, and only a subset of text is needed for downstream
    processing and storage. The processing scripts can parse the documents using the
    structures in the JSON file to identify and extract the specific data points required.
    Then, it can input those data points into the downstream databases or systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 贷款数据处理流程关注处理数据提取过程的JSON输出。JSON文档包含整个文档的原始文本和结构细节，并且只需要下游处理和存储的文本子集。处理脚本可以使用JSON文件中的结构解析文档，以识别和提取所需的具体数据点。然后，可以将这些数据点输入到下游数据库或系统中。
- en: Media processing and analysis workflow
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 媒体处理和分析工作流程
- en: The media and entertainment industry has accumulated a huge number of digital
    media assets over the years, and the growth of these new digital assets is accelerating.
    One key capability in digital asset management is search and discovery. This capability
    not only impacts the user experience but also the effective monetization of media
    content. To quickly surface the most relevant content, media companies need to
    enrich the content with metadata for indexing and searching.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，媒体和娱乐行业积累了大量的数字媒体资产，这些新数字资产的增长正在加速。数字资产管理中的一个关键能力是搜索和发现。这种能力不仅影响用户体验，还影响媒体内容的有效货币化。为了快速呈现最相关的内容，媒体公司需要通过元数据来丰富内容，以便进行索引和搜索。
- en: 'In this particular business challenge, we can identify the following ML problems:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的商业挑战中，我们可以识别以下机器学习问题：
- en: '**Speech-to-text transcription**: The audio portion of videos and audio files
    need to be transcribed into text transcripts. The transcripts can then be further
    analyzed for additional information.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音转文字转录**：视频和音频文件中的音频部分需要转录成文字脚本。然后，这些脚本可以进一步分析以获取更多信息。'
- en: '**Text NLP analysis**: NLP analysis such as entity extraction, sentiment analysis,
    and topic modeling can be performed on the transcripts.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本自然语言处理分析**：可以在脚本上执行自然语言处理分析，如实体提取、情感分析和主题建模。'
- en: '**Object/people/scene/activity detection**: Compute vision tasks can be performed
    on video frames and images to extract objects, people, scenes, and activities.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象/人物/场景/活动检测**：可以在视频帧和图像上执行计算视觉任务，以提取对象、人物、场景和活动。'
- en: 'The following diagram shows an architecture that uses Transcribe, Comprehend,
    and Rekognition to perform the identified ML tasks:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了使用Transcribe、Comprehend和Rekognition执行所识别的机器学习任务的架构：
- en: '![Graphical user interface, diagram  Description automatically generated](img/B20836_11_02.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，图表描述自动生成](img/B20836_11_02.png)'
- en: 'Figure 11.2: Media tagging and analysis architecture'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：媒体标签和分析架构
- en: In this architecture, we build a pipeline for subtitle and text analysis of
    video content, video tagging and analysis, and image tagging and analysis.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，我们为视频内容、视频标签和分析、图像标签和分析构建了一个管道。
- en: For live video sources such as broadcasting, the AWS Elemental services can
    take live broadcasting streams, process them, and store them in S3\. You can find
    more details about the Elemental services at [https://aws.amazon.com/elemental-live/](https://aws.amazon.com/elemental-live/).
    Images and video file data sources can be ingested into S3 using a variety of
    different capabilities, including S3 APIs or higher-level services such as AWS
    Transfer for **Secure File Transfer Protocol** (**SFTP**).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直播视频源，如广播，AWS Elemental服务可以接收直播流，处理它们，并将它们存储在S3中。您可以在[https://aws.amazon.com/elemental-live/](https://aws.amazon.com/elemental-live/)找到有关Elemental服务的更多详细信息。可以使用各种不同的功能将图像和视频文件数据源摄入到S3中，包括S3
    API或更高级别的服务，如用于**安全文件传输协议**（**SFTP**）的AWS Transfer。
- en: 'As there are multiple parallel processing streams in the pipeline, we can use
    AWS Step Functions to orchestrate the parallel execution of different streams.
    These can generate the following output streams:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于管道中有多个并行处理流，我们可以使用AWS Step Functions来编排不同流的并行执行。这些可以生成以下输出流：
- en: '**Subtitle and text analysis stream**: This stream primarily uses the Amazon
    Transcribe and Amazon Comprehend AI services. Transcribe transcribes the audio
    portion of the videos and generates both subtitle files and regular transcripts.
    The regular transcripts are then used by Comprehend to run text analysis. Some
    example metadata that’s extracted from this stream can include the entities of
    people and places, the language used, and sentiment for different sections of
    the transcripts.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字幕和文本分析流**：此流主要使用Amazon Transcribe和Amazon Comprehend AI服务。Transcribe转录视频的音频部分，并生成字幕文件和常规脚本。然后，常规脚本由Comprehend用于运行文本分析。从这个流中提取的一些示例元数据可以包括人物和地点的实体、使用的语言以及脚本不同部分的情感。'
- en: '**Video tagging and analysis stream**: This stream identifies objects, scenes,
    activities, people, celebrities, and text with timestamps in the different video
    frames.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频标签和分析流**：此流识别不同视频帧中的对象、场景、活动、人物、名人和带时间戳的文本。'
- en: '**Image tagging and analysis stream**: This stream identifies objects, scenes,
    activities, celebrities, and text in different images.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像标签和分析流**：此流识别不同图像中的对象、场景、活动、名人和文本。'
- en: The outputs from the media processing streams can be further processed and organized
    as useful metadata for the different media assets. Once this has been done, they
    are stored in a media metadata repository to support content search and discovery.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体处理流输出的结果可以进一步处理和组织为有用的元数据，用于不同的媒体资产。一旦完成，它们就会被存储在媒体元数据仓库中，以支持内容搜索和发现。
- en: E-commerce product recommendation
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电子商务产品推荐
- en: Product recommendation is an important capability in e-commerce. It is a key
    enabler for increasing sales, improving engagement experience, and retaining customer
    loyalty.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 产品推荐是电子商务中的一个重要功能。它是增加销售额、改善参与体验和保持客户忠诚度的关键推动力。
- en: 'In e-commerce product recommendation, multiple functional requirements can
    be framed as ML problems:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子商务产品推荐中，多个功能需求可以构建为机器学习问题：
- en: '**Recommendations based on customer behaviors and profiles**: ML algorithms
    can learn the intrinsic characteristics and purchasing patterns of customers from
    their past e-commerce interactions to predict the products they will like.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于客户行为和档案的推荐**：机器学习算法可以从客户过去的电子商务互动中学习客户的内在特征和购买模式，以预测他们可能会喜欢的商品。'
- en: '**Ability to address recommendations of cold items (items without history)**:
    ML algorithms can explore customers’ reactions toward cold items and adjust their
    recommendations to balance explore (recommending new items) and exploit (recommending
    known items).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理冷门商品推荐（无历史记录的商品）的能力**：机器学习算法可以探索客户对冷门商品的反应，并调整推荐以平衡探索（推荐新商品）和利用（推荐已知商品）。'
- en: '**Ability to recommend similar items**: ML algorithms can learn the intrinsic
    characteristics of products based on product attributes and collective interaction
    patterns from a group of customers to determine product similarity.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐相似商品的能力**：机器学习算法可以根据产品属性和一组客户的集体互动模式来学习产品的内在特征，以确定产品相似性。'
- en: 'With these functional requirements in mind, the following architecture diagram
    illustrates an e-commerce architecture that uses Amazon Personalize as the recommendation
    engine:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些功能需求，以下架构图展示了使用Amazon Personalize作为推荐引擎的电子商务架构：
- en: '![Figure 12.3 – e-commerce site and recommendation architecture ](img/B20836_11_03.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – 电子商务网站和推荐架构](img/B20836_11_03.png)'
- en: 'Figure 11.3: E-commerce site and recommendation architecture'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：电子商务网站和推荐架构
- en: In this architecture, we use Personalize as the recommendation engine to power
    both the online user experience as well as the target user marketing experience.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在此架构中，我们使用Personalize作为推荐引擎，以支持在线用户体验以及目标用户营销体验。
- en: The RDS database, DynamoDB, and Elasticsearch are the main data sources for
    item, user, and interaction data. Glue ETL jobs are used to transform the source
    data into the datasets required for Personalize solution building.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: RDS数据库、DynamoDB和Elasticsearch是商品、用户和交互数据的主要数据源。Glue ETL作业用于将源数据转换为Personalize解决方案构建所需的数据集。
- en: Once a Personalize solution has been evaluated to meet the desired criteria,
    it is deployed as a Personalize campaign to serve recommendation requests from
    customers visiting the e-commerce website.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦个性化解决方案经过评估，符合所需标准，它就会被部署为一个个性化活动，以服务于访问电子商务网站的客户的推荐请求。
- en: Amazon Pinpoint is a managed target marketing service. You can use Pinpoint
    to manage user segmentation and send email and SMS marketing campaigns. In this
    architecture, the Pinpoint service gets a list of recommended products for a group
    of target customers and sends out email or SMS campaigns to those users with personalized
    recommendations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Pinpoint是一个托管的目标营销服务。您可以使用Pinpoint来管理用户细分并发送电子邮件和短信营销活动。在此架构中，Pinpoint服务获取一组目标客户的推荐产品列表，并向这些用户发送带有个性化推荐的电子邮件或短信活动。
- en: Customer self-service automation with intelligent search
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 智能搜索实现客户自助服务自动化
- en: Good customer service boosts customer satisfaction and builds long-term customer
    loyalty. However, customer support is very labor-intensive and can result in poor
    customer satisfaction due to long waiting times and unknowledgeable support agents.
    The customer self-service capability has been widely adopted by organizations
    in different industries to deflect customer support call volumes and improve customer
    satisfaction.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 优质的客户服务可以提升客户满意度并建立长期的客户忠诚度。然而，客户支持工作非常劳动密集，可能会因为长时间的等待和知识不足的支持人员而导致客户满意度下降。客户自助服务能力已被不同行业的组织广泛采用，以减少客户支持电话量并提高客户满意度。
- en: 'In a customer self-service scenario, we can identify the following ML problems:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户自助服务场景中，我们可以识别以下机器学习问题：
- en: '**Automatic speech recognition** (**ASR**): This ML task recognizes human speech
    and converts it into text, and then uses NLU to understand the meaning of the
    text.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动语音识别**（**ASR**）：这个机器学习任务识别人类语音并将其转换为文本，然后使用NLU来理解文本的意义。'
- en: '**Natural language understanding** (**NLU**): NLU is a subfield of NLP, and
    it deals with intent understanding and reading comprehension. NLU focuses on the
    meaning and intent of the text. For example, if the text is *Can I get the cash
    balance in my savings account?*, then the intent here is *get account balance*.
    Another example of NLU is understanding the text and extracting specific information
    from it based on the semantic meaning of the question and the text.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言理解**（**NLU**）：NLU是自然语言处理（NLP）的一个子领域，它处理意图理解和阅读理解。NLU关注文本的意义和意图。例如，如果文本是*我能查看我的储蓄账户的现金余额吗？*，那么这里的意图是*获取账户余额*。另一个NLU的例子是根据问题的语义意义和文本提取特定信息。'
- en: '**Text to speech**: This ML task converts text into natural human voices.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本转语音**：这个机器学习任务将文本转换为自然的人类声音。'
- en: 'The following diagram shows a sample architecture for implementing a self-service
    chat functionality for customers to look up customer-related details, as well
    as general information and FAQs:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示展示了为顾客查找客户相关详情、一般信息和常见问题解答（FAQs）而实现自助服务聊天功能的一个示例架构：
- en: '![Figure 12.4 – Self-service chat portal with an intelligent virtual assistant
    ](img/B20836_11_04.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4 – 带有智能虚拟助手的自助服务聊天门户](img/B20836_11_04.png)'
- en: 'Figure 11.4: Self-service chat portal with an intelligent virtual assistant'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：带有智能虚拟助手的自助服务聊天门户
- en: In this architecture, an Amazon Lex bot is used to provide the text-based conversational
    interface for customer engagement. The customer uses the self-service chat portal
    to initiate the conversation and the chat portal integrates with the Lex bots
    via the Lex API.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，使用Amazon Lex机器人提供基于文本的对话界面以供客户互动。客户使用自助服务聊天门户来启动对话，聊天门户通过Lex API与Lex机器人集成。
- en: Lex bots support several different intents, such as *look up account info*,
    *update customer profile*, and *How do I return a purchase?*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Lex机器人支持多种不同的意图，例如*查找账户信息*、*更新客户资料*和*我如何退货？*。
- en: Depending on the intent, the Lex bot will route the fulfillment requests to
    a different backend. For customer account-related inquiries, it will use a Lambda
    function for fulfillment. For information search-related questions, the Lex bot
    will send the query to a Kendra index for fulfillment.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 根据意图，Lex机器人会将满足请求路由到不同的后端。对于与客户账户相关的查询，它将使用Lambda函数进行满足。对于信息搜索相关的问题，Lex机器人会将查询发送到Kendra索引进行满足。
- en: Having explored the various AI services and their practical business applications,
    the subsequent sections will focus on operational considerations related to the
    adoption of these services. This includes delving into MLOps, code promotion,
    and monitoring processes to enhance the operational efficiency of AI implementations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了各种人工智能服务和它们的实际商业应用之后，接下来的章节将重点关注采用这些服务相关的运营考虑因素。这包括深入研究MLOps、代码推广和监控流程，以提高人工智能实施的运营效率。
- en: Designing an MLOps architecture for AI services
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计人工智能服务的MLOps架构
- en: Implementing custom AI service models requires a data engineering, model training,
    and model deployment pipeline. This process is similar to the process of building,
    training, and deploying models using an ML platform. As such, we can also adopt
    MLOps practice for AI services when running them at scale.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自定义人工智能服务模型需要数据工程、模型训练和模型部署管道。这个过程类似于使用机器学习平台构建、训练和部署模型的过程。因此，当我们在大规模运行人工智能服务时，也可以采用MLOps实践。
- en: Fundamentally, MLOps for AI services intends to deliver similar benefits as
    MLOps for the ML platform, including process consistency, tooling reusability,
    reproducibility, delivery scalability, and auditability. Architecturally, we can
    implement a similar MLOps pattern for AI services.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，人工智能服务的 MLOps 的目的是提供与机器学习平台 MLOps 相似的益处，包括流程一致性、工具可重用性、可重复性、交付可扩展性和可审计性。在架构上，我们可以为人工智能服务实现类似的
    MLOps 模式。
- en: AWS account setup strategy for AI services and MLOps
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 人工智能服务和 MLOps 的账户设置策略
- en: 'To isolate the different environments, we can adopt a multi-account strategy
    for configuring the MLOps environment for AI services. The following diagram illustrates
    a design pattern for a multi-account AWS environment. Depending on your organizational
    requirements for separation of duties and control, you may also consider consolidating
    these into fewer environments:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了隔离不同的环境，我们可以采用多账户策略来配置人工智能服务的 MLOps 环境。以下图表展示了多账户 AWS 环境的设计模式。根据您对职责分离和控制的要求，您也可以考虑将这些合并为更少的环境：
- en: '![Figure 12.5 – MLOps architecture for AI services on AWS  ](img/B20836_11_05.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – AWS 人工智能服务的 MLOps 架构](img/B20836_11_05.png)'
- en: 'Figure 11.5: MLOps architecture for AI services on AWS'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：AWS 人工智能服务的 MLOps 架构
- en: In this multi-account AWS environment, developers use the custom model development
    environment to build and test the pipelines for data engineering, model training,
    and model deployment. When ready, the pipelines are promoted for formal model
    building and testing using production training data in the model development environment.
    Since trained AI services models cannot normally be exported, we will need to
    replicate the model training workflow in the production environment for model
    deployment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个多账户 AWS 环境中，开发者使用自定义模型开发环境来构建和测试数据工程、模型训练和模型部署的管道。当准备就绪时，管道将在模型开发环境中使用生产训练数据进行正式的模型构建和测试。由于训练好的
    AI 服务模型通常无法导出，我们需要在部署环境中复制模型训练工作流程以进行模型部署。
- en: 'The shared services environment hosts CI/CD tools such as AWS CodePipeline
    and AWS CodeBuild. You use the CI/CD tools to build different pipelines for data
    engineering, model building, and model deployment running in different environments.
    For example, a pipeline for the UAT environment could have the following components
    and steps:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 共享服务环境托管 CI/CD 工具，如 AWS CodePipeline 和 AWS CodeBuild。您使用 CI/CD 工具为在不同环境中运行的数据工程、模型构建和模型部署构建不同的管道。例如，UAT
    环境的管道可能包含以下组件和步骤：
- en: '**CodePipeline definition**: This definition would have a CodeBuild step, a
    CloudFormation execution step, and a Step Functions workflow execution step.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CodePipeline 定义**：此定义将包含一个 CodeBuild 步骤、一个 CloudFormation 执行步骤和一个 Step Functions
    工作流程执行步骤。'
- en: '**CodeBuild step**: The CodeBuild step enriches the CloudFormation template
    with additional inputs needed to create a Step Functions workflow that orchestrates
    data engineering, dataset creation, data ingestion, model training, and model
    deployment.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CodeBuild 步骤**：CodeBuild 步骤通过添加创建 Step Functions 工作流程所需的额外输入来丰富 CloudFormation
    模板，该工作流程协调数据工程、数据集创建、数据摄入、模型训练和模型部署。'
- en: '**CloudFormation execution step**: This step executes the CloudFormation template
    to create the Step Functions workflow.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CloudFormation 执行步骤**：此步骤执行 CloudFormation 模板以创建 Step Functions 工作流程。'
- en: '**Step Functions workflow execution step**: This step kicks off the Step Functions
    workflow to run the various steps, such as data engineering and model training,
    in the workflow. For example, if we build a Step Functions workflow for Personalize
    model training and deployment, the workflow will consist of six steps: create
    dataset group, create dataset, import dataset, create solution, create solution
    version, and create campaign.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Step Functions 工作流程执行步骤**：此步骤启动 Step Functions 工作流程以运行工作流程中的各种步骤，例如数据工程和模型训练。例如，如果我们为
    Personalize 模型训练和部署构建一个 Step Functions 工作流程，该工作流程将包括以下六个步骤：创建数据集组、创建数据集、导入数据集、创建解决方案、创建解决方案版本和创建活动。'
- en: In a multi-account environment, there could also be other purpose-built accounts
    for data management, monitoring, and security.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在多账户环境中，也可能存在其他专门用于数据管理、监控和安全的账户。
- en: Code promotion across environments
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在不同环境中进行代码推广
- en: Similar to the pattern we use for the ML platform, we can use a code repository
    as the mechanism to promote code to different environments. For example, during
    code development, a developer creates code artifacts such as data engineering
    scripts for Glue ETL jobs and CloudFormation template skeletons and builds specification
    files for CodeBuild to run different commands. Once the code is deemed ready to
    promote them for formal model building and testing, the developer checks the code
    into a release branch in the code repository. The code check-in event can trigger
    a CodePipeline job to run the CodeBuild step in the shared services and then run
    a Step Functions workflow step in the model development environment. When it is
    ready for production release, a deployment CodePipeline job can be triggered in
    the shared services environment to execute a CloudFormation template to deploy
    the model in the production environment.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们用于ML平台的模式类似，我们可以使用代码仓库作为将代码推送到不同环境的机制。例如，在代码开发期间，开发者创建代码工件，如Glue ETL作业的数据工程脚本和CloudFormation模板框架，并为CodeBuild构建运行不同命令的规范文件。一旦代码被认为准备好，可以将其提交到代码仓库的发布分支以进行正式模型构建和测试，开发者将代码检查到代码仓库的发布分支。代码检查事件可以触发CodePipeline作业在共享服务中运行CodeBuild步骤，然后在模型开发环境中运行Step
    Functions工作流程步骤。当它准备好进行生产发布时，可以在共享服务环境中触发部署CodePipeline作业以执行CloudFormation模板，在生产环境中部署模型。
- en: Monitoring operational metrics for AI services
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控AI服务的运营指标
- en: 'AI services emit operational statuses to CloudWatch. For example, Amazon Personalize
    sends metrics such as the number of successful recommendation calls or training
    job errors. Rekognition sends metrics such as successful request counts and response
    time. Alarms can be configured to send alerts when specified metrics meet a defined
    threshold. The following diagram shows a sample monitoring architecture for Amazon
    Personalize:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: AI服务向CloudWatch发送运营状态。例如，Amazon Personalize发送诸如成功推荐调用次数或训练作业错误之类的指标。Rekognition发送诸如成功请求计数和响应时间之类的指标。可以配置警报，在指定的指标达到定义的阈值时发送警报。以下图表显示了Amazon
    Personalize的示例监控架构：
- en: '![Figure 12.6 – Monitoring architecture for Amazon Personalize ](img/B20836_11_06.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – Amazon Personalize的监控架构](img/B20836_11_06.png)'
- en: 'Figure 11.6: Monitoring architecture for Amazon Personalize'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：Amazon Personalize的监控架构
- en: With this monitoring architecture, CloudWatch collects metrics from the Personalize
    service. A scheduled CloudWatch event triggers a Lambda function, which pulls
    a set of CloudWatch metrics and sends events to the EventBridge service. EventBridge
    rules can be configured to trigger Lambda functions to update Personalize configuration,
    such as updating `minProvisionedTPS` configuration for Personalize when throttling
    is detected or sending an email notification when certain errors occur.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种监控架构，CloudWatch从Personalize服务收集指标。一个计划的CloudWatch事件触发一个Lambda函数，该函数拉取一组CloudWatch指标并将事件发送到EventBridge服务。EventBridge规则可以配置为触发Lambda函数以更新Personalize配置，例如在检测到节流时更新Personalize的`minProvisionedTPS`配置，或者在发生某些错误时发送电子邮件通知。
- en: You can also adopt similar monitoring architecture patterns to other AI services,
    such as Comprehend and Rekognition.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以采用类似的监控架构模式应用于其他AI服务，例如Comprehend和Rekognition。
- en: Hands-on lab – running ML tasks using AI services
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践实验室 – 使用AI服务运行ML任务
- en: 'In this hands-on lab, you will perform a list of ML tasks using Rekognition,
    Comprehend, Textract, Personalize and Transcribe. After the lab, you will have
    developed hands-on experience with the core features of several AI services and
    how they can be used for various ML tasks. Follow these steps to get started:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实践实验室中，您将使用Rekognition、Comprehend、Textract、Personalize和Transcribe执行一系列ML任务。实验室结束后，您将获得使用几个AI服务的核心功能和它们如何用于各种ML任务的实践经验。按照以下步骤开始：
- en: Launch the SageMaker Studio profile you created in *Chapter 8*, *Building a
    Data Science Environment Using AWS ML Services*. You will create and run new notebooks
    in this profile.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动您在*第8章*，*使用AWS ML服务构建数据科学环境*中创建的SageMaker Studio配置文件。您将在该配置文件中创建和运行新的笔记本。
- en: We need to provide the new notebooks with permission to access AI services.
    To do this, find the Studio execution role for the Studio environment and attach
    the `AdministratorAccess` IAM policy to it. We will use this policy for simplicity
    here. In a controlled environment, you would need to design a policy to provide
    the specific permissions needed to access different services.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要为新笔记本提供访问 AI 服务的权限。为此，找到 Studio 环境的 Studio 执行角色，并将其附加 `AdministratorAccess`
    IAM 策略。在这里我们将使用此策略以简化操作。在一个受控环境中，您需要设计一个策略以提供访问不同服务所需的具体权限。
- en: Clone [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/)
    into your Studio environment using the `git clone https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/`
    command if you have not already done so.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您尚未这样做，请使用 `git clone https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/`
    命令将 [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/)
    克隆到您的 Studio 环境中。
- en: 'Run NLP tasks using Comprehend:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Comprehend 运行 NLP 任务：
- en: Open the `comprehend.ipynb` notebook in the `Chapter11` directory. This notebook
    performs a list of ML tasks using Comprehend, including language detection, entity
    detection, sentiment detection, PII detection, key phrase detection, and syntax
    analysis.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Chapter11` 目录中打开 `comprehend.ipynb` 笔记本。此笔记本使用 Comprehend 执行一系列 ML 任务，包括语言检测、实体检测、情感检测、PII
    检测、关键短语检测和语法分析。
- en: Create some sample text you would like to run NLP analysis on and save it as
    `comprehend_sample.txt` in the data directory.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一些您想要进行 NLP 分析的样本文本，并将其保存为 `comprehend_sample.txt` 到数据目录中。
- en: 'Run the following code in the notebook to import the library and set up the
    `boto3` client for Comprehend:'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以导入库并设置用于 Comprehend 的 `boto3` 客户端：
- en: '[PRE7]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the following code in the notebook to detect the dominant language in the
    text:'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测文本中的主导语言：
- en: '[PRE8]'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Run the following code in the notebook to detect entities:'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测实体：
- en: '[PRE9]'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Run the following code in the notebook to detect sentiment:'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测情感：
- en: '[PRE10]'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Run the following code in the notebook to detect PII entities:'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测 PII 实体：
- en: '[PRE11]'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run the following code in the notebook to detect key phrases:'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测关键短语：
- en: '[PRE12]'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the following code in the notebook to detect syntax:'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测语法：
- en: '[PRE13]'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Run an audio transcription job using Transcribe:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Transcribe 运行音频转录作业：
- en: Open the `transcribe.ipynb` notebook in the `Chapter11` directory. This notebook
    runs a transcription job using a sample audio file in the data directory.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Chapter11` 目录中打开 `transcribe.ipynb` 笔记本。此笔记本使用数据目录中的样本音频文件运行转录作业。
- en: Find a sample MP3 audio file that you would like to run transcription on and
    save it as `transcribe_sample.mp3` in the data directory.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一个您想要进行转录的样本 MP3 音频文件，并将其保存为 `transcribe_sample.mp3` 到数据目录中。
- en: 'Run the following code in the notebook to set up a `boto3` client for Transcribe:'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以设置用于 Transcribe 的 `boto3` 客户端：
- en: '[PRE14]'
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Run the following code in the notebook to create an S3 bucket for storing the
    audio file:'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以创建用于存储音频文件的 S3 存储桶：
- en: '[PRE15]'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run the following code in the notebook to kick off the transcription job:'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以启动转录作业：
- en: '[PRE16]'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Navigate to the **Transcribe** console. Under the **Transcription Jobs** section,
    you will see the newly created transcription job.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 **Transcribe** 控制台。在 **Transcription Jobs** 部分中，您将看到新创建的转录作业。
- en: Wait until the status changes to **Complete** and click on the job link; you
    will see the transcripts under the **Text** tab in the **transcription preview**
    section.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待状态变为 **完成** 并点击作业链接；您将在 **transcription preview** 部分的 **Text** 选项卡下看到转录内容。
- en: 'Run computer vision with Rekognition:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Rekognition 运行计算机视觉：
- en: Open the `rekognition.ipynb` notebook in the `Chapter11` directory. This notebook
    runs a list of text extraction tasks, including text extraction, table extraction,
    and form extraction.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Chapter11` 目录中打开 `rekognition.ipynb` 笔记本。此笔记本运行一系列文本提取任务，包括文本提取、表格提取和表单提取。
- en: Save a sample image for analysis as `textract_sample.jpeg` in the `data` directory.
    Try to use a sample image with text, tables, and forms in it.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用于分析的样本图像保存为 `textract_sample.jpeg` 到 `data` 目录中。尝试使用包含文本、表格和表单的样本图像。
- en: 'Run the following code in the notebook to set up a `boto3` client for Textract:'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以设置 Textract 的 `boto3` 客户端：
- en: '[PRE17]'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run the following code in the notebook to load the image:'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以加载图像：
- en: '[PRE18]'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Run the following code in the notebook to detect tables and forms:'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测表格和表单：
- en: '[PRE19]'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Run the following code in the notebook to detect text:'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下代码以检测文本：
- en: '[PRE20]'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Train a recommendation model using Personalize:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Personalize 训练推荐模型：
- en: Open the `personalize.ipynb` notebook in the `Chapter11` directory. This notebook
    trains a Personalize model for movie review recommendations using the movie lens
    dataset. It goes through the process of creating a dataset group/dataset, importing
    the data, building the solution, and creating a Personalize campaign.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“Chapter11”目录中打开 `personalize.ipynb` 笔记本。这个笔记本使用电影镜头数据集训练了一个用于电影评论推荐的 Personalize
    模型。它涵盖了创建数据集组/数据集、导入数据、构建解决方案和创建 Personalize 营销活动的过程。
- en: Follow the instructions in the notebook and run all the cells in sequence to
    complete all the steps.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照笔记本中的说明，依次运行所有单元格以完成所有步骤。
- en: Congratulations! You have successfully used several AWS AI services and their
    APIs. As you can see, it is quite straightforward to use AI services with pre-trained
    models to perform different ML tasks. Training a custom model using AI services
    involves some additional steps, but the underlying infrastructure and data science
    details are abstracted away to make it easy for non-data-scientists to use these
    services as well.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功使用了几个 AWS 人工智能服务和它们的 API。正如你所见，使用预训练模型执行不同的机器学习任务非常简单。使用人工智能服务训练自定义模型涉及一些额外的步骤，但底层基础设施和数据分析细节被抽象化，以便非数据科学家也能轻松使用这些服务。
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered topics surrounding AI services. We went over a list
    of AWS AI services and where they can be used to build ML solutions. We also talked
    about adopting MLOps for AI services deployment. Now, you should have a good understanding
    of what AI services are and know that you don’t need to always build custom models
    to solve ML problems. AI services provide you with a quick way to build AI-enabled
    applications when they are a good fit.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了围绕人工智能服务的话题。我们回顾了 AWS 人工智能服务列表以及它们可以用于构建机器学习解决方案的地方。我们还讨论了采用 MLOps
    进行人工智能服务部署。现在，你应该对人工智能服务有很好的理解，并知道你不必总是构建自定义模型来解决机器学习问题。人工智能服务为你提供了一种快速构建人工智能应用程序的方法，当它们是一个好的选择时。
- en: In the next chapter, we will dive deep into AI risk management, an important
    area for ML practitioners to become familiar with as it is critical to understand
    the key risks and mitigation approaches throughout the entire ML lifecycle.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨人工智能风险管理，这是机器学习从业者需要熟悉的重要领域，因为它对于理解整个机器学习生命周期中的关键风险和缓解方法至关重要。
- en: Join our community on Discord
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mlsah](https://packt.link/mlsah )'
- en: '![](img/QR_Code70205728346636561.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code70205728346636561.png)'
