- en: '*Chapter 1*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*'
- en: Introduction to Clustering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类简介
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Distinguish between supervised learning and unsupervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分监督学习与无监督学习
- en: Explain the concept of clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释聚类的概念
- en: Implement k-means clustering algorithms using built-in Python packages
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内建的 Python 包实现 k-means 聚类算法
- en: Calculate the Silhouette Score for your data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算数据的轮廓系数（Silhouette Score）
- en: In this chapter, we will have a look at the concept of clustering.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论聚类的概念。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Have you ever been asked to take a look at some data and come up empty handed?
    Maybe you were not familiar with the dataset, or maybe you didn't even know where
    to start. This may have been extremely frustrating, and even embarrassing, depending
    on who asked you to take care of the task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经被要求查看一些数据，但最终一无所获？也许你不熟悉该数据集，或者甚至不知道从哪里开始。这可能非常令人沮丧，甚至会让你感到尴尬，尤其是当是别人让你处理这个任务时。
- en: You are not alone, and, interestingly enough, there are many times the data
    itself is simply too confusing to be made sense of. As you try and figure out
    what all those numbers in your spreadsheet mean, you're most likely mimicking
    what many unsupervised algorithms do when they try to find meaning in data. The
    reality is that many datasets in the real world don't have any rhyme or reason
    to them. You will be tasked with analyzing them with little background preparation.
    Don't fret, however – this book will prepare you so that you'll never be frustrated
    again when dealing with data exploration tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你不是孤单的，实际上，数据本身很多时候也过于混乱，难以理解。你可能正在模仿很多无监督算法的做法，试图从数据中找出意义。当你试图理解电子表格中的那些数字时，可能正是这些无监督算法在做的事情。现实中，很多真实世界中的数据集并没有任何规律或合理性，你将被要求在几乎没有背景准备的情况下分析它们。不过不用担心——本书将为你提供所需的知识，以便你在处理数据探索任务时不再感到沮丧。
- en: For this book, we have developed some best-in-class content to help you understand
    how unsupervised algorithms work and where to use them. We'll cover some of the
    foundations of finding clusters in your data, how to reduce the size of your data
    so it's easier to understand, and how each of these sides of unsupervised learning
    can be applied in the real world. We hope you will come away from this book with
    a strong real-world understanding of unsupervised learning, the problems that
    it can solve, and those it cannot.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们为你开发了一些最佳内容，帮助你理解无监督算法如何工作以及如何使用它们。我们将涵盖如何在数据中寻找聚类的基础知识，如何减少数据的规模以便更容易理解，以及无监督学习的各个方面如何应用于实际世界。希望你能够通过本书，深入理解无监督学习，了解它能解决的问题以及不能解决的问题。
- en: Thanks for joining us and we hope you enjoy the ride!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的加入，祝你旅途愉快！
- en: Unsupervised Learning versus Supervised Learning
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习与监督学习的区别
- en: '**Unsupervised learning** is one of the most exciting areas of development
    in machine learning today. If you have explored machine learning bookwork before,
    you are probably familiar with the common breakout of problems in either supervised
    or unsupervised learning. **Supervised learning** encompasses the problem set
    of having a labeled dataset that can be used to either classify (for example,
    predicting smokers and non-smokers if you''re looking at a lung health dataset)
    or fit a regression line on (for example, predicting the sale price of a home
    based on how many bedrooms it has). This model most closely mirrors an intuitive
    human approach to learning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**是当前机器学习领域最令人兴奋的研究方向之一。如果你之前有研究过机器学习教材，可能已经熟悉过常见的监督学习和无监督学习问题的区分。**监督学习**包括使用带标签的数据集来进行分类（例如，在研究肺部健康数据集时预测吸烟者和非吸烟者）或拟合回归线（例如，根据房间数量预测房屋销售价格）。这种模型最接近人类直观的学习方法。'
- en: 'If you wanted to learn how to not burn your food with a basic understanding
    of cooking, you could build a dataset by putting your food on the burner and seeing
    how long it takes (input) for your food to burn (output). Eventually, as you continue
    to burn your food, you will build a mental model of when burning will occur and
    avoid it in the future. Development in supervised learning was once fast-paced
    and valuable, but it has since simmered down in recent years – many of the obstacles
    to knowing your data have already been tackled:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想通过基本的烹饪知识来学习如何不把食物烧焦，你可以通过将食物放在炉子上并观察它烧焦所需的时间（输入），来构建一个数据集。最终，随着你不断地烧焦食物，你会建立一个关于何时会烧焦的心理模型，并避免将来再次发生。监督学习的发展曾经是快速而有价值的，但近年来已经逐渐平缓——许多了解数据的障碍已经被克服：
- en: '![Figure 1.1: Differences between unsupervised and supervised learning'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：无监督学习与监督学习的区别'
- en: '](img/C12626_01_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_01.jpg)'
- en: 'Figure 1.1: Differences between unsupervised and supervised learning'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.1：无监督学习与监督学习的区别
- en: Conversely, unsupervised learning encompasses the problem set of having a tremendous
    amount of data that is unlabeled. Labeled data, in this case, would be data that
    has a supplied "target" outcome that you are trying to find the correlation to
    with supplied data (you know that you are looking for whether your food was burned
    in the preceding example). Unlabeled data is when you do not know what the "target"
    outcome is, and you only have supplied input data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，无监督学习涵盖了大量无标签数据的问题。在这种情况下，有标签的数据是指提供了“目标”结果的数据，你试图找出与提供的数据的相关性（例如，在之前的例子中你知道你在找的是食物是否被烧焦）。无标签数据是指你不知道“目标”结果是什么，只有提供的输入数据。
- en: Building upon the previous example, imagine you were just dropped on planet
    Earth with zero knowledge of how cooking works. You are given 100 days, a stove,
    and a fridge full of food without any instructions on what to do. Your initial
    exploration of a kitchen could go in infinite directions – on day 10, you may
    finally learn how to open the fridge; on day 30, you may learn that food can go
    on the stove; and after many more days, you may unwittingly make an edible meal.
    As you can see, trying to find meaning in a kitchen devoid of adequate informational
    structure leads to very noisy data that is completely irrelevant to actually preparing
    a meal.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前的例子，假设你被丢到了地球上，对烹饪一无所知。你得到了100天的时间、一台炉子和一冰箱，里面满是食物，但没有任何关于该做什么的指示。你对厨房的初步探索可能会有无数种方向——在第10天，你可能终于学会如何打开冰箱；在第30天，你可能会学到食物可以放在炉子上；而在许多天之后，你可能会不经意间做出一道可食用的餐点。正如你所看到的，试图在没有足够信息结构的厨房中找到意义，会导致产生非常嘈杂且与实际做饭完全无关的数据。
- en: Unsupervised learning can be an answer to this problem. By looking back at your
    100 days of data, **clustering** can be used to find patterns of similar days
    where a meal was produced, and you can easily review what you did on those days.
    However, unsupervised learning isn't a magical answer –simply finding clusters
    can be just as likely to help you to find pockets of similar yet ultimately useless
    data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以成为解决这个问题的答案。通过回顾你100天的数据，**聚类**可以用来寻找在某些天生产了餐点的相似模式，这样你就可以轻松回顾那些日子里你做了什么。然而，无监督学习并不是一种神奇的答案——仅仅发现聚类同样可能帮助你找到一些相似但最终无用的数据。
- en: This challenge is what makes unsupervised learning so exciting. How can we find
    smarter techniques to speed up the process of finding clusters of information
    that are beneficial to our end goals?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战正是让无监督学习如此令人兴奋的原因。我们如何才能找到更聪明的技术，加速发现对最终目标有益的信息聚类的过程？
- en: Clustering
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Being able to find groups of similar data that exist in your dataset can be
    extremely valuable if you are trying to find its underlying meaning. If you were
    a store owner and you wanted to understand which customers are more valuable without
    a set idea of what valuable is, clustering would be a great place to start to
    find patterns in your data. You may have a few high-level ideas of what denotes
    a valuable customer, but you aren't entirely sure in the face of a large mountain
    of available data. Through clustering you can find commonalities among similar
    groups in your data. If you look more deeply at a cluster of similar people, you
    may learn that everyone in that group visits your website for longer periods of
    time than others. This can show you what the value is and also provides a clean
    sample size for future supervised learning experiments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想找出数据集中的潜在含义，能够找到其中相似数据的分组是非常有价值的。如果你是商店老板，并且想要了解哪些客户更有价值，但并没有一个明确的“有价值”标准，那么聚类将是一个很好的起点，帮助你在数据中找到模式。你可能有一些关于“有价值客户”高层次的想法，但在面对大量数据时，你并不完全确定。通过聚类，你可以找到数据中相似群体之间的共性。如果你更深入地研究一个相似的群体，可能会发现这个群体中的每个人在你的网站上停留的时间比其他人都长。这能帮助你识别出“价值”的标准，并为未来的监督学习实验提供清晰的样本数据。
- en: Identifying Clusters
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 识别聚类
- en: 'The following figure shows two scatterplots:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了两个散点图：
- en: '![Figures 1.2: Two distinct scatterplots'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2：两个不同的散点图'
- en: '](img/C12626_01_02.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_02.jpg)'
- en: 'Figures 1.2: Two distinct scatterplots'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.2：两个不同的散点图
- en: 'The following figure separates the scatterplots into two distinct clusters:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下图将散点图分为两个不同的聚类：
- en: '![Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3：散点图清晰展示了在提供的数据集中存在的聚类'
- en: '](img/C12626_01_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_03.jpg)'
- en: 'Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.3：散点图清晰展示了在提供的数据集中存在的聚类
- en: Both figures display randomly generated number pairs (x,y coordinates) pulled
    from a Gaussian distribution. Simply by glancing at *Figure 1.2*, it should be
    plainly obvious where the clusters exist in your data – in real life, it will
    never be this easy. Now that you know that the data can be clearly separated into
    two clusters, you can start to understand what differences exist between the two
    groups.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两个图都显示了从高斯分布中随机生成的数字对（x，y 坐标）。仅仅通过瞥一眼*图 1.2*，你就应该能清楚地看到数据中聚类的位置——但在现实生活中，永远不会这么简单。现在你知道数据可以清晰地分成两个聚类，你可以开始理解这两个群体之间的差异。
- en: Rewinding a bit from where unsupervised learning fits into the larger machine
    learning environment, let's begin by understanding the building blocks of clustering.
    The most basic definition finds clusters simply as groupings of similar data as
    subsets of a larger dataset. As an example, imagine that you had a room with 10
    people in it and each person had a job either in finance or as a scientist. If
    you told all of the financial workers to stand together and all the scientists
    to do the same, you would have effectively formed two clusters based on job types.
    Finding clusters can be immensely valuable in identifying items that are more
    similar, and, on the other end of the scale, quite different from each other.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从无监督学习在更大范围的机器学习环境中的位置稍作回顾，让我们从理解聚类的基本构成开始。最基本的定义将聚类视为大型数据集的子集中的相似数据组。举个例子，假设你有一个房间，里面有10个人，每个人的职业要么是金融行业，要么是科学家。如果你让所有金融行业的人员站在一起，所有科学家也站到一起，那么你就实际上形成了基于职业类型的两个聚类。找到聚类在识别更相似的项时极具价值，而在规模的另一端，它也能帮助识别相互之间有很大差异的项。
- en: Two-Dimensional Data
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二维数据
- en: 'To understand this, imagine that you were given a simple 1,000-row dataset
    by your employer that had two columns of numerical data as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，假设你的雇主给了你一个简单的包含1,000行数据的数据集，其中有两列数字数据，如下所示：
- en: '![Figures 1.4: Two-dimensional raw data in a NumPy array'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.4：NumPy 数组中的二维原始数据'
- en: '](img/C12626_01_04.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_04.jpg)'
- en: 'Figures 1.4: Two-dimensional raw data in a NumPy array'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.4：NumPy 数组中的二维原始数据
- en: At first glance, this dataset provides no real structure or understanding –
    confusing to say the least!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这个数据集没有提供任何实际的结构或理解——至少可以说是令人困惑的！
- en: 'A **dimension** in a dataset is another way of simply counting the number of
    features available. In most organized data tables, you can view the number of
    features as the number of columns. So, using the 1,000-row dataset example of
    size (1,000 x 2), you will have 1,000 observations across two dimensions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的**维度**是另一种简单的计数特征数量的方法。在大多数组织良好的数据表中，你可以通过查看列的数量来知道特征的数量。因此，使用一个大小为（1,000
    x 2）的1,000行数据集，你将有1,000个观测值，涵盖两个维度：
- en: You begin by plotting the first column against the second column to get a better
    idea of what the data structure looks like. There will be plenty of times where
    the cause of differences between groups will prove to be underwhelming, however
    the cases that have differences that you can take action on are extremely rewarding!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将第一列与第二列绘制出来，更好地了解数据的结构。会有很多时候，组间差异的原因看起来微不足道，但那些你能够采取行动的差异案例往往是非常有意义的！
- en: 'Exercise 1: Identifying Clusters in Data'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 1：识别数据中的聚类
- en: You are given two-dimensional plots. Please look at the provided two-dimensional
    graphs and identify the clusters, to drive the point home that machine learning
    is important. Without using any algorithmic approaches, identify where the clusters
    exist in the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到二维图形。请查看提供的二维图形并识别聚类，以强调机器学习的重要性。在不使用任何算法方法的情况下，识别数据中的聚类位置。
- en: 'This exercise will help start to build your intuition of how we identify clusters
    using our own eyes and thought processes. As you complete the exercises, think
    of the rationale of why a group of data points should be considered a cluster
    versus a group that should not be considered a cluster:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习将帮助你开始培养通过自身的眼睛和思维过程识别聚类的直觉。在完成这些练习时，思考一下为什么一组数据点应该被视为一个聚类，而另一组数据点不应该被视为聚类：
- en: Identify the clusters in the following scatterplot:![Figure1.5 Two-dimensional
    scatterplot
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别以下散点图中的聚类：![图1.5 二维散点图
- en: '](img/C12626_01_05.jpg)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_01_05.jpg)'
- en: Figure1.5 Two-dimensional scatterplot
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.5 二维散点图
- en: 'The clusters are as follows:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类如下：
- en: '![Figure 1.6: Clusters in the scatterplot'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图1.6：散点图中的聚类'
- en: '](img/C12626_01_06.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_01_06.jpg)'
- en: 'Figure 1.6: Clusters in the scatterplot'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.6：散点图中的聚类
- en: 'Identify the clusters in the scatterplot:![Figure1.7: Two-dimensional scatterplot'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别散点图中的聚类：![图1.7：二维散点图
- en: '](img/C12626_01_07.jpg)'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_01_07.jpg)'
- en: 'Figure1.7: Two-dimensional scatterplot'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.7：二维散点图
- en: 'The clusters are as follows:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类如下：
- en: '![Figure 1.8: Clusters in the scatterplot'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图1.8：散点图中的聚类'
- en: '](img/C12626_01_08.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_01_08.jpg)'
- en: 'Figure 1.8: Clusters in the scatterplot'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.8：散点图中的聚类
- en: 'Identify the clusters in the scatterplot:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别散点图中的聚类：
- en: '![Figure1.9: Two-dimensional scatterplot'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.9：二维散点图'
- en: '](img/C12626_01_09.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_09.jpg)'
- en: 'Figure1.9: Two-dimensional scatterplot'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.9：二维散点图
- en: 'The clusters are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类如下：
- en: '![Figure 1.10: Clusters in the scatterplot'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.10：散点图中的聚类'
- en: '](img/C12626_01_10.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_10.jpg)'
- en: 'Figure 1.10: Clusters in the scatterplot'
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.10：散点图中的聚类
- en: Most of these examples were likely quite easy for you to understand – and that's
    the point! The human brain and eyes are incredible at finding patterns in the
    real world. Within milliseconds of viewing each plot, you could tell what fitted
    together and what didn't. While it is easy for you, a computer does not have the
    ability to see and process plots in the same manner that we do. However, this
    is not always a bad thing – look back at Figure 1.10\. Were you able to find the
    six discrete clusters in the data just by looking at the plot? You probably found
    only three to four clusters in this figure, while a computer is able to see all
    six. The human brain is magnificent, but it also lacks the nuances that come within
    a strictly logic-based approach. Through algorithmic clustering, you will learn
    how to build a model that works even better than a human at these tasks!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子大多数你可能很容易理解——这就是重点！人类的大脑和眼睛在发现现实世界中的模式方面非常出色。仅仅通过几毫秒的查看，你就能分辨出哪些数据是组合在一起的，哪些不是。虽然对你来说很容易，但计算机无法像我们一样查看和处理图表。然而，这并不总是坏事——回顾图1.10，你能仅凭观察图表就找到数据中的六个离散聚类吗？你可能只找到了三个到四个聚类，而计算机可以看到所有六个。人类的大脑非常强大，但它也缺乏基于严格逻辑方法所能处理的细微差别。通过算法聚类，你将学习如何建立一个比人类在这些任务中表现更好的模型！
- en: Let's look at the algorithm in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中看看这个算法。
- en: Introduction to k-means Clustering
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类简介
- en: Hopefully, by now, you can see that finding clusters is extremely valuable in
    a machine learning workflow. However, how can you actually find these clusters?
    One of the most basic yet popular approaches is by using a cluster analysis called
    **k-means clustering**. k-means works by searching for K clusters in your data
    and the workflow is actually quite intuitive – we will start with the no-math
    introduction to k-means, followed by an implementation in Python.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在为止，你已经可以看到，在机器学习的工作流中，寻找聚类是非常有价值的。那么，如何实际找到这些聚类呢？其中一个最基础但最流行的方法是使用一种叫做
    **k-means 聚类** 的聚类分析方法。k-means 通过在你的数据中寻找 K 个聚类，整个工作流程实际上非常直观——我们将从 k-means 的无数学介绍开始，随后进行
    Python 实现。
- en: No-Math k-means Walkthrough
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无数学 k-means 解析
- en: 'Here is the no-math algorithm of k-means clustering:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 k-means 聚类的无数学算法：
- en: 'Pick K centroids (K = expected distinct # of clusters).'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 K 个质心（K = 期望的不同聚类数量）。
- en: Randomly place K centroids anywhere amongst your existing training data.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机地将 K 个质心放置在你的现有训练数据中。
- en: Calculate the Euclidean distance from each centroid to all the points in your
    training data.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个质心到你训练数据中所有点的欧几里得距离。
- en: Training data points get grouped in with their nearest centroid.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据点会根据它们与质心的距离进行分组。
- en: Amongst the data points grouped into each centroid, calculate the mean data
    point and move your centroid to that location.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个质心分组中的数据点中，计算均值数据点，并将质心移动到该位置。
- en: Repeat this process until convergence, or when the membership in each group
    no longer changes.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这个过程，直到收敛，或者每个组内的成员不再变化。
- en: 'And that''s it! Here is the process laid out step-by-step with a simple cluster
    example:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！下面是一步步的过程，带有一个简单的聚类示例：
- en: '![Figure 1.11: Original raw data charted on x,y coordinates'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11: 原始数据图，标注在 x,y 坐标上'
- en: '](img/C12626_01_11.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_11.jpg)'
- en: 'Figure 1.11: Original raw data charted on x,y coordinates'
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 1.11: 原始数据图，标注在 x,y 坐标上'
- en: 'Provided with the original data in Figure 1.11, we can show the iterative process
    of k-means by showing the predicted clusters in each step:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1.11 中给出的原始数据的基础上，我们可以通过展示每一步的预测聚类来显示 k-means 的迭代过程：
- en: '![Figure 1.12: Reading from left to right – red points are randomly initialized
    centroids, and the closest data points are assigned to groupings of each centroid'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12: 从左到右读取——红色点是随机初始化的质心，最接近的数据点被分配到各个质心的分组中'
- en: '](img/C12626_01_12.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_12.jpg)'
- en: 'Figure 1.12: Reading from left to right – red points are randomly initialized
    centroids, and the closest data points are assigned to groupings of each centroid'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 1.12: 从左到右读取——红色点是随机初始化的质心，最接近的数据点被分配到各个质心的分组中'
- en: k-means Clustering In-Depth Walkthrough
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means 聚类深度解析
- en: 'To understand k-means at a deeper level, let''s walk through the example given
    in the introductory section again with some of the math that supports k-means.
    The key component at play is the Euclidean distance formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解 k-means，让我们再次走过介绍部分给出的示例，并加入一些支持 k-means 的数学内容。这里的关键组件是欧几里得距离公式：
- en: '![Figure 1.13: Euclidean distance formula'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13: 欧几里得距离公式'
- en: '](img/C12626_01_13.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_13.jpg)'
- en: 'Figure 1.13: Euclidean distance formula'
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 1.13: 欧几里得距离公式'
- en: Centroids are randomly set at the beginning as points in your n-dimensional
    space. Each of these centers is fed into the preceding formula as (a,b), and a
    point in your space is fed in as (x,y). Distances are calculated between each
    point and the coordinates of every centroid, with the centroid the shortest distance
    away chosen as the point's group.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 质心在开始时随机设置为你n维空间中的点。每个质心作为 (a,b) 输入到前面的公式中，而你空间中的点作为 (x,y) 输入。计算每个点与每个质心坐标之间的距离，选择距离最短的质心作为该点所属的组。
- en: 'The process is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程如下：
- en: 'Random Centroids: [ (2,5) , (8,3) , (4, 5) ]'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机质心：[ (2,5) , (8,3) , (4, 5) ]
- en: 'Arbitrary point x: (0, 8)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '任意点 x: (0, 8)'
- en: 'Distance from point to each centroid: [ 3.61, 9.43, 5.00 ]'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从点到每个质心的距离：[ 3.61, 9.43, 5.00 ]
- en: Point x is assigned to Centroid 1.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点 x 被分配给质心 1。
- en: Alternative Distance Metric – Manhattan Distance
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 曼哈顿距离（替代距离度量）
- en: Euclidean distance is the most common distance metric for many machine learning
    applications and is often known colloquially as the distance metric; however,
    it is not the only, or even the best, distance metric for every situation. Another
    popular distance metric in use for clustering is **Manhattan distance**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是许多机器学习应用中最常用的距离度量，通常被称为距离度量；然而，它并不是唯一的，也不是在每种情况下最好的距离度量。另一个在聚类中常用的距离度量是**曼哈顿距离**。
- en: 'Manhattan distance is called as such because the intuition behind the metric
    is as though you were driving a car through a metropolis (such as New York City)
    that has many square blocks. Euclidean distance relies on diagonals due to it
    being based on Pythagorean theorem, while Manhattan distance constrains distance
    to only right angles. The formula for Manhattan distance is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离之所以如此命名，是因为该度量的直觉就像是你在一个大城市（比如纽约市）里开车，城市有许多方形街区。欧几里得距离依赖于对角线，因为它基于勾股定理，而曼哈顿距离则将距离限制为只有直角。曼哈顿距离的公式如下：
- en: '![Figure 1.14: Manhattan distance formula'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：曼哈顿距离公式'
- en: '](img/C12626_01_14.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_14.jpg)'
- en: 'Figure 1.14: Manhattan distance formula'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.14：曼哈顿距离公式
- en: Here, ![](img/C12626_01_Formula_01.png) are vectors as in Euclidean distance.
    Building upon our examples of Euclidean distance, where we want to find the distance
    between two points, if ![](img/C12626_01_Formula_02.png) and ![](img/C12626_01_Formula_03.png),
    then the Manhattan distance would equal ![](img/C12626_01_Formula_04.png). This
    functionality scales to any number of dimensions. In practice, Manhattan distance
    may outperform Euclidean distance when it comes to higher dimensional data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/C12626_01_Formula_01.png)是像欧几里得距离一样的向量。在我们之前关于欧几里得距离的例子中，我们希望找到两个点之间的距离，如果![](img/C12626_01_Formula_02.png)和![](img/C12626_01_Formula_03.png)，那么曼哈顿距离将等于![](img/C12626_01_Formula_04.png)。这个功能适用于任何维度。在实践中，曼哈顿距离可能在处理高维数据时表现得比欧几里得距离更好。
- en: Deeper Dimensions
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更深的维度
- en: The preceding examples are clear to visualize when your data is only two-dimensional.
    This is for convenience, to help drive the point home of how k-means works and
    could lead you into a false understanding of how easy clustering is. In many of
    your own applications, your data will likely be orders of magnitude larger to
    the point that it cannot be perceived by visualization (anything beyond three
    dimensions will be imperceivable to humans). In the previous examples, you could
    mentally work out a few two-dimensional lines to separate the data into its own
    groups. At higher dimensions, you will need to be aided by a computer to find
    an n-dimensional hyperplane that adequately separates the dataset. In practice,
    this is where clustering methods such as k-means provide significant value.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据只有二维时，前面的例子很容易可视化。这是为了方便，帮助说明 k-means 是如何工作的，但也可能会让你产生聚类很简单的误解。在许多应用中，你的数据可能会大得多，甚至大到无法通过可视化感知（超过三维的数据对于人类来说是无法感知的）。在前面的例子中，你可以通过心算一些二维线条来将数据分成不同的组。而在更高维度时，你将需要计算机的帮助，找到一个适合分隔数据集的
    n 维超平面。在实践中，这就是像 k-means 这样的聚类方法能够提供巨大价值的地方。
- en: '![Figure 1.15: Two-dimensional, three-dimensional, and n-dimensional plots'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15：二维、三维和 n 维图'
- en: '](img/C12626_01_15.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_15.jpg)'
- en: 'Figure 1.15: Two-dimensional, three-dimensional, and n-dimensional plots'
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.15：二维、三维和 n 维图
- en: In the next exercise, we will calculate Euclidean distance. We will use the
    `NumPy` and `Math` packages. `NumPy` is a scientific computing package for Python
    that pre-packages common mathematical functions in highly-optimized formats. By
    using a package such as `NumPy` or `Math`, we help cut down the time spent creating
    custom math functions from scratch and instead focus on developing our solutions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将计算欧几里得距离。我们将使用`NumPy`和`Math`包。`NumPy`是一个用于 Python 的科学计算包，它将常见的数学函数以高度优化的格式进行预打包。通过使用像`NumPy`或`Math`这样的包，我们可以减少从头编写自定义数学函数所花费的时间，从而专注于开发我们的解决方案。
- en: 'Exercise 2: Calculating Euclidean Distance in Python'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2：在 Python 中计算欧几里得距离
- en: In this exercise, we will create an example point along with three sample centroids
    to help illustrate how Euclidean distance works. Understanding this distance formula
    is foundational to the rest of our work in clustering.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将创建一个示例点以及三个样本中心点，以帮助说明欧几里得距离是如何工作的。理解这个距离公式是我们进行聚类工作的基础。
- en: By the end of this exercise, we will be able to implement Euclidean distance
    from scratch and fully understand what it does to points in a feature space.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习结束时，我们将能够从零开始实现欧几里得距离，并完全理解它在特征空间中对点的作用。
- en: 'In this exercise, we will be using the standard Python built-in `math` package.
    There are no prerequisites for using the `math` package and it is included in
    all standard installations of Python. As the name suggests, this package is very
    useful, allowing to use a variety of basic math building blocks off the shelf,
    such as exponentials, square roots, and others:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用标准的 Python 内置 `math` 包。使用 `math` 包没有任何前提要求，并且它包含在所有 Python 的标准安装中。顾名思义，这个包非常有用，允许我们直接使用各种基本的数学构件，如指数、平方根等：
- en: 'Open a Jupyter notebook and create a naïve formula that captures the direct
    math of Euclidean distance, as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Jupyter Notebook，并创建一个天真的公式来计算欧几里得距离，具体如下：
- en: '[PRE0]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This approach is considered naïve because it performs element-wise calculations
    on your data points (slow) compared to a more real-world implementation using
    vectors and matrix math to achieve significant performance increases.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种方法被认为是天真的，因为它对数据点执行逐元素计算（慢），相比之下，使用向量和矩阵运算的更实际实现能够显著提高性能。
- en: 'Create the data points in Python as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式在 Python 中创建数据点：
- en: '[PRE1]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the formula you created to calculate the Euclidean distance between the
    example point and each of the three centroids you were provided:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你创建的公式，计算示例点与所提供的三个质心之间的欧几里得距离：
- en: '[PRE2]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since Python is zero-indexed, a position of zero as the minimum in our list
    of centroid distances signals to us that the example point, x, will be assigned
    to the number one centroid of three.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 Python 是零索引的，列表中质心距离的零位置向我们表明，示例点 x 将被分配给三个质心中的第一个。
- en: This process is repeated for every point in the dataset until each point is
    assigned to a cluster. After each point is assigned, the mean point is calculated
    among all of the points within each cluster. The calculation of the mean among
    these points is the same as calculating a mean between single integers.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会对数据集中的每个点重复，直到每个点都被分配到一个聚类。每分配一个点后，会计算每个聚类中所有点的平均点。计算这些点的平均值与计算单个整数的平均值相同。
- en: Now that you have found clusters in your data using Euclidean distance as the
    primary metric, think back to how you did this easily in *Exercise 2*, *Calculating
    Euclidean Distance in Python*. It is very intuitive for our human minds to see
    groups of dots on a plot and determine which dots belong to discrete clusters.
    However, how do we ask a naïve computer to repeat this same task? By understanding
    this exercise, you help teach a computer an approach to forming clusters of its
    own with the notion of distance. We will build upon how we use these distance
    metrics in the next exercise.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你已经通过欧几里得距离作为主要度量方法在数据中找到了聚类，回想一下你是如何在 *练习 2* 中轻松完成这一任务的，*在 Python 中计算欧几里得距离*。对于我们的人类思维来说，看到图中的点群并确定哪些点属于不同的聚类是非常直观的。然而，我们如何让一个天真的计算机重复这一任务呢？通过理解这个练习，你帮助计算机学习一种通过距离来形成聚类的方法。我们将在下一个练习中继续使用这些距离度量。
- en: 'Exercise 3: Forming Clusters with the Notion of Distance'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3：通过距离的概念形成聚类
- en: 'By understanding this exercise, you''ll help to teach a computer an approach
    to forming clusters of its own with the notion of distance. We will build upon
    how we use these distance metrics in this exercise:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这个练习，你将帮助计算机学习通过距离来形成聚类的方法。我们将在本次练习中继续使用这些距离度量：
- en: 'Store the points [ (0,8), (3,8), (3,4) ] that are assigned to cluster one:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储分配给聚类一的点 [ (0,8), (3,8), (3,4) ]：
- en: '[PRE4]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Calculate the mean point between all of the points to find the new centroid:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有点的平均点以找到新的质心：
- en: '[PRE5]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE6]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After a new centroid is calculated, you will repeat the cluster membership calculation
    seen in *Exercise 2*, *Calculating Euclidean Distance in Python*, and then the
    previous two steps to find the new cluster centroid. Eventually, the new cluster
    centroid will be the same as the one you had entering the problem, and the exercise
    will be complete. How many times this repeats depends on the data you are clustering.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算出新的质心后，您将重复在*练习 2*中看到的聚类成员计算，即*在 Python 中计算欧几里得距离*，然后再进行前两步来找到新的聚类质心。最终，新的聚类质心将与进入问题时的质心相同，练习也将完成。重复的次数取决于您正在聚类的数据。
- en: Once you have moved the centroid location to the new mean point of (2, 6.67),
    you can compare it to the initial list of centroids you entered the problem with.
    If the new mean point is different than the centroid that is currently in your
    list, that means you have to go through another iteration of the preceding two
    exercises. Once the new mean point you calculate is the same as the centroid you
    started the problem with, you have completed a run of k-means and reached a point
    called **convergence**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您将质心位置移动到新的均值点（2, 6.67），可以将其与您进入问题时输入的初始质心列表进行比较。如果新的均值点与当前列表中的质心不同，这意味着您需要再执行前两个练习的迭代。直到您计算出的新的均值点与您开始时的质心相同，您就完成了
    k-means 的一次运行，并达到了称为**收敛**的点。
- en: In the next exercise, we will implement k-means from scratch.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将从头实现 k-means。
- en: 'Exercise 4: Implementing k-means from Scratch'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4：从头实现 k-means
- en: 'In this exercise, we will have a look at the implementation of k-means from
    scratch. This exercise relies on scikit-learn, an open-source Python package that
    enables the fast prototyping of popular machine learning models. Within scikit-learn,
    we will be using the `datasets` functionality to create a synthetic blob dataset.
    In addition to harnessing the power of scikit-learn, we will also rely on Matplotlib,
    a popular plotting library for Python that makes it easy for us to visualize our
    data. To do this, perform the following steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将研究从头实现 k-means。这个练习依赖于 scikit-learn，一个开源的 Python 包，它使得快速原型化流行的机器学习模型成为可能。在
    scikit-learn 中，我们将使用 `datasets` 功能来创建一个合成的簇数据集。除了利用 scikit-learn 的强大功能外，我们还将依赖
    Matplotlib，这是一个流行的 Python 绘图库，它使得我们可以轻松地可视化数据。为此，请执行以下步骤：
- en: 'Import the necessary libraries:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE7]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Generate a random cluster dataset to experiment on X = coordinate points, y
    = cluster labels, and define random centroids:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机的聚类数据集进行实验，X = 坐标点，y = 聚类标签，并定义随机质心：
- en: '[PRE8]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Print the data:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据：
- en: '[PRE9]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Plot the coordinate points as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式绘制坐标点：
- en: '[PRE11]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The plot looks as follows:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图如下所示：
- en: '![Figure 1.16: Plot of the coordinates'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.16：坐标点的绘图'
- en: '](img/C12626_01_16.jpg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_01_16.jpg)'
- en: 'Figure 1.16: Plot of the coordinates'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.16：坐标点的绘图
- en: 'Print the array of `y`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 `y` 数组：
- en: '[PRE12]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Plot the coordinate points with the correct cluster labels:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正确的聚类标签绘制坐标点：
- en: '[PRE14]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The plot looks as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图如下所示：
- en: '![Figure 1.17: Plot of the coordinates with correct cluster labels'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.17：带有正确聚类标签的坐标点绘图'
- en: '](img/C12626_01_17.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_17.jpg)'
- en: 'Figure 1.17: Plot of the coordinates with correct cluster labels'
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.17：带有正确聚类标签的坐标点绘图
- en: 'Exercise 5: Implementing k-means with Optimization'
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5：实现带优化的 k-means
- en: 'Let''s recreate these results on our own! We will go over an example implementing
    this with some optimizations. This exercise is built on top of the previous exercise
    and should be performed in the same Jupyter notebook. For this exercise, we will
    rely on SciPy, a Python package that allows easy access to highly optimized versions
    of scientific calculations. In particular, we will be implementing Euclidean distance
    with `cdist`, the functionally of which replicates the barebones implementation
    of our distance metric in a much more efficient manner:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们自己重新创建这些结果！我们将通过一个例子来实现这个过程，并进行一些优化。这个练习是在前一个练习的基础上构建的，应在同一个 Jupyter notebook
    中进行。对于这个练习，我们将依赖 SciPy，一个 Python 包，它提供了对高效版本科学计算的便捷访问。特别是，我们将使用 `cdist` 实现欧几里得距离，该函数的功能以更高效的方式复制了我们距离度量的基本实现：
- en: 'A non-vectorized implementation of Euclidean distance is as follows:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欧几里得距离的非向量化实现如下：
- en: '[PRE15]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, implement the optimized Euclidean distance:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，实现优化的欧几里得距离：
- en: '[PRE16]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Store the values of X:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储 X 的值：
- en: '[PRE17]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE18]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Calculate the distances and choose the index of the shortest distance as a
    cluster:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算距离并选择最短距离的索引作为一个聚类：
- en: '[PRE19]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the `k_means` function as follows and initialize k-centroids randomly.
    Repeat the process until the difference between new/old `centroids` equal `0`
    using the `while` loop:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`k_means`函数如下，并随机初始化k个质心。使用`while`循环重复该过程，直到新旧`centroids`之间的差值为`0`：
- en: '[PRE20]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Do not break this code, as it might lead to an error.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请不要破坏这段代码，因为这样可能会导致错误。
- en: 'Zip together the historical steps of centers and their labels:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将中心的历史步骤及其标签压缩在一起：
- en: '[PRE21]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first plot is as follows:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一张图如下所示：
- en: '![Figure 1.18: First scatterplot'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.18：第一次散点图'
- en: '](img/C12626_01_18.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_18.jpg)'
- en: 'Figure 1.18: First scatterplot'
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.18：第一次散点图
- en: 'The second plot is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张图如下所示：
- en: '![Figure 1.19: Second scatterplot'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.19：第二次散点图'
- en: '](img/C12626_01_19.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_19.jpg)'
- en: 'Figure 1.19: Second scatterplot'
  id: totrans-192
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.19：第二次散点图
- en: 'The third plot is as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第三张图如下所示：
- en: '![Figure 1.20: Third scatterplot'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.20：第三次散点图'
- en: '](img/C12626_01_20.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_20.jpg)'
- en: 'Figure 1.20: Third scatterplot'
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.20：第三次散点图
- en: As you can see in the above figures, k-means takes an iterative approach to
    refining optimal clusters based on distance. The algorithm starts with random
    initialization and depending on the complexity of the data, quickly finds the
    separations that make the most sense.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在上面的图中看到的，K-means采用迭代方法，通过距离不断精细化最佳聚类。该算法从随机初始化开始，根据数据的复杂性，迅速找到最合理的分隔。
- en: 'Clustering Performance: Silhouette Score'
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类性能：轮廓分数
- en: Understanding the performance of unsupervised learning methods is inherently
    much more difficult than supervised learning methods because, often, there is
    no clear-cut "best" solution. For supervised learning, there are many robust performance
    metrics – the most straightforward of these being accuracy in the form of comparing
    model-predicted labels to actual labels and seeing how many the model got correct.
    Unfortunately, for clustering, we do not have labels to rely on and need to build
    an understanding of how "different" our clusters are. We achieve this with the
    Silhouette Score metric. Inherent to this approach, we can also use Silhouette
    Scores to find optimal "K" numbers of clusters for our unsupervised learning methods.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 理解无监督学习方法的性能本质上比监督学习方法要困难得多，因为通常没有明确的“最佳”解决方案。对于监督学习，有许多可靠的性能指标——其中最直接的指标就是通过将模型预测标签与实际标签进行比较，并查看模型预测正确的数量来衡量准确度。不幸的是，对于聚类，我们没有标签可以依赖，需要建立对聚类“差异”的理解。我们通过轮廓分数（Silhouette
    Score）指标来实现这一点。这个方法的固有特性是，我们还可以使用轮廓分数来寻找无监督学习方法的最佳“K”聚类数量。
- en: The Silhouette metric works by analyzing how well a point fits within its cluster.
    The metric ranges from -1 to 1 – If the average silhouette score across your clustering
    is one, then you will have achieved perfect clusters and there will be minimal
    confusion about which point belongs where. If you think of the plots in our last
    exercise, the Silhouette score will be much closer to one, since the blobs are
    tightly condensed and there is a fair amount of distance between each blob. This
    is very rare though – the Silhouette Score should be treated as an attempt at
    doing the best you can, since hitting one is highly unlikely.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓分数指标通过分析一个点在其聚类中的适配程度来工作。该指标的范围是从-1到1——如果你在聚类中计算的平均轮廓分数为1，那么你将达成完美的聚类，且不会有任何混淆，知道哪个点属于哪个聚类。如果你想象我们上一个练习中的散点图，轮廓分数会接近1，因为聚类之间的“球”非常紧凑，并且每个“球”之间有明显的距离。不过这种情况非常罕见——轮廓分数应视为尽力而为的结果，因为获得1的可能性极低。
- en: Mathematically, the Silhouette Score calculation is quite straightforward via
    the Simplified Silhouette Index (SSI), as ![](img/C12626_01_Formula_05.png) where
    ![](img/C12626_01_Formula_06.png) is the distance from point *i* to its own cluster
    centroid and ![](img/C12626_01_Formula_07.png) is the distance from point i to
    the nearest cluster centroid.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，轮廓分数的计算通过简化的轮廓指数（SSI）非常简单，如下所示：![](img/C12626_01_Formula_05.png)，其中 ![](img/C12626_01_Formula_06.png)
    是点 *i* 到其所在聚类质心的距离，![](img/C12626_01_Formula_07.png) 是点 *i* 到最近的聚类质心的距离。
- en: The intuition captured here is that ![](img/C12626_01_Formula_08.png) represents
    how cohesive point *i*'s cluster is as a clear cluster, and ![](img/C12626_01_Formula_07.png)
    represents how far apart the clusters lie. We will use the optimized implementation
    of `silhouette_score` in scikit-learn for *Activity 1*, *Implementing k-means
    Clustering*. Using it is simple and only requires you to pass in the feature array
    and the predicted cluster labels from your k-means clustering method.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里捕捉到的直觉是，![](img/C12626_01_Formula_08.png)表示点*i*的簇作为一个明确簇的凝聚度，![](img/C12626_01_Formula_07.png)表示簇之间的距离。我们将使用scikit-learn中`silhouette_score`的优化实现来完成*活动1*，*实现k-means聚类*。使用它非常简单，只需传入特征数组和从k-means聚类方法中预测的簇标签。
- en: In the next exercise, we will use the pandas library to read a CSV. Pandas is
    a Python library that makes data wrangling easier through the use of DataFrames.
    To read data in Python, you will use `variable_name = pd.read_csv('file_name.csv',
    header=None)`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将使用pandas库来读取CSV。Pandas是一个Python库，通过使用DataFrame使得数据处理变得更容易。要在Python中读取数据，你可以使用`variable_name
    = pd.read_csv('file_name.csv', header=None)`。
- en: 'Exercise 6: Calculating the Silhouette Score'
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6：计算轮廓系数
- en: In this exercise, we're going to learn how to calculate the Silhouette Score
    of a dataset with a fixed number of clusters. For this, we will use the Iris dataset,
    which is available at [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何计算一个数据集的轮廓系数（Silhouette Score），并且该数据集有一个固定数量的簇。为此，我们将使用Iris数据集，数据集可以在[https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06)找到。
- en: Note
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset was downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    It can be accessed at [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是从[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)下载的，可以通过[https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Exercise06)访问。
- en: 'Load the Iris data file using pandas, a package that makes data wrangling much
    easier through the use of DataFrames:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas加载Iris数据文件，pandas是一个通过使用DataFrame使数据处理变得更加容易的库：
- en: '[PRE22]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Separate the `X` features, since we want to treat this as an unsupervised learning
    problem:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离`X`特征，因为我们希望将其视为无监督学习问题：
- en: '[PRE23]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Bring back the `k_means` function we made earlier for reference:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引入我们之前做的`k_means`函数作为参考：
- en: '[PRE24]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Convert our Iris `X` feature DataFrame to a `NumPy` matrix:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的Iris `X`特征DataFrame转换为`NumPy`矩阵：
- en: '[PRE25]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run our `k_means` function on the Iris matrix:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Iris矩阵上运行我们的`k_means`函数：
- en: '[PRE26]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Calculate the Silhouette Score for the `PetalLengthCm` and `PetalWidthCm` columns:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`PetalLengthCm`和`PetalWidthCm`列的轮廓系数（Silhouette Score）：
- en: '[PRE27]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is similar to:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果类似于：
- en: '[PRE28]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this exercise, we calculated the Silhouette Score for the `PetalLengthCm`
    and `PetalWidthCm` columns of the Iris dataset.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们计算了Iris数据集的`PetalLengthCm`和`PetalWidthCm`列的轮廓系数。
- en: 'Activity 1: Implementing k-means Clustering'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动1：实现k-means聚类
- en: '**Scenario**: You are asked in an interview to implement a k-means clustering
    algorithm from scratch to prove that you understand how it works. We will be using
    the Iris dataset provided by the UCI ML repository. The Iris dataset is a classic
    in the data science world and has features that are used to predict Iris species.
    The download location can be found later in this activity.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**情景**：在面试中，你被要求从零实现一个k-means聚类算法，以证明你理解其工作原理。我们将使用UCI ML库提供的Iris数据集。Iris数据集是数据科学界的经典，具有用于预测鸢尾花物种的特征。下载链接将在后面的活动中提供。'
- en: For this activity, you are able to use Matplotlib, NumPy, scikit-learn metrics,
    and pandas.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个活动，你可以使用Matplotlib、NumPy、scikit-learn指标和pandas。
- en: By loading and reshaping data easily, you can focus more on learning k-means
    instead of writing dataloader functionality.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过轻松加载和重塑数据，你可以更多地专注于学习k-means，而不是编写数据加载器功能。
- en: 'Iris data columns are provided as follows for reference:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Iris数据列如下所示，供参考：
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Aim**: To truly understand how something works, you need to build it from
    scratch. Take what you have learned in the previous sections and implement k-means
    from scratch in Python.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**：要真正理解某个事物如何运作，您需要从头开始构建它。将您在前面的章节中学到的知识付诸实践，并在Python中从零开始实现k-means。'
- en: 'Please open your favorite editing platform and try the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开您最喜欢的编辑平台并尝试以下内容：
- en: Using `NumPy` or the `math` package and the Euclidean distance formula and write
    a function that calculates the distance between two coordinates.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`NumPy`或`math`包及欧几里得距离公式，编写一个计算两个坐标之间距离的函数。
- en: Write a function that calculates the distance from centroids to each of the
    points in your dataset and returns the cluster membership.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数，计算数据集中每个点到质心的距离，并返回簇的成员身份。
- en: 'Write a k-means function that takes in a dataset and the number of clusters
    (K) and returns the final cluster centroids, as well as the data points that make
    up that cluster''s membership. After implementing k-means from scratch, apply
    your custom algorithm to the Iris dataset, located here: [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01).'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个k-means函数，接受一个数据集和簇的数量(K)，并返回最终的聚类质心以及构成该聚类的成员数据点。在从零开始实现k-means后，将您的自定义算法应用到鸢尾花数据集，数据集位置如下：[https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01)。
- en: Note
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset was downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    It can be accessed at [https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01).
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个数据集是从[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)下载的。可以在[https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01](https://github.com/TrainingByPackt/Unsupervised-Learning-with-Python/tree/master/Lesson01/Activity01)访问。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCI机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。
- en: Remove the classes supplied in this dataset and see if your k-means algorithm
    can group the different Iris species into their proper groups just based on plant
    characteristics!
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除此数据集中提供的类别，看看您的k-means算法能否仅根据植物特征将不同的鸢尾花物种分到正确的组别！
- en: Calculate the Silhouette Score using the scikit-learn implementation.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现计算轮廓系数。
- en: 'Outcome: By completing this exercise, you will gain hands-on experience of
    tuning a k-means clustering algorithm for a real-world dataset. The Iris dataset
    is seen as a classic "hello world" type problem in the data science space and
    is helpful for testing foundational techniques on. Your final clustering algorithm
    should do a decent job of finding the three clusters of Iris species types that
    exist in the data, as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：通过完成这个练习，您将获得调优k-means聚类算法以适应真实世界数据集的实践经验。鸢尾花数据集被视为数据科学领域经典的“hello world”问题，适合用于测试基础技术。您的最终聚类算法应该能够较好地找到数据中存在的三类鸢尾花物种，具体如下：
- en: '![Figure 1.21: Expected plot of three clusters of Iris species'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.21：鸢尾花物种的三类聚类期望图'
- en: '](img/C12626_01_21.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_01_21.jpg)'
- en: 'Figure 1.21: Expected plot of three clusters of Iris species'
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.21：鸢尾花物种的三类聚类期望图
- en: Note
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 306.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第306页找到。
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explored what clustering is and why it is important
    in a variety of data challenges. Building upon this foundation of clustering knowledge,
    you implemented k-means, which is one of the simplest yet most popular methods
    of unsupervised learning. If you have reached this summary and can repeat what
    k-means does step-by-step to your fellow classmate, good job! If not, please go
    back and review the previous material – the content only grows in complexity from
    here. From here, we will be moving on to hierarchical clustering, which, in one
    configuration, reuses the centroid learning approach that we used in k-means.
    We will build upon this approach by outlining additional clustering methodologies
    and approaches in the next chapter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了聚类的定义以及它在各种数据挑战中的重要性。基于这一聚类知识的基础，您实现了k均值算法，这是一种最简单但也最流行的无监督学习方法之一。如果您能够在这里总结并能够逐步向您的同学解释k均值算法的操作步骤，那么干得漂亮！如果不能，请返回并复习之前的材料——从这里开始，内容将变得更加复杂。接下来，我们将转向层次聚类，其中的一种配置重复使用了我们在k均值中使用的质心学习方法。在下一章中，我们将进一步阐述其他聚类方法和方法。
