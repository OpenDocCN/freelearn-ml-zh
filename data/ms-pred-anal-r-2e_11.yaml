- en: Chapter 11. Topic Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。主题建模
- en: Topic modeling is a relatively recent and exciting area that originated from
    the fields of natural language processing and information retrieval, but has seen
    applications in a number of other domains as well. Many problems in classification,
    such as sentiment analysis, involve assigning a single class to a particular observation.
    In topic modeling, the key idea is that we can assign a mixture of different classes
    to an observation. As this field takes its inspiration from information retrieval,
    we often think of our observations as documents and our output classes as topics.
    In many applications, this is actually the case and so we will focus on the domain
    of text documents and their topics, this being a very natural way to learn about
    this important model. In particular, we'll focus on a technique known as **Latent
    Dirichlet Allocation** (**LDA**), which is the most prominently used method for
    topic modeling.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一个相对较新且令人兴奋的领域，它起源于自然语言处理和信息检索领域，但同时也被应用于许多其他领域。在分类中，许多问题，如情感分析，涉及将单个类别分配给特定的观察对象。在主题建模中，关键思想是我们可以将不同类别的混合分配给一个观察对象。由于这个领域从信息检索中汲取灵感，我们通常将我们的观察对象视为文档，将我们的输出类别视为主题。在许多应用中，这实际上就是情况，因此我们将重点关注文本文档及其主题的领域，这是一个非常自然的方式来了解这个重要的模型。特别是，我们将关注一种称为**潜在狄利克雷分配**（**LDA**）的技术，这是主题建模中最广泛使用的方法。
- en: An overview of topic modeling
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模概述
- en: In [Chapter 10](part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 10. Probabilistic Graphical Models"), *Probabilistic Graphical Models*,
    we saw how we can use a bag of words as a feature of a Naïve Bayes model in order
    to perform sentiment analysis. There, the specific predictive task involved determining
    whether a particular movie review was expressing a positive sentiment or a negative
    sentiment. We explicitly assumed that the movie review was exclusively expressing
    only one possible sentiment. Each of the words used as features (such as *bad*,
    *good*, *fun*, and so on) had a different likelihood of appearing in a review
    under each sentiment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7 "第10章。概率图模型")《概率图模型》中，我们看到了如何使用词袋作为朴素贝叶斯模型的特征来执行情感分析。在那里，具体的预测任务涉及确定某个电影评论是表达正面情感还是负面情感。我们明确假设电影评论只表达了一种可能的情感。用作特征的每个单词（如*bad*、*good*、*fun*等）在每个情感下出现在评论中的可能性都不同。
- en: To compute the model's decision, we basically computed the likelihood of all
    the words in a particular review under one class, and compared this to the likelihood
    of all the words having been generated by the other class. We adjusted these likelihoods
    using the prior probability of each class, so that, when we know that one class
    is more popular in the training data, we expect to find it more frequently represented
    on unseen data in the future. There was no opportunity for a movie review to be
    partially positive, so that some of the words came from the positive class, and
    partially negative, so that the rest of the words occurred in the negative class.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算模型的决策，我们基本上计算了特定评论中所有单词在一个类别下的可能性，并将其与所有单词由另一个类别生成的可能性进行比较。我们使用每个类别的先验概率调整这些可能性，这样，当我们知道训练数据中某个类别更受欢迎时，我们预计在未来未见数据中会更多地发现它。没有机会让电影评论部分为正面，即一些单词来自正面类别，部分为负面，即其余单词出现在负面类别中。
- en: The core premise behind **topic models** is that in our problem we have a set
    of features and a set of hidden or latent variables that generate these features.
    Crucially, each observation in our data contains features that have been generated
    from a mixture or? a subset of these hidden variables. For example, an essay,
    website, or news article might have a central topic or theme such as politics,
    but might also include one or more elements from other themes as well, such as
    human rights, history, or economics.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题模型**背后的核心前提是我们的问题中有一组特征和一组生成这些特征的隐藏或潜在变量。关键的是，我们数据中的每个观察对象都包含由这些隐藏变量的混合或子集生成的特征。例如，一篇文章、网站或新闻文章可能有一个中心主题或主题，如政治，但也可能包括来自其他主题的一个或多个元素，如人权、历史或经济学。'
- en: In the image domain, we might be interested in identifying a particular object
    in a scene from a set of visual features such as shadows and surfaces. These,
    in turn, might be the product of a mixture of different objects. Our task in topic
    modeling is to observe the words inside a document, or the pixels and visual features
    of an image, and from these determine the underlying mix of topics and objects
    respectively.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像领域，我们可能对从阴影和表面等视觉特征集合中识别场景中的特定对象感兴趣。这些特征反过来可能是不同对象的混合产物。在主题建模中，我们的任务是观察文档内的单词，或图像的像素和视觉特征，并从这些特征中确定潜在的混合主题和对象。
- en: Topic modeling on text data can be used in a number of different ways. One possible
    application is to group together similar documents, either based on their most
    predominant topic or based on their topical mix. Thus, it can be viewed as a form
    of clustering. By studying the topic composition, the most frequent words, as
    well as the relative sizes of the clusters we obtain, we are able to summarize
    information about a particular collection of documents.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据上的主题建模可以用多种方式使用。一种可能的应用是将相似的文档分组在一起，无论是基于它们最占主导地位的主题，还是基于它们的主题混合。因此，它可以被视为一种聚类形式。通过研究主题组成、最频繁出现的单词以及我们获得的集群的相对大小，我们能够总结关于特定文档集合的信息。
- en: We can use the most frequent words and topics of a cluster to describe a cluster
    directly, and in turn this might be useful for automatically generating tags,
    for example to improve the search capabilities of an information retrieval service
    for our documents. Yet another example might be to automatically recommending
    Twitter hashtags once we have built a topic model for a database of tweets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用集群中最频繁出现的单词和主题直接描述一个集群，这反过来可能对自动生成标签有用，例如提高我们文档的信息检索服务的搜索能力。另一个例子可能是在为推文数据库构建了主题模型之后，自动推荐Twitter标签。
- en: When we describe documents such as websites using a bag of words approach, each
    document is essentially a vector indexed by the words in our dictionary. The elements
    of the vector are either counts of the various words or binary variables capturing
    whether a word was present in the document. Either way, this representation is
    a good method of encoding text into a numerical format, but the result is a sparse
    vector in a high-dimensional space as the word dictionary is typically large.
    Under a topic model, each document is represented by a mixture of topics. As this
    number tends to be much smaller than the dictionary size, topic modeling can also
    function as a form of dimensionality reduction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用词袋方法描述如网站之类的文档时，每个文档本质上是一个由我们词典中的单词索引的向量。向量的元素是各种单词的计数或捕获单词是否出现在文档中的二元变量。无论如何，这种表示是编码文本到数值格式的好方法，但结果是，由于单词词典通常很大，结果是一个高维空间中的稀疏向量。在主题模型下，每个文档由主题的混合表示。由于这个数字通常比词典大小小得多，主题建模也可以作为一种降维的形式。
- en: Finally, topic modeling can also be viewed as a predictive task for classification.
    If we have a collection of documents labeled with a predominant theme label, we
    can perform topic modeling on this collection. If the predominant topic clustering
    we obtain from this method coincides with our labeled categories, we can use the
    model to predict a topical mixture for an unknown document and classify it according
    to the most dominant topic. We'll see an example of this later on in this chapter.
    We will now introduce the most well-known technique for performing topic modeling,
    Latent Dirichlet Allocation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，主题建模也可以被视为一个分类的预测任务。如果我们有一组用主要主题标签标记的文档，我们可以在该集合上执行主题建模。如果我们从这种方法中获得的主要主题聚类与我们的标记类别一致，我们可以使用该模型来预测未知文档的主题混合，并根据最占主导地位的主题对其进行分类。我们将在本章后面看到一个例子。现在，我们将介绍执行主题建模最著名的技巧，即潜在狄利克雷分配。
- en: Latent Dirichlet Allocation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: '**Latent Dirichlet Allocation** (**LDA**) is the prototypical method of performing
    topic modeling. Rather unfortunately, the acronym LDA is also used for another
    method in machine learning. This latter method is completely different from LDA
    and is commonly used as a way to perform dimensionality reduction and classification.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）是进行主题建模的原型方法。遗憾的是，LDA这个缩写也被用于机器学习中的另一种方法。这种方法与LDA完全不同，通常用作执行降维和分类的一种方式。'
- en: Although LDA involves a substantial amount of mathematics, it is worth exploring
    some of its technical details in order to understand how the model works and the
    assumptions that it uses. First and foremost, we should learn about the **Dirichlet
    distribution**, which lends its name to LDA.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LDA涉及大量的数学，但了解其一些技术细节以了解模型的工作原理和它所使用的假设是值得的。首先，我们应该了解**狄利克雷分布**，它为LDA命名。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'An excellent reference for a fuller treatment of Topic Models with LDA is the
    *Topic Models* chapter in *Text Mining: Classification, Clustering, and Applications*,
    edited by *A. Srivastava* and *M. Sahami* and published by *Chapman & Hall*, 2009.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LDA主题模型的更全面处理，一个优秀的参考是*A. Srivastava*和*M. Sahami*编辑的*《文本挖掘：分类、聚类与应用》*一书中关于*主题模型*的章节，由*Chapman
    & Hall*于2009年出版。
- en: The Dirichlet distribution
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 狄利克雷分布
- en: Suppose we have a classification problem with *K* classes and the probability
    of each class is fixed. Given a vector of length *K* containing counts of the
    occurrence of each class, we can estimate the probabilities of each class by just
    dividing each entry in the vector by the sum of all the counts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个具有*K*个类别的分类问题，并且每个类别的概率是固定的。给定一个长度为*K*的向量，包含每个类别的发生次数，我们可以通过将向量中的每个条目除以所有计数之和来估计每个类别的概率。
- en: 'Now suppose we would like to predict the number of times each class will occur
    over a series of *N* trials. If we have two classes, we can model this with a
    binomial distribution, as we would normally do in a coin flip experiment. For
    *K* classes, the binomial distribution generalizes to the **multinomial distribution**,
    where the probability of each class, *pi*, is fixed and the sum of all instances
    of *pi* equals one. Now, suppose that we wanted to model the random selection
    of a particular multinomial distribution with *K* categories. The Dirichlet distribution
    achieves just that. Here is its form:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想要预测每个类别在*N*次试验中出现的次数。如果我们有两个类别，我们可以用二项分布来模拟，就像我们在抛硬币实验中通常做的那样。对于*K*个类别，二项分布推广到**多项式分布**，其中每个类别的概率*pi*是固定的，所有*pi*实例的总和等于一。现在，假设我们想要模拟具有*K*个类别的特定多项式分布的随机选择。狄利克雷分布正是如此。以下是它的形式：
- en: '![The Dirichlet distribution](img/00183.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![狄利克雷分布](img/00183.jpeg)'
- en: This equation seems complex, but if we break it down to its constituent parts
    and label the symbols used, we will then be able to understand it better. To begin
    with, the ![The Dirichlet distribution](img/00184.jpeg) term is a vector with
    *K* components, *x[k]*, representing a particular multinomial distribution. The
    vector ![The Dirichlet distribution](img/00185.jpeg) is also a *K* component vector
    containing the *K* parameters, *α[k]*, of the Dirichlet distribution. Thus, we
    are computing the probability of selecting a particular multinomial distribution
    given a particular parameter combination. Notice that we provide the Dirichlet
    distribution with a parameter vector whose length is the same as the number of
    classes of the multinomial distribution that it will return.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程看起来很复杂，但如果我们将其分解为其组成部分并标注所用的符号，我们就能更好地理解它。首先，![狄利克雷分布](img/00184.jpeg)项是一个具有*K*个分量的向量，*x[k]*，代表一个特定的多项式分布。![狄利克雷分布](img/00185.jpeg)向量也是一个*K*个分量的向量，包含狄利克雷分布的*K*个参数，*α[k]*。因此，我们正在计算在给定特定参数组合的情况下选择特定多项式分布的概率。请注意，我们向狄利克雷分布提供了一个参数向量，其长度与它将返回的多项式分布的类别数相同。
- en: 'The fraction before the large product on the right-hand side of the equation
    is a normalizing constant, which depends only on the values of the Dirichlet parameters
    and is expressed in terms of the **gamma function**. For completeness, the gamma
    function, a generalization of the factorial function, is given by the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式右侧大乘积之前的分数是一个归一化常数，它只依赖于狄利克雷参数的值，并以**伽马函数**的形式表示。为了完整性，伽马函数，是阶乘函数的推广，其表达式如下：
- en: '![The Dirichlet distribution](img/00186.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![狄利克雷分布](img/00186.jpeg)'
- en: Lastly, in the final product, we see that every parameter, *α[k]*, is paired
    with the corresponding component of the multinomial distribution, *x[k]*, in forming
    the terms of the product. The important point to remember about this distribution
    is that, by modifying the *α[k]* parameters, we are modifying the probabilities
    of the different multinomial distributions that we can draw.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在最终产品中，我们看到每个参数，*α[k]*，都与多项式分布的相应分量*x[k]*配对，形成乘积的项。关于这个分布的重要一点是，通过修改*α[k]*参数，我们正在修改我们可以绘制的不同多项式分布的概率。
- en: We are especially interested in the total sum of the *α[k]* parameters as well
    as the relative proportions among them. A large total for the *α[k]* parameters
    tends to produce a smoother distribution involving a mix of many topics and this
    distribution is more likely to follow the pattern of alpha parameters in their
    relative proportions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特别关注*α[k]*参数的总和以及它们之间的相对比例。*α[k]*参数的总和较大往往会产生一个更平滑的分布，涉及许多主题的混合，并且这种分布更有可能遵循α参数的相对比例模式。
- en: A special case of the Dirichlet distribution is the **Symmetrical Dirichlet
    distribution**, in which all the *α[k]* parameters have an identical value. When
    the *α[k]* parameters are identical and large in value, we are likely to sample
    a multinomial distribution that is close to being uniform. Thus, the symmetrical
    Dirichlet distribution is used when we have no information about a preference
    over a particular topic distribution and we consider all topics to be equally
    likely.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分布的一个特殊情况是**对称狄利克雷分布**，其中所有*α[k]*参数具有相同的值。当*α[k]*参数相同且值较大时，我们很可能会抽取一个接近均匀的多项式分布。因此，当我们对特定主题分布没有信息，并且认为所有主题的可能性相等时，我们使用对称狄利克雷分布。
- en: Similarly, suppose we had a skewed vector of *α[k]* parameters with large absolute
    values. For example, we might have a vector in which one of the *α[k]* parameters
    was much higher than the others, indicating a preference for selecting one of
    the topics. If we used this as an input to the Dirchlet distribution, we would
    likely sample a multinomial distribution in which the aforementioned topic was
    probable.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，假设我们有一个偏斜的*α[k]*参数向量，其绝对值很大。例如，我们可能有一个向量，其中一个*α[k]*参数远高于其他参数，表明倾向于选择其中一个主题。如果我们将其作为狄利克雷分布的输入，我们很可能会抽取一个多项式分布，其中上述主题是可能的。
- en: 'By contrast, if the *α[k]* parameters add up to a small number, this usually
    results in a peaky distribution, in which only one or two of the topics are likely
    and the rest are unlikely. Consequently, if we wanted to model the process of
    drawing a multinomial with only a few topics selected, we would use low values
    for the *α[k]* parameters, whereas, if we wanted a good mix, we would use larger
    values. The following two plots will help visualize this behavior. The first plot
    is for a symmetric Dirichlet distribution:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，如果*α[k]*参数的总和是一个小数，这通常会导致一个峰度分布，其中只有一个或两个主题是可能的，其余的则不太可能。因此，如果我们只想对只选择少数主题的多项式抽样过程进行建模，我们会使用低值的*α[k]*参数，而如果我们想要一个良好的混合，我们会使用较大的值。以下两个图将有助于可视化这种行为。第一个图是对称的狄利克雷分布：
- en: '![The Dirichlet distribution](img/00187.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![狄利克雷分布](img/00187.jpeg)'
- en: In this plot, each column contains four random samples of a multinomial distribution
    generated using a symmetric Dirichlet distribution for five topics. In the first
    column, all the *α[k]* parameters are set to 0.1\. Note that the distributions
    are very peaky and, because all the *α[k]* parameters are equally likely, there
    is no preference for which topic will tend to be chosen as the highest peak.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，每一列包含使用对称狄利克雷分布为五个主题生成的四个多项式分布的随机样本。在第一列中，所有*α[k]*参数都被设置为0.1。请注意，分布非常峰度，并且由于所有*α[k]*参数的可能性相等，没有偏好于哪个主题将倾向于被选择为最高的峰值。
- en: In the middle column, the *α[k]* parameters are set to 1 and, as the sum of
    the parameters is now larger, we see a greater mix of topics, even if the distribution
    is still skewed. When we set the *α[k]* parameters to the value of 10 in the third
    column, we see that the samples are now much closer to a uniform distribution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间列中，*α[k]*参数被设置为1，由于参数的总和现在更大，我们看到主题的混合更加丰富，即使分布仍然偏斜。当我们把第三列中的*α[k]*参数设置为10时，我们看到样本现在与均匀分布非常接近。
- en: 'In many scenarios, we use the Dirichlet distribution as a **prior distribution**;
    that is, a distribution that describes our prior beliefs about the multinomial
    distribution that we are trying to sample. When the sum of the *α[k]* parameters
    is high, we tend to think of our prior as having very strong beliefs. In the next
    plot, we will skew the distribution of our *α[k]* parameters to favor the first
    topic:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们使用狄利克雷分布作为**先验分布**；也就是说，一个描述我们关于尝试抽取的多项式分布的先验信念的分布。当*α[k]*参数的总和较高时，我们倾向于认为我们的先验信念非常强烈。在下一张图中，我们将调整*α[k]*参数的分布，以偏袒第一个主题：
- en: '![The Dirichlet distribution](img/00188.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![狄利克雷分布](img/00188.jpeg)'
- en: In the first column, the average value of the *α[k]* parameters is 0.1, but
    we adjusted their distribution so that *α[1]*, which corresponds to the first
    topic, now has a value four times as high as the others. We see that this has
    increased the probability of the first topic featuring prominently in the sampled
    multinomial distribution, but it is not guaranteed to be the distribution's mode.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一列中，*α[k]*参数的平均值为0.1，但我们调整了它们的分布，使得*α[1]*，对应于第一个主题，现在的值是其他主题的四倍。我们看到这增加了第一个主题在抽取的多项式分布中显著出现的概率，但它并不保证是分布的模态。
- en: In the middle column, where the average of the *α[k]* parameters is now 1 but
    with the same skew, Topic 1 is the mode of distribution in all the samples. Additionally,
    there is still a high variance in how the other topics will be selected. In the
    third column, we have a smoother distribution that simultaneously mixes all five
    topics but enforces the preference for the first topic.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间列中，*α[k]*参数的平均值现在是1，但具有相同的偏斜，主题1在所有样本中都是分布的模态。此外，其他主题的选择仍然存在很高的变异性。在第三列中，我们有一个更平滑的分布，它同时混合了所有五个主题，但强制偏好第一个主题。
- en: Now that we have an idea of how this distribution works, we will see in the
    next section how it is used to build topic models with LDA.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了这种分布的工作原理，我们将在下一节中看到它是如何用于构建LDA主题模型的。
- en: The generative process
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成过程
- en: We delved into the Dirichlet distribution with significant detail because it
    is at the heart of how topic modeling with LDA operates. Armed with this understanding,
    we'll now describe the **generative process** behind LDA.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对狄利克雷分布进行了深入探讨，因为它在LDA主题建模的核心。有了这种理解，我们现在将描述LDA背后的**生成过程**。
- en: 'The generative process is aptly named as it describes how the LDA model assumes
    that documents, topics, and words are generated in our data. This process is essentially
    an illustration of the model''s assumptions. The optimization procedures that
    are used in order to fit an LDA model to data essentially estimate the parameters
    of the generative process. We''ll now see how this process works:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程被恰当地命名，因为它描述了LDA模型假设我们的数据中的文档、主题和单词是如何生成的。这个过程本质上是对模型假设的说明。为了将LDA模型拟合到数据中，所使用的优化过程实际上估计了生成过程的参数。我们现在将看到这个过程是如何工作的：
- en: For each of our K topics, draw a multinomial distribution, *φ[k]*, over the
    words in our vocabulary using a Dirichlet distribution parameterized by a vector,
    α. This vector has length *V*, the size of our vocabulary. Even though we sample
    from the same Dirichlet distribution each time, we've seen that the sampled multinomial
    distributions will likely differ from each other.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的K个主题中的每一个，我们使用一个由向量α参数化的狄利克雷分布，α的长度为*V*，即我们的词汇表大小，来对词汇表中的单词抽取一个多项式分布。尽管我们每次都从同一个狄利克雷分布中抽取，但我们已经看到，抽取的多项式分布可能彼此不同。
- en: 'For every document, *d*, that we want to generate:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们想要生成的每个文档，*d*：
- en: Determine the mix of topics for this document by drawing a multinomial distribution,
    *θ[k]*, from a Dirichlet distribution, this time parameterized by a vector *β*
    of length *K*, the number of topics. Each document will thus have a different
    mix of topics.
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从狄利克雷分布中抽取一个多项式分布，*θ[k]*，该分布由长度为*K*的向量*β*参数化，*K*是主题的数量，来确定这个文档的主题组合。因此，每个文档都将有不同的主题组合。
- en: 'For every word, *w*, in the document that we want to generate:'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们想要生成的文档中的每个单词，*w*：
- en: Use the multinomial topic distribution for this document, *θ[k]*, to draw a
    topic with which this word will be associated
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这个文档的多项式主题分布，*θ[k]*，来抽取一个与这个单词相关联的主题
- en: Use that particular topic's distribution, *φ[k]*, to pick the actual word
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定主题的分布，*φ[k]*，来选择实际的单词
- en: Note that we use two differently parameterized Dirichlet distributions in our
    generative process, one for drawing a multinomial distribution of topics and another
    for drawing a multinomial distribution of words. Although the model is simple,
    it does capture certain intuitions about documents and topics. In particular,
    it captures the notion that documents about different topics will, in general,
    contain different words in them and in different proportions. Additionally, a
    particular word can be associated with more than one topic, but for some topics
    it will appear at a higher frequency than others. Documents may have a central
    topic, but they may also discuss other topics as well, and therefore we can think
    of a document as dealing with a mixture of topics. A topic that is more important
    in a document will be so because a greater percentage of the words in the document
    deal with that topic.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的生成过程中，我们使用了两个不同参数化的狄利克雷分布，一个用于抽取主题的多项式分布，另一个用于抽取单词的多项式分布。尽管模型很简单，但它确实捕捉到了关于文档和主题的一些直觉。特别是，它捕捉到了这样一个观点：关于不同主题的文档通常将包含不同的单词，并且比例也不同。一个特定的单词可以与多个主题相关联，但对于某些主题，它出现的频率可能比其他主题高。文档可能有一个中心主题，但它们也可能讨论其他主题，因此我们可以将文档视为处理主题的混合。一个在文档中更重要的主题将是因为文档中处理该主题的单词百分比更高。
- en: Dirichlet distributions can be smooth or skewed, and the mixture of components
    can be controlled via the *α[k]* parameters. Consequently, by tuning the Dirichlet
    distributions appropriately, this process can produce documents with a single
    theme as well as documents covering many topics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 狄利克雷分布可以是平滑的或偏斜的，组件的混合可以通过*α[k]*参数来控制。因此，通过适当调整狄利克雷分布，这个过程可以生成具有单一主题的文档，也可以生成涵盖许多主题的文档。
- en: At the same time, it is important to bear in mind the limitations of the model
    through some of the simplifying assumptions that it makes. The model completely
    ignores the word order inside a document, and the generative process is memoryless
    in that, when it generates the *n^(th)* word of a document, it does not take into
    account the existing *n-1* words that were previously drawn for that document.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，重要的是要记住模型通过它所做出的某些简化假设的限制。该模型完全忽略了文档内部的单词顺序，并且在生成过程中是无记忆的，这意味着当它生成文档的第*n*个单词时，它不会考虑之前为该文档抽取的*n-1*个单词。
- en: Furthermore, LDA does not attempt to model any relationships between the topics
    that are drawn for a document so that we do not try to organize topics that are
    more likely to co-occur, such as weather and travel or biology and chemistry.
    This is a significant limitation of the LDA model, for which there are proposed
    solutions. For example, one variant of LDA, known as the **Correlated Topic Model**
    (**CTM**), follows the same generative process as LDA but uses a different distribution
    that allows one to also model the correlations between the topics. In our experimental
    section, we will also see an implementation of the CTM model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LDA不试图对文档中抽取的主题之间的关系进行建模，因此我们不会尝试组织更可能共同出现的主题，例如天气和旅行或生物学和化学。这是LDA模型的一个重大限制，为此已经提出了解决方案。例如，LDA的一个变体，称为**相关主题模型**（**CTM**），遵循与LDA相同的生成过程，但使用不同的分布，允许对主题之间的相关性进行建模。在我们的实验部分，我们还将看到CTM模型的实现。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The correlated topic model was presented in *A Correlated Topic Model of Science*
    by *D. M. Blei* and *J. D. Lafferty*, published by the *Annals of Applied Statistics*
    in 2007.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 相关主题模型在D. M. Blei和J. D. Lafferty的《科学相关主题模型》一文中提出，该文由2007年的《应用统计年鉴》出版。
- en: Fitting an LDA model
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将LDA模型拟合
- en: Fitting an LDA model to a corpus of documents essentially involves computationally
    estimating the multinomial topic and word distributions, *φ[k]* and *θ[d]*, that
    would most likely be able to generate the data, assuming the LDA generative process.
    These variables are hidden or latent, which explains why the method is known as
    LDA.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将LDA模型拟合到文档语料库本质上涉及通过LDA生成过程，计算估计最有可能生成数据的多项式主题和单词分布，即*φ[k]*和*θ[d]*。这些变量是隐藏的或潜在的，这也是为什么这种方法被称为LDA的原因。
- en: A number of optimization procedures have been proposed to solve this problem,
    but the mathematical details are beyond the scope of this book. We will mention
    two of these, which we will encounter in the next section. The first method is
    known as **Variational Expectation Maximization** (**VEM**) and is a variant of
    the well-known **Expectation Maximization** (**EM**) algorithm. The second is
    known as Gibbs sampling, and is a method based on **Markov Chain Monte Carlo**
    (**MCMC**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了多种优化过程来解决此问题，但数学细节超出了本书的范围。我们将提到其中两种，我们将在下一节中遇到。第一种方法被称为**变分期望最大化**（**VEM**），是著名的**期望最大化**（**EM**）算法的一个变体。第二种被称为Gibbs抽样，是一种基于**马尔可夫链蒙特卡洛**（**MCMC**）的方法。
- en: Note
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For a tutorial on the EM algorithm and VEM, we recommend *The Variational Approximation
    for Bayesian Inference* by *Dimitris G. Tzikas* and others in the November 2008
    issue of the *IEEE Signal Processing Magazine*. For Gibbs sampling, there is a
    1992 article in *The American Statistician* by *George Casella*, entitled *Explaining
    the Gibbs Sampler*. Both are readable, but quite technical. A more thorough tutorial
    on Gibbs sampling is *Gibbs Sampling for the Uninitiated* by *Philip Resnik* and
    *Eric Hardisty*. This last reference is less mathematically demanding and can
    be found online at [http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf](http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于EM算法和VEM的教程，我们推荐*《贝叶斯推理的变分近似》*，由*Dimitris G. Tzikas*等人撰写，发表在2008年11月的*IEEE信号处理杂志*上。对于Gibbs抽样，有一篇1992年的文章发表在*《美国统计学家》*上，题为*解释Gibbs抽样器*。这两篇文章都很好读，但相当技术性。关于Gibbs抽样的更全面的教程是*《Gibbs抽样入门》*，由*Philip
    Resnik*和*Eric Hardisty*撰写。最后一个参考资料数学要求较低，可以在[http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf](http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf)在线找到。
- en: Modeling the topics of online news stories
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线新闻故事的主题建模
- en: To see how topic models perform on real data, we will look at two datasets containing
    articles originating from BBC News during the period of 2004-2005\. The first
    dataset, which we will refer to as the *BBC dataset*, contains 2,225 articles
    that have been grouped into five topics. These are *business*, *entertainment*,
    *politics*, *sports*, and *technology*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看主题模型在真实数据上的表现，我们将查看两个包含2004-2005年期间来自BBC新闻的文章的数据集。第一个数据集，我们将称之为*BBC数据集*，包含2,225篇文章，这些文章被分为五个主题。这些主题是*商业*、*娱乐*、*政治*、*体育*和*技术*。
- en: The second dataset, which we will call the *BBCSports dataset*, contains 737
    articles only on sports. These are also grouped into five categories according
    to the type of sport being described. The five sports in question are *athletics*,
    *cricket*, *football*, *rugby*, and *tennis*. Our objective will be to see if
    we can build topic models for each of these two datasets that will group together
    articles from the same major topic.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据集，我们将称之为*BBCSports数据集*，仅包含737篇关于体育的文章。这些文章也根据所描述的体育类型分为五个类别。涉及的五种运动是*田径*、*板球*、*足球*、*橄榄球*和*网球*。我们的目标将是看看我们是否可以为这两个数据集中的每一个构建主题模型，这些模型将把同一主要主题的文章分组在一起。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Both BBC datasets were presented in a paper by *D. Greene* and *P. Cunningham*,
    entitled *Producing Accurate Interpretable Clusters from High-Dimensional Data*
    and published in the proceedings of the 9th European Conference on *Principles
    and Practice of Knowledge Discovery in Databases (PKDD'05)* in October 2005.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 两个BBC数据集都由*D. Greene*和*P. Cunningham*在2005年10月发表的论文中提出，该论文题为*从高维数据生成准确可解释的聚类*，并发表在第九届欧洲知识发现与数据挖掘会议（PKDD'05）的论文集中。
- en: The two datasets can be found at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    When downloaded, each dataset is a folder containing a few different files. We
    will use the variables `bbc_folder` and `bbcsports_folder` to store the paths
    of these folders on our computer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数据集可以在[http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html)找到。下载后，每个数据集都是一个包含几个不同文件的文件夹。我们将使用变量`bbc_folder`和`bbcsports_folder`来存储这些文件夹在计算机上的路径。
- en: Each folder contains three important files. The file with the extension `.mtx`
    is essentially a file containing a term document matrix in sparse matrix form.
    Concretely, the rows of the matrix are terms that can be found in the articles
    and the columns are the articles themselves. An entry *M[i,j]* in this matrix
    contains the number of times the term corresponding to row *i* was found in the
    document corresponding to column *j*. A term document matrix is thus a transposed
    document term matrix, which we encountered in [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Probabilistic Graphical Models*. The
    specific format used to store the matrix in the file is a format known as the
    **Matrix Market format**, where each line corresponds to a nonempty cell in the
    matrix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件夹包含三个重要文件。具有`.mtx`扩展名的文件实际上是一个包含稀疏矩阵形式的项文档矩阵的文件。具体来说，矩阵的行是可以在文章中找到的术语，列是文章本身。矩阵中的条目*M[i,j]*包含对应于行*i*的术语在对应于列*j*的文档中出现的次数。因此，项文档矩阵是一个转置的文档术语矩阵，我们在[第8章](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "第8章。降维")的*概率图模型*中遇到过。在文件中存储矩阵的特定格式是一种称为**矩阵市场格式**的格式，其中每一行对应于矩阵中的一个非空单元格。
- en: Typically, when working with text such as news articles, we need to perform
    some preprocessing steps, such as stop-word removal, just as we did when using
    the `tm` package in our example on sentiment analysis in [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Probabilistic Graphical Models*. Fortunately
    for us, the articles in these datasets have already been processed so that they
    have been stemmed; stop words have been removed, as have any terms that appear
    fewer than three times.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们处理文本，如新闻文章时，我们需要执行一些预处理步骤，例如去除停用词，就像我们在[第8章](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "第8章。降维")的例子中使用`tm`包进行情感分析时做的那样，*概率图模型*。幸运的是，这些数据集中的文章已经过处理，以便它们已经被词干提取；停用词已被移除，以及任何出现次数少于三次的术语。
- en: In order to interpret the term document matrix, the files with the extension
    `.terms` contain the actual terms, one per line, which are the row names of the
    term document matrix. Similarly, the document names that are the column names
    in the term document matrix are stored in the files with the extension `.docs`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释项文档矩阵，具有`.terms`扩展名的文件包含实际的术语，每行一个，它们是项文档矩阵的行名。同样，项文档矩阵中的文档名（列名）存储在具有`.docs`扩展名的文件中。
- en: 'We first create variables for the paths to the three files that we need for
    each dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为每个数据集所需的三个文件路径创建变量：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to load data into R from a file in Market Matrix format, we can use
    the `readMM()` function from the `Matrix` R package. This function loads the data
    and stores it into a sparse matrix object. We can convert this into a term document
    matrix that the `tm` package can interpret, using the `as.TermDocumentMatrix()`
    function in the `tm` package. Aside from the matrix object that is the first argument
    to that function, we also need to specify the `weighting` parameter. This parameter
    describes what the numbers in the original matrix correspond to. In our case,
    we have raw term frequencies, so we specify the value `weightTf`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据从Market Matrix格式的文件加载到R中，我们可以使用`Matrix` R包中的`readMM()`函数。这个函数将数据加载并存储到一个稀疏矩阵对象中。我们可以使用`tm`包中的`as.TermDocumentMatrix()`函数将这个稀疏矩阵转换为`tm`包可以解释的项文档矩阵。除了该函数的第一个参数矩阵对象外，我们还需要指定`weighting`参数。这个参数描述了原始矩阵中的数字代表什么。在我们的例子中，我们有原始的词频，所以我们指定值为`weightTf`：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we load the terms and document identifiers from the two remaining files
    and use these to create appropriate row and column names respectively for the
    term document matrices. We can use the standard `scan()` function to read files
    with a single entry per line and load the entries into vectors. Once we have a
    term vector and a document identifier vector, we will use these to update the
    row and column names for the term document matrix. Finally, we''ll transpose this
    matrix into a document term matrix, as this is the format we will need for subsequent
    steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载剩余两个文件中的术语和文档标识符，并分别使用这些来为术语-文档矩阵创建适当的行和列名称。我们可以使用标准的`scan()`函数来读取每行只有一个条目的文件，并将条目加载到向量中。一旦我们有了术语向量和文档标识符向量，我们将使用这些来更新术语-文档矩阵的行和列名称。最后，我们将这个矩阵转置成一个文档-术语矩阵，因为这是我们后续步骤所需的格式：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have the document term matrices for our two datasets ready. We can see
    that there are roughly twice as many terms for the BBC dataset as there are for
    the BBCSports dataset, and the latter also has about a third of the number of
    documents, so it is a much smaller dataset. Before we build our topic models,
    we must also create the vectors containing the original topic classification of
    the articles. If we examine the document IDs, we can see that the format of each
    document identifier is `<topic>.<counter>`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了两个数据集的文档-术语矩阵。我们可以看到BBC数据集的术语数量大约是BBCSports数据集的两倍，后者也有大约三分之一的文档数量，因此它是一个规模较小的数据集。在我们构建主题模型之前，我们还必须创建包含文章原始主题分类的向量。如果我们检查文档ID，我们可以看到每个文档标识符的格式是`<topic>.<counter>`：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To create a vector with the correct topic assignments, we simply need to strip
    out the last four characters of each entry. If we then convert the result into
    a factor, we can see how many documents we have per topic:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建具有正确主题分配的向量，我们只需简单地去除每个条目最后的四个字符。如果我们然后将结果转换为因子，我们就可以看到每个主题有多少个文档：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This shows that the BBC dataset is fairly even in the distribution of its topics.
    In the BBCSports data, however, we see that there are roughly twice as many articles
    on football than the other four sports.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明BBC数据集在主题分布上相当均匀。然而，在BBCSports数据中，我们发现足球文章的数量大约是其他四种运动的两倍。
- en: 'For each of our two datasets, we will now build some topic models using the
    package `topicmodels`. This is a very useful package as it allows us to use data
    structures created with the `tm` package to perform topic modeling. For each dataset,
    we will build the following four different topic models:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的两个数据集，我们现在将使用`topicmodels`包构建一些主题模型。这是一个非常有用的包，因为它允许我们使用`tm`包创建的数据结构来执行主题建模。对于每个数据集，我们将构建以下四个不同的主题模型：
- en: '`LDA_VEM`: This is an LDA model trained with the **Variational Expectation
    Maximization** (**VEM**) method. This method automatically estimates the `α`Dirichlet
    parameter vector.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LDA_VEM`: 这是一个使用**变分期望最大化**（**VEM**）方法训练的LDA模型。这种方法自动估计`α`Dirichlet参数向量。'
- en: '`LDA_VEM_α`: This is an LDA model trained with VEM, but the difference here
    is that the `α`Dirichlet parameter vector is not estimated.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LDA_VEM_α`: 这是一个使用VEM训练的LDA模型，但这里的区别在于`α`Dirichlet参数向量没有估计。'
- en: '`LDA_GIB`: This is an LDA model trained with Gibbs sampling.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LDA_GIB`: 这是一个使用吉布斯抽样的LDA模型。'
- en: '`CTM_VEM`: This is an implementation of the **Correlated Topic Model** (**CTM**)
    model trained with VEM. Currently, the `topicmodels` package does not support
    training this method with Gibbs sampling.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CTM_VEM`: 这是一个使用VEM训练的**相关主题模型**（**CTM**）的实现。目前，`topicmodels`包不支持使用吉布斯抽样的方法进行训练。'
- en: To train an LDA model, the `topicmodels` package provides us with the `LDA()`
    function. We will use four key parameters for this function. The first of these
    specifies the document term matrix for which we want to build an LDA model. The
    second of these, `k`, specifies the target number of topics we want to have in
    our model. The third parameter, `method`, allows us to select which training algorithm
    to use. This is set to `VEM` by default, so we only need to specify this for our
    `LDA_GIB` model , which uses Gibbs sampling.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个LDA模型，`topicmodels`包为我们提供了`LDA()`函数。我们将为此函数使用四个关键参数。第一个参数指定我们想要为它构建LDA模型的文档-术语矩阵。第二个参数`k`指定我们希望在模型中拥有的目标主题数量。第三个参数`method`允许我们选择要使用的训练算法。默认情况下，它设置为`VEM`，所以我们只需要指定我们的`LDA_GIB`模型，它使用吉布斯抽样。
- en: Finally, there is a `control` parameter, which takes in a list of parameters
    that affect the fitting process. As there is an inherent random component involved
    in the training of topic models, we can specify a `seed` parameter in this list
    in order to make the results reproducible. Additionally, this is where we can
    specify whether we want to estimate the *α*Dirichlet parameter. This is also where
    we can include parameters for the Gibbs sampling procedure, such as the number
    of omitted Gibbs iterations at the start of the training procedure (`burnin`),
    the number of omitted in-between iterations (`thin`), and the total number of
    Gibbs iterations (`iter`). To train a CTM model, the `topicmodels` package provides
    us with the `CTM()` function, which has a similar syntax to the `LDA()` function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有一个`控制`参数，它接受一个影响拟合过程的参数列表。由于主题模型的训练中涉及固有的随机成分，我们可以在该列表中指定一个`种子`参数，以便使结果可重复。此外，这也是我们指定是否想要估计*α*Dirichlet参数的地方。这也是我们可以包括吉布斯抽样过程参数的地方，例如训练过程开始时省略的吉布斯迭代次数（`burnin`）、省略的中间迭代次数（`thin`）和总的吉布斯迭代次数（`iter`）。为了训练一个CTM模型，`topicmodels`包为我们提供了`CTM()`函数，其语法与`LDA()`函数类似。
- en: 'Using this knowledge, we''ll define a function that creates a list of four
    trained models given a particular document term matrix, the number of topics required,
    and the seed. For this function, we have used some standard values for the aforementioned
    training parameters with which the reader is encouraged to experiment, ideally
    after investigating the references provided for the two optimization methods:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些知识，我们将定义一个函数，该函数根据特定的文档词矩阵、所需的主题数量和种子创建一个包含四个训练模型的列表。对于这个函数，我们使用了上述训练参数的一些标准值，鼓励读者进行实验，理想情况下是在调查了两种优化方法提供的参考文献之后：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We''ll now use this function to train a list of models for the two datasets:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用这个函数为两个数据集训练一系列模型：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To get a sense of how the topic models have performed, let's first see whether
    the five topics learned by each model correspond to the five topics to which the
    articles were originally assigned. Given one of these trained models, we can use
    the `topics()` function to get a vector of the most likely topic chosen for each
    document.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解主题模型的表现，让我们首先看看每个模型学习的五个主题是否对应于文章最初分配的五个主题。给定这些训练模型之一，我们可以使用`topics()`函数来获取每个文档最可能选择的主题的向量。
- en: 'This function actually takes a second parameter, *k*, which is by default set
    to `1` and returns the top *k* topics predicted by the model. We only want one
    topic per model in this particular instance. Having found the most likely topic,
    we can then tabulate the predicted topics against the vector of labeled topics.
    These are the results for the `LDA_VEM` model for the BBC dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数实际上接受第二个参数，*k*，默认设置为`1`，并返回模型预测的前* k* 个主题。在这个特定实例中，我们只想在每个模型中有一个主题。找到最可能的主题后，我们就可以将预测的主题与标记主题的向量进行表格化。这是BBC数据集的`LDA_VEM`模型的预测结果：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Looking at this table, we can see that topic 5 corresponds almost exclusively
    to the *sports* category. Similarly, topics 4 and 3 seem to match the *politics*
    and *business* categories respectively. Unfortunately, models 1 and 2 both contain
    a mixture of *entertainment* and *technology* articles, and as a result this model
    hasn't really succeeded in distinguishing between the categories that we want.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 看这张表格，我们可以看到主题5几乎完全对应于*体育*类别。同样，主题4和3似乎分别对应于*政治*和*商业*类别。不幸的是，模型1和2都包含*娱乐*和*技术*文章的混合，因此这个模型并没有真正成功地区分我们想要的类别。
- en: 'It should be clear that, in an ideal situation, each model topic should match
    to one gold topic (we often use the adjective *gold* to refer to the correct or
    labeled value of a particular variable. This is derived from the expression *gold
    standard*, which refers to a widely accepted standard). We can repeat this process
    on the `LDA_GIB` model, where the story is different:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 应该很明显，在理想情况下，每个模型主题应该匹配到一个金主题（我们经常用形容词*金*来指代特定变量的正确或标记值。这源于表达*金标准*，它指的是一个广泛接受的标准）。我们可以在`LDA_GIB`模型上重复这个过程，那里的情况不同：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Intuitively, we feel that this topic model is a better match to our original
    topics than the first, as evidenced by the fact that each model topic selects
    articles from primarily one gold topic.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地感觉，这个主题模型比第一个模型更好地匹配我们的原始主题，正如每个模型主题主要选择一个金主题的文章所证明的那样。
- en: 'A rough way to estimate the quality of the match between a topic model and
    our target vector of topics is to say that the largest value in every row corresponds
    to the gold topic assigned to the model topic represented by that row. Then, the
    total accuracy is the ratio of these maximum row values over the total number
    of documents. In the preceding example, for the `LDA_GIB` model, this number would
    be *(471+506+371+399+364)/2225 = 2111/2225= 94.9 %*. The following function computes
    this value given a model and a vector of gold topics:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 估计主题模型与我们的目标主题向量之间匹配质量的一个粗略方法是说，每一行中的最大值对应于分配给该行表示的模型主题的黄金主题。然后，总准确度是这些最大行值与总文档数的比率。在先前的例子中，对于`LDA_GIB`模型，这个数字将是*(471+506+371+399+364)/2225
    = 2111/2225= 94.9 %*。以下函数计算给定模型和黄金主题向量时的这个值：
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Using this notion of accuracy, let''s see which model performs better in our
    two datasets:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个准确度的概念，让我们看看在两个数据集中哪个模型表现更好：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For the BBC dataset, we see that the `LDA_GIB` model significantly outperforms
    the others and the `CTM_VEM` model is significantly worse than the LDA models.
    For the BBCSports dataset, all the models perform roughly the same, but the `LDA_VEM`
    model is slightly better.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BBC数据集，我们看到`LDA_GIB`模型显著优于其他模型，而`CTM_VEM`模型则显著劣于LDA模型。对于BBCSports数据集，所有模型的表现大致相同，但`LDA_VEM`模型略好。
- en: 'Another way to assess the quality of a model fit is by computing the log likelihood
    of the data given the model, remembering that the larger this value, the better
    the fit. We can do this with the `logLik()` function in the `topicmodels` package,
    which suggests that the best model is the LDA model trained with Gibbs sampling
    in both cases:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型拟合质量的另一种方法是计算给定模型的数据的对数似然，记住这个值越大，拟合越好。我们可以使用`topicmodels`包中的`logLik()`函数来做这件事，它表明在两种情况下，最佳模型都是使用吉布斯采样训练的LDA模型：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Model stability
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型稳定性
- en: It turns out that the random component of the optimization procedures involved
    in fitting these models often has a significant impact on the model that is trained.
    Put differently, we may find that if we use different random number seeds, the
    results may sometimes change appreciably.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在拟合这些模型的过程中，优化程序中的随机成分往往对训练的模型有显著影响。换句话说，我们可能会发现，如果我们使用不同的随机数种子，结果有时可能会显著变化。
- en: 'Ideally, we would like our model to be **stable**, which is to say that we
    would like the effect of the initial conditions of the optimization procedure,
    which are determined by a random number seed, to be minimal. It is a good idea
    to investigate the effect of different seeds on our four models by training them
    on multiple seeds:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望我们的模型是**稳定的**，这意味着我们希望优化过程的初始条件（由随机数种子确定）的影响最小。通过在多个种子上训练我们的四个模型来研究不同种子对模型的影响是一个好主意：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we used a sequence of five consecutive seeds and trained our models on
    both datasets five times. Having done this, we can investigate the accuracy of
    our models for the various seeds. If the accuracy of a method does not vary by
    a large degree across the seeds, we can infer that the method is quite stable
    and produces similar topic models (although, in this case, we are only considering
    the most prominent topic per document).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了五个连续的种子序列，并在两个数据集上分别训练了五次模型。完成这些后，我们可以调查不同种子下我们模型的准确度。如果一个方法的准确度在种子之间变化不大，我们可以推断该方法相当稳定，并产生相似的主题模型（尽管在这种情况下，我们只考虑每份文档中最突出的主题）。
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: On both datasets, we can clearly see that Gibbs sampling results in a more stable
    model and, in the case of the BBC dataset, it is also the clear winner in terms
    of accuracy. Gibbs sampling generally tends to produce more accurate models but,
    even though it was not readily apparent on these datasets, it can become significantly
    slower than VEM methods once the dataset becomes large.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个数据集中，我们可以清楚地看到，吉布斯采样导致模型更稳定，在BBC数据集中，它在准确度方面也是明显的赢家。吉布斯采样通常倾向于产生更准确的模型，但尽管在这些数据集中并不明显，一旦数据集变得很大，它可能会比VEM方法慢得多。
- en: The two LDA models trained with variational methods exhibit scores that vary
    within a roughly 10 % range on both datasets. On both datasets, we see that `LDA_VEM`
    is consistently better than `LDA_VEM_a` by a small amount. This method also produces,
    on average, better accuracy among all models in the BBCSports dataset. The CTM
    model is the least stable of all the models, exhibiting a high degree of variance
    on both datasets. Interestingly, though, the highest performance of the CTM model
    across the five iterations performs marginally worse than the best accuracy possible
    using the other methods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变分方法训练的两个LDA模型在两个数据集上都表现出大约10%的分数变化。在两个数据集上，我们看到`LDA_VEM`始终比`LDA_VEM_a`略好。这种方法在BBCSports数据集中，平均而言，所有模型中的准确性也更好。CTM模型是最不稳定的模型，在两个数据集上都表现出很高的变异性。有趣的是，尽管如此，CTM模型在五次迭代中的最高性能略逊于使用其他方法可能达到的最佳准确性。
- en: If we see that our model is not very stable across a few seeded iterations,
    we can specify the `nstart` parameter during training, which specifies the number
    of random restarts that are used during the optimization procedure. To see how
    this works in practice, we have created a modified `compute_model_list()` function
    that we named `compute_model_list_r()`, which takes in an extra parameter, `nstart`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们发现我们的模型在几个种子迭代中不是很稳定，我们可以在训练期间指定`nstart`参数，该参数指定了在优化过程中使用的随机重启次数。为了看到这在实践中是如何工作的，我们创建了一个修改后的`compute_model_list()`函数，我们将其命名为`compute_model_list_r()`，它接受一个额外的参数，`nstart`。
- en: 'The other difference is that the `seed` parameter now needs a vector of seeds
    with as many entries as the number of random restarts. To deal with this, we will
    simply create a suitably sized range of seeds starting from the one provided.
    Here is our new function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不同之处在于，现在的`seed`参数需要一个与随机重启次数一样多的种子向量。为了处理这个问题，我们将简单地从提供的种子开始创建一个适当大小的种子范围。以下是我们的新函数：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will use this function to create a new model list. Note that using random
    restarts means we are increasing the amount of time needed to train, so these
    next few commands will take some time to complete:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个函数来创建一个新的模型列表。请注意，使用随机重启意味着我们正在增加训练所需的时间，所以接下来的几个命令将需要一些时间才能完成：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that, even after using only five random restarts, the accuracy of the models
    has improved. More importantly, we now see that using random restarts has overcome
    the fluctuations that the CTM model experiences, and as a result it is now performing
    almost as well as the best model in each dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，即使只使用了五个随机重启，模型的准确性也得到了提高。更重要的是，我们现在看到使用随机重启已经克服了CTM模型所经历的波动，因此现在它的表现几乎与每个数据集中最好的模型一样好。
- en: Finding the number of topics
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找主题数量
- en: In this predictive task, the number of different topics was known beforehand.
    This turned out to be very important because it is provided as an input to the
    functions that trained our models. The number of topics might not be known when
    we are using topic modeling as a form of exploratory analysis where our goal is
    simply to cluster documents together based on the similarity of their topics.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个预测任务中，不同主题的数量是事先已知的。这证明非常重要，因为它被用作训练我们模型的函数的输入。当我们使用主题建模作为探索性分析的一种形式时，我们的目标是简单地根据主题的相似性将文档聚在一起，这时主题的数量可能并不知道。
- en: This is a challenging question and bears some similarity to the general problem
    of selecting the number of clusters when we perform clustering. One proposed solution
    to this problem is to perform cross-validation over a range of different numbers
    of topics. This approach will not scale well at all when the dataset is large,
    especially since training a single topic model is already quite computationally
    intensive when we factor, issues such as random restarts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具有挑战性的问题，与我们在执行聚类时选择聚类数量的通用问题有一些相似之处。针对这个问题的一个解决方案是在不同数量的主题范围内进行交叉验证。当数据集很大时，这种方法根本无法扩展，特别是当我们考虑到训练单个主题模型已经相当计算密集，尤其是考虑到随机重启等问题。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A paper that discusses a number of different approaches for estimating the number
    of topics in topic models is *Reconceptualizing the classification of PNAS articles*
    by *Edoardo M. Airoldi* and others. This appears in the *Proceedings of the National
    Academy of Sciences*, volume 107, 2010.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇讨论了多种估计主题模型中主题数量方法的文章是*Edoardo M. Airoldi*和其他人撰写的*重新概念化PNAS文章的分类*。这篇文章发表在*美国国家科学院院刊*，第107卷，2010年。
- en: Topic distributions
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题分布
- en: 'We saw in the description of the generative process that we use a Dirichlet
    distribution to sample a multinomial distribution of topics. In the `LDA_VEM`
    model, the *αk* parameter vector is estimated. Note that, in all cases, a symmetric
    distribution is used in this implementation so that we are only estimating the
    value of *α*, which is the value that all the *α[k]* parameters take on. For LDA
    models, we can investigate which value of this parameter is used with and without
    estimation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成过程的描述中，我们看到了我们使用狄利克雷分布来采样主题的多项分布。在`LDA_VEM`模型中，估计*αk*参数向量。请注意，在所有情况下，此实现中均使用对称分布，因此我们只估计*α*的值，这是所有*α[k]*参数所取的值。对于LDA模型，我们可以调查在估计和不估计的情况下使用此参数的哪个值：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As we can see, when we estimate the value of *α*, we obtain a much lower value
    of *α* than we use by default, indicating that, for both datasets, the topic distribution
    is thought to be peaky. We can use the `posterior()` function in order to view
    the distribution of topics for each model. For example, for the `LDA_VEM` model
    on the BBC dataset, we find the following distributions of topics for the first
    few articles:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，当我们估计*α*的值时，我们获得的*α*值比默认使用的值要低得多，这表明对于两个数据集，主题分布被认为是峰值的。我们可以使用`posterior()`函数来查看每个模型的主题分布。例如，对于BBC数据集上的`LDA_VEM`模型，我们发现前几篇文章的主题分布如下：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following plot is a histogram of the posterior probability of the most
    likely topic predicted by our four models. The `LDA_VEM` model assumes a very
    peaky distribution, whereas the other models have a wider spread. The `CTM_VEM`
    model also has a peak at very high probabilities but, unlike `LDA_VEM`, the probability
    mass is spread over a wide range of values. We can see that the minimum probability
    for the most likely topic is 0.2 because we have five topics:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图是四个模型预测的最可能主题的后验概率直方图。`LDA_VEM`模型假设一个非常峰值分布，而其他模型则分布更广。`CTM_VEM`模型在非常高的概率处也有峰值，但与`LDA_VEM`不同，概率质量分布在广泛的值范围内。我们可以看到，最可能主题的最小概率为0.2，因为我们有五个主题：
- en: '![Topic distributions](img/00189.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![主题分布](img/00189.jpeg)'
- en: 'Another approach to estimating the smoothness of the topic distributions is
    to compute the *model entropy*. We will define this as the average entropy of
    all the topic distributions across the different documents. Smooth distributions
    will exhibit higher entropy than peaky distributions. To compute the entropy of
    our model, we will define two functions. The function `compute_entropy()` computes
    the entropy of a particular topic distribution of a document, and the `compute_model_mean_entropy()`
    function computes the average entropy across all the different documents in the
    model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种估计主题分布平滑度的方法是计算*模型熵*。我们将定义为不同文档中所有主题分布的平均熵。平滑分布将比峰值分布具有更高的熵。为了计算我们模型的熵，我们将定义两个函数。函数`compute_entropy()`计算文档特定主题分布的熵，而`compute_model_mean_entropy()`函数计算模型中所有不同文档的平均熵：
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Using these functions, we can compute the average model entropies for the models
    trained on our two datasets:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，我们可以计算在我们两个数据集上训练的模型的平均模型熵：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These results are consistent with what the preceding plots show, which is that
    the `LDA_VEM` model, which is the peakiest, has a much lower entropy than the
    other models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与前面的图表显示的一致，即峰值最高的`LDA_VEM`模型比其他模型的熵要低得多。
- en: Word distributions
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词分布
- en: 'Just as with the previous section, where we looked at the distribution of topics
    across different documents, we are often also interested in understanding the
    most important terms that are frequent in documents that are assigned to the same
    topic. We can see the *k* most frequent terms of the topics of a model using the
    function `terms()`. This takes in a model and a number specifying the number of
    most frequent terms that we want retrieved. Let''s see the 10 most important words
    per topic in the `LDA_GIB` model of the BBC dataset:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一个部分，我们查看不同文档的主题分布一样，我们通常也感兴趣于了解分配给相同主题的文档中频繁出现的重要术语。我们可以使用`terms()`函数查看模型主题的*
    k* 个最频繁术语。这个函数接受一个模型和一个指定我们想要检索的最频繁术语数量的数字。让我们看看BBC数据集中`LDA_GIB`模型的每个主题的10个最重要的单词：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see, given this list of word stems, one could easily guess which of
    the five topic labels we should assign to each topic. A very handy way to visualize
    frequent terms in a collection of documents is through a **word cloud**. The R
    package `wordcloud` is useful for creating these. The function `wordcloud()` allows
    us to specify a vector of terms followed by a vector of their frequencies, and
    this information is then used for plotting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，给定这个词根列表，我们可以很容易地猜测我们应该将哪个五个主题标签分配给每个主题。通过词云可视化文档集中的频繁词是一个非常方便的方法。R包`wordcloud`对于创建这些词云非常有用。`wordcloud()`函数允许我们指定一个术语向量以及它们的频率向量，然后这些信息被用于绘图。
- en: 'Unfortunately, we will have to do some manipulation on the document term matrices
    in order to get the word frequencies by topic so that we can feed them into this
    function. To that end, we''ve created our own function `plot_wordcloud()`, as
    follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们将不得不对文档词矩阵进行一些操作，以便通过主题计算词频，这样我们就可以将它们输入到这个函数中。为此，我们创建了自己的函数`plot_wordcloud()`，如下所示：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our function takes in a model, a document term matrix, an index of a topic,
    and the number of most frequent terms that we want to display in the word cloud.
    We begin by first computing the most frequent terms for the model by topic, as
    we did earlier. We also compute the most probable topic assignments. Next, we
    subset the document term matrix so that we obtain only the cells involving the
    terms we are interested in and the documents corresponding to the topic with the
    index that we passed in as a parameter.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能接受一个模型、一个文档词矩阵、一个主题索引以及我们希望在词云中显示的最频繁词的数量。我们首先通过主题计算模型中最频繁的词，就像我们之前做的那样。我们还计算了最可能的主题分配。接下来，我们子集文档词矩阵，以便我们只获得涉及我们感兴趣的词和对应于我们作为参数传递的索引的主题的文档的单元格。
- en: 'From this reduced document term matrix, we sum over the columns to compute
    the frequencies of the most frequent terms, and finally we plot the word cloud.
    We''ve used this function to plot word clouds for the topics in the BBC dataset
    using the `LDA_GIB` model and 25 words per topic. This is shown here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个简化的文档词矩阵中，我们对列进行求和以计算最频繁词的频率，最后我们绘制词云。我们使用这个函数绘制了BBC数据集中主题的词云，使用了`LDA_GIB`模型和每个主题25个词。如下所示：
- en: '![Word distributions](img/00190.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![词分布](img/00190.jpeg)'
- en: LDA extensions
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA扩展
- en: Topic models are an active area of research, and as a result several extensions
    for the LDA model have been proposed. We will briefly mention two of these. The
    first is the **supervised LDA** model, an implementation of which can be found
    in the `lda` R package. This is a more direct way to model a response variable
    with the standard LDA method and would be a good next step for investigating the
    application discussed in this chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型是研究的热点领域，因此已经提出了几个LDA模型的扩展。我们将简要介绍其中的两个。第一个是**监督LDA**模型，其实现可以在`lda`R包中找到。这是一种更直接的方式，使用标准LDA方法来建模响应变量，并且将是调查本章讨论的应用的一个很好的下一步。
- en: A second interesting extension is the **author-topic model**. This is designed
    to add an extra step in the generative process to account for authorship information
    and is a good model to use when building models that summarize or predict the
    writing habits and topics of authors.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个有趣的扩展是**作者-主题模型**。这是为了在生成过程中添加一个额外的步骤来考虑作者信息而设计的，当构建总结或预测作者写作习惯和主题的模型时，这是一个很好的模型。
- en: Note
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The standard reference for supervised LDA is the paper *Supervised Topic Models*
    by *David M. Blei* and *Jon D. McAuliffe*. This was published in 2007 in the journal
    *Neural Information Processing Systems*. For the author-topic model, consult the
    paper entitled *The Author-Topic Model for Authors and Documents* by *Michal Rosen-Zvi*
    and others. This appears in the proceedings of the *20th conference on Uncertainty
    In Artificial Intelligence*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 监督LDA的标准参考文献是David M. Blei和Jon D. McAuliffe合著的论文《Supervised Topic Models》。该论文发表于2007年《神经信息处理系统》期刊。对于作者-主题模型，请参阅Michal
    Rosen-Zvi等人撰写的论文《The Author-Topic Model for Authors and Documents》。该论文发表在《第20届不确定人工智能会议》的论文集中。
- en: Modeling tweet topics
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模推文主题
- en: In machine learning and natural language processing, a *topic model* is a type
    of statistical model used to discover the abstract topics that occur in a collection
    of documents. A good example or use case to illustrate this concept is *Twitter*.
    Suppose we could analyze an individual's (or an organization's) tweets to discover
    any overriding trend. Let's look at a simple example.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和自然语言处理中，*主题模型*是一种用于发现文档集合中出现的抽象主题的统计模型。一个很好的例子或用例来阐述这个概念是*Twitter*。假设我们可以分析个人的（或组织的）推文以发现任何主导趋势。让我们看看一个简单的例子。
- en: If you have a Twitter account, you can perform this exercise pretty easily (you
    can then apply the same process to an archive of tweets you want to focus on and/or
    model). First, we need to create a tweet archive file.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个Twitter账户，你可以非常容易地完成这个练习（然后你可以将相同的流程应用于你想要关注的推文存档和/或模型）。首先，我们需要创建一个推文存档文件。
- en: 'Under **Settings**, you can submit a request to receive your tweets in an archive
    file. Once it''s ready, you''ll get an email with a link to download it:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在**设置**中，你可以提交一个请求以接收你的推文存档文件。一旦准备好，你将收到一封包含下载链接的电子邮件：
- en: '![Modeling tweet topics](img/00191.jpeg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![推文主题建模](img/00191.jpeg)'
- en: 'And then save your file locally:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将你的文件保存在本地：
- en: '![Modeling tweet topics](img/00192.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![推文主题建模](img/00192.jpeg)'
- en: 'Now that we have a data source to work with, we can move the tweets into a
    list *object* (we''ll call it *x*) and then convert that into an R data frame
    object (df1):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了可以工作的数据源，我们可以将推文移动到一个列表*对象*（我们将它称为*x*）中，然后将其转换为R数据框对象（df1）：
- en: '![Modeling tweet topics](img/00193.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![推文主题建模](img/00193.jpeg)'
- en: 'The tweets were first converted to a *data frame* before using the R `tm` package
    to convert them to a *corpus* or Corpus collection (of text documents) object:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用R的`tm`包将推文转换为*数据框*之前，首先将推文转换为*语料库*或语料库集合（文本文档对象）：
- en: '![Modeling tweet topics](img/00194.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![推文主题建模](img/00194.jpeg)'
- en: 'Next, we convert the Corpus to a *Document-Term Matrix* object with the following
    code. This creates a *mathematical matrix* that describes the *frequency of terms*
    that occur in a collection of documents, in this case, our collection of tweets:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码将语料库转换为*文档-词矩阵*对象。这创建了一个*数学矩阵*，它描述了在文档集合中出现的*词频*，在这种情况下，我们的推文集合：
- en: '![Modeling tweet topics](img/00195.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![推文主题建模](img/00195.jpeg)'
- en: Word clouding
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词云生成
- en: 'After building a document-term matrix (shown earlier), we can more easily show
    the importance of the words found within our tweets with a *word cloud* (also
    known as a tag cloud). We can do this using the R package `wordcloud`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建文档-词矩阵（之前已展示）后，我们可以更轻松地通过*词云*（也称为标签云）来展示我们推文中找到的单词的重要性。我们可以使用R包`wordcloud`来完成这项操作：
- en: '![Word clouding](img/00196.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![词云生成](img/00196.jpeg)'
- en: 'Finally, let''s generate the word cloud visual:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们生成词云视觉图：
- en: '![Word clouding](img/00197.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![词云生成](img/00197.jpeg)'
- en: Seems like there may be a theme involved here! The word cloud shows us that
    the words **south** and **carolinas** are the most important words.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这里可能有一个主题！词云显示，单词**south**和**carolinas**是最重要的单词。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter was devoted to learning about topic models; after sentiment analysis
    on movie reviews, this was our second foray into working with real-life text data.
    This time, our predictive task was classifying the topics of news articles on
    the web. The primary technique for topic modeling on which we focused was LDA.
    This derives its name from the fact that it assumes that the topic and word distributions
    that can be found inside a document arise from hidden multinomial distributions
    that are sampled from Dirichlet priors. We saw that the generative process of
    sampling words and topics from these multinomial distributions mirrors many of
    the natural intuitions that we have about this domain; however, it signally fails
    to account for correlations between the various topics that can co-occur inside
    a document.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于学习主题模型；在电影评论的情感分析之后，这是我们第二次涉足处理真实文本数据。这次，我们的预测任务是分类网络新闻文章的主题。我们主要关注的主题建模技术是LDA。这个名字来源于它假设文档内部可以找到的主题和词分布是由从Dirichlet先验中采样的隐藏的多项式分布产生的。我们看到了从这些多项式分布中采样单词和主题的生成过程与我们对这个领域的许多自然直觉相吻合；然而，它明显没有考虑到文档内部可能同时出现的各种主题之间的相关性。
- en: In our experiments with LDA, we saw that there is more than one way to fit an
    LDA model, and in particular we saw that the method known as Gibbs sampling tends
    to be more accurate, even if it often is more computationally expensive. In terms
    of performance, we saw that, when the topics in question are quite distinct from
    each other, such as the topics in the BBC dataset, we got very high accuracy in
    our topic prediction.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的LDA实验中，我们发现拟合LDA模型的方式不止一种，特别是我们发现被称为Gibbs抽样的方法通常更准确，即使它通常计算成本更高。在性能方面，我们发现，当涉及的主题彼此非常不同时，例如BBC数据集中的主题，我们在主题预测中获得了非常高的准确率。
- en: At the same time, however, when we classified documents with topics that are
    more similar to each other, such as the different sports documents in the BBCSports
    dataset, we saw that this posed more of a challenge and our results were not quite
    as high. In our case, another factor that probably played a role is that both
    the documents and the available features were much fewer in number than in the
    BBCSports dataset. Currently, an increasing number of variations on LDA are being
    researched and developed in order to deal with limitations in both performance
    and training speed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，然而，当我们对具有更相似主题的文档进行分类，例如BBCSports数据集中的不同体育文档时，我们发现这带来了更大的挑战，我们的结果并不那么高。在我们的案例中，另一个可能起作用的因素是，文档和可用的特征数量都比BBCSports数据集要少得多。目前，越来越多的LDA变体正在被研究和开发，以应对性能和训练速度的限制。
- en: As an interesting exercise, we also downloaded an archive of tweets and used
    R commands to create a **document-term matrix** object, which we then used as
    an input for creating a word cloud object that visualized the words found within
    the tweets.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项有趣的练习，我们还下载了一个推文存档，并使用R命令创建了一个**文档-词矩阵**对象，然后我们将其用作创建可视化推文中找到的单词的词云对象的输入。
- en: Topic models can be viewed as a form of clustering, and this was our first glimpse
    in to this area. In the next chapter on recommendation systems, we will delve
    more deeply into the field of clustering in order to understand the way in which
    websites such as Amazon are able to make product recommendations by predicting
    which products a shopper is most likely to be interested in based on their previous
    shopping history and the shopping habits of similar shoppers.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型可以被视为一种聚类形式，这是我们首次涉足这个领域。在下一章关于推荐系统的章节中，我们将更深入地探讨聚类领域，以便理解像亚马逊这样的网站是如何通过预测购物者最可能感兴趣的产品来做出产品推荐的，这些预测基于他们的购物历史和类似购物者的购物习惯。
