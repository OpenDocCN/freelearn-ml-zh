- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Data Imbalance in Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的数据不平衡
- en: Class imbalanced data is a common issue for deep learning models. When one or
    more classes have significantly fewer samples, the performance of deep learning
    models can suffer as they tend to prioritize learning from the majority class,
    resulting in poor generalization for the minority class(es).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 类不平衡数据是深度学习模型的一个常见问题。当一个或多个类别样本显著较少时，深度学习模型的性能可能会受到影响，因为它们倾向于优先从多数类别中学习，从而导致少数类别（或类别组）的泛化能力较差。
- en: 'A lot of real-world data is imbalanced, which presents challenges to deep learning
    classification tasks. *Figure 6**.1* shows some common categories of imbalanced
    data problems in various deep learning applications:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的数据是不平衡的，这对深度学习分类任务提出了挑战。*图6.1*展示了在各种深度学习应用中常见的不平衡数据问题的一些类别：
- en: '![](img/B17259_06_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_01.jpg)'
- en: Figure 6.1 – Some common categories of imbalanced data problems
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 一些常见的不平衡数据问题类别
- en: 'We will cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: A brief introduction to deep learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Data imbalance in deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中的数据不平衡
- en: Overview of deep learning techniques to handle data imbalance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据不平衡的深度学习技术概述
- en: Multi-label classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签分类
- en: By the end of this chapter, we’ll have a foundational understanding of deep
    learning and neural networks. We’ll have also grasped the impact of data imbalance
    on these models and gained a high-level overview of various strategies to address
    the challenges of imbalanced data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将对深度学习和神经网络有一个基础的理解。我们还将掌握数据不平衡对这些模型的影响，并了解解决不平衡数据挑战的各种策略的高级概述。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `PyTorch`. `PyTorch` is an open source machine learning library that’s used
    for deep learning tasks and has grown in popularity recently because of its flexibility
    and ease of use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用常见的库，如`numpy`、`scikit-learn`和`PyTorch`。`PyTorch`是一个开源的机器学习库，用于深度学习任务，因其灵活性和易用性而近年来越来越受欢迎。
- en: You can install `PyTorch` using `pip` or `conda`. Visit the official PyTorch
    website ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/))
    to get the appropriate command for your system configuration.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`pip`或`conda`安装`PyTorch`。访问官方PyTorch网站([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/))以获取适合您系统配置的相应命令。
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06)。
- en: A brief introduction to deep learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Deep learning is a subfield of machine learning that focuses on artificial neural
    networks with multiple layers (deep models typically have three or more layers,
    including input, output, and hidden layers). These models have demonstrated remarkable
    capabilities in various applications, including image and speech recognition,
    natural language processing, and autonomous driving.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子领域，它专注于具有多层（深度模型通常有三层或更多，包括输入、输出和隐藏层）的人工神经网络。这些模型在各种应用中展示了非凡的能力，包括图像和语音识别、自然语言处理和自动驾驶。
- en: The prevalence of “big data” (large volumes of structured or unstructured data,
    often challenging to manage with traditional data processing software) problems
    greatly benefited from the development of **Graphical Processing Units** (**GPUs**),
    which were initially designed for graphics processing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: “大数据”（大量结构化或非结构化数据，通常难以用传统的数据处理软件管理）问题的普遍存在，极大地得益于**图形处理单元**（**GPU**）的发展，这些GPU最初是为图形处理设计的。
- en: In this section, we will provide a concise introduction to the foundational
    elements of deep learning, discussing only what is necessary for the problems
    associated with data imbalance in deep learning. For a more in-depth introduction,
    we recommend referring to a more dedicated book on deep learning (please refer
    to the resources listed as [1] and [2] in the *References* section).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍深度学习的基石，仅讨论与深度学习中数据不平衡问题相关的内容。对于更深入的介绍，我们建议参考更专业的深度学习书籍（请参阅参考文献部分列出的[1]和[2]）。
- en: Neural networks
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: Neural networks are the foundation of deep learning. Inspired by the structure
    and function of the human brain, neural networks consist of interconnected nodes
    or artificial neurons organized in layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是深度学习的基础。受人类大脑的结构和功能启发，神经网络由层状组织的相互连接的节点或人工神经元组成。
- en: 'The core data structure in `PyTorch` is tensors. Tensors are multi-dimensional
    arrays, similar to NumPy arrays, but with GPU acceleration support and capabilities
    for automatic differentiation. Tensors can be created, manipulated, and operated
    using `PyTorch` functions, as shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch` 的核心数据结构是张量。张量是多维数组，类似于NumPy数组，但具有GPU加速支持和自动微分的能力。可以使用 `PyTorch` 函数创建、操作和执行张量，如下所示：'
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Mixing CUDA tensors with CPU-bound tensors will lead to errors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将CUDA张量与CPU密集型张量混合会导致错误：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s the output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`autograd` is a powerful feature in `PyTorch` that allows automatic differentiation
    for tensor operations. This is particularly useful for backpropagation (discussed
    later) in neural networks:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`autograd` 是 `PyTorch` 中一个强大的功能，它允许对张量操作进行自动微分。这在神经网络中的反向传播（稍后讨论）中特别有用：'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will see the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s review how this code works. It creates a PyTorch tensor, `x`, performs
    some operations to compute `y` and `z`, and then computes the gradient of `z`
    concerning `x` using the `backward()` method. The gradients are stored in `x.grad`.
    The operations are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这段代码是如何工作的。它创建了一个PyTorch张量 `x`，执行一些操作来计算 `y` 和 `z`，然后使用 `backward()`
    方法计算 `z` 关于 `x` 的梯度。梯度存储在 `x.grad` 中。操作如下：
- en: '`y = x + 2` increases each element in `x` by 2'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y = x + 2` 将 `x` 中的每个元素增加2'
- en: '`z = y * y * 3` squares each element in `y` and then multiplies by 3'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`z = y * y * 3` 将 `y` 中的每个元素平方然后乘以3'
- en: 'The gradient calculation, in this case, involves applying the chain rule to
    these operations to compute  dz _ dx:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，梯度计算涉及到对这些操作应用链式法则来计算 dz/dx：
- en: dz _ dx  =  dz _ dy  ⋅  dy _ dx
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: dz/dx = dz/dy ⋅ dy/dx
- en: 'Ok, let’s calculate each piece in this equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们计算这个方程中的每一部分：
- en: 'First, let’s compute  dz _ dy:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们计算 dz/dy：
- en: dz _ dy  = 2 * y * 3 = 6 * y = 6 *(x + 2)
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dz/dy = 2 * y * 3 = 6 * y = 6 *(x + 2)
- en: 'dy _ dx = 1 because *y* is a linear function of *x*:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: dy/dx = 1 因为 *y* 是 *x* 的线性函数：
- en: 'Finally, let’s compute  dz _ dx:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们计算 dz/dx：
- en: dz _ dx  =  dz _ dy  ⋅  dy _ dx  = 6 ⋅ (x + 2) ⋅ 1 = 6 ⋅ (x + 2)
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dz/dx = dz/dy ⋅ dy/dx = 6 ⋅ (x + 2) ⋅ 1 = 6 ⋅ (x + 2)
- en: 'Given `x = [[1., 2.], [3., 4.]]`, the output when we print `x.grad` should
    be as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 `x = [[1., 2.], [3., 4.]]`，当我们打印 `x.grad` 时的输出应该是以下内容：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output tensor corresponds to the evaluated gradients of `z` concerning `x`
    at the specific values of `x`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出张量对应于在特定值 `x` 处 `z` 关于 `x` 的评估梯度。
- en: Perceptron
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器
- en: 'The perceptron is the most basic unit of a neural network. It is a simple,
    linear classifier that takes a set of input values, multiplies them by their corresponding
    weights, adds a bias term, sums the results, and applies an activation function
    to produce an output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是神经网络中最基本的单元。它是一个简单的线性分类器，它接受一组输入值，将它们乘以相应的权重，加上一个偏置项，将结果相加，并应用激活函数以产生输出：
- en: '![](img/B17259_06_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_02.jpg)'
- en: Figure 6.2 – A simple perceptron
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 一个简单的感知器
- en: So, what’s an activation function, then?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，激活函数是什么呢？
- en: Activation functions
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions introduce non-linearity into neural networks and help determine
    the output of a neuron. Common activation functions include sigmoid, tanh, **Rectified
    Linear Unit** (**ReLU**), and softmax. These functions enable the network to learn
    complex, non-linear patterns in the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数将非线性引入神经网络，并帮助确定神经元的输出。常见的激活函数包括sigmoid、tanh、**线性整流单元**（**ReLU**）和softmax。这些函数使网络能够学习数据中的复杂非线性模式。
- en: Let’s get into the various components of an artificial neural network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解人工神经网络的各种组成部分。
- en: Layers
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层
- en: A neural network typically consists of an input layer, one or more hidden (intermediate)
    layers, and an output layer. The input layer receives the raw data, while the
    hidden layers perform various transformations, and the output layer produces the
    final result.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络通常由输入层、一个或多个隐藏（中间）层和输出层组成。输入层接收原始数据，而隐藏层执行各种转换，输出层产生最终结果。
- en: The number of layers in a neural network is called the **depth** of the network,
    while the number of neurons in each layer is called the **width** of the network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的层数称为网络的**深度**，而每层的神经元数量称为网络的**宽度**。
- en: Counterintuitively, deeper and wider neural networks, though they have more
    capacity to learn complex patterns and representations in the training data, are
    not necessarily more robust to imbalanced datasets than shallower and narrower
    networks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与直觉相反，尽管深度和宽度更大的神经网络在训练数据中具有学习更复杂模式和表示的更多能力，但它们并不一定比浅层和窄网络对不平衡数据集更鲁棒。
- en: Deeper and wider networks are more prone to overfitting, especially in the context
    of imbalanced datasets, because large networks can memorize the patterns of the
    majority class(es), which can hamper the performance of minority class(es).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深度和宽度更大的网络更容易过拟合，尤其是在不平衡数据集的背景下，因为大网络可以记住多数类别的模式，这可能会妨碍少数类别的性能。
- en: Feedforward neural networks
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: A feedforward neural network is a kind of neural network that has a unidirectional
    flow of information, starting from the input layer, progressing through any hidden
    layers, and ending at the output layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是一种具有单向信息流的神经网络，从输入层开始，通过任何隐藏层，最终到达输出层。
- en: There are no feedback loops or connections between layers that cycle back to
    previous layers, hence the name feedforward. These networks are widely used for
    tasks such as image classification, regression, and others.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有反馈回路或层与层之间的循环连接，因此得名前馈。这些网络广泛用于图像分类、回归和其他任务。
- en: '`PyTorch` provides the `nn` module for creating and training neural networks
    – the `nn.Module` class is the base class for all neural network modules. The
    following code snippet defines a simple feedforward neural network:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch`提供了`nn`模块用于创建和训练神经网络——`nn.Module`类是所有神经网络模块的基类。以下代码片段定义了一个简单的前馈神经网络：'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Loss functions** quantify the difference between the predicted output and
    target values. Common loss functions include **Mean Squared Error** (**MSE**),
    cross-entropy, and hinge loss.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**量化了预测输出和目标值之间的差异。常见的损失函数包括**均方误差**（**MSE**）、交叉熵和Hinge损失。'
- en: The most commonly used loss function, `CrossEntropyLoss`, which we used in the
    previous snippet, tends to favor the majority class examples in imbalanced datasets.
    This occurs because the majority class examples significantly outnumber those
    of the minority class. As a result, the loss function becomes biased toward the
    majority class and fails to account for the error in the minority classes adequately.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的损失函数`CrossEntropyLoss`，我们在前面的代码片段中使用过，在类别不平衡的数据集中往往倾向于多数类别的示例。这是因为多数类别的示例数量显著多于少数类别的示例。因此，损失函数偏向多数类别，并且未能充分考虑到少数类别的错误。
- en: We will learn about several loss function modifications more suited to class
    imbalance problems in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level
    Deep* *Learning Techniques*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第8章*](B17259_08.xhtml#_idTextAnchor235)“算法级深度学习技术”中学习更多关于针对类别不平衡问题更适合的损失函数修改。
- en: MultiLayer Perceptron
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A MultiLayer Perceptron (MLP) is a feedforward neural network consisting of
    an input layer, one or more hidden layers, and an output layer. Each layer is
    fully connected, meaning every neuron in one layer is connected to all neurons
    in the next layer. MLPs can be used for various tasks, including classification,
    regression, and feature extraction. They are particularly suited for problems
    where the input data can be represented as a fixed-size vector, such as tabular
    data or flattened images. *Figure 6**.3* shows a multilayer perceptron with two
    input nodes, two hidden nodes, and one output node, using the specified weights
    w1 to w6:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）是一个前馈神经网络，由输入层、一个或多个隐藏层和输出层组成。每一层都是全连接的，这意味着一个层中的每个神经元都与下一层中的所有神经元相连。MLPs可用于各种任务，包括分类、回归和特征提取。它们特别适合于输入数据可以表示为固定大小向量的问题，例如表格数据或展平的图像。*图6.3*展示了具有两个输入节点、两个隐藏节点和一个输出节点的多层感知器，使用指定的权重w1到w6：
- en: '![](img/B17259_06_03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_03.jpg)'
- en: Figure 6.3 – A multilayer perceptron
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 多层感知器
- en: Training neural networks
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Training neural networks involves finding the optimal weights and biases that
    minimize a loss function. Two essential algorithms are used in this process –
    gradient descent and backpropagation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及找到最小化损失函数的最优权重和偏置。在此过程中使用了两个基本算法 – 梯度下降和反向传播：
- en: '**Gradient descent**: Gradient descent is an optimization algorithm that minimizes
    the loss function by iteratively updating the weights and biases.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降**：梯度下降是一种优化算法，通过迭代更新权重和偏置来最小化损失函数。'
- en: '**Backpropagation**: Backpropagation is a critical algorithm for training neural
    networks. It computes the gradient of the loss function concerning each weight
    using the chain rule (a method for finding the derivative of composite functions),
    efficiently calculating the gradients from the output layer back to the input
    layer.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：反向传播是训练神经网络的关键算法。它使用链式法则（一种寻找复合函数导数的方法）计算损失函数相对于每个权重的梯度，有效地从输出层计算梯度到输入层。'
- en: 'Training a neural network involves iterating through a dataset, feeding the
    data into the network, computing the loss, and updating the weights using backpropagation,
    as shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及遍历数据集，将数据输入网络，计算损失，并使用反向传播更新权重，如图所示：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code produces the following output, which shows the loss for each epoch
    (note that the loss goes down as the training progresses):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成以下输出，显示了每个epoch的损失（注意，随着训练的进行，损失会下降）：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the previous code snippet, the line containing `loss.backward()` maps to
    the backpropagation process. The `optimizer.zero_grad()` and `optimizer.step()`
    lines both represent one step of the gradient descent process. `optimizer.zero_grad()`
    clears old gradients from the last step (otherwise, they will accumulate), while
    `optimizer.step()` performs the actual update of the parameters (weights and biases)
    in the direction that reduces the loss the most.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，包含`loss.backward()`的行对应于反向传播过程。`optimizer.zero_grad()`和`optimizer.step()`这两行都代表梯度下降过程的一步。`optimizer.zero_grad()`清除上一步的旧梯度（否则它们会累积），而`optimizer.step()`执行参数（权重和偏置）的实际更新，以减少损失。
- en: 'The following flowchart depicts the training logic in PyTorch:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程图描述了PyTorch中的训练逻辑：
- en: '![](img/B17259_06_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_04.jpg)'
- en: Figure 6.4 – PyTorch training logic flowchart
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – PyTorch训练逻辑流程图
- en: '**Overfitting** is a common problem in machine learning, where a model performs
    well on the training data but fails to generalize to unseen data.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**是机器学习中常见的问题，模型在训练数据上表现良好，但无法泛化到未见数据。'
- en: '**Underfitting** happens when the model is too simple (think about when we
    used a linear regression model when we needed a decision tree regressor model)
    and does not capture the underlying patterns in the data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**欠拟合**发生在模型过于简单时（想想当我们需要决策树回归模型时却使用了线性回归模型的情况）并且无法捕捉数据中的潜在模式。'
- en: Both overfitting and underfitting lead to poor performance on unseen data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合都会导致在未见数据上的性能不佳。
- en: Regularization
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: 'Regularization techniques help mitigate overfitting by adding additional constraints
    the model must meet beyond merely optimizing the loss function. If L is the loss
    function, the most commonly used types of regularization are L1 and L2, which
    add a penalty term to the loss function, discouraging overly complex models:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化技术通过为模型必须满足的约束条件添加额外的限制来帮助减轻过拟合。如果L是损失函数，最常用的正则化类型是L1和L2，它们将惩罚项添加到损失函数中，从而阻止过于复杂的模型：
- en: L1-loss = L + λ∑ i=1 n |w i|
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: L1损失 = L + λ∑i=1n|w_i|
- en: L2-loss = L + λ∑ i=1 n w i 2
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: L2损失 = L + λ∑i=1nw_i^2
- en: Here, λ is the regularization strength and w i are the model parameters (weights).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，λ是正则化强度，w_i是模型参数（权重）。
- en: 'There are a few other regularization techniques as well:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种正则化技术：
- en: '`PyTorch`, the `torch.nn.Dropout` class implements dropout and takes the dropout
    rate (the probability of the neuron being zeroed) as a parameter.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PyTorch`中的`torch.nn.Dropout`类实现了dropout功能，并接受一个参数，即dropout率（神经元被置零的概率）。'
- en: '`torch.nn.BatchNorm1d` and `torch.nn.BatchNorm2d`) is a technique that’s used
    to improve the training of deep neural networks. It normalizes the inputs to each
    layer by adjusting their mean and standard deviation, which helps stabilize and
    accelerate the training process, allowing the use of higher learning rates and
    reducing sensitivity to weight initialization.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.BatchNorm1d`和`torch.nn.BatchNorm2d`是一种用于改进深度神经网络训练的技术。它通过调整每个层的均值和标准差来规范化每个层的输入，这有助于稳定和加速训练过程，允许使用更高的学习率并减少对权重初始化的敏感性。'
- en: '**Early stopping**: Early stopping is a technique that’s used to prevent overfitting
    during the training of neural networks. It involves monitoring the model’s performance
    on a validation set and stopping the training process when the performance stops
    improving or starts to degrade. This helps with finding the point at which the
    model generalizes well to new data without the need to memorize the training set:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早期停止**：早期停止是一种在神经网络训练过程中防止过拟合的技术。它涉及监控模型在验证集上的性能，并在性能停止提高或开始下降时停止训练过程。这有助于找到模型在没有需要记住训练集的情况下，对新数据泛化良好的点：'
- en: '![](img/B17259_06_05.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_05.jpg)'
- en: Figure 6.5 – Applying the early stopping technique when validation loss starts
    to increase with more epochs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 当验证损失随着更多轮次增加时应用早期停止技术
- en: The effect of the learning rate on data imbalance
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率对数据不平衡的影响
- en: We have seen that gradient descent takes a step in the direction of the negative
    gradient – the size of that step is called the **learning rate**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，梯度下降在负梯度的方向上迈出一步——这一步的大小称为**学习率**。
- en: Choosing the right learning rate is crucial for models operating on imbalanced
    datasets.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的学习率对于在数据不平衡的数据集上运行的模型至关重要。
- en: The learning rate should be selected so that the model learns patterns from
    both majority and minority classes effectively. Monitoring training and validation
    loss and other evaluation metrics such as precision, recall, and F1-score can
    help fine-tune the learning rate.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的选择应确保模型能够有效地从多数类和少数类中学习模式。监控训练和验证损失以及其他评估指标，如精确度、召回率和F1分数，可以帮助微调学习率。
- en: A high learning rate means that the model’s weights are updated more drastically
    during each iteration of training. When applied to the minority class, these rapid,
    large updates can cause the model to skip over the optimal set of weights that
    minimize the loss for that class. It’s often beneficial to use techniques such
    as adaptive learning rates [3][4] or even class-specific learning rates [5] to
    ensure that the model learns effectively from both the majority and minority classes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 高学习率意味着模型在每次训练迭代中更新权重更为剧烈。当应用于少数类时，这些快速、大的更新可能导致模型跳过最小化该类损失的优化权重集。通常，使用自适应学习率[3][4]或甚至类特定学习率[5]来确保模型从多数类和少数类中有效地学习是有益的。
- en: Now, let’s review some particular kinds of neural networks that have been quite
    useful for image and text domains.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一些对图像和文本领域非常有用的特定类型的神经网络。
- en: Image processing using Convolutional Neural Networks
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像处理
- en: '**Convolutional Neural Networks** (**CNNs**) is a deep learning model designed
    for image and video processing tasks. It consists of convolutional layers, pooling
    layers, and fully connected layers:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是一种为图像和视频处理任务设计的深度学习模型。它由卷积层、池化层和全连接层组成：'
- en: Convolutional layers apply filters to the input data, learning to detect local
    features such as edges or textures. These filters slide across the input data,
    performing element-wise multiplication and summing the results, which creates
    a feature map representing the presence of specific features.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层将滤波器应用于输入数据，学习检测局部特征，如边缘或纹理。这些滤波器在输入数据上滑动，执行逐元素乘法和求和结果，从而创建一个特征图，表示特定特征的存在。
- en: Pooling layers, such as max-pooling or average pooling, reduce the spatial dimensions
    of the data, aggregating information and reducing computational complexity. These
    layers help build invariance to small translations and distortions in the input
    data.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层，如最大池化或平均池化，减少了数据的空间维度，聚合信息并降低计算复杂度。这些层有助于构建对输入数据中小的平移和扭曲的不变性。
- en: Fully connected layers process the high-level features extracted by the convolutional
    and pooling layers to make predictions. Fully connected layers are traditional
    neural network layers where each neuron is connected to every neuron in the previous
    layer.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层处理卷积层和池化层提取的高级特征，以进行预测。完全连接层是传统的神经网络层，其中每个神经元都与前一层的每个神经元相连。
- en: CNNs tend to overfit minority classes, which is in contrast with traditional
    machine learning algorithms, which usually underfit these minority classes [6].
    The intuition here is that CNNs, with their multiple layers and a large number
    of parameters, are designed to capture complex patterns. Additionally, being data-hungry,
    CNNs can learn intricate details from the data. When faced with imbalanced data,
    CNNs may focus excessively on the minority class, essentially “memorizing” the
    minority class instances.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）倾向于过度拟合少数类别，这与通常欠拟合这些少数类别的传统机器学习算法形成对比[6]。这里的直觉是，CNNs具有多层和大量参数，旨在捕捉复杂模式。此外，由于对数据有很强的需求，CNNs可以从数据中学习到复杂的细节。当面对不平衡数据时，CNNs可能会过度关注少数类别，本质上“记忆”了少数类别的实例。
- en: We will discuss this in more detail in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*, in the *Class-Dependent Temperature
    (CDT)* *loss* section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第8章*（B17259_08.xhtml#_idTextAnchor235）的*算法级深度学习技术*部分，*类依赖温度（CDT）*损失部分中更详细地讨论这个问题。
- en: 'The imbalance problems in the computer vision domain can be categorized as
    shown in *Figure* *6**.6* [7]:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉领域中的不平衡问题可以按照*图6.6*所示进行分类[7]：
- en: '![](img/B17259_06_06.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_06.jpg)'
- en: Figure 6.6 – Categorization of imbalance problems in computer vision (adapted
    from [7])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 计算机视觉中不平衡问题的分类（改编自[7]）
- en: Text analysis using Natural Language Processing
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自然语言处理进行文本分析
- en: '**Natural Language Processing** (**NLP**) is another branch of AI that helps
    computers understand and analyze human language for extracting insights and organizing
    information. *Figure 6**.7* shows the categorization of imbalance problems in
    text based on data complexity levels, while *Figure 6**.8* shows the categorization
    based on some of the popular NLP application areas:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是人工智能的另一个分支，它帮助计算机理解和分析人类语言，以提取见解和组织信息。*图6.7*展示了基于数据复杂度级别的文本不平衡问题的分类，而*图6.8*展示了基于一些流行的NLP应用领域的分类：'
- en: '![](img/B17259_06_07.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_07.jpg)'
- en: Figure 6.7 – Categorization of imbalance problems in NLP based on the form of
    textual data
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 基于文本数据形式的NLP中不平衡问题的分类
- en: '![](img/B17259_06_08.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_08.jpg)'
- en: Figure 6.8 – Categorization of imbalance problems in NLP based on applications
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 基于应用的NLP中不平衡问题的分类
- en: With the basics out of the way, let’s see how data imbalance affects deep learning
    models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本概念介绍完毕后，让我们看看数据不平衡如何影响深度学习模型。
- en: Data imbalance in deep learning
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的数据不平衡
- en: While many classical machine learning problems that use tabular data are limited
    to binary classes and are interested in predicting the minority class, this is
    not the norm in domains where deep learning is often applied, especially computer
    vision or NLP problems.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多使用表格数据的经典机器学习问题仅限于二进制类别，并且关注于预测少数类别，但在深度学习经常应用的领域中，这并不是常态，尤其是在计算机视觉或NLP问题中。
- en: Even benchmark datasets such as MNIST (a collection of handwritten digits containing
    grayscale images from 0 to 9) and CIFAR10 (color images with 10 different classes)
    have 10 classes to predict. So, we can say that **multi-class classification**
    is typical in problems that use deep learning models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是像MNIST（包含0到9的灰度图像的手写数字集合）和CIFAR10（包含10个不同类别的彩色图像）这样的基准数据集也有10个类别需要预测。因此，我们可以说在应用深度学习模型的领域中，**多类别分类**是典型的。
- en: 'This data skew or imbalance can severely impact the model performance. We should
    review what we discussed about the typical kinds of imbalance in datasets in [*Chapter
    1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance in Machine
    Learning*. To simulate real-world data imbalance scenarios, two types of imbalance
    are usually investigated in the literature:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据倾斜或不平衡可能会严重影响模型的表现。我们应该回顾一下在[*第一章*](B17259_01.xhtml#_idTextAnchor015)，《机器学习中的数据不平衡介绍》中讨论的典型不平衡类型。为了模拟现实世界的数据不平衡场景，文献中通常研究两种类型的不平衡：
- en: '**Step imbalance**: All the minority classes have the same or almost the same
    number of examples, as do all the majority classes:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步长不平衡**：所有少数类别和所有多数类别的示例数量相同或几乎相同：'
- en: '![](img/B17259_06_09.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_09.jpg)'
- en: Figure 6.9 – Step imbalance
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 步长不平衡
- en: '**Long-tailed imbalance**: The number of examples across different classes
    follows an exponential decay. The plot usually has a long tail toward the left
    or the right:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长尾不平衡**：不同类别的示例数量遵循指数衰减。图表通常向左或向右有长长的尾巴：'
- en: '![](img/B17259_06_10.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_10.jpg)'
- en: Figure 6.10 – Long-tailed imbalance
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 长尾不平衡
- en: 'Generally, when comparing two datasets, the one with a higher imbalance ratio
    is likely to yield a lower ROC-AUC score [8]. There can be issues of class overlap,
    noisy labels, and concept complexity in imbalanced datasets [9][10]. These issues
    can significantly impact the performance of deep learning models:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当比较两个数据集时，不平衡比率较高的数据集可能得到较低的ROC-AUC分数[8]。在不平衡数据集中可能存在类别重叠、标签噪声和概念复杂性的问题[9][10]。这些问题可能会严重影响深度学习模型的表现：
- en: '**Class overlap**: Class overlap occurs when instances from different classes
    are close to each other in the feature space. Deep learning models often rely
    on complex decision boundaries to separate classes. When instances from different
    classes are close in the feature space, as is common in imbalanced datasets, the
    majority class can dominate these decision boundaries. This makes it especially
    challenging for deep learning models to accurately classify minority class instances.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别重叠**：当不同类别的实例在特征空间中彼此靠近时，就会发生类别重叠。深度学习模型通常依赖于复杂的决策边界来区分类别。当不同类别的实例在特征空间中靠近时，这在不平衡数据集中很常见，多数类别可能会主导这些决策边界。这使得深度学习模型准确分类少数类别实例变得特别具有挑战性。'
- en: '**Noisy labels**: Noisy labels refers to instances in a dataset that have incorrect
    or ambiguous class labels. In imbalanced datasets, noisy labels can disproportionately
    affect the minority class as the model has fewer instances to learn from. Deep
    learning models are data-hungry and highly sensitive to the quality of the training
    data. This can lead to poor generalization in deep learning models, affecting
    their performance on new, unseen data.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声标签**：噪声标签指的是数据集中具有错误或不明确类别标签的实例。在不平衡数据集中，噪声标签可能会不成比例地影响少数类别，因为模型有较少的实例可以学习。深度学习模型对数据非常渴求，并且对训练数据的质量高度敏感。这可能导致深度学习模型泛化能力差，影响它们在新、未见过的数据上的表现。'
- en: '**Concept complexity**: Concept complexity is the inherent difficulty in distinguishing
    between classes based on the given features. Deep learning models excel at capturing
    intricate patterns in the data. However, the complexity of the relationships between
    features and class labels in imbalanced datasets can make it difficult for these
    models to effectively learn the minority class. The limited number of instances
    available for the minority class often compounds this issue.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念复杂性**：概念复杂性是指根据给定的特征区分类别的固有难度。深度学习模型擅长捕捉数据中的复杂模式。然而，在不平衡数据集中特征与类别标签之间关系的复杂性可能会使得这些模型难以有效地学习少数类别。少数类别可用的实例数量有限，这通常会使问题更加严重。'
- en: The impact of data imbalance on deep learning models
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据不平衡对深度学习模型的影响
- en: 'Let’s also review how data imbalance affects deep learning models compared
    to classical machine learning models:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再回顾一下数据不平衡如何影响深度学习模型与经典机器学习模型相比：
- en: '**Model sensitivity**: The performance of deep learning models can be significantly
    impacted as the imbalance ratio of a dataset increases (*Figure 6**.11*):'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型敏感性**：随着数据集不平衡率的增加，深度学习模型的表现可能会受到显著影响（*图6**.11*）：'
- en: '![](img/B17259_06_11.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_11.jpg)'
- en: Figure 6.11– Model performance on an imbalanced version of CIFAR-10 with a fixed
    number of minority classes (adapted from [8])
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11–在具有固定少数类数量的不平衡版本的CIFAR-10上的模型性能（改编自[8]）
- en: '**Feature engineering**: Classical machine learning models usually require
    manual feature engineering, which can help address data imbalance by creating
    new features or transforming existing ones to highlight the minority class. In
    deep learning models, feature engineering is typically performed automatically
    through the learning process, making it less reliant on human intervention to
    address the imbalance.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：经典机器学习模型通常需要手动特征工程，这有助于通过创建新特征或转换现有特征来突出显示少数类，从而解决数据不平衡问题。在深度学习模型中，特征工程通常通过学习过程自动执行，这使得它对人类干预来解决不平衡问题不那么依赖。'
- en: '**Techniques to handle imbalance**: In classical machine learning, standard
    techniques for handling data imbalance include resampling (oversampling the minority
    class or undersampling the majority class), generating synthetic samples (for
    example, using the **Synthetic Minority Over-Sampling Technique** (**SMOTE**)),
    and using cost-sensitive learning (assigning different misclassification costs
    to different classes).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理不平衡的技术**：在经典机器学习中，处理数据不平衡的标准技术包括重采样（对少数类进行过采样或对多数类进行欠采样）、生成合成样本（例如，使用**合成少数类过采样技术**（SMOTE））和使用代价敏感学习（为不同类别分配不同的误分类成本）。'
- en: Some techniques from classical machine learning can also be applied in deep
    learning, but additional methods have been developed specifically for deep learning
    models. These include transfer learning (leveraging pre-trained models to learn
    from imbalanced data), using focal loss (a loss function that focuses on hard-to-classify
    examples), and employing data augmentation techniques to generate more varied
    and balanced training data. These data augmentation techniques will be discussed
    in detail in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep*
    *Learning Methods*.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些来自经典机器学习的技术也可以应用于深度学习，但已经开发出专门针对深度学习模型的方法。这些包括迁移学习（利用预训练模型从不平衡数据中学习）、使用焦点损失（一个关注难以分类示例的损失函数）以及采用数据增强技术来生成更多样化和平衡的训练数据。这些数据增强技术将在[*第7章*](B17259_07.xhtml#_idTextAnchor205)，*数据级深度学习方法*中详细讨论。
- en: '**Model interpretability**: Classical machine learning models are often more
    interpretable, which can help us understand the impact of data imbalance on the
    model’s decision-making process. Deep learning models, on the other hand, are
    often referred to as “black boxes” due to their lack of interpretability. This
    lack of interpretability can make it harder to understand how the model handles
    imbalanced data and whether it is learning meaningful patterns or simply memorizing
    the majority class.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可解释性**：经典机器学习模型通常更可解释，这有助于我们了解数据不平衡对模型决策过程的影响。另一方面，由于缺乏可解释性，深度学习模型通常被称为“黑盒”。这种缺乏可解释性使得理解模型如何处理不平衡数据以及它是否在学习有意义的模式或仅仅是记忆多数类变得更加困难。'
- en: '**Training data size**: Deep learning models typically require large amounts
    of training data to achieve optimal performance. In cases of severe data imbalance,
    gathering sufficient data for the minority class may be more challenging, hindering
    the performance of deep learning models. Additionally, if a large dataset is available,
    it is more likely that instances of the minority class will be found within that
    vast amount of data. In contrast, in a smaller dataset, the minority class might
    never appear at all! On the other hand, classical machine learning algorithms
    can often achieve decent performance with smaller datasets, which is an advantage
    when dealing with imbalanced data.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据大小**：深度学习模型通常需要大量的训练数据以实现最佳性能。在数据严重不平衡的情况下，收集足够的数据以供少数类使用可能更具挑战性，这可能会阻碍深度学习模型的表现。此外，如果有一个大型数据集可用，那么在大量数据中找到少数类的实例的可能性更大。相比之下，在较小的数据集中，少数类可能根本不会出现！另一方面，经典机器学习算法通常可以在较小的数据集上实现相当不错的性能，这在处理不平衡数据时是一个优势。'
- en: '**The impact of depth (number of layers) on deep learning models trained on
    imbalanced data problems**: The pros of more layers are the improved capacity
    to learn complex patterns and features in the data and improved generalization
    of the model. The cons of adding more layers can be the model overfitting and
    the problem of vanishing or exploding gradients worsening (as depth increases,
    the gradients during backpropagation can become very small (vanish) or very large
    (explode), making it challenging to train the model). This is summarized in *Figure
    6**.12*:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度（层数）对不平衡数据问题训练的深度学习模型的影响**：更多层的优点是提高了学习数据中复杂模式和特征的能力，以及提高了模型的泛化能力。增加更多层的缺点可能是模型过拟合和梯度消失或爆炸问题恶化（随着深度的增加，反向传播中的梯度可以变得非常小（消失）或非常大（爆炸），这使得训练模型变得具有挑战性）。这总结在*图6.12*中：'
- en: '![](img/B17259_06_12.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_12.jpg)'
- en: Figure 6.12 – Summarizing the impact of depth on deep learning models
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 总结深度对深度学习模型的影响
- en: Overall, the influence of depth on a deep learning model varies with the data
    and model architecture requiring empirical evaluation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，深度对深度学习模型的影响随着数据和模型架构的不同而变化，需要经验评估。
- en: '**Oversampling and undersampling**: The study titled *A systematic study of
    the* *class imbalance* *problem in convolutional neural networks*, by Mateusz
    Buda [8], thoroughly examined how class imbalance affects the performance of CNNs.
    The research utilized three well-known datasets – MNIST, CIFAR-10, and ImageNet
    – and employed models such as LeNet-5, All-CNN, and ResNet-10 for the experiments.
    Their key findings were as follows:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样和欠采样**：Mateusz Buda[8]撰写的题为*卷积神经网络中类别不平衡问题的系统研究*的研究，详细考察了类别不平衡如何影响CNN的性能。该研究使用了三个著名的数据集——MNIST、CIFAR-10和ImageNet——并使用了LeNet-5、All-CNN和ResNet-10等模型进行实验。他们的主要发现如下：'
- en: In almost all analyzed cases, oversampling proved to be the most effective technique
    for mitigating class imbalance
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在几乎所有分析案例中，过采样都被证明是减轻类别不平衡的最有效技术
- en: Oversampling was more effective when the imbalance was eliminated while undersampling
    yielded better results when only reducing the imbalance partially
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当消除不平衡时，过采样更有效，而欠采样在仅部分减少不平衡时产生更好的结果
- en: Contrary to some traditional machine learning algorithms, CNNs did not overfit
    when oversampling was applied
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与一些传统的机器学习算法相反，当应用过采样时，CNN没有过拟合
- en: In some cases, undersampling performs on par with oversampling, even when using
    less data
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，欠采样与过采样表现相当，即使使用更少的数据
- en: Undersampling generally showed poor performance compared to the baseline and
    never showed a notable advantage over oversampling
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠采样通常与基线相比表现较差，从未显示出比过采样明显的优势
- en: If the focus is on correctly classifying examples from minority classes, undersampling
    may be preferable to oversampling
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果重点是正确分类少数类别的示例，欠采样可能比过采样更可取
- en: If there’s too much data, undersampling might be the preferred or only option
    to save time, resources, and the cost of training.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果数据太多，欠采样可能是节省时间、资源和训练成本的首选或唯一选项
- en: '**Threshold adjustment**: We discussed threshold adjustment in detail in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*. As a refresher,
    a decision threshold is a value that determines the boundary between different
    classes or outcomes in a classification problem. The paper by Johnson et al. [11]
    emphasized that the optimal threshold is linearly correlated with the minority
    class size (that is, the lower the minority class size, the lower the threshold).
    They trained and tested for fraud detection using deep learning models with two
    and four hidden layers on CMS Medicare data [12]. The default threshold often
    leads to poor classification, especially for the minority class, highlighting
    the need for threshold adjustment based on a validation set.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阈值调整**：我们在[*第五章*](B17259_05.xhtml#_idTextAnchor151)中详细讨论了阈值调整，*成本敏感学习*。作为复习，决策阈值是一个确定分类问题中不同类别或结果之间边界的值。Johnson等人[11]的论文强调了最佳阈值与少数类大小呈线性相关（即少数类大小越低，阈值越低）。他们使用深度学习模型在CMS医疗保险数据[12]上进行了两个和四个隐藏层的欺诈检测训练和测试。默认阈值通常会导致分类效果不佳，特别是对于少数类，突出了基于验证集进行阈值调整的必要性。'
- en: A note about the datasets used in the second half of this book
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本书后半部分使用的数据集的说明
- en: 'In the remainder of this book, we will primarily use imbalanced versions of
    the MNIST and CIFAR10-LT (“LT” stands for “long-tailed”) datasets for training:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余部分，我们将主要使用MNIST和CIFAR10-LT（“LT”代表“长尾”）数据集的不平衡版本进行训练：
- en: '• **MNIST**: A small grayscale image dataset with 10 classes containing digits
    from 0 to 9\. It consists of 60,000 training images and 10,000 test images, each
    28 pixels x 28 pixels. It’s faster to train/test compared to CIFAR-10.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: • **MNIST**：一个包含0到9数字的10个类别的灰度图像数据集。它由60,000个训练图像和10,000个测试图像组成，每个图像为28像素 x
    28像素。与CIFAR-10相比，训练/测试速度更快。
- en: '• **CIFAR-10**: Used for object recognition, this color image dataset also
    has 10 classes. It includes 50,000 training images and 10,000 test images, each
    32 pixels x 32 pixels.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: • **CIFAR-10**：用于物体识别，这个彩色图像数据集也有10个类别。它包括50,000个训练图像和10,000个测试图像，每个图像为32像素
    x 32像素。
- en: 'Though the training sets are imbalanced, the corresponding test sets have an
    equal number of examples in each class. This balanced test set approach provides
    several benefits:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练集是不平衡的，但相应的测试集中每个类别的示例数量是相等的。这种平衡测试集方法提供了几个好处：
- en: '• **Comparability**: Balanced test sets allow unbiased comparison across various
    classes and models'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: • **可比性**：平衡的测试集允许在不同类别和模型之间进行无偏比较
- en: '• **Repeatability and reproducibility**: Using simple datasets such as MNIST
    and CIFAR-10 ensures ease of code execution and understanding'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: • **可重复性和可再现性**：使用如MNIST和CIFAR-10等简单数据集确保了代码执行和理解上的便捷性
- en: '• **Efficiency**: Smaller datasets enable quicker iterations, allowing us to
    try, test, and retry running the code in a reasonable timeframe'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: • **效率**：较小的数据集能够加快迭代速度，使我们能够在合理的时间内尝试、测试和重试运行代码
- en: '• **Alignment with research**: This approach is consistent with most research
    studies on long-tailed learning and imbalanced datasets, providing a common framework
    for comparison'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: • **与研究的契合度**：这种方法与大多数关于长尾学习和不平衡数据集的研究是一致的，提供了一个通用的比较框架
- en: The next section will give us an overview of deep learning strategies for managing
    data imbalance. We will also see how various techniques for handling imbalanced
    datasets that were initially developed for classical machine learning techniques
    can be easily extended to deep learning models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将为我们概述处理数据不平衡的深度学习策略。我们还将看到，最初为经典机器学习技术开发的处理不平衡数据集的各种技术如何轻松扩展到深度学习模型。
- en: Overview of deep learning techniques to handle data imbalance
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据不平衡的深度学习技术概述
- en: 'Much like the first half of this book, where we focused on classical machine
    learning techniques, the major categories typically include sampling techniques,
    cost-sensitive techniques, threshold adjustment techniques, or a combination of
    these:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书的前半部分类似，我们当时专注于经典机器学习技术，主要类别通常包括采样技术、成本敏感型技术、阈值调整技术，或这些技术的组合：
- en: The sampling techniques comprise either undersampling the majority class or
    oversampling the minority class data. Data augmentation is a fundamental technique
    in computer vision problems that’s used to increase the diversity of the training
    set. While not directly an oversampling method aimed at addressing class imbalance,
    data augmentation does have the effect of expanding the training data. We will
    discuss these techniques in more detail in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep* *Learning Methods*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本技术包括对多数类进行欠采样或对少数类数据进行过采样。数据增强是计算机视觉问题中的一个基本技术，用于增加训练集的多样性。虽然它不是直接针对解决类别不平衡的过采样方法，但数据增强确实具有扩展训练数据的效果。我们将在[*第7章*](B17259_07.xhtml#_idTextAnchor205)的*数据级深度学习方法*中更详细地讨论这些技术。
- en: Cost-sensitive techniques usually involve changing the model loss function in
    some way to accommodate the higher cost of misclassifying the minority class examples.
    Some standard loss functions, such as `CrossEntropyLoss` in PyTorch, support the
    weight parameter to accommodate such costs. We will cover many of those, including
    several custom loss functions, in detail in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep* *Learning Techniques*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本敏感型技术通常涉及以某种方式更改模型损失函数，以适应少数类样本分类错误的较高成本。一些标准损失函数，如PyTorch中的`CrossEntropyLoss`，支持权重参数以适应这些成本。我们将在[*第8章*](B17259_08.xhtml#_idTextAnchor235)的*算法级深度学习技术*中详细介绍其中许多，包括几个自定义损失函数。
- en: Hybrid deep learning techniques integrate the data-level and algorithm-level
    approaches. This fusion allows for more nuanced and effective solutions to tackle
    class imbalance. We’ll discuss this in [*Chapter 9*](B17259_09.xhtml#_idTextAnchor256),
    *Hybrid Deep* *Learning Methods*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合深度学习技术整合了数据级和算法级方法。这种融合允许更细致和有效的解决方案来处理类别不平衡。我们将在[*第九章*](B17259_09.xhtml#_idTextAnchor256)，*混合深度学习方法*中讨论这一点。
- en: 'Threshold adjustment techniques are applied to the scores produced from the
    model after the model has been trained. These techniques can help adjust the threshold
    so that the model metric, say, F1-score or geometric mean, gets optimized. This
    was discussed in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151)*,* *Cost-Sensitive
    Learning*:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值调整技术应用于模型训练后产生的分数。这些技术可以帮助调整阈值，从而使模型指标，例如F1分数或几何平均数，得到优化。这在[*第五章*](B17259_05.xhtml#_idTextAnchor151)*，*成本敏感学习*中已有讨论：
- en: '![](img/B17259_06_13.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_13.jpg)'
- en: Figure 6.13 – Categorization of deep learning techniques covered
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 深度学习技术的分类
- en: We will not use deep learning methods for tabular data from this chapter onwards
    because classical models such as XGBoost, LightGBM, and CatBoost tend to perform
    well on such structured data. Several studies ([13] and [14]) have shown that
    traditional machine learning models often outperform deep learning models in supervised
    learning tasks involving tabular data. However, an ensemble of an XGBoost model
    and a deep learning model can outperform the XGBoost model alone [13]. Therefore,
    tasks using tabular datasets can still benefit from deep learning models. It’s
    likely only a matter of time before deep learning models catch up to the performance
    of classical models on tabular data. Nevertheless, we will focus our implementation
    and examples on vision and NLP problems when using deep learning models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们将不会使用深度学习方法处理表格数据，因为像XGBoost、LightGBM和CatBoost这样的经典模型在处理这种结构化数据时往往表现良好。几项研究([13]和[14])表明，在涉及表格数据的监督学习任务中，传统机器学习模型通常优于深度学习模型。然而，XGBoost模型和深度学习模型的集成可以优于单独的XGBoost模型[13]。因此，使用表格数据集的任务仍然可以从深度学习模型中受益。深度学习模型在表格数据上的性能赶上经典模型可能只是时间问题。尽管如此，当我们使用深度学习模型时，我们将重点关注视觉和NLP问题。
- en: This concludes our high-level discussion of various techniques for dealing with
    imbalanced datasets when using deep learning models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对使用深度学习模型处理不平衡数据集的各种技术的高级讨论。
- en: Multi-label classification
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多标签分类
- en: 'Multi-label classification is a classification task where each instance can
    be assigned to multiple classes or labels simultaneously. In other words, an instance
    can belong to more than one category or have multiple attributes. For example,
    a movie can belong to multiple genres, such as action, comedy, and romance. Similarly,
    an image can have multiple objects in it (*Figure 6**.14*):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类是一种分类任务，其中每个实例可以同时分配给多个类别或标签。换句话说，一个实例可以属于多个类别或具有多个属性。例如，一部电影可以属于多个类型，如动作、喜剧和浪漫。同样，一张图片可以包含多个对象(*图6**.14*)：
- en: '![](img/B17259_06_14.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_14.jpg)'
- en: Figure 6.14 – Multi-label image classification with prediction probabilities
    shown
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 显示预测概率的多标签图像分类
- en: But how is it different from multi-class classification? Multi-class classification
    is a classification task where each instance can be assigned to only one class
    or label. In this case, the classes or categories are mutually exclusive, meaning
    an instance can belong to just one category. For example, a handwritten digit
    recognition task would be multi-class since each digit can belong to only one
    class (0-9).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 但它与多类分类有何不同？多类分类是一种分类任务，其中每个实例只能分配给一个类别或标签。在这种情况下，类别或类别是互斥的，这意味着一个实例只能属于一个类别。例如，手写数字识别任务将是多类的，因为每个数字只能属于一个类别（0-9）。
- en: 'In summary, the main difference between multi-label and multi-class classification
    is that in multi-label classification, instances can have multiple labels. In
    contrast, in multi-class classification, instances can have only one label. This
    is summarized in *Figure 6**.15*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，多标签分类和多类分类的主要区别在于，在多标签分类中，实例可以有多个标签。相比之下，在多类分类中，实例只能有一个标签。这总结在*图6**.15*中：
- en: '![](img/B17259_06_15.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_15.jpg)'
- en: Figure 6.15 – Distinction between multi-label and multi-class classification
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – 多标签分类和多类分类的区别
- en: Many real-world problems are inherently multi-labeled and experience class imbalance.
    Deep learning models are particularly useful here, especially when the data involved
    is unstructured, such as images, videos, or text.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的问题本质上是多标签的，并且存在类别不平衡。深度学习模型在这里特别有用，尤其是当涉及的数据是无结构化的，如图像、视频或文本时。
- en: In **Multi-Label Datasets** (**MLDs**), there can be tens or hundreds of labels,
    and each instance can be associated with a subset of those labels. The more different
    labels exist, the more possibilities there are that some have a very low presence,
    leading to a significant imbalance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在**多标签数据集**（**MLDs**）中，可能有成十或上百个标签，每个实例可以与这些标签的子集相关联。标签种类越多，某些标签出现频率非常低的可能性就越大，从而导致显著的失衡。
- en: 'The data imbalance in multi-label classification can occur at many levels [15]:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类中的数据不平衡可能发生在多个层面[15]：
- en: '**Imbalance within labels**: A large disparity between negative and positive
    instances in each label can happen, causing classification models to struggle
    with minority classes'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签内部的不平衡**：每个标签中负实例和正实例之间可能存在很大的差异，这可能导致分类模型在少数类别上遇到困难'
- en: '**Imbalance between labels**: Unequal distribution of positive instances among
    labels, leading to poor performance for minority classes'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签之间的不平衡**：标签中正实例的分布不均，导致少数类别的性能较差'
- en: '**Imbalance among label sets**: Sparse frequency of label sets (a combination
    of various labels) due to label sparseness, making it challenging for classification
    models to learn effectively'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签集之间的不平衡**：由于标签稀疏性，标签集（各种标签的组合）的频率稀疏，这使得分类模型难以有效学习'
- en: '*Figure 6**.16* sums up these kinds of imbalances when classifying multi-label
    datasets:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.16*总结了在分类多标签数据集时这些不平衡类型：'
- en: '![](img/B17259_06_16.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_16.jpg)'
- en: Figure 6.16 – Kinds of imbalances in multi-label datasets
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 多标签数据集中的不平衡类型
- en: The imbalance handling approaches and metrics are similar to those used for
    imbalanced multi-class classification methods. Typical approaches include resampling
    methods, ensemble approaches, and cost-sensitive methods.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不平衡处理方法和度量标准与用于不平衡多类分类方法的方法类似。典型的方法包括重采样方法、集成方法和成本敏感方法。
- en: The most straightforward approach is to convert the multi-label dataset into
    the multi-class dataset and then apply various re-sampling methods such as random
    oversampling, random undersampling, **Edited Nearest Neighbors** (**ENN**), and
    others. Similar strategies have been used in the literature to adapt the cost-sensitive
    approaches to fit multi-label classification problems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法是将多标签数据集转换为多类数据集，然后应用各种重采样方法，如随机过采样、随机欠采样、**编辑最近邻**（**ENN**）等。文献中已经使用了类似的策略，将成本敏感方法适应到多标签分类问题。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning has become essential in many fields, from computer vision and
    natural language processing to healthcare and finance. This chapter has provided
    a brief introduction to the core concepts and techniques in deep learning. We
    talked about PyTorch, the fundamentals of deep learning, activation functions,
    and data imbalance challenges. We also got a bird’s-eye view of the various techniques
    we will discuss in the following few chapters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在许多领域已成为必要的技术，从计算机视觉和自然语言处理到医疗保健和金融。本章简要介绍了深度学习的核心概念和技术。我们讨论了PyTorch、深度学习的基础知识、激活函数和数据不平衡挑战。我们还对接下来几章将要讨论的各种技术进行了鸟瞰。
- en: Understanding these fundamentals will equip you with the knowledge necessary
    to explore more advanced topics and applications and ultimately contribute to
    the ever-evolving world of deep learning.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些基础知识将使你具备探索更高级主题和应用的知识，并最终为不断发展的深度学习世界做出贡献。
- en: In the next chapter, we will look at data-level deep learning methods.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨数据层面的深度学习方法。
- en: Questions
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are some challenges in porting data imbalance handling methods from classical
    machine learning models to deep learning models?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据不平衡处理方法从经典机器学习模型迁移到深度学习模型中面临哪些挑战？
- en: How could an imbalanced version of the MNIST dataset be created?
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何创建一个不平衡版本的MNIST数据集？
- en: Use the MNIST dataset to train a CNN model with varying degrees of imbalance
    in the data. Record the model’s overall accuracy on a fixed test set. Plot how
    the overall accuracy changes as the imbalance in the training data increases.
    Observe whether the overall accuracy declines as the training data becomes more
    imbalanced.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MNIST数据集训练一个具有不同数据不平衡程度的CNN模型。记录模型在固定测试集上的整体准确率。绘制整体准确率随训练数据不平衡程度增加的变化情况。观察整体准确率是否随着训练数据的不平衡而下降。
- en: What is the purpose of using random oversampling with deep learning models?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用深度学习模型进行随机过采样有什么目的？
- en: What are some of the data augmentation techniques that can be applied when dealing
    with limited or imbalanced data?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理有限或不平衡数据时，有哪些数据增强技术可以应用？
- en: How does undersampling work in handling data imbalance, and what are its limitations?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理数据不平衡时，欠采样是如何工作的，以及它的局限性是什么？
- en: Why is it important to ensure that the data augmentation techniques preserve
    the original labels of the dataset?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么确保数据增强技术保留数据集的原始标签很重要？
- en: References
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: A. W. Trask, Grokking Deep Learning (Manning, Shelter Island, NY, 2019).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. W. Trask, Grokking Deep Learning (Manning, Shelter Island, NY, 2019)。
- en: F. Chollet, *Deep Learning with Python*. Manning Publications, 2021.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F. Chollet, *使用Python进行深度学习*。Manning Publications，2021年。
- en: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “*Class-Balanced Loss Based
    on Effective Number of Samples*,” p. 10.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, 和 S. Belongie, “*基于有效样本数的类平衡损失*”，第10页。
- en: K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, *Learning Imbalanced Datasets
    with Label- Distribution-Aware Margin Loss* [Online]. Available at [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Cao, C. Wei, A. Gaidon, N. Arechiga, 和 T. Ma, *具有标签分布感知边缘损失的平衡学习数据集* [在线]。可在[https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf)获取。
- en: 'R. Jantanasukon and A. Thammano, *Adaptive Learning Rate for Dealing with Imbalanced
    Data in Classification Problems*. In 2021 Joint International Conference on Digital
    Arts, Media and Technology with ECTI Northern Section Conference on Electrical,
    Electronics, Computer and Telecommunication Engineering, Cha-am, Thailand: IEEE,
    Mar. 2021, pp. 229–232, doi: 10.1109/ECTIDAMTNCON51128.2021.9425715.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'R. Jantanasukon 和 A. Thammano, *在分类问题中处理不平衡数据的自适应学习率*。在2021年数字艺术、媒体和技术联合国际会议以及ECTI北方分会电气、电子、计算机和电信工程会议，泰国差安，IEEE，2021年3月，第229–232页，doi:
    10.1109/ECTIDAMTNCON51128.2021.9425715。'
- en: 'H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, *Identifying and Compensating
    for Feature Deviation in Imbalanced Deep Learning*. arXiv, Jul. 10, 2022\. Accessed:
    Dec. 14, 2022\. [Online]. Available at [http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385).'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H.-J. Ye, H.-Y. Chen, D.-C. Zhan, 和 W.-L. Chao, *在不平衡深度学习中识别和补偿特征偏差*。arXiv，2022年7月10日。访问日期：2022年12月14日。[在线]。可在[http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385)获取。
- en: 'V. Sampath, I. Maurtua, J. J. Aguilar Martín, and A. Gutierrez, *A survey on
    generative adversarial networks for imbalance problems in computer vision tasks*.
    J Big Data, vol. 8, no. 1, p. 27, Dec. 2021, doi: 10.1186/s40537-021-00414-0.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'V. Sampath, I. Maurtua, J. J. Aguilar Martín, 和 A. Gutierrez, *关于计算机视觉任务中不平衡问题的生成对抗网络综述*。J
    Big Data，第8卷，第1期，第27页，2021年12月，doi: 10.1186/s40537-021-00414-0。'
- en: 'M. Buda, A. Maki, and M. A. Mazurowski, *A systematic study of the class imbalance
    problem in convolutional neural networks*. Neural Networks, vol. 106, pp. 249–259,
    Oct. 2018, doi: 10.1016/j.neunet.2018.07.011.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'M. Buda, A. Maki, 和 M. A. Mazurowski, *关于卷积神经网络中类不平衡问题的系统研究*。Neural Networks，第106卷，第249–259页，2018年10月，doi:
    10.1016/j.neunet.2018.07.011。'
- en: 'K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, *On the
    combined effect of class imbalance and concept complexity in deep learning*. arXiv,
    Jul. 29, 2021\. Accessed: Mar. 28, 2023\. [Online]. Available at [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194).'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, 和 N. Japkowicz, *在深度学习中类不平衡和概念复杂性的综合影响*。arXiv，2021年7月29日。访问日期：2023年3月28日。[在线]。可在[http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194)获取。
- en: 'K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, *On the
    combined effect of class imbalance and concept complexity in deep learning*. arXiv,
    Jul. 29, 2021\. Accessed: Mar. 28, 2023\. [Online]. Available at [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194).'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, 和 N. Japkowicz, *在深度学习中类不平衡和概念复杂性的综合影响*.
    arXiv，2021 年 7 月 29 日。访问时间：2023 年 3 月 28 日。[在线]。可在 [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194)
    获取。
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Medicare fraud detection using neural
    networks*. J Big Data, vol. 6, no. 1, p. 63, Dec. 2019, doi: 10.1186/s40537-019-0225-0.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'J. M. Johnson 和 T. M. Khoshgoftaar, *使用神经网络进行医疗保险欺诈检测*. 大数据杂志，第 6 卷，第 1 期，第
    63 页，2019 年 12 月，doi: 10.1186/s40537-019-0225-0.'
- en: '*Medicare fraud & abuse: prevention, detection, and reporting*. Centers for
    Medicare & Medicaid Services. 2017\. [https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244](https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244).'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*医疗保险欺诈与滥用：预防、检测和报告*. 医疗保险与医疗补助服务中心。2017。[https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244](https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244).'
- en: 'R. Shwartz-Ziv and A. Armon, *Tabular Data: Deep Learning is Not All You Need*.
    arXiv, Nov. 23, 2021\. Accessed: Apr. 10, 2023\. [Online]. Available at [http://arxiv.org/abs/2106.03253](http://arxiv.org/abs/2106.03253).'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Shwartz-Ziv 和 A. Armon, *表格数据：深度学习并非一切所需*. arXiv，2021 年 11 月 23 日。访问时间：2023
    年 4 月 10 日。[在线]。可在 [http://arxiv.org/abs/2106.03253](http://arxiv.org/abs/2106.03253)
    获取。
- en: 'V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, and G. Kasneci,
    *Deep Neural Networks and Tabular Data: A Survey*. IEEE Trans. Neural Netw. Learning
    Syst., pp. 1–21, 2022, doi: 10.1109/TNNLS.2022.3229161.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, 和 G. Kasneci, *深度神经网络和表格数据：综述*.
    IEEE 交易神经网络与学习系统，第 1-21 页，2022 年，doi: 10.1109/TNNLS.2022.3229161.'
- en: 'A. N. Tarekegn, M. Giacobini, and K. Michalak, *A review of methods for imbalanced
    multi-label classification*. Pattern Recognition, vol. 118, p. 107965, Oct. 2021,
    doi: 10.1016/j. patcog.2021.107965.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'A. N. Tarekegn, M. Giacobini, 和 K. Michalak, *不平衡多标签分类方法综述*. 模式识别，第 118 卷，第
    107965 页，2021 年 10 月，doi: 10.1016/j.patcog.2021.107965.'
