- en: Natural Language Processing Using Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行自然语言处理
- en: 'In this chapter, we''ll study and implement common algorithms that are used
    in NLP, which can help us develop machines that are capable of automatically analyzing
    and understanding human text and speech in context. Specifically, we will study
    and implement the following classes of computer science algorithms related to
    NLP:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究和实现常用的自然语言处理（NLP）算法，这些算法可以帮助我们开发能够自动分析和理解人类文本和语音的机器。具体来说，我们将研究和实现以下几类与NLP相关的计算机科学算法：
- en: 'Feature transformers, including the following:'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征转换器，包括以下内容：
- en: Tokenization
  id: totrans-3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Stemming
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatization
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原
- en: Normalization
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化
- en: 'Feature extractors, including the following :'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取器，包括以下内容：
- en: Bag of words
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋模型
- en: Term frequency–inverse document frequency
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词频-逆文档频率
- en: Feature transformers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征转换器
- en: The fundamental concept behind natural language processing is treating human
    text and speech as data—just like the structured and unstructured numerical and
    categorical data sources we have encountered in this book thus far—while preserving
    its *context*. However, natural language is notoriously difficult to understand,
    even for humans, let alone machines! Not only does natural language consist of
    hundreds of different spoken languages, with different writing systems, but it
    also poses other challenges, such as different tones, inflections, slang, abbreviations,
    metaphors, and sarcasm. Writing systems and communication platforms in particular
    provide us with text that may contain spelling mistakes, unconventional grammar,
    and sentences that are loosely structured.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理背后的基本概念是将人类文本和语音视为数据——就像我们在本书中迄今为止遇到的结构化和非结构化数值和分类数据源一样——同时保留其*上下文*。然而，自然语言是出了名的难以理解，即使是对于人类来说也是如此，更不用说机器了！自然语言不仅包括数百种不同的口语语言，具有不同的书写系统，而且还提出了其他挑战，如不同的语调、屈折、俚语、缩写、隐喻和讽刺。特别是书写系统和通信平台为我们提供了可能包含拼写错误、非传统语法和结构松散的句子的文本。
- en: Our first challenge, therefore, is to convert natural language into data that
    can be used by a machine while preserving its underlying context. Furthermore,
    when applied to machine learning, we also need to convert natural language into
    feature vectors in order to train machine learning models. Well, there are two
    broad classes of computer science algorithms that help us with these challenges—**feature
    extractors**, which help us extract relevant features from the natural language
    data, and **feature transformers**, which help us scale, convert, and/or modify
    these features in preparation for subsequent modelling. In this subsection, we
    will discuss feature transformers and how they can help us convert our natural
    language data into structures that are easier to process. First, let's introduce
    some common definitions within NLP.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的第一个挑战是将自然语言转换为机器可以使用的、同时保留其潜在上下文的数据。此外，当应用于机器学习时，我们还需要将自然语言转换为特征向量，以便训练机器学习模型。好吧，有两种广泛的计算机科学算法帮助我们应对这些挑战——**特征提取器**，它帮助我们从自然语言数据中提取相关特征，以及**特征转换器**，它帮助我们缩放、转换和/或修改这些特征，以便为后续建模做准备。在本小节中，我们将讨论特征转换器以及它们如何帮助我们将自然语言数据转换为更容易处理的结构。首先，让我们介绍一些NLP中的常见定义。
- en: Document
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档
- en: In NLP, a document represents a logical container of text. The container itself
    can be anything that makes sense within the context of your use case. For example,
    one document could refer to a single article, record, social media posting, or
    tweet.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，文档代表文本的逻辑容器。容器本身可以是任何在您的用例上下文中有意义的东西。例如，一个文档可以指一篇单独的文章、记录、社交媒体帖子或推文。
- en: Corpus
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库
- en: Once you have defined what your document represents, a corpus is defined as
    a logical collection of documents. Using the previous examples, a corpus could
    represent a collection of articles (for example, a magazine or blog) or a collection
    of tweets (for example, tweets with a particular hashtag).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了你的文档代表什么，语料库就被定义为一系列逻辑上的文档集合。使用之前的例子，语料库可以代表一系列文章（例如，一本杂志或博客）或一系列推文（例如，带有特定标签的推文）。
- en: Preprocessing pipeline
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理管道
- en: One of the basic tasks involved in NLP is the preprocessing of your documents
    in an attempt to standardize the text from different sources as much as possible.
    Not only does preprocessing help us to standardize text, it often reduces the
    size of the raw text, thereby reducing the computational complexity of subsequent
    processes and models. The following subsections describe common preprocessing
    techniques that may constitute a typical ordered preprocessing pipeline.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理中涉及的基本任务之一是尝试尽可能标准化来自不同来源的文档，以进行预处理。预处理不仅帮助我们标准化文本，通常还能减少原始文本的大小，从而降低后续过程和模型计算复杂度。以下小节描述了可能构成典型有序预处理管道的常见预处理技术。
- en: Tokenization
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization refers to the technique of splitting up your text into individual
    *tokens* or terms. Formally, a token is defined as a sequence of characters that
    represents a subset of the original text. Informally, tokens are typically just
    the different words that make up the original text, and that have been segmented
    using the whitespace and other punctuation characters. For example, the sentence
    "Machine Learning with Apache Spark" may result in a collection of tokens persisted
    in an array or list expressed as `["Machine", "Learning", "with", "Apache", "Spark"]`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是指将文本分割成单个*标记*或术语的技术。正式来说，一个标记被定义为代表原始文本子集的字符序列。非正式来说，标记通常是组成原始文本的不同单词，并且这些单词是通过使用空白和其他标点符号进行分割的。例如，句子“使用Apache
    Spark的机器学习”可能产生一个以数组或列表形式持久化的标记集合，表示为`["Machine", "Learning", "with", "Apache",
    "Spark"]`。
- en: Stop words
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词
- en: Stop words are common words in a given language that are used to structure a
    sentence grammatically, but that are not necessarily helpful in determining its
    underlying meaning or sentiment. For example, in the English language, common
    stop words include *and*, *I*, *there*, *this*, and *with*. A common preprocessing
    technique is to therefore remove these words from the collection of tokens by
    filtering based on a language-specific lookup of stop words. Using our previous
    example, our filtered list of tokens would be `["Machine", "Learning", "Apache",
    "Spark]`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是在给定语言中常用的单词，用于结构化句子语法，但它们在确定其潜在意义或情感方面不一定有帮助。例如，在英语中，常见的停用词包括*and*、*I*、*there*、*this*和*with*。因此，一个常见的预处理技术是通过基于特定语言的停用词查找来过滤这些单词，从而从标记集合中移除它们。使用我们之前的例子，我们的过滤标记列表将是`["Machine",
    "Learning", "Apache", "Spark"]`。
- en: Stemming
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取
- en: Stemming refers to the technique of reducing words to a common base or *stem*.
    For example, the words "connection", "connections", "connective", "connected",
    and "connecting" can all be reduced to their common stem of "connect". Stemming
    is not a perfect process, and stemming algorithms are liable to make mistakes.
    However, for the purposes of reducing the size of a dataset in order to train
    a machine learning model, it is a valuable technique. Using our previous example,
    our filtered list of stems would be `["Machin", "Learn", "Apach", "Spark"]`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是指将单词还原到共同基础或*词干*的技术。例如，单词“connection”、“connections”、“connective”、“connected”和“connecting”都可以还原到它们的共同词干“connect”。词干提取不是一个完美的过程，词干提取算法可能会出错。然而，为了减少数据集的大小以训练机器学习模型，它是一种有价值的技巧。使用我们之前的例子，我们的过滤词干列表将是`["Machin",
    "Learn", "Apach", "Spark"]`。
- en: Lemmatization
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词形还原
- en: While stemming quickly reduces words to a base form, it does not take into account
    the context, and can therefore not differentiate between words that have different
    meanings depending on their position within a sentence or context. Lemmatization
    does not crudely reduce words purely based on a common stem, but instead aims
    to remove inflectional endings only in order to return a dictionary form of a
    word called its *lemma*. For example, the words *am*, *is*, *being*, and *was*
    can be reduced to the lemma *be*, while a stemmer would not be able to infer this
    contextual meaning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词干提取可以快速将单词还原到基本形式，但它并没有考虑到上下文，因此不能区分在句子或上下文中位置不同而具有不同意义的单词。词形还原并不是简单地基于共同词干来还原单词，而是旨在仅移除屈折词尾，以便返回一个称为*词元*的单词的词典形式。例如，单词*am*、*is*、*being*和*was*可以被还原为词元*be*，而词干提取器则无法推断出这种上下文意义。
- en: While lemmatization can be used to preserve context and meaning to a better
    extent, it comes at the cost of additional computational complexity and processing
    time. Using our previous example, our filtered list of lemmas may therefore look
    like `["Machine", "Learning", "Apache", "Spark"]`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词形还原可以在更大程度上保留上下文和意义，但它是以额外的计算复杂度和处理时间为代价的。因此，使用我们之前的例子，我们的过滤词元列表可能看起来像 `["Machine",
    "Learning", "Apache", "Spark"]`。
- en: Normalization
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正规化
- en: Finally, normalization refers to a wide variety of common techniques that are
    used to standardize text. Typical normalization techniques include converting
    all text to lowercase, removing selected characters, punctuation and other sequences
    of characters (typically using regular expressions), and expanding abbreviations
    by applying language-specific dictionaries of common abbreviations and slang terms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正规化指的是一系列常用的技术，用于标准化文本。典型的正规化技术包括将所有文本转换为小写，删除选定的字符、标点符号和其他字符序列（通常使用正则表达式），以及通过应用特定于语言的常用缩写和俚语词典来扩展缩写。
- en: '*Figure 6.1* illustrates a typical ordered preprocessing pipeline that is used
    to standardize raw written text:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1* 展示了一个典型的有序预处理管道，该管道用于标准化原始书面文本：'
- en: '![](img/2166c5b8-2ae4-494c-aeff-0addd02bdefa.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2166c5b8-2ae4-494c-aeff-0addd02bdefa.png)'
- en: 'Figure 6.1: Typical preprocessing pipeline'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：典型的预处理管道
- en: Feature extractors
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取器
- en: We have seen how feature transformers allow us to convert, modify, and standardize
    our documents using a preprocessing pipeline, resulting in the conversion of raw
    text into a collection of tokens. *Feature extractors* take these tokens and generate
    feature vectors from them that may then be used to train machine learning models.
    Two common examples of typical feature extractors that are used in NLP are the
    **bag of words** and **term frequency–inverse document frequency** (**TF–IDF**)
    algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到特征转换器如何通过预处理管道将我们的文档进行转换、修改和标准化，从而将原始文本转换为一系列标记。*特征提取器*将这些标记提取出来，并生成特征向量，这些向量可以用于训练机器学习模型。在NLP中使用的典型特征提取器的两个常见例子是**词袋模型**和**词频-逆文档频率**（**TF-IDF**）算法。
- en: Bag of words
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'The *bag of words* approach simply counts the number of occurrences of each
    unique word in the raw or tokenized text. For example, given the text "Machine
    Learning with Apache Spark, Apache Spark''s MLlib and Apache Kafka", the bag of
    words approach would provide us with the following numerical feature vector:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*词袋模型*方法简单地计算每个独特单词在原始或标记文本中出现的次数。例如，给定文本 "Machine Learning with Apache Spark,
    Apache Spark''s MLlib and Apache Kafka"，词袋模型将为我们提供一个以下数值特征向量：'
- en: '| Machine | Learning | with | Apache | Spark | MLlib | Kafka |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Machine | Learning | with | Apache | Spark | MLlib | Kafka |'
- en: '| 1 | 1 | 1 | 3 | 2 | 1 | 1 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 3 | 2 | 1 | 1 |'
- en: Note that each unique word is a feature or dimension, and that the bag of words
    approach is a simple technique that is often employed as a baseline model with
    which to compare the performance of more advanced feature extractors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个独特的单词都是一个特征或维度，而词袋模型方法是一种简单的技术，通常用作基准模型，以比较更高级特征提取器的性能。
- en: Term frequency–inverse document frequency
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词频-逆文档频率
- en: '**TF–IDF** aims to improve upon the bag of words approach by providing an indication
    of how *important* each word is, taking into account how often that word appears
    across the entire corpus.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF** 旨在通过提供每个词在整个语料库中出现的频率的**重要性**指标来改进词袋模型方法。'
- en: 'Let us use *TF(t, d)* to denote the **term frequency**, which is the number
    of times that a term, *t*, appears in a document, *d*. Let''s also use *DF(t,
    D)* to denote the **document frequency**, which is the number of documents in
    our corpus, *D*, that contain the term *t*. We can then define the **inverse document
    frequency** *IDF(t, D)* as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 *TF(t, d)* 来表示**词频**，即一个词 *t* 在文档 *d* 中出现的次数。我们还可以用 *DF(t, D)* 来表示**文档频率**，即包含该词
    *t* 的文档数量，在我们的语料库 *D* 中。然后我们可以定义**逆文档频率** *IDF(t, D)* 如下：
- en: '![](img/a66d7f42-032c-4cd8-8362-92c147274cbc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a66d7f42-032c-4cd8-8362-92c147274cbc.png)'
- en: The IDF provides us with a measure of how important a term is, taking into account
    how often that term appears across the entire corpus, where *|D|* is the total
    number of documents in our corpus, *D*. Terms that are less common across the
    corpus have a higher IDF metric. Note, however, that because of the use of the
    logarithm, if a term appears in all documents, its IDF becomes 0—that is, *log(1)*.
    IDF, therefore, provides a metric whereby more value is placed on rarer terms
    that are important in describing documents.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: IDF为我们提供了一个衡量一个词重要性的度量，考虑到该词在整个语料库中出现的频率，其中*|D|*是我们语料库中文档的总数，*D*。在语料库中不那么常见的词具有更高的IDF度量。然而，请注意，由于使用了对数，如果一个词出现在所有文档中，其IDF变为0——即*log(1)*。因此，IDF提供了一个度量标准，更重视描述文档中重要但罕见的词。
- en: 'Finally, to calculate the TF–IDF measure, we simply multiply the term frequency
    by the inverse document frequency as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了计算TF–IDF度量，我们只需将词频乘以逆文档频率，如下所示：
- en: '![](img/fc877716-e3b7-4aba-a75e-9a744a014d5c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc877716-e3b7-4aba-a75e-9a744a014d5c.png)'
- en: This implies that the TF–IDF measure increases proportionally with the number
    of times that a word appears in a document, offset by the frequency of the word
    across the entire corpus. This is important because the term frequency alone may
    highlight words such as "a", "I", and "the" that appear very often in a given
    document but that do not help us determine the underlying meaning or sentiment
    of the text. By employing TF–IDF, we can reduce the impact of these types of words
    on our analysis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着TF–IDF度量与一个词在文档中出现的次数成比例增加，同时抵消了该词在整个语料库中的频率。这一点很重要，因为仅凭词频可能突出显示像“a”、“I”和“the”这样的词，这些词在特定文档中非常常见，但并不能帮助我们确定文本的潜在含义或情感。通过使用TF–IDF，我们可以减少这些类型词语对我们分析的影响。
- en: Case study – sentiment analysis
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 情感分析
- en: Let's now apply these feature transformers and feature extractors to a very
    modern real-world use case—sentiment analysis. In sentiment analysis, the goal
    is to classify the underlying human sentiment—for example, whether the writer
    is positive, neutral, or negative towards the subject of a text. To many organizations,
    sentiment analysis is an important technique that is used to better understand
    their customers and target markets. For example, sentiment analysis can be used
    by retailers to gauge the public's reaction to a particular product, or by politicians
    to assess public mood towards a policy or news item. In our case study, we will
    examine tweets about airlines in order to predict whether customers are saying
    positive or negative things about them. Our analysis could then be used by airlines
    in order to improve their customer service by focusing on those tweets that have
    been classified as negative in sentiment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这些特征转换器和特征提取器应用于一个非常现代的真实世界用例——情感分析。在情感分析中，目标是分类潜在的文本情感——例如，作者对文本主题是积极、中立还是消极。对许多组织来说，情感分析是一项重要的技术，用于更好地了解他们的客户和目标市场。例如，零售商可以使用情感分析来衡量公众对特定产品的反应，或政治家可以评估公众对政策或新闻条目的情绪。在我们的案例研究中，我们将研究关于航空公司的推文，以预测客户是否对他们表示正面或负面的评论。我们的分析然后可以被航空公司用来通过关注那些被分类为负面情感的推文来改善他们的客户服务。
- en: The corpus of tweets that we will use for our case study has been downloaded
    from **Figure Eight**, a company that provides businesses with high-quality training
    datasets for real-world machine learning. Figure Eight also provides a Data for
    Everyone platform containing open datasets that are available for download by
    the public, and which may be found at [https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于案例研究的推文语料库已从**Figure Eight**下载，这是一家为商业提供高质量真实世界机器学习训练数据集的公司。Figure Eight还提供了一个“数据人人共享”平台，包含可供公众下载的开放数据集，网址为[https://www.figure-eight.com/data-for-everyone/](https://www.figure-eight.com/data-for-everyone/)。
- en: 'If you open `twitter-data/airline-tweets-labelled-corpus.csv` in any text editor
    from either the GitHub repository accompanying this book or from Figure Eight''s
    Data for Everyone platform, you will find a collection of 14,872 tweets about
    major airlines that were scraped from Twitter in February 2015\. These tweets
    have also been pre-labelled for us, with a sentiment classification of positive,
    negative, or neutral. The pertinent columns in this dataset are described in the
    following table:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从本书附带的 GitHub 仓库或 Figure Eight 的 Data for Everyone 平台上的任何文本编辑器打开 `twitter-data/airline-tweets-labelled-corpus.csv`，你将找到一组
    14,872 条关于主要航空公司的推文，这些推文是在 2015 年 2 月从 Twitter 上抓取的。这些推文也已经为我们预先标记，包括正面、负面或中性的情感分类。该数据集中的相关列在以下表中描述：
- en: '| **Column Name** | **Data Type** | **Description** |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **列名** | **数据类型** | **描述** |'
- en: '| `unit_id` | `Long` | Unique identifier (primary key) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `unit_id` | `Long` | 唯一标识符（主键） |'
- en: '| `airline_sentiment` | `String` | Sentiment classification—positive, neutral,
    or negative |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `airline_sentiment` | `String` | 情感分类——正面、中性或负面 |'
- en: '| `airline` | `String` | Name of the airline |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `airline` | `String` | 航空公司名称 |'
- en: '| `text` | `String` | Textual content of the tweet |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `text` | `String` | 推文的文本内容 |'
- en: Our goal will be to use this corpus of tweets in order to train a machine learning
    model to predict whether future tweets about a given airline are positive or negative
    in sentiment towards that airline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标将是使用这个推文语料库来训练一个机器学习模型，以预测关于特定航空公司的未来推文对该航空公司的情感是正面还是负面。
- en: NLP pipeline
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP 流程
- en: 'Before we look at the Python code for our case study, let''s visualize the
    end-to-end NLP pipeline that we will construct. Our NLP pipeline for this case
    study is illustrated in *Figure 6.2*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看案例研究的 Python 代码之前，让我们可视化我们将构建的端到端 NLP 流程。本案例研究的 NLP 流程如图 *6.2* 所示：
- en: '![](img/e322457d-6d42-4f82-999a-a2a446d5862e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e322457d-6d42-4f82-999a-a2a446d5862e.png)'
- en: 'Figure 6.2: End-to-end NLP pipeline'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：端到端 NLP 流程
- en: NLP in Apache Spark
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 中的 NLP
- en: As of Spark 2.3.2, tokenization and stop-word removal feature transformers (among
    a wide variety of others), and the TF–IDF feature extractor is available natively
    in `MLlib`. Although stemming, lemmatization, and standardization can be achieved
    indirectly through transformations on Spark dataframes in Spark 2.3.2 (via **user-defined
    functions** (**UDFs**) and map functions that are applied to RDDs), we will be
    using a third-party Spark library called `spark-nlp` to perform these feature
    transformations. This third-party library has been designed to extend the features
    already available in `MLlib` by providing an easy-to-use API for distributed NLP
    annotations on Spark dataframes at scale. To learn more about `spark-nlp`, please
    visit [https://nlp.johnsnowlabs.com/](https://nlp.johnsnowlabs.com/). Finally,
    we will use the estimators and transformers that are already available natively
    in `MLlib`—as we have seen in previous chapters—to train our final machine learning
    classification models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 Spark 2.3.2 版本，标记化（tokenization）和停用词去除（stop-word removal）功能转换器（以及其他众多功能），以及
    TF–IDF 特征提取器在 `MLlib` 中原生支持。尽管在 Spark 2.3.2 中可以通过对 Spark 数据框上的转换（通过**用户定义函数**（**UDFs**）和应用于
    RDDs 的映射函数）间接实现词干提取（stemming）、词形还原（lemmatization）和标准化，但我们将使用一个名为 `spark-nlp` 的第三方
    Spark 库来执行这些特征转换。这个第三方库已被设计用来通过提供一个易于使用的 API 来扩展 `MLlib` 中已有的功能，以便在 Spark 数据框上进行大规模的分布式
    NLP 标注。要了解更多关于 `spark-nlp` 的信息，请访问 [https://nlp.johnsnowlabs.com/](https://nlp.johnsnowlabs.com/)。最后，我们将使用
    `MLlib` 中已经原生支持的估计器和转换器——正如我们在前面的章节中所见——来训练我们的最终机器学习分类模型。
- en: Note that by using feature transformers and extractors native to `MLlib` followed
    by feature transformers provided by the third-party `spark-nlp` library, before
    finally applying native `MLlib` estimators, we will be required to explicitly
    define and develop data transformation stages in our pipeline in order to conform
    to the underlying data structures expected by the two different libraries. While
    this is not recommended for production-grade pipelines because of its inefficiencies,
    one of the purposes of this section is to demonstrate how to use both libraries
    for NLP. Readers will then be in an informed position to choose a suitable library,
    depending on the requirements of the use case in question.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通过使用 `MLlib` 内置的特征转换器和提取器，然后使用第三方 `spark-nlp` 库提供的特征转换器，最后应用本地的 `MLlib` 估算器，我们将在我们的流程中需要显式定义和开发数据转换阶段，以符合两个不同库期望的底层数据结构。虽然这由于其低效性不推荐用于生产级流程，但本节的一个目的就是演示如何使用这两个库进行
    NLP。读者将能够根据所讨论用例的要求选择合适的库。
- en: 'Depending on your environment setup, there are a few methods that are available
    that can be used to install `spark-nlp`, as described at [https://nlp.johnsnowlabs.com/quickstart.html](https://nlp.johnsnowlabs.com/quickstart.html).
    However, based on the local development environment that we provisioned in [Chapter
    2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml),* Setting Up a Local Development
    Environment*, we will install `spark-nlp` using `pip`, which is another commonly
    used Python package manager that comes bundled with the Anaconda distribution
    that we have already installed (at the time of writing, `spark-nlp` is not available
    via the `conda` repositories, and so we shall use `pip` instead). To install `spark-nlp`
    for our Python environment, simply execute the following command, which will install
    version 1.7.0 of `spark-nlp` (which is the latest version as of writing, and which
    is compatible with Spark 2.x):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的环境设置，有几种方法可以用来安装 `spark-nlp`，具体描述见[https://nlp.johnsnowlabs.com/quickstart.html](https://nlp.johnsnowlabs.com/quickstart.html)。然而，根据我们在[第2章](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml)中配置的本地开发环境* 设置本地开发环境*，我们将使用
    `pip` 来安装 `spark-nlp`，这是 Anaconda 分发版中捆绑的另一个常用 Python 包管理器（截至写作时，`spark-nlp` 通过
    `conda` 仓库不可用，因此我们将使用 `pip`）。要为我们的 Python 环境安装 `spark-nlp`，只需执行以下命令，这将安装 `spark-nlp`
    的 1.7.0 版本（这是截至写作时的最新版本，并且与 Spark 2.x 兼容）：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then need to tell Spark where it can find the `spark-nlp` library. We can
    do this either by defining an additional parameter in `{SPARK_HOME}/conf/spark-defaults.conf`
    or by setting the `spark.jars` configuration within our code when instantiating
    a Spark context, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要告诉 Spark 它可以在哪里找到 `spark-nlp` 库。我们可以通过在 `{SPARK_HOME}/conf/spark-defaults.conf`
    中定义一个额外的参数，或者在实例化 Spark 上下文时在我们的代码中设置 `spark.jars` 配置来实现，如下所示：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please refer to [Chapter 2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting
    Up a Local Development Environment*, for further details regarding defining the
    configuration for Apache Spark. Note that in a multinode Spark cluster, all third-party
    Python packages either need to be installed on all Spark nodes or your Spark application
    itself needs to be packaged into a self-contained file containing all third-party
    dependencies. This self-contained file is then distributed to all nodes in the
    Spark cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第2章](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml)，*设置本地开发环境*，以获取有关定义 Apache
    Spark 配置的更多详细信息。请注意，在多节点 Spark 集群中，所有第三方 Python 包要么需要在所有 Spark 节点上安装，要么您的 Spark
    应用程序本身需要打包成一个包含所有第三方依赖项的自包含文件。然后，这个自包含文件将被分发到 Spark 集群的所有节点上。
- en: 'We are now ready to develop our NLP pipeline in Apache Spark in order to perform
    sentiment analysis on our corpus of airline tweets! Let''s go through the following
    steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好在 Apache Spark 中开发我们的 NLP 流程，以便对航空推文语料库进行情感分析！让我们按以下步骤进行：
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp06-01-natural-language-processing.ipynb`.
    It can be found in the GitHub repository accompanying this book.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节描述了对应于本用例的 Jupyter 笔记本中相关的每个单元格，该笔记本称为 `chp06-01-natural-language-processing.ipynb`。它可以在本书附带的
    GitHub 仓库中找到。
- en: 'As well as importing the standard PySpark dependencies, we also need to import
    the relevant `spark-nlp` dependencies, including its `Tokenizer`, `Stemmer`, and
    `Normalizer` classes, as follows:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了导入标准的PySpark依赖项之外，我们还需要导入相关的`spark-nlp`依赖项，包括其`Tokenizer`、`Stemmer`和`Normalizer`类，如下所示：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we instantiate a `SparkContext` as usual. Note, however, that in this
    case, we explicitly tell Spark where to find the `spark-nlp` library using the
    `spark-jars` configuration parameter. We can then invoke the `getConf()` method
    on our `SparkContext` instance to review the current Spark configuration, as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们像往常一样实例化一个`SparkContext`。请注意，然而，在这种情况下，我们明确告诉Spark使用`spark-jars`配置参数在哪里找到`spark-nlp`库。然后，我们可以调用我们的`SparkContext`实例上的`getConf()`方法来查看当前的Spark配置，如下所示：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After loading our corpus of airline tweets from `twitter-data/airline-tweets-labelled-corpus.csv`
    into a Spark dataframe called `airline_tweets_df`, we generate a new label column.
    The existing dataset already contains a label column called `airline_sentiment`,
    which is either `"positive"`, `"neutral"`, or `"negative"` based on a manual pre-classification.
    Although positive messages are naturally always welcome, in reality, the most
    useful messages are usually the negative ones. By automatically identifying and
    studying the negative messages, organizations can focus more efficiently on how
    to improve their products and services based on negative feedback. Therefore,
    we will create a new label column called `negative_sentiment_label` that is `"true"`
    if the underlying sentiment has been classified as `"negative"` and `"false"`
    otherwise, as shown in the following code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将我们的航空公司推文语料库从`twitter-data/airline-tweets-labelled-corpus.csv`加载到名为`airline_tweets_df`的Spark数据框之后，我们生成一个新的标签列。现有的数据集已经包含一个名为`airline_sentiment`的标签列，该列基于手动预分类，可以是`"positive"`、`"neutral"`或`"negative"`。尽管积极的信息自然是始终受欢迎的，但在现实中，最有用的信息通常是负面的。通过自动识别和研究负面信息，组织可以更有效地关注如何根据负面反馈改进他们的产品和服务。因此，我们将创建一个名为`negative_sentiment_label`的新标签列，如果基础情感被分类为`"negative"`则为`"true"`，否则为`"false"`，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now ready to build and apply our preprocessing pipeline to our corpus
    of raw tweets! Here, we demonstrate how to utilize the feature transformers native
    to Spark''s `MLlib`, namely its `Tokenizer` and `StopWordsRemover` transformers.
    First, we tokenize the raw textual content of each tweet using the `Tokenizer`
    transformer, resulting in a new column containing a list of parsed tokens. We
    then pass this column containing the tokens to the `StopWordsRemover` transformer,
    which removes English language (default) stop words from this list, resulting
    in a new column containing the list of filtered tokens. In the next cell, we will
    demonstrate how to utilize the feature transformers available in the `spark-nlp` third-party
    library. However, `spark-nlp` requires a column of a `string` type as its initial
    input, not a list of tokens. Therefore, the final statement concatenates the list
    of filtered tokens back into a whitespace-delimited `string` column, as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备构建并应用我们的预处理管道到我们的原始推文语料库！在这里，我们展示了如何利用Spark的`MLlib`自带的特征转换器，即其`Tokenizer`和`StopWordsRemover`转换器。首先，我们使用`Tokenizer`转换器对每条推文的原始文本内容进行分词，从而得到一个包含解析后的标记列表的新列。然后，我们将包含标记的该列传递给`StopWordsRemover`转换器，该转换器从列表中移除英语语言（默认）的停用词，从而得到一个包含过滤后标记列表的新列。在下一单元格中，我们将展示如何利用`spark-nlp`第三方库中可用的特征转换器。然而，`spark-nlp`需要一个`string`类型的列作为其初始输入，而不是标记列表。因此，最终的语句将过滤后的标记列表重新连接成一个空格分隔的`string`列，如下所示：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now demonstrate how to utilize the feature transformers and annotators
    available in the `spark-nlp` third-party library, namely its `DocumentAssember`
    transformer and `Tokenizer`, `Stemmer` , and `Normalizer` annotators. First, we
    create annotated documents from our string column that are required as the initial
    input into the `spark-nlp` pipelines. Then, we apply the `spark-nlp` `Tokenizer`
    and `Stemmer` annotators to convert our filtered list of tokens into a list of
    *stems*. Finally, we apply its `Normalizer` annotator, which converts the stems
    into lowercase by default. All of these stages are defined within a *pipeline*, which,
    as we saw in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, is an ordered list of machine learning and data
    transformation steps that is executed on a Spark dataframe.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以展示如何利用`spark-nlp`第三方库中可用的功能转换器和标注器，即其`DocumentAssember`转换器和`Tokenizer`、`Stemmer`、`Normalizer`标注器。首先，我们从字符串列创建标注文档，这些文档作为`spark-nlp`管道的初始输入。然后，我们应用`spark-nlp`的`Tokenizer`和`Stemmer`标注器，将我们的过滤令牌列表转换为词根列表。最后，我们应用其`Normalizer`标注器，该标注器默认将词根转换为小写。所有这些阶段都在一个*管道*中定义，正如我们在[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)中看到的，*使用Apache
    Spark进行监督学习*，这是一个有序的机器学习和数据转换步骤列表，在Spark数据帧上执行。
- en: 'We execute our pipeline on our dataset, resulting in a new dataframe called
    `preprocessed_df` from which we keep only the relevant columns that are required
    for subsequent analysis and modelling, namely `unit_id` (unique record identifier),
    `text` (original raw textual content of the tweet), `negative_sentiment_label` (our
    new label), and `normalised_stems` (a `spark-nlp` array of filtered, stemmed,
    and normalized tokens as a result of our preprocessing pipeline), as shown in
    the following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们的数据集上执行我们的管道，得到一个新的数据帧`preprocessed_df`，我们只保留后续分析和建模所需的相关列，即`unit_id`（唯一记录标识符）、`text`（推文的原始原始文本内容）、`negative_sentiment_label`（我们新的标签）和`normalised_stems`（作为预处理管道结果的一个过滤、词根化和归一化的`spark-nlp`数组），如下面的代码所示：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Before we can create feature vectors from our array of stemmed tokens using
    `MLlib`''s native feature extractors, there is one final preprocessing step. The
    column containing our stemmed tokens, namely `normalised_stems`, persists these
    tokens in a specialized `spark-nlp` array structure. We need to convert this `spark-nlp`
    array back into a standard list of tokens so that we may apply `MLlib`''s native
    TF–IDF algorithms to it. We achieve this by first exploding the `spark-nlp` array
    structure, which has the effect of creating a new dataframe observation for every
    element in this array. We then group our Spark dataframe by `unit_id`, which is
    the primary key for each unique tweet, before aggregating the stems using the
    whitespace delimiter into a new string column called `tokens`. Finally, we apply
    the `split` function to this column to convert the aggregated string into a list
    of strings or tokens, as shown in the following code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们能够使用`MLlib`的本地特征提取器从我们的词根令牌数组创建特征向量之前，还有一个最后的预处理步骤。包含我们的词根令牌的列，即`normalised_stems`，将这些令牌持久化在专门的`spark-nlp`数组结构中。我们需要将这个`spark-nlp`数组转换回标准的令牌列表，以便我们可以应用`MLlib`的本地TF-IDF算法。我们通过首先分解`spark-nlp`数组结构来实现这一点，这会产生一个新数据帧观察结果，对应于数组中的每个元素。然后，我们在`unit_id`上对Spark数据帧进行分组，这是每个唯一推文的键，在将词根使用空格分隔符聚合到一个新的字符串列`tokens`之前。最后，我们应用`split`函数到这个列上，将聚合的字符串转换为字符串列表或令牌，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are now ready to generate feature vectors from our list of filtered, stemmed,
    and normalized tokens! As discussed, we will be using the TF–IDF feature extractor
    to generate feature vectors rather than the basic bag of words approach. The TF–IDF
    feature extractor is native to `MLlib` and comes in two parts. First, we generate the **term
    frequency** (**TF**) feature vectors by passing our list of tokens into `MLlib`''s
    `HashingTF` transformer. We then *fit* `MLlib`''s **inverse document frequency**
    (**IDF**) estimator to our dataframe containing the term frequency feature vectors,
    as shown in the following code. The result is a new Spark dataframe with our TF–IDF
    feature vectors contained in a column called `features`:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好从我们的过滤、词干提取和归一化的标记列表中生成特征向量了！正如所讨论的，我们将使用TF–IDF特征提取器来生成特征向量，而不是基本的词袋方法。TF–IDF特征提取器是`MLlib`的本地功能，分为两部分。首先，我们通过将我们的标记列表传递到`MLlib`的`HashingTF`转换器中来生成**词频**（**TF**）特征向量。然后，我们将`MLlib`的**逆文档频率**（**IDF**）估计器拟合到包含词频特征向量的数据框中，如下面的代码所示。结果是包含在名为`features`的列中的新的Spark数据框，其中包含我们的TF–IDF特征向量：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we saw in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, since our label column is categorical in nature,
    we need to apply `MLlib`''s `StringIndexer` to it in order to identify and index
    all possible classifications. The result is a new Spark dataframe with an indexed
    label column called `"label"`, which is 0.0 if `negative_sentiment_label` is `true`,
    and 1.0 if `negative_sentiment_label` is `false`, as shown in the following code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)，*使用Apache Spark进行监督学习*中所见，由于我们的标签列本质上是分类的，我们需要将其应用到`MLlib`的`StringIndexer`中，以便识别和索引所有可能的分类。结果是包含索引标签列的新Spark数据框，名为`"label"`，如果`negative_sentiment_label`为`true`，则其值为0.0，如果`negative_sentiment_label`为`false`，则其值为1.0，如下面的代码所示：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We are now ready to create training and test dataframes in order to train and
    evaluate subsequent machine learning models. We achieve this as normal, using
    the `randomSplit` method (as shown in the following code), but in this case, 90%
    of all observations will go into our training dataframe, with the remaining 10%
    going into our test dataframe:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好创建训练和测试数据框，以便训练和评估后续的机器学习模型。我们像往常一样使用`randomSplit`方法（如下面的代码所示）来实现这一点，但在这个案例中，90%的所有观察结果将进入我们的训练数据框，剩下的10%将进入我们的测试数据框：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this example, we will be training a supervised decision tree classifier
    (see [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised Learning
    Using Apache Spark*) in order to help us classify whether a given tweet is positive
    or negative in sentiment. As in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml),
    *Supervised Learning Using Apache Spark*, we fit `MLlib`''s `DecisionTreeClassifier`
    estimator to our training dataframe in order to train our classification tree,
    as shown in the following code:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将训练一个监督决策树分类器（参见[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)，*使用Apache
    Spark进行监督学习*），以便帮助我们判断给定的推文是正面情绪还是负面情绪。正如[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)，*使用Apache
    Spark进行监督学习*中所述，我们将`MLlib`的`DecisionTreeClassifier`估计器拟合到我们的训练数据框中，以训练我们的分类树，如下面的代码所示：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that we have a trained classification tree, we can apply it to our test
    dataframe in order to classify test tweets. As we did in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, we apply our trained classification tree to the
    test dataframe using the `transform()` method (as shown in the following code),
    and afterwards study its predicted classifications:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了一个分类树，我们可以将其应用到我们的测试数据框中，以便对测试推文进行分类。正如我们在[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)，*使用Apache
    Spark进行监督学习*中所做的那样，我们使用`transform()`方法（如下面的代码所示）将我们的训练好的分类树应用到测试数据框中，然后研究其预测的分类：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For example, our decision tree classifier has predicted that the following
    tweets from our test dataframe are negative in sentiment:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的决策树分类器预测了以下来自我们的测试数据框的推文是负面情绪：
- en: '"I need you...to be a better airline. ^LOL"'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"我需要你...成为一个更好的航空公司。^LOL"'
- en: '"if you can''t guarantee parents will sit with their children, don''t sell
    tickets with that promise"'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"如果不能保证家长会和孩子一起坐，就不要承诺卖票"'
- en: '"resolved and im sick and tired of waiting on you. I want my refund and I''d
    like to speak to someone about it."'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"解决了，我很烦等你们。我想退款，并想和某人谈谈这件事。"'
- en: '"I would have loved to respond to your website until I saw the really long
    form. In business the new seats are bad"'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"我本想回复你的网站，直到我看到那个真的很长的形式。在商业中，新座位很糟糕"'
- en: A human would also probably classify these tweets as negative in sentiment!
    But more importantly, airlines can use this model and the tweets that it identifies
    to focus on areas for improvement. Based on this sample of tweets, such areas
    would include website usability, ticket marketing, and the time taken to process
    refunds.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 人类也可能将这些推文归类为负面情感！但更重要的是，航空公司可以使用这个模型以及它识别的推文来关注改进的领域。根据这个推文样本，这些领域可能包括网站可用性、票务营销以及处理退款所需的时间。
- en: 'Finally, in order to quantify the accuracy of our trained classification tree,
    let''s compute its confusion matrix on the test data using the following code:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了量化我们训练的分类树的准确性，让我们使用以下代码在测试数据上计算其混淆矩阵：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting confusion matrix looks as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的混淆矩阵如下所示：
- en: '|  | **Predict *y *= 0 (Negative)** | **Predict *y* = 1 (Non-Negative)** |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测 *y *= 0 (负面)** | **预测 *y* = 1 (非负)** |'
- en: '| **Actual *y* = 0****(Negative)** | 725 | 209 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **实际 *y* = 0****(负面)** | 725 | 209 |'
- en: '| **Actual *y* = 1****(Non-Negative)** | 244 | 325 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **实际 *y* = 1****(非负)** | 244 | 325 |'
- en: 'We can interpret this confusion matrix as follows—out of a total of 1,503 test
    tweets, our model exhibits the following properties:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样解释这个混淆矩阵——在总共1,503条测试推文中，我们的模型表现出以下特性：
- en: Correctly classifies 725 tweets as negative in sentiment that are actually negative
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确地将725条实际上是情感负面的推文分类为负面情感
- en: Correctly classifies 325 tweets as non-negative in sentiment that are actually
    non-negative
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确地将325条实际上是情感非负的推文分类为非负情感
- en: Incorrectly classifies 209 tweets as non-negative in sentiment that are actually
    negative
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误地将209条实际上是负情感的推文分类为非负情感
- en: Incorrectly classifies 244 tweets as negative in sentiment that are actually
    non-negative
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误地将244条实际上是情感非负的推文分类为负面情感
- en: Overall accuracy = 70%
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体准确率 = 70%
- en: Overall error rate = 30%
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体错误率 = 30%
- en: Sensitivity = 57%
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵敏度 = 57%
- en: Specificity = 78%
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特异性 = 78%
- en: So, based on a default threshold value of 0.5 (which in this case study is fine
    because we have no preference over what type of error is better), our decision
    tree classifier has an overall accuracy rate of 70%, which is quite good!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于默认的阈值值0.5（在这个案例研究中是合适的，因为我们没有对哪种错误更好有偏好），我们的决策树分类器有70%的整体准确率，这相当不错！
- en: 'For the sake of completeness, let''s train a decision tree classifier, but
    using the feature vectors that are derived from the bag of words algorithm. Note
    that we already computed these feature vectors when we applied the `HashingTF`
    transformer to our preprocessed corpus to calculate the term frequency (TF) feature
    vectors. Therefore, we can just repeat our machine learning pipeline, but based
    only on the output of the `HashingTF` transformer instead, as follows:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完整性，让我们训练一个决策树分类器，但使用从词袋算法中导出的特征向量。请注意，当我们应用`HashingTF`转换器到我们的预处理语料库以计算词频（TF）特征向量时，我们已经计算了这些特征向量。因此，我们只需重复我们的机器学习流程，但仅基于`HashingTF`转换器的输出，如下所示：
- en: '[PRE14]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that the resulting confusion matrix is exactly the same as when we applied
    our decision tree classifier that had been trained on the *scaled* feature vectors
    using the `IDF` estimator (given the same random split seed and size of the training
    dataframe). This is because of the fact that our corpus of tweets is relatively
    small at 14,872 documents, and therefore the effect of scaling the term frequency
    (`TF`) feature vectors based on the frequency across the corpus will have a negligible
    impact on the predictive quality of this specific model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，得到的混淆矩阵与我们使用`IDF`估计器在*缩放*特征向量上训练的决策树分类器时得到的混淆矩阵完全相同。这是因为我们的推文语料库相对较小，有14,872个文档，因此基于语料库中词频的缩放词频（TF）特征向量将对这个特定模型的预测质量产生微乎其微的影响。
- en: 'A very useful feature provided by `MLlib` is the ability to save trained machine
    learning models to disk for later use. We can take advantage of this feature by
    saving our trained decision tree classifier to the local disk of our single-node
    development environment. In multi-node clusters, trained models may also be saved
    to a distributed file system, such as the Apache Hadoop Distributed File system
    (see [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*)
    by simply using the relevant file system prefix (for example `hdfs://<HDFS NameNode
    URL>/<HDFS Path>`), as shown in the following code:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MLlib` 提供的一个非常有用的功能是将训练好的机器学习模型保存到磁盘上以供以后使用。我们可以通过将训练好的决策树分类器保存到单节点开发环境的本地磁盘上，来利用这个功能。在多节点集群中，训练好的模型也可以简单地通过使用相关的文件系统前缀（例如
    `hdfs://<HDFS NameNode URL>/<HDFS Path>`）保存到分布式文件系统，如 Apache Hadoop 分布式文件系统（参见[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，*大数据生态系统*），如下面的代码所示：'
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our trained decision tree classifier for performing sentiment analysis of airline
    tweets has also been pushed to the GitHub repository accompanying this book, and
    may be found in `chapter06/models/airline-sentiment-analysis-decision-tree-classifier`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于对航空公司推文进行情感分析的训练好的决策树分类器也已推送到本书配套的 GitHub 仓库中，可以在 `chapter06/models/airline-sentiment-analysis-decision-tree-classifier`
    中找到。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have studied, implemented, and evaluated common algorithms
    that are used in natural language processing. We have preprocessed a corpus of
    documents using feature transformers and generated feature vectors from the resulting
    processed corpus using feature extractors. We have also applied these common NLP
    algorithms to machine learning. We trained and tested a sentiment analysis model
    that we used to predict the underlying sentiment of tweets so that organizations
    may improve their product and service offerings. In [Chapter 8](cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml),
    *Real-Time Machine Learning Using Apache Spark*, we will extend our sentiment
    analysis model to operate in real time using Spark Streaming and Apache Kafka.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了、实现了并评估了在自然语言处理中常用的算法。我们使用特征转换器对文档语料库进行了预处理，并使用特征提取器从处理后的语料库中生成了特征向量。我们还将这些常见的
    NLP 算法应用于机器学习。我们训练并测试了一个情感分析模型，该模型用于预测推文的潜在情感，以便组织可以改进其产品和服务的提供。在[第8章](cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml)，*使用
    Apache Spark 的实时机器学习*中，我们将扩展我们的情感分析模型，使其能够使用 Spark Streaming 和 Apache Kafka 在实时环境中运行。
- en: In the next chapter, we will take a hands-on exploration through the exciting
    and cutting-edge world of deep learning!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将亲身体验探索令人兴奋且前沿的深度学习世界！
