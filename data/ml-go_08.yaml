- en: Neural Networks and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习
- en: In this book, we have talked a lot about training, or teaching, machines to
    make predictions. To this end, we have employed a variety of useful and interesting
    algorithms including various types of regression, decisions trees, and nearest
    neighbors. However, let's take a step back and think about what entity we might
    want to mimic if we are trying to make accurate predictions and learn about data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们已经讨论了很多关于训练或教导机器进行预测的内容。为此，我们采用了各种有用且有趣的算法，包括各种类型的回归、决策树和最近邻算法。然而，让我们退一步思考，如果我们试图做出准确的预测并了解数据，我们可能想要模仿哪种实体。
- en: Well, the most obvious answer to this question is that we should mimic our own
    brains. We as humans have a natural ability to recognize objects, predict quantities,
    recognize frauds, and more, and these are all things that we would like machines
    to do artificially. Granted, we are not perfect at these activities, but we are
    pretty good!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个问题的最明显答案是我们应该模仿我们自己的大脑。作为人类，我们天生具有识别物体、预测数量、识别欺诈等能力，这些都是我们希望机器能够人工完成的事情。诚然，我们在这些活动中并不完美，但我们的表现相当不错！
- en: This type of thinking was what lead to the development of **artificial neural
    networks** (also known as **neural networks** or just **neural nets**). These
    models attempt to roughly mimic certain structures in our brains, such as **neurons**.
    They have been widely successful across industries and are currently being applied
    to solve a variety of interesting problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这种思维方式导致了**人工神经网络**（也称为**神经网络**或简称**神经网**）的发展。这些模型试图大致模仿我们大脑中的某些结构，例如**神经元**。它们在各个行业中都取得了广泛的成功，并且目前正在被应用于解决各种有趣的问题。
- en: Recently, more specialized and complicated types of neural networks have been
    attracting a lot of interest and attention. These neural networks fall under the
    category of **deep learning** and generally have a **deeper** structure than what
    would be considered regular neural networks. That is, they have many hidden layers
    of structure and could be parameterized with tens of millions of parameters or
    weights.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，更多专业和复杂的神经网络类型吸引了大量兴趣和关注。这些神经网络属于**深度学习**类别，通常比常规神经网络的结构更深。也就是说，它们具有许多隐藏层结构，并且可以用数千万个参数或权重进行参数化。
- en: We will attempt to introduce both Go-based neural networks and deep learning
    models in this chapter. These topics are incredibly broad and there are entire
    books on deep learning alone. Thus, we will only be touching the surface here.
    That being said, this following content should provide you with a solid starting
    point to build neural networks in Go.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试在本章中介绍基于Go的神经网络和深度学习模型。这些主题非常广泛，仅深度学习就有整本书的篇幅。因此，我们在这里只会触及表面。话虽如此，以下内容应该为你提供一个坚实的起点，以便在Go中构建神经网络。
- en: Understanding neural net jargon
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络术语
- en: 'There are a huge variety of neural network flavors, and each of these flavors
    has its own set of jargon. However, there is some common jargon that we should
    know regardless of the type of neural network that we are utilizing. This jargon
    is presented in the following points:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络种类繁多，每种类型都有自己的术语集。然而，无论我们使用哪种类型的神经网络，都有一些常见的术语是我们应该知道的。以下是一些常见的术语：
- en: '**Nodes**, **perceptrons**, or **neurons**: These interchangeable terms refer
    to the basic building blocks of a neural network. Each node or neuron takes in
    input data and performs an operation on this data. After performing the operation,
    the node/neuron may or may not pass the results of the operation on to other nodes/neurons.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**、**感知器**或**神经元**：这些可以互换使用的术语指的是神经网络的基本构建块。每个节点或神经元都会接收输入数据并对这些数据进行操作。在执行操作后，节点/神经元可能会也可能不会将操作的结果传递给其他节点/神经元。'
- en: '**Activation**: The output or values associated with the operation of a node.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活**：与节点操作相关的输出或值。'
- en: '**Activation function**: The definition of the function that transforms the
    inputs to a node into the output, or activation.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：将输入转换为节点输出或激活的函数的定义。'
- en: '**Weights** or **biases**: These values define the relationships between input
    and output data in the activation function.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**或**偏差**：这些值定义了激活函数中输入和输出数据之间的关系。'
- en: '**Input layer**: The input layer of a neural network includes a series of nodes
    that take the initial input into the neural network model (a series of features
    or attributes, for example).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：神经网络中的输入层包括一系列节点，这些节点将初始输入传递到神经网络模型中（例如一系列特征或属性）。'
- en: '**Output layer**: The output layer of a neural network includes a series of
    nodes that take information passed inside the neural network and transform it
    into a final output.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：神经网络中的输出层包括一系列节点，这些节点接收神经网络内部传递的信息，并将其转换为最终输出。'
- en: '**Hidden layers**: These layers of a neural network exist between the input
    and output layers and are thus **hidden** from outside inputs or outputs.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：这些层位于输入层和输出层之间，因此对外部输入或输出来说是隐藏的。'
- en: '**Feedforward** or **feed forward**: This refers to a scenario in which data
    is fed into the input layer of a neural network and transferred in a forward direction
    to the output layer (without any cycles).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向传播**或**前向传播**：这指的是数据被输入到神经网络的输入层，并向前传递到输出层（没有循环）的情况。'
- en: '**Backpropagation:** This is a method for training neural network models that
    involves feeding forward values through the network, calculating generated errors,
    and then transferring changes based on these errors back into the network.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：这是一种训练神经网络模型的方法，涉及通过网络传递正向值，计算生成的错误，然后根据这些错误将更改传递回网络。'
- en: '**Architecture**: The overall structure of how neurons are connected together
    in the neural network is called an **architecture**.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：神经网络中神经元相互连接的整体结构称为架构。'
- en: 'To solidify these concepts, consider the following schematic of a neural network:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固这些概念，考虑以下神经网络示意图：
- en: '![](img/f2dedab7-b035-4fcf-b7c9-fdbec00e31de.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f2dedab7-b035-4fcf-b7c9-fdbec00e31de.png)'
- en: This is a basic feedforward (that is, acyclic or recursive) neural network.
    It has two hidden layers, accepts two inputs, and outputs two class values (or
    results).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的正向传播（即，无环或有回溯）神经网络。它有两个隐藏层，接受两个输入，并输出两个类别值（或结果）。
- en: Don't worry if all this jargon seems a little bit overwhelming right now. We
    will be looking at a concrete example next that should solidify all of these pieces.
    Also, if you get down in the weeds with the various examples in this chapter and
    get confused with terminology, circle back here as a reminder.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在所有这些术语看起来有点令人不知所措，请不要担心。我们将在下一个具体示例中查看，这将巩固所有这些内容。此外，如果你在本章的各种示例中陷入困境并因术语而感到困惑，请回到这里作为提醒。
- en: Building a simple neural network
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建简单的神经网络
- en: Many neural network packages and applications of neural networks treat the models
    as **black boxes**. That is, people tend to utilize some framework that allows
    them to quickly build a neural network using a bunch of default values and automation.
    They are often able to produce some results, but this sort of convenience usually
    does not build much intuition about how the models actually work. As a result,
    when the models do not behave as expected, it's very hard to understand why they
    might be making weird predictions or having trouble converging.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多神经网络包和神经网络的应用将模型视为**黑盒**。也就是说，人们倾向于使用一些框架，允许他们快速使用一些默认值和自动化构建神经网络。他们通常能够产生一些结果，但这种便利通常不会对模型实际工作方式产生太多直观理解。因此，当模型的行为不符合预期时，很难理解它们为什么可能会做出奇怪的预测或遇到收敛困难。
- en: Before jumping into more complicated neural networks, let's build up some basic
    intuition about neural networks such that we do not fall into this pattern. We
    are going to build a simple neural network from scratch to learn about the basic
    components of neural networks and how they operate together.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究更复杂的神经网络之前，让我们建立一些关于神经网络的基本直觉，这样我们就不至于陷入这种模式。我们将从头开始构建一个简单的神经网络，以了解神经网络的基本组件以及它们是如何协同工作的。
- en: '**Note**: that even though we are going to build our neural net from scratch
    here (which you may also want to do in certain scenarios), there are a variety
    of Go packages that help you build, train, and make predictions with neural networks.
    These include `github.com/tleyden/neurgo`, `github.com/fxsjy/gonn`, `github.com/NOX73/go-neural`,
    `github.com/milosgajdos83/gosom`, `github.com/made2591/go-perceptron-go`, and
    `github.com/chewxy/gorgonia`.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：尽管我们在这里将从零开始构建我们的神经网络（你可能在某些情况下也想这样做），但存在各种 Go 包可以帮助你构建、训练和预测神经网络。这些包括
    `github.com/tleyden/neurgo`、`github.com/fxsjy/gonn`、`github.com/NOX73/go-neural`、`github.com/milosgajdos83/gosom`、`github.com/made2591/go-perceptron-go`
    和 `github.com/chewxy/gorgonia`。'
- en: 'We are going to utilize neural networks for classification in this chapter.
    However, neural nets can also be utilized to perform regression. You can learn
    more about that topic here: [https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/](https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中利用神经网络进行分类。然而，神经网络也可以用于执行回归。你可以在这里了解更多关于该主题的信息：[https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/](https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/)。
- en: Nodes in the network
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络中的节点
- en: 'The nodes, or neurons, of our neural network have a relatively simple functionality
    in and of themselves. Each neuron will take in one or more values (*x[1]*, *x[2]*,
    and so on), combine these values according to an activation function, and produce
    a single output. The following is the output pictured:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们神经网络中的节点或神经元本身具有相对简单的功能。每个神经元将接受一个或多个值（*x[1]*、*x[2]* 等等），根据激活函数将这些值组合，并产生一个输出。以下是一个输出示例：
- en: '![](img/bfb6068c-26a3-4800-8b3a-7bdbf9dd764d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bfb6068c-26a3-4800-8b3a-7bdbf9dd764d.png)'
- en: 'How exactly should we combine the inputs to get the output? Well, we need a
    method to combine the inputs that is adjustable (such that we can train the model),
    and we have already seen that combining variables using coefficients and an intercept
    is one trainable way to combine inputs. Just think back to [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression*. In this spirit, we are going to combine the inputs linearly with
    some coefficients (**weights**) and an intercept (the **bias**):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何组合输入以获得输出？嗯，我们需要一个可调整的输入组合方法（这样我们就可以训练模型），我们已经看到使用系数和截距组合变量是一种可训练的输入组合方法。只需回想一下第
    [4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml) 章 *回归*。本着这种精神，我们将使用一些系数（**权重**）和截距（**偏差**）线性组合输入：
- en: '![](img/3feefece-dc75-4015-a1f8-e222859fdbac.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3feefece-dc75-4015-a1f8-e222859fdbac.jpg)'
- en: Here, *w[1]*, *w[2]*, and so on are our weights and *b* is the bias.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w[1]*、*w[2]* 等等是我们的权重，*b* 是偏差。
- en: 'This combination of inputs is a good start, but it is linear at the end of
    the day, and thus not able to model non-linear relationships between the input
    and output. To introduce some non-linearity, we are going to apply an activation
    function to this linear combination of inputs. The activation function that we
    will use here is similar to the logistic function that was introduced in [Chapter
    5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*. In the context
    of neural networks and in the following form, the logistic function is referred
    to as the **sigmoid** **function**:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种输入组合是一个良好的开始，但最终它是线性的，因此无法对输入和输出之间的非线性关系进行建模。为了引入一些非线性，我们将对这个输入的线性组合应用一个激活函数。我们将使用的激活函数与第
    [5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml) 章 *分类* 中引入的对数函数类似。在神经网络和以下形式中，对数函数被称为
    **sigmoid** **函数**：
- en: '![](img/ad4415d6-4781-4d70-b883-b83fecd57f96.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad4415d6-4781-4d70-b883-b83fecd57f96.jpg)'
- en: The `sigmoid` function is a good choice for use in our node because it introduces
    non-linearity, but it also has a limited range (between *0* and *1*), has a simply
    defined derivative (which we will use during training of the network), and it
    can have a probabilistic interpretation (as discussed further in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 函数是我们节点中使用的良好选择，因为它引入了非线性，但它也有一个有限的范围（在 *0* 和 *1* 之间），有一个简单定义的导数（我们将在网络的训练中使用它），并且它可以有概率解释（如第
    [5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml) 章 *分类* 中进一步讨论）。'
- en: The `sigmoid` function is by no means the only choice for an activation function
    in a neural network. Other popular choices include the hyperbolic tangent function,
    softmax, and rectified linear units. Choices of activation functions along with
    their advantages and disadvantages are further discussed at [https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f](https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 函数绝不是神经网络中激活函数的唯一选择。其他流行的选择包括双曲正切函数、softmax 和修正线性单元。激活函数的选择及其优缺点将在
    [https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f](https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f)
    中进一步讨论。'
- en: 'Let''s go ahead and define our activation function in Go along with its derivative.
    These definitions are shown in the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续在 Go 中定义我们的激活函数及其导数。这些定义在以下代码中显示：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Network architecture
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络架构
- en: 'The simple neural network that we are going to build will contain an input
    and output layer (as does any neural network). The network will include a single
    hidden layer between the input and output layer. This architecture is depicted
    as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的简单神经网络将包含一个输入层和一个输出层（任何神经网络都是如此）。网络将在输入层和输出层之间包含一个单一的隐藏层。这种架构如下所示：
- en: '![](img/a0b2fc3d-f452-4196-83b0-a162870f3293.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0b2fc3d-f452-4196-83b0-a162870f3293.png)'
- en: In particular, we are going to include four nodes in the input layer, three
    nodes in the hidden layer, and three nodes in the output layer. The four nodes
    in the input layer correspond to the number of attributes that we are going to
    feed into the network. Think about these four inputs as something like the four
    measurements we used to classify iris flowers in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*. The output layer will have three nodes because we will be setting
    up our network to make classifications for the iris flowers, which could be in
    one of three classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将在输入层中包含四个节点，在隐藏层中包含三个节点，在输出层中包含三个节点。输入层中的四个节点对应于我们将要输入网络中的属性数量。想想这四个输入就像我们在第
    5 章 [f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml]，*分类* 中用来分类鸢尾花所使用的四个测量值。输出层将有三节点，因为我们将会设置网络对鸢尾花进行分类，这些分类可能属于三个类别之一。
- en: Now, regarding the hidden layer--why are we using one hidden layer with three
    nodes? Well, one hidden layer is sufficient for a very large majority of simple
    tasks. If you have a large amount of non-linearity in your data, many inputs,
    and/or large amounts of training data, you may need to introduce more hidden layers
    (as further discussed later in this chapter in relation to deep learning). The
    choice of three nodes in the hidden layer can be tuned based on an evaluation
    metric and a little trial and error. You can also search the numbers of nodes
    in the hidden layer to automate your choice.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于隐藏层——为什么我们使用一个有三个节点的隐藏层？嗯，一个隐藏层对于绝大多数简单任务来说是足够的。如果你有大量的非线性数据、许多输入和/或大量的训练数据，你可能需要引入更多的隐藏层（如本章后面在讨论深度学习时进一步讨论的那样）。隐藏层中三个节点数量的选择可以根据评估指标和一点试错来调整。你也可以搜索隐藏层中节点的数量来自动化你的选择。
- en: Keep in mind that the more nodes you introduce into the hidden layer, the more
    perfectly your network can learn your training set. In other words, you are putting
    yourself in danger of overfitting your model such that it does not generalize.
    When adding this complication, it is important to validate your model very carefully
    to ensure that it generalizes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，你向隐藏层引入的节点越多，你的网络就能越完美地学习你的训练集。换句话说，你正在使你的模型面临过拟合的风险，以至于它无法泛化。在添加这种复杂性时，非常重要的一点是要非常仔细地验证你的模型，以确保它能够泛化。
- en: Why do we expect this architecture to work?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么期望这种架构能起作用？
- en: Let's pause for a moment and think about why setting up a series of nodes in
    the preceding arrangement would allow us to predict something. As you can see,
    all we are doing is combining inputs over and over to produce some result. How
    can we expect this result to be useful in making binary classifications?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，思考一下为什么按照前面的安排设置一系列节点可以让我们预测某些东西。正如你所看到的，我们一直在做的就是反复组合输入以产生某种结果。我们怎么能期望这个结果在做出二元分类时是有用的呢？
- en: Well, what exactly do we mean when we define a binary classification problem,
    or any classification or regression problem, for that matter? We are basically
    saying that we expect there to be some rules or relationships that exist between
    a series of inputs (our attributes) and an output (our labels or response). In
    essence, we are saying that there exists some simple or complicated function that
    is able to transform our attributes into the response or labels that we are after.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，当我们定义二元分类问题，或者任何分类或回归问题时，我们究竟意味着什么？我们基本上是在说，我们期望在一系列输入（我们的属性）和输出（我们的标签或响应）之间存在某些规则或关系。本质上，我们是在说，存在一些简单或复杂的函数能够将我们的属性转换为我们想要的响应或标签。
- en: We could make a choice for a type of function that might model this transformation
    of attributes to output, as we do in linear or logistic regression. However, we
    could also set up a series of chained and adjustable functions that we could algorithmically
    train to detect the relationships and rules between our inputs and output. This
    is what we are doing with the neural network!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择一种可能模拟属性到输出转换的函数类型，就像我们在线性或逻辑回归中所做的那样。然而，我们也可以设置一系列连锁的可调整函数，我们可以通过算法训练来检测输入和输出之间的关系和规则。这正是我们在神经网络中所做的！
- en: The nodes of the neural network are like sub-functions that are adjusted until
    they mimic the rules and relationships between our supplied inputs and the output,
    regardless of what those rules and relationships actually are (linear, non-linear,
    dynamic, and so on). If underlying rules exist between the inputs and outputs,
    it is likely that there exists some neural network that can mimic the rules.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的节点就像是被调整的子函数，直到它们模仿我们提供的输入和输出之间的规则和关系，无论这些规则和关系实际上是什么（线性、非线性、动态等）。如果输入和输出之间存在底层规则，那么很可能存在一些可以模仿这些规则的神经网络。
- en: Training our neural network
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的神经网络
- en: Okay, so now we have some good motivation for why this combination of nodes
    might help us make predictions. How are we actually going to adjust all of the
    sub-functions of our neural network nodes based on some input data? The answer
    is called **backpropagation**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在我们有一些很好的理由说明为什么这种节点组合可能有助于我们进行预测。我们究竟如何根据一些输入数据调整我们神经网络节点的所有子函数呢？答案就是**反向传播**。
- en: 'Backpropagation is a method for training our neural network that involves doing
    the following iteratively over a series of **epochs** (or exposure to our training
    dataset):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种训练我们神经网络的方法，它涉及在一系列**时代**（或对训练数据集的暴露）上迭代执行以下操作：
- en: Feeding our training data forward through the neural network to calculate an
    output
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的训练数据正向通过神经网络来计算输出
- en: Calculating errors in the output
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算输出中的错误
- en: Using gradient descent (or other relevant method) to determine how we should
    change our weights and biases based on the errors
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度下降（或其他相关方法）来确定我们应该如何根据错误来改变我们的权重和偏差
- en: Backpropagating these weight/bias changes into the network
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些权重/偏差变化反向传播到网络中
- en: We will implement this process later and go through some of the details. However,
    certain things like gradient descent are covered in more detail in the [Appendix](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml),
    *Algorithms/Techniques Related to Machine Learning*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后实施此过程，并探讨一些细节。然而，像梯度下降这样的内容在[附录](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml)“与机器学习相关的算法/技术”中有更详细的介绍。
- en: This backpropagation method of training a neural network is ubiquitous in the
    modeling world, but there are tons of unique network architectures and methods
    that we do not have space to cover here. Neural nets are an active area of research
    in academia and industry.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练神经网络的反向传播方法在建模世界中无处不在，但这里没有足够的空间来涵盖大量的独特网络架构和方法。神经网络是学术界和工业界的一个活跃的研究领域。
- en: 'First, let''s define a struct `neuralNetConfig` that will contain all of the
    parameters that define our network architecture and how we will go about our backpropagation
    iterations. Let''s also define a struct `neuralNet` that will contain all of the
    information that defines a trained neural network. Later on, we will utilize trained
    `neuralNet` values to make predictions. These definitions are shown in the following
    code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个`neuralNetConfig`结构，它将包含定义我们网络架构以及我们如何进行反向传播迭代的参数。同时，我们也定义一个`neuralNet`结构，它将包含定义一个训练好的神经网络的全部信息。稍后，我们将利用训练好的`neuralNet`值来进行预测。这些定义在以下代码中展示：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `wHidden` and `bHidden` are the weights and biases for the hidden layer
    of the network and `wOut` and `bOut` are the weights and biases for the output
    layer of the network, respectively. Note that we are using `gonum.org/v1/gonum/mat`
    matrices for all the weights and biases, and we will use similar matrices to represent
    our inputs and outputs. This will let us easily perform the operations related
    to the backpropagation and generalize our training to any number of nodes in the
    input, hidden, and output layers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`wHidden`和`bHidden`是网络隐藏层的权重和偏差，而`wOut`和`bOut`分别是网络输出层的权重和偏差。请注意，我们使用`gonum.org/v1/gonum/mat`矩阵来表示所有的权重和偏差，我们还将使用类似的矩阵来表示我们的输入和输出。这将使我们能够轻松执行与反向传播相关的操作，并将我们的训练推广到输入、隐藏和输出层中的任何数量的节点。
- en: 'Next, let''s define a function that initializes a new neural network based
    on a `neuralNetConfig` value and a method that trains a `neuralNet` value based
    on a matrix of inputs (*x*) and a matrix of labels (*y*):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个函数，它根据`neuralNetConfig`值初始化一个新的神经网络，以及一个基于输入矩阵（*x*）和标签矩阵（*y*）的训练`neuralNet`值的函数：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the `train()` method, we need to complete our backpropagation method and
    place the resulting trained weights and biases into the receiver. First, let''s
    initialize the weights and biases randomly in the `train()` method, as shown in
    the following code:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train()`方法中，我们需要完成我们的反向传播方法，并将生成的训练好的权重和偏差放入接收器中。首先，让我们在`train()`方法中随机初始化权重和偏差，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we need to loop over each of our epochs completing the backpropagation
    of the network. This again involves a feed forward stage in which outputs are
    calculated and a backpropagating stage in which changes to the weights and biases
    are applied. The backpropagation process will be discussed in more detail in the
    [Appendix](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml), *Algorithms/Techniques
    Related to Machine Learning*, if you are interested in diving a little deeper.
    For now, let''s focus on the implementation. The loop over epochs look like the
    following, with the various pieces of the backpropagation process indicated by
    comments:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要遍历我们的每个epoch，完成网络的反向传播。这又涉及到一个前向阶段，其中计算输出，以及一个反向传播阶段，其中应用权重和偏差的变化。如果您想深入了解，反向传播过程将在[附录](718ca26d-465a-47c9-91b9-14e749be0c30.xhtml)“与机器学习相关的算法/技术”中详细讨论。现在，让我们专注于实现。epoch的循环如下，其中通过注释指出了反向传播过程的各个部分：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the feed forward section of this loop, the inputs are propagated forward
    through our network of nodes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环的前向部分，输入通过我们的节点网络向前传播：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, once we have an output from the feed forward process, we go backward
    through the network calculating deltas (or changes) for the output and hidden
    layers, as can be seen in the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦我们从前向过程中得到输出，我们就通过网络向后计算输出层和隐藏层的delta（或变化），如下面的代码所示：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The deltas are then utilized to update the weights and biases of our network.
    A **learning rate** is also used to scale these changes, which can help the algorithm
    converge. This final piece of the backprogation loop is implemented here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用这些delta值来更新我们网络的权重和偏差。同时，使用**学习率**来缩放这些变化，这有助于算法收敛。反向传播循环的这一最后部分在此实现：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note, here we have utilized another helper function that allows us to sum matrices
    along one dimension while keeping the other dimension intact. This `sumAlongAxis()`
    function is shown in the following code for completeness:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里我们使用了另一个辅助函数，它允许我们在保持另一个维度完整的同时，对矩阵沿一个维度求和。这个`sumAlongAxis()`函数如下所示，以示完整：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The last thing that we will do in the `train()` method is add the trained weights
    and biases to the receiver value and return:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train()`方法中，我们最后要做的就是将训练好的权重和偏差添加到接收器值中并返回：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Awesome! That wasn''t so bad. We have a method to train our neural network
    in about 100 lines of Go code. To check whether this code runs and to get a sense
    about what our weights and biases might look like, let''s train a neural network
    on some simple dummy data. We will perform a more realistic example with the network
    in the next section, but for now, let''s just sanity check ourselves with some
    dummy data, as shown in the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！这并不那么糟糕。我们已经有了一种在约100行Go代码中训练我们的神经网络的方法。为了检查代码是否运行，以及了解我们的权重和偏差可能的样子，让我们在一些简单的虚拟数据上训练一个神经网络。我们将在下一节中用更真实的例子来展示网络，但现在，让我们用一些虚拟数据来对自己进行合理性检查，如下面的代码所示：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Compiling and running this neural network training code results in the following
    output weights and biases:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行此神经网络训练代码将产生以下输出权重和偏差：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: These weights and biases fully define our trained neural network. That is, `wHidden`
    and `bHidden` allow us to translate our inputs into values that are input to the
    output layer of our network, and `wOut` and `bOut` allow us to translate those
    values into the final output of our neural net.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重和偏差完全定义了我们的训练好的神经网络。也就是说，`wHidden`和`bHidden`使我们能够将输入转换为网络输出层的输入值，而`wOut`和`bOut`使我们能够将这些值转换为神经网络最终的输出。
- en: Because of our use of random numbers, your program may return slightly different
    results from those which are described earlier. However, you should see a similar
    range of values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了随机数，你的程序可能返回的结果与之前描述的不同。然而，你应该看到类似的价值范围。
- en: Utilizing the simple neural network
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用简单的神经网络
- en: Now that we have some neural network training functionality that appears to
    be working, let's try to utilize this functionality in a more realistic modeling
    scenario. In particular, let's bring back our favorite classification dataset,
    the iris flower dataset (utilized in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一些看起来似乎可以正常工作的神经网络训练功能，让我们尝试在更实际的建模场景中利用这些功能。特别是，让我们恢复我们最喜欢的分类数据集，鸢尾花数据集（在第5章[分类](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)中使用）。
- en: If you remember, when trying to classify iris flowers using this dataset, we
    are trying to classify them into one of three species (setosa, virginica, or versicolor).
    As our neural net is expecting matrices of float values, we need to encode the
    three species into numerical columns. One way to do this is to create a column
    in our dataset for each species. We will then set that column's values to either
    *1.0* or *0.0* depending on whether the corresponding row's measurements correspond
    to that species (*1.0*) or to another species (*0.0*).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，当尝试使用此数据集对鸢尾花进行分类时，我们试图将它们分类为三种物种之一（setosa、virginica或versicolor）。由于我们的神经网络期望的是浮点值矩阵，我们需要将三种物种编码为数值列。一种方法是为每种物种在数据集中创建一个列。然后，我们将该列的值设置为*1.0*或*0.0*，具体取决于对应行的测量值是否对应于该物种（*1.0*）或另一个物种（*0.0*）。
- en: 'We are also going to standardize our data for reasons similar to those that
    were discussed in reference to logistic regression in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml),
    *Classification*. This makes our dataset look slightly different than it did for
    previous examples, as you can see here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将标准化我们的数据，原因与在[第5章](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)“分类”中讨论的类似逻辑回归。这使得我们的数据集看起来与之前的示例略有不同，如下所示：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will use an 80/20 training and test split of our data to evaluate the model.
    The training data will be stored in `train.csv` and the testing data will be stored
    in `test.csv`, as shown here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用80/20的训练和测试数据分割来评估模型。训练数据将存储在`train.csv`中，测试数据将存储在`test.csv`中，如下所示：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Training the neural network on real data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在真实数据上训练神经网络
- en: 'To train our neural network on this iris data, we need to read in the training
    data and create two matrices. The first matrix will hold all of the attributes
    (matrix `inputs`), and the second matrix will hold all the labels (matrix `labels`).
    We are going to construct these matrices as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这个鸢尾花数据上训练我们的神经网络，我们需要读取训练数据并创建两个矩阵。第一个矩阵将包含所有属性（矩阵`inputs`），第二个矩阵将包含所有标签（矩阵`labels`）。我们将按照以下方式构建这些矩阵：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then initialize and train our neural network, similar to how we did
    with the dummy data. The only difference is that we are going to use three output
    neurons corresponding to our three output classes. The training process is as
    follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以初始化并训练我们的神经网络，类似于我们处理虚拟数据的方式。唯一的区别是我们将使用三个输出神经元，对应于我们的三个输出类别。训练过程如下：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Evaluating the neural network
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估神经网络
- en: 'To make predictions with the trained neural network, we will create a `predict()`
    method for the `neuralNet` type. This `predict` method will take in new inputs
    and complete and feed the new inputs forward through the network to produce predicted
    outputs, as shown here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用训练好的神经网络进行预测，我们将为`neuralNet`类型创建一个`predict()`方法。这个`predict()`方法将接受新的输入，并将新的输入通过网络前向传播以产生预测输出，如下所示：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can then use this `predict()` method on the testing data to evaluate our
    trained model. Specifically, we will read in the data in `test.csv` into two new
    matrices `testInputs` and `testLabels`, similar to how we read in `train.csv`
    (as such, we will not include the details here). `testInputs` can be supplied
    to the `predict()` method to get our predictions and then we can compare our predictions
    and `testLabels` to calculate an evaluation metric. In this case, we will calculate
    the accuracy of our model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个`predict()`方法在测试数据上评估我们的训练模型。具体来说，我们将读取`test.csv`文件到两个新的矩阵`testInputs`和`testLabels`中，类似于我们读取`train.csv`的方式（因此，这里将不包含细节）。`testInputs`可以提供给`predict()`方法以获取我们的预测，然后我们可以将我们的预测与`testLabels`进行比较，以计算评估指标。在这种情况下，我们将计算我们模型的准确率。
- en: One detail that we will take care of as we calculate the accuracy is determining
    a single output prediction from our model. The network actually produces one output
    between 0.0 and 1.0 for each of the iris species. We will take the highest of
    these as the predicted species.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算准确率时，我们将注意的一个细节是从我们的模型中确定单个输出预测。网络实际上为每种鸢尾花物种产生一个介于0.0和1.0之间的输出。我们将取这些输出中的最高值作为预测物种。
- en: 'The calculation of accuracy for our model is shown in the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型准确率的计算如下代码所示：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Compiling and running the evaluation yields something similar to the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 编译和运行评估结果类似于以下内容：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Yeah! That's pretty good. 97% accuracy in our predictions for the iris flower
    species with our from-scratch neural network. As mentioned earlier, these single
    hidden layer neural networks are pretty powerful when it comes to a majority of
    classification tasks, and you can see that the underlying principles are not really
    that complicated.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！这相当不错。我们使用从头开始的神经网络对鸢尾花物种的预测准确率达到97%。如前所述，这些单隐藏层神经网络在大多数分类任务中都非常强大，你可以看到其基本原理并不真的那么复杂。
- en: Introducing deep learning
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍深度学习
- en: Although simple neural networks, like the one utilized in the preceding section,
    are extremely powerful for many scenarios, deep neural network architectures have
    been applied across industries in recent years on various types of data. These
    more complicated architectures have been used to beat champions at board/video
    games, drive cars, generate art, transform images, and much more. It almost seems
    like you can throw anything at these models and they will do something interesting,
    but they seem to be particularly well-suited for computer vision, speech recognition,
    textual inference, and other very complicated and hard-to-define tasks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像前述章节中使用的简单神经网络在许多场景中非常强大，但近年来深度神经网络架构已经在各个行业中应用于各种类型的数据。这些更复杂的架构已被用于在棋类/视频游戏中击败冠军、驾驶汽车、生成艺术、转换图像等等。几乎感觉你可以把这些模型扔给任何东西，它们都会做一些有趣的事情，但它们似乎特别适合计算机视觉、语音识别、文本推理和其他非常复杂且难以定义的任务。
- en: We are going to introduce deep learning here and run a deep learning model in
    Go. However, the application and diversity of deep learning models is huge and
    growing every day.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里介绍深度学习，并在Go中运行一个深度学习模型。然而，深度学习模型的用途和多样性非常巨大，并且每天都在增长。
- en: 'There are many books and tutorials on the subject, so if this brief introduction
    sparks your interest, we recommend that you look into one of the following resources:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题有许多书籍和教程，所以如果这个简短的介绍激发了你的兴趣，我们建议你查看以下资源之一：
- en: 'Hands-On Deep Learning with TensorFlow: [https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行动手深度学习：[https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow)
- en: 'Deep Learning by Example: [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习实例：[https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example)
- en: 'Deep Learning for Computer Vision: [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习计算机视觉：[https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision)
- en: These resources might not all mention or acknowledge Go but, as you will see
    in the following example, Go is perfectly capable of applying these techniques
    and interfacing with things like TensorFlow or H2O.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些资源可能并不都会提到或承认Go语言，但正如你将在下面的示例中看到的，Go语言完全能够应用这些技术并与TensorFlow或H2O等工具进行接口交互。
- en: What is a deep learning model?
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型是什么？
- en: The deep in deep learning refers to the successive combination and utilization
    of various neural network structures to form an architecture that is large and
    complicated. These large and complicated architectures generally require large
    amounts of data to train, and the resulting structure is very hard to interpret.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的“深度”指的是连续组合和利用各种神经网络结构，形成一个庞大且复杂的架构。这些庞大且复杂的架构通常需要大量的数据进行训练，并且生成的结构非常难以解释。
- en: 'To give an example of the scale and complexity of modern deep learning models,
    take Google''s LeNet ([http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf))
    as an example. This model, which can be utilized for object recognition, is depicted
    here (image courtesy of [https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明现代深度学习模型的规模和复杂性，以谷歌的LeNet([http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf))为例。这个可用于物体识别的模型在此处展示（图片由[https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)提供）：
- en: '![](img/9b4148b1-6241-4279-8d3e-9190f72f1e2e.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b4148b1-6241-4279-8d3e-9190f72f1e2e.png)'
- en: This structure is hard to digest in and of itself. However, it is made even
    more impressive when you realize that each of the blocks in this diagram can be
    a complicated, neural network primitive in and of itself (as shown in the legend).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结构本身就很复杂。然而，当你意识到这个图中的每个块本身都可以是一个复杂的神经网络原语（如图例所示）时，它变得更加令人印象深刻。
- en: 'Generally, deep learning models are constructed by chaining together and, in
    some cases, rechaining together, one or more of the following structures:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，深度学习模型是通过将以下结构之一或多个链接在一起，并在某些情况下重新链接在一起来构建的：
- en: 'A fully connected neural network architecture: This building block might be
    something similar to what we built previously in this chapter for iris flower
    detection. It includes a number of neural network nodes arranged in fully connected
    layers. That is, the layers flow from one to the other and all the nodes are connected
    from one layer to the next.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接的神经网络架构：这个构建块可能类似于我们在本章之前为鸢尾花检测构建的。它包括一系列按完全连接层排列的神经网络节点。也就是说，层从一层流向另一层，所有节点都从一层连接到下一层。
- en: 'A **convolutional neural network** (**ConvNet** or **CNN**): This is a neural
    network that implements at least one **convolutional layer**. A convolutional
    layer contains neurons parameterized by a set of weights (also called a convolutional
    kernel or filter) that is only partially connected to the input data. Most ConvNets
    include a hierarchical combination of these convolutional layers allows a ConvNet
    to respond to low level features in the input data.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**ConvNet**或**CNN**）：这是一种至少实现了一个**卷积层**的神经网络。卷积层包含由一组权重（也称为卷积核或过滤器）参数化的神经元，这些神经元仅部分连接到输入数据。大多数ConvNet包括这些卷积层的分层组合，这使得ConvNet能够对输入数据中的低级特征做出响应。'
- en: 'A **recurrent neural network** and/or **Long Short-Term Memory** (**LSTM**)
    **cells**: These components attempt to factor in something like memory into the
    model. Recurrent neural nets make their output dependent on a sequence of inputs
    rather than assuming that every input is independent. LSTM cells are related to
    recurrent neural networks but also try to factor in a longer term sort of memory.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）和/或**长短期记忆**（**LSTM**）单元：这些组件试图将某种形式的记忆因素纳入模型。循环神经网络使它们的输出依赖于一系列输入，而不是假设每个输入都是独立的。LSTM单元与循环神经网络相关，但还试图纳入一种更长期的记忆。'
- en: 'Although we will utilize image data to illustrate deep learning in this chapter,
    deep learning models using recurrent neural nets and LSTMs are particularly well-adapted
    at solving text-based and other NLP problems. The memory that is built into these
    networks allows them to understand what words might come next in a sentence, for
    example. If you are interested in running a deep learning model that is based
    on text data, we recommend looking at this example that uses the `github.com/chewxy/gorgonia`
    package:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将在本章中使用图像数据来展示深度学习，但使用循环神经网络（RNN）和长短期记忆网络（LSTM）的深度学习模型特别适合解决基于文本和其他自然语言处理（NLP）问题。这些网络内置的内存使它们能够理解句子中可能出现的下一个单词，例如。如果你对运行基于文本数据的深度学习模型感兴趣，我们建议查看使用
    `github.com/chewxy/gorgonia` 包的此示例：
- en: '[https://github.com/chewxy/gorgonia/tree/master/examples/charRNN.](https://github.com/chewxy/gorgonia/tree/master/examples/charRNN)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/chewxy/gorgonia/tree/master/examples/charRNN.](https://github.com/chewxy/gorgonia/tree/master/examples/charRNN)'
- en: 'Deep learning models are powerful! Especially for tasks like computer vision.
    However, you should also keep in mind that complicated combinations of these neural
    net components are also extremely hard to interpret. That is, determining why
    the model made a certain prediction can be near impossible. This can be a problem
    when you need to maintain compliance in certain industries and jurisdictions,
    and it also might inhibit debugging or maintenance of your applications. That
    being said, there are some major efforts to improve the interpretability of deep
    learning models. Notable among these efforts is the LIME project:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型非常强大！特别是对于计算机视觉等任务。然而，你也应该记住，这些神经网络组件的复杂组合也非常难以解释。也就是说，确定模型做出特定预测的原因可能几乎是不可能的。这在需要维护某些行业和司法管辖区合规性的情况下可能是一个问题，也可能阻碍应用程序的调试或维护。尽管如此，有一些主要努力旨在提高深度学习模型的解释性。在这些努力中，LIME
    项目是值得注意的：
- en: '[https://github.com/marcotcr/lime.](https://github.com/marcotcr/lime)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/marcotcr/lime.](https://github.com/marcotcr/lime)'
- en: Deep learning with Go
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Go 进行深度学习
- en: 'There are a variety of options when you are looking to build or utilize deep
    learning models from Go. This, as with deep learning itself, is an ever changing
    landscape. However, the options for building, training and utilizing deep learning
    models in Go are generally as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要从 Go 语言构建或使用深度学习模型时，有多种选择。这与深度学习本身一样，是一个不断变化的领域。然而，在 Go 语言中构建、训练和利用深度学习模型的选择通常如下：
- en: '**Use a Go package**: There are Go packages that allow you to use Go as your
    main interface to build and train deep learning models. The most features and
    developed of these packages is `github.com/chewxy/gorgonia`. `github.com/chewxy/gorgonia`
    treats Go as a first-class citizen and is written in Go, even if it does make
    significant usage of `cgo` to interface with numerical libraries.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Go 包**：有一些 Go 包允许你使用 Go 作为构建和训练深度学习模型的主要接口。其中功能最全面且发展最成熟的包是 `github.com/chewxy/gorgonia`。`github.com/chewxy/gorgonia`
    将 Go 视为第一等公民，并且是用 Go 编写的，即使它确实使用了 `cgo` 来与数值库进行交互。'
- en: '**Use an API or Go client for a non-Go DL framework**: You can interface with
    popular deep learning services and frameworks from Go including TensorFlow, MachineBox,
    H2O, and the various cloud providers or third-party API offerings (such as IBM
    Watson). TensorFlow and Machine Box actually have Go bindings or SDKs, which are
    continually improving. For the other services, you may need to interact via REST
    or even call binaries using `exec`.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用非 Go 语言深度学习框架的 API 或 Go 客户端**：你可以从 Go 语言与流行的深度学习服务和框架进行交互，包括 TensorFlow、MachineBox、H2O
    以及各种云服务提供商或第三方 API 提供商（如 IBM Watson）。TensorFlow 和 Machine Box 实际上提供了 Go 绑定或 SDK，这些绑定或
    SDK 正在持续改进。对于其他服务，你可能需要通过 REST 或甚至使用 `exec` 调用二进制文件来进行交互。'
- en: '**Use cgo**: Of course, Go can talk to and integrate with C/C++ libraries for
    deep learning, including the TensorFlow libraries and various libraries from Intel.
    However, this is a difficult road, and it is only recommended when absolutely
    necessary.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 cgo**：当然，Go 可以与 C/C++ 库进行交互和集成，用于深度学习，包括 TensorFlow 库和来自 Intel 的各种库。然而，这是一条艰难的道路，只有在绝对必要时才推荐使用。'
- en: As TensorFlow is by far the most popular framework for deep learning (at the
    moment), we will briefly explore the second category listed here. However, the
    Tensorflow Go bindings are under active development and some functionality is
    quite crude at the moment. The TensorFlow team recommends that if you are going
    to use a TensorFlow model in Go, you first train and export this model using Python.
    That pre-trained model can then be utilized from Go, as we will demonstrate in
    the next section.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TensorFlow 目前是深度学习最受欢迎的框架，我们将简要探讨这里列出的第二类。然而，TensorFlow Go 绑定目前正处于积极开发中，一些功能相当原始。TensorFlow
    团队建议，如果你打算在 Go 中使用 TensorFlow 模型，你首先应该使用 Python 训练并导出此模型。然后，这个预训练模型可以从 Go 中使用，正如我们将在下一节中展示的那样。
- en: 'There are a number of members of the community working very hard to make Go
    more of a first-class citizen for TensorFlow. As such, it is likely that the rough
    edges of the TensorFlow bindings will be smoothed over the coming year. To keep
    updated on this front, make sure and join the #data-science channel on Gophers
    Slack ([https://invite.slack.golangbridge.org/](https://invite.slack.golangbridge.org/)).
    This is a frequent topic of conversation there and would be a great place to ask
    questions and get involved.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '社区中有许多成员正在努力使 Go 成为 TensorFlow 的首选公民。因此，TensorFlow 绑定的粗糙边缘可能会在未来一年中得到平滑处理。为了保持在这一方面的更新，请确保加入
    Gophers Slack 上的 #data-science 频道 ([https://invite.slack.golangbridge.org/](https://invite.slack.golangbridge.org/))。那里是经常讨论的话题，也是提问和参与的好地方。'
- en: Even though we will explore a quick TensorFlow example here, we highly recommend
    that you should look into `github.com/chewxy/gorgonia`, especially if you are
    considering doing more custom or extensive modeling in Go. This package is super
    powerful and is used in production.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这里将探索一个快速的 TensorFlow 示例，但我们强烈建议你应该查看 `github.com/chewxy/gorgonia`，特别是如果你考虑在
    Go 中进行更多自定义或广泛的建模。这个包非常强大，并且已在生产中使用。
- en: Setting up TensorFlow for use with Go
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 Go 设置 TensorFlow
- en: 'The TensorFlow team has provided some good docs to install TensorFlow and get
    it ready for usage with Go. These docs can be found at [https://www.tensorflow.org/install/install_go](https://www.tensorflow.org/install/install_go).
    There are a couple of preliminary steps, but once you have the TensorFlow C libraries
    installed, you can get the following Go package:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 团队提供了一些很好的文档来安装 TensorFlow 并使其准备好与 Go 一起使用。这些文档可以在 [https://www.tensorflow.org/install/install_go](https://www.tensorflow.org/install/install_go)
    找到。有一些初步步骤，但一旦你安装了 TensorFlow C 库，你就可以获取以下 Go 包：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Everything should be good to go if you were able to get `github.com/tensorflow/tensorflow/tensorflow/go`
    without error, but you can make sure that you are ready to use TensorFlow by executing
    the following tests:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能无错误地获取 `github.com/tensorflow/tensorflow/tensorflow/go`，那么一切应该都准备就绪了，但你可以通过执行以下测试来确保你已准备好使用
    TensorFlow：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Retrieving and calling a pretrained TensorFlow model
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取并调用预训练的 TensorFlow 模型
- en: 'The model that we are going to use is a Google model for object recognition
    in images called Inception. The model can be retrieved as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的模型是 Google 用于图像中物体识别的模型，称为 Inception。模型可以通过以下方式获取：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: After unzipping the compressed model, you should see a `*.pb` file. This is
    a `protobuf` file that represents a frozen state of the model. Think back to our
    simple neural network. The network was fully defined by a series of weights and
    biases. Although more complicated, this model can be defined in a similar way
    and these definitions are stored in this `protobuf` file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 解压缩压缩模型后，你应该看到一个 `*.pb` 文件。这是一个表示模型冻结状态的 `protobuf` 文件。回想一下我们简单的神经网络。网络完全由一系列权重和偏差定义。尽管更复杂，但这个模型可以用类似的方式定义，并且这些定义存储在这个
    `protobuf` 文件中。
- en: To call this model, we will use some example code from the TensorFlow Go bindings
    docs--[https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go).
    This code loads the model and uses the model to detect and label the contents
    of a `*.jpg` image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用此模型，我们将使用 TensorFlow Go 绑定文档中的示例代码--[https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)。此代码加载模型并使用模型检测和标记
    `*.jpg` 图像的内容。
- en: 'As the code is included in the TensorFlow docs, I will spare the details and
    just highlight a couple of snippets. To load the model, we perform the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码包含在 TensorFlow 文档中，我将省略细节，只突出几个片段。为了加载模型，我们执行以下操作：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then we load the graph definition of the deep learning model and create a new
    TensorFlow session with the graph, as shown in the following code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载深度学习模型的图定义，并使用该图创建一个新的 TensorFlow 会话，如下面的代码所示：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can make an inference using the model as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下方式使用模型进行推理：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Object detection using TensorFlow from Go
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 从 Go 进行目标检测
- en: 'The Go program for object detection, as specified in the TensorFlow GoDocs,
    can be called as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 根据TensorFlow GoDocs中指定的Go程序进行目标检测，可以按以下方式调用：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When the program is called, it will utilize the pretrained and loaded model
    to infer the contents of the specified image. It will then output the most likely
    contents of that image along with its calculated probability.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序被调用时，它将利用预训练和加载的模型来推断指定图像的内容。然后，它将输出该图像最可能的内容及其计算出的概率。
- en: 'To illustrate this, let''s try performing the object detection on the following
    image of an airplane, saved as `airplane.jpg`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们尝试在以下飞机图片上进行目标检测，保存为`airplane.jpg`：
- en: '![](img/b9a16e04-62b2-4c65-b75e-ce06878aa534.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9a16e04-62b2-4c65-b75e-ce06878aa534.png)'
- en: 'Running the TensorFlow model from Go gives the following results:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从Go运行TensorFlow模型得到以下结果：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After some suggestions about speeding up CPU computations, we get a result:
    `airliner`. Wow! That''s pretty cool. We just performed object recognition with
    TensorFlow right from our Go program!'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些关于加快CPU计算的建议之后，我们得到了一个结果：`airliner`。哇！这真是太酷了。我们刚刚在我们的Go程序中用TensorFlow进行了目标识别！
- en: 'Let try another one. This time, we will use `pug.jpg`, which looks like the
    following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次。这次，我们将使用`pug.jpg`，它看起来如下：
- en: '![](img/d3701af5-dc11-4561-9bad-135d546807e6.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3701af5-dc11-4561-9bad-135d546807e6.jpg)'
- en: 'Running our program again with this image gives the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用此图片运行我们的程序得到以下结果：
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Success! Not only did the model detect that there was a dog in the picture,
    it correctly identified that there was a pug dog in the picture.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！模型不仅检测到图片中有一只狗，而且正确地识别出图片中是一只斗牛犬。
- en: 'Let try just one more. As this is a Go book, we cannot resist trying `gopher.jpg`,
    which looks like the following (huge thanks to Renee French, the artist behind
    the Go gopher):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次。由于这是一本Go书，我们忍不住想试试`gopher.jpg`，它看起来如下（向Go gopher背后的艺术家Renee French表示巨大的感谢）：
- en: '![](img/fb3144d9-4aef-4af3-b3f4-cd22b41df16f.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb3144d9-4aef-4af3-b3f4-cd22b41df16f.jpg)'
- en: 'Running the model gives the following result:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 运行模型得到以下结果：
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Well, I guess we can't win them all. Looks like we need to refactor our model
    to be able to recognize Go gophers. More specifically, we should probably add
    a bunch of Go gophers to our training dataset, because a Go gopher is definitely
    not a safety pin!
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我想我们不可能每次都赢。看起来我们需要重构我们的模型，以便能够识别Go gopher。更具体地说，我们可能需要向我们的训练数据集中添加一些Go gopher，因为Go
    gopher绝对不是安全别针！
- en: References
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Basic neural networks:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 基本神经网络：
- en: 'A quick introduction to neural networks: [https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络快速入门：[https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)
- en: 'A friendly introduction to deep learning and neural networks: [https://youtu.be/BR9h47Jtqyw](https://youtu.be/BR9h47Jtqyw)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习和神经网络友好入门：[https://youtu.be/BR9h47Jtqyw](https://youtu.be/BR9h47Jtqyw)
- en: '`github.com/tleyden/neurgo` docs: [https://godoc.org/github.com/tleyden/neurgo](https://godoc.org/github.com/tleyden/neurgo)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/tleyden/neurgo` 文档：[https://godoc.org/github.com/tleyden/neurgo](https://godoc.org/github.com/tleyden/neurgo)'
- en: '`github.com/fxsjy/gonn/gonn` docs: [https://godoc.org/github.com/fxsjy/gonn/gonn](https://godoc.org/github.com/fxsjy/gonn/gonn)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/fxsjy/gonn/gonn` 文档：[https://godoc.org/github.com/fxsjy/gonn/gonn](https://godoc.org/github.com/fxsjy/gonn/gonn)'
- en: 'More detailed deep learning resources:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的深度学习资源：
- en: Hands-On Deep Learning with TensorFlow - [https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行实战深度学习 - [https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow](https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow)
- en: Deep Learning by Example - [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过实例学习深度学习 - [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example)
- en: Deep Learning for Computer Vision - [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉的深度学习 - [https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision](https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision)
- en: 'Deep learning with Go:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Go 进行深度学习：
- en: 'TensorFlow Go bindings docs: [https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Go 绑定文档：[https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go)
- en: '`github.com/chewxy/gorgonia` docs: [https://godoc.org/github.com/chewxy/gorgonia](https://godoc.org/github.com/chewxy/gorgonia)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/chewxy/gorgonia` 文档：[https://godoc.org/github.com/chewxy/gorgonia](https://godoc.org/github.com/chewxy/gorgonia)'
- en: 'MachineBox Go SDK docs: [https://godoc.org/github.com/machinebox/sdk-go](https://godoc.org/github.com/machinebox/sdk-go)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MachineBox Go SDK 文档：[https://godoc.org/github.com/machinebox/sdk-go](https://godoc.org/github.com/machinebox/sdk-go)
- en: 'Calling pretrained models using `cgo` example: [https://github.com/gopherdata/dlinfer](https://github.com/gopherdata/dlinfer)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `cgo` 调用预训练模型的示例：[https://github.com/gopherdata/dlinfer](https://github.com/gopherdata/dlinfer)
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations! We have gone from parsing data with Go to calling deep learning
    models from Go. You now know the basics of neural networks and can implement them
    and utilize them in your Go programs. In the next chapter, we will discuss how
    to get these models and applications off of your laptops and run them at production
    scale in data pipelines.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们已经从使用 Go 解析数据转变为从 Go 调用深度学习模型。你现在已经了解了神经网络的基础知识，并且可以在你的 Go 程序中实现和使用它们。在下一章中，我们将讨论如何将这些模型和应用从你的笔记本电脑上移除，并在数据管道中以生产规模运行。
