- en: 1\. Fundamentals
  id: totrans-0
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1\. 基础知识
- en: Overview
  id: totrans-1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: 'This chapter introduces you to supervised learning, using Anaconda to manage
    coding environments, and using Jupyter notebooks to create, manage, and run code.
    It also covers some of the most common Python packages used in supervised learning:
    pandas, NumPy, Matplotlib, and seaborn. By the end of this chapter, you will be
    able to install and load Python libraries into your development environment for
    use in analysis and machine learning problems. You will also be able to load an
    external data source using pandas, and use a variety of methods to search, filter,
    and compute descriptive statistics of the data. This chapter will enable you to
    gauge the potential impact of various issues such as missing data, class imbalance,
    and low sample size within the data source.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向你介绍监督学习，使用 Anaconda 管理编码环境，以及使用 Jupyter notebooks 创建、管理和运行代码。它还涵盖了一些在监督学习中最常用的
    Python 包：pandas、NumPy、Matplotlib 和 seaborn。本章结束时，你将能够安装并加载 Python 库到你的开发环境中，用于分析和机器学习问题。你还将能够使用
    pandas 加载外部数据源，并使用多种方法搜索、过滤和计算数据的描述性统计信息。本章将使你能够评估数据源中诸如缺失数据、类别不平衡和样本量不足等问题的潜在影响。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 介绍
- en: 'The study and application of machine learning and artificial intelligence has
    recently been the source of much interest and research in the technology and business
    communities. Advanced data analytics and machine learning techniques have shown
    great promise in advancing many sectors, such as personalized healthcare and self-driving
    cars, as well as in solving some of the world''s greatest challenges, such as
    combating climate change (see Tackling Climate Change with Machine Learning: https://arxiv.org/pdf/1906.05433.pdf).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和人工智能的研究和应用最近引起了技术和商业界的广泛关注。先进的数据分析和机器学习技术在推动许多领域取得巨大进展方面表现出巨大的潜力，例如个性化医疗、自驾汽车，以及解决世界上一些最大的挑战，例如应对气候变化（参见《使用机器学习应对气候变化》：https://arxiv.org/pdf/1906.05433.pdf）。
- en: This book has been designed to help you to take advantage of the unique confluence
    of events in the field of data science and machine learning today. Across the
    globe, private enterprises and governments are realizing the value and efficiency
    of data-driven products and services. At the same time, reduced hardware costs
    and open source software solutions are significantly reducing the barriers to
    entry of learning and applying machine learning techniques.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助你利用当今数据科学和机器学习领域中的独特发展机遇。全球范围内，私营企业和政府都在认识到数据驱动的产品和服务的价值和效率。与此同时，硬件成本的降低和开源软件解决方案显著降低了学习和应用机器学习技术的门槛。
- en: Here, we will focus on supervised machine learning (or, supervised learning
    for short). We'll explain the different types of machine learning shortly, but
    let's begin with some quick information. The now-classic example of supervised
    learning is developing an algorithm to distinguish between pictures of cats and
    dogs. The supervised part arises from two aspects; first, we have a set of pictures
    where we know the correct answers. We call such data labeled data. Second, we
    carry out a process where we iteratively test our algorithm's ability to predict
    "cat" or "dog" given pictures, and we make corrections to the algorithm when the
    predictions are incorrect. This process, at a high level, is similar to teaching
    children. However, it generally takes a lot more data to train an algorithm than
    to teach a child to recognize cats and dogs! Fortunately, there are rapidly growing
    sources of data at our disposal. Note the use of the words learning and train
    in the context of developing our algorithm. These might seem to be giving human
    qualities to our machines and computer programs, but they are already deeply ingrained
    in the machine learning (and artificial intelligence) literature, so let's use
    them and understand them. Training in our context here always refers to the process
    of providing labeled data to an algorithm and making adjustments to the algorithm
    to best predict the labels given the data. Supervised means that the labels for
    the data are provided within the training, allowing the model to learn from these labels.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将重点介绍监督式机器学习（简称监督学习）。我们稍后将解释不同类型的机器学习，但我们先从一些简单的信息开始。监督学习的经典示例是开发一个算法来区分猫和狗的图片。监督的部分源于两个方面；首先，我们有一组图片，其中正确答案是已知的。我们称这种数据为标签数据。其次，我们进行一个过程，不断测试我们的算法是否能够根据图片预测“猫”或“狗”，并在预测错误时对算法进行修正。从高层次上讲，这个过程类似于教孩子。然而，训练一个算法通常需要比教一个孩子识别猫和狗更多的数据！幸运的是，我们有越来越多的数据源可以利用。请注意，在开发我们的算法时，使用了“学习”和“训练”这两个词。这些词看起来似乎是赋予机器和计算机程序人的特质，但它们已经深深根植于机器学习（和人工智能）文献中，所以我们就用它们，并理解它们。在这里，“训练”指的是将标签数据提供给算法，并对算法进行调整，以便根据数据最好地预测标签。监督的意思是，数据的标签在训练过程中已提供，使得模型能够从这些标签中学习。
- en: Let's now understand the distinction between supervised learning and other forms
    of machine learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解监督学习与其他形式机器学习的区别。
- en: When to Use Supervised Learning
  id: totrans-8
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 何时使用监督学习
- en: 'Generally, if you are trying to automate or replicate an existing process,
    the problem is a supervised learning problem. As an example, let''s say you are
    the publisher of a magazine that reviews and ranks hairstyles from various time
    periods. Your readers frequently send you far more images of their favorite hairstyles
    for review than you can manually process. To save some time, you would like to
    automate the sorting of the hairstyle images you receive based on time periods,
    starting with hairstyles from the 1960s and 1980s, as you can see in the following
    figure:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你试图自动化或复制一个现有的过程，那么问题就是一个监督学习问题。举个例子，假设你是一本评论并排名各个时期发型的杂志的出版商。你的读者经常向你发送远多于你能够手动处理的发型图片进行评审。为了节省时间，你希望自动化排序你收到的发型图片，按照时间顺序从1960年代和1980年代的发型开始，正如你在下图中所看到的：
- en: '![Figure 1.1: Images of hairstyles from different time periods'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.1：来自不同时期的发型图片'
- en: '](img/image-S9FJQL0H.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-S9FJQL0H.jpg)'
- en: 'Figure 1.1: Images of hairstyles from different time periods'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：来自不同时期的发型图片
- en: To create your hairstyles-sorting algorithm, you start by collecting a large
    sample of hairstyle images and manually labeling each one with its corresponding
    time period. Such a dataset (known as a labeled dataset) is the input data (hairstyle
    images) for which the desired output information (time period) is known and recorded.
    This type of problem is a classic supervised learning problem; we are trying to
    develop an algorithm that takes a set of inputs and learns to return the answers
    that we have told it are correct.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建你的发型排序算法，你首先需要收集大量发型图片，并手动为每一张图片标注对应的时间时期。这样的数据集（称为标签数据集）是输入数据（发型图片），其所需的输出信息（时间时期）是已知并记录下来的。这类问题是经典的监督学习问题；我们正在尝试开发一个算法，它可以接收一组输入，并学会返回我们告知它正确的答案。
- en: Python Packages and Modules
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Python 包和模块
- en: Python is one of the most popular programming languages used for machine learning,
    and is the language used here.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是最常用的机器学习编程语言之一，并且是本书中使用的语言。
- en: While the standard features that are included in Python are certainly feature-rich,
    the true power of Python lies in the additional libraries (also known as packages),
    which, thanks to open source licensing, can be easily downloaded and installed
    through a few simple commands. In this book, we generally assume your system has
    been configured using Anaconda, which is an open source environment manager for
    Python. Depending on your system, you can configure multiple virtual environments
    using Anaconda, each one configured with specific packages and even different
    versions of Python. Using Anaconda takes care of many of the requirements to get
    ready to perform machine learning, as many of the most common packages come pre-built
    within Anaconda. Refer to the preface for Anaconda installation instructions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Python 中包含的标准功能确实功能丰富，但 Python 的真正强大之处在于额外的库（也称为包），这些库由于开源许可，可以通过几个简单的命令轻松下载和安装。在本书中，我们一般假设您的系统已经使用
    Anaconda 配置，Anaconda 是一个用于 Python 的开源环境管理器。根据您的系统，您可以使用 Anaconda 配置多个虚拟环境，每个环境都配置有特定的包，甚至不同版本的
    Python。使用 Anaconda 可以解决许多准备进行机器学习的要求，因为许多最常用的包都已经预构建在 Anaconda 中。有关 Anaconda 安装说明，请参阅前言。
- en: 'In this book, we will be using the following additional Python packages:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用以下附加的 Python 包：
- en: 'NumPy (pronounced Num Pie and available at https://www.numpy.org/): NumPy (short
    for numerical Python) is one of the core components of scientific computing in
    Python. NumPy provides the foundational data types from which a number of other
    data structures derive, including linear algebra, vectors and matrices, and key
    random number functionality.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy（发音为 Num Pie，可在 https://www.numpy.org/ 获取）：NumPy（即数字 Python 的简称）是 Python
    科学计算的核心组成部分之一。NumPy 提供了多个数据类型，这些数据类型衍生出了许多其他数据结构，包括线性代数、向量和矩阵，以及关键的随机数功能。
- en: 'SciPy (pronounced Sigh Pie and available at https://www.scipy.org): SciPy,
    along with NumPy, is a core scientific computing package. SciPy provides a number
    of statistical tools, signal processing tools, and other functionality, such as
    Fourier transforms.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy（发音为 Sigh Pie，可在 https://www.scipy.org 获取）：SciPy 与 NumPy 一起，是核心的科学计算包。SciPy
    提供了许多统计工具、信号处理工具和其他功能，如傅里叶变换。
- en: 'pandas (available at https://pandas.pydata.org/): pandas is a high-performance
    library for loading, cleaning, analyzing, and manipulating data structures.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: pandas（可在 https://pandas.pydata.org/ 获取）：pandas 是一个高性能的库，用于加载、清洗、分析和操作数据结构。
- en: 'Matplotlib (available at https://matplotlib.org/): Matplotlib is the foundational
    Python library for creating graphs and plots of datasets and is also the base
    package from which other Python plotting libraries derive. The Matplotlib API
    has been designed in alignment with the Matlab plotting library to facilitate
    an easy transition to Python.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib（可在 https://matplotlib.org/ 获取）：Matplotlib 是 Python 中用于创建数据集图表和绘图的基础库，也是其他
    Python 绘图库的基础包。Matplotlib 的 API 设计与 Matlab 的绘图库对齐，以便于轻松过渡到 Python。
- en: 'Seaborn (available at https://seaborn.pydata.org/): Seaborn is a plotting library
    built on top of Matplotlib, providing attractive color and line styles as well
    as a number of common plotting templates.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn（可在 https://seaborn.pydata.org/ 获取）：Seaborn 是一个基于 Matplotlib 构建的绘图库，提供了吸引人的颜色和线条样式，以及一些常见的绘图模板。
- en: 'Scikit-learn (available at https://scikit-learn.org/stable/): Scikit-learn
    is a Python machine learning library that provides a number of data mining, modeling,
    and analysis techniques in a simple API. Scikit-learn includes a number of machine
    learning algorithms out of the box, including classification, regression, and
    clustering techniques.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn（可在 https://scikit-learn.org/stable/ 获取）：Scikit-learn 是一个 Python
    机器学习库，提供了许多数据挖掘、建模和分析技术，且有一个简单的 API。Scikit-learn 内置了许多机器学习算法，包括分类、回归和聚类技术。
- en: 'These packages form the foundation of a versatile machine learning development
    environment, with each package contributing a key set of functionalities. As discussed,
    by using Anaconda, you will already have all of the required packages installed
    and ready for use. If you require a package that is not included in the Anaconda
    installation, it can be installed by simply entering and executing the following
    code in a Jupyter notebook cell:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包构成了一个多功能的机器学习开发环境的基础，每个包都提供了一个关键功能集。如前所述，使用Anaconda时，您已经安装了所有必需的包，并且可以随时使用。如果需要安装Anaconda安装包中未包含的包，可以通过在Jupyter笔记本单元格中输入并执行以下代码来安装：
- en: '!conda install <package name>'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '!conda install <package name>'
- en: 'As an example, if we wanted to install Seaborn, we''d run the following command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想安装Seaborn，可以运行以下命令：
- en: '!conda install seaborn'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '!conda install seaborn'
- en: 'To use one of these packages in a notebook, all we need to do is import it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要在笔记本中使用这些包，我们只需要导入它：
- en: import matplotlib
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib
- en: Loading Data in Pandas
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在Pandas中加载数据
- en: pandas has the ability to read and write a number of different file formats
    and data structures, including CSV, JSON, and HDF5 files, as well as SQL and Python
    Pickle formats. The pandas input/output documentation can be found at https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html.
    We will continue to look into the pandas functionality by loading data via a CSV
    file.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: pandas能够读取和写入多种不同的文件格式和数据结构，包括CSV、JSON、HDF5文件，以及SQL和Python Pickle格式。pandas的输入/输出文档可以在https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html找到。我们将继续通过加载CSV文件来研究pandas的功能。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset used in this chapter is available on our GitHub repository via
    the following link: https://packt.live/2vjyPK9\. Once you download the entire
    repository on your system, you can find the dataset in the Datasets folder. Furthermore,
    this dataset is the Titanic: Machine Learning from Disaster dataset, which was
    originally made available at https://www.kaggle.com/c/Titanic/data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '本章使用的数据集可以通过以下链接在我们的GitHub仓库中找到：https://packt.live/2vjyPK9。下载整个仓库到您的系统后，您可以在
    Datasets 文件夹中找到数据集。此外，这个数据集是Titanic: Machine Learning from Disaster数据集，最初可以在https://www.kaggle.com/c/Titanic/data找到。'
- en: The dataset contains a roll of the guests on board the famous ship Titanic,
    as well as their age, survival status, and number of siblings/parents. Before
    we get started with loading the data into Python, it is critical that we spend
    some time looking over the information provided for the dataset so that we can
    have a thorough understanding of what it contains. Download the dataset and place
    it in the directory you're working in.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含了著名的泰坦尼克号上的乘客名单，以及他们的年龄、幸存状态和兄弟姐妹/父母的数量。在开始将数据加载到Python之前，关键是花时间查看数据集提供的信息，以便我们能够深入理解它所包含的内容。下载数据集并将其放置在您的工作目录中。
- en: 'Looking at the description for the data, we can see that we have the following
    fields available:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据描述中，我们可以看到我们有以下字段：
- en: 'survival: This tells us whether a given person survived (0 = No, 1 = Yes).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'survival: 这告诉我们一个人是否幸存（0 = 否，1 = 是）。'
- en: 'pclass: This is a proxy for socio-economic status, where first class is upper,
    second class is middle, and third class is lower status.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 'pclass: 这是社会经济地位的代理，其中头等舱代表上层，二等舱代表中层，三等舱代表下层。'
- en: 'sex: This tells us whether a given person is male or female.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'sex: 这告诉我们一个人是男性还是女性。'
- en: 'age: This is a fractional value if less than 1; for example, 0.25 is 3 months.
    If the age is estimated, it is in the form of xx.5.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'age: 如果年龄小于1，则这是一个分数值；例如，0.25表示3个月。如果年龄是估算的，它将以xx.5的形式表示。'
- en: 'sibsp: A sibling is defined as a brother, sister, stepbrother, or stepsister,
    and a spouse is a husband or wife.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'sibsp: 兄弟姐妹定义为兄弟、姐妹、继兄或继姐，配偶定义为丈夫或妻子。'
- en: 'parch: A parent is a mother or father, while a child is a daughter, son, stepdaughter,
    or stepson. Children that traveled only with a nanny did not travel with a parent.
    Thus, 0 was assigned for this field.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'parch: 父母是母亲或父亲，而子女则是女儿、儿子、继女或继子。仅与保姆一起旅行的孩子没有与父母一起旅行。因此，该字段赋值为0。'
- en: 'ticket: This gives the person''s ticket number.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'ticket: 这是乘客的票号。'
- en: 'fare: This is the passenger''s fare.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'fare: 这是乘客的票价。'
- en: 'cabin: This tells us the passenger''s cabin number.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'cabin: 这告诉我们乘客的舱号。'
- en: 'embarked: The point of embarkation is the location where the passenger boarded
    the ship.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'embark: 登船地点是乘客登船的地点。'
- en: Note that the information provided with the dataset does not give any context
    as to how the data was collected. The survival, pclass, and embarked fields are
    known as categorical variables as they are assigned to one of a fixed number of
    labels or categories to indicate some other information. For example, in embarked,
    the C label indicates that the passenger boarded the ship at Cherbourg, and the
    value of 1 in survival indicates they survived the sinking.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集提供的信息没有说明数据是如何收集的。survival、pclass 和 embarked 字段被称为分类变量，因为它们被分配到一组固定的标签或类别中，以指示其他信息。例如，在
    embarked 中，C 标签表示乘客在瑟堡登船，而 survival 中的 1 表示他们在沉船事故中幸存。
- en: 'Exercise 1.01: Loading and Summarizing the Titanic Dataset'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 1.01：加载并总结 Titanic 数据集
- en: 'In this exercise, we will read our Titanic dataset into Python and perform
    a few basic summary operations on it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将把 Titanic 数据集读入 Python 并进行一些基本的总结操作：
- en: Open a new Jupyter notebook.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter notebook。
- en: 'Import the pandas and numpy packages using shorthand notation:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简写语法导入 pandas 和 numpy 包：
- en: import pandas as pd
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: 'Open the titanic.csv file by clicking on it in the Jupyter notebook home page
    as shown in the following figure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击 Jupyter notebook 首页上的 titanic.csv 文件来打开并读取文件，如下图所示：
- en: '![Figure 1.2: Opening the CSV file'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2：打开 CSV 文件'
- en: '](img/image-UD4G36A4.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-UD4G36A4.jpg)'
- en: 'Figure 1.2: Opening the CSV file'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：打开 CSV 文件
- en: 'The file is a CSV file, which can be thought of as a table, where each line
    is a row in the table and each comma separates columns in the table. Thankfully,
    we don''t need to work with these tables in raw text form and can load them using pandas:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件是一个 CSV 文件，可以将其视为一个表格，每一行代表表格中的一行，每个逗号分隔表格中的列。幸运的是，我们无需以原始文本形式处理这些表格，可以使用
    pandas 加载它们：
- en: '![Figure 1.3: Contents of the CSV file'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3：CSV 文件内容'
- en: '](img/image-4IABVFBO.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-4IABVFBO.jpg)'
- en: 'Figure 1.3: Contents of the CSV file'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：CSV 文件内容
- en: Note
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: Take a moment to look up the pandas documentation for the read_csv function
    at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html.
    Note the number of different options available for loading CSV data into a pandas
    DataFrame.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 花点时间查阅 pandas 文档中关于 read_csv 函数的说明，网址：https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html。注意加载
    CSV 数据到 pandas DataFrame 中的不同选项。
- en: 'In an executable Jupyter notebook cell, execute the following code to load
    the data from the file:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在可执行的 Jupyter notebook 单元格中，执行以下代码来从文件加载数据：
- en: df = pd.read_csv(r'..\Datasets\titanic.csv')
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.read_csv(r'..\Datasets\titanic.csv')
- en: The pandas DataFrame class provides a comprehensive set of attributes and methods
    that can be executed on its own contents, ranging from sorting, filtering, and
    grouping methods to descriptive statistics, as well as plotting and conversion.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的 DataFrame 类提供了一整套可以在其内容上执行的属性和方法，涵盖从排序、过滤、分组方法到描述性统计，以及绘图和转换等功能。
- en: Note
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: Open and read the documentation for pandas DataFrame objects at https://pandas.pydata.org/pandas-docs/stable/reference/frame.html.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 pandas DataFrame 对象的文档，网址：https://pandas.pydata.org/pandas-docs/stable/reference/frame.html。
- en: 'Read the first ten rows of data using the head() method of the DataFrame:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataFrame 的 head() 方法读取前十行数据：
- en: Note
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: 'The # symbol in the code snippet below denotes a code comment. Comments are
    added into code to help explain specific bits of logic.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '以下代码片段中的 # 符号表示代码注释。注释是添加到代码中以帮助解释特定逻辑的部分。'
- en: 'df.head(10) # Examine the first 10 samples'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.head(10) # 检查前 10 个样本'
- en: 'The output will be as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.4: Reading the first 10 rows'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.4：读取前 10 行'
- en: '](img/image-A5PCLLFO.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-A5PCLLFO.jpg)'
- en: 'Figure 1.4: Reading the first 10 rows'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4：读取前 10 行
- en: Note
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: To access the source code for this specific section, please refer to https://packt.live/2Ynb7sf.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2Ynb7sf。
- en: You can also run this example online at https://packt.live/2BvTRrG. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，网址：https://packt.live/2BvTRrG。你必须执行整个笔记本以获得预期结果。
- en: In this sample, we have a visual representation of the information in the DataFrame.
    We can see that the data is organized in a tabular, almost spreadsheet-like structure.
    The different types of data are organized into columns, while each sample is organized
    into rows. Each row is assigned an index value and is shown as the numbers 0 to
    9 in bold on the left-hand side of the DataFrame. Each column is assigned to a
    label or name, as shown in bold at the top of the DataFrame.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们展示了 DataFrame 中信息的可视化表示。我们可以看到数据以表格化的方式组织，几乎像电子表格一样。不同类型的数据被组织成列，而每个样本被组织成行。每一行都分配了一个索引值，并以粗体数字
    0 到 9 显示在 DataFrame 的左侧。每一列都分配了一个标签或名称，如粗体所示，在 DataFrame 的顶部。
- en: The idea of a DataFrame as a kind of spreadsheet is a reasonable analogy. As
    we will see in this chapter, we can sort, filter, and perform computations on
    the data just as you would in a spreadsheet program. While it's not covered in
    this chapter, it is interesting to note that DataFrames also contain pivot table
    functionality, just like a spreadsheet (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 作为一种电子表格的类比是合理的。如本章所示，我们可以像在电子表格程序中那样对数据进行排序、过滤和计算。虽然本章未涉及，但值得注意的是，DataFrame
    还包含了类似电子表格的透视表功能（https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html）。
- en: 'Exercise 1.02: Indexing and Selecting Data'
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 1.02：索引和选择数据
- en: 'Now that we have loaded some data, let''s use the selection and indexing methods
    of the DataFrame to access some data of interest. This exercise is a continuation
    of Exercise 1.01, Loading and Summarizing the Titanic Dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了一些数据，让我们使用 DataFrame 的选择和索引方法来访问一些感兴趣的数据。这个练习是练习 1.01，“加载并总结泰坦尼克数据集”的延续：
- en: 'Select individual columns in a similar way to a regular dictionary by using
    the labels of the columns, as shown here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用列的标签，以类似于常规字典的方式选择单个列，如下所示：
- en: df['Age']
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: df['Age']
- en: 'The output will be as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: 0 22.0
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 0 22.0
- en: 1 38.0
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 1 38.0
- en: 2 26.0
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2 26.0
- en: 3 35.0
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 3 35.0
- en: 4 35.0
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 4 35.0
- en: '...'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: 1304 NaN
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 1304 NaN
- en: 1305 39.0
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 1305 39.0
- en: 1306 38.5
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 1306 38.5
- en: 1307 NaN
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 1307 NaN
- en: 1308 NaN
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 1308 NaN
- en: 'Name: Age, Length: 1309, dtype: float64'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：年龄，长度：1309，数据类型：float64
- en: 'If there are no spaces in the column name, we can also use the dot operator.
    If there are spaces in the column names, we will need to use the bracket notation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列名中没有空格，我们还可以使用点操作符。如果列名中有空格，则需要使用括号表示法：
- en: df.Age
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: df.Age
- en: 'The output will be as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: 0 22.0
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 0 22.0
- en: 1 38.0
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1 38.0
- en: 2 26.0
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2 26.0
- en: 3 35.0
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 3 35.0
- en: 4 35.0
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 4 35.0
- en: '...'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: 1304 NaN
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 1304 NaN
- en: 1305 39.0
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 1305 39.0
- en: 1306 38.5
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 1306 38.5
- en: 1307 NaN
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 1307 NaN
- en: 1308 NaN
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 1308 NaN
- en: 'Name: Age, Length: 1309, dtype: float64'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 名称：年龄，长度：1309，数据类型：float64
- en: 'Select multiple columns at once using bracket notation, as shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用括号表示法一次选择多个列，如下所示：
- en: df[['Name', 'Parch', 'Sex']]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: df[['Name', 'Parch', 'Sex']]
- en: 'The output will be as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.5: Selecting multiple columns'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.5：选择多个列'
- en: '](img/image-T3OOZHPG.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-T3OOZHPG.jpg)'
- en: 'Figure 1.5: Selecting multiple columns'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：选择多个列
- en: Note
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The output has been truncated for presentation purposes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于展示需要，输出已被截断。
- en: 'Select the first row using iloc:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 iloc 选择第一行：
- en: df.iloc[0]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: df.iloc[0]
- en: 'The output will be as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.6: Selecting the first row'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.6：选择第一行'
- en: '](img/image-HCT3JIYP.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-HCT3JIYP.jpg)'
- en: 'Figure 1.6: Selecting the first row'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：选择第一行
- en: 'Select the first three rows using iloc:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 iloc 选择前三行：
- en: df.iloc[[0,1,2]]
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: df.iloc[[0,1,2]]
- en: 'The output will be as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.7: Selecting the first three rows'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.7：选择前三行'
- en: '](img/image-I49IQNZH.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-I49IQNZH.jpg)'
- en: 'Figure 1.7: Selecting the first three rows'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：选择前三行
- en: 'Next, get a list of all of the available columns:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，获取所有可用列的列表：
- en: 'columns = df.columns # Extract the list of columns'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'columns = df.columns # 提取列名列表'
- en: print(columns)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: print(columns)
- en: 'The output will be as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.8: Getting all the columns'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.8：获取所有列'
- en: '](img/image-KWXX3C2L.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-KWXX3C2L.jpg)'
- en: 'Figure 1.8: Getting all the columns'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：获取所有列
- en: 'Use this list of columns and the standard Python slicing syntax to get columns
    2, 3, and 4, and their corresponding values:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此列名列表和标准的 Python 切片语法，获取第 2、3、4 列及其对应的值：
- en: 'df[columns[1:4]] # Columns 2, 3, 4'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'df[columns[1:4]] # 第 2、3、4 列'
- en: 'The output will be as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.9: Getting the second, third, and fourth columns'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9：获取第二、第三和第四列'
- en: '](img/image-WE9Q3XRE.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-WE9Q3XRE.jpg)'
- en: 'Figure 1.9: Getting the second, third, and fourth columns'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：获取第二列、第三列和第四列
- en: 'Use the len operator to get the number of rows in the DataFrame:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 len 运算符获取 DataFrame 中的行数：
- en: len(df)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: len(df)
- en: 'The output will be as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '1309'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '1309'
- en: 'Get the value for the Fare column in row 2 using the row-centric method:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以行中心的方法获取第 2 行的 Fare 列值：
- en: 'df.iloc[2][''Fare''] # Row centric'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.iloc[2][''Fare''] # 以行为中心'
- en: 'The output will be as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '7.925'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '7.925'
- en: 'Use the dot operator for the column, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用点操作符来访问列，如下所示：
- en: 'df.iloc[2].Fare # Row centric'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.iloc[2].Fare # 以行为中心'
- en: 'The output will be as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '7.925'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '7.925'
- en: 'Use the column-centric method, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以列为中心的方法，如下所示：
- en: 'df[''Fare''][2] # Column centric'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'df[''Fare''][2] # 以列为中心'
- en: 'The output will be as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '7.925'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '7.925'
- en: 'Use the column-centric method with the dot operator, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以列为中心的方法并使用点操作符，如下所示：
- en: 'df.Fare[2] # Column centric'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.Fare[2] # 以列为中心'
- en: 'The output will be as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '7.925'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '7.925'
- en: Note
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2YmA7jb.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一特定部分的源代码，请参考 [https://packt.live/2YmA7jb](https://packt.live/2YmA7jb)。
- en: You can also run this example online at https://packt.live/3dmk0qf. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/3dmk0qf](https://packt.live/3dmk0qf) 上在线运行此示例。你必须执行整个笔记本才能得到预期的结果。
- en: In this exercise, we have seen how to use pandas' read_csv() function to load
    data into Python within a Jupyter notebook. We then explored a number of ways
    that pandas, by presenting the data in a DataFrame, facilitates selecting specific
    items in a DataFrame and viewing the contents. With these basics understood, let's
    look at some more advanced ways to index and select data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何使用 pandas 的 read_csv() 函数将数据加载到 Python 中的 Jupyter 笔记本中。然后，我们探讨了
    pandas 通过以 DataFrame 形式呈现数据，如何便捷地选择 DataFrame 中的特定项并查看其内容。在理解了这些基础知识后，我们将进一步探索更高级的索引和数据选择方法。
- en: 'Exercise 1.03: Advanced Indexing and Selection'
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 1.03：高级索引和选择
- en: 'With the basics of indexing and selection under our belt, we can turn our attention
    to more advanced indexing and selection. In this exercise, we will look at a few
    important methods for performing advanced indexing and selecting data. This exercise
    is a continuation of Exercise 1.01, Loading and Summarizing the Titanic Dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了基本的索引和选择方法之后，我们可以将注意力转向更高级的索引和选择方法。在这个练习中，我们将探索几种重要的方法来执行高级索引和数据选择。这个练习是练习
    1.01 "加载并总结 Titanic 数据集" 的延续：
- en: 'Create a list of the passengers'' names and ages for those passengers under
    the age of 21, as shown here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个包含21岁以下乘客姓名和年龄的列表，如下所示：
- en: child_passengers = df[df.Age < 21][['Name', 'Age']]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: child_passengers = df[df.Age < 21][['Name', 'Age']]
- en: child_passengers.head()
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: child_passengers.head()
- en: 'The output will be as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 1.10: List of passengers'' names and ages for those passengers under
    the age of 21'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10：21岁以下乘客的姓名和年龄列表](img/image-W5SL6I98.jpg)'
- en: '](img/image-W5SL6I98.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-W5SL6I98.jpg)'
- en: 'Figure 1.10: List of passengers'' names and ages for those passengers under
    the age of 21'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：21岁以下乘客的姓名和年龄列表
- en: 'Count how many child passengers there were, as shown here:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 计算有多少名儿童乘客，如下所示：
- en: print(len(child_passengers))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: print(len(child_passengers))
- en: 'The output will be as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '249'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '249'
- en: 'Count how many passengers were between the ages of 21 and 30\. Do not use Python''s
    and logical operator for this step, but rather the ampersand symbol (&). Do this
    as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 计算年龄在 21 岁到 30 岁之间的乘客数量。此步骤不使用 Python 的逻辑与运算符，而是使用和符号（&）。具体操作如下：
- en: Note
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The code snippet shown here uses a backslash ( \ ) to split the logic across
    multiple lines. When the code is executed, Python will ignore the backslash, and
    treat the code on the next line as a direct continuation of the current line.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的代码片段使用了反斜杠（\）将逻辑分割成多行。当代码执行时，Python 会忽略反斜杠，并将下一行代码视为当前行的直接延续。
- en: young_adult_passengers = df.loc[(df.Age > 21) \
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: young_adult_passengers = df.loc[(df.Age > 21) \
- en: '& (df.Age < 30)]'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '& (df.Age < 30)]'
- en: len(young_adult_passengers)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: len(young_adult_passengers)
- en: 'The output will be as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '279'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '279'
- en: 'Find the passengers who were either first- or third-class ticket holders. Again,
    we will not use the Python logical or operator but the pipe symbol (|) instead.
    Do this as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 查找第一类或第三类票的乘客。我们不会使用 Python 的逻辑或运算符，而是使用管道符号（|）。具体操作如下：
- en: df.loc[(df.Pclass == 3) | (df.Pclass ==1)]
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: df.loc[(df.Pclass == 3) | (df.Pclass ==1)]
- en: 'The output will be as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.11: The number of passengers who were either first- or third-class
    ticket holders'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11：既是头等舱票持有者又是三等舱票持有者的乘客数量'
- en: '](img/image-8DU9WLXA.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-8DU9WLXA.jpg)'
- en: 'Figure 1.11: The number of passengers who were either first- or third-class
    ticket holders'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：既是头等舱票持有者又是三等舱票持有者的乘客数量
- en: 'Find the passengers who were not holders of either first- or third-class tickets.
    Do not simply select those second-class ticket holders, but use the ~ symbol for
    the not logical operator instead. Do this as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 查找不是头等或者三等舱票持有者的乘客。不要简单地选择二等舱票持有者，而是使用 ~ 符号作为非逻辑运算符。操作如下：
- en: df.loc[~((df.Pclass == 3) | (df.Pclass == 1))]
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: df.loc[~((df.Pclass == 3) | (df.Pclass == 1))]
- en: 'The output will be as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.12: Count of passengers who were not holders of either first- or
    third-class tickets'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12：不是头等舱或三等舱票持有者的乘客数量'
- en: '](img/image-H15XD8JZ.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-H15XD8JZ.jpg)'
- en: 'Figure 1.12: Count of passengers who were not holders of either first- or third-class
    tickets'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：不是头等舱或三等舱票持有者的乘客数量
- en: 'We no longer need the Unnamed: 0 column, so delete it using the del operator:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再需要未命名的 0 列，因此使用 del 运算符删除它：
- en: 'del df[''Unnamed: 0'']'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 'del df[''Unnamed: 0'']'
- en: df.head()
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: df.head()
- en: 'The output will be as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.13: The del operator'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：del 运算符'
- en: '](img/image-DSH2JKHK.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-DSH2JKHK.jpg)'
- en: 'Figure 1.13: The del operator'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：del 运算符
- en: Note
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3empSRO.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/3empSRO。
- en: You can also run this example online at https://packt.live/3fEsPgK. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在 https://packt.live/3fEsPgK 上在线运行此示例。为了获得期望的结果，您必须执行整个笔记本。
- en: In this exercise, we have seen how to select data from a DataFrame using conditional
    operators that inspect the data and return the subsets we want. We also saw how
    to remove a column we didn't need (in this case, the Unnamed column simply contained
    row numbers that are not relevant to analysis). Now, we'll dig deeper into some
    of the power of pandas.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何使用条件运算符从 DataFrame 中选择数据，并返回我们想要的子集。我们还看到如何删除我们不需要的列（在这种情况下，未命名列只包含对分析无关的行号）。现在，让我们深入了解一些
    pandas 的强大功能。
- en: Pandas Methods
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Pandas 方法
- en: Now that we are confident with some pandas basics, as well as some more advanced
    indexing and selecting tools, let's look at some other DataFrame methods. For
    a complete list of all methods available in a DataFrame, we can refer to the class
    documentation.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对一些 pandas 基础知识和一些更高级的索引和选择工具感到自信，让我们看一些其他 DataFrame 方法。要获取 DataFrame 中可用的所有方法的完整列表，可以参考类文档。
- en: Note
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The pandas documentation is available at https://pandas.pydata.org/pandas-docs/stable/reference/frame.html.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 的文档可以在 https://pandas.pydata.org/pandas-docs/stable/reference/frame.html
    找到。
- en: You should now know how many methods are available within a DataFrame. There
    are far too many to cover in detail in this chapter, so we will select a few that
    will give you a great start in supervised machine learning.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您应该知道 DataFrame 中有多少方法可用。这些方法太多，无法在本章节详细介绍，因此我们将选择一些能为您提供优秀启动的方法。
- en: 'We have already seen the use of one method, head(), which provides the first
    five lines of the DataFrame. We can select more or fewer lines if we wish by providing
    the number of lines as an argument, as shown here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一个方法的用法，head()，它提供了 DataFrame 的前五行。如果需要，我们可以通过提供行数来选择更多或更少的行，如下所示：
- en: 'df.head(n=20) # 20 lines'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.head(n=20) # 20 行'
- en: 'df.head(n=32) # 32 lines'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.head(n=32) # 32 行'
- en: Alternatively, you can use the tail() function to see a specified number of
    lines at the end of the DataFrame.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用 tail() 函数查看 DataFrame 结尾的指定行数。
- en: 'Another useful method is describe, which is a super-quick way of getting the
    descriptive statistics of the data within a DataFrame. We can see next that the
    sample size (count), mean, minimum, maximum, standard deviation, and the 25th,
    50th, and 75th percentiles are returned for all columns of numerical data in the
    DataFrame (note that text columns have been omitted):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的方法是 describe，这是一种快速获取 DataFrame 中数据描述统计信息的方法。我们可以看到样本大小（计数）、均值、最小值、最大值、标准差以及第
    25、50 和 75 百分位数对所有数值数据列返回（请注意，文本列已被省略）：
- en: df.describe()
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: df.describe()
- en: 'The output will be as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.14: The describe method'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：describe 方法'
- en: '](img/image-ZLC1M3ID.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-ZLC1M3ID.jpg)'
- en: 'Figure 1.14: The describe method'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：describe 方法
- en: Note that only columns of numerical data have been included within the summary.
    This simple command provides us with a lot of useful information; looking at the
    values for count (which counts the number of valid samples), we can see that there
    are 1,046 valid samples in the Age category, but 1,308 in Fare, and only 891 in
    Survived. We can see that the youngest person was 0.17 years, the average age
    is 29.898, and the eldest passenger was 80\. The minimum fare was £0, with £33.30
    the average and £512.33 the most expensive. If we look at the Survived column,
    we have 891 valid samples, with a mean of 0.38, which means about 38% survived.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有数值型数据的列被包含在摘要中。这个简单的命令为我们提供了大量有用的信息；通过查看 count（即有效样本数）的值，我们可以看到年龄（Age）类别中有
    1,046 个有效样本，但票价（Fare）有 1,308 个，而生存（Survived）列只有 891 个有效样本。我们可以看到，最年轻的人是 0.17 岁，平均年龄为
    29.898 岁，最年长的乘客为 80 岁。最小票价为 £0，平均票价为 £33.30，最贵票价为 £512.33。如果我们看一下 Survived 列，我们有
    891 个有效样本，均值为 0.38，这意味着大约 38% 的人存活。
- en: 'We can also get these values separately for each of the columns by calling
    the respective methods of the DataFrame, as shown here:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过调用 DataFrame 的相应方法，分别获取每一列的这些值，如下所示：
- en: df.count()
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: df.count()
- en: 'The output will be as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: Cabin 295
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Cabin 295
- en: Embarked 1307
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Embarked 1307
- en: Fare 1308
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Fare 1308
- en: Pclass 1309
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Pclass 1309
- en: Ticket 1309
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Ticket 1309
- en: Age 1046
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Age 1046
- en: Name 1309
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Name 1309
- en: Parch 1309
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Parch 1309
- en: Sex 1309
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Sex 1309
- en: SibSp 1309
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: SibSp 1309
- en: Survived 891
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Survived 891
- en: 'dtype: int64'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 'dtype: int64'
- en: 'But we have some columns that contain text data, such as Embarked, Ticket,
    Name, and Sex. So what about these? How can we get some descriptive information
    for these columns? We can still use describe; we just need to pass it some more
    information. By default, describe will only include numerical columns and will
    compute the 25th, 50th, and 75th percentiles, but we can configure this to include
    text-based columns by passing the include = ''all'' argument, as shown here:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也有一些包含文本数据的列，例如 Embarked、Ticket、Name 和 Sex。那么这些怎么办呢？我们如何获取这些列的描述性信息？我们仍然可以使用
    describe 方法，只是需要传递更多的信息。默认情况下，describe 只会包含数值列，并计算 25%、50% 和 75% 的分位数，但我们可以通过传递
    `include = 'all'` 参数来配置它以包含文本列，如下所示：
- en: df.describe(include='all')
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: df.describe(include='all')
- en: 'The output will be as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.15: The describe method with text-based columns'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15：带文本列的 describe 方法'
- en: '](img/image-1ZRAJN7I.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-1ZRAJN7I.jpg)'
- en: 'Figure 1.15: The describe method with text-based columns'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：带文本列的 describe 方法
- en: That's better—now we have much more information. Looking at the Cabin column,
    we can see that there are 295 entries, with 186 unique values. The most common
    values are C32, C25, and C27, and they occur 6 times (from the freq value). Similarly,
    if we look at the Embarked column, we see that there are 1,307 entries, 3 unique
    values, and that the most commonly occurring value is S, with 914 entries.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这样更好了——现在我们有了更多信息。查看 Cabin 列，我们可以看到有 295 个条目，其中有 186 个唯一值。最常见的值是 C32、C25 和 C27，它们出现了
    6 次（来自 freq 值）。类似地，查看 Embarked 列，我们看到有 1,307 个条目，3 个唯一值，最常出现的值是 S，共有 914 个条目。
- en: Notice the occurrence of NaN values in our describe output table. NaN, or Not
    a Number, values are very important within DataFrames as they represent missing
    or not available data. The ability of the pandas library to read from data sources
    that contain missing or incomplete information is both a blessing and a curse.
    Many other libraries would simply fail to import or read the data file in the
    event of missing information, while the fact that it can be read also means that
    the missing data must be handled appropriately.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在 describe 输出表中出现了 NaN 值。NaN，即非数字（Not a Number），在 DataFrame 中非常重要，因为它表示缺失或不可用的数据。pandas
    库能够读取包含缺失或不完整信息的数据源，既是一个福音，也是一个诅咒。许多其他库在遇到缺失信息时会直接无法导入或读取数据文件，而 pandas 能够读取数据也意味着必须妥善处理这些缺失数据。
- en: 'When looking at the output of the describe method, you should notice that the
    Jupyter notebook renders it in the same way as the original DataFrame that we
    read in using read_csv. There is a very good reason for this, as the results returned
    by the describe method are themselves a pandas DataFrame and thus possess the
    same methods and characteristics as the data read in from the CSV file. This can
    be easily verified using Python''s built-in type function, as in the following
    code:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看describe方法的输出时，你会注意到Jupyter笔记本将其呈现为与我们通过read_csv读取的原始DataFrame相同的方式。这样做是有充分理由的，因为describe方法返回的结果本身就是一个pandas
    DataFrame，因此它具备与从CSV文件读取的数据相同的方法和特征。可以使用Python内置的type函数轻松验证这一点，如以下代码所示：
- en: type(df.describe(include='all'))
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: type(df.describe(include='all'))
- en: 'The output will be as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: pandas.core.frame.DataFrame
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: pandas.core.frame.DataFrame
- en: Now that we have a summary of the dataset, let's dive in with a little more
    detail to get a better understanding of the available data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了数据集的摘要，让我们深入研究一下，以便更好地理解可用数据。
- en: Note
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A comprehensive understanding of the available data is critical in any supervised
    learning problem. The source and type of the data, the means by which it is collected,
    and any errors potentially resulting from the collection process all have an effect
    on the performance of the final model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对可用数据的全面理解对于任何监督学习问题都至关重要。数据的来源和类型、收集的方式，以及可能由于收集过程中的错误所导致的问题，都对最终模型的性能产生影响。
- en: Hopefully, by now, you are comfortable with using pandas to provide a high-level
    overview of the data. We will now spend some time looking into the data in greater detail.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在为止，你已经习惯使用pandas提供数据的高级概览。接下来，我们将花一些时间更深入地研究数据。
- en: 'Exercise 1.04: Using the Aggregate Method'
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习1.04：使用聚合方法
- en: 'We have already seen how we can index or select rows or columns from a DataFrame
    and use advanced indexing techniques to filter the available data based on specific
    criteria. Another handy method that allows for such selection is the groupby method,
    which provides a quick method for selecting groups of data at a time and provides
    additional functionality through the DataFrameGroupBy object. This exercise is
    a continuation of Exercise 1.01, Loading and Summarizing the Titanic Dataset:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何索引或选择DataFrame中的行或列，并使用高级索引技术根据特定标准过滤可用数据。另一个方便的选择方法是groupby方法，它提供了一种快速选择一组数据的方法，并通过DataFrameGroupBy对象提供额外的功能。本练习是练习1.01《加载并总结泰坦尼克号数据集》的延续：
- en: 'Use the groupby method to group the data under the Embarked column to find
    out how many different values for Embarked there are:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用groupby方法对Embarked列下的数据进行分组，以找出Embarked有多少种不同的值：
- en: embarked_grouped = df.groupby('Embarked')
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: embarked_grouped = df.groupby('Embarked')
- en: print(f'There are {len(embarked_grouped)} Embarked groups')
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: print(f'共有{len(embarked_grouped)}个Embarked组')
- en: 'The output will be as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: There are 3 Embarked groups
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 共有3个Embarked组
- en: 'Display the output of embarked_grouped.groups to find what the groupby method
    actually does:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 显示embarked_grouped.groups的输出，以查看groupby方法实际执行了什么操作：
- en: embarked_grouped.groups
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: embarked_grouped.groups
- en: 'The output will be as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.16: Output of embarked_grouped.groups'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.16：embarked_grouped.groups的输出'
- en: '](img/image-PM7LCIIW.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-PM7LCIIW.jpg)'
- en: 'Figure 1.16: Output of embarked_grouped.groups'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16：embarked_grouped.groups的输出
- en: We can see here that the three groups are C, Q, and S, and that embarked_grouped.groups
    is actually a dictionary where the keys are the groups. The values are the rows
    or indexes of the entries that belong to that group.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这三个组分别是C、Q和S，并且embarked_grouped.groups实际上是一个字典，其中键是组，值是属于该组的行或索引。
- en: 'Use the iloc method to inspect row 1 and confirm that it belongs to embarked
    group C:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 使用iloc方法检查第1行，并确认它属于Embarked组C：
- en: df.iloc[1]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: df.iloc[1]
- en: 'The output will be as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.17: Inspecting row 1'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.17：检查第1行'
- en: '](img/image-R9TRRNJC.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-R9TRRNJC.jpg)'
- en: 'Figure 1.17: Inspecting row 1'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17：检查第1行
- en: 'As the groups are a dictionary, we can iterate through them and execute computations
    on the individual groups. Compute the mean age for each group, as shown here:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些组是字典类型，我们可以遍历它们并对每个单独的组执行计算。计算每个组的平均年龄，如下所示：
- en: 'for name, group in embarked_grouped:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于name, group在embarked_grouped中：
- en: print(name, group.Age.mean())
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: print(name, group.Age.mean())
- en: 'The output will be as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: C 32.33216981132075
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: C 32.33216981132075
- en: Q 28.63
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Q 28.63
- en: S 29.245204603580564
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: S 29.245204603580564
- en: 'Another option is to use the aggregate method, or agg for short, and provide
    it with the function to apply across the columns. Use the agg method to determine
    the mean of each group:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用aggregate方法，简称agg，并提供一个函数来应用于各列。使用agg方法来确定每个组的均值：
- en: embarked_grouped.agg(np.mean)
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: embarked_grouped.agg(np.mean)
- en: 'The output will be as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.18: Using the agg method'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.18：使用agg方法](img/image-SIFV6CSB.jpg)'
- en: '](img/image-0S7SC7MC.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-0S7SC7MC.jpg)'
- en: 'Figure 1.18: Using the agg method'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18：使用agg方法
- en: So, how exactly does agg work and what type of functions can we pass it? Before
    we can answer these questions, we need to first consider the data type of each
    column in the DataFrame, as each column is passed through this function to produce
    the result we see here. Each DataFrame comprises a collection of columns of pandas
    series data, which, in many ways, operates just like a list. As such, any function
    that can take a list or a similar iterable and compute a single value as a result
    can be used with agg.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，agg究竟是如何工作的，我们可以传递什么类型的函数给它呢？在回答这些问题之前，我们首先需要考虑DataFrame中每一列的数据类型，因为每一列都会通过此函数产生我们所看到的结果。每个DataFrame由一组pandas序列数据的列组成，这在许多方面类似于一个列表。因此，任何可以接受列表或类似可迭代对象并计算出单一结果值的函数，都可以与agg一起使用。
- en: 'Define a simple function that returns the first value in the column and then
    pass that function through to agg, as an example:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个简单的函数，返回列中的第一个值，然后将该函数传递给agg，作为示例：
- en: 'def first_val(x):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 'def first_val(x):'
- en: return x.values[0]
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: return x.values[0]
- en: embarked_grouped.agg(first_val)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: embarked_grouped.agg(first_val)
- en: 'The output will be as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.19: Using the .agg method with a function'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.19：使用.agg方法与函数](img/image-SIFV6CSB.jpg)'
- en: '](img/image-SIFV6CSB.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-0S7SC7MC.jpg)'
- en: 'Figure 1.19: Using the .agg method with a function'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.19：使用.agg方法与函数
- en: Note
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2NlEkgM.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 https://packt.live/2NlEkgM。
- en: You can also run this example online at https://packt.live/2AZnq51\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/2AZnq51 上在线运行此示例。你必须执行整个Notebook才能获得预期的结果。
- en: In this exercise, we have seen how to group data within a DataFrame, which then
    allows additional functions to be applied using .agg(), such as to calculate group
    means. These sorts of operations are extremely common in analyzing and preparing
    data for analysis.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们展示了如何在DataFrame中对数据进行分组，从而允许使用.agg()应用额外的函数，比如计算组的均值。这类操作在分析和准备数据时非常常见。
- en: Quantiles
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分位数
- en: The previous exercise demonstrated how to find the mean. In statistical data
    analysis, we are also often interested in knowing the value in a dataset below
    or above which a certain fraction of the points lie. Such points are called quantiles.
    For example, if we had a sequence of numbers from 1 to 10,001, the quantile for
    25% is the value 2,501\. That is, at the value 2,501, 25% of our data lies below
    that cutoff. Quantiles are often used in data visualizations because they convey
    a sense of the distribution of the data. In particular, the standard boxplot in
    Matplotlib draws a box bounded by the first and third of 4 quantiles.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个练习展示了如何找到均值。在统计数据分析中，我们也经常感兴趣的是数据集中某个值以下或以上，某个比例的点会落在其中。这些点被称为分位数。例如，如果我们有一个从1到10,001的数字序列，25%的分位数值为2,501。也就是说，在值2,501处，25%的数据位于该截止值以下。分位数在数据可视化中经常使用，因为它们能传达数据分布的感觉。特别是，Matplotlib中的标准箱线图会绘制一个由四个分位数中的第一和第三个分位数界定的箱体。
- en: 'For example, let''s establish the 25% quantile of the following dataframe:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们建立以下DataFrame的25%分位数：
- en: import pandas as pd
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: df = pd.DataFrame({"A":[1, 6, 9, 9]})
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: df = pd.DataFrame({"A":[1, 6, 9, 9]})
- en: '#calculate the 25% quantile over the dataframe'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '#计算DataFrame的25%分位数'
- en: df.quantile(0.25, axis = 0)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: df.quantile(0.25, axis = 0)
- en: 'The output will be as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: A 4.75
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: A 4.75
- en: 'Name: 0.25, dtype: float64'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 'Name: 0.25, dtype: float64'
- en: As you can see from the preceding output, 4.75 is the 25% quantile value for
    the DataFrame.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中可以看出，4.75是DataFrame的25%分位数值。
- en: Note
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on quantile methods, refer to https://pandas.pydata.org/pandas-docs/stable/reference/frame.html.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 有关分位数方法的更多信息，请参考 https://pandas.pydata.org/pandas-docs/stable/reference/frame.html。
- en: Later in this book, we'll use the idea of quantiles as we explore the data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 本书稍后我们将继续使用分位数的概念，深入探讨数据。
- en: Lambda Functions
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Lambda函数
- en: One common and useful way of implementing agg is through the use of Lambda functions.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 实现 agg 的一种常见且有用的方法是通过使用 Lambda 函数。
- en: 'Lambda, or anonymous, functions (also known as inline functions in other languages)
    are small, single-expression functions that can be declared and used without the
    need for a formal function definition via the use of the def keyword. Lambda functions
    are essentially provided for convenience and aren''t intended to be used for extensive
    periods. The main benefit of Lambda functions is that they can be used in places
    where a function might not be appropriate or convenient, such as inside other
    expressions or function calls. The standard syntax for a Lambda function is as
    follows (always starting with the lambda keyword):'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 或匿名函数（在其他语言中也称为内联函数）是小型的单表达式函数，可以声明并使用，而无需通过 def 关键字进行正式的函数定义。Lambda
    函数本质上是为了方便而提供的，通常不用于长时间的复杂任务。Lambda 函数的主要优点是它们可以在不适合或不方便定义函数的地方使用，例如在其他表达式或函数调用中。Lambda
    函数的标准语法如下（总是以 lambda 关键字开头）：
- en: 'lambda <input values>: <computation for values to be returned>'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 'lambda <输入值>: <计算返回的值>'
- en: Let's now do an exercise and create some interesting Lambda functions.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来做一个练习，创建一些有趣的 Lambda 函数。
- en: 'Exercise 1.05: Creating Lambda Functions'
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 1.05：创建 Lambda 函数
- en: 'In this exercise, we will create a Lambda function that returns the first value
    in a column and use it with agg. This exercise is a continuation of Exercise 1.01,
    Loading and Summarizing the Titanic Dataset:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将创建一个 Lambda 函数，返回列中的第一个值，并与 agg 一起使用。本练习是练习 1.01“加载和汇总泰坦尼克数据集”的延续：
- en: 'Write the first_val function as a Lambda function, passed to agg:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 将 first_val 函数写为 Lambda 函数，并传递给 agg：
- en: embarked_grouped = df.groupby('Embarked')
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: embarked_grouped = df.groupby('Embarked')
- en: 'embarked_grouped.agg(lambda x: x.values[0])'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 'embarked_grouped.agg(lambda x: x.values[0])'
- en: 'The output will be as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.20: Using the agg method with a Lambda function'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.20：使用 agg 方法和 Lambda 函数'
- en: '](img/image-888JAY5N.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-888JAY5N.jpg)'
- en: 'Figure 1.20: Using the agg method with a Lambda function'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20：使用 agg 方法和 Lambda 函数
- en: Obviously, we get the same result, but notice how much more convenient the Lambda
    function was to use, especially given the fact that it is only intended to be
    used briefly.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们得到相同的结果，但请注意，Lambda 函数的使用更加方便，尤其是考虑到它仅仅是暂时使用的。
- en: 'We can also pass multiple functions to agg via a list to apply the functions
    across the dataset. Pass the Lambda function as well as the NumPy mean and standard
    deviation functions, like this:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过列表将多个函数传递给 agg，从而在数据集上应用这些函数。传递 Lambda 函数以及 NumPy 的均值和标准差函数，如下所示：
- en: 'embarked_grouped.agg([lambda x: x.values[0], np.mean, np.std])'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 'embarked_grouped.agg([lambda x: x.values[0], np.mean, np.std])'
- en: 'The output will be as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.21: Using the agg method with multiple Lambda functions'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.21：使用 agg 方法和多个 Lambda 函数'
- en: '](img/image-JXL36A0N.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-JXL36A0N.jpg)'
- en: 'Figure 1.21: Using the agg method with multiple Lambda functions'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21：使用 agg 方法和多个 Lambda 函数
- en: 'Apply numpy.sum to the Fare column and the Lambda function to the Age column
    by passing agg a dictionary where the keys are the columns to apply the function
    to, and the values are the functions themselves to be able to apply different
    functions to different columns in the DataFrame:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 将 numpy.sum 应用于 Fare 列，并将 Lambda 函数应用于 Age 列，通过传递一个包含列名（作为字典的键）和相应函数（作为字典的值）的字典给
    agg，从而使得可以对 DataFrame 中的不同列应用不同的函数：
- en: 'embarked_grouped.agg({''Fare'': np.sum, \'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 'embarked_grouped.agg({''Fare'': np.sum, \'
- en: '''Age'': lambda x: x.values[0]})'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '''Age'': lambda x: x.values[0]})'
- en: 'The output will be as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.22: Using the agg method with a dictionary of different columns'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.22：使用 agg 方法和包含不同列的字典'
- en: '](img/image-ZL21TYBZ.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-ZL21TYBZ.jpg)'
- en: 'Figure 1.22: Using the agg method with a dictionary of different columns'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22：使用 agg 方法和包含不同列的字典
- en: 'Finally, execute the groupby method using more than one column. Provide the
    method with a list of the columns (Sex and Embarked) to groupby, like this:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用多个列执行 groupby 方法。提供包含列列表（性别和登船港口）以进行分组，如下所示：
- en: age_embarked_grouped = df.groupby(['Sex', 'Embarked'])
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: age_embarked_grouped = df.groupby(['Sex', 'Embarked'])
- en: age_embarked_grouped.groups
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: age_embarked_grouped.groups
- en: 'The output will be as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.23: Using the groupby method with more than one column'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.23：使用 groupby 方法和多个列'
- en: '](img/image-H8XDKHM7.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-H8XDKHM7.jpg)'
- en: 'Figure 1.23: Using the groupby method with more than one column'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.23：使用 groupby 方法进行多列分组
- en: Similar to when the groupings were computed by just the Embarked column, we
    can see here that a dictionary is returned where the keys are the combination
    of the Sex and Embarked columns returned as a tuple. The first key-value pair
    in the dictionary is a tuple, ('Male', 'S'), and the values correspond to the
    indices of rows with that specific combination. There will be a key-value pair
    for each combination of unique values in the Sex and Embarked columns.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅通过 Embarked 列进行分组计算时类似，我们可以看到这里返回了一个字典，其中键是 Sex 和 Embarked 列的组合，以元组形式返回。字典中的第一个键值对是元组
    ('Male', 'S')，值对应于具有该特定组合的行索引。每个 Sex 和 Embarked 列中唯一值的组合都会有一个键值对。
- en: Note
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2B1jAZl.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此特定部分的源代码，请参考 https://packt.live/2B1jAZl。
- en: You can also run this example online at https://packt.live/3emqwPe. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/3emqwPe 上在线运行这个示例。你必须执行整个 Notebook 才能获得期望的结果。
- en: This concludes our brief exploration of data inspection and manipulation. We
    now move on to one of the most important topics in data science, data quality.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对数据检查和处理的简要探索的结束。接下来，我们将进入数据科学中最重要的话题之一——数据质量。
- en: Data Quality Considerations
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据质量考虑
- en: The quality of data used in any machine learning problem, supervised or unsupervised,
    is critical to the performance of the final model, and should be at the forefront
    when planning any machine learning project. As a simple rule of thumb, if you
    have clean data, in sufficient quantity, with a good correlation between the input
    data type and the desired output, then the specifics regarding the type and details
    of the selected supervised learning model become significantly less important
    in achieving a good result.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习问题中，无论是监督学习还是无监督学习，所使用数据的质量对最终模型的表现至关重要，因此在规划任何机器学习项目时，数据质量应处于首要位置。作为一个简单的经验法则，如果你拥有干净的数据、足够的数量，并且输入数据类型与期望输出之间有良好的相关性，那么选用哪种类型的监督学习模型及其具体细节对获得良好的结果变得不那么重要。
- en: In reality, however, this is rarely the case. There are usually some issues
    regarding the quantity of available data, the quality or signal-to-noise ratio
    in the data, the correlation between the input and output, or some combination
    of all three factors. As such, we will use this last section of this chapter to
    consider some of the data quality problems that may occur and some mechanisms
    for addressing them. Previously, we mentioned that in any machine learning problem,
    having a thorough understanding of the dataset is critical if we are to construct
    a high-performing model.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际上这种情况很少发生。通常，关于数据量、数据质量或信噪比、输入与输出之间的相关性，或三者的某种组合，都会存在一些问题。因此，我们将在本章的最后一部分考虑可能出现的一些数据质量问题以及解决这些问题的一些机制。我们之前提到，在任何机器学习问题中，深入了解数据集对于构建高性能模型至关重要。
- en: This is particularly the case when looking into data quality and attempting
    to address some of the issues present within the data. Without a comprehensive
    understanding of the dataset, additional noise or other unintended issues may
    be introduced during the data cleaning process, leading to further degradation
    of performance.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究数据质量并尝试解决数据中存在的一些问题时，尤其如此。如果没有对数据集的全面理解，数据清理过程中可能会引入额外的噪声或其他未预见的问题，从而导致性能进一步下降。
- en: Note
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed description of the Titanic dataset and the type of data included
    is contained in the Loading Data in pandas section. If you need a quick refresher,
    go back and review that section now.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: Titanic 数据集的详细描述和其中包含的数据类型可以在 pandas 中的加载数据部分找到。如果你需要快速回顾，可以回去复习那部分内容。
- en: Managing Missing Data
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 管理缺失数据
- en: 'As we discussed earlier, the ability of pandas to read data with missing values
    is both a blessing and a curse and, arguably, is the most common issue that needs
    to be managed before we can continue with developing our supervised learning model.
    The simplest, but not necessarily the most effective, method is to just remove
    or ignore those entries that are missing data. We can easily do this in pandas
    using the dropna method on the DataFrame:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，pandas 读取带有缺失值的数据的能力既是一种福音也是一种诅咒，可以说是在继续开发我们的监督学习模型之前需要解决的最常见问题。最简单但不一定最有效的方法是简单地删除或忽略那些缺失数据的条目。我们可以在
    pandas 中轻松地使用 DataFrame 上的 dropna 方法来实现这一点：
- en: complete_data = df.dropna()
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 完整数据 = df.dropna()
- en: 'There is one very significant consequence of simply dropping rows with missing
    data and that is we may be throwing away a lot of important information. This
    is highlighted very clearly in the Titanic dataset as a lot of rows contain missing
    data. If we were to simply ignore these rows, we would start with a sample size
    of 1,309 and end with a sample of 183 entries. Developing a reasonable supervised
    learning model with a little over 10% of the data would be very difficult indeed.
    The following code displays the use of the dropna() method to handle the missing
    entries:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅删除具有缺失数据的行有一个非常重要的后果，那就是我们可能会丢失很多重要信息。这在泰坦尼克号数据集中非常明显，因为很多行包含缺失数据。如果我们简单地忽略这些行，我们将从
    1,309 个样本开始，最终只剩下 183 个条目。使用略多于 10% 的数据开发一个合理的监督学习模型将非常困难。以下代码显示了使用 dropna() 方法处理缺失条目的示例：
- en: len(df)
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: len(df)
- en: 'The preceding input produces the following output:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输入产生以下输出：
- en: '1309'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '1309'
- en: 'The dropna() method is implemented as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: dropna() 方法的实现如下：
- en: len(df.dropna())
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: len(df.dropna())
- en: 'The preceding input produces the following output:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输入产生以下输出：
- en: '183'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '183'
- en: 'So, with the exception of the early, explorative phase, it is rarely acceptable
    to simply discard all rows with invalid information. We can identify which rows
    are actually missing information and whether the missing information is a problem
    unique to certain columns or is consistent throughout all columns of the dataset.
    We can use aggregate to help us here as well:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了早期的探索阶段外，简单丢弃所有具有无效信息的行通常是不可接受的。我们可以确定哪些行实际上缺少信息，以及缺失信息是某些列特有的问题还是整个数据集中所有列都存在的问题。我们也可以使用
    aggregate 来帮助我们：
- en: 'df.aggregate(lambda x: x.isna().sum())'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 'df.aggregate(lambda x: x.isna().sum())'
- en: 'The output will be as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: Cabin 1014
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 船舱 1014
- en: Embarked 2
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 登船港口 2
- en: Fare 1
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 票价 1
- en: Pclass 0
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 客舱等级 0
- en: Ticket 0
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 票号 0
- en: Age 263
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄 263
- en: Name 0
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 0
- en: Parch 0
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 父母/子女 0
- en: Sex 0
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 性别 0
- en: SibSp 0
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 兄弟姐妹/配偶 0
- en: Survived 418
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 生还 418
- en: 'dtype: int64'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 类型：int64
- en: Now, this is useful! We can see that the vast majority of missing information
    is in the Cabin column, some in Age, and a little more in Survived. This is one
    of the first times in the data cleaning process that we may need to make an educated
    judgment call.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这很有用！我们可以看到绝大多数缺失信息在船舱列中，一些在年龄中，还有一些在生还中。这是数据清洗过程中我们可能需要做出明智判断的首次之一。
- en: What do we want to do with the Cabin column? There is so much missing information
    here that, in fact, it may not be possible to use it in any reasonable way. We
    could attempt to recover the information by looking at the names, ages, and number
    of parents/siblings and see whether we can match some families together to provide
    information, but there would be a lot of uncertainty in this process. We could
    also simplify the column by using the level of the cabin on the ship rather than
    the exact cabin number, which may then correlate better with name, age, and social
    status. This is unfortunate as there could be a good correlation between Cabin
    and Survived, as perhaps those passengers in the lower decks of the ship may have
    had a harder time evacuating. We could examine only the rows with valid Cabin
    values to see whether there is any predictive power in the Cabin entry; but, for
    now, we will simply disregard Cabin as a reasonable input (or feature).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 对于船舱列我们想要做什么？这里缺失的信息太多了，事实上，可能无法以任何合理的方式使用它。我们可以尝试通过查看姓名、年龄和父母/兄弟姐妹的数量来恢复信息，看看是否可以将一些家庭匹配在一起提供信息，但在这个过程中会有很多不确定性。我们也可以简化列，使用船上舱位的级别而不是确切的舱位号，这样可能会更好地与姓名、年龄和社会地位相关联。这很不幸，因为船舱和生还之间可能存在很好的相关性，也许那些在船的下层甲板上的乘客可能更难撤离。我们可以仅检查具有有效船舱值的行，看看舱位信息是否具有任何预测能力；但是，目前，我们将简单地将船舱视为一个合理的输入（或特征）而忽略。
- en: 'We can see that the Embarked and Fare columns only have three missing samples
    between them. If we decided that we needed the Embarked and Fare columns for our
    model, it would be a reasonable argument to simply drop these rows. We can do
    this using our indexing techniques, where ~ represents the not operation, or flipping
    the result (that is, where df.Embarked is not NaN and df.Fare is not NaN):'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Embarked 和 Fare 列之间只有三条缺失样本。如果我们决定模型需要这两列，那么直接删除这些行是一个合理的选择。我们可以使用索引技巧来实现这一点，其中
    ~ 代表取反操作，或者反转结果（也就是说，当 df.Embarked 和 df.Fare 不为 NaN 时）：
- en: df_valid = df.loc[(~df.Embarked.isna()) & (~df.Fare.isna())]
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid = df.loc[(~df.Embarked.isna()) & (~df.Fare.isna())]
- en: 'The missing age values are a little more interesting, as there are too many
    rows with missing age values to just discard them. But we also have a few more
    options here, as we can have a little more confidence in some plausible values
    to fill in. The simplest option would be to simply fill in the missing age values
    with the mean age for the dataset:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失的年龄值更为有趣，因为有太多缺失年龄值的行，无法简单地将它们丢弃。但我们在这里有更多的选项，因为我们可以对一些合理的值有更高的信心。最简单的选择是直接用数据集的平均年龄填充缺失的年龄值：
- en: df_valid[['Age']] = df_valid[['Age']]\
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid[['Age']] = df_valid[['Age']]\
- en: .fillna(df_valid.Age.mean())
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: .fillna(df_valid.Age.mean())
- en: 'This is okay, but there are probably better ways of filling in the data rather
    than just giving all 263 people the same value. Remember, we are trying to clean
    up the data with the goal of maximizing the predictive power of the input features
    and the survival rate. Giving everyone the same value, while simple, doesn''t
    seem too reasonable. What if we were to look at the average ages of the members
    of each of the classes (Pclass)? This may give a better estimate, as the average
    age reduces from class 1 through 3, as you can see in the following code:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这样是可以的，但可能有更好的填充数据的方式，而不仅仅是给所有 263 个人赋相同的值。记住，我们的目标是清理数据，以最大化输入特征和生存率的预测能力。虽然简单，但让每个人都有相同的值似乎不太合理。如果我们查看每个票务等级（Pclass）成员的平均年龄呢？这可能会提供一个更好的估计，因为随着票务等级从
    1 到 3 下降，平均年龄也逐渐降低，以下代码可以看到这一点：
- en: df_valid.loc[df.Pclass == 1, 'Age'].mean()
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid.loc[df.Pclass == 1, 'Age'].mean()
- en: 'The preceding input produces the following output:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输入会产生以下输出：
- en: '37.956806510096975'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '37.956806510096975'
- en: 'Average age for class 2 is as follows:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 第二等级的平均年龄如下：
- en: df_valid.loc[df.Pclass == 2, 'Age'].mean()
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid.loc[df.Pclass == 2, 'Age'].mean()
- en: 'The preceding input produces the following output:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输入会产生以下输出：
- en: '29.52440879717283'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '29.52440879717283'
- en: 'Average age for class 3 is as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 第三等级的平均年龄如下：
- en: df_valid.loc[df.Pclass == 3, 'Age'].mean()
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid.loc[df.Pclass == 3, 'Age'].mean()
- en: 'The preceding input produces the following output:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输入会产生以下输出：
- en: '26.23396338788047'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '26.23396338788047'
- en: 'What if we were to consider the sex of the person as well as ticket class (social
    status)? Do the average ages differ here too? Let''s find out:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们同时考虑人的性别和票务等级（社会地位）呢？这里的平均年龄也会有所不同吗？让我们来探讨一下：
- en: 'for name, grp in df_valid.groupby([''Pclass'', ''Sex'']):'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 'for name, grp in df_valid.groupby([''Pclass'', ''Sex'']):'
- en: print('%i' % name[0], name[1], '%0.2f' % grp['Age'].mean())
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: print('%i' % name[0], name[1], '%0.2f' % grp['Age'].mean())
- en: 'The output will be as follows:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: 1 female 36.84
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 1 女性 36.84
- en: 1 male 41.03
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 1 男性 41.03
- en: 2 female 27.50
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 2 女性 27.50
- en: 2 male 30.82
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 2 男性 30.82
- en: 3 female 22.19
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 3 女性 22.19
- en: 3 male 25.86
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 3 男性 25.86
- en: 'We can see here that males in all ticket classes are typically older. This
    combination of sex and ticket class provides much more resolution than simply
    filling in all missing fields with the mean age. To do this, we will use the transform
    method, which applies a function to the contents of a series or DataFrame and
    returns another series or DataFrame with the transformed values. This is particularly
    powerful when combined with the groupby method:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，所有票务等级中的男性通常年龄较大。性别和票务等级的组合提供的信息比仅仅用平均年龄填充缺失值要丰富得多。为此，我们将使用 transform
    方法，它可以对 Series 或 DataFrame 的内容应用一个函数，并返回另一个具有变换值的 Series 或 DataFrame。当与 groupby
    方法结合使用时，这非常强大：
- en: mean_ages = df_valid.groupby(['Pclass', 'Sex'])['Age'].\
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: mean_ages = df_valid.groupby(['Pclass', 'Sex'])['Age'].\
- en: 'transform(lambda x: \'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 'transform(lambda x: \'
- en: x.fillna(x.mean()))
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: x.fillna(x.mean()))
- en: df_valid.loc[:, 'Age'] = mean_ages
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid.loc[:, 'Age'] = mean_ages
- en: 'There is a lot in these two lines of code, so let''s break them down into components.
    Let''s look at the first line:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行代码包含了很多内容，我们来逐步解析。首先看看第一行：
- en: mean_ages = df_valid.groupby(['Pclass', 'Sex'])['Age'].\
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: mean_ages = df_valid.groupby(['Pclass', 'Sex'])['Age'].\
- en: 'transform(lambda x: \'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 'transform(lambda x: \'
- en: x.fillna(x.mean()))
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: x.fillna(x.mean()))
- en: 'We are already familiar with df_valid.groupby([''Pclass'', ''Sex''])[''Age''],
    which groups the data by ticket class and sex and returns only the Age column.
    The lambda x: x.fillna(x.mean()) Lambda function takes the input pandas series
    and fills the NaN values with the mean value of the series.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经熟悉df_valid.groupby([''Pclass'', ''Sex''])[''Age'']，它根据票务等级和性别对数据进行分组，并返回仅包含Age列的数据。lambda
    x: x.fillna(x.mean()) Lambda函数将输入的pandas系列填充NaN值为该系列的均值。'
- en: 'The second line assigns the filled values within mean_ages to the Age column.
    Note the use of the loc[:, ''Age''] indexing method, which indicates that all
    rows within the Age column are to be assigned the values contained within mean_ages:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行将mean_ages中的填充值分配给Age列。注意使用了loc[:, 'Age']索引方法，表示将Age列中的所有行赋值为mean_ages中的值：
- en: df_valid.loc[:, 'Age'] = mean_ages
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: df_valid.loc[:, 'Age'] = mean_ages
- en: 'We have described a few different ways of filling in the missing values within
    the Age column, but by no means has this been an exhaustive discussion. There
    are many more methods that we could use to fill the missing data: we could apply
    random values within one standard deviation of the mean for the grouped data,
    and we could also look at grouping the data by sex and the number of parents/children
    (Parch) or by the number of siblings, or by ticket class, sex, and the number
    of parents/children. What is most important about the decisions made during this
    process is the end result of the prediction accuracy. We may need to try different
    options, rerun our models, and consider the effect on the accuracy of final predictions.
    Thus, selecting the features or components that provide the model with the most
    predictive power. You will find that, during this process, you will try a few
    different features, run the model, look at the end result, and repeat this process
    until you are happy with the performance.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经描述了几种填充Age列缺失值的方法，但这绝不是一个详尽的讨论。我们可以使用更多的方法来填充缺失数据：我们可以在分组数据的均值的一个标准差范围内应用随机值，也可以考虑根据性别、父母/子女数量（Parch）或兄弟姐妹数量，甚至是票务等级、性别和父母/子女数量来对数据进行分组。关于此过程中做出的决策，最重要的是最终的预测准确性。我们可能需要尝试不同的选项，重新运行模型，并考虑对最终预测准确性的影响。因此，选择能为模型提供最大预测能力的特征或组件。你会发现，在这个过程中，你会尝试不同的特征，运行模型，查看最终结果，然后重复这个过程，直到你对性能感到满意。
- en: The ultimate goal of this supervised learning problem is to predict the survival
    of passengers on the Titanic given the information we have available. So, that
    means that the Survived column provides our labels for training. What are we going
    to do if we are missing 418 of the labels? If this was a project where we had
    control over the collection of the data and access to its origins, we would obviously
    correct this by recollecting or asking for the labels to be clarified. With the
    Titanic dataset, we do not have this ability so we must make another educated
    judgment call. One approach would be to drop those rows from the training data,
    and later use a model trained on the (smaller) training set to predict the outcome
    for the others (this is, in fact, the task given in the Kaggle Titanic competition).
    In some business problems, we may not have the option of simply ignoring these
    rows; we might be trying to predict future outcomes of a very critical process
    and this data is all we have. We could try some unsupervised learning techniques
    to see whether there are some patterns in the survival information that we could
    use. However, by estimating the ground truth labels by means of unsupervised techniques,
    we may introduce significant noise into the dataset, reducing our ability to accurately
    predict survival.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这个监督学习问题的最终目标是根据我们所掌握的信息预测泰坦尼克号乘客的生存情况。所以，这意味着“是否生存”这一列提供了我们训练的标签。如果我们缺少418个标签该怎么办？如果这是一个我们可以控制数据收集并访问数据来源的项目，我们显然可以通过重新收集数据或请求明确标签来进行修正。但在泰坦尼克号数据集中，我们没有这种能力，因此必须做出另一个有根据的判断。一个方法是从训练数据中删除这些行，之后用在（更小的）训练集上训练的模型来预测其他人的结果（这实际上是Kaggle泰坦尼克号比赛中的任务）。在某些商业问题中，我们可能没有简单忽略这些行的选项；我们可能在尝试预测一个非常关键过程的未来结果，而这些数据就是我们所拥有的。我们可以尝试一些无监督学习技术，看看是否能发现一些生存信息的模式以供使用。然而，通过无监督技术来估计真实标签，可能会给数据集引入显著的噪声，从而降低我们准确预测生存情况的能力。
- en: Class Imbalance
  id: totrans-436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 类别不平衡
- en: 'Missing data is not the only problem that may be present within a dataset.
    Class imbalance – that is, having more of one class or classes compared to another
    – can be a significant problem, particularly in the case of classification problems
    (we''ll see more on classification in Chapter 5, Classification Techniques), where
    we are trying to predict which class (or classes) a sample is from. Looking at
    our Survived column, we can see that there are far more people who perished (Survived
    equals 0) than survived (Survived equals 1) in the dataset, as you can see in
    the following code:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据并不是数据集中可能存在的唯一问题。类别不平衡——即一个类别或多个类别的样本数量大大超过其他类别——可能是一个显著问题，特别是在分类问题中（我们将在第五章“分类技术”中深入讨论分类问题），即我们试图预测一个样本属于哪个类别。通过查看我们的“Survived”列，可以看到数据集中死亡的人数（Survived等于0）远多于生还的人数（Survived等于1），如下代码所示：
- en: len(df.loc[df.Survived ==1])
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: len(df.loc[df.Survived ==1])
- en: 'The output is as follows:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '342'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '342'
- en: 'The number of people who perished are:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 死亡人数为：
- en: len(df.loc[df.Survived ==0])
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: len(df.loc[df.Survived ==0])
- en: 'The output is as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '549'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '549'
- en: If we don't take this class imbalance into account, the predictive power of
    our model could be significantly reduced as, during training, the model would
    simply need to guess that the person did not survive to be correct 61% (549 /
    (549 + 342)) of the time. If, in reality, the actual survival rate was, say, 50%,
    then when being applied to unseen data, our model would predict did not survive
    too often.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不考虑这个类别不平衡问题，我们模型的预测能力可能会大大降低，因为在训练过程中，模型只需要猜测这个人没有生还，就可以正确预测61%的时间（549
    / (549 + 342)）。如果实际生还率是50%，那么在应用于未见数据时，我们的模型会过于频繁地预测“没有生还”。
- en: There are a few options available for managing class imbalance, one of which,
    similar to the missing data scenario, is to randomly remove samples from the over-represented
    class until balance has been achieved. Again, this option is not ideal, or perhaps
    even appropriate, as it involves ignoring available data. A more constructive
    example may be to oversample the under-represented class by randomly copying samples
    from the under-represented class in the dataset to boost the number of samples.
    While removing data can lead to accuracy issues due to discarding useful information,
    oversampling the under-represented class can lead to being unable to predict the
    label of unseen data, also known as overfitting (which we will cover in Chapter
    6, Ensemble Modeling).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 管理类别不平衡有几种可选方法，其中一种方法，类似于缺失数据场景，是随机删除过度代表类的样本，直到达到平衡为止。再次强调，这种方法并不理想，甚至可能不适当，因为它涉及忽略可用的数据。一个更具建设性的例子可能是通过随机复制数据集中不足代表类的样本来进行过采样，以增加样本数量。虽然删除数据可能导致由于丢失有用信息而产生准确性问题，但对不足代表类进行过采样可能会导致无法预测未见数据的标签，这也被称为过拟合（我们将在第六章“集成建模”中讨论）。
- en: 'Adding some random noise to the input features for oversampled data may prevent
    some degree of overfitting, but this is highly dependent on the dataset itself.
    As with missing data, it is important to check the effect of any class imbalance
    corrections on the overall model performance. It is relatively straightforward
    to copy more data into a DataFrame using the append method, which works in a very
    similar fashion to lists. If we wanted to copy the first row to the end of the
    DataFrame, we would do this:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 向过采样数据的输入特征中添加一些随机噪声可能有助于防止一定程度的过拟合，但这在很大程度上取决于数据集本身。与缺失数据一样，检查任何类别不平衡修正对整体模型性能的影响非常重要。使用append方法将更多数据复制到DataFrame中是相对简单的，append方法的工作方式与列表非常相似。如果我们想把第一行复制到DataFrame的末尾，可以这样做：
- en: df_oversample = df.append(df.iloc[0])
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: df_oversample = df.append(df.iloc[0])
- en: Low Sample Size
  id: totrans-449
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 样本量过小
- en: The field of machine learning can be considered a branch of the larger field
    of statistics. As such, the principles of confidence and sample size can also
    be applied to understand the issues with a small dataset. Recall that if we were
    to take measurements from a data source with high variance, then the degree of
    uncertainty in the measurements would also be high and more samples would be required
    to achieve a specified confidence in the value of the mean. The sample principles
    can be applied to machine learning datasets. Those datasets with a variance in
    the features with the most predictive power generally require more samples for
    reasonable performance as more confidence is also required.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域可以视为更大统计学领域的一个分支。因此，置信度和样本量的原理也可以用于理解小数据集的问题。回想一下，如果我们从一个高方差的数据源中获取测量数据，那么这些测量的
    不确定性程度也会很高，并且为了在均值的值上获得特定的置信度，我们需要更多的样本。样本原理可以应用于机器学习数据集。那些特征方差较大的数据集，通常需要更多的样本才能获得合理的性能，因为更高的置信度也是必需的。
- en: There are a few techniques that can be used to compensate for a reduced sample
    size, such as transfer learning. However, these lie outside the scope of this
    book. Ultimately, though, there is only so much that can be done with a small
    dataset, and significant performance increases may only occur once the sample
    size is increased.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术可以用来弥补样本量减少的问题，比如迁移学习。然而，这些技术超出了本书的范围。然而，最终来说，使用小数据集所能做的事有限，显著的性能提升可能只有在增加样本量后才会发生。
- en: 'Activity 1.01: Implementing Pandas Functions'
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 1.01：实现 Pandas 函数
- en: In this activity, we will test ourselves on the various pandas functions we
    have learned about in this chapter. We will use the same Titanic dataset for this.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将测试我们在本章中学到的各种 Pandas 函数。我们将使用相同的 Titanic 数据集进行练习。
- en: 'The steps to be performed are as follows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 执行的步骤如下：
- en: Open a new Jupyter notebook.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter 笔记本。
- en: Use pandas to load the Titanic dataset and use the head function on the dataset
    to display the top rows of the dataset. Describe the summary data for all columns.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pandas 加载 Titanic 数据集，并对数据集使用 `head` 函数以显示数据集的顶部行。描述所有列的总结数据。
- en: 'We don''t need the Unnamed: 0 column. In Exercise 1.03: Advanced Indexing and
    Selection, we demonstrated how to remove the column using the del command. How
    else could we remove this column? Remove this column without using del.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '我们不需要 `Unnamed: 0` 列。在练习 1.03: 高级索引和选择中，我们演示了如何使用 `del` 命令删除该列。还有其他方法可以删除此列吗？不使用
    `del` 删除此列。'
- en: Compute the mean, standard deviation, minimum, and maximum values for the columns
    of the DataFrame without using describe. Note that you can find the minimum and
    maximum values using the df.min() and df.max() functions.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 DataFrame 列的均值、标准差、最小值和最大值，而不使用 `describe`。注意，可以使用 `df.min()` 和 `df.max()`
    函数找到最小值和最大值。
- en: Use the quantile method to get values for the 33, 66, and 99% quantiles.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `quantile` 方法获取33%、66% 和 99%的分位数值。
- en: Find how many passengers were from each class using the groupby method.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `groupby` 方法找出每个舱位的乘客数量。
- en: Find how many passengers were from each class answer by using selecting/indexing
    methods to count the members of each class. You can use the unique() method to
    find out the unique values of each class.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择/索引方法计算每个舱位的乘客数量。可以使用 `unique()` 方法找出每个舱位的唯一值。
- en: Confirm that the answers to Step 6 and Step 7 match.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 确认第6步和第7步的答案是否匹配。
- en: Determine who the eldest passenger in third class was.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 确定第三舱中最年长的乘客是谁。
- en: For a number of machine learning problems, it is very common to scale the numerical
    values between 0 and 1\. Use the agg method with Lambda functions to scale the
    Fare and Age columns between 0 and 1.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多机器学习问题，常常需要将数值数据缩放到0和1之间。使用 `agg` 方法与 Lambda 函数将 Fare 和 Age 列的数据缩放到0和1之间。
- en: 'There is one individual in the dataset without a listed Fare value, which can
    be established as follows:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有一位个体没有列出票价值，可以通过以下方式确认：
- en: df_nan_fare = df.loc[(df.Fare.isna())]
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: df_nan_fare = df.loc[(df.Fare.isna())]
- en: df_nan_fare
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: df_nan_fare
- en: 'The output will be as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.24: Individual without a listed fare value'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.24：没有列出票价的个体'
- en: '](img/image-WVCTUMST.jpg)'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-WVCTUMST.jpg)'
- en: 'Figure 1.24: Individual without a listed fare value'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.24：没有列出票价的个体
- en: Replace the NaN value of this row in the main DataFrame with the mean Fare value
    for those corresponding to the same class and Embarked location using the groupby
    method.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 使用groupby方法，将主数据框中该行的NaN值替换为与该行对应的相同类别和登船地点的平均票价值。
- en: 'The output will be as follows:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 1.25: Output for the individual without listed fare details'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.25：没有列出票价详情的个人输出'
- en: '](img/image-H95O9GVO.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-H95O9GVO.jpg)'
- en: 'Figure 1.25: Output for the individual without listed fare details'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.25：没有列出票价详情的个人输出
- en: Note
  id: totrans-477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以通过此链接找到。
- en: With this activity, we have reviewed all the basic data loading, inspection,
    and manipulation methods, as well as some basic summary statistics methods.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本活动，我们回顾了所有基本的数据加载、检查和操作方法，以及一些基本的总结统计方法。
- en: Summary
  id: totrans-480
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the concept of supervised machine learning, along
    with a number of use cases, including the automation of manual tasks such as identifying
    hairstyles from the 1960s and 1980s. In this introduction, we encountered the
    concept of labeled datasets and the process of mapping one information set (the
    input data or features) to the corresponding labels. We took a practical approach
    to the process of loading and cleaning data using Jupyter notebooks and the extremely
    powerful pandas library. Note that this chapter has only covered a small fraction
    of the functionality within pandas, and that an entire book could be dedicated
    to the library itself. It is recommended that you become familiar with reading
    the pandas documentation and continue to develop your pandas skills through practice.
    The final section of this chapter covered a number of data quality issues that
    need to be considered to develop a high-performing supervised learning model,
    including missing data, class imbalance, and low sample sizes. We discussed a
    number of options for managing such issues and emphasized the importance of checking
    these mitigations against the performance of the model. In the next chapter, we
    will extend the data cleaning process that we covered and investigate the data
    exploration and visualization process. Data exploration is a critical aspect of
    any machine learning solution since without a comprehensive knowledge of the dataset,
    it would be almost impossible to model the information provided.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了监督学习的概念，并提供了一些应用案例，包括自动化手动任务，如识别1960年代和1980年代的发型。在此介绍中，我们遇到了标记数据集的概念，以及将一个信息集（输入数据或特征）映射到相应标签的过程。我们通过实践方法，使用Jupyter笔记本和功能强大的pandas库，讲解了数据加载和清理的过程。请注意，本章仅涵盖了pandas功能的一小部分，实际上，整个书籍都可以专门讲解这个库。建议您熟悉阅读pandas文档，并通过实践继续提升您的pandas技能。本章的最后部分讨论了在开发高性能监督学习模型时需要考虑的一些数据质量问题，包括缺失数据、类别不平衡和样本量过小等问题。我们讨论了管理这些问题的多种选择，并强调了将这些缓解措施与模型性能进行对比检查的重要性。在下一章中，我们将扩展本章所涉及的数据清理过程，并研究数据探索和可视化过程。数据探索是任何机器学习解决方案中的关键部分，因为如果没有对数据集的全面了解，几乎不可能对所提供的信息进行建模。
