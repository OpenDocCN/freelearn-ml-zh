- en: Performance Evaluation Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能评估方法
- en: Your method of performance evaluation will vary by the type of machine learning
    algorithm that you choose to implement. In general, there are different metrics
    that can potentially determine how well your model is performing at its given
    task for classification, regression, and unsupervised machine learning algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 性能评估的方法会根据你选择实现的机器学习算法类型有所不同。一般来说，针对分类、回归和无监督机器学习算法，会有不同的评估指标来衡量你的模型在特定任务上的表现。
- en: 'In this chapter, we will explore how the different performance evaluation methods
    can help you to better understand your model. The chapter will be split into three
    sections, as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索不同的性能评估方法，帮助你更好地理解模型。章节将分为以下三个部分：
- en: Performance evaluation for classification algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类算法的性能评估
- en: Performance evaluation for regression algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归算法的性能评估
- en: Performance evaluation for unsupervised algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督算法的性能评估
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, NumPy ≥ 1.15.1, Matplotlib ≥ 3.0.0, and Scikit-plot ≥ 0.3.7 installed
    on your system.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在系统中安装Python 3.6或更高版本、Pandas ≥ 0.23.4、Scikit-learn ≥ 0.20.0、NumPy ≥ 1.15.1、Matplotlib
    ≥ 3.0.0，以及Scikit-plot ≥ 0.3.7。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_08.ipynb)'
- en: 'Check out the following video to see the code in action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码的实际效果：
- en: '[http://bit.ly/2EY4nJU](http://bit.ly/2EY4nJU)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2EY4nJU](http://bit.ly/2EY4nJU)'
- en: Why is performance evaluation critical?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么性能评估至关重要？
- en: 'It is key for you to understand why we need to evaluate the performance of
    a model in the first place. Some of the potential reasons why performance evaluation
    is critical are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么我们首先需要评估模型的性能是非常关键的。以下是一些可能的原因，说明为什么性能评估至关重要：
- en: '**It prevents overfitting**:Overfitting occurs when your algorithm hugs the
    data too tightly and makes predictions that are specific to only one dataset.
    In other words, your model cannot generalize its predictions outside of the data
    that it was trained on.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它防止过拟合**：过拟合发生在算法过度拟合数据，并做出仅针对一个数据集的特定预测。换句话说，模型无法将预测推广到它未接触过的数据。'
- en: '**It prevents underfitting**:This is the exact opposite of overfitting. In
    this case, the model is very generic in nature.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它防止欠拟合**：这恰恰与过拟合相反。在这种情况下，模型的性质非常通用。'
- en: '**Understanding predictions**:Performance evaluation methods will help you
    to understand, in greater detail, how your model makes predictions, along with
    the nature of those predictions and other useful information, such as the accuracy
    of your model.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解预测**：性能评估方法将帮助你更详细地了解模型是如何做出预测的，以及这些预测的性质和其他有用信息，例如模型的准确性。'
- en: Performance evaluation for classification algorithms
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类算法的性能评估
- en: 'In order to evaluate the performance of classification, let''s consider the
    two classification algorithms that we have built in this book: k-nearest neighbors
    and logistic regression.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估分类算法的性能，我们可以考虑在本书中构建的两种分类算法：k最近邻和逻辑回归。
- en: 'The first step will be to implement both of these algorithms in the fraud detection
    dataset. We can do this by using the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将这两种算法实现到欺诈检测数据集中。我们可以通过以下代码来实现：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we read the fraud detection dataset into our notebook
    and split the data into the features and target variables, as usual. We then split
    the data into training and test sets, and build the k-nearest neighbors and logistic
    regression models in the training data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将欺诈检测数据集读取到笔记本中，并将数据分成特征和目标变量，和往常一样。然后，我们将数据分为训练集和测试集，并在训练数据中构建k最近邻和逻辑回归模型。
- en: 'In this section, you will learn how to evaluate the performance of a single
    model: k-nearest neighbors. You will also learn how to compare and contrast multiple
    models. Therefore, you will learn about the following things:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何评估单一模型的性能：k-最近邻。您还将学习如何比较和对比多个模型。因此，您将学习以下内容：
- en: Confusion matrix
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Normalized confusion matrix
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化混淆矩阵
- en: Area under the curve (`auc` score)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线下面积（`auc` 分数）
- en: Cumulative gains curve
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积增益曲线
- en: Lift curve
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升曲线
- en: K-S statistic plot
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-S 统计量图
- en: Calibration plot
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 校准图
- en: Learning curve
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习曲线
- en: Cross-validated box plot
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证箱线图
- en: Some of the visualizations in this section will require a package titled `scikit-plot`.
    The `scikit-plot` package is very effective, and it is used to visualize the various
    performance measures of machine learning models. It was specifically made for
    models that are built using scikit-learn.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的一些可视化图表将需要一个名为 `scikit-plot` 的包。`scikit-plot` 包非常有效，专门用于可视化机器学习模型的各种性能指标。它特别为使用
    scikit-learn 构建的模型而设计。
- en: 'In order to install `scikit-plot` on your local machine, using `pip` in Terminal,
    we use the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在本地机器上安装 `scikit-plot`，可以在终端使用 `pip` 安装，命令如下：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you are using the Anaconda distribution to manage your Python packages,
    you can install `scikit-plot` by using the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是 Anaconda 发行版来管理您的 Python 包，可以通过以下代码安装 `scikit-plot`：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The confusion matrix
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Until now, we have used the accuracy as the sole measure of model performance.
    That was fine, because we have a balanced dataset. A balanced dataset is a dataset
    in which there are almost the same numbers of labels for each category. In the
    dataset that we are working with, 8,000 labels belong to the fraudulent transactions,
    while 12,000 belong to the non-fraudulent transactions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们一直使用准确率作为唯一的模型性能衡量标准。这是可以的，因为我们有一个平衡的数据集。平衡数据集是指每个类别的标签数量几乎相等。在我们正在处理的数据集中，8,000个标签属于欺诈交易，12,000个标签属于非欺诈交易。
- en: Imagine a situation in which 90% of our data had non-fraudulent transactions,
    while only 10% of the transactions had fraudulent cases. If the classifier reported
    an accuracy of 90%, it wouldn't make sense, because most of the data that it has
    seen thus far were the non-fraudulent cases and it has seen very little of the
    fraudulent cases. So, even if it classified 90% of the cases accurately, it would
    mean that most of the cases that it classified would belong to the non-fraudulent
    cases. That would provide no value to us.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有这样一种情况：90%的数据是非欺诈交易，只有10%的交易是欺诈案件。如果分类器报告的准确率为90%，这就没有意义，因为它所看到的大部分数据是非欺诈案件，而它看到的欺诈案件非常少。所以，即使它准确地分类了90%的案件，也意味着它所分类的大多数案件将属于非欺诈案件。这对我们没有任何价值。
- en: 'A **confusion matrix** is a performance evaluation technique that can be used
    in such cases, which do not involve a balanced dataset. The confusion matrix for
    our dataset would look as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**是一种性能评估技术，可以用于处理数据集不平衡的情况。我们数据集的混淆矩阵如下所示：'
- en: '![](img/5fb4aee3-f8bc-4ef3-810d-e6102797028e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5fb4aee3-f8bc-4ef3-810d-e6102797028e.png)'
- en: Confusion matrix for fraudulent transactions
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈交易的混淆矩阵
- en: The goal of the confusion matrix is to maximize the number of true positives
    and true negatives, as this gives the correct predictions; it also minimizes the
    number of false negatives and false positives, as they give us the wrong predictions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵的目标是最大化真正例和真负例的数量，因为这能给出正确的预测；它还最小化假阴性和假阳性的数量，因为它们给出的是错误的预测。
- en: Depending on your problem, the false positives might be more problematic than
    the false negatives (and vice versa), and thus, the goal of building the right
    classifier should be to solve your problem in the best possible way.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的问题，假阳性可能比假阴性更为问题（反之亦然），因此，构建正确分类器的目标应该是以最佳的方式解决您的问题。
- en: 'In order to implement the confusion matrix in scikit-learn, we use the following
    code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 scikit-learn 中实现混淆矩阵，我们使用以下代码：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '![](img/2f160e07-2447-4a3f-a8e2-a6c2076082f2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f160e07-2447-4a3f-a8e2-a6c2076082f2.png)'
- en: The confusion matrix output from our classifier for fraudulent transactions
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类器输出的欺诈交易混淆矩阵
- en: In the preceding code, we create a set of predictions using the `.predict()`method
    on the test training data, and then we use the `confusion_matrix()`function on
    the test set of the target variable and the predictions that were created earlier.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `.predict()` 方法对测试训练数据生成一组预测结果，然后对目标变量的测试集和之前创建的预测结果使用 `confusion_matrix()`
    函数。
- en: The preceding confusion matrix looks almost perfect, as most cases are classified
    into the true positive and true negative categories, along the main diagonal.
    Only 46 cases are classified incorrectly, and this number is almost equal. This
    means that the numbers of false positives and false negatives are minimal and
    balanced, and one does not outweigh the other. This is an example of the ideal
    classifier.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的混淆矩阵看起来几乎完美，因为大多数情况都被分类为真正例和真反例，沿着主对角线排列。只有46个案例被错误分类，而且这个数字几乎相等。这意味着假阳性和假阴性的数量最小且平衡，两者之间没有明显的偏向。这是理想分类器的一个例子。
- en: Three other metrics that can be derived from the confusion matrix are **precision**,
    **recall,** and **F1-score**. A high value of precision indicates that not many
    non-fraudulent transactions are classified as fraudulent, while a high value of
    recall indicates that most of the fraudulent cases were predicted correctly.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中可以推导出的另外三个度量标准是**精准率**、**召回率**和**F1分数**。较高的精准率表示较少的非欺诈交易被误分类为欺诈交易，而较高的召回率则表示大部分欺诈交易被正确预测。
- en: The F1-score is the weighted average of the precision and recall.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是精准率和召回率的加权平均值。
- en: 'We can compute the precision and recall by using the following code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码计算精准率和召回率：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/a13ae6ab-fe62-495e-896f-a5130759e7f4.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a13ae6ab-fe62-495e-896f-a5130759e7f4.png)'
- en: Classification report
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 分类报告
- en: 'In the preceding code, we use the `classificiation_report()` function with
    two arguments: the test set of the target variable and the prediction variable
    that we created for the confusion matrix earlier.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `classification_report()` 函数，传入两个参数：目标变量的测试集和我们之前为混淆矩阵创建的预测变量。
- en: In the output, the precision, recall, and F1-score are all high, because we
    have built the ideal machine learning model. These values range from 0 to 1, with
    1 being the highest.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，精准率、召回率和F1分数都很高，因为我们已经构建了理想的机器学习模型。这些值的范围从0到1，1为最高。
- en: The normalized confusion matrix
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化混淆矩阵
- en: 'A **normalized confusion matrix** makes it easier for the data scientist to
    visually interpret how the labels are being predicted. In order to construct a
    normalized confusion matrix, we use the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化混淆矩阵**使数据科学家更容易直观地理解标签是如何被预测的。为了构建标准化混淆矩阵，我们使用以下代码：'
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following normalized confusion matrix:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下标准化混淆矩阵：
- en: '![](img/c3c87197-7a32-4095-a4f8-7ea43559918d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3c87197-7a32-4095-a4f8-7ea43559918d.png)'
- en: Normalized confusion matrix for the K-NN model
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN模型的标准化混淆矩阵
- en: In the preceding plot, the predicted labels are along the *x* axis, while the
    true (or actual) labels are along the *y* axis. We can see that the model has
    0.01, or 1%, of the predictions for the fraudulent transactions incorrect, while
    0.99, or 99%, of the fraudulent transactions have been predicted correctly. We
    can also see that the K-NN model predicted 100% of the non-fraudulent transactions
    correctly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，预测标签位于 *x* 轴，而真实（或实际）标签位于 *y* 轴。我们可以看到，该模型对欺诈交易的预测错误率为0.01，即1%，而99%的欺诈交易预测正确。我们还可以看到，K-NN模型对所有非欺诈交易的预测准确率达到了100%。
- en: 'Now, we can compare the performance of the logistic regression model by using
    a normalized confusion matrix, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过使用标准化混淆矩阵来比较逻辑回归模型的表现，如下所示：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following normalized confusion matrix:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下标准化混淆矩阵：
- en: '![](img/125d0f78-4a87-415d-a3d0-f383eac558fb.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/125d0f78-4a87-415d-a3d0-f383eac558fb.png)'
- en: Normalized confusion matrix for the logistic regression model
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的标准化混淆矩阵
- en: In the preceding confusion matrix, it is clear that the logistic regression
    model only predicted 42% of the non-fraudulent transactions correctly. This indicates,
    almost instantly, that the k-nearest neighbor model performed better.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的混淆矩阵中，可以明显看出，逻辑回归模型仅正确预测了42%的非欺诈交易。这几乎立刻表明，K-NN模型的表现更好。
- en: Area under the curve
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曲线下的面积
- en: 'The curve, in this case, is the **receiver operator characteristics** (**ROC**)
    curve. This is a plot between the true positive rate and the false positive rate.
    We can plot this curve as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个曲线在本例中是**接收者操作特征**（**ROC**）曲线。这是一个表示真实正例率与假正例率之间关系的图。我们可以通过以下方式绘制该曲线：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the following curve:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成如下曲线：
- en: '![](img/7bfbb951-cb4e-4052-b9fe-4ca056717412.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bfbb951-cb4e-4052-b9fe-4ca056717412.png)'
- en: ROC curve
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线
- en: In the preceding code, first, we create a set of probabilities for each of the
    predicted labels. For instance, the predicted label of 1 would have a certain
    set of probabilities associated with it, while the label 0 would have a certain
    set of probabilities associated with it. Using these probabilities, we use the
    `roc_curve()`function, along with the target test set, to generate the ROC curve.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，首先，我们为每个预测标签创建一组概率。例如，预测标签1会有一组与之相关的概率，而标签0则有另一组概率。利用这些概率，我们使用`roc_curve()`函数，并结合目标测试集，来生成ROC曲线。
- en: The preceding curve is an example of a perfect ROC curve. The preceding curve
    has a true positive rate of 1.0, which indicates accurate predictions, while it
    has a false positive rate of 0, which indicates a lack of wrong predictions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的曲线是一个完美的ROC曲线示例。该曲线的真实正例率为1.0，表示预测准确，而假正例率为0，表示没有错误预测。
- en: 'Such a curve also has the most area under the curve, as compared to the curves
    of models that have a lower accuracy. In order to compute the area under the curve
    score, we use the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的曲线通常具有最大的曲线下方区域，与准确度较低的模型曲线相比。为了计算曲线下面积得分，我们使用以下代码：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This produces a score of 0.99\. A higher `auc` score indicates a better performing
    model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生0.99的得分。较高的`auc`得分表示模型表现更好。
- en: Cumulative gains curve
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 累积增益曲线
- en: When building multiple machine learning models, it is important to understand
    which of the models in question produces the type of predictions that you want
    it to generate. The **cumulative gains curve** helps you with the process of model
    comparison, by telling you about the percentage of a category/class that appears
    within a percentage of the sample population for a particular model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建多个机器学习模型时，了解哪个模型产生了你期望的预测类型非常重要。**累积增益曲线**可以帮助你进行模型比较，通过告诉你某个类别/类在特定模型的样本数据中占据的百分比。
- en: 'In simple terms, in the fraud detection dataset, we might want to pick a model
    that can predict a larger number of fraudulent transactions, as opposed to a model
    that cannot. In order to construct the cumulative gains plot for the k-nearest
    neighbors model, we use the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，在欺诈检测数据集中，我们可能希望选择一个能够预测更多欺诈交易的模型，而不是一个无法做到这一点的模型。为了构建k最近邻模型的累积增益图，我们使用以下代码：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in the following plot:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下图形：
- en: '![](img/8db348f7-5c94-4fe9-9382-216276efc920.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8db348f7-5c94-4fe9-9382-216276efc920.png)'
- en: Cumulative gains plot for the k-nearest neighbors model
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: k最近邻模型的累积增益图
- en: 'In the preceding code, the following applies:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，以下内容适用：
- en: First, we import the `scikit-plot` package, which generates the preceding plot.
    We then compute the probabilities for the target variable, which, in this case,
    are the probabilities if a particular mobile transaction is fraudulent or not
    on the test data.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们导入`scikit-plot`包，它用于生成前面的图。然后，我们计算目标变量的概率，在本例中，这些概率表示某个特定移动交易是否为欺诈交易的可能性，基于测试数据。
- en: Finally, we use the `plot_cumulative_gain()`function on these probabilities
    and the test data target labels, in order to generate the preceding plot.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们在这些概率和测试数据的目标标签上使用`plot_cumulative_gain()`函数，以生成前面的图。
- en: 'How do we interpret the preceding plot? We simply look for the point at which
    a certain percentage of the data contains 100% of the target class. This is illustrated
    in the following diagram:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读前面的图形？我们只需寻找一个点，在该点上，数据中的某个百分比包含了100%的目标类别。下图为此示意：
- en: '![](img/6f432ac3-7149-40a5-9ed4-d7fb0c139012.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f432ac3-7149-40a5-9ed4-d7fb0c139012.png)'
- en: Point at which 100% of the target class exists
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 100%目标类别存在的点
- en: The point defined in the preceding diagram corresponds to a value of 0.3 on
    the *x* axis and 1.0 on the *y* axis. This means that 0.3 to 1.0 (or 30% to 100%)
    of the data will consist of the target class, 1, which are the fraudulent transactions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 前图中的点对应于*x*轴上的0.3和*y*轴上的1.0。这意味着30%到100%的数据将包含目标类1，即欺诈交易。
- en: 'This can also be interpreted as follows: 70% of the total data will contain
    100% of the fraudulent transaction predictions if you use the k-nearest neighbors
    model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以这样解释：如果你使用K最近邻模型，70%的数据将包含100%的欺诈交易预测。
- en: 'Now, let''s compute the cumulative gains curve for the logistic regression
    model, and see if it is different. In order to do this, we use the following code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算逻辑回归模型的累积增益曲线，看看它是否有所不同。为此，我们使用以下代码：
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following plot:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![](img/438c000a-eafd-4c9e-b1c2-b604b65f299e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/438c000a-eafd-4c9e-b1c2-b604b65f299e.png)'
- en: Cumulative gains plot for the logistic regression model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的累积增益图
- en: The preceding plot is similar to the cumulative gains plot that was previously
    produced by the K-NN model, in that 70% of the data contains 100% of the target
    class. Therefore, using either the K-NN or the logistic regression model will
    yield similar results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图与K-NN模型先前生成的累积增益图相似，都是70%的数据包含100%的目标类。因此，使用K-NN模型或逻辑回归模型都会得到类似的结果。
- en: However, it is a good practice to compare how different models behave by using
    the cumulative gains chart, in order to gain a fundamental understanding of how
    your model makes predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用累积增益图来比较不同模型的表现是一个好的实践，这样可以从根本上理解模型如何进行预测。
- en: Lift curve
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升曲线
- en: 'A **lift curve** gives you information about how well you can make predictions
    by using a machine learning model, as opposed to when you are not using one. In
    order to construct a lift curve for the k-nearest neighbor model, we use the following
    code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升曲线**可以告诉你，通过使用机器学习模型进行预测的效果如何，相较于不使用模型的情况。为了构建K最近邻模型的提升曲线，我们使用以下代码：'
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following plot:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![](img/cba26353-d293-4a22-9cae-f6e6315a92bd.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cba26353-d293-4a22-9cae-f6e6315a92bd.png)'
- en: Lift curve for the K-NN model
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN模型的提升曲线
- en: 'How do we interpret the preceding lift curve? We have to look for the point
    at which the curve dips. This is illustrated for you in the following diagram:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读前面的提升曲线呢？我们需要找到曲线下滑的点。以下图为您展示了这个点：
- en: '![](img/94c581f3-70aa-49f6-8d32-298e88da3e6c.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94c581f3-70aa-49f6-8d32-298e88da3e6c.png)'
- en: Point of interest in the lift curve
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 提升曲线中的兴趣点
- en: In the preceding plot, the point that is highlighted is the point that we want
    to look for in any lift curve. The point tells us that 0.3, or 30%, of our total
    data will perform 3.5 times better when using the K-NN predictive model, as opposed
    to when we do not use any model at all to predict the fraudulent transactions.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，突出显示的点是我们在任何提升曲线中都要寻找的点。这个点告诉我们，当使用K-NN预测模型时，0.3或30%的数据比完全不使用任何模型预测欺诈交易时，表现要好3.5倍。
- en: 'Now, we can construct the lift curve for the logistic regression model, in
    order to compare and contrast the performance of the two models. We can do this
    by using the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建逻辑回归模型的提升曲线，以便比较两种模型的表现。我们可以通过使用以下代码来实现：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following plot:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![](img/a99e0447-e440-49ac-9f06-98e3c4779cce.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a99e0447-e440-49ac-9f06-98e3c4779cce.png)'
- en: Lift curve for the logistic regression model
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的提升曲线
- en: Although the plot tells us that 30% of the data will see an improved performance
    (similar to that of the K-NN model that we built earlier in order to predict the
    fraudulent transactions), there is a difference when it comes to predicting the
    non-fraudulent transactions (the blue line).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该图告诉我们30%的数据会看到性能改善（类似于我们之前构建的K-NN模型来预测欺诈交易），但在预测非欺诈交易时（蓝线）存在差异。
- en: For a small percentage of the data, the lift curve for the non-fraudulent transactions
    is actually lower than the baseline (the dotted line). This means that the logistic
    regression model does worse than not using a predictive model for a small percentage
    of the data when it comes to predicting the non-fraudulent transactions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于少部分数据，非欺诈交易的提升曲线实际上低于基线（虚线）。这意味着在预测非欺诈交易时，逻辑回归模型在少数数据上的表现甚至不如不使用预测模型。
- en: K-S statistic plot
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-S统计图
- en: 'The **K-S statistic plot**, or the **Kolmogorov Smirnov** statistic plot, is
    a plot that tells you whether the model gets confused when it comes to predicting
    the different labels in your dataset. In order to illustrate what the term *confused*
    means in this case, we will construct the K-S statistic plot for the K-NN model
    by using the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-S统计图**，或称**科尔莫哥洛夫-斯米尔诺夫**统计图，是一种能够告诉你模型是否在预测数据集中不同标签时产生混淆的图。为了说明在这种情况下“混淆”是什么意思，我们将通过以下代码为K-NN模型构建K-S统计图：'
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following plot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图：
- en: '![](img/0d914ee1-1c8e-406d-8137-0e1885305642.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d914ee1-1c8e-406d-8137-0e1885305642.png)'
- en: K-S statistic plot for the K-NN model
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN模型的K-S统计图
- en: 'In the preceding plot, the following applies:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，以下内容适用：
- en: The dotted line is the distance between the predictions for the fraudulent transactions
    (the yellow line at the bottom) and the non-fraudulent transactions (the blue
    line at the top). This distance is 0.985, as indicated by the plot.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚线表示欺诈交易（底部的黄色线）和非欺诈交易（顶部的蓝色线）预测之间的距离。这一距离为0.985，如图所示。
- en: A K-S statistic score that is close to 1 is usually a good indication that the
    model does not get confused between predicting the two different target labels,
    and can make a clear distinction when it comes to predicting the labels.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-S统计得分接近1通常是一个很好的指标，表明模型在预测这两种不同的目标标签时没有混淆，并且在预测标签时可以清楚地区分它们。
- en: In the preceding plot, the score of 0.985 can be observed as the difference
    between the two classes of predictions, for up to 70% (0.7) of the data. This
    can be observed along the *x* axis, as a threshold of 0.7 still has the maximum
    separation distance.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前面的图中，可以观察到0.985的得分为两类预测之间的差异，最大可达70%（0.7）的数据。这个差异可以在*X*轴上看到，因为0.7的阈值仍然具有最大分离距离。
- en: 'We can now compute the K-S statistic plot for the logistic regression model,
    in order to compare which of the two models provides a better distinction in predictions
    between the two class labels. We can do this by using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算逻辑回归模型的K-S统计图，以便比较这两个模型在预测两个类别标签之间的区分能力。我们可以通过以下代码来实现：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following plot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图：
- en: '![](img/acbae8fc-3ed1-4f58-b477-86449a66fcf2.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/acbae8fc-3ed1-4f58-b477-86449a66fcf2.png)'
- en: K-S statistic plot for the logistic regression model
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型的K-S统计图
- en: Although the two models have the same separation score of 0.985, the threshold
    at which the separation occurs is quite different. In the case of logistic regression,
    this distance only occurs for the bottom 43% of the data, since the maximum separation
    starts at a threshold of 0.57, along the *x* axis.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管两个模型的分离分数相同，都是0.985，但分离发生的阈值却有所不同。在逻辑回归模型中，这一距离仅出现在数据的下43%部分，因为最大分离开始的阈值为0.57，沿着*X*轴。
- en: This means that the k-nearest neighbors model, which has a large distance for
    about 70% of the total data, is much better at making predictions about fraudulent
    transactions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着K近邻模型对于大约70%的总数据，其距离较大，在预测欺诈交易时要比其他模型更为准确。
- en: Calibration plot
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 校准图
- en: 'A **calibration plot**, as the name suggests, tells you how well calibrated
    your model is. A well-calibrated model will have a prediction score equal to the
    fraction of the positive class (in this case, the fraudulent transactions). In
    order to plot a calibration plot, we use the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**校准图**，顾名思义，是用来告诉你模型的校准情况。一个校准良好的模型，其预测得分应该等于正类的比例（在此例中为欺诈交易）。为了绘制校准图，我们使用以下代码：'
- en: '[PRE15]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This results in the following calibration plot:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下校准图：
- en: '![](img/1ce52083-1671-4729-b9f4-f13b2696bca9.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ce52083-1671-4729-b9f4-f13b2696bca9.png)'
- en: Calibration plot for the two models
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的校准图
- en: 'In the preceding code, the following applies:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，以下内容适用：
- en: First, we compute the probability that the positive class (fraudulent transactions)
    will be predicted for each model.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们计算每个模型预测正类（欺诈交易）的概率。
- en: Then, we store these probabilities and the model names in a list.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将这些概率和模型名称存储在一个列表中。
- en: Finally, we use the `plot_calibration_curve()`function from the `scikit-plot`
    package with these probabilities, the test labels, and the model names, in order
    to create the calibration plot.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`scikit-plot`包中的`plot_calibration_curve()`函数，结合这些概率、测试标签和模型名称，来创建校准图。
- en: 'This results in the preceding calibration plot, which can be explained as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成前面的校准图，解释如下：
- en: The dotted line represents the perfect calibration plot. This is because the
    mean prediction value has the exact value of the fraction of the positive class
    at each and every point.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚线代表完美的校准图。这是因为在每个点，平均预测值的准确度与正类的比例完全一致。
- en: From the plot, it is clear that the k-nearest neighbors model is much better
    calibrated than the calibration plot of the logistic regression model.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图中可以看出，k近邻模型的校准效果明显优于逻辑回归模型的校准图。
- en: This is because the calibration plot of the k-nearest neighbors model follows
    that of the ideal calibration plot much more closely than the calibration plot
    of the logistic regression model.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是因为k近邻模型的校准图比逻辑回归模型的校准图更接近理想的校准图。
- en: Learning curve
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: 'A **learning curve** is a plot that compares how the training accuracy scores
    and the test accuracy scores vary as the number of samples/rows added to the data
    increases. In order to construct the learning curve for the k-nearest neighbors
    model, we use the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习曲线**是一个图表，用于比较随着样本/行数的增加，训练准确率和测试准确率的变化。为了构建K近邻模型的学习曲线，我们使用以下代码：'
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following plot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/07066563-bdff-4d65-b444-a13ba26b829f.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07066563-bdff-4d65-b444-a13ba26b829f.png)'
- en: Learning curve for the K-NN model
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN模型的学习曲线
- en: 'In the preceding curve, the following applies:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的曲线中，以下内容适用：
- en: The training score and the test score are only the highest when the number of
    samples is 15,000\. This suggests that even if we had only 15,000 samples (instead
    of the 17,500), we would still get the best possible results.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当样本数为15,000时，训练得分和测试得分才是最高的。这表明即使我们只有15,000个样本（而不是17,500个），我们仍然能够得到最好的结果。
- en: Anything under the 15,000 samples will mean that the test cross-validated scores
    will be much lower than the training scores, suggesting that the model is overfit.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何少于15,000个样本的情况都会导致测试的交叉验证得分远低于训练得分，表明模型出现过拟合。
- en: Cross-validated box plot
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证箱形图
- en: 'In this plot, we compare the cross-validated accuracy scores of multiple models
    by making use of box plots. In order to do so, we use the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，我们通过使用箱形图比较多个模型的交叉验证准确率得分。为此，我们使用以下代码：
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in the following plot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/1c850faa-94c3-4070-90a1-5d19b28b7899.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c850faa-94c3-4070-90a1-5d19b28b7899.png)'
- en: Cross-validated box plot
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证箱形图
- en: 'In the preceding code, the following applies:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，以下内容适用：
- en: First, we store the models that we want to compare in a list.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将要比较的模型存储在一个列表中。
- en: Then, we initialize two empty lists, in order to store the results of the cross-validated
    accuracy scores and the names of the models, so that we can use them later, in
    order to create the box plots.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化两个空列表，用于存储交叉验证准确率得分和模型名称的结果，以便后续使用，以便生成箱形图。
- en: We then iterate over each model in the list of models, and use the `model_selection.KFold()`function
    in order to split the data into a five-fold cross-validated set.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们遍历模型列表中的每个模型，使用`model_selection.KFold()`函数将数据划分为五折交叉验证集。
- en: Next, we extract the five-fold cross-validated scores by using the
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过使用`model_selection.cross_val_scores()`函数提取五折交叉验证得分，并将得分和模型名称添加到我们在代码开头初始化的列表中。
- en: '`model_selection.cross_val_scores()`function and append the scores, along with
    the model names, into the lists that we initialized at the beginning of the code.'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`model_selection.cross_val_scores()`函数，并将得分与模型名称一起追加到我们在代码开头初始化的列表中。
- en: Finally, a box plot is created, displaying the cross-validated scores in a box
    plot.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，创建了一个箱形图，展示了交叉验证得分的箱形图。
- en: The list that we created consists of the five cross-validated scores, along
    with the model names. A box plot takes these five scores for each model and computes
    the min, max, median, first, and third quartiles, in the form of a box plot.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的列表由五个交叉验证得分和模型名称组成。箱线图将这五个得分应用于每个模型，计算最小值、最大值、中位数、第一个四分位数和第三个四分位数，并以箱线图的形式展示。
- en: 'In the preceding plot, the following applies:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，以下内容适用：
- en: It is clear that the K-NN model has the highest value of accuracy, with the
    lowest difference between the minimum and maximum values.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 很明显，K-NN 模型的准确度最高，并且最小值与最大值之间的差异最小。
- en: The logistic regression model, on the other hand, has the greatest difference
    between the minimum and maximum values, and has an outlier in its accuracy score,
    as well.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一方面，逻辑回归模型在最小值和最大值之间的差异最大，并且在其准确度得分中也存在异常值。
- en: Performance evaluation for regression algorithms
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归算法的性能评估
- en: 'There are three main metrics that you can use to evaluate the performance of
    the regression algorithm that you built, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个主要指标可以用来评估你构建的回归算法的性能，具体如下：
- en: '**Mean absolute error** (**MAE**)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方绝对误差** (**MAE**)'
- en: '**Mean squared error** (**MSE**)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差** (**MSE**)'
- en: '**Root mean squared error** (**RMSE**)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根误差** (**RMSE**)'
- en: 'In this section, you will learn what the three metrics are, how they work,
    and how you can implement them using scikit-learn. The first step is to build
    the linear regression algorithm. We can do this by using the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解这三种指标是什么，它们是如何工作的，以及如何使用 scikit-learn 实现它们。第一步是构建线性回归算法。我们可以通过使用以下代码来实现：
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Mean absolute error
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方绝对误差
- en: 'The mean absolute error is given by the following formula:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 均方绝对误差的公式如下：
- en: '![](img/8406d412-d72f-4741-886f-84610b68299e.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8406d412-d72f-4741-886f-84610b68299e.png)'
- en: MAE formula
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 公式
- en: In the preceding formula, ![](img/6b9d56b7-d0be-4784-bc16-c611b85cc303.png)
    represents the true (or actual) value of the output, while the ![](img/bfcb7fbb-92c2-4a74-b7d8-a723cfe02682.png)
    hat represents the predicted output values. Therefore, by computing the summation
    of the difference between the true value and the predicted value of the output
    for each row in your data, and then dividing it by the total number of observations,
    you get the mean value of the absolute error.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，![](img/6b9d56b7-d0be-4784-bc16-c611b85cc303.png) 代表输出的真实值（或实际值），而 ![](img/bfcb7fbb-92c2-4a74-b7d8-a723cfe02682.png)
    代表预测的输出值。因此，通过计算每一行数据中真实值和预测值之间的差异总和，然后将其除以观测数据的总数，得到绝对误差的均值。
- en: 'In order to implement the MAE in scikit-learn, we use the following code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 scikit-learn 中实现 MAE，我们使用以下代码：
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code, the `mean_absolute_error()`function from the `metrics`
    module in scikit-learn is used to compute the MAE. It takes in two arguments:
    the real/true output, which is the target, and the predictions, which are the
    predicted outputs.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，scikit-learn 中 `metrics` 模块的 `mean_absolute_error()` 函数用于计算 MAE。它接受两个参数：真实输出（目标值）和预测输出（预测值）。
- en: Mean squared error
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差
- en: 'The mean squared error is given by the following formula:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差的公式如下：
- en: '![](img/0c21cb85-d073-4e73-8dc4-bee43b5bc301.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c21cb85-d073-4e73-8dc4-bee43b5bc301.png)'
- en: MSE formula
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 公式
- en: The preceding formula is similar to the formula that we saw for the mean absolute
    error, except that instead of computing the absolute difference between the true
    and predicted output values, we compute the square of the difference.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公式与我们看到的均方绝对误差公式相似，不同之处在于，我们不是计算真实值和预测值之间的绝对差异，而是计算它们差异的平方。
- en: 'In order to implement the MSE in scikit-learn, we use the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 scikit-learn 中实现 MSE，我们使用以下代码：
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We use the `mean_squared_error()`function from the `metrics` module, with the
    real/true output values and the predictions as arguments. The mean squared error
    is better at detecting larger errors, because we square the errors, instead of
    depending on only the difference.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `metrics` 模块中的 `mean_squared_error()` 函数，传入真实输出值和预测值作为参数。均方误差对于检测较大的误差更有效，因为我们对误差进行了平方，而不仅仅依赖于差异。
- en: Root mean squared error
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方根误差
- en: 'The root mean squared error is given by the following formula:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差的公式如下：
- en: '![](img/1960f41c-0b17-4d04-86de-f8af9e2b6b77.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1960f41c-0b17-4d04-86de-f8af9e2b6b77.png)'
- en: The preceding formula is very similar to that of the mean squared error, except
    for the fact that we take the square root of the MSE formula.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公式与均方误差（MSE）非常相似，唯一不同的是我们对MSE公式进行了平方根处理。
- en: 'In order to compute the RMSE in scikit-learn, we use the following code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在scikit-learn中计算RMSE，我们使用以下代码：
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, we use the `mean_squared_error()`function with the true/real
    output and the predictions, and then we take the square root of this answer by
    using the `np.sqrt()`function from the `numpy` package.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用了`mean_squared_error()`函数来计算真实输出与预测结果之间的误差，然后通过使用`numpy`包中的`np.sqrt()`函数计算该值的平方根。
- en: Compared to the MAE and the MSE, the RMSE is the best possible metric that you
    can use in order to evaluate the linear regression model, since this detects large
    errors and gives you the value in terms of the output units. The key takeaway
    from using any one of the three metrics is that the value that these `metrics`
    gives you should be as low as possible, indicating that the model has relatively
    low error values.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于MAE和MSE，RMSE是评估线性回归模型时最合适的指标，因为它能够检测到较大的误差，并且以输出单位表示该值。使用这三种指标中的任何一个时，关键的结论是，这些`metrics`给出的值应该尽可能低，表明模型的误差较小。
- en: Performance evaluation for unsupervised algorithms
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督算法的性能评估
- en: 'In this section, you will learn how to evaluate the performance of an unsupervised
    machine learning algorithm, such as the k-means algorithm. The first step is to
    build a simple k-means model. We can do so by using the following code:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何评估无监督机器学习算法的性能，例如k-means算法。第一步是构建一个简单的k-means模型。我们可以通过使用以下代码来完成：
- en: '[PRE22]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now that we have a simple k-means model with two clusters, we can proceed to
    evaluate the model''s performance. The different visual performance charts that
    can be deployed are as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个简单的k-means模型，其中包含两个聚类，我们可以继续评估该模型的性能。可以使用的不同可视化性能图表如下：
- en: Elbow plot
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肘部图
- en: Silhouette analysis plot
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮廓分析图
- en: In this section, you will learn how to create and interpret each of the preceding
    plots.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何创建和解释上述每一个图表。
- en: Elbow plot
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 肘部图
- en: 'In order to construct an elbow plot, we use the following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建肘部图，我们使用以下代码：
- en: '[PRE23]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following plot:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成如下图所示的图表：
- en: '![](img/8acf8d3f-2174-46de-a8e1-d9ab51207753.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8acf8d3f-2174-46de-a8e1-d9ab51207753.png)'
- en: Elbow plot
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部图
- en: The elbow plot is a plot between the number of clusters that the model takes
    into consideration along the *x* axis and the sum of the squared errors along
    the *y* axis.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 肘部图是一个图表，表示模型考虑的聚类数在*x*轴上，而平方误差总和则在*y*轴上。
- en: 'In the preceding code, the following applies:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，以下内容适用：
- en: We use the `plot_elbow_curve()`function with the k-means model, the data, and
    the number of clusters that we want to evaluate
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`plot_elbow_curve()`函数，输入k-means模型、数据和我们想要评估的聚类数。
- en: In this case, we define a range of 1 to 19 clusters
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们定义了1到19个聚类的范围。
- en: 'In the preceding plot, the following applies:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，以下内容适用：
- en: It is clear that the elbow point, or the point at which the sum of the squared
    errors (*y* axis) starts decreasing very slowly, is where the number of clusters
    is 4.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很明显，肘部点，或者说平方误差总和（*y*轴）开始非常缓慢下降的点，出现在聚类数为4时。
- en: The plot also gives you another interesting metric on the *y* axis (right-hand
    side), which is the clustering duration (in seconds). This indicates the amount
    of time it took for the algorithm to create the clusters, in seconds.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该图表还在*y*轴（右侧）提供了一个有趣的指标，即聚类时长（以秒为单位）。这表示算法创建聚类所花费的时间，单位为秒。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned how to evaluate the performances of the three
    different types of machine learning algorithms: classification, regression, and
    unsupervised.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学会了如何评估三种不同类型的机器学习算法的性能：分类、回归和无监督学习。
- en: For the classification algorithms, you learned how to evaluate the performance
    of a model by using a series of visual techniques, such as the confusion matrix,
    normalized confusion matrix, area under the curve, K-S statistic plot, cumulative
    gains plot, lift curve, calibration plot, learning curve, and cross-validated
    box plot.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类算法，你学会了如何通过一系列可视化技术评估模型的性能，例如混淆矩阵、归一化混淆矩阵、曲线下面积、K-S统计图、累计增益图、提升曲线、校准图、学习曲线和交叉验证箱线图。
- en: 'For the regression algorithms, you learned how to evaluate the performance
    of a model by using three metrics: the mean squared error, mean absolute error,
    and root mean squared error.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归算法，你学习了如何通过三种度量标准来评估模型的性能：均方误差、平均绝对误差和均方根误差。
- en: Finally, for the unsupervised machine learning algorithms, you learned how to
    evaluate the performance of a model by using the elbow plot.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于无监督学习算法，你学习了如何通过使用肘部法则图来评估模型的性能。
- en: 'Congratulations! You have now made it to the end of your machine learning journey
    with scikit-learn. You''ve made your way through eight chapters, which gave you
    the quickest entry point into the wonderful world of machine learning with one
    of the world''s most popular machine learning frameworks: scikit-learn.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经顺利完成了使用 scikit-learn 的机器学习之旅。你已经通过了八章内容，这些内容为你提供了快速入门的途径，帮助你进入了机器学习的奇妙世界，并使用了全球最受欢迎的机器学习框架之一：scikit-learn。
- en: 'In this book, you learned about the following topics:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你学习了以下主题：
- en: What machine learning is (in a nutshell) and the different types and applications
    of machine learning
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是机器学习（简而言之），以及机器学习的不同类型和应用
- en: Supervised machine learning algorithms, such as K-NN, logistic regression, Naive
    Bayes, support vector machines, and linear regression
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习算法，如 K-NN、逻辑回归、朴素贝叶斯、支持向量机和线性回归
- en: Unsupervised machine learning algorithms, such as the k-means algorithm
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习算法，如 k-means 算法
- en: Algorithms that can perform both classification and regression, such as decision
    trees, random forests, and gradient-boosted trees
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 既能执行分类任务又能进行回归的算法，如决策树、随机森林和梯度提升树
- en: I hope that you can make the best possible use of the application based on the
    knowledge that this book has given you, allowing you to solve many real-world
    problems by using machine learning as your tool!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能充分利用本书所提供的知识，应用这些知识，通过机器学习作为工具，解决许多现实世界中的问题！
