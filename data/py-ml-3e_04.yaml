- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Building Good Training Datasets – Data Preprocessing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建良好的训练数据集—数据预处理
- en: The quality of the data and the amount of useful information that it contains
    are key factors that determine how well a machine learning algorithm can learn.
    Therefore, it is absolutely critical to ensure that we examine and preprocess
    a dataset before we feed it to a learning algorithm. In this chapter, we will
    discuss the essential data preprocessing techniques that will help us to build
    good machine learning models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的质量以及它所包含的有用信息的量是决定机器学习算法学习效果的关键因素。因此，在将数据集输入学习算法之前，确保我们检查并预处理数据集是至关重要的。在本章中，我们将讨论一些基本的数据预处理技术，它们将帮助我们构建良好的机器学习模型。
- en: 'The topics that we will cover in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论的主题如下：
- en: Removing and imputing missing values from the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中删除和填补缺失值
- en: Getting categorical data into shape for machine learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类数据整理成适合机器学习算法的格式
- en: Selecting relevant features for the model construction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型构建选择相关特征
- en: Dealing with missing data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: It is not uncommon in real-world applications for our training examples to be
    missing one or more values for various reasons. There could have been an error
    in the data collection process, certain measurements may not be applicable, or
    particular fields could have been simply left blank in a survey, for example.
    We typically see missing values as blank spaces in our data table or as placeholder
    strings such as `NaN`, which stands for "not a number," or `NULL` (a commonly
    used indicator of unknown values in relational databases). Unfortunately, most
    computational tools are unable to handle such missing values or will produce unpredictable
    results if we simply ignore them. Therefore, it is crucial that we take care of
    those missing values before we proceed with further analyses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们的训练样本因各种原因可能缺少一个或多个值，这并不罕见。数据收集过程中可能发生了错误，某些测量可能不适用，或者某些字段可能在调查中被留空。例如，我们通常会在数据表中看到缺失值以空白的形式呈现，或以占位符字符串如`NaN`（表示“不是数字”）或`NULL`（在关系数据库中常用的未知值指示符）出现。不幸的是，大多数计算工具无法处理这些缺失值，或者如果我们简单地忽略它们，可能会产生不可预测的结果。因此，在进行进一步分析之前，务必处理这些缺失值。
- en: In this section, we will work through several practical techniques for dealing
    with missing values by removing entries from our dataset or imputing missing values
    from other training examples and features.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过几种实际技术来处理缺失值，这些技术包括从数据集中删除条目或从其他训练样本和特征中填补缺失值。
- en: Identifying missing values in tabular data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别表格数据中的缺失值
- en: 'Before we discuss several techniques for dealing with missing values, let''s
    create a simple example data frame from a **comma-separated values** (**CSV**)
    file to get a better grasp of the problem:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论处理缺失值的几种技术之前，让我们从一个**逗号分隔值**（**CSV**）文件中创建一个简单的示例数据框，以便更好地理解这个问题：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the preceding code, we read CSV-formatted data into a pandas `DataFrame`
    via the `read_csv` function and noticed that the two missing cells were replaced
    by `NaN`. The `StringIO` function in the preceding code example was simply used
    for the purposes of illustration. It allowed us to read the string assigned to
    `csv_data` into a pandas `DataFrame` as if it was a regular CSV file on our hard
    drive.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们通过`read_csv`函数将CSV格式的数据读入pandas的`DataFrame`，并注意到那两个缺失的单元格被`NaN`替换了。前面代码示例中的`StringIO`函数仅用于演示目的。它让我们能够像读取硬盘上的常规CSV文件一样，将赋值给`csv_data`的字符串读取到pandas的`DataFrame`中。
- en: 'For a larger `DataFrame`, it can be tedious to look for missing values manually;
    in this case, we can use the `isnull` method to return a `DataFrame` with Boolean
    values that indicate whether a cell contains a numeric value (`False`) or if data
    is missing (`True`). Using the `sum` method, we can then return the number of
    missing values per column as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个较大的`DataFrame`，手动查找缺失值可能非常繁琐；在这种情况下，我们可以使用`isnull`方法返回一个布尔值的`DataFrame`，指示某个单元格是否包含数值（`False`）或者数据是否缺失（`True`）。接着，使用`sum`方法，我们可以返回每列缺失值的数量，如下所示：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This way, we can count the number of missing values per column; in the following
    subsections, we will take a look at different strategies for how to deal with
    this missing data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以统计每列缺失值的数量；在接下来的小节中，我们将探讨不同的策略来处理这些缺失数据。
- en: '**Convenient data handling with pandas'' data frames**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用pandas的DataFrame便捷地处理数据**'
- en: 'Although scikit-learn was originally developed for working with NumPy arrays
    only, it can sometimes be more convenient to preprocess data using pandas'' `DataFrame`.
    Nowadays, most scikit-learn functions support `DataFrame` objects as inputs, but
    since NumPy array handling is more mature in the scikit-learn API, it is recommended
    to use NumPy arrays when possible. Note that you can always access the underlying
    NumPy array of a `DataFrame` via the `values` attribute before you feed it into
    a scikit-learn estimator:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 scikit-learn 最初是为了仅处理 NumPy 数组而开发的，但有时使用 pandas 的 `DataFrame` 进行数据预处理会更为方便。如今，大多数
    scikit-learn 函数支持将 `DataFrame` 对象作为输入，但由于 NumPy 数组的处理在 scikit-learn API 中更为成熟，因此建议在可能的情况下使用
    NumPy 数组。请注意，在将 `DataFrame` 输入到 scikit-learn 估算器之前，您始终可以通过 `values` 属性访问 `DataFrame`
    的底层 NumPy 数组：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Eliminating training examples or features with missing values
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除包含缺失值的训练示例或特征
- en: 'One of the easiest ways to deal with missing data is simply to remove the corresponding
    features (columns) or training examples (rows) from the dataset entirely; rows
    with missing values can easily be dropped via the `dropna` method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据最简单的方法之一就是完全删除相应的特征（列）或训练示例（行）；可以通过 `dropna` 方法轻松删除包含缺失值的行：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, we can drop columns that have at least one `NaN` in any row by setting
    the `axis` argument to `1`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也可以通过将 `axis` 参数设置为 `1`，删除任何行中至少包含一个 `NaN` 的列：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `dropna` method supports several additional parameters that can come in
    handy:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`dropna` 方法支持多个额外的参数，这些参数可能会很有用：'
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Although the removal of missing data seems to be a convenient approach, it
    also comes with certain disadvantages; for example, we may end up removing too
    many samples, which will make a reliable analysis impossible. Or, if we remove
    too many feature columns, we will run the risk of losing valuable information
    that our classifier needs to discriminate between classes. In the next section,
    we will look at one of the most commonly used alternatives for dealing with missing
    values: interpolation techniques.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管删除缺失数据似乎是一个方便的方法，但它也有一定的缺点；例如，我们可能会删除过多的样本，从而使得可靠的分析变得不可能。或者，如果我们删除了过多的特征列，就可能会失去分类器区分不同类别所需的重要信息。在下一节中，我们将介绍处理缺失值时最常用的替代方法之一：插值技术。
- en: Imputing missing values
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: 'Often, the removal of training examples or dropping of entire feature columns
    is simply not feasible, because we might lose too much valuable data. In this
    case, we can use different interpolation techniques to estimate the missing values
    from the other training examples in our dataset. One of the most common interpolation
    techniques is **mean imputation**, where we simply replace the missing value with
    the mean value of the entire feature column. A convenient way to achieve this
    is by using the `SimpleImputer` class from scikit-learn, as shown in the following
    code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，删除训练示例或整个特征列是不可行的，因为我们可能会丧失太多宝贵的数据。在这种情况下，我们可以使用不同的插值技术来根据数据集中其他训练示例估算缺失值。最常见的插值技术之一是
    **均值填充**，即我们简单地用整个特征列的均值替换缺失值。实现这一点的一种方便方法是使用 scikit-learn 中的 `SimpleImputer`
    类，如以下代码所示：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we replaced each `NaN` value with the corresponding mean, which is separately
    calculated for each feature column. Other options for the `strategy` parameter
    are `median` or `most_frequent`, where the latter replaces the missing values
    with the most frequent values. This is useful for imputing categorical feature
    values, for example, a feature column that stores an encoding of color names,
    such as red, green, and blue. We will encounter examples of such data later in
    this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将每个 `NaN` 值替换为相应的均值，该均值是单独为每个特征列计算的。`strategy` 参数的其他选项包括 `median` 或 `most_frequent`，其中后者将缺失值替换为最频繁的值。这对于填补类别特征值很有用，例如存储颜色名称编码的特征列，如红色、绿色和蓝色。我们将在本章后面遇到这类数据的例子。
- en: 'Alternatively, an even more convenient way to impute missing values is by using
    pandas'' `fillna` method and providing an imputation method as an argument. For
    example, using pandas, we could achieve the same mean imputation directly in the
    `DataFrame` object via the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更方便的填充缺失值的方法是使用 pandas 的 `fillna` 方法，并提供一个填充方法作为参数。例如，使用 pandas，我们可以通过以下命令直接在
    `DataFrame` 对象中实现相同的均值填充：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B13208_04_01.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_01.png)'
- en: Understanding the scikit-learn estimator API
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 scikit-learn 估算器 API
- en: In the previous section, we used the `SimpleImputer` class from scikit-learn
    to impute missing values in our dataset. The `SimpleImputer` class belongs to
    the so-called **transformer** classes in scikit-learn, which are used for data
    transformation. The two essential methods of those estimators are `fit` and `transform`.
    The `fit` method is used to learn the parameters from the training data, and the
    `transform` method uses those parameters to transform the data. Any data array
    that is to be transformed needs to have the same number of features as the data
    array that was used to fit the model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了scikit-learn中的`SimpleImputer`类来填补数据集中的缺失值。`SimpleImputer`类属于scikit-learn中所谓的**变换器**类，用于数据转换。此类估算器的两个基本方法是`fit`和`transform`。`fit`方法用于从训练数据中学习参数，而`transform`方法则使用这些参数来转换数据。任何需要转换的数据数组必须与用于拟合模型的数据数组具有相同的特征数量。
- en: 'The following figure illustrates how a transformer, fitted on the training
    data, is used to transform a training dataset as well as a new test dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何在训练数据上安装变换器，并将其用于转换训练数据集和新的测试数据集：
- en: '![](img/B13208_04_02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_02.png)'
- en: 'The classifiers that we used in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*, belong to the so-called **estimators** in scikit-learn, with
    an API that is conceptually very similar to the transformer class. Estimators
    have a `predict` method but can also have a `transform` method, as you will see
    later in this chapter. As you may recall, we also used the `fit` method to learn
    the parameters of a model when we trained those estimators for classification.
    However, in supervised learning tasks, we additionally provide the class labels
    for fitting the model, which can then be used to make predictions about new, unlabeled
    data examples via the `predict` method, as illustrated in the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*《使用scikit-learn的机器学习分类器巡礼》中使用的分类器属于scikit-learn中所谓的**估算器**，其API在概念上与变换器类非常相似。估算器具有`predict`方法，但也可以具有`transform`方法，正如你将在本章后面看到的那样。正如你可能记得的，我们在训练这些估算器进行分类时，也使用了`fit`方法来学习模型的参数。然而，在监督学习任务中，我们还提供了类标签来拟合模型，然后可以通过`predict`方法对新的未标记数据进行预测，如下图所示：
- en: '![](img/B13208_04_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_03.png)'
- en: Handling categorical data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理类别数据
- en: So far, we have only been working with numerical values. However, it is not
    uncommon for real-world datasets to contain one or more categorical feature columns.
    In this section, we will make use of simple yet effective examples to see how
    to deal with this type of data in numerical computing libraries.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了数值型数据。然而，现实世界中的数据集常常包含一个或多个类别特征列。在本节中，我们将使用简单而有效的示例，看看如何在数值计算库中处理这类数据。
- en: 'When we are talking about categorical data, we have to further distinguish
    between **ordinal** and **nominal** features. Ordinal features can be understood
    as categorical values that can be sorted or ordered. For example, t-shirt size
    would be an ordinal feature, because we can define an order: *XL* > *L* > *M*.
    In contrast, nominal features don''t imply any order and, to continue with the
    previous example, we could think of t-shirt color as a nominal feature since it
    typically doesn''t make sense to say that, for example, red is larger than blue.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论类别数据时，我们需要进一步区分**有序**和**无序**特征。可以理解为，有序特征是可以排序或排列的类别值。例如，T恤的尺寸是一个有序特征，因为我们可以定义顺序：*XL*
    > *L* > *M*。相比之下，无序特征没有任何排序含义，继续以T恤颜色为例，T恤颜色是无序特征，因为通常没有意义说红色比蓝色大。
- en: Categorical data encoding with pandas
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用pandas进行类别数据编码
- en: 'Before we explore different techniques for handling such categorical data,
    let''s create a new `DataFrame` to illustrate the problem:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨处理此类类别数据的不同技术之前，让我们创建一个新的`DataFrame`来说明问题：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see in the preceding output, the newly created `DataFrame` contains
    a nominal feature (`color`), an ordinal feature (`size`), and a numerical feature
    (`price`) column. The class labels (assuming that we created a dataset for a supervised
    learning task) are stored in the last column. The learning algorithms for classification
    that we discuss in this book do not use ordinal information in class labels.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的输出中所看到的，新创建的 `DataFrame` 包含一个名义特征（`color`）、一个顺序特征（`size`）和一个数值特征（`price`）列。类别标签（假设我们为监督学习任务创建了一个数据集）存储在最后一列。在本书中讨论的分类学习算法并不使用类别标签中的顺序信息。
- en: Mapping ordinal features
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射顺序特征
- en: 'To make sure that the learning algorithm interprets the ordinal features correctly,
    we need to convert the categorical string values into integers. Unfortunately,
    there is no convenient function that can automatically derive the correct order
    of the labels of our `size` feature, so we have to define the mapping manually.
    In the following simple example, let''s assume that we know the numerical difference
    between features, for example, *XL* = *L* + 1 = *M* + 2:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保学习算法正确解读顺序特征，我们需要将类别字符串值转换为整数。遗憾的是，没有方便的函数可以自动推导出我们 `size` 特征标签的正确顺序，因此我们必须手动定义映射。在以下简单示例中，假设我们知道特征之间的数值差异，例如，*XL*
    = *L* + 1 = *M* + 2：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we want to transform the integer values back to the original string representation
    at a later stage, we can simply define a reverse-mapping dictionary, `inv_size_mapping
    = {v: k for k, v in size_mapping.items()}`, which can then be used via the pandas
    `map` method on the transformed feature column and is similar to the `size_mapping`
    dictionary that we used previously. We can use it as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们希望在稍后的阶段将整数值转换回原始的字符串表示，我们可以简单地定义一个反向映射字典，`inv_size_mapping = {v: k for
    k, v in size_mapping.items()}`，然后通过 pandas 的 `map` 方法将其应用于转换后的特征列，它与我们之前使用的 `size_mapping`
    字典类似。我们可以按如下方式使用它：'
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Encoding class labels
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码类别标签
- en: 'Many machine learning libraries require that class labels are encoded as integer
    values. Although most estimators for classification in scikit-learn convert class
    labels to integers internally, it is considered good practice to provide class
    labels as integer arrays to avoid technical glitches. To encode the class labels,
    we can use an approach similar to the mapping of ordinal features discussed previously.
    We need to remember that class labels are *not* ordinal, and it doesn''t matter
    which integer number we assign to a particular string label. Thus, we can simply
    enumerate the class labels, starting at `0`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习库要求类别标签以整数值进行编码。虽然 scikit-learn 中的大多数分类估计器会内部将类别标签转换为整数，但提供整数数组作为类别标签是良好的实践，避免了技术问题。为了编码类别标签，我们可以使用与之前讨论的顺序特征映射类似的方法。我们需要记住，类别标签是*非顺序的*，并且我们分配给特定字符串标签的整数值无关紧要。因此，我们可以简单地枚举类别标签，从
    `0` 开始：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can use the mapping dictionary to transform the class labels into
    integers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用映射字典将类别标签转换为整数：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can reverse the key-value pairs in the mapping dictionary as follows to
    map the converted class labels back to the original string representation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过如下方式反转映射字典中的键值对，将转换后的类别标签映射回原始的字符串表示：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Alternatively, there is a convenient `LabelEncoder` class directly implemented
    in scikit-learn to achieve this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用 scikit-learn 中直接实现的 `LabelEncoder` 类来方便地完成这个任务：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that the `fit_transform` method is just a shortcut for calling `fit` and
    `transform` separately, and we can use the `inverse_transform` method to transform
    the integer class labels back into their original string representation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`fit_transform` 方法实际上是 `fit` 和 `transform` 方法的快捷方式，我们可以使用 `inverse_transform`
    方法将整数类别标签转换回原始的字符串表示：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Performing one-hot encoding on nominal features
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对名义特征进行独热编码
- en: 'In the previous *Mapping ordinal features* section, we used a simple dictionary-mapping
    approach to convert the ordinal `size` feature into integers. Since scikit-learn''s
    estimators for classification treat class labels as categorical data that does
    not imply any order (nominal), we used the convenient `LabelEncoder` to encode
    the string labels into integers. It may appear that we could use a similar approach
    to transform the nominal `color` column of our dataset, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面*映射顺序特征*的部分，我们使用了一个简单的字典映射方法，将有序的 `size` 特征转换为整数。由于 `scikit-learn` 的分类估算器将类标签视为没有任何顺序的类别数据（名义型数据），我们使用了方便的
    `LabelEncoder` 将字符串标签编码为整数。看起来我们也可以使用类似的方法，将数据集中的名义型 `color` 列转换为整数，如下所示：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After executing the preceding code, the first column of the NumPy array, `X`,
    now holds the new `color` values, which are encoded as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述代码后，NumPy 数组 `X` 的第一列现在保存了新的 `color` 值，这些值被编码如下：
- en: '`blue = 0`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blue = 0`'
- en: '`green = 1`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`green = 1`'
- en: '`red = 2`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`red = 2`'
- en: If we stop at this point and feed the array to our classifier, we will make
    one of the most common mistakes in dealing with categorical data. Can you spot
    the problem? Although the color values don't come in any particular order, a learning
    algorithm will now assume that `green` is larger than `blue`, and `red` is larger
    than `green`. Although this assumption is incorrect, the algorithm could still
    produce useful results. However, those results would not be optimal.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在此停止，并将数组输入到分类器中，我们将犯下处理类别数据时最常见的错误之一。你能发现问题吗？尽管颜色值没有任何特定顺序，但学习算法现在会假设 `green`
    大于 `blue`，并且 `red` 大于 `green`。尽管这个假设不正确，算法仍然可能产生有用的结果。然而，这些结果并不会是最优的。
- en: 'A common workaround for this problem is to use a technique called **one-hot
    encoding**. The idea behind this approach is to create a new dummy feature for
    each unique value in the nominal feature column. Here, we would convert the `color`
    feature into three new features: `blue`, `green`, and `red`. Binary values can
    then be used to indicate the particular `color` of an example; for example, a
    `blue` example can be encoded as `blue=1`, `green=0`, `red=0`. To perform this
    transformation, we can use the `OneHotEncoder` that is implemented in scikit-learn''s
    `preprocessing` module:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的常见方法是使用一种称为**独热编码**的技术。这种方法的思路是为名义型特征列中的每个唯一值创建一个新的虚拟特征。在这里，我们将把 `color`
    特征转换为三个新特征：`blue`、`green` 和 `red`。然后可以使用二进制值来表示一个示例的具体 `color`；例如，一个 `blue` 示例可以被编码为
    `blue=1`、`green=0`、`red=0`。为了执行这一转换，我们可以使用 `scikit-learn` 的 `preprocessing` 模块中实现的
    `OneHotEncoder`：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that we applied the `OneHotEncoder` to only a single column, `(X[:, 0].reshape(-1,
    1)))`, to avoid modifying the other two columns in the array as well. If we want
    to selectively transform columns in a multi-feature array, we can use the `ColumnTransformer`,
    which accepts a list of `(name, transformer, column(s))` tuples as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们只对单列 `(X[:, 0].reshape(-1, 1))` 应用了 `OneHotEncoder`，以避免修改数组中的其他两列。如果我们希望在多特征数组中有选择地转换列，可以使用
    `ColumnTransformer`，它接受一组 `(name, transformer, column(s))` 元组，如下所示：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code example, we specified that we want to modify only the
    first column and leave the other two columns untouched via the `'passthrough'`
    argument.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，我们通过`'passthrough'`参数指定只修改第一列，保持其他两列不变。
- en: 'An even more convenient way to create those dummy features via one-hot encoding
    is to use the `get_dummies` method implemented in pandas. Applied to a `DataFrame`,
    the `get_dummies` method will only convert string columns and leave all other
    columns unchanged:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pandas` 实现的 `get_dummies` 方法，是通过独热编码创建虚拟特征的一个更便捷的方式。应用于 `DataFrame` 时，`get_dummies`
    方法只会转换字符串类型的列，而保持其他列不变：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When we are using one-hot encoding datasets, we have to keep in mind that this
    introduces multicollinearity, which can be an issue for certain methods (for instance,
    methods that require matrix inversion). If features are highly correlated, matrices
    are computationally difficult to invert, which can lead to numerically unstable
    estimates. To reduce the correlation among variables, we can simply remove one
    feature column from the one-hot encoded array. Note that we do not lose any important
    information by removing a feature column, though; for example, if we remove the
    column `color_blue`, the feature information is still preserved since if we observe
    `color_green=0` and `color_red=0`, it implies that the observation must be `blue`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用独热编码数据集时，我们需要注意，这会引入多重共线性问题，这对某些方法（例如需要矩阵求逆的方法）可能是个问题。如果特征高度相关，则矩阵求逆在计算上会变得困难，这可能导致数值不稳定的估计。为了减少变量之间的相关性，我们可以简单地从独热编码数组中删除一个特征列。请注意，删除特征列并不会丢失任何重要信息；例如，如果我们删除了
    `color_blue` 列，特征信息仍然得到保留，因为如果我们观察到 `color_green=0` 和 `color_red=0`，则意味着观察结果必须是
    `blue`。
- en: 'If we use the `get_dummies` function, we can drop the first column by passing
    a `True` argument to the `drop_first` parameter, as shown in the following code
    example:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `get_dummies` 函数，我们可以通过将 `drop_first` 参数设置为 `True` 来删除第一列，如以下代码示例所示：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In order to drop a redundant column via the `OneHotEncoder`, we need to set
    `drop=''first''` and set `categories=''auto''` as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过 `OneHotEncoder` 删除冗余列，我们需要设置 `drop='first'`，并将 `categories='auto'` 设置如下：
- en: '[PRE21]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Optional: encoding ordinal features**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**可选：编码有序特征**'
- en: 'If we are unsure about the numerical differences between the categories of
    ordinal features, or the difference between two ordinal values is not defined,
    we can also encode them using a threshold encoding with 0/1 values. For example,
    we can split the feature `size` with values M, L, and XL into two new features,
    "x > M" and "x > L". Let''s consider the original DataFrame:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不确定有序特征类别之间的数值差异，或者两个有序值之间的差异没有定义，我们也可以使用阈值编码来对它们进行编码，使用 0/1 值。例如，我们可以将特征
    `size` 的值 M、L 和 XL 拆分为两个新特征，“x > M”和“x > L”。让我们来看一下原始 DataFrame：
- en: '[PRE22]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can use the apply method of pandas'' DataFrames to write custom lambda expressions
    in order to encode these variables using the value-threshold approach:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 pandas DataFrame 的 apply 方法编写自定义的 lambda 表达式，采用值阈值方法对这些变量进行编码：
- en: '[PRE23]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Partitioning a dataset into separate training and test datasets
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据集划分为独立的训练集和测试集
- en: We briefly introduced the concept of partitioning a dataset into separate datasets
    for training and testing in *Chapter 1*, *Giving Computers the Ability to Learn
    from Data*, and *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*.
    Remember that comparing predictions to true labels in the test set can be understood
    as the unbiased performance evaluation of our model before we let it loose on
    the real world. In this section, we will prepare a new dataset, the **Wine** dataset.
    After we have preprocessed the dataset, we will explore different techniques for
    feature selection to reduce the dimensionality of a dataset.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第 1 章*《让计算机从数据中学习》和 *第 3 章*《使用 scikit-learn 进行机器学习分类器的概览》中简要介绍了将数据集划分为独立的训练集和测试集的概念。请记住，在测试集上比较预测结果与真实标签，可以理解为我们对模型在将其应用于真实世界之前进行的无偏性能评估。在本节中，我们将准备一个新的数据集——**Wine**
    数据集。预处理完数据集后，我们将探索不同的特征选择技术，以减少数据集的维度。
- en: The Wine dataset is another open source dataset that is available from the UCI
    machine learning repository ([https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine));
    it consists of 178 wine examples with 13 features describing their different chemical
    properties.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Wine 数据集是另一个可以从 UCI 机器学习库获取的开源数据集（[https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)）；它包含
    178 个葡萄酒样本，具有描述不同化学属性的 13 个特征。
- en: '**Obtaining the Wine dataset**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取 Wine 数据集**'
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the dataset at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable on the UCI server. For instance, to load the Wine dataset
    from a local directory, you can replace this line:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中找到 Wine 数据集（以及本书中使用的所有其他数据集）的副本，如果你在离线工作或 UCI 服务器上的 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    临时不可用，可以使用该副本。例如，要从本地目录加载 Wine 数据集，你可以替换这一行：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'with the following one:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 采用如下方式：
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using the pandas library, we will directly read in the open source Wine dataset
    from the UCI machine learning repository:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pandas 库，我们将直接从 UCI 机器学习库中读取开源的 Wine 数据集：
- en: '[PRE26]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The 13 different features in the Wine dataset, describing the chemical properties
    of the 178 wine examples, are listed in the following table:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Wine 数据集中的 13 个不同特征，描述了 178 个葡萄酒样本的化学属性，列在下表中：
- en: '![](img/B13208_04_04.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_04.png)'
- en: The examples belong to one of three different classes, `1`, `2`, and `3`, which
    refer to the three different types of grape grown in the same region in Italy
    but derived from different wine cultivars, as described in the dataset summary
    ([https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names)).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些样本属于三种不同的类别，`1`、`2` 和 `3`，分别表示意大利同一地区种植的三种不同类型的葡萄，但它们来自不同的葡萄酒品种，具体见数据集概述（[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names)）。
- en: 'A convenient way to randomly partition this dataset into separate test and
    training datasets is to use the `train_test_split` function from scikit-learn''s
    `model_selection` submodule:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方便的方式是使用 scikit-learn 中 `model_selection` 子模块的 `train_test_split` 函数，随机将该数据集分割成测试集和训练集：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: First, we assigned the NumPy array representation of the feature columns 1-13
    to the variable `X` and we assigned the class labels from the first column to
    the variable `y`. Then, we used the `train_test_split` function to randomly split
    `X` and `y` into separate training and test datasets. By setting `test_size=0.3`,
    we assigned 30 percent of the wine examples to `X_test` and `y_test`, and the
    remaining 70 percent of the examples were assigned to `X_train` and `y_train`,
    respectively. Providing the class label array `y` as an argument to `stratify`
    ensures that both training and test datasets have the same class proportions as
    the original dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将特征列 1-13 的 NumPy 数组表示赋值给变量 `X`，将第一列的类别标签赋值给变量 `y`。然后，我们使用 `train_test_split`
    函数将 `X` 和 `y` 随机分割为训练集和测试集。通过设置 `test_size=0.3`，我们将 30% 的葡萄酒样本分配给 `X_test` 和 `y_test`，其余
    70% 的样本分别分配给 `X_train` 和 `y_train`。将类别标签数组 `y` 作为 `stratify` 的参数，确保训练集和测试集的类别比例与原始数据集一致。
- en: '**Choosing an appropriate ratio for partitioning a dataset into training and
    test datasets**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择适当的比例将数据集分割为训练集和测试集**'
- en: If we are dividing a dataset into training and test datasets, we have to keep
    in mind that we are withholding valuable information that the learning algorithm
    could benefit from. Thus, we don't want to allocate too much information to the
    test set. However, the smaller the test set, the more inaccurate the estimation
    of the generalization error. Dividing a dataset into training and test datasets
    is all about balancing this tradeoff. In practice, the most commonly used splits
    are 60:40, 70:30, or 80:20, depending on the size of the initial dataset. However,
    for large datasets, 90:10 or 99:1 splits are also common and appropriate. For
    example, if the dataset contains more than 100,000 training examples, it might
    be fine to withhold only 10,000 examples for testing in order to get a good estimate
    of the generalization performance. More information and illustrations can be found
    in section one of my article *Model evaluation, model selection, and algorithm
    selection in machine learning*, which is freely available at [https://arxiv.org/pdf/1811.12808.pdf](https://arxiv.org/pdf/1811.12808.pdf).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据集划分为训练集和测试集，我们必须记住，我们是在扣留学习算法可能受益的有价值信息。因此，我们不希望将过多信息分配给测试集。然而，测试集越小，泛化误差的估计就会越不准确。将数据集划分为训练集和测试集的关键在于平衡这种权衡。在实践中，最常用的划分比例是60:40、70:30或80:20，具体取决于初始数据集的大小。然而，对于大型数据集，90:10或99:1的划分也很常见且适用。例如，如果数据集包含超过100,000个训练样本，可能只需保留10,000个样本用于测试，就能很好地估算泛化性能。更多信息和示例可以在我的文章《模型评估、模型选择和算法选择》中找到，该文章在[https://arxiv.org/pdf/1811.12808.pdf](https://arxiv.org/pdf/1811.12808.pdf)上可以免费获取。
- en: Moreover, instead of discarding the allocated test data after model training
    and evaluation, it is a common practice to retrain a classifier on the entire
    dataset, as it can improve the predictive performance of the model. While this
    approach is generally recommended, it could lead to worse generalization performance
    if the dataset is small and the test dataset contains outliers, for example. Also,
    after refitting the model on the whole dataset, we don't have any independent
    data left to evaluate its performance.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型训练和评估后，不是丢弃分配的测试数据，而是通常会在整个数据集上重新训练分类器，因为这样可以提高模型的预测性能。虽然这种方法通常被推荐，但如果数据集较小且测试数据集中包含异常值，比如说，可能会导致更差的泛化性能。另外，在对整个数据集重新拟合模型后，我们就没有任何独立的数据来评估其性能了。
- en: Bringing features onto the same scale
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将特征映射到相同的尺度
- en: '**Feature scaling** is a crucial step in our preprocessing pipeline that can
    easily be forgotten. **Decision trees** and **random forests** are two of the
    very few machine learning algorithms where we don''t need to worry about feature
    scaling. Those algorithms are scale invariant. However, the majority of machine
    learning and optimization algorithms behave much better if features are on the
    same scale, as we saw in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*, when we implemented the **gradient descent optimization**
    algorithm.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征缩放**是我们预处理管道中的一个关键步骤，然而它很容易被遗忘。**决策树**和**随机森林**是少数几个我们不需要担心特征缩放的机器学习算法。这些算法对尺度具有不变性。然而，大多数机器学习和优化算法在特征处于相同尺度时表现得更好，正如我们在《第2章：训练简单机器学习算法进行分类》中看到的，当我们实现**梯度下降优化**算法时就是这样。'
- en: The importance of feature scaling can be illustrated by a simple example. Let's
    assume that we have two features where one feature is measured on a scale from
    1 to 10 and the second feature is measured on a scale from 1 to 100,000, respectively.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放的重要性可以通过一个简单的例子来说明。假设我们有两个特征，其中一个特征的量表范围是1到10，另一个特征的量表范围是1到100,000。
- en: 'When we think of the squared error function in Adaline from *Chapter 2*, *Training
    Simple Machine Learning Algorithms for Classification*, it makes sense to say
    that the algorithm will mostly be busy optimizing the weights according to the
    larger errors in the second feature. Another example is the **k-nearest neighbors**
    (**KNN**) algorithm with a Euclidean distance measure: the computed distances
    between examples will be dominated by the second feature axis.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们回想起《第2章：训练简单机器学习算法进行分类》中的Adaline的平方误差函数时，我们可以理解为什么说该算法主要会根据第二个特征中的较大误差来优化权重。另一个例子是**k近邻**（**KNN**）算法，它使用欧几里得距离度量：计算出的样本之间的距离将主要受第二个特征轴的影响。
- en: 'Now, there are two common approaches to bringing different features onto the
    same scale: **normalization** and **standardization**. Those terms are often used
    quite loosely in different fields, and the meaning has to be derived from the
    context. Most often, normalization refers to the rescaling of the features to
    a range of [0, 1], which is a special case of **min-max scaling**. To normalize
    our data, we can simply apply the min-max scaling to each feature column, where
    the new value, ![](img/B13208_04_001.png), of an example, ![](img/B13208_04_002.png),
    can be calculated as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有两种常见的方法将不同的特征带到相同的尺度：**归一化**和**标准化**。这些术语在不同领域中经常被使用得比较随意，其含义需要从上下文中推断出来。通常，归一化指的是将特征重缩放到[0,
    1]的范围内，这实际上是**最小-最大缩放**的一种特殊情况。为了归一化我们的数据，我们可以简单地对每个特征列应用最小-最大缩放，其中某个示例的新的值![](img/B13208_04_001.png)，![](img/B13208_04_002.png)，可以按如下方式计算：
- en: '![](img/B13208_04_003.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_003.png)'
- en: Here, ![](img/B13208_04_004.png) is a particular example, ![](img/B13208_04_005.png)
    is the smallest value in a feature column, and ![](img/B13208_04_006.png) is the
    largest value.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_04_004.png)是一个特定的例子，![](img/B13208_04_005.png)是特征列中的最小值，而![](img/B13208_04_006.png)是最大值。
- en: 'The min-max scaling procedure is implemented in scikit-learn and can be used
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放过程已经在scikit-learn中实现，可以如下使用：
- en: '[PRE28]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Although normalization via min-max scaling is a commonly used technique that
    is useful when we need values in a bounded interval, standardization can be more
    practical for many machine learning algorithms, especially for optimization algorithms
    such as gradient descent. The reason is that many linear models, such as the logistic
    regression and SVM from *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    scikit-learn*, initialize the weights to 0 or small random values close to 0\.
    Using standardization, we center the feature columns at mean 0 with standard deviation
    1 so that the feature columns have the same parameters as a standard normal distribution
    (zero mean and unit variance), which makes it easier to learn the weights. Furthermore,
    standardization maintains useful information about outliers and makes the algorithm
    less sensitive to them in contrast to min-max scaling, which scales the data to
    a limited range of values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过最小-最大缩放进行归一化是一种常用的技术，适用于我们需要将值限定在一个有界区间内的情况，但标准化对于许多机器学习算法来说可能更为实用，特别是对于像梯度下降这样的优化算法。原因是许多线性模型，如*第3章*中提到的逻辑回归和SVM（*使用scikit-learn进行机器学习分类器巡礼*），通常将权重初始化为0或接近0的小随机值。通过标准化，我们将特征列的均值置为0，标准差为1，这样特征列具有与标准正态分布（零均值和单位方差）相同的参数，从而更容易学习权重。此外，标准化保持了关于离群值的有用信息，并且使得算法对离群值不那么敏感，而与最小-最大缩放不同，后者将数据缩放到有限的值范围内。
- en: 'The procedure for standardization can be expressed by the following equation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的过程可以通过以下方程式表示：
- en: '![](img/B13208_04_007.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_007.png)'
- en: Here, ![](img/B13208_04_008.png) is the sample mean of a particular feature
    column, and ![](img/B13208_04_009.png) is the corresponding standard deviation.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_04_008.png)是特定特征列的样本均值，而![](img/B13208_04_009.png)是对应的标准差。
- en: 'The following table illustrates the difference between the two commonly used
    feature scaling techniques, standardization and normalization, on a simple example
    dataset consisting of numbers 0 to 5:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了标准化和归一化两种常用特征缩放技术在一个简单示例数据集（包含0到5的数字）中的区别：
- en: '| **Input** | **Standardized** | **Min-max normalized** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **标准化** | **最小-最大归一化** |'
- en: '| 0.0 | -1.46385 | 0.0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 0.0 | -1.46385 | 0.0 |'
- en: '| 1.0 | -0.87831 | 0.2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | -0.87831 | 0.2 |'
- en: '| 2.0 | -0.29277 | 0.4 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | -0.29277 | 0.4 |'
- en: '| 3.0 | 0.29277 | 0.6 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 3.0 | 0.29277 | 0.6 |'
- en: '| 4.0 | 0.87831 | 0.8 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 4.0 | 0.87831 | 0.8 |'
- en: '| 5.0 | 1.46385 | 1.0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 5.0 | 1.46385 | 1.0 |'
- en: 'You can perform the standardization and normalization shown in the table manually
    by executing the following code examples:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过执行以下代码示例手动进行标准化和归一化，如表所示：
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Similar to the `MinMaxScaler` class, scikit-learn also implements a class for
    standardization:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与`MinMaxScaler`类类似，scikit-learn还实现了标准化的类：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Again, it is also important to highlight that we fit the `StandardScaler` class
    only once—on the training data—and use those parameters to transform the test
    dataset or any new data point.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们仅在训练数据上拟合一次`StandardScaler`类，并使用这些参数来转换测试数据集或任何新的数据点。
- en: Other, more advanced methods for feature scaling are available from scikit-learn,
    such as the `RobustScaler`. The `RobustScaler` is especially helpful and recommended
    if we are working with small datasets that contain many outliers. Similarly, if
    the machine learning algorithm applied to this dataset is prone to **overfitting**,
    the `RobustScaler` can be a good choice. Operating on each feature column independently,
    the `RobustScaler` removes the median value and scales the dataset according to
    the 1st and 3rd quartile of the dataset (that is, the 25th and 75th quantile,
    respectively) such that more extreme values and outliers become less pronounced.
    The interested reader can find more information about the `RobustScaler` in the
    official scikit-learn documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了其他更先进的特征缩放方法，如 `RobustScaler`。如果我们处理的是包含许多异常值的小型数据集，`RobustScaler`尤其有帮助并且推荐使用。类似地，如果应用于该数据集的机器学习算法容易**过拟合**，那么
    `RobustScaler` 是一个不错的选择。`RobustScaler` 独立地操作每个特征列，去除中位数值，并根据数据集的第1四分位数和第3四分位数（即第25和第75百分位数）对数据集进行缩放，从而使极端值和异常值不那么显著。感兴趣的读者可以在官方的
    scikit-learn 文档中找到有关 `RobustScaler` 的更多信息，链接：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)
- en: Selecting meaningful features
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择有意义的特征
- en: 'If we notice that a model performs much better on a training dataset than on
    the test dataset, this observation is a strong indicator of overfitting. As we
    discussed in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*,
    overfitting means the model fits the parameters too closely with regard to the
    particular observations in the training dataset, but does not generalize well
    to new data; we say that the model has a **high variance**. The reason for the
    overfitting is that our model is too complex for the given training data. Common
    solutions to reduce the generalization error are as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们注意到模型在训练数据集上的表现远远优于在测试数据集上的表现，这一观察结果是过拟合的强烈指示。正如我们在*第三章*，*使用 scikit-learn
    进行机器学习分类器巡礼*中所讨论的，过拟合意味着模型对训练数据集中的特定观察结果拟合得过于紧密，但对新数据的泛化能力较差；我们称模型具有**高方差**。过拟合的原因是我们的模型对于给定的训练数据过于复杂。减少泛化误差的常见解决方案如下：
- en: Collect more training data
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多的训练数据
- en: Introduce a penalty for complexity via regularization
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过正则化引入复杂度的惩罚
- en: Choose a simpler model with fewer parameters
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个更简单的模型，减少参数数量
- en: Reduce the dimensionality of the data
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低数据的维度
- en: Collecting more training data is often not applicable. In *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we will learn
    about a useful technique to check whether more training data is helpful. In the
    following sections, we will look at common ways to reduce overfitting by regularization
    and dimensionality reduction via feature selection, which leads to simpler models
    by requiring fewer parameters to be fitted to the data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 收集更多训练数据通常不可行。在*第六章*，*模型评估和超参数调优的最佳实践学习*中，我们将学习一种有用的技术来检查更多的训练数据是否有效。在接下来的章节中，我们将探讨通过正则化和特征选择进行降维的常见方法，从而通过减少需要拟合到数据的参数数量来简化模型。
- en: L1 and L2 regularization as penalties against model complexity
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化作为针对模型复杂度的惩罚
- en: 'You will recall from *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    scikit-learn*, that **L2 regularization** is one approach to reduce the complexity
    of a model by penalizing large individual weights. We defined the squared L2 norm
    of our weight vector, *w*, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得在*第三章*，*使用 scikit-learn 进行机器学习分类器巡礼*中，**L2 正则化**是通过惩罚较大的单个权重来减少模型复杂度的一种方法。我们将权重向量
    *w* 的平方 L2 范数定义如下：
- en: '![](img/B13208_04_010.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_010.png)'
- en: 'Another approach to reduce the model complexity is the related **L1 regularization**:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少模型复杂度的方法是相关的**L1 正则化**：
- en: '![](img/B13208_04_011.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_011.png)'
- en: Here, we simply replaced the square of the weights by the sum of the absolute
    values of the weights. In contrast to L2 regularization, L1 regularization usually
    yields sparse feature vectors and most feature weights will be zero. Sparsity
    can be useful in practice if we have a high-dimensional dataset with many features
    that are irrelevant, especially in cases where we have more irrelevant dimensions
    than training examples. In this sense, L1 regularization can be understood as
    a technique for feature selection.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是将权重的平方替换为权重绝对值的和。与L2正则化不同，L1正则化通常会产生稀疏的特征向量，大多数特征权重会为零。如果我们有一个高维数据集，且许多特征是无关的，稀疏性在实践中会非常有用，特别是在特征维度比训练样本还多的情况下。从这个角度看，L1正则化可以理解为一种特征选择技术。
- en: A geometric interpretation of L2 regularization
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2正则化的几何解释
- en: As mentioned in the previous section, L2 regularization adds a penalty term
    to the cost function that effectively results in less extreme weight values compared
    to a model trained with an unregularized cost function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，L2正则化向成本函数添加一个惩罚项，这使得相比于使用未正则化成本函数训练的模型，得到的权重值更加温和。
- en: To better understand how L1 regularization encourages sparsity, let's take a
    step back and take a look at a geometric interpretation of regularization. Let's
    plot the contours of a convex cost function for two weight coefficients, ![](img/B13208_04_012.png)
    and ![](img/B13208_04_013.png).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解L1正则化如何鼓励稀疏性，让我们退一步，看看正则化的几何解释。我们将绘制两个权重系数的凸成本函数的等高线，![](img/B13208_04_012.png)
    和 ![](img/B13208_04_013.png)。
- en: 'Here, we will consider the **sum of squared errors** (**SSE**) cost function
    that we used for Adaline in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*, since it is spherical and easier to draw than the cost function
    of logistic regression; however, the same concepts apply. Remember that our goal
    is to find the combination of weight coefficients that minimize the cost function
    for the training data, as shown in the following figure (the point in the center
    of the ellipses):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将考虑**平方误差和**（**SSE**）成本函数，这是我们在*第二章*《训练简单的机器学习分类算法》中为Adaline使用的，因为它是球形的，比逻辑回归的成本函数更容易绘制；但是，相同的概念适用。记住，我们的目标是找到最小化训练数据成本函数的权重系数组合，如下图所示（椭圆中心的点）：
- en: '![](img/B13208_04_05.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_05.png)'
- en: 'We can think of regularization as adding a penalty term to the cost function
    to encourage smaller weights; in other words, we penalize large weights. Thus,
    by increasing the regularization strength via the regularization parameter, ![](img/B13208_04_014.png),
    we shrink the weights toward zero and decrease the dependence of our model on
    the training data. Let''s illustrate this concept in the following figure for
    the L2 penalty term:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把正则化看作是向成本函数中添加一个惩罚项，以鼓励较小的权重；换句话说，我们对大权重进行惩罚。因此，通过通过正则化参数增加正则化强度，![](img/B13208_04_014.png)，我们将权重收缩至零，并减少模型对训练数据的依赖。让我们通过以下图示来说明L2惩罚项的概念：
- en: '![](img/B13208_04_06.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_06.png)'
- en: The quadratic L2 regularization term is represented by the shaded ball. Here,
    our weight coefficients cannot exceed our regularization budget—the combination
    of the weight coefficients cannot fall outside the shaded area. On the other hand,
    we still want to minimize the cost function. Under the penalty constraint, our
    best effort is to choose the point where the L2 ball intersects with the contours
    of the unpenalized cost function. The larger the value of the regularization parameter,
    ![](img/B13208_04_015.png), gets, the faster the penalized cost grows, which leads
    to a narrower L2 ball. For example, if we increase the regularization parameter
    towards infinity, the weight coefficients will become effectively zero, denoted
    by the center of the L2 ball. To summarize the main message of the example, our
    goal is to minimize the sum of the unpenalized cost plus the penalty term, which
    can be understood as adding bias and preferring a simpler model to reduce the
    variance in the absence of sufficient training data to fit the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 二次 L2 正则化项由阴影球体表示。在这里，我们的权重系数不能超过我们的正则化预算——权重系数的组合不能超出阴影区域。另一方面，我们仍然希望最小化成本函数。在惩罚约束下，我们的最佳努力是选择
    L2 球体与未加惩罚的成本函数等高线交点的位置。正则化参数的值越大，惩罚成本增长得越快，从而导致 L2 球体变得更窄。例如，如果我们将正则化参数增大至无穷大，权重系数将有效地变为零，表示为
    L2 球体的中心。总结这个例子的主要信息，我们的目标是最小化未加惩罚的成本与惩罚项的和，可以理解为增加偏置并倾向于选择更简单的模型，以减少在缺乏足够训练数据的情况下对模型的方差拟合。
- en: Sparse solutions with L1 regularization
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 正则化下的稀疏解
- en: 'Now, let''s discuss L1 regularization and sparsity. The main concept behind
    L1 regularization is similar to what we discussed in the previous section. However,
    since the L1 penalty is the sum of the absolute weight coefficients (remember
    that the L2 term is quadratic), we can represent it as a diamond-shape budget,
    as shown in the following figure:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下 L1 正则化与稀疏性。L1 正则化背后的主要概念与我们在前一节讨论的相似。然而，由于 L1 惩罚是绝对权重系数的和（记住 L2 项是二次的），我们可以将其表示为一个菱形预算，如下图所示：
- en: '![](img/B13208_04_07.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_07.png)'
- en: In the preceding figure, we can see that the contour of the cost function touches
    the L1 diamond at ![](img/B13208_04_016.png). Since the contours of an L1 regularized
    system are sharp, it is more likely that the optimum—that is, the intersection
    between the ellipses of the cost function and the boundary of the L1 diamond—is
    located on the axes, which encourages sparsity.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到成本函数的等高线在 ![](img/B13208_04_016.png) 处触及 L1 菱形。由于 L1 正则化系统的等高线较为尖锐，因此最优解——即成本函数的椭圆与
    L1 菱形边界的交点——更可能位于坐标轴上，这有助于稀疏性。
- en: '**L1 regularization and sparsity**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1 正则化与稀疏性**'
- en: The mathematical details of why L1 regularization can lead to sparse solutions
    are beyond the scope of this book. If you are interested, an excellent explanation
    of L2 versus L1 regularization can be found in *Section 3.4*, *The Elements of
    Statistical Learning*, *Trevor Hastie*, *Robert Tibshirani*, and *Jerome Friedman*,
    *Springer Science*+*Business Media*, *2009*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化能够导致稀疏解的数学细节超出了本书的范围。如果你有兴趣，可以参考*《统计学习的元素》*（*The Elements of Statistical
    Learning*）*第三章 3.4 节*，其中对 L2 与 L1 正则化的解释非常出色，作者为*Trevor Hastie*、*Robert Tibshirani*
    和 *Jerome Friedman*，出版于*Springer Science*+*Business Media*，*2009*年。
- en: 'For regularized models in scikit-learn that support L1 regularization, we can
    simply set the `penalty` parameter to `''l1''` to obtain a sparse solution:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于支持 L1 正则化的 scikit-learn 正则化模型，我们只需将 `penalty` 参数设置为 `'l1'`，即可获得稀疏解：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that we also need to select a different optimization algorithm (for example,
    `solver=''liblinear''`), since `''lbfgs''` currently does not support L1-regularized
    loss optimization. Applied to the standardized Wine data, the L1 regularized logistic
    regression would yield the following sparse solution:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还需要选择一个不同的优化算法（例如，`solver='liblinear'`），因为 `'lbfgs'` 当前不支持 L1 正则化的损失优化。应用于标准化后的葡萄酒数据，L1
    正则化的逻辑回归将得到如下稀疏解：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Both training and test accuracies (both 100 percent) indicate that our model
    does a perfect job on both datasets. When we access the intercept terms via the
    `lr.intercept_` attribute, we can see that the array returns three values:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试的准确率（均为 100%）表明我们的模型在两个数据集上都表现得非常完美。当我们通过 `lr.intercept_` 属性访问截距项时，可以看到返回的数组包含三个值：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Since we fit the `LogisticRegression` object on a multiclass dataset via the
    **one-vs.-rest** (**OvR**) approach, the first intercept belongs to the model
    that fits class 1 versus classes 2 and 3, the second value is the intercept of
    the model that fits class 2 versus classes 1 and 3, and the third value is the
    intercept of the model that fits class 3 versus classes 1 and 2:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通过**一对其余**（**OvR**）方法在多类数据集上拟合了`LogisticRegression`对象，第一个截距属于拟合类1与类2、类3之间的模型，第二个值是拟合类2与类1、类3之间的模型的截距，第三个值是拟合类3与类1、类2之间的模型的截距：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The weight array that we accessed via the `lr.coef_` attribute contains three
    rows of weight coefficients, one weight vector for each class. Each row consists
    of 13 weights, where each weight is multiplied by the respective feature in the
    13-dimensional Wine dataset to calculate the net input:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`lr.coef_`属性访问到的权重数组包含三行权重系数，每行对应一个类别的权重向量。每一行包含13个权重，每个权重与13维的Wine数据集中的相应特征相乘，以计算净输入：
- en: '![](img/B13208_04_017.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_017.png)'
- en: '**Accessing the bias and weight parameters of scikit-learn estimators**'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**访问scikit-learn估计器的偏置和权重参数**'
- en: In scikit-learn, the `intercept_` corresponds to ![](img/B13208_04_018.png)
    and `coef_`corresponds to the values ![](img/B13208_04_019.png) for *j* > 0.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`intercept_`对应于 ![](img/B13208_04_018.png)，`coef_`对应于 *j* > 0时的值
    ![](img/B13208_04_019.png)。
- en: As a result of L1 regularization, which, as mentioned, serves as a method for
    feature selection, we just trained a model that is robust to the potentially irrelevant
    features in this dataset. Strictly speaking, though, the weight vectors from the
    previous example are not necessarily sparse because they contain more non-zero
    than zero entries. However, we could enforce sparsity (more zero entries) by further
    increasing the regularization strength—that is, choosing lower values for the
    `C` parameter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 由于L1正则化的结果，正如前文所述，L1正则化作为特征选择的一种方法，我们刚刚训练了一个对数据集中可能不相关特征具有鲁棒性的模型。然而，严格来说，前一个示例中的权重向量不一定是稀疏的，因为它们包含的非零项多于零项。然而，我们可以通过进一步增加正则化强度来强制执行稀疏性（即选择较低的`C`参数值）。
- en: 'In the last example on regularization in this chapter, we will vary the regularization
    strength and plot the regularization path—the weight coefficients of the different
    features for different regularization strengths:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章最后一个关于正则化的示例中，我们将调整正则化强度并绘制正则化路径——不同正则化强度下不同特征的权重系数：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting plot provides us with further insights into the behavior of L1
    regularization. As we can see, all feature weights will be zero if we penalize
    the model with a strong regularization parameter (*C* < 0.01); *C* is the inverse
    of the regularization parameter, ![](img/B13208_04_020.png):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表为我们提供了进一步的见解，帮助我们了解L1正则化的行为。如我们所见，如果我们用强正则化参数惩罚模型（*C* < 0.01），所有特征权重将为零；*C*是正则化参数的倒数，！[](img/B13208_04_020.png)：
- en: '![](img/B13208_04_08.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_08.png)'
- en: Sequential feature selection algorithms
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序特征选择算法
- en: 'An alternative way to reduce the complexity of the model and avoid overfitting
    is **dimensionality reduction** via feature selection, which is especially useful
    for unregularized models. There are two main categories of dimensionality reduction
    techniques: **feature selection** and **feature extraction**. Via feature selection,
    we select a subset of the original features, whereas in feature extraction, we
    derive information from the feature set to construct a new feature subspace.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型复杂度并避免过拟合的另一种方法是通过特征选择进行**降维**，这对于未经正则化的模型尤其有用。降维技术主要分为两类：**特征选择**和**特征提取**。通过特征选择，我们选择原始特征的子集，而在特征提取中，我们从特征集合中提取信息来构造一个新的特征子空间。
- en: In this section, we will take a look at a classic family of feature selection
    algorithms. In the next chapter, *Chapter 5*, *Compressing Data via Dimensionality
    Reduction*, we will learn about different feature extraction techniques to compress
    a dataset onto a lower-dimensional feature subspace.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一类经典的特征选择算法。在下一章，*第5章*，*通过降维压缩数据*，我们将学习不同的特征提取技术，将数据集压缩到低维特征子空间。
- en: Sequential feature selection algorithms are a family of greedy search algorithms
    that are used to reduce an initial *d*-dimensional feature space to a *k*-dimensional
    feature subspace where *k*<*d*. The motivation behind feature selection algorithms
    is to automatically select a subset of features that are most relevant to the
    problem, to improve computational efficiency, or to reduce the generalization
    error of the model by removing irrelevant features or noise, which can be useful
    for algorithms that don't support regularization.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序特征选择算法是一类贪心搜索算法，旨在将初始的 *d* 维特征空间减少到一个 *k* 维的特征子空间，其中 *k*<*d*。特征选择算法的动机是自动选择与问题最相关的特征子集，以提高计算效率，或者通过移除无关特征或噪声来减少模型的泛化误差，这对于不支持正则化的算法尤为有用。
- en: A classic sequential feature selection algorithm is **sequential backward selection**
    (**SBS**), which aims to reduce the dimensionality of the initial feature subspace
    with a minimum decay in the performance of the classifier to improve upon computational
    efficiency. In certain cases, SBS can even improve the predictive power of the
    model if a model suffers from overfitting.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经典的顺序特征选择算法是 **顺序后向选择**（**SBS**），其目标是在尽量减少分类器性能下降的情况下，降低初始特征子空间的维度，从而提高计算效率。在某些情况下，如果模型存在过拟合，SBS甚至可以提高模型的预测能力。
- en: '**Greedy search algorithms**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪心搜索算法**'
- en: '**Greedy algorithms** make locally optimal choices at each stage of a combinatorial
    search problem and generally yield a suboptimal solution to the problem, in contrast
    to **exhaustive search algorithms**, which evaluate all possible combinations
    and are guaranteed to find the optimal solution. However, in practice, an exhaustive
    search is often computationally not feasible, whereas greedy algorithms allow
    for a less complex, computationally more efficient solution.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪心算法**在组合搜索问题的每个阶段做出局部最优选择，通常会导致次优解，而与之相对的是 **穷举搜索算法**，它评估所有可能的组合，并保证找到最优解。然而，在实际应用中，穷举搜索通常计算上不可行，而贪心算法则提供了一种更简单、计算上更高效的解决方案。'
- en: 'The idea behind the SBS algorithm is quite simple: SBS sequentially removes
    features from the full feature subset until the new feature subspace contains
    the desired number of features. In order to determine which feature is to be removed
    at each stage, we need to define the criterion function, *J*, that we want to
    minimize.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: SBS算法背后的思想相当简单：SBS顺序地从完整特征子集中移除特征，直到新的特征子空间包含所需的特征数量。为了确定每个阶段要移除哪个特征，我们需要定义准则函数
    *J*，以便最小化它。
- en: 'The criterion calculated by the criterion function can simply be the difference
    in performance of the classifier before and after the removal of a particular
    feature. Then, the feature to be removed at each stage can simply be defined as
    the feature that maximizes this criterion; or in more simple terms, at each stage
    we eliminate the feature that causes the least performance loss after removal.
    Based on the preceding definition of SBS, we can outline the algorithm in four
    simple steps:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由准则函数计算出的准则可以简单地是分类器在移除特定特征前后的性能差异。然后，在每个阶段要移除的特征可以简单地定义为最大化该准则的特征；或者用更简单的说法，每个阶段我们移除那个在移除后造成性能损失最小的特征。根据前述的SBS定义，我们可以将算法概括为四个简单的步骤：
- en: Initialize the algorithm with *k* = *d*, where *d* is the dimensionality of
    the full feature space, ![](img/B13208_04_021.png).
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化算法时，设定 *k* = *d*，其中 *d* 是完整特征空间的维度，![](img/B13208_04_021.png)。
- en: 'Determine the feature, ![](img/B13208_04_022.png), that maximizes the criterion:
    ![](img/B13208_04_023.png), where ![](img/B13208_04_024.png).'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定最大化准则的特征，![](img/B13208_04_022.png)，其准则为：![](img/B13208_04_023.png)，其中！[](img/B13208_04_024.png)。
- en: 'Remove the feature, ![](img/B13208_04_025.png), from the feature set: ![](img/B13208_04_026.png).'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征集 ![](img/B13208_04_026.png) 中移除特征 ![](img/B13208_04_025.png)。
- en: Terminate if *k* equals the number of desired features; otherwise, go to step
    2.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *k* 等于所需的特征数，则终止；否则，转到步骤2。
- en: '**A resource on sequential feature algorithms**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序特征算法的资源**'
- en: You can find a detailed evaluation of several sequential feature algorithms
    in *Comparative Study of Techniques for Large-Scale Feature Selection*, *F. Ferri*,
    *P. Pudil*, *M. Hatef*, and *J. Kittler*, pages 403-413, *1994*.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *《大规模特征选择技术的比较研究》* 中找到对几种顺序特征算法的详细评估，*F. Ferri*、*P. Pudil*、*M. Hatef* 和
    *J. Kittler*，第403-413页，*1994*。
- en: 'Unfortunately, the SBS algorithm has not been implemented in scikit-learn yet.
    But since it is so simple, let''s go ahead and implement it in Python from scratch:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，SBS算法还未在scikit-learn中实现。但由于它非常简单，我们就来手动实现它，使用Python从头开始编写：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding implementation, we defined the `k_features` parameter to specify
    the desired number of features we want to return. By default, we use the `accuracy_score`
    from scikit-learn to evaluate the performance of a model (an estimator for classification)
    on the feature subsets.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的实现中，我们定义了`k_features`参数来指定我们希望返回的特征数量。默认情况下，我们使用来自scikit-learn的`accuracy_score`来评估模型（分类估计器）在特征子集上的表现。
- en: Inside the `while` loop of the `fit` method, the feature subsets created by
    the `itertools.combination` function are evaluated and reduced until the feature
    subset has the desired dimensionality. In each iteration, the accuracy score of
    the best subset is collected in a list, `self.scores_`, based on the internally
    created test dataset, `X_test`. We will use those scores later to evaluate the
    results. The column indices of the final feature subset are assigned to `self.indices_`,
    which we can use via the `transform` method to return a new data array with the
    selected feature columns. Note that, instead of calculating the criterion explicitly
    inside the `fit` method, we simply removed the feature that is not contained in
    the best performing feature subset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在`fit`方法的`while`循环内部，`itertools.combination`函数创建的特征子集被评估并逐步减少，直到特征子集具有所需的维度。在每次迭代中，基于内部创建的测试数据集`X_test`，最佳子集的准确度得分会被收集到一个列表`self.scores_`中。我们稍后会使用这些得分来评估结果。最终特征子集的列索引被分配给`self.indices_`，我们可以通过`transform`方法使用它，返回一个包含所选特征列的新数据数组。请注意，`fit`方法内部并没有显式地计算标准，而是通过直接移除不在最佳表现特征子集中的特征来实现。
- en: 'Now, let''s see our SBS implementation in action using the KNN classifier from
    scikit-learn:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看使用scikit-learn中的KNN分类器的SBS实现效果：
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Although our SBS implementation already splits the dataset into a test and training
    dataset inside the `fit` function, we still fed the training dataset, `X_train`,
    to the algorithm. The SBS `fit` method will then create new training subsets for
    testing (validation) and training, which is why this test set is also called the
    **validation dataset**. This approach is necessary to prevent our *original* test
    set from becoming part of the training data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的SBS实现已经在`fit`函数内部将数据集拆分为测试集和训练集，但我们仍然将训练数据集`X_train`输入给算法。SBS的`fit`方法会为测试（验证）和训练创建新的训练子集，这就是为什么这个测试集也被称为**验证数据集**。这种方法是必要的，以防我们的*原始*测试集成为训练数据的一部分。
- en: 'Remember that our SBS algorithm collects the scores of the best feature subset
    at each stage, so let''s move on to the more exciting part of our implementation
    and plot the classification accuracy of the KNN classifier that was calculated
    on the validation dataset. The code is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的SBS算法会在每个阶段收集最佳特征子集的得分，因此让我们进入实现中更激动人心的部分，绘制基于验证数据集计算的KNN分类器的分类准确率。代码如下：
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As we can see in the following figure, the accuracy of the KNN classifier improved
    on the validation dataset as we reduced the number of features, which is likely
    due to a decrease in the **curse of dimensionality** that we discussed in the
    context of the KNN algorithm in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*. Also, we can see in the following plot that the classifier
    achieved 100 percent accuracy for *k*={*3, 7, 8, 9, 10, 11, 12*}:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们可以看到，当我们减少特征数量时，KNN分类器在验证数据集上的准确度得到了提升，这很可能是由于我们在*第3章*《使用scikit-learn进行机器学习分类器之旅》中讨论的**维度灾难**的减少。同时，我们也可以在下图中看到，分类器在*k*={*3,
    7, 8, 9, 10, 11, 12*}时，达到了100%的准确率：
- en: '![](img/B13208_04_09.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_09.png)'
- en: 'To satisfy our own curiosity, let''s see what the smallest feature subset (*k*=*3*),
    which yielded such a good performance on the validation dataset, looks like:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足我们的好奇心，让我们看看最小的特征子集（*k*=*3*），它在验证数据集上表现得如此出色：
- en: '[PRE39]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Using the preceding code, we obtained the column indices of the three-feature
    subset from the 11th position in the `sbs.subsets_` attribute and returned the
    corresponding feature names from the column index of the pandas Wine `DataFrame`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们从`11`号位置的`sbs.subsets_`属性中获取了三特征子集的列索引，并从pandas Wine `DataFrame`的列索引中返回了相应的特征名称。
- en: 'Next, let''s evaluate the performance of the KNN classifier on the original
    test dataset:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们评估KNN分类器在原始测试数据集上的表现：
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the preceding code section, we used the complete feature set and obtained
    approximately 97 percent accuracy on the training dataset and approximately 96
    percent accuracy on the test dataset, which indicates that our model already generalizes
    well to new data. Now, let''s use the selected three-feature subset and see how
    well KNN performs:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码部分，我们使用了完整的特征集，并在训练数据集上获得了约97%的准确率，在测试数据集上获得了约96%的准确率，这表明我们的模型已经能够很好地泛化到新数据上。现在，让我们使用选定的三个特征子集，看看KNN的表现如何：
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When using less than a quarter of the original features in the Wine dataset,
    the prediction accuracy on the test dataset declined slightly. This may indicate
    that those three features do not provide less discriminatory information than
    the original dataset. However, we also have to keep in mind that the Wine dataset
    is a small dataset and is very susceptible to randomness—that is, the way we split
    the dataset into training and test subsets, and how we split the training dataset
    further into a training and validation subset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Wine数据集中使用不到原始特征的四分之一时，测试数据集上的预测准确性略有下降。这可能表明这三个特征提供的信息并不比原始数据集少。然而，我们也必须记住，Wine数据集是一个小数据集，非常容易受到随机性的影响——也就是说，我们如何将数据集划分为训练集和测试集，以及如何进一步将训练集划分为训练集和验证集。
- en: While we did not increase the performance of the KNN model by reducing the number
    of features, we shrank the size of the dataset, which can be useful in real-world
    applications that may involve expensive data collection steps. Also, by substantially
    reducing the number of features, we obtain simpler models, which are easier to
    interpret.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过减少特征数量并没有提高KNN模型的性能，但我们缩小了数据集的大小，这在实际应用中可能很有用，尤其是当数据收集步骤非常昂贵时。另外，通过大幅减少特征数量，我们获得了更简洁的模型，*更易于解释*。
- en: '**Feature selection algorithms in scikit-learn**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**scikit-learn中的特征选择算法**'
- en: There are many more feature selection algorithms available via scikit-learn.
    Those include **recursive backward elimination** based on feature weights, tree-based
    methods to select features by importance, and univariate statistical tests. A
    comprehensive discussion of the different feature selection methods is beyond
    the scope of this book, but a good summary with illustrative examples can be found
    at [http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html).
    You can find implementations of several different flavors of sequential feature
    selection related to the simple SBS that we implemented previously in the Python
    package `mlxtend` at [http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 通过scikit-learn，提供了更多的特征选择算法。这些算法包括基于特征权重的**递归后向消除**、通过重要性选择特征的基于树的方法，以及单变量统计检验。对不同特征选择方法的全面讨论超出了本书的范围，但可以在[http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)找到一个很好的总结，并附有示例。你还可以在Python包`mlxtend`中找到与我们之前实现的简单SBS相关的几种不同类型的顺序特征选择的实现，地址是[http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)。
- en: Assessing feature importance with random forests
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林评估特征重要性
- en: 'In previous sections, you learned how to use L1 regularization to zero out
    irrelevant features via logistic regression and how to use the SBS algorithm for
    feature selection and apply it to a KNN algorithm. Another useful approach for
    selecting relevant features from a dataset is using a **random forest**, an ensemble
    technique that was introduced in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*. Using a random forest, we can measure the feature importance
    as the averaged impurity decrease computed from all decision trees in the forest,
    without making any assumptions about whether our data is linearly separable or
    not. Conveniently, the random forest implementation in scikit-learn already collects
    the feature importance values for us so that we can access them via the `feature_importances_`
    attribute after fitting a `RandomForestClassifier`. By executing the following
    code, we will now train a forest of 500 trees on the Wine dataset and rank the
    13 features by their respective importance measures—remember from our discussion
    in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn* that
    we don''t need to use standardized or normalized features in tree-based models:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，你学习了如何通过逻辑回归使用 L1 正则化来将无关特征的系数归零，以及如何使用 SBS 算法进行特征选择并将其应用于 KNN 算法。另一种选择数据集中相关特征的有用方法是使用**随机森林**，这是一种集成技术，在*第
    3 章*《使用 scikit-learn 进行机器学习分类器之旅》中有介绍。通过使用随机森林，我们可以通过计算森林中所有决策树的平均不纯度减少来衡量特征的重要性，而无需对数据是否线性可分作出任何假设。方便的是，scikit-learn
    中的随机森林实现已经为我们收集了特征重要性值，因此在拟合 `RandomForestClassifier` 后，我们可以通过 `feature_importances_`
    属性访问这些值。通过执行以下代码，我们将训练一个包含 500 棵树的森林，并根据它们各自的重要性度量对葡萄酒数据集中的 13 个特征进行排名——请记住，在*第
    3 章*《使用 scikit-learn 进行机器学习分类器之旅》中我们讨论过，树模型不需要使用标准化或归一化的特征：
- en: '[PRE42]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After executing the code, we created a plot that ranks the different features
    in the Wine dataset by their relative importance; note that the feature importance
    values are normalized so that they sum up to 1.0:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们创建了一个图表，将葡萄酒数据集中不同特征按其相对重要性进行排名；请注意，特征重要性值已归一化，以便它们的总和为 1.0：
- en: '![](img/B13208_04_10.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_04_10.png)'
- en: We can conclude that the proline and flavonoid levels, the color intensity,
    the OD280/OD315 diffraction, and the alcohol concentration of wine are the most
    discriminative features in the dataset based on the average impurity decrease
    in the 500 decision trees. Interestingly, two of the top-ranked features in the
    plot are also in the three-feature subset selection from the SBS algorithm that
    we implemented in the previous section (alcohol concentration and OD280/OD315
    of diluted wines).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，葡萄酒的脯氨酸和类黄酮水平、颜色强度、OD280/OD315 衍射以及酒精浓度是数据集中最具辨别力的特征，这一结论基于 500 棵决策树中计算的平均不纯度减少。有趣的是，图表中排名靠前的两个特征也出现在我们在上一节中实现的
    SBS 算法的三特征子集选择中（酒精浓度和稀释酒的 OD280/OD315）。
- en: However, as far as interpretability is concerned, the random forest technique
    comes with an important *gotcha* that is worth mentioning. If two or more features
    are highly correlated, one feature may be ranked very highly while the information
    on the other feature(s) may not be fully captured. On the other hand, we don't
    need to be concerned about this problem if we are merely interested in the predictive
    performance of a model rather than the interpretation of feature importance values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就可解释性而言，随机森林技术有一个值得注意的重要*陷阱*。如果两个或多个特征高度相关，一个特征可能会被排名非常高，而另一个特征（或多个特征）的信息可能没有完全捕获。另一方面，如果我们仅对模型的预测性能感兴趣，而不是对特征重要性值的解释，则不需要担心这个问题。
- en: 'To conclude this section about feature importance values and random forests,
    it is worth mentioning that scikit-learn also implements a `SelectFromModel` object
    that selects features based on a user-specified threshold after model fitting,
    which is useful if we want to use the `RandomForestClassifier` as a feature selector
    and intermediate step in a scikit-learn `Pipeline` object, which allows us to
    connect different preprocessing steps with an estimator, as you will see in *Chapter
    6*, *Learning Best Practices for Model Evaluation and Hyperparameter Tuning*.
    For example, we could set the `threshold` to `0.1` to reduce the dataset to the
    five most important features using the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一节关于特征重要性值和随机森林的内容时，值得一提的是，scikit-learn 还实现了一个`SelectFromModel`对象，该对象在模型拟合后根据用户指定的阈值选择特征。如果我们想将`RandomForestClassifier`作为特征选择器，并作为scikit-learn
    `Pipeline`对象中的一个中间步骤，这非常有用，`Pipeline`允许我们将不同的预处理步骤与估计器连接起来，正如你将在*第六章*，*模型评估与超参数调优的最佳实践*中看到的那样。例如，我们可以将`threshold`设置为`0.1`，使用以下代码将数据集缩减为五个最重要的特征：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by looking at useful techniques to make sure that we
    handle missing data correctly. Before we feed data to a machine learning algorithm,
    we also have to make sure that we encode categorical variables correctly, and
    in this chapter, we saw how we can map ordinal and nominal feature values to integer
    representations.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时探讨了确保正确处理缺失数据的有用技巧。在将数据输入到机器学习算法之前，我们还必须确保正确地编码分类变量，在这一章中，我们展示了如何将有序和名义特征值映射到整数表示。
- en: Moreover, we briefly discussed L1 regularization, which can help us to avoid
    overfitting by reducing the complexity of a model. As an alternative approach
    to removing irrelevant features, we used a sequential feature selection algorithm
    to select meaningful features from a dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们简要讨论了L1正则化，它通过减少模型的复杂性来帮助我们避免过拟合。作为移除无关特征的另一种方法，我们使用了一个序列特征选择算法，从数据集中选择有意义的特征。
- en: 'In the next chapter, you will learn about yet another useful approach to dimensionality
    reduction: feature extraction. It allows us to compress features onto a lower-dimensional
    subspace, rather than removing features entirely as in feature selection.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习到另一种有用的降维方法：特征提取。它允许我们将特征压缩到一个较低维度的子空间，而不是像特征选择那样完全去除特征。
