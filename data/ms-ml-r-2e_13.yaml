- en: Text Mining
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本挖掘
- en: '"I think it''s much more interesting to live not knowing than to have answers
    which might be wrong."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '"我认为活在未知中比拥有可能错误的答案更有趣。"'
- en: '- Richard Feynman'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 理查德·费曼'
- en: The world is awash with textual data. If you Google, Bing, or Yahoo how much
    of that data is unstructured, that is, in a textual format, estimates would range
    from 80 to 90 percent. The real number doesn't matter. What does matter is that
    a large proportion of the data is in a text format. The implication is that anyone
    seeking to find insights in that data must develop the capability to process and
    analyze text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 世界充满了文本数据。如果你在Google、Bing或Yahoo上搜索有多少数据是无结构的，也就是说，以文本格式存在，估计会从80%到90%。实际数字并不重要。重要的是，大量数据是以文本格式存在的。这意味着任何寻求从这些数据中寻找洞察的人都必须发展出处理和分析文本的能力。
- en: When I first started out as a market researcher, I used to manually pore through
    page after page of moderator-led focus groups and interviews with the hope of
    capturing some qualitative insight an Aha! moment if you will-and then haggle
    with fellow team members over whether they had the same insight or not. Then,
    you would always have that one individual in a project who would swoop in and
    listen to two interviews-out of the 30 or 40 on the schedule and, alas, they had
    their mind made up on what was really happening in the world. Contrast that with
    the techniques being used now, where an analyst can quickly distill data into
    meaningful quantitative results, support qualitative understanding, and maybe
    even sway the swooper.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我最初作为一名市场研究员开始工作时，我过去常常手动翻阅一页又一页的由主持人引导的焦点小组和访谈，希望能捕捉到一些定性洞察，或者说是一个Aha！时刻——然后与团队成员争论他们是否也有同样的洞察。然后，你总会遇到一个在项目中突然出现并听取两场访谈——30或40场预定访谈中的两场——的人，唉，他们已经对世界上真正发生的事情有了自己的看法。与现在使用的技巧相比，分析师可以快速将数据提炼成有意义的定量结果，支持定性理解，甚至可能影响那些突然出现的人。
- en: Over the last few years, I've applied the techniques discussed here to mine
    physician-patient interactions, understand FDA fears on prescription drug advertising,
    and capture patient concerns about a rare cancer, to name just a few. Using R
    and the methods in this chapter, you too can extract the powerful information
    in textual data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，我已经将这里讨论的技术应用于挖掘医患互动，理解FDA对处方药广告的担忧，以及捕捉患者对罕见癌症的关注，仅举几个例子。使用R和本章中的方法，你也可以从文本数据中提取强大的信息。
- en: Text mining framework and methods
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本挖掘框架和方法
- en: There are many different methods to use in text mining. The goal here is to
    provide a basic framework to apply to such an endeavor. This framework is not
    all-inclusive of the possible methods but will cover those that are probably the
    most important for the vast majority of projects that you will work on. Additionally,
    I will discuss the modeling methods in as succinct and clear a manner as possible,
    because they can get quite complicated. Gathering and compiling text data is a
    topic that could take up several chapters. Therefore, let's begin with the assumption
    that our data is available from Twitter, a customer call center, scraped off the
    web, or whatever, and is contained in some sort of text file or files.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘有许多不同的方法可以使用。这里的目的是提供一个基本的框架，可以应用于这样的任务。这个框架并不包括所有可能的方法，但会涵盖那些在大多数你将要工作的项目中可能最重要的方法。此外，我将尽可能简洁、清晰地讨论建模方法，因为它们可能会变得相当复杂。收集和整理文本数据是一个可能占用几章内容的话题。因此，让我们假设我们的数据来自Twitter、客户呼叫中心、从网络上抓取的，或者
    whatever，并且包含在某些形式的文本文件或文件中。
- en: The first task is to put the text files in one structured file referred to as
    a **corpus**. The number of documents could be just one, dozens, hundreds, or
    even thousands. R can handle a number of raw text files, including RSS feeds,
    PDF files, and MS Word documents. With the corpus created, the data preparation
    can begin with the text transformation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项任务是把这些文本文件放入一个结构化的文件中，称为**语料库**。文档的数量可能只有一个，几十个，几百个，甚至几千个。R可以处理包括RSS源、PDF文件和MS
    Word文档在内的多种原始文本文件。语料库创建后，就可以开始数据准备，包括文本转换。
- en: 'The following list is comprised of probably some of the most common and useful
    transformations for text files:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表包含了一些文本文件中最常见和有用的转换：
- en: Change capital letters to lowercase
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将大写字母转换为小写
- en: Remove numbers
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除数字
- en: Remove punctuation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: Remove stop words
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除停用词
- en: Remove excess whitespace
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除多余的空白
- en: Word stemming
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Word replacement
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词替换
- en: In transforming the corpus, you are creating not only a more compact dataset,
    but also simplifying the structure in order to facilitate relationships among
    the words, thereby leading to an increased understanding. However, keep in mind
    that not all of these transformations are necessary all the time and judgment
    must be applied, or you can iterate to find the transformations that make the
    most sense.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换语料库的过程中，你不仅创建了一个更紧凑的数据集，而且还简化了结构，以便于促进单词之间的关系，从而提高理解。然而，请注意，并非所有这些转换在所有时候都是必要的，必须应用判断，或者你可以迭代以找到最有意义的转换。
- en: By changing words to lowercase, you can prevent the improper counting of words.
    Say that you have a count for hockey three times and Hockey once, where it is
    the first word in a sentence. R will not give you a count of hockey=4, but hockey=3
    and Hockey=1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将单词转换为小写，你可以防止单词的不当计数。比如说，你有hockey这个词出现了三次，而Hockey只出现了一次，它是句子中的第一个单词。R不会给你hockey=4的计数，而是hockey=3和Hockey=1。
- en: Removing punctuation also achieves the same purpose, but as we will see in the
    business case, punctuation is important if you want to split your documents by
    sentences.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 删除标点符号也能达到相同的目的，但正如我们将在业务案例中看到的，如果你想要根据句子分割文档，标点是重要的。
- en: In removing stop words, you are getting rid of the common words that have no
    value; in fact, they are detrimental to the analysis, as their frequency masks
    important words. Examples of stop words *are, and, is, the, not,* and *to*. Removing
    whitespace makes a more compact corpus by getting rid of things such as tabs,
    paragraph breaks, double-spacing, and so on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除停用词时，你正在去除那些没有价值的常见单词；实际上，它们对分析有害，因为它们的频率掩盖了重要的单词。停用词的例子包括*是，和，是，的，不*和*到*。删除空白字符通过去除诸如制表符、段落分隔、双倍间距等东西，使语料库更加紧凑。
- en: The stemming of words can get a bit tricky and might add to your confusion because
    it deletes word suffixes, creating the base word, or what is known as the **radical**.
    I personally am not a big fan of stemming and the analysts I've worked with agree
    with that sentiment. However, you can use the stemming algorithm included in the
    R package, `tm`, where the function calls the **porter stemming algorithm** from
    the `SnowballC` package. An example of stemming would be where your corpus has
    family and families. Recall that R would count this as two separate words. By
    running the stemming algorithm, the stemmed word for the two instances would become
    *famili*. This would prevent the incorrect count, but in some cases, it can be
    odd to interpret and is not very visually appealing in a wordcloud for presentation
    purposes. In some cases, it may make sense to run your analysis with both stemmed
    and unstemmed words in order to see which one makes sense.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的词干提取可能会有些棘手，并可能增加你的困惑，因为它删除了单词的后缀，创建了基词，或称为**词根**。我个人并不是特别喜欢词干提取，而且与我合作的分析师们也同意这种观点。然而，你可以使用R包`tm`中包含的词干提取算法，该函数调用`SnowballC`包中的**Porter词干提取算法**。一个词干提取的例子是，如果你的语料库中有family和families。回想一下，R会将它们计为两个不同的单词。通过运行词干提取算法，这两个实例的词干单词将变为*famili*。这将防止错误的计数，但在某些情况下，它可能难以解释，并且在用于展示目的的词云中不太吸引人。在某些情况下，可能有必要同时使用词干提取和未提取的单词进行分析，以查看哪一个更有意义。
- en: Probably the most optional of the transformations is to replace the words. The
    goal of replacement is to combine the words with a similar meaning, for example,
    management and leadership. You can also use it in lieu of stemming. I once examined
    the outcome of stemmed and unstemmed words and concluded that I could achieve
    a more meaningful result by replacing about a dozen words instead of stemming.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最可选的转换之一是替换单词。替换的目标是将具有相似意义的单词组合在一起，例如管理和管理。你还可以用它来代替词干提取。我曾经检查过词干提取和未提取的单词的结果，并得出结论，通过替换大约十二个单词而不是词干提取，我可以得到更有意义的结果。
- en: With the transformation of the corpus completed, the next step is to create
    either a **Document-Term Matrix** (**DTM**) or **Term-Document Matrix** (**TDM**).
    What either of these matrices does is create a matrix of word counts for each
    individual document in the matrix. A DTM would have the documents as rows and
    the words as columns, while in a TDM, the reverse is true. Text mining can be
    performed on either matrix.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当语料库的转换完成时，下一步是创建一个**文档-词矩阵**（**DTM**）或**词-文档矩阵**（**TDM**）。这两个矩阵中的任何一个都会为矩阵中的每个单独文档创建一个单词计数的矩阵。在DTM中，文档作为行，单词作为列，而在TDM中，情况正好相反。可以在任何一个矩阵上执行文本挖掘。
- en: With a matrix, you can begin to analyze the text by examining word counts and
    producing visualizations such as `wordclouds`. One can also find word associations
    by producing correlation lists for specific words. It also serves as a necessary
    data structure in order to build topic models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用矩阵，您可以通过检查单词计数和生成如 `wordclouds` 这样的可视化来开始分析文本。还可以通过为特定单词生成相关性列表来找到单词关联。它还作为构建主题模型所必需的数据结构。
- en: Topic models
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题模型
- en: Topic models are a powerful method to group documents by their main topics.
    Topic models allow probabilistic modeling of term frequency occurrences in documents.
    The fitted model can be used to estimate the similarity between documents, as
    well as between a set of specified keywords using an additional layer of latent
    variables, which are referred to as topics (Grun and Hornik, 2011). In essence,
    a document is assigned to a topic based on the distribution of the words in that
    document, and the other documents in that topic will have roughly the same frequency
    of words.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型是一种强大的方法，可以根据文档的主要主题对文档进行分组。主题模型允许对文档中术语频率出现的概率建模。拟合的模型可以用来估计文档之间的相似性，以及通过一个额外的潜在变量层（称为主题）来估计一组指定关键词之间的相似性（Grun
    和 Hornik，2011）。本质上，一个文档是根据该文档中单词的分布分配给一个主题的，并且该主题中的其他文档将具有大致相同的单词频率。
- en: The algorithm that we will focus on is **Latent Dirichlet Allocation** (**LDA**)
    with Gibbs sampling, which is probably the most commonly used sampling algorithm.
    In building topic models, the number of topics must be determined before running
    the algorithm (k-dimensions). If no apriori reason for the number of topics exists,
    then you can build several and apply judgment and knowledge to the final selection.
    LDA with Gibbs sampling is quite complicated mathematically, but my intent is
    to provide an introduction so that you are at least able to describe how the algorithm
    learns to assign a document to a topic in layman terms. If you are interested
    in mastering the math, block out a couple of hours on your calendar and have a
    go at it. Excellent background material is available at [http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注的算法是**潜在狄利克雷分配**（**LDA**）与吉布斯采样，这可能是最常用的采样算法。在构建主题模型之前，必须确定主题的数量（k 维度）。如果不存在主题数量的先验原因，则可以构建几个并应用判断和知识来最终选择。LDA
    与吉布斯采样在数学上相当复杂，但我的目的是提供一个介绍，以便您至少能够用通俗易懂的语言描述算法如何学习将文档分配给一个主题。如果您对掌握数学感兴趣，请在您的日历上预留几个小时，尝试一下。有关背景材料，请参阅[http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)。
- en: 'LDA is a generative process, and so, the following will iterate to a steady
    state:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 是一个生成过程，因此以下将迭代到一个稳定状态：
- en: For each document (*j*), there are *1* to *j* documents. We will randomly assign
    it a multinomial distribution (**dirichlet** **distribution**) to the topics (*k*)
    with *1* to *k* topics, for example, document *A* is 25 percent topic one, 25
    percent topic two, and 50 percent topic three.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档 (*j*)，有 *1* 到 *j* 个文档。我们将随机地将它分配给主题 (*k*) 的多项式分布（**dirichlet** **分布**），主题
    (*k*) 有 *1* 到 *k* 个主题，例如，文档 *A* 是 25% 主题一，25% 主题二，50% 主题三。
- en: Probabilistically, for each word (*i*), there are *1* to *i* words to a topic
    (*k*); for example, the word *mean* has a probability of 0.25 for the topic statistics.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 概率上，对于每个单词 (*i*)，有 *1* 到 *i* 个单词属于主题 (*k*)；例如，单词 *mean* 在主题统计中的概率为 0.25。
- en: For each word(*i*) in document(*j*) and topic(*k*), calculate the proportion
    of words in that document assigned to that topic; note it as the probability of
    topic(*k*) with document(*j*), *p(k|j)*, and the proportion of word(*i*) in topic(*k*)
    from all the documents containing the word. Note it as the probability of word(*i*)
    with topic(*k*), *p(i|k)*.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文档 (*j*) 中每个单词(*i*)和主题(*k*)，计算分配给该主题的单词比例；将其记为文档(*j*)的主题(*k*)的概率，*p(k|j)*，以及单词(*i*)在包含该单词的所有文档中属于主题(*k*)的比例。将其记为单词(*i*)的主题(*k*)的概率，*p(i|k)*。
- en: Resample, that is, assign *w* a new *t* based on the probability that *t* contains
    *w*, which is based on *p(k|j)* times *p(i|k)*.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新采样，即根据 *t* 包含 *w* 的概率来给 *w* 分配一个新的 *t*，这个概率是基于 *p(k|j)* 乘以 *p(i|k)*。
- en: Rinse and repeat; over numerous iterations, the algorithm finally converges
    and a document is assigned a topic based on the proportion of words assigned to
    a topic in that document.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上述步骤；经过多次迭代，算法最终收敛，并根据文档中分配给主题的单词比例将文档分配给一个主题。
- en: The LDA that we will be doing assumes that the order of words and documents
    does not matter. There has been work done to relax these assumptions in order
    to build models of language generation and sequence models over time (known as
    **dynamic topic modelling**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要进行的LDA假设单词和文档的顺序不重要。为了构建语言生成和序列模型（称为**动态主题建模**），已经有人对这些假设进行了放松。
- en: Other quantitative analyses
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他定量分析
- en: We will now shift gears to analyze text semantically based on sentences and
    the tagging of words based on the parts of speech, such as noun, verb, pronoun,
    adjective, adverb, preposition, singular, plural, and so on. Often, just examining
    the frequency and latent topics in the text will suffice for your analysis. However,
    you may find occasions when a deeper understanding of the style is required in
    order to compare the speakers or writers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转换方向，根据句子和基于词性的单词标记来分析文本的语义，例如名词、动词、代词、形容词、副词、介词、单数、复数等等。通常，仅仅检查文本中的频率和潜在主题就足以进行分析。然而，你可能会发现需要更深入地理解风格的情况，以便比较说话者或作者。
- en: 'There are many methods to accomplish this task, but we will focus on the following
    five:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这项任务有许多方法，但我们将关注以下五种：
- en: Polarity (sentiment analysis)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极性（情感分析）
- en: Automated readability index (complexity)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动可读性指数（复杂性）
- en: Formality
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正式性
- en: Diversity
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样性
- en: Dispersion
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 散布度
- en: '**Polarity** is often referred to as sentiment analysis, which tells you how
    positive or negative the text is. By analyzing polarity in R with the `qdap` package,
    a score will be assigned to each sentence and you can analyze the average and
    standard deviation of polarity by groups such as different authors, text, or topics.
    Different polarity dictionaries are available and `qdap` defaults to one created
    by Hu and Liu, 2004\. You can alter or change this dictionary according to your
    requirements.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**极性**通常被称为情感分析，它告诉你文本是积极的还是消极的。通过使用R中的`qdap`包分析极性，每个句子都会被分配一个分数，你可以通过不同作者、文本或主题等群体分析极性的平均值和标准差。有多个极性词典可用，`qdap`默认使用的是胡和刘于2004年创建的词典。你可以根据你的需求修改或更改这个词典。'
- en: The algorithm works by first tagging the words with a positive, negative, or
    neutral sentiment based on the dictionary. The tagged words are then clustered
    based on the four words prior and two words after a tagged word, and these clusters
    are tagged with what are known as **valence shifters** (neutral, negator, amplifier,
    and de-amplifier). A series of weights based on their number and position are
    applied to both the words and clusters. This is then summed and divided by the
    square root of the number of words in that sentence.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法首先根据词典将单词标记为积极、消极或中性情感。然后，根据标记词前后的四个单词和两个单词将标记词进行聚类，这些聚类被标记为所谓的**极性转换词**（中性、否定者、放大器和去放大器）。根据它们的数量和位置应用一系列权重到单词和聚类上。然后，将这个总和除以该句子中单词数量的平方根。
- en: 'Automated readability index is a measure of the text complexity and a reader''s
    ability to understand. A specific formula is used to calculate this index: *4.71(#
    of characters / #of words) + 0.5(# of words / # of sentences) - 21.43*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 自动可读性指数是文本复杂性和读者理解能力的度量。使用特定的公式来计算这个指数：*4.71（字符数/单词数）+ 0.5（单词数/句子数）- 21.43*。
- en: The index produces a number, which is a rough estimate of a student's grade
    level to fully comprehend. If the number is 9, then a high school freshman, aged
    13 to 15, should be able to grasp the meaning of the text.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 索引产生一个数字，这个数字是对学生达到完全理解水平所需年级的粗略估计。如果这个数字是9，那么一个13到15岁的高中生应该能够理解文本的含义。
- en: The formality measure provides an understanding of how a text relates to the
    reader or speech relates to a listener. I like to think of it as a way to understand
    how comfortable the person producing the text is with the audience, or an understanding
    of the setting where this communication takes place. If you want to experience
    formal text, attend a medical conference or read a legal document. Informal text
    is said to be contextual in nature.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正式性度量提供了对文本如何与读者或演讲如何与听众相关联的理解。我喜欢把它想成是理解文本生产者对受众的舒适度或理解这种沟通发生的背景的一种方式。如果你想体验正式文本，可以参加医学会议或阅读法律文件。非正式文本被认为具有情境性。
- en: 'The formality measure is called **F-Measure**. This measure is calculated as
    follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正式性度量被称为**F-Measure**。这个度量是这样计算的：
- en: Formal words (*f*) are nouns, adjectives, prepositions, and articles
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正式词汇 (*f*) 是名词、形容词、介词和冠词
- en: Contextual words (*c*) are pronouns, verbs, adverbs, and interjections
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文词汇 (*c*) 是代词、动词、副词和感叹词
- en: '*N = sum of (f + c + conjunctions)*'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N = (f + c + 并列词)*'
- en: '*Formality Index = 50((sum of f - sum of c / N) + 1)*'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正式度指数 = 50((f的总和 - c的总和 / N) + 1)*'
- en: This is totally irrelevant, but when I was in Iraq, one of the army generals-who
    shall remain nameless. I had to brief and write situation reports for was absolutely
    adamant that adverbs were not to be used, ever, or there would be wrath. The idea
    was that you can't quantify words such as highly or mostly because they mean different
    things to different people. Five years later, I still scour my business e-mails
    and PowerPoint presentations for unnecessary adverbs. Formality writ large!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全无关紧要，但当我还在伊拉克时，一位军队将军——我将不透露他的名字。我必须简要汇报并撰写情况报告，他坚决反对使用副词，否则就会大发雷霆。这种想法是，你不能量化像“非常”或“大部分”这样的词，因为它们对不同的人意味着不同的东西。五年后，我仍然在我的商业电子邮件和PowerPoint演示文稿中寻找不必要的副词。形式主义到了极致！
- en: 'Diversity, as it relates to text mining, refers to the number of different
    words used in relation to the total number of words used. This can also mean the
    expanse of the text producer''s vocabulary or lexicon richness. The `qdap` package
    provides five--that''s right, five--different measures of diversity: Simpson,
    Shannon, Collision, Bergen Parker, and Brillouin. I won''t cover these five in
    detail but will only say that the algorithms are used not only for communication
    and information science retrieval, but also for biodiversity in nature.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本挖掘方面，多样性指的是与总词汇量相比使用的不同词汇的数量。这也可能意味着文本生产者的词汇量或词汇丰富度。`qdap` 包提供了五种——没错，五种——不同的多样性度量：辛普森、香农、碰撞、伯根·帕克和布里渊。我不会详细讲解这五种，但只想说这些算法不仅用于通信和信息科学检索，还用于自然界的生物多样性。
- en: Finally, dispersion, or lexical dispersion, is a useful tool in order to understand
    how words are spread throughout a document and serves as an excellent way to explore
    a text and identify patterns. The analysis is conducted by calling the specific
    word or words of interest, which are then produced in a plot showing when the
    word or words occurred in the text over time. As we will see, the `qdap` package
    has a built-in plotting function to analyze the text dispersion.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分散度，或词汇分散度，是理解词汇如何在文档中分布的有用工具，并且是探索文本和识别模式的一种极好方式。分析是通过调用特定的单词或单词组来进行的，这些单词或单词组随后在显示单词或单词组在文本中随时间出现的时间序列图中生成。正如我们将看到的，`qdap`
    包内置了一个用于分析文本分散度的绘图功能。
- en: We covered a framework on text mining about how to prepare the text, count words,
    and create topic models and, finally, dived deep into other lexical measures.
    Now, let's apply all this and do some real-world text mining.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一个关于文本挖掘的框架，介绍了如何准备文本、计数单词、创建主题模型，最后深入探讨了其他词汇度量。现在，让我们应用所有这些，做一些真正的文本挖掘。
- en: Business understanding
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务理解
- en: For this case study, we will take a look at president Obama's State of the Union
    speeches. I have no agenda here; just curious as to what can be uncovered in particular
    and if and how his message changed over time. Perhaps this will serve as a blueprint
    to analyze any politician's speech in order to prepare an opposing candidate in
    a debate or speech of their own. If not, so be it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例研究，我们将查看奥巴马总统的国情咨文。我这里没有议程；只是好奇能否发现某些特别之处，以及他的信息是否以及如何随时间变化。也许这可以作为分析任何政治家演讲的蓝图，以便为辩论或他们自己的演讲准备反对候选人。如果不行，那就这样吧。
- en: The two main analytical goals are to build topic models on the six State of
    the Union speeches and then compare the first speech in 2010 and the last in January,
    2016 for sentence-based textual measures, such as sentiment and dispersion.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 两个主要的分析目标是基于六篇国情咨文建立主题模型，然后比较2010年的第一篇和2016年1月的最后一篇基于句子的文本度量，如情感和分散度。
- en: Data understanding and preparation
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'The primary package that we will use is `tm`, the text mining package. We will
    also need `SnowballC` for the stemming of the words, `RColorBrewer` for the color
    palettes in `wordclouds`, and the `wordcloud` package. Please ensure that you
    have these packages installed before attempting to load them:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的首要包是 `tm`，文本挖掘包。我们还需要 `SnowballC` 用于词干提取，`RColorBrewer` 用于 `wordclouds`
    中的调色板，以及 `wordcloud` 包。请在尝试加载它们之前确保已安装这些包：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The data files are available for download in [https://github.com/datameister66/data](https://github.com/datameister66/data).
    Please ensure you put the text files into a separate directory because it will
    all go into our corpus for analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件可在[https://github.com/datameister66/data](https://github.com/datameister66/data)下载。请确保将文本文件放入单独的目录中，因为它们都将进入我们的语料库进行分析。
- en: 'Download the seven `.txt` files, for example `sou2012.txt`, into your working
    R directory. You can identify your current working directory and set it with these
    functions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 将七个`.txt`文件，例如`sou2012.txt`，下载到你的工作R目录中。你可以使用这些函数识别你的当前工作目录并设置它：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now begin to create the corpus by first creating an object with the
    path to the speeches and then seeing how many files are in this directory and
    what they are named:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始创建语料库，首先创建一个包含演讲路径的对象，然后查看这个目录中有多少文件以及它们的名称：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will name our corpus `docs` and create it with the `Corpus()` function,
    wrapped around the directory source function, `DirSource()`, which is also part
    of the `tm` package:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将命名我们的语料库为`docs`，并使用`Corpus()`函数创建它，该函数围绕目录源函数`DirSource()`，这也是`tm`包的一部分：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that there is no `corpus` or `document level` metadata. There are functions
    in the `tm` package to apply things such as author's names and timestamp information,
    among others, at both `document level` and `corpus`. We will not utilize this
    for our purposes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，没有`corpus`或`document level`元数据。`tm`包中有函数可以应用作者姓名和时间戳信息等，在`document level`和`corpus`级别。我们不会为此目的使用这些功能。
- en: 'We can now begin the text transformations using the `tm_map()` function from
    the `tm` package. These will be the transformations that we discussed previously--lowercase
    letters, remove numbers, remove punctuation, remove stop words, strip out the
    whitespace, and stem the words:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用`tm`包中的`tm_map()`函数开始文本转换。这些是我们之前讨论过的转换——小写字母、去除数字、去除标点符号、去除停用词、去除空白字符，以及词干提取：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point, it is a good idea to eliminate unnecessary words. For example,
    during the speeches, when `Congress` applauds a statement, you will find `(Applause)`
    in the text. This must be removed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，消除不必要的单词是个好主意。例如，在演讲中，当`Congress`对一项声明表示赞同时，你会在文本中找到`(Applause)`。这必须被移除：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After completing the transformations and removal of other words, make sure
    that your documents are plain text, put it in a document-term matrix, and check
    the dimensions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成转换和去除其他单词后，确保您的文档是纯文本，将其放入文档-词矩阵中，并检查维度：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The six speeches contain `4738` words. It is optional, but one can remove the
    sparse terms with the `removeSparseTerms()` function. You will need to specify
    a number between zero and one where the higher the number, the higher the percentage
    of `sparsity` in the matrix. Sparsity is the relative frequency of a term in the
    documents. So, if your sparsity threshold is 0.75, only terms with sparsity greater
    than 0.75 are removed. For us that would be *(1 - 0.75) * 7*, which is equal to
    1.75\.  Therefore, any term in fewer than two documents would be removed:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 六篇演讲包含`4738`个单词。使用`removeSparseTerms()`函数去除稀疏项是可选的。你需要指定一个介于零和一之间的数字，数值越高，矩阵中的`sparsity`百分比越高。稀疏性是术语在文档中的相对频率。所以，如果你的稀疏性阈值是0.75，只有稀疏性大于0.75的术语会被移除。对我们来说，这将是`(1
    - 0.75) * 7`，等于1.75。因此，任何在不到两个文档中出现的术语都会被移除：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we don''t have the metadata on the documents, it is important to name the
    rows of the matrix so that we know which document is which:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有文档的元数据，因此给矩阵的行命名很重要，这样我们才能知道哪个文档是哪个：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using the `inspect()` function, you can examine the matrix. Here, we will look
    at the seven rows and the first five columns:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`inspect()`函数，你可以检查矩阵。在这里，我们将查看七个行和前五列：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It appears that our data is ready for analysis, starting with looking at the
    word frequency counts. Let me point out that the output demonstrates why I've
    been trained to not favor wholesale stemming. You may be thinking that 'ability'
    and 'able' could be combined. If you stemmed the document you would end up with
    'abl'. How does that help the analysis? I think you lose context, at least in
    the initial analysis. Again, I recommend applying stemming thoughtfully and judiciously.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的数据已经准备好进行分析了，首先从查看单词频率计数开始。让我指出，输出展示了为什么我被训练成不倾向于全面词干提取。你可能认为`ability`和`able`可以合并。如果你对文档进行了词干提取，你最终会得到`abl`。这如何有助于分析？我认为你失去了上下文，至少在初始分析中是这样。再次，我建议谨慎和明智地应用词干提取。
- en: Modeling and evaluation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: Modeling will be broken into two distinct parts. The first will focus on word
    frequency and correlation and culminate in the building of a topic model. In the
    next portion, we will examine many different quantitative techniques by utilizing
    the power of the `qdap` package in order to compare two different speeches.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 建模将被分为两个不同的部分。第一部分将专注于单词频率和相关性，并以构建主题模型结束。在下一部分，我们将利用`qdap`包的强大功能来检查许多不同的定量技术，以便比较两个不同的演讲。
- en: Word frequency and topic models
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词频率和主题模型
- en: 'As we have everything set up in the document-term matrix, we can move on to
    exploring word frequencies by creating an object with the column sums, sorted
    in descending order. It is necessary to use `as.matrix()` in the code to sum the
    columns. The default order is ascending, so putting `-` in front of `freq` will
    change it to descending:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在文档-词矩阵中已经设置好了一切，我们可以继续通过创建一个按降序排列的列总和的对象来探索单词频率。在代码中，使用`as.matrix()`来求列的和是必要的。默认顺序是升序，所以在`freq`前面加上`-`将使其变为降序：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will examine the `head` and `tail` of the object with the following code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码检查对象的前`head`和`tail`：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The most frequent word is `new` and, as you might expect, the president mentions `america` frequently.
    Also notice how important employment is with the frequency of `jobs`. I find it
    interesting that he mentions Youngstown, for Youngstown, OH, a couple of times.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最常出现的单词是`new`，正如你所预期的，总统经常提到`america`。同时，注意`jobs`的频率如何显示就业的重要性。我发现他提到Youngstown很有趣，因为Youngstown，OH，他提到了几次。
- en: 'To look at the frequency of the word frequency, you can create tables, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看单词频率的频率，你可以创建如下表格：
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: What these tables show is the number of words with that specific frequency.
    So 354 words occurred three times; and one word, `new` in our case, occurred 193 times.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表格显示的是具有该特定频率的单词数量。所以有354个单词出现了三次；在我们的例子中，一个单词`new`出现了193次。
- en: 'Using `findFreqTerms()`, we can see which words occurred at least `125` times:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`findFreqTerms()`，我们可以看到哪些单词至少出现了`125`次：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can find associations with words by correlation with the `findAssocs()`
    function. Let''s look at `jobs` as two examples using `0.85` as the correlation
    cutoff:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用`findAssocs()`函数来通过相关性找到与单词的关联。让我们以`jobs`为例，使用`0.85`作为相关性截止值：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For visual portrayal, we can produce `wordclouds` and a bar chart. We will
    do two `wordclouds` to show the different ways to produce them: one with a minimum
    frequency and the other by specifying the maximum number of words to include.
    The first one with minimum frequency, also includes code to specify the color.
    The scale syntax determines the minimum and maximum word size by frequency; in
    this case, the minimum frequency is `70`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了视觉表现，我们可以生成`wordclouds`和条形图。我们将做两个`wordclouds`来展示不同的生成方式：一个指定最小频率，另一个指定包含单词的最大数量。第一个使用最小频率的，还包括指定颜色的代码。缩放语法通过频率确定最小和最大单词大小；在这种情况下，最小频率是`70`：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_12_01.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_12_01.png)'
- en: 'One can forgo all the fancy graphics, as we will in the following image, capturing
    the `25` most frequent words:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 可以省略所有花哨的图形，就像我们在下面的图像中所做的那样，捕捉到最常出现的`25`个单词：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_12_02.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_12_02.png)'
- en: 'To produce a bar chart, the code can get a bit complicated, whether you use
    base R, `ggplot2`, or `lattice`. The following code will show you how to produce
    a bar chart for the `10` most frequent words in base R:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成条形图，代码可能会变得有些复杂，无论你使用的是基础R、`ggplot2`还是`lattice`。以下代码将展示如何在基础R中生成最常出现`10`个单词的条形图：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_12_03-1.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_12_03-1.png)'
- en: 'We will now move on to building topic models using the `topicmodels` package,
    which offers the `LDA()` function. The question now is how many topics to create.
    It seems logical to solve for three  `topics` (`k=3`). Certainly, I encourage
    you to try other numbers of topics:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转向使用`topicmodels`包构建主题模型，该包提供了`LDA()`函数。现在的问题是创建多少个主题。似乎解决三个`主题`（`k=3`）是合理的。当然，我鼓励你尝试其他数量的主题：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can see an interesting transition over time. The first and last addresses
    have the same topic grouping, almost as if he opened and closed his tenure with
    the same themes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一个有趣的时间过渡。第一个和最后一个地址有相同的主题分组，几乎就像他带着相同的主题开始了他的任期并结束了它。
- en: 'Using the `terms()` function produces a list of an ordered word frequency for
    each topic. The list of words is specified in the function, so let''s look at
    the top `20` per topic:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`terms()`函数可以生成每个主题的有序词频列表。函数中指定了单词列表，因此让我们看看每个主题的前`20`个：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Topic 2` covers the first and last speeches. Nothing really stands out as
    compelling in that topic like the others. It will be interesting to see how the
    next analysis can yield insights into those speeches.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic 2` 涵盖了第一次和最后一次演讲。在那个主题中并没有像其他主题那样真正引人注目的内容。将很有趣地看到下一次分析如何揭示这些演讲的见解。'
- en: '`Topic 1` covers the next three speeches. Here, the message transitions to
    `"jobs"`, `"energy"`, `"reform"`, and the `"deficit"`, not to mention the comments
    about `"education"` and as we saw above, the correlation of `"jobs"` and `"colleges"`.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic 1` 涵盖了接下来的三次演讲。在这里，信息过渡到 `"jobs"`、`"energy"`、`"reform"` 和 `"deficit"`，更不用说关于
    `"education"` 的评论以及我们上面看到的 `"jobs"` 和 `"colleges"` 之间的相关性。'
- en: '`Topic 3` brings us to the next two speeches. The focus seems to really shift
    on to the economy and business with mentions to `"security"` and healthcare.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`Topic 3` 带我们进入接下来的两次演讲。重点似乎真正转向了经济和商业，提到了 `"security"` 和医疗保健。'
- en: In the next section, we can dig into the exact speech content further, along
    with comparing and contrasting the first and last State of the Union addresses.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们可以进一步挖掘具体的演讲内容，同时对比和对照第一次和最后一次国情咨文演讲。
- en: Additional quantitative analysis
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的定量分析
- en: 'This portion of the analysis will focus on the power of the `qdap` package.
    It allows you to compare multiple documents over a wide array of measures. Our
    effort will be on comparing the 2010 and 2016 speeches. For starters, we will
    need into turn the text into data frames, perform sentence splitting, and then
    combine them to one data frame with a variable created that specifies the year
    of the speech. We will use this as our grouping variable in the analyses. Dealing
    with text data, even in R, can be tricky. The code that follows seemed to work
    the best in this case to get the data loaded and ready for analysis. We first
    load the `qdap` package. Then, to bring in the data from a text file, we will
    use the `readLines()` function from base R, collapsing the results to eliminate
    unnecessary whitespace. I also recommend putting your text encoding to ASCII,
    otherwise you may run into some bizarre text that will mess up your analysis.
    That is done with the `iconv()` function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分分析将专注于`qdap`包的力量。它允许你通过广泛的指标比较多个文档。我们的努力将集中在比较2010年和2016年的演讲。首先，我们需要将文本转换为数据框，执行句子分割，然后将它们合并到一个数据框中，创建一个变量来指定演讲的年份。我们将使用这个变量作为分析中的分组变量。在R中处理文本数据可能会很棘手。下面的代码似乎在这种情况下工作得最好，以加载数据并准备好分析。我们首先加载`qdap`包。然后，为了从文本文件中引入数据，我们将使用R的基础函数`readLines()`，将结果压缩以消除不必要的空白。我还建议将你的文本编码设置为ASCII，否则你可能会遇到一些奇怪的文本，这会搞乱你的分析。这是通过`iconv()`函数完成的：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The warning message is not an issue as it is just telling us that the final
    line of text is not the same length as the other lines in the `.txt` file. We
    now apply the `qprep()` function from `qdap`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 警告信息并不是问题，因为它只是告诉我们文本的最后一行长度与其他`.txt`文件中的行不同。我们现在应用来自`qdap`的`qprep()`函数。
- en: 'This function is a wrapper for a number of other replacement functions and
    using it will speed pre-processing, but it should be used with caution if more
    detailed analysis is required. The functions it passes through are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是多个其他替换函数的包装器，使用它将加快预处理速度，但如果需要更详细的分析，则应谨慎使用。它传递的函数如下：
- en: '`bracketX()`: apply bracket removal'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bracketX()`: 应用括号去除'
- en: '`replace_abbreviation()`: replaces abbreviations'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_abbreviation()`: 替换缩写'
- en: '`replace_number()`: numbers to words, for example ''100'' becomes ''one hundred'''
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_number()`: 数字转换为文字，例如 ''100'' 变为 ''one hundred'''
- en: '`replace_symbol()`: symbols become words, for example @ becomes ''at'''
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replace_symbol()`: 符号变为文字，例如 @ 变为 ''at'''
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The other pre-processing we should do is to replace contractions (can''t to
    cannot), remove stopwords, in our case the top 100, and remove unwanted characters,
    with the exception of periods and question marks. They will come in handy shortly:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做的其他预处理工作是将缩写（can't to cannot）替换掉，移除停用词，在我们的例子中是前100个，以及移除不需要的字符，除了句号和问号。它们很快就会派上用场：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Critical to this analysis is to now split it into sentences and add what will
    be the grouping variable, the year of the speech. This also creates the `tot`
    variable, which stands for Turn of Talk, serving as an indicator of sentence order.
    This is especially helpful in a situation where you are analyzing dialogue, say
    in a debate or question and answer session:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次分析来说，现在将其拆分为句子，并添加将成为分组变量的演讲年份非常重要。这也创建了`tot`变量，代表谈话转换，作为句子顺序的指标。这在分析对话时特别有用，比如在辩论或问答环节：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Repeat the steps for the 2010 speech:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 重复2010年演讲的步骤：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Concatenate the separate years into one dataframe:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将单独的年份合并到一个数据框中：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'One of the great things about the `qdap` package is that it facilitates basic
    text exploration, as we did before. Let''s see a plot of frequent terms:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`qdap` 包的一个优点是它促进了基本的文本探索，就像我们之前做的那样。让我们看看频繁词的图表：'
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_12_04.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_12_04.png)'
- en: 'You can create a word frequency matrix that provides the counts for each word
    by speech:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建一个通过语音提供的每个单词计数的词频矩阵：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This can also be converted into a document-term matrix with the function `as.dtm()`
    should you so desire. Let''s next build `wordclouds`, by year with `qdap` functionality:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意，这也可以通过`as.dtm()`函数转换为文档-词矩阵。接下来，我们将使用`qdap`功能按年份构建`wordclouds`：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding command produces the following two images:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令生成了以下两个图像：
- en: '![](img/image_12_05.png)![](img/image_12_06.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_12_05.png)![](img/image_12_06.png)'
- en: 'Comprehensive word statistics are available. Here is a plot of the stats available
    in the package. The plot loses some of its visual appeal with just two speeches,
    but is revealing nonetheless. A complete explanation of the stats is available
    under `?word_stats`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了全面的单词统计信息。以下是该包中可用统计信息的图表。仅用两个演讲，这个图表的视觉吸引力有所下降，但仍然很有启发性。关于统计的完整解释可以在`?word_stats`下找到：
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_12_07.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_12_07.png)'
- en: Notice that the 2016 speech was much shorter, with over a hundred fewer sentences
    and almost a thousand fewer words. Also, there seems to be the use of asking questions
    as a rhetorical device in 2016 versus 2010 (n.quest 10 versus n.quest 4).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到2016年的演讲要短得多，有超过一百个句子和近一千个单词。此外，似乎在2016年比2010年更多地使用了提问作为修辞手段（n.quest 10比n.quest
    4）。
- en: 'To compare the polarity (sentiment scores), use the `polarity()` function,
    specifying the text and grouping variables:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要比较极性（情感分数），请使用`polarity()`函数，指定文本和分组变量：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `stan.mean.polarity` value represents the standardized mean polarity, which
    is the average polarity divided by the standard deviation. We see that `2015`
    was slightly higher (`0.267`) than `2010` (`0.121`). This is in line with what
    we would expect, wanting to end on a more positive note. You can also plot the
    data. The plot produces two charts. The first shows the polarity by sentences
    over time and the second shows the distribution of the polarity:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`stan.mean.polarity`值表示标准化的平均极性，即平均极性除以标准差。我们看到`2015`年略高（`0.267`），而`2010`年较低（`0.121`）。这与我们的预期相符，希望以更积极的语气结束。您也可以绘制数据。这个图表产生了两个图表。第一个显示了随时间变化的句子极性，第二个显示了极性的分布：'
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_12_08.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_12_08.png)'
- en: 'This plot may be a challenge to read in this text, but let me do my best to
    interpret it. The `2010` speech starts out with a strong negative sentiment and
    is slightly more negative than `2016`. We can identify the most negative sentiment
    sentence by creating a dataframe of the `pol` object, find the sentence number,
    and produce it:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表在这个文本中可能难以阅读，但我会尽力解释它。`2010`年的演讲一开始就带有强烈的负面情绪，并且比`2016`年稍微负面一些。我们可以通过创建`pol`对象的dataframe来识别最负面的句子，找到句子编号，并生成它：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that is negative sentiment! Ironically, the government is even more in
    debt today. We will look at the readability index next:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是负面情绪！具有讽刺意味的是，政府现在的债务甚至更多。接下来，我们将查看可读性指数：
- en: '[PRE33]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'I think it is no surprise that they are basically the same. Formality analysis
    is next. This takes a couple of minutes to run in R:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这并不令人惊讶，它们基本上是相同的。接下来是正式性分析。在R中运行这个分析需要几分钟时间：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This looks to be very similar. We can examine the proportion of the parts of
    the speech. A plot is available, but adds nothing to the analysis, in this instance:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来非常相似。我们可以检查演讲各部分的占比。虽然有一个图表可用，但在这个例子中它并没有为分析增加任何东西：
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, the diversity measures are produced. Again, they are nearly identical.
    A plot is also available, (`plot(div)`), but being so similar, it once again adds
    no value. It is important to note that Obama''s speech writer for 2010 was Jon
    Favreau, and in 2016, it was Cody Keenan:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，多样性度量已经生成。再次强调，它们几乎完全相同。还有一个可用的图表（`plot(div)`），但由于它们如此相似，它再次没有增加任何价值。重要的是要注意，2010年奥巴马的演讲稿作者是乔恩·法夫罗，而2016年则是科迪·基南：
- en: '[PRE36]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'One of my favorite plots is the dispersion plot. This shows the dispersion
    of a word throughout the text. Let''s examine the dispersion of `"jobs"`, `"families",`
    and `"economy"`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的图表之一是散点图。这个图表显示了单词在整个文本中的分布。让我们来考察“jobs”、“families”和“economy”的分布：
- en: '[PRE37]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出如下：
- en: '![](img/image_12_09.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_12_09.png)'
- en: This is quite interesting as you can visualize how much longer the 2010 speech
    is. In 2010, the first half of his speech was focused heavily on jobs while in
    2016 it appears it was more about the state of the overall economy; no doubt how
    much of a hand he played in saving it from the brink of disaster. In 2010, security
    was not brought in until later in the speech versus placed throughout the final
    address. You can see and understand how text analysis can provide insight into
    what someone is thinking, what their priorities are, and how they go about communicating them.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有趣，因为你可以直观地看到2010年的演讲有多长。在2010年，他演讲的前半部分主要关注就业问题，而在2016年，似乎更多地关注整体经济状况；毫无疑问，他在避免灾难边缘发挥了多大的作用。在2010年，安全议题直到演讲的后期才被提及，而在最后的演讲中则贯穿始终。你可以看到并理解文本分析如何提供对某人思考方式、优先事项以及他们如何进行沟通的洞察。
- en: This completes our analysis of the two speeches. I must confess that I did not
    listen to any of these speeches. In fact, I haven't watched a State of the Union
    address since Reagan was president, probably with the exception of the 2002 address.
    This provided some insight for me on how the topics and speech formats have changed
    over time to accommodate political necessity, while the overall style of formality
    and sentence structure has remained consistent. Keep in mind that this code can
    be adapted to text for dozens, if not hundreds, of documents and with multiple
    speakers, for example screenplays, legal proceedings, interviews, social media,
    and on and on. Indeed, text mining can bring quantitative order to what has been
    qualitative chaos.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对两篇演讲的分析。我必须承认，我没有听过这些演讲中的任何一篇。事实上，自从里根总统以来，我就没有看过国情咨文演讲，可能只有2002年的那次例外。这为我提供了关于随着时间的推移，为了适应政治需求，主题和演讲格式如何变化的见解，而整体风格和句子结构则保持一致。记住，这段代码可以适应数十篇，甚至数百篇文档，以及多个演讲者，例如剧本、法律程序、访谈、社交媒体等等。确实，文本挖掘可以为定性混沌带来定量秩序。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to address the massive volume of textual data
    that exists through text mining methods. We looked at a useful framework for text
    mining, including preparation, word frequency counts and visualization, and topic
    models using LDA with the `tm` package. Included in this framework were other
    quantitative techniques, such as polarity and formality, in order to provide a
    deeper lexical understanding, or what one could call style, with the `qdap` package.
    The framework was then applied to president Obama's seven State of the Union addresses,
    which showed that, although the speeches had a similar style, the core messages
    changed over time as the political landscape changed. Despite it not being practical
    to cover every possible text mining technique, those discussed in this chapter
    should be adequate for most problems that one might face. In the next chapter,
    we are going to shift gears away from building models and focus on a technique
    to get R on the cloud, allowing you to scale your machine learning to whatever
    problem you may be trying to solve.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过文本挖掘方法来处理大量存在的文本数据。我们研究了一个有用的文本挖掘框架，包括准备、词频统计和可视化，以及使用`tm`包的LDA主题模型。该框架还包括其他定量技术，如极性和正式程度，以便通过`qdap`包提供更深入的词汇理解，或者说是一种风格。然后，我们将该框架应用于奥巴马总统的七次国情咨文演讲，结果显示，尽管演讲风格相似，但随着政治格局的变化，核心信息随着时间的推移而发生了变化。尽管全面覆盖所有可能的文本挖掘技术并不实用，但本章中讨论的技术应该足以应对大多数人可能遇到的大部分问题。在下一章中，我们将转变方向，不再专注于构建模型，而是关注一种将R语言部署到云上的技术，这样你就可以将你的机器学习扩展到任何你试图解决的问题。
