- en: Implementing Parametric Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现参数化模型
- en: 'In the previous chapter, we got started with the basics of supervised machine
    learning. In this chapter, we will dive into the guts of several popular supervised
    learning algorithms within the parametric modeling family. We''ll start this chapter
    by formally introducing parametric models. Then, we''ll introduce two very popular
    parametric models: linear and logistic regression. We''ll spend some time looking
    at their inner workings and then we''ll jump into Python and actually code those
    workings from scratch.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了监督式机器学习的基础知识。在本章中，我们将深入探讨几个流行的监督学习算法，这些算法属于参数化建模范畴。我们将从正式介绍参数化模型开始。然后，我们将介绍两个非常流行的参数化模型：线性回归和逻辑回归。我们将花些时间了解它们的内部工作原理，然后跳进Python，实际编写这些工作原理的代码，从零开始实现。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Parametric models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数化模型
- en: Implementing linear regression from scratch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始实现线性回归
- en: Logistic regression models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: Implementing logistic regression from scratch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始实现逻辑回归
- en: The pros and cons of parametric models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数化模型的优缺点
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need to install the following software, if you haven''t
    already done so:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章内容，如果你还没有安装以下软件，请安装：
- en: Jupyter Notebook
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: Anaconda
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Python
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: The code files for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在[https:/​/​github.​com/​PacktPublishing/](https:/​/​github.​com/​PacktPublishing/)找到。
- en: Supervised-Machine-Learning-with-Python](https:/%E2%80%8B/%E2%80%8Bgithub.%E2%80%8Bcom/%E2%80%8BPacktPublishing/%20Supervised-Machine-Learning-with-Python).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[Supervised-Machine-Learning-with-Python](https:/%E2%80%8B/%E2%80%8Bgithub.%E2%80%8Bcom/%E2%80%8BPacktPublishing/%20Supervised-Machine-Learning-with-Python).'
- en: Parametric models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数化模型
- en: 'When it comes to supervised learning, there are two families of learning algorithms:
    **parametric** and **non-parametric**. This area also happens to be a hotbed for
    gatekeeping and opinion-based conjecture regarding which is better. Basically,
    parametric models are finite-dimensional, which means that they can learn only
    a defined number of model parameters. Their learning stage is typically categorized
    by learning some vector theta, which is also called a **coefficient**. Finally,
    the learning function is often a known form, which we will clarify later in this
    section.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，有两类学习算法：**参数化**和**非参数化**。这一领域也恰好是关于哪种方法更好的门槛和基于观点的猜测的热议话题。基本上，参数化模型是有限维度的，这意味着它们只能学习有限数量的模型参数。它们的学习阶段通常是通过学习一些向量θ来进行的，这个向量也叫做**系数**。最后，学习函数通常是已知的形式，我们将在本节稍后做出解释。
- en: Finite-dimensional models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有限维度模型
- en: 'If we go back to our definition of supervised learning, recall that we need
    to learn some function, *f*. A parametric model will summarize the mapping between
    *X*, our matrix, and *y*, our target, within a constrained number of summary points.
    The number of points is typically related to the number of features in the input
    data. So, if there are three variables, *f* will try to summarize the relationship
    between *X* and *y* given that there are three values in theta. These are called
    **model parameters** *f: y=f(X)*.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们回顾一下监督学习的定义，记住我们需要学习某个函数，*f*。一个参数化模型将通过有限数量的总结点来概括*X*（我们的矩阵）与*y*（我们的目标）之间的映射关系。总结点的数量通常与输入数据中的特征数量相关。因此，如果有三个变量，*f*将尝试在θ中给定三个值的情况下，总结*X*和*y*之间的关系。这些被称为**模型参数**
    *f: y=f(X)*。'
- en: Let's back up and explain some of the characteristics of parametric models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下并解释一些参数化模型的特性。
- en: The characteristics of parametric learning algorithms
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数化学习算法的特性
- en: 'We will now cover different features of parametric learning algorithms:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍参数化学习算法的不同特点：
- en: Model parameters are generally constrained to the same dimensionalities of the
    input space
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数通常被限制在与输入空间相同的维度
- en: You can point to a variable and its corresponding parameter value and typically
    learn something about variable importance or its relationship to *y*, our target
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以指向一个变量及其相应的参数值，通常可以了解该变量的重要性或它与*y*（我们的目标）的关系
- en: Finally, they are conventionally fast and do not require much data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，传统上它们运行速度较快，并且不需要太多数据
- en: Parametric model example
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数化模型示例
- en: Imagine that we are asked to estimate the price of a house given its square
    footage and number of bathrooms. How many parameters are we going to need to learn?
    How many parameters will we have to learn for our example?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们被要求根据房屋的面积和浴室数量来估算房屋的价格。我们需要学习多少个参数？在我们的例子中，我们将需要学习多少个参数？
- en: Well, given the square footage and number of bathrooms we will have to learn
    two parameters. So, our function can be expressed as the estimated price given
    two variables—square footage and the number of bathrooms—*P1* and *P2*. *P1* will
    be the relationship between square footage and price. *P2* will be the relationship
    between the number of bathrooms and price.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，给定面积和浴室数量，我们将需要学习两个参数。因此，我们的函数可以表示为给定两个变量——面积和浴室数量——的估计价格——*P1*和*P2*。*P1*表示面积与价格之间的关系。*P2*表示浴室数量与价格之间的关系。
- en: 'The following code shows the problem set up in Python. `x1` is our first variable—the
    amount of square footage. You can see that this ranges from anything as small
    as `1200` to as high as `4000` and our price range is anywhere from `200000` to
    `500000`. In `x2`, we have the number of bathrooms. This ranges from as few as
    `1` to as many as `4`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了在 Python 中设置的问题。`x1`是我们的第一个变量——房屋的面积。你可以看到它的范围从最小的`1200`到最高的`4000`，我们的价格范围从`200000`到`500000`。在`x2`中，我们有浴室的数量。这个范围从最少的`1`到最多的`4`：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, you can see from our plots that there seems to be a positive trend going
    on here. And that makes sense. But we''re going to find out as we dig into this
    example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以从我们的图表中看到，这里似乎有一个正相关的趋势，这很有道理。但我们会随着深入这个例子而发现更多：
- en: '![](img/3082ec42-423e-4e6f-a435-fd878d94150c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3082ec42-423e-4e6f-a435-fd878d94150c.png)'
- en: Now, we want to learn a function that estimates the price, and that function
    is simply going to be the inner product of our estimated parameters in each vector row of
    our data. So, we are performing linear regression here. Linear regression can
    conveniently be solved by using the least squares equation. Since we technically
    have an infinite number of possible solutions to this problem, least squares will
    find the solution that minimizes the sum of the squared error.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要学习一个估算价格的函数，这个函数实际上就是我们数据中每一行向量的估计参数的内积。因此，我们在这里执行的是线性回归。线性回归可以通过最小二乘法方程方便地求解。由于我们技术上有无数个可能的解，最小二乘法将找到一个解，该解最小化平方误差的和。
- en: 'Here, we complete our least squares on `X`, and learn our parameters in the
    first cell. Then, in the next cell, we multiply `X` by the theta parameters that
    we just learned to get the predictions. So, if you really dig into it, there''s
    only one home that we grossly underestimate in value: the second to last one,
    which is `1200` square foot and has one bathroom. So, it''s probably an apartment
    and it may be located in a really hot part of town, which is why it was priced
    so highly to begin with:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对`X`完成了最小二乘法，并在第一个单元格中学习了我们的参数。然后，在下一个单元格中，我们将`X`与我们刚学习的theta参数相乘以得到预测结果。因此，如果你深入研究，你会发现只有一个房子我们严重低估了它的价值：倒数第二个房子，它的面积是`1200`平方英尺，只有一个浴室。所以，它可能是一个公寓，而且可能位于市区的一个非常热门的地方，这就是为什么它最初被定价这么高的原因：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出结果如下：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now to pick apart our parameters. With each square foot that we add to our house,
    the estimated price jumps by 142 dollars and 58 cents, which intuitively makes
    sense. However, for each bathroom we add, our house decreases in value by 24,000
    dollars: *Price = 142.58 *sqft + -23629.43*bathrooms*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，来分析我们的参数。每增加一平方英尺，估算价格将上涨142.58美元，这在直观上是有道理的。然而，每增加一个浴室，我们的房子价值将减少24,000美元：*Price
    = 142.58 *sqft + -23629.43*bathrooms*。
- en: There's another conundrum here. By this logic, if we had a house with 3,000
    square feet and 0 bathrooms, it would be priced in the ballpark of what a 4,000-square-feet
    home that has four bathrooms is. So, there's obviously some limitations with our
    model here. When we try to summarize the mapping with few features and data, there
    are going to be some non sequiturs that emerge. But there are some other factors
    that we didn't consider that can help us out when we're fitting our linear regression.
    First of all, we did not fit an intercept and we did not center our features.
    So, if you go back to middle school or even early high school algebra, you will
    remember that, when you're fitting your good line on a Cartesian plot, the intercept
    is where the line intersects the *y* axis, and we did not fit one of those.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有另一个难题。根据这个逻辑，如果我们有一座3000平方英尺且没有浴室的房子，它的价格应该和一座4000平方英尺、拥有四个浴室的房子差不多。所以显然，我们的模型存在一些局限性。当我们尝试用少量特征和数据来总结映射关系时，肯定会出现一些不合逻辑的情况。但是，还有一些我们未曾考虑的因素可以在拟合线性回归时帮助我们。首先，我们没有拟合截距，也没有对特征进行中心化。所以，如果你回忆起初中的代数，或者甚至早期高中的代数，你会记得，当你在笛卡尔坐标图上拟合最优直线时，截距是线与*y*轴的交点，而我们没有拟合这样的截距。
- en: 'In the following code, we have centered our data before solving the least squares
    and estimated an intercept, which is simply the average of `y`, the actual prices
    minus the inner product of the `X` var, which is the means of the columns of `X`
    and the estimated parameters:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们在求解最小二乘法之前已对数据进行了中心化，并估计了截距，该截距实际上是`y`的平均值，实际价格减去`X`变量的内积，`X`的列均值与估计的参数的内积：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the preceding code snippet is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段的输出如下：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So, that summarizes the introduction to the math and the concept behind linear
    regression, as well as that of parametric learning. In linear regression, we are
    simply fitting the best line across a number of points, trying to minimize the
    sum of squared errors there. In the next section, we will learn about PyCharm,
    and walk through how to actually code a linear regression class from scratch.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就总结了线性回归背后的数学和概念介绍，以及参数化学习的概念。在线性回归中，我们只是在一组数据点上拟合一条最佳直线，试图最小化平方误差的和。在接下来的部分，我们将了解PyCharm，并逐步演示如何从头开始编写一个线性回归类。
- en: Implementing linear regression from scratch
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头实现线性回归
- en: Linear regression solves the least squares equation to discover the parameters
    vector theta. In this section, we will walk through the source code for a linear
    regression class in the `packtml` Python library and then cover a brief graphical
    example in the `examples` directory.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归通过求解最小二乘方程来发现参数向量theta。在本节中，我们将逐步讲解`packtml` Python库中线性回归类的源代码，并简要介绍`examples`目录中的图形示例。
- en: 'Before we look at the code, we will be introduced to the interface that backs
    all of the estimators in the book. It is called `BaseSimpleEstimator`, which is
    an abstract class. It''s going to enforce only one method, which is `predict`.
    Different subclass layers are going to enforce other methods for different model
    families. But this layer backs all the models that we will build, as everything
    that we are putting together is supervised, so it''s all going to need to be able
    to predict. You will notice that the signature is prescribed in the `dock` string.
    Every model will accept `X` and `y` in the signature, as well as any other model
    hyperparameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看代码之前，我们将介绍书中所有估计器背后的接口。它被称为`BaseSimpleEstimator`，是一个抽象类。它将强制执行唯一的一个方法，即`predict`。不同的子类层将强制执行其他方法，以适应不同的模型家族。但这个层次支持我们将构建的所有模型，因为我们正在构建的所有内容都是监督学习的，所以它们都需要具备预测能力。你会注意到，这个签名在`dock`字符串中有明确规定。每个模型的签名都会接受`X`和`y`，以及其他任何模型超参数：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The BaseSimpleEstimator interface
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BaseSimpleEstimator接口
- en: 'The first flush is similar to that of a scikit-learn base estimator interface.
    But there are several differences. First of all, we''re not going to permit as
    many options when we build a model. Furthermore, the model is trained at the moment
    it''s instantiated. This also differs from scikit-learn in the fact that we don''t
    have a `fit` method. Scikit-learn has a `fit` method to permit grid searches and
    hyperparameter tuning. So, this is just one more reason that we''re differing
    from their signature. With that, let''s go ahead and look into linear regression:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次刷洗（flush）与scikit-learn的基础估计器接口类似。但也有几个不同之处。首先，当我们构建模型时，我们不会允许太多选项。此外，模型在实例化时立即开始训练。这与scikit-learn不同，因为我们没有`fit`方法。scikit-learn有一个`fit`方法来允许网格搜索和超参数调优。因此，这也是我们与它们的签名不同的另一个原因。好了，接下来让我们看一下线性回归：
- en: 'If you have PyCharm, go ahead and open it up. We are going to be inside the
    `packtml` `Hands-on-Supervised-Machine-Learning-with-Python` library, as shown
    in the following code. You can see this is in PyCharm. This is just the root level
    of the project level and the package we''re going to be working with is `packtml`.
    We are just going to walk through how all of the `simple_regression.py` file code
    works. If you are not using PyCharm, Sublime is an alternative, or you can use
    any other text editor of your preference:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你有PyCharm，赶快打开它。我们将位于`packtml`的`Hands-on-Supervised-Machine-Learning-with-Python`库中，如下所示的代码所示。你可以看到这是在PyCharm中。这只是项目的根目录，而我们将要使用的包是`packtml`。我们将逐步讲解`simple_regression.py`文件代码的工作原理。如果你没有使用PyCharm，Sublime是一个替代选择，或者你可以使用任何你喜欢的文本编辑器：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`base.py`, which is is located inside our package level, will provide the interface
    for `BaseSimpleEstimator`, which we will use across the entire package. The only
    method that is going to be enforced on the abstract level for everything is the
    `predict` function. This function will take one argument, which is `X`. We already
    mentioned that supervised learning means that we will learn a function, *f,* given
    `X` and `y`, such that we can approximate ![](img/05adbda5-3f19-428e-b0da-e3beccb26ed9.png) given
    ![](img/634ff51a-a39a-4a3c-a23a-a3cdc1e4313c.png), or the `X` test in this case.
    Since every subclass is going to implement a different `predict` method, we will
    use the abstract method, which is `base`, as shown in the following code snippet:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`base.py`，它位于我们的包级别内，将提供`BaseSimpleEstimator`的接口，我们将在整个包中使用。唯一将在抽象级别强制执行的方法是`predict`函数。该函数将接受一个参数，即`X`。我们已经提到过，监督学习意味着我们将学习一个函数，*f*，给定`X`和`y`，从而能够近似！[](img/05adbda5-3f19-428e-b0da-e3beccb26ed9.png)，给定！[](img/634ff51a-a39a-4a3c-a23a-a3cdc1e4313c.png)，或者在这种情况下的`X`测试。由于每个子类将实现不同的`predict`方法，我们将使用抽象方法，即`base`，如下所示的代码片段：'
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, inside the `regression` submodule, we will open the `simple_regression.py` file.
    This file will implement a class called `SimpleLinearRegression`. We call it simple
    just so you don''t confuse it with the scikit-learn linear regression:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在`regression`子模块内，我们将打开`simple_regression.py`文件。这个文件将实现一个名为`SimpleLinearRegression`的类。我们称它为“简单”，是为了避免与scikit-learn的线性回归混淆：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`SimpleLinearRegression` is going to take two arguments. `X`, which is our
    matrix covariance, and `y`, the training targets, explained as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleLinearRegression`将接受两个参数。`X`，即我们的协方差矩阵，和`y`，即训练目标，具体解释如下：'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, in our signature, the very first thing that we will do inside the `init`
    function is run this through scikit-learn's `check_X_y`. We will make sure that
    the dimensionality matches between `X` and `y`, as it won't work for us to pass
    a vector of training targets that is smaller than that of the number of samples
    in `X` and vice versa. We are also enforcing that everything that is in `y` is
    numeric.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在我们的签名中，我们将在`init`函数内做的第一件事是通过scikit-learn的`check_X_y`运行它。我们将确保`X`和`y`之间的维度匹配，因为如果传递的训练目标向量小于`X`中的样本数或反之，将无法正常工作。我们还强制要求`y`中的所有内容都是数值型的。
- en: 'The next thing we need to do is compute the mean of the columns in `X`, so
    that we can center everything, and the mean of the values in `y`, so that we can
    center them. In this entire function, it is from the least squares optimization
    function that we pulled out of the NumPy library. So, we''re just going to feed
    in `X` and `y`, which are now centered in `lstsq`. We will get back three things,
    the first of which is theta, which is the learned parameter. So, `X.theta` is
    going to be the best approximate value of `y`. We''re then going to get the rank,
    which is the rank of `matrix` and `singular_values`, in case you want to dig into
    the decomposition of the actual solution. As discussed in the last section, regarding
    the mean house cost, if we''re computing the value of a house minus the inner
    product of `X_means`, the column means is a vector times theta, another vector.
    So, we''re going to get a scalar value here for the intercept and we''re going
    to assign some `self` attributes:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是计算`X`中每列的均值，以便我们可以对所有数据进行中心化，同时计算`y`中值的均值，以便我们也可以对其进行中心化。在这个整个函数中，我们提取自NumPy库的最小二乘优化函数。因此，我们只需要输入现在已经中心化的`X`和`y`，并传递给`lstsq`。我们将返回三样东西，其中第一项是theta，这是学习得到的参数。所以，`X.theta`将是`y`的最佳近似值。接下来我们将得到秩，这就是`matrix`的秩，以及`singular_values`，如果你想深入研究实际解的分解过程。正如上一节所讨论的关于房屋平均成本的问题，如果我们计算一栋房子价值减去`X_means`的内积，列均值是一个向量乘以theta，另一个向量。因此，在这里我们将得到一个标量值作为截距，并将分配一些`self`属性：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The moment that you instantiate a class, you have fit a linear regression.
    However, we have to override the `predict` functions from the `BaseSimpleEstimator` superclass.
    To predict this, all you have to do is compute the inner product of `X`, the new
    matrix on `theta`, and the parameters that we''ve already learned, and then add
    the intercept. Now, what differs here from what you saw on the constructor is
    that we don''t have to re-center `X`. If an `X` test comes in, the only time we
    center the data is when we''re learning the parameters and not when we''re applying
    them. Then, we will multiply `X` times the parameters, the inner product there,
    and then add the intercept. Now we have a vector of predicted ![](img/77b9e227-5b76-4859-9918-cf731a200011.png) values:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你实例化了一个类，你就完成了线性回归的拟合。然而，我们必须重写`BaseSimpleEstimator`超类中的`predict`函数。为了进行预测，所要做的就是计算`X`和`theta`的内积，以及我们已经学习到的参数，再加上截距。与构造函数中看到的不同之处在于，我们不需要重新中心化`X`。如果一个`X`测试数据进来，唯一需要中心化数据的时刻是在学习参数时，而不是在应用它们时。然后，我们将用`X`乘以参数，计算内积，再加上截距。现在我们得到了一个预测值的向量！[](img/77b9e227-5b76-4859-9918-cf731a200011.png)：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So, now, we can go ahead and look at one of our examples. Open up the `examples`
    directory at the project level, and then open up `regression`. We will look at
    the `example_linear_regression.py` file, as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以继续查看我们的一个示例。在项目级别打开`examples`目录，然后打开`regression`目录。我们将查看`example_linear_regression.py`文件，如下所示：
- en: '![](img/fe4735c6-9f3c-4fbc-9e3f-f88a0b11ded3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe4735c6-9f3c-4fbc-9e3f-f88a0b11ded3.png)'
- en: 'Let''s walk through exactly what happens here, just to show you how we can
    apply this to real data. We will load up the linear regression that we just created
    and import scikit-learn''s linear regression so that we can compare the results.
    The first thing we''re going to do is create the `X` matrix of random values with
    `500` samples and `2` dimensions. We will then create the `y` matrix, which will
    be a linear combination of the first `X` variable and `0`, which will be `2` times
    the first column plus `1.5` times the second column. The reason we are doing this
    is to show that our linear regression class is going to learn these exact parameters, `2`
    and `1.5`, as shown in the following code snippets:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过这里发生的过程，向你展示如何将它应用于真实数据。我们将加载刚才创建的线性回归，并导入scikit-learn的线性回归，以便进行结果比较。我们要做的第一件事是创建一个包含`500`个样本和`2`个维度的随机值`X`矩阵。然后我们将创建`y`矩阵，它将是第一个`X`变量和`0`的线性组合，即第一个列的`2`倍加上第二列的`1.5`倍。我们这么做是为了展示我们的线性回归类将学习到这些确切的参数，`2`和`1.5`，如以下代码片段所示：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As we''ve already discussed, we want to split our data. You never want to just
    evaluate and fit against your in-sample data; otherwise, you''re prone to overfitting:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论过的，我们需要分割数据。你绝不应该仅仅对样本数据进行评估和拟合；否则，你容易发生过拟合：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will fit our linear regression and compute our predictions. So, we
    can also show with our assertion that the theta that we learned is incredibly
    close to the actual theta that we expected; that is, `2` and `1.5`. Therefore,
    our predictions should resemble the `y` train input:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将拟合我们的线性回归并计算我们的预测结果。因此，我们还可以通过我们的断言来显示我们学习到的θ与我们期望的实际θ非常接近，即`2`和`1.5`。因此，我们的预测应该与`y`训练输入相似：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will fit a scikit-learn regression to show that we get a similar result,
    if not the exact same result. We''re showing that the theta in the class that
    we just created matches the coefficients that scikit-learn produces. Scikit-learn
    is an incredibly well-tested and well-known library. So, the fact that they match
    shows that we are on the right track. Finally, we can show that our predictions
    are very close to the scikit-learn solution:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将拟合一个scikit-learn回归模型，展示我们获得的结果与之前的相似，如果不是完全相同的话。我们将展示我们刚刚创建的类中的θ与scikit-learn生成的系数相匹配。scikit-learn是一个经过充分测试和广泛使用的库。因此，它们匹配的事实表明我们走在正确的道路上。最后，我们可以展示我们的预测结果非常接近scikit-learn的解决方案：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We will now fit a linear regression on a class, so that we can look at a plot.
    To do this, let''s go ahead and run the following example:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在一个类上进行线性回归拟合，这样我们就可以查看图表。为此，请运行以下示例：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Go to the Terminal inside the `Hands-on-Supervised-Machine-Learning-with-Python-master`
    top level: the project level. Remember to source the content environment. So,
    if you''ve not already done that, you will need to `source activate` for Unix
    users, or just activate by typing the following:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入`Hands-on-Supervised-Machine-Learning-with-Python-master`的顶部项目目录下的终端：即项目级别。记得激活环境。如果还没有激活环境，Unix用户需要输入`source
    activate`，或者直接通过输入以下命令激活：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run this example by typing the name of the file, which is `examples/regression/example_linear_regression.py`:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过输入文件名运行此示例，即`examples/regression/example_linear_regression.py`：
- en: '![](img/adf40228-6a5b-486e-b22c-ed1b546c7a9c.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adf40228-6a5b-486e-b22c-ed1b546c7a9c.png)'
- en: 'When we run the preceding code, we should get our plot, as shown in the following
    screenshot:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们应该会看到我们的图表，如下图所示：
- en: '![](img/23f870bd-3a57-4f1a-99ac-4803aa1e57f8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23f870bd-3a57-4f1a-99ac-4803aa1e57f8.png)'
- en: We can see that our sum of residuals is essentially zero, meaning that we were
    spot on in our predictions. It is easy in this case, because we created a scenario
    where we learned our exact theta values. You can see here the line that we're
    fitting across one variable. This is a bit more approximated given that we only
    learned it on one variable. It seems to both qualitatively and quantitatively
    match what we expected via scikit-learn's predictions and coefficients.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到残差的总和基本为零，这意味着我们的预测非常准确。这个情况比较简单，因为我们创建了一个可以学习到确切θ值的场景。你可以看到这里我们正在对一个变量进行拟合。由于我们仅在一个变量上进行学习，所以这是一个更为近似的结果。从定性和定量上看，这与我们通过scikit-learn的预测和系数所期望的结果相匹配。
- en: In the next section, we will learn about logistic regression models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习逻辑回归模型。
- en: Logistic regression models
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: In this section, we will look at logistic regression, which is the first hill-climbing
    algorithm that we'll cover, and we will have a brief recap of linear regression.
    We will also look at how logistic regression differs both mathematically and conceptually.
    Finally, we will learn the core algorithm and explain how it makes predictions.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍逻辑回归，它是我们将要覆盖的第一个爬坡算法，并简要回顾线性回归。我们还将看看逻辑回归在数学上和概念上的不同之处。最后，我们将学习核心算法并解释它是如何进行预测的。
- en: The concept
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概念
- en: Logistic regression is conceptually the inverse of linear regression. What if,
    rather than a real value, we want a discrete value or a class? We have already
    seen one example of this type of question early on when we wanted to predict whether
    or not an email was spam. So, with logistic regression, rather than predicting
    a real value, we can predict the probability of class membership, also known as
    classification.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归在概念上是线性回归的逆。如果我们想要的是离散值或类别，而不是一个真实值呢？我们已经看到过这种类型的问题的一个例子，早期我们曾想预测一封电子邮件是否是垃圾邮件。因此，使用逻辑回归时，我们可以预测类别成员的概率，也就是所谓的分类。
- en: The math
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学原理
- en: 'Mathematically, logistic regression is very similar to linear regression. The
    inner product of our parameters and *X* represent the log odds of the membership
    of a class, which is simply the natural log of the probabilities over *1* minus
    the probabilities:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，逻辑回归与线性回归非常相似。我们参数和*X*的内积表示类别成员的对数几率，这实际上是概率除以1减去概率的自然对数：
- en: '![](img/1c378b72-3456-40b9-a380-9b0bcecbc30c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c378b72-3456-40b9-a380-9b0bcecbc30c.png)'
- en: What we really want are the probabilities of the class membership. We can back
    out of the log odds and determine the probabilities using the sigmoid or logistic
    function.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想要的是类别成员的概率。我们可以从对数几率中反推出概率，并使用sigmoid或逻辑函数来确定这些概率。
- en: The logistic (sigmoid) transformation
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑（sigmoid）变换
- en: 'In the following code, we will create an `X` vector of values between `-10`
    and `10` and then apply the logistic transformation to get `y`, which we can then
    plot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们将创建一个`X`向量，其值介于`-10`和`10`之间，然后应用逻辑变换得到`y`，然后我们可以将其绘制出来：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see, we get an S-shaped curve with the original `X` values on the
    *x* axis and the `y` values on the *y* axis. Notice that everything is mapped
    between zero and one on the *y* axis. These can now be interpreted as probabilities:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们得到一个S形曲线，原始的`X`值在*x*轴上，`y`值在*y*轴上。请注意，所有的值都被映射到*y*轴上的0到1之间。现在，这些可以被解释为概率：
- en: '![](img/aaaab465-81eb-4407-ad05-b3699b28a07d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaaab465-81eb-4407-ad05-b3699b28a07d.png)'
- en: The algorithm
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: 'Now, we have already covered the logistic regression algorithm briefly in the
    earlier section. But here''s a recap of how we learn our parameters:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经简要介绍了前面章节中的逻辑回归算法。这里是我们如何学习参数的回顾：
- en: 'We start out by initializing theta as a zero vector:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将theta初始化为零向量：
- en: '![](img/d7b1d136-7920-4d85-8c0c-b2a8fbb157cb.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7b1d136-7920-4d85-8c0c-b2a8fbb157cb.png)'
- en: 'As this is a hill-climbing algorithm, it is iterative. So, for each iteration,
    we compute the log odds as theta transpose *X* and then transform them via the
    logistic transformation:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个爬坡算法，它是迭代的。因此，对于每次迭代，我们计算对数几率，即theta转置乘以*X*，然后通过逻辑变换进行转换：
- en: '![](img/11564b99-d4fb-4fd5-9fcf-81418826430c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11564b99-d4fb-4fd5-9fcf-81418826430c.png)'
- en: 'Next, we compute the gradient, which is a vector of partial derivatives of
    the slope of our function, which we covered in the last section. We simply compute
    this as *X* transpose times the residuals, *y - ![](img/e35e83e5-16a8-41b6-b237-2a443870a922.png).*
    Keep in mind that *![](img/26c98c1f-1dc1-478e-8a0b-2b25162bb75b.png) *is the probability
    now following the logistic transformation:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算梯度，这是我们函数斜率的偏导数向量，这在上一节中已经介绍过。我们只需要计算这个值为*X*转置乘以残差，*y - ![](img/e35e83e5-16a8-41b6-b237-2a443870a922.png).*
    请记住，*![](img/26c98c1f-1dc1-478e-8a0b-2b25162bb75b.png)*现在是经过逻辑变换后的概率：
- en: '![](img/9d1b6e26-2d31-468d-8c23-c0a2e1ad025a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d1b6e26-2d31-468d-8c23-c0a2e1ad025a.png)'
- en: 'Finally, we can update our coefficients as theta plus the gradient. You can
    also see a ![](img/1185d50d-f929-492f-8fb9-57e0a9b27bd5.png) parameter here, which
    is simply a learning rate parameter. This controls how radically we allow the
    coefficients to grow for each step:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过将theta加上梯度来更新我们的系数。你还可以看到一个！[](img/1185d50d-f929-492f-8fb9-57e0a9b27bd5.png)参数，这仅仅是一个学习率参数。它控制我们在每一步中允许系数增长的幅度：
- en: '![](img/b40eca30-20d9-43e9-869d-cf52a42fc3f2.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b40eca30-20d9-43e9-869d-cf52a42fc3f2.png)'
- en: Creating predictions
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建预测
- en: 'We finally converge our gradient, which is no longer updating our coefficients,
    and we are left with a bunch of class probabilities. So, how do we produce the
    predictions? All we have to do is get above a given threshold and we can get classes.
    So, in this section, we will be using a binary problem. But, for multi-class,
    we could just use the argmax functions for each class. Now, we will produce discrete
    predictions, as shown in the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终使梯度收敛，梯度不再更新我们的系数，最终留下的是一堆类别概率。那么，我们该如何生成预测呢？我们只需要超越一个给定的阈值，就能得到类别。因此，在本节中，我们将使用一个二分类问题。但是，对于多分类问题，我们只需对每个类别使用argmax函数。现在，我们将生成离散预测，如以下代码所示：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the next section, we will walk through the implementation of logistic regression
    from scratch in the `packtml` package.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示如何从头开始在`packtml`包中实现逻辑回归。
- en: Implementing logistic regression from scratch
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始实现逻辑回归
- en: In this section, we will walk through the implementation of logistic regression
    in Python within the `packtml` package. We will start off with a brief recap of
    what logistic regression seeks to accomplish and then go over the source code
    and look at an example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步讲解在`packtml`包中使用Python实现逻辑回归的过程。我们将简要回顾一下逻辑回归的目标，然后讲解源代码并查看一个示例。
- en: Recall that logistic regression seeks to classify a sample into a discrete category,
    also known as **classification**. The logistic transformation allows us to transform
    the log odds that we get from the inner product of our parameters and `X`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，逻辑回归旨在将一个样本分类到一个离散的类别中，也称为**分类**。逻辑转换使我们能够将从参数和`X`的内积中得到的对数赔率转换过来。
- en: Notice that we have three Python files open. One is `extmath.py`, from within
    the `utils` directory inside of `packtml`; another is `simple_logistic.py`, from
    within the `regression` library in `packtml`; and the final one is an `example_logistic_regression.py` file,
    inside the `examples` directory and `regression`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们打开了三个Python文件。一个是`extmath.py`，位于`packtml`中的`utils`目录下；另一个是`simple_logistic.py`，位于`packtml`中的`regression`库下；最后一个是`example_logistic_regression.py`文件，位于`examples`目录和`regression`文件夹中。
- en: 'We will dive right into the code base using the following steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤直接进入代码库：
- en: 'We will start with the `extmath.py` file. There are two functions that we will
    be using here. The first is `log_likelihood`, which is the objective function
    that we would like to maximize inside of the logistic regression:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从`extmath.py`文件开始。这里有两个函数我们将使用。第一个是`log_likelihood`，这是我们希望在逻辑回归中最大化的目标函数：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The specifics of the `log_likelihood` function are not necessarily critical
    for understanding how logistic regression works. But, essentially, what you can
    see here is that we will be summing up `y` times the log odds, minus the log of
    `1` plus the exponential of the log odds. Weighted here is essentially the log
    odds, that is, `X.dot(w)`, `w` being the theta that we are learning. This is the
    objective function. So, we''re summing over those logs:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_likelihood`函数的具体实现并不是理解逻辑回归工作原理的关键。但本质上，你可以看到我们将对`y`与对数赔率的乘积求和，再减去`1`加上对数赔率指数的对数。这里加权的实际上是对数赔率，即`X.dot(w)`，其中`w`是我们正在学习的θ。这就是目标函数。所以，我们在对这些对数进行求和：'
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The second is the `logistic_sigmoid` function, which we will now learn in greater
    depth. This is how we can back out of the log odds to get the class probabilities,
    which is simply `1` over `1` plus the exponential of the negative log odds, where
    `x` is the log odds in this case:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个是`logistic_sigmoid`函数，我们将深入学习它。这是我们如何从对数赔率中反推得到类别概率的方法，它的计算方式是`1`除以`1`加上对数赔率的指数，其中`x`在这里是对数赔率：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will use both of these functions inside the logistic regression class. So,
    inside of `simple_logistic.py`, you will see a class that resembles the linear
    regression class that we used in the last section:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在逻辑回归类中使用这两个函数。因此，在`simple_logistic.py`中，你将看到一个类似于我们在上一节中使用的线性回归类的类：
- en: '[PRE24]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, this function, or class, extends `BaseSimpleEstimator`. We will override
    the `predict` function at some point and the constructor will fit the model and
    learn the parameters. So, we have four hyperparameters here that come in for this
    class. The first of which is `X`, which is our training data; then `y`, as our
    training labels; and `n_steps` recalls that logistic regression as an iterative
    model. So, `n_steps` is the number of iterations that we will perform to which
    the `learning_rate` is our lambda. If you go back to the algorithm itself, this
    controls how quickly we update our theta given the gradients, and, lastly, `loglik_interval`.
    This is just a helper parameter. Computing the log likelihood can be pretty expensive.
    We can see this explanation in the following code snippet:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，这个函数或类扩展了`BaseSimpleEstimator`。我们将稍后重写`predict`函数，构造函数将拟合模型并学习参数。因此，这个类有四个超参数。第一个是`X`，即我们的训练数据；然后是`y`，作为我们的训练标签；`n_steps`回忆一下，逻辑回归是一个迭代模型。所以，`n_steps`是我们将执行的迭代次数，`learning_rate`是我们的λ。如果你回顾一下算法本身，这控制着我们如何根据梯度快速更新θ；最后是`loglik_interval`，这是一个辅助参数。计算对数似然可能非常昂贵。我们可以在下面的代码片段中看到这个解释：
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'At the end, we get `theta`, the parameters, `intercept`, and then `log_likelihood`,
    which is just a list of the computed log likelihoods at each of the intervals.
    We will first check that our `X` and `y` are as we want them to be, which is `0,
    1`. We won''t do anything close to what scikit-learn is capable of. We will also
    not allow different string classes either:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们得到`theta`、参数、`intercept`以及`log_likelihood`，后者只是每个区间计算出的对数似然值的列表。我们将首先检查我们的`X`和`y`是否符合预期，即`0,
    1`。我们不会做接近scikit-learn能够完成的事情。我们也不会允许不同的字符串类别：
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, we want to make sure that it's actually binary. The reason for this is
    that we're performing logistic regression, which is discrete between `0` and `1`.
    There is a generalization of the regression, called **softmax regression**, which
    will allow us to use a number of different classes. it's a multi-class classification.
    We will get to this when we get into neural nets. For now, we're constraining
    this to be a binary problem.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们想要确保它实际上是二值的。这是因为我们正在执行逻辑回归，这是一个在`0`和`1`之间离散的过程。回归有一个广义的形式，叫做**softmax回归**，它允许我们使用多个类别进行分类。这是一个多分类问题。当我们深入神经网络时，会讨论到这个。现在，我们将这个问题限制为二分类问题。
- en: 'Next, we want to center and standardize our `X` matrix. That means we''re going
    to subtract the column `means` from `X` and divide it by its standard deviation.
    So, we have mean `0`, and standard deviation `1`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们想要对`X`矩阵进行中心化和标准化。这意味着我们将从`X`中减去列的`均值`并将其除以标准差。所以，我们有均值为`0`，标准差为`1`：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we can do something a little bit clever here when we are learning our
    linear regression parameters, or the logistic regression parameters that we could
    not do in linear regression. We can add the intercept to the matrix while we learn
    it, rather than having to compute it after the fact. We will create a vector of
    ones as a new feature on our `X` matrix, as shown:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以在学习线性回归参数或逻辑回归参数时做一些巧妙的事情，这是线性回归中无法做到的。我们可以在学习时将截距添加到矩阵中，而不是事后计算它。我们将在`X`矩阵上创建一个全为1的向量，作为新的特征，如下所示：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As we defined in our algorithm, we start out by defining that theta is equal
    to zero. There are as many parameters as there are columns in `X`. For each iteration,
    we will compute the log odds here. Then, we transform this with a logistic sigmoid.
    We will compute our residuals as `y - preds`. So, at this point, `preds` is probabilities.
    `y` can be considered to be class probabilities for a binary classification problem
    where `1` is 100% probable that something belongs to class `1`, and `0` is 0%
    probable that something belongs to class `1`:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在算法中定义的那样，我们从定义θ等于零开始。参数的数量等于`X`中的列数。对于每次迭代，我们将在这里计算对数几率。然后，我们使用逻辑 sigmoid
    对其进行转换。我们将计算我们的残差为`y - preds`。所以，在这一点上，`preds`是概率。`y`可以视为二分类问题中的类别概率，其中`1`表示某事属于类别`1`的概率为100%，而`0`表示某事属于类别`1`的概率为0%：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So, we can subtract the probabilities from `y` to get our residuals. In order
    to get our gradient, we will perform `X` times the residuals, which is the inner
    product there. Keep in mind that a gradient is a vector of partial derivatives
    for the slope of our function.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以从`y`中减去概率来得到我们的残差。为了得到我们的梯度，我们将进行`X`与残差的乘法，即那里的内积。请记住，梯度是我们函数的斜率的偏导数向量。
- en: 'We will update `theta` and the parameters by adding the gradient times our
    learning rate. The learning rate is the lambda function that controls how quickly
    we learn. As you may remember, if we learn too quickly, we can overstep a global
    optimum and end up getting a non-optimal solution. If we go too slowly, then we''re
    going to fit for a long time. Logistic regression is an interesting case; as this
    is actually a convex optimization problem, we will have enough iterations to reach
    the global optimum. So, `learning_rate` here is a little bit tongue-in-cheek,
    but this is how, in general, hill-climbing functions work by using `learning_rate`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过将梯度乘以学习率来更新`theta`和参数。学习率是一个控制学习速度的lambda函数。你可能记得，如果我们学习得太快，可能会越过全局最优解，最终得到一个非最优解。如果我们学习得太慢，那我们将需要很长时间来拟合。逻辑回归是一个有趣的案例；因为它实际上是一个凸优化问题，我们将有足够的迭代次数来达到全局最优解。所以，这里的`learning_rate`有点讽刺意味，但一般来说，爬坡函数是通过使用`learning_rate`来工作的：
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The very last step here is that, if we are at the proper intervals, we will
    compute `log_likelihood`. Now, again, you could compute this function at every
    iteration, but it would take you a very long time. We can opt to make this happen
    after every 5 or 10 minutes, which will allow us to see that we're optimizing
    this function. But, at the same time, it means that we don't have to compute it
    at every iteration.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是，如果我们处于适当的间隔，我们将计算`log_likelihood`。现在，虽然你可以在每次迭代时都计算这个函数，但那样会花费很长时间。我们可以选择每5分钟或10分钟计算一次，这样可以让我们看到我们正在优化这个函数。但同时，这也意味着我们不需要在每次迭代时都计算它。
- en: 'Finally, we will save all of these as instance parameters for a class. Notice
    that we are stripping out the intercept and keeping `1` onward as far as the parameters
    go. These are the non-intercept parameters that we''ll just compute in our inner
    product for the predictions:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将把所有这些作为类的实例参数保存。注意，我们正在去掉截距，并从`1`开始保留参数。这些是我们将在内积中计算的非截距参数，用于预测：
- en: '[PRE31]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'So, we take the logistic transformation of `X` times `theta.T` and then add
    in `intercept` after we have centered and standardized our input, `X`, which would
    then give us the probabilities:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们对`X`乘以`theta.T`进行逻辑变换，然后在居中和标准化输入`X`后加上`intercept`，这将给我们带来概率：
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'But, to get the actual prediction, we just round up the probabilities. So,
    in the `predict` function, we will take `predict_proba` and round it up or down
    to either zero or one and get the type as `int`, which will give us our classes
    zero and one:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 但为了获得实际的预测，我们只需将概率四舍五入。所以，在`predict`函数中，我们将使用`predict_proba`并将其四舍五入为0或1，返回类型为`int`，这将给我们类别0和1：
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Example of logistic regression
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归示例
- en: 'Now, as an example, we will look at our `example_logistic_regression.py` script.
    We will compare the output of our `simple_logistic_regression.py` file with that
    of scikit-learn and prove that we get similar, if not exactly equal, parameters
    learned in our output. We use the scikit-learn `make_classification` function
    to create `100` samples and two features and do `train_test_split`. First, we
    will fit our own `SimpleLogisticRegression` with the model that we just walked
    through and take `50` steps, as this is a `50` iteration, as shown in the following
    code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，作为一个例子，我们将查看我们的`example_logistic_regression.py`脚本。我们将比较我们`simple_logistic_regression.py`文件的输出与scikit-learn的输出，并证明我们得到的参数是相似的，如果不是完全相等的话。我们使用scikit-learn的`make_classification`函数创建`100`个样本和两个特征，并进行`train_test_split`。首先，我们将用我们刚刚讲解过的模型拟合我们自己的`SimpleLogisticRegression`并进行`50`步训练，这是一个`50`次迭代，如以下代码所示：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we will compute scikit-learn''s `LogisticRegression` with almost no regularization
    and fit it as shown:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算scikit-learn的`LogisticRegression`，几乎没有正则化，并按照如下方式进行拟合：
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We will run this code. Make sure that you've got your Anaconda environment already
    activated by typing `source activate packt-sml`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行这段代码。确保你已经通过输入`source activate packt-sml`激活了你的Anaconda环境。
- en: If you're on Windows, this would just be `activate packt-sml`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Windows系统，这只需要输入`activate packt-sml`。
- en: 'We see that our test accuracy is 96%, which is pretty close to `Sklearn` at
    100%. Scikit-learn runs more iterations, which is why it gets better accuracy.
    If we ran more iterations, we could theoretically get perfect accuracy. In the
    following output, you can see a perfectly linearly separable boundary here. But,
    since we haven''t run as many iterations, we''re not hitting it. So, what you
    can see in this diagram is that we have this linear boundary, which is the decision
    function we''ve learned, separating these two classes. On the left, you have one
    class, and on the right, you have another, as shown:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们的测试准确率为96%，这与`Sklearn`的100%非常接近。Scikit-learn进行更多的迭代，因此它获得了更好的准确率。如果我们进行更多的迭代，理论上我们可以得到完美的准确率。在以下输出中，你可以看到一个完美的线性可分界限。但由于我们没有进行那么多迭代，我们还没有达到这个结果。所以，在这个图中你可以看到我们有一个线性边界，这就是我们学习到的决策函数，分隔了这两个类别。左边是一个类别，右边是另一个类别，如下所示：
- en: '![](img/b26fd4cd-0ca3-4d1f-bdb5-732d5062ffbc.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b26fd4cd-0ca3-4d1f-bdb5-732d5062ffbc.png)'
- en: 'The output of the preceding code is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/e3eddc15-ee9b-4acb-ab29-e3407fb81f18.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3eddc15-ee9b-4acb-ab29-e3407fb81f18.png)'
- en: Hypothetically, if we ran this code for a hundred or even more iterations, we
    could achieve a perfectly linearly separable plane, which could guarantee a linearly
    separable class, because logistic regression will always reach a global optimum.
    We also know that our formulation is exactly the same as scikit-learn's. So, it's
    just a matter of how many iterations we ran there.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对这段代码进行一百次甚至更多次的迭代，我们可以得到一个完美的线性可分平面，从而保证一个线性可分的类别，因为逻辑回归总是能够达到全局最优。我们也知道，我们的公式与
    scikit-learn 的完全相同。因此，这只取决于我们在这里进行了多少次迭代。
- en: In the next section, we're going to look at some of the pros and cons of parametric
    models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨参数模型的优缺点。
- en: The pros and cons of parametric models
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数模型的优缺点
- en: Parametric models have some really convenient attributes. Namely, they are fast
    to fit, don't require too much data, and can be very easily explained. In the
    case of linear and logistic regression, it's easy to look at coefficients and
    directly explain the impact of fluctuating one variable in either direction. In
    regulated industries, such as finance or insurance, parametric models tend to
    reign supreme, since they can be easily explained to regulators. Business partners
    tend to really rely on the insights that the coefficients produce. However, as
    is evident in what we've already seen so far, they tend to oversimplify. So, as
    an example, the logistic regression decision boundary that we looked at in the
    last section assumes a perfect linear boundary between two classes.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 参数模型具有一些非常方便的特点。即，它们拟合速度快、不需要太多数据，并且可以很容易地解释。在线性和逻辑回归的情况下，我们可以轻松查看系数并直接解释单个变量变化的影响。在金融或保险等受监管行业中，参数模型往往占据主导地位，因为它们可以很容易地向监管机构解释。业务合作伙伴通常非常依赖系数所提供的见解。然而，正如我们迄今所看到的那样，它们往往过于简化。因此，作为一个例子，我们在上一节中看到的逻辑回归决策边界假设了两个类别之间有一个完美的线性边界。
- en: It is rare that the real world can be constrained into linear relationships.
    That said, the models are very simple. They don't always capture the true nuances
    of relationships between variables, which is a bit of a double-edged sword. Also,
    they're heavily impacted by outliers and data scale. So, you have to take great
    care with data preparation. This is one of the reasons that we had to make sure
    we centered and scaled our data before fitting. Finally, if you add data to your
    models, it's unlikely that they're going to get much better. This introduces a
    new concept, which we're going to call bias.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界很少能够被限制为线性关系。话虽如此，模型仍然非常简单。它们并不总能捕捉到变量之间关系的真正细微差别，这有点像双刃剑。此外，它们受异常值和数据规模的影响很大。所以，你必须非常小心地处理数据。这也是我们在拟合之前必须确保对数据进行中心化和标准化的原因之一。最后，如果你向模型中添加数据，模型的表现也不太可能会大幅提升。这引入了一个新的概念，我们称之为偏差。
- en: 'Error due to bias is a concept we will talk about in subsequent chapters. It''s
    the result of a model that is oversimplified. In the following diagram, our model
    oversimplifies a `logit` function by treating it as linear:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差导致的误差是我们将在后续章节中讨论的一个概念。这是一个模型过于简化的结果。在下面的图示中，我们的模型通过将 `logit` 函数视为线性，过度简化了该函数：
- en: '![](img/c3f57086-32f9-43e4-9c09-66a912959d67.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3f57086-32f9-43e4-9c09-66a912959d67.png)'
- en: This is also known as **underfitting**, which is common within the parametric
    model family. There are several ways to combat high bias, most of which we will
    introduce in the next chapter. But, in the spirit of exploring the drawbacks of
    parametric models, it's worth pointing some of them out here. As mentioned before,
    we cannot add more data to learn a better function in high-bias situations. If
    we take the previous example, if you were to add more samples along the logit
    line, our learned or blue line would not approach the true function any more than
    it already has, because it's linear. It's not complex enough to model the true
    underlying function, which is an unfortunate consequence of the simplicity of
    many parametric models. More model complexity and complex nonlinear features usually
    help to correct high bias.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为**欠拟合**，这是参数模型家族中常见的问题。应对高偏差有几种方法，我们将在下一章介绍其中的大部分。但在探讨参数模型的缺点时，在这里指出其中一些是值得的。如前所述，在高偏差的情况下，我们无法通过增加更多数据来学习更好的函数。如果我们以之前的例子为例，假设你沿着
    logit 线增加更多样本，我们学习到的或者蓝色的那条线不会比已经得到的更接近真实函数，因为它是线性的。它不够复杂，无法建模真实的基础函数，这是许多参数模型简化性的一个不幸后果。更多的模型复杂性和复杂的非线性特征通常有助于纠正高偏差。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to parametric models. We then walked through
    the low-level math of linear logistic regression, before moving on to implementations
    in Python. Now that we've covered some of the pros and cons of parametric models,
    in the next chapter, we will take a look at some non-parametric models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了参数化模型。接着，我们详细讲解了线性逻辑回归的基础数学内容，然后转向了 Python 中的实现。现在我们已经讨论了一些参数化模型的优缺点，在下一章中，我们将看看一些非参数化模型。
