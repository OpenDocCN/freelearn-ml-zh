- en: Evaluation and Validation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估与验证
- en: In order to have sustainable, responsible machine learning workflows and develop
    machine learning applications that produce true value, we need to be able to measure
    how well our machine learning models perform. We also need to ensure that our
    machine learning models generalize to data that they will see in production. If
    we don't do these things, we are basically shooting in the dark. We will have
    no understanding of the expected behavior of our models and we won't be able to
    improve them over time.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拥有可持续、负责任的机器学习工作流程，并开发出能够产生真正价值的机器学习应用，我们需要能够衡量我们的机器学习模型表现的好坏。我们还需要确保我们的机器学习模型能够泛化到它们在生产中可能会看到的数据。如果我们不这样做，我们基本上就是在黑暗中射击。我们将无法理解我们模型预期的行为，并且我们无法随着时间的推移来改进它们。
- en: The process of measuring how a model is performing (with respect to certain
    data) is called **evaluation**. The process of ensuring that our model generalizes
    to data that we might expect to encounter is called **validation**. Both processes
    need to be present in every machine learning workflow and application, and we
    will cover both in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 测量模型表现（相对于某些数据）的过程称为**评估**。确保我们的模型泛化到我们可能预期遇到的数据的过程称为**验证**。这两个过程都需要在每个机器学习工作流程和应用中存在，我们将在本章中介绍这两个过程。
- en: Evaluation
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: A basic tenet of science is measurement, and the science of machine learning
    is not an exception. We need to be able to measure, or evaluate, how well our
    models are performing, so we can continue to improve on them, compare one model
    to another, and detect when our models are behaving poorly.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 科学的一个基本原则是测量，机器学习的科学也不例外。我们需要能够衡量或评估我们的模型表现如何，这样我们才能继续改进它们，比较一个模型与另一个模型，并检测我们的模型何时表现不佳。
- en: There's only one problem. How do we evaluate how our models are performing?
    Should we measure how fast they can be trained or make inferences? Should we measure
    how many times they get the right answer? How do we know what the right answer
    is? Should we measure how far we deviated from the observed values? How do we
    measure that distance?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个问题。我们如何评估我们的模型表现如何？我们应该衡量它们训练或推理的速度有多快？我们应该衡量它们正确回答的次数有多少？我们如何知道正确答案是什么？我们应该衡量我们偏离观察值的程度有多大？我们如何衡量这个距离？
- en: As you can see, there are a lot of decisions to make around how we evaluate
    our models. What really matters is the context. In some cases, efficiency definitely
    matters, but every machine learning context requires us to measure how our predictions,
    inferences, or results match the ideal predictions, inferences, or results. Thus,
    measuring this comparison between computed results and ideal results should always
    take priority over speed optimizations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们在如何评估我们的模型方面有很多决定要做。真正重要的是上下文。在某些情况下，效率确实很重要，但每个机器学习上下文都要求我们衡量我们的预测、推理或结果与理想的预测、推理或结果之间的匹配程度。因此，测量计算结果与理想结果之间的比较应该始终优先于速度优化。
- en: 'Generally, there are some types of results that we will need to evaluate:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有一些结果类型是我们需要评估的：
- en: '**Continuous**: Results such as total sales, stock price, and temperature that
    can take any continuous numerical value ($12102.21, 92 degrees, and so on)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续**：结果如总销售额、股价和温度等，可以取任何连续数值（$12102.21、92度等）'
- en: '**Categorical**: Results such as fraud/not fraud, activity, and name that can
    take one of a finite number of categories (fraud, standing, Frank, and so on)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：结果如欺诈/非欺诈、活动、名称等，可以属于有限数量的类别（欺诈、站立、弗兰克等）'
- en: Each of these types of results have corresponding evaluation metrics that will
    be covered here. However, remember that your choice of evaluation metric depends
    on what you are trying to achieve with your machine learning model. There is no
    one-size-fits-all metric, and in some cases, you may even need to create your
    own metric.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果类型中的每一种都有相应的评估指标，这里将进行介绍。然而，请记住，你选择的评估指标取决于你试图通过你的机器学习模型实现什么。没有一种适合所有情况的指标，在某些情况下，你可能甚至需要创建自己的指标。
- en: Continuous metrics
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续指标
- en: 'Let''s say that we have a model that is supposed to predict some continuous
    value, like a stock price. Suppose that we have accumulated some predicted values
    that we can compare to actual observed values:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个应该预测某些连续值的模型，比如股价。假设我们已经积累了一些可以与实际观察值进行比较的预测值：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, how do we measure the performance of this model? Well, the first step
    would be taking the difference between the observed and predicted values to get
    an `error`:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何衡量这个模型的性能呢？首先一步是计算观察值和预测值之间的差异以得到一个`error`：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The error gives us a general idea of *how far off we were* from the value that
    we were supposed to predict. However, it's not really feasible or practical to
    look at all the error values individually, especially when there is a lot of data.
    There could be a million or more of these error values. Thus, we need a way to
    understand the errors in aggregate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 误差给我们一个大致的概念，即我们离我们本应预测的值有多远。然而，实际上或实际地查看所有误差值是不切实际的，尤其是在有大量数据的情况下。可能会有数百万或更多的这些误差值。因此，我们需要一种方法来理解误差的总体情况。
- en: 'The **mean squared error** (**MSE**) and **mean absolute error** (**MAE**)
    provide us with a view on errors in aggregate:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）和**平均绝对误差**（**MAE**）为我们提供了对误差的总体视图：'
- en: MSE or **mean squared deviation** (**MSD**) is the average of the squares of
    all the errors
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MSE或**均方偏差**（**MSD**）是所有误差平方的平均值
- en: MAE is the average of the absolute values of all the errors
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAE是所有误差绝对值的平均值
- en: Both MSE and MAE give us a good overall picture of how good our predictions
    are, but they do have differences. As the MSE takes the squares of the errors,
    large error values (for example, corresponding to outliers) are emphasized more
    than in the MAE. In other words, MSE is more sensitive to outliers. MAE, on the
    other hand, maintains the same units as the variable that we are trying to predict,
    and is thus directly comparable to these values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MSE和MAE都给我们提供了一个关于我们的预测有多好的整体图景，但它们确实有一些区别。由于MSE取误差的平方，因此相对于MAE，大误差值（例如，对应于异常值）被强调得更多。换句话说，MSE对异常值更敏感。另一方面，MAE与我们要预测的变量的单位相同，因此可以直接与这些值进行比较。
- en: 'For this dataset, we can parse the observed and predicted values and calculate
    the MAE and MSE as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，我们可以解析观察到的和预测的值，并如下计算MAE和MSE：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For our example data, this results in the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据，这导致以下结果：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To judge if these are good values or not, we need to compare them to the values
    in our observed data. In particular, the MAE is `2.55` and the mean of our observed
    values is 14.0, so our MAE is about 20% of our mean value. Not very good, depending
    on the context.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了判断这些值是否良好，我们需要将它们与我们的观察数据中的值进行比较。特别是，MAE是`2.55`，我们观察值的平均值是14.0，因此我们的MAE大约是平均值的20%。根据上下文，这并不很好。
- en: Along with the MSE and MAE, you will likely see **R-squared** (also known as
    **R²** or **R2**), or the **coefficient of determination**, used as an evaluation
    metric for continuous variable models. R-squared also gives us a general idea
    about the deviations of our predictions, but the idea of R-squared is slightly
    different.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MSE和MAE之外，你可能会看到**R-squared**（也称为**R²**或**R2**），或**确定系数**，用作连续变量模型的评估指标。R-squared也给我们一个关于我们预测偏差的一般概念，但R-squared的想法略有不同。
- en: R-squared measures the proportion of the variance in the observed values that
    we capture in the predicted values. Remember that the values that we are trying
    to predict have some variability. For example, we might be trying to predict stock
    prices, interest rates, or disease progressions, which, by their very nature,
    aren't all the same. We are attempting to create a model that can predict this
    variability in the observed values, and the percentage of the variation that we
    capture is represented by R-squared.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: R-squared衡量的是观察值中我们捕捉到的预测值的方差比例。记住，我们试图预测的值有一些变异性。例如，我们可能试图预测股价、利率或疾病进展，它们本质上并不完全相同。我们试图创建一个可以预测观察值中这种变异性的模型，而我们捕捉到的变异百分比由R-squared表示。
- en: 'Conveniently, `gonum.org/v1/gonum/stat` has a built-in function to calculate
    R-squared:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 便利的是，`gonum.org/v1/gonum/stat`有一个内置函数来计算R-squared：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Running the preceding code for our example dataset results in the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例数据集上运行前面的代码会产生以下结果：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So, is this a good or bad R-squared? Remember that R-squared is a percentage
    and higher percentages are better. Here, we are capturing about 37% of the variance
    in the variable that we are trying to predict. Not very good.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这是一个好的还是坏的R-squared？记住，R-squared是一个百分比，百分比越高越好。在这里，我们捕捉到了我们试图预测的变量中大约37%的方差。并不很好。
- en: Categorical metrics
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类度量
- en: 'Let''s say that we have a model that is supposed to predict some discrete value,
    such as fraud/not fraud, standing/sitting/walking, approved/not approved, and
    so on. Our data might look something like the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个模型，该模型应该预测某些离散值，例如欺诈/非欺诈、站立/坐着/行走、批准/未批准等等。我们的数据可能看起来像以下这样：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The observed values could take any one of a finite number of values (in this
    case 1, 2, or 3). Each of these values represents one of the discrete categories
    in our data (class 1 might correspond to a fraudulent transaction, class 2 might
    correspond to a transaction that is not fraudulent, and class 3 might correspond
    to an invalid transaction, for example). The predicted values could also take
    one of these discrete values. In evaluating our predictions, we want to somehow
    measure how right we were in making those discrete predictions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 观察值可以取有限数量中的任何一个值（在这种情况下是1、2或3）。这些值中的每一个代表我们数据中的一个离散类别（类别1可能对应欺诈交易，类别2可能对应非欺诈交易，类别3可能对应无效交易，例如）。预测值也可以取这些离散值之一。在评估我们的预测时，我们希望以某种方式衡量我们在做出这些离散预测时的正确性。
- en: Individual evaluation metrics for categorical variables
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类别变量的个体评估指标
- en: Actually, there are a huge number of ways to evaluate discrete predictions with
    metrics, including accuracy, precision, recall, specificity, sensitivity, fallout,
    false omission rate, and many more. As with continuous variables, there is no
    one-size-fits-all metric for evaluation. Each time you approach a problem, you
    need to determine the metric that fits the problem and matches the goals of the
    project. You don't want to optimize for the wrong things and then waste a bunch
    of time reimplementing your model based on other metrics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有大量方法可以用指标来评估离散预测，包括准确率、精确度、召回率、特异性、灵敏度、漏报率、假遗漏率等等。与连续变量一样，没有一种适合所有情况的评估指标。每次你面对一个问题时，你需要确定适合该问题的指标，并符合项目的目标。你不想优化错误的事情，然后浪费大量时间根据其他指标重新实现你的模型。
- en: 'To understand these metrics and determine which is appropriate for our use
    case, we need to realize that there are a number of different scenarios that could
    occur when we are making discrete predictions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些指标并确定哪个适合我们的用例，我们需要意识到，当我们进行离散预测时可能会发生多种不同的场景：
- en: '**True Positive** (**TP**): We predicted a certain category, and the observation
    was actually that category (for example, we predicted fraud and the observation
    was fraud)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**（**TP**）：我们预测了某个特定类别，而观察到的确实是那个类别（例如，我们预测欺诈，而观察到的确实是欺诈）'
- en: '**False Positive** (**FP**): We predicted a certain category, but the observation
    was actually another category (for example, we predicted fraud but the observation
    was not fraud)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**（**FP**）：我们预测了某个特定类别，但观察到的实际上是另一个类别（例如，我们预测欺诈，但观察到的不是欺诈）'
- en: '**True Negative** (**TN**): We predicted that the observation wasn''t a certain
    category, and the observation was not that category (for example, we predicted
    not fraud and the observation was not fraud)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**（**TN**）：我们预测观察到的不是某个特定类别，而观察到的确实不是那个类别（例如，我们预测不是欺诈，而观察到的确实不是欺诈）'
- en: '**False Negative** (**FN**): We predicted that the observation wasn''t a certain
    category, but the observation was actually that category (for example, we predicted
    not fraud but the observation was fraud)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）：我们预测观察到的不是某个特定类别，但实际上确实是那个类别（例如，我们预测不是欺诈，但观察到的确实是欺诈）'
- en: 'You can see that there are a number of ways we can combine, aggregate, and
    measure these scenarios. In fact, we could even aggregate/measure them in some
    sort of unique way related to our specific problem. However, there are some pretty
    standard ways of aggregating and measuring these scenarios that result in the
    following common metrics:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们有多种方式可以组合、汇总和衡量这些场景。实际上，我们甚至可以根据我们特定的问题以某种独特的方式汇总/衡量它们。然而，有一些相当标准的汇总和衡量这些场景的方法，结果产生了以下常见的指标：
- en: '**Accuracy**: The percentage of predictions that were right, or *(TP + TN)/(TP
    + TN + FP + FN)*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：预测正确的百分比，或 *(TP + TN)/(TP + TN + FP + FN)*'
- en: '**Precision**: The percentage of positive predictions that were actually positive,
    or *TP/(TP + FP)*'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：实际为正的预测的百分比，或 *TP/(TP + FP)*'
- en: '**Recall**: The percentage of positive predictions that were identified as
    positive, or *TP/(TP + FN)*'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：被识别为正的预测的百分比，或 *TP/(TP + FN)*'
- en: Even though I'm going to emphasize these here, you should take a look at other
    common metrics and their implications. A good overview can be found at [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我将在这里强调这些，但你应该看看其他常见的指标及其含义。一个很好的概述可以在[https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)找到。
- en: 'The following is an example that parses our data and calculates accuracy. First,
    we read in our `labeled.csv` file, create a CSV reader, and initialize two slices
    that will hold our parsed observed/predicted values:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个解析我们的数据并计算准确率的示例。首先，我们读取`labeled.csv`文件，创建一个CSV读取器，并初始化两个切片，将保存我们的解析观察值/预测值：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we will iterate over the records in the CSV parsing the values, and we
    will compare the observed and predicted values to calculate accuracy:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将遍历CSV中的记录，解析值，并将观察值和预测值进行比较以计算准确率：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Running this results in the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会产生以下结果：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 97%! That's pretty good. That means we were right 97% of the time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 97%！这相当不错。这意味着我们97%的时候是正确的。
- en: 'We can similarly calculate precision and recall. However, you may have noticed
    that there are a couple of ways we can do this when we have more than two categories
    or classes. We could consider class 1 as positive and the other classes as negative,
    class 2 as positive and the other classes as negative, and so on. That is, we
    could calculate a precision or recall for each of our classes, as shown in the
    following code sample:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似地计算精确度和召回率。然而，你可能已经注意到，当我们有超过两个类别或类时，我们可以用几种方式来做这件事。我们可以将类别1视为正类，其他类别视为负类，将类别2视为正类，其他类别视为负类，依此类推。也就是说，我们可以为我们的每个类别计算一个精确度或召回率，如下面的代码示例所示：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running this code results in the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码会产生以下结果：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice that the precision and recall are slightly different metrics and have
    different implications. If we wanted to get an overall precision or recall, we
    could average the per-class precisions and recalls. In fact, if certain classes
    were more important than other classes, we could take a weighted average of these
    and use that as our evaluation metric.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，精确度和召回率是稍微不同的指标，有不同的含义。如果我们想得到一个整体的精确度或召回率，我们可以平均每个类别的精确度和召回率。事实上，如果某些类别比其他类别更重要，我们可以对这些结果进行加权平均，并将其用作我们的评估指标。
- en: You can see that a couple of the metrics are 100%. This seems good, but it might
    actually indicate a problem, as we will further discuss.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，有几个指标是100%。这看起来很好，但实际上可能表明了一个问题，我们将在后面进一步讨论。
- en: In some cases, such as finance and banking, false positives or other cases may
    be very costly for certain classes. For example, a mislabeling a transaction as
    fraudulent might result in significant losses. On the other hand, certain results
    for other classes might be negligible. These scenario might warrant the use of
    a custom metric or cost function that weights certain classes, certain results,
    or certain combinations of results as more important than others.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，例如金融和银行，假阳性或其他情况对于某些类别可能是非常昂贵的。例如，将交易错误标记为欺诈可能会造成重大损失。另一方面，其他类别的某些结果可能可以忽略不计。这些场景可能需要使用自定义指标或成本函数，该函数将某些类别、某些结果或某些结果的组合视为比其他结果更重要。
- en: Confusion matrices, AUC, and ROC
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵、AUC和ROC
- en: In addition to calculating individual numerical metrics for our models, there
    are a variety of techniques to combine various metrics into a form that gives
    you a more complete representation of model performance. These include, but are
    certainly not limited to, **confusion matrices** and **area under the curve**
    (**AUC**)/**Receiver Operating Characteristic** (**ROC**) **curves**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为我们的模型计算单个数值指标外，还有各种技术可以将各种指标组合成一种形式，为你提供一个更完整的模型性能表示。这包括但不限于**混淆矩阵**和**曲线下面积**（**AUC**）/**接收者操作特征**（**ROC**）曲线。
- en: 'Confusion matrices allow us to visualize the various **TP**, **TN**, **FP**,
    and **FN** values that we predict in a two-dimensional format. A confusion matrix
    has rows corresponding to the categories that you were supposed to predict, and
    columns corresponding to categories that were predicted. Then, the value of each
    element is the corresponding count:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵允许我们以二维格式可视化我们预测的各种**TP**、**TN**、**FP**和**FN**值。混淆矩阵的行对应于你应该预测的类别，列对应于预测的类别。然后，每个元素的值是对应的计数：
- en: '![](img/56626b65-b9f3-4207-8ddf-f69de05aa45f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/56626b65-b9f3-4207-8ddf-f69de05aa45f.png)'
- en: As you can see, the ideal situation is that your confusion matrix only has entries
    on the diagonal (**TP**, **TN**). The diagonal elements represent predicting a
    certain category and the observation actually being in that category. The off-diagonal
    elements include counts for predictions that were incorrect.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，理想的情况是混淆矩阵只在对角线上有值（**TP**，**TN**）。对角线元素表示预测某个类别，而观察结果实际上就在那个类别中。非对角线元素包括预测错误的计数。
- en: This type of confusion matrix can be especially useful for problems that have
    more than two categories. For example, you may be trying to predict various activities
    based on mobile accelerator and position data. These activities may include more
    than two categories, such as standing, sitting, running, driving, and so on. The
    confusion matrix for this problem with be larger than 2 x 2 and would allow you
    to quickly gauge the overall performance of your model on all categories, and
    identify categories in which your model is performing poorly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的混淆矩阵对于具有超过两个类别的实际问题特别有用。例如，你可能正在尝试根据移动加速器和位置数据预测各种活动。这些活动可能包括超过两个类别，如站立、坐着、跑步、驾驶等。这个问题的混淆矩阵将大于2
    x 2，这将使你能够快速评估模型在所有类别上的整体性能，并识别模型表现不佳的类别。
- en: In addition to confusion matrices, ROC curves are commonly used to get an overall
    picture of the performance of binary classifiers (or models that are trained to
    predict one of two categories). ROC curves plot the recall versus false positive
    rate (*FP/(FP + TN)*) for every possible classification threshold.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了混淆矩阵外，ROC曲线通常用于获得二元分类器（或训练用于预测两个类别之一的模型）的整体性能图。ROC曲线绘制了每个可能的分类阈值下的召回率与假阳性率（*FP/(FP
    + TN)*）。
- en: 'The thresholds used in an ROC curve represent various boundaries or rankings
    in which you are separating the two categories of your classification. That is,
    the model that is evaluated by the ROC curve must make a prediction for the two
    classes based on probability, ranking, or score (referred to as a score in the
    following image). In every example mentioned earlier, a score is classified one
    way, and vise versa:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线中使用的阈值代表你在分类的两个类别之间分离的各种边界或排名。也就是说，由ROC曲线评估的模型必须基于概率、排名或分数（在下图中称为分数）对两个类别进行预测。在前面提到的每个例子中，分数以一种方式分类，反之亦然：
- en: '![](img/d9847ec0-35b3-40fb-b4bf-f58bc3ba01ee.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d9847ec0-35b3-40fb-b4bf-f58bc3ba01ee.png)'
- en: 'To generate an ROC curve, we plot a point for each score or rank in our testing
    examples (recall, false positive rate). We can then connect these to form a curve.
    In many cases, you will see a straight line plotted down the diagonal of the ROC
    curve plot. This straight line is a reference line for a classifier, with approximately
    random predictive power:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成ROC曲线，我们为测试示例中的每个分数或排名绘制一个点（召回率、假阳性率）。然后我们可以将这些点连接起来形成曲线。在许多情况下，你会在ROC曲线图的对角线上看到一条直线。这条直线是分类器的参考线，具有大约随机的预测能力：
- en: '![](img/1d839efe-add3-4dc7-ab73-e408fc5a7523.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1d839efe-add3-4dc7-ab73-e408fc5a7523.png)'
- en: 'A good ROC curve is one that is in the upper left section of the plot, which
    means that our model has better than random predictive power. The more that the
    ROC curve hugs the upper left hand side of the plot, the better. This means that
    good ROC curves have more AUC; AUC for ROC curves is also used as an evaluation
    metric. Refer to the following figure:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的ROC曲线是位于图表右上方的曲线，这意味着我们的模型具有比随机预测能力更好的预测能力。ROC曲线越靠近图表的右上角，越好。这意味着好的ROC曲线具有更高的AUC；ROC曲线的AUC也用作评估指标。参见图：
- en: '![](img/72073033-e4d7-40f2-96f2-ba2453df4935.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/72073033-e4d7-40f2-96f2-ba2453df4935.png)'
- en: '`gonum.org/v1/gonum/stat` has some built-in functions and types that help you
    build ROC curves and AUC metrics:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`gonum.org/v1/gonum/stat`提供了一些内置函数和类型，可以帮助你构建ROC曲线和AUC指标：'
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is a quick example that calculates the AUC for an ROC curve with gonum:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用gonum快速计算ROC曲线AUC的示例：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running this code results in the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将产生以下结果：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Validation
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证
- en: So, now we know some ways to measure how well our model is performing. In fact,
    if we wanted to, we could create a super sophisticated, complicated model that
    could predict every observation without error. For example, we could create a
    model that would take the index of the row of the observation and return the exact
    answer for each of those rows. It might be a really big function with a lot of
    parameters, but it would return the correct answers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道了一些衡量我们的模型表现如何的方法。实际上，如果我们想的话，我们可以创建一个非常复杂、精确的模型，可以无误差地预测每一个观测值。例如，我们可以创建一个模型，它会取观测值的行索引，并为每一行返回精确的答案。这可能是一个具有很多参数的非常大的函数，但它会返回正确的答案。
- en: So, what's the problem with this? Well, the problem is that it would not generalize
    to new data. Our complicated model would predict really well for the data that
    we would expose it to, but once we try some new input data (that isn't part of
    our training dataset), the model would likely perform poorly.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这有什么问题呢？问题是，它不会泛化到新数据。我们复杂的模型在我们向其展示的数据上会预测得很好，但一旦我们尝试一些新的输入数据（这些数据不是我们的训练数据集的一部分），模型很可能会表现不佳。
- en: We call this type of model (that doesn't generalize) a model that has been **overfit**.
    That is, our process of making the model more and more complicated based on the
    data that was available to us was overfitting the model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们把这种（不能泛化）的模型称为**过拟合**的模型。也就是说，我们基于我们所拥有的数据，使模型越来越复杂的过程，是对模型进行了过拟合。
- en: 'Overfitting can happen when predicting continuous values or discrete/categorical
    values:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合可能在预测连续值或离散/分类值时发生：
- en: '![](img/b61c8e32-cce8-4f5f-bc80-632475a1f966.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b61c8e32-cce8-4f5f-bc80-632475a1f966.png)'
- en: To prevent overfitting, we need to validate our model. There are multiple ways
    to perform validation, and we will cover a couple of these here.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，我们需要验证我们的模型。有多种方式进行验证，我们在这里将介绍其中的一些。
- en: Every time you are productionizing a model, you need to ensure that you have
    validated your model and understand how it will generalize to new data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你将模型投入生产时，你需要确保你已经验证了你的模型，并了解它如何泛化到新数据。
- en: Training and test sets
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集和测试集
- en: The first method to help prevent overfitting is to train or fit your model on
    a portion of your dataset and then test or evaluate your model on a different
    portion of your dataset. Training your model generally consists of parameterizing
    one or more functions that make up your model, such that the functions that predict
    what you are trying to predict. Then, you can evaluate this trained model using
    one or more of the evaluation metrics that we discussed previously. The important
    thing here is that you do not want to test/evaluate your model on the same data
    that is used to train your model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过拟合的第一种方法是使用数据集的一部分来训练或拟合你的模型，然后在数据集的另一部分上测试或评估你的模型。训练模型通常包括参数化一个或多个组成你的模型的功能，使得这些功能可以预测你想要预测的内容。然后，你可以使用我们之前讨论的评估指标之一或多个来评估这个训练好的模型。这里重要的是，你不想在用于训练模型的数据上测试/评估你的模型。
- en: By reserving part of your data for testing, you are simulating the scenario
    in which your model sees new data. That is, the model is making predictions based
    on data that was not used in parameterizing the model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保留部分数据用于测试，你是在模拟模型看到新数据的情况。也就是说，模型是基于未用于参数化模型的数据进行预测。
- en: '![](img/d041cb25-7cdc-4f3d-b862-81f27c597e7e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d041cb25-7cdc-4f3d-b862-81f27c597e7e.png)'
- en: Many people start by splitting 80% of their data into a training data set and
    20% into a test set (an 80/20 split). However, you will see different people splitting
    their datasets up in different proportions. The proportion of test to training
    data depends a little bit on the type and amount of data that you have and the
    model that you are trying to train. Generally, you want to ensure that both your
    training data and test data are a fairly accurate representation of your data
    on a large scale.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人开始时将80%的数据分成训练数据集，20%分成测试集（80/20的分割）。然而，你会看到不同的人以不同的比例分割他们的数据集。测试数据与训练数据的比例取决于你拥有的数据类型和数量以及你试图训练的模型。一般来说，你想要确保你的训练数据和测试数据都能相当准确地代表你在大规模上的数据。
- en: For example, if you are trying to predict one of a few different categories,
    A, B, and C, you wouldn't want your training data to include observations that
    only correspond to A and B. A model trained on such a dataset would likely only
    be able to predict the A and B categories. Likewise, you wouldn't want your test
    set to include some subset of the categories, or artificially weighted proportions
    of the categories. This could very easily happen, depending on how your data was
    generated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你试图预测几个不同类别中的一个，比如A、B和C，你不想你的训练数据只包含与A和B相对应的观察结果。在这样一个数据集上训练的模型可能只能预测A和B类别。同样，你也不想你的测试集包含某些类别的子集，或者类别的加权比例是人为的。这很容易发生，具体取决于你的数据是如何生成的。
- en: In addition, you want to make sure that you have enough training data to reduce
    the variability in your determined parameters as they are computed over and over.
    If you have too few training data points, or poorly sampled training data points,
    your model training may produce parameters with a lot of variability, or it may
    not even be able to converge numerically. These are indications that your model
    lacks predictive power.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你需要确保你有足够的训练数据，以减少在反复计算过程中确定的参数的变异性。如果你有太多的训练数据点，或者训练数据点采样不佳，你的模型训练可能会产生具有很多变异性的参数，甚至可能无法进行数值收敛。这些都是表明你的模型缺乏预测能力的迹象。
- en: Typically, as you increase the complexity of your model, you will be able to
    improve the evaluation metric that you are using for your training data, but at
    some point, the evaluation metric will start getting worse for your test data.
    When the evaluation metric starts to get worse for your test data, you are starting
    to overfit your model. The ideal scenario is when you are able to increase your
    model complexity up to the inflection point, where the test evaluation metric
    starts to degrade. Another way of putting this (which fits very well into our
    general philosophy for model building in this book) is that we want the most interpretable
    model (or simplistic model) that can produce valuable results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，随着你增加模型的复杂性，你将能够提高你用于训练数据的评估指标，但到了某个点，评估指标将开始对你的测试数据变差。当评估指标开始对测试数据变差时，你开始过度拟合你的模型。理想的情况是，你能够将模型复杂性增加到拐点，此时测试评估指标开始下降。另一种说法（这与本书中关于模型构建的一般哲学非常契合）是，我们希望得到最可解释的模型（或最简模型），它能产生有价值的结果。
- en: '![](img/82f10fbd-6a15-46dc-bcff-829c780a6f2c.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/82f10fbd-6a15-46dc-bcff-829c780a6f2c.png)'
- en: 'One way to quickly split a dataset into training and test sets is with `github.com/kniren/gota/dataframe`.
    Let''s demonstrate this using a dataset, which includes a bunch of anonymized
    information about medical patients and a corresponding indication of the progression
    of disease and diabetes:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 快速将数据集分割成训练集和测试集的一种方法就是使用`github.com/kniren/gota/dataframe`。让我们用一个包括大量匿名医疗患者信息和相应疾病进展及糖尿病指示的数据集来演示这一点：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can retrieve this data set here: [https://archive.ics.uci.edu/ml/datasets/diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里检索这个数据集：[https://archive.ics.uci.edu/ml/datasets/diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes)。
- en: 'To split this data with `github.com/kniren/gota/dataframe`, we can do the following
    (where we save the training and test splits to respective CSV files):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`github.com/kniren/gota/dataframe`来分割这些数据，我们可以这样做（我们将训练和测试分割保存到相应的CSV文件中）：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running this results in the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作会产生以下结果：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Holdout set
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留集
- en: 'We are making progress to ensure that our models generalize using training
    and test sets. However, imagine the following scenario:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在努力确保我们的模型使用训练集和测试集进行泛化。然而，想象以下场景：
- en: We develop a first version of our model based on our training set.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据我们的训练集开发我们模型的第一版。
- en: We test this first version of our model on the test set.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在测试集上测试我们模型的第一版。
- en: We aren't satisfied with the result on the test set, so we loop back to step
    1 and repeat.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对测试集上的结果不满意，所以我们会回到步骤1并重复。
- en: This process might seem logical, but you are probably already seeing a problem
    that can result from this procedure. We can actually overfit our model on the
    test data by iteratively exposing the model to our test set.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能看起来合乎逻辑，但你可能已经看到了由此程序可能产生的问题。实际上，我们可以通过迭代地将模型暴露于测试集来过度拟合我们的模型。
- en: There are a couple of ways to deal with this extra level of overfitting. The
    first is by simply creating another split of our data called a **holdout set**
    (also known as a **validation set**). So, now we would have a training set, test
    set, and holdout set. This is sometimes called the three dataset validation, for
    obvious reasons.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以处理这种额外的过拟合级别。第一种是简单地创建我们数据的另一个分割，称为**保留集**（也称为**验证集**）。因此，现在我们将有一个训练集、测试集和保留集。这有时被称为三数据集验证，原因很明显。
- en: '![](img/9421ffa1-cc2d-4dbf-911a-8a474792dc25.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9421ffa1-cc2d-4dbf-911a-8a474792dc25.png)'
- en: Keep in mind that, to truly get an idea about the general performance of your
    model, your holdout set must never be used in training and testing. You should
    reserve this dataset for validation after you have gone through the process of
    training your model, making adjustments to the model, and getting an acceptable
    performance on the test dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，为了真正了解你模型的泛化性能，你的保留集绝不能用于训练和测试。你应该在你完成模型的训练、调整模型并得到测试数据集的可接受性能后，将此数据集保留用于验证。
- en: You might be wondering how you can manage this splitting of data over time and
    recover different sets of data used to train or test certain models. This "provenance"
    of data is crucial when trying to maintain integrity in your machine learning
    workflows. This is also exactly what Pachyderm's data versioning (introduced in
    Chapter 1, *Gathering and Organizing Data*) was created to handle. We will see
    exactly how this plays out at scale later in Chapter 9, *Deploying and distributing
    Analyses and Models*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道如何管理随时间推移的数据分割，并恢复用于训练或测试某些模型的不同的数据集。这种“数据来源”对于在机器学习工作流程中保持完整性至关重要。这正是Pachyderm的数据版本控制（在第1章“收集和组织数据”中介绍）被创建来处理的。我们将在第9章“部署和分发分析和模型”中看到这一过程如何在规模上展开。
- en: Cross validation
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: In addition to reserving a holdout set for validation, cross validation is a
    common technique to validate the generality of a model. In cross validation, or
    k-fold cross validation, you actually perform *k* random splits of your dataset
    into different training and test combinations. Think of these as *k* experiments.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为验证保留一个保留集之外，交叉验证是验证模型泛化能力的一种常见技术。在交叉验证中，或者说是k折交叉验证中，你实际上是将你的数据集随机分成不同的训练和测试组合的*k*次。将这些看作*k*次实验。
- en: 'Once you have performed each split, you train your model on the training data
    for that split, and then evaluate it on the test data for that split. This process
    results in an evaluation metric result for each random split of your data. You
    can then average these evaluation metrics to get an overall evaluation metric
    that is a more general representation of model performance than any one of the
    individual evaluation metrics by themselves. You can also look at the variance
    in the evaluation metrics to get an idea about the stability of your various experiments.
    This process is illustrated in the following image:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成每个分割后，你将在该分割的训练数据上训练你的模型，然后在该分割的测试数据上评估它。这个过程为你的数据每个随机分割产生一个评估指标结果。然后你可以对这些评估指标进行平均，得到一个整体评估指标，它比任何单个评估指标本身更能代表模型性能。你还可以查看评估指标的变化，以了解你各种实验的稳定性。这个过程在以下图像中得到了说明：
- en: '![](img/6c333632-adaf-4adc-b750-2693dc863aa9.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c333632-adaf-4adc-b750-2693dc863aa9.png)'
- en: 'Some advantages of using cross validation, in comparison to dataset validation,
    are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据集验证相比，使用交叉验证的一些优点如下：
- en: You are making use of your entire dataset, and thus, are actually exposing your
    model to more training examples and more testing examples
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在使用你的整个数据集，因此实际上是在让你的模型接触到更多的训练示例和更多的测试示例。
- en: There are some convenience functions and packaging already written for cross
    validation
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经有一些方便的函数和打包用于交叉验证。
- en: It helps prevent the biases that may result from choosing a single validation
    set
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有助于防止由于选择单个验证集而可能产生的偏差。
- en: The `github.com/sjwhitworth/golearn` is one Go package that provides some convenience
    functions for cross validation. Actually, `github.com/sjwhitworth/golearn` includes
    a bunch of machine learning functionality that we will cover later on in the book,
    but for now, let's just look at what functionality is available for cross validation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`github.com/sjwhitworth/golearn`是一个Go包，它提供了一些交叉验证的方便函数。实际上，`github.com/sjwhitworth/golearn`包含了一系列我们将在本书后面部分介绍的机器学习功能，但就目前而言，让我们看看交叉验证可用哪些功能。'
- en: 'If you look at the `github.com/sjwhitworth/golearn/evaluation` package Godocs,
    you will see the following function that is available for cross validation:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看 `github.com/sjwhitworth/golearn/evaluation` 包的 Godocs，你会看到以下可用于交叉验证的函数：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This function can actually be used with a variety of models, but here is an
    example using a decision tree model (don''t worry about the details of the model
    here):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数实际上可以与各种模型一起使用，但这里有一个使用决策树模型的例子（这里不需要担心模型的细节）：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: References
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Evaluation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：
- en: 'Essay on overfitting: [http://scott.fortmann-roe.com/docs/MeasuringError.html](http://scott.fortmann-roe.com/docs/MeasuringError.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合论文：[http://scott.fortmann-roe.com/docs/MeasuringError.html](http://scott.fortmann-roe.com/docs/MeasuringError.html)
- en: 'Essay on bias-variance trade-off: [http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡论文：[http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- en: 'Comparison of categorical evaluation metrics: [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类别评估指标的比较：[https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)
- en: '`gonum.org/v1/gonum/stat` docs: [https://godoc.org/gonum.org/v1/gonum/stat](https://godoc.org/gonum.org/v1/gonum/stat)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gonum.org/v1/gonum/stat` 文档：[https://godoc.org/gonum.org/v1/gonum/stat](https://godoc.org/gonum.org/v1/gonum/stat)'
- en: '`github.com/sjwhitworth/golearn/evaluation` docs: [https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/sjwhitworth/golearn/evaluation` 文档：[https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)'
- en: 'Validation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 验证：
- en: '`github.com/kniren/gota/dataframe` docs: [https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/kniren/gota/dataframe` 文档：[https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)'
- en: '`github.com/sjwhitworth/golearn/evaluation` docs: [https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`github.com/sjwhitworth/golearn/evaluation` 文档：[https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)'
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Choosing an appropriate evaluation metric and laying out a procedure for evaluation/validation
    are essential parts of any machine learning project. You have learned about a
    variety of relevant evaluation metrics and how to avoid overfitting using holdout
    sets and/or cross validation. In the next chapter, we will start looking at machine
    learning models and we will build our first model using linear regression!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的评估指标并制定评估/验证流程是任何机器学习项目的关键部分。你已经了解了各种相关的评估指标以及如何使用保留集和/或交叉验证来避免过拟合。在下一章中，我们将开始探讨机器学习模型，并使用线性回归来构建我们的第一个模型！
