- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Exploring Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Transformer
- en: '**Transformers** are a new type of machine learning model that has completely
    revolutionized the way human language is processed and understood. These models
    can analyze huge amounts of data, find and understand complex patterns with hitherto
    unmatched accuracy, and produce insights that would otherwise be impossible for
    humans to obtain on tasks such as translation, text summarization, and text generation.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer** 是一种全新的机器学习模型，它彻底改变了人类语言的处理和理解方式。这些模型可以分析大量数据，以前所未有的准确性发现和理解复杂模式，并在诸如翻译、文本摘要和文本生成等任务上产生人类难以获得的见解。'
- en: Transformers are powerful because they can handle large amounts of data, and
    learn from previous examples to make better predictions. They have totally “transformed”
    (pun intended) **NLP** and have outperformed traditional methods in many NLP tasks,
    quickly becoming state of the art.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 强大之处在于它们可以处理大量数据，并从先前示例中学习以做出更好的预测。它们已经“彻底改变”了（有意为之）**NLP**，并在许多
    NLP 任务中优于传统方法，迅速成为行业最佳实践。
- en: In this chapter, we will introduce transformers, discuss how they work, and
    look at some of their key components. We will then present Hugging Face and see
    how it helps in our task before introducing some useful existing transformer models.
    We’ll also show how we can use Hugging Face to implement two models using transformers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 Transformer，讨论它们的工作原理，并查看一些关键组件。然后，我们将介绍 Hugging Face，并展示它是如何帮助我们完成任务，然后再介绍一些有用的现有
    Transformer 模型。我们还将展示如何使用 Hugging Face 实现 two 模型，并使用 Transformer。
- en: This chapter will demonstrate how to build transformer models while taking you
    through important steps such as linking Google Colab to Google Drive so that files
    can be persisted, preparing the data, using auto classes, and ultimately building
    models that can be used for classification.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将演示如何构建 Transformer 模型，同时引导您完成重要步骤，例如将 Google Colab 连接到 Google Drive 以持久保存文件，准备数据，使用自动类，并最终构建可用于分类的模型。
- en: 'We will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Introduction to transformers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 简介
- en: How data flows through the transformer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据如何在 Transformer 中流动
- en: Hugging Face
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Existing models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有模型
- en: Transformers for classification
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类的 Transformer
- en: Implementing transformers
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 Transformer
- en: Let’s begin by having a closer look at transformers, who invented them, and
    how they work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先更深入地了解 Transformer，它们的发明者以及它们是如何工作的。
- en: Introduction to transformers
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 简介
- en: 'In this chapter, we will briefly introduce transformers. In language and NLP
    tasks, context plays a crucial role – that is, to know what a word means, knowledge
    about the situation (that is, the context) must also be taken into account. Before
    transformers came along, sequence-to-sequence models were used for many NLP tasks.
    These are models that generate an output sequence by predicting a single word
    at a time and encode the source text to gain knowledge about the context. However,
    the problem with languages is that they are complex, fluid, and difficult to turn
    into a rigid rule-based structure. The context itself is also hard to track as
    it is often found far away (that is, many words, sentences, or even paragraphs)
    from where it is required. To address this problem, sequence-to-sequence models
    work by using neural networks, which have some limited form of memory:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要介绍 Transformer。在语言和 NLP 任务中，上下文起着至关重要的作用——也就是说，要知道一个词的含义，必须考虑关于情况（即上下文）的知识。在
    Transformer 出现之前，序列到序列模型被用于许多 NLP 任务。这些模型通过一次预测一个单词来生成输出序列，并将源文本编码以获取上下文知识。然而，语言的问题在于它们是复杂、流动且难以转化为严格的基于规则的结构的。上下文本身也很难追踪，因为它通常位于所需位置（即许多单词、句子或甚至段落）很远的地方。为了解决这个问题，序列到序列模型通过使用具有某种有限形式的记忆的神经网络来工作：
- en: '![Figure 9.1 – Sequence-to-sequence model versus transformer](img/B18714_09_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 序列到序列模型与 Transformer 对比](img/B18714_09_01.jpg)'
- en: Figure 9.1 – Sequence-to-sequence model versus transformer
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 序列到序列模型与 Transformer 对比
- en: 'The ubiquitous paper on transformers, *Attention Is All You Need*, was published
    by Vaswani et al., in 2017\. They presented a new type of neural network architecture,
    known as **transformers**, which could be used for NLP tasks. Transformers have
    several components, as can be seen in *Figure 9**.2*, with an “encoder” (on the
    left), a “decoder” (on the right), and a block of attention and feed-forward components
    that repeat *N* times:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于变压器的普遍论文《Attention Is All You Need》由Vaswani等人于2017年发表。他们提出了一种新的神经网络架构，称为**变压器**，可用于NLP任务。从*图9*.2中可以看到，变压器有几个组件，包括“编码器”（在左侧），“解码器”（在右侧），以及重复*N*次的注意力和前馈组件块：
- en: '![Figure 9.2 – From the "Attention Is All You Need" paper by Vaswani et al.](img/B18714_09_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 来自Vaswani等人撰写的《Attention Is All You Need》论文](img/B18714_09_02.jpg)'
- en: Figure 9.2 – From the "Attention Is All You Need" paper by Vaswani et al.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 来自Vaswani等人撰写的《Attention Is All You Need》论文。
- en: The transformer consists of encoder and decoder layers; each usually has more
    than one identical instance (for example, six as in the original research paper),
    and each has its own set of weights. On the left-hand side, the encoder’s job
    is to convert a sequence of inputs into a set of continuous representations. On
    the right-hand side, the decoder uses the output from the encoder as well as the
    output from the previous time step to produce a sequence of outputs. The first
    encoder and decoder in each stack of the architecture has an embedding layer and
    positional encoding as inputs. Each encoder contains a self-attention layer that
    calculates the relationships between different words and a feed-forward layer.
    Each decoder also contains a feed-forward layer, but it has two self-attention
    layers. The output from the last encoder is used as the input to the first decoder.
    These components combine to make the transformer architecture faster and more
    efficient and allow it to handle much longer sequences, making the separation
    between words irrelevant. Consequently, the architecture can outperform other,
    more traditional, methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器由编码器和解码器层组成；每个通常有多个相同的实例（例如，原始研究论文中的六个），并且每个都有自己的权重集。在左侧，编码器的任务是转换输入序列为一组连续表示。在右侧，解码器使用编码器的输出以及前一时间步的输出来生成输出序列。架构中的每个堆叠的第一个编码器和解码器都有嵌入层和位置编码作为输入。每个编码器包含一个自注意力层，该层计算不同单词之间的关系，以及一个前馈层。每个解码器也包含一个前馈层，但它有两个自注意力层。最后一个编码器的输出被用作第一个解码器的输入。这些组件结合使变压器架构更快、更高效，并允许它处理更长的序列，使得单词之间的分隔变得无关紧要。因此，该架构可以优于其他更传统的其他方法。
- en: The transformer architecture described by Vaswani et al. was created with the
    goal of translation. The encoder is fed inputs (that is, sentences) in one language
    (for example, English) during training, while the decoder is fed the same inputs
    (that is, sentences) in the intended target language (for example, French). The
    attention layers in the encoder make use of each word in an input sentence, but
    the encoder operates sequentially and can only focus on the words in the translated
    text (that is, only the words before the word that is currently being generated).
    For example, if the first *N* words of the translated target have been predicted,
    these are input to the decoder, which uses all the inputs of the encoder to predict
    the word at *N+1*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani等人描述的变压器架构是为了翻译而创建的。在训练期间，编码器接收一种语言的输入（即句子，例如英语），而解码器接收相同输入（即句子）的预期目标语言（例如法语）。编码器中的注意力层利用输入句子中的每个单词，但编码器是顺序操作的，并且只能关注翻译文本中的单词（即当前正在生成的单词之前的单词）。例如，如果已经预测了翻译目标的前*N*个单词，这些单词将被输入到解码器中，解码器使用编码器的所有输入来预测*N+1*位置的单词。
- en: The decoder is supplied with the entire target sentence but is constrained from
    using forthcoming words. Consequently, when predicting a word, the decoder cannot
    refer to any words beyond it in the target sentence. For instance, while predicting
    the *Nth* word, only the words in positions *1* to *N-1* can be considered by
    the attention layer. This constraint is crucial to guarantee that the task remains
    suitably demanding for the model to acquire knowledge competently.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器提供了整个目标句子，但被限制不能使用即将到来的单词。因此，在预测一个单词时，解码器不能参考目标句子中它之后的任何单词。例如，在预测*N*th个单词时，只有位置*1*到*N-1*的单词可以被注意力层考虑。这个限制对于确保任务对模型来说足够具有挑战性以获得知识至关重要。
- en: How data flows through the transformer model
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器模型中的数据流动
- en: In this section, we will take a closer look at how data flows through the transformer
    model. Understanding how data flows through a transformer, and the steps that
    transform raw input into meaningful output, is crucial to understanding its power
    and potential. The transformer enables efficient and effective modeling of long-range
    dependencies in data, making it highly capable of capturing context and semantics.
    By exploring these inner mechanisms of data flow within the transformer, we will
    gain a deeper understanding of its ability to process and understand language.
    We will look at input embeddings first.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更深入地探讨数据是如何在变换器模型中流动的。理解数据在变换器中的流动方式，以及将原始输入转换为有意义的输出的步骤，对于理解其功能和潜力至关重要。变换器能够高效且有效地对数据中的长距离依赖关系进行建模，使其能够高度捕捉上下文和语义。通过探索变换器内部的数据流动机制，我们将更深入地理解其处理和理解语言的能力。我们首先将查看输入嵌入。
- en: Input embeddings
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: Starting on the left-hand side, the inputs into the encoder are word tokens
    from the source text. This textual data has to be converted into a numeric representation
    (of size 512 according to the authors) using methods such as GloVe or Word2Vec,
    among others.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从左侧开始，编码器的输入是源文本中的单词标记。这种文本数据必须使用GloVe或Word2Vec等方法（以及其他方法）转换为数值表示（根据作者的说法，大小为512），以便进行转换。
- en: Positional encoding
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: A positional element is then added to these embeddings. This is important as
    it allows the transformer to discover information about the distances between
    words and the order of the words. This information is then passed to the self-attention
    layer of the first encoder block.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将一个位置元素添加到这些嵌入中。这很重要，因为它允许变换器发现关于单词之间距离和单词顺序的信息。然后，这些信息被传递到第一个编码器块的自注意力层。
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Positional encodings do not alter the vector dimensions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码不会改变向量维度。
- en: Encoders
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: 'Each encoder has several sub-layers within it:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个编码器内部都有几个子层：
- en: '**Multi-headed attention**: This allows the transformer to attend to different
    parts of the input sequence at the same time, thus improving its input processing
    abilities, and allowing it to obtain more context and make much better-informed
    decisions. It is the most important part of the architecture and is also the most
    computationally expensive. When working on words in the input, self-attention
    relates every word in the input to every other word. It is interesting to consider
    how the transformer decides which set of weights will yield the best results.
    The aim is that the attention value should be large for words that are somehow
    related in a sentence, and vice versa. For example, let’s look at the sentence
    *The weather was* *very sunny.*'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力**：这允许变换器同时关注输入序列的不同部分，从而提高其输入处理能力，并使其能够获得更多上下文并做出更明智的决策。这是架构中最重要的部分，也是计算成本最高的部分。当处理输入中的单词时，自注意力将输入中的每个单词与每个其他单词相关联。考虑变换器如何决定哪组权重将产生最佳结果是非常有趣的。目标是使与句子中某些方式相关的单词的注意力值很大，反之亦然。例如，让我们看看这个句子：“*天气非常晴朗*。”'
- en: 'The words *weather* and *sunny* are related and hence should generate a high
    attention value; conversely, the attention value for *The* and *was* should be
    small. As described earlier, transformers are trained on embeddings. Consequently,
    the transformer will learn from them and be able to produce the vectors required
    so that words produce attention values that correspond to the relatedness of the
    words. Furthermore, instead of just considering individual meanings, the self-attention
    mechanism weighs the input words differently according to their importance and
    their relationships with other words. This allows it to be able to handle the
    aforementioned long-distance context problems and hence achieve much better performance
    on NLP tasks. Briefly, the attention value is computed using three matrices, with
    each row in each matrix representing an input word. It is important to note that
    the values in these rows are learned by the model so that the desired outputs
    are generated. Let’s take a look at each of these important matrices in turn:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 单词*weather*和*sunny*是相关的，因此应该生成较高的关注值；相反，*The*和*was*的关注值应该较小。如前所述，变压器是在嵌入上进行训练的。因此，变压器将从它们中学习，并能够产生所需的向量，以便单词产生与单词相关性的关注值。此外，除了考虑单个含义之外，自注意力机制根据单词的重要性及其与其他单词的关系对输入单词进行不同的加权。这使得它能够处理上述长距离上下文问题，从而在NLP任务上实现更好的性能。简而言之，关注值是使用三个矩阵计算的，每个矩阵中的每一行代表一个输入单词。重要的是要注意，这些行中的值是由模型学习的，以便生成所需的输出。让我们依次查看这些重要的矩阵：
- en: '**Query**: Each row corresponds to the embeddings of an input word. In other
    words, the query word is the specific word for which an attention value is being
    calculated.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**：每一行对应于输入单词的嵌入。换句话说，查询词是正在计算关注值的特定单词。'
- en: '**Key**: Each word in the input text that the model is comparing the query
    to – that is, the word that is being paid attention to – to calculate its importance
    to the query word.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键**：模型正在将其与查询比较的输入文本中的每个单词——即被关注的单词——以计算其对查询词的重要性。'
- en: '**Value**: The information that the model is trying to generate based on the
    comparison between the query and key matrices. The key and value matrices can
    be the same.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值**：模型试图根据查询和键矩阵之间的比较来生成的信息。键和值矩阵可以是相同的。'
- en: Given these matrices, the attention value is obtained by calculating the dot
    product of the query and key matrices. These are then used to weigh the value
    matrix, hence effectively allowing the model to “learn” which words in the input
    text should be focused upon.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些矩阵，通过计算查询和键矩阵的点积来获得关注值。然后使用这些值来加权值矩阵，从而有效地允许模型“学习”输入文本中应该关注的单词。
- en: '**Add and norm**: These layers comprise a residual connection layer followed
    by a normalization layer. For our purposes, it is enough to know that they help
    address the vanishing gradient problem and improve the model’s performance.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加和归一化**：这些层由一个残差连接层后面跟一个归一化层组成。就我们的目的而言，重要的是要知道它们有助于解决梯度消失问题并提高模型性能。'
- en: '**Feed-forward neural network**: This is a neural network that processes the
    attention vector inputs and transforms them into a form, of the same dimensions
    as the input, that can be input into the next layer. These attention vectors are
    independent of each other; consequently, parallelization can be used in this stage,
    rather than processing them sequentially as in the sequence-to-sequence architecture.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络**：这是一个处理注意力向量输入并将它们转换为与输入相同维度的形式的神经网络，可以输入到下一层。这些注意力向量彼此独立；因此，在这个阶段可以使用并行化，而不是像在序列到序列架构中那样按顺序处理它们。'
- en: Let’s now move on to decoders.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续讨论解码器。
- en: Decoders
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: The output from the encoder is used as the input to the second layer of each
    decoder in the decoder stack.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输出被用作解码器堆栈中每个解码器的第二层的输入。
- en: Similar to the encoder, masked multi-headed attention takes an output embedding
    and a positional embedding. The target for the transformer is to learn how to
    generate the output, given both the input and the required output.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与编码器类似，掩码多头注意力接受一个输出嵌入和一个位置嵌入。变换器的目标是学习如何根据输入和所需的输出生成输出。
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: During training, the required output (for example, a translation) is provided
    to the decoder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，所需的输出（例如，翻译）被提供给解码器。
- en: Some of the words are masked so that the model can learn how to predict them.
    These are changed during each iteration. The decoders process this along with
    the encoded representation from the encoders to produce a target sequence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些单词被遮蔽，以便模型可以学习如何预测它们。这些单词在每次迭代中都会改变。解码器会处理这些单词，以及来自编码器的编码表示，以生成目标序列。
- en: However, during prediction, an empty sequence (with a special **start of sentence**
    (**<SOS>**) token) is used. This is converted into an embedding; positional encoding
    is added and is used as input to the decoder. The decoder and other layers work
    as before but the last word from the output sequence is used to fill in the first
    blank of the input sequence, hence the input is now the <SOS> and the first predicted
    word. This is, again, fed into the decoder and the process is repeated until the
    end of the sentence.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在预测过程中，使用一个空序列（带有特殊的**句子开始**（**<SOS>**)标记）。这被转换为一个嵌入；添加位置编码后用作解码器的输入。解码器和其他层的工作方式与之前相同，但输出序列的最后一个单词被用来填充输入序列的第一个空白，因此输入现在是<SOS>和第一个预测的单词。这再次被送入解码器，并重复这个过程，直到句子的结尾。
- en: Linear layer
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性层
- en: The output from the decoder is used as input to this linear layer, a simple
    fully connected neural network that generates the vectors for the next layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的输出被用作这个线性层的输入，这是一个简单的全连接神经网络，它生成下一层的向量。
- en: Softmax layer
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Softmax层
- en: The softmax layer transforms the input into a probability distribution – that
    is, it takes a set of numbers and turns them into positive numbers that sum up
    to 1, applying higher importance to higher values and less importance to smaller
    values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax层将输入转换为概率分布——也就是说，它将一组数字转换为总和为1的正面数字，对较高值赋予更高的权重，对较小值赋予较低的权重。
- en: Output probabilities
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出概率
- en: Finally, the output probabilities are the word tokens for the target. The transformer
    compares this output with the target sequence that came from the training data
    and uses it to improve the results via back-propagation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出概率是目标单词标记。变压器将这个输出与来自训练数据的目标序列进行比较，并使用它通过反向传播来改进结果。
- en: Now that we’ve learned how transformers work, in the next section, we will have
    a very brief look at how one organization has made the process of implementing
    and experimenting with transformer-based models easy, making it accessible to
    all.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了变压器的工作原理，在下一节中，我们将简要地看看一个组织是如何使基于变压器的模型的实施和实验变得简单，使其对所有用户都易于访问。
- en: Hugging Face
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Transformers need a *lot* of data to be effective and produce good results.
    Furthermore, a huge amount of computing power and time is also needed; the best
    models are usually trained using multiple GPUs and can take days (or even longer)
    to complete training. Consequently, not everyone can afford to train such models
    and usually, this is done by the big players such as Google, Facebook, and OpenAI.
    Luckily, there are pretrained models available.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器需要大量的数据才能有效并产生良好的结果。此外，还需要巨大的计算能力和时间；最好的模型通常使用多个GPU进行训练，可能需要几天（甚至更长）的时间来完成训练。因此，并不是每个人都能负担得起训练这样的模型，通常这由像谷歌、Facebook和OpenAI这样的大型企业来完成。幸运的是，已经有预训练的模型可供使用。
- en: The name Hugging Face (named after the emoji with a smiling face and open hands)
    is synonymous with transformers, models, and NLP. Hugging Face ([https://huggingface.co](https://huggingface.co))
    provides a repository for pretrained transformer (and other) models to be published.
    These can then be easily downloaded, free of charge, and used for a wide range
    of NLP tasks. Furthermore, if the task involves a domain that has unique nomenclature,
    terminology, and domain-specific language, then models can be “fine-tuned” to
    improve the model’s performance. Fine-tuning is a process that uses the weights
    from a pretrained model as a starting point and uses new domain-specific data
    to update them so that the model becomes better at the domain-specific task. As
    well as publishing models, Hugging Face also provides services that allow models
    to be fine-tuned, trained, and much more.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face（以微笑脸和展开双手的emoji命名）与变换器、模型和NLP同义。Hugging Face ([https://huggingface.co](https://huggingface.co))提供了一个用于发布预训练变换器（和其他）模型的存储库。这些模型可以免费下载并用于广泛的NLP任务。此外，如果任务涉及具有独特命名法、术语和特定领域语言的领域，则可以对模型进行“微调”以提高模型在该特定领域的性能。微调是一个使用预训练模型的权重作为起点，并使用新的特定领域数据来更新它们的过程，从而使模型在特定领域任务上表现得更好。除了发布模型外，Hugging
    Face还提供了一项服务，允许模型进行微调、训练以及更多操作。
- en: 'Hugging Face also provides a Python library that provides a high-level interface
    for NLP tasks. The library offers a wide range of state-of-the-art pretrained
    models, including BERT, GPT, RoBERTa, T5, and many others (see the next section).
    It can be installed using the following command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face还提供了一个Python库，为NLP任务提供了一个高级接口。该库提供了一系列最先进的预训练模型，包括BERT、GPT、RoBERTa、T5等（见下一节）。可以使用以下命令进行安装：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Apart from downloading pretrained models, the library can also be used to download
    tokenizers. Both of these can then be used with your datasets to fine-tune tasks
    such as classification to create state-of-the-art NLP systems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 除了下载预训练模型外，该库还可以用于下载标记化器。这两者都可以与您的数据集一起使用，以微调分类等任务，以创建最先进的NLP系统。
- en: In summary, the Hugging Face `transformers` library is a powerful tool for working
    with NLP models, making it easy to work with transformer models. It has an intuitive
    design and extensive model selection and is well worth a look.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Hugging Face的`transformers`库是一个强大的工具，用于处理NLP模型，使得与变换器模型的工作变得简单。它具有直观的设计和广泛的模型选择，非常值得一看。
- en: Existing models
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有模型
- en: 'Driven by boosts in computing, storage, and data capacity, transformers have
    taken the world by storm. Some of the more famous pretrained models include the
    following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动于计算、存储和数据容量的提升，变换器在全球范围内掀起了一场风暴。一些更著名的预训练模型包括以下内容：
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**): Created
    by the Google AI team, and trained on a huge corpus of text data, BERT takes the
    context from both the left and right sides of each word into account.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来自变换器的双向编码器表示** (**BERT**): 由谷歌AI团队创建，并在大量的文本数据语料库上进行训练，BERT考虑了每个单词左右两边的上下文。'
- en: '**Efficiently Learning an Encoder that Classifies Token Accurately** (**ELECTRA**):
    ELECTRA uses a generator-discriminator model to distinguish between generated
    and real text. The generator is trained to generate text that is similar to real
    text, while the discriminator is trained to distinguish between real and generated
    text.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效学习准确分类标记的编码器** (**ELECTRA**): ELECTRA使用生成器-判别器模型来区分生成的文本和真实文本。生成器被训练生成与真实文本相似的文本，而判别器被训练区分真实文本和生成文本。'
- en: '**Generative Pre-trained Transformer 3** (**GPT-3**): Developed by OpenAI and
    pretrained on a huge range of internet text, GPT-3 has 175 billion parameters
    and is one of the largest models available to date.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成式预训练变换器3** (**GPT-3**): 由OpenAI开发，并在广泛的互联网文本上进行预训练，GPT-3拥有1750亿个参数，是目前可用的最大模型之一。'
- en: '**Megatron (a large transformer model trained by NVIDIA)**: Developed by NVIDIA,
    Megatron is scalable and can be trained on hundreds of GPUs, so it can use much
    larger models.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NVIDIA训练的大型变换器模型** (Megatron): 由NVIDIA开发，Megatron可扩展，可以在数百个GPU上训练，因此可以使用更大的模型。'
- en: '**Robustly Optimized BERT** (**RoBERTa**): Based on BERT, RoBERTa is designed
    to improve upon BERT by using a larger training corpus and more training steps
    to learn more robust representations of text.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒优化BERT** (**RoBERTa**): 基于BERT，RoBERTa通过使用更大的训练语料库和更多的训练步骤来学习更鲁棒的文字表示，旨在改进BERT。'
- en: '**Text-to-Text Transfer Transformer** (**T5**): Developed by Google, T5 treats
    NLP problems as “text-to-text” problems. It is trained on unlabeled and labeled
    data and then fine-tunes it individually for a variety of tasks.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本到文本迁移转换器**（**T5**）：由Google开发，T5将NLP问题视为“文本到文本”问题。它在未标记和标记数据上进行了训练，然后针对各种任务进行单独微调。'
- en: '**Transformer with extra-long context** (**Transformer-XL**): This model introduces
    a memory module that allows the model to handle and understand long-term dependencies
    much better.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有额外长上下文的转换器**（**Transformer-XL**）：该模型引入了一个记忆模块，使得模型能够更好地处理和理解长期依赖关系。'
- en: '**XLNet (generalized autoregressive pretraining)**: Developed by Google, XLNet
    takes the best bits from Transformer-XL and BERT and models dependencies between
    all input words.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XLNet（广义自回归预训练）**：由Google开发，XLNet从Transformer-XL和BERT中吸取了最佳元素，并建立了所有输入词之间的依赖关系。'
- en: 'In the next section, we look will look closer at how transformers are trained
    for the task we are interested in: classification. Inspiration for this section
    came from the Hugging Face pages.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地探讨转换器是如何为我们感兴趣的分类任务进行训练的：灵感来源于Hugging Face页面。
- en: Transformers for classification
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于分类的转换器
- en: Transformer models are trained as language models. These are a type of algorithm
    that has been trained by analyzing patterns of human language to understand and
    produce human language.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器模型被训练为语言模型。这是一种通过分析人类语言模式来理解和生成人类语言的算法类型。
- en: They have knowledge of grammar, syntax, and semantics, and can discern patterns
    and connections among words and phrases. Moreover, they can detect named entities,
    such as individuals, locations, and establishments, and interpret the context
    in which they are referenced. Essentially, a transformer model is a computer program
    that uses statistical models to analyze and generate language.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它们了解语法、句法和语义，并能辨别单词和短语之间的模式和联系。此外，它们可以检测命名实体，如个人、地点和机构，并解释它们被引用的上下文。本质上，转换器模型是一个使用统计模型来分析和生成语言的计算机程序。
- en: Language models are trained in a self-supervised manner on large amounts of
    text data, such as books, articles, and online content, to learn patterns and
    relationships between words and phrases. Some of the popular datasets used for
    pretraining transformers include Common Crawl, Wikipedia, and BooksCorpus. For
    example, BERT was trained using around 3.5 billion words in total with around
    2.5 billion from Wikipedia and around 1 billion from BooksCorpus. This allows
    the model to predict the likelihood of a certain word or phrase occurring after
    a given sequence of words. The outputs of a pretrained large language model typically
    involve predictions based on the input text. The model may output probabilities
    of certain words or phrases being used next in a sentence, make predictions of
    the most likely word to follow a given input word, or generate an entire sentence
    or paragraph based on the input text. The output can be used for various purposes,
    such as text generation, translation, sentiment analysis, and more.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是在大量文本数据上以自监督的方式进行训练的，例如书籍、文章和在线内容，以学习单词和短语之间的模式和关系。用于预训练转换器的流行数据集包括Common
    Crawl、Wikipedia和BooksCorpus。例如，BERT总共使用了大约35亿个单词进行训练，其中大约25亿来自Wikipedia，大约10亿来自BooksCorpus。这使得模型能够预测在给定单词序列之后出现某个单词或短语的可能性。预训练的大型语言模型的输出通常涉及基于输入文本的预测。模型可能会输出某些单词或短语在句子中接下来使用的概率，预测给定输入词后最可能跟随的词，或者根据输入文本生成整个句子或段落。输出可用于各种目的，如文本生成、翻译、情感分析等。
- en: Self-supervised learning is a type of machine learning where the model learns
    to extract useful information from unlabeled data without requiring any explicit
    labels or supervision. Instead, the model is trained on a task such as predicting
    the missing part of an image or reconstructing a corrupted sentence. Consequently,
    this type of model builds an understanding of the language it has been trained
    on – but only from a statistical point of view. However, this approach lacks practicality
    for everyday tasks, and therefore, a generalized **pretrained** model must be
    customized through supervised **fine-tuning** using human-annotated labels specific
    to the task at hand.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是一种机器学习方法，模型学习从未标记的数据中提取有用的信息，而不需要任何明确的标签或监督。相反，模型是在预测图像缺失部分或重建损坏句子等任务上训练的。因此，这类模型对其训练过的语言有了理解——但仅从统计的角度来看。然而，这种方法在日常任务中缺乏实用性，因此，必须通过使用针对当前任务的人类标注标签的监督微调来定制通用的**预训练**模型。
- en: Pretraining (that is, training a model from scratch) requires huge amounts of
    data, and hence the process can take weeks or months. Fine-tuning is then performed
    *on* the pretrained model, so a pretrained language model is required to do the
    fine-tuning. Essentially, fine-tuning is a further training step with a dataset
    that suits the task.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练（即从头开始训练一个模型）需要大量的数据，因此这个过程可能需要数周或数月。然后，在预训练模型上进行微调，因此需要预训练的语言模型来进行微调。本质上，微调是一个进一步训练的步骤，使用适合任务的数据库集。
- en: Fine-tuning a model typically adjusts the weights of the model’s pretrained
    layers to better fit a new dataset or task. The process of fine-tuning involves
    initializing the weights of the pretrained layers, and then training the entire
    model on the new dataset or task. During training, the weights of the pretrained
    layers are updated along with the weights of the newly added layers, allowing
    the model to learn more nuanced features from the new dataset while preserving
    the knowledge learned from the pretrained model. The degree to which the weights
    of the pretrained layers are updated during fine-tuning depends on the specifics
    of the new task and the amount of available data. In some cases, only the weights
    of the newly added layers are updated, while in others, the weights of the pretrained
    layers may be updated significantly. Another option is to keep all layers fixed
    apart from the final layer, whose weights are then modified during training. Consequently,
    fixing the layers with these techniques, and using a smaller learning rate, during
    the fine-tuning process, often yields a performance improvement. Sometimes, this
    goes hand in hand with adding new layers on top of the old architecture, thus
    persisting the old fixed weights and only allowing the weights of new layers to
    be changed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型通常调整模型预训练层的权重，以更好地适应新的数据集或任务。微调的过程包括初始化预训练层的权重，然后在新的数据集或任务上对整个模型进行训练。在训练过程中，预训练层的权重会随着新添加的层权重一起更新，使模型能够从新的数据集中学习更细微的特征，同时保留从预训练模型中学到的知识。预训练层权重在微调过程中更新的程度取决于新任务的特定情况和可用数据量。在某些情况下，只有新添加层的权重被更新，而在其他情况下，预训练层的权重可能被显著更新。另一种选择是除了最后一层之外的所有层都保持固定，其权重在训练过程中被修改。因此，在微调过程中使用这些技术，并使用较小的学习率，通常会产生性能提升。有时，这伴随着在旧架构之上添加新层，从而保持旧固定权重，只允许新层的权重被改变。
- en: But what exactly happens when fine-tuning? There are various techniques, but
    the general thought is that early layers learn generic patterns that are irrelevant
    to the actual task (for example, classification), while later layers learn the
    patterns that are relevant to the task. This intuition has been verified by various
    research teams.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但微调时究竟发生了什么？有各种技术，但一般的想法是，早期层学习与实际任务（例如，分类）无关的通用模式，而后期层学习与任务相关的模式。这种直觉已经被各个研究团队所验证。
- en: Note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: During the gradient descent calculation, the size of the step that’s taken in
    each iteration is determined by the learning rate, with the overall goal being
    to locate the minimum of a loss function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降计算过程中，每次迭代的步长大小由学习率决定，整体目标是找到损失函数的最小值。
- en: In practical terms, for classification, we would download a model such as **BertForSequenceClassification**
    – a BERT model with a linear layer for sentence classification. So, the final
    layer produces a probability vector that indicates the probability of each potential
    class label for the input sequence.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，对于分类任务，我们会下载一个如**BertForSequenceClassification**的模型——这是一个用于句子分类的BERT模型，它包含一个线性层。因此，最后一层生成一个概率向量，表示输入序列每个潜在类标签的概率。
- en: In short, fine-tuning a model enables it to adapt features it has learned to
    a new task or dataset, which can result in improved performance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，微调模型使其能够将学习到的特征适应新的任务或数据集，这可能导致性能提升。
- en: We looked at the individual bits of the model earlier in this chapter and saw
    how the model has encoder and decoder blocks. Depending on the task, each of these
    parts can be utilized separately. For the classification task, an encoder-only
    model is recommended. For more details, there are some great Packt books available,
    such as *Transformers for Natural Language Processing*, by Denis Rothman.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们探讨了模型的各个部分，并看到了模型具有编码器和解码器块。根据任务，这些部分中的每一个都可以单独使用。对于分类任务，建议使用仅编码器模型。更多细节，有一些很好的Packt书籍可供参考，例如Denis
    Rothman的《自然语言处理中的变压器》。
- en: In the next section, we will fine-tune a model using a training dataset, make
    some predictions on a test dataset, and evaluate the results.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用训练数据集微调模型，在测试数据集上进行一些预测，并评估结果。
- en: Implementing transformers
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现变压器
- en: In this section, we will work through the code for implementing transformers
    for both single-emotion and multi-emotion datasets. We will be using **Google
    Colaboratory** (**Colab**) as it simplifies the implementation of transformers
    by providing a powerful cloud-based environment with pre-installed libraries and
    resources. So, let’s begin by looking at that.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过代码实现针对单情感和多情感数据集的变压器的实现。我们将使用**Google Colaboratory**（**Colab**），因为它通过提供一个强大的基于云的环境，预装了库和资源，从而简化了变压器的实现。因此，让我们先看看这一点。
- en: Google Colab
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Colab
- en: Google Colab is a free notebook environment service that runs in the cloud ([https://colab.research.google.com](https://colab.research.google.com)).
    There are many benefits of using Colab; for example, it allows developers to start
    programming rapidly without having to worry about setup, it allows code to be
    shared with people who do not have the correct software installed locally, and
    it also integrates well with GitHub. However, one of the biggest advantages is
    that Google provides free access to GPUs. Machine learning, at its core, involves
    lots and lots of mathematical operations – something that GPUs are good at. In
    practical terms, even for simple models with small training datasets, the time-saving
    between GPU and non-GPU-powered systems can be many hours (for example, 10 minutes
    compared to 10 hours).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab是一个免费的笔记本环境服务，它在云端运行（[https://colab.research.google.com](https://colab.research.google.com)）。使用Colab有许多好处；例如，它允许开发者快速开始编程，无需担心设置，它允许与没有正确软件安装的本地用户共享代码，并且它与GitHub很好地集成。然而，最大的优势之一是Google提供了免费的GPU访问。机器学习的核心涉及大量的数学运算——这是GPU擅长的。从实际应用的角度来看，即使是具有小型训练数据集的简单模型，GPU和非GPU系统之间的节省时间也可以是数小时（例如，10分钟与10小时相比）。
- en: A few words of warning, though. Colab is ephemeral – in other words, files (for
    example, data files) uploaded to a session or generated by a session (for example,
    results) will eventually disappear. The workaround for this is to upload files
    to Google Drive and give permission for Colab to access them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，还有一些注意事项。Colab是短暂的——换句话说，上传到会话或由会话生成（例如，结果）的文件（例如，数据文件）最终会消失。解决这个问题的方法是上传文件到Google
    Drive，并允许Colab访问它们。
- en: 'Debugging on Colab is also a little more cumbersome than, say, via VS Code.
    It involves installing and importing the `ipdb` (IPython-enabled Python Debugger)
    package:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在Colab上的调试也比通过VS Code等工具要繁琐一些。它涉及到安装和导入`ipdb`（IPython启用Python调试器）包：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Breakpoints are useful for developers and these can be set via code to cause
    the debugger to stop:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 断点对开发者很有用，这些可以通过代码设置，以使调试器停止：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can use command-line arguments to control the debugger, such as the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用命令行参数来控制调试器，如下所示：
- en: '`c`: Continue execution'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c`：继续执行'
- en: '`n`: Move to the next line'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`：移动到下一行'
- en: '`r`: Continue execution until the current function returns'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r`：继续执行，直到当前函数返回'
- en: 'Debugging can be globally turned off using the following command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令全局关闭调试：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Debugging can also be globally turned on using the following command:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用以下命令全局打开调试：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we know what a transformer is, how it works, and how to implement it,
    let’s implement a transformer in Python using Colab to classify the datasets introduced
    in the previous chapters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了什么是转换器，它是如何工作的，以及如何实现它，让我们使用 Colab 在 Python 中实现一个转换器来分类前几章中介绍的数据集。
- en: Single-emotion datasets
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单一情感数据集
- en: 'We will implement two transformers to cater to the two different types of datasets.
    Let’s start with the single-emotion task. Broadly, we will be following these
    steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现两个转换器来满足两种不同类型的数据集。让我们从单一情感任务开始。总的来说，我们将遵循以下步骤：
- en: Install the necessary libraries.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装必要的库。
- en: Import the necessary libraries.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Provide access to Google Drive.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供对 Google Drive 的访问权限。
- en: Create dataset and model variables.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集和模型变量。
- en: Load and prepare the datasets.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并准备。
- en: Tokenize the datasets.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记数据集。
- en: Load a model for classification.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载用于分类的模型。
- en: Set up trainer arguments.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置训练器参数。
- en: Train the model.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Use the trained model to predict.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型进行预测。
- en: Evaluate.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估。
- en: Classifying the single-emotion tweets is a somewhat easier task, so let’s being
    with this.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对单一情感推文进行分类是一个相对容易的任务，所以让我们从这里开始。
- en: 'Let’s begin by installing some libraries:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装一些库开始：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These libraries are used to easily access datasets, evaluate the results of
    a model, and access the pretrained models available from Hugging Face, respectively.
    We can now import these into our code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库用于轻松访问数据集、评估模型的输出结果以及访问 Hugging Face 上可用的预训练模型。我们现在可以将它们导入到我们的代码中：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As mentioned previously, we want to upload our training and test files once
    and access them on-demand whenever we need them, so let’s give Colab access to
    our Google Drive, which is where the files are uploaded. In reality, some of these
    files are already available via the `datasets` library, but for now, let’s assume
    we want to access them from our repository:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们希望一次性上传我们的训练和测试文件，并在需要时按需访问它们，因此让我们让 Colab 获取我们的 Google Drive 访问权限，这是文件上传的地方。实际上，其中一些文件已经通过
    `datasets` 库可用，但为了现在，我们假设我们想从我们的存储库中访问它们：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You should replace `BASE_PATH` with your own path.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您应将 `BASE_PATH` 替换为您自己的路径。
- en: 'We should now set up a few things to make our task easier for us. Different
    datasets need different parameters, so `enum` can be used to control the execution
    flow of the code. We must name our files so that the names contain the language
    code of the tweets within the file (that is, `AR`, `ES`, and `EN`), and then use
    `enum` and the filename to set variables that are useful in the code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该设置一些事情来使我们的任务更容易。不同的数据集需要不同的参数，因此可以使用 `enum` 来控制代码的执行流程。我们必须命名我们的文件，使文件名包含文件内推文的语言代码（即
    `AR`、`ES` 和 `EN`），然后使用 `enum` 和文件名来设置在代码中有用的变量：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For now, we will also set a variable such as `NUM_LABELS` to tell the model
    how many labels there are. Later, we will see that we don’t need to do this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还将设置一个变量，例如 `NUM_LABELS`，以告诉模型有多少个标签。稍后，我们将看到我们不需要这样做：
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can use `enum` to set some dataset-specific variables. This way, when
    we want to try other datasets, we only need to modify the `ds` variable:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `enum` 来设置一些特定于数据集的变量。这样，当我们想尝试其他数据集时，我们只需要修改 `ds` 变量：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We must also set `model_name` to tell the program which language-specific model
    to use:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须设置 `model_name` 以告诉程序使用哪种特定语言的模型：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we can set various file path variables:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以设置各种文件路径变量：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we must set a variable called `stub`, which we will use to save our
    model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须设置一个名为 `stub` 的变量，我们将用它来保存我们的模型：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The Hugging Face `transformers` library works well with the `datasets` library.
    So, next, we will load the data files, remove any unwanted columns, and create
    a `DatasetDict` object that will be used in subsequent parts of the pipeline:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的 `transformers` 库与 `datasets` 库配合得很好。因此，接下来，我们将加载数据文件，删除任何不需要的列，并创建一个
    `DatasetDict` 对象，该对象将在管道的后续部分中使用：
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we must create a function that tokenizes the tweets from the training
    and test datasets that are held in the `dataset` variable. Simply put, the job
    of the tokenizer is to prepare the data, making it ready for input to a model.
    It does this by splitting sentences into words (tokens) and then splitting words
    into pieces (for example, *flyfishing* would be split into *fly*, *fish*, and
    *ing*). These tokens are then split into IDs (numbers) via a lookup table. Typically,
    you would use the tokenizer associated with the model that you are using. For
    example, for the `bert-base-cased` model, you would use `BertTokenizer`. However,
    in the following code, we have used something called `AutoTokenizer`. `AutoTokenizer`
    is a generic tokenizer auto class that automatically fetches the correct tokenizer
    class from the Hugging Face tokenizers library, as well as the data associated
    with the model’s tokenizer. An auto class is a generic class that simplifies the
    coding process by automatically finding the architecture of a pretrained model
    based on its name. All we need to do is choose the appropriate `AutoModel` for
    our task. Essentially, they are more flexible and make the programming somewhat
    simpler:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须创建一个函数，该函数将存储在`dataset`变量中的训练集和测试集中的推文进行分词。简单来说，分词器的工作是准备数据，使其准备好输入到模型中。它是通过将句子拆分成单词（标记）然后将单词拆分成片段（例如，*flyfishing*会被拆分成*fly*，*fish*和*ing*）来完成的。然后这些标记通过查找表拆分成ID（数字）。通常，你会使用与你使用的模型关联的分词器。例如，对于`bert-base-cased`模型，你会使用`BertTokenizer`。然而，在下面的代码中，我们使用了名为`AutoTokenizer`的东西。`AutoTokenizer`是一个通用的分词器自动类，它自动从Hugging
    Face分词器库中获取正确的分词器类以及与模型分词器相关的数据。一个自动类是一个通用类，通过自动根据其名称找到预训练模型的架构来简化编码过程。我们只需要为我们的任务选择合适的`AutoModel`。本质上，它们更加灵活，使编程变得稍微简单一些：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now comes the interesting part! We need to load a model for classification.
    As before, we could have used a specific BERT model that has been trained for
    sentence classification, such as `BertForSequenceClassification`. However, we
    have chosen to use an auto class to obtain the text classification model. In this
    case, since we are classifying text, we used `AutoModelForSequenceClassification`
    as `AutoModel`. We just supplied the name of the model and the number of labels
    that we are dealing with – the library takes care of the rest:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分！我们需要加载一个用于分类的模型。和之前一样，我们可以使用一个专门为句子分类训练的特定BERT模型，比如`BertForSequenceClassification`。然而，我们选择使用一个自动类来获取文本分类模型。在这种情况下，由于我们正在对文本进行分类，我们使用了`AutoModelForSequenceClassification`作为`AutoModel`。我们只需提供模型的名称和我们正在处理的标签数量——库会处理其余部分：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We are now ready to train the model, but first, we need to set up some arguments
    specifying what we want the trainer to do. We can do this by simply creating a
    `TrainingArguments` instance telling it where to save our model and that we want
    it to evaluate at the end of each epoch. The arguments are passed to `Trainer`,
    along with the model and the training and test datasets. Now, it is a simple matter
    of invoking the training and waiting for the results. Note how we save the resultant
    model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练模型，但首先，我们需要设置一些参数来指定我们希望训练器执行的操作。我们可以通过简单地创建一个`TrainingArguments`实例来实现，告诉它将模型保存在哪里，以及我们希望在每个epoch结束时进行评估。这些参数通过`Trainer`传递，包括模型和训练集以及测试集。现在，调用训练并等待结果就变得简单了。注意我们如何保存生成的模型：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If all went well, you should see something like this (truncated):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会看到类似以下的内容（截断）：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The algorithms and programs used in this chapter all randomize aspects of the
    data, particularly the initial assignments of weights to internal nodes of the
    network, and hence the results that you obtain by running the same scripts on
    the same data may vary slightly from the results in the text.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的算法和程序都会随机化数据的一些方面，尤其是网络内部节点的权重初始分配，因此，你在相同数据上运行相同脚本获得的结果可能与文本中的结果略有不同。
- en: 'Now that we have a model fine-tuned on our dataset, we can see whether it does
    a good job on our test dataset:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对我们的数据集进行了微调的模型，我们可以看到它在我们测试数据集上的表现如何：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we can set up a dictionary of measures and iterate through them, computing
    and printing as we go:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以设置一个度量字典并遍历它们，边计算边打印：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will generate something like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成类似以下的内容：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The results from the model are summarized in the following table:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的结果总结在以下表格中：
- en: '| **Dataset** | **Precision** | **Recall** | **micro F1** | **macro F1** |
    **Jaccard** |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **精确度** | **召回率** | **micro F1** | **macro F1** | **Jaccard** |'
- en: '| SEM4-EN | 0.962 | 0.964 | 0.962 | 0.962 | 0.927 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.962 | 0.964 | 0.962 | 0.962 | 0.927 |'
- en: '| WASSA-EN | 0.855 | 0.861 | 0.855 | 0.856 | 0.753 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.855 | 0.861 | 0.855 | 0.856 | 0.753 |'
- en: '| CARER-EN | 0.881 | 0.921 | 0.927 | 0.896 | 0.816 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.881 | 0.921 | 0.927 | 0.896 | 0.816 |'
- en: '| SEM4-AR | 0.817 | 0.837 | 0.843 | 0.825 | 0.710 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.817 | 0.837 | 0.843 | 0.825 | 0.710 |'
- en: '| SEM4-ES | 0.791 | 0.786 | 0.807 | 0.787 | 0.663 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | 0.791 | 0.786 | 0.807 | 0.787 | 0.663 |'
- en: '| IMDB-EN | 0.905 | 0.905 | 0.905 | 0.905 | 0.826 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.905 | 0.905 | 0.905 | 0.905 | 0.826 |'
- en: Table 9.1 – Scores for transformer-based models for the single-label datasets
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 – 单标签数据集基于transformer模型的分数
- en: Most of the scores here are better than the scores that we obtained using the
    classifiers earlier in this book, though the best classifier for WASSA-EN and
    CARER-EN remains the single-class SVM. The scores for SEM4-AR and SEM4-ES are
    both significantly better than the previous scores, possibly because the pretrained
    models do a better job of finding roots, and maybe even doing disambiguation,
    than the simple stemmers we used in the earlier chapters. It is very hard to extract
    intermediate results from a complex DNN such as a transformer, so it is even more
    difficult than was the case in previous chapters to analyze why one classifier
    of this kind does better than another, but it seems likely that this is a key
    factor in these cases.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的大部分分数都优于我们在这本书早期使用分类器获得的分数，尽管WASSA-EN和CARER-EN的最佳分类器仍然是单类SVM。SEM4-AR和SEM4-ES的分数都显著优于之前的分数，这可能是由于预训练模型在寻找根和可能进行消歧方面做得比我们在早期章节中使用的简单词干提取器要好。从复杂的DNN（如transformer）中提取中间结果非常困难，因此分析为什么这种类型的某个分类器比另一个分类器表现更好比之前章节中更困难，但似乎这在这些情况下是一个关键因素。
- en: Multi-emotion datasets
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多情绪数据集
- en: 'Now, let’s build a transformer model to classify multi-label tweets. Much of
    the code is similar, so we won’t reproduce that, concentrating instead on the
    interesting bits of the multi-classification problem. We will be following these
    steps:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个用于分类多标签推文的transformer模型。大部分代码是相似的，所以我们将不会重新展示，而是专注于多分类问题的有趣部分。我们将遵循以下步骤：
- en: Install the necessary libraries.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装必要的库。
- en: Import the necessary libraries.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Provide access to Google Drive.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供对Google Drive的访问权限。
- en: Create dataset variables.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集变量。
- en: Convert datasets into `DatasetDict`.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集转换为`DatasetDict`。
- en: Load and prepare the datasets.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并准备数据集。
- en: Tokenize the datasets.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集进行分词。
- en: Load a model for classification.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载用于分类的模型。
- en: Define metric functions.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义度量函数。
- en: Set up trainer arguments.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置训练器参数。
- en: Train the model.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Evaluate.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估。
- en: Let’s begin!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'We must install and import the libraries as before, and also allow access to
    Google Drive as before. Now, let’s get the data files from the online repository.
    However, the KWT files are in Google Drive, so we need some code to load and convert
    these into a `DatasetDict` object:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须像以前一样安装和导入库，并且像以前一样允许访问Google Drive。现在，让我们从在线存储库中获取数据文件。然而，KWT文件在Google
    Drive中，因此我们需要一些代码来加载并将这些转换为`DatasetDict`对象：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Our `Dataset` enum now also reflects the fact that we are working with different
    files, so let’s use `enum` to get the right data files and set the model:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Dataset`枚举现在也反映了我们正在处理不同的文件，因此让我们使用`enum`来获取正确的数据文件并设置模型：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'There are three types of datasets: train, test, and validation. We will use
    the train and validation datasets:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有三种类型：训练集、测试集和验证集。我们将使用训练集和验证集：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note how in the first example, we had to set `NUM_LABELS` and we had no idea
    of what the actual labels were. Here, we are going to dynamically work out the
    labels and also create some lookup tables that allow us to easily go from an emotion
    to a label and vice versa:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在第一个例子中，我们必须设置`NUM_LABELS`，而我们不知道实际的标签是什么。在这里，我们将动态确定标签，并创建一些查找表，使我们能够轻松地从情绪到标签以及反之：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following output clarifies what each of these looks like:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出解释了这些看起来是什么样子：
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we need to tokenize the datasets, as we did previously. The task is slightly
    more complicated here as we have multiple labels for each tweet, and the labels
    are loaded as `True` and `False`, whereas we need `0` and `1` for our model. The
    `tokenize_function` takes 1,000 tweets at a time, tokenizes the tweet text as
    before, and converts the labels into an array of 1s and 0s:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要对数据集进行分词，就像我们之前做的那样。这里的任务稍微复杂一些，因为我们每个推文都有多个标签，标签被加载为`True`和`False`，而我们需要`0`和`1`来供我们的模型使用。`tokenize_function`一次处理1000条推文，像之前一样对推文文本进行分词，并将标签转换为1s和0s的数组：
- en: '[PRE27]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We are using `pytorch` in this example, so we need to set the format of the
    dataset so that it’s compatible:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用`pytorch`，因此我们需要设置数据集的格式，使其兼容：
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we can instantiate an auto class, as we did previously. Note how we have
    to set `problem_type` and also pass in the `id-label` and `label-id` mapping objects:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化一个自动类，就像我们之前做的那样。注意我们如何设置`problem_type`以及传递`id-label`和`label-id`映射对象：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we need to define some functions to calculate some metrics for us. Because
    we have multi-labels, we are dealing with probabilities. Consequently, we need
    a threshold to distinguish between a 0 and a 1 for the emotion – we have arbitrarily
    set this to `0.5` for now. In practice, this would need to be carefully determined.
    These probabilities are turned into 0s and 1s using the threshold and, as before,
    we piggyback on `scikit-learn` functions to do the heavy lifting for us:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义一些函数来为我们计算一些度量。因为我们有多个标签，所以我们处理的是概率。因此，我们需要一个阈值来区分情感中的0和1 – 我们现在任意地将这个值设置为`0.5`。在实践中，这需要仔细确定。这些概率通过阈值转换为0和1，并且，就像之前一样，我们利用`scikit-learn`函数为我们做繁重的工作：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can now set up some `TrainingArguments` and train the model:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以设置一些`TrainingArguments`并训练模型：
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The final step is to evaluate the results using our metric functions. Notice
    how we pass the name of the `compute_metrics` function as a parameter. This function,
    in turn, calls `compute_multi_label_metrics` to calculate the various metrics:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用我们的度量函数评估结果。注意我们如何将`compute_metrics`函数的名称作为参数传递。这个函数反过来调用`compute_multi_label_metrics`来计算各种度量：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The final results should look something like this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果应该看起来像这样：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The results from the model are summarized in the following table:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的结果总结在下表中：
- en: '| **Dataset** | **Precision** | **Recall** | **micro F1** | **macro F1** |
    **Jaccard** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **精确度** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| SEM11-EN | 0.694 | 0.496 | 0.710 | 0.539 | 0.418 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.694 | 0.496 | 0.710 | 0.539 | 0.418 |'
- en: '| SEM11-AR | 0.552 | 0.441 | 0.658 | 0.462 | 0.359 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.552 | 0.441 | 0.658 | 0.462 | 0.359 |'
- en: '| KWT.M-AR | 0.132 | 0.074 | 0.224 | 0.092 | 0.053 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.132 | 0.074 | 0.224 | 0.092 | 0.053 |'
- en: '| SEM11-ES | 0.594 | 0.399 | 0.597 | 0.463 | 0.340 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.594 | 0.399 | 0.597 | 0.463 | 0.340 |'
- en: Table 9.2 – Scores for transformer-based models for the multi-class datasets
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.2 – 基于转换器的模型在多类数据集上的得分
- en: 'Again, the transformer-based models work better for some, but not all, datasets.
    It is worth noting that the previous best classifier for SEM11-AR was the stemmed
    version of the simple lexical model from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector-Space Models*, with a Jaccard score of 0.386\.
    For SEM11-ES, it was the stemmed version of the conditional probability model,
    also from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons
    and Vector Space Models*, with a Jaccard score of 0.278\. As with the single-class
    datasets, it seems likely that using the pretrained models may have helped us
    with identifying and disambiguating tokens, but this time, the underlying model
    is less good at handling multi-class cases. The score for the KWT.M-AR dataset
    is particularly poor: using transformers of the kind described here does not seem
    to be a good way to handle datasets with large numbers of tweets with no emotion
    ascribed to them.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，对于某些数据集，基于转换器的模型表现更好，但不是所有。值得注意的是，SEM11-AR之前最好的分类器是来自[*第5章*](B18714_05.xhtml#_idTextAnchor116)的简单词法模型的词干版本，其Jaccard得分为0.386。对于SEM11-ES，它是条件概率模型的词干版本，也来自[*第5章*](B18714_05.xhtml#_idTextAnchor116)，其Jaccard得分为0.278。与单类数据集一样，使用预训练模型可能有助于我们识别和消除歧义，但这次，基础模型在处理多类情况时表现较差。KWT.M-AR数据集的得分尤其糟糕：使用这里描述的转换器似乎不是处理没有情感归属的大量推文数据集的好方法。
- en: 'In several cases, using transformers produced better results than the classifiers
    from previous chapters. The following table shows the scores for a range of classifiers
    on our datasets (given the number of classifiers we have looked at now, this table
    only includes ones that were the best on at least one dataset):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在几个案例中，使用transformers产生的结果比之前章节中的分类器更好。以下表格显示了我们在数据集上对一系列分类器的评分（考虑到我们现在已经查看的分类器数量，这个表格只包括至少在一个数据集上表现最好的那些）：
- en: '|  | **LEX (unstemmed)** | **LEX (stemmed)** | **SVM (single)** | **SNN (single)**
    | **Transformers** |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | **LEX (unstemmed)** | **LEX (stemmed)** | **SVM (single)** | **SNN (single)**
    | **Transformers** |'
- en: '| SEM4-EN | 0.503 | 0.497 | 0.845 | 0.829 | 0.927 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.503 | 0.497 | 0.845 | 0.829 | 0.927 |'
- en: '| SEM11-EN | 0.347 | 0.348 | 0.224 | 0.242 | 0.418 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.347 | 0.348 | 0.224 | 0.242 | 0.418 |'
- en: '| WASSA-EN | 0.445 | 0.437 | 0.770 | 0.737 | 0.753 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.445 | 0.437 | 0.770 | 0.737 | 0.753 |'
- en: '| CARER-EN | 0.350 | 0.350 | 0.770 | 0.820 | 0.816 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.350 | 0.350 | 0.770 | 0.820 | 0.816 |'
- en: '| IMDB-EN | 0.722 | 0.667 | 0.736 | 0.793 | 0.826 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.722 | 0.667 | 0.736 | 0.793 | 0.826 |'
- en: '| SEM4-AR | 0.506 | 0.509 | 0.514 | 0.504 | 0.710 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.506 | 0.509 | 0.514 | 0.504 | 0.710 |'
- en: '| SEM11-AR | 0.378 | 0.386 | 0.216 | 0.221 | 0.359 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.378 | 0.386 | 0.216 | 0.221 | 0.359 |'
- en: '| KWT.M-AR | 0.687 | 0.663 | 0.631 | 0.028 | 0.053 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.687 | 0.663 | 0.631 | 0.028 | 0.053 |'
- en: '| SEM4-ES | 0.425 | 0.420 | 0.412 | 0.337 | 0.663 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | 0.425 | 0.420 | 0.412 | 0.337 | 0.663 |'
- en: '| SEM11-ES | 0.269 | 0.271 | 0.226 | 0.221 | 0.340 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.269 | 0.271 | 0.226 | 0.221 | 0.340 |'
- en: Table 9.3 – Best scores to date for the standard datasets – Jaccard scores
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.3 – 标准数据集至今的最佳分数 – Jaccard分数
- en: The results of using transformers are better than any of the previous classifiers
    for 6 of our 10 datasets, though surprisingly, the very simple lexicon-based classifiers
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and
    Vector Space Models* still produce the best results for the multi-class Arabic
    datasets!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用transformers的结果比我们10个数据集中任何先前分类器的结果都要好，尽管令人惊讶的是，来自[*第5章*](B18714_05.xhtml#_idTextAnchor116)“情感词典和向量空间模型”的非常简单的基于词典的分类器对于多类阿拉伯语数据集仍然产生了最佳结果！
- en: It remains the case that there is a drop-off in performance between the single-emotion
    datasets and the multi-emotion datasets. As before, this is likely to be due to
    a combination of factors. The fact that the multi-class datasets have more labels
    than the others also makes the task harder, simply because there is more scope
    for mistakes. However, we know that multi-emotion classification is much more
    difficult than single-emotion classification because it involves working out how
    many emotions a text expresses, from zero upward, rather than just choosing the
    one with the highest scores. We will look at ways of dealing with this kind of
    data in more detail in [*Chapter* *10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然存在单情感数据集和多情感数据集之间性能下降的情况。正如之前所说，这可能是由于多种因素的综合作用。多类数据集比其他数据集拥有更多的标签，这也使得任务更加困难，因为错误的空间更大。然而，我们知道多情感分类比单情感分类要困难得多，因为它需要确定文本表达了多少种情感，从零开始向上，而不是仅仅选择得分最高的那个。我们将在[*第10章*](B18714_10.xhtml#_idTextAnchor193)“多分类器”中更详细地探讨处理这类数据的方法。
- en: An interesting, natural question here is, why do transformers perform better
    than other methods? We have already seen how the self-attention mechanism allows
    transformers to focus on different parts of the input sequence when making predictions,
    hence allowing them to capture important long-range dependencies and contextual
    information. This is important for robust classification. Furthermore, we have
    also seen how transformers use multi-head attention, which allows them to attend
    to different parts of the input sequence simultaneously, thus making them more
    effective at capturing the different types of information that may be important
    for robust classification. Transformers also handle long input sequences without
    losing important information, and this may also be more useful in classification
    than other tasks. Finally, as we have seen, transformers are pretrained on huge
    datasets. Hence, even before fine-tuning, they already know general representations
    of language. These concepts can be combined in a highly effective way to create
    a mechanism that can generate good results.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣且自然的问题在这里是，为什么变换器比其他方法表现更好？我们已经看到，自注意力机制如何允许变换器在预测时关注输入序列的不同部分，从而使其能够捕捉重要的长距离依赖关系和上下文信息。这对于稳健的分类非常重要。此外，我们还看到了变换器如何使用多头注意力，这允许它们同时关注输入序列的不同部分，从而使其在捕捉对稳健分类可能重要的不同类型信息方面更加有效。变换器还能处理长输入序列而不会丢失重要信息，这在分类任务中可能比其他任务更有用。最后，正如我们所看到的，变换器是在大量数据集上预训练的。因此，即使在微调之前，它们也已经知道语言的通用表示。这些概念可以以高度有效的方式结合，以创建一个能够产生良好结果的机制。
- en: Now, let’s summarize what we’ve learned in this chapter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Transformers have proved to be very successful in a range of natural language
    tasks, with numerous recently released chatbots outperforming existing models
    in their ability to understand and manipulate human language. In this chapter,
    we looked at how transformers can be used for the task of assigning emotions to
    informal texts and investigated how well they perform on this task with a range
    of datasets. We started by taking a brief look at transformers, focusing on the
    individual components of a transformer, and how data flows through them. Transformers
    need a lot of data to be effective and produce good results, and a huge amount
    of computing power and time is also needed. Then, we introduced Hugging Face,
    discussed why it was useful, and introduced some of the more common pretrained
    models that are available on the Hugging Face platform, before moving on to discussing
    how transformers are used for classification. Finally, we showed how to code classifiers
    using transformers for single-emotion datasets and multi-emotion datasets before
    rounding off this chapter by discussing the results. In the next chapter, we will
    look at multiclassifiers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器在一系列自然语言任务中已被证明非常成功，最近发布的许多聊天机器人已经超越了现有模型，在理解和操纵人类语言的能力上表现出色。在本章中，我们探讨了如何使用变换器为非正式文本分配情感，并调查了它们在各种数据集上执行此任务的效果。我们首先简要地了解了变换器，重点关注变换器的各个组成部分以及数据是如何通过它们的。变换器需要大量的数据才能有效并产生良好的结果，同时还需要大量的计算能力和时间。然后，我们介绍了Hugging
    Face，讨论了它的有用之处，并介绍了Hugging Face平台上一些更常见的预训练模型，之后转向讨论变换器在分类中的应用。最后，我们展示了如何使用变换器为单情感数据集和多情感数据集编写分类器代码，在讨论结果后结束本章。在下一章中，我们将探讨多分类器。
- en: References
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 若想了解更多本章涉及的主题，请参阅以下资源：
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, Ł. and Polosukhin, I., 2017\. *Attention Is All You Need*. Advances in
    neural information processing systems, 30.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, Ł. and Polosukhin, I., 2017\. *Attention Is All You Need*. Advances in
    neural information processing systems, 30.
- en: 'Rothman, D., 2021\. *Transformers for Natural Language Processing: Build innovative
    deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT,
    RoBERTa, and more*. Packt Publishing Ltd.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rothman, D., 2021\. *Transformers for Natural Language Processing: Build innovative
    deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT,
    RoBERTa, and more*. Packt Publishing Ltd.'
- en: 'Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., 2018\. *Bert: Pre-training
    of deep bidirectional transformers for language understanding*. arXiv preprint
    arXiv:1810.04805.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德夫林，J.，张，M. W.，李，K.，和图托诺瓦，K.，2018\. *Bert：用于语言理解的深度双向转换器的预训练*. arXiv预印本 arXiv:1810.04805.
- en: 'Clark, K., Luong, M. T., Le, Q. V. and Manning, C. D., 2020\. *Electra: Pre-training
    text encoders as discriminators rather than generators*. arXiv preprint arXiv:2003.10555.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克拉克，K.，卢昂，M. T.，黎，Q. V.，和曼宁，C. D.，2020\. *Electra：将文本编码器预训练为判别器而不是生成器*. arXiv预印本
    arXiv:2003.10555.
- en: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., and Agarwal, S., 2020\. *Language models
    are few-shot learners*. Advances in neural information processing systems, 33,
    pp.1877-1901.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布朗，T.，曼，B.，莱德，N.，苏比亚哈，M.，卡普兰，J. D.，达里瓦尔，P.，尼拉卡坦，A.，希亚姆，P.，萨斯特里，G.，阿斯凯尔，A.，和阿加瓦尔，S.，2020\.
    *语言模型是少样本学习者*. 神经信息处理系统进展，33，pp.1877-1901.
- en: 'Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
    B., 2019\. *Megatron-lm: Training multi-billion parameter language models using
    model parallelism*. arXiv preprint arXiv:1909.08053.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖伊比，M.，帕特瓦里，M.，普里，R.，莱格雷斯利，P.，卡斯珀，J.，和卡坦扎罗，B.，2019\. *Megatron-lm：使用模型并行训练数十亿参数的语言模型*.
    arXiv预印本 arXiv:1909.08053.
- en: 'Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., and Stoyanov, V., 2019\. *RoBERTa: A Robustly Optimized BERT
    Pretraining Approach*. arXiv preprint arXiv:1907.11692.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘，Y.，奥特，M.，高亚尔，N.，杜，J.，乔希，M.，陈，D.，利维，O.，刘易斯，M.，泽特莱莫伊，L.，和斯托扬诺夫，V.，2019\. *RoBERTa：一种鲁棒优化的BERT预训练方法*.
    arXiv预印本 arXiv:1907.11692.
- en: Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
    Y., Li, W., and Liu, P. J., 2020\. *Exploring the limits of transfer learning
    with a unified text-to-text transformer*. J. Mach. Learn. Res., 21(140), pp.1-67.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉费尔，C.，沙泽尔，N.，罗伯茨，A.，李，K.，纳兰，S.，马特纳，M.，周，Y.，李，W.，和刘，P. J.，2020\. *探索统一文本到文本转换器在迁移学习中的极限*.
    J. Mach. Learn. Res.，21(140)，pp.1-67.
- en: 'Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R.,
    2019\. *Transformer-xl: Attentive language models beyond a fixed-length context*.
    arXiv preprint arXiv:1901.02860.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 戴，Z.，杨，Z.，杨，Y.，卡本内尔，J.，黎，Q. V.，和沙拉胡丁诺夫，R.，2019\. *Transformer-xl：超越固定长度上下文的注意力语言模型*.
    arXiv预印本 arXiv:1901.02860.
- en: 'Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q.
    V., 2019\. *XLNet: Generalized autoregressive pretraining for language understanding*.
    Advances in neural information processing systems, 32.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨，Z.，戴，Z.，杨，Y.，卡本内尔，J.，沙拉胡丁诺夫，R. R.，和黎，Q. V.，2019\. *XLNet：用于语言理解的泛化自回归预训练*.
    神经信息处理系统进展，32.
