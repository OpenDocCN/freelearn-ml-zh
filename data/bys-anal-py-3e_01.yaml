- en: Chapter 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章
- en: Programming Probabilistically
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概率编程
- en: Our golems rarely have a physical form, but they too are often made of clay
    living in silicon as computer code. – Richard McElreath
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的魔像很少有实体形式，但它们通常也由硅中生长的黏土做成，作为计算机代码存在。—— 理查德·麦克埃尔里斯
- en: Now that we have a very basic understanding of probability theory and Bayesian
    statistics, we are going to learn how to build probabilistic models using computational
    tools. Specifically, we are going to learn about probabilistic programming with
    PyMC [[Abril-Pla et al.](Bibliography.xhtml#Xpymc2023), [2023](Bibliography.xhtml#Xpymc2023)].
    The basic idea is that we use code to specify statistical models and then PyMC
    will solve those models for us. We will not need to write Bayes’ theorem in explicit
    form. This is a good strategy for two reasons. First, many models do not lead
    to an analytic closed form, and thus we can only solve those models using numerical
    techniques. Second, modern Bayesian statistics is mainly done by writing code.
    We will be able to see that probabilistic programming offers an effective way
    to build and solve complex models and allows us to focus more on model design,
    evaluation, and interpretation, and less on mathematical or computational details.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对概率理论和贝叶斯统计有了非常基础的理解，接下来我们将学习如何使用计算工具构建概率模型。具体来说，我们将学习如何使用 PyMC 进行概率编程 [[Abril-Pla
    等人](Bibliography.xhtml#Xpymc2023), [2023](Bibliography.xhtml#Xpymc2023)]。基本的思想是，我们通过代码来指定统计模型，然后
    PyMC 将为我们求解这些模型。我们不需要显式地写出贝叶斯定理。这样做有两个原因。首先，许多模型无法得到解析解，因此我们只能使用数值方法来求解这些模型。其次，现代贝叶斯统计主要通过编写代码来完成。我们将看到，概率编程提供了一种有效的方法来构建和求解复杂模型，它让我们能够更多地关注模型设计、评估和解释，而少关注数学或计算的细节。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Probabilistic programming
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率编程
- en: A PyMC primer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyMC 入门
- en: The coin-flipping problem revisited
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬币抛掷问题重访
- en: Summarizing the posterior
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结后验分布
- en: The Gaussian and Student t models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯模型和学生 t 分布模型
- en: Comparing groups and the effect size
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较不同组别和效应大小
- en: 2.1 Probabilistic programming
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 概率编程
- en: Bayesian statistics is conceptually very simple. We have the *knowns* and the
    *unknowns*, and we use Bayes’ theorem to condition the latter on the former. If
    we are lucky, this process will reduce the uncertainty about the *unknowns*. Generally,
    we refer to the *knowns* as **data** and treat it like constants, and the *unknowns*
    as **parameters** and treat them as *random variables*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯统计从概念上讲非常简单。我们有*已知*和*未知*，然后使用贝叶斯定理将后者条件化于前者。如果我们足够幸运，这个过程将减少对*未知*的 uncertainty。通常，我们把*已知*称为**数据**并将其视为常数，*未知*则称为**参数**并将其视为*随机变量*。
- en: Although conceptually simple, fully probabilistic models often lead to analytically
    intractable expressions. For many years, this was a real problem and one of the
    main issues that hindered the adoption of Bayesian methods beyond some niche applications.
    The arrival of the computational era and the development of numerical methods
    that, at least in principle, can be used to solve any inference problem, have
    dramatically transformed the Bayesian data analysis practice. We can think of
    these numerical methods as *universal inference engines*. The possibility of automating
    the inference process has led to the development of **probabilistic programming
    languages** (**PPLs**), which allows a clear separation between model creation
    and inference. In the PPL framework, users specify a full probabilistic model
    by writing a few lines of code, and then inference follows automatically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从概念上讲很简单，完全概率模型常常导致解析上不可处理的表达式。多年来，这一直是一个真正的问题，也是贝叶斯方法未能广泛应用于一些利基领域的主要原因之一。计算时代的到来和数值方法的发展，至少在原理上可以用来解决任何推断问题，已经极大地改变了贝叶斯数据分析的实践。我们可以将这些数值方法视为*通用推断引擎*。自动化推断过程的可能性催生了**概率编程语言**（**PPLs**），它使得模型创建和推断之间有了清晰的分离。在
    PPL 框架中，用户通过编写几行代码来指定完整的概率模型，然后推断过程会自动进行。
- en: It is expected that probabilistic programming will have a major impact on data
    science and other disciplines by enabling practitioners to build complex probabilistic
    models in a less time-consuming and less error-prone way. I think one good analogy
    for the impact that programming languages can have on scientific computing is
    the introduction of the Fortran programming language more than six decades ago.
    While nowadays Fortran has lost its shine, at one time, it was considered revolutionary.
    For the first time, scientists moved away from computational details and began
    focusing on building numerical methods, models, and simulations more naturally.
    It is interesting to see that some folks are working on making Fortran cool again,
    if you are interested you can check their work at [https://fortran-lang.org/en](https://fortran-lang.org/en).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预计概率编程将在数据科学和其他学科中产生重大影响，能够使从业人员以更省时且更少出错的方式构建复杂的概率模型。我认为，编程语言对科学计算的影响可以通过六十多年前
    Fortran 编程语言的引入来类比。虽然现在 Fortran 已经不再流行，但曾几何时，它被认为是革命性的。科学家们第一次摆脱了计算细节，开始更自然地专注于构建数值方法、模型和仿真。很有趣的是，有些人正在努力让
    Fortran 再度焕发光彩，如果你有兴趣，可以查看他们的工作：[https://fortran-lang.org/en](https://fortran-lang.org/en)。
- en: 2.1.1 Flipping coins the PyMC way
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 使用 PyMC 抛硬币
- en: 'Let’s revisit the coin-flipping problem from *Chapter [1](CH01.xhtml#x1-160001)*,
    but this time using PyMC. We will use the same synthetic data we used in that
    chapter. Since we are generating the data, we know the true value of *θ*, called
    `theta_real`, in the following block of code. Of course, for a real dataset, we
    will not have this knowledge:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下*第 [1](CH01.xhtml#x1-160001) 章*中的抛硬币问题，这次使用 PyMC。我们将使用与那一章中相同的合成数据。因为我们生成了数据，所以我们知道以下代码块中
    *θ* 的真实值，称为 `theta_real`。当然，对于实际数据集，我们没有这些知识：
- en: '**Code 2.1**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.1**'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have the data, we need to specify the model. Remember that this
    is done by specifying the likelihood and the prior. For the likelihood, we will
    use the Binomial distribution with parameters *n* = 1, *p* = *θ*, and for the
    prior, a Beta distribution with the parameters *α* = *β* = 1\. A Beta distribution
    with such parameters is equivalent to a Uniform distribution on the interval [0,
    1]. Using mathematical notation we can write the model as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，我们需要指定模型。记住，这是通过指定似然性和先验来完成的。对于似然性，我们将使用参数 *n* = 1，*p* = *θ* 的二项分布；对于先验，我们将使用参数
    *α* = *β* = 1 的 Beta 分布。具有此类参数的 Beta 分布等同于区间 [0, 1] 上的均匀分布。使用数学符号我们可以将模型写为：
- en: '![θ ∼ Beta(𝛼 = 1,𝛽 = 1) Y ∼ Binomial(n = 1,p = θ) ](img/file59.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![θ ∼ Beta(𝛼 = 1,𝛽 = 1) Y ∼ Binomial(n = 1,p = θ)](img/file59.jpg)'
- en: 'This statistical model has an almost one-to-one translation to PyMC:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个统计模型几乎可以直接翻译为 PyMC：
- en: '**Code 2.2**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.2**'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The first line of the code creates a container for our model. Everything inside
    the `with` block will be automatically added to `our_first_model`. You can think
    of this as syntactic sugar to ease model specification as we do not need to manually
    assign variables to the model. The second line specifies the prior. As you can
    see, the syntax follows the mathematical notation closely. The third line specifies
    the likelihood; the syntax is almost the same as for the prior, except that we
    pass the data using the `observed` argument. The observed values can be passed
    as a Python list, a tuple, a NumPy array, or a pandas DataFrame. With that, we
    are finished with the model specification! Pretty neat, right?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一行创建了我们模型的容器。`with` 块中的所有内容将自动添加到 `our_first_model` 中。你可以把这看作是一种语法糖，它简化了模型的指定，因为我们不需要手动将变量分配给模型。第二行指定了先验。如你所见，语法紧密遵循了数学符号。第三行指定了似然性；语法与先验几乎相同，唯一不同的是我们使用
    `observed` 参数传递数据。观测值可以作为 Python 列表、元组、NumPy 数组或 pandas DataFrame 传递。至此，我们完成了模型的指定！是不是很简洁？
- en: We still have one more line of code to explain. The last line is where the magic
    happens. Behind this innocent line, PyMC has hundreds of *oompa loompas* singing
    and baking a delicious Bayesian inference just for you! Well, not exactly, but
    PyMC is automating a lot of tasks. For the time being, we are going to treat that
    line as a black box that will give us the correct result. What is important to
    understand is that under the hood we will be using numerical methods to compute
    the posterior distribution. In principle, these numerical methods are capable
    of solving any model we can write. The cost we pay for this generality is that
    the solution is going to take the form of samples from the posterior. Later, we
    will be able to corroborate that these samples come from a Beta distribution,
    as we learned from the previous chapter. Because the numerical methods are stochastic,
    the samples will vary every time we run them. However, if the inference process
    works as expected, the samples will be representative of the posterior distribution
    and thus we will obtain the same conclusion from any of those samples. The details
    of what happens under the hood and how to check if the samples are indeed trustworthy
    will be explained in *Chapter [10](CH10.xhtml#x1-18900010)*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一行代码需要解释。最后一行就是魔法发生的地方。背后这个看似无害的代码行，PyMC 正在自动化许多任务，简直就像是有成百上千的 *oompa loompas*
    在为您唱歌并烤制美味的贝叶斯推断！嗯，虽然并非如此，但 PyMC 确实在自动化很多任务。暂时，我们将把这一行视作一个黑箱，它将为我们提供正确的结果。重要的是要理解，在底层，我们将使用数值方法来计算后验分布。原则上，这些数值方法能够解决我们可以编写的任何模型。我们为这种通用性所付出的代价是，结果将以来自后验的样本形式呈现。稍后，我们将能够证实这些样本来自一个
    Beta 分布，正如我们在上一章学到的那样。由于数值方法是随机的，每次运行时样本都会有所不同。然而，如果推断过程按预期进行，这些样本将代表后验分布，因此我们将从这些样本中得出相同的结论。有关底层发生的详细情况以及如何检查样本是否值得信任，将在
    *第 [10](CH10.xhtml#x1-18900010) 章* 中解释。
- en: 'One more thing: the `idata` variable is an `InferenceData` object, which is
    a container for all the data generated by PyMC. We will learn more about this
    later in this chapter.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事：`idata` 变量是一个 `InferenceData` 对象，它是一个容器，包含 PyMC 生成的所有数据。我们将在本章稍后学习更多有关此的内容。
- en: 'OK, so on the last line, we are asking for 1,000 samples from the posterior.
    If you run the code, you will get a message like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，在最后一行中，我们请求从后验中获得 1,000 个样本。如果您运行代码，您将看到类似这样的消息：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first and second lines tell us that PyMC has automatically assigned the
    NUTS sampler (one inference engine that works very well for continuous variables),
    and has used a method to initialize that sampler (these methods need some initial
    guess of where to start sampling). The third line says that PyMC will run four
    chains in parallel, thus we will get four independent samples from the posterior.
    As PyMC attempts to parallelize these chains across the available processors in
    your machine, we will get the four for the price of one. The exact number of chains
    is computed taking into account the number of processors in your machine; you
    can change it using the `chains` argument for the `sample` function. The next
    line tells us which variables are being sampled by which sampler. For this particular
    case, this line is not adding new information because NUTS is used to sample the
    only variable we have, *θ*. However, this is not always the case because PyMC
    can assign different samplers to different variables. PyMC has rules to ensure
    that each variable is associated with the best possible sampler. Users can manually
    assign samplers using the `step` argument of the `sample` function, but you will
    hardly need to do that.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行和第二行告诉我们，PyMC 已经自动分配了 NUTS 采样器（一个非常适合连续变量的推断引擎），并使用了一种方法来初始化该采样器（这些方法需要对开始采样的位置进行一些初步猜测）。第三行说
    PyMC 将并行运行四个链，因此我们将从后验中获得四个独立的样本。由于 PyMC 尝试将这些链并行化到机器上可用的处理器中，我们将以一个价格获得四个样本。链的确切数量是根据机器中的处理器数量计算的；您可以使用
    `chains` 参数来更改 `sample` 函数的链数。接下来的这一行告诉我们每个采样器正在采样哪些变量。对于这个特定的例子，这一行并没有添加新信息，因为
    NUTS 被用来采样我们唯一的变量 *θ*。然而，这并不总是这样，因为 PyMC 可以为不同的变量分配不同的采样器。PyMC 有规则来确保每个变量都与最佳的采样器关联。用户可以使用
    `sample` 函数的 `step` 参数手动分配采样器，但您几乎不需要这么做。
- en: Finally, the last line is a progress bar, with several related metrics indicating
    how fast the sampler is working, including the number of iterations per second.
    If you run the code, you will see the progress bar get updated really fast. Here,
    we are seeing the last stage when the sampler has finished its work. You will
    notice that we have asked for 1,000 samples, but PyMC is computing 8,000 samples.
    We have 1,000 draws per chain to tune the sampling algorithm (NUTS, in this example).
    These draws will be discarded by default; PyMC uses them to increase the efficiency
    and reliability of the sampling method, which are both important to obtain a useful
    approximation to the posterior. We also have 1,000 productive draws per chain
    for a total of 4,000\. These are the ones we are going to use as our posterior.
    We can change the number of tuning steps with the `tune` argument of the sample
    function and the number of draws with the `draw` argument.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一行是一个进度条，显示了与采样器工作速度相关的几个指标，包括每秒的迭代次数。如果你运行代码，你会看到进度条更新得非常快。在这里，我们看到的是最后阶段，即采样器已经完成了它的工作。你会注意到，我们要求了
    1,000 个样本，但 PyMC 实际上计算了 8,000 个样本。每条链有 1,000 个抽样用于调节采样算法（在本例中是 NUTS）。这些抽样默认会被丢弃；PyMC
    使用它们来提高采样方法的效率和可靠性，这对于获得有用的后验近似是非常重要的。每条链还有 1,000 个有效抽样，共计 4,000 个。这些是我们将用作后验分布的样本。我们可以通过
    `tune` 参数调整调节步骤的数量，通过 `draw` 参数调整抽样的数量。
- en: Faster Sampling
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 更快的采样
- en: Under the hood, PyMC uses PyTensor, a library that allows one to define, optimize,
    and efficiently evaluate mathematical expressions involving multi-dimensional
    arrays. PyTensor significantly enhances the speed and performance of PyMC. Despite
    the advantages, it’s worth noting that the samplers in PyMC are implemented in
    Python, which may result in slower execution at times. To address this limitation,
    PyMC allows external samplers. I recommend using nutpie, a sampler written in
    Rust. For more information on how to install and call nutpie from PyMC, please
    check *Chapter [10](CH10.xhtml#x1-18900010)*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，PyMC 使用 PyTensor，这是一个可以定义、优化和高效评估涉及多维数组的数学表达式的库。PyTensor 大大提高了 PyMC 的速度和性能。尽管有这些优势，但值得注意的是，PyMC
    中的采样器是用 Python 实现的，这有时可能导致执行速度较慢。为了应对这个限制，PyMC 允许使用外部采样器。我推荐使用 nutpie，一个用 Rust
    编写的采样器。有关如何从 PyMC 安装和调用 nutpie 的更多信息，请查阅 *第 [10](CH10.xhtml#x1-18900010)* 章。
- en: 2.2 Summarizing the posterior
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 总结后验分布
- en: 'Generally, the first task we will perform after sampling from the posterior
    is to check what the results look like. The `plot_trace` function from ArviZ is
    ideally suited to this task:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在从后验分布中采样后，我们执行的第一项任务是检查结果的表现。ArviZ 的 `plot_trace` 函数非常适合这项任务：
- en: '**Code 2.3**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.3**'
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![PIC](img/file60.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file60.png)'
- en: '**Figure 2.1**: A trace plot for the posterior of `our_first_model`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.1**：`our_first_model` 后验的追踪图'
- en: '*Figure [2.1](#x1-47005r1)* shows the default result when calling `az.plot_trace`;
    we get two subplots for each unobserved variable. The only unobserved variable
    in our model is *θ*. Notice that *y* is an observed variable representing the
    data; we do not need to sample that because we already know those values. Thus
    we only get two subplots. On the left, we have a **Kernel Density Estimation**
    (**KDE**) plot; this is like the smooth version of the histogram. Ideally, we
    want all chains to have a very similar KDE, like in *Figure [2.1](#x1-47005r1)*.
    On the right, we get the individual values at each sampling step; we get as many
    lines as chains. Ideally, we want it to be something that looks noisy, with no
    clear pattern, and we should have a hard time identifying one chain from the others.
    In *Chapter [10](CH10.xhtml#x1-18900010)*, we give more details on how to interpret
    these plots. The gist is that if we ran many chains, we would expect them to be
    practically indistinguishable from each other. The sampler did a good job and
    we can trust the samples.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [2.1](#x1-47005r1)* 显示了调用 `az.plot_trace` 时的默认结果；我们为每个未观察到的变量获得两个子图。我们模型中唯一未观察到的变量是
    *θ*。注意，*y* 是一个表示数据的观察变量；我们不需要对其进行采样，因为我们已经知道这些值。因此，我们只得到两个子图。左边是一个 **核密度估计**（**KDE**）图；这类似于直方图的平滑版本。理想情况下，我们希望所有链条的
    KDE 非常相似，就像 *图 [2.1](#x1-47005r1)* 中所示。右边，我们得到每个采样步骤的个体值；每条链条对应一条线。理想情况下，我们希望它看起来杂乱无章，没有明显的模式，我们应该很难区分不同的链条。在
    *第 [10](CH10.xhtml#x1-18900010)* 章中，我们提供了更多关于如何解读这些图表的细节。关键是，如果我们运行了许多链条，我们期望它们几乎无法区分。采样器表现得很好，我们可以信任这些样本。'
- en: As with other ArviZ functions, `az.plot_trace` has many options. For instance,
    we can run this function with the `combined` argument set to `True` to get a single
    KDE plot for all chains and with `kind=rank_bars` to get a **rank** **plot**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他 ArviZ 函数一样，`az.plot_trace` 也有许多选项。例如，我们可以通过设置 `combined=True` 参数来生成所有链的单个
    KDE 图，并通过设置 `kind=rank_bars` 来生成一个 **排名** **图**。
- en: '**Code 2.4**:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.4**：'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![PIC](img/file61.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file61.png)'
- en: '**Figure 2.2**: A trace plot for the posterior of `our_first_model`, using
    the options `kind="rank_bars"`, `combined=True`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.2**：`our_first_model` 后验的痕迹图，使用了选项 `kind="rank_bars"` 和 `combined=True`。'
- en: A rank plot is another way to check if we can trust the samples; for this plot,
    we get one histogram per chain and we want all of them to be as uniform as possible,
    like in *Figure [2.2](#x1-47009r2)*. Some small deviations for uniformity are
    expected due to random sampling, but large deviations from uniformity are a signal
    that chains are exploring different regions of the posteriors. Ideally, we want
    all chains to explore the entire posterior. In *Chapter [10](CH10.xhtml#x1-18900010)*,
    we provide further details on how to interpret rank plots and how they are constructed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 排名图是另一种检查我们是否可以信任样本的方法；对于这个图，我们为每条链生成一个直方图，并希望它们尽可能均匀，像在 *图 [2.2](#x1-47009r2)*
    中那样。由于随机抽样，一些小的均匀性偏差是可以接受的，但大的均匀性偏差则表明链正在探索后验的不同区域。理想情况下，我们希望所有的链都能探索整个后验。在 *第
    [10](CH10.xhtml#x1-18900010) 章* 中，我们提供了如何解释排名图及其构建方法的更多细节。
- en: ArviZ provides several other plots to help interpret the posterior, and we will
    see them in the following pages. We may also want to have a numerical summary
    of the posterior. We can get that using `az.summary`, which will return a pandas
    DataFrame as shown in *Table [2.1](#x1-47014r1)*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ArviZ 提供了几种其他的图表来帮助解释后验，我们将在接下来的页面中看到它们。我们也可能希望获得后验的数值总结。我们可以使用 `az.summary`
    来实现，它将返回一个 pandas DataFrame，如 *表 [2.1](#x1-47014r1)* 所示。
- en: '**Code 2.5**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.5**'
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  | mean | sd | hdi_3% | hdi_97% |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | 均值 | 标准差 | hdi_3% | hdi_97% |'
- en: '| *θ* | 0.34 | 0.18 | 0.03 | 0.66 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| *θ* | 0.34 | 0.18 | 0.03 | 0.66 |'
- en: '**Table 2.1**: Summary statistics'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2.1**：总结统计数据'
- en: On the first column we have the name of the variable, the second column is the
    mean of the posterior, the third column is the standard deviation of the posterior,
    and the last two columns are the lower and upper boundaries of the 94% highest
    density interval. Thus, according to our model and data, we think the value of
    *θ* is likely to be 0.34 with a 94% probability that it is actually between 0.03
    and 0.66\. We can report a similar summary using the standard deviation. The advantage
    of the standard deviation over the HDI is that it is a more popular statistic.
    As a disadvantage, we have to be more careful interpreting it; otherwise, it can
    lead to meaningless results. For example, if we compute the mean ± 2 standard
    deviations, we will get the intervals (-0.02, 0.7); the upper value is not that
    far from 0.66, which we got from the HDI, but the lower bound is actually outside
    the possible values of *θ*, which is between 0 and 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一列，我们有变量的名称，第二列是后验的均值，第三列是后验的标准差，最后两列是 94% 最高密度区间的下限和上限。因此，根据我们的模型和数据，我们认为
    *θ* 的值很可能是 0.34，并且以 94% 的概率，它的真实值在 0.03 和 0.66 之间。我们也可以使用标准差报告一个类似的总结。标准差相比 HDI
    的优点是它是一种更常见的统计量。但缺点是我们需要更小心地解释它，否则可能会得出毫无意义的结果。例如，如果我们计算均值 ± 2 个标准差，我们将得到区间 (-0.02,
    0.7)；上限值与我们从 HDI 得到的 0.66 相差不远，但下限实际上超出了 *θ* 的可能值（它应该在 0 和 1 之间）。
- en: Another way to visually summarize the posterior is to use the `az.plot_posterior`
    function that comes with ArviZ (see *Figure [2.3](#x1-47019r3)*). We used this
    function in the previous chapter for a fake posterior. We are going to use it
    now for a real posterior.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种直观总结后验的方法是使用 ArviZ 中的 `az.plot_posterior` 函数（见 *图 [2.3](#x1-47019r3)*）。我们在前一章中已经用它处理了一个虚假的后验。现在，我们将使用它来处理一个真实的后验。
- en: '**Code 2.6**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.6**'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![PIC](img/file62.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file62.png)'
- en: '**Figure 2.3**: The plot shows the posterior distribution of *θ* and the 94%
    HDI'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.3**：该图显示了 *θ* 的后验分布及 94% HDI。'
- en: By default, `plot_posterior` shows a histogram for discrete variables and KDEs
    for continuous variables. We also get the mean of the distribution (we can ask
    for the median or mode using the `point_estimate` argument) and the 94% HDI as
    a black line at the bottom of the plot. Different interval values can be set for
    the HDI with the `hdi_prob` argument. This type of plot was introduced by John
    K. Kruschke in his great book Doing Bayesian Data Analysis [[Kruschke](Bibliography.xhtml#Xkruschke_2014), [2014](Bibliography.xhtml#Xkruschke_2014)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`plot_posterior` 为离散变量显示直方图，为连续变量显示核密度估计（KDE）。我们还可以得到分布的均值（可以通过`point_estimate`参数请求中位数或众数）和94%的HDI，该黑线位于图表底部。可以使用`hdi_prob`参数为HDI设置不同的区间值。这种类型的图表是由John
    K. Kruschke在他的伟大著作《Doing Bayesian Data Analysis》中首次提出的[[Kruschke](Bibliography.xhtml#Xkruschke_2014),
    [2014](Bibliography.xhtml#Xkruschke_2014)]。
- en: 2.3 Posterior-based decisions
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 基于后验的决策
- en: 'Sometimes, describing the posterior is not enough. We may need to make decisions
    based on our inferences and reduce a continuous estimation to a dichotomous one:
    yes-no, healthy-sick, contaminated-safe, and so on. For instance, is the coin
    fair? A fair coin is one with a *θ* value of exactly 0.5\. We can compare the
    value of 0.5 against the HDI interval. From *Figure [2.3](#x1-47019r3)*, we can
    see that the HDI goes from 0.03 to 0.7 and hence 0.5 is included in the HDI. We
    can interpret this as an indication that the coin may be tail-biased, but we cannot
    completely rule out the possibility that the coin is actually fair. If we want
    a sharper decision, we will need to collect more data to reduce the spread of
    the posterior, or maybe we need to find out how to define a more informative prior.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅仅描述后验是不够的。我们可能需要基于推断做出决策，并将连续估计转化为二分法：是-否，健康-生病，污染-安全，等等。例如，硬币是否公平？一个公平的硬币是*θ*值恰好为0.5的硬币。我们可以将0.5的值与HDI区间进行比较。从*图
    [2.3](#x1-47019r3)*中，我们可以看到HDI区间从0.03到0.7，因此0.5包含在HDI内。我们可以将此解读为硬币可能有尾部偏向，但我们不能完全排除硬币实际上是公平的可能性。如果我们希望做出更明确的决策，我们需要收集更多的数据来减少后验的分布范围，或者可能需要重新定义一个更具信息性的先验。
- en: 2.3.1 Savage-Dickey density ratio
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Savage-Dickey 密度比
- en: 'One way to evaluate how much support the posterior provides for a given value
    is to compare the ratio of the posterior and prior densities at that value. This
    is called the Savage-Dickey density ratio and we can compute it with ArviZ using
    the `az.plot_bf` function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 评估后验提供给某个特定值的支持力度的一种方法是比较该值下后验密度和先验密度的比值。这称为Savage-Dickey密度比，我们可以使用ArviZ通过`az.plot_bf`函数来计算：
- en: '**Code 2.7**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.7**'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![PIC](img/file63.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file63.png)'
- en: '**Figure 2.4**: The plot shows the prior and posterior for `our_first_model`;
    the black dots represent their values evaluated at the reference value 0.5'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.4**：该图显示了`our_first_model`的先验和后验；黑色点表示在参考值0.5处评估的值'
- en: 'From *Figure [2.4](#x1-49008r4)*, we can see that the value of `BF_01` is 1.3,
    which means that the value of *θ* = 0*.*5 is 1.3 times more likely under the posterior
    distribution than under the prior distribution. To compute this value we just
    divided the height of the posterior at *θ* = 0*.*5 by the height of the prior
    at *θ* = 0*.*5\. The value of `BF_10` is just the inverse ![-1- 1.3](img/file64.jpg)
    ≈ 0*.*8\. We can think of this as the value of **θ*≠*0*.*5 being 0.76 times more
    likely under the posterior than under the prior. How do we interpret these numbers?
    With a pinch of salt...the following table shows one possible interpretation originally
    proposed by [Kass and Raftery](Bibliography.xhtml#Xkass_1995) [[1995](Bibliography.xhtml#Xkass_1995)]:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 [2.4](#x1-49008r4)*中我们可以看到，`BF_01`的值为1.3，这意味着在后验分布下，*θ* = 0.5的值比在先验分布下更有可能，概率是1.3倍。为了计算这个值，我们将后验在*θ*
    = 0.5处的高度除以前验在*θ* = 0.5处的高度。`BF_10`的值则是其倒数！[-1- 1.3](img/file64.jpg) ≈ 0.8。我们可以理解为在后验分布下，**θ≠0.5**的可能性是先验分布下的0.76倍。我们如何解读这些数字？要带有一丝怀疑……以下表格展示了[Kass
    和 Raftery](Bibliography.xhtml#Xkass_1995)在[1995](Bibliography.xhtml#Xkass_1995)年提出的其中一种可能的解释：
- en: '| BF_01 | Interpretation |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| BF_01 | 解释 |'
- en: '| 1 to 3.2 | Not worth more than a bare mention |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1 到 3.2 | 不值一提 |'
- en: '| 3.2 to 10 | Substantial |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 3.2 到 10 | 实质性 |'
- en: '| 10 to 100 | Strong |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 10 到 100 | 强 |'
- en: '| *>* 100 | Decisive |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| *>* 100 | 决定性 |'
- en: '**Table 2.2**: Interpretation of Bayes Factors (BF_01)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2.2**：贝叶斯因子（BF_01）的解释'
- en: The Savage-Dickey density ratio is a particular way to compute what is called
    the Bayes Factor. We will learn more about Bayes Factors, and their caveats, in
    *Chapter [5](CH05.xhtml#x1-950005)*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Savage-Dickey 密度比率是一种特定的计算方法，用来求解所谓的贝叶斯因子。我们将在 *第 [5](CH05.xhtml#x1-950005)
    章* 中学习更多关于贝叶斯因子及其注意事项。
- en: 2.3.2 Region Of Practical Equivalence
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 实际等价区域
- en: 'Strictly speaking, the chance of observing exactly 0.5 (that is, with infinite
    trailing zeros) is zero. Also, in practice, we generally do not care about exact
    results but results within a certain margin. Accordingly, in practice, we can
    relax the definition of fairness and we can say that a fair coin is one with a
    value of *around* 0.5\. For example, we could say that any value in the interval
    [0.45, 0.55] will be, for our purposes, practically equivalent to 0.5\. We call
    this interval a Region Of Practical Equivalence (ROPE). Once the ROPE is defined,
    we compare it with the HDI. We can get at least three scenarios:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，观察到精确的 0.5（即带有无限多个零）的概率为零。此外，在实践中，我们通常不关心精确结果，而是关心某个范围内的结果。因此，在实践中，我们可以放宽对公平性的定义，认为一个公平的硬币的值应该是
    *接近* 0.5。例如，我们可以说，区间 [0.45, 0.55] 内的任何值对于我们的目的来说，实际上都等同于 0.5。我们称这个区间为实际等价区域（ROPE）。一旦定义了
    ROPE，我们就可以将其与 HDI 进行比较。我们可以得到至少三种情况：
- en: The ROPE does not overlap the HDI; we can say the coin is not fair
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROPE 不与 HDI 重叠；我们可以说硬币是不公平的
- en: The ROPE contains the entire HDI; we can say the coin is fair
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROPE 包含了整个 HDI；我们可以说硬币是公平的
- en: The ROPE partially overlaps HDI; we cannot say the coin is fair or unfair
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROPE 部分与 HDI 重叠；我们不能说硬币是公平的还是不公平的
- en: If we choose the ROPE to match the support of a parameter, like [0, 1] for the
    coin-flipping example, we will always say we have a fair coin. Notice that we
    do not need to collect data to perform any type of inference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择将 ROPE 与参数的支持区域匹配，例如在硬币投掷示例中使用 [0, 1]，我们将总是认为硬币是公平的。请注意，我们不需要收集数据来进行任何推断。
- en: 'The choice of ROPE is completely arbitrary: we can choose any value we want.
    Some choices are not very useful. If, for the coin-flipping example, we choose
    the ROPE to be [0, 1], then we will always say the coin is fair. Even more, we
    don’t need to collect data or perform any analysis to reach this conclusion, this
    is a trivial example. More worrisome is to pick the ROPE after performing the
    analysis. This is problematic because we can accommodate the results to say whatever
    we want them to say. But why do we even bother to do an analysis, if we are going
    to accommodate the result to our expectations? The ROPE should be informed from
    domain knowledge.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ROPE 的选择完全是任意的：我们可以选择任何我们想要的值。有些选择并没有什么实际意义。例如，对于硬币投掷示例，如果我们选择 ROPE 为 [0, 1]，那么我们将总是认为硬币是公平的。更重要的是，我们不需要收集数据或进行任何分析来得出这个结论，这是一个微不足道的例子。更值得担心的是，在进行分析之后选择
    ROPE。这是有问题的，因为我们可以调整结果来使它符合我们想要的结论。但如果我们要调整结果以符合我们的预期，那我们为什么还要做分析呢？ROPE 应该由领域知识来决定。
- en: 'We can use the `plot_posterior` function to plot the posterior with the HDI
    interval and the ROPE. The ROPE appears as a semi-transparent thick (gray) line:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `plot_posterior` 函数来绘制包含 HDI 区间和 ROPE 的后验分布。ROPE 显示为半透明的粗（灰色）线：
- en: '**Code 2.8**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.8**'
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![PIC](img/file65.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file65.png)'
- en: '**Figure 2.5**: The plot shows the posterior distribution of *θ* and the 94%
    HDI. The ROPE is shown as a thick light-gray line'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.5**：该图展示了 *θ* 的后验分布和 94% HDI。ROPE 以一条粗的浅灰色线表示'
- en: 'Another tool we can use to help us make a decision is to compare the posterior
    against a reference value. We can do this using `plot_posterior`. As you can see
    in *Figure [2.6](#x1-50011r6)*, we get a vertical (gray) line and the proportion
    of the posterior above and below our reference value:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们可以用来帮助决策的工具是将后验分布与参考值进行比较。我们可以使用 `plot_posterior` 来实现。如 *图 [2.6](#x1-50011r6)*
    所示，我们得到一条垂直的（灰色）线，并且可以看到后验分布在参考值之上和之下的比例：
- en: '**Code 2.9**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.9**'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![PIC](img/file66.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file66.png)'
- en: '**Figure 2.6**: The plot shows the posterior distribution of *θ* and the 94%
    HDI. The reference value is shown as a gray vertical line'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.6**：该图展示了 *θ* 的后验分布和 94% HDI。参考值显示为灰色垂直线'
- en: For a more detailed discussion on the use of the ROPE, you could read Chapter
    12 of Doing Bayesian Data Analysis by [Kruschke](Bibliography.xhtml#Xkruschke_2014) [[2014](Bibliography.xhtml#Xkruschke_2014)].
    That chapter also discusses how to perform hypothesis testing in a Bayesian framework
    and the caveats of hypothesis testing, whether in a Bayesian or non-Bayesian setting.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 ROPE 使用的更详细讨论，可以阅读 [Kruschke](Bibliography.xhtml#Xkruschke_2014) 的《Doing
    Bayesian Data Analysis》第12章[[2014](Bibliography.xhtml#Xkruschke_2014)]。该章节还讨论了如何在贝叶斯框架中进行假设检验及其注意事项，无论是在贝叶斯还是非贝叶斯设置下。
- en: 2.3.3 Loss functions
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 损失函数
- en: 'If you think these ROPE rules sound a little bit clunky and you want something
    more formal, loss functions are what you are looking for! To make a good decision,
    it is important to have the highest possible level of precision for the estimated
    value of the relevant parameters, but it is also important to take into account
    the cost of making a mistake. The cost/benefit trade-off can be mathematically
    formalized using loss functions. The names for loss functions or their inverses
    vary across different fields, and we could find names such as cost functions,
    objective functions, fitness functions, utility functions, and so on. No matter
    the name, the key idea is to use a function that captures how different the true
    value and the estimated value of a parameter are. The larger the value of the
    loss function, the worse the estimation is (according to the loss function). Some
    common examples of loss functions are:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这些 ROPE 规则听起来有些繁琐，并且想要更正式的东西，损失函数就是你所寻找的！为了做出一个好的决策，重要的是对相关参数的估计值具有尽可能高的精度，但同样重要的是要考虑犯错的代价。成本/收益的权衡可以通过数学化的方式使用损失函数来形式化。损失函数或其逆的名称在不同领域中各不相同，我们可能会遇到诸如成本函数、目标函数、适应度函数、效用函数等名称。无论名称如何，关键思想是使用一个函数来捕捉参数的真实值和估计值之间的差异。损失函数的值越大，估计结果越差（根据损失函数的定义）。一些常见的损失函数示例如下：
- en: The absolute loss function, |*θ* − ![](img/hat_theta.png)|
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绝对损失函数，|*θ* − ![](img/hat_theta.png)|
- en: The quadratic loss function, (*θ* − ![](img/hat_theta.png))²
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二次损失函数，(*θ* − ![](img/hat_theta.png))²
- en: The 0-1 loss function, 1(**θ* ≠* ![](img/hat_theta.png)), where ![](img/one.PNG)
    is the indicator function
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0-1 损失函数，1(**θ* ≠* ![](img/hat_theta.png))，其中 ![](img/one.PNG) 是指示函数
- en: In practice, we don’t know the value of the true parameter. Instead, we have
    an estimation in the form of a posterior distribution. Thus, what we can do is
    find out the value of *θ* that minimizes the expected loss function. By expected
    loss function, we mean the loss function averaged over the whole posterior distribution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们并不知道真实参数的值。相反，我们有一个后验分布形式的估计。因此，我们可以做的是找出一个 *θ*，使其最小化期望损失函数。所谓的期望损失函数是指在整个后验分布上平均的损失函数。
- en: 'In the following block of code, we have two loss functions: the absolute loss
    (`lossf_a`) and the quadratic loss (`lossf_b`). We will explore the value of over
    a grid of 200 points. We will then plot those curves and we will also include
    the value of *θ* that minimizes each loss function. The following block shows
    the Python code without the plotting part:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们有两个损失函数：绝对损失（`lossf_a`）和二次损失（`lossf_b`）。我们将探索一个包含 200 个点的网格上的值。然后，我们会绘制这些曲线，并且还会包括最小化每个损失函数的
    *θ* 值。以下代码块展示了没有绘图部分的 Python 代码：
- en: '**Code 2.10**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.10**'
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![PIC](img/file67.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file67.png)'
- en: '**Figure 2.7**: The absolute (black) and quadratic (gray) loss functions applied
    to the posterior from `our_first_model`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.7**：应用于 `our_first_model` 后验的绝对（黑色）和二次（灰色）损失函数'
- en: 'What is more interesting from *Figure [2.7](#x1-51011r7)* is that the value
    we got from the absolute loss is equal to the median of the posterior and the
    one we got from the quadratic loss is equal to the mean of the posterior. You
    can check this for yourself by computing `np.mean(`*θ*`_pos)`, `np.median(`*θ*`_pos)`.
    This is no coincidence: different loss functions are related to different point
    estimates. The mean is the point estimate that minimizes the quadratic loss, the
    median, the absolute loss, and the mode, the 1-0 loss.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是，从*图 [2.7](#x1-51011r7)*可以看到，我们从绝对损失得到的值等于后验分布的中位数，而从二次损失得到的值等于后验分布的均值。你可以通过计算`np.mean(`*θ*`_pos)`，`np.median(`*θ*`_pos)`来验证这一点。这并非巧合：不同的损失函数与不同的点估计有关。均值是最小化二次损失的点估计，中位数是最小化绝对损失的点估计，而众数是最小化
    1-0 损失的点估计。
- en: 'If we want to be formal and we want to compute a single-point estimate, we
    must decide which loss function we want. Conversely, if we choose a point estimate,
    we are implicitly (and maybe unconsciously) choosing a loss function. The advantage
    of explicitly choosing a loss function is that we can tailor the function to our
    problem instead of using a predefined rule. It is very common to observe that
    the cost of making a decision is asymmetric; for example, vaccines can produce
    an overreaction of the immune system, but the benefit to the vaccinated persons
    and even non-vaccinated persons overcomes the risk, usually by many orders of
    magnitude. Thus, if our problem demands it, we can construct an asymmetric loss
    function. It is also important to notice that, as the posterior is in the form
    of numerical samples, we can compute complex loss functions that don’t need to
    be restricted by mathematical convenience or mere simplicity. The following code,
    and *Figure [2.8](#x1-51021r8)* generated from it, is just a silly example of
    this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想正式一点，并且想要计算一个单点估计值，我们必须决定选择哪个损失函数。相反地，如果我们选择一个点估计值，我们实际上（可能是无意识地）选择了一个损失函数。显式选择损失函数的优点是我们可以根据自己的问题量身定制函数，而不是使用预定义的规则。我们常常会发现，做决策的成本是非对称的；例如，疫苗可能会引起免疫系统的过度反应，但接种疫苗的人，甚至未接种疫苗的人，所获得的益处通常远远超过风险，差距通常很大。因此，如果我们的任务要求，我们可以构建一个非对称的损失函数。还需要注意的是，由于后验分布是数值样本的形式，我们可以计算复杂的损失函数，这些函数不必受限于数学便利性或单纯的简化。下面的代码，以及由它生成的*图
    2.8*，就是这个概念的一个简单示例：
- en: '**Code 2.11**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.11**'
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![PIC](img/file68.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file68.png)'
- en: '**Figure 2.8**: A weird loss function applied to the posterior from `our_first_model`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.8**：应用于`our_first_model`后验分布的一个奇怪损失函数'
- en: Up until now, we have been discussing the main notions of Bayesian statistics
    and probabilistic programming using the BetaBinomial model mainly because of its
    simplicity. In our path to build more complex models, we now shift our focus to
    delve into the realm of Gaussian inference.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们一直在讨论贝叶斯统计和概率编程的主要概念，主要通过 BetaBinomial 模型，因为它的简洁性。在构建更复杂模型的过程中，我们现在将焦点转向高斯推理的领域。
- en: 2.4 Gaussians all the way down
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 高斯分布的深入探索
- en: Gaussians are very appealing from a mathematical point of view. Working with
    them is relatively easy, and many operations applied to Guassians return another
    Gaussian. Additionally, many natural phenomena can be nicely approximated using
    Gaussians; essentially, almost every time that we measure the average of something,
    using a *big enough* sample size, that average will be distributed as a Gaussian.
    The details of when this is true, when this is not true, and when this is more
    or less true, are elaborated in the **central limit theorem** (CLT); you may want
    to stop reading now and search about this really *central* statistical concept
    (terrible pun intended).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，高斯分布非常吸引人。与高斯分布进行工作相对容易，许多应用于高斯分布的操作会返回另一个高斯分布。此外，许多自然现象可以使用高斯分布进行很好的近似；实际上，几乎每次我们测量某物的平均值时，只要样本量*足够大*，该平均值就会呈高斯分布。这种现象何时成立、何时不成立以及何时大致成立的细节在**中心极限定理**（CLT）中有详细阐述；你可能现在就想停下来，去查找一下这个非常*核心*的统计学概念（有意的恶搞）。
- en: Well, we were saying that many phenomena are indeed averages. Just to follow
    a cliché, the height (and almost any other trait of a person, for that matter)
    is the result of many environmental factors and many genetic factors, and hence
    we get a nice Gaussian distribution for the height of adult people. Indeed, we
    get a mixture of two Gaussians, which is the result of overlapping the distribution
    of heights of women and men, but you get the idea. In summary, Gaussians are easy
    to work with and abundant in natural phenomena; hence, many of the statistical
    methods you may already know assume normality. Thus, it is important to learn
    how to build these models, and then it is equally important to learn how to relax
    the normality assumptions, something that is surprisingly easy in a Bayesian framework
    and with modern computational tools such as PyMC.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们刚才说过，许多现象确实是平均值的结果。举个老套的例子，身高（几乎所有人的其他特征也是如此）是许多环境因素和许多遗传因素的综合结果，因此我们可以得到成年人的身高符合高斯分布。事实上，我们得到的是两个高斯分布的混合，这是女性和男性身高分布重叠的结果，但你大概明白了。总之，高斯分布是易于使用且在自然现象中普遍存在的；因此，许多你可能已经了解的统计方法假设数据是符合正态分布的。因此，学习如何建立这些模型很重要，而同样重要的是学会如何放宽正态性假设，这在贝叶斯框架和现代计算工具如PyMC中是非常容易的。
- en: 2.4.1 Gaussian inferences
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 高斯推断
- en: Nuclear magnetic resonance (NMR) is a powerful technique used to study molecules
    and also living things such as humans, sunflowers, and yeast (because, after all,
    *we are just a bunch of molecules*). NMR allows you to measure different kinds
    of observable quantities related to interesting unobservable molecular properties
    [Arroyuelo et al.](Bibliography.xhtml#Xarroyuelo_2021) [[2021](Bibliography.xhtml#Xarroyuelo_2021)].
    One of these observables is known as chemical shift, which we can only get for
    the nuclei of certain types of atoms. The details belong to quantum chemistry
    and they are irrelevant to this discussion. For all we care at the moment, we
    could have been measuring the height of a group of people, the average time to
    travel back home, or the weights of bags of oranges. In these examples the variables
    are continuous, and it makes sense to think of them as an average value plus a
    dispersion. Sometimes we can use a Gaussian model for discrete variables if the
    number of possible values is large enough; for example, bonobos are very promiscuous,
    so maybe we can model the number of sexual partners of our cousins with a Gaussian.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 核磁共振（NMR）是一种强大的技术，用于研究分子以及诸如人类、向日葵和酵母等生物（毕竟，*我们不过是一堆分子*）。NMR使得我们能够测量与有趣的不可观察分子性质相关的不同种类的可观察量
    [Arroyuelo 等人](Bibliography.xhtml#Xarroyuelo_2021) [[2021](Bibliography.xhtml#Xarroyuelo_2021)]。其中一种可观察量被称为化学位移，这个值仅能通过某些类型原子的核来获取。具体的细节属于量子化学的范畴，跟本讨论无关。就我们目前关心的来说，我们也可以测量一群人的身高、回家所需的平均时间或一袋袋橙子的重量。在这些例子中，变量是连续的，因此把它们看作是一个平均值加上一个离散度是有意义的。有时如果可能的值足够多，我们可以用高斯模型来处理离散变量；例如，倭黑猩猩非常好色，所以也许我们可以用高斯分布来建模我们表亲的性伴侣数量。
- en: Going back to our example, we have 48 chemical shift values represented in a
    boxplot in *Figure [2.9](#x1-53004r9)*. We can see that the median (the line inside
    the box) is around 53 and the interquartile range (the box) is around 52 and 55\.
    We can see that there are two values far away from the rest of the data (empty
    circles).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，我们在*图 [2.9](#x1-53004r9)* 中通过箱型图表示了48个化学位移值。我们可以看到，中位数（箱内的线）大约是53，而四分位距（箱子的范围）大约是52和55。我们还可以看到，有两个值远离其余数据（空心圆）。
- en: '![PIC](img/file69.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file69.png)'
- en: '**Figure 2.9**: Boxplot of the 48 chemical shift values. We observed two values
    above 60, far away from the rest of the data'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.9**：48个化学位移值的箱型图。我们观察到有两个值超过60，远离其余数据。'
- en: 'Let’s forget about those two points for a moment and assume that a Gaussian
    distribution is a good description of the data. Since we do not know the mean
    or the standard deviation, we must set priors for both of them. Therefore, a reasonable
    model could be:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时忘记这两个点，假设高斯分布是描述数据的一个合理模型。由于我们不知道均值或标准差，因此必须为它们设置先验分布。因此，一个合理的模型可以是：
- en: '![μ ∼ 𝒰 (l,h ) σ ∼ ℋ 𝒩 (σσ) Y ∼ 𝒩 (μ, σ) ](img/file70.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![μ ∼ 𝒰 (l,h ) σ ∼ ℋ 𝒩 (σσ) Y ∼ 𝒩 (μ, σ) ](img/file70.jpg)'
- en: '![](img/U.PNG)(*l,h*) is the Uniform distribution between *l*, and *h*, ![](img/HN.PNG)(*σ*[*σ*])
    is the HalfNormal distribution with scale *σ*[*σ*], and ![](img/N.PNG)(*μ,σ*)
    is the Gaussian distribution with mean *μ* and standard deviation *σ*. A HalfNormal
    distribution considers the absolute values of Normal distribution centered around
    zero. *Figure [2.10](#x1-53005r10)* shows the graphical representation of this
    model.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/U.PNG)(*l,h*)是*l*和*h*之间的均匀分布，![](img/HN.PNG)(*σ*[*σ*])是带有尺度*σ*[*σ*]的HalfNormal分布，![](img/N.PNG)(*μ,σ*)是均值为*μ*，标准差为*σ*的高斯分布。HalfNormal分布考虑的是以零为中心的正态分布的绝对值。*图
    [2.10](#x1-53005r10)*展示了该模型的图形表示。'
- en: '![PIC](img/file71.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file71.png)'
- en: '**Figure 2.10**: Graphical representation of `model_g`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.10**: `model_g`的图形表示'
- en: 'If we do not know the possible values of *μ* and *σ*, we can set priors reflecting
    our ignorance. One option is to set the boundaries of the Uniform distribution
    to be *l* = 40, *h* = 75, which is a range that is larger than the range of the
    data. Alternatively, we can choose a range based on our previous knowledge. For
    instance, we may know that this is not physically possible to have values below
    0 or above 100 for this type of measurement and thus use those values as the boundaries
    of the Uniform distribution. For the HalfNormal, and in the absence of more information,
    we can choose a large value compared to the scale of the data. The PyMC code for
    the model represented in *Figure [2.10](#x1-53005r10)* is:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不知道*μ*和*σ*的可能值，可以设置反映我们无知的先验分布。一种选择是将均匀分布的边界设置为*l* = 40，*h* = 75，这是一个比数据范围更大的范围。另一种选择是根据我们之前的知识选择一个范围。例如，我们可能知道这种类型的测量值不可能低于0或高于100，因此可以将这些值作为均匀分布的边界。对于HalfNormal分布，在没有更多信息的情况下，我们可以选择一个相对于数据规模较大的值。表示在*图
    [2.10](#x1-53005r10)*中模型的PyMC代码是：
- en: '**Code 2.12**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.12**'
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s see what the posterior looks like. *Figure [2.11](#x1-53013r11)* was generated
    with the ArviZ function `plot_trace`. It has one row for each parameter. For this
    model, the posterior is bidimensional, so each row shows one marginal distribution.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看后验分布的形态。*图 [2.11](#x1-53013r11)*是通过ArviZ的`plot_trace`函数生成的。它每一行对应一个参数。对于该模型，后验分布是二维的，因此每一行展示一个边际分布。
- en: '![PIC](img/file72.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file72.png)'
- en: '**Figure 2.11**: Posterior from `model_g` ploted using `az.plot_trace(idata_g)`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.11**: 使用`az.plot_trace(idata_g)`绘制的`model_g`后验分布'
- en: 'We can use the `plot_pair` function from ArviZ to see what the bi-dimensional
    posterior looks like, together with the marginal distributions for *μ* and *σ*.
    See *Figure [2.12](#x1-53014r12)*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用ArviZ的`plot_pair`函数查看二维后验分布的形态，并同时查看*μ*和*σ*的边际分布。请参见*图 [2.12](#x1-53014r12)*：
- en: '![PIC](img/file73.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file73.png)'
- en: '**Figure 2.12**: Posterior from `model_g` ploted using `az.plot_pair(idata_g,
    kind=’kde’, marginals=True)`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.12**: 使用`az.plot_pair(idata_g, kind=’kde’, marginals=True)`绘制的`model_g`后验分布'
- en: 'We are going to print the summary for later use (see *Table [2.3](#x1-53018r3)*).
    We use the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将打印汇总以供以后使用（参见*表 [2.3](#x1-53018r3)*）。我们使用以下代码：
- en: '**Code 2.13**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.13**'
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|  | mean | sd | hdi_3% | hdi_97% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 均值 | 标准差 | hdi_3% | hdi_97% |'
- en: '| *μ* | 53.50 | 0.52 | 52.51 | 54.44 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| *μ* | 53.50 | 0.52 | 52.51 | 54.44 |'
- en: '| *σ* | 3.52 | 0.38 | 2.86 | 4.25 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| *σ* | 3.52 | 0.38 | 2.86 | 4.25 |'
- en: '**Table 2.3**: Summary statistics for *μ* and *σ*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2.3**: *μ*和*σ*的汇总统计'
- en: 2.5 Posterior predictive checks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 后验预测检查
- en: 'One of the nice elements of the Bayesian toolkit is that once we have a posterior
    *p*(*θ*|*Y* ), it is possible to use it to generate predictions *p*(*Ỹ*). Mathematically,
    this can be done by computing:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯工具包的一个优点是，一旦我们得到了后验*p*(*θ*|*Y* )，就可以用它来生成预测*p*(*Ỹ*)。在数学上，这可以通过计算来完成：
- en: '![ ∫ p(˜Y | Y ) = p(˜Y | θ) p(θ | Y )dθ ](img/file74.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ p(˜Y | Y ) = p(˜Y | θ) p(θ | Y )dθ ](img/file74.jpg)'
- en: This distribution is known as the **posterior predictive distribution**. It
    is *predictive* because it is used to make predictions, and *posterior* because
    it is computed using the posterior distribution. So we can think of this as the
    distribution of future data given the model, and observed data.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布被称为**后验预测分布**。它是*预测性的*，因为它用于进行预测，并且是*后验的*，因为它是使用后验分布计算的。因此，我们可以将其视为给定模型和观察数据后未来数据的分布。
- en: 'Using PyMC is easy to get posterior predictive samples; we don’t need to compute
    any integral. We just need to call the `sample_posterior_predictive` function
    and pass the `InferenceData` object as the first argument. We also need to pass
    the `model` object, and we can use the `extend_inferencedata` argument to add
    the posterior predictive samples to the `InferenceData` object. The code is:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyMC 获取后验预测样本非常简单；我们无需计算任何积分。只需要调用`sample_posterior_predictive`函数，并将`InferenceData`对象作为第一个参数传入。我们还需要传入`model`对象，并可以使用`extend_inferencedata`参数将后验预测样本添加到`InferenceData`对象中。代码如下：
- en: '**Code 2.14**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.14**'
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One common use of the posterior predictive distribution is to perform posterior
    predictive checks. These are a set of tests that can be used to check if the model
    is a good fit for the data. We can use the `plot_ppc` function from ArviZ to visualize
    the posterior predictive distribution and the observed data. The code is:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 后验预测分布的一个常见用途是进行后验预测检验。这是一组可以用来检查模型是否适合数据的测试。我们可以使用 ArviZ 中的`plot_ppc`函数来可视化后验预测分布和观测数据。代码如下：
- en: '**Code 2.15**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.15**'
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![PIC](img/file75.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file75.png)'
- en: '**Figure 2.13**: Posterior predictive check for `model_g` ploted using `az.plot_ppc`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.13**：使用`az.plot_ppc`绘制的`model_g`后验预测检验'
- en: In *Figure [2.13](#x1-54009r13)*, the black line is a KDE of the data and the
    gray lines are KDEs computed from each one of the 100 posterior predictive samples.
    The gray lines reflect the uncertainty we have about the distribution of the predicted
    data. The plots look *hairy* or *wonky*; this will happen when you have very few
    data points. By default, the KDEs in ArviZ are estimated within the actual range
    of the data and assumed to be zero outside. While some might consider this a bug,
    I think it’s a feature, since it’s reflecting a property of the data instead of
    over-smoothing it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 [2.13](#x1-54009r13)*中，黑色线条是数据的 KDE，灰色线条是从每一个 100 个后验预测样本中计算出的 KDE。灰色线条反映了我们对预测数据分布的不确定性。当数据点很少时，图像看起来会显得*杂乱*或*奇怪*；这是常见的情况。默认情况下，ArviZ
    中的 KDE 是在实际数据范围内估算的，并假设在数据范围之外为零。虽然有人可能认为这是一个 bug，但我认为它是一个特性，因为它反映了数据的一个特性，而不是过度平滑处理。
- en: From *Figure [2.13](#x1-54009r13)*, we can see that the mean of the simulated
    data is slightly displaced to the right and that the variance seems to be larger
    for the simulated data than for the actual data. The source of this discrepancy
    can be attributed to the combination of our choice of likelihood and the two observations
    away from the bulk of the data (the empty dots in *Figure [2.9](#x1-53004r9)*).
    How can we interpret this plot? Is the model wrong or right? Can we use it or
    do we need a different model? Well, it depends. The interpretation of a model
    and its evaluation and criticism are always context-dependent. Based on my experience
    with this kind of measurement, I would say this model is a reasonable enough representation
    of the data and a useful one for most of my analysis. Nevertheless, it is important
    to keep in mind that we could find other models that better accommodate the whole
    dataset, including the two observations that are far from the bulk of the data.
    Let’s see how we can do that.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 [2.13](#x1-54009r13)*中，我们可以看到模拟数据的均值略微向右偏移，并且模拟数据的方差似乎比实际数据大。这个差异的来源可以归因于我们选择的似然函数和两个偏离数据主体的观测值（在*图
    [2.9](#x1-53004r9)*中为空心点）。我们如何解释这个图？模型是错误的还是正确的？我们可以使用它吗，还是需要换一个模型？其实，这取决于情况。模型的解释、评估和批评始终是依赖于具体情境的。根据我在这种测量中的经验，我认为这个模型是对数据的一个合理且足够的表示，并且对于我大多数分析来说非常有用。不过，重要的是要记住，我们可能会找到其他更好地适应整个数据集的模型，包括那两个远离数据主体的观测值。让我们看看如何做到这一点。
- en: 2.6 Robust inferences
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 鲁棒推断
- en: One objection we may have with `model_g` is that we are assuming a Normal distribution,
    but we have two data points away from the bulk of the data. By using a Normal
    distribution for the likelihood, we are indirectly assuming that we are not expecting
    to see a lot of data points far away from the bulk. *Figure [2.13](#x1-54009r13)*
    shows the result of combining these assumptions with the data. Since the tails
    of the Normal distribution fall quickly as we move away from the mean, the Normal
    distribution (at least an anthropomorphized one) is *surprised by seeing* those
    two points and *reacts* in two ways, moving its mean towards those points and
    increasing its standard deviation. Another intuitive way of interpreting this
    is by saying that those points have an excessive weight in determining the parameters
    of the Normal distribution.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对`model_g`可能有一个异议，那就是我们假设了正态分布，但数据中有两个点远离数据的主体。通过使用正态分布作为似然性，我们间接假设我们不期望看到大量远离主体的数据点。*图
    [2.13](#x1-54009r13)* 显示了将这些假设与数据结合的结果。由于正态分布的尾部随着离均值越远而迅速下降，正态分布（至少是人性化的正态分布）对这两个点的出现感到*惊讶*，并以两种方式做出*反应*，将其均值向这两个点移动，并增加其标准差。另一种直观的解释方式是，认为这些点在决定正态分布参数时有过大的权重。
- en: So, what can we do? One option is to check for errors in the data. If we retrace
    our steps we may find an error in the code while cleaning or preprocessing the
    data, or we can relate the putative anomalous values to the malfunction of the
    measuring equipment. Unfortunately, this is not always an option. Many times,
    the data was collected by others and we don’t have a good register of how it was
    collected, measured or processed. Anyway, inspecting the data before modeling
    is always a good idea, that’s a good practice in general.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该怎么做呢？一个选项是检查数据中的错误。如果我们回溯步骤，可能会发现清理或预处理数据时代码出错，或者可以将这些假定的异常值与测量设备故障关联起来。不幸的是，这并不总是一个选项。很多时候，数据是由别人收集的，我们没有很好的记录它是如何收集、测量或处理的。无论如何，在建模之前检查数据总是一个好主意，这在一般情况下都是一种良好的做法。
- en: 'Another option is to declare those points outliers and remove them from the
    data. Two common rules of thumb for identifying outliers in a dataset are:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是将这些点声明为离群值并将其从数据中移除。识别数据集离群值的两条常见经验法则是：
- en: 'Using the interquartile range (IQR): Any data point that falls below 1.5 times
    the IQR from the lower quartile, or above 1.5 times the IQR from the upper quartile,
    is considered an outlier.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用四分位间距（IQR）：任何低于下四分位数1.5倍IQR，或高于上四分位数1.5倍IQR的数据点，都被认为是离群值。
- en: 'Using the standard deviation: Any data point that falls below or above *N*
    times the standard deviation of the data is considered an outlier. With *N* usually
    being 2 or 3.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标准差：任何低于或高于数据标准差*N*倍的数据点都被认为是离群值。通常*N*的值是2或3。
- en: However, it’s important to note that, like any automatic method, these rules
    of thumb are not perfect and may result in discarding valid data points.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，像任何自动化方法一样，这些经验法则并不完美，可能会导致丢弃有效的数据点。
- en: From a modeling perspective, instead of blaming the data we can blame the model
    and change it, as explained in the next section. As a general rule, Bayesians
    prefer to encode assumptions directly into the model by using different priors
    and likelihoods rather than through ad hoc heuristics such as outlier removal
    rules.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从建模的角度来看，我们可以将问题归咎于模型，而不是数据，并进行修改，正如下一节所解释的那样。一般来说，贝叶斯方法更倾向于通过使用不同的先验和似然性，将假设直接编码到模型中，而不是通过诸如去除离群值规则等临时启发式方法。
- en: 2.6.1 Degrees of normality
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.1 正常性的程度
- en: 'There is one distribution that looks very similar to a Normal distribution.
    It has three parameters: a location parameter *μ*, a scale parameter *σ*, and
    a normality parameter *ν*. This distribution’s name is Student’s t-distribution.
    *Figure [2.14](#x1-56002r14)* shows members of this family. When *ν* = ∞ the distribution
    is the Normal distribution, *μ* is the mean and *σ* is the standard deviation.
    When *ν* = 1, we get the Cauchy or Lorentz distribution. *ν* can go from 0 to
    ∞. The lower this number, the heavier their tails. We can also say that the lower
    the value of *ν*, the higher the kurtosis. The kurtosis is the fourth moment,
    as you may remember from the previous chapter. By heavy tails, we mean that it
    is more probable to find values away from the mean compared to a Normal, or in
    other words, values are not as concentrated around the mean as in a lighter tail
    distribution like the Normal. For example, 95% of the values from a Student’s
    t (*μ* = 0*,σ* = 1*,ν* = 1) are found between -12.7 and 12.7\. Instead, for a
    Normal (*μ* = 0*,σ* = 1*,ν* = ∞), this occurs between -1.96 and 1.96.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个分布看起来非常类似于正态分布。它有三个参数：位置参数*μ*，尺度参数*σ*，以及正态性参数*ν*。这个分布被称为学生t分布。*图2.14*展示了这一家族的成员。当*ν*
    = ∞时，分布就是正态分布，*μ*是均值，*σ*是标准差。当*ν* = 1时，我们得到的是柯西分布或洛伦兹分布。*ν*的取值范围是从0到∞。这个值越小，分布的尾部越重。我们还可以说，*ν*值越小，峰度越高。峰度是第四阶矩，你可能还记得前一章中提到的。所谓“重尾”，是指在这种分布中，偏离均值的值比在正态分布中更为常见，换句话说，值的分布不像正态分布那样集中在均值附近。例如，95%的学生t分布值（*μ*
    = 0，*σ* = 1，*ν* = 1）会落在-12.7到12.7之间。而对于正态分布（*μ* = 0，*σ* = 1，*ν* = ∞），这一范围是-1.96到1.96。
- en: '![PIC](img/file76.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file76.png)'
- en: '**Figure 2.14**: The Student’s t-distribution'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.14**：学生t分布'
- en: 'A very curious feature of the Student’s t-distribution is that it has no defined
    mean value when *ν* ≤ 1\. While any finite sample from a Student’s t-distribution
    is just a bunch of numbers from which it is always possible to compute an empirical
    mean, the theoretical distribution itself is the one without a defined value for
    the mean. Intuitively, this can be understood as follows: the tails of the distribution
    are so heavy that at any moment we might get a sampled value from almost anywhere
    from the real line, so if we keep getting numbers, we will never approach a fixed
    value. Instead, the estimate will keep wandering around.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 学生t分布的一个非常有趣的特点是，当*ν* ≤ 1时，它没有定义的均值。虽然从学生t分布中得到的任何有限样本都可以计算出经验均值，但理论上的分布本身没有定义均值的值。直观地说，可以理解为：分布的尾部非常重，以至于在任何时候我们可能从实数线上几乎任何位置抽取一个样本值，因此如果我们不断地得到数值，我们永远不会接近一个固定的值。相反，估计值将不断地游走。
- en: Degrees of what?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 什么自由度？
- en: In most textbooks, the parameter *ν* from the Student’s t-distribution is referred
    to as the degrees of freedom parameter. However, I prefer to follow Kruschke’s
    suggestion and call it the normality parameter. This name is more descriptive
    of the parameter’s role in the distribution, especially as used for robust regression.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数教科书中，学生t分布的参数*ν*被称为自由度参数。然而，我更倾向于遵循Kruschke的建议，将其称为正态性参数。这个名称更能描述该参数在分布中的作用，特别是在用于鲁棒回归时。
- en: Similarly, the variance of this distribution is only defined for values of *ν
    >* 2\. So, it’s important to note that the scale of the Student’s t-distribution
    isn’t the same as its standard deviation. The scale and the standard deviation
    become closer and closer as *ν* approaches infinity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这个分布的方差只有在*ν >* 2时才被定义。因此，重要的是要注意，学生t分布的尺度与其标准差不同。随着*ν*趋近于无穷大，尺度和标准差会越来越接近。
- en: 2.6.2 A robust version of the Normal model
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6.2 正态模型的鲁棒版本
- en: We are going to rewrite the previous model (`model_g`) by replacing the Gaussian
    distribution with the Student’s t-distribution. Because the Student’s t-distribution
    has one more parameter, *ν*, than the Gaussian, we need to specify one more prior,
    for this model we decided to use the exponential distribution, but other distributions
    restricted to the positive interval could also work.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将高斯分布替换为学生t分布来重写前面的模型（`model_g`）。由于学生t分布比高斯分布多了一个参数*ν*，因此我们需要指定一个额外的先验，对于这个模型我们决定使用指数分布，但其他限制在正区间的分布也可以适用。
- en: '![μ ∼ 𝒰(l,h) σ ∼ ℋ 𝒩 (σ ) σ ν ∼ Exp (λ ) Y ∼ 𝒯 (ν,μ, σ) ](img/file77.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![μ ∼ 𝒰(l,h) σ ∼ ℋ 𝒩 (σ ) σ ν ∼ Exp (λ ) Y ∼ 𝒯 (ν,μ, σ)](img/file77.jpg)'
- en: '*Figure [2.15](#x1-57002r15)* shows the graphical representation of this model'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [2.15](#x1-57002r15)* 显示了该模型的图形表示'
- en: '![PIC](img/file78.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file78.png)'
- en: '**Figure 2.15**: Graphical representation of `model_t`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.15**：`model_t` 的图形表示'
- en: Let’s write this model in PyMC; as usual, we can (re)write models by specifying
    a few lines. The only cautionary word here is that by default the Exponential
    distribution in PyMC is parameterized with the inverse of the mean. We are going
    to set *ν* as an Exponential distribution with a mean of 30\. From *Figure [2.14](#x1-56002r14)*,
    we can see that a Student’s t-distribution with *ν* = 30 looks pretty similar
    to a Gaussian (even when it is not). In fact, from the same diagram, we can see
    that *most of the action* happens for relatively small values of *ν*. Hence, we
    can say that the Exponential prior with a mean of 30 is a weakly informative prior
    telling the model we more or less think should be around 30 but can move to smaller
    and larger values with ease. In many problems, estimating *ν* is of no direct
    interest.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 PyMC 编写这个模型；像往常一样，我们可以通过指定几行代码来（重新）编写模型。唯一需要注意的是，PyMC 中的指数分布默认使用均值的倒数作为参数化方式。我们将设置
    *ν* 为均值为 30 的指数分布。从 *图 [2.14](#x1-56002r14)* 中可以看到，*ν* = 30 的学生 t 分布与高斯分布相似（即使它并非真正的高斯分布）。事实上，从相同的图中我们可以看到，*大部分的变化*发生在较小的
    *ν* 值上。因此，我们可以说，均值为 30 的指数先验是一种弱信息先验，告知模型我们大致认为 *ν* 应该在 30 附近，但也能容易地向较小或较大的值偏移。在许多问题中，估计
    *ν* 并非直接关注的重点。
- en: '**Code 2.16**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.16**'
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Compare the trace from `model_g` (*Figure [2.11](#x1-53013r11)*) with the trace
    of `model_t` (*Figure [2.16](#x1-57011r16)*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 `model_g` 的轨迹（*图 [2.11](#x1-53013r11)*) 与 `model_t` 的轨迹（*图 [2.16](#x1-57011r16)*)：
- en: '![PIC](img/file79.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file79.png)'
- en: '**Figure 2.16**: Posterior from `model_t` plotted using `az.plot_trace(idata_t)`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.16**：使用 `az.plot_trace(idata_t)` 绘制的 `model_t` 后验'
- en: Now, print the summary of `model_t`. You should get something like *Table [2.4](#x1-57012r4)*.
    Compare the results with those from `model_g`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打印出 `model_t` 的总结。你应该得到类似于 *表 [2.4](#x1-57012r4)* 的内容。将结果与 `model_g` 的结果进行比较。
- en: '|  | mean | sd | hdi_3% | hdi_97% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | 均值 | 标准差 | hdi_3% | hdi_97% |'
- en: '| *μ* | 53.02 | 0.39 | 52.27 | 53.71 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| *μ* | 53.02 | 0.39 | 52.27 | 53.71 |'
- en: '| *σ* | 2.21 | 0.42 | 1.46 | 3.01 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| *σ* | 2.21 | 0.42 | 1.46 | 3.01 |'
- en: '| *ν* | 4.94 | 5.45 | 1.07 | 10.10 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| *ν* | 4.94 | 5.45 | 1.07 | 10.10 |'
- en: '**Table 2.4**: Summary statistics for *μ*, *σ*, and *ν*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2.4**：*μ*、*σ* 和 *ν* 的总结统计'
- en: Before you keep reading, take a moment to compare the preceding results with
    those from `model_g` and spot the difference between both results. Did you notice
    something interesting?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续阅读之前，花点时间将前面的结果与 `model_g` 的结果进行比较，并找出两者之间的差异。你发现了什么有趣的地方吗？
- en: The estimation of *μ* between both models is similar, with a difference of ≈
    0*.*5\. The estimation of *σ* is ≈ 3*.*5 for `model_g` and ≈ 2*.*2 for `model_t`.
    This is a consequence of the Student’s t-distribution allocating less weight to
    values away from the mean. Loosely speaking, the Student’s t-distribution is *less
    surprised* by values away from the mean. We can also see that the mean of *ν*
    is ≈ 5, meaning that we have a heavy-tailed distribution and not a Gaussian-like
    distribution.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型之间的 *μ* 估计值相似，差异约为 ≈ 0*.*5\. *σ* 的估计值对于 `model_g` 为 ≈ 3*.*5， 对于 `model_t`
    为 ≈ 2*.*2。这是因为学生 t 分布将较远离均值的值分配较小的权重。简单来说，学生 t 分布对远离均值的值*不那么惊讶*。我们还可以看到 *ν* 的均值为
    ≈ 5，这意味着我们有一个重尾分布，而非类似高斯分布的分布。
- en: '*Figure [2.17](#x1-57014r17)* shows a posterior predictive check for `model_t`.
    Let’s compare it with the one from `model_g` (*Figure [2.13](#x1-54009r13)*).
    Using the Student’s t-distribution in our model leads to predictive samples that
    seem to better fit the data in terms of the location of the peak of the distribution
    and also its spread. Notice how the samples extend far away from the bulk of the
    data, and how a few of the predictive samples look very flat. This is a direct
    consequence of the Student’s t-distribution expecting to see data points far away
    from the mean or bulk of the data. If you check the code used to generate *Figure
    [2.17](#x1-57014r17)* you will see that we have used `ax.set_xlim(40, 70)`.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [2.17](#x1-57014r17)* 显示了 `model_t` 的后验预测检验。让我们将其与 `model_g` 的结果（*图 [2.13](#x1-54009r13)）进行比较。使用学生
    t 分布的模型产生的预测样本似乎在分布的峰值位置以及扩展范围上更好地拟合数据。请注意，样本如何从数据的主要部分延伸得很远，且一些预测样本看起来非常平坦。这是学生
    t 分布期望看到远离均值或数据主体的数据点的直接结果。如果你查看用于生成 *图 [2.17](#x1-57014r17)* 的代码，你会发现我们使用了 `ax.set_xlim(40,
    70)`。'
- en: '![PIC](img/file80.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file80.png)'
- en: '**Figure 2.17**: Posterior predictive check for `model_t`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.17**：`model_t` 的后验预测检查'
- en: The Student’s t-distribution allows us to have a more **robust estimation**
    of the mean and standard deviation because the outliers have the effect of decreasing
    *ν* instead of pulling the mean or increasing the standard deviation. Thus, the
    mean and the scale are estimated by weighting the data points close to the bulk
    more than those apart from it. As a rule of thumb, for values of *ν >* 2 and *not
    too small*, we can consider the scale of a Student’s t-distribution as a reasonable
    practical proxy for the standard deviation of the data after removing outliers.
    This is a rule of thumb because we know that the scale is not the standard deviation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 学生 t 分布使我们能够更 **稳健地估计**均值和标准差，因为异常值对 *ν* 的影响是减少，而不是拉动均值或增大标准差。因此，均值和尺度是通过加权靠近主体的数据点来估计的，而远离主体的数据点则权重较小。作为经验法则，对于
    *ν >* 2 且 *不太小* 的值，我们可以将学生 t 分布的尺度视为去除异常值后的数据标准差的合理实用代理。这是一个经验法则，因为我们知道尺度并不是标准差。
- en: 2.7 InferenceData
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 InferenceData
- en: InferenceData is a rich container for the results of Bayesian inference. A modern
    Bayesian analysis potentially generates many sets of data including posterior
    samples and posterior predictive samples. But we also have observed data, samples
    from the prior, and even statistics generated by the sampler. All this data, and
    more, can be stored in an InferenceData object. To help keep all this information
    organized, each one of these sets of data has its own group. For instance, the
    posterior samples are stored in the `posterior` group. The observed data is stored
    in the `observed_data` group.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: InferenceData 是一个包含贝叶斯推断结果的丰富容器。现代的贝叶斯分析可能会生成多组数据，包括后验样本和后验预测样本。但是我们也有观察数据、先验样本，甚至是由采样器生成的统计量。所有这些数据以及更多内容都可以存储在一个
    InferenceData 对象中。为了帮助组织这些信息，每一组数据都有其自己的组。例如，后验样本存储在 `posterior` 组中，观察数据存储在 `observed_data`
    组中。
- en: '*Figure [2.18](#x1-58002r18)* shows an HTML representation of the InferenceData
    for `model_g`. We can see 4 groups: `posterior`, `posterior_predictive`, `sample_stats`,
    and `observed_data`. All of them are collapsed except for the `posterior` group.
    We can see we have two coordinates `chain` and `draw` of dimensions 4 and 1000
    respectively. We also have 2 variables *μ* and *σ*.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [2.18](#x1-58002r18)* 显示了 `model_g` 的 InferenceData 对象的 HTML 表示。我们可以看到 4
    个组：`posterior`、`posterior_predictive`、`sample_stats` 和 `observed_data`。除了 `posterior`
    组外，其他组都已折叠。我们可以看到有两个坐标 `chain` 和 `draw`，其维度分别为 4 和 1000。我们还有 2 个变量 *μ* 和 *σ*。'
- en: '![PIC](img/file81.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file81.png)'
- en: '**Figure 2.18**: InferenceData object for `model_g`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.18**：`model_g` 的 InferenceData 对象'
- en: 'So far, PyMC has generated an InferenceData object and ArviZ has used that
    to generate plots or numerical summaries. But we can also manipulate an InferenceData
    object. Some common operations are to access specific groups. For instance, to
    access the posterior group we can write:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，PyMC 已经生成了一个 InferenceData 对象，而 ArviZ 已使用该对象生成图表或数值摘要。但是，我们也可以操作 InferenceData
    对象。一些常见的操作是访问特定的组。例如，要访问后验组，我们可以这样写：
- en: '**Code 2.17**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.17**'
- en: '[PRE17]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will return an xarray dataset. If you are not familiar with xarray [[Hoyer
    and Hamman](Bibliography.xhtml#Xxarray_2017), [2017](Bibliography.xhtml#Xxarray_2017)]
    ( [https://docs.xarray.dev/en/stable/](https://docs.xarray.dev/en/stable/)), imagine
    NumPy multidimensional arrays but with labels! This makes many operations easier
    as you don’t have to remember the order of the dimensions. For example, the following
    code will return the first draw from chain 0 and chain 2:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个 xarray 数据集。如果你不熟悉 xarray [[Hoyer 和 Hamman](Bibliography.xhtml#Xxarray_2017)，
    [2017](Bibliography.xhtml#Xxarray_2017)]（ [https://docs.xarray.dev/en/stable/](https://docs.xarray.dev/en/stable/)），可以把它想象成带标签的
    NumPy 多维数组！这使得许多操作变得更简单，因为你不需要记住维度的顺序。例如，以下代码将返回链条 0 和链条 2 的第一次抽样：
- en: '**Code 2.18**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.18**'
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We use the `sel` method to select a range of values, like the first 100 draws
    from all chains:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `sel` 方法选择一系列值，例如从所有链条中选择前 100 次抽样：
- en: '**Code 2.19**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.19**'
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Additionally, the following returns the mean for *μ* and *σ* computed over
    all draws and chains:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下代码返回了在所有抽样和链条上计算的 *μ* 和 *σ* 的均值：
- en: '**Code 2.20**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.20**'
- en: '[PRE20]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Meanwhile, the following code returns the mean over the draws, i.e., this returns
    four values for *μ* and four values for *σ*, one per chain:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，以下代码返回抽样的均值，即，它会返回 *μ* 和 *σ* 的四个值，每个链条一个：
- en: '**Code 2.21**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.21**'
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'More often than not, we don’t care about chains and draws, we just want to
    get the posterior samples. In those cases, we can use the `az.extract` function:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不关心链和绘图，只想获得后验样本。在这种情况下，我们可以使用`az.extract`函数：
- en: '**Code 2.22**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.22**'
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This combines the `chain` and `draw` into a `sample` coordinate which can make
    further operations easier. By default, `az.extract` works on the posterior, but
    you can specify other groups with the `group` argument. You can also use `az.extract`
    to get a random sample of the posterior:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这将`chain`和`draw`合并为一个`sample`坐标，这可以使后续操作更为简便。默认情况下，`az.extract`作用于后验分布，但你也可以通过`group`参数指定其他组。你还可以使用`az.extract`来获取后验的随机样本：
- en: '**Code 2.23**'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.23**'
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are going to use the InferenceData object all the time in this book, so you
    will have the time to get familiar with it and learn more about it in the coming
    pages.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将一直使用InferenceData对象，因此你将有时间熟悉它并在接下来的页面中了解更多。
- en: 2.8 Groups comparison
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.8 组间比较
- en: 'One pretty common statistical analysis is group comparison. We may be interested
    in how well patients respond to a certain drug, the reduction of car accidents
    by the introduction of new traffic regulations, student performance under different
    teaching approaches, and so on. Sometimes, this type of question is framed under
    the hypothesis testing scenario and the goal is to declare a result *statistically
    significant*. Relying only on statistical significance can be problematic for
    many reasons: on the one hand, statistical significance is not equivalent to practical
    significance; on the other hand, a really small effect can be declared significant
    just by collecting enough data.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的统计分析是组间比较。我们可能会对患者对某种药物的反应、交通法规引入后车祸减少、不同教学方法下的学生表现等感兴趣。有时，这类问题会以假设检验的形式呈现，目标是声明某个结果*具有统计学意义*。仅仅依赖统计学显著性可能会带来很多问题：一方面，统计学显著性不等于实际意义；另一方面，单纯通过收集足够的数据，就可能宣布一个非常小的效应为显著。
- en: The idea of hypothesis testing is connected to the concept of p-values. This
    is not a fundamental connection but a cultural one; people are used to thinking
    that way mostly because that’s what they learn in most introductory statistical
    courses. There is a long record of studies and essays showing that, more often
    than not, p-values are used and interpreted the wrong way, even by people who
    are using them daily. Instead of doing hypothesis testing, we are going to take
    a different route and we are going to focus on estimating the effect size, that
    is, quantifying the difference between two groups. One advantage of thinking in
    terms of effect size is that we move away from yes-no questions like ”Does it
    work?” or ”Is there any effect?” and into the more nuanced type of questions like
    ”How well does it work?” or ”How large is the effect?”.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验的概念与p值的概念相关。这种联系并非根本性的，而是一种文化上的联系；人们习惯于这样思考，主要是因为这是大多数入门统计课程中教授的内容。长期以来有很多研究和论文表明，p值经常被错误使用和解读，甚至是那些每天都在使用它们的人。我们不会进行假设检验，而是采取不同的途径，专注于估计效应量，即量化两组之间的差异。思考效应量的一个好处是，我们可以摆脱“是否有效？”或“是否有影响？”这样的简单是非问题，转而提出更细致的问题，如“效果如何？”或“效应有多大？”。
- en: Sometimes, when comparing groups, people talk about a control group and a treatment
    group. For example, when we want to test a new drug, we want to compare the new
    drug (the treatment) against a placebo (the control group). The placebo effect
    is a psychological phenomenon where a patient experiences perceived improvements
    in their symptoms or condition after receiving an inactive substance or treatment.
    By comparing the effects of the drug with a placebo group in clinical trials,
    researchers can discern whether the drug is genuinely effective. The placebo effect
    is an example of the broader challenge in experimental design and statistical
    analysis of the difficulty of accounting for all factors in an experiment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在比较不同组时，人们会提到对照组和实验组。例如，当我们想要测试一种新药时，我们希望将新药（实验组）与安慰剂（对照组）进行比较。安慰剂效应是一种心理现象，患者在接受无效物质或治疗后，可能会感觉症状或状况有所改善。通过在临床试验中将药物与安慰剂组的效果进行比较，研究人员可以辨别药物是否真的有效。安慰剂效应是实验设计和统计分析中的一个广泛挑战的例子，表明在实验中考虑所有因素是困难的。
- en: One interesting alternative to this design is to compare the new drug with the
    commercially available most popular or efficient drug to treat that illness. In
    such a case, the control group cannot be a placebo; it should be the other drug.
    Bogus control groups are a splendid way to lie using statistics.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计的一个有趣替代方案是，将新药与市面上最受欢迎或最有效的药物进行比较，用以治疗该疾病。在这种情况下，控制组不能是安慰剂；它应该是另一种药物。虚假的控制组是用统计学撒谎的一个绝佳方法。
- en: For example, imagine you work for a dairy product company that wants to sell
    overly sugared yogurts to kids by telling their dads and moms that this particular
    yogurt boosts the immune system or helps their kids grow stronger. One way to
    cheat with data is by using milk or even water as a control group, instead of
    another cheaper, less sugary, less marketed yogurt. It may sound silly when I
    put it this way, but I am describing actual experiments published in actual scientific
    journals. When someone says something is harder, better, faster, or stronger,
    remember to ask what the baseline used for the comparison was.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你为一家乳制品公司工作，这家公司想通过告诉父母这种酸奶可以增强免疫系统或帮助孩子更强壮来向孩子们推销过多含糖的酸奶。一种通过数据作假的方式是使用牛奶或甚至水作为对照组，而不是另一种更便宜、含糖量更少、市场推广较少的酸奶。把事情说成这样时，可能会显得很荒谬，但我正在描述的是在实际科学期刊上发表的真实实验。当有人说某物更难、更好、更快或更强时，记得问一下用于比较的基准是什么。
- en: 2.8.1 The tips dataset
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.1 小费数据集
- en: 'To explore the subject matter of this section, we are going to use the tips
    dataset [[Bryant and Smith](Bibliography.xhtml#Xbryant_1995), [1995](Bibliography.xhtml#Xbryant_1995)].
    We want to study the effect of the day of the week on the tips earned at a restaurant.
    For this example, the different groups are the days. Notice there is no control
    group or treatment group. If we wish, we can arbitrarily establish one day (for
    example, Thursday) as the reference or control. For now, let’s start the analysis
    by loading the dataset as a pandas DataFrame using just one line of code. If you
    are not familiar with pandas, the `tail` command is used to show the last rows
    of a DataFrame (see *Table [2.5](#x1-60007r5)*), you may want to try using `head`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探讨本节的主题，我们将使用提示数据集[[Bryant 和 Smith](Bibliography.xhtml#Xbryant_1995)，[1995](Bibliography.xhtml#Xbryant_1995)]。我们想研究星期几对餐厅小费的影响。在这个例子中，不同的组别是不同的星期几。请注意，这里没有控制组或处理组。如果我们愿意，可以随意将某一天（例如星期四）设定为参考组或控制组。现在，让我们通过仅用一行代码将数据集加载为
    pandas DataFrame 来开始分析。如果你不熟悉 pandas，`tail` 命令用于显示 DataFrame 的最后几行（见 *表 [2.5](#x1-60007r5)*），你也可以尝试使用
    `head`：
- en: '**Code 2.24**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.24**'
- en: '[PRE24]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|  | total_bill | tip | sex | smoker | day | time | size |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | 总账单 | 小费 | 性别 | 吸烟者 | 星期几 | 餐次 | 人数 |'
- en: '| 239 | 29.03 | 5.92 | Male | No | Sat | Dinner | 3 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 239 | 29.03 | 5.92 | 男性 | 否 | 星期六 | 晚餐 | 3 |'
- en: '| 240 | 27.18 | 2.00 | Female | Yes | Sat | Dinner | 2 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 240 | 27.18 | 2.00 | 女性 | 是 | 星期六 | 晚餐 | 2 |'
- en: '| 241 | 22.67 | 2.00 | Male | Yes | Sat | Dinner | 2 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 241 | 22.67 | 2.00 | 男性 | 是 | 星期六 | 晚餐 | 2 |'
- en: '| 242 | 17.82 | 1.75 | Male | No | Sat | Dinner | 2 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 242 | 17.82 | 1.75 | 男性 | 否 | 星期六 | 晚餐 | 2 |'
- en: '| 243 | 18.78 | 3.00 | Female | No | Thurs | Dinner | 2 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 243 | 18.78 | 3.00 | 女性 | 否 | 星期四 | 晚餐 | 2 |'
- en: '**Table 2.5**: Sample data from a restaurant'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2.5**：餐厅样本数据'
- en: From this DataFrame, we are only going to use the day and tip columns. *Figure
    [2.19](#x1-60009r19)* shows the distributions of this data using ridge plots.
    This figure was done with ArviZ. Even though ArviZ is designed for Bayesian model
    analysis, some of its functions can be useful for data analysis.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 DataFrame 中，我们只使用 `day` 和 `tip` 两列。*图 [2.19](#x1-60009r19)* 显示了使用 ridge
    图展示的数据分布。这个图是用 ArviZ 绘制的。尽管 ArviZ 主要用于贝叶斯模型分析，但它的一些功能在数据分析中也很有用。
- en: '![PIC](img/file82.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file82.png)'
- en: '**Figure 2.19**: Distribution of tips by day'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.19**：按星期几分布的小费'
- en: We are going to do some small preprocessing of the data. First, we are going
    to create the `tip` variable representing the tips in dollars. Then we create
    the `idx` variable, a categorical dummy variable encoding the days with numbers,
    that is, `[0, 1, 2, 3]` instead of `[’Thur’, ’Fri’, ’Sat’, ’Sun’]`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对数据进行一些小的预处理。首先，我们将创建一个 `tip` 变量，表示以美元为单位的小费。然后我们创建 `idx` 变量，一个类别虚拟变量，用数字编码星期几，也就是说，`[0,
    1, 2, 3]` 代替 `[’Thur’, ’Fri’, ’Sat’, ’Sun’]`。
- en: '**Code 2.25**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.25**'
- en: '[PRE25]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The model for this problem is almost the same as `model_g`; the only difference
    is that now *μ* and *σ* are going to be vectors instead of scalars. PyMC syntax
    is extremely helpful for this situation: instead of writing for loops, we can
    write our model in a vectorized way.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的模型几乎与`model_g`相同，唯一的区别是现在*μ*和*σ*将变成向量而非标量。PyMC的语法在这种情况下非常有帮助：我们可以以矢量化的方式编写模型，而不是使用循环。
- en: '**Code 2.26**'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.26**'
- en: '[PRE26]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice how we passed a `shape` argument for the prior distribution. For *μ*,
    this means that we are specifying four independent ![](img/N.PNG)(0*,*10) and
    for *σ* four independent ![](img/HN.PNG)(10). Also, notice how we use the `idx`
    variable to properly index the values of *μ* and *σ* we pass to the likelihood.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意我们如何为先验分布传递了一个`shape`参数。对于*μ*，这意味着我们指定了四个独立的![](img/N.PNG)(0*,*10)，而对于*σ*，则是四个独立的![](img/HN.PNG)(10)。另外，请注意我们如何使用`idx`变量来正确索引传递给似然函数的*μ*和*σ*的值。
- en: PyMC provides an alternative syntax, which consists of specifying coordinates
    and dimensions. The advantage of this alternative is that it allows better integration
    with ArviZ.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: PyMC提供了一种替代语法，要求指定坐标和维度。这种替代方法的优势在于它能更好地与ArviZ集成。
- en: In this example, we have 4 values for the means and 4 for the standard deviations,
    and that’s why we use `shape=4`. The InferenceData will have 4 indices `0, 1,
    2, 3` mapping to each of the 4 days. However, it is the user’s job to associate
    those numerical indices with the days. By using coordinates and dimensions we,
    and ArviZ, can use the labels `"Thur", "Fri", "Sat", "Sun"` to easily map parameters
    to their associated days.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有4个均值和4个标准差，因此我们使用`shape=4`。InferenceData将具有4个索引`0, 1, 2, 3`，分别对应每个4天。然而，将这些数值索引与具体的日期关联起来是用户的任务。通过使用坐标和维度，我们以及ArviZ可以使用标签`"Thur",
    "Fri", "Sat", "Sun"`轻松地将参数映射到相应的日期。
- en: We are going to specify two coordinates; `"days"`, with the dimensions `"Thur",
    "Fri", "Sat", "Sun"`; and `"days_flat"`, which will contain the same labels but
    repeated according to the order and length that corresponds to each observation.
    `"days_flat"` will be useful later for posterior predictive tests.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定两个坐标；`"days"`，其维度为`"Thur", "Fri", "Sat", "Sun"`；和`"days_flat"`，它将包含相同的标签，但根据每个观察值的顺序和长度重复。`"days_flat"`将在后续的后验预测测试中发挥作用。
- en: '**Code 2.27**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.27**'
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Once the posterior distribution is computed, we can do all the analyses that
    we believe are pertinent. For instance, we can do a posterior predictive test.
    With the help of ArviZ, we can do it by calling `az.plot_ppc`. We use the `coords`
    and `flatten` parameters to get one subplot per day.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦后验分布被计算出来，我们就可以进行所有我们认为相关的分析。例如，我们可以进行后验预测测试。在ArviZ的帮助下，我们可以通过调用`az.plot_ppc`来实现。我们使用`coords`和`flatten`参数来为每一天获取一个子图。
- en: '**Code 2.28**'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 2.28**'
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: From the following figure, we can see that the model can capture the general
    shape of the distributions, but still, some details are elusive. This may be due
    to the relatively small sample size, factors other than day influencing the tips,
    or a combination of both.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从下图中可以看出，模型能够捕捉到分布的一般形状，但仍有一些细节难以捉摸。这可能是由于样本量相对较小，除了日期外，其他因素对小费的影响，或者两者的结合。
- en: '![PIC](img/file83.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file83.png)'
- en: '**Figure 2.20**: Posterior predictive checks for the tips dataset'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.20**：Tips数据集的后验预测检查'
- en: For now, we are going to consider that the model is good enough for us and move
    to explore the posterior. We can explain the results in terms of their average
    values and then find for which days that average is higher. But there are other
    alternatives; for instance, we may want to express the results in terms of differences
    in posterior means. In addition, we might want to use some measure of effect size
    that is popular with our audiences, such as the probability of superiority or
    Cohen’s d. In the next sections, we explain these alternatives.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们认为该模型足够好，可以继续探索后验分布。我们可以从平均值的角度解释结果，然后找出哪些日期的平均值较高。但也有其他方法；例如，我们可能希望用后验均值差异来表达结果。此外，我们可能想要使用一些对我们受众来说更常见的效应大小度量，如优势概率或Cohen's
    d。在接下来的部分中，我们将解释这些替代方法。
- en: 2.8.2 Cohen’s d
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.2 Cohen's d
- en: 'A common way to measure the effect size is Cohen’s d, which is defined as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 测量效应大小的常见方法是Cohen's d，定义如下：
- en: '![-μ2 −-μ1 ∘ σ21+σ22- --2-- ](img/file84.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![-μ2 −-μ1 ∘ σ21+σ22- --2-- ](img/file84.jpg)'
- en: Because we have a posterior distribution we can compute a distribution of Cohen’s
    d, and if we want a single value we can compute the mean or median of that distribution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有后验分布，因此可以计算Cohen’s d的分布。如果我们需要一个单一值，可以计算该分布的均值或中位数。
- en: This expression tells us that the effect size is the difference between the
    means scaled by the pooled standard deviation of both groups. By taking the pooled
    standard deviation, we are standardizing the differences of means. This is important
    because when you have a difference of 1 and a standard deviation of 0.1, the effect
    size is larger than the same difference when the standard deviation is 10\. A
    Cohen’s d can be interpreted as a Z-score (a standard score). A Z-score is the
    signed number of standard deviations by which a value differs from the mean value
    of what is being observed or measured. Thus, a value of 0.5 Cohen’s d could be
    interpreted as a difference of 0.5 standard deviations from one group to the other.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式告诉我们，效应大小是均值差异与两组合并标准差的比值。通过取合并标准差，我们对均值差异进行了标准化。这一点非常重要，因为当差异为1且标准差为0.1时，效应大小大于标准差为10时相同差异的效应大小。Cohen’s
    d可以被解释为Z得分（标准分数）。Z得分是一个有符号的标准差数，表示一个值与被观测或测量的均值的差异。因此，0.5的Cohen’s d可以被解释为两组之间差异为0.5个标准差。
- en: Even when the differences of means are standardized, we may still need to calibrate
    ourselves based on the context of a given problem to be able to say if a given
    value is big, small, medium, and so on. For instance, if we are used to performing
    several analyses for the same or similar problems, we can get used to a Cohen’s
    d of say 1\. So when we get a Cohen’s d of say 2, we know that we have something
    important (or someone made a mistake somewhere!). If you do not have this practice
    yet, you can ask a domain expert for their valuable input.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 即使均值差异已被标准化，我们仍然可能需要根据特定问题的上下文进行校准，才能判断某个给定值是大是小是中等。例如，如果我们习惯于对相同或相似的问题进行多次分析，我们可以习惯于Cohen’s
    d为1。于是，当我们得到一个Cohen’s d为2时，我们就知道我们得到了重要的结果（或者是某处出错了！）。如果您还没有这种经验，您可以请教领域专家，获取他们宝贵的意见。
- en: A very nice web page to explore what different values of Cohen’s d look like
    is [http://rpsychologist.com/d3/cohend](http://rpsychologist.com/d3/cohend). On
    that page, you will also find other ways to express an effect size; some of them
    could be more intuitive, such as the probability of superiority, which we will
    discuss next.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常好的网页，可以探索Cohen’s d的不同值是什么样子的，网址是[http://rpsychologist.com/d3/cohend](http://rpsychologist.com/d3/cohend)。在该页面上，您还可以找到表示效应大小的其他方法；其中一些可能更为直观，例如超越概率，我们将在下一部分讨论。
- en: 2.8.3 Probability of superiority
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.3 超越概率
- en: 'This is another way to report the effect size, and this is defined as the probability
    that a data point taken at random from one group has a larger value than one also
    taken at random from the other group. If we assume that the data we are using
    is normally distributed, we can compute the probability of superiority from Cohen’s
    d using the following expression:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这是报告效应大小的另一种方式，它定义为从一个组中随机抽取的数据点大于从另一个组中随机抽取的一个数据点的概率。如果我们假设所使用的数据是正态分布的，那么我们可以使用以下公式通过Cohen’s
    d计算超越概率：
- en: '![ ( δ ) ps = Φ √--- 2 ](img/file85.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![ ( δ ) ps = Φ √--- 2 ](img/file85.jpg)'
- en: Φ is the cumulative Normal distribution and *δ* is the Cohen’s d.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Φ是累积正态分布，而*δ*是Cohen’s d。
- en: If we are OK with the normality assumption, we can use this formula to get the
    probability of superiority from the value of Cohen’s d. Otherwise, we can compute
    the probability of superiority directly from the posterior samples just by taking
    random samples from two groups and counting how many times one value is larger
    than the other. To do that we don’t need Cohen’s d or assume normality (see the
    Exercises section). This is an example of an advantage of using **Markov Chain
    Monte Carlo** (**MCMC**) methods; once we get samples from the posterior, we can
    compute many quantities from it often in ways that are easier than with other
    methods.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们接受正态性假设，那么可以使用此公式从Cohen’s d的值计算超越概率。否则，我们可以直接通过从两个组中随机抽取样本并计算有多少次一个值大于另一个值来计算超越概率。这样，我们不需要Cohen’s
    d或假设正态性（请参见练习部分）。这是使用**马尔可夫链蒙特卡洛**（**MCMC**）方法的一个优点；一旦我们从后验分布中获得样本，就可以通过这种方式计算许多量，通常比其他方法更为简单。
- en: 2.8.4 Posterior analysis of mean differences
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8.4 平均差异的后验分析
- en: To conclude our previous discussions, let’s compute the posterior distributions
    for differences in means, Cohen’s d, and the probability of superiority, and integrate
    them into a single plot. *Figure [2.21](#x1-63003r21)* has a lot of information.
    Depending on the audience, the plot may be overloaded, or too crowded. Perhaps
    it is useful for a discussion within your team, but for the general public, it
    may be convenient to remove elements or distribute the information between a figure
    and a table or two figures. Anyway, here we show it precisely so you can compare
    different ways of presenting the same information, so take some time to ponder
    this figure.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们之前的讨论，让我们计算均值差异、Cohen's d和优势概率的后验分布，并将它们整合到一个图表中。*图 [2.21](#x1-63003r21)*包含了大量信息。根据受众的不同，图表可能会过载，或者太拥挤。或许它适合你团队内部的讨论，但对于大众而言，去除某些元素或将信息分布在一个图表和一个表格，或者两个图表之间，可能会更方便。无论如何，在这里我们精确地展示它，以便你可以比较展示相同信息的不同方式，所以请花些时间思考这个图表。
- en: '![PIC](img/file86.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file86.png)'
- en: '**Figure 2.21**: Posterior distributions of the differences of means, Cohen’s
    d, and the probability of superiority for the tips dataset'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.21**：小费数据集的均值差异、Cohen’s d 和优势概率的后验分布'
- en: One way to read *Figure [2.21](#x1-63003r21)* is to compare the reference value,
    of zero difference, with the HDI interval. We have only one case when the 94%
    HDI excludes the reference value, that is, the difference in tips between Thursday
    and Sunday. For all the other comparisons, we cannot rule out a difference of
    zero, at least according to the HDI-reference-value-overlap criteria. But even
    for that case, the average difference is ≈ 0*.*5 dollars. Is that difference large
    enough? Is that difference enough to accept working on Sunday and missing the
    opportunity to spend time with family or friends? Is that difference enough to
    justify averaging the tips over the four days and giving every waitress and waiter
    the same amount of tip money?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读*图 [2.21](#x1-63003r21)*的一种方法是将零差异的参考值与HDI区间进行比较。只有一个案例，94%的HDI排除了参考值，也就是星期四和星期天的小费差异。在所有其他比较中，根据HDI-参考值重叠标准，我们不能排除零差异。但即使是那个案例，平均差异也约为0*.*5美元。这个差异足够大吗？这个差异足够支持接受在星期天工作，并错过和家人或朋友共度时光的机会吗？这个差异足够让我们接受将小费平均分配到四天，并给每个女服务员和男服务员相同的小费金额吗？
- en: The short answer is that those kinds of questions cannot be answered by statistics;
    they can only be informed by statistics. I hope you don’t feel cheated by that
    answer, but we cannot get automatic answers unless we include in the analysis
    all the values that are important to the stakeholders. Formally, that requires
    the definition of a loss function or at least the definition of some threshold
    value for the effect size, which should be informed by those values.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的回答是，这类问题不能通过统计学来回答；它们只能通过统计学来提供信息。我希望你不会因为这个答案而感到失望，但除非我们在分析中包含所有对相关方重要的值，否则我们无法得到自动的答案。正式来说，这要求定义一个损失函数，或至少定义一个效应大小的阈值，这个阈值应该由这些值来决定。
- en: 2.9 Summary
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.9 总结
- en: Although Bayesian statistics is conceptually simple, fully probabilistic models
    often lead to analytically intractable expressions. For many years, this was a
    huge barrier, hindering the wide adoption of Bayesian methods. Fortunately, maths,
    statistics, physics, and computer science came to the rescue in the form of numerical
    methods that are capable—at least in principle—of solving any inference problem.
    The possibility of automating the inference process has led to the development
    of probabilistic programming languages, allowing a clear separation between model
    definition and inference. PyMC is a Python library for probabilistic programming
    with a very simple, intuitive, and easy-to-read syntax that is also very close
    to the statistical syntax used to describe probabilistic models.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管贝叶斯统计在概念上很简单，但完全概率模型常常导致解析上不可处理的表达式。多年来，这一直是一个巨大的障碍，阻碍了贝叶斯方法的广泛应用。幸运的是，数学、统计学、物理学和计算机科学通过数值方法来拯救这一局面，这些方法在原则上能够解决任何推断问题。自动化推断过程的可能性促使了概率编程语言的发展，这些语言清晰地将模型定义和推断分开。PyMC是一个用于概率编程的Python库，具有非常简单、直观且易读的语法，而且非常接近描述概率模型的统计语法。
- en: We introduced the PyMC library by revisiting the coin-flip model from *Chapter
    [1](CH01.xhtml#x1-160001)*, this time without analytically deriving the posterior.
    PyMC models are defined inside a context manager. To add a probability distribution
    to a model, we just need to write a single line of code. Distributions can be
    combined and can be used as priors (unobserved variables) or likelihoods (observed
    variables). If we pass data to a distribution, it becomes a likelihood. Sampling
    can be achieved with a single line as well. PyMC allows us to get samples from
    the posterior distribution. If everything goes right, these samples will be representative
    of the correct posterior distribution and thus they will be a representation of
    the logical consequences of our model and data. We can explore the posterior generated
    by PyMC using ArviZ, a Python library that works hand-in-hand with PyMC and can
    be used, among other tasks, to help us interpret and visualize posterior distributions.
    One way of using a posterior to help us make inference-driven decisions is by
    comparing the ROPE against the HDI interval. We also briefly mentioned the notion
    of loss functions, a formal way to quantify the trade-offs and costs associated
    with making decisions in the presence of uncertainty. We learned that loss functions
    and point estimates are intimately associated.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过重新回顾*第[1章](CH01.xhtml#x1-160001)*中的抛硬币模型来介绍PyMC库，这次我们没有通过解析方法推导后验分布。PyMC模型定义在上下文管理器中。为了将概率分布添加到模型中，我们只需写一行代码。分布可以组合使用，可以作为先验（未观察变量）或似然（已观察变量）。如果我们将数据传递给分布，它将成为似然。采样也可以通过一行代码实现。PyMC允许我们从后验分布中获取样本。如果一切顺利，这些样本将代表正确的后验分布，因此它们将成为我们模型和数据的逻辑后果的表示。我们可以使用与PyMC配合使用的Python库ArviZ来探索PyMC生成的后验分布，ArviZ可以帮助我们解释和可视化后验分布。在使用后验帮助我们做推理驱动的决策时，一种方法是将ROPE与HDI区间进行比较。我们还简要提到过损失函数的概念，这是量化在不确定性下做决策时的权衡和成本的一种正式方式。我们学到，损失函数和点估计是紧密相关的。
- en: Up to this point, the discussion was restricted to a simple one-parameter model.
    Generalizing to an arbitrary number of parameters is trivial with PyMC; we exemplify
    how to do this with the Gaussian and Student’s t models. The Gaussian distribution
    is a special case of the Student’s t-distribution and we showed you how to use
    the latter to perform robust inferences in the presence of outliers. In the next
    chapters, we will look at how these models can be used as part of linear regression
    models. We used a Gaussian model to compare groups. While this is sometimes framed
    in the context of hypothesis testing, we take another route and frame this task
    as a problem of inferring the effect size, an approach we generally consider to
    be richer and more productive. We also explored different ways to interpret and
    report effect sizes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，讨论仅限于一个简单的单参数模型。使用PyMC将其推广到任意数量的参数是非常简单的；我们通过高斯分布和学生t分布模型来展示如何做到这一点。高斯分布是学生t分布的特例，我们向你展示了如何使用后者在存在离群值的情况下进行稳健推断。在接下来的章节中，我们将探讨这些模型如何作为线性回归模型的一部分进行使用。我们使用高斯模型来比较不同组之间的差异。虽然这有时会以假设检验的方式进行框架化，但我们采取了另一种方法，将这一任务框架化为推断效应量的问题，我们通常认为这种方法更丰富、更有成效。我们还探讨了不同的效应量解释和报告方式。
- en: With all that we have learned in this and the previous chapter, we are ready
    to study one of the most important concepts in this book, hierarchical models.
    That will be the topic of the next chapter.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章及上一章的学习，我们已经准备好学习本书中最重要的概念之一——层级模型。那将是下一章的主题。
- en: 2.10 Exercises
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.10 练习
- en: Using PyMC, change the parameters of the prior Beta distribution in `our_first_model`
    to match those of the previous chapter. Compare the results to the previous chapter.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PyMC，将`our_first_model`中Beta分布的先验参数更改为与上一章相匹配的参数。将结果与上一章进行比较。
- en: Compare the model `our_first_model` with prior *θ* ∼ Beta(1*,*1) with a model
    with prior *θ* ∼![](img/U.PNG)(0*,*1). Are the posteriors similar or different?
    Is the sampling slower, faster, or the same? What about using a Uniform over a
    different interval such as [-1, 2]? Does the model run? What errors do you get?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型`our_first_model`与先前的*θ* ∼ Beta(1*,*1)的模型进行比较，再与具有先验*θ* ∼ ![](img/U.PNG)(0*,*1)的模型进行比较。后验分布相似还是不同？采样是更慢、更快还是相同？如果使用不同区间（如[-1,
    2]）的均匀分布会怎样？模型能运行吗？会遇到什么错误？
- en: PyMC has a function `pm.model_to_graphviz` that can be used to visualize the
    model. Use it to visualize the model `our_first_model`. Compare the result with
    the Kruschke diagram. Use `pm.model_to_graphviz` to visualize model `comparing_groups`.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyMC有一个函数`pm.model_to_graphviz`，可以用来可视化模型。使用它来可视化`our_first_model`模型。将结果与Kruschke图进行比较。使用`pm.model_to_graphviz`来可视化模型`comparing_groups`。
- en: Read about the coal mining disaster model that is part of the PyMC documentation
    ( [https://shorturl.at/hyCX2](https://shorturl.at/hyCX2)). Try to implement and
    run this model by yourself.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读关于PyMC文档中煤矿灾难模型的内容（[https://shorturl.at/hyCX2](https://shorturl.at/hyCX2)）。尝试自己实现并运行这个模型。
- en: Modify `model_g`, change the prior for the mean to a Gaussian distribution centered
    at the empirical mean, and play with a couple of reasonable values for the standard
    deviation of this prior. How robust/sensitive are the inferences to these changes?
    What do you think of using a Gaussian, which is an unbounded distribution (goes
    from −inf to inf), to model bounded data such as this? Remember that we said it
    is not possible to observe values below 0 or above 100.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`model_g`，将均值的先验改为以经验均值为中心的高斯分布，并尝试一些合理的标准差值来调整这个先验。推断对这些变化的稳健性/敏感度如何？你认为使用高斯分布（一种无界分布，范围从-∞到∞）来建模像这样的有界数据（数据在0到100之间）怎么样？记住我们曾说过，不可能观察到低于0或高于100的值。
- en: Using the data from the `chemical_shifts.csv` file, compute the empirical mean
    and the standard deviation with and without outliers. Compare those results to
    the Bayesian estimation using the Gaussian and Student’s t-distribution. What
    do you observe?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`chemical_shifts.csv`文件中的数据，计算有无异常值的经验均值和标准差。将这些结果与使用高斯分布和Student’s t分布的贝叶斯估计结果进行比较。你观察到了什么？
- en: Repeat the previous exercise by adding more outliers to `chemical_shifts.csv`,
    and compute new posteriors for `model_g` and `model_t` using this new data. What
    do you observe?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过向`chemical_shifts.csv`中添加更多异常值来重复上一个练习，并使用这些新数据为`model_g`和`model_t`计算新的后验分布。你观察到了什么？
- en: Explore the InferenceData object `idata_cg`.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索`idata_cg`的InferenceData对象。
- en: How many groups does it contain?
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含多少个组？
- en: Inspect the posterior distribution of the parameter *μ* for a specific day using
    the `sel` method.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`sel`方法检查特定日期参数*μ*的后验分布。
- en: Compute the distributions of mean differences between Thursday and Sunday. What
    are the coordinates and dimensions of the resulting DataArray?
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算星期四和星期日之间均值差异的分布。结果的DataArray的坐标和维度是什么？
- en: For the tips example compute the probability of superiority directly from the
    posterior (without computing Cohen’s d first). You can use the `pm.sample_posterior_predictive()`
    function to take a sample from each group. Is it different from the calculation
    assuming normality? Can you explain the result?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于提示示例，直接从后验分布计算优势概率（无需先计算Cohen’s d）。你可以使用`pm.sample_posterior_predictive()`函数从每个组中采样。这与假设正态分布的计算结果有何不同？你能解释结果吗？
- en: Join our community Discord space
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，并与5000多名成员一起学习：[https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
