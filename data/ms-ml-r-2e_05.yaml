- en: More Classification Techniques - K-Nearest Neighbors and Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多分类技术 - K-最近邻和支持向量机
- en: '"Statistical thinking will one day be as necessary for efficient citizenship
    as the ability to read and write."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “统计思维终将像阅读和写作能力一样，对于有效公民是必要的。”
- en: '- H.G. Wells'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- H.G.威尔斯'
- en: In [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic Regression
    and Discriminant Analysis*, we discussed using logistic regression to determine
    the probability that a predicted observation belongs to a categorical response
    what we refer to as a classification problem. Logistic regression was just the
    beginning of classification methods, with a number of techniques that we can use
    to improve our predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml)《逻辑回归与判别分析》中，我们讨论了使用逻辑回归来确定预测观察值属于分类响应的概率，我们称之为分类问题。逻辑回归只是分类方法的开端，我们有多种技术可以用来提高我们的预测。
- en: 'In this chapter, we will delve into two nonlinear techniques: **K-Nearest Neighbors**
    (**KNN**) and **Support Vector Machines** (**SVM**). These techniques are more
    sophisticated than what we''ve discussed earlier because the assumptions on linearity
    can be relaxed, which means a linear combination of the features in order to define
    the decision boundary is not needed. Be forewarned though, that this does not
    always equal superior predictive ability. Additionally, these models can be a
    bit problematic to interpret for business partners and they can be computationally
    inefficient. When used wisely, they provide a powerful complement to the other
    tools and techniques discussed in this book. They can be used for continuous outcomes
    in addition to classification problems; however, for the purposes of this chapter,
    we will focus only on the latter.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究两种非线性技术：**K-最近邻**（**KNN**）和**支持向量机**（**SVM**）。这些技术比我们之前讨论的更复杂，因为可以放宽对线性的假设，这意味着不需要特征线性组合来定义决策边界。但请提前警告，这并不总是等于更优越的预测能力。此外，这些模型对于商业伙伴来说可能有点难以解释，它们在计算上可能效率不高。当明智地使用时，它们为本书中讨论的其他工具和技术提供了强大的补充。除了分类问题外，它们还可以用于连续结果；然而，为了本章的目的，我们将仅关注后者。
- en: After a high-level background on the techniques, we will lay out the business
    case and then put both of them to the test in order to determine the best method
    of the two, starting with KNN.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在对技术进行高级背景介绍之后，我们将阐述商业案例，然后对这两种方法进行测试，以确定两种方法中最好的方法，从KNN开始。
- en: K-nearest neighbors
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻
- en: In our previous efforts, we built models that had coefficients or, said another
    way, parameter estimates for each of our included features. With KNN, we have
    no parameters as the learning method is the so-called instance-based learning.
    In short, *The labeled examples (inputs and corresponding output labels) are stored
    and no action is taken until a new input pattern demands an output value*. (Battiti
    and Brunato, 2014, p. 11). This method is commonly called **lazy learning**, as
    no specific model parameters are produced. The `train` instances themselves represent
    the knowledge. For the prediction of any new instance (a new data point), the
    `train` data is searched for an instance that most resembles the new instance
    in question. KNN does this for a classification problem by looking at the closest
    points-the nearest neighbors to determine the proper class. The *k* comes into
    play by determining how many neighbors should be examined by the algorithm, so
    if *k=5*, it will examine the five nearest points. A weakness of this method is
    that all five points are given equal weight in the algorithm even if they are
    less relevant in learning. We will look at the methods using R and try to alleviate
    this issue.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的努力中，我们构建了具有系数或，换句话说，每个包含特征的参数估计值的模型。在KNN中，我们没有参数，因为学习方法被称为基于实例的学习。简而言之，*标记的示例（输入和相应的输出标签）被存储起来，直到一个新的输入模式需要输出值时才采取行动*。（Battiti和Brunato，2014，第11页）。这种方法通常被称为**懒惰学习**，因为它不产生特定的模型参数。`train`实例本身代表知识。对于任何新的实例（新的数据点）的预测，算法将搜索与所讨论的新实例最相似的实例。KNN通过查看最近的点——最近的邻居来确定适当的类别。*k*的作用在于确定算法应该检查多少个邻居，所以如果*k=5*，它将检查五个最近的点。这种方法的一个弱点是，即使在学习上不那么相关，算法中仍然会给所有五个点相同的权重。我们将研究使用R的方法，并试图减轻这个问题。
- en: 'The best way to understand how this works is with a simple visual example of
    a binary classification learning problem. In the following figure, we have a plot
    of whether a tumor is **benign** or **malignant** based on two predictive features.
    The **X** in the plot indicates a new observation that we would like to predict.
    If our algorithm considers **K=3**, the circle encompasses the three observations
    that are nearest to the one that we want to score. As the most commonly occurring
    classifications are **malignant**, the **X** data point is classified as **malignant,**
    as shown in the following figure:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一过程最好的方式是使用一个简单的二元分类学习问题的视觉例子。在下面的图中，我们有一个基于两个预测特征的肿瘤是**良性**还是**恶性**的图。图中的**X**表示我们想要预测的新观察值。如果我们的算法考虑**K=3**，则圆圈包括与我们要评分的那个点最近的三个观察值。由于最常见的分类是**恶性**，所以**X**数据点被分类为**恶性**，如下面的图所示：
- en: '![](img/B06473_05_01.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06473_05_01.png)'
- en: Even from this simple example, it is clear that the selection of *k* for the
    nearest neighbors is critical. If *k* is too small, you may have a high variance
    on the `test` set observations even though you have a low bias. On the other hand,
    as *k* grows, you may decrease your variance but the bias may be unacceptable.
    Cross-validation is necessary to determine the proper *k*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 即使从这个简单的例子中，也可以清楚地看出，对于最近邻算法中 *k* 的选择至关重要。如果 *k* 太小，即使你具有很低的偏差，测试集上的观察值可能仍然会有很高的方差。另一方面，随着
    *k* 的增加，你可能降低了方差，但偏差可能是不可以接受的。交叉验证是确定合适的 *k* 的必要手段。
- en: 'It is also important to point out the calculation of the distance or the nearness
    of the data points in our feature space. The default distance is **Euclidean Distance**.
    This is simply the straight-line distance from point `A` to point `B`-as the crow
    flies-or you can utilize the formula that it is equivalent to the square root
    of the sum of the squared differences between the corresponding points. The formula
    for `Euclidean Distance`, given point `A` and `B` with coordinates `p1`, `p2`,
    ... `pn` and `q1`, `q2`,... `qn` respectively, would be as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 还很重要的一点是要指出，在我们的特征空间中计算数据点的距离或邻近度。默认的距离是**欧几里得距离**。这简单地说就是从点 `A` 到点 `B` 的直线距离——就像飞鸟一样飞过去——或者你可以利用公式，它等同于对应点之间平方差的和的平方根。给定点
    `A` 和 `B` 的坐标 `p1`，`p2`，... `pn` 和 `q1`，`q2`，... `qn` 的欧几里得距离公式如下：
- en: '![](img/image_05_02-2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_02-2.png)'
- en: This distance is highly dependent on the scale that the features were measured
    on, so it is critical to standardize them. Other distance calculations as well
    as weights, can be used depending on the distance. We will explore this in the
    upcoming example.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种距离高度依赖于特征所测量的尺度，因此标准化它们是至关重要的。根据距离，还可以使用其他距离计算以及权重。我们将在接下来的例子中探讨这一点。
- en: Support vector machines
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: The first time I heard of support vector machines, I have to admit that I was
    scratching my head, thinking that this was some form of an academic obfuscation
    or inside joke. However, my open-minded review of SVM has replaced this natural
    skepticism with a healthy respect for the technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次听说支持向量机时，必须承认我感到困惑，认为这可能是某种学术上的混淆或内部玩笑。然而，我对SVM的开放性审查取代了这种自然的怀疑，对这项技术产生了健康的尊重。
- en: '*SVMs have been shown to perform well in a variety of settings and are often
    considered one of the best "out-of-the-box" classifiers *(James, G., 2013). To
    get a practical grasp of the subject, let''s look at another simple visual example.
    In the following figure, you will see that the classification task is linearly
    separable. However, the dotted line and solid line are just two among an infinite
    number of possible linear solutions.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*支持向量机（SVMs）已被证明在各种环境中表现良好，通常被认为是最好的“开箱即用”分类器之一（James, G., 2013）。为了对这一主题有一个实际的了解，让我们来看另一个简单的视觉例子。在下面的图中，你会看到分类任务是线性可分的。然而，虚线和实线只是无数可能的线性解中的两种。'
- en: 'You would have separating hyperplanes in a problem that has more than two dimensions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个超过两个维度的问题上，你会有一个分离的超平面：
- en: '![](img/image_05_02-1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_02-1.png)'
- en: So many solutions can be problematic for generalization because whatever solution
    you choose, any new observation to the right of the line will be classified as
    **benign**, and to the left of the line, it will be classified as **malignant**.
    Therefore, either line has no bias on the `train` data but may have a widely divergent
    error on any data to test. This is where the support vectors come into play. The
    probability that a point falls on the wrong side of the linear separator is higher
    for the dotted line than the solid line, which means that the solid line has a
    higher margin of safety for classification. Therefore, as Battiti and Brunato
    say, *SVMs are linear separators with the largest possible margin and the support
    vectors the ones touching the safety margin region on both sides*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，许多解决方案都可能对泛化造成问题，因为无论你选择哪种解决方案，任何位于线右侧的新观测值都将被分类为**良性**，而位于线左侧的观测值将被分类为**恶性**。因此，任何一条线在`train`数据上都没有偏差，但在任何测试数据上可能存在广泛的误差。这就是支持向量发挥作用的地方。点落在线性分隔符错误一侧的概率对于虚线比实线更高，这意味着实线在分类上具有更高的安全边际。因此，正如Battiti和Brunato所说，*支持向量机（SVMs）是具有可能最大间隔的线性分隔符，而支持向量是接触两侧安全边际区域的那些*。
- en: 'The following figure illustrates this idea. The thin solid line is the optimal
    linear separator to create the aforementioned largest possible margin, thus increasing
    the probability that a new observation will fall on the correct side of the separator.
    The thicker black lines correspond to the safety margin, and the shaded data points
    constitute the support vectors. If the support vectors were to move, then the
    margin and, subsequently, the decision boundary would change. The distance between
    the separators is known as the **margin**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了这一概念。细实线是创建上述可能的最大间隔的最优线性分隔符，从而增加了新观测值落在分隔符正确一侧的概率。较粗的黑线对应于安全间隔，而阴影数据点构成了支持向量。如果支持向量移动，那么间隔以及随之而来的决策边界将发生变化。分隔符之间的距离被称为**间隔**：
- en: '![](img/B06473_05_03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06473_05_03.png)'
- en: This is all fine and dandy, but the real-world problems are not so clear cut.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来都很不错，但现实世界的问题并不那么清晰。
- en: In data that is not linearly separable, many observations will fall on the wrong
    side of the margin (the so-called slack variables), which is a misclassification.
    The key to building an SVM algorithm is to solve for the optimal number of support
    vectors via cross-validation. Any observation that lies directly on the wrong
    side of the margin for its class is known as a **support vector**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性可分的数据中，许多观测值将落在间隔的错误一侧（所谓的松弛变量），这是一种误分类。构建SVM算法的关键是通过交叉验证求解最优支持向量数量。任何位于其类别间隔错误一侧的观测值被称为**支持向量**。
- en: If the tuning parameter for the number of errors is too large, which means that
    you have many support vectors, you will suffer from a high bias and low variance.
    On the other hand, if the tuning parameter is too small, the opposite might occur.
    According to James et al., who refer to the tuning parameter as `C`, as `C` decreases,
    the tolerance for observations being on the wrong side of the margin decreases
    and the margin narrows. This `C`, or rather, the cost function, simply allows
    for observations to be on the wrong side of the margin. If `C` were set to zero,
    then we would prohibit a solution where any observation violates the margin.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果错误数量的调整参数太大，这意味着你有许多支持向量，你将遭受高偏差和低方差。另一方面，如果调整参数太小，相反的情况可能发生。James等人将调整参数称为`C`，随着`C`的减小，对观测值位于间隔错误一侧的容忍度降低，间隔变窄。这个`C`，或者更确切地说，成本函数，只是允许观测值位于间隔错误一侧。如果`C`设置为零，那么我们将禁止任何违反间隔的解决方案。
- en: Another important aspect of SVM is the ability to model nonlinearity with quadratic
    or higher order polynomials of the input features. In SVMs, this is known as the
    **kernel trick**. These can be estimated and selected with cross-validation. In
    the example, we will look at the alternatives.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的另一个重要方面是能够使用输入特征的二次或更高阶多项式来建模非线性。在SVM中，这被称为**核技巧**。这些可以通过交叉验证进行估计和选择。在示例中，我们将查看替代方案。
- en: As with any model, you can expand the number of features using polynomials to
    various degrees, interaction terms, or other derivations. In large datasets, the
    possibilities can quickly get out of control. The kernel trick with SVMs allows
    us to efficiently expand the feature space, with the goal that you achieve an
    approximate linear separation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何模型一样，您可以使用多项式以各种程度扩展特征数量，交互项或其他推导。在大数据集中，可能性可能会迅速失控。SVM 中的核技巧允许我们有效地扩展特征空间，目标是实现近似的线性分离。
- en: 'To check out how this is done, first look at the SVM optimization problem and
    its constraints. We are trying to achieve the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这是如何实现的，首先看看 SVM 优化问题和其约束。我们试图实现以下目标：
- en: Create weights that maximize the margin
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建最大化边界的权重
- en: Subject to the constraints, no (or as few as possible) data points should lie
    within that margin
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在约束条件下，没有（或尽可能少）数据点应该位于该边界内
- en: Now, unlike linear regression, where each observation is multiplied by a weight,
    in SVM, the weights are applied to the inner products of just the support vector
    observations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与线性回归不同，在 SVM 中，权重应用于仅仅是支持向量观察值的内积。
- en: 'What does this mean? Well, an inner product for two vectors is just the sum
    of the paired observations'' product. For example, if vector one is *3*, *4*,
    and *2* and vector two is *1*, *2*, and *3*, then you end up with *(3x1) + (4x2)
    + (2x3)* or *17*. With SVMs, if we take a possibility that an inner product of
    each observation has an inner product of every other observation, this amounts
    to the formula that there would be *n(n-1)/2* combinations, where *n* is the number
    of observations. With just *10* observations, we end up with *45* inner products.
    However, SVM only concerns itself with the support vectors'' observations and
    their corresponding weights. For a linear SVM classifier, the formula is the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么？嗯，两个向量的内积只是配对观察值的乘积之和。例如，如果向量一是 *3*，*4* 和 *2*，向量二是 *1*，*2* 和 *3*，那么你得到
    *(3x1) + (4x2) + (2x3)* 或 *17*。在 SVM 中，如果我们假设每个观察值的内积与每个其他观察值的内积都有内积，这相当于有 *n(n-1)/2*
    种组合，其中 *n* 是观察值的数量。仅用 *10* 个观察值，我们就会得到 *45* 个内积。然而，SVM 只关注支持向量观察值及其相应的权重。对于线性
    SVM 分类器，公式如下：
- en: '![](img/image_05_05-1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_05-1.png)'
- en: Here, `(x, xi)` are the inner products of the support vectors, as `α` is non-zero
    only when an observation is a support vector.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`(x, xi)` 是支持向量的内积，因为只有当观察值是支持向量时，`α` 才是非零的。
- en: This leads to far fewer terms in the classification algorithm and allows the
    use of the `kernel` function, commonly referred to as the kernel trick.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致分类算法中的术语数量大大减少，并允许使用所谓的核函数，通常称为核技巧。
- en: The trick in this is that the `kernel` function mathematically summarizes the
    transformation of the features in higher dimensions instead of creating them explicitly.
    In a simplistic sense, a kernel function computes a dot product between two vectors.
    This has the benefit of creating the higher dimensional, nonlinear space, and
    decision boundary while keeping the optimization problem computationally efficient.
    The `kernel` functions compute the inner product in a higher dimensional space
    without transforming them into the higher dimensional space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的技巧在于，核函数在数学上总结了特征在更高维度的转换，而不是显式地创建它们。从简单意义上讲，核函数计算两个向量之间的点积。这有利于创建更高维的非线性空间和决策边界，同时保持优化问题在计算上高效。核函数在更高维空间中计算内积，而不将它们转换到更高维空间。
- en: 'The notation for popular kernels is expressed as the inner (dot) product of
    the features, with `x[i]` and `x[j]` representing vectors, gamma, and `c` parameters,
    as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流行的核函数的表示是特征的内积（点积），其中 `x[i]` 和 `x[j]` 代表向量，gamma 和 `c` 参数，如下所示：
- en: '![](img/image_05_06-1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_06-1.png)'
- en: As for the selection of the nonlinear techniques, they require some trial and
    error, but we will walk through the various selection techniques.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 至于非线性技术的选择，它们需要一些尝试和错误，但我们将介绍各种选择技术。
- en: Business case
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业案例
- en: In the upcoming case study, we will apply KNN and SVM to the same dataset. This
    will allow us to compare the R code and learning methods on the same problem,
    starting with KNN. We will also spend some time drilling down into the confusion
    matrix, comparing a number of statistics to evaluate model accuracy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的案例研究中，我们将对同一数据集应用KNN和SVM。这将使我们能够在同一问题上比较R代码和学习方法，从KNN开始。我们还将花时间深入研究混淆矩阵，比较多个统计数据以评估模型准确性。
- en: Business understanding
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: The data that we will examine was originally collected by the **National Institute
    of Diabetes and Digestive and Kidney Diseases** (**NIDDK**). It consists of `532`
    observations and eight input features along with a binary outcome (`Yes`/`No`).
    The patients in this study were of Pima Indian descent from South Central Arizona.
    The NIDDK data shows that for the last 30 years, research has helped scientists
    to prove that obesity is a major risk factor in the development of diabetes. The
    Pima Indians were selected for the study as one-half of the adult Pima Indians
    have diabetes and 95 per cent of those with diabetes are overweight. The analysis
    will focus on adult women only. Diabetes was diagnosed according to the WHO criteria
    and was of the type of diabetes that is known as **type 2**. In this type of diabetes,
    the pancreas is still able to function and produce insulin and it used to be referred
    to as non-insulin-dependent diabetes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要分析的数据最初是由**美国糖尿病和消化系统及肾脏疾病研究所**（**NIDDK**）收集的。它包括`532`个观测值和八个输入特征，以及一个二元结果（`是`/`否`）。本研究中的患者来自亚利桑那州南部的皮马印第安人。NIDDK数据显示，在过去30年里，研究帮助科学家证明肥胖是糖尿病发展中的一个主要风险因素。皮马印第安人被选中进行研究，因为一半的成年皮马印第安人患有糖尿病，其中95%的糖尿病患者体重超重。分析将仅关注成年女性。糖尿病是根据世界卫生组织（WHO）的标准进行诊断的，并且是被称为**2型**的糖尿病。在这种类型的糖尿病中，胰腺仍然能够产生胰岛素并发挥作用，它过去被称为非胰岛素依赖型糖尿病。
- en: Our task is to examine and predict those individuals that have diabetes or the
    risk factors that could lead to diabetes in this population. Diabetes has become
    an epidemic in the USA, given the relatively sedentary lifestyle and high-caloric
    diet. According to the **American Diabetes Association** (**ADA**), the disease
    was the seventh leading cause of death in the USA in 2010, despite being underdiagnosed.
    Diabetes is also associated with a dramatic increase in comorbidities, such as
    hypertension, dyslipidemia, stroke, eye diseases, and kidney diseases. The costs
    of diabetes and its complications are enormous. The ADA estimates that the total
    cost of the disease in 2012 was approximately $490 billion. For further background
    information on the problem, refer to ADA's website at [http://www.diabetes.org/diabetes-basics/statistics/](http://www.diabetes.org/diabetes-basics/statistics/).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是检查和预测这个群体中患有糖尿病或可能导致糖尿病的风险因素的个人。鉴于相对久坐的生活方式和高热量饮食，糖尿病在美国已成为一种流行病。根据**美国糖尿病协会**（**ADA**）的数据，2010年，尽管糖尿病的确诊率较低，但该疾病是美国第七大死因。糖尿病还与许多合并症的增加有关，如高血压、血脂异常、中风、眼科疾病和肾脏疾病。糖尿病及其并发症的成本是巨大的。ADA估计，2012年该疾病的总成本约为4900亿美元。有关该问题的更多背景信息，请参阅ADA网站[http://www.diabetes.org/diabetes-basics/statistics/](http://www.diabetes.org/diabetes-basics/statistics/)。
- en: Data understanding and preparation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'The dataset for the `532` women is in two separate data frames. The variables
    of interest are as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`532`名女性的数据集分为两个独立的数据框。感兴趣的变量如下：'
- en: '`npreg`: This is the number of pregnancies'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`npreg`: 这代表怀孕次数'
- en: '`glu`: This is the plasma glucose concentration in an oral glucose tolerance
    test'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glu`: 这代表口服葡萄糖耐量测试中的血浆葡萄糖浓度'
- en: '`bp`: This is the diastolic blood pressure (mm Hg)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bp`: 这代表舒张压（毫米汞柱）'
- en: '`skin`: This is triceps skin-fold thickness measured in mm'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skin`: 这代表三头肌皮肤褶皱厚度（毫米）'
- en: '`bmi`: This is the body mass index'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bmi`: 这代表体质指数'
- en: '`ped`: This is the diabetes pedigree function'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ped`: 这代表糖尿病家系函数'
- en: '`age`: This is the age in years'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`: 这代表年龄（年）'
- en: '`type`: This is diabetic, `Yes` or `No`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`type`: 这代表糖尿病，`是`或`否`'
- en: The datasets are contained in the R package, `MASS`. One data frame is named
    `Pima.tr` and the other is named `Pima.te`. Instead of using these as separate
    `train` and `test` sets, we will combine them and create our own in order to discover
    how to do such a task in R.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含在R包`MASS`中。一个数据框命名为`Pima.tr`，另一个命名为`Pima.te`。我们不会将它们用作独立的`训练`和`测试`集，而是将它们合并，并创建我们自己的，以便发现如何在R中完成此类任务。
- en: 'To begin, let''s load the following packages that we will need for the exercise:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载以下我们将需要用于练习的包：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will now load the datasets and check their structure, ensuring that they
    are the same, starting with `Pima.tr`, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将加载数据集并检查它们的结构，确保它们相同，从`Pima.tr`开始，如下所示：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Looking at the structures, we can be confident that we can combine the data
    frames into one. This is very easy to do using the `rbind()` function, which stands
    for row binding and appends the data. If you had the same observations in each
    frame and wanted to append the features, you would bind them by columns using
    the `cbind()` function. You will simply name your new data frame and use this
    syntax: `new data = rbind(data frame1, data frame2)`. Our code thus becomes the
    following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 观察结构，我们可以确信我们可以将数据框合并为一个。这使用`rbind()`函数非常容易完成，该函数代表行绑定并附加数据。如果你在每个框架中都有相同的观测值并且想要附加特征，你会使用`cbind()`函数按列绑定它们。你只需命名你的新数据框并使用此语法：`new
    data = rbind(data frame1, data frame2)`。因此，我们的代码如下所示：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As always, double-check the structure. We can see that there are no issues:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，请再次检查结构。我们可以看到没有问题：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s do some exploratory analysis by putting this in boxplots. For this,
    we want to use the outcome variable, `"type"`, as our ID variable. As we did with
    logistic regression, the `melt()` function will do this and prepare a data frame
    that we can use for the boxplots. We will call the new data frame `pima.melt`,
    as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过箱线图进行一些探索性分析。为此，我们希望使用结果变量`"type"`作为我们的ID变量。正如我们在逻辑回归中所做的那样，`melt()`函数将执行此操作并准备一个我们可以用于箱线图的数据框。我们将新数据框命名为`pima.melt`，如下所示：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The boxplot layout using the `ggplot2` package is quite effective, so we will
    use it. In the `ggplot()` function, we will specify the data to use, the `x` and
    `y` variables, and what type of plot, and create a series of plots with two columns.
    In the following code, we will put the response variable as `x` and its value
    as `y` in `aes()`. Then, `geom_boxplot()` creates the boxplots. Finally, we will
    build the boxplots in two columns with `facet_wrap()`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ggplot2`包的箱线图布局非常有效，因此我们将使用它。在`ggplot()`函数中，我们将指定要使用的数据、`x`和`y`变量以及图表类型，并创建一系列两列的图表。在以下代码中，我们将响应变量作为`x`和其值作为`y`在`aes()`中。然后，`geom_boxplot()`创建箱线图。最后，我们将使用`facet_wrap()`在两列中构建箱线图：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is the output of the preceding command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为前一个命令的输出：
- en: '![](img/B06473_05_04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06473_05_04.png)'
- en: 'This is an interesting plot because it is difficult to discern any dramatic
    differences in the plots, probably with the exception of **glucose** (**glu**).
    As you may have suspected, the fasting glucose appears to be significantly higher
    in the patients currently diagnosed with diabetes. The main problem here is that
    the plots are all on the same y-axis scale. We can fix this and produce a more
    meaningful plot by standardizing the values and then re-plotting. R has a built-in
    function, `scale()`, which will convert the values to a mean of zero and a standard
    deviation of one. Let''s put this in a new data frame called `pima.scale`, converting
    all of the features and leaving out the `type` response. Additionally, while doing
    KNN, it is important to have the features on the same scale with a mean of zero
    and a standard deviation of one. If not, then the distance calculations in the
    nearest neighbor calculation are flawed. If something is measured on a scale of
    1 to 100, it will have a larger effect than another feature that is measured on
    a scale of 1 to 10\. Note that when you scale a data frame, it automatically becomes
    a matrix. Using the `data.frame()` function, convert it back to a data frame,
    as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的图表，因为很难在图表中看出任何明显的差异，可能除了**葡萄糖**（**glu**）之外。正如你可能所怀疑的，空腹血糖似乎在目前被诊断为糖尿病的患者中显著更高。这里的主要问题是所有图表都在相同的y轴刻度上。我们可以通过标准化值然后重新绘图来解决这个问题，从而生成一个更有意义的图表。R有一个内置函数`scale()`，它将值转换为均值为零和标准差为一。让我们将其放入一个新的数据框`pima.scale`中，转换所有特征并排除`type`响应。此外，在进行KNN时，确保特征具有相同的尺度，均值为零和标准差为一非常重要。如果不是这样，那么最近邻计算中的距离计算将是错误的。如果某个东西是在1到100的尺度上测量的，它将比在1到10的尺度上测量的另一个特征有更大的影响。请注意，当你缩放数据框时，它自动变成一个矩阵。使用`data.frame()`函数，将其转换回数据框，如下所示：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we will need to include the response in the data frame, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要在数据框中包含响应，如下所示：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s just repeat the boxplotting process again with `melt()` and `ggplot()`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次使用`melt()`和`ggplot()`重复箱线图的过程：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the output of the preceding command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在先前的命令输出：
- en: '![](img/image_05_05.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_05_05.png)'
- en: With the features scaled, the plot is easier to read. In addition to glucose,
    it appears that the other features may differ by `type`, in particular, `age`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放后，图表更容易阅读。除了葡萄糖之外，其他特征似乎可能因`type`而异，特别是`age`。
- en: 'Before splitting this into `train` and `test` sets, let''s have a look at the
    correlation with the R function, `cor()`. This will produce a matrix instead of
    a plot of the Pearson correlations:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据分割成`train`和`test`集之前，让我们看看与R函数`cor()`的相关性。这将产生一个矩阵而不是皮尔逊相关性的图表：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are a couple of correlations to point out: `npreg`/`age` and `skin`/`bmi`.
    Multicollinearity is generally not a problem with these methods, assuming that
    they are properly trained and the hyperparameters are tuned.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个相关性需要指出：`npreg`/`age`和`skin`/`bmi`。在适当训练和调整超参数的情况下，多重共线性通常不是问题。
- en: 'I think we are now ready to create the `train` and `test` sets, but before
    we do so, I recommend that you always check the ratio of `Yes` and `No` in our
    response. It is important to make sure that you will have a balanced split in
    the data, which may be a problem if one of the outcomes is sparse. This can cause
    a bias in a classifier between the majority and minority classes. There is no
    hard and fast rule on what is an improper balance. A good rule of thumb is that
    you strive for at least a 2:1 ratio in the possible outcomes (He and Wa, 2013):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们现在已经准备好创建`train`和`test`集了，但在这样做之前，我建议您始终检查我们响应中`Yes`和`No`的比例。确保数据有平衡的分割很重要，如果其中一个结果稀疏，可能会成为问题。这可能导致分类器在多数和少数类别之间产生偏差。关于什么是不适当的平衡没有硬性规定。一个很好的经验法则是，您应努力实现至少2:1的可能结果比例（He和Wa，2013）：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The ratio is 2:1 so we can create the `train` and `test` sets with our usual
    syntax using a 70/30 split in the following way:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 比例是2:1，因此我们可以使用我们常用的语法创建`train`和`test`集，以下是这样进行70/30分割的方式：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: All seems to be in order, so we can move on to building our predictive models
    and evaluating them, starting with KNN.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一切似乎都井然有序，因此我们可以继续构建我们的预测模型并评估它们，从KNN开始。
- en: Modeling and evaluation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: Now we will discuss various aspects pertaining to modeling and evaluation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论与建模和评估相关的各个方面。
- en: KNN modeling
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN建模
- en: 'As previously mentioned, it is critical to select the most appropriate parameter
    (`k` or `K`) when using this technique. Let''s put the `caret` package to good
    use again in order to identify `k`. We will create a grid of inputs for the experiment,
    with `k` ranging from `2` to `20` by an increment of `1`. This is easily done
    with the `expand.grid()` and `seq()` functions. The `caret` package parameter
    that works with the KNN function is simply `.k`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在使用此技术时，选择最合适的参数（`k`或`K`）至关重要。让我们再次充分利用`caret`包来识别`k`。我们将为实验创建一个输入网格，`k`的范围从`2`到`20`，增量是`1`。这可以通过`expand.grid()`和`seq()`函数轻松完成。与KNN函数一起工作的`caret`包参数是简单的`.k`：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will also incorporate cross-validation in the selection of the parameter,
    creating an object called `control` and utilizing the `trainControl()` function
    from the `caret` package, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将结合交叉验证来选择参数，创建一个名为`control`的对象，并使用`caret`包中的`trainControl()`函数，如下所示：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we can create the object that will show us how to compute the optimal
    `k` value with the `train()` function, which is also part of the `caret` package.
    Remember that while conducting any sort of random sampling, you will need to set
    the `seed` value as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个对象，该对象将显示如何使用`train()`函数计算最优的`k`值，这也是`caret`包的一部分。记住，在进行任何形式的随机抽样时，您需要设置`seed`值如下：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The object created by the `train()` function requires the model formula, `train`
    data name, and an appropriate method. The model formula is the same as we''ve
    used before-`y~x`. The method designation is simply `knn`. With this in mind,
    this code will create the object that will show us the optimal `k` value, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由`train()`函数创建的对象需要模型公式、`train`数据名称和适当的方法。模型公式与我们之前使用的一样-`y~x`。方法指定是简单的`knn`。考虑到这一点，以下代码将创建一个对象，该对象将显示最优的`k`值，如下所示：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Calling the object provides us with the `k` parameter that we are seeking,
    which is `k=17`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 调用对象为我们提供了我们正在寻找的`k`参数，即`k=17`：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In addition to the results that yield `k=17`, we get the information in the
    form of a table on the `Accuracy` and `Kappa` statistics and their standard deviations
    from the cross-validation. `Accuracy` tells us the percentage of observations
    that the model classified correctly. `Kappa` refers to what is known as **Cohen's
    Kappa statistic**. The `Kappa` statistic is commonly used to provide a measure
    of how well two evaluators can classify an observation correctly. It provides
    an insight into this problem by adjusting the accuracy scores, which is done by
    accounting for the evaluators being totally correct by mere chance. The formula
    for the statistic is *Kappa = (per cent of agreement - per cent of chance agreement)
    / (1 - per cent of chance agreement)*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了产生`k=17`的结果外，我们还以表格的形式获得了`Accuracy`和`Kappa`统计量及其标准差的信息，这些信息来自交叉验证。`Accuracy`告诉我们模型正确分类的观测值的百分比。`Kappa`指的是所谓的**Cohen's
    Kappa统计量**。`Kappa`统计量通常用于衡量两个评估者正确分类观测值的能力。它通过调整准确率得分来提供对这一问题的洞察，这是通过考虑到评估者仅通过偶然完全正确来实现的。该统计量的公式为
    *Kappa = (一致百分比 - 偶然一致百分比) / (1 - 偶然一致百分比)*。
- en: The *per cent of agreement* is the rate that the evaluators agreed on for the
    class (accuracy), and *percent of chance agreement* is the rate that the evaluators
    randomly agreed on. The higher the statistic, the better they performed with the
    maximum agreement being one. We will work through an example when we will apply
    our model on the `test` data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**一致百分比**是评估者对类别的（准确率）达成一致的比例，**偶然一致百分比**是评估者随机达成一致的比例。该统计量越高，表现越好，最大一致度为1。当我们将在`test`数据上应用我们的模型时，我们将通过一个示例来演示这一点。'
- en: 'To do this, we will utilize the `knn()` function from the `class` package.
    With this function, we will need to specify at least four items. These would be
    the `train` inputs, the `test` inputs, correct labels from the `train` set, and
    `k`. We will do this by creating the `knn.test` object and see how it performs:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将利用`class`包中的`knn()`函数。使用此函数，我们需要指定至少四个项目。这些将是`train`输入、`test`输入、来自`train`集的正确标签以及`k`。我们将通过创建`knn.test`对象并查看其表现来完成此操作：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With the object created, let''s examine the confusion matrix and calculate
    the accuracy and `kappa`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了对象后，让我们检查混淆矩阵并计算准确率和`kappa`：
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The accuracy is done by simply dividing the correctly classified observations
    by the total observations:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是通过简单地将正确分类的观测值除以总观测值来计算的：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The accuracy of 71 per cent is less than that we achieved on the `train` data,
    which was almost eighty per cent. We can now produce the `kappa` statistic as
    follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 71%的准确率低于我们在`train`数据集上实现的准确率，后者接近80%。我们现在可以按照以下方式产生`kappa`统计量：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `kappa` statistic of 0.49 is what we achieved with the `train` set. Altman(1991)
    provides a heuristic to assist us in the interpretation of the statistic, which
    is shown in the following table:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`train`集，我们实现的`kappa`统计量为0.49。Altman（1991）提供了一个启发式方法来帮助我们解释这个统计量，如下表所示：
- en: '| **Value of *K*** | **Strength of Agreement** |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **K*的值** | **一致性强度** |'
- en: '| --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| <0.20 | Poor |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| <0.20 | 差 |'
- en: '| 0.21-0.40 | Fair |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 0.21-0.40 | 一般 |'
- en: '| 0.41-0.60 | Moderate |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 0.41-0.60 | 中等 |'
- en: '| 0.61-0.80 | Good |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 0.61-0.80 | 良好 |'
- en: '| 0.81-1.00 | Very good |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 0.81-1.00 | 非常好 |'
- en: With our `kappa` only moderate and with an accuracy just over 70 per cent on
    the `test` set, we should see whether we can perform better by utilizing weighted
    neighbors. A weighting schema increases the influence of neighbors that are closest
    to an observation versus those that are farther away. The farther away the observation
    is from a point in space, the more penalized its influence is. For this technique,
    we will use the `kknn` package and its `train.kknn()` function to select the optimal
    weighting scheme.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在`test`集上，我们的`kappa`仅中等，准确率略超过70%，我们应该看看是否可以通过利用加权邻居来表现得更好。加权方案增加了与观测值最近的邻居相对于较远邻居的影响力。观测值在空间中的位置越远，其影响力受到的惩罚就越大。对于这项技术，我们将使用`kknn`包及其`train.kknn()`函数来选择最佳加权方案。
- en: The `train.kknn()` function uses LOOCV that we examined in the prior chapters
    in order to select the best parameters for the optimal `k` neighbors, one of the
    two distance measures, and a `kernel` function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`train.kknn()`函数使用我们在前几章中检查过的LOOCV来选择最佳参数，包括最优`k`邻居、两种距离度量之一以及`kernel`函数。'
- en: The unweighted `k` neighbors algorithm that we created uses the Euclidian distance,
    as we discussed previously. With the `kknn` package, there are options available
    to compare the sum of the absolute differences versus the Euclidian distance.
    The package refers to the distance calculation used as the `Minkowski` parameter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的无权重`k`邻居算法使用的是欧几里得距离，正如我们之前讨论的那样。在`kknn`包中，有选项可以比较绝对差异之和与欧几里得距离。该包将使用的距离计算称为`Minkowski`参数。
- en: As for the weighting of the distances, many different methods are available.
    For our purpose, the package that we will use has ten different weighting schemas,
    which includes the unweighted ones. They are rectangular (unweighted), triangular,
    epanechnikov, biweight, triweight, cosine, inversion, gaussian, rank, and optimal.
    A full discussion of these weighting techniques is available in *Hechenbichler
    K.* and *Schliep K.P.* (2004).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 至于距离的加权，有许多不同的方法可供选择。为了我们的目的，我们将使用的包有十种不同的加权方案，包括无权重的方案。它们是矩形（无权重）、三角形、Epanechnikov、双权重、三权重、余弦、倒数、高斯、等级和最优。关于这些加权技术的全面讨论可以在*Hechenbichler
    K.*和*Schliep K.P.*（2004）中找到。
- en: 'For simplicity, let''s focus on just two: `triangular` and `epanechnikov`.
    Prior to having the weights assigned, the algorithm standardizes all the distances
    so that they are between zero and one. The triangular weighting method multiplies
    the observation distance by one minus the distance. With Epanechnikov, the distance
    is multiplied by ¾ times (one minus the distance two). For our problem, we will
    incorporate these weighting methods along with the standard unweighted version
    for comparison purposes.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们只关注两个：`triangular`和`epanechnikov`。在分配权重之前，算法将所有距离标准化，使它们介于零和一之间。三角形加权方法将观测距离乘以1减去距离。对于Epanechnikov，距离乘以3/4倍（1减去距离的平方）。对于我们的问题，我们将结合这些加权方法以及标准的无权重版本进行比较。
- en: 'After specifying a random seed, we will create the `train` set object with
    `kknn()`. This function asks for the maximum number of *k* values (`kmax`), `distance`
    (one is equal to Euclidian and two is equal to absolute), and `kernel`. For this
    model, `kmax` will be set to `25` and `distance` will be `2`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定随机种子后，我们将使用`kknn()`创建`train`集对象。此函数要求最大*k*值（`kmax`）、`distance`（一等于欧几里得距离，二等于绝对距离）和`kernel`。对于此模型，`kmax`将设置为`25`，`distance`将设置为`2`：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'A nice feature of the package is the ability to plot and compare the results,
    as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 该包的一个不错特性是能够绘制并比较结果，如下所示：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following is the output of the preceding command:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_05_06.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_06.png)'
- en: 'This plot shows **k** on the x-axis and the percentage of misclassified observations
    by `kernel`. To my pleasant surprise, the unweighted (**rectangular**) version
    at `k: 19` performs the best. You can also call the object to see what the classification
    error and the best parameter are in the following way:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '此图显示了x轴上的**k**值和由`kernel`错误分类的观测值的百分比。令我惊讶的是，在`k: 19`的无权重（**矩形**）版本表现最佳。您也可以调用该对象，以下列方式查看分类误差和最佳参数：'
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'So, with this data, weighting the distance does not improve the model accuracy
    in training and, as we can see here, didn''t even do as well on the test set:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用这些数据，对距离进行加权并没有提高模型在训练中的准确性，正如我们在这里所看到的，甚至在测试集上的表现也不尽如人意：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: There are other weights that we could try, but as I tried these other weights,
    the results that I achieved were not more accurate than these. We don't need to
    pursue KNN any further. I would encourage you to experiment with various parameters
    on your own to see how they perform.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试其他权重，但在我尝试了这些其他权重后，我所获得的结果并不比这些更准确。我们不需要进一步追求KNN。我鼓励您自己尝试不同的参数，看看它们的性能如何。
- en: SVM modeling
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM建模
- en: 'We will use the `e1071` package to build our SVM models. We will start with
    a linear support vector classifier and then move on to the nonlinear versions.
    The `e1071` package has a nice function for SVM called `tune.svm()`, which assists
    in the selection of the tuning parameters/kernel functions. The `tune.svm()` function
    from the package uses cross-validation to optimize the tuning parameters. Let''s
    create an object called `linear.tune` and call it using the `summary()` function,
    as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`e1071`包来构建我们的SVM模型。我们将从一个线性支持向量分类器开始，然后转向非线性版本。`e1071`包有一个用于SVM的不错函数，名为`tune.svm()`，它有助于选择调整参数/核函数。该包中的`tune.svm()`函数使用交叉验证来优化调整参数。让我们创建一个名为`linear.tune`的对象，并使用`summary()`函数调用它，如下所示：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The optimal `cost` function is one for this data and leads to a misclassification
    error of roughly 21 per cent. We can make predictions on the `test` data and examine
    that as well using the `predict()` function and applying `newdata = test`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，最优的`cost`函数会导致大约21%的错误分类率。我们可以使用`predict()`函数和`newdata = test`来对`test`数据进行预测并检查：
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The linear support vector classifier has slightly outperformed KNN on both the
    `train` and `test` sets. The `e1071` package has a nice function for SVM called
    `tune.svm()` that assists in the selection of the `tuning parameters/kernel` functions.
    We will now see if nonlinear methods will improve our performance and also use
    cross-validation to select tuning parameters.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 线性支持向量机分类器在`train`和`test`集上都略优于KNN。`e1071`包有一个用于SVM的不错函数`tune.svm()`，它有助于选择`调整参数/核函数`。现在我们将看看非线性方法是否会提高我们的性能，并使用交叉验证来选择调整参数。
- en: 'The first `kernel` function that we will try is `polynomial`, and we will be
    tuning two parameters: a degree of polynomial (`degree`) and kernel coefficient
    (`coef0`). The `polynomial` order will be `3`, `4`, and `5` and the coefficient
    will be in increments from `0.1` to `4`, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试的第一个核函数是`polynomial`，我们将调整两个参数：多项式的次数（`degree`）和核系数（`coef0`）。多项式的次数将是`3`、`4`和`5`，系数将从`0.1`增加到`4`，如下所示：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The model has selected `degree` of `3` for the polynomial and coefficient of
    `0.1`. Just as the linear SVM, we can create predictions on the `test` set with
    these parameters, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择了多项式的`degree`为`3`和系数为`0.1`。正如线性SVM一样，我们可以使用这些参数在`test`集上创建预测，如下所示：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This did not perform quite as well as the linear model. We will now run the
    radial basis function. In this instance, the one parameter that we will solve
    for is `gamma`, which we will examine in increments of `0.1` to `4`. If `gamma`
    is too small, the model will not capture the complexity of the decision boundary;
    if it is too large, the model will severely overfit:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这并没有表现得像线性模型那样好。现在我们将运行径向基函数。在这种情况下，我们将求解的参数是`gamma`，我们将以`0.1`到`4`的增量进行检查。如果`gamma`太小，模型将无法捕捉决策边界的复杂性；如果太大，模型将严重过拟合：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The best `gamma` value is 0.5, and the performance at this setting does not
    seem to improve much over the other SVM models. We will check for the `test` set
    as well in the following way:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的`gamma`值是0.5，在这个设置下，性能似乎并没有比其他SVM模型有太多改进。我们将在以下方式中检查`test`集：
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The performance is downright abysmal. One last shot to improve here would be
    with `kernel = "sigmoid"`. We will be solving for two parameters-- `gamma` and
    the kernel coefficient (`coef0`):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表现简直糟糕透顶。在这里再试一次来提高性能的方法是使用`kernel = "sigmoid"`。我们将求解两个参数——`gamma`和核系数（`coef0`）：
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This error rate is in line with the linear model. It is now just a matter of
    whether it performs better on the `test` set or not:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误率与线性模型相匹配。现在只是看它是否在`test`集上表现更好：
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Lo and behold! We finally have a test performance that is in line with the performance
    on the `train` data. It appears that we can choose the sigmoid kernel as the best
    predictor.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 看看！我们终于得到了与`train`数据性能相匹配的测试性能。看起来我们可以选择sigmoid核作为最佳预测器。
- en: So far we've played around with different models. Now, let's evaluate their
    performance along with the linear model using metrics other than just the accuracy.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经尝试了不同的模型。现在，让我们使用除了准确率之外的指标来评估它们的性能，并与线性模型进行比较。
- en: Model selection
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择
- en: We've looked at two different types of modeling techniques here, and for all
    intents and purposes, KNN has fallen short. The best accuracy on the `test` set
    for KNN was only around 71 per cent. Conversely, with SVM, we could obtain an
    accuracy close to 80 per cent. Before just simply selecting the most accurate
    mode, in this case, the SVM with the sigmoid kernel, let's look at how we can
    compare them with a deep examination of the confusion matrices.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了两种不同的建模技术，从所有意义上讲，KNN的表现都未能达到预期。KNN在`test`集上的最佳准确率只有大约71%。相反，使用SVM，我们可以获得接近80%的准确率。在简单地选择最准确的模式之前，在这种情况下，选择sigmoid核的SVM，让我们看看我们如何可以通过对混淆矩阵的深入分析来比较它们。
- en: 'For this exercise, we can turn to our old friend, the `caret` package and utilize
    the `confusionMatrix()` function.  Keep in mind that we previously used the same
    function from the `InformationValue` package.  The `caret` package version provides
    much more detail and it will produce all of the statistics that we need in order
    to evaluate and select the best model. Let''s start with the last model that we
    built first, using the same syntax that we used in the base `table()` function
    with the exception of specifying the `positive` class, as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们可以求助于我们老朋友`caret`包，并使用`confusionMatrix()`函数。记住，我们之前已经从`InformationValue`包中使用了相同的函数。`caret`包版本提供了更多细节，并且将产生我们评估和选择最佳模型所需的所有统计数据。让我们首先从我们构建的最后一个模型开始，使用与我们在基础`table()`函数中使用的相同语法，除了指定`positive`类别，如下所示：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The function produces some items that we already covered such as `Accuracy`
    and `Kappa`. Here are the other statistics that it produces:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数产生了一些我们已讨论过的项目，例如`Accuracy`和`Kappa`。以下是它产生的其他统计数据：
- en: '`No Information Rate` is the proportion of the largest class; 63 per cent did
    not have diabetes.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`无信息率`是最大类别的比例；63%的人没有糖尿病。'
- en: '`P-Value` is used to test the hypothesis that the accuracy is actually better
    than `No Information Rate`.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P-Value`用于检验假设，即准确率实际上优于`无信息率`。'
- en: We will not concern ourselves with `Mcnemar's Test`, which is used for the analysis
    of the matched pairs, primarily in epidemiology studies.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将不会关注`McNemar's Test`，这是一种用于匹配对分析的方法，主要应用于流行病学研究中。
- en: '`Sensitivity` is the true positive rate; in this case, the rate of those not
    having diabetes has been correctly identified as such.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`灵敏度`是真正率；在这种情况下，没有糖尿病的人被正确识别为这样的比率。'
- en: '`Specificity` is the true negative rate or, for our purposes, the rate of a
    diabetic that has been correctly identified.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`特异性`是真正负率，或者在我们的情况下，正确识别为糖尿病的比率。'
- en: 'The positive predictive value (`Pos Pred Value`) is the probability of someone
    in the population classified as being diabetic and truly has the disease. The
    following formula is used:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正性预测值（`Pos Pred Value`）是指被归类为糖尿病人群的概率，并且确实患有疾病。以下公式被使用：
- en: '![](img/image_05_10.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_10.jpg)'
- en: 'The negative predictive value (`Neg Pred Value`) is the probability of someone
    in the population classified as not being diabetic and truly does not have the
    disease. The formula for this is as follows:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阴性预测值（`Neg Pred Value`）是指被归类为非糖尿病人群的概率，并且确实没有疾病。该公式的计算如下：
- en: '![](img/image_05_11.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_05_11.jpg)'
- en: '`Prevalence` is the estimated population prevalence of the disease, calculated
    here as the total of the second column (the `Yes` column) divided by the total
    observations.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`患病率`是疾病的估计人群患病率，在此计算为第二列（`Yes`列）的总和除以总观测值。'
- en: '`Detection Rate` is the rate of the true positives that have been identified,
    in our case, 35, divided by the total observations.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`检测率`是已识别的真正正例的比率，在我们的情况下，是35除以总观测值。'
- en: '`Detection Prevalence` is the predicted prevalence rate, or in our case, the
    bottom row divided by the total observations.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`检测患病率`是预测的患病率，或者在我们的情况下，底部行除以总观测值。'
- en: '`Balanced Accuracy` is the average accuracy obtained from either class. This
    measure accounts for a potential bias in the classifier algorithm, thus potentially
    overpredicting the most frequent class. This is simply *Sensitivity + Specificity
    divided by 2*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`平衡准确率`是从任一类别获得的平均准确率。这一指标考虑了分类器算法中可能存在的偏差，从而可能高估最频繁的类别。这简单来说是*灵敏度 + 特异性除以2*。'
- en: 'The sensitivity of our model is not as powerful as we would like and tells
    us that we are missing some features from our dataset that would improve the rate
    of finding the true diabetic patients. We will now compare these results with
    the linear SVM, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的灵敏度不如我们希望的那样强大，这告诉我们我们在数据集中遗漏了一些特征，这些特征会提高找到真正糖尿病患者的比率。我们现在将比较这些结果与线性SVM，如下所示：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As we can see by comparing the two models, the linear SVM is inferior across
    the board. Our clear winner is the sigmoid kernel SVM. However, there is one thing
    that we are missing here and that is any sort of feature selection. What we have
    done is just thrown all the variables together as the feature input space and
    let the blackbox SVM calculations give us a predicted classification. One of the
    issues with SVMs is that the findings are very difficult to interpret. There are
    a number of ways to go about this process that I feel are beyond the scope of
    this chapter; this is something that you should begin to explore and learn on
    your own as you become comfortable with the basics that have been outlined previously.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较两个模型，我们可以看到线性SVM在各个方面都劣于其他模型。我们的明显胜者是sigmoid核SVM。然而，这里我们遗漏了一件事，那就是任何形式的特征选择。我们所做的是将所有变量一起作为特征输入空间，让黑盒SVM计算给出预测分类。SVM的一个问题是其发现非常难以解释。有几种处理这个过程的方法，我认为这超出了本章的范围；这是您应该开始探索和自学的东西，随着您对之前概述的基本知识的熟悉。
- en: Feature selection for SVMs
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM的特征选择
- en: However, all is not lost on feature selection and I want to take some space
    to show you a quick way of how to begin exploring this matter. It will require
    some trial and error on your part. Again, the `caret` package helps out in this
    matter as it will run a cross-validation on a linear SVM based on the `kernlab`
    package.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在特征选择方面并非一切都已失去，我想占用一些篇幅来向您展示一种快速探索这一问题的方法。这需要您进行一些尝试和错误。再次强调，`caret`包在这方面有所帮助，因为它将基于`kernlab`包的线性SVM进行交叉验证。
- en: 'To do this, we will need to set the random seed, specify the cross-validation
    method in the caret''s `rfeControl()` function, perform a recursive feature selection
    with the `rfe()` function, and then test how the model performs on the `test`
    set. In `rfeControl()`, you will need to specify the function based on the model
    being used. There are several different functions that you can use. Here we will
    need `lrFuncs`. To see a list of the available functions, your best bet is to
    explore the documentation with `?rfeControl` and `?caretFuncs`. The code for this
    example is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要设置随机种子，在`caret`的`rfeControl()`函数中指定交叉验证方法，使用`rfe()`函数执行递归特征选择，然后测试模型在`test`集上的表现。在`rfeControl()`中，您需要根据所使用的模型指定函数。您可以使用几种不同的函数。在这里，我们需要`lrFuncs`。要查看可用函数的列表，您最好的选择是使用`?rfeControl`和`?caretFuncs`探索文档。本例的代码如下：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To create the `svm.features` object, it was important to specify the inputs
    and response factor, number of input features via `sizes`, and linear method from
    `kernlab`, which is the `svmLinear` syntax. Other options are available using
    this method, such as `svmPoly`. No method for a sigmoid kernel is available. Calling
    the object allows us to see how the various feature sizes perform, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建`svm.features`对象，重要的是要指定输入和响应因子、通过`sizes`指定的输入特征数量以及来自`kernlab`的线性方法，即`svmLinear`语法。使用此方法还有其他选项，例如`svmPoly`。没有sigmoid核的方法。调用对象使我们能够看到各种特征大小如何表现，如下所示：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Counter-intuitive as it is, the five variables perform quite well by themselves
    as well as when `skin` and `bp` are included. Let''s try this out on the `test`
    set, remembering that the accuracy of the full model was 76.2 per cent:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看似反直觉，但五个变量在自身以及包含`skin`和`bp`时表现都相当好。让我们在`test`集上尝试一下，记住完整模型的准确率为76.2%：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This did not perform as well and we can stick with the full model. You can see
    through trial and error how this technique can play out in order to determine
    some simple identification of feature importance. If you want to explore the other
    techniques and methods that you can apply here, and for blackbox techniques in
    particular, I recommend that you start by reading the work by Guyon and Elisseeff
    (2003) on this subject.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法表现不佳，我们可以坚持使用完整模型。您可以通过尝试和错误来了解这种技术如何发挥作用，以确定特征重要性的简单识别。如果您想探索这里可以应用的其他技术和方法，特别是对于黑盒技术，我建议您从阅读Guyon和Elisseeff（2003）关于这一主题的工作开始。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we reviewed two new classification techniques: KNN and SVM.
    The goal was to discover how these techniques work, and the differences between
    them, by building and comparing models on a common dataset in order to predict
    if an individual had diabetes. KNN involved both the unweighted and weighted nearest
    neighbor algorithms. These did not perform as well as the SVMs in predicting whether
    an individual had diabetes or not.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了两种新的分类技术：KNN和SVM。目标是通过对一个共同数据集上的模型进行构建和比较，以了解这些技术的工作原理以及它们之间的差异，从而预测一个人是否患有糖尿病。KNN涉及无权重和有权重的最近邻算法。这些算法在预测一个人是否患有糖尿病方面并不如SVM表现得好。
- en: We examined how to build and tune both the linear and nonlinear support vector
    machines using the `e1071` package. We used the extremely versatile `caret` package
    to compare the predictive ability of a linear and nonlinear support vector machine
    and saw that the nonlinear support vector machine with a sigmoid kernel performed
    the best.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了如何使用`e1071`包构建和调整线性和非线性支持向量机。我们使用了极其通用的`caret`包来比较线性和非线性支持向量机的预测能力，并发现具有sigmoid核的非线性支持向量机表现最佳。
- en: Finally, we touched on how you can use the `caret` package to perform a crude
    feature selection, as this is a difficult challenge with a blackbox technique
    such as SVM. This can be a major challenge when using these techniques and you
    will need to consider how viable they are in order to address the business question.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要介绍了如何使用`caret`包进行粗略的特征选择，因为这对于像SVM这样的黑盒技术来说是一个具有挑战性的任务。当使用这些技术时，这可能会成为一个重大的挑战，你需要考虑它们在解决业务问题上的可行性。
