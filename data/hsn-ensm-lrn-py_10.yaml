- en: Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Bagging is generally used to reduce variance of a model. It achieves it by creating
    an ensemble of base learners, each one trained on a unique bootstrap sample of
    the original train set. This forces diversity between the base learners. Random
    Forests expand on bagging by inducing randomness not only on each base learner's
    train samples, but in the features as well. Furthermore, their performance is
    similar to boosting techniques, although they do not require as much fine-tuning
    as boosting methods.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging通常用于降低模型的方差。它通过创建一个基础学习器的集成，每个学习器都在原始训练集的独特自助样本上进行训练，从而实现这一目标。这迫使基础学习器之间保持多样性。随机森林在Bagging的基础上进行扩展，不仅在每个基础学习器的训练样本上引入随机性，还在特征选择上也引入了随机性。此外，随机森林的性能类似于提升方法，尽管它们不像提升方法那样需要进行大量的精调。
- en: 'In this chapter, we will provide the basic background of random forests, as
    well as discuss the strengths and weaknesses of the method. Finally, we will present
    usage examples, using the scikit-learn implementation. The main topics covered
    in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供关于随机森林的基本背景，并讨论该方法的优缺点。最后，我们将展示使用scikit-learn实现的使用示例。本章涵盖的主要内容如下：
- en: How Random Forests build their base learners
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林如何构建基础学习器
- en: How randomness can be utilized in order to build better random forest ensembles
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何利用随机性来构建更好的随机森林集成模型
- en: The strengths and weaknesses of Random Forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林的优缺点
- en: Utilizing scikit-learn's implementation for regression and classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现进行回归和分类
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解Python的约定和语法。最后，熟悉NumPy库将极大地帮助读者理解一些自定义算法的实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter07)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2LY5OJR](http://bit.ly/2LY5OJR).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，查看代码的实际应用：[http://bit.ly/2LY5OJR](http://bit.ly/2LY5OJR)。
- en: Understanding random forest trees
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解随机森林树
- en: 'In this section, we will go over the methodology of building a basic random
    forest tree. There are other methods that can be employed, but they all strive
    to achieve the same goal: diverse trees that serve as the ensemble''s base learners.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍构建基本随机森林树的方法论。虽然有其他方法可以使用，但它们的目标都是一致的：构建多样化的树，作为集成模型的基础学习器。
- en: Building trees
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建树
- en: As mentioned in [Chapter 1](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml), *A
    Machine Learning Refresher*, create a tree by selecting at each node a single
    feature and split point, such that the train set is best split. When an ensemble
    is created, we wish the base learners to be as uncorrelated (diverse) as possible.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](57f23be7-7e0d-4fa5-b7a5-08e0caf8e704.xhtml)《*机器学习回顾*》所述，在每个节点选择一个特征和分割点来创建一棵树，以便最佳地划分训练集。当创建一个集成模型时，我们希望基础学习器尽可能地不相关（多样化）。
- en: 'Bagging is able to produce reasonably uncorrelated trees by diversifying each
    tree''s train set through bootstrapping. But bagging only diversifies the trees
    by acting on one axis: each set''s instances. There is still a second axis on
    which we can introduce diversity, the features. By selecting a subset of the available
    features during training, the generated base learners can be even more diverse.
    In random forests, for each tree and at each node, only a subset of the available
    features is considered when choosing the best feature/split point combination.
    The number of features that will be selected can be optimized by hand, but one-third
    of all features for regression problems and the square root of all features are
    considered to be a good starting point.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 通过引导采样使每棵树的训练集多样化，从而能够生成合理不相关的树。但 bagging 仅通过一个轴进行树的多样化：每个集合的实例。我们仍然可以在第二个轴上引入多样性，即特征。在训练过程中通过选择可用特征的子集，生成的基学习器可以更加多样化。在随机森林中，对于每棵树和每个节点，在选择最佳特征/分裂点组合时，仅考虑可用特征的一个子集。选择的特征数量可以通过手动优化，但回归问题通常选用所有特征的三分之一，而所有特征的平方根被认为是一个很好的起点。
- en: 'The algorithm''s steps are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的步骤如下：
- en: Select the number of features *m* that will be considered at each node
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择在每个节点上将要考虑的特征数量 *m*
- en: 'For each base learner, do the following:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个基学习器，执行以下操作：
- en: Create a bootstrap train sample
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建引导训练样本
- en: Select the node to split
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择要拆分的节点
- en: Select *m* features randomly
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择 *m* 个特征
- en: Pick the best feature and split point from *m*
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *m* 中选择最佳特征和分裂点
- en: Split the node into two nodes
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将节点拆分为两个节点
- en: Repeat from step 2-2 until a stopping criterion is met, such as maximum tree
    depth
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从步骤 2-2 开始重复，直到满足停止准则，如最大树深度
- en: Illustrative example
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例说明
- en: 'In order to better illustrate the process, let''s consider the following dataset,
    indicating whether a second shoulder dislocation has occurred after the first
    (recurrence):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地展示过程，我们考虑以下数据集，表示第一次肩部脱位后是否发生了第二次肩部脱位（复发）：
- en: '| **Age** | **Operated** | **Sex** | **Recurrence** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **手术** | **性别** | **复发** |'
- en: '| 15 | y | m | y |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 15 | y | m | y |'
- en: '| 45 | n | f | n |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 45 | n | f | n |'
- en: '| 30 | y | m | y |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 30 | y | m | y |'
- en: '| 18 | n | m | n |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 18 | n | m | n |'
- en: '| 52 | n | f | y |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 52 | n | f | y |'
- en: Shoulder dislocation recurrence dataset
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 肩部脱位复发数据集
- en: 'In order to build a Random Forest tree, we must first decide the number of
    features that will be considered in each split. As we have three features, we
    will use the square root of 3, which is approximately 1.7\. Usually, we use the
    floor of this number (we round it down to the closest integer), but as we want
    to illustrate the process, we will use two features in order to better demonstrate
    it. For the first tree, we generate a bootstrap sample. The second row is an instance
    that was chosen twice from the original dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个随机森林树，我们必须首先决定在每次分裂时将考虑的特征数量。由于我们有三个特征，我们将使用3的平方根，约为1.7。通常，我们使用该数字的下取整（将其四舍五入到最接近的整数），但为了更好地展示过程，我们将使用两个特征。对于第一棵树，我们生成一个引导样本。第二行是从原始数据集中被选择了两次的实例：
- en: '| **Age** | **Operated** | **Sex** | **Recurrence** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **手术** | **性别** | **复发** |'
- en: '| 15 | y | m | y |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 15 | y | m | y |'
- en: '| 15 | y | m | y |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 15 | y | m | y |'
- en: '| 30 | y | m | y |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 30 | y | m | y |'
- en: '| 18 | n | m | n |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 18 | n | m | n |'
- en: '| 52 | n | f | y |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 52 | n | f | y |'
- en: The bootstrap sample
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 引导样本
- en: 'Next, we create the root node. First, we randomly select two features to consider.
    We choose **operated** and **sex**. The best split is given for **operated**,
    as we get a leaf with 100% accuracy and one node with 50% accuracy. The resulting
    tree is depicted as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建根节点。首先，我们随机选择两个特征进行考虑。我们选择**手术**和**性别**。在**手术**特征上进行最佳分裂，结果得到一个准确率为100%的叶子节点和一个准确率为50%的节点。生成的树如下所示：
- en: '![](img/2585e8c5-1dc8-4d1d-842b-0ca00748fc81.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2585e8c5-1dc8-4d1d-842b-0ca00748fc81.png)'
- en: The tree after the first split
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次分裂后的树
- en: Next, we again select two features at random and the one that offers the best
    split. We now choose **operated** and **age**. As both misclassified instances
    were not operated, the best split is offered through the age feature.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们再次随机选择两个特征，并选择提供最佳分裂的特征。我们现在选择**手术**和**年龄**。由于两个误分类的实例均未进行手术，因此最佳分裂通过年龄特征来实现。
- en: 'Thus, the final tree is a tree with three leaves, where if someone is operated
    they have a recurrence, while if they are not operated and are over the age of
    18 they do not:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的树是一个具有三个叶子节点的树，其中如果某人做了手术，他们会复发；如果他们没有做手术并且年龄超过18岁，则不会复发：
- en: Note that medical research indicates that young males have the highest chance
    for shoulder dislocation recurrence. The dataset here is a toy example that does
    not reflect reality.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，医学研究表明，年轻男性肩膀脱位复发的几率最高。这里的数据集是一个玩具示例，并不反映现实。
- en: '![](img/dcf0023c-0cb0-499e-9972-8eb6714ef467.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcf0023c-0cb0-499e-9972-8eb6714ef467.png)'
- en: The final decision tree
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的决策树
- en: Extra trees
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Extra Trees
- en: 'Another method to create trees in a Random Forest ensemble is Extra Trees (extremely randomized
    trees). The main difference with the previous method is that the feature and split
    point combination does not have to be the optimal. Instead, a number of split
    points are randomly generated, one for each available feature. The best split
    point of those generated is selected. The algorithm constructs a tree as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 创建随机森林集成中的另一种方法是Extra Trees（极度随机化树）。与前一种方法的主要区别在于，特征和分割点的组合不需要是最优的。相反，多个分割点会被随机生成，每个可用特征生成一个。然后选择这些生成的分割点中的最佳点。该算法构造树的步骤如下：
- en: Select the number of features *m* that will be considered at each node and the
    minimum number of samples *n* in order to split a node
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择每个节点将要考虑的特征数*m*以及分割节点所需的最小样本数*n*
- en: 'For each base learner, do the following:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个基础学习器，执行以下操作：
- en: Create a bootstrap train sample
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个自助法训练样本
- en: Select the node to split (the node must have at least *n* samples)
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择要分割的节点（该节点必须至少包含*n*个样本）
- en: Select *m* features randomly
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择*m*个特征
- en: Randomly generate *m* split points, with values between the minimum and maximum
    value of each feature
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机生成*m*个分割点，值介于每个特征的最小值和最大值之间
- en: Select the best of these split points
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择这些分割点中的最佳点
- en: Split the node into two nodes and repeat from step 2-2 until there are no available
    nodes
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将节点分割成两个节点，并从步骤2-2开始重复，直到没有可用节点为止
- en: Creating forests
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建森林
- en: 'By creating a number of trees using any valid randomization method, we have
    essentially created a forest, hence the algorithm''s name. After generating the
    ensemble''s trees, their predictions must be combined in order to have a functional
    ensemble. This is usually achieved through majority voting for classification
    problems and through averaging for regression problems. There are a number of
    hyperparameters associated with Random Forests, such as the number of features
    to consider at each node split, the number of trees in the forest, and the individual
    tree''s size. As mentioned earlier, a good starting point for the number of features
    to consider is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用任何有效的随机化方法创建多棵树，我们基本上就创建了一个森林，这也是该算法名称的由来。在生成集成的树之后，必须将它们的预测结果结合起来，才能形成一个有效的集成。这通常通过分类问题的多数投票法和回归问题的平均法来实现。与随机森林相关的超参数有许多，例如每个节点分割时考虑的特征数、森林中的树木数量以及单棵树的大小。如前所述，考虑的特征数量的一个良好起始点如下：
- en: The square root of the number of total features for classification problems
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类问题，选择总特征数的平方根
- en: One-third of the number of total features for regression problems
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归问题，选择总特征数的三分之一
- en: The total number of trees can be fine-tuned by hand, as the ensemble's error
    converges to a limit when this number increases. Out-of-bag errors can be utilized
    to find an optimal value. Finally, the size of each tree can be a deciding factor
    in overfitting. Thus, if overfitting is observed, the tree size should be reduced.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总树的数量可以手动微调，因为随着该数量的增加，集成的误差会收敛到一个极限。可以利用袋外误差来找到最佳值。最后，每棵树的大小可能是过拟合的决定性因素。因此，如果观察到过拟合，应减小树的大小。
- en: Analyzing forests
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析森林
- en: Random Forests provide information about the underlying dataset that most of
    other methods cannot easily provide. A prominent example is the importance of
    each individual feature in the dataset. One method to estimate feature importance
    is to use the Gini index for each node of each tree and compare each feature's
    cumulative value. Another method uses the out-of-bag samples. First, the out-of-bag
    accuracy is recorded for all base learners. Then, a single feature is chosen and
    its values are shuffled in the out-of-bag samples. This results in out-of-bag
    sample sets with the same statistical properties as the original sets, but any
    predictive power that the chosen feature might have is removed (as there is now
    zero correlation between the selected feature's values and the target). The difference
    in accuracy between the original and the partially random dataset is used as measure
    for the selected feature's importance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林提供了许多其他方法无法轻易提供的关于底层数据集的信息。一个突出的例子是数据集中每个特征的重要性。估计特征重要性的一种方法是使用基尼指数计算每棵树的每个节点，并比较每个特征的累计值。另一种方法则使用袋外样本。首先，记录所有基学习器的袋外准确度。然后，选择一个特征，并在袋外样本中打乱该特征的值。这会导致袋外样本集具有与原始集相同的统计特性，但任何可能与目标相关的预测能力都会被移除（因为此时所选特征的值与目标之间的相关性为零）。通过比较原始数据集与部分随机化数据集之间的准确度差异，可以作为评估所选特征重要性的标准。
- en: 'Concerning bias and variance, although random forests seem to cope well with
    both, they are certainly not immune. Bias can appear when the available features
    are great in number, but only few are correlated to the target. When using the
    recommended number of features to consider at each split (for example, the square
    root of the number of total features), the probability that a relevant feature
    will be selected can be small. The following graph shows the probability that
    at least one relevant feature will be selected, as a function of relevant and
    irrelevant features (when the square root of the number of total features is considered
    at each split):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关于偏差与方差，尽管随机森林似乎能够很好地应对这两者，但它们显然并非完全免疫。当可用特征数量很大，但只有少数与目标相关时，可能会出现偏差。在使用推荐的每次划分时考虑的特征数量（例如，总特征数的平方根）时，相关特征被选中的概率可能较小。以下图表展示了作为相关特征和无关特征函数的情况下，至少选中一个相关特征的概率（当每次划分时考虑总特征数的平方根）：
- en: '![](img/89e05ebd-3f52-4666-9694-b76675e7d70b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89e05ebd-3f52-4666-9694-b76675e7d70b.png)'
- en: Probability to select at least one relevant feature as a function of the number
    of relevant and irrelevant features
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 选择至少一个相关特征的概率与相关特征和无关特征数量的关系
- en: The Gini index measures the frequency of incorrect classifications, assuming
    that a randomly sampled instance would be classified according to the label distribution
    dictated by a specific node.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数衡量错误分类的频率，假设随机抽样的实例会根据特定节点所规定的标签分布进行分类。
- en: Variance can also appear in Random Forests, although the method is sufficiently
    resistant to it. Variance usually appears when the individual trees are allowed
    to grow fully. We have previously mentioned that as the number of trees increases,
    the error approximates a certain limit. Although this claim still holds true,
    it is possible that the limit itself overfits the data. Restricting the tree size
    (by increasing the minimum number of samples per leaf or reducing the maximum
    depth) can potentially help in such circumstances.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 方差在随机森林中也可能出现，尽管该方法对其有足够的抵抗力。通常，当允许单个树完全生长时，会出现方差。我们之前提到过，随着树木数量的增加，误差会接近某个极限。虽然这一说法依然成立，但该极限本身可能会过拟合数据。在这种情况下，限制树的大小（例如，通过增加每个叶节点的最小样本数或减少最大深度）可能会有所帮助。
- en: Strengths and weaknesses
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势与劣势
- en: Random Forests are a very robust ensemble learning method, able to reduce both
    bias and variance, similar to boosting. Furthermore, the algorithm's nature allows
    it to be fully parallelized, both during training, as well as during prediction.
    This is a considerable advantage over boosting methods, especially when large
    datasets are concerned. Furthermore, they require less hyperparameter fine-tuning,
    compared to boosting techniques, especially XGBoost.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种非常强大的集成学习方法，能够减少偏差和方差，类似于提升方法。此外，该算法的性质使得它在训练和预测过程中都可以完全并行化。这相较于提升方法，尤其是在处理大数据集时，是一个显著的优势。此外，与提升技术（尤其是
    XGBoost）相比，随机森林需要更少的超参数微调。
- en: The main weaknesses of random forests are their sensitivity to class imbalances,
    as well as the problem we mentioned earlier, which involves a low ratio of relevant
    to irrelevant features in the train set. Furthermore, when the data contains low-level
    non-linear patterns (such as in raw, high-resolution image recognition), Random
    Forests usually are outperformed by deep neural networks. Finally, Random Forests
    can be computationally expensive when very large datasets are used combined with
    unrestricted tree depth.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的主要弱点是它们对类别不平衡的敏感性，以及我们之前提到的问题，即训练集中相关特征和无关特征的比例较低。此外，当数据包含低级非线性模式（例如原始高分辨率图像识别）时，随机森林通常会被深度神经网络超越。最后，当使用非常大的数据集并且树深度没有限制时，随机森林的计算成本可能非常高。
- en: Using scikit-learn
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn
- en: scikit-learn implements both conventional Random Forest trees, as well as Extra
    Trees. In this section, we will provide basic regression and classification examples
    with both algorithms, using the scikit-learn implementations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 实现了传统的随机森林树和 Extra Trees。在本节中，我们将提供使用 scikit-learn 实现的两种算法的基本回归和分类示例。
- en: Random forests for classification
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类
- en: The Random Forests classification class is implemented in `RandomForestClassifier`,
    under the `sklearn.ensemble` package. It has a number of parameters, such as the
    ensemble's size, the maximum tree depth, the number of samples required to make
    or split a node, and many more.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类类在 `RandomForestClassifier` 中实现，位于 `sklearn.ensemble` 包下。它有许多参数，例如集成的大小、最大树深度、构建或拆分节点所需的样本数等。
- en: 'In this example, we will try to classify the hand-written digits dataset, using
    the Random Forest classification ensemble. As usual, we load the required classes
    and data and set the seed for our random number generator:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将尝试使用随机森林分类集成来对手写数字数据集进行分类。像往常一样，我们加载所需的类和数据，并为随机数生成器设置种子：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Following this, we create the ensemble, by setting the `n_estimators` and `n_jobs` parameters.
    These parameters dictate the number of trees that will be generated and the number
    of parallel jobs that will be run. We train the ensemble using the `fit` function
    and evaluate it on the test set by measuring its achieved accuracy:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过设置 `n_estimators` 和 `n_jobs` 参数来创建集成模型。这些参数决定了将生成的树的数量和将要运行的并行作业数。我们使用
    `fit` 函数训练集成，并通过测量其准确率在测试集上进行评估：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The classifier is able to achieve an accuracy of 93%, which is even higher
    than the previously best-performing method, XGBoost ([Chapter 6](a1a92022-31ce-4c9b-9712-6b8282fac1af.xhtml), *Boosting*).
    We can visualize the approximation of the error limit we mentioned earlier, by
    plotting validation curves (from [Chapter 2](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml), *Getting
    Started with Ensemble Learning*) for a number of ensemble sizes. We test for sizes
    of 10, 50, 100, 150, 200, 250, 300, 350, and 400 trees. The curves are depicted
    in the following graph. We can see that the ensemble approaches a 10-fold cross-validation
    error of 96%:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该分类器能够实现 93% 的准确率，甚至高于之前表现最好的方法 XGBoost（见[第6章](a1a92022-31ce-4c9b-9712-6b8282fac1af.xhtml)，*Boosting*）。我们可以通过绘制验证曲线（来自[第2章](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml)，*Getting
    Started with Ensemble Learning*），来可视化我们之前提到的误差极限的近似值。我们测试了 10、50、100、150、200、250、300、350
    和 400 棵树的集成大小。曲线如下图所示。我们可以看到，集成模型的 10 倍交叉验证误差接近 96%：
- en: '![](img/fcce0cc8-1a54-41c7-8ebc-200301ca4071.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcce0cc8-1a54-41c7-8ebc-200301ca4071.png)'
- en: Validation curves for a number of ensemble sizes
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不同集成大小的验证曲线
- en: Random forests for regression
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: 'Scikit-learn also implements random forests for regression purposes in the
    `RandomForestRegressor` class. It is also highly parameterizable, with hyper-parameters
    concerning both the ensemble as a whole, as well as the individual trees. Here,
    we will generate an ensemble in order to model the diabetes regression dataset.
    The code follows the standard procedure of loading libraries and data, creating
    the ensemble and calling the `fit` and predict methods, along with calculating
    the MSE and R-squared values:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 还在 `RandomForestRegressor` 类中实现了用于回归的随机森林。它也具有高度的可参数化性，具有与集成整体以及单个树相关的超参数。在这里，我们将生成一个集成模型来对糖尿病回归数据集进行建模。代码遵循加载库和数据、创建集成模型并调用
    `fit` 和 `predict` 方法的标准过程，同时计算 MSE 和 R² 值：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The ensemble is able to produce an R-squared of 0.51 and an MSE of 2722.67 on
    the test set. As the R-squared and MSE on the train set are 0.92 and 468.13 respectively,
    it is safe to assume that the ensemble overfits. This is a case where the error
    limit overfits, and thus we need to regulate the individual trees in order to
    achieve better results. By reducing the minimum number of samples required to
    be at each leaf node (increased to 20, from the default value of 2) through `min_samples_leaf=20`,
    we are able to increase R-squared to 0.6 and reduce MSE to 2206.6\. Furthermore,
    by increasing the ensemble size to 1000, R-squared is further increased to 0.61
    and MSE is further decreased to 2158.73.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该集成方法能够在测试集上实现 0.51 的 R 方和 2722.67 的 MSE。由于训练集上的 R 方和 MSE 分别为 0.92 和 468.13，因此可以合理推断该集成方法存在过拟合。这是一个误差限制过拟合的例子，因此我们需要调节单个树木以获得更好的结果。通过减少每个叶节点所需的最小样本数（将其从默认值
    2 增加到 20）通过 `min_samples_leaf=20`，我们能够将 R 方提高到 0.6，并将 MSE 降低到 2206.6。此外，通过将集成大小增加到
    1000，R 方进一步提高到 0.61，MSE 进一步降低到 2158.73。
- en: Extra trees for classification
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Extra Trees 用于分类
- en: 'Apart from conventional Random Forests, scikit-learn also implements Extra
    Trees. The classification implementation lies in the `ExtraTreesClassifier`, in
    the `sklearn.ensemble` package. Here, we repeat the hand-written digit recognition
    example, using the Extra Trees classifier:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的随机森林，scikit-learn 还实现了 Extra Trees。分类实现位于 `ExtraTreesClassifier`，在 `sklearn.ensemble`
    包中。这里，我们重复手写数字识别的例子，使用 Extra Trees 分类器：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you may notice, the only difference with the previous example is the switch
    from `RandomForestClassifier` to `ExtraTreesClassifier`. Nonetheless, the ensemble
    achieves an even higher test accuracy score of 94%. Once again, we create validation
    curves for a number of ensemble sizes, depicted as follows. The 10-fold cross
    validation error limit for this ensemble is approximately at 97%, which further
    confirms that it outperforms the conventional Random Forest approach:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，唯一的不同之处在于将 `RandomForestClassifier` 切换为 `ExtraTreesClassifier`。尽管如此，该集成方法仍然实现了更高的测试准确率，达到了
    94%。我们再次为多个集成大小创建了验证曲线，结果如下所示。该集成方法的 10 折交叉验证误差限制大约为 97%，进一步确认了它优于传统的随机森林方法：
- en: '![](img/fbe8dc8b-dd5a-49f5-b170-43944c0d5d51.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbe8dc8b-dd5a-49f5-b170-43944c0d5d51.png)'
- en: Extra Trees validation curves for a number of ensemble sizes
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Extra Trees 在多个集成大小下的验证曲线
- en: Extra trees regression
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Extra Trees 回归
- en: 'Finally, we present the regression implementation of Extra Trees, implemented
    in `ExtraTreesRegressor`. In the following code, we repeat the previously presented
    example of modeling the diabetes dataset, using the regression version of Extra
    Trees:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示了 Extra Trees 的回归实现，位于 `ExtraTreesRegressor` 中。在以下代码中，我们重复之前展示的使用 Extra
    Trees 回归版本对糖尿病数据集建模的示例：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Similar to the classification examples, Extra Trees outperform conventional
    random forests by achieving a test R-squared of 0.55 (0.04 better than Random
    Forests) and an MSE of 2479.18 (a difference of 243.49). Still, the ensemble seems
    to overfit, as it perfectly predicts in-sample data. By setting `min_samples_leaf=10` and
    the ensemble size to 1000, we are able to produce an R-squared of 0.62 and an
    MSE of 2114.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类示例类似，Extra Trees 通过实现 0.55 的测试 R 方（比随机森林高 0.04）和 2479.18 的 MSE（差异为 243.49）来超越传统的随机森林。不过，集成方法似乎仍然出现过拟合，因为它能够完美预测样本内数据。通过设置
    `min_samples_leaf=10` 和将集成大小设置为 1000，我们能够使 R 方达到 0.62，MSE 降低到 2114。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed Random Forests, an ensemble method utilizing
    decision trees as its base learners. We presented two basic methods of constructing
    the trees: the conventional Random Forests approach, where a subset of features
    is considered at each split, as well as Extra Trees, where the split points are
    chosen almost randomly. We discussed the basic characteristics of the ensemble
    method. Furthermore, we presented regression and classification examples using
    the scikit-learn implementations of Random Forests and Extra Trees. The key points
    of this chapter that summarize its contents are provided below.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了随机森林，这是一种利用决策树作为基本学习器的集成方法。我们介绍了两种构建树的基本方法：传统的随机森林方法，其中每次分裂时考虑特征的子集，以及
    Extra Trees 方法，在该方法中，分裂点几乎是随机选择的。我们讨论了集成方法的基本特征。此外，我们还展示了使用 scikit-learn 实现的随机森林和
    Extra Trees 的回归和分类示例。本章的关键点总结如下。
- en: '**Random Forests** use bagging in order to create train sets for their base
    learners. At each node, each tree considers only a subset of the available features
    and computes the optimal feature/split point combination. The number of features
    to consider at each point is a hyper-parameter that must be tuned. Good starting
    points are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**使用装袋技术来为其基学习器创建训练集。在每个节点，每棵树只考虑一部分可用特征，并计算最佳特征/分割点组合。每个点考虑的特征数量是一个必须调整的超参数。良好的起点如下：'
- en: The square root of the total number of parameters for classification problems
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类问题的总参数平方根
- en: One-third of the total number of parameters for regression problems
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归问题的总参数的三分之一
- en: '**Extra trees** and random forests use the **whole dataset** for each base
    learner. In extra trees and random forests, instead of calculating the optimal
    feature/split-point combination of the feature subset at each node, a random split
    point is generated for each feature in the subset and the best is selected. Random
    forests can give information regarding the importance of each feature. Although
    relatively resistant to overfitting, random forests are not immune to it. Random
    forests can exhibit high bias when the ratio of relevant to irrelevant features
    is low. Random forests can exhibit high variance, although the ensemble size does
    not contribute to the problem. In the next chapter, we will present ensemble learning
    techniques that can be applied to unsupervised learning methods (clustering).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**极端随机树**和随机森林对每个基学习器使用**整个数据集**。在极端随机树和随机森林中，每个特征子集的每个节点不再计算最佳特征/分割点组合，而是为子集中的每个特征生成一个随机分割点，并选择最佳的。随机森林可以提供关于每个特征重要性的信息。虽然相对抗过拟合，但随机森林并非免疫。当相关特征与不相关特征的比例较低时，随机森林可能表现出高偏差。随机森林可能表现出高方差，尽管集成规模并不会加剧问题。在下一章中，我们将介绍可以应用于无监督学习方法（聚类）的集成学习技术。'
