- en: Locating Objects in the World
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在世界中定位物体
- en: 'So far, we have limited ourselves to recognizing the single most dominant object
    within an image using a **convolutional neural network** (**CNN**). We have seen
    how a model can be trained to take in a image and extract a series of feature
    maps that are then fed into a **fully connected layer **to output a probability
    distribution of a set of classes. This is then interpreted to classify the object
    within the image, as shown here:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只限于使用**卷积神经网络**（**CNN**）识别图像中单个最显著的物体。我们看到了如何训练一个模型来接受图像并提取一系列特征图，然后将其输入到**全连接层**以输出一系列类别的概率分布。然后，通过激活层将这些输出解释为对图像中物体的分类，如下所示：
- en: '![](img/046899f6-8490-4d77-b184-fa56fff667b3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/046899f6-8490-4d77-b184-fa56fff667b3.png)'
- en: In this chapter, we will build on this and explore how we can detect and locate
    multiple objects within a single image. We will start by building up our understanding
    of how this works and then walk through implementing a image search for a photo
    gallery application. This application allows the user to filter and sort images
    not only based on what objects are present in the image, but also on their position
    relative to one another (object composition). Along the way, we will also get
    hands-on experience with Core ML Tools, the tool set Apple released to support
    converting models from popular **machine learning** (**ML**) frameworks to Core
    ML.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在此基础上构建，探讨如何检测和定位单个图像中的多个物体。我们将首先建立对这个过程的理解，然后通过实现一个用于相册应用的图像搜索功能来演示。这个应用允许用户根据图像中存在的物体以及它们之间的相对位置（物体组成）来过滤和排序图像。在这个过程中，我们还将亲身体验Core
    ML Tools，这是苹果公司发布的一套工具，用于将流行的**机器学习**（**ML**）框架中的模型转换为Core ML。
- en: Let's begin by first understanding what it takes to detect multiple objects
    in an image.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解如何在图像中检测多个物体需要哪些条件。
- en: Object localization and object detection
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标定位和目标检测
- en: As mentioned in the introduction of this chapter, we have already been introduced
    to the concepts behind object recognition using CNNs. For this case, we used a
    trained model to perform classification; it achieved this by learning a set of
    feature maps using convolutional layers that are fed into fully connected (or
    dense) layers and, finally, their output, through an activation layer which gave
    us the probability for each of the classes. The class was inferred by selecting
    the one with the largest probability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章引言中所述，我们已经介绍了使用CNN进行目标识别背后的概念。在这种情况下，我们使用一个训练好的模型来执行分类；它通过使用卷积层学习一系列特征图，然后输入到全连接（或密集）层，并通过激活层最终输出，从而给出了每个类别的概率。通过选择概率最大的类别来推断类别。
- en: Let's differentiate between object recognition, object localization, and object
    detection. Object recognition is the task of classifying the most dominant object
    in a image while object localization performs classification and predicts an object's
    bounding box. Object detection further extends this and allows multiple classes
    to be detected and located, and that's the topic of this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们区分一下目标识别、目标定位和目标检测。目标识别是识别图像中最主要物体的任务，而目标定位则执行分类并预测物体的边界框。目标检测进一步扩展了这一概念，允许检测和定位多个类别，这也是本章的主题。
- en: 'This process is known as **object recognition** and is a classification problem,
    but here we don''t get the full picture (pun intended). What about the location
    of the detected object? That would be useful for increasing the perception capabilities
    of robotic systems or increasing the scope for intelligent interfaces, such as
    intelligent cropping and image enhancements. What about detecting multiple objects
    and their locations? The former, detecting the location of a single object, is
    known as **object localization,** while the later is generally known as **object
    detection**, illustrated as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为**目标识别**，是一个分类问题，但在这里我们并没有获得完整的图像（有意为之）。那么，检测到的物体的位置如何？这将有助于提高机器人系统的感知能力或扩展智能界面的应用范围，例如智能裁剪和图像增强。那么，检测多个物体及其位置呢？前者，检测单个物体的位置，被称为**目标定位**，而后者通常被称为**目标检测**，如下所示：
- en: '![](img/b2e938f9-2cd5-4c6f-8657-ee51140ad2e4.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2e938f9-2cd5-4c6f-8657-ee51140ad2e4.png)'
- en: We will start by introducing object localization and then work our way to object
    detection. The concepts are complementary and the former can be seen as an extension
    of object recognition, which you are already familiar with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍目标定位，然后逐步过渡到目标检测。这些概念是互补的，前者可以看作是目标识别的扩展，这是你已经熟悉的。
- en: 'When training a model for classification, we tune the weights so that they
    achieve minimum loss for predicting a single class. For object localization, we
    essentially want to extend this to predict not only the class, but also the location
    of the recognized object. Let''s work through a concrete example to help illustrate
    the concept. Imagine we are training a model to recognize and locate a cat, dog,
    or person. For this, our model would need to output the probabilities for each
    class (cat, dog, or person) as we have already seen, and also their location.
    This can be described using the center *x* and *y* position and the width and
    height of the object. To simplify the task of training, we also include a value
    indicating whether an object exists or not. The following figure illustrates two
    input images and their associated outputs. Let''s assume here that our one-hot
    encoded classes are in the order of cat, dog, and person. That is, cat would be
    encoded as *(1,0,0)*, dog as *(0,1,0)*, and person as *(**0,0,1)*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练用于分类的模型时，我们调整权重以使它们在预测单个类别时达到最小损失。对于目标定位，我们本质上希望扩展这一功能，不仅预测类别，还要预测识别出的对象的位置。让我们通过一个具体的例子来帮助说明这个概念。想象我们正在训练一个模型来识别和定位猫、狗或人。为此，我们的模型需要输出每个类别的概率（猫、狗或人），正如我们之前看到的，还需要它们的定位。这可以用物体的中心
    *x* 和 *y* 位置以及宽度和高度来描述。为了简化训练任务，我们还包含一个表示对象是否存在或否的值。以下图展示了两个输入图像及其相关的输出。假设这里我们的独热编码类别顺序为猫、狗和人。也就是说，猫将被编码为
    *(1,0,0)*，狗为 *(0,1,0)*，人为 *(**0,0,1)*：
- en: '![](img/f10dbb68-c666-46d6-9ed2-3756bc9b113f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f10dbb68-c666-46d6-9ed2-3756bc9b113f.png)'
- en: 'The structure of the output, shown in the preceding image, consists of these
    elements:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图像中显示的输出结构由以下元素组成：
- en: '![](img/da6eafe8-786a-45b6-8466-bb1c4dc9ad83.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da6eafe8-786a-45b6-8466-bb1c4dc9ad83.png)'
- en: Here, we have three classes, but this can be generalized to include any arbitrary
    number of classes. What is of note is that if there is no object detected (the
    first element in our output), then we ignore the remaining elements. The other
    important point to highlight is that the bounding box is described in units rather
    than absolute values.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有三个类别，但这可以推广到包括任意数量的类别。值得注意的是，如果没有检测到对象（我们输出的第一个元素），则忽略其余元素。另一个需要强调的重要点是，边界框是用单位而不是绝对值来描述的。
- en: 'For example, a value of *0.5* for *b[x]* would indicate half of the width of
    the image, where the top left is **(0, 0)** and bottom right is **(1, 1)**, as
    illustrated here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*b[x]* 的值为 *0.5* 将表示图像宽度的一半，其中左上角是 **(0, 0)**，右下角是 **(1, 1)**，如图所示：
- en: '![](img/50b813d6-eae3-463f-baa0-ce445442a090.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/50b813d6-eae3-463f-baa0-ce445442a090.png)'
- en: 'We can borrow a lot of the structure of a typical CNN used for classification.
    Therein, the image is fed through a series of convolutional layers and their output,
    a feature vector, through a series of fully connected layers, before being squeezed
    through a softmax activation for multi-class classification (giving us the probability
    distribution across all classes). Instead of just passing the feature vector from
    the convolutional layers to a single set of fully connected layers, we can also
    pass them to a layer (or layers) for binary classification (the fourth element:
    object present or not) and another layer (or series of layers) for predicting
    the bounding box using regression.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以借鉴用于分类的典型 CNN 的很多结构。在那里，图像通过一系列卷积层，其输出是一个特征向量，然后通过一系列全连接层，最后通过 softmax 激活进行多类别分类（给出所有类别的概率分布）。我们不仅可以将卷积层的特征向量传递到单个全连接层，还可以将它们传递到一层（或几层）进行二元分类（第四个元素：对象是否存在）和另一层（或一系列层）来预测边界框，使用回归方法。
- en: 'The structure of these modifications can be seen in the following figure, with
    the amendments in bold:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修改的结构可以在以下图中看到，其中修改的内容用粗体表示：
- en: '![](img/45b9bfc5-e790-4474-aa98-6ae1f66c1a50.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/45b9bfc5-e790-4474-aa98-6ae1f66c1a50.png)'
- en: This is a good start but, more often than not, our images consist of many objects.
    So, let's briefly describe how we can approach this problem.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好的开始，但我们的图像通常包含许多对象。因此，让我们简要描述如何解决这个问题。
- en: We are now moving into object detection, where we are interested in detecting
    and locating multiple objects (of different classes) within a single image. So
    far, we have seen how we can detect a single object and its location from an image,
    so a logical progression from this is reshaping our problem around this architecture.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在进入目标检测领域，我们感兴趣的是在单个图像中检测和定位多个对象（不同类别的对象）。到目前为止，我们已经看到如何从图像中检测单个对象及其位置，因此从这个角度来看，逻辑上的下一步是将我们的问题围绕这个架构进行重塑。
- en: 'By this, I mean we can use this or a similar approach, but rather than passing
    the full image, we can pass in cropped regions of the image; the regions are selected
    by sliding a window across the image, as follows (and in keeping with our cat
    theme):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，我们可以使用这种方法或类似的方法，但不是传递完整的图像，而是可以传递图像的裁剪区域；这些区域是通过在图像上滑动窗口来选择的，如下所示（并且与我们的猫主题保持一致）：
- en: '![](img/cbbdab03-082b-405d-9e22-396a48f3cd4d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cbbdab03-082b-405d-9e22-396a48f3cd4d.png)'
- en: This, for obvious reasons, is called the **sliding window detection algorithm**
    and should be familiar to those who have experience in computer vision (used in template
    matching, among many others). It's important to also emphasize the difference
    in training; in object localization, we trained the network by passing the full
    image along with the associated output vector (*b[x], b[y], b[w], h[y], p[c], c[1],
    c[2], c[3], ...*), while here the network is trained on **tightly cropped images**
    for each of the objects, which may occupy our window size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于显而易见的原因，这被称为**滑动窗口检测算法**，对于那些有计算机视觉经验的人来说应该是熟悉的（在模板匹配等许多应用中使用）。同时，也需要强调训练上的差异；在目标定位中，我们通过传递完整的图像及其相关的输出向量（*b[x]，b[y]，b[w]，h[y]，p[c]，c[1]，c[2]，c[3]，...*）来训练网络，而在这里，网络是在每个对象的**紧密裁剪图像**上训练的，这些图像可能占据我们的窗口大小。
- en: For the inquisitive reader wondering how this algorithm detects objects that
    don't fit nicely into the window size, one approach could be to simply resize
    your image (both decreasing and increasing) or, similarly, to use different-sized
    windows, that is, a set of small, medium, and large windows.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于好奇的读者想知道这个算法如何检测那些不适合窗口大小的对象，一个方法可能是简单地调整图像的大小（既减小也增大），或者类似地，使用不同大小的窗口，即一组小、中、大窗口。
- en: This approach has two major drawbacks; the first is that it is computationally
    expensive, and the second is that it doesn't allow for very accurate bounding
    boxes due to the dependency on window sizes and stride size. The former can be
    resolved by rearchitecting the CNN so that it performs the sliding window algorithm
    in a single pass, but we are still left with inaccurate bounding boxes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有两个主要缺点；第一个是它计算成本高，第二个是由于依赖于窗口大小和步长大小，它不允许非常精确的边界框。前者可以通过重新设计CNN，使其在单次遍历中执行滑动窗口算法来解决，但我们仍然面临着边界框不准确的问题。
- en: 'Fortunately, for us, in 2015, J. Redmon, S. Divvala, R. Girshick, and A. Farhadi
    released their paper *You Only Look Once (YOLO): Unified, Real-Time Object Detection*.
    It describes an approach that requires just a single network capable of predicting
    bounding boxes and probabilities from a full image in a single pass. And because
    it is a unified pipeline, the whole process is efficient enough to be performed
    in real time, hence the network we use in this chapter.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于2015年的我们，J. Redmon、S. Divvala、R. Girshick和A. Farhadi发布了他们的论文《你只需看一次（YOLO）：统一、实时目标检测》。它描述了一种只需要单个网络的方法，该网络能够在单次遍历中从完整图像中预测边界框和概率。由于它是一个统一的流程，整个过程足够高效，可以在实时中进行，因此我们本章使用的网络就是这样的。
- en: 'The paper *You Only Look Once: Unified, Real-Time Object Detection* is available
    here: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《你只需看一次：统一、实时目标检测》的链接如下：[https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)。
- en: Let's spend some time getting acquainted with the algorithm YOLO, where we will
    briefly look at the general concepts of the algorithm and interpreting the output
    to make use of it later in the example application for this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花些时间熟悉YOLO算法，我们将简要了解算法的一般概念和输出解释，以便在后续章节的示例应用中使用。
- en: 'One of the major differences of YOLO compared to the previous approaches we
    have discussed in this chapter is how the model is trained. Similar to the first,
    when object localization was introduced, the model was trained on a image and
    label pair, and the elements of the label consisted of (*b[x]*, *b[y]*, *b[w]*,
    *b[h]*, *p[c]*, *c[1]*, *c[2]*, ...), so too is the case with the YOLO network.
    But instead of training on the whole image, the image is broken down into a grid,
    with each cell having an associated label, as outlined before. In the next figure,
    you can see this. The grid illustrated here is a 3 x 3 for legibility; these are
    typically more dense, as you''ll soon see:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章中我们讨论的先前方法相比，YOLO的一个主要区别在于模型的训练方式。与第一个方法类似，当引入目标定位时，模型是在图像和标签对上训练的，标签的元素包括(*b[x]*,
    *b[y]*, *b[w]*, *b[h]*, *p[c]*, *c[1]*, *c[2]*, ...)，YOLO网络也是如此。但是，YOLO网络不是在整张图像上训练，而是将图像分解成网格，每个单元格都有一个相关的标签，正如之前概述的那样。在下一张图中，你可以看到这一点。这里展示的网格是一个3
    x 3的，为了可读性；这些通常更密集，正如你很快就会看到的：
- en: '![](img/80e6dea6-0a0c-4bf7-9caa-0a3c57addce3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80e6dea6-0a0c-4bf7-9caa-0a3c57addce3.png)'
- en: 'When training, we feed in the image and the network is structured so that it
    outputs a **vector** (as shown before) for **each grid cell**. To make this more
    concrete, the following figure illustrates how some of these outputs would look
    for the cells within this grid:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，我们输入图像，网络结构使得它为每个网格单元格输出一个**向量**（如之前所示）。为了使这一点更具体，以下图展示了这些输出中的一些是如何在这个网格的单元格中呈现的：
- en: '![](img/c0b85d59-a5c9-41fd-b971-dd44c3e5378a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0b85d59-a5c9-41fd-b971-dd44c3e5378a.png)'
- en: This should look familiar to you as it is, as previously mentioned, very similar
    to the approach we first introduced for object localization. The only major difference
    is that it is performed on each grid cell as opposed to the whole image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该对你来说很熟悉，因为正如之前提到的，这与我们最初介绍的目标定位方法非常相似。唯一的重大区别是，它是在每个网格单元格上进行的，而不是在整个图像上。
- en: It's worth noting that despite the object spanning across multiple cells, the
    training samples only label an object using a single cell (normally the cell in
    the center of the image), with the other encompassing cells having no object assigned
    to them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管对象跨越多个单元格，但训练样本只使用单个单元格（通常是图像中心的单元格）来标记对象，其他包含单元格没有分配给任何对象。
- en: 'Before we move on, let''s unpack the bounding box variables. In the preceding
    figure, I have entered approximate values for each of the objects; like object
    localization, these values are normalized between *0.0* and *1.0* for values that
    fall within the cell. But, unlike object localization, these values are **local**
    to the cell itself rather than the image. Using the first cat as an example, we
    can see that the central position is *0.6* in the *x* axis and *0.35* in the *y*
    axis; this can be interpreted as being at a position 60% along the *x* axis of
    the cell and 35% along the *y* axis of the cell. Because our bounding box extends
    beyond the cell, there are assigned values greater than one, as we saw in the
    preceding figure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们来解释一下边界框变量。在先前的图中，我为每个对象输入了近似值；就像目标定位一样，这些值在单元格内的值之间归一化到*0.0*和*1.0*。但是，与目标定位不同，这些值是**局部**于单元格本身的，而不是图像。以第一只猫为例，我们可以看到它在*x*轴上的中心位置是*0.6*，在*y*轴上的中心位置是*0.35*；这可以解释为在单元格*x*轴上的位置是60%，在单元格*y*轴上的位置是35%。因为我们的边界框超出了单元格，所以分配的值大于一，正如我们在先前的图中看到的：
- en: '![](img/2cd83249-1015-49b0-8408-ff2ad8748e31.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cd83249-1015-49b0-8408-ff2ad8748e31.png)'
- en: Previously, we highlighted that the training sample only assigns a single cell
    per object but, given that we are running object detection and localization on
    each cell, it is likely that we will end up with multiple predictions. To manage
    this, YOLO uses something called **non-max suppression**, which we will introduce
    over the next few paragraphs, and you'll get a chance to implement it in the upcoming
    example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们强调训练样本只为每个对象分配一个单元格，但鉴于我们在每个单元格上运行对象检测和定位，我们很可能会得到多个预测。为了管理这一点，YOLO使用了一种称为**非极大值抑制**的方法，我们将在接下来的几段中介绍，你将在即将到来的示例中实现它。
- en: 'As mentioned before, because we are performing object detection and localization
    on each grid cell, it is likely that we will end up with multiple bounding boxes.
    This is shown in the following figure. To simplify the illustration, we will just
    concentrate on a single object, but this of course applies to all detected objects:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，因为我们对每个网格单元格执行对象检测和定位，所以我们最终可能会得到多个边界框。这在下图中显示。为了简化说明，我们只关注一个对象，但当然这也适用于所有检测到的对象：
- en: '![](img/2c42812f-b4cd-4d60-8543-2461ec466366.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2c42812f-b4cd-4d60-8543-2461ec466366.png)'
- en: In the previous figure, we can see that the network has predicted and located
    the cat in three cells. For each cell, I have added a fictional confidence value,
    located at the top-right, to each of their associated bounding boxes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的图中，我们可以看到网络在三个单元格中预测并定位了猫。对于每个单元格，我在它们相关的边界框的右上角添加了一个虚构的置信值。
- en: The **confidence value** is calculated by multiplying the object present probability
    (*p[c]*) with the most likely class, that is, the class with the highest probability.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**置信值**是通过将对象存在概率（*p[c]*）与最可能的类别相乘来计算的，即概率最高的类别。'
- en: 'The first step in non-max suppression is simply to filter out predictions that
    don''t meet a certain threshold; for all intents and purposes, let''s set our
    object threshold to *0.3*. With this value, we can see that we filter out one
    of the predictions (with a confidence value of *0.2*), leaving us with just two,
    as shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 非极大值抑制的第一步很简单，就是过滤掉不满足一定阈值的预测；从所有目的来看，让我们将我们的对象阈值设为*0.3*。使用这个值，我们可以看到我们过滤掉了一个预测（置信值为*0.2*），留下我们只有两个，如下图所示：
- en: '![](img/fbcb36de-1806-49f1-a12b-9f95bdd3a72c.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fbcb36de-1806-49f1-a12b-9f95bdd3a72c.png)'
- en: In the next step, we iterate over all detected boxes, from the one with the
    largest confidence to the least, removing any other bounding boxes that occupy
    the same space. In the preceding figure, we can clearly see that both bounding
    boxes are essentially occupying the same area, but how do we determine this programmatically?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们遍历所有检测到的框，从置信度最高的框到最低的框，移除任何占用相同空间的其它边界框。在先前的图中，我们可以清楚地看到这两个边界框实际上占用了相同的空间，但我们如何程序化地确定这一点呢？
- en: 'To determine this, we calculate what is called the **intersection over union**
    (**IoU**) and test the returned value against a threshold. We calculate this,
    as the name implies, by dividing the **intersection area** of the two bounding
    boxes by their **union area**. A value of *1.0* tells us that the two bounding
    boxes occupy exactly the same space (a value you would get if you performed this
    calculation using a single bounding box with itself). Anything below this gives
    us a ratio of overlapped occupancy; a typical threshold is *0.5*. The following
    figure illustrates the intersection and union areas of our example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定这一点，我们计算所谓的**交并比**（**IoU**）并将返回值与一个阈值进行比较。正如其名所示，我们通过将两个边界框的**交集面积**除以它们的**并集面积**来计算这个值。值为*1.0*告诉我们两个边界框正好占用了相同的空间（如果你使用单个边界框自身进行此计算，你会得到这个值）。任何低于此值的都给出了重叠占用的比率；一个典型的阈值是*0.5*。下图说明了我们示例的交集和并集面积：
- en: '![](img/830b7548-9fcf-4473-a286-252b13806d68.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/830b7548-9fcf-4473-a286-252b13806d68.png)'
- en: 'Because these two bounding boxes would return a relatively high IoU, we would
    end up pruning the least probable one (the one with the lower confidence score)
    and end up with a single bounding box, as shown in the following figure:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个边界框的交并比（IoU）相对较高，我们将剪除最不可能的框（置信度分数较低的框），最终得到一个单一的边界框，如下图所示：
- en: '![](img/23a6b3ce-185a-4eee-9bdb-15093c7adb22.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/23a6b3ce-185a-4eee-9bdb-15093c7adb22.png)'
- en: We repeat this process until we have iterated over all of the model's predictions.
    There is just one more concept to introduce before moving on to the example project
    for this chapter and starting to write some code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复此过程，直到遍历了模型的所有预测。在进入本章的示例项目并开始编写代码之前，还有一个概念需要介绍。
- en: 'Up until this point, we have assumed (or have been constrained by the idea) that
    each cell is associated with either no object or a single object. But what about
    the case of objects overlapping, where two objects'' center positions occupy the
    same grid cell? To handle circumstances like this, the YOLO algorithm implements
    something called **anchor boxes**. Anchor boxes allow multiple objects to occupy
    a single grid cell given that their bounding shape differs. Let''s make this more
    concrete by explaining it visually. In the next figure, we have a image where
    the centers of two objects occupy the same grid cell. With our current output
    vector, we would need to label the cell as either a person or a bike, as shown
    in the following figure:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设（或受限于这种想法）每个单元格都与没有对象或单个对象相关联。但是，对于两个对象重叠的情况，即两个对象的重心位置占据相同的网格单元格，又该如何处理呢？为了处理这种情况，YOLO算法实现了一种称为**锚框**的东西。锚框允许在边界形状不同的情况下，多个对象占据单个网格单元格。让我们通过直观的解释来使这一点更加具体。在下一张图中，我们有一个图像，其中两个对象的重心占据相同的网格单元格。根据我们当前的输出向量，我们需要将单元格标记为人物或自行车，如下所示：
- en: '![](img/46fb4511-7627-440b-a97e-394862e35787.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46fb4511-7627-440b-a97e-394862e35787.png)'
- en: The idea with anchor boxes is that we extend our output vector to include different
    variations of anchor boxes (illustrated here as two, but these can be of any number).
    This allows each cell to encode multiple objects so long as they have different
    bounding shapes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框的思路是我们将输出向量扩展到包括不同类型的锚框（在此处表示为两个，但实际上可以是任何数量）。这允许每个单元格编码多个对象，只要它们的边界形状不同。
- en: 'From the previous figure, we see that we can use two anchor boxes, one for
    the person and the other for the bike, as shown here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一张图中，我们可以看到我们可以使用两个锚框，一个用于人物，另一个用于自行车，如下所示：
- en: '![](img/2eb10ad6-92a7-4029-8aa6-d76c5f03b343.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2eb10ad6-92a7-4029-8aa6-d76c5f03b343.png)'
- en: 'With our anchor boxes now defined, we extend our vector output such that for
    each grid cell we can encode the output for both anchor boxes, as illustrated
    in the following figure:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了锚框，我们将向量输出扩展，以便对于每个网格单元格，我们可以编码两个锚框的输出，如下所示：
- en: '![](img/803ec1ef-98a0-4285-847e-b5f0b5b1b1cd.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/803ec1ef-98a0-4285-847e-b5f0b5b1b1cd.png)'
- en: Each anchor box can be treated independently of each other output of the same
    cell and other cells; that is, we handle it exactly as we did previously, the
    only addition now being that we have more bounding boxes to process.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个锚框可以独立于同一单元格和其他单元格的任何其他输出进行处理；也就是说，我们处理它的方式与之前完全相同，唯一的增加是我们现在有更多的边界框需要处理。
- en: Just to clarify, anchor boxes are not constrained to a specific class even though
    some shapes are more suitable than others. They are typically generalized shapes
    found using some type of unsupervised learning algorithm, such as **k-means**, within
    an existing dataset to find the dominant shapes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了澄清，尽管某些形状比其他形状更适合，但锚框并不局限于特定的类别。它们通常是使用某种无监督学习算法（如**k-means**）在现有数据集中找到的典型通用形状。
- en: This concludes all the concepts we need to understand for this chapter's example
    and what we will be implementing in the coming sections but, before we do, let's
    walk through converting a Keras model to Core ML using the Core ML Tools Python
    package.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们需要理解本章示例的所有概念以及我们将在接下来的章节中实现的内容，但在我们这样做之前，让我们通过使用Core ML Tools Python包将Keras模型转换为Core
    ML的过程进行回顾。
- en: Converting Keras Tiny YOLO to Core ML
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Keras Tiny YOLO转换为Core ML
- en: In the previous section, we discussed the concepts of the model and algorithm
    we will be using in this chapter. In this section, we will be moving one step
    closer to realizing the example project for this chapter by converting a trained
    Keras model of Tiny YOLO to Core ML using Apple's Core ML Tools Python package;
    but, before doing so, we will quickly discuss the model and the data it was trained
    on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了本章将使用的模型和算法的概念。在本节中，我们将通过使用苹果的Core ML Tools Python包将训练好的Tiny YOLO
    Keras模型转换为Core ML，从而更接近实现本章的示例项目；但在这样做之前，我们将简要讨论该模型及其训练所使用的数据。
- en: YOLO was conceived on a neural network framework called **darknet**, which is
    currently not supported by the default Core ML Tools package; fortunately, the
    authors of YOLO and darknet have made the architecture and weights of the trained
    model publicly available on their website at [https://pjreddie.com/darknet/yolov2/](https://pjreddie.com/darknet/yolov2/).
    There are a few variations of YOLO that have been trained on either the dataset
    from **Common Objects in Context **(**COCO**), which consists of 80 classes, or
    The PASCAL **Visual Object Classes** (**VOC**) Challenge 2007, which consists
    of 20 classes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO是在一个名为**darknet**的神经网络框架上构思的，目前默认的Core ML工具包不支持它；幸运的是，YOLO和darknet的作者已经在他们的网站上公开了训练模型的架构和权重，网址为[https://pjreddie.com/darknet/yolov2/](https://pjreddie.com/darknet/yolov2/)。YOLO有一些变体，它们是在**Common
    Objects in Context**（**COCO**）数据集上训练的，该数据集包含80个类别，或者是在PASCAL **Visual Object Classes**（**VOC**）Challenge
    2007上训练的，该挑战包含20个类别。
- en: The official website can be found at [http://cocodataset.org](http://cocodataset.org),
    and *The PASCAL VOC Challenge 2007* at [http://host.robots.ox.ac.uk/pascal/VOC/index.html](http://host.robots.ox.ac.uk/pascal/VOC/index.html).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 官方网站可以在[http://cocodataset.org](http://cocodataset.org)找到，*The PASCAL VOC Challenge
    2007*的网站在[http://host.robots.ox.ac.uk/pascal/VOC/index.html](http://host.robots.ox.ac.uk/pascal/VOC/index.html)。
- en: In this chapter, we will be using the Tiny version of YOLOv2 and the weights
    from the model trained on the *The PASCAL VOC Challenge 2007* dataset. The Keras
    model we will be using was modeled on the configuration file and weights available
    on the official site (link presented previously).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用YOLOv2的Tiny版本以及从*The PASCAL VOC Challenge 2007*数据集上训练的模型权重。我们将使用的Keras模型是基于官方网站上可用的配置文件和权重构建的（之前提供的链接）。
- en: 'As usual, we will be omitting a lot of details of the model and will instead
    provide the model in its diagrammatic form, shown next. We''ll then discuss some
    of the relevant parts before moving on to convert it into a Core ML model:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们将省略模型的大部分细节，而是以图表形式提供模型，如下所示。然后我们将讨论一些相关部分，然后再将其转换为Core ML模型：
- en: '![](img/68b12837-9f34-460f-b84f-72051d5898e0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68b12837-9f34-460f-b84f-72051d5898e0.png)'
- en: The first thing to notice is the shape of the input and output; this indicates
    what our model will be expecting to be fed and what it will be outputting for
    us to use. As shown before, the input size is 416 x 416 x 3, which, as you might
    suspect, is a 416 x 416 RGB image. The output shape needs a little more explanation
    and it will become more apparent when we arrive at coding the example for this
    chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是输入和输出的形状；这表明我们的模型将期望输入什么，以及它将输出什么供我们使用。如前所述，输入大小是416 x 416 x 3，这正如你可能所怀疑的，是一个416
    x 416的RGB图像。输出形状需要更多的解释，当我们到达本章的示例编码时，它将变得更加明显。
- en: 'The output shape is 13 x 13 x 125\. The 13 x 13 tells us the size of the grid
    being applied, that is, the 416 x 416 image is broken into a grid of 13 x 13 cells,
    as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状是13 x 13 x 125。13 x 13告诉我们应用网格的大小，也就是说，416 x 416的图像被分割成13 x 13的单元格，如下所示：
- en: '![](img/f7f5353d-1f9f-4c88-864f-09eaa71d0960.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f7f5353d-1f9f-4c88-864f-09eaa71d0960.png)'
- en: 'As discussed previously, each cell has a 125*-*vector encoding of the probability
    of an object being present and, if so, the bounding box and probabilities across
    all 20 classes; visually, this is explained as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个单元都有一个125*-*向量的编码，表示对象存在的概率，如果存在，则包括边界框和所有20个类别的概率；直观上，这可以这样解释：
- en: '![](img/e83bddf8-2a30-4205-a45f-36d08e13191c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e83bddf8-2a30-4205-a45f-36d08e13191c.png)'
- en: 'The final point about the model that I want to highlight is its simplicity;
    the bulk of the network is made up of convolutional blocks consisting of a convolutional
    layer: **Batch Normalization**, **LeakyReLU** activation, and finally a **MaxPooling**
    layer. This progressively increases the filter size (depth) of the network until
    it has reached the desired grid size and then transforms the data using only the
    convolutional layer, **Batch Normalization**, and **LeakyReLU** activation, dropping
    the **MaxPooling**.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要强调的关于这个模型的最后一个要点是其简洁性；网络的大部分由由卷积层组成的卷积块构成：**批量归一化**、**LeakyReLU**激活，最后是一个**最大池化**层。这逐渐增加了网络的滤波器大小（深度），直到达到所需的网格大小，然后仅使用卷积层、**批量归一化**和**LeakyReLU**激活来转换数据，并丢弃**最大池化**。
- en: Now, we have introduced the terms batch normalization and leaky ReLU, which
    may be unfamiliar to some; here I will provide a brief description of each, starting
    with batch normalization. It's considered best practice to normalize the input
    layer before feeding it into a network. For example, we normally divide pixel
    values by 255 to force them into a range of 0 to 1\. We do this to make it easier
    for the network to learn by removing any large values (or large variance in values),
    which may cause our network to oscillate when adjusting the weights during training.
    Batch normalization performs this same adjustment for the outputs of the hidden
    layers rather than the inputs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们介绍了批归一化和漏斗ReLU等术语，这些可能对一些人来说不熟悉；在这里，我将简要描述每个术语，从批归一化开始。在将输入层馈送到网络之前对其进行归一化被认为是最佳实践。例如，我们通常将像素值除以255，将它们强制进入0到1的范围。我们这样做是为了使网络更容易学习，通过消除任何大值（或值的较大方差），这可能导致我们在训练过程中调整权重时网络发生振荡。批归一化对隐藏层的输出而不是输入执行相同的调整。
- en: ReLU is an activation function that sets anything below 0 to 0, that is, it
    doesn't allow non-positive values to propagate through the network. Leaky ReLU
    provides a less strict implementation of ReLU, allowing a small non-zero gradient
    to slip through when the neuron is not active.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是一个激活函数，将所有小于0的值设置为0，即它不允许非正值在网络中传播。漏斗ReLU提供了ReLU的较宽松实现，当神经元不活跃时允许一个小的非零梯度通过。
- en: 'This concludes our brief overview of the model. You can learn more about it
    from the official paper *YOLO9000: Better, Faster, Stronger* by J. Redmon and
    A. Farhadi, available at [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242).
    Let''s now turn our attention to converting the trained Keras model of the Tiny
    YOLO to Core ML.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对模型的简要概述。您可以从J. Redmon和A. Farhadi的官方论文《YOLO9000：更好、更快、更强》中了解更多信息，该论文可在[https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242)找到。现在，让我们将注意力转向将训练好的Tiny
    YOLO Keras模型转换为Core ML。
- en: As alluded to in [Chapter 1](7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml), *Introduction
    to Machine Learning*, Core ML is more of a suite of tools than a single framework.
    One part of this suite is the Core ML Tools Python package, which assists in converting
    trained models from other frameworks to Core ML for easy and rapid integration.
    Currently, official converters are available for Caffe, Keras, LibSVM, scikit-learn,
    and XGBoost, but the package is open source, with many other converters being
    made available for other popular ML frameworks, such as TensorFlow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml)《机器学习导论》中所述，Core ML更像是工具套件而不是单一框架。套件的一部分是Core
    ML Tools Python包，它帮助将其他框架训练的模型转换为Core ML，以便于快速集成。目前，官方转换器支持Caffe、Keras、LibSVM、scikit-learn和XGBoost，但该包是开源的，为其他流行的机器学习框架提供了许多其他转换器，例如TensorFlow。
- en: 'At its core, the conversion process generates a model specification that is
    a machine-interpretable representation of the learning models and is used by Xcode
    to generate the Core ML models, consisting of the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 转换过程的核心是生成一个模型规范，它是学习模型的机器可解释表示，并由Xcode用于生成Core ML模型，包括以下内容：
- en: '**Model description**: Encodes names and type information of the inputs and
    outputs of the model'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型描述**：编码模型输入和输出的名称和类型信息'
- en: '**Model parameters**: The set of parameters required to represent a specific
    instance of the model (model weights/coefficients)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数**：表示模型特定实例所需的一组参数（模型权重/系数）'
- en: '**Additional metadata**: Information about the origin, license, and author
    of the model'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附加元数据**：关于模型来源、许可和作者的信息'
- en: In this chapter, we present the most simplistic of flows, but we will revisit
    the Core ML Tools package in [Chapter 6](40971e0d-b260-42e1-a9fb-5c4a56b0ebb2.xhtml),
    *Creating Art with Style Transfer*, to see how to deal with custom layers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了最简单的流程，但我们将回顾第6章[Creating Art with Style Transfer](40971e0d-b260-42e1-a9fb-5c4a56b0ebb2.xhtml)中的Core
    ML Tools包，以了解如何处理自定义层。
- en: To avoid any complications when setting up the environment on your local or
    remote machine, we will leverage the free Jupyter cloud service provided by Microsoft.
    Head over to [https://notebooks.azure.com](https://notebooks.azure.com) and log
    in, or register if you haven't already.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在本地或远程机器上设置环境时出现任何复杂性，我们将利用微软提供的免费Jupyter云服务。请访问[https://notebooks.azure.com](https://notebooks.azure.com)并登录，或者如果您还没有，请注册。
- en: 'Once logged in, click on the Libraries menu link from the navigation bar, which
    will take you to a page containing a list of all your libraries, similar to what
    is shown in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，点击导航栏中的“库”菜单链接，这将带您到一个包含所有库列表的页面，类似于以下截图所示：
- en: '![](img/9c8f12d6-2b5b-4c41-a724-6903324fa026.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9c8f12d6-2b5b-4c41-a724-6903324fa026.png)'
- en: 'Next, click on the + New Library link to bring up the Create New Library dialog:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击“+新建库”链接以打开创建新库对话框：
- en: '![](img/19bb5167-2674-4f8c-bc58-c8b64bab75b9.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/19bb5167-2674-4f8c-bc58-c8b64bab75b9.png)'
- en: Then click on the From GitHub tab and enter `https://github.com/packtpublishing/machine-learning-with-core-ml`
    in the GitHub repository field. After that, give your library a meaningful name
    and click on the Import button to begin the process of cloning the repository
    and creating the library.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后点击“从GitHub”标签，并在GitHub仓库字段中输入`https://github.com/packtpublishing/machine-learning-with-core-ml`。之后，为您的库起一个有意义的名称，并点击导入按钮以开始克隆仓库并创建库的过程。
- en: 'Once the library has been created, you will be redirected to the root; from
    here, click on the `Chapter5/Notebooks` folder to open up the relevant folder
    for this chapter. Finally, click on the Notebook `Tiny YOLO_Keras2CoreML.ipynb`.
    To help ensure that we are all on the same page (pun intended), here is a screenshot
    of what you should see after clicking on the `Chapter5/Notebooks` folder:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 创建库后，您将被重定向到根目录；从这里，点击`Chapter5/Notebooks`文件夹以打开本章的相关文件夹。最后，点击笔记本`Tiny YOLO_Keras2CoreML.ipynb`。为了确保我们都在同一页面上（有意为之），以下是点击`Chapter5/Notebooks`文件夹后您应该看到的截图：
- en: '![](img/3d07e169-339e-41dd-904d-d5db7e83baff.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d07e169-339e-41dd-904d-d5db7e83baff.png)'
- en: 'With our Notebook now loaded, it''s time to walk through each of the cells
    to create our Core ML model; all of the required code exists and all that remains
    is executing each of the cells sequentially. To execute a cell, you can either
    use the shortcut keys *Shift* + *Enter* or click on the Run button in the toolbar
    (which will run the currently selected cell), as shown in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了笔记本，是时候逐个检查每个单元格以创建我们的Core ML模型了；所有必要的代码都已存在，剩下的只是依次执行每个单元格。要执行一个单元格，您可以使用快捷键*Shift*
    + *Enter*，或者点击工具栏中的运行按钮（这将运行当前选定的单元格），如下面的截图所示：
- en: '![](img/c956b413-3944-4894-9ebb-915ccba65bfd.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c956b413-3944-4894-9ebb-915ccba65bfd.png)'
- en: I will provide a brief explanation of what each cell does; ensure that you execute
    each cell as we walk through them so that we all end up with the converted model,
    which we will then download and use in the next section for our iOS project.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我将对每个单元格的功能进行简要说明；确保我们在浏览它们时执行每个单元格，这样我们最终都会得到转换后的模型，然后我们将下载并用于下一节的iOS项目。
- en: 'We start by ensuring that the Core ML Tools Python package is available in
    the environment, by running the following cell:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过运行以下单元格来确保环境中可用Core ML Tools Python包：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once installed, we make the package available by importing it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们通过导入它来使包可用：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The model architecture and weights have been serialized and saved to the file
    `tinyyolo_voc2007_modelweights.h5`; in the following cell, we will pass this into
    the convert function of the Keras converter, which will return the converted Core
    ML model (if no errors occur). Along with the file, we also pass in values for
    the parameters `input_names`, `image_input_names`, `output_names`, and `image_scale`.
    The `input_names` parameter takes in a single string, or a list of strings for
    multiple inputs, and is used to explicitly set the names that will be used in
    the interface of the Core ML model to refer to the inputs of the Keras model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构和权重已序列化并保存到文件`tinyyolo_voc2007_modelweights.h5`中；在下面的单元格中，我们将将其传递给Keras转换器的转换函数，该函数将返回转换后的Core
    ML模型（如果无错误发生）。除了文件外，我们还传递了`input_names`、`image_input_names`、`output_names`和`image_scale`参数的值。`input_names`参数接受单个字符串或字符串列表（用于多个输入），并用于在Core
    ML模型的接口中显式设置用于引用Keras模型输入的名称。
- en: 'We also pass this input name to the `image_input_names` parameter so that the
    converter treats the input as an image rather than an N-dimensional array. Similar
    to `input_names`, values passed to `output_names` will be used in the interface
    of the Core ML model to refer to the outputs of the Keras model. The last parameter,
    `image_scale`, allows us to add a scaling factor to our input before being passed
    to the model. Here, we are dividing each pixel by 255, which forces each pixel
    to be in a range of *0.0* to *1.0*, a typical preprocessing task when working
    with images. There are plenty more parameters available, allowing you to tune
    and tweak the inputs and outputs of your model. You can learn more about these
    at the official documentation site here at [https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html](https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html).
    In the next snippet, we perform the actual conversion using what we have just
    discussed:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将此输入名称传递给`image_input_names`参数，以便转换器将输入视为图像而不是N维数组。与`input_names`类似，传递给`output_names`的值将在Core
    ML模型的界面中用于引用Keras模型的输出。最后一个参数`image_scale`允许我们在将输入传递给模型之前添加一个缩放因子。在这里，我们将每个像素除以255，这迫使每个像素处于*0.0*到*1.0*的范围内，这是处理图像时的典型预处理任务。还有许多其他参数可供选择，允许您调整和微调模型的输入和输出。您可以在官方文档网站上了解更多信息，网址为[https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html](https://apple.github.io/coremltools/generated/coremltools.converters.keras.convert.html)。在下一个片段中，我们将使用我们刚刚讨论的内容进行实际转换：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With reference to the converted model, `coreml_model`, we add metadata, which
    will be made available and displayed in Xcode''s ML model views:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 参考转换后的模型`coreml_model`，我们添加了元数据，这些数据将在Xcode的ML模型视图中提供和显示：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to save our model; run the final cell to save the converted
    model:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好保存我们的模型；运行最后的单元格以保存转换后的模型：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With our model now saved, we return to the previous tab showing the contents
    of the `Chapter5` directory and download the `tinyyolo_voc2007.mlmodel` file.
    We do so by either right-clicking on it and selecting the Download menu item,
    or by clicking on the Download toolbar item, as shown in the following screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经保存了我们的模型，我们回到之前显示`Chapter5`目录内容的标签页，并下载`tinyyolo_voc2007.mlmodel`文件。我们可以通过右键点击它并选择下载菜单项，或者点击下载工具栏项来完成，如下面的截图所示：
- en: '![](img/4e089f81-195a-4c68-ac48-937d747bb77e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4e089f81-195a-4c68-ac48-937d747bb77e.png)'
- en: With our converted model in hand, it's now time to jump into Xcode and work
    through the example project for this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有我们转换后的模型在手，现在是时候跳入Xcode并处理本章的示例项目了。
- en: Making it easier to find photos
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使查找照片变得更简单
- en: In this section, we will put our model to work in an intelligent search application;
    we'll start off by quickly introducing the application, giving us a clear vision
    of what we intend to build. Then, we'll work through implementing the functionality
    related to interpreting the model's output and search heuristic for the desired
    functionality. We will be omitting a lot of the usual iOS functionality so that
    we can stay focused on the intelligent aspect of the application.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把我们的模型应用于智能搜索应用中；我们首先快速介绍该应用，以便我们清楚地了解我们想要构建的内容。然后，我们将实现与解释模型输出和搜索启发式相关联的功能。我们将省略许多常规的iOS功能，以便我们能够专注于应用程序的智能方面。
- en: Over the past few years, we have seen a surge of intelligence being embedded
    in photo gallery applications, providing us with efficient ways of surfacing those
    cat photos hidden deep in hundreds (if not thousands) of photos we have accumulated
    over the years. In this section, we want to continue with this theme but push
    that level of intelligence a little bit further by taking advantage of the semantic
    information gained through object detection. Our users will be able to search
    for not only specific objects within a image, but also photos based on the objects
    and their relative positioning. For example, they can search for an image, or
    images, with two people standing side by side and in front of a car.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，我们看到了智能在相册应用中的大量嵌入，为我们提供了高效的方法来展示那些隐藏在数百（如果不是数千）张我们多年来积累的照片中的猫照片。在本节中，我们希望继续这一主题，但通过利用通过目标检测获得的语义信息，将智能水平提升一点。我们的用户将能够搜索图像中的特定对象，以及基于对象及其相对位置的照片。例如，他们可以搜索两个人并排站立在车前的图像或图像。
- en: The user interface allows the user to draw the objects as they would like them
    positioned and their relative sizes. It will be our job in this section to implement
    the intelligence that returns relevant images based on this search criteria.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面允许用户绘制他们希望定位的对象及其相对大小。在本节中，我们的任务是实现基于这些搜索标准的智能，以返回相关图像。
- en: 'The next figure shows the user interface; the first two screenshots show the
    search screen, where the user can visually articulate what they are looking for.
    By using labeled bounding boxes, the user is able to describe what they are looking
    for, how they would like these objects arranged, and relative object sizes. The
    last two screenshots show the result of a search, and when expanded (last screenshot),
    the image will be overlaid with the detected objects and associated bounding boxes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图示显示了用户界面；前两个截图显示了搜索屏幕，用户可以在其中直观地表达他们想要寻找的内容。通过使用标记的边界框，用户能够描述他们想要寻找的内容，他们希望如何排列这些对象以及相对对象的大小。最后两个截图显示了搜索结果，当展开（最后一个截图）时，图像将叠加检测到的对象及其关联的边界框：
- en: '![](img/c2d18585-97a8-4c18-980f-5106f4e79979.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2d18585-97a8-4c18-980f-5106f4e79979.png)'
- en: Let's start by taking a tour of the existing project before importing the model
    we have just converted and downloaded.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入我们刚刚转换和下载的模型之前，让我们先浏览一下现有的项目。
- en: 'If you haven''t already, pull down the latest code from the accompanying repository: 
    [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter5/Start/` and open the project
    `ObjectDetection.xcodeproj`. Once loaded, you will see the project for this chapter,
    as shown in the following screenshot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有做，请从配套仓库中拉取最新代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载完成后，导航到目录
    `Chapter5/Start/` 并打开项目 `ObjectDetection.xcodeproj`。加载后，您将看到本章的项目，如下面的截图所示：
- en: '![](img/e23df0d7-0d32-4113-a52d-9d5d45500745.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e23df0d7-0d32-4113-a52d-9d5d45500745.png)'
- en: I will leave exploring the full project as an exercise for you, and I'll just
    concentrate on the files `PhotoSearcher.swift` and `YOLOFacade.swift` for this
    section. `PhotoSearcher.swift` is where we will implement the cost functions responsible
    for filtering and sorting the photos based on the search criteria and detected
    objects from `YOLOFacade.swift`, whose sole purpose is to wrap the Tiny YOLO model
    and implement the functionality to interpret its output. But before jumping into
    the code, let's quickly review the flow and data structures we will be working
    with.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把探索整个项目作为您的练习，而我会专注于本节中的文件 `PhotoSearcher.swift` 和 `YOLOFacade.swift`。`PhotoSearcher.swift`
    是我们将实现负责根据搜索标准和 `YOLOFacade.swift` 中检测到的对象过滤和排序照片的成本函数的地方，`YOLOFacade.swift` 的唯一目的是封装
    Tiny YOLO 模型并实现解释其输出的功能。但在深入代码之前，让我们快速回顾一下我们将要工作的流程和数据结构。
- en: The following diagram illustrates the general flow of the application; the user
    first defines the search criteria via `SearchViewController`, which is described
    as an array of normalized `ObjectBounds`. We'll cover more details on these later.
    When the user initiates the search (top-right search icon) these are passed to
    `SearchResultsViewController`, which delegates the task of finding suitable images
    to `PhotoSearcher`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示说明了应用程序的一般流程；用户首先通过 `SearchViewController` 定义搜索标准，该控制器被描述为一个包含归一化 `ObjectBounds`
    的数组。我们稍后会详细介绍这些内容。当用户启动搜索（右上角的搜索图标）时，这些标准会被传递给 `SearchResultsViewController`，它将找到合适图像的任务委托给
    `PhotoSearcher`。
- en: '`PhotoSearcher` proceeds to iterate through all of our photos, passing each
    of them through to `YOLOFacade` to perform object detection using the model we
    converted in the previous section. The results of these are passed back to `PhotoSearcher`,
    which evaluates the cost of each with respect to the search criteria and then
    filters and orders the results, before passing them back to `SearchResultsViewController`
    to be displayed:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`PhotoSearcher` 继续遍历我们所有的照片，将每一张照片通过 `YOLOFacade` 进行对象检测，使用我们在上一节中转换的模型。这些结果被传回
    `PhotoSearcher`，它根据搜索标准评估每一项的成本，然后过滤和排序结果，最后将它们传回 `SearchResultsViewController`
    以供显示：'
- en: '![](img/d31059c4-70ff-44fe-b7bf-c737a34299ab.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d31059c4-70ff-44fe-b7bf-c737a34299ab.png)'
- en: 'Each component communicates with the another using either the data object `ObjectBounds`
    or `SearchResult`. Because we will be working with them throughout the rest of
    this chapter, let''s quickly introduce them here, all of which are defined in
    the `DataObjects.swift` file. Let''s start with `ObjectBounds`, the structure
    shown in the following snippet:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组件都通过数据对象 `ObjectBounds` 或 `SearchResult` 与另一个组件进行通信。因为我们将在本章的其余部分使用它们，所以让我们快速介绍它们，所有这些都在
    `DataObjects.swift` 文件中定义。让我们从以下片段中的 `ObjectBounds` 开始：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As the name suggests, `ObjectBounds` is just that—it encapsulates the boundary
    of an object using the variables `origin` and `size`. The `object` itself is of
    type `DetectableObject`, which provides a structure to store both the class index
    and its associated label. It also provides a static array of objects that are
    available in our search, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，`ObjectBounds` 正是这样——它使用 `origin` 和 `size` 变量封装了对象的边界。`object` 本身是 `DetectableObject`
    类型，它提供了一个结构来存储类索引及其关联的标签。它还提供了一个静态对象数组，这些对象可用于我们的搜索，如下所示：
- en: '[PRE6]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`ObjectBounds` are used for both the search criteria defined by the user and
    search results returned by `YOLOFacade`; in the former, they describe where and
    which objects the user is interested in finding (search criteria), and the latter
    encapsulates the results from object detection.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`ObjectBounds` 用于用户定义的搜索标准和 `YOLOFacade` 返回的搜索结果；在前者中，它们描述了用户感兴趣寻找的位置和对象（搜索标准），而在后者中，它们封装了对象检测的结果。'
- en: '`SearchResult` doesn''t get any more complex; it''s intended to encapsulate
    the result of a search with the addition of the image and cost, which is set during
    the cost evaluation stage (*step 8*), as shown in the previous diagram. For the
    complete code, the structure is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`SearchResult` 并不复杂；它的目的是封装搜索结果，并添加图像和成本，这些成本在成本评估阶段（*步骤 8*）设置，如前图所示。对于完整的代码，结构如下：'
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It's worth noting that the `ObjectBounds` messages, in the previous diagram,
    annotated with the word **Normalized**, refer to the values being in unit values
    based on the source or target size; that is, an origin of *x = 0.5* and *y = 0.5*
    defines the center of the source image it was defined on. The reason for this
    to ensure that the bounds are invariant to changes in the images they are operating
    on. You will soon see that, before passing images to our model, we need to resize
    and crop to a size of 416 x 416 (the expected input to our model), but we need
    to transform them back to the original for rendering the results.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在之前的图中，用词 **Normalized** 注释的 `ObjectBounds` 消息，指的是基于源或目标大小的单位值；也就是说，*x
    = 0.5* 和 *y = 0.5* 的原点定义了源图像的中心。这样做是为了确保边界不受它们操作图像变化的影响。你很快就会看到，在将图像传递给我们的模型之前，我们需要将其调整大小并裁剪到
    416 x 416 的大小（这是我们模型的预期输入），但我们需要将它们转换回原始大小以渲染结果。
- en: Now, we have a better idea of what objects we will be consuming and generating;
    let's proceed with implementing the `YOLOFacade` and work our way up the stack.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对我们将消费和生成哪些对象有了更好的了解；让我们继续实现 `YOLOFacade` 并逐步构建。
- en: 'Let''s start by importing the model we have just converted in the previous
    section; locate the downloaded `.mlmodel` file and drag it onto Xcode. Once imported,
    select it from the left-hand panel to inspect the metadata to remind ourselves
    what we need to implement. It should resemble this screenshot:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入上一节中刚刚转换的模型开始；找到下载的 `.mlmodel` 文件并将其拖放到 Xcode 中。导入后，从左侧面板中选择它以检查元数据，以便提醒自己需要实现的内容。它应该类似于以下截图：
- en: '![](img/c09a525c-cff6-494f-b272-cd1d0681949f.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c09a525c-cff6-494f-b272-cd1d0681949f.png)'
- en: With our model now imported, let's walk through implementing the functionality
    `YOLOFacade` is responsible for; this includes preprocessing the image, passing
    it to our model for inference, and then parsing the model's output, including
    performing **non-max supression**. Select `YOLOFacade.swift` from the left-hand
    panel to bring up the code in the main window.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经导入了模型，让我们来了解 `YOLOFacade` 负责的功能实现；这包括预处理图像，将其传递给我们的模型进行推理，然后解析模型的输出，包括执行
    **非最大值抑制**。从左侧面板中选择 `YOLOFacade.swift` 以在主窗口中显示代码。
- en: 'The class is broken into three parts, via an extension, with the first including
    the variables and entry point; the second including the functionality for performing
    inference and parsing the models outputs; and the third part including the non-max
    supression algorithm we discussed at the start of this chapter. Let''s start at
    the beginning which currently looks like this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 该类通过扩展分为三个部分，第一部分包括变量和入口点；第二部分包括执行推理和解析模型输出的功能；第三部分包括我们在本章开头讨论的非最大抑制算法。让我们从开始的地方开始，目前看起来是这样的：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `asyncDetectObjects` method is the entry point of the class and is called
    by `PhotoSearcher` for each image it receives from the Photos framework; when
    called, this method simply delegates the task to the method `detectObject` in
    the background and waits for the results, before passing them back to the caller
    on the main thread. I have annotated the class with `TODO` to help you keep focused.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`asyncDetectObjects`方法是类的入口点，由`PhotoSearcher`在接收到来自Photos框架的每一张图片时调用；当被调用时，此方法简单地委托任务到后台的`detectObject`方法，并等待结果，然后将它们传递回主线程上的调用者。我已经用`TODO`注释了这个类，以帮助您保持专注。'
- en: 'Let''s start by declaring the target size required by our model; this will
    be used for preprocessing of the input of our model and transforming the normalized
    bounds to those of the source image. Add the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先声明模型所需的目标大小；这将用于预处理模型的输入并将归一化边界转换为源图像的尺寸。添加以下代码：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define properties of our model that are used during parsing of the
    output; these include grid size, number of classes, number of anchor boxes, and
    finally, the dimensions for each of the anchor boxes (each pair describes the
    width and height, respectively). Make the following amendments to your `YOLOFacade`
    class:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型在输出解析过程中使用的属性；这些包括网格大小、类别数量、锚框数量，以及每个锚框的尺寸（每一对分别描述宽度和高度）。对您的`YOLOFacade`类进行以下修改：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s now implement the model property; in this example, we will take advantage
    of the Vision framework for handling the preprocessing. For this, we will need
    to wrap our model in an instance of `VNCoreMLModel` so that we can pass it into
    a `VNCoreMLRequest`; make the following amendments, as shown in bold:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现模型属性；在这个例子中，我们将利用Vision框架来处理预处理。为此，我们需要将我们的模型包装在一个`VNCoreMLModel`实例中，这样我们就可以将其传递给`VNCoreMLRequest`；按照以下所示进行以下修改，加粗显示：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let's now turn our attention to the `detectObjects` method. It will be responsible
    for performing inference via `VNCoreMLRequest` and `VNImageRequestHandler`, passing
    the model's output to the `detectObjectsBounds` method (which we will come to
    next), and finally transforming the normalized bounds to the dimensions of the
    original (source) image.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将注意力转向`detectObjects`方法。它将负责通过`VNCoreMLRequest`和`VNImageRequestHandler`进行推理，将模型的输出传递给`detectObjectsBounds`方法（我们将在下一章中介绍），并将归一化边界转换为原始（源）图像的尺寸。
- en: In this chapter, we will postpone the discussion around the Vision framework
    classes (`VNCoreMLModel`, `VNCoreMLRequest`, and `VNImageRequestHandler`) until
    the next chapter, where we will elaborate a little on what each does and how they
    work together.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将推迟对Vision框架类（`VNCoreMLModel`、`VNCoreMLRequest`和`VNImageRequestHandler`）的讨论，直到下一章，届时我们将简要说明每个类的作用以及它们是如何协同工作的。
- en: 'Within the `detectObjects` method of `YOLOFacade`, replace the comment `//
    TODO preprocess image and pass to model` with the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在`YOLOFacade`类的`detectObjects`方法中，将注释`// TODO preprocess image and pass to model`替换为以下代码：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding snippet, we start by creating an instance of `VNCoreMLRequest`,
    passing in our model, which itself has been wrapped with an instance of `VNCoreMLModel`.
    This request performs the heavy lifting, including preprocessing (inferred by
    the model's metadata) and performing inference. We set its `imageCropAndScaleOption`
    property to `centerCrop`, which determines, as you might expect, how the image
    is resized to fit into the model's input. The request itself doesn't actually
    execute the task; this is the responsibility of `VNImageRequestHandler`, which
    we declare next by passing in our source image and then executing the request
    via the handler's `perform` method.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们首先创建了一个`VNCoreMLRequest`实例，并将我们的模型传递给它，该模型本身已经被一个`VNCoreMLModel`实例包装。这个请求执行了繁重的工作，包括预处理（由模型的元数据推断）和执行推理。我们将它的`imageCropAndScaleOption`属性设置为`centerCrop`，正如你可能预期的，这决定了图像如何调整大小以适应模型输入。请求本身并不实际执行任务；这是`VNImageRequestHandler`的责任，我们通过传递我们的源图像并执行处理器的`perform`方法来声明它。
- en: 'If all goes to plan, we should expect to have the model''s output available
    via the request''s results property. Let''s move on to the last snippet for this
    method; replace the comment `// TODO pass models results to detectObjectsBounds(::)`
    and the following statement, `completionHandler(nil)`, with this code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按计划进行，我们应该期望通过请求的结果属性获得模型的输出。让我们继续到最后一个代码片段；将注释`// TODO pass models results
    to detectObjectsBounds(::)`和随后的语句`completionHandler(nil)`替换为以下代码：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We begin by trying to cast the results to an array of `VNCoreMLFeatureValueObservation`,
    a type of image analysis observation that provides key-value pairs. One of them
    is `multiArrayValue`, which we then pass to the `detectObjectsBounds` method to
    parse the output and return the detected objects and their bounding boxes. Once
    `detectObjectsBounds` returns, we map each of the results with the `ObjectBounds`
    method `transformFromCenteredCropping`, which is responsible for transforming
    the normalized bounds into the space of the source image. Once each of the bounds
    has been transformed, we call the completion handler, passing in the detected
    bounds.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先尝试将结果转换为`VNCoreMLFeatureValueObservation`数组，这是一种图像分析观察类型，它提供键值对。其中之一是`multiArrayValue`，然后我们将其传递给`detectObjectsBounds`方法以解析输出并返回检测到的对象及其边界框。一旦`detectObjectsBounds`返回，我们就使用`ObjectBounds`方法的`transformFromCenteredCropping`将每个结果映射，该方法负责将归一化边界转换为源图像的空间。一旦每个边界都被转换，我们就调用完成处理程序，传递检测到的边界。
- en: The next two methods encapsulate the bulk of the YOLO algorithm and the bulk
    of the code for this class. Let's start with the `detectObjectsBounds` method,
    making our way through it in small chunks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下两个方法封装了YOLO算法的大部分内容和此类的大部分代码。让我们从`detectObjectsBounds`方法开始，逐步进行。
- en: 'This method will receive an `MLMultiArray` with the shape of *(125, 13, 13)*;
    this will hopefully look familiar to you (although reversed) where the *(13, 13)*
    is the size of our grid and the 125 encodes five blocks (coinciding with our five
    anchor boxes) each containing the bounding box, the probability of an object being
    present (or not), and the probability distribution across 20 classes. For your
    convenience, I have again added the diagram illustrating this structure:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将接收一个形状为*(125, 13, 13)*的`MLMultiArray`；这应该对你来说很熟悉（尽管是反过来的），其中*(13, 13)*是我们网格的大小，而125编码了五个块（与我们的五个锚框相对应），每个块包含边界框、对象存在的概率（或不存在）以及跨越20个类别的概率分布。为了方便起见，我再次添加了说明此结构的图解：
- en: '![](img/ca1f312e-358e-4a58-98f1-80da52f41fc8.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca1f312e-358e-4a58-98f1-80da52f41fc8.png)'
- en: 'To improve performance, we will access the `MLMultiArray`''s raw data directly,
    rather than through the `MLMultiArray` subscript. Although having direct access
    gives us a performance boost, it does have a trade-off of requiring us to correctly
    calculate the index for each value. Let''s define the constants that we will use
    when calculating these indexes, as well as obtaining access to the raw data buffer
    and some arrays to store the intermediate results; add the following code within
    your `detectObjectsBounds` method:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，我们将直接访问`MLMultiArray`的原始数据，而不是通过`MLMultiArray`下标。虽然直接访问给我们带来了性能提升，但它也有一个权衡，即我们需要正确计算每个值的索引。让我们定义我们将用于计算这些索引的常量，以及获取对原始数据缓冲区和一些用于存储中间结果的数组的访问权限；在你的`detectObjectsBounds`方法中添加以下代码：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As mentioned before, we start by defining constants of stride values for the
    grid, row, and column—each used to calculate the current value. These values are
    obtained through the `strides` property of `MLMultiArray`, which gives us the
    number of data elements in each dimension. In this case, this would be 125, 13,
    and 13 respectively. Next, we get a reference to the underlying buffer of the
    `MLMultiArray` and, finally, we create two arrays to store the bounds and associated
    confidence value.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先定义了网格、行和列的步长常量，每个常量都用于计算当前值。这些值通过`MLMultiArray`的`strides`属性获得，它给出了每个维度的数据元素数量。在这种情况下，这将是125、13和13。接下来，我们获取`MLMultiArray`的底层缓冲区的引用，最后，我们创建两个数组来存储边界和相关的置信度值。
- en: 'Next, we want to iterate through the model''s output and process each of the
    grid cells and their subsequent anchor boxes independently; we do this by using
    three nested loops and then calculating the relevant index. Let''s do that by
    adding the following snippet:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要遍历模型的输出并独立处理每个网格单元及其后续的锚框；我们通过使用三个嵌套循环并计算相关索引来实现这一点。让我们通过添加以下代码片段来完成这个任务：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The important values here are `gridOffset` and `anchorBoxOffset`; `gridOffset`
    gives us the relevant offset for the specific grid cell (as the name implies),
    while `anchorBoxOffset` gives us the index of the associated anchor box. Now that
    we have these values, we can access each of the elements using the `[(anchorBoxOffset
    + INDEX_TO_VALUE) * gridStride + gridOffset` index, where `INDEX_TO_VALUE` is
    the relevant value within the anchor box vector we want to access, as illustrated
    in this diagram:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里重要的值是`gridOffset`和`anchorBoxOffset`；`gridOffset`给出了特定网格单元的相关偏移量（正如其名称所暗示的），而`anchorBoxOffset`给出了相关锚框的索引。现在我们有了这些值，我们可以使用`[(anchorBoxOffset
    + INDEX_TO_VALUE) * gridStride + gridOffset` 索引来访问每个元素，其中`INDEX_TO_VALUE`是我们想要访问的锚框向量中的相关值，如图所示：
- en: '![](img/7f430c34-c32a-4b53-bfad-50114ad51c93.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7f430c34-c32a-4b53-bfad-50114ad51c93.png)'
- en: 'Now we know how to access each bounding box for each grid cell in our buffer,
    let''s use it to find the most probable class and put in our first test of ignoring
    any prediction if it doesn''t meet our threshold (defined as a method parameter
    with the default value of 0.3*:* `objectThreshold:Float = 0.3`). Add the following
    code, replacing the comment `// TODO calculate the confidence of each class, ignoring
    if under threshold`, as seen previously:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何访问我们缓冲区中每个网格单元的每个边界框，让我们用它来找到最可能的类别，并在我们的第一次测试中忽略任何未达到我们阈值（定义为方法参数，默认值为0.3*：`objectThreshold:Float
    = 0.3`）的预测。添加以下代码，替换掉之前的注释`// TODO calculate the confidence of each class, ignoring
    if under threshold`：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code snippet, we first obtain the probability of an object
    being present and store it in the constant `confidence`. Then, we populate an
    array with the probabilities of all the classes, before applying a softmax across
    them all. This will squash the values so that the accumulated value of them equals
    *1.0,* essentially providing us with our probability distribution across all classes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们首先获取一个对象存在的概率并将其存储在常量`confidence`中。然后，我们用一个包含所有类别概率的数组填充数组，在它们之上应用softmax。这将压缩这些值，使得它们的累积值等于*1.0*，本质上为我们提供了所有类别的概率分布。
- en: We then find the class index with the largest probability and multiply it with
    our `confidence` constant, which gives us the class confidence we will threshold
    against and use during non-max suppression, ignoring the prediction if it doesn't
    meet our threshold.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们找到概率最大的类别索引，并将其与我们的`confidence`常量相乘，这给了我们我们将要阈值化并用于非极大值抑制的类别置信度。如果预测未达到我们的阈值，我们将忽略该预测。
- en: 'Before continuing with the procedure, I want to take a quick detour to highlight
    and explain a couple of the methods used in the preceding snippet, namely the
    `softmax` method and `argmax` property of the classes array. Softmax is a logistic
    function that essentially squashes a vector of numbers so that all values in the
    vector add up to 1; it''s an activation function commonly used when dealing with
    multi-class classification problems where the result is interpreted as the likelihood
    of each class, typically taking the class with the largest value as the predicted
    class (within a threshold). The implementation can be found in the `Math.swift`
    file, which makes use of the Accelerate framework to improve performance. The
    equation and implementation are shown here for completeness, but the details are
    omitted and left for you to explore:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行程序之前，我想快速地绕道一下，突出并解释一下前面代码片段中使用的一些方法，即`softmax`方法和类数组中的`argmax`属性。Softmax是一种逻辑函数，它本质上将一个数字向量压缩，使得向量中的所有值加起来等于1；它是一种在处理多类分类问题时常用的激活函数，其中结果被解释为每个类的可能性，通常将具有最大值的类作为预测类（在阈值内）。实现可以在`Math.swift`文件中找到，该文件利用Accelerate框架来提高性能。为了完整性，这里展示了方程和实现，但细节被省略，留给您去探索：
- en: '![](img/afcd28b0-c339-49fc-a38a-f254a0f0c909.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afcd28b0-c339-49fc-a38a-f254a0f0c909.png)'
- en: Here, we use a slightly modified version of the equation shown previously; in
    practice, calculating the softmax values can be problematic if any of the values
    are very large. Applying an exponential operation on it will make it explode,
    and dividing any value by a huge value can cause arithmetic computation problems.
    To avoid this, it is often best practice to subtract the maximum value from all
    elements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了一个略微修改过的之前显示的方程；在实践中，如果任何值非常大，计算softmax值可能会出现问题。对它应用指数运算会使它爆炸，而将任何值除以一个巨大的值可能会导致算术计算问题。为了避免这种情况，通常最好的做法是从所有元素中减去最大值。
- en: 'Because there are quite a few functions for this operation, let''s build it
    up piece by piece, from the inside out. The following is the function that performs
    element-wise subtraction:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因为有相当多的函数用于这个操作，让我们从内到外逐步构建。以下是一个执行逐元素减法的函数：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, the function that computes the element-wise exponential for an array:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算数组逐元素指数的函数：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, the function to perform summation on an array, as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，执行数组求和的函数，如下所示：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is the last function used by the softmax function! This will be responsible
    for performing element-wise division for a given scalar, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是softmax函数使用的最后一个函数！它将负责对给定的标量执行逐元素除法，如下所示：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we the softmax function (using the max trick as described previously):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用softmax函数（使用之前描述的max技巧）：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In addition to the preceding functions, it uses an extension property, `maxValue`,
    of Swift''s array class; this extension also includes the `argmax` property alluded
    to previously. So, we will present both together in the following snippet, found
    in the `Array+Extension.swift` file. Before presenting the code, just a reminder
    about the function of the `argmax` property—its purpose is to return the index
    of the largest value within the array, a common method available in the Python
    package NumPy:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的函数之外，它还使用Swift数组类的扩展属性`maxValue`；这个扩展还包括之前提到的`argmax`属性。因此，我们将它们一起在下面的代码片段中展示，该片段位于`Array+Extension.swift`文件中。在展示代码之前，提醒一下`argmax`属性的功能——它的目的是返回数组中最大值的索引，这是Python包NumPy中常见的方法：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s now turn our attention back to the parsing of the model''s output and
    extracting the detected objects and associated bounding boxes. Within the loop,
    we now have a prediction we are somewhat confident with, having passed our threshold
    filter. The next task is to extract and transform the bounding box of the predicted
    object. Add the following code, replacing the line `// TODO obtain bounding box
    and transform to image dimensions`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转回到模型输出的解析和提取检测到的对象及其关联的边界框。在循环中，我们现在有一个我们相当有信心的预测，因为它已经通过了我们的阈值过滤器。下一个任务是提取和转换预测对象的边界框。添加以下代码，替换掉`//
    TODO obtain bounding box and transform to image dimensions`这一行：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We start by getting the first four values of from the grid cell's anchor box
    segment; this returns the center position and size relative to the grid. The next
    block is responsible for transforming these values from the grid coordinate system
    to the image coordinate system. For the center position, we pass the returned
    value through a `sigmoid` function, keeping it between *0.0 - 1.0*, and offset
    based on the relevant column (or row). Finally we divide it by the grid size (13).
    Similarly with the dimensions, we first get the associated anchor box, multiplying
    it by the exponential of the predicted dimension and then dividing it by the grid
    size.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从网格单元的锚框段获取前四个值；这返回了相对于网格的中心位置和大小。下一个块负责将这些值从网格坐标系转换到图像坐标系。对于中心位置，我们通过一个`sigmoid`函数传递返回的值，使其保持在*0.0
    - 1.0*之间，并根据相关列（或行）进行偏移。最后，我们将其除以网格大小（13）。同样，对于尺寸，我们首先获取相关的锚框，将其乘以预测尺寸的指数，然后除以网格大小。
- en: 'As we have done previously, I now present the implementations for the function
    `sigmoid` for reference, which can found in the `Math.swift` file. The equation
    is shown as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我现在提供函数`sigmoid`的实现，供参考，该函数可以在`Math.swift`文件中找到。方程如下：
- en: '![](img/1fb2ae26-e3be-4ffa-9f64-beffc326387d.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fb2ae26-e3be-4ffa-9f64-beffc326387d.png)'
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The final chunk of code simply creates an instance `ObjectBounds`, passing
    in the transformed bounding box and the associated `DetectableObject` class (filtering
    on the class index). Add the following code, replacing the comment `// TODO create
    a ObjectBounds instance and store it in our array of candidates`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后部分只是创建了一个`ObjectBounds`实例，传递了转换后的边界框和相关的`DetectableObject`类（根据类索引进行过滤）。添加以下代码，替换注释`//
    TODO create a ObjectBounds instance and store it in our array of candidates`：
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In addition to storing the `ObjectBounds`, we also store `confidence`, which
    will be used when we get to implementing non-max suppression.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储`ObjectBounds`之外，我们还存储`confidence`，这将在我们实现非极大值抑制时使用。
- en: 'This completes the functionality required within the nested loops; by the end
    of this process, we have an array populated with our candidate detected objects.
    Our next task will be to filter them. Near the end of the `detectObjectsBounds`
    method, add the following statement (outside any loops):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了嵌套循环内所需的功能；到这个过程结束时，我们有一个填充了候选检测对象的数组。我们的下一个任务将是过滤它们。在`detectObjectsBounds`方法的末尾，添加以下语句（在任意循环之外）：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, we are simply returning the results from the `filterDetectedObjects`
    method, which we will now turn our attention to. The method has been blocked out
    but is vacant of functionality, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是返回`filterDetectedObjects`方法的结果，我们现在将关注这个方法。该方法已被屏蔽，但尚未实现任何功能，如下所示：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Our job will be to implement the non-max suppression algorithm; just to recap,
    the algorithm can be described as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作是实现非极大值抑制算法；为了回顾，算法可以描述如下：
- en: Order the detected boxes from most confident to least
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照置信度从高到低对检测到的框进行排序
- en: 'While valid boxes remain, do the following:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当有效框仍然存在时，执行以下操作：
- en: Pick the box with the highest confidence value (the top of our ordered array)
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择置信度值最高的框（我们排序数组中的顶部）
- en: Iterate through all the remaining boxes, discarding any with an IoU value greater
    than a predefined threshold
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历所有剩余的框，丢弃任何IoU值大于预定义阈值的框
- en: 'Let''s start by creating a clone of the confidence array passed into the method;
    we will use this to obtain an array of sorted indices, as well as to flag any
    boxes that are sufficiently overlapped by the preceding box. This is done by simply
    setting its confidence value to 0\. Add the following statement to do just this,
    along with creating the sorted array of indices, replacing the comment `// TODO
    implement Non-Max Suppression`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建传递给方法的置信度数组的副本开始；我们将使用它来获取一个排序索引数组，以及标记任何被前一个框充分重叠的框。这是通过简单地将其置信度值设置为0来完成的。添加以下语句来完成这项工作，同时创建排序索引数组，替换注释`//
    TODO implement Non-Max Suppression`：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As mentioned previously, we start by cloning the confidence array, assigning
    it to the variable `detectionConfidence`. Then, we sort the indices in descending
    order and, finally, create an array to store the boxes we want to keep and return.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先克隆置信度数组，将其分配给变量`detectionConfidence`。然后，我们按降序排序索引，最后创建一个数组来存储我们想要保留和返回的框。
- en: 'Next, we will create the loops that embody the bulk of the algorithm, including
    picking the next box with the highest confidence and storing it in our `bestObjectsBounds`
    array. Add the following code, replacing the comment `// TODO iterate through
    each box`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建包含算法大部分内容的循环，包括选择置信度最高的下一个框并将其存储在我们的`bestObjectsBounds`数组中。添加以下代码，替换注释`//
    TODO iterate through each box`：
- en: '[PRE29]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Most of the code should be self-explanatory; what's worth noting is that within
    each loop, we test that the associated boxes confidence is greater than 0\. As
    mentioned before, we use this to indicate that an object has been discarded due
    to being sufficiently overlapped by a box with higher confidence.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码应该是自解释的；值得注意的一点是，在每一个循环中，我们测试相关框的置信度是否大于0。如前所述，我们使用这个方法来表示一个对象由于被置信度更高的框充分重叠而被丢弃。
- en: 'What remains is calculating the IoU between `objectBounds` and `otherObjectBounds`,
    and invaliding `otherObjectBounds` if it doesn''t meet our IoU threshold, `nmsThreshold`.
    Replace the comment `// TODO calculate IoU and compare against our threshold`,
    with this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作是计算`objectBounds`和`otherObjectBounds`之间的IoU，如果`otherObjectBounds`不满足我们的IoU阈值`nmsThreshold`，则使其无效。用以下代码替换注释`//
    TODO calculate IoU and compare against our threshold`：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, we are using a `CGRect` extension method, `computeIOU`, to handle the
    calculation. Let''s have a peek at this, implemented in the file `CGRect+Extension.swift`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用一个`CGRect`扩展方法`computeIOU`来处理计算。让我们看看这个在`CGRect+Extension.swift`文件中实现的代码：
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Thanks to the existing `intersection` and `union` of the `CGRect` structure,
    this method is nice and concise.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`CGRect`结构体中已经存在`intersection`和`union`，这个方法既简洁又方便。
- en: One final thing to do before we finish with the `YOLOFacade` class as well as
    the YOLO algorithm is to return the results. At the bottom of the `filterDetectedObjects`
    method, return the array `bestObjectsBounds`; with that done, we can now turn
    our attention to the last piece of functionality before implementing our intelligent
    search photo application.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成`YOLOFacade`类以及YOLO算法之前，还有一件最后的事情要做，那就是返回结果。在`filterDetectedObjects`方法的底部，返回数组`bestObjectsBounds`；完成这个步骤后，我们现在可以将注意力转向实现智能搜索照片应用程序之前的功能的最后部分。
- en: This chapter does a good job highlighting that most of the effort integrating
    ML into your applications surrounds the **preprocessing** of the data before feeding
    it into the model and **interpreting** the output of the model. The **Vision framework**
    does a good job alleviating the preprocessing tasks, but there is still significant
    effort handling the output. Fortunately, no doubt because object detection is
    compelling for many applications, Apple has added a new observation type explicitly
    for object detection called **VNRecognizedObjectObservation**. Although we don't
    cover it here; I encourage you to review the official documentation [https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation](https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本章很好地强调了将机器学习集成到您的应用程序中的大部分工作都围绕着在将数据输入模型之前对数据进行**预处理**以及在模型输出上进行**解释**。**Vision框架**很好地减轻了预处理任务，但处理输出仍然需要大量的工作。幸运的是，无疑是因为对象检测对许多应用程序来说非常有吸引力，苹果明确添加了一个新的观察类型，用于对象检测，称为**VNRecognizedObjectObservation**。尽管我们在这里没有涉及；我鼓励您查阅官方文档[https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation](https://developer.apple.com/documentation/vision/vnrecognizedobjectobservation)。
- en: 'The next piece of functionality is concerned with evaluating a cost on each
    of the returned detected objects with respect to the user''s search criteria;
    by this, I mean filtering and sorting the photos so that the results are relevant
    to what the user sought. As a reminder, the search criteria is defined by an array
    of `ObjectBounds`, collectively describing the objects the user wants within a
    image, their relative positions, as well as the sizes relative to each other and
    to the image itself. The following figure shows how the user defines their search
    within our application:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个功能部分是评估每个返回的检测对象相对于用户搜索标准的花费；我的意思是过滤和排序照片，以便结果与用户所寻求的内容相关。作为提醒，搜索标准由一个`ObjectBounds`数组定义，共同描述用户在图像中想要的对象、它们之间的相对位置，以及相对于彼此和图像本身的尺寸。以下图显示了用户如何在我们的应用程序中定义他们的搜索：
- en: '![](img/334ca2eb-c83f-4a10-8aa7-a224e2a3e50e.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/334ca2eb-c83f-4a10-8aa7-a224e2a3e50e.png)'
- en: Here, we will implement only two of the four evaluations, but it should provide
    a sufficient base for you to implement the remaining two yourself.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只实现四个评估中的两个，但这应该为你自己实现剩下的两个提供了一个足够的基础。
- en: 'The cost evaluation is performed within the `PhotoSearcher` class once the
    `YOLOFacade` has returned the detected objects for all of the images. This code
    resides in the `asyncSearch` method (within the `PhotoSearcher.swift` file), highlighted
    in the following code snippet:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在`YOLOFacade`返回所有图像的检测对象后，在`PhotoSearcher`类中执行成本评估。此代码位于`asyncSearch`方法（在`PhotoSearcher.swift`文件中），如下代码片段所示：
- en: '[PRE32]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`calculateCostForObjects` takes in the search criteria and results from the
    `YOLOFacade` and returns an array of `SearchResults` from the `detectObjects`
    with their cost properties set, after which they are filtered and sorted before
    being returned to the delegate.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculateCostForObjects`方法接收来自`YOLOFacade`的搜索条件和结果，并返回一个`SearchResults`数组，其中包含从`detectObjects`返回的具有成本属性的搜索结果，之后它们会被过滤和排序，然后返回给代理。'
- en: 'Let''s jump into the `calculateCostForObjects` method and discuss what we mean
    by cost; the code of the method `calculateCostForObjects` is as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入到`calculateCostForObjects`方法，并讨论一下我们所说的成本；`calculateCostForObjects`方法的代码如下：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: A `SearchResult` incurs a cost each time it differs from the user's search criteria,
    meaning that the results with the least cost are those that better match the search
    criteria. We perform cost evaluation on four different heuristics; each method
    will be responsible for adding the calculated cost to each result. Here we will
    only implement `costForObjectPresences` and `costForObjectRelativePositioning`,
    leaving the remaining two as an exercise for your.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`SearchResult`每次与用户的搜索条件不同时都会产生成本，这意味着成本最低的结果是与搜索条件匹配得更好的结果。我们对四个不同的启发式方法执行成本评估；每种方法将负责将计算出的成本添加到每个结果中。在这里，我们只实现`costForObjectPresences`和`costForObjectRelativePositioning`，剩下的两个将作为你的练习。'
- en: 'Let''s jump straight in and start implementing the `costForObjectPresences`
    method; at the moment, it''s nothing more than a stub, as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入并开始实现`costForObjectPresences`方法；目前，它不过是一个占位符，如下所示：
- en: '[PRE34]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Before writing the code, let's quickly discuss what we are evaluating for. Maybe,
    a better name for this function would have been `costForDifference` as we not
    only want to assess that the image has objects declared in the search criteria,
    but also we equally want to increase the cost for additional objects. That is,
    if the user searches for just two dogs but a photo has three dogs or two dogs
    and a cat, we want to increase the cost for these additional objects such that
    we are favoring the one that is most similar to the search criteria (just two
    dogs).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写代码之前，让我们快速讨论一下我们要评估的内容。也许，这个函数更好的名字应该是`costForDifference`，因为我们不仅想要评估图像是否在搜索条件中声明了对象，而且我们也同样希望增加额外对象的成本。也就是说，如果用户只搜索了两只狗，但照片上有三只狗或两只狗和一只猫，我们希望增加这些额外对象的成本，以便我们更倾向于与搜索条件（仅两只狗）最相似的对象。
- en: 'To calculate this, we simply need to find the absolute difference between the
    two arrays; to do this, we first create a dictionary of counts for all classes
    in both `detectedObject` and `searchCriteria`. The directory''s key will be the
    object''s label and the corresponding value will be the count of objects within
    the array. The following figure illustrates these arrays and formula used to calculate:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们只需要找到两个数组之间的绝对差异；为了做到这一点，我们首先为`detectedObject`和`searchCriteria`中的所有类别创建一个计数字典。字典的键将是对象的标签，相应的值将是数组中对象的计数。以下图示了这些数组和用于计算的公式：
- en: '![](img/a4ce80a3-a157-4be3-b033-2c3e76108094.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4ce80a3-a157-4be3-b033-2c3e76108094.png)'
- en: 'Let''s now implement it; add the following code to do this, replacing the comment
    `// TODO implement cost function for object presence`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来实现它；添加以下代码来完成这个任务，替换掉注释`// TODO implement cost function for object presence`：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, with our count dictionaries created and populated, it''s simply a matter
    of iterating over all available classes (using the items in `DetectableObject.objects`)
    and calculating the cost based on the absolute difference between the two. Add
    the following code, which does this, by replacing the comment `// TODO accumulate
    cost based on the difference`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们的计数字典创建并填充，我们只需遍历所有可用的类别（使用`DetectableObject.objects`中的项）并根据两个数组之间的绝对差异来计算成本。添加以下代码，它执行此操作，通过替换注释`//
    TODO accumulate cost based on the difference`：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The result of this is a cost that is larger for images that differ the most
    from the search criteria; the last thing worth noting is that the cost is multiplied
    by a weight before being returned (function parameter). Each evaluation method
    has a weight parameter which allows for easy tuning (during either design time
    or runtime) of the search, giving preference to one evaluation over another.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这的结果是对于与搜索条件差异最大的图像，成本更大；最后要指出的是，成本在返回之前乘以一个权重（函数参数）。每个评估方法都有一个权重参数，允许在设计和运行时轻松调整搜索，优先考虑一种评估方法而不是另一种。
- en: 'The next, and last, cost evaluation function we are going to implement is the
    method `costForObjectRelativePositioning`; the stub of this method is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要实现的是下一个，也是最后一个成本评估函数，即`costForObjectRelativePositioning`方法；此方法的存根如下：
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we did before, let's quickly discuss the motivation behind this evaluation
    and how we plan to implement it. This method is used to favor items that match
    the composition of the user's search; this allows our search to surface images
    that closely resemble the arrangement the user is searching for. For example,
    the user may be looking for an image or images where two dogs are sitting next
    to each other, side by side, or they may want an image with two dogs sitting next
    to each other on a sofa.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，让我们快速讨论一下这个评估背后的动机以及我们计划如何实现它。此方法用于优先考虑与用户搜索组成相匹配的项目；这允许我们的搜索显示与用户搜索的排列非常相似的图像。例如，用户可能正在寻找两张狗并排坐着或并排的图像，或者他们可能想要一张两张狗并排坐在沙发上的图像。
- en: 'There are no doubt many approaches you could take for this, and it''s perhaps
    a use case for a neural network, but the approach taken here is the simplest I
    could think of to avoid having to explain complicated code; the algorithm used
    is described as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，无疑有许多你可以采取的方法，这可能是一个神经网络用例，但这里采取的方法是我能想到的最简单的方法，以避免不得不解释复杂的代码；所使用的算法描述如下：
- en: For each object (`a`) of type `ObjectBounds` within `searchCriteria`
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`searchCriteria`中每个类型为`ObjectBounds`的对象（`a`）
- en: Find the closest object (`b`) in proximity (still within `searchCriteria`)
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在邻近度（仍在`searchCriteria`内）中找到最近的对象（`b`）
- en: Create a normalized direction vector from `a` to `b`
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`a`到`b`创建一个归一化方向向量
- en: Find the matching object `a'` (the same class) within the `detectedObject`
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`detectedObject`中找到匹配的对象`a'`（相同的类别）
- en: Search all other objects (`b'`) in `detectedObject` that have the same class
    as `b`
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`detectedObject`中搜索所有其他具有与`b`相同类别的对象（`b'`）
- en: Create a normalized direction vector from `a'` to `b'`
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`a'`到`b'`创建一个归一化方向向量
- en: Calculate the dot product between the two vectors (angle); in this case, our
    vectors are `a->b` and `a'->b'`
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两个向量（角度）的点积；在这种情况下，我们的向量是`a->b`和`a'->b'`
- en: Using `a'` and `b'`, which have the lowest dot product, increment the cost by
    how much the angle differs from the search criteria and images
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用具有最低点积的`a'`和`b'`，根据角度与搜索条件的差异来增加成本
- en: Essentially, what we are doing is finding two matching pairs from the `searchCriteria`
    and `detectedObject` arrays, and calculating the cost based on the difference
    in the angles.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们所做的是从`searchCriteria`和`detectedObject`数组中找到两个匹配的对，并基于角度差异来计算成本。
- en: A direction vector of two objects is calculated by subtracting one's position
    from the other and then normalizing it. The dot product can then be used on two
    (normalized) vectors to find their angle, where *1.0* would be returned if the
    vectors are pointing in the same direction, *0.0* if they are perpendicular, and
    *-1.0* if pointing in opposite directions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 两个对象的方向向量是通过从一个对象的位置减去另一个对象并归一化它来计算的。然后可以在两个（归一化）向量上使用点积来找到它们的角度，其中如果向量指向同一方向，则返回*1.0*；如果它们垂直，则返回*0.0*；如果指向相反方向，则返回*-1.0*。
- en: 'The following figure presents part of this process; we first find an object
    pair in close proximity within the search criteria. After calculating the dot
    product, we iterate over all the objects detected in the image and find the most
    suitable pair; "suitable" here means the same object type and the closest angle
    to the search criteria within the possible matching pairs:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这一过程的一部分；我们首先在搜索条件中找到一对接近的对象。在计算点积后，我们遍历图像中检测到的所有对象，并找到最合适的对；“合适”在这里意味着相同的对象类型和可能的匹配对中与搜索条件最接近的角度：
- en: '![](img/b6556913-07e2-45cd-90be-1fe8e929762b.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b6556913-07e2-45cd-90be-1fe8e929762b.png)'
- en: 'Once comparable pairs are found, we calculate the cost based on the difference
    in angle, as we will soon see. But we are getting a little ahead of ourselves;
    we first need a way to find the closest object. Let''s do this using a nested
    function we can call within our `costForObjectRelativePositioning` method. Add
    the following code, replacing the comment `// TODO implement cost function for
    relative positioning`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到可比较的成对对象，我们将根据角度差异计算成本，正如我们很快将看到的。但我们在自己之前走得太远了；我们首先需要一种找到最近对象的方法。让我们使用一个嵌套函数来实现，我们可以在`costForObjectRelativePositioning`方法中调用它。添加以下代码，替换掉注释`//
    TODO implement cost function for relative positioning`：
- en: '[PRE38]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding function will be used to find the closest object, given an array
    of `ObjectBounds` and index of the object we are searching against. From there,
    it simply iterates over all of the items in the array, returning the one that
    is, well, closest.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的功能将用于在给定`ObjectBounds`数组和我们要搜索的对象索引的情况下找到最近的对象。从那里，它只是遍历数组中的所有项目，返回那个，嗯，最近的对象。
- en: 'With our helper function now implemented, let''s create the loop that will
    inspect the search item pair from the user''s search criteria. Append the following
    code to the `costForObjectRelativePositioning` method, replacing the comment `//
    TODO Iterate over all items in the searchCriteria array`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了辅助函数，让我们创建一个循环来检查用户搜索标准中的搜索项对。将以下代码添加到`costForObjectRelativePositioning`方法中，替换掉注释`//
    TODO Iterate over all items in the searchCriteria array`：
- en: '[PRE39]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We start by searching for the closest object to the current object, jumping
    to the next item if nothing is found. Once we have our search pair, we proceed
    to calculate the direction by subtracting the first bound's center from its pair
    and normalizing the result.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先搜索当前对象最近的对象，如果没有找到，则跳到下一个项目。一旦我们有了搜索对，我们就通过从第一个边界的中点减去其配对并归一化结果来计算方向。
- en: 'We now need to find all objects of both classes, whereby we will proceed to
    evaluate each of them to find the best match. Before that, let''s get all the
    classes with the index of `searchAClassIndex` and `searchBClassIndex`; add the
    following code, replacing the comment `// TODO Find matching pair`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要找到两个类中的所有对象，我们将逐一评估它们以找到最佳匹配。在此之前，让我们先获取`searchAClassIndex`和`searchBClassIndex`索引的所有类；添加以下代码，替换掉注释`//
    TODO Find matching pair`：
- en: '[PRE40]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If we are unable to find a matching pair, we continue to the next item, knowing
    that a cost has already been added for the mismatch in objects of both arrays.
    Next, we iterate over all pairs. For each pair, we calculate the normalized direction
    vector and then the dot product against our `searchDirection` vector, taking the
    one that has the closest dot product (closest in angle). Add the following code
    in place of the comment `// TODO Search for the most suitable pair`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们无法找到匹配的成对对象，我们将继续到下一个项目，知道已经为两个数组中对象的不匹配添加了成本。接下来，我们遍历所有成对对象。对于每一对，我们计算归一化方向向量，然后与我们的`searchDirection`向量计算点积，取点积最接近的那个（角度最接近）。将以下代码替换掉注释`//
    TODO Search for the most suitable pair`：
- en: '[PRE41]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Similar to what we did with our search pair, we calculate the direction vector
    by subtracting the pair's center positions and then normalize the result. Then,
    with the two vectors `searchDirection` and `detectedDirection`, we calculate the
    dot product, keeping reference to it if it is the first or lowest dot product
    so far.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们处理搜索对的方式类似，我们通过减去成对的中心位置来计算方向向量，然后对结果进行归一化。然后，使用两个向量`searchDirection`和`detectedDirection`，我们计算点积，如果它是第一个或迄今为止点积最低的，则保留引用。
- en: 'There is just one last thing we need to do for this method, and this project.
    But before doing so, let''s take a little detour and look at a couple of extensions
    made to `CGPoint`, specifically the `dot` and `normalize` used previously. You
    can find these extensions in the `CGPoint+Extension.swift` file. As I did previously,
    I will list the code for reference rather than describing the details, most of
    which we have already touched upon:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个方法和这个项目，我们还需要做最后一件事。但在这样做之前，让我们稍微偏离一下，看看对`CGPoint`所做的几个扩展，特别是之前使用的`dot`和`normalize`。这些扩展可以在`CGPoint+Extension.swift`文件中找到。像之前一样，我将列出代码以供参考，而不是描述细节，其中大部分我们已经讨论过了：
- en: '[PRE42]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, back to the `costForObjectRelativePositioning` method to finish our method
    and project. Our final task is to add to the cost; this is done simply by subtracting
    the stored `closestDotProduct` from `1.0` (remembering that we want to increase
    the cost for larger differences where the dot product of two normalized vectors
    pointing in the same direction is `1.0`) and ensuring that the value is positive
    by wrapping it in an `abs` function. Let''s do that now; add the following code,
    replacing the comment `// TODO add cost`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到`costForObjectRelativePositioning`方法，完成我们的方法和项目。我们的最终任务是添加到成本中；这很简单，只需从`1.0`中减去存储的`closestDotProduct`（记住，我们希望增加较大差异的成本，其中两个归一化向量指向同一方向时点积为`1.0`），并通过将其包装在`abs`函数中来确保值是正的。现在让我们来做这件事；添加以下代码，替换注释`//
    TODO add cost`：
- en: '[PRE43]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With that done, we have finished this method, and the coding for this chapter.
    Well done! It''s time to test it out; build and run the project to see your hard
    work in action. Shown here are a few searches and their results:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些工作后，我们已经完成了这个方法，以及本章的编码。做得好！现在是时候测试一下了；构建并运行项目，看看你的辛勤工作是如何发挥作用的。下面展示了一些搜索及其结果：
- en: '![](img/02d7b7ca-c6d5-44b5-86b0-a4b7242dc890.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02d7b7ca-c6d5-44b5-86b0-a4b7242dc890.png)'
- en: Although the YOLO algorithm is performant and feasible for near real-time use,
    our example is far from optimized and unlikely to perform well on large sets of
    photos. With the release of Core ML 2, Apple provides one avenue we can use to
    make our process more efficient. This will be the topic of the next section before
    wrapping up.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管YOLO算法性能良好，适用于近实时使用，但我们的示例远未优化，不太可能在大量照片集上表现良好。随着Core ML 2的发布，Apple提供了一条途径，我们可以用它来使我们的过程更高效。这将是下一节的主题，在总结之前。
- en: Optimizing with batches
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量优化
- en: At the moment, our process involves iterating over each photo and performing
    inference on each one individually. With the release of Core ML 2, we now have
    the option to create a batch and pass this batch to our model for inference. As
    with efficiencies gained with economies of scale, here, we also gain significant
    improvements; so let's walk through adapting our project to process our photos
    in a single batch rather than individually.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的过程涉及遍历每一张照片并对每一张单独进行推理。随着Core ML 2的发布，我们现在可以选择创建一个批处理并将这个批处理传递给我们的模型进行推理。就像规模经济带来的效率一样，在这里，我们也获得了显著的改进；所以让我们通过将我们的项目调整为以单个批处理而不是单个处理我们的照片来适应我们的项目。
- en: 'Let''s work our way up the stack, starting in our `YOLOFacade` class and moving
    up to the `PhotoSearcher`. For this we will be using our model directly rather
    than proxying through Vision, so our first task is to replace the `model` property
    of our `YOLOFacade` class with the following declaration:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从栈的底部开始工作，从我们的`YOLOFacade`类开始，然后移动到`PhotoSearcher`。为此，我们将直接使用我们的模型而不是通过Vision代理，所以我们的第一个任务是替换`YOLOFacade`类的`model`属性，如下所示：
- en: '[PRE44]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s rewrite the `detectObjects` method to handle an array of photos
    rather than a single instance; because this is where most of the changes reside,
    we will start from scratch. So, go ahead and delete the method from your `YOLOFacade`
    class and replace it with the following stub:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重写`detectObjects`方法，以处理照片数组而不是单个实例；因为大多数更改都集中在这里，我们将从头开始。所以，请从你的`YOLOFacade`类中删除该方法，并用以下存根替换它：
- en: '[PRE45]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: I have made the changes to the signature of the method boldand listed our remaining
    tasks. The first is to create an array of `MLFeatureProvider`. If you recall from
    [Chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml), *Recognizing Objects
    in the World*, when we import a Core ML model into Xcode, it generates interfaces
    for the model, its input and output. The input and output are subclasses of `MLFeatureProvider`,
    so here we want to create an array of `tinyyolo_voc2007Input`, which can be instantiated
    with instances of `CVPixelBuffer`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经对方法的签名进行了修改，并列举了我们的剩余任务。第一个任务是创建一个`MLFeatureProvider`数组。如果你还记得[第3章](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml)中的内容，*识别世界中的物体*，当我们将Core
    ML模型导入Xcode时，它会为模型、输入和输出生成接口。输入和输出是`MLFeatureProvider`的子类，因此在这里我们想要创建一个`tinyyolo_voc2007Input`数组，它可以由`CVPixelBuffer`的实例化对象来实例化。
- en: 'To create this, we will transform the array of photos passed into the method,
    including the required preprocessing steps (resizing to 416 x 416). Replace the
    comment `// TODO batch items (array of MLFeatureProvider)` with the following
    code:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建这个，我们将转换传递给方法的照片数组，包括所需的预处理步骤（调整大小为416 x 416）。将注释`// TODO batch items (array
    of MLFeatureProvider)`替换为以下代码：
- en: '[PRE46]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: For reasons of simplicity and readability, we are omitting any kind of error
    handling; obviously, in production you want to handle exceptions appropriately.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单和可读性，我们省略了任何类型的错误处理；显然，在生产环境中，您需要适当地处理异常。
- en: 'To perform inference on a batch, we need to have our input conform to the `MLBatchProvider`
    interface. Fortunately, Core ML provides a concrete implementation that conveniently
    wraps array. Let''s do this now; replace the comment `// TODO Wrap our items in
    an instance of MLArrayBatchProvider` with the following code:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要对一个批次进行推理，我们需要确保我们的输入符合`MLBatchProvider`接口。幸运的是，Core ML提供了一个具体的实现，它方便地封装了数组。现在让我们来做这件事；将注释`//
    TODO Wrap our items in an instance of MLArrayBatchProvider`替换为以下代码：
- en: '[PRE47]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To perform inference, it''s simply a matter of calling the `predictions` method
    on our model; as usual, replace the comment `// TODO Perform inference on the
    batch` with the following code:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行推理，只需在我们的模型上调用`predictions`方法即可；像往常一样，将注释`// TODO Perform inference on the
    batch`替换为以下代码：
- en: '[PRE48]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'What we get back is an instance of `MLBatchProvider` (if successful); this
    is more or less a collection of results for each of our samples (inputs). We can
    access an specific result via the batch providers `features(at: Int)` method,
    which returns an instance of `MLFeatureProvider` (in our case, an in `tinyyolo_voc2007Output`).'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '我们得到的是一个`MLBatchProvider`的实例（如果成功）；这基本上是我们每个样本（输入）的结果集合。我们可以通过批提供者的`features(at:
    Int)`方法访问特定的结果，该方法返回一个`MLFeatureProvider`的实例（在我们的案例中，是`tinyyolo_voc2007Output`）。'
- en: 'Here we simply process each result as we had done before to obtain the most
    salient; replace the comment `// TODO (As we did before) Process the outputs of
    the model` with the following code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简单地像之前一样处理每个结果，以获得最显著的结果；将注释`// TODO (As we did before) Process the outputs
    of the model`替换为以下代码：
- en: '[PRE49]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The only difference here, than before, is that we are iterating over a batch
    of outputs rather than a single one. The last thing we need to do is call the
    handler; replace the comment `// TODO Return results via the callback handler`
    with the following statement:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前相比，唯一的区别是我们现在正在迭代一个输出批次而不是单个输出。我们需要做的最后一件事是调用处理器；将注释`// TODO Return results
    via the callback handler`替换为以下语句：
- en: '[PRE50]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This now completes the changes required to our `YOLOFacade` class; let's jump
    into the `PhotoSearcher` and make the necessary, and final, changes.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经完成了对`YOLOFacade`类所需的所有更改；让我们跳转到`PhotoSearcher`并做出必要的、最后的更改。
- en: 'The big change here is that we now need to pass in all photos at once rather
    than passing each one individually. Locate the `detectObjects` method and replace
    its body with the following code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重大变化是我们现在需要一次性传递所有照片，而不是逐个传递。定位`detectObjects`方法，并用以下代码替换其主体：
- en: '[PRE51]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Same code but organised a little differently to handle batch inputs and output
    from and to the `YOLOFacade` class. Now is a good time to build, deploy and run
    the application; paying particular attention to efficiencies gained from adapting
    batch inference. When you return; we will conclude this chapter with a quick summary.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 代码相同，但组织得略有不同，以处理从`YOLOFacade`类输入和输出的批次。现在是构建、部署和运行应用程序的好时机；特别注意从适应批量推理中获得的高效性。当你回来时，我们将用简要的总结结束本章。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we introduced the concept of object detection, comparing it
    with object recognition and object localization. While the other two are limited
    to a single dominant object, object detection allows multi-object classification,
    including predicting their bounding boxes. We then spent some time introducing
    one particular algorithm, YOLO, before getting acquainted with Apple's Core ML
    Tools Python package, walking through converting a trained Keras model to Core
    ML. Once we had the model in hand, we moved on to implementing YOLO in Swift with
    the goal of creating an intelligent search application.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了目标检测的概念，将其与目标识别和目标定位进行了比较。虽然其他两个概念仅限于单个主要目标，但目标检测允许多目标分类，包括预测它们的边界框。然后我们花了一些时间介绍了一个特定的算法YOLO，在熟悉Apple的Core
    ML Tools Python包之后，我们逐步将训练好的Keras模型转换为Core ML。一旦我们有了模型，我们就继续用Swift实现YOLO，目标是创建一个智能搜索应用程序。
- en: Despite this being a fairly lengthy chapter, I hope you found it valuable and
    gained deeper intuition into how deep neural networks learn and understand images
    and how they can be applied in novel ways to create new experiences. It's helpful
    to remind ourselves that using the same architecture, we can create devise new
    applications by simply swapping the data we train it on. For example, you could
    train this model on a dataset of hands and their corresponding bounding boxes
    to create a more immersive **augmented reality** (**AR**) experience by allowing
    the user to interact with digital content through touch.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一章相当长的内容，但我希望您觉得它很有价值，并且对深度神经网络如何学习和理解图像以及它们如何以新颖的方式应用于创造新体验有了更深的直觉。提醒自己，使用相同的架构，我们只需更换训练数据，就可以创建新的应用。例如，您可以在手及其对应边界框的数据集上训练这个模型，通过允许用户通过触摸与数字内容互动，从而创建一个更加沉浸式的**增强现实**（AR）体验。
- en: But for now, let's continue our journey of understanding Core ML and explore
    how else we can apply it. In the next chapter, you will see how the popular Prisma
    creates those stunning photos with style transform.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，让我们继续我们的Core ML理解之旅，探索我们还可以如何应用它。在下一章中，您将看到流行的Prisma如何通过风格转换创建那些令人惊叹的照片。
