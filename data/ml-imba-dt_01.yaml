- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to Data Imbalance in Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中数据不平衡的介绍
- en: Machine learning algorithms have helped solve real-world problems as diverse
    as disease prediction and online shopping. However, many problems we would like
    to address with machine learning involve imbalanced datasets. In this chapter,
    we will discuss and define imbalanced datasets, explaining how they differ from
    other types of datasets. The ubiquity of imbalanced data will be demonstrated
    with examples of common problems and scenarios. We will also go through the basics
    of machine learning and cover the essentials, such as loss functions, regularization,
    and feature engineering. We will also learn about common evaluation metrics, particularly
    those that can be very helpful for imbalanced datasets. We will then introduce
    the `imbalanced-learn` library.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法已经帮助解决了各种现实世界问题，从疾病预测到在线购物。然而，我们希望用机器学习解决的问题中，许多都涉及不平衡数据集。在本章中，我们将讨论并定义不平衡数据集，解释它们与其他类型数据集的不同之处。我们将通过常见问题和场景的例子来展示不平衡数据的普遍性。我们还将学习机器学习的基础知识，包括损失函数、正则化和特征工程等基本概念。我们还将了解常见的评估指标，特别是那些对于不平衡数据集非常有帮助的指标。然后我们将介绍`imbalanced-learn`库。
- en: 'In particular, we will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其，我们将学习以下主题：
- en: Introduction to imbalanced datasets
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平衡数据集的介绍
- en: Machine learning 101
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习101
- en: Types of datasets and splits
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的类型和拆分
- en: Common evaluation metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的评估指标
- en: Challenges and considerations when dealing with imbalanced data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据时的挑战和考虑因素
- en: When can we have an imbalance in datasets?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们何时会在数据集中出现不平衡？
- en: Why can imbalanced data be a challenge?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么不平衡数据会成为一个挑战？
- en: When to not worry about data imbalance
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时不必担心数据不平衡
- en: Introduction to the `imbalanced-learn` library
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`库的介绍'
- en: General rules to follow
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要遵循的一般规则
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will utilize common libraries such as `numpy` and `scikit-learn`
    and introduce the `imbalanced-learn` library. The code and notebooks for this
    chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01).
    You can fire up the GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of this chapter’s notebook or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用常见的库，如`numpy`和`scikit-learn`，并介绍`imbalanced-learn`库。本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter01)。您可以通过点击本章笔记本顶部的**Open
    in Colab**图标或通过使用笔记本的GitHub URL在[https://colab.research.google.com](https://colab.research.google.com)启动来使用Google
    Colab打开GitHub笔记本。
- en: Introduction to imbalanced datasets
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡数据集的介绍
- en: Machine learning algorithms learn from collections of examples that we call
    **datasets**. These datasets contain multiple data samples or points, which we
    may refer to as examples, samples, or instances interchangeably throughout this
    book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法从我们所称的**数据集**的集合中学习。这些数据集包含多个数据样本或点，我们可以在整本书中将这些样本、样本或实例互换使用。
- en: 'A dataset can be said to have a balanced distribution when all the target classes
    have a similar number of examples, as shown in *Figure 1**.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '当所有目标类别具有相似数量的示例时，可以说数据集具有平衡分布，如图*图1**.1*所示： '
- en: '![](img/B17259_01_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_01.jpg)'
- en: Figure 1.1 – Balanced distribution with an almost equal number of examples for
    each class
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 类别间示例数量几乎相等的平衡分布
- en: 'Imbalanced datasets or skewed datasets are those that have some target classes
    (also called labels) that outnumber the rest of the classes (*Figure 1**.2*).
    Though this generally applies to classification problems (for example, fraud detection)
    in machine learning, they inevitably occur in regression problems (for example,
    house price prediction) too:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集或倾斜数据集是指某些目标类别（也称为标签）的数量超过其他类别的数据集（*图1**.2*）。尽管这通常适用于机器学习中的分类问题（例如，欺诈检测），但在回归问题（例如，房价预测）中也会不可避免地发生：
- en: '![](img/B17259_01_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_02.jpg)'
- en: Figure 1.2 – An imbalanced dataset with five classes and a varying number of
    samples
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 具有五个类别和不同样本数量的不平衡数据集
- en: 'We label the class with more instances as the “majority” or “negative” class
    and the one with fewer instances as the “minority” or “positive” class. Most of
    the time, our main interest lies in the minority class, which is why we often
    refer to the minority class as the “positive” class and to the majority class
    as the “negative” class:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将具有更多实例的类别标记为“多数”或“负”类，将具有较少实例的类别标记为“少数”或“正”类。大多数时候，我们的主要兴趣在于少数类，这就是为什么我们经常将少数类称为“正”类，将多数类称为“负”类：
- en: '![](img/B17259_01_03.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_01_03.jpg)'
- en: Figure 1.3 – A visual guide to common terminology used in imbalanced classification
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 不平衡分类中常用术语的视觉指南
- en: This can be scaled to more than two classes, and such classification problems
    are called multi-class classification. In the first half of this book, we will
    focus our attention only on binary class classification to keep the material easier
    to grasp. It’s relatively easy to extend the concepts to multi-class classification.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以扩展到超过两个类别，这样的分类问题称为多类分类。在这本书的前半部分，我们将只关注二元分类，以使材料更容易理解。将概念扩展到多类分类相对容易。
- en: 'Let’s look at a few examples of imbalanced datasets:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个不平衡数据集的例子：
- en: '**Fraud detection** is where fraudulent transactions need to be detected out
    of several transactions. This problem is often encountered and widely used in
    finance, healthcare, and e-commerce industries.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测**是指从多笔交易中检测出欺诈交易。这个问题在金融、医疗和电子商务行业中经常遇到并被广泛使用。'
- en: '**Network intrusion detection** using machine learning involves analyzing large
    volumes of network traffic data to detect and prevent instances of unauthorized
    access and misuse of computer systems.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习的**网络入侵检测**涉及分析大量网络流量数据，以检测和防止未经授权的访问和计算机系统的滥用。
- en: '**Cancer detection**. Cancer is not rare, but we still may want to use machine
    learning to analyze medical data to identify potential cases of cancer earlier
    and improve treatment outcomes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**癌症检测**。癌症并不罕见，但我们仍然可能想使用机器学习来分析医疗数据，以更早地识别潜在的癌症病例并改善治疗效果。'
- en: In this book, we would like to focus on the class imbalance problem in general
    and look at various solutions where we see that class imbalance is affecting the
    performance of our model. A typical problem is that models perform quite poorly
    on the minority classes for which the model has seen a very low number of examples
    during model training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们希望专注于一般类别不平衡问题，并查看各种解决方案，在这些解决方案中，我们看到类别不平衡正在影响我们模型的性能。一个典型的问题是在模型训练期间模型看到的少数类示例非常少，模型在这些少数类上的表现相当差。
- en: Machine learning 101
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习101
- en: 'Let’s do a quick overview of machine learning and its related fields:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速概述一下机器学习及其相关领域：
- en: '**Artificial intelligence** is the superset of all intelligence-related problems.
    Classical machine learning encompasses problems that can be solved by training
    traditional classical models (such as decision trees or logistic regression) and
    predicting the target values. They typically work on tabular data, require extensive
    feature engineering (manual development of features), and are less effective on
    text and image data. Deep learning tends to do far better on image, text, speech,
    and video data, wherein, typically, no manual feature engineering is needed, and
    various layers in the neural network automatically do feature engineering for
    us.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能**是所有与智能相关问题的超集。经典机器学习包括可以通过训练传统经典模型（如决策树或逻辑回归）并预测目标值来解决的问题。它们通常在表格数据上工作，需要大量的特征工程（手动开发特征），并且在文本和图像数据上的效果较差。深度学习在图像、文本、语音和视频数据上往往表现得更好，在这些情况下，通常不需要手动特征工程，神经网络中的各个层自动为我们进行特征工程。'
- en: In **supervised learning**, we have both inputs and outputs (labels) in the
    dataset, and the model learns to predict the output during the training. Each
    input can be represented as a list of features. The output or labels can be a
    finite set of classes (classification), a real number (regression), or something
    more complex. A classic example of supervised learning in classification is the
    Iris flowers classification. In this case, the dataset includes features such
    as petal length, petal width, sepal length, and sepal width, and the labels are
    the species of the Iris flowers (setosa, versicolor, or virginica). A model can
    be trained on this dataset and then be used to classify new, unseen Iris flowers
    as one of these species.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**监督学习**中，数据集既有输入也有输出（标签），模型在训练期间学习预测输出。每个输入可以表示为一个特征列表。输出或标签可以是有限类别的集合（分类）、实数（回归）或更复杂的东西。分类中的监督学习的一个经典例子是鸢尾花分类。在这种情况下，数据集包括花瓣长度、花瓣宽度、萼片长度和萼片宽度等特征，标签是鸢尾花的种类（setosa、versicolor或virginica）。可以在该数据集上训练一个模型，然后使用该模型将新的、未见过的鸢尾花分类为这些种类之一。
- en: In **unsupervised learning**, models either don’t have access to the labels
    or don’t use the labels and then try to make some predictions – for example, clustering
    the examples in the dataset into different groups.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**无监督学习**中，模型要么无法访问标签，要么不使用标签，然后尝试做出一些预测——例如，将数据集中的示例聚类到不同的组中。
- en: In **reinforcement learning**, the model tries to learn by making mistakes and
    optimizing a goal or profit variable. An example would be training a model to
    play chess and adjusting its strategy based on feedback received through rewards
    and penalties.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**强化学习**中，模型通过犯错误和优化目标或利润变量来尝试学习。一个例子是训练一个模型来下棋，并根据通过奖励和惩罚获得的反馈调整其策略。
- en: 'In supervised learning (which is the focus of this book), there are two main
    types of problems: classification and regression. Classification problems involve
    categorizing data into predefined classes or labels, such as “fraud” or “non-fraud”
    and “spam” or “non-spam.” On the other hand, regression problems aim to predict
    a continuous variable, such as the price of a house.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中（本书的重点），主要有两种类型的问题：分类和回归。分类问题涉及将数据分类到预定义的类别或标签中，例如“欺诈”或“非欺诈”以及“垃圾邮件”或“非垃圾邮件”。另一方面，回归问题旨在预测一个连续变量，例如房屋的价格。
- en: While data imbalance can also affect regression problems, this book will concentrate
    solely on classification problems. This focus is due to several factors, such
    as the limited scope of this book and the well-established techniques available
    for classification. In some cases, you might even be able to reframe a regression
    problem as a classification problem, making the methods discussed in this book
    still relevant.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据不平衡也可能影响回归问题，但本书将仅专注于分类问题。这种关注是由于几个因素，例如本书的范围有限以及分类中可用的技术已经建立。在某些情况下，甚至可以将回归问题重新构造成分类问题，使得本书中讨论的方法仍然相关。
- en: 'When it comes to various kinds of models that are popular for classification
    problems, we have quite a few categories of classical supervised machine learning
    models:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到用于分类问题的各种流行模型时，我们有相当多的经典监督机器学习模型类别：
- en: '**Logistic regression**: This is a supervised machine learning algorithm that’s
    used for binary classification problems. It predicts the probability of a binary
    target variable based on a set of predictor variables (features) by fitting a
    logistic function to the data, which outputs a value between 0 and 1.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：这是一种用于二元分类问题的监督机器学习算法。它通过拟合逻辑函数到数据，根据一组预测变量（特征）预测二元目标变量的概率，该函数输出介于0和1之间的值。'
- en: '**Support Vector Machines** (**SVMs**): These are supervised machine learning
    algorithms that are mainly used for classification and can be extended to regression
    problems. SVMs classify data by finding the optimal hyperplane that maximally
    separates the different classes in the input data, thus making it a powerful tool
    for binary and multiclass classification tasks.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVMs**）：这些是主要用于分类的监督机器学习算法，也可以扩展到回归问题。SVMs通过找到最优的超平面来最大化分离输入数据中的不同类别，从而成为二元和多类别分类任务的强大工具。'
- en: '**K-Nearest Neighbors** (**KNN**): This is a supervised machine learning algorithm
    that’s used for classification and regression analysis. It predicts the target
    variable based on the *k*-nearest neighbors in the training dataset. The value
    of *k* determines the number of neighbors to consider when making a prediction,
    and it can be tuned to optimize the model’s performance.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-最近邻**（**KNN**）：这是一种用于分类和回归分析的监督机器学习算法。它根据训练数据集中的*k*个最近邻来预测目标变量。*k*的值决定了在做出预测时考虑的邻居数量，并且可以调整以优化模型性能。'
- en: '**Tree models**: These are a type of supervised machine learning algorithm
    that’s used for classification and regression analysis. They recursively split
    the data into smaller subsets based on the most important features to create a
    decision tree that predicts the target variable based on the input features.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树模型**：这是一种用于分类和回归分析的监督机器学习算法。它们通过根据最重要的特征递归地将数据分割成更小的子集来创建决策树，该决策树根据输入特征预测目标变量。'
- en: '**Ensemble models**: These combine multiple individual models to improve predictive
    accuracy and reduce overfitting (explained later in this chapter). Ensemble techniques
    include bagging (for example, random forest), boosting (for example, XGBoost),
    and stacking. They are commonly used for classification as well as regression
    analysis.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成模型**：这些模型结合多个单个模型以提高预测准确性和减少过拟合（本章后面将解释）。集成技术包括袋装（例如，随机森林）、提升（例如，XGBoost）和堆叠。它们常用于分类以及回归分析。'
- en: '**Neural networks**: These models are inspired by the human brain, consist
    of multiple layers with numerous neurons in each, and are capable of learning
    complex functions. We will discuss these in more detail in [*Chapter 6*](B17259_06.xhtml#_idTextAnchor185),
    *Data Imbalance in* *Deep Learning*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络**：这些模型受人类大脑的启发，由多个层组成，每层有众多神经元，能够学习复杂函数。我们将在[*第6章*](B17259_06.xhtml#_idTextAnchor185)中更详细地讨论这些内容，*深度学习中的数据不平衡*。'
- en: '*Figure 1**.4* displays the decision boundaries of various classifiers we have
    reviewed so far. It shows that logistic regression has a linear decision boundary,
    while tree-based models such as decision trees, random forests, and XGBoost work
    by dividing examples into axis-parallel rectangles to form their decision boundary.
    SVM, on the other hand, transforms the data to a different space so that it can
    plot its non-linear decision boundary. Neural networks have a non-linear decision
    boundary:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.4*显示了迄今为止我们已审查的各种分类器的决策边界。它显示逻辑回归具有线性决策边界，而基于树的模型，如决策树、随机森林和XGBoost，通过将示例分割成轴平行的矩形来形成它们的决策边界。另一方面，SVM将数据转换到不同的空间，以便它可以绘制其非线性决策边界。神经网络具有非线性决策边界：'
- en: '![](img/B17259_01_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_01_04.jpg)'
- en: Figure 1.4 – The decision boundaries of popular machine learning algorithms
    on an imbalanced dataset
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 在不平衡数据集上流行机器学习算法的决策边界
- en: Next, we’ll delve into the principles underlying the process of model training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨模型训练过程背后的原理。
- en: What happens during model training?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练过程中会发生什么？
- en: In the training phase of a machine learning model, we provide a dataset consisting
    of examples, each with input features and a corresponding label, to the model.
    Let X represent the list of features used for training, and y be the list of labels
    in the training dataset. The goal of the model is to learn a function, f, such
    that f(X) ≈ y.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型的训练阶段，我们向模型提供一个由示例组成的训练数据集，每个示例都有输入特征和相应的标签。让X代表用于训练的特征列表，y代表训练数据集中的标签列表。模型的目标是学习一个函数f，使得f(X)
    ≈ y。
- en: The model has adjustable parameters, denoted as θ, which are fine-tuned during
    the training process. The error function, commonly referred to as the **loss function**,
    is defined as L(f(X; θ), y). This error function needs to be minimized by a learning
    algorithm, which finds the optimal setting of these parameters, θ.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型具有可调整的参数，表示为θ，这些参数在训练过程中进行微调。误差函数，通常称为**损失函数**，定义为L(f(X; θ), y)。该误差函数需要通过学习算法最小化，该算法找到这些参数θ的最佳设置。
- en: 'In classification problems, our typical loss functions are cross-entropy loss
    (also called the log loss):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，我们的典型损失函数是交叉熵损失（也称为对数损失）：
- en: CrossEntropyLoss(p) = {− log(p) if y = 1  − log(1 − p) otherwise
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: CrossEntropyLoss(p) = {− log(p) if y = 1  − log(1 − p) otherwise
- en: Here, p is the predicted probability from the model when y = 1.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，p是当y = 1时模型预测的概率。
- en: When the model’s prediction closely agrees with the target label, the loss function
    will approach zero. However, when the prediction deviates significantly from the
    target, the loss can become arbitrarily large, indicating a poor model fit.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的预测与目标标签非常接近时，损失函数将趋近于零。然而，当预测与目标有显著偏差时，损失可以变得任意大，这表明模型拟合不良。
- en: 'As training progresses, the training loss keeps going down (*Figure 1**.5*):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，训练损失持续下降（*图1**.5*）：
- en: '![](img/B17259_01_05.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_05.jpg)'
- en: Figure 1.5 – Rate of change of the loss function as training progresses
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 随着训练的进行损失函数的变化率
- en: 'This brings us to the concept of the fit of a model:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了模型拟合的概念：
- en: A model is said to **underfit** if it is too simple and can’t capture the data’s
    complexity. It performs poorly on both training and new data.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个模型过于简单，无法捕捉数据的复杂性，则称为**欠拟合**。它在训练数据和新的数据上表现都较差。
- en: A model is of **right fit** if it accurately captures data patterns without
    learning noise. It performs well on both training and new data.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型如果能够准确捕捉数据模式而不包含学习噪声，则被认为是**合适的**。它在训练数据和新的数据上表现良好。
- en: 'An **overfit** model is too complex and learns noise along with data patterns.
    It performs well on training data but poorly on new data (*Figure 1**.6*):'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**过拟合**的模型过于复杂，并学会了数据模式中的噪声。它在训练数据上表现良好，但在新的数据上表现较差（*图1**.6*）：
- en: '![](img/B17259_01_06.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_06.jpg)'
- en: Figure 1.6 – Underfit, right fit, and overfit models for classification task
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 分类任务中的欠拟合、合适拟合和过拟合模型
- en: 'Next, let’s briefly try to learn about two important concepts in machine learning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们简要了解机器学习中的两个重要概念：
- en: '**Regularization** is a set of techniques that are used to prevent the overfitting
    of a model to the training data. One type of regularization (namely L1 or L2)
    adds a penalty term to the loss function, which encourages the model to have smaller
    weights and reduces its complexity. This helps prevent the model from fitting
    too closely to the training data and generalizes better to unseen data.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**是一组用于防止模型过度拟合训练数据的技巧。一种正则化类型（即L1或L2）将惩罚项添加到损失函数中，这鼓励模型具有更小的权重并降低其复杂性。这有助于防止模型过于接近训练数据，并更好地泛化到未见数据。'
- en: '**Feature engineering** is the process of selecting and transforming the input
    features of a model to improve its performance. Feature engineering involves selecting
    the most relevant features for the problem, transforming them to make them more
    informative, and creating new features from the existing ones. Good feature engineering
    can make a huge difference in the performance of a model and can often be more
    important than the choice of algorithm or hyperparameters.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**是选择和转换模型输入特征以改进其性能的过程。特征工程包括选择与问题最相关的特征，将它们转换以使其更具信息性，并从现有特征中创建新特征。良好的特征工程可以在模型性能上产生巨大差异，并且通常比算法或超参数的选择更重要。'
- en: Types of dataset and splits
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集类型和分割
- en: Typically, we train our model on the training set and test the model on an independent
    unseen dataset called the test set. We do this to do a fair evaluation of the
    model. If we don’t do this and train the model on the full dataset and evaluate
    the model on the same dataset, we don’t know how good the model would do on unseen
    data, plus the model will likely be overfitted.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在训练集上训练模型，并在一个独立的未见数据集上测试模型，这个数据集被称为测试集。我们这样做是为了对模型进行公平的评估。如果我们不这样做，而是在整个数据集上训练模型并在同一数据集上评估模型，我们就不知道模型在未见数据上的表现如何，此外，模型很可能会过拟合。
- en: 'We may encounter three kinds of datasets in machine learning:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们可能会遇到三种类型的数据集：
- en: '**Training set**: A dataset on which the model is trained.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：模型训练的数据集。'
- en: '**Validation set**: A dataset used for tuning the hyperparameters of the model.
    A validation set is often referred to as a development set.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：用于调整模型超参数的数据集。验证集通常被称为开发集。'
- en: '**Evaluation set or test set**: A dataset used for evaluating the performance
    of the model.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估集或测试集**：用于评估模型性能的数据集。'
- en: When working with small example datasets, it’s common to allocate 80% of the
    data for the training set, 10% for the validation set, and 10% for the test set.
    However, the specific ratio between training and test sets is not as important
    as ensuring that the test set is large enough to provide statistically meaningful
    evaluation results. In the context of big data, a split of 98%, 1%, and 1% for
    training, validation, and test sets, respectively, could be appropriate.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理小型示例数据集时，通常将80%的数据分配给训练集，10%分配给验证集，10%分配给测试集。然而，训练集和测试集之间的具体比例并不像确保测试集足够大以提供具有统计意义的评估结果那样重要。在大数据环境中，训练集、验证集和测试集分别以98%、1%和1%的比例分割可能是合适的。
- en: Often, people don’t have a dedicated validation set for hyperparameter tuning
    and refer to the test set as an evaluation set. This can happen when the hyperparameter
    tuning is not performed as a part of the regular training cycle and is a one-off
    activity.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们没有为超参数调整设置专门的验证集，而是将测试集作为评估集。这可能发生在超参数调整不是作为常规训练周期的一部分进行，而是一次性活动时。
- en: Cross-validation
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: '**Cross-validation** can be a confusing term to guess its meaning. Breaking
    it down: cross + validation, so it’s some sort of validation on an extended (cross)
    something. *Something* here is the test set for us.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**可能是一个令人困惑的术语，猜测其含义。分解它：交叉+验证，所以它是对扩展（交叉）某种东西的验证。在这里，“某种东西”对我们来说是测试集。'
- en: 'Let’s see what cross-validation is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看什么是交叉验证：
- en: Cross-validation is a technique that’s used to estimate how accurately a model
    will perform in practice
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证是一种技术，用于估计模型在实际应用中的准确性。
- en: It is typically used to detect overfitting – that is, failing to generalize
    patterns in data, particularly when the amount of data may be limited
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通常用于检测过拟合——即未能泛化数据中的模式，尤其是在数据量可能有限的情况下。
- en: 'Let’s look at the different types of cross-validation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看不同的交叉验证类型：
- en: '**Holdout**: In the holdout method, we randomly assign data points to two sets,
    usually called the training set and the test set, respectively. We then train
    (build a model) on the *training set* and test (evaluate its performance) on the
    *test set*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保留集（Holdout）**：在保留集方法中，我们将数据点随机分配到两个集合中，通常分别称为训练集和测试集。然后我们在**训练集**上训练（构建模型），在**测试集**上测试（评估其性能）。'
- en: '**k-fold**: This works as follows:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**k折**：它的工作方式如下：'
- en: We randomly shuffle the data.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们随机打乱数据。
- en: We divide all the data into *k* parts, also known as folds. We train the model
    on *k*-1 folds and evaluate it on the remaining fold. We record the performance
    of this model using our chosen model evaluation metric, then discard this model.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将所有数据分成*k*部分，也称为折。我们在*k*-1折上训练模型，并在剩余的折上评估它。我们使用我们选择的模型评估指标记录这个模型的性能，然后丢弃这个模型。
- en: We repeat this process *k* times, each time holding out a different subset for
    testing. We take an average of the evaluation metric values (for example, accuracy)
    from all the previous models. This average represents the overall performance
    measure of the model.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们重复这个过程*k*次，每次都保留不同的子集进行测试。我们从所有之前的模型中取评估指标值（例如，准确率）的平均值。这个平均值代表了模型的总体性能指标。
- en: '**k-fold cross-validation** is mainly used when you have limited data points,
    say 100 points. Using 5 or 10 folds is the most common when doing cross-validation.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**k折交叉验证**主要用于数据点有限的情况，比如100个点。在执行交叉验证时，使用5或10折是最常见的。'
- en: Let’s look at the common evaluation metrics in machine learning, with a special
    focus on the ones relevant to problems with imbalanced data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看机器学习中的常见评估指标，特别关注与不平衡数据问题相关的指标。
- en: Common evaluation metrics
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见评估指标
- en: Several machine learning and deep learning metrics are used for evaluating the
    performance of classification models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 几种机器学习和深度学习指标被用于评估分类模型的性能。
- en: Let’s look at some of the helpful metrics that can help evaluate the performance
    of our model on the test set.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些有助于评估我们在测试集上模型性能的有用指标。
- en: Confusion matrix
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: 'Given a model that tries to classify an example as belonging to the positive
    or negative class, there are four possibilities:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个试图将示例分类为正类或负类的模型，有四种可能性：
- en: '**True Positive (TP)**: This occurs when the model correctly predicts a sample
    as part of the positive class, which is its actual classification'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性（TP）**：这发生在模型正确地将样本预测为正类的一部分时，这是其实际的分类。'
- en: '**False Negative (FN)**: This happens when the model incorrectly classifies
    a sample from the positive class as belonging to the negative class'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性 (FN)**: 这发生在模型错误地将正类样本分类为负类'
- en: '**True Negative (TN)**: This refers to instances where the model correctly
    identifies a sample as part of the negative class, which is its actual classification'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性 (TN)**: 这指的是模型正确地将样本识别为负类，这是其实际分类'
- en: '**False Positive (FP)**: This occurs when the model incorrectly predicts a
    sample from the negative class as belonging to the positive class'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性 (FP)**: 这发生在模型错误地将负类样本预测为正类'
- en: '*Table 1.1* shows in what ways the model can get “confused” when making predictions,
    aptly called the **confusion matrix**. The confusion matrix forms the basis of
    many common metrics in machine learning:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 1.1* 展示了模型在做出预测时可能会“混淆”的方式，恰当地称为**混淆矩阵**。混淆矩阵是许多常见机器学习指标的基础：'
- en: '|  | **Predicted Positive** | **Predicted Negative** |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测正样本** | **预测负样本** |'
- en: '| **Actually Positive** | True Positive (TP) | False Negative (FN) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **实际正样本** | 真正例 (TP) | 假阴性 (FN) |'
- en: '| **Actually Negative** | False Positive (FP) | True Negative (TN) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **实际负样本** | 假阳性 (FP) | 真阴性 (TN) |'
- en: Table 1.1 – Confusion matrix
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 – 混淆矩阵
- en: 'Let’s look at some of the most common metrics in machine learning:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看机器学习中一些最常见的指标：
- en: '**True Positive Rate** (**TPR**) measures the proportion of actual positive
    examples correctly classified by the model:'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例率** (**TPR**) 衡量模型正确分类的实际正例的比例：'
- en: TPR = Positives classified correctly  ______________ Total positives  =  TP _ TP
    + FN
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TPR = 正确分类的正样本  ______________  总正样本 =  TP _ TP + FN
- en: '**False Positive Rate** (**FPR**) measures the proportion of actual negative
    examples that are incorrectly identified as positives by the model:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性率** (**FPR**) 衡量模型错误地将实际负样本识别为正样本的比例：'
- en: FPR = Negatives classified incorrectly  _______________  Total negatives  =
     FP _ FP + TN
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FPR = 错误分类的负样本  _______________  总负样本 =  FP _ FP + TN
- en: '`sklearn` library as `sklearn.metrics.accuracy_score`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` 库中的 `sklearn.metrics.accuracy_score`。'
- en: '`sklearn` library under the name `sklearn.metrics.precision_score`. Precision
    =  TP _ TP + FP.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` 库下的 `sklearn.metrics.precision_score`。精确率 =  TP _ TP + FP。'
- en: '`sklearn` library under the name `sklearn.metrics.recall_score`. Recall =  TP _ TP
    + FN.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` 库下的 `sklearn.metrics.recall_score`。召回率 =  TP _ TP + FN。'
- en: '*Table 1.2* summarizes the differences between precision and recall:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*表 1.2* 总结了精确率和召回率之间的差异：'
- en: '|  | **Precision** | **Recall** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确率** | **召回率** |'
- en: '| Definition | Precision is a measure of trustworthiness | Recall is a measure
    of completeness |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 定义 | 精确率是可信度的衡量标准 | 召回率是完整性的衡量标准 |'
- en: '| Question to ask | When the model says something is positive, how often is
    it right? | Out of all the positive instances, how many did the model correctly
    identify? |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 要问的问题 | 当模型说某件事是正样本时，它有多经常是正确的？ | 在所有正样本中，模型正确识别了多少？ |'
- en: '| Example (using an email filter) | Precision measures how many of the emails
    the model flags as spam are actually spam, as a percentage of all the flagged
    emails | Recall measures how many of the actual spam emails the model catches,
    as a percentage of all the spam emails in the dataset |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 示例（使用电子邮件过滤器） | 精确率衡量模型标记为垃圾邮件的电子邮件中有多少实际上是垃圾邮件，占所有标记电子邮件的百分比 | 召回率衡量模型捕获的实际垃圾邮件数量，占数据集中所有垃圾邮件的百分比
    |'
- en: '| Formula | Precision =  TP _ TP + FP  | Recall =  TP _ TP + FN  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 公式 | 精确率 =  TP _ TP + FP  | 召回率 =  TP _ TP + FN  |'
- en: Table 1.2 – Precision versus recall
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.2 – 精确率与召回率
- en: Why can accuracy be a bad metric for imbalanced datasets?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么准确率对于不平衡数据集来说可能是一个糟糕的指标？
- en: Let’s assume we have an imbalanced dataset with 1,000 examples, with 100 labels
    belonging to class 1 (the minority class) and 900 belonging to class 0 (the majority
    class).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个不平衡的数据集，包含 1,000 个示例，其中 100 个标签属于类别 1（少数类）和 900 个属于类别 0（多数类）。
- en: Let’s say we have a model that always predicts 0 for all examples. The model’s
    accuracy for the minority class is   900 + 0 _ (900 + 0+ 100 + 0 ) = 90%.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个模型，总是对所有示例预测 0。该模型对少数类的准确率为 900 + 0 _ (900 + 0+ 100 + 0 ) = 90%。
- en: '![](img/B17259_01_07.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_07.jpg)'
- en: Figure 1.7 – A comic showing accuracy may not always be the right metric
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 一幅漫画显示准确率可能并不总是正确的指标
- en: This brings us to the **precision-recall trade-off** in machine learning. Usually,
    precision and recall are inversely correlated – that is, when recall increases,
    precision most often decreases. Why? Note that recall =  TP _ TP + FN and for
    recall to increase, FN should decrease. This means the model needs to classify
    more items as positive. However, if the model classifies more items as positive,
    some of these will likely be incorrect classifications, leading to an increase
    in the number of **false positives** (**FPs**). As the number of FPs increases,
    precision, defined as  TP _ TP + FP, will decrease. With similar logic, you can
    argue that when recall decreases, precision often increases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到机器学习中的 **精度-召回率权衡**。通常，精度和召回率是负相关的，也就是说，当召回率增加时，精度通常会降低。为什么？注意，召回率 = TP
    _ TP + FN，为了召回率增加，FN 应该减少。这意味着模型需要将更多项目分类为正类。然而，如果模型将更多项目分类为正类，其中一些可能是错误的分类，导致假阳性（**FP**）的数量增加。随着假阳性数量的增加，定义为
    TP _ TP + FP 的精度将降低。用类似的逻辑，你可以论证当召回率降低时，精度通常会提高。
- en: 'Next, let’s try to understand some of the precision and recall-based metrics
    that can help measure the performance of models trained on imbalanced data:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试理解一些基于精度和召回率的指标，这些指标可以帮助衡量在不平衡数据上训练的模型的性能：
- en: '`sklearn` library as `sklearn.metrics.f1_score`.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` 库中的 `sklearn.metrics.f1_score`。'
- en: '`sklearn` library as `sklearn.metrics.fbeta_score`.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` 库中的 `sklearn.metrics.fbeta_score`。'
- en: '`sklearn` library as `sklearn.metrics.balanced_accuracy_score`.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` 库中的 `sklearn.metrics.balanced_accuracy_score`。'
- en: '**Specificity (SPE)**: Specificity is a measure of the model’s ability to correctly
    identify the negative samples. In binary classification, it is calculated as the
    ratio of true negative predictions to the total number of negative samples. High
    specificity indicates that the model is good at identifying the negative class,
    while low specificity indicates that the model is biased toward the positive class.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特异性 (SPE)**：特异性是衡量模型正确识别负样本能力的指标。在二分类中，它被计算为真阴性预测与负样本总数之比。高特异性表明模型擅长识别负类，而低特异性表明模型倾向于正类。'
- en: '`sklearn.metrics.precision_recall_fscore_support` and `imblearn.metrics.classification_report_imbalanced`
    APIs.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.metrics.precision_recall_fscore_support` 和 `imblearn.metrics.classification_report_imbalanced`
    API。'
- en: '`imbalanced-learn`, `geometric_mean_score()` is defined by the geometric mean
    of “accuracy on positive class examples” (recall or sensitivity or TPR) and “accuracy
    on negative class examples” (specificity or TNR). So, even if one class is heavily
    outnumbered by the other class, the metric will still be representative of the
    model’s overall performance.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imbalanced-learn` 库中的 `geometric_mean_score()` 定义为“正类示例上的准确率”（召回率或灵敏度或 TPR）和“负类示例上的准确率”（特异性或
    TNR）的几何平均值。因此，即使一个类别被另一个类别大量超过，该指标仍然能代表模型的总体性能。'
- en: '`imblearn.metrics.classification_report_imbalanced`.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imblearn.metrics.classification_report_imbalanced`。'
- en: '*Table 1.3* shows the associated metrics and their formulas as an extension
    of the confusion matrix:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 1.3* 展示了与混淆矩阵相关的指标及其公式作为扩展：'
- en: '|  | **Predicted Positive** | **Predicted Negative** |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测为正** | **预测为负** |  |'
- en: '| **Actually** **Positive** | True positive (TP) | False negative (FN) | *Recall
    = Sensitivity = True positive* *rate (TPR) =*  TP _ TP + FN |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **实际上** **正** | 真阳性 (TP) | 假阴性 (FN) | *召回率 = 灵敏度 = 真阳性率 (TPR) = TP _ TP +
    FN*'
- en: '| **Actually** **Negative** | False positive (FP) | True negative (TN) | Specificity
    =  TN _ TN + FP  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **实际上** **负** | 假阳性 (FP) | 真阴性 (TN) | 特异性 =  TN _ TN + FP |'
- en: '|  | Precision = TP/(TP+FP)FPR = FP/(FP+TN) |  | Accuracy =  TP + TN ______________  TP
    + TN + FP + FN F1 − score =  2 * Precision * Recall  _______________  Precision
    + Recall  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | 精度 = TP/(TP+FP) 假阳性率 (FPR) = FP/(FP+TN) |  | 准确率 =  TP + TN ________________________
    TP + TN + FP + FN F1 分数 = 2 * 精度 * 召回率 ________________________ 精度 + 召回率 |'
- en: Table 1.3 – Confusion matrix with various metrics and their definitions
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3 – 带有各种指标及其定义的混淆矩阵
- en: ROC
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC
- en: '**Receiver Operating Characteristics**, commonly known as **ROC** curves, are
    plots that display the **TPR** on the *y*-axis against the **FPR** on the *x*-axis
    for various threshold values:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**，通常称为 **ROC** 曲线，是显示各种阈值值下 *y* 轴上的 **TPR** 与 *x* 轴上的 **FPR** 的图表：'
- en: The ROC curve essentially represents the proportion of correctly predicted positive
    instances on the *y*-axis, contrasted with the proportion of incorrectly predicted
    negative instances on the *x*-axis.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线本质上表示了*y*轴上正确预测的阳性实例的比例，与*x*轴上错误预测的阴性实例的比例形成对比。
- en: In classification tasks, a threshold is a cut-off value that’s used to determine
    the class of an example. For instance, if a model classifies an example as “positive,”
    a threshold of 0.5 might be set to decide whether the instance should be labeled
    as belonging to the “positive” or “negative” class. The ROC curve can be used
    to identify the optimal threshold for a model. This topic will be discussed in
    detail in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类任务中，阈值是一个用于确定示例类别的截止值。例如，如果一个模型将一个示例分类为“阳性”，则可能设置0.5的阈值来决定该实例是否应被标记为属于“阳性”或“阴性”类别。ROC曲线可以用来识别模型的最佳阈值。这个主题将在[*第5章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*中详细讨论。
- en: 'To create the ROC curve, we calculate the TPR and FPR for many various threshold
    values of the model’s predicted probabilities. For each threshold, the corresponding
    TPR value is plotted on the *y*-axis, and the FPR value is plotted on the *x*-axis,
    creating a single point. By connecting these points, we generate the ROC curve
    (*Figure 1**.8*):'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要创建ROC曲线，我们计算模型预测概率的许多不同阈值值的TPR和FPR。对于每个阈值，相应的TPR值被绘制在*y*轴上，FPR值被绘制在*x*轴上，形成一个点。通过连接这些点，我们生成ROC曲线（*图1**.8*）：
- en: '![](img/B17259_01_08.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_08.jpg)'
- en: Figure 1.8 – The ROC curve as a plot of TPR versus FPR (the dotted line shows
    a model with no skill)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 – ROC曲线作为TPR与FPR的图表（虚线表示没有技能的模型）
- en: 'Some properties of the ROC curve are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线的一些特性如下：
- en: 'The **Area Under Curve** (**AUC**) of a ROC curve (also called **AUC-ROC**)
    serves a specific purpose: it provides a single numerical value that represents
    the model’s performance across all possible classification thresholds:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线下的**面积**（**AUC**）（也称为**AUC-ROC**）具有特定的用途：它提供了一个单一的数值，表示模型在所有可能的分类阈值上的性能：
- en: AUC-ROC represents the degree of separability of the classes. This means that
    the higher the AUC-ROC, the more the model can distinguish between the classes
    and predict a positive class example as positive and a negative class example
    as negative. A poor model with an AUC near 0 essentially predicts a positive class
    as a negative class and vice versa.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC-ROC表示类别的可分离程度。这意味着AUC-ROC越高，模型区分类别和预测阳性类别示例为阳性以及阴性类别示例为阴性的能力就越强。AUC接近0的差劲模型实际上将阳性类别预测为阴性类别，反之亦然。
- en: The AUC-ROC of a random classifier is 0.5 and is the diagonal joining the points
    (0,0) and (1,0) on the ROC curve.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机分类器的AUC-ROC为0.5，并且是连接ROC曲线上的点(0,0)和(1,0)的对角线。
- en: 'The AUC-ROC has a probabilistic interpretation: an AUC of 0.9 indicates a 90%
    likelihood that the model will assign a higher score to a randomly chosen positive
    class example than to a negative class example. That is, AUC-ROC can be depicted
    as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC-ROC具有概率解释：AUC为0.9表示模型将随机选择的阳性类别示例分配的分数高于阴性类别示例的概率为90%。也就是说，AUC-ROC可以表示如下：
- en: P(score(x+ ) > score(x− ))
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: P(score(𝑥+ ) > score(𝑥− ))
- en: Here, 𝑥+ denotes the positive (minority) class, and 𝑥− denotes the negative
    (majority) class.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，𝑥+ 表示阳性（少数）类别，而𝑥− 表示阴性（多数）类别。
- en: 'In the context of evaluating model performance, it’s crucial to use a test
    set that reflects the distribution of the data the model will encounter in real-world
    scenarios. This is particularly relevant when considering metrics such as the
    ROC curve, which remains consistent regardless of changes in class imbalance within
    the test data. Whether we have 1:1, 1:10, or 1:100 as the minority_class: majority_class
    distribution in the test set, the ROC curve remains the same [2]. The reason for
    this is that both of these rates are independent of the class distribution in
    the test data because they are calculated only based on the correctly and incorrectly
    classified instances of each class, not the total number of instances of each
    class. This is not to be confused with the change in imbalance in the training
    data, which can adversely impact the model’s performance and would be reflected
    in the ROC curve.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估模型性能的背景下，使用一个反映模型将在现实世界场景中遇到的数据分布的测试集至关重要。这在考虑如ROC曲线等指标时尤其相关，因为ROC曲线在测试数据类别失衡变化时保持一致[2]。之所以如此，是因为这两个比率都是独立于测试数据类别分布的，因为它们仅基于每个类别的正确和错误分类实例计算，而不是每个类别的实例总数。这不同于训练数据中失衡的变化，这可能会对模型性能产生不利影响，并反映在ROC曲线上。
- en: 'Now, let’s look at some of the problems in using ROC for imbalanced datasets:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看使用ROC曲线评估失衡数据集时可能遇到的一些问题：
- en: ROC does not distinguish between the various classes – that is, it does not
    emphasize one class more over the other. This can be a problem for imbalanced
    datasets where, often, the minority class is more important to detect than the
    majority class. Because of this, it may not reflect the minority class well. For
    example, we may want better recall over precision.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线不区分各种类别——也就是说，它不会强调一个类别比另一个类别更重要。这对于失衡数据集来说可能是一个问题，因为在失衡数据集中，通常检测少数类别比检测多数类别更重要。正因为如此，它可能无法很好地反映少数类别。例如，我们可能希望召回率比精确率更好。
- en: 'While ROC curves can be useful for comparing the performance of models across
    a full range of FPRs, they may not be as relevant for specific applications that
    require a very low FPR, such as fraud detection in financial transactions or banking
    applications. The reason the FPR needs to be very low is that such applications
    usually require limited manual intervention. The number of transactions that can
    be manually checked may be as low as 1% or even 0.1% of all the data, which means
    the FPR can’t be higher than 0.001\. In these cases, anything to the right of
    an FPR equal to 0.001 on the ROC curve becomes irrelevant [3]. To further understand
    this point, let’s consider an example:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然ROC曲线可以用于比较模型在全范围内FPR的性能，但对于需要非常低FPR的特定应用，如金融交易或银行应用中的欺诈检测，它们可能不那么相关。FPR需要非常低的原因是，此类应用通常需要有限的手动干预。可以手动检查的交易数量可能低至所有数据的1%甚至0.1%，这意味着FPR不能高于0.001。在这些情况下，ROC曲线上的FPR等于0.001右侧的所有内容都变得无关紧要[3]。为了进一步理解这一点，让我们考虑一个例子：
- en: Let’s say that for a test set, we have a total of 10,000 examples and only 100
    examples of the positive class, making up 1% of the examples. So, any FPR higher
    than 1% - that is, 0.01 – is going to raise too many alerts to be handled manually
    by investigators.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设对于一个测试集，我们总共有10,000个示例，其中只有100个正类示例，占所有示例的1%。因此，任何高于1%的FPR——即0.01——都会引发过多的警报，无法由调查人员手动处理。
- en: The performance on the far left-hand side of the ROC curve becomes crucial in
    most real-world problems, which are often dominated by a large number of negative
    instances. As a result, most of the ROC curve becomes irrelevant for applications
    that need to maintain a very low FPR.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ROC曲线的左侧远端的表现对于大多数现实世界问题至关重要，这些问题通常由大量负实例主导。因此，对于需要保持非常低FPR的应用，ROC曲线的大部分内容都变得无关紧要。
- en: Precision-Recall curve
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确率-召回率曲线
- en: 'Similar to ROC curves, **Precision-Recall** (**PR**) curves plot a pair of
    metrics for different threshold values. But unlike ROC curves, which plot TPR
    and FPR, PR curves plot precision and recall. To demonstrate the difference between
    the two curves, let’s say we compare the performance of two models – Model 1 and
    Model 2 – on a particular handcrafted imbalanced dataset:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与ROC曲线类似，**精确率-召回率**（**PR**）曲线在不同阈值下绘制了一对指标。但与绘制TPR和FPR的ROC曲线不同，PR曲线绘制的是精确率和召回率。为了展示这两条曲线之间的差异，让我们假设我们比较了两个模型——模型1和模型2——在特定手工制作的失衡数据集上的性能：
- en: In *Figure 1**.9 (a)*, the ROC curves for both models appear to be close to
    the top-left corner (point (0, 1)), which might lead you to conclude that both
    models are performing well. However, this can be misleading, especially in the
    context of imbalanced datasets.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*图1.9（a）*中，两个模型的ROC曲线似乎接近左上角（点（0, 1）），这可能会让你得出结论，两个模型都表现良好。然而，这在不平衡数据集的背景下可能会误导。
- en: When we turn our attention to the PR curves in *Figure 1**.9 (b)*, a different
    story unfolds. Model 2 comes closer to the ideal top-right corner (point (1, 1))
    of the plot, indicating that its performance is much better than Model 1 when
    precision and recall are considered.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将注意力转向*图1.9（b）*中的PR曲线时，故事发生了变化。模型2接近图表的理想右上角（点（1, 1）），这表明在考虑精度和召回率时，其性能远优于模型1。
- en: The PR curve reveals that Model 2 has an advantage over Model 1.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PR曲线显示模型2相对于模型1有优势。
- en: 'This discrepancy between the ROC and PR curves also underscores the importance
    of using multiple metrics for model evaluation, particularly when dealing with
    imbalanced data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线和PR曲线之间的这种差异也强调了使用多个指标进行模型评估的重要性，尤其是在处理不平衡数据时：
- en: '![](img/B17259_01_09.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_01_09.jpg)'
- en: Figure 1.9 – The PR curve can show obvious differences between models compared
    to the ROC curve
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9 – 与ROC曲线相比，PR曲线可以显示模型之间的明显差异
- en: Let’s try to understand these observations in detail. While the ROC curve shows
    very little difference between the performance of the two models, the PR curve
    shows a much bigger gap. The reason for this is that the ROC curve uses FPR, which
    is FP/(FP+TN). Usually, TN is really high for an imbalanced dataset, and hence
    even if FP changes by a decent amount, FPR’s overall value is overshadowed by
    TN. Hence, ROC doesn’t change by a whole lot.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试详细理解这些观察结果。虽然ROC曲线显示两种模型性能之间的差异很小，但PR曲线显示了更大的差距。原因在于ROC曲线使用FPR，即FP/(FP+TN)。通常，对于不平衡数据集，TN非常高，因此即使FP有相当大的变化，FPR的整体值也会被TN所掩盖。因此，ROC变化不大。
- en: The conclusion of which classifier is superior can change with the distribution
    of classes in the test set. In the case of skewed datasets, the PR curve can more
    clearly show that the model did not work well compared to the ROC curve, as shown
    in the preceding figure.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个分类器更优越的结论可能会随着测试集中类别的分布而改变。在数据集倾斜的情况下，PR曲线可以比ROC曲线更清楚地显示出模型表现不佳，如图所示。
- en: The `sklearn` is `sklearn.metrics.average_precision_score`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`中的`average_precision_score`。'
- en: Relation between the ROC curve and PR curve
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC曲线与PR曲线之间的关系
- en: The primary distinction between the ROC curve and the PR curve lies in the fact
    that while ROC assesses how well the model can “calculate” both positive and negative
    classes, PR solely focuses on the positive class. Therefore, when dealing with
    a balanced dataset scenario and you are concerned with both the positive and negative
    classes, ROC AUC works exceptionally well. In contrast, when dealing with an imbalanced
    situation, PR AUC is more suitable. However, it’s important to keep in mind that
    PR AUC only evaluates the model’s ability to “calculate” the positive class. *Because
    PR curves are more sensitive to the positive (minority) class, we will be using
    PR curves throughout the first half of* *this book.*
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线和PR曲线之间的主要区别在于，ROC评估模型在“计算”正负类别方面的表现，而PR仅关注正类别。因此，当处理平衡数据集的情况，并且你关心正负类别时，ROC
    AUC表现得非常好。相比之下，当处理不平衡情况时，PR AUC更为合适。然而，重要的是要记住，PR AUC仅评估模型“计算”正类的能力。*因为PR曲线对正类（少数类）更敏感，所以本书的前半部分我们将使用PR曲线。*
- en: We can reimagine the PR curve with precision on the *x*-axis and TPR, also known
    as recall, on the *y*-axis. The key difference between the two curves is that
    while the ROC curve uses FPR, the PR curve uses precision.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*x*轴上以精度重新想象PR曲线，在*y*轴上则是TPR，也称为召回率。这两条曲线的关键区别在于，虽然ROC曲线使用FPR，PR曲线则使用精度。
- en: As discussed earlier, FPR tends to be very low when dealing with imbalanced
    datasets. This aspect of having low FPR values is crucial in certain applications
    such as fraud detection, where the capacity for manual investigations is inherently
    limited. Consequently, this perspective can alter the perceived performance of
    classifiers. As shown in *Figure 1**.9*, it’s also possible that the performances
    of the two models seem reversed when compared using average precision (0.69 versus
    0.90) instead of AUC-ROC (0.97 and 0.95).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当处理不平衡数据集时，FPR（假正率）往往非常低。这种低FPR值的特性在诸如欺诈检测等某些应用中至关重要，因为这些应用中手动调查的能力本质上有限。因此，这种观点可能会改变对分类器性能的认识。如图1.9所示，当使用平均精度（0.69比0.90）而不是AUC-ROC（0.97和0.95）进行比较时，两个模型的性能似乎也出现了反转。
- en: 'Let’s summarize this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下：
- en: The AUC-ROC is the area under the curve plotted with TPR on the *y*-axis and
    FPR on the *x*-axis.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC-ROC是在y轴上以TPR为基准，x轴上以FPR为基准的曲线下的面积。
- en: The AUC-PR is the area under the curve plotted with precision on the *y*-axis
    and recall on the *x*-axis.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUC-PR是在y轴上以精确率为基准，x轴上以召回率为基准的曲线下的面积。
- en: 'As TPR equals recall, the two plots only differ in what recall is compared
    to – either precision or FPR. Additionally, the plots are rotated by 90 degrees
    relative to each other:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TPR等于召回率，这两个图表仅在比较召回率时有所不同——要么是精确率，要么是FPR。此外，这两个图表相对于彼此旋转了90度：
- en: '|  | **AUC-ROC** | **AUC-PR** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | **AUC-ROC** | **AUC-PR** |'
- en: '| --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| General formula | AUC(TPR, FPR) | AUC(Precision, Recall) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 一般公式 | AUC(TPR, FPR) | AUC(精确率，召回率) |'
- en: '| Expanded formula | AUC(  TP _ TP + FN ,  FP _ FP + TN ) | AUC(  TP _ TP +
    FP ,  TP _ TP + FN ) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 扩展公式 | AUC(  TP _ TP + FN ,  FP _ FP + TN ) | AUC(  TP _ TP + FP ,  TP _ TP
    + FN ) |'
- en: '| Equivalence | AUC(Recall, FPR) | AUC(Precision, Recall) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 对应关系 | AUC(召回率，FPR) | AUC(精确率，召回率) |'
- en: Table 1.4 – Comparing the ROC and PR curves
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.4 – 比较ROC和PR曲线
- en: In the next few sections, we’ll explore the circumstances that lead to imbalances
    in datasets, the challenges these imbalances can pose, and the situations where
    data imbalance might not be a concern.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探讨导致数据集不平衡的情况，这些不平衡可能带来的挑战，以及数据不平衡可能不是问题的情况。
- en: Challenges and considerations when dealing with imbalanced data
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不平衡数据时的挑战和考虑因素
- en: In certain instances, directly using data for machine learning without worrying
    about data imbalance can yield usable results suitable for a given business scenario.
    Yet, there are situations where a more dedicated effort is needed to manage the
    effects of imbalanced data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，直接使用数据用于机器学习而不担心数据不平衡，可能会得到适合特定业务场景的可用的结果。然而，在某些情况下，需要更多的努力来管理不平衡数据的影响。
- en: Broad statements claiming that you must always or never adjust for imbalanced
    classes tend to be misleading. The truth is that the need to address class imbalance
    is contingent on the specific characteristics of the data, the problem at hand,
    and the definition of an acceptable solution. Therefore, the approach to dealing
    with class imbalance should be tailored according to these factors.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛的说法，声称你必须始终或永远调整不平衡类别，往往具有误导性。事实是，解决类别不平衡的需要取决于数据的特定特征、手头的问题以及可接受解决方案的定义。因此，处理类别不平衡的方法应根据这些因素量身定制。
- en: When can we have an imbalance in datasets?
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们在什么情况下会遇到数据集的不平衡？
- en: 'In this section, we’ll explore various situations and causes leading to an
    imbalance in datasets, such as rare event occurrences or skewed data collection
    processes:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨导致数据集不平衡的各种情况和原因，例如罕见事件的发生或数据收集过程的偏差：
- en: '**Inherent in the problem**: Sometimes, the task we need to solve involves
    detecting outliers in datasets – for example, patients with a certain disease
    or fraud cases in a set of transactions. In such cases, the dataset is inherently
    imbalanced because the target events are rare to begin with.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题固有的**：有时，我们需要解决的任务涉及检测数据集中的异常值——例如，患有某种疾病的患者或一组交易中的欺诈案例。在这种情况下，数据集固有不平衡，因为目标事件本身就很罕见。'
- en: '**High cost of data collection while bootstrapping a machine learning solution**:
    The cost of collecting data might be too high for certain classes. For example,
    collecting data on COVID-19 patients incurs high costs due to the need for specialized
    medical tests, protective equipment, and the ethical and logistical challenges
    of obtaining informed consent in a high-stress healthcare environment.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在构建机器学习解决方案时数据收集的高成本**：对于某些类别，收集数据可能成本过高。例如，由于需要专业的医疗测试、防护设备，以及在高度紧张的医疗环境中获取知情同意的伦理和物流挑战，收集COVID-19患者的数据成本很高。'
- en: '**Noisy labels for certain classes**: This may happen when a lot of noise is
    introduced into the labels of the dataset for certain classes during data collection.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**某些类别的噪声标签**：这可能在数据收集过程中，某些类别的标签中引入了大量噪声时发生。'
- en: '**Labeling errors**: Errors in labeling can also contribute to data imbalance.
    For example, if some samples are mistakenly labeled as negative when they are
    positive, this can result in an imbalance in the dataset. Additionally, if a class
    is already inherently rare, human annotators might be biased and overlook the
    few examples of that rare class that do exist.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签错误**：标签错误也可能导致数据不平衡。例如，如果一些样本被错误地标记为负例，而实际上它们是正例，这可能导致数据集中的不平衡。此外，如果一个类别本身就很罕见，人类标注者可能会存在偏见，并忽略那些存在的罕见类别的少数示例。'
- en: '**Sampling bias**: Data collection methods can sometimes introduce bias in
    the dataset. For example, if a survey is conducted in a specific geographical
    area or among a specific group of people, the resulting dataset may not be representative
    of the entire population.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样偏差**：数据收集方法有时会在数据集中引入偏差。例如，如果一项调查在特定的地理区域或特定的人群中进行，那么得到的数据集可能无法代表整个群体。'
- en: '**Data cleaning**: During the data cleaning or filtering process, some classes
    or samples may be removed due to incomplete or missing data. This can result in
    an imbalance in the remaining dataset.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：在数据清洗或过滤过程中，由于数据不完整或缺失，某些类别或样本可能会被移除。这可能导致剩余数据集中的不平衡。'
- en: Why can imbalanced data be a challenge?
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不平衡数据会是一个挑战？
- en: 'Let’s delve into the difficulties posed by imbalanced data on model predictions
    and their impact on model performance:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨不平衡数据对模型预测的困难及其对模型性能的影响：
- en: '**Failure of metrics such as accuracy**: As we discussed previously, conventional
    metrics such as accuracy can be misleading in the context of imbalanced data (a
    99% imbalanced dataset would still achieve 99% accuracy). Threshold-invariant
    metrics such as the PR curve or ROC curve attempt to expose the performance of
    the model over a wide range of thresholds. The real challenge lies in the disproportionate
    influence of the “true negative” cell in the confusion matrix. Metrics that focus
    less on “true negatives,” such as precision, recall, or F1 score, are more appropriate
    for evaluating model performance. It’s important to note that these metrics have
    a hidden hyperparameter – the classification threshold – that should not be ignored
    but optimized for real-world applications (refer to [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, to learn more about threshold tuning).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量指标如准确率的失败**：正如我们之前讨论的，在数据不平衡的背景下（一个99%不平衡的数据集仍然可以达到99%的准确率），传统的度量指标如准确率可能会产生误导。阈值不变的度量指标，如PR曲线或ROC曲线，试图揭示模型在广泛阈值范围内的性能。真正的挑战在于混淆矩阵中“真正负例”单元格的不成比例影响。那些较少关注“真正负例”的度量指标，如精确率、召回率或F1分数，更适合评估模型性能。需要注意的是，这些度量指标有一个隐藏的超参数——分类阈值——不应被忽视，而应在实际应用中进行优化（参考[*第五章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*，了解更多关于阈值调整的信息）。'
- en: '**Imbalanced data can be a challenge for a model’s loss function**: This may
    happen because the loss function is typically designed to minimize the errors
    between the predicted outputs and the true labels of the training data. When the
    data is imbalanced, there are more instances of one class than another, and the
    model may become biased toward the majority class. We will discuss solutions to
    this issue in more detail in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151),
    *Cost-Sensitive Learning*, and [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep* *Learning Techniques*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平衡数据对模型的损失函数可能构成挑战**：这可能是因为损失函数通常设计为最小化预测输出与训练数据真实标签之间的错误。当数据不平衡时，一个类别的实例比另一个类别的实例多，模型可能会偏向多数类。我们将在[*第5章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*和[*第8章*](B17259_08.xhtml#_idTextAnchor235)，*算法级深度学习技术*中更详细地讨论解决这个问题。'
- en: '**Different misclassification costs for different classes**: Often, it may
    be more expensive to misclassify positive examples than to misclassify negative
    examples. We may have false positives that are more expensive than false negatives.
    For example, usually, the cost of misclassifying a patient with cancer as healthy
    (false negative) will be much higher than misclassifying a healthy patient as
    having cancer (false positive). Why? Because it’s much cheaper to go through some
    extra tests to revalidate the test results in the second case instead of detecting
    it much later in the first case. This is called the cost of misclassification,
    which could be different for the majority and minority classes, making things
    complicated for imbalanced datasets. We will discuss more about this in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同类别的误分类成本不同**：通常，误分类正例的成本可能比误分类负例的成本更高。我们可能有的假阳性比假阴性更昂贵。例如，通常将患有癌症的患者误诊为健康（假阴性）的成本将远高于将健康患者误诊为患有癌症（假阳性）。为什么？因为第二种情况下，通过进行一些额外的测试来重新验证测试结果的成本要低得多，而不是在第一种情况下晚得多地检测到它。这被称为误分类成本，对于多数类和少数类可能不同，这使得不平衡数据集变得复杂。我们将在[*第5章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*中进一步讨论这个问题。'
- en: '**Constraints on computational resources**: In sectors such as finance, healthcare,
    and retail, handling big data is a common challenge. Training on these large datasets
    is not only time-consuming but also costly due to the computational power needed.
    In such scenarios, downsampling or undersampling the majority class becomes essential,
    as will be discussed in [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079), *Undersampling
    Methods*. Additionally, acquiring more samples for the minority class can further
    increase dataset size and computational costs. Memory limitations may also restrict
    the amount of data that can be processed.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源受限**：在金融、医疗和零售等行业，处理大数据是一个常见的挑战。在这些大型数据集上进行训练不仅耗时，而且由于所需的计算能力，成本也很高。在这种情况下，对多数类进行下采样或欠采样变得至关重要，这将在[*第3章*](B17259_03.xhtml#_idTextAnchor079)，*欠采样方法*中讨论。此外，为少数类获取更多样本还可以进一步增加数据集大小和计算成本。内存限制也可能限制可以处理的数据量。'
- en: '**Not enough variation in the minority class examples to sufficiently represent
    its distribution**: Often, an absolute number of samples of the minority class
    is not as big of a problem as the **variation** in the samples of the minority
    class. The dataset might look large, but there might not be many variations or
    varieties in the samples that adequately represent the distribution of minority
    classes. This can lead to the model not being able to learn the classification
    boundary properly, which would lead to poor performance of the model (*Figure
    1**.10*). This can often happen in computer vision problems, such as object detection,
    where we may have very few samples of certain classes. In such cases, data augmentation
    techniques (discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level
    Deep Learning Methods*) can help significantly:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少数类样本变化不足，无法充分代表其分布**：通常，少数类样本的绝对数量问题并不像少数类样本的**变化**那样严重。数据集可能看起来很大，但样本中可能没有很多变化或种类，足以代表少数类的分布。这可能导致模型无法正确学习分类边界，从而导致模型性能不佳（**图1.10**）。这种情况在计算机视觉问题中经常发生，例如目标检测，我们可能只有少数几个特定类别的样本。在这种情况下，数据增强技术（在第[*7章*](B17259_07.xhtml#_idTextAnchor205)，*数据级深度学习方法*中讨论）可以显著帮助：'
- en: '![](img/B17259_01_10.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_01_10.jpg)'
- en: Figure 1.10 – Change in decision boundary with a different distribution of minority
    class examples – the crosses denote the majority class, and the circles denote
    the minority class
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 不同分布的少数类示例对决策边界的影响 – 十字表示多数类，圆圈表示少数类
- en: '**Poor performance of uncalibrated models**: Imbalanced data can be a challenge
    for uncalibrated models. Uncalibrated models are models that do not output well-calibrated
    probabilities, which means that the predicted probabilities may not reflect the
    true likelihood of the predicted classes:'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未校准模型的性能不佳**：不平衡数据对未校准模型来说可能是一个挑战。未校准模型是指那些不输出良好校准概率的模型，这意味着预测概率可能不会反映预测类别的真实可能性：'
- en: When dealing with imbalanced data, uncalibrated models can be particularly susceptible
    to producing biased predictions toward the majority class as they may not be able
    to effectively differentiate between the minority and majority classes. This can
    lead to poor performance in the minority class, where the model may produce overly
    confident predictions or predictions that are too conservative.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当处理不平衡数据时，未校准的模型可能会特别容易产生偏向多数类的预测，因为它们可能无法有效地区分少数类和多数类。这可能导致模型在少数类上的性能不佳，模型可能会产生过于自信的预测或过于保守的预测。
- en: For example, an uncalibrated model that is trained on imbalanced data may incorrectly
    classify instances that belong to the minority class as majority class examples,
    often with high confidence. This is because the model may not have learned to
    adjust its predictions based on the imbalance in the data and may not have a good
    understanding of the minority class examples.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，在不平衡数据上训练的未校准模型可能会错误地将属于少数类的实例分类为多数类示例，通常具有很高的置信度。这是因为模型可能没有学会根据数据的不平衡调整其预测，也可能对少数类示例没有很好的理解。
- en: To address this challenge, it is important to use well-calibrated models [4]
    that can output probabilities that reflect the true likelihood of the predicted
    classes. This can be achieved through techniques such as Platt scaling or isotonic
    regression, which can calibrate the predicted probabilities of an uncalibrated
    model to produce more accurate and reliable probabilities. Model calibration will
    be discussed in detail in [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model
    Calibration*.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，使用能够输出反映预测类别真实可能性的良好校准模型[4]非常重要。这可以通过如Platt缩放或等调回归等技术实现，这些技术可以将未校准模型的预测概率校准，以产生更准确和可靠的概率。模型校准将在[*第10章*](B17259_10.xhtml#_idTextAnchor279)，*模型校准*中详细讨论。
- en: '**Poor performance of models because of non-adjusted thresholds**: It’s important
    to use intelligent thresholding when making predictions using models trained on
    imbalanced datasets. Simply predicting 1 when the model probability is over 0.5
    may not always be the best approach. Instead, we should consider other thresholds
    that may be more effective. This can be achieved by examining the PR curve of
    the model rather than relying solely on its success rate with a default probability
    threshold of 0.5\. Threshold adjustment can be quite important, even for models
    trained on naturally or artificially balanced datasets. We will discuss threshold
    adjustment in detail in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive
    Learning*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**由于未调整阈值导致的模型性能不佳**：在使用不平衡数据集训练的模型进行预测时，使用智能阈值选择非常重要。当模型概率超过0.5时简单地预测1可能并不总是最佳方法。相反，我们应该考虑其他可能更有效的阈值。这可以通过检查模型的PR曲线来实现，而不是仅仅依赖于默认概率阈值0.5的成功率。阈值调整对于在自然或人工平衡数据集上训练的模型来说可能非常重要。我们将在[*第5章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*中详细讨论阈值调整。'
- en: Next, let’s try to see when we shouldn’t do anything about data imbalance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看何时我们不应该对数据不平衡采取任何措施。
- en: When to not worry about data imbalance
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时不必担心数据不平衡
- en: Class imbalance may not always negatively impact performance, and using imbalance-specific
    methods can sometimes worsen results [5]. Therefore, it’s crucial to accurately
    assess whether a task is genuinely affected by class imbalance before applying
    any specialized techniques. One such strategy can be as simple as setting up a
    baseline model without worrying about class imbalance and observing the model’s
    performance on various classes using various performance metrics.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 类不平衡不一定总是对性能产生负面影响，有时使用特定于不平衡的方法可能会使结果变得更糟[5]。因此，在应用任何专门技术之前，准确评估任务是否真正受到类不平衡的影响至关重要。一种策略可以简单到设置一个不考虑类不平衡的基线模型，并观察模型在各种类别上使用各种性能指标的性能。
- en: 'Let’s explore scenarios where data imbalance may not be a concern and no corrective
    measures may be needed:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨数据不平衡可能不是问题，且不需要纠正措施的场景：
- en: '**When the imbalance is small**: If the imbalance in the dataset is relatively
    small, with the ratio of the minority class to the majority class being only slightly
    skewed (say 4:5 or 2:3), the impact on the model’s performance may be minimal.
    In such cases, the model may still perform reasonably well without requiring any
    special techniques to handle the imbalance.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当不平衡程度较小时**：如果数据集中的不平衡相对较小，少数类到多数类的比例只有轻微倾斜（比如说4:5或2:3），对模型性能的影响可能最小。在这种情况下，模型可能仍然表现良好，无需任何特殊技术来处理不平衡。'
- en: '**When the goal is to predict the majority class**: In some cases, the focus
    may be on predicting the majority class accurately, and the minority class may
    not be of particular interest. For example, in online ad placement, the focus
    can be on targeting users (majority class) likely to click on ads to maximize
    click-through rates and immediate revenue, while less attention is given to users
    (minority class) who may find ads annoying.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当目标是预测多数类**：在某些情况下，重点可能在于准确预测多数类，而少数类可能不是特别感兴趣。例如，在线广告定位时，重点可以放在针对可能点击广告的用户（多数类）以最大化点击率和即时收入，而对可能觉得广告烦人的用户（少数类）的关注较少。'
- en: '**When the cost of misclassification is nearly equal for both classes**: In
    some applications, the cost of misclassifying a positive class example is not
    high (that is, false negative). An example is classifying emails as spam or non-spam.
    It’s totally fine to miss a spam email once in a while and misclassify it as non-spam.
    In such cases, the impact of misclassification on the performance metrics may
    be negligible, and the imbalance may not be a concern.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当两类误分类的成本几乎相等时**：在某些应用中，将正类样本误分类的成本并不高（即假阴性）。例如，将电子邮件分类为垃圾邮件或非垃圾邮件。偶尔错过一封垃圾邮件并将其误分类为非垃圾邮件是完全正常的。在这种情况下，误分类对性能指标的影响可能可以忽略不计，不平衡可能不是问题。'
- en: '**When the dataset is sufficiently large**: Even if the ratio of minority to
    majority class samples is very low, such as 1:100, and if the dataset is sufficiently
    large, with a large number of samples in both the minority and majority classes,
    the impact of data imbalance on the model’s performance may be reduced. With a
    larger dataset, the model may be able to learn the patterns in the minority class
    more effectively. However, it would still be advisable to compare the baseline
    model’s performance with the performance of models that take the data imbalance
    into account. For example, compare a baseline model to models with threshold adjustment,
    oversampling, and undersampling ([*Chapter 2*](B17259_02.xhtml#_idTextAnchor042),
    *Oversampling Methods*, and [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079), *Undersampling
    Methods*), and algorithm-based techniques such as cost-sensitive learning ([*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当数据集足够大时**：即使少数类到多数类样本的比例非常低，例如1:100，并且如果数据集足够大，两个类别中都有大量样本，数据不平衡对模型性能的影响可能会降低。随着数据集的增大，模型可能能够更有效地学习少数类中的模式。然而，仍然建议将基线模型的性能与考虑数据不平衡的模型的性能进行比较。例如，将基线模型与具有阈值调整、过采样和欠采样的模型（[*第2章*](B17259_02.xhtml#_idTextAnchor042)，*过采样方法*，和[*第3章*](B17259_03.xhtml#_idTextAnchor079)，*欠采样方法*），以及基于算法的技术（[*第5章*](B17259_05.xhtml#_idTextAnchor151)，*成本敏感学习*）进行比较。'
- en: In the next section, we will become familiar with a library that can be very
    useful when dealing with imbalanced data. We will train a model on an imbalanced
    toy dataset and look at some metrics to evaluate the performance of the trained
    model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将熟悉一个在处理不平衡数据时非常有用的库。我们将在一个不平衡的玩具数据集上训练一个模型，并查看一些指标来评估训练模型的性能。
- en: Introduction to the imbalanced-learn library
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡-learn 库简介
- en: '`imbalanced-learn` (imported as `imblearn`) is a Python package that offers
    several techniques to deal with data imbalance. In the first half of this book,
    we will rely heavily on this library. Let’s install the `imbalanced-learn` library:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`（导入为 `imblearn`）是一个 Python 包，提供了处理数据不平衡的几种技术。在这本书的前半部分，我们将大量依赖这个库。让我们安装
    `imbalanced-learn` 库：'
- en: '[PRE0]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can use `imbalanced-learn` to create a synthetic dataset for our analysis:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `imbalanced-learn` 为我们的分析创建一个合成数据集：
- en: '[PRE1]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s analyze the generated dataset:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析生成的数据集：
- en: '[PRE2]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s the output:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE3]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/B17259_01_11.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_11.jpg)'
- en: Figure 1.11 – 2 class dataset with two features
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – 具有两个特征的 2 类数据集
- en: 'Let’s split this dataset into training and test sets:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个数据集分成训练集和测试集：
- en: '[PRE4]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s the output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE5]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note the usage of `stratify` in the `train_test_split` API of `sklearn`. Specifying
    `stratify=y` ensures we maintain the same ratio of majority and minority classes
    in both the training set and the test set. Let’s understand stratification in
    more detail.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `sklearn` 的 `train_test_split` API 中 `stratify` 的使用。指定 `stratify=y` 确保我们在训练集和测试集中保持多数和少数类别的相同比例。让我们更详细地了解分层。
- en: '**Stratified sampling** is a way to split the dataset into various subgroups
    (called “strata”) based on certain characteristics they share. It can be highly
    valuable when dealing with imbalanced datasets because it ensures that the train
    and test datasets have the same proportions of class labels as the original dataset.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**分层抽样**是一种根据某些共享特征将数据集分成各种子组（称为“层”）的方法。在处理不平衡数据集时，它非常有价值，因为它确保训练集和测试集具有与原始数据集相同的类别标签比例。'
- en: In an imbalanced dataset, the minority class constitutes a small fraction of
    the total data. If we perform a simple random split without any stratification,
    there’s a risk that the minority class may not be adequately represented in the
    training set or could be entirely left out from the test set, which may lead to
    poor performance and unreliable evaluation metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在不平衡数据集中，少数类占总数据的很小一部分。如果我们不进行分层进行简单的随机分割，少数类可能无法在训练集中得到充分代表，甚至可能完全被排除在测试集之外，这可能导致性能不佳和不可靠的评估指标。
- en: With stratified sampling, the proportion of each class in the overall dataset
    is preserved in both training and test sets, ensuring representative sampling
    and a better chance for the model to learn from the minority class. This leads
    to a more robust model and a more reliable evaluation of the model’s performance.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分层抽样，每个类别在整体数据集中的比例在训练集和测试集中都得到保留，确保了代表性的抽样，并为模型从少数类中学习提供了更好的机会。这导致了一个更稳健的模型和更可靠的模型性能评估。
- en: The scikit-learn APIs for stratification
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的分层 API
- en: The `scikit-learn` APIs, such as `RepeatedStratifiedKFold` and `StratifiedKFold`,
    employ the concept of stratification to evaluate model performance through cross-validation,
    especially when working with imbalanced datasets.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 的 API，例如 `RepeatedStratifiedKFold` 和 `StratifiedKFold`，使用分层概念通过交叉验证评估模型性能，尤其是在处理不平衡数据集时。'
- en: 'Now, let’s train a logistic regression model on training data:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在训练数据上训练一个逻辑回归模型：
- en: '[PRE6]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s get the report metrics from the `sklearn` library:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `sklearn` 库中获取报告指标：
- en: '[PRE7]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This outputs the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下内容：
- en: '[PRE8]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s get the report metrics from `imblearn`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `imblearn` 获取报告指标：
- en: '[PRE9]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This outputs a lot more columns:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出更多列：
- en: '![](img/B17259_01_12.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_01_12.jpg)'
- en: Figure 1.12 – Output of the classification report from imbalanced-learn
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – imbalanced-learn 的分类报告输出
- en: 'Do you notice the extra metrics here compared to the API of `sklearn`? We got
    three additional metrics: `spe` for specificity, `geo` for geometric mean, and
    `iba` for index balanced accuracy.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到这里与 `sklearn` API 相比有额外的指标吗？我们得到了三个额外的指标：`spe` 用于特异性，`geo` 用于几何平均值，`iba`
    用于指数平衡准确率。
- en: The `imblearn.metrics` module has several such functions that can be helpful
    for imbalanced datasets. Apart from `classification_report_imbalanced()`, it offers
    APIs such as `sensitivity_specificity_support()`, `geometric_mean_score()`, `sensitivity_score()`,
    and `specificity_score()`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`imblearn.metrics`模块有几个这样的函数，可以帮助处理不平衡数据集。除了`classification_report_imbalanced()`之外，它还提供了`sensitivity_specificity_support()`、`geometric_mean_score()`、`sensitivity_score()`和`specificity_score()`等API。'
- en: General rules to follow
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要遵循的一般规则
- en: Usually, the first step in any machine learning pipeline should be to split
    the data into train/test/validation sets. We should avoid applying any techniques
    to handle the imbalance until after the data has been split. We should begin by
    splitting the data into training, testing, and validation sets and then proceed
    with any necessary adjustments to the training data. Applying techniques such
    as oversampling (see [*Chapter 2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling
    Methods*) before splitting the data can result in data leakage, overfitting, and
    over-optimism [6].
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，任何机器学习流程的第一步应该是将数据分为训练集、测试集和验证集。我们应该避免在数据分割之后应用任何处理不平衡的技术。我们应该首先将数据分为训练集、测试集和验证集，然后对训练数据进行任何必要的调整。在分割数据之前应用如过采样（见[*第2章*](B17259_02.xhtml#_idTextAnchor042)，*过采样方法*）等技术可能会导致数据泄露、过拟合和过度乐观[6]。
- en: We should ensure that the validation data closely resembles the test data. Both
    validation data and test data should represent real-world scenarios on which the
    model will be used for prediction. Avoid applying any sampling techniques or modifications
    to the validation set. The only requirement is to include a sufficient number
    of samples from all classes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该确保验证数据与测试数据非常相似。验证数据和测试数据都应该代表模型将用于预测的真实世界场景。避免对验证集应用任何采样技术或修改。唯一的要求是包含来自所有类别的足够数量的样本。
- en: Let’s switch to discussing a bit about using unsupervised learning algorithms.
    **Anomaly detection** or **outlier detection** is a class of problems that can
    be used for dealing with imbalanced data problems. Anomalies or outliers are data
    points that deviate significantly from the rest of the data. These anomalies often
    correspond to the minority class in an imbalanced dataset, making unsupervised
    methods potentially useful.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微讨论一下使用无监督学习算法。**异常检测**或**离群值检测**是一类可以用于处理不平衡数据问题的算法。异常或离群值是显著偏离其他数据的点。这些异常通常对应于不平衡数据集中的少数类，使得无监督方法可能很有用。
- en: The term that’s often used for these kinds of problems is **one-class classification**.
    This technique is particularly beneficial when the positive (minority) cases are
    sparse or when gathering them before the training is not feasible. The model is
    trained exclusively on what is considered the “normal” or majority class. It then
    classifies new instances as “normal” or “anomalous,” effectively identifying what
    could be the minority class. This can be especially useful for binary imbalanced
    classification problems, where the majority class is deemed “normal,” and the
    minority class is considered an anomaly.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的常用术语是**单类分类**。当正例（少数类）稀疏或收集它们在训练之前不可行时，这种技术特别有益。模型仅针对被认为是“正常”或多数类的数据进行训练。然后它将新实例分类为“正常”或“异常”，有效地识别可能属于少数类的实例。这对于二元不平衡分类问题特别有用，其中多数类被认为是“正常”，而少数类被认为是异常。
- en: 'However, it does have a drawback: outliers or positive cases during training
    are discarded [7], which could lead to the potential loss of valuable information.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它确实有一个缺点：在训练期间丢弃了离群值或正例[7]，这可能导致潜在的有价值信息的丢失。
- en: In summary, while unsupervised methods such as one-class classification offer
    an alternative for managing class imbalance, our discussion in this book will
    remain centered on supervised learning algorithms. Nevertheless, we recommend
    that you explore and experiment with such solutions when you find them appropriate.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然像单类分类这样的无监督方法为处理类别不平衡提供了一个替代方案，但本书的讨论将始终集中在监督学习算法上。尽管如此，我们建议您在认为适当的时候探索和实验这些解决方案。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Let’s summarize what we’ve learned so far. Imbalanced data is a common problem
    in machine learning, where there are significantly more instances of one class
    than another. Imbalanced datasets can arise from various situations, including
    rare event occurrences, high data collection costs, noisy labels, labeling errors,
    sampling bias, and data cleaning. This can be a challenge for machine learning
    models as they may be biased toward the majority class.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止学到的内容。不平衡数据是机器学习中常见的问题，其中一个类别的实例数量显著多于另一个类别。不平衡数据集可能源于各种情况，包括罕见事件发生、高数据收集成本、标签噪声、标签错误、采样偏差和数据清洗。这可能会对机器学习模型构成挑战，因为它们可能会偏向多数类。
- en: Several techniques can be used to deal with imbalanced data, such as oversampling,
    undersampling, and cost-sensitive learning. The best technique to use depends
    on the specific problem and the data.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用几种技术来处理不平衡数据，例如过采样、欠采样和成本敏感学习。最佳技术取决于具体问题和数据。
- en: In some cases, data imbalance may not be a concern. When the dataset is sufficiently
    large, the impact of data imbalance on the model’s performance may be reduced.
    However, it is still advisable to compare the baseline model’s performance with
    the performance of models that have been built using techniques that address data
    imbalance, such as threshold adjustment, data-based techniques (oversampling and
    undersampling), and algorithm-based techniques.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，数据不平衡可能不是问题。当数据集足够大时，数据不平衡对模型性能的影响可能会降低。然而，仍然建议比较基线模型性能与使用解决数据不平衡的技术（如阈值调整、基于数据的技术（过采样和欠采样）以及基于算法的技术）构建的模型性能。
- en: Traditional performance metrics such as accuracy can fail in imbalanced datasets.
    Some more useful metrics for imbalanced datasets are the ROC curve, the PR curve,
    precision, recall, and F1 score. While ROC curves are suitable for balanced datasets,
    PR curves are more suitable for imbalanced datasets when one class is more important
    than the other.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的性能指标，如准确率，在不平衡数据集中可能会失效。一些更有用的不平衡数据集指标是ROC曲线、PR曲线、精确率、召回率和F1分数。虽然ROC曲线适用于平衡数据集，但PR曲线在某一类比另一类更重要时，更适合不平衡数据集。
- en: The `imbalanced-learn` library is a Python package that offers several techniques
    to deal with data imbalance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`imbalanced-learn`库是一个Python包，提供了处理数据不平衡的几种技术。'
- en: There are some general rules to follow, such as splitting the data into train/test/validation
    sets before applying any techniques to handle the imbalance in the data, ensuring
    that the validation data closely resembles the test data and that test data represents
    the data on which the model will make final predictions, and avoiding applying
    any sampling techniques or modifications to the validation set and test set.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些一般规则需要遵循，例如在应用任何处理数据不平衡的技术之前将数据分为训练/测试/验证集，确保验证数据与测试数据非常相似，并且测试数据代表模型将最终进行预测的数据，以及避免对验证集和测试集应用任何采样技术或修改。
- en: One-class classification or anomaly detection is another technique that can
    be used for dealing with unsupervised imbalanced data problems. In this book,
    we will focus our discussion on supervised learning algorithms only.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一类分类或异常检测是另一种可以用于处理无监督不平衡数据问题的技术。在这本书中，我们将只关注监督学习算法的讨论。
- en: In the next chapter, we will look at one of the common ways to handle the data
    imbalance problem in datasets by applying oversampling techniques.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨通过应用过采样技术来处理数据不平衡问题的一种常见方法。
- en: Questions
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How does the choice of loss function when training a model affect the performance
    of the model on imbalanced datasets?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型时选择损失函数如何影响不平衡数据集上模型的性能？
- en: Can you explain why the PR curve is more informative than the ROC curve when
    dealing with highly skewed datasets?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释为什么PR曲线在处理高度倾斜的数据集时比ROC曲线更有信息量吗？
- en: What are some of the potential issues with using accuracy as a metric for model
    performance on imbalanced datasets?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用准确率作为不平衡数据集上模型性能的指标可能会出现哪些潜在问题？
- en: How does the concept of “class imbalance” affect the process of feature engineering
    in machine learning?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “类别不平衡”的概念如何影响机器学习中的特征工程过程？
- en: In the context of imbalanced datasets, how does the choice of “k” in k-fold
    cross-validation affect the performance of the model? How would you fix the issue?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不平衡数据集的背景下，k折交叉验证中“k”的选择如何影响模型的性能？你将如何解决这个问题？
- en: How does the distribution of classes in the test data affect the PR curve, and
    why? What about the ROC curve?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试数据中类别的分布如何影响PR曲线，为什么？ROC曲线又是如何？
- en: What are the implications of having a high AUC-ROC but a low AUC-PR in the context
    of an imbalanced dataset?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不平衡数据集的背景下，高AUC-ROC但低AUC-PR意味着什么？
- en: How does the concept of “sampling bias” contribute to the challenge of imbalanced
    datasets in machine learning?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “采样偏差”的概念如何有助于解决机器学习中不平衡数据集的挑战？
- en: How does the concept of “labeling errors” contribute to the challenge of imbalanced
    datasets in machine learning?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “标签错误”的概念如何有助于解决机器学习中不平衡数据集的挑战？
- en: What are some of the real-world scenarios where dealing with imbalanced datasets
    is inherently part of the problem?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在哪些现实世界场景中，处理不平衡数据集是问题固有的部分？
- en: '`fetch_dataset` API and then compute the values of MCC, accuracy, precision,
    recall, and F1 score. See if the MCC value can be a useful metric for this dataset.'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `fetch_dataset` API 然后计算MCC、准确率、精确率、召回率和F1分数的值。看看MCC值是否可以成为该数据集的有用指标。
- en: References
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'V. García, R. A. Mollineda, and J. S. Sánchez, *Index of Balanced Accuracy:
    A Performance Measure for Skewed Class Distributions*, in Pattern Recognition
    and Image Analysis, vol. 5524, H. Araujo, A. M. Mendonça, A. J. Pinho, and M.
    I. Torres, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009, pp. 441–448\.
    Accessed: Mar. 18, 2023\. [Online]. Available at [http://link.springer.com/10.1007/978-3-642-02172-5_57](http://link.springer.com/10.1007/978-3-642-02172-5_57).'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: V. García, R. A. Mollineda, 和 J. S. Sánchez, *平衡精度指数：偏斜类别分布的性能度量*, 见《模式识别与图像分析》，第5524卷，H.
    Araujo, A. M. Mendonça, A. J. Pinho, 和 M. I. Torres 编著. 柏林，海德堡：Springer Berlin
    Heidelberg，2009，第441–448页\. 访问时间：2023年3月18日\. [在线]. 可在 [http://link.springer.com/10.1007/978-3-642-02172-5_57](http://link.springer.com/10.1007/978-3-642-02172-5_57)
    获取。
- en: 'T. Fawcett, *An introduction to ROC analysis*, Pattern Recognition Letters,
    vol. 27, no. 8, pp. 861–874, Jun. 2006, doi: 10.1016/j.patrec.2005.10.010.'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'T. Fawcett, *ROC分析的介绍*, 模式识别信函，第27卷，第8期，第861–874页，2006年6月，doi: 10.1016/j.patrec.2005.10.010.'
- en: Y.-A. Le Borgne, W. Siblini, B. Lebichot, and G. Bontempi, *Reproducible Machine
    Learning for Credit Card Fraud Detection - Practical Handbook*. Université Libre
    de Bruxelles, 2022\. [Online]. Available at [https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook).
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y.-A. Le Borgne, W. Siblini, B. Lebichot, 和 G. Bontempi, *可复现的机器学习用于信用卡欺诈检测
    - 实用手册*. 自由布鲁塞尔大学，2022\. [在线]. 可在 [https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook)
    获取。
- en: 'W. Siblini, J. Fréry, L. He-Guelton, F. Oblé, and Y.-Q. Wang, *Master your
    Metrics with Calibration*, vol. 12080, 2020, pp. 457–469\. doi: 10.1007/978-3-030-44584-3_36.'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'W. Siblini, J. Fréry, L. He-Guelton, F. Oblé, 和 Y.-Q. Wang, *通过校准掌握你的指标*，第12080卷，2020，第457–469页\.
    doi: 10.1007/978-3-030-44584-3_36.'
- en: 'Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou, *Exploratory Undersampling for Class-Imbalance
    Learning*, IEEE Trans. Syst., Man, Cybern. B, vol. 39, no. 2, pp. 539–550, Apr.
    2009, doi: 10.1109/TSMCB.2008.2007853.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Xu-Ying Liu, Jianxin Wu, 和 Zhi-Hua Zhou, *探索性下采样用于类别不平衡学习*, IEEE Trans. Syst.,
    Man, Cybern. B, 第39卷，第2期，第539–550页，2009年4月，doi: 10.1109/TSMCB.2008.2007853.'
- en: 'M. S. Santos, J. P. Soares, P. H. Abreu, H. Araujo, and J. Santos, *Cross-Validation
    for Imbalanced Datasets: Avoiding Overoptimistic and Overfitting Approaches [Research
    Frontier]*, IEEE Comput. Intell. Mag., vol. 13, no. 4, pp. 59–76, Nov. 2018, doi:
    10.1109/MCI.2018.2866730.'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'M. S. Santos, J. P. Soares, P. H. Abreu, H. Araujo, 和 J. Santos, *不平衡数据集的交叉验证：避免过度乐观和过度拟合方法
    [研究前沿]*，IEEE Comput. Intell. Mag., 第13卷，第4期，第59–76页，2018年11月，doi: 10.1109/MCI.2018.2866730.'
- en: A. Fernández, S. García, M. Galar, R. Prati, B. Krawczyk, and F. Herrera,
    *Learning from Imbalanced Data Sets*. Springer International Publishing, 2018
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Fernández, S. García, M. Galar, R. Prati, B. Krawczyk, 和 F. Herrera, *从不平衡数据集中学习*.
    Springer 国际出版社，2018
