- en: Neural Networks – Here Comes Deep Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络 – 深度学习的到来
- en: It is not uncommon to read news articles or encounter people who misuse the
    term *deep learning* in place of *machine learning*. This is due to the fact that
    this particular sub-field of machine learning has become very successful at solving
    plenty of previously unsolvable image processing and natural language processing
    problems. This success has caused many to confuse the child field with its parent.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读新闻文章或遇到一些误用*深度学习*这一术语来代替*机器学习*的情况并不罕见。这是因为深度学习作为机器学习的一个子领域，已经在解决许多以前无法解决的图像处理和自然语言处理问题上取得了巨大的成功。这种成功使得许多人将这个子领域与其父领域混淆。
- en: The term *deep learning* refers to deep **Artificial Neural Networks** (**ANNs**).
    The latter concept comes in different forms and shapes. In this chapter, we are
    going to cover one subset of **feedforward neural networks** known as the **Multilayer
    Perceptron** (**MLP**). It is one of the most commonly used types and is implemented
    by scikit-learn. As its name suggests, it is composed of multiple layers, and
    it is a feedforward network as there are no cyclic connections between its layers.
    The more layers there are, the deeper the network is. These deep networks can
    exist in multiple forms, such as **MLP**, **Convolutional Neural Networks** (**CNNs**),
    or **Long Short-Term Memory** (**LSTM**). The latter two are not implemented by
    scikit-learn, yet this will not stop us from discussing the main concepts behind
    CNNs and manually mimicking them using the tools from the scientific Python ecosystem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*这一术语指的是深度**人工神经网络**（**ANNs**）。后者的概念有多种形式和形态。在本章中，我们将讨论一种**前馈神经网络**的子集，称为**多层感知器**（**MLP**）。它是最常用的类型之一，并由scikit-learn实现。顾名思义，它由多层组成，且它是一个前馈网络，因为其层之间没有循环连接。层数越多，网络就越深。这些深度网络可以有多种形式，如**MLP**、**卷积神经网络**（**CNNs**）或**长短期记忆网络**（**LSTM**）。后两者并未由scikit-learn实现，但这并不会妨碍我们讨论CNN的基本概念，并使用科学Python生态系统中的工具手动模拟它们。'
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Getting to know MLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解MLP
- en: Classifying items of clothing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类衣物
- en: Untangling convolutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解开卷积的谜团
- en: MLP regressors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP回归器
- en: Getting to know MLP
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解MLP
- en: 'When learning a new algorithm, you can get discouraged by the number of hyperparameters
    and find it hard to decide where to start. Therefore, I suggest we start by answering
    the following two questions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习一种新的算法时，你可能会因为超参数的数量而感到灰心，并且发现很难决定从哪里开始。因此，我建议我们从回答以下两个问题开始：
- en: How has the algorithm been architected?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法的架构是如何设计的？
- en: How does the algorithm train?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法是如何训练的？
- en: In the following sections, we are going to answer both of these questions and
    learn about the corresponding hyperparameters one by one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将逐一回答这两个问题，并了解相应的超参数。
- en: Understanding the algorithm's architecture
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解算法的架构
- en: 'Luckily, the knowledge we gained about linear models in[Chapter 3](f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml),
    *Making Decisions with Linear Equations*, will give us a good headstart here.
    In brief, linear models can be outlined in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在[第三章](f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml)中获得的关于线性模型的知识，*利用线性方程做决策*，将在这里给我们一个良好的开端。简而言之，线性模型可以在以下图示中概述：
- en: '![](img/360de1e7-1de9-4902-b4f3-5f7ed0243feb.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/360de1e7-1de9-4902-b4f3-5f7ed0243feb.png)'
- en: 'Each of the input features (x[i]) is multiplied by a weight (w[i]), and the
    sum of these products is the output of the model (y). Additionally, we sometimes
    add an extra bias (threshold), along with its weight. Nevertheless, one main problem
    with linear models is that they are in fact linear (duh!). Furthermore, each feature
    gets its own weight, regardless of its neighbors. This simple architecture prevents
    the model from capturing any interactions between its features. So, you can stack
    more layers next to each other, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入特征（x[i]）都会乘以一个权重（w[i]），这些乘积的总和就是模型的输出（y）。此外，我们有时还会添加一个额外的偏置（阈值）及其权重。然而，线性模型的一个主要问题是它们本质上是线性的（显然！）。此外，每个特征都有自己的权重，而不考虑它的邻居。这种简单的架构使得模型无法捕捉到特征之间的任何交互。因此，你可以将更多的层堆叠在一起，如下所示：
- en: '![](img/61f146e5-1b11-4dec-8157-9f62fd99efb5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61f146e5-1b11-4dec-8157-9f62fd99efb5.png)'
- en: 'This sounds like a potential solution; however, based on simple mathematical
    derivations, these combinations of multiplications and summations can still be
    reduced into a single linear equation. It is as if all these layers have no effect
    at all. Therefore, to get to the desired effect, we want to apply non-linear transformations
    after each summation. These non-linear transformations are known as activation
    functions, and they turn the model to a non-linear one. Let''s see where they
    fit into the model, then I will explain further:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来像是一个潜在的解决方案；然而，根据简单的数学推导，这些乘法和加法组合仍然可以简化为一个线性方程。就好像所有这些层根本没有任何效果一样。因此，为了达到预期效果，我们希望在每次加法后应用非线性变换。这些非线性变换被称为激活函数，它们将模型转化为非线性模型。让我们看看它们如何融入模型中，然后我会进一步解释：
- en: '![](img/3468d483-682c-4cbe-b87a-60d41ef47070.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3468d483-682c-4cbe-b87a-60d41ef47070.png)'
- en: 'This model has a single hidden layer with two hidden nodes, which are shown
    inside the box. In practice, you may have multiple hidden layers with a number
    of nodes. The aforementioned activation functions are applied at the outputs of
    the hidden nodes. Here, we used a **R****ectified Linear Unit** (**ReLU**), it
    is an activation function; for the negative values, it returns `0`, and it keeps
    the positive values unchanged. In addition to the `relu`function, `identity`,
    `logistic`, and `tanh` activation functions are also supported for the hidden
    layers, and they are set using the `activation` hyperparameter. Here is how each
    of these four activation functions look:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有一个包含两个隐藏节点的单一隐藏层，如框内所示。在实际应用中，你可能会有多个隐藏层和多个节点。前述的激活函数应用于隐藏节点的输出。这里，我们使用了**修正线性单元**（**ReLU**）作为激活函数；对于负值，它返回`0`，而对正值则保持不变。除了`relu`函数，`identity`、`logistic`和`tanh`激活函数也支持用于隐藏层，并且可以通过`activation`超参数进行设置。以下是这四种激活函数的表现形式：
- en: '![](img/d9b355fc-cc60-4a52-bf77-ce13c309b88d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9b355fc-cc60-4a52-bf77-ce13c309b88d.png)'
- en: As mentioned earlier, since the `identity` function keeps its inputs untouched
    without any non-linear transformations, it is seldom used as it will end up reducing
    the model into a simple linear one. It is also cursed by having a constant gradient,
    which is not very helpful for the gradient descent algorithm used for training.
    Therefore, the `relu` function is usually a good non-linear alternative. It is
    the current default setting and is a good first choice; the `logistic` or the
    `tanh` activation functions are the next alternative options.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，由于`identity`函数不会对其输入进行任何非线性变换，因此很少使用，因为它最终会将模型简化为一个线性模型。它还存在着梯度恒定的问题，这对用于训练的梯度下降算法帮助不大。因此，`relu`函数通常是一个不错的非线性替代方案。它是当前的默认设置，也是一个不错的首选；`logistic`或`tanh`激活函数则是下一个可选方案。
- en: 'The output layer also has its own activation function, but it serves a different
    purpose. If you recall from [Chapter 3](f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml),
    *Making Decisions with Linear Equations,* we used the `logistic` function to turn
    a linear regression into a classifier—that is, logistic regression. The output''s
    activation function serves the exact same purpose here as well. This list has
    the possible output activation functions and their corresponding use cases:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层也有其自己的激活函数，但它起着不同的作用。如果你还记得[第3章](f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml)，《使用线性方程做决策》，我们使用`logistic`函数将线性回归转变为分类器——也就是逻辑回归。输出的激活函数在这里起着完全相同的作用。下面列出了可能的输出激活函数及其对应的应用场景：
- en: '**Identity function**: Set when doing regression using `MLPRegressor`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Identity函数**：在使用`MLPRegressor`进行回归时设置'
- en: '**Logistic function**: Set when performing a binary classification using `MLPClassifier`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Logistic函数**：在使用`MLPClassifier`进行二分类时设置'
- en: '**Softmax function**:**Set when using `MLPClassifier`to differentiate between
    three or more classes**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax函数**：**在使用`MLPClassifier`区分三类或更多类别时设置**'
- en: '**We do not set the output activation functions by hand; they are automatically
    chosen based on whether`MLPRegressor` or`MLPClassifier`is*used and on the number
    of classes available for the latter to classify.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们不手动设置输出激活函数；它们会根据是否使用`MLPRegressor`或`MLPClassifier`以及后者用于分类的类别数自动选择。**'
- en: '*If we look at the network architecture, it is clear that another important
    hyperparameter to set is the number of hidden layers and the number of nodes in
    each layer. This is set using the `hidden_layer_sizes`*hyperparameter, which accepts
    tuples. To achieve the architecture in the previous figure—that is, one hidden
    layer with two nodes—we will set `hidden_layer_sizes` to `2`. Setting it to `(10,
    10, 5)` gives us three hidden layers; the first two have 10 nodes each, while
    the third one has 5 nodes.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果我们看一下网络架构，很明显，另一个需要设置的重要超参数是隐藏层的数量以及每层的节点数。这个设置通过`hidden_layer_sizes`超参数来完成，它接受元组类型的值。为了实现前面图中的架构——也就是一个隐藏层，包含两个节点——我们将`hidden_layer_sizes`设置为`2`。将其设置为`(10,
    10, 5)`则表示有三个隐藏层，前两个层每层包含10个节点，而第三层包含5个节点。*'
- en: '*## Training the neural network'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*## 训练神经网络'
- en: '"Psychologists tell us that in order to learn from experience, two ingredients
    are necessary: frequent practice and immediate feedback."'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '"心理学家告诉我们，要从经验中学习，必须具备两个要素：频繁的练习和即时的反馈。"'
- en: – Richard Thaler
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: – 理查德·塞勒
- en: 'A big chunk of researchers'' time is spent on improving how their neural networks
    train. This is also reflected in the number of hyperparameters related to the
    training algorithms used. To better understand these hyperparameters, we need
    to examine the following training workflow:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究人员的时间花费在改进他们神经网络的训练上。这也反映在与训练算法相关的超参数数量上。为了更好地理解这些超参数，我们需要研究以下的训练工作流程：
- en: Get a subset of the training samples.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练样本的子集。
- en: Run them through the network and make predictions.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据通过网络，进行预测。
- en: Calculate the training loss by comparing the actual values and predictions.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过比较实际值和预测值来计算训练损失。
- en: Use the calculated loss to update the network weights.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计算出的损失来更新网络权重。
- en: Return to *step 1* to get more samples, and if all the samples are already used,
    go through the training data over and over until the training process converges.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到*步骤1*获取更多样本，如果所有样本都已使用完，则反复遍历训练数据，直到训练过程收敛。
- en: Going through these steps one by one, you can seethe need to set the size of
    the training subset at the first stage. This is what the `batch_size`parameter
    sets. As we will see in a bit, you can go from using one sample at a time to using
    the entire training set all at once to anything in between. The first and second
    steps are straightforward, but the third step dictates that we should know which
    loss function to use. As for the available loss functions, we do not have much
    choice when working with scikit-learn. A**log loss function**is selected for us
    when performing classifications and**mean squared error**is what is available
    for regression. The fourth step is the trickiest part with the most of hyperparameters
    to set. We calculate the gradient of the loss function with respect to the network
    weights.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步执行这些步骤，你可以看到在第一阶段需要设置训练子集的大小。这就是`batch_size`参数所设置的内容。正如我们稍后会看到的，你可以从一次使用一个样本，到一次使用整个训练集，再到介于两者之间的任何方式。第一步和第二步是直接的，但第三步要求我们知道应该使用哪种损失函数。至于可用的损失函数，当使用scikit-learn时，我们没有太多选择。在进行分类时，**对数损失函数**会自动为我们选择，而**均方误差**则是回归任务的默认选择。第四步是最棘手的部分，需要设置最多的超参数。我们计算损失函数相对于网络权重的梯度。
- en: This gradient tells us the direction to move toward to decrease the loss function.
    In other words, we use the gradient to update the weights in the hope that we
    can iteratively decrease the loss function to its minimum. The logic responsible
    for this operation is known as the solver. Solvers deserve their own separate
    section, though, which will come in a bit. Finally, the number of times we go
    through the training data over and over is called epochs and is set using the
    `max_iter`hyperparameter. We also may decide to stop earlier (`early_stopping`)
    if the model is not learning any more. The`validation_fraction`, `n_iter_no_change`,
    and `tol`hyperparameters help us decide when to stop. More on how they work in
    the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个梯度告诉我们应该朝哪个方向移动，以减少损失函数。换句话说，我们利用梯度更新权重，希望通过迭代降低损失函数至最小值。负责这一操作的逻辑被称为求解器（solver）。不过，求解器值得单独一节，稍后会详细介绍。最后，我们多次遍历训练数据的次数被称为“迭代次数”（epochs），它通过`max_iter`超参数来设置。如果模型停止学习，我们也可以决定提前停止（`early_stopping`）。`validation_fraction`、`n_iter_no_change`和`tol`这些超参数帮助我们决定何时停止训练。更多关于它们如何工作的内容将在下一节讨论。
- en: Configuring the solvers
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置求解器。
- en: 'After calculating the loss function (also known as the cost or objective function),
    we need to find the optimum network weights that minimize the loss function. In
    the linear models from [Chapter 3](f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml),
    *Making Decisions with Linear Equations*, the loss functions were chosen to be
    convex. A convex function, as seen in the following figure, has one minimum, which
    is both its global minimum as well as its local one. This simplifies the solvers''
    job when trying to optimize this function. In the case of non-linear neural networks,
    the loss function is typically non-convex, which requires extra care during training,
    hence more attention is given to the solvers here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失函数（也称为成本函数或目标函数）后，我们需要找到能够最小化损失函数的最优网络权重。在[第3章](f5590b35-517b-42bb-821f-66d4fdc8059a.xhtml)的线性模型中，*使用线性方程做决策*，损失函数被选择为凸函数。正如下面的图形所示，凸函数有一个最小值，这个最小值既是全局最小值也是局部最小值。这使得在优化该函数时，求解器的工作变得简单。对于非线性神经网络，损失函数通常是非凸的，这就需要在训练过程中更加小心，因此在这里给予求解器更多的关注：
- en: '![](img/a6203d0d-adb7-4a27-a8d4-b9b8619b9ce4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6203d0d-adb7-4a27-a8d4-b9b8619b9ce4.png)'
- en: The supported solvers for MLP can be grouped into **Limited Memory****Broyden–Fletcher–Goldfarb–Shanno**
    (**LBFGS**) and **gradient descent** (**Stochastic Gradient Descent** (**SGD**)
    and**Adam). In both variants, we want to pick a random point on the loss function,
    calculate its slope (gradient), and use it to figure out in which direction we
    should move next. Remember that in reality, we are dealing with way higher dimensions
    than the two-dimensional graphs shown here. Furthermore, we cannot usually see
    the entire graph as we can now:**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的支持求解器可以分为**有限记忆**Broyden–Fletcher–Goldfarb–Shanno（**LBFGS**）和**梯度下降**（**随机梯度下降**（**SGD**）和**Adam**）。在这两种变体中，我们希望从损失函数中随机选择一个点，计算其斜率（梯度），并使用它来确定下一步应该朝哪个方向移动。请记住，在实际情况中，我们处理的维度远远超过这里展示的二维图形。此外，我们通常无法像现在这样看到整个图形：
- en: '***   The **LBFGS** algorithm uses both the slope (first derivative) and the
    rate of change of the slope (second derivative), which helps in providing better
    coverage; however, it doesn''t scale well with the size of the training data.
    It can be very slow to train, and so this algorithm is recommended for smaller
    datasets, unless more powerful concurrent machines come to the rescue.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **LBFGS**算法同时使用斜率（一阶导数）和斜率变化率（二阶导数），这有助于提供更好的覆盖；然而，它在训练数据规模较大时表现不佳。训练可能非常缓慢，因此推荐在数据集较小的情况下使用该算法，除非有更强大的并行计算机来帮助解决。'
- en: The **gradient descent****algorithm relies on the first derivative only. So,
    more effort is needed to help it move effectively. The calculated gradient is
    combined with `learning_rate`. This controls how much it moves each time after
    calculating the gradient. Moving too quickly may result in overshooting and missing
    the local minimum, while moving too slowly may cause the algorithm not to converge
    soon enough. We start our quest with a rate defined by `learning_rate_init`. If
    we set `learning_rate='constant'`, the initial rate is kept unchanged throughout
    the training process. Otherwise, we can set the rate to decrease with each step
    (in scaling) or to only decrease whenever the model is not able to learn enough
    anymore (adaptive).**
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降**算法仅依赖于一阶导数。因此，需要更多的努力来帮助它有效地移动。计算出的梯度与`learning_rate`结合。这控制了每次计算梯度后，它的移动步长。移动过快可能会导致超过最小值并错过局部最小值，而移动过慢可能导致算法无法及时收敛。我们从由`learning_rate_init`定义的速率开始。如果我们设置`learning_rate=''constant''`，初始速率将在整个训练过程中保持不变。否则，我们可以设置速率在每一步中减少（按比例缩放），或者仅在模型无法再继续学习时才减少（自适应）。'
- en: '***   **Gradient descent**can use the entire training data to calculate the
    gradient, use a single sample at a time (`sgd`), or consume the data in small
    subsets (mini-batch gradient descent). These choices are controlled by `batch_size`.
    Having a dataset that cannot fit into memory may prevent us from using the entire
    dataset at once, while using small batches may cause the loss function to fluctuate.
    We will see this effect in practice in the next section.*   The problem with the
    learning rate is that it doesn''t adapt to the shape of the curve, especially
    as we are only using the first derivative here. We want to control the learning
    speed, depending on how steep the curve at our feet is. One notable adjustment
    to make the learning process smarter is the concept of `momentum`. This adapts
    the learning process based on current and previous updates. The `momentum` is
    enabled for the `sgd` solver by default, and its magnitude can be set using the
    `momentum`hyperparameter. The`adam`solver incorporates this concept and combines
    it with the ability to compute separate learning rates for each one of the network
    weights. It is parameterized by `beta_1`and`beta_2`. They are usually kept at
    their default values of `0.9` and `0.999`, respectively. The `adam`solver is the
    default solver since it requires fewer tuning efforts compared to the `sgd` solver.
    Nevertheless, the `sgd` solver can converge to better solutions if tuned correctly.*   Finally,
    deciding when to stop the training process is another essential decision to make.
    We loop over the data more than once, bounded by the `max_iter` setting. Yet,
    we can stop before `max_iter` is reached if we feel that we aren''t learning enough.
    We define how much learning is enough using `tol`, then we can stop the training
    process right away or give it a few more chances (`n_iter_no_change`) before we
    decide to stop it. Furthermore, we can set a separate fraction of the training
    set aside (`validation_fraction`) and use it to evaluate our learning process
    better. Then, if we set `early_stopping =True`, the training process will stop
    once the improvement for the validation set does not meet the `tol`threshold for**`n_iter_no_change`epochs.****'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **梯度下降**可以使用整个训练数据集计算梯度，使用每次一个样本（`sgd`），或者以小批量的方式消耗数据（小批量梯度下降）。这些选择由`batch_size`控制。如果数据集无法完全加载到内存中，可能会阻止我们一次性使用整个数据集，而使用小批量可能会导致损失函数波动。我们将在接下来的部分中实际看到这种效果。*   学习率的问题在于它不能适应曲线的形状，特别是我们这里只使用了一阶导数。我们希望根据当前曲线的陡峭程度来控制学习速度。使学习过程更智能的一个显著调整是`动量`的概念。它根据当前和以前的更新来调整学习过程。`sgd`求解器默认启用`动量`，并且其大小可以通过`momentum`超参数进行设置。`adam`求解器将这一概念结合，并与为每个网络权重计算独立学习率的能力结合在一起。它通过`beta_1`和`beta_2`来参数化。通常它们的默认值分别为`0.9`和`0.999`。由于`adam`求解器相比`sgd`求解器需要更少的调整工作，因此它是默认的求解器。然而，如果正确调整，`sgd`求解器也可以收敛到更好的解。*   最后，决定何时停止训练过程是另一个重要的决策。我们会多次遍历数据，直到达到`max_iter`设置的上限。然而，如果我们认为学习进展不足，可以提前停止。我们通过`tol`定义多少学习是足够的，然后可以立即停止训练过程，或者再给它一些机会（`n_iter_no_change`），然后决定是否停止。此外，我们可以将训练集的一部分单独留出（`validation_fraction`），用来更好地评估我们的学习过程。然后，如果我们设置`early_stopping
    = True`，训练过程将在验证集的改进未达到`tol`阈值并且已达到`n_iter_no_change`个周期时停止。****'
- en: '****Now that we have a good high-level picture of how things work, I feel the
    best way forward is to put all these hyperparameters into practice and see their
    effect on real data. In the next section, we will load an image dataset and use
    it to learn more about the aforementioned hyperparameters.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '****现在我们对事情如何运作有了一个高层次的了解，我认为最好的前进方式是将所有这些超参数付诸实践，并观察它们在真实数据上的效果。在接下来的部分中，我们将加载一个图像数据集，并利用它来进一步了解前述的超参数。'
- en: Classifying items of clothing
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类服装项
- en: 'In this section, we are going to classify clothing items based on their images.
    We are going to use a dataset release by Zalando. Zalando is an e-commerce website
    based in Berlin. They released a dataset of 70,000 pictures of clothing items,
    along with their labels. Each item belongs to one of the following 10 labels:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将根据衣物图像对服装项进行分类。我们将使用 Zalando 发布的一个数据集。Zalando 是一家总部位于柏林的电子商务网站。他们发布了一个包含
    70,000 张服装图片的数据集，并附有相应标签。每个服装项都属于以下 10 个标签之一：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The data is published on the OpenML platform, so we can easily download it using
    the built-in downloader in scikit-learn.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据已发布在 OpenML 平台上，因此我们可以通过 scikit-learn 中的内置下载器轻松下载它。
- en: Downloading the Fashion-MNIST dataset
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载Fashion-MNIST数据集
- en: 'Each dataset on the OpenML platform has a specific ID. We can give this ID
    to`fetch_openml()`to download the required dataset, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: OpenML平台上的每个数据集都有一个特定的ID。我们可以将这个ID传递给`fetch_openml()`来下载所需的数据集，代码如下：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The class labels are given as numbers. To extract their names, we can parse
    the following line from the description, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 类别标签以数字形式给出。为了提取它们的名称，我们可以从描述中解析出以下内容：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also create a function similar to the one we created in [Chapter 5](b95b628d-5913-477e-8897-989ce2afb974.xhtml),
    *Image Processing with Nearest Neighbors,* to display the images in the dataset:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以创建一个类似于我们在[第5章](b95b628d-5913-477e-8897-989ce2afb974.xhtml)中创建的函数，*使用最近邻的图像处理*，来显示数据集中的图片：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The previous function expects an image and a target label in addition to the
    `matplotlib` axis to display the image on. We are going to see how to use it in
    the upcoming sections.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数除了`matplotlib`坐标轴外，还期望接收一张图片和一个目标标签来显示该图片。我们将在接下来的章节中看到如何使用它。
- en: Preparing the data for classification
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备分类数据
- en: When developing a model and optimizing its hyperparameters, you will need to
    run it over and over multiple times. Therefore, it is advised that you start working
    with a smaller dataset to minimize the training time. Once you reach an acceptable
    model, you can then add more data and do your final hyperparameter-tuning. Later
    on, we will see how to tell whether the data at hand is enough and whether more
    samples are needed; but for now, let's stick to a subset of 10,000 images.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发模型并优化其超参数时，你需要多次运行模型。因此，建议你先使用较小的数据集以减少训练时间。一旦达到可接受的模型效果，就可以添加更多数据并进行最终的超参数调优。稍后，我们将看到如何判断手头的数据是否足够，以及是否需要更多样本；但现在，让我们先使用一个包含10,000张图片的子集。
- en: I deliberately avoided setting any random states when sampling from the original
    dataset and when splitting the sampled data into a training and a test set. By
    not setting a random state, you should expect the final results to vary from one
    run to the other. I made this choice since my main objective here is to focus
    on the underlying concepts, and I did not want you to obsess over the final results.
    In the end, the data you will deal with in real-life scenarios will vary from
    one problem to the other, and we have already learned in previous chapters how
    to better understand the boundaries of our model's performance via cross-validation.
    So, in this chapter, as in many other chapters in this book, don't worry too much
    if the mentioned model's accuracy numbers, coefficients, or learning behavior
    vary slightly from yours.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意避免在从原始数据集进行采样时以及将采样数据拆分为训练集和测试集时设置任何随机状态。由于没有设置随机状态，你应该期望最终结果在每次运行中有所不同。我做出这个选择是因为我的主要目标是专注于底层概念，而不希望你过于纠结最终结果。最终，你在现实场景中处理的数据会因问题的不同而有所不同，我们在前面的章节中已经学会了如何通过交叉验证更好地理解模型性能的边界。所以，在这一章中，和本书中的许多其他章节一样，不必太担心提到的模型的准确率、系数或学习行为与你的结果有所不同。
- en: 'We will use the`train_test_split()` function twice. Initially, we will use
    it for sampling. Afterward, we will reuse it for its designated purpose of splitting
    the data into training and test sets:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`train_test_split()`函数两次。最初，我们将用它进行采样。之后，我们将再次使用它来执行将数据拆分为训练集和测试集的任务：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The pixels here take values between `0` and `255`. Usually, this is fine; however,
    the solvers we will use converge better when the data is put into tighter ranges.`MinMaxScaler`
    is going to help us achieve this, as can be seen in the following code, whereas
    `StandardScaler` is also an option:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的像素值在`0`和`255`之间。通常，这样是可以的；然而，我们将要使用的求解器在数据落在更紧凑的范围内时收敛得更好。`MinMaxScaler`将帮助我们实现这一点，如下所示，而`StandardScaler`也是一个选择：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now translate the numerical labels into names using the function we
    created in the previous section:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用我们在上一节中创建的函数，将数字标签转换为名称：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If your original labels came in as strings, you can use `LabelEncoder` to convert
    them into numerical values:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的原始标签是字符串格式，可以使用`LabelEncoder`将其转换为数值：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, let''s use the following code to see how the images look:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用以下代码查看这些图片的样子：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we see 10 random images alongside their labels. We loop over 10 random
    images and use the display function we created earlier to display them next to
    each other:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到10张随机图片及其标签。我们循环显示10张随机图片，并使用我们之前创建的显示函数将它们并排展示：
- en: '![](img/80f63e13-3b73-4a80-acc6-045da971f865.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80f63e13-3b73-4a80-acc6-045da971f865.png)'
- en: Now that the data is ready, it is time to see the effect of the hyperparameters
    in practice.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经准备好，接下来是时候看看超参数在实践中的效果了。
- en: Experiencing the effects of the hyperparameters
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 体验超参数的效果
- en: After the neural network is trained, you can check its weights (`coefs_`), intercepts
    (`intercepts_`), and the final value of the loss function (`loss_`). One additional
    piece of information is the computed loss after each epoch (`loss_curve_`). This
    trace of calculated losses is very useful for the learning process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练完成后，你可以检查它的权重（`coefs_`）、截距（`intercepts_`）以及损失函数的最终值（`loss_`）。另外一个有用的信息是每次迭代后的计算损失（`loss_curve_`）。这一损失曲线对于学习过程非常有帮助。
- en: 'Here, we train a neural network with two hidden layers of 100 nodes each, and
    we set the maximum number of epoch to `500`. We leave all the other hyperparameters
    to their default values for now:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们训练了一个神经网络，包含两个隐藏层，每个层有100个节点，并将最大迭代次数设置为`500`。目前，我们将其他所有超参数保持默认值：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After the network is trained, we can plot the loss curve using the following
    line of code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 网络训练完成后，我们可以使用以下代码绘制损失曲线：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us the following graph:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们如下图：
- en: '![](img/546e0a22-d910-4e30-920a-e1bd13a308ef.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/546e0a22-d910-4e30-920a-e1bd13a308ef.png)'
- en: Despite the fact that the algorithm was told to continue learning for up to
    `500` epochs, it stopped after the 107^(th) epoch. The default value for `n_iter_no_change`
    is `10` epochs. This means that the learning rate was not improving enough since
    the 97^(th) epoch, and so the network came to a halt 10 epochs later. Keep in
    mind that `early_stopping` is set to `False` by default, which means that this
    decision was made regardless of the `10%` validation set that was set aside by
    default. If we want to use the validation set for the early stopping decision,
    we should set `early_stopping=True`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管算法被告知最多继续学习`500`个周期，但它在第107^(次)周期后停止了。`n_iter_no_change`的默认值是`10`个周期。这意味着，自第97^(次)周期以来，学习率没有足够改善，因此网络在10个周期后停了下来。请记住，默认情况下`early_stopping`是`False`，这意味着这个决策是在不考虑默认设置的`10%`验证集的情况下做出的。如果我们希望使用验证集来决定是否提前停止，我们应该将`early_stopping`设置为`True`。
- en: Learning not too quickly and not too slowly
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习得不太快也不太慢
- en: 'As mentioned earlier, the gradient of the loss function (*J*) with respect
    to the weights (*w*) is used to update the network''s weights. The updates are
    done according to the following equation, where *lr* is the learning rate:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，损失函数（*J*）相对于权重（*w*）的梯度被用来更新网络的权重。更新是按照以下方程进行的，其中*lr*是学习率：
- en: '![](img/2443e935-17a8-4772-b7b0-f37c0e36e430.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2443e935-17a8-4772-b7b0-f37c0e36e430.png)'
- en: You might wonder about the need for a learning rate; why don't we just use the
    gradient as it is by setting *lr = 1*? In this section, we are going to answer
    this question by witnessing the effect of the learning rate on the training process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么需要学习率？为什么不直接通过设置*lr = 1*来使用梯度呢？在这一节中，我们将通过观察学习率对训练过程的影响来回答这个问题。
- en: Another hidden gem in the MLP estimator is`validation_scores_`. Like `loss_curve_`,
    this one is also not documented, and its interface may change with future releases.
    In the case of `MLPClassifier`, `validation_scores_` keeps track of the classifier's
    accuracy on the validation set, whereas for`MLPRegressor`, it keeps track of the
    regressor's R² score instead.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: MLP估算器中的另一个隐藏的宝藏是`validation_scores_`。像`loss_curve_`一样，这个参数也没有文档说明，并且其接口可能在未来的版本中发生变化。在`MLPClassifier`中，`validation_scores_`跟踪分类器在验证集上的准确度，而在`MLPRegressor`中，它跟踪回归器的R²得分。
- en: We are going to use the validation score (`validation_scores_`) to see the effect
    of the different learning rates. Since these scores are stored only when `early_stopping`
    is set to `True` and we do not want to stop early, we will also set `n_iter_no_change`to
    be the same value as`max_iter`to cancel the early stopping effect.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用验证得分（`validation_scores_`）来查看不同学习率的效果。由于这些得分只有在`early_stopping`设置为`True`时才会存储，而且我们不想提前停止，所以我们还将`n_iter_no_change`设置为与`max_iter`相同的值，以取消提前停止的效果。
- en: 'The default learning rate is`0.001`, and it stays constant during the training
    process by default. Here, we are going to take an even smaller subset of the training
    data—1,000 samples—and try different learning rates from`0.0001`to`1`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的学习率是`0.001`，并且在训练过程中默认保持不变。在这里，我们将选择一个更小的训练数据子集——1,000个样本——并尝试从`0.0001`到`1`的不同学习率：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following graphs compare the progress of the validation scores for the
    different learning rates. The code used for formatting the axes was omitted for
    brevity:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表比较了不同学习率下验证得分的进展。为了简洁起见，格式化坐标轴的代码被省略：
- en: '![](img/2c4f4fc2-45f2-4095-9a26-86b990738610.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c4f4fc2-45f2-4095-9a26-86b990738610.png)'
- en: As we can see, when setting the learning rate to `1`, the network wasn't able
    to learn and the accuracy score was stuck around 10%. This is because the bigger
    steps taken to update the weights caused the gradient descent to overshoot and
    miss the local minima. Ideally, we want the gradient descent to move wisely over
    the curve; it shouldn't rush and miss the optimum solutions. On the other hand,
    we can see that a very slow learning rate, `0.0001`, caused the network to take
    forever to train. It's clear that `120` epochs wasn't enough, then, and more epochs
    were needed. For this example, a learning rate of `0.01` looks like a good balance.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，当将学习率设置为`1`时，网络无法学习，准确度停留在约10%。这是因为较大的步伐更新权重导致梯度下降过度，错过了局部最小值。理想情况下，我们希望梯度下降能够在曲线上智慧地移动；它不应该急于求成，错过最优解。另一方面，我们可以看到，学习率非常低的`0.0001`导致网络训练时间过长。显然，`120`轮训练不够，因此需要更多的轮次。在这个例子中，学习率为`0.01`看起来是一个不错的平衡。
- en: The concept of learning rate is commonly used in iterative methods to prevent
    overshooting. It might have different names and different justifications, but
    in essence, it serves the same purpose. For example, in the **reinforcement learning**
    field, the **discount factor** in the **Bellman equation** might resemble the
    learning rate here.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的概念通常在迭代方法中使用，以防止过度跳跃。它可能有不同的名称和不同的解释，但本质上它起到相同的作用。例如，在**强化学习**领域，**贝尔曼方程**中的**折扣因子**可能类似于这里的学习率。
- en: Picking a suitable batch size
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的批量大小
- en: 'When dealing with massive training data, you don''t want to use it all at once
    when calculating the gradient, especially when it is not possible to fit this
    data in memory. Using the data in small subsets is something we can configure.
    Here, we are going to try different batch sizes while keeping everything else
    constant. Keep in mind that with`batch_size`set to `1`, the model is going to
    be very slow as it updates its weights after each training instance:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大量训练数据时，你不希望在计算梯度时一次性使用所有数据，尤其是当无法将这些数据完全加载到内存时。使用数据的小子集是我们可以配置的选项。在这里，我们将尝试不同的批量大小，同时保持其他设置不变。请记住，当`batch_size`设置为`1`时，模型会非常慢，因为它在每个训练实例后都更新一次权重：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This figure gives us a visual comparison between the four batch size settings
    and their effects. Parts of the formatting code were omitted for brevity:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图给我们提供了四种批量大小设置及其效果的可视化比较。为了简洁起见，部分格式化代码被省略：
- en: '![](img/6ade3c82-da3d-46fa-9ecb-48a266a7efa1.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ade3c82-da3d-46fa-9ecb-48a266a7efa1.png)'
- en: You can see why the use of mini-batch gradient descent is becoming the norm
    among practitioners, not only because of the memory constraints but also because
    smaller batches helped our model here learn better. This final outcome was achieved
    despite the higher fluctuations in the validation scores for the smaller batch
    sizes. On the other hand, setting `batch_size` to `1` slows down the learning
    process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，为什么小批量梯度下降在实践中成为了常态，不仅是因为内存限制，还因为较小的批次帮助我们的模型在此处更好地学习。尽管小批次大小下验证得分的波动较大，最终的结果还是达到了预期。另一方面，将`batch_size`设置为`1`会减慢学习过程。
- en: 'So far, we have tweaked multiple hyperparameters and witnessed their effects
    on the training process. In addition to these hyperparameters, two additional
    questions are still waiting for answers:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经调整了多个超参数，并见证了它们对训练过程的影响。除了这些超参数，还有两个问题仍然需要回答：
- en: How many training samples are enough?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多少训练样本足够？
- en: How many epochs are enough?
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多少轮训练足够？
- en: Checking whether more training samples are needed
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查是否需要更多的训练样本
- en: 'We want to compare when the entire training sample (100%) is used when 75%,
    50%, 25%, 10%, and 5% of it is used. The`learning_curve`function is useful for
    this comparison. It uses cross-validation to calculate the average training and
    test scores for the different sample sizes. Here, we are going to define the different
    sampling ratios and specify that three-fold cross-validation is needed:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望比较当使用整个训练样本（100%）时，使用75%、50%、25%、10%和5%的效果。`learning_curve`函数在这种比较中很有用。它使用交叉验证来计算不同样本量下的平均训练和测试分数。在这里，我们将定义不同的采样比例，并指定需要三折交叉验证：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When done, we can use the following code to plot the progress of the training
    and test scores with an increase in the sample size:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以使用以下代码绘制训练和测试分数随着样本量增加的进展：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The resulting graphs show the increase in the classifier''s accuracy with more
    training data. Notice how the training score is constant, while the test score
    is what we really care about, and it seems to saturate after a certain amount
    of data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示了随着更多训练数据的增加，分类器准确度的提升。注意到训练分数保持不变，而测试分数才是我们真正关心的，它在一定数据量后似乎趋于饱和：
- en: '![](img/556e220c-917d-4989-8626-32b5f245f67a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/556e220c-917d-4989-8626-32b5f245f67a.png)'
- en: Earlier in this chapter, we took a sample of 10,000 images out of the original
    70,000 images. We then split it into 8,000 for training and 2,000 for testing.
    From the learning curve graph, we can see that it is possible to settle for an
    even smaller training set. Somewhere after 2,000 the additional samples don't
    add much value.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，我们从原始的70,000张图片中抽取了10,000张样本。然后将其拆分为8,000张用于训练，2,000张用于测试。从学习曲线图中我们可以看到，实际上可以选择一个更小的训练集。在2,000张样本之后，额外的样本并没有带来太大的价值。
- en: Usually, we want to use as many data samples as we have to train our models.
    Nevertheless, when tuning the model's hyperparameters, you need to compromise
    and use a smaller sample to speed up the development process. Once that's done,
    it is then advised to train the final model on the entire dataset.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望使用尽可能多的数据样本来训练我们的模型。然而，在调整模型超参数时，我们需要做出妥协，使用较小的样本来加速开发过程。一旦完成这些步骤，就建议在整个数据集上训练最终模型。
- en: Checking whether more epochs are needed
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查是否需要更多的训练轮次
- en: 'This time, we are going to use the `validation_curve` function. It works in
    a similar fashion to the `learning_curve` function, but rather than comparing
    the different training sample sizes, it compares the different hyperparameter
    settings. Here, we will see the effect of using different values for `max_iter`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们将使用`validation_curve`函数。它的工作原理类似于`learning_curve`函数，但它比较的是不同的超参数设置，而不是不同的训练样本量。在这里，我们将看到使用不同`max_iter`值的效果：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With the training and test scores, we can plot them as we did in the previous
    section to get the following graph:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练和测试分数，我们可以像在上一节中一样绘制它们，从而得到以下图表：
- en: '![](img/58a6a67e-9f20-48a1-b3d2-e96329b83132.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58a6a67e-9f20-48a1-b3d2-e96329b83132.png)'
- en: In this example, we can see that the test score stopped improving roughly after
    `25` epochs. The training score continued to improve beyond that until it reached
    100%, which is a symptom of overfitting. In practice, we may not need this graph
    as we use the `early_stopping`, `tol`, and `n_iter_no_change` hyperparameters
    to stop the training process once we have learned enough and before we overfit.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们可以看到，测试分数大约在`25`轮后停止提高。训练分数在此之后继续提升，直到达到100%，这是过拟合的表现。实际上，我们可能不需要这个图表，因为我们使用`early_stopping`、`tol`和`n_iter_no_change`超参数来停止训练过程，一旦学习足够并且避免过拟合。
- en: Choosing the optimum architecture and hyperparameters
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择最佳的架构和超参数
- en: So far, we haven't talked about the network architecture. How many layers should
    we have and how many nodes should we put in each layer? We also haven't compared
    the different activation functions. As you can see, there are plenty of hyperparameters
    to choose from. Previously in this book, we mentioned tools such as `GridSearchCV`
    and `RandomizedSearchCV` that help you pick the best hyperparameters. These are
    still good tools to use, but they can be too slow if we decide to use them to
    tune every possible value for every parameter we have. They can also become too
    slow when used with too many training samples or for too many epochs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有讨论网络架构。我们应该有多少层，每层应该有多少节点？我们也没有比较不同的激活函数。正如你所看到的，有许多超参数可以选择。在本书之前的部分，我们提到过一些工具，如`GridSearchCV`和`RandomizedSearchCV`，它们帮助你选择最佳超参数。这些仍然是很好的工具，但如果我们决定使用它们来调节每个参数的所有可能值，它们可能会太慢。如果我们在使用过多的训练样本或进行太多的训练轮次时，它们也可能变得过于缓慢。
- en: The tools we have seen in the previous sections should help us find our needle
    in a slightly smaller haystack by ruling out some hyperparameter ranges. They
    will also allow us to stick to smaller datasets and for shorter training times.
    Then, we can effectively use `GridSearchCV` and`RandomizedSearchCV`to fine-tune
    our neural network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面部分看到的工具应该能帮助我们通过排除一些超参数范围，在一个稍微小一点的“大堆”中找到我们的“针”。它们还将允许我们坚持使用更小的数据集并缩短训练时间。然后，我们可以有效地使用`GridSearchCV`和`RandomizedSearchCV`来微调我们的神经网络。
- en: Parallelism is also advised where possible. `GridSearchCV` and**`RandomizedSearchCV`
    allow us to use the different processors on our machines to train multiple models
    at the same time. We can achieve that via the `n_jobs` setting. This means that
    you can significantly speed up the hyperparameter-tuning process by using machines
    with a high number of processors. As for the data size, since we are going to
    perform k-fold cross-validation and the training data will be split further down,
    we should add more data than the amount estimated in the previous section. Now,
    without further ado, let's use`GridSearchCV` to tune our network:**
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，建议使用并行化。`GridSearchCV`和**`RandomizedSearchCV`允许我们利用机器上的不同处理器同时训练多个模型。我们可以通过`n_jobs`设置来实现这一点。这意味着，通过使用处理器数量较多的机器，你可以显著加速超参数调优过程。至于数据量，考虑到我们将执行k折交叉验证，并且训练数据会被进一步划分，我们应该增加比前一部分估算的数据量更多的数据。现在，话不多说，让我们使用`GridSearchCV`来调优我们的网络：
- en: '**[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE16]**'
- en: 'It ran for 14 minutes on four CPUs, and the following hyperparameters were
    picked:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 它在四个CPU上运行了14分钟，选择了以下超参数：
- en: '**Activation**: `relu`'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：`relu`'
- en: '**Hidden layer sizes**: `(500, 100)`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层大小**：`(500, 100)`'
- en: '**Initial learning rate**: `0.01`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始学习率**：`0.01`'
- en: '**Solver**: `adam`'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：`adam`'
- en: 'The selected model achieved a **micro F-score** of **85.6%** on the test set.
    By using the `precision_recall_fscore_support` function, you can see in more detail
    which classes were easier to predict than others:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所选模型在测试集上达到了**85.6%**的**微F得分**。通过使用`precision_recall_fscore_support`函数，你可以更详细地看到哪些类别比其他类别更容易预测：
- en: '![](img/f4e18add-f143-458a-893f-b185ce43422a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4e18add-f143-458a-893f-b185ce43422a.png)'
- en: Ideally, we should retrain again using the entire training set, but I've left
    this for now. In the end, developing an optimum neural network is usually seen
    as a mixture of art and science. Nevertheless, knowing your hyperparameters and
    how to measure their effects should make it a straightforward endeavor. Then,
    tools such as `GridSearchCV` and `RandomizedSearchCV` are at your disposal for
    automating parts of the process. Automation trumps dexterity many times.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们应该使用整个训练集重新训练，但我现在先跳过这一部分。最终，开发一个最佳的神经网络通常被看作是艺术与科学的结合。然而，了解你的超参数及其效果的衡量方式应该让这一过程变得简单明了。然后，像`GridSearchCV`和`RandomizedSearchCV`这样的工具可以帮助你自动化部分过程。自动化在很多情况下优于技巧。
- en: Before moving on to the next topic, I'd like to digress a bit and show you how
    to build your own activation function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一个话题之前，我想稍微离题一下，给你展示如何构建自己的激活函数。
- en: Adding your own activation function
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加你自己的激活函数
- en: 'One common problem with many activation functions is the vanishing gradient
    problem. If you look at the curves for the `logistic` and `tanh` activation functions,
    you can see that for high positive and negative values, the curve is almost horizontal.
    This means that the gradient of the curve is almost constant for these high values.
    This hinders the learning process. The `relu` activation function tried to solve
    this problem for one part but failed to deal with it for the negative values.
    This drove the researchers to keep proposing different activations functions.
    Here, we are going to compare the **ReLU** activation to a modified version of
    it, **Leaky ReLU**:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 许多激活函数的一个常见问题是梯度消失问题。如果你观察 `logistic` 和 `tanh` 激活函数的曲线，你会发现对于高正值和负值，曲线几乎是水平的。这意味着在这些高值下，曲线的梯度几乎是常数。这会阻碍学习过程。`relu`
    激活函数尝试解决这个问题，但它仅解决了正值部分的问题，未能处理负值部分。这促使研究人员不断提出不同的激活函数。在这里，我们将把**ReLU**激活函数与其修改版**Leaky
    ReLU**进行对比：
- en: '![](img/18e43923-6e2d-415a-b2a7-bc78953f2730.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18e43923-6e2d-415a-b2a7-bc78953f2730.png)'
- en: 'As you can see in the **Leaky ReLU** example, the line is not constant for
    the negative values anymore, but rather, decreasing at a small rate. To add **Leaky
    ReLU**, I had to look for how the `relu` function is built in scikit-learn and
    shamelessly modify the code for my needs. There are basically two methods to build.
    The methods are used in the forward path and just apply the activation function
    on its inputs, while the second method applies the derivative of the activation
    function to the calculated error. Here are the two existing methods for `relu`,
    after I slightly modified the code for brevity:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在**Leaky ReLU**的示例中看到的，负值部分的线条不再是常数，而是以一个小的速率递减。为了添加**Leaky ReLU**，我需要查找
    scikit-learn 中 `relu` 函数的构建方式，并毫不犹豫地修改代码以满足我的需求。基本上有两种方法可以构建。第一种方法用于前向传播路径，并仅将激活函数应用于其输入；第二种方法则将激活函数的导数应用于计算得到的误差。以下是我稍作修改以便简洁的
    `relu` 的两个现有方法：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the first method, NumPy's `clip()` method is used to set the negative values
    to `0`. Since the `clip` method requires both the lower- and upper-bounds, the
    cryptic part of the code just gets the maximum values of this data type to set
    it as the upper-bound. The second method takes the output of the activation function
    (`Z`), as well as the calculated error (`delta`). It is supposed to multiply the
    error by the gradient of the activation's output. Nevertheless, for this particular
    activation function, the gradient is `1` for positive values and `0` for the negative
    values. So, the error was set to `0` for the negative values—that is, it was set
    to `0` whenever `relu` returned `0`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，使用了 NumPy 的 `clip()` 方法将负值设置为 `0`。由于 `clip` 方法需要设置上下界，因此代码中的难懂部分是获取该数据类型的最大值，将其作为上界。第二种方法获取激活函数的输出（`Z`）以及计算得到的误差（`delta`）。它应该将误差乘以激活输出的梯度。然而，对于这种特定的激活函数，正值的梯度为
    `1`，负值的梯度为 `0`。因此，对于负值，误差被设置为 `0`，即当 `relu` 返回 `0` 时，误差就被设置为 `0`。
- en: '`leaky_relu` keeps the positive values unchanged and multiplies the negative
    values by a small value, `0.01`. Now, all we need to do is to build out new methods
    using this information:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`leaky_relu` 保持正值不变，并将负值乘以一个小的值 `0.01`。现在，我们只需使用这些信息来构建新的方法：'
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Recall that the slope for `leaky_relu` is `1` for the positive values and is
    equal to the `leaky_relu_slope` constant for the negative values. That''s why
    we multiplied the deltas where `Z` is negative by `leaky_relu_slope`. Now, before
    using our new methods, we have to inject them into the scikit-learn''s code base,
    as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，`leaky_relu` 在正值时的斜率为 `1`，而在负值时的斜率为 `leaky_relu_slope` 常量。因此，我们将 `Z` 为负值的部分的增量乘以
    `leaky_relu_slope`。现在，在使用我们新的方法之前，我们需要将它们注入到 scikit-learn 的代码库中，具体如下：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, you can just use `MLPClassifier` as if it were there from the beginning:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以像最初就有 `MLPClassifier` 一样直接使用它：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Hacking libraries like these forces us to read its source code and understand
    it better. It also shows the value of open source, where you are not bounded by
    what is already there. In the next section, we are going to continue hacking and
    build our own convolutional layers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样黑客攻击库迫使我们去阅读其源代码，并更好地理解它。它也展示了开源的价值，让你不再受限于现有的代码。接下来的部分，我们将继续进行黑客攻击，构建我们自己的卷积层。
- en: Untangling the convolutions
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解开卷积的复杂性
- en: '"Look deep into nature, and then you will understand everything better"'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '"深入观察大自然，你将更好地理解一切"'
- en: – Albert Einstein
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: – 阿尔伯特·爱因斯坦
- en: No chapter about the use of neural networks to classify images is allowed to
    end without touching on CNNs. Despite the fact that scikit-learn does not implement
    convolutional layers, we can still understand the concept and see how it works.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用神经网络进行图像分类的章节，不能不提到卷积神经网络（CNN）。尽管scikit-learn并未实现卷积层，但我们仍然可以理解这一概念，并了解它是如何工作的。
- en: 'Let''s start with the following *5* x *5* image and see how to apply a convolutional
    layer to it:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下的*5* x *5* 图像开始，看看如何将卷积层应用于它：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In natural language processing, words usually serve as a middle ground between
    characters and entire sentences when it comes to feature extraction. In this image,
    maybe smaller patches serve as better units of information than a separate pixel.
    The objective of this section is to find ways to represent these small *2 x 2*,
    *3 x 3*, or *N x N* patches in an image. We can start with averages as summaries.
    We can basically take the average of each *3 x 3* patch by multiplying each pixel
    in it by 1, and then dividing the total by 9; there are 9 pixels in the patch.
    For the pixels on the edges, as they don't have neighbors in all directions, we
    can pretend that there is an extra 1-pixel border around the image where all pixels
    are set to 0\. By doing so, we get another *5 x 5* array.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，词语通常作为字符与整个句子之间的中介来进行特征提取。在这张图中，也许较小的块比单独的像素更适合作为信息单元。本节的目标是寻找表示这些小的*2
    x 2*、*3 x 3* 或 *N x N* 块的方法。我们可以从平均值作为总结开始。我们基本上可以通过将每个像素乘以1，然后将总和除以9，来计算每个*3
    x 3* 块的平均值；这个块中有9个像素。对于边缘上的像素，因为它们在所有方向上没有邻居，我们可以假装在图像周围有一个额外的1像素边框，所有像素都设置为0。通过这样做，我们得到另一个*5
    x 5* 的数组。
- en: 'This kind of operation is known as **convolutions**, and **SciPy** provides
    a way of doing it. The *3 x 3* all-ones matrix used and is also known as the kernel
    or weights. Here, we specify the all-ones kernel and divide by 9 later. We also
    specify the need for an all-zeros border by setting`mode` to `constant` and `cval`
    to `0`, as you can see in the following code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作被称为**卷积**，而**SciPy**提供了一种实现卷积的方法。*3 x 3* 的全1矩阵也被称为卷积核或权重。在这里，我们指定全1的卷积核，并在后面进行9的除法。我们还指定需要一个全零的边框，通过将`mode`设置为`constant`，`cval`设置为`0`，正如您在以下代码中所看到的那样：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is a comparison between the original image and the output of the convolution:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是原始图像与卷积输出之间的对比：
- en: '![](img/0d901206-8df7-41d1-86c1-e1d2b9dc94ee.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d901206-8df7-41d1-86c1-e1d2b9dc94ee.png)'
- en: Calculating the mean gave us a blurred version of the original image, so next
    time you need to blur an image, you know what to do. Multiplying each pixel by
    a certain weight and calculating the sum of these products sounds like a linear
    model. Furthermore, we can think of averages as linear models where all weights
    are set to ![](img/f1e2a388-dfb2-4f7a-ba68-5bd41656920e.png). So, you may say
    that we are building mini-linear models for each patch of the image. Keep this
    analogy in mind, but for now, we have to set the model's weights by hand.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平均值给我们带来了模糊的原始图像版本，所以下次当你需要模糊图像时，你知道该怎么做。将每个像素乘以某个权重并计算这些乘积的总和听起来像是一个线性模型。此外，我们可以将平均值看作是线性模型，其中所有权重都设置为
    ![](img/f1e2a388-dfb2-4f7a-ba68-5bd41656920e.png)。因此，你可以说我们正在为图像的每个块构建迷你线性模型。记住这个类比，但现在，我们必须手动设置模型的权重。
- en: While each patch gets the exact same linear model as the others, there is nothing
    stopping the pixels within each patch from being multiplied by different weights.
    In fact, different kernels with different weights give different effects. In the
    next section, we are going to witness the effect of the different kernels on our
    **Fashion-MNIST** dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个块使用的线性模型与其他块完全相同，但没有什么可以阻止每个块内的像素被不同的权重相乘。事实上，不同的卷积核和不同的权重会产生不同的效果。在下一节中，我们将看到不同卷积核对我们**Fashion-MNIST**数据集的影响。
- en: Extracting features by convolving
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过卷积提取特征
- en: 'Rather than dealing with the images one by one, we can tweak the code to convolve
    over multiple images at once. The images in our Fashion-MNIST dataset are flattened,
    so we need to reshape them into *28* x *28* pixels each. Then, we convolve using
    the given kernel, and finally, make sure that all pixel values are between `0`
    and `1` using our favorite `MinMaxScaler` parameter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与其逐个处理图像，我们可以调整代码一次性对多张图像进行卷积。我们的Fashion-MNIST数据集中的图像是平铺的，因此我们需要将它们重新调整为*28*
    x *28* 像素的格式。然后，我们使用给定的卷积核进行卷积，最后，确保所有像素值都在`0`和`1`之间，使用我们最喜爱的`MinMaxScaler`参数：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we can use it as our training and test data, as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将其作为我们的训练和测试数据，如下所示：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here are few kernels: the first one is used to sharpen an image, then comes
    a kernel to emphasize vertical edges, while the last one emphasizes the horizontal
    ones:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个卷积核：第一个用于锐化图像，接着是一个用于强调垂直边缘的卷积核，而最后一个则强调水平边缘：
- en: '**Sharpen**: `[[0,-1,0], [-1,5,-1], [0,-1,0]]`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锐化**：`[[0,-1,0], [-1,5,-1], [0,-1,0]]`'
- en: '**V-edge**: `[[-1,0,1], [-2,0,2], [-1,0,1]]`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垂直边缘**： `[[-1,0,1], [-2,0,2], [-1,0,1]]`'
- en: '**H-edge**: `[[-1,-2,-1], [0,0,0], [1,2,1]]`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**水平边缘**： `[[-1,-2,-1], [0,0,0], [1,2,1]]`'
- en: 'Giving those kernels to the convolve function we have just created will give
    us the following effects:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些卷积核传给我们刚刚创建的卷积函数将得到以下效果：
- en: '![](img/19a88a69-803a-46a5-870e-471a223e3b9b.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19a88a69-803a-46a5-870e-471a223e3b9b.png)'
- en: You can find more kernels on the internet, or you can try your own and see the
    effects they make. The kernels are also intuitive; the sharpening kernel clearly
    gives more weight to the central pixels versus its surroundings.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在互联网上找到更多的卷积核，或者你也可以尝试自己定义，看看它们的效果。卷积核也是直观的；锐化卷积核显然更重视中央像素而非其周围的像素。
- en: Each of the different convolutional transformations captures certain information
    from our images. We can thussee them as a feature engineering layer, where we
    extract features to feed to our classifier. Nevertheless, the size of our data
    will grow with each additional convolutional transformation we append to our data.
    In the next section, we will see how to deal with this issue.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 每个不同的卷积变换从我们的图像中捕获了特定的信息。因此，我们可以将它们看作一个特征工程层，从中提取特征供分类器使用。然而，随着我们将更多的卷积变换添加到数据中，数据的大小会不断增长。在下一部分，我们将讨论如何处理这个问题。
- en: Reducing the dimensionality of the data via max pooling
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过最大池化来减少数据的维度
- en: Ideally, we would like to feed the outputs from more than one of the previous
    convolutional transformations into our neural network. Nevertheless, if our images
    are made of 784 pixels, then concatenating the outputs of just three convolutional
    functions will result in 2,352 features, *784 x 3*. This will slow down our training
    process, and as we have learned earlier in this book, the more features are not
    always the merrier.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望将前几个卷积变换的输出输入到我们的神经网络中。然而，如果我们的图像由784个像素构成，那么仅仅连接三个卷积函数的输出将会产生2,352个特征，*784
    x 3*。这将会减慢我们的训练过程，而且正如我们在本书前面所学到的，特征越多并不总是越好。
- en: 'To shrink an image to one-quarter of its size—that is, half its width and half
    its height—you can divide it into multiple *2 x 2* patches, then take the maximum
    value in each of these patches to represent the entire patch. This is exactly
    what **max pooling** does. To implement it, we need to install another library
    called `scikit-image` using `pip` in your computer terminal:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要将图像缩小为原来大小的四分之一——即宽度和高度各缩小一半——你可以将图像划分为多个*2 x 2*的补丁，然后在每个补丁中取最大值来表示整个补丁。这正是**最大池化**的作用。为了实现它，我们需要在计算机终端通过`pip`安装另一个名为`scikit-image`的库：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we can create our max pooling function, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建我们的最大池化函数，如下所示：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can then apply it to the outputs of one of the convolutions, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将它应用到其中一个卷积的输出上，具体如下：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Applying max pooling on *5 x 5* patches will reduce the size of our data from
    *28 x 28* to *6 x 6*, which is less than 5% of its original size.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在*5 x 5*的补丁上应用最大池化将把数据的大小从*28 x 28*缩小到*6 x 6*，即原始大小的不到5%。
- en: Putting it all together
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将一切整合在一起
- en: 'The `FeatureUnion` pipeline in scikit-learn can combine the output of multiple
    transformers. In other words, if scikit-learn had transformers that could convolve
    over images and max pool the output of these convolutions, you would have been
    able to combine the outputs of more than one of these transformers, each with
    its specific kernel. Luckily, we can build this transformer ourselves and combine
    their outputs via `FeatureUnion`. We just need them to provide the fit, transform,
    and fit_transform methods, as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`FeatureUnion`管道可以在scikit-learn中将多个转换器的输出组合起来。换句话说，如果scikit-learn有可以对图像进行卷积并对这些卷积输出进行最大池化的转换器，那么你就可以将多个转换器的输出结合起来，每个转换器都使用不同的卷积核。幸运的是，我们可以自己构建这个转换器，并通过`FeatureUnion`将它们的输出结合起来。我们只需要让它们提供fit、transform和fit_transform方法，如下所示：'
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You can specify the kernel to use at the initialization step. You can also
    skip the max pooling part by setting `max_pool` to `False`. Here, we define our
    three kernels and combine their outputs while pooling each *4 x 4* patch in our
    images:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在初始化步骤中指定使用的卷积核。你还可以通过将`max_pool`设置为`False`来跳过最大池化部分。这里，我们定义了三个卷积核，并在对每个*4
    x 4*图像块进行池化时，组合它们的输出：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, we can use the output of the `FeatureUnion` pipeline into our neural
    network, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将`FeatureUnion`管道的输出传递到我们的神经网络中，如下所示：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This network achieved a **micro F-score** of **79%**. You may try adding more
    kernel and tune the network's hyperparameters and see whether we can achieve a
    better score than the one we got without the convolutions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络达到了**微 F 值**为**79%**。你可以尝试添加更多的卷积核并调整网络的超参数，看看我们是否能比没有卷积层时获得更好的得分。
- en: We had to set the kernel weights of the convolutions by hand. We then displayed
    their outputs to see whether they make intuitive sense and hope they will improve
    our model's performance when used. This doesn't sound like a real data-driven
    approach. You would ideally want the weights to be learned from the data. That's
    exactly what the real CNNs do. I would suggest you look into TensorFlow and PyTorch
    for their CNN implementations. It would be nice if you could compare their accuracy
    to the model we have built here.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须手动设置卷积层的核权重。然后，我们显示它们的输出，以查看它们是否直观合理，并希望它们在使用时能提高我们模型的表现。这听起来不像是一个真正的数据驱动方法。理想情况下，你希望权重能够从数据中学习。这正是实际的卷积神经网络（CNN）所做的。我建议你了解TensorFlow和PyTorch，它们提供了CNN的实现。如果你能将它们的准确度与我们这里构建的模型进行比较，那将非常好。
- en: MLP regressors
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP 回归器
- en: As well as `MLPClassifier`, there is its regressor sibling,`MLPRegressor`. The
    two share an almost identical interface.The main difference between the two is
    the loss functions used by each of them and the activation functions of the output
    layer. The regressor optimizes a squared loss, and the last layer is activated
    by an identity function. All other hyperparameters are the same, including the
    four activation options for the hidden layers.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`MLPClassifier`，还有它的回归兄弟`MLPRegressor`。这两者共享几乎相同的接口。它们之间的主要区别在于每个使用的损失函数和输出层的激活函数。回归器优化平方损失，最后一层由恒等函数激活。所有其他超参数相同，包括隐藏层的四种激活选项。
- en: Both estimators have a `partial_fit()`method. You can use it to update the model
    once you get a hold of additional training data after the estimator has already
    been fitted.`score()`in `MLPRegressor`calculates the regressor's**R*²,* as opposed
    to the classifier's accuracy, which is calculated by`MLPClassifier`*.***
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 两个估算器都有一个`partial_fit()`方法。你可以在估算器已经拟合后，获取额外的训练数据时，使用它来更新模型。在`MLPRegressor`中，`score()`计算的是回归器的**R*²**，而分类器的准确度则由`MLPClassifier`计算。
- en: '****# Summary'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '****# 总结'
- en: We have now developed a good understanding of ANNs and their underlying technologies.
    I'd recommend libraries suchas TensorFlow and PyTorch for more complex architecture
    and for scaling up the training process on GPUs. However, you have a good headstart
    already. Most of the concepts discussed here are transferable to any other library.
    You will be using more or less the same activation functions and the same solvers,
    as well as most of the other hyperparameters discussed here. scikit-learn's implementation
    is still good for prototyping and for cases where we want to move beyond linear
    models without the need for too many hidden layers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经对人工神经网络（ANNs）及其底层技术有了很好的理解。我推荐使用像TensorFlow和PyTorch这样的库来实现更复杂的架构，并且可以在GPU上扩展训练过程。不过，你已经有了很好的起步。这里讨论的大部分概念可以转移到任何其他库上。你将使用相似的激活函数和求解器，以及这里讨论的大部分其他超参数。scikit-learn的实现仍然适用于原型开发以及我们想要超越线性模型的情况，且不需要太多隐藏层。
- en: Furthermore, the solvers discussed here, such as gradient descent, are so ubiquitous
    in the field of machine learning, and so understanding their concepts is also
    helpful for understanding other algorithms that aren't neural networks. We saw
    earlier how gradient descent is used in training linear and logistic regressors
    as well as support vector machines. We are also going to use them with the gradient
    boosting algorithms we will look at in the next chapter.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，像梯度下降这样的求解器在机器学习领域是如此普遍，理解它们的概念对于理解其他不是神经网络的算法也很有帮助。我们之前看到梯度下降如何用于训练线性回归器、逻辑回归器以及支持向量机。我们还将在下一章中使用它们与梯度提升算法。
- en: Concepts such as the learning rate and how to estimate the amount of training
    data needed are good to have at your disposal, regardless of what algorithm you
    are using. These concepts were easily applied here, thanks to the helpful tools
    provided by scikit-learn. I sometimes find myself using scikit-learn's tools even
    when I'm not building a machine learning solution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用什么算法，像学习率以及如何估计所需训练数据量等概念都是非常有用的。得益于 scikit-learn 提供的有用工具，这些概念得以轻松应用。即使在我并非在构建机器学习解决方案时，我有时也会使用
    scikit-learn 的工具。
- en: If ANNs and deep learning are the opium of the media, then ensemble algorithms
    are the bread and butter for most practitioners when solving any business problem
    or when competing for a $10,000 prize on Kaggle.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人工神经网络（ANNs）和深度学习是媒体的鸦片，那么集成算法就是大多数从业者在解决任何商业问题或在 Kaggle 上争夺$10,000奖金时的“面包和黄油”。
- en: In the next chapter, we are going to learn about the different ensemble methods
    and their theoretical background, and then get our hands dirty fine-tuning their
    hyperparameters.****************
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习不同的集成方法及其理论背景，然后亲自动手调优它们的超参数。****************
