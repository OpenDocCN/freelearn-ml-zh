- en: Analyzing Visitor Patterns to Make Recommendations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过分析游客模式来制定推荐
- en: This chapter will focus on the problem of finding similar visitors based on
    the theme park attractions they attend, to make improved marketing recommendations.
    Collaborative filtering methods will be introduced with examples showing how to
    train and obtain custom recommendations both in Apache Spark (EMR) and through
    the AWS SageMaker built-in algorithms. Many companies leverage the kinds of algorithms
    we describe in this chapter to improve the engagement of their customers by recommending
    products that have a proven record of being relevant to similar customers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于基于游客参加的主题公园景点寻找相似游客的问题，以制定改进的营销推荐。我们将介绍协同过滤方法，并通过示例展示如何在Apache Spark（EMR）和通过AWS
    SageMaker内置算法中训练和获取定制推荐。许多公司利用我们在本章中描述的算法类型来通过推荐与类似客户相关联的产品来提高客户的参与度。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Making theme park attraction recommendations through Flickr data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Flickr数据推荐主题公园景点
- en: Finding recommendations through Apache Spark's Alternating Least Squares method
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Apache Spark的交替最小二乘法寻找推荐
- en: Recommending attractions through SageMaker Factorization Machines
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过SageMaker因子分解机推荐景点
- en: Making theme park attraction recommendations through Flickr data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Flickr数据推荐主题公园景点
- en: Throughout this chapter, we will make use of the dataset from [https://sites.google.com/site/limkwanhui/datacode](https://sites.google.com/site/limkwanhui/datacode),
    which consists of Flickr data from users who take photos at different locations,
    these photos are then mapped to known theme park attractions. Flickr is an image-hosting
    service. Let's assume Flickr wants to create a plug-in on their mobile app that,
    as users take photos on the different attractions, identifies user preferences
    and provides recommendations on other attractions that might be of interest to
    them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用来自[https://sites.google.com/site/limkwanhui/datacode](https://sites.google.com/site/limkwanhui/datacode)的数据集，该数据集包含在不同地点拍照的用户上传的Flickr数据，这些照片随后被映射到已知的主题公园景点。Flickr是一个图像托管服务。假设Flickr希望在他们的移动应用上创建一个插件，当用户在各个景点拍照时，能够识别用户偏好并提供可能对他们感兴趣的其它景点的推荐。
- en: Let's also suppose that the number of photos a user takes on a particular attraction
    is an indicator of their interest in the attraction. Our goal is to analyze a
    dataset with triples of the *user ID, attraction, number of photos taken* form
    so that given an arbitrary set of attractions visited by a user, the model is
    able to recommend new attractions that similar users found interesting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再假设用户在特定景点拍摄的照片数量是他们对该景点兴趣的一个指标。我们的目标是分析包含*用户ID、景点、拍摄照片数量*的三元组数据集，以便给定用户访问的任意景点集合，模型能够推荐类似用户发现有趣的景点。
- en: Collaborative filtering
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协同过滤
- en: Collaborative filtering is a process for providing recommendations to users
    based on their behavior by analyzing the behaviors of a lot of users. We observe
    the effects of this algorithm in our day-to-day life in a large number of applications.
    For example, when you are using streaming services, such as Netflix or YouTube,
    it recommends videos that you may be interested in based on your streaming history.
    Social networks, such as Twitter and LinkedIn, suggest people for you to follow
    or connect with based on your current contacts. Services such as Instagram and
    Facebook curate posts from your friends and tailor your timeline based on the
    posts that you read or like. As a data scientist, collaborative filtering algorithms
    are really useful when you are building recommendation systems based on a large
    amount of user data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤是一种基于分析大量用户行为来向用户提供推荐的过程。我们在日常生活中的大量应用中观察到这种算法的影响。例如，当你使用流媒体服务，如Netflix或YouTube时，它会根据你的观看历史推荐你可能感兴趣的视频。社交媒体，如Twitter和LinkedIn，会根据你的当前联系人建议你关注或连接的人。Instagram和Facebook等服务会根据你阅读或赞过的帖子来编辑你朋友的帖子，并定制你的时间线。作为一名数据科学家，当你在基于大量用户数据构建推荐系统时，协同过滤算法非常有用。
- en: There are various ways in which collaborative filtering can be implemented on
    a dataset. In this chapter, we will be discussing the memory-based approach and
    the model-based approach.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤可以在数据集上有多种实现方式。在本章中，我们将讨论基于记忆的方法和基于模型的方法。
- en: Memory-based approach
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于记忆的方法
- en: In the memory-based approach, we generate recommendations in two phases. Consider
    a situation where we are trying to generate recommendations for a given user based
    on their interests. In the first phase, we discover users who are similar to the
    given user based on their interests. We rank all the users based on how similar
    they are to a given user. In the second phase, we discover the top interests among
    the group of users that are most similar to a given user. The top interests are
    ranked based on their similarity to the set of top-ranked users. This ranked list
    of interests is then presented to the original user as recommendations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于记忆的方法中，我们分两个阶段生成推荐。考虑这样一种情况，即我们试图根据用户的兴趣为特定用户生成推荐。在第一阶段，我们根据兴趣发现与给定用户相似的用户。我们根据用户与给定用户的相似程度对所有用户进行排名。在第二阶段，我们在与给定用户最相似的用户组中找到最感兴趣的兴趣点。这些顶级兴趣根据它们与顶级用户集的相似性进行排名。然后，这个按排名排列的兴趣列表被呈现给原始用户作为推荐。
- en: For example, in the process of movie recommendations, we look at the movies
    a user is interested in or has watched recently and discover other users who have
    watched similar movies. Based on the top-ranked list of similar users, we look
    at the movies they have watched recently and rank them based on the similarity
    to the list of ranked users. Then, the top-ranked movies are then presented as
    recommendations to the user.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在电影推荐的过程中，我们查看用户感兴趣或最近观看的电影，并发现观看过类似电影的其他用户。基于相似用户的高排名列表，我们查看他们最近观看的电影，并根据与排名用户列表的相似性进行排名。然后，将排名最高的电影作为推荐呈现给用户。
- en: 'To find a similarity between users, we use functions called similarity measures.
    Similarity measures are popularly used in search engines to rank a similarity
    between query terms and documents. In this section, we discuss the cosine similarity
    measure, which is commonly used in collaborative filtering. We treat each user''s
    interest as a vector. To discover users with similar interests, we calculate the
    cosine of the angle between two users'' interest vector. It can be represented
    as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到用户之间的相似性，我们使用称为相似度度量的函数。相似度度量在搜索引擎中广泛用于对查询词和文档之间的相似性进行排序。在本节中，我们讨论了余弦相似度度量，它在协同过滤中常用。我们将每个用户的兴趣视为一个向量。为了发现具有相似兴趣的用户，我们计算两个用户兴趣向量之间的余弦角。它可以表示如下：
- en: '![](img/e44e8858-e65a-442f-86e8-cc65325e8e62.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e44e8858-e65a-442f-86e8-cc65325e8e62.png)'
- en: Based on the similarity between a given user and all users in the dataset, we
    select the top k users. We then aggregate the interest vectors of all users to
    discover the top-ranked interests and recommend it to the user.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定用户与数据集中所有用户的相似性，我们选择前k个用户。然后，我们聚合所有用户的兴趣向量，以发现顶级兴趣并推荐给用户。
- en: Note that memory-based models do not use any modeling algorithms that were discussed
    in the previous chapters. They only rely on simple arithmetic to generate recommendations
    for users based on their interests.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，基于记忆的模型不使用前几章中讨论的任何建模算法。它们仅依赖于简单的算术来根据用户的兴趣为用户生成推荐。
- en: Model-based approach
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于模型的方法
- en: For the model-based approach, we use machine learning techniques to train a
    model that can predict the probability of each interest being relevant to a given
    user. Various algorithms, such as Bayesian models or clustering models, can be
    applied for model-based collaborative filtering. However, in this chapter, we
    focus on the matrix-factorization-based approach.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于模型的方法，我们使用机器学习技术训练一个模型，该模型可以预测每个兴趣与给定用户的相关概率。可以应用各种算法，如贝叶斯模型或聚类模型，用于基于模型的协同过滤。然而，在本章中，我们专注于基于矩阵分解的方法。
- en: Matrix factorization
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: The matrix factorization approach works by decomposing a matrix of users and
    the interests of the users. In this methodology, we map the user data and the
    information about the interests to a set of factors. The score of a user to interest
    is calculated by taking a dot product of the vector scores for the user and the
    interest.  These factors can be inferred from the user ratings or from the external
    information about the interests in the algorithm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解方法通过分解用户和用户兴趣的矩阵来工作。在本方法中，我们将用户数据和关于兴趣的信息映射到一组因子。用户对兴趣的评分是通过计算用户和兴趣的向量评分的点积来计算的。这些因子可以从用户评分或从算法中关于兴趣的外部信息中推断出来。
- en: 'For example, consider a matrix where one dimension represents the users and
    the other dimension represents the movies the users have rated:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个矩阵，其中一个维度代表用户，另一个维度代表用户已评分的电影：
- en: '|  | **Avengers** | **Jumanji** | **Spiderman** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | **复仇者联盟** | **勇敢者游戏** | **蜘蛛侠** |'
- en: '| User 1 | 4 |  | 4 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 用户1 | 4 |  | 4 |'
- en: '| User 2 | 1 | 5 | 2 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 用户2 | 1 | 5 | 2 |'
- en: '| User n | 5 | 2 | 4 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 用户n | 5 | 2 | 4 |'
- en: The values in the matrix are the ratings provided by the users. The matrix factorization
    methodology maps the movies into shorter vectors that represent concepts, such
    as the genre of the movie. These vectors are known as latent factors. We map the
    movies to genres and also map what genres users are interested in based on how
    they rate movies in each genre. Using this information, we can calculate the similarity
    between a user and movie based on the dot product (multiplying the interest of
    the user in genres by the likelihood of the movie to belong to genres) between
    both the vectors. Thus, the unknown ratings in the matrix can be predicted using
    the knowledge of known ratings by consolidating the users and interests to less
    granular items (that is, genres). In our previous example, we assume that we already
    have a known mapping of movies to genre. However, we cannot make an assumption
    that we will always have explicit data to generate such mappings to latent factors.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的值是用户提供的评分。矩阵分解方法将电影映射到更短的向量，这些向量代表概念，例如电影的类型。这些向量被称为潜在因素。我们将电影映射到类型，并根据用户对每个类型的电影评分来映射用户感兴趣的类型。使用这些信息，我们可以根据两个向量之间的点积（将用户对类型的兴趣乘以电影属于类型的可能性）来计算用户和电影之间的相似性。因此，矩阵中的未知评分可以通过通过将用户和兴趣合并到更粗粒度的项目（即类型）来使用已知评分的知识进行预测。在我们的前一个例子中，我们假设我们已经有了一个已知的电影到类型的映射。然而，我们不能假设我们总是有显式数据来生成这样的映射到潜在因素。
- en: 'Hence, we explore methodologies that can help us generate such mappings automatically
    based on the data.  Matrix factorization models therefore need to be able to generate
    a map between users and interests through a latent factors vector. To ensure we
    can generate a dot product between the latent factors of a user and the item,
    the length of the latent factors is set to a fixed value. Each interest item, ![](img/b1751de6-aac4-4c85-b8e3-cda4ce8defca.png),
    is represented by a vector, ![](img/d82d8f27-75f4-420f-82a4-670c39aff92a.png),
    and each user, ![](img/0e7f0f13-8c2c-4560-9ff1-528999b61e2a.png), is represented
    by a vector, ![](img/7a61b9cd-ac9f-4912-b6f4-68d4ea25bac9.png). The ![](img/f16968da-4c4d-4caf-bfbe-beffe6d887e2.png)
    and ![](img/760e2953-72f9-4f46-915e-7da0de7cd402.png) vectors are both latent
    factors that are derived from the data. The rating for an item, ![](img/bb19f614-4e7e-4d27-95ad-737235fd23e8.png),
    for a user, ![](img/2cf02bfe-0617-47cc-865a-32914451a6bb.png), is represented
    as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们探索可以基于数据自动生成此类映射的方法。因此，矩阵分解模型需要能够通过潜在因素向量在用户和兴趣之间生成映射。为了确保我们可以生成用户潜在因素和项目潜在因素之间的点积，潜在因素长度被设置为固定值。每个兴趣项目，![图片](img/b1751de6-aac4-4c85-b8e3-cda4ce8defca.png)，由一个向量，![图片](img/d82d8f27-75f4-420f-82a4-670c39aff92a.png)，表示，每个用户，![图片](img/0e7f0f13-8c2c-4560-9ff1-528999b61e2a.png)，由一个向量，![图片](img/7a61b9cd-ac9f-4912-b6f4-68d4ea25bac9.png)，表示。向量![图片](img/f16968da-4c4d-4caf-bfbe-beffe6d887e2.png)和![图片](img/760e2953-72f9-4f46-915e-7da0de7cd402.png)都是从数据中导出的潜在因素。用户对项目的评分，![图片](img/bb19f614-4e7e-4d27-95ad-737235fd23e8.png)，对用户的评分，![图片](img/2cf02bfe-0617-47cc-865a-32914451a6bb.png)，表示如下：
- en: '![](img/ef81c489-1e91-4258-9b3f-b17578757613.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef81c489-1e91-4258-9b3f-b17578757613.png)'
- en: 'In general, if we already have a partial set of ratings from users to interests,
    we can use that to model ratings between other users and interests. We use optimization
    techniques to calculate this. Our objective is to predict the values of ![](img/0b81e02c-349c-4ee0-be51-a825132cce01.png)
    and ![](img/edfabc37-59ef-44c3-88bb-61f64d4263e8.png). Hence, we do that by minimizing
    the regularized error when predicting these vectors by using the known ratings.
    This is represented in the following formula:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果我们已经从用户到兴趣的部分评分集合，我们可以使用它来建模其他用户和兴趣之间的评分。我们使用优化技术来计算这个。我们的目标是预测![图片](img/0b81e02c-349c-4ee0-be51-a825132cce01.png)和![图片](img/edfabc37-59ef-44c3-88bb-61f64d4263e8.png)的值。因此，我们通过最小化预测这些向量时使用的已知评分的正则化误差来实现这一点。这可以用以下公式表示：
- en: '![](img/08e98314-a4eb-49c1-bd23-0cb4916391f8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08e98314-a4eb-49c1-bd23-0cb4916391f8.png)'
- en: Here, *k* is a set of ![](img/b89e6751-a35a-462f-9921-9fbbd6021e15.png) where
    the rating, ![](img/9e086582-82c0-4fda-b029-dee387107679.png), is known. Now,
    let's look at study two approaches for minimizing the preceding equation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*k* 是一个包含 ![](img/b89e6751-a35a-462f-9921-9fbbd6021e15.png) 的集合，其中已知的评分
    ![](img/9e086582-82c0-4fda-b029-dee387107679.png)。现在，让我们看看两种最小化前面方程的方法。
- en: Stochastic gradient descent
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: 'We studied the stochastic gradient-descent algorithm in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms* regarding linear regression. A similar
    methodology is used to minimize the function to predict the correct latent factors
    for each user and interest. We use an iterative approach, where during each iteration,
    we calculate the error of predicting ![](img/a7ba6505-3b19-432d-a187-5f83f41d271c.png)
    and ![](img/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png) based on all the known ratings:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml)，“使用回归算法预测房屋价值”中研究了随机梯度下降算法，关于线性回归。类似的方法用于最小化预测每个用户和兴趣的正确潜在因子的函数。我们使用迭代方法，在每次迭代中，我们根据所有已知的评分计算预测
    ![](img/a7ba6505-3b19-432d-a187-5f83f41d271c.png) 和 ![](img/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png)
    的误差：
- en: '![](img/55123d63-6338-4689-a390-47ed3a1e378b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/55123d63-6338-4689-a390-47ed3a1e378b.png)'
- en: 'Based on the magnitude of error, we update the values of  ![](img/629216ac-47e5-43be-b576-bb9812831018.png)
    and ![](img/bb88a91e-797e-45b5-bd34-99e1820eb0a2.png) in the opposite direction
    of the error:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据误差的大小，我们更新 ![](img/629216ac-47e5-43be-b576-bb9812831018.png) 和 ![](img/bb88a91e-797e-45b5-bd34-99e1820eb0a2.png)
    的值，方向与误差相反：
- en: '![](img/b4231d69-9439-4118-af42-1eeb63226ff3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b4231d69-9439-4118-af42-1eeb63226ff3.png)'
- en: '![](img/3653b777-1627-4d63-98e9-6983a00d2316.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3653b777-1627-4d63-98e9-6983a00d2316.png)'
- en: We stop the iterations after the values of ![](img/a7ba6505-3b19-432d-a187-5f83f41d271c.png)
    and ![](img/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png) converge. Stochastic gradient
    descent is also used in algorithms such as **Factorization Machines **(**FMs**),
    which uses it to compute values of vectors. FMs are a variant of **support vector
    machine** (**SVM**) models that can be applied in a collaborative filtering framework.
    We do not explain support vector machines or FMs in detail in this book, but encourage
    you to understand how they work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/a7ba6505-3b19-432d-a187-5f83f41d271c.png) 和 ![](img/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png)
    的值收敛后，我们停止迭代。随机梯度下降也用于如**因子分解机**（**FMs**）等算法中，它使用它来计算向量的值。FMs 是 **支持向量机**（**SVM**）模型的变体，可以在协同过滤框架中应用。我们在这本书中不会详细解释支持向量机或
    FMs，但鼓励你了解它们是如何工作的。
- en: Alternating Least Squares
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交替最小二乘法
- en: One of the challenges of minimizing the optimization function to predict the
    values of both ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png) and ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png)
    is that the equation is not convex. This is because we are trying to optimize
    two values at the same time. However, if we used a constant for one of the values,
    or ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png) or ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png),
    we can solve the equation optimally for the other variable. Hence, in the Alternating
    Least Squares technique, we alternatively set the values of ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png)
    and ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png) as constant while optimizing
    for the other vector.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化预测 ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png) 和 ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png)
    值的优化函数的一个挑战是，方程不是凸的。这是因为我们试图同时优化两个值。然而，如果我们为其中一个值使用常数，或者 ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png)
    或 ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png)，我们可以为另一个变量最优地解决方程。因此，在交替最小二乘技术中，我们交替地将
    ![](img/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png) 和 ![](img/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png)
    的值设为常数，同时优化另一个向量。
- en: Hence, in the first step, we set base values for both the vectors. Assuming
    that one of the values is constant, we use linear programming to optimize the
    other vector. In the next step, we set the value of the optimized vector as constant
    and optimize for the other variable. We will not explain how linear programming
    is used to optimize for quadratic questions as it is an entire field of study
    and not in the scope of this book. This methodology optimizes each vector until
    convergence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在第一步中，我们为两个向量设定了基础值。假设其中一个值是常数，我们使用线性规划来优化另一个向量。在下一步中，我们将优化向量的值设为常数，并优化另一个变量。我们不会解释如何使用线性规划来优化二次问题，因为这是一个完整的研究领域，并且超出了本书的范围。这种方法将优化每个向量，直到收敛。
- en: The advantage of stochastic gradient descent is that it is faster than the ALS
    method, as it depends on predicting the values of both the vectors in each step
    while modifying the vectors based on the proportion of errors. However, in the
    ALS methodology, the system calculates the values of each vector independently,
    and hence leads to better optimization. Moreover, when the matrix is dense, the
    gradient descent methodology has to learn from each set of data, making it less
    efficient than the ALS methodology.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降的优点是它比ALS方法更快，因为它在修改向量时基于错误比例预测每个步骤中每个向量的值。然而，在ALS方法中，系统独立地计算每个向量的值，因此导致更好的优化。此外，当矩阵密集时，梯度下降方法必须从每一组数据中学习，这使得它比ALS方法效率更低。
- en: Finding recommendations through Apache Spark's ALS
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Apache Spark的ALS找到推荐
- en: In this section, we will go through the process of creating recommendations
    in Apache Spark using **Alternating Least Squares** (**ALS**).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过Apache Spark使用**交替最小二乘法**（**ALS**）创建推荐的过程。
- en: Data gathering and exploration
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集和探索
- en: 'The first step is to download the data from [https://sites.google.com/site/limkwanhui/datacode](https://sites.google.com/site/limkwanhui/datacode)
    . We will be using the `poiList-sigir17` dataset with photos taken by users at
    different theme park attractions (identified as points of interest by Flickr).
    There are following two datasets we''re interested in:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从[https://sites.google.com/site/limkwanhui/datacode](https://sites.google.com/site/limkwanhui/datacode)下载数据。我们将使用`poiList-sigir17`数据集，其中包含用户在不同主题公园景点（由Flickr识别为兴趣点）拍摄的图片。我们感兴趣的以下两个数据集：
- en: 'The list of points of interests, which captures the names and other properties
    of each attraction:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该兴趣点列表，它捕捉了每个景点的名称和其他属性：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following screenshot shows the first few lines of the `poi_df` dataframe:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`poi_df`数据框的前几行：
- en: '![](img/bf8ca937-b9e2-4f10-919c-8acb409538fd.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bf8ca937-b9e2-4f10-919c-8acb409538fd.png)'
- en: 'The photos taken by Flickr users at different points of interest:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flickr用户在不同兴趣点拍摄的图片：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following screenshot shows a sample of the `visits_df` dataframe:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`visits_df`数据框的一个样本：
- en: '![](img/eb169bd1-9b0d-4330-bb02-2deae5faff7e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eb169bd1-9b0d-4330-bb02-2deae5faff7e.png)'
- en: In this dataset, we will be using the `nsid` field (indicating the user taking
    the photo) and `poiID`, which indicates the actual point of interest or attraction
    visited while taking the photo. For our purposes, we will ignore the rest of the
    fields.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们将使用`nsid`字段（表示拍照的用户）和`poiID`，它表示拍照时实际访问的兴趣点或景点。为了我们的目的，我们将忽略其余的字段。
- en: 'Let''s do some basic inspection on our dataset. The dataset has about 300,000
    rows of data. By taking a sample of 1,000 entries, we can see that there are 36
    unique Flickr users:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对我们的数据集做一些基本的检查。数据集大约有30万行数据。通过抽取1000个条目的样本，我们可以看到有36个独特的Flickr用户：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding `describe()` command is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个`describe()`命令的输出如下：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is important, as we need to have enough entries per user to ensure we have
    enough information about users to make predictions. Furthermore, it's actually
    more relevant to know whether users visit different attractions. One the nice
    things about Apache Spark is that one can work on datasets using SQL. Finding
    the number of distinct attractions users see on average can easily be done with
    SQL.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为我们需要每个用户有足够多的条目来确保我们有足够的信息关于用户来做出预测。此外，了解用户是否访问了不同的景点实际上更为相关。Apache Spark的一个优点是，可以使用SQL在数据集上工作。使用SQL轻松地找到用户平均看到的独特景点的数量。
- en: 'In order to work with SQL, we first need to give a table name to the dataset.
    This is done by registering a temp table:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SQL，我们首先需要给数据集一个表名。这是通过注册一个临时表来完成的：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we register the tables, we can do queries, such as finding the number
    of unique attractions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们注册了表，我们就可以进行查询，例如找到独特景点的数量：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Or we can combine SQL with other dataset operations, such as `.describe()`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以将SQL与其他数据集操作相结合，例如`.describe()`：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot contains the result of the output of the `show()`
    command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图包含了`show()`命令输出的结果：
- en: '![](img/a6020f4a-0a8b-4555-afe9-877e3e6886b5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a6020f4a-0a8b-4555-afe9-877e3e6886b5.png)'
- en: The preceding SQL command finds the number of distinct attractions each user
    visits. The describe dataset operation finds statistics on these users, which
    tells us that, on average, users visit about five different locations. This is
    important as we need to have enough attractions per user to be able to correctly
    identify user patterns.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的SQL命令找出每个用户访问的独特景点的数量。`describe`数据集操作找出这些用户的统计数据，这告诉我们，平均而言，用户访问大约五个不同的地点。这很重要，因为我们需要每个用户有足够的景点，以便能够正确识别用户模式。
- en: 'Similarly, we should look at the number of photos users take at each location,
    to validate that in fact we can use the number of photos taken as an indicator
    of the user''s interest. We do that through the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们应该查看用户在每个地点拍摄的照片数量，以验证实际上我们可以使用拍摄照片的数量作为用户兴趣的指标。我们通过以下命令来完成：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the preceding command is shown by the following screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令的输出如下所示：
- en: '![](img/92832e20-1d60-4f6d-8293-1c2682b54201.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/92832e20-1d60-4f6d-8293-1c2682b54201.png)'
- en: The SQL command counts the number of entries for each user and attraction, and
    then we find a statistical summary using the describe. We can conclude therefore
    that on average, each user takes about eight pictures at every location they visit.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SQL命令计算每个用户和景点的条目数量，然后我们使用`describe`找到统计摘要。因此，我们可以得出结论，平均而言，每个用户在每个访问的地点大约拍摄八张照片。
- en: Training the model
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'To train our model, we will construct a dataset that computes the number of
    photos taken by each user at each location:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们将构建一个数据集，该数据集计算每个用户在每个地点拍摄的照片数量：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following screenshot shows the first few lines of the `train_df` dataframe:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`train_df`数据框的前几行：
- en: '![](img/fdcd6859-698d-47f8-bf86-8cec71cca1b0.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fdcd6859-698d-47f8-bf86-8cec71cca1b0.png)'
- en: We hash the user because the ALS trainer just supports numerical values as features.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对用户进行哈希处理，因为ALS训练器只支持数值作为特征。
- en: 'To train the model, we simply need to construct an instance of ALS and provide
    the user column, item column (in this case the attraction IDs), and the rating
    column (in this case, `pictures_takes` is used as a proxy for rating). `coldStartStrategy`
    is set to drop as we''re not interested in making predictions for users or attractions
    not present in the dataset (that is, predictions for such entries will be dropped
    rather than returning NaN):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们只需构建一个ALS实例，并提供用户列、项目列（在这种情况下是景点ID）和评分列（在这种情况下，`pictures_takes`用作评分的代理）。`coldStartStrategy`设置为丢弃，因为我们不感兴趣为数据集中不存在（即，预测此类条目将被丢弃而不是返回NaN）的用户或景点进行预测：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Getting recommendations
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取推荐
- en: 'Once we build a model, we can generate predictions for all users in our dataset:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了一个模型，我们就可以为我们数据集中的所有用户生成预测：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding command will pick the top 10 recommendations for each user. Note
    that because of how ALS works, it might actually recommend attractions already
    visited by the user, so we need to discard that for our purposes, as we will see
    later on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令将为每个用户选择前10个推荐。请注意，由于ALS的工作方式，它实际上可能会推荐用户已经访问过的景点，因此我们需要在我们的目的中丢弃这些推荐，正如我们稍后将要看到的。
- en: 'The recommendations look as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐看起来如下：
- en: '![](img/debd84dc-dfbb-44cb-bce9-2a1bef310250.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/debd84dc-dfbb-44cb-bce9-2a1bef310250.png)'
- en: 'Each user gets a list of tuples with the recommended attraction as well as
    the score for the recommendation. In this case, the score represents the estimated
    number of photos we would expect each user to take at the recommended location.
    Even though the model just provides the IDs of the attractions, we would like
    to inspect a few of these recommendations to make sure they are good. In order
    to do that, we will construct a dictionary of IDs to attraction names (point of
    interest names) by collecting the result of a query that finds the name of each
    attraction in the points table:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户都会得到一个包含推荐景点以及推荐得分的元组列表。在这种情况下，得分代表我们预计每个用户在推荐地点会拍摄的照片数量。尽管模型只提供景点的ID，但我们想检查其中的一些推荐，以确保它们是好的。为了做到这一点，我们将通过收集查询结果来构建一个ID到景点名称（兴趣点名称）的字典，该查询找出每个景点在景点表中的名称：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The map contains the following entries:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 地图包含以下条目：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For each user, we want to remove the recommendations for already-visited sites
    and output the recommendations. To do that, we need to process the list of tuples
    on each row. Apache Spark provides a convenient way to do this by allowing users
    to create custom SQL functions, or **user-defined functions** (**UDFs**). We will
    define and register a UDF that is capable of extracting the names of each recommended
    attraction through the use of the preceding map:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个用户，我们希望移除已访问地点的推荐并输出推荐。为此，我们需要处理每行的元组列表。Apache Spark通过允许用户创建自定义SQL函数或**用户定义函数**（**UDFs**）提供了一种方便的方式来做到这一点。我们将定义并注册一个UDF，它能够通过使用前面的映射提取每个推荐景点的名称：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `poi_names` function receives the recommendations tuple for a user as well
    as the attractions visited and then returns a string that contains all recommended
    attraction names that were not in the set of visited, as well as an enumeration
    of the visited attractions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`poi_names`函数接收用户的推荐元组以及访问的景点，然后返回一个包含所有未在访问集合中的推荐景点名称以及已访问景点枚举的字符串。'
- en: 'We then register the recommendations as a table so it can be used in our next
    query:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将推荐注册为一个表，以便在下一个查询中使用：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding query joins the user recommendations table with the visits table
    and joins by user, collecting all points of interest visited by each user, and
    through the UDF it outputs the recommended attractions as well as the names of
    the already-visited attractions. We sample and collect a few instances of the
    table to inspect. In the companion notebook, we can observe the entries:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的查询将用户推荐表与访问表连接，通过用户进行连接，收集每个用户访问的所有兴趣点，并通过UDF输出推荐的景点以及已访问景点的名称。我们采样并收集一些表实例以进行检查。在配套的笔记本中，我们可以观察到以下条目：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can observe that this user visited a number of adventure-like attractions
    and the model recommended a few more. Here, the reader can inspect a couple more
    recommendations:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到这位用户访问了许多类似冒险的景点，模型推荐了更多。在这里，读者可以检查更多推荐：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Recommending attractions through SageMaker Factorization Machines
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过SageMaker因子分解机推荐景点
- en: FMs are one of the most widely used algorithms for making recommendations when
    it comes to very sparse input. It is similar to the **stochastic gradient descent** (**SGD**)
    algorithm we discussed under the model-based matrix factorization methodology.
    In this section, we will show how to use AWS' built-in algorithm implementation
    of FMs to get recommendations for our theme park visitors.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到非常稀疏的输入时，FM（因子分解机）是制作推荐中最广泛使用的算法之一。它类似于我们在基于模型的矩阵分解方法下讨论的**随机梯度下降**（**SGD**）算法。在本节中，我们将展示如何使用AWS内置的FM算法实现来为我们主题公园的游客获取推荐。
- en: Preparing the dataset for learning
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备学习数据集
- en: In order to use such an algorithm, we need to prepare our dataset in a different
    way. We will pose the recommendation problem as a regression problem in which
    the input are a pair of user and attraction, and the output is the expected level
    of interest this user will have toward the attraction. The training dataset must
    have the actual empirical interest (measured by the number of photos taken) for
    each pair of user and attraction. With this data, the FM model will then be able
    to predict the interest of an arbitrary attraction for any user. Hence, to obtain
    recommendations for a user, we just need to find the list of attractions that
    yields the highest predicted level of interest.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用这样的算法，我们需要以不同的方式准备我们的数据集。我们将推荐问题设定为一个回归问题，其中输入是一个用户和吸引力的配对，输出是这个用户对吸引力的预期兴趣水平。训练数据集必须包含每个用户和吸引力配对的实际经验兴趣（通过拍照数量衡量）。有了这些数据，FM模型就能预测任何用户对任意吸引力的兴趣。因此，为了获得用户的推荐，我们只需找到那些能产生最高预测兴趣水平的吸引力列表。
- en: '**So then how do we encode the user and the attractions in a dataset?**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么我们如何在数据集中编码用户和景点呢？**'
- en: Given that FMs are extremely good at dealing with high-dimensional features,
    we can one-hot encode our input. Since there are 8,903 users and 31 attractions,
    our input vector will be of length 8,934 where the first 31 vector components
    will correspond to the 31 different attractions, and the remaining positions correspond
    to each user. The vector will always have zeros except for the positions corresponding
    to the user and attraction, which will have a value of 1\. The target feature
    (label) used in our model will be the level of interest, which we will discretize
    to a value of 1 to 5 by normalizing the number of pictures taken according to
    their corresponding quantile.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于FM在处理高维特征方面非常出色，我们可以对输入进行one-hot编码。由于有8,903个用户和31个景点，我们的输入向量长度将为8,934，其中前31个向量分量将对应于31个不同的景点，其余位置对应于每个用户。向量除了对应于用户和景点的位置外，其余位置都将为零。我们模型中使用的目标特征（标签）将是兴趣水平，我们将通过根据相应的分位数对拍照数量进行归一化将其离散化到1到5的值。
- en: 'The following figure shows how such a training dataset could look:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了这样一个训练数据集可能的样子：
- en: '![](img/351fd038-4fe8-4e44-adfc-5d92549a6256.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/351fd038-4fe8-4e44-adfc-5d92549a6256.png)'
- en: 'As you can imagine, this matrix is extremely sparse, therefore we need to encode
    our rows using a sparse representation. Like most SageMaker algorithms, we must
    drop our data in S3 to allow SageMaker to train the data. In past chapters, we
    used CSV as an input. However, CSV is not a good representation for our dataset;
    given its sparse nature, it would occupy too much space (with a lot of repeated
    zeros!). In fact, at the time of writing, SageMaker doesn''t even support CSV
    as an input format. In a sparse representation, each vector must indicate the
    following three values:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想象，这个矩阵非常稀疏，因此我们需要使用稀疏表示来编码我们的行。像大多数SageMaker算法一样，我们必须将数据放入S3，以便SageMaker进行训练。在过去的章节中，我们使用了CSV作为输入。然而，CSV并不是我们数据集的好表示；鉴于其稀疏性，它将占用太多空间（有很多重复的零！）。实际上，在撰写本文时，SageMaker甚至不支持CSV作为输入格式。在稀疏表示中，每个向量必须指示以下三个值：
- en: The size of the vector
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的尺寸
- en: The positions in which we have a value other than 0
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有非零值的位位置
- en: The values at each of these non-zero positions
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些非零位置上的值
- en: 'For example, the sparse representation for the first row in the preceding figure
    would be the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，前图中第一行的稀疏表示如下：
- en: Vector size = 8934
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量大小 = 8934
- en: Non-zero positions = [1, 33]
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非零位置 = [1, 33]
- en: Values at non-sero positions = [1, 1]
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非零位置上的值 = [1, 1]
- en: The only input format FMs currently supports is called protobuf recordIO. Protobuf,
    short for **Protocol buffers**, is a language-neutral, platform-neutral extensible
    mechanism for serializing structured data initially developed by Google. In our
    case, the structure will be the sparse representation of our matrix. Each record
    in the protobuf file we store in S3 will have all three items necessary for sparse
    representation, as well as the target feature (label).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: FM（特征矩阵）目前支持的唯一输入格式称为protobuf recordIO。protobuf，即**协议缓冲区**，是一种语言无关、平台无关的可扩展机制，用于序列化结构化数据，最初由谷歌开发。在我们的情况下，结构将是矩阵的稀疏表示。我们存储在S3中的protobuf文件中的每个记录都将包含稀疏表示所需的所有三个项目，以及目标特征（标签）。
- en: Following, we will go through the process of preparing the dataset and uploading
    it to S3.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍准备数据集并将其上传到S3的过程。
- en: 'We will start with the Spark dataframe that we used for training in the previous
    section (`train_df`) and apply a `Pipeline` that does the one-hot encoding as
    well as normalizing the photos-taken target feature:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从上一节中用于训练的Spark dataframe（`train_df`）开始，并应用一个`Pipeline`，该`Pipeline`执行one-hot编码以及归一化拍照目标特征：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The pipeline is similar to pipelines we've built in the previous chapters, the
    difference being that we have not included a machine learning algorithm as a final
    step (since this stage will run through SageMaker's FMs once the dataset is in
    S3). We first string index the user and attraction (point of interest) features,
    and then chain them into a one-hot encoder. The quantile discretizer will reduce
    the photos taken feature into five buckets according to their percentile. We will
    name this feature `interest_level`. Additionally, we will assemble a vector with
    these encoded attractions and user vectors.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 管道与我们在前几章中构建的管道类似，不同之处在于我们没有将机器学习算法作为最终步骤（因为一旦数据集存入S3，这一阶段将通过SageMaker的FMs运行）。我们首先对用户和吸引力（兴趣点）特征进行字符串索引，然后将它们链接到一个独热编码器。分位数离散化器将拍摄照片特征根据其百分位数减少到五个桶中。我们将此特征命名为`interest_level`。此外，我们将这些编码的吸引力和用户向量组装成一个向量。
- en: 'Next, we transform the training dataset by applying the model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过应用模型来转换训练集：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will produce a dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个数据集：
- en: '![](img/771a08f9-086d-423d-8188-f2bcca0be7b9.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/771a08f9-086d-423d-8188-f2bcca0be7b9.png)'
- en: Note how the encoded fields (`user_hash_id_encoded`, `poi_id_encoded`, and features)
    show the sparse representation of the vectors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意编码字段（`user_hash_id_encoded`、`poi_id_encoded`和`features`）显示了向量的稀疏表示。
- en: Once we have this encoded dataset, we can split them into testing and training.
    SageMaker will use the training dataset for fitting and the test dataset for finding
    the validation errors at each epoch upon training. We need to convert each of
    these datasets into recordio format and upload them to s3.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个编码后的数据集，我们可以将其分为测试集和训练集。SageMaker将使用训练集进行拟合，并在训练过程中每个epoch使用测试集来查找验证错误。我们需要将这些数据集转换为recordio格式并上传到s3。
- en: 'If we were working in Scala (the native programming language used by Spark),
    we could do something like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用Scala（Spark使用的本地编程语言），我们可以做类似这样的事情：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Unfortunately, `pyspark` does not support writing a dataframe directly into
    recordio format at the time of this writing. Instead we will collect all our spark
    dataframes in memory and convert each row to a sparse vector, and then upload
    it to S3.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在撰写本文时，`pyspark`不支持直接将数据框写入recordio格式。相反，我们将收集所有spark数据框到内存中，并将每一行转换为稀疏向量，然后上传到S3。
- en: 'The following `spark_vector_to_sparse_matrix` function does exactly that. It
    takes a Spark dataframe row and converts it into a sparse `csr_matrix` (from `scipy`,
    a Python library with scientific utilities). The `upload_matrices_to_s3` function
    receives a Spark dataset (either training or testing), collects each row, builds
    a sparse vector with the features, and stacks them into a matrix. Additionally,
    it builds a target feature vector with all the interest levels. Given this matrix
    and label vector, we use the utility function `write_spmatrix_to_sparse_tensor`,
    of the `sagemaker` library to write the data in recordio format. Finally, we upload
    that object to S3\. To do this, let''s first import all the necessary dependencies:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的`spark_vector_to_sparse_matrix`函数正是这样做的。它接受一个Spark数据框行并将其转换为`scipy`稀疏矩阵（来自`scipy`，一个具有科学工具的Python库）。`upload_matrices_to_s3`函数接收一个Spark数据集（无论是训练集还是测试集），收集每一行，使用特征构建一个稀疏向量，并将它们堆叠成一个矩阵。此外，它构建一个包含所有兴趣级别的目标特征向量。给定这个矩阵和标签向量，我们使用`sagemaker`库的实用函数`write_spmatrix_to_sparse_tensor`将数据写入recordio格式。最后，我们将该对象上传到S3。为此，让我们首先导入所有必要的依赖项：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, let''s define two auxiliary functions: `spark_vector_to_sparse_matrix`,
    which will take a row and produce a `scipy` sparse matrix, and `upload_matrices_to_s3`,
    which is responsible for uploading the test or training dataset to s3:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义两个辅助函数：`spark_vector_to_sparse_matrix`，它将接受一行并生成一个`scipy`稀疏矩阵，以及`upload_matrices_to_s3`，它负责将测试集或训练集上传到s3：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we need to upload the training and testing dataset by calling the `upload_matrices_to_s3`
    method on both variables:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要通过在两个变量上调用`upload_matrices_to_s3`方法来上传训练集和测试集：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Training the model
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now that we have the data in S3 in the right format for learning, we can start
    training our model to get recommendations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据以适合学习的正确格式存入S3，我们可以开始训练我们的模型以获取推荐。
- en: 'We will instantiate the SageMaker session and define the paths where to read
    and write the data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实例化SageMaker会话并定义读取和写入数据的路径：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With the session, we can instantiate the SageMaker estimator by setting the
    number and type of computers to use. We also specify the hyperparameters. Two
    important parameters to consider are the feature dim (which is the length of our
    training vectors) and the predictor type. Since our problem is posed as a regression,
    we will use regressor. If instead of interest level, we had modeled it as a presence/no
    presence of interest, we would have used the `binary_classifier` value:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '通过会话，我们可以通过设置要使用的计算机的数量和类型来实例化SageMaker估计器。我们还可以指定超参数。两个需要考虑的重要参数是特征维度（这是我们训练向量的长度）和预测器类型。由于我们的问题是设定为回归，我们将使用回归器。如果我们不是将兴趣水平建模为存在/不存在兴趣，而是将其建模为`binary_classifier`值： '
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The logs will show some validation stats and a confirmation for when the model
    has completed:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 日志将显示一些验证统计信息和模型完成时的确认信息：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Getting recommendations
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取推荐
- en: 'Once the model is fitted, we can launch a predictor web service:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型拟合完成，我们可以启动预测器网络服务：
- en: '[PRE26]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will launch the web service endpoint that hosts the trained model and
    is now ready to receive requests with predictions. Let''s take one user from our
    recommendations made with Spark''s ALS and compare it to the predictions made
    by SageMaker:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动托管训练模型的网络服务端点，现在可以接收带有预测请求的请求。让我们从Spark的ALS推荐中选取一个用户，并将其与SageMaker的预测进行比较：
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can collect the features of that user:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以收集那个用户的特征：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, `build_request` is a convenient function to create a JSON request compatible
    with how SageMaker expects the sparse-encoded requests:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`build_request`是一个方便的函数，用于创建与SageMaker期望的稀疏编码请求兼容的JSON请求：
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As we know, the user ID position in the vector is `3297` and the attraction
    position is `4`. We can call the service to get a prediction for the service:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，用户ID在向量中的位置是`3297`，景点位置是`4`。我们可以调用该服务以获取服务的预测：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here''s the output:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: More details about the formats of the JSON requests and responses can be found
    here: [https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 关于JSON请求和响应格式的更多详细信息，请在此处查看：[https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html)。
- en: 'Since we can ask the predictor for the score for an arbitrary pair of (user,
    attraction), we''ll find the scores of all 31 attractions for the user in question
    and then sort by score:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以向预测器询问任意一对（用户，景点）的分数，因此我们将找到问题用户的所有31个景点的分数，然后按分数排序：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Given those scores, we can find the names of the highest-ranking attractions,
    excluding those already visited:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些分数，我们可以找到排名最高的景点名称，排除那些已经访问过的：
- en: '[PRE33]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s compare this with the recommendations made by Spark:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与Spark提供的推荐进行比较：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As the reader might notice, there are many overlapping recommendations. For
    a more thorough analysis regarding the quality of the model and its predictive
    power, we can use the evaluation methods discussed in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms,* as this problem is posed as a regression.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如读者可能注意到的，有许多重叠的建议。为了更深入地分析模型的质量及其预测能力，我们可以使用第3章中讨论的评估方法，即*使用回归算法预测房价*，因为这个问题被设定为回归问题。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied a new type of machine learning algorithm called
    collaborative filtering. This algorithm is used in recommendation systems. We
    looked at memory-based approaches that use similarity measures to find users similar
    to a given user and discover recommendations based on the collective interests
    of the top-ranked similar users. We also studied a model-based approach called
    matrix factorization, that maps users and interests to latent factors and generate
    recommendations based on these factors. We also studied the implementations of
    various collaborative filtering approaches in Apache Spark and SageMaker.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了称为协同过滤的新类型机器学习算法。该算法用于推荐系统。我们研究了基于相似度度量来寻找与给定用户相似的用户并基于排名最高的相似用户的集体兴趣发现推荐的基于内存的方法。我们还研究了称为矩阵分解的基于模型的方法，该方法将用户和兴趣映射到潜在因子，并基于这些因子生成推荐。我们还研究了Apache
    Spark和SageMaker中各种协同过滤方法的实现。
- en: 'In the next chapter, we will focus on a very popular topic: deep learning.
    We will cover the theory behind this advanced field as well as a few modern applications.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将关注一个非常热门的主题：深度学习。我们将涵盖这个高级领域背后的理论以及一些现代应用。
- en: Exercises
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Find an example of a recommendation system that is not described in this chapter.
    Evaluate which approach of collaborative filtering would fit that approach.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一个在本章中没有描述的推荐系统示例。评估哪种协同过滤方法适合那种方法。
- en: For a movie-recommendation engine, explore how the issue of sparsity of data
    affects each algorithm listed in this chapter.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于电影推荐引擎，探讨数据稀疏性问题如何影响本章中列出的每个算法。
