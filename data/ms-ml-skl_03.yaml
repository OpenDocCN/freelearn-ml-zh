- en: Chapter 3. Feature Extraction and Preprocessing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 特征提取与预处理
- en: The examples discussed in the previous chapter used simple numeric explanatory
    variables, such as the diameter of a pizza. Many machine learning problems require
    learning from observations of categorical variables, text, or images. In this
    chapter, you will learn basic techniques for preprocessing data and creating feature
    representations of these observations. These techniques can be used with the regression
    models discussed in [Chapter 2](ch02.html "Chapter 2. Linear Regression"), *Linear
    Regression*, as well as the models we will discuss in subsequent chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章讨论的示例使用了简单的数字解释变量，例如比萨饼的直径。许多机器学习问题需要从类别变量、文本或图像的观察中学习。在本章中，你将学习处理数据的基本技术，并为这些观察创建特征表示。这些技术可以与在[第2章](ch02.html
    "第2章. 线性回归")中讨论的*线性回归*模型一起使用，也可以与我们将在后续章节讨论的模型一起使用。
- en: Extracting features from categorical variables
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从类别变量中提取特征
- en: Many machine learning problems have **categorical**, or **nominal**, rather
    than continuous features. For example, an application that predicts a job's salary
    based on its description might use categorical features such as the job's location.
    Categorical variables are commonly encoded using **one-of-K** or **one-hot** encoding,
    in which the explanatory variable is encoded using one binary feature for each
    of the variable's possible values.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题具有**类别**特征或**名义**特征，而不是连续特征。例如，一个根据职位描述预测薪资的应用程序，可能使用诸如职位所在地点之类的类别特征。类别变量通常使用**一对K**或**独热**编码进行编码，其中解释变量通过每个可能值的一个二进制特征进行编码。
- en: 'For example, let''s assume that our model has a `city` explanatory variable
    that can take one of three values: `New York`, `San Francisco`, or `Chapel Hill`.
    One-hot encoding represents this explanatory variable using one binary feature
    for each of the three possible cities.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们的模型有一个 `city` 解释变量，能够取三个值之一：`New York`、`San Francisco` 或 `Chapel Hill`。独热编码使用每个可能的城市一个二进制特征来表示该解释变量。
- en: 'In scikit-learn, the `DictVectorizer` class can be used to one-hot encode categorical
    features:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，`DictVectorizer` 类可用于对类别特征进行独热编码：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that resulting features will not necessarily be ordered in the feature
    vector as they were encountered. In the first training example, the `city` feature's
    value is `New York`. The second element in the feature vectors corresponds to
    the `New York` value and is set to `1` for the first instance. It may seem intuitive
    to represent the values of a categorical explanatory variable with a single integer
    feature, but this would encode artificial information. For example, the feature
    vectors for the previous example would have only one dimension. `New York` could
    be represented by `0`, `San Francisco` by `1`, and `Chapel Hill` by `2`. This
    representation would encode an order for the values of the variable that does
    not exist in the real world; there is no natural order of cities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，生成的特征在特征向量中的顺序不一定与它们出现的顺序相同。在第一个训练示例中，`city` 特征的值为 `New York`。特征向量中的第二个元素对应于
    `New York` 值，并且在第一次出现时被设置为 `1`。直观上看，可能会认为用单个整数特征来表示类别解释变量的值，但这将编码虚假的信息。例如，前一个示例的特征向量将只有一个维度。`New
    York` 可以表示为 `0`，`San Francisco` 为 `1`，`Chapel Hill` 为 `2`。这种表示法会为变量的值编码一个在现实世界中不存在的顺序；城市之间没有自然顺序。
- en: Extracting features from text
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文本中提取特征
- en: 'Many machine learning problems use text as an explanatory variable. Text must
    be transformed to a different representation that encodes as much of its meaning
    as possible in a feature vector. In the following sections we will review variations
    of the most common representation of text that is used in machine learning: the
    bag-of-words model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题使用文本作为解释变量。文本必须转换为一种不同的表示形式，将其尽可能多的含义编码到特征向量中。在接下来的章节中，我们将回顾机器学习中最常见的文本表示方式的变体：词袋模型。
- en: The bag-of-words representation
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋表示法
- en: The most common representation of text is the **bag-of-words** model. This representation
    uses a multiset, or bag, that encodes the words that appear in a text; the bag-of-words
    does not encode any of the text's syntax, ignores the order of words, and disregards
    all grammar. Bag-of-words can be thought of as an extension to one-hot encoding.
    It creates one feature for each word of interest in the text. The bag-of-words
    model is motivated by the intuition that documents containing similar words often
    have similar meanings. The bag-of-words model can be used effectively for document
    classification and retrieval despite the limited information that it encodes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的最常见表示方法是**袋模型**。这种表示方法使用一个多重集（或袋），该袋编码了文本中出现的单词；袋模型不编码文本的任何语法，忽略单词的顺序，并不考虑任何语法规则。袋模型可以被视为对独热编码（one-hot
    encoding）的扩展。它为文本中每个感兴趣的单词创建一个特征。袋模型的直觉是，包含相似单词的文档通常具有相似的意义。尽管袋模型编码的信息有限，但它仍然可以有效地用于文档分类和检索。
- en: 'A collection of documents is called a **corpus**. Let''s use a corpus with
    the following two documents to examine the bag-of-words model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一组文档称为**语料库**。让我们使用一个包含以下两个文档的语料库来检查袋模型：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This corpus contains eight unique words: `UNC`, `played`, `Duke`, `in`, `basketball`,
    `lost`, `the`, and `game`. The corpus''s unique words comprise its **vocabulary**.
    The bag-of-words model uses a feature vector with an element for each of the words
    in the corpus''s vocabulary to represent each document. Our corpus has eight unique
    words, so each document will be represented by a vector with eight elements. The
    number of elements that comprise a feature vector is called the vector''s **dimension**.
    A **dictionary** maps the vocabulary to indices in the feature vector.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语料库包含八个独特的单词：`UNC`、`played`、`Duke`、`in`、`basketball`、`lost`、`the` 和 `game`。语料库的独特单词构成了它的**词汇**。袋模型使用一个特征向量，每个元素对应语料库词汇中的一个单词，来表示每个文档。我们的语料库有八个独特的单词，因此每个文档将由一个包含八个元素的向量来表示。构成特征向量的元素数量称为向量的**维度**。**字典**将词汇映射到特征向量中的索引位置。
- en: In the most basic bag-of-words representation, each element in the feature vector
    is a binary value that represents whether or not the corresponding word appeared
    in the document. For example, the first word in the first document is `UNC`. The
    first word in the dictionary is `UNC`, so the first element in the vector is equal
    to one. The last word in the dictionary is `game`. The first document does not
    contain the word `game`, so the eighth element in its vector is set to `0`. The
    `CountVectorizer` class can produce a bag-of-words representation from a string
    or file. By default, `CountVectorizer` converts the characters in the documents
    to lowercase, and **tokenizes** the documents. Tokenization is the process of
    splitting a string into **tokens**,or meaningful sequences of characters. Tokens
    frequently are words, but they may also be shorter sequences including punctuation
    characters and affixes. The `CountVectorizer` class tokenizes using a regular
    expression that splits strings on whitespace and extracts sequences of characters
    that are two or more characters in length.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本的袋模型表示中，特征向量中的每个元素都是一个二进制值，表示对应的单词是否出现在文档中。例如，第一个文档中的第一个单词是 `UNC`。字典中的第一个单词是
    `UNC`，因此向量中的第一个元素为1。字典中的最后一个单词是 `game`，第一个文档中没有出现 `game`，因此向量中的第八个元素被设为 `0`。`CountVectorizer`
    类可以从字符串或文件中生成袋模型表示。默认情况下，`CountVectorizer` 将文档中的字符转换为小写，并**分词**。分词是将字符串拆分为**词元**的过程，或者是将字符分割成有意义的序列。词元通常是单词，但也可以是包括标点符号和词缀在内的较短序列。`CountVectorizer`
    类使用一个正则表达式来分词，按空白字符拆分字符串，并提取长度为两个或更多字符的字符序列。
- en: 'The documents in our corpus are represented by the following feature vectors:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们语料库中的文档由以下特征向量表示：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now let''s add a third document to our corpus:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们向语料库中添加第三个文档：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Our corpus''s dictionary now contains the following ten unique words. Note
    that `I` and `a` were not extracted as they do not match the default regular expression
    that CountVectorizer uses to tokenize strings:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们语料库的字典现在包含以下十个独特的单词。注意，`I` 和 `a` 没有被提取出来，因为它们不符合 `CountVectorizer` 用来分词的默认正则表达式：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, our feature vectors are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的特征向量如下：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The meanings of the first two documents are more similar to each other than
    they are to the third document, and their corresponding feature vectors are more
    similar to each other than they are to the third document''s feature vector when
    using a metric such as **Euclidean distance**. The Euclidean distance between
    two vectors is equal to the **Euclidean norm**, or L2 norm, of the difference
    between the two vectors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 前两篇文章的含义比它们与第三篇文章的含义更为相似，而且在使用如**欧几里得距离**这样的度量标准时，它们的特征向量也比与第三篇文章的特征向量更为相似。两个向量之间的欧几里得距离等于两个向量之间差异的**欧几里得范数**，或称L2范数：
- en: '![The bag-of-words representation](img/8365OS_03_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![词袋模型表示](img/8365OS_03_01.jpg)'
- en: 'Recall that the Euclidean norm of a vector is equal to the vector''s magnitude,
    which is given by the following equation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，向量的欧几里得范数等于该向量的大小，大小由以下方程给出：
- en: '![The bag-of-words representation](img/8365OS_03_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![词袋模型表示](img/8365OS_03_02.jpg)'
- en: 'scikit-learn''s `euclidean_distances` function can be used to calculate the
    distance between two or more vectors, and it confirms that the most semantically
    similar documents are also the closest to each other in space. In the following
    example, we will use the `euclidean_distances` function to compare the feature
    vectors for our documents:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的`euclidean_distances`函数可以用来计算两个或多个向量之间的距离，并且它确认了语义上最相似的文档在空间中也是彼此最接近的。以下示例中，我们将使用`euclidean_distances`函数来比较我们的文档特征向量：
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now let's assume that we are using a corpus of news articles instead of our
    toy corpus. Our dictionary may now have hundreds of thousands of unique words
    instead of only twelve. The feature vectors representing the articles will each
    have hundreds of thousands of elements, and many of the elements will be zero.
    Most sports articles will not have any of the words particular to finance articles
    and most culture articles will not have any of the words particular to articles
    about finance. High-dimensional feature vectors that have many zero-valued elements
    are called **sparse vectors**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们使用的是新闻文章语料库，而不是我们之前的简单语料库。我们的词典可能包含数十万个独特的单词，而不仅仅是十二个。表示文章的特征向量将包含数十万个元素，并且许多元素将是零。大多数体育文章不会包含任何特定于财经文章的词汇，大多数文化文章也不会包含任何特定于财经文章的词汇。具有大量零值元素的高维特征向量称为**稀疏向量**。
- en: Using high-dimensional data creates several problems for all machine learning
    tasks, including those that do not involve text. The first problem is that high-dimensional
    vectors require more memory than smaller vectors. NumPy provides some data types
    that mitigate this problem by efficiently representing only the nonzero elements
    of sparse vectors.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高维数据会为所有机器学习任务带来一些问题，包括那些不涉及文本的任务。第一个问题是，高维向量需要比小型向量更多的内存。NumPy提供了一些数据类型，通过高效地表示稀疏向量的非零元素，缓解了这个问题。
- en: The second problem is known as the **curse of dimensionality**, or the **Hughes
    effect**. As the feature space's dimensionality increases, more training data
    is required to ensure that there are enough training instances with each combination
    of the feature's values. If there are insufficient training instances for a feature,
    the algorithm may overfit noise in the training data and fail to generalize. In
    the following sections, we will review several strategies to reduce the dimensionality
    of text features. In [Chapter 7](ch07.html "Chapter 7. Dimensionality Reduction
    with PCA"), *Dimensionality Reduction with PCA*, we will review techniques for
    numerical dimensionality reduction.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题被称为**维度灾难**，或**Hughes效应**。随着特征空间的维度增加，需要更多的训练数据以确保每个特征值组合都有足够的训练实例。如果某个特征的训练实例不足，算法可能会对训练数据中的噪声进行过拟合，导致无法泛化。在接下来的部分，我们将回顾几种减少文本特征维度的策略。在[第7章](ch07.html
    "第7章 PCA降维")中，*PCA降维*一节中，我们将回顾数值维度减少的技术。
- en: Stop-word filtering
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词过滤
- en: A basic strategy to reduce the dimensions of the feature space is to convert
    all of the text to lowercase. This is motivated by the insight that the letter
    case does not contribute to the meanings of most words; `sandwich` and `Sandwich`
    have the same meaning in most contexts. Capitalization may indicate that a word
    is at the beginning of a sentence, but the bag-of-words model has already discarded
    all information from word order and grammar.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 降低特征空间维度的一个基本策略是将所有文本转换为小写。这一策略源于这样一个观点：字母的大小写不会影响大多数单词的含义；`sandwich`和`Sandwich`在大多数语境下具有相同的意义。大写字母可能表示某个单词位于句子的开头，但袋装词模型（bag-of-words
    model）已经丢弃了所有关于单词顺序和语法的信息。
- en: 'A second strategy is to remove words that are common to most of the documents
    in the corpus. These words, called **stop** **words**, include determiners such
    as `the`, `a`, and `an`; auxiliary verbs such as `do`, `be`, and `will`; and prepositions
    such as `on`, `around`, and `beneath`. Stop words are often functional words that
    contribute to the document''s meaning through grammar rather than their denotations.
    The `CountVectorizer` class can filter stop words provided as the `stop_words`
    keyword argument and also includes a basic English stop list. Let''s recreate
    the feature vectors for our documents using stop filtering:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略是删除语料库中大多数文档都常见的单词。这些单词被称为**停用词**，包括限定词如`the`、`a`和`an`；助动词如`do`、`be`和`will`；以及介词如`on`、`around`和`beneath`。停用词通常是功能性词汇，它们通过语法而非词汇本身的定义为文档的含义做出贡献。`CountVectorizer`类可以通过`stop_words`关键字参数过滤停用词，并且还包含一个基本的英语停用词表。我们来重新创建我们的文档特征向量，并应用停用词过滤：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The feature vectors have now fewer dimensions, and the first two document vectors
    are still more similar to each other than they are to the third document.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量现在的维度较少，前两个文档向量彼此之间仍然比与第三个文档更相似。
- en: Stemming and lemmatization
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: While stop filtering is an easy strategy for dimensionality reduction, most
    stop lists contain only a few hundred words. A large corpus may still have hundreds
    of thousands of unique words after filtering. Two similar strategies for further
    reducing dimensionality are called **stemming** and **lemmatization**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然停用词过滤是一种简单的降维策略，但大多数停用词表只包含几百个单词。即便经过过滤，大型语料库仍可能包含数十万个独特的词汇。两种进一步降维的相似策略被称为**词干提取**和**词形还原**。
- en: A high-dimensional document vector may separately encode several derived or
    inflected forms of the same word. For example, `jumping` and `jumps` are both
    forms of the word `jump`; a document vector in a corpus of long-jumping articles
    may encode each inflected form with a separate element in the feature vector.
    Stemming and lemmatization are two strategies to condense inflected and derived
    forms of a word into a single feature.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 高维文档向量可能单独编码同一单词的多种派生或屈折形式。例如，`jumping`和`jumps`都是`jump`这个词的形式；在一个关于跳远的语料库中，文档向量可能会用特征向量中的单独元素编码每个屈折形式。词干提取和词形还原是将单词的屈折形式和派生形式压缩成单一特征的两种策略。
- en: 'Let''s consider another toy corpus with two documents:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑另一个包含两个文档的玩具语料库：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The documents have similar meanings, but their feature vectors have no elements
    in common. Both documents contain a conjugation of `ate` and an inflected form
    of `sandwich`. Ideally, these similarities should be reflected in the feature
    vectors. Lemmatization is the process of determining the **lemma**, or the morphological
    root, of an inflected word based on its context. Lemmas are the base forms of
    words that are used to key the word in a dictionary. **Stemming** has a similar
    goal to lemmatization, but it does not attempt to produce the morphological roots
    of words. Instead, stemming removes all patterns of characters that appear to
    be affixes, resulting in a token that is not necessarily a valid word. Lemmatization
    frequently requires a lexical resource, like WordNet, and the word's part of speech.
    Stemming algorithms frequently use rules instead of lexical resources to produce
    stems and can operate on any token, even without its context.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文档具有相似的含义，但它们的特征向量没有共同的元素。两个文档都包含了`ate`的一个词形和`sandwich`的一个屈折形式。理想情况下，这些相似性应该体现在特征向量中。词形还原是根据词语的上下文确定屈折词的**词形**，即形态学根词的过程。词形是单词的基础形式，用来在字典中查找单词。**词干提取**与词形还原的目标相似，但它并不试图生成单词的形态学根词。相反，词干提取移除所有看起来像是词缀的字符模式，结果是一个不一定是有效单词的词形。词形还原通常需要一个词汇资源，如WordNet，以及单词的词性。词干提取算法通常使用规则而非词汇资源来生成词干，并且可以在任何令牌上运行，即使没有其上下文。
- en: 'Let''s consider lemmatization of the word `gathering` in two documents:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑在两个文档中对单词`gathering`进行词形还原：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the first sentence `gathering` is a verb, and its lemma is `gather`. In
    the second sentence `gathering` is a noun, and its lemma is `gathering`. We will
    use the **Natural Language Tool Kit** (**NTLK**) to stem and lemmatize the corpus.
    NLTK can be installed using the instructions at [http://www.nltk.org/install.html](http://www.nltk.org/install.html).
    After installation, execute the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一句中，`gathering`是动词，其词形还原为`gather`。在第二句中，`gathering`是名词，其词形还原为`gathering`。我们将使用**自然语言工具包**（**NLTK**）来进行词干提取和词形还原。可以通过[http://www.nltk.org/install.html](http://www.nltk.org/install.html)上的说明安装NLTK。安装完成后，执行以下代码：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then follow the instructions to download the corpora for NLTK.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按照说明下载NLTK的语料库。
- en: 'Using the parts of speech of `gathering`, NLTK''s `WordNetLemmatizer` correctly
    lemmatizes the words in both documents as shown in the following example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gathering`的词性，NLTK的`WordNetLemmatizer`正确地对两个文档中的单词进行了词形还原，结果如以下示例所示：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s compare lemmatization with stemming. The Porter stemmer cannot consider
    the inflected form''s part of speech and returns `gather` for both documents:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将词形还原与词干提取进行比较。Porter 词干提取器无法考虑屈折形式的词性，并且对两个文档都返回`gather`：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now let''s lemmatize our toy corpus:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对我们的示例语料库进行词形还原：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Through stemming and lemmatization, we reduced the dimensionality of our feature
    space. We produced feature representations that more effectively encode the meanings
    of the documents despite the fact that the words in the corpus's dictionary are
    inflected differently in the sentences.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过词干提取和词形还原，我们减少了特征空间的维度。尽管语料库字典中的单词在句子中屈折形式不同，我们仍然生成了更有效地编码文档含义的特征表示。
- en: Extending bag-of-words with TF-IDF weights
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展词袋模型并加入TF-IDF权重
- en: In the previous section we used the bag-of-words representation to create feature
    vectors that encode whether or not a word from the corpus's dictionary appears
    in a document. These feature vectors do not encode grammar, word order, or the
    frequencies of words. It is intuitive that the frequency with which a word appears
    in a document could indicate the extent to which a document pertains to that word.
    A long document that contains one occurrence of a word may discuss an entirely
    different topic than a document that contains many occurrences of the same word.
    In this section, we will create feature vectors that encode the frequencies of
    words, and discuss strategies to mitigate two problems caused by encoding term
    frequencies.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了词袋模型（bag-of-words）表示法来创建特征向量，这些向量编码了语料库字典中的单词是否出现在文档中。这些特征向量并没有编码语法、单词顺序或单词的频率。直观地说，单词在文档中出现的频率可以表明该文档与该单词的相关程度。包含一个单词出现的长文档可能讨论的主题完全不同于包含该单词多次出现的文档。在本节中，我们将创建编码单词频率的特征向量，并讨论解决由编码术语频率所引发的两个问题的策略。
- en: Instead of using a binary value for each element in the feature vector, we will
    now use an integer that represents the number of times that the words appeared
    in the document.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在不再使用二进制值表示特征向量中的每个元素，而是使用一个整数，表示单词在文档中出现的次数。
- en: 'We will use the following corpus. With stop word filtering, the corpus is represented
    by the following feature vector:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下语料库。经过停用词过滤后，语料库通过以下特征向量表示：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The element for `dog` is now set to `1` and the element for `sandwich` is set
    to `2` to indicate that the corresponding words occurred once and twice, respectively.
    Note that the `binary` keyword argument of `CountVectorizer` is omitted; its default
    value is `False`, which causes it to return raw term frequencies rather than binary
    frequencies. Encoding the terms' raw frequencies in the feature vector provides
    additional information about the meanings of the documents, but assumes that all
    of the documents are of similar lengths.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`dog` 的元素现在设置为 `1`，而 `sandwich` 的元素设置为 `2`，表示相应的单词分别出现了一次和两次。请注意，`CountVectorizer`
    的 `binary` 关键字参数被省略了；它的默认值是 `False`，这导致它返回的是原始的词频而非二值频率。在特征向量中对词汇的原始频率进行编码提供了更多关于文档含义的信息，但假设所有文档的长度大致相同。'
- en: 'Many words might appear with the same frequency in two documents, but the documents
    could still be dissimilar if one document is many times larger than the other.
    scikit-learn''s `TfdfTransformer` object can mitigate this problem by transforming
    a matrix of term frequency vectors into a matrix of normalized term frequency
    weights. By default, `TfdfTransformer` smoothes the raw counts and applies L2
    normalization. The smoothed, normalized term frequencies are given by the following
    equation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个文档中，可能会有许多单词的出现频率相同，但如果一个文档的长度比另一个大得多，两个文档仍然可能是不同的。scikit-learn 的 `TfdfTransformer`
    对象可以通过将词频向量矩阵转换为标准化的词频权重矩阵来缓解这个问题。默认情况下，`TfdfTransformer` 会对原始计数进行平滑处理，并应用L2标准化。平滑后的标准化词频由以下公式给出：
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![扩展词袋模型与TF-IDF权重](img/8365OS_03_03.jpg)'
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_10.jpg) is the
    frequency of term ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_16.jpg)
    in document ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_13.jpg)
    and ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_11.jpg) is the
    L2 norm of the count vector. In addition to normalizing raw term counts, we can
    improve our feature vectors by calculating **logarithmically scaled term frequencies**,
    which scale the counts to a more limited range,or **augmented term frequencies**,
    which further mitigates the bias for longer documents. Logarithmically scaled
    term frequencies are given by the following equation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![扩展词袋模型与TF-IDF权重](img/8365OS_03_10.jpg) 是文档 ![扩展词袋模型与TF-IDF权重](img/8365OS_03_13.jpg)
    中词 ![扩展词袋模型与TF-IDF权重](img/8365OS_03_16.jpg) 的频率，且 ![扩展词袋模型与TF-IDF权重](img/8365OS_03_11.jpg)
    是计数向量的L2范数。除了对原始词频进行归一化之外，我们还可以通过计算**对数缩放的词频**来改进特征向量，这将词频缩放到更有限的范围，或者通过计算**增强的词频**来进一步减轻较长文档的偏差。对数缩放的词频由以下公式给出：'
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![扩展词袋模型与TF-IDF权重](img/8365OS_03_04.jpg)'
- en: 'The `TfdfTransformer` object calculates logarithmically scaled term frequencies
    when its `sublinear_tf` keyword argument is set to `True`. Augmented frequencies
    are given by the following equation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `TfdfTransformer` 对象的 `sublinear_tf` 关键字参数设置为 `True` 时，它会计算对数缩放的词频。增强的频率由以下公式给出：
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_05.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![扩展词袋模型与TF-IDF权重](img/8365OS_03_05.jpg)'
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_12.jpg) is the
    greatest frequency of all of the words in document ![Extending bag-of-words with
    TF-IDF weights](img/8365OS_03_13.jpg). scikit-learn 0.15.2 does not implement
    augmented term frequencies, but the output of `CountVectorizer` can be easily
    transformed.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![扩展词袋模型与TF-IDF权重](img/8365OS_03_12.jpg) 是文档 ![扩展词袋模型与TF-IDF权重](img/8365OS_03_13.jpg)
    中所有单词的最大频率。scikit-learn 0.15.2 版本没有实现增强词频，但可以轻松地将 `CountVectorizer` 的输出转换。'
- en: 'Normalization, logarithmically scaled term frequencies, and augmented term
    frequencies can represent the frequencies of terms in a document while mitigating
    the effects of different document sizes. However, another problem remains with
    these representations. The feature vectors contain large weights for terms that
    occur frequently in a document, even if those terms occur frequently in most documents
    in the corpus. These terms do not help to represent the meaning of a particular
    document relative to the rest of the corpus. For example, most of the documents
    in a corpus of articles about Duke''s basketball team could include the words
    `basketball`, `Coach K`, and `flop`. These words can be thought of as corpus-specific
    stop words and may not be useful to calculate the similarity of documents. The
    **inverse document frequency** (**IDF**) is a measure of how rare or common a
    word is in a corpus. The inverse document frequency is given by the following
    equation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化、对数尺度的词频和增强的词频可以表示文档中词项的频率，同时减轻文档大小不同所带来的影响。然而，这些表示方法仍然存在一个问题。特征向量中对于在文档中频繁出现的词项赋予了很大的权重，即使这些词项在语料库的大多数文档中也频繁出现。这些词项并不能有效地帮助表示特定文档相对于整个语料库的含义。例如，大部分关于杜克大学篮球队的文章可能都会出现`basketball`、`Coach
    K`和`flop`这些词汇。这些词汇可以被视为语料库特有的停用词，可能对计算文档相似度没有帮助。**逆文档频率**（**IDF**）是衡量词项在语料库中稀有或常见程度的指标。逆文档频率的计算公式如下：
- en: '![Extending bag-of-words with TF-IDF weights](img/8365OS_03_06.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![扩展词袋模型与TF-IDF权重](img/8365OS_03_06.jpg)'
- en: 'Here, ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_14.jpg) is
    the total number of documents in the corpus and ![Extending bag-of-words with
    TF-IDF weights](img/8365OS_03_15.jpg) is the number of documents in the corpus
    that contain the term ![Extending bag-of-words with TF-IDF weights](img/8365OS_03_16.jpg).
    A term''s **TF-IDF** value is the product of its term frequency and inverse document
    frequency. `TfidfTransformer` returns TF-IDF''s weight when its `use_idf` keyword
    argument is set to its default value, `True`. Since TF-IDF weighted feature vectors
    are commonly used to represent text, scikit-learn provides a `TfidfVectorizer`
    class that wraps `CountVectorizer` and `TfidfTransformer`. Let''s use `TfidfVectorizer`
    to create TF-IDF weighted feature vectors for our corpus:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![扩展词袋模型与TF-IDF权重](img/8365OS_03_14.jpg)表示语料库中文档的总数，![扩展词袋模型与TF-IDF权重](img/8365OS_03_15.jpg)表示包含术语![扩展词袋模型与TF-IDF权重](img/8365OS_03_16.jpg)的文档数量。一个术语的**TF-IDF**值是其词频和逆文档频率的乘积。`TfidfTransformer`在其`use_idf`关键字参数设置为默认值`True`时返回TF-IDF的权重。由于TF-IDF加权特征向量通常用于表示文本，scikit-learn提供了一个`TfidfVectorizer`类，它封装了`CountVectorizer`和`TfidfTransformer`。我们可以使用`TfidfVectorizer`为我们的语料库创建TF-IDF加权特征向量：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: By comparing the TF-IDF weights to the raw term frequencies, we can see that
    words that are common to many of the documents in the corpus, such as `sandwich`,
    have been penalized.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将TF-IDF权重与原始词频进行比较，我们可以看到，像`沙拉`这种在语料库中许多文档中都出现的词汇已经被削弱权重。
- en: Space-efficient feature vectorizing with the hashing trick
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空间高效的特征向量化方法，使用哈希技巧
- en: 'In this chapter''s previous examples, a dictionary containing all of the corpus''s
    unique tokens is used to map a document''s tokens to the elements of a feature
    vector. Creating this dictionary has two drawbacks. First, two passes are required
    over the corpus: the first pass is used to create the dictionary and the second
    pass is used to create feature vectors for the documents. Second, the dictionary
    must be stored in memory, which could be prohibitive for large corpora. It is
    possible to avoid creating this dictionary through applying a hash function to
    the token to determine its index in the feature vector directly. This shortcut
    is called the **hashing trick**. The following example uses `HashingVectorizer`
    to demonstrate the hashing trick:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章之前的示例中，使用一个包含语料库所有唯一词项的字典，将文档中的词项映射到特征向量的元素。创建这个字典有两个缺点。首先，语料库需要进行两次遍历：第一次遍历用来创建字典，第二次遍历用来为文档创建特征向量。其次，字典必须存储在内存中，对于大型语料库来说，这可能是不可接受的。通过对词项应用哈希函数来直接确定其在特征向量中的索引，可以避免创建字典。这个捷径称为**哈希技巧**。以下示例使用`HashingVectorizer`来演示哈希技巧：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The hashing trick is stateless. It can be used to create feature vectors in
    both parallel and online, or streaming, applications because it does not require
    an initial pass over the corpus.. Note that `n_features` is an optional keyword
    argument. Its default value, ![Space-efficient feature vectorizing with the hashing
    trick](img/8365OS_03_17.jpg), is adequate for most problems; it is set to `6`
    here so that the matrix will be small enough to print and still display all of
    the nonzero features. Also, note that some of the term frequencies are negative.
    Since hash collisions are possible, `HashingVectorizer` uses a signed hash function.
    The value of a feature takes the same sign as its token's hash; if the term `cats`
    appears twice in a document and is hashed to `-3`, the fourth element of the document's
    feature vector will be decremented by two. If the term `dogs` also appears twice
    and is hashed to `3`, the fourth element of the feature vector will be incremented
    by two. Using a signed hash function creates the possibility that errors from
    hash collisions will cancel each other out rather than accumulate; a loss of information
    is preferable to a loss of information and the addition of spurious information.
    Another disadvantage of the hashing trick is that the resulting model is more
    difficult to inspect, as the hashing function cannot recall what input token is
    mapped to each element of the feature vector.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希技巧是无状态的。它可以用于在并行和在线或流式应用中创建特征向量，因为它不需要对语料库进行初始遍历。请注意，`n_features`是一个可选的关键字参数。它的默认值，![空间高效的特征向量化与哈希技巧](img/8365OS_03_17.jpg)，对于大多数问题来说是足够的；这里设置为`6`，以使矩阵足够小，既可以打印出来，又能显示所有非零特征。此外，请注意，某些词频是负数。由于哈希冲突是可能的，`HashingVectorizer`使用了一个带符号的哈希函数。特征的值与其标记的哈希值具有相同的符号；如果词语`cats`在文档中出现两次，并且其哈希值为`-3`，则文档特征向量的第四个元素将减少二。若词语`dogs`也出现两次，并且哈希值为`3`，特征向量的第四个元素将增加二。使用带符号的哈希函数可以使哈希冲突的错误互相抵消，而不是累积；信息丢失优于信息丢失和引入虚假信息。哈希技巧的另一个缺点是，生成的模型更难以检查，因为哈希函数无法回忆出每个特征向量元素映射的输入标记是什么。
- en: Extracting features from images
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从图像中提取特征
- en: Computer vision is the study and design of computational artifacts that process
    and understand images. These artifacts sometimes employ machine learning. An overview
    of computer vision is far beyond the scope of this book, but in this section we
    will review some basic techniques used in computer vision to represent images
    in machine learning problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉是研究和设计处理和理解图像的计算工件。这些工件有时会使用机器学习。计算机视觉的概述远远超出了本书的范围，但在本节中，我们将回顾一些在计算机视觉中用于表示机器学习问题中图像的基本技术。
- en: Extracting features from pixel intensities
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从像素强度中提取特征
- en: A digital image is usually a raster, or pixmap, that maps colors to coordinates
    on a grid. An image can be viewed as a matrix in which each element represents
    a color. A basic feature representation for an image can be constructed by reshaping
    the matrix into a vector by concatenating its rows together. **Optical character
    recognition** (**OCR**) is a canonical machine learning problem. Let's use this
    technique to create basic feature representations that could be used in an OCR
    application for recognizing hand-written digits in character-delimited forms.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像通常是一个栅格（或像素图），它将颜色映射到网格上的坐标。图像可以被视为一个矩阵，其中每个元素代表一个颜色。图像的基本特征表示可以通过将矩阵的行连接在一起并重新塑形为一个向量来构造。**光学字符识别**（**OCR**）是一个典型的机器学习问题。我们将使用此技术创建可以用于OCR应用程序的基本特征表示，用于识别字符分隔的表单中的手写数字。
- en: 'The `digits` dataset included with scikit-learn contains grayscale images of
    more than 1,700 hand-written digits between zero and nine. Each image has eight
    pixels on a side. Each pixel is represented by an intensity value between zero
    and 16; white is the most intense and is indicated by zero, and black is the least
    intense and is indicated by 16\. The following figure is an image of a hand-written
    digit taken from the dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn附带的`digits`数据集包含超过1,700个手写数字（从零到九）的灰度图像。每个图像的边长为八个像素。每个像素由一个强度值表示，范围在零到16之间；白色是最强的，表示为零，黑色是最弱的，表示为16。下图是从数据集中提取的手写数字图像：
- en: '![Extracting features from pixel intensities](img/8365OS_03_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![从像素强度中提取特征](img/8365OS_03_07.jpg)'
- en: 'Let''s create a feature vector for the image by reshaping its 8 x 8 matrix
    into a 64-dimensional vector:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将8 x 8的矩阵重新塑形为64维向量，为图像创建一个特征向量：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This representation can be effective for some basic tasks, like recognizing
    printed characters. However, recording the intensity of every pixel in the image
    produces prohibitively large feature vectors. A tiny 100 x 100 grayscale image
    would require a 10,000-dimensional vector, and a 1920 x 1080 grayscale image would
    require a 2,073,600-dimensional vector. Unlike the TF-IDF feature vectors we created,
    in most problems these vectors are not sparse. Space-complexity is not the only
    disadvantage of this representation; learning from the intensities of pixels at
    particular locations results in models that are sensitive to changes in the scale,
    rotation, and translation of images. A model trained on our basic feature representations
    might not be able to recognize the same zero if it were shifted a few pixels in
    any direction, enlarged, or rotated a few degrees. Furthermore, learning from
    pixel intensities is itself problematic, as the model can become sensitive to
    changes in illumination. For these reasons, this representation is ineffective
    for tasks that involve photographs or other natural images. Modern computer vision
    applications frequently use either hand-engineered feature extraction methods
    that are applicable to many different problems, or automatically learn features
    without supervision problem using techniques such as deep learning. We will focus
    on the former in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方法对于一些基本任务（如识别打印字符）是有效的。然而，记录图像中每个像素的强度会生成过于庞大的特征向量。一张100 x 100的灰度图像将需要一个10,000维的向量，而一张1920
    x 1080的灰度图像将需要一个2,073,600维的向量。与我们之前创建的TF-IDF特征向量不同，在大多数问题中，这些向量并不是稀疏的。空间复杂度并不是这种表示方法的唯一缺点；从特定位置像素的强度中学习会导致模型对图像的尺度、旋转和位移变化敏感。训练于我们基本特征表示的模型可能无法识别即使是轻微位移、放大或旋转几度后的相同数字。此外，从像素强度中学习本身也有问题，因为模型可能会对光照变化产生敏感性。因此，这种表示方法对于涉及照片或其他自然图像的任务是无效的。现代计算机视觉应用通常使用手工设计的特征提取方法，这些方法适用于许多不同的问题，或者使用深度学习等技术自动学习无监督问题的特征。在下一节中，我们将重点介绍前者。
- en: Extracting points of interest as features
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将兴趣点提取为特征
- en: 'The feature vector we created previously represents every pixel in the image;
    all of the informative attributes of the image are represented and all of the
    noisy attributes are represented too. After inspecting the training data, we can
    see that all of the images have a perimeter of white pixels; these pixels are
    not useful features. Humans can quickly recognize many objects without observing
    every attribute of the object. We can recognize a car from the contours of the
    hood without observing the rear-view mirrors, and we can recognize an image of
    a human face from a nose or mouth. This intuition is motivation to create representations
    of only the most informative attributes of an image. These informative attributes,
    or **points of interest**, are points that are surrounded by rich textures and
    can be reproduced despite perturbing the image. **Edges** and **corners** are
    two common types of points of interest. An edge is a boundary at which pixel intensity
    rapidly changes, and a corner is an intersection of two edges. Let''s use scikit-image
    to extract points of interest from the following figure:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前创建的特征向量表示了图像中的每个像素；图像的所有信息属性和噪声属性都被表示出来。检查训练数据后，我们可以发现所有图像的边缘都有一圈白色像素；这些像素并不是有用的特征。人类可以快速识别许多物体，而不需要观察物体的每个属性。我们可以通过观察车盖的轮廓来识别一辆车，而无需观察后视镜；我们可以通过鼻子或嘴巴来识别一张人脸的图像。这种直觉促使我们创建仅包含图像最有信息量属性的表示。这些信息属性或**兴趣点**是那些被丰富纹理包围，并且即使在图像扰动的情况下也能被重现的点。**边缘**和**角点**是两种常见的兴趣点类型。边缘是像素强度快速变化的边界，角点是两条边缘的交点。我们使用scikit-image提取下图中的兴趣点：
- en: '![Extracting points of interest as features](img/8365OS_03_08.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![将兴趣点提取为特征](img/8365OS_03_08.jpg)'
- en: 'The code to extract the points of interest is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 提取兴趣点的代码如下：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The following figure plots the extracted points of interest. Of the image's
    230400 pixels, 466 were extracted as points of interest. This representation is
    much more compact; ideally, there is enough variation proximal to the points of
    interest to reproduce them despite changes in the image's illumination.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了提取的兴趣点。在该图像的230400个像素中，有466个被提取为兴趣点。这种表示方式更为紧凑；理想情况下，兴趣点附近有足够的变化，能够重建它们，即使图像的光照发生了变化。
- en: '![Extracting points of interest as features](img/8365OS_03_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![提取兴趣点作为特征](img/8365OS_03_09.jpg)'
- en: SIFT and SURF
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SIFT和SURF
- en: '**Scale-Invariant Feature Transform** (**SIFT**) is a method for extracting
    features from an image that is less sensitive to the scale, rotation, and illumination
    of the image than the extraction methods we have previously discussed. Each SIFT
    feature, or descriptor, is a vector that describes edges and corners in a region
    of an image. Unlike the points of interest in our previous example, SIFT also
    captures information about the composition of each point of interest and its surroundings.
    **Speeded-Up Robust Features** (**SURF**) is another method of extracting interesting
    points of an image and creating descriptions that are invariant of the image''s
    scale, orientation, and illumination.SURF can be computed more quickly than SIFT,
    and it is more effective at recognizing features across images that have been
    transformed in certain ways.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**尺度不变特征变换**（**SIFT**）是一种从图像中提取特征的方法，相较于我们之前讨论的提取方法，它对图像的尺度、旋转和光照的敏感度较低。每个SIFT特征或描述符是一个向量，用于描述图像某一区域的边缘和角点。与我们之前示例中的兴趣点不同，SIFT还捕捉了每个兴趣点及其周围环境的组成信息。**加速稳健特征**（**SURF**）是另一种从图像中提取兴趣点并创建与图像的尺度、方向和光照无关的描述的方法。SURF的计算速度比SIFT更快，并且在识别经过某些变换的图像特征时更为有效。'
- en: Explaining how SIFT and SURF extraction are implemented is beyond the scope
    of this book. However, with an intuition for how they work, we can still effectively
    use libraries that implement them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 解释SIFT和SURF提取的实现超出了本书的范围。然而，通过对它们工作原理的直觉理解，我们仍然可以有效地使用实现这些算法的库。
- en: In this example, we will extract SURF from the following image using the `mahotas`
    library.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用`mahotas`库从以下图像中提取SURF。
- en: '![SIFT and SURF](img/8365OS_03_18.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![SIFT和SURF](img/8365OS_03_18.jpg)'
- en: 'Like the extracted points of interest, the extracted SURF are only the first
    step in creating a feature representation that could be used in a machine learning
    task. Different SURF will be extracted for each instance in the training set.
    In [Chapter 6](ch06.html "Chapter 6. Clustering with K-Means"), *Clustering with
    K-Means*, we will cluster extracted SURF to learn features that can be used by
    an image classifier. In the following example we will use `mahotas` to extract
    SURF descriptors:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就像提取的兴趣点一样，提取的SURF只是创建特征表示的第一步，这些表示可以用于机器学习任务。在训练集中，每个实例都会提取不同的SURF。在[第6章](ch06.html
    "第6章：K均值聚类")，*K均值聚类*中，我们将聚类提取的SURF，以学习可以被图像分类器使用的特征。在以下示例中，我们将使用`mahotas`库来提取SURF描述符：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Data standardization
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据标准化
- en: 'Many estimators perform better when they are trained on standardized data sets.
    Standardized data has **zero mean** and **unit variance**. An explanatory variable
    with zero mean is centered about the origin; its average value is zero. A feature
    vector has unit variance when the variances of its features are all of the same
    order of magnitude. For example, assume that a feature vector encodes two explanatory
    variables. The first values of the first variable range from zero to one. The
    values of the second explanatory variable range from zero to 100,000\. The second
    feature must be scaled to a range closer to {0,1} for the data to have unit variance.
    If a feature''s variance is orders of magnitude greater than the variances of
    the other features, that feature may dominate the learning algorithm and prevent
    it from learning from the other variables. Some learning algorithms also converge
    to the optimal parameter values more slowly when data is not standardized. The
    value of an explanatory variable can be standardized by subtracting the variable''s
    mean and dividing the difference by the variable''s standard deviation. Data can
    be easily standardized using scikit-learn''s `scale` function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 许多估算器在训练标准化数据集时表现更好。标准化数据具有**零均值**和**单位方差**。一个具有零均值的解释变量是围绕原点居中的；它的平均值为零。特征向量具有单位方差，当其特征的方差都在同一数量级时，才满足单位方差。例如，假设一个特征向量编码了两个解释变量，第一个变量的值范围从零到一，第二个解释变量的值范围从零到100,000。第二个特征必须缩放到接近{0,1}的范围，以确保数据具有单位方差。如果一个特征的方差比其他特征大几个数量级，那么这个特征可能会主导学习算法，导致其无法从其他变量中学习。一些学习算法在数据没有标准化时，也会更慢地收敛到最佳参数值。可以通过减去变量的均值并将差值除以变量的标准差来标准化一个解释变量。使用scikit-learn的`scale`函数可以轻松地标准化数据：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed feature extraction and developed an understanding
    about the basic techniques for transforming arbitrary data into feature representations
    that can be used by machine learning algorithms. First, we created features from
    categorical explanatory variables using one-hot encoding and scikit-learn''s `DictVectorizer`.Then,
    we discussed the creation of feature vectors for one of the most common types
    of data used in machine learning problems: text. We worked through several variations
    of the bag-of-words model, which discards all syntax and encodes only the frequencies
    of the tokens in a document. We began by creating basic binary term frequencies
    with `CountVectorizer`. You learned to preprocess text by filtering stop words
    and stemming tokens, and you also replaced the term counts in our feature vectors
    with TF-IDF weights that penalize common words and normalize for documents of
    different lengths. Next, we created feature vectors for images. We began with
    an optical character recognition problem in which we represented images of hand-written
    digits with flattened matrices of pixel intensities. This is a computationally
    costly approach. We improved our representations of images by extracting only
    their most interesting points as SURF descriptors.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了特征提取，并对将任意数据转化为机器学习算法可以使用的特征表示的基本技术有了理解。首先，我们使用独热编码和scikit-learn的`DictVectorizer`从类别型解释变量中创建了特征。然后，我们讨论了为机器学习问题中最常见的数据类型之一——文本数据——创建特征向量。我们研究了词袋模型的几种变体，该模型舍弃了所有语法信息，仅对文档中令牌的频率进行编码。我们从创建基本的二进制词频开始，使用了`CountVectorizer`。你学会了通过过滤停用词和词干提取令牌来预处理文本，并且将特征向量中的词频替换为TF-IDF权重，这些权重对常见词语进行了惩罚，并对不同长度的文档进行了标准化。接下来，我们为图像创建了特征向量。我们从一个光学字符识别问题开始，其中我们将手写数字的图像表示为像素强度的平铺矩阵。这是一种计算上比较昂贵的方法。我们通过仅提取图像中最有趣的点作为SURF描述符，改进了图像的表示。
- en: Finally, you learned to standardize data to ensure that our estimators can learn
    from all of the explanatory variables and can converge as quickly as possible.
    We will use these feature extraction techniques in the subsequent chapters' examples.
    In the next chapter, we will combine the bag-of-words representation with a generalization
    of multiple linear regressions to classify documents.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学会了如何标准化数据，以确保我们的估算器能够从所有解释变量中学习，并且能够尽可能快速地收敛。我们将在后续章节的示例中使用这些特征提取技术。在下一章，我们将结合词袋模型表示法和多重线性回归的泛化方法来分类文档。
