- en: 'Chapter 10: Optimizing Model Hosting and Inference Costs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章：优化模型托管和推理成本
- en: The introduction of more powerful computers (notably with **graphical processing
    units**, or **GPUs**) and powerful **machine learning** (**ML**) frameworks such
    as TensorFlow has resulted in a generational leap in ML capabilities. As ML practitioners,
    our purview now includes optimizing the use of these new capabilities to maximize
    the value we get for the time and money we spend.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 更强大的计算机（特别是**图形处理单元**，或**GPU**）和强大的**机器学习**（**ML**）框架，如TensorFlow的引入，导致了机器学习能力的一个代际飞跃。作为机器学习从业者，我们的视野现在包括优化这些新功能的使用，以最大化我们投入的时间和金钱所获得的价值。
- en: In this chapter, you'll learn how to use multiple deployment strategies to meet
    your training and inference requirements. You'll learn when to get and store inferences
    in advance versus getting them on demand, how to scale inference services to meet
    fluctuating demand, and how to use multiple models for model testing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用多种部署策略来满足您的训练和推理需求。您将学习何时提前获取和存储推理，何时按需获取，如何扩展推理服务以满足波动需求，以及如何使用多个模型进行模型测试。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Real-time inference versus batch inference
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时推理与批量推理
- en: Deploying multiple models behind a single inference endpoint
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个推理端点后面部署多个模型
- en: Scaling inference endpoints to meet inference traffic demands
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展推理端点以满足推理流量需求
- en: Using Elastic Inference for deep learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用弹性推理进行深度学习模型
- en: Optimizing models with SageMaker Neo
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker Neo优化模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which walks you through the setup process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个AWS账户来运行本章包含的示例。如果您尚未设置数据科学环境，请参阅[*第2章*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*，数据科学环境*，该章节将指导您完成设置过程。
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter10](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter10).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 书中包含的代码示例可在GitHub上找到，网址为[https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter10](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter10)。您需要安装Git客户端才能访问它们（[https://git-scm.com/](https://git-scm.com/))。
- en: The code for this chapter is in the `CH10` folder of the GitHub repository.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于GitHub仓库的`CH10`文件夹中。
- en: Real-time inference versus batch inference
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时推理与批量推理
- en: 'SageMaker provides two ways to obtain inferences:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供了两种获取推理的方法：
- en: '**Real-time inference** lets you get a single inference per request, or a small
    number of inferences, with very low latency from a live inference endpoint.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时推理**允许您从实时推理端点以非常低的延迟获取单个推理或少量推理。'
- en: '**Batch inference** lets you get a large number of inferences from a batch
    processing job.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量推理**允许您从批量处理作业中获取大量推理。'
- en: Batch inference is more efficient and more cost-effective. Use it whenever your
    inference requirements allow. We'll explore batch inference first, and then pivot
    to real-time inference.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 批量推理更高效且更具成本效益。只要您的推理需求允许，就使用它。我们将首先探讨批量推理，然后转向实时推理。
- en: Batch inference
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量推理
- en: In many cases, we can make inferences in advance and store them for later use.
    For example, if you want to generate product recommendations for users on an e-commerce
    site, those recommendations may be based on the users' prior purchases and which
    products you want to promote the next day. You can generate the recommendations
    nightly and store them for your e-commerce site to call up when the users browse
    the site.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们可以提前进行推理并将它们存储起来以供以后使用。例如，如果您想为电子商务网站上的用户生成产品推荐，这些推荐可能基于用户的先前购买以及您希望第二天推广的产品。您可以在夜间生成这些推荐并将它们存储起来，以便电子商务网站在用户浏览网站时调用。
- en: 'There are several options for storing batch inferences. Amazon DynamoDB is
    a common choice for several reasons, such as the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 存储批量推理有多种选择。由于以下原因，Amazon DynamoDB是常见的选择，例如：
- en: It is fast. You can look up single values within a few milliseconds.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它很快。您可以在几毫秒内查找单个值。
- en: It is scalable. You can store millions of values at a low cost.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是可扩展的。您可以在低成本下存储数百万个值。
- en: The best access pattern for DynamoDB is looking up values by a high-cardinality
    primary key. This fits well with many inference usage patterns, for example, when
    we want to look up a stored recommendation for an individual user.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于DynamoDB来说，最佳访问模式是通过高基数主键查找值。这非常适合许多推断使用模式，例如，当我们想要查找为单个用户存储的推荐时。
- en: You can use other data stores, including DocumentDB and Aurora, depending on
    your access patterns.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的访问模式，您可以使用其他数据存储，包括DocumentDB和Aurora。
- en: 'In the `CH10` folder of the GitHub repository, you''ll find the `optimize.ipynb`
    notebook. The *Real-time and Batch Inference* section of this repository walks
    you through performing both batch and real-time inference using a simple XGBoost
    model. The following code lets you run a batch inference job:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库的`CH10`文件夹中，您会找到`optimize.ipynb`笔记本。该仓库的*实时和批量推断*部分会指导您使用简单的XGBoost模型进行批量推断和实时推断。以下代码允许您运行批量推断作业：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This job takes approximately 3 minutes to run.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个作业大约需要3分钟来运行。
- en: Real-time inference
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时推断
- en: When you deploy a SageMaker model to a real-time inference endpoint, SageMaker
    deploys the model artifact and your inference code (packaged in a Docker image)
    to one or more inference instances. You now have a live API endpoint for inference,
    and you can invoke it from other software services on demand.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将SageMaker模型部署到实时推断端点时，SageMaker会将模型工件和您的推断代码（打包在Docker镜像中）部署到一个或多个推断实例。现在您有一个用于推断的实时API端点，并且可以根据需要从其他软件服务中调用它。
- en: 'You pay for the inference endpoints (instances) as long as they are running.
    Use real-time inference in the following situations:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 只要推断端点（实例）在运行，您就需要为其付费。在以下情况下使用实时推断：
- en: The inferences are dependent on *context*. For example, if you want to recommend
    a video to watch, the inference may depend on the show your user just finished.
    If you have a large video catalog, you can't generate all the possible permutations
    of recommendations in advance.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断依赖于*上下文*。例如，如果您想推荐一部视频观看，推断可能取决于用户刚刚看完的节目。如果您有一个大型视频目录，您无法提前生成所有可能的推荐排列。
- en: You may need to provide inferences for *new events*. For example, if you are
    trying to classify a credit card transaction as fraudulent or not, you need to
    wait until your user actually attempts a transaction.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能需要为*新事件*提供推断。例如，如果您试图将信用卡交易分类为欺诈或非欺诈，您需要等待您的用户实际尝试交易。
- en: 'The following code deploys an inference endpoint:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码部署了一个推断端点：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the endpoint is live, we can obtain inferences using the endpoint we just
    deployed:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦端点上线，我们可以使用我们刚刚部署的端点来获取推断：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using our simple XGBoost model, an inference takes approximately 30 milliseconds
    to complete.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们简单的XGBoost模型，一个推断大约需要30毫秒来完成。
- en: Cost comparison
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本比较
- en: Consider a scenario where we want to predict the measurements for the next day
    for all of our weather stations and make them available for lookup on an interactive
    website. We have approximately 11,000 unique stations and 7 different parameters
    to predict for each station.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个场景，我们想要预测我们所有气象站第二天测量的数据，并在交互式网站上提供查询。我们大约有11,000个独特的气象站和每个站点的7个不同的预测参数。
- en: With a real-time endpoint using the `ml.m5.2xlarge` instance type, we pay $0.538
    per hour, or approximately $387 per month. With batch inference, we pay $1.075
    per hour for an `ml.m5.4xlarge` instance. The job takes 3 minutes to run per day,
    or 90 minutes per month. That's about $1.61.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ml.m5.2xlarge`实例类型的实时端点，我们每小时支付0.538美元，或大约每月387美元。使用批量推断，我们每小时支付1.075美元用于`ml.m5.4xlarge`实例。每天运行作业需要3分钟，每月需要90分钟。这大约是1.61美元。
- en: The batch inference approach is typically much more cost-effective if you do
    not need context-sensitive real-time predictions. Serving predictions out of a
    NoSQL database is a better option.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不需要上下文相关的实时预测，批量推断方法通常更具成本效益。从NoSQL数据库中提供预测是一个更好的选择。
- en: Deploying multiple models behind a single inference endpoint
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单个推断端点后面部署多个模型
- en: A SageMaker inference endpoint is a logical entity that actually holds a load
    balancer and one or more instances of your inference container. You can deploy
    either multiple versions of the same model or entirely different models behind
    a single endpoint. In this section, we'll look at these two use cases.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker推理端点是实际包含负载均衡器和一个或多个推理容器实例的逻辑实体。你可以在单个端点后面部署同一模型的多个版本或完全不同的模型。在本节中，我们将探讨这两个用例。
- en: Multiple versions of the same model
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同一模型的多个版本
- en: 'A SageMaker endpoint lets you host multiple models that serve different percentages
    of traffic for incoming requests. That capability supports common **continuous
    integration** (**CI**)/**continuous delivery** (**CD**) practices such as canary
    and blue/green deployments. While these practices are similar, they have slightly
    different purposes, as explained here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker端点允许你托管多个模型，这些模型为传入请求提供不同百分比的流量。这种能力支持常见的**持续集成**（**CI**）和**持续交付**（**CD**）实践，如金丝雀和蓝绿部署。虽然这些实践相似，但它们的目的略有不同，如以下所述：
- en: A **canary deployment** means that you let the new version of a model host a
    small percentage of traffic that lets you test a new version of the model on a
    subset of traffic until you are satisfied that it is working well.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金丝雀部署**意味着你让模型的新版本承载一小部分流量，这样你就可以在流量的一小部分上测试模型的新版本，直到你对它的工作效果满意。'
- en: A **blue/green deployment** means that you run two versions of the model at
    the same time, keeping an older version around for quick failover if a problem
    occurs in the new version.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝绿部署**意味着你同时运行模型的两个版本，保留一个较老的版本以便在新的版本出现问题时快速切换。'
- en: In practice, these are variations on a theme. In SageMaker, you designate how
    much traffic each model variant handles. For canary deployments, you'd start with
    a small fraction (usually 1-5%) for the new model versions. For blue/green deployments,
    you'd use 100% for the new version but flip back to 0% if a problem occurs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这些都是主题的变体。在SageMaker中，你指定每个模型变体处理的流量量。对于金丝雀部署，你可能会从新模型版本的小部分（通常为1-5%）开始。对于蓝绿部署，你会使用100%的新版本，但如果出现问题，会切换回0%。
- en: There are other ways to accomplish these deployment modes. For example, you
    can use two inference endpoints and handle traffic shaping using DNS (Route 53),
    a load balancer, or Global Accelerator. But managing the traffic through SageMaker
    simplifies your operational burden and reduces cost, as you don't have to have
    two endpoints running.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些部署模式还有其他方法。例如，你可以使用两个推理端点，并使用DNS（Route 53）、负载均衡器或全球加速器来处理流量整形。但通过SageMaker管理流量可以简化你的运营负担并降低成本，因为你不需要运行两个端点。
- en: 'In the *A/B Testing* section of the notebook, we''ll create another version
    of the model and create a new endpoint that uses both models:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的*A/B测试*部分，我们将创建模型的另一个版本，并创建一个新的端点，该端点使用这两个模型：
- en: 'We''ll start by training another version of the model with a hyperparameter
    change (maximum tree depth of `10` instead of `5`), as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先通过以下方式训练模型的一个新版本，并改变超参数（最大树深度从`5`变为`10`）：
- en: '[PRE3]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we define endpoint variants for each model version. The most important
    parameter here is `initial_weight`, which specifies how much traffic should go
    to each model version. By setting both versions to `1`, the traffic will split
    evenly between them. For an A/B test, you might start with weights of `20` for
    the existing version and `1` for the new version:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为每个模型版本定义端点变体。这里最重要的参数是`initial_weight`，它指定了应该有多少流量流向每个模型版本。通过将两个版本都设置为`1`，流量将在这两个版本之间平均分配。对于A/B测试，你可能从现有版本的权重为`20`，新版本的权重为`1`开始：
- en: '[PRE4]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we deploy a new model using the following two model variants:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用以下两种模型变体部署一个新的模型：
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we can test the new endpoint:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以测试新的端点：
- en: '[PRE6]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You''ll see output that looks like this:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你会看到类似以下输出的内容：
- en: '[PRE7]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Notice that the traffic is flipping between the two versions of the model according
    to the weights we specified. In a production use case, you should automate the
    model endpoint update in your CI/CD or MLOps automation tools.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，流量会根据我们指定的权重在模型的两个版本之间切换。在生产环境中，你应该在CI/CD或MLOps自动化工具中自动化模型端点的更新。
- en: Multiple models
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个模型
- en: In other cases, you may need to run entirely different models. For example,
    perhaps you want one model to serve weather inferences for the United States and
    another model to serve weather inferences for Germany. You can build models that
    are sensitive to differences between these two countries. You can host both models
    behind the same endpoint and direct traffic to them based on the incoming request.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，你可能需要运行完全不同的模型。例如，你可能希望有一个模型为美国提供天气推断，另一个模型为德国提供天气推断。你可以构建对这两个国家之间的差异敏感的模型。你可以在同一个端点后面托管这两个模型，并根据传入的请求将流量引导到它们。
- en: Or, for an A/B test, you might want to control which traffic goes to your new
    model version rather than letting a load balancer perform random weighted distribution.
    If you have an application server that identifies which consumers should use the
    new model version, you can direct that traffic to a specific model behind an inference
    endpoint.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，对于A/B测试，你可能希望控制哪些流量流向你的新模型版本，而不是让负载均衡器执行随机的加权分布。如果你有一个应用服务器可以识别哪些消费者应该使用新的模型版本，你可以将流量引导到推理端点后面的特定模型。
- en: 'In the *Multiple models in a single endpoint* notebook section, we''ll walk
    through an example of creating models optimized for different air quality parameters.
    When we want a prediction, we specify which type of parameter we want, and the
    endpoint directs our request to the appropriate model. This use case is quite
    realistic; it may turn out that it''s difficult to predict both particulate matter
    (`PM25`) and ozone (`O3`) using the same model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在“单个端点中的多个模型”笔记本部分，我们将通过创建针对不同空气质量参数优化的模型的示例来演示。当我们需要预测时，我们指定我们想要的参数类型，端点将我们的请求引导到适当的模型。这个用例非常现实；可能很难使用相同的模型同时预测颗粒物（`PM25`）和臭氧（`O3`）：
- en: 'First, we''re going to prepare new datasets that only contain data for a single
    parameter by creating a Spark processing job:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将通过创建Spark处理作业来准备只包含单个参数数据的新数据集：
- en: '[PRE8]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We'll repeat the preceding step for `PM25` and `O3`.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将对`PM25`和`O3`重复前面的步骤。
- en: 'Now, we will train new XGBoost models against the single-parameter training
    sets, as follows:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将根据单个参数训练集训练新的XGBoost模型，如下所示：
- en: '[PRE9]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define the multi-model class:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义多模型类：
- en: '[PRE10]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we deploy the multi-model endpoint:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将部署多模型端点：
- en: '[PRE11]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point, the endpoint does not actually have any models behind it. We
    need to add them next:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，端点实际上没有任何模型在其后面。我们需要在下一步添加它们：
- en: '[PRE12]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We''re ready to test the endpoint. Download two test files, one for each parameter:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好测试端点。下载两个测试文件，每个参数一个：
- en: '[PRE13]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Read the files and get inferences, specifying which model we want to use:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取文件并获取推断，指定我们想要使用的模型：
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that we've seen how to deploy multiple models for testing or other purposes,
    let's turn to handling fluctuating traffic demands.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何部署多个模型进行测试或其他目的，让我们转向处理波动流量需求。
- en: Scaling inference endpoints to meet inference traffic demands
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展推理端点以满足推理流量需求
- en: When we need a real-time inference endpoint, the processing power requirements
    may vary based on incoming traffic. For example, if we are providing air quality
    inferences for a mobile application, usage will likely fluctuate based on time
    of day. If we provision the inference endpoint for peak load, we will pay too
    much during off-peak times. If we provision the inference endpoint for a smaller
    load, we may hit performance bottlenecks during peak times. We can use inference
    endpoint auto-scaling to adjust capacity to demand.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要实时推理端点时，处理能力需求可能会根据传入流量而变化。例如，如果我们为移动应用程序提供空气质量推断，使用量可能会根据一天中的时间而波动。如果我们为峰值负载配置推理端点，我们将在非峰值时段支付过多。如果我们为较小的负载配置推理端点，我们可能在峰值时段遇到性能瓶颈。我们可以使用推理端点自动扩展来调整容量以满足需求。
- en: There are two types of scaling, vertical and horizontal. **Vertical scaling**
    means that we adjust the size of an individual endpoint instance. **Horizontal
    scaling** means that we adjust the number of endpoint instances. We prefer horizontal
    scaling as it results in less disruption for end users; a load balancer can redistribute
    traffic without having an impact on end users.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种扩展类型，垂直扩展和水平扩展。**垂直扩展**意味着我们调整单个端点实例的大小。**水平扩展**意味着我们调整端点实例的数量。我们更喜欢水平扩展，因为它对最终用户的影响较小；负载均衡器可以在不影响最终用户的情况下重新分配流量。
- en: 'There are four steps to configure autoscaling for a SageMaker inference endpoint:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 配置SageMaker推理端点自动扩展有四个步骤：
- en: Set the minimum and maximum number of instances.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置最小和最大实例数量。
- en: Choose a scaling metric.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择缩放指标。
- en: Set the scaling policy.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置缩放策略。
- en: Set the cooldown period.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置冷却时间。
- en: 'Although you can set up autoscaling automatically using the API, in this section,
    we''ll go through the steps in the console. To begin, go to the **Endpoints**
    section of the SageMaker console, as shown in the following screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以使用API自动设置自动缩放，但在这个部分，我们将通过控制台中的步骤进行说明。首先，前往SageMaker控制台的**端点**部分，如下面的截图所示：
- en: '![Figure 10.1 – Endpoints listed in the SageMaker console'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – SageMaker控制台中的端点列表]'
- en: '](img/B17249_10_01.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_10_01.jpg]'
- en: Figure 10.1 – Endpoints listed in the SageMaker console
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – SageMaker控制台中的端点列表]'
- en: 'Select one of your endpoints, and in the section called **Endpoint runtime
    settings**, choose **Configure auto scaling**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 选择您的端点之一，在名为**端点运行时设置**的部分，选择**配置自动缩放**：
- en: '![Figure 10.2 – Endpoint runtime settings'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 端点运行时设置]'
- en: '](img/B17249_10_02.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_10_02.jpg]'
- en: Figure 10.2 – Endpoint runtime settings
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 端点运行时设置]'
- en: Now, let's walk through the more detailed inference endpoint settings.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细了解推理端点的设置。
- en: Setting the minimum and maximum capacity
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置最小和最大容量
- en: 'You can set boundaries on the minimum and maximum number of instances an endpoint
    can use. These boundaries let you protect against surges in demand that will result
    in unexpected costs. If you anticipate periodic spikes, build a circuit breaker
    into your application to shed load before it hits the inference endpoint. The
    following screenshot shows these settings in the console:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为端点使用的最小和最大实例数量设置边界。这些边界可以帮助您防止需求激增导致的不预期成本。如果您预计会有周期性的峰值，请在达到推理端点之前在您的应用程序中构建一个断路器来减轻负载。以下截图显示了控制台中的这些设置：
- en: '![Figure 10.3 – Setting minimum and maximum capacity'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 设置最小和最大容量]'
- en: '](img/B17249_10_03.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_10_03.jpg]'
- en: Figure 10.3 – Setting minimum and maximum capacity
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 设置最小和最大容量]'
- en: If your load is highly variable, you can start with a small instance type and
    scale up aggressively. This prevents you from paying for a larger instance type
    that you don't always need.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的负载高度可变，您可以从小型实例类型开始，并积极地进行扩展。这可以防止您为不需要的大型实例类型付费。
- en: Choosing a scaling metric
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择缩放指标
- en: 'We need to decide when to trigger a scaling action. We do that by specifying
    a CloudWatch metric. By default, SageMaker provides two useful metrics:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要决定何时触发缩放操作。我们通过指定CloudWatch指标来完成此操作。默认情况下，SageMaker提供了两个有用的指标：
- en: '`InvocationsPerInstance` reports the number of inference requests sent to each
    endpoint instance over some time period.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InvocationsPerInstance`报告在一段时间内发送到每个端点实例的推理请求数量。'
- en: '`ModelLatency` is the time in microseconds to respond to inference requests.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ModelLatency`是响应推理请求的微秒数。'
- en: We recommend `ModelLatency` as a metric for autoscaling, as it reports on the
    end user experience. Setting the actual value for the metric will depend on your
    requirements and some observation of endpoint performance over time. For example,
    you may find that latency over 100 milliseconds results in a degraded user experience
    if the inference result passes through several other services that add their own
    latency before the result reaches the end user.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议将`ModelLatency`作为自动缩放的指标，因为它报告了最终用户的使用体验。设置该指标的实际值将取决于您的需求和一段时间内端点性能的观察。例如，您可能会发现，如果推理结果通过几个其他服务（这些服务在结果到达最终用户之前增加了自己的延迟），超过100毫秒的延迟会导致用户体验下降。
- en: Setting the scaling policy
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置缩放策略
- en: You can choose between **target tracking** and **step scaling**. Target tracking
    policies are more useful and try to adjust capacity to keep some target metric
    within a given boundary. Step scaling policies are more advanced and increase
    capacity in incremental steps.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择**目标跟踪**和**步进缩放**。目标跟踪策略更有用，并试图调整容量以保持某些目标指标在给定边界内。步进缩放策略更高级，以增量步骤增加容量。
- en: Setting the cooldown period
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置冷却时间
- en: The **cooldown period** is how long the endpoint will wait after one scaling
    action before starting another scaling action. If you let the endpoint respond
    instantaneously, you'd end up scaling too often. As a general rule, scale up aggressively
    and scale down conservatively.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**冷却时间**是指端点在开始下一次缩放操作之前等待的时间。如果您让端点即时响应，您可能会过于频繁地进行缩放。一般来说，积极地进行扩展，保守地进行缩减。'
- en: 'The following screenshot shows how to configure the target metric value and
    cooldown period if you use the default scaling policy:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了如果你使用默认的缩放策略，如何配置目标指标值和冷却期：
- en: '![Figure 10.4 – Setting a target metric value and cooldown period'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4 – 设置目标指标值和冷却期'
- en: '](img/B17249_10_04.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_10_04.jpg)'
- en: Figure 10.4 – Setting a target metric value and cooldown period
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 设置目标指标值和冷却期
- en: Next, let's look at another optimization technique for deep learning models.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看为深度学习模型提供的另一种优化技术。
- en: Using Elastic Inference for deep learning models
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Elastic Inference进行深度学习模型
- en: If you examine the overall cost of ML, you may be surprised to see that the
    bulk of your monthly cost comes from real-time inference endpoints. Training jobs,
    while potentially resource-intensive, run for some time and then terminate. Managed
    notebook instances can be shut down during off hours. But inference endpoints
    run 24 hours a day, 7 days a week. If you are using a deep learning model, inference
    endpoint costs become more pronounced, as instances with dedicated GPU capacity
    are more expensive than other comparable instances.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你检查机器学习的整体成本，你可能会惊讶地发现，你每月的大部分成本来自实时推理端点。训练作业虽然可能资源密集，但运行一段时间后会终止。托管笔记本实例可以在非工作时间关闭。但推理端点每天24小时，每周7天都在运行。如果你使用深度学习模型，推理端点的成本会更加明显，因为具有专用GPU容量的实例比其他类似实例更昂贵。
- en: When you obtain inferences from a deep learning model, you do not need as much
    GPU capacity as you need during training. **Elastic Inference** lets you attach
    fractional GPU capacity to regular EC2 instances or **Elastic Container Service**
    (**ECS**) containers. As a result, you can get deep learning inferences quickly
    at a reduced cost.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从深度学习模型获取推理时，你不需要像训练期间那样多的GPU容量。**Elastic Inference** 允许你将部分GPU容量附加到常规EC2实例或**Elastic
    Container Service** (**ECS**) 容器。因此，你可以以较低的成本快速获得深度学习推理。
- en: 'The *Elastic Inference* section in the notebook shows how to attach an Elastic
    Inference accelerator to an endpoint, as you can see in the following code block:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的 *Elastic Inference* 部分展示了如何将Elastic Inference加速器附加到端点，如下面的代码块所示：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Consider a case where we need some GPU capacity for inference. Let''s consider
    three options for the instance type and compare the cost. Assume that we run the
    endpoint for 720 hours per month. The next table compares the monthly cost for
    different inference options, using published prices at the time of writing:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个我们需要一些GPU容量进行推理的情况。让我们考虑实例类型的三个选项并比较成本。假设我们每月运行端点720小时。下表比较了不同推理选项的月度成本，使用的是撰写时的公布价格：
- en: '![Figure 10.5 – Inference cost comparison'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5 – 推理成本比较'
- en: '](img/B17249_10_05.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17249_10_05.jpg)'
- en: Figure 10.5 – Inference cost comparison
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 推理成本比较
- en: You'll need to look at your specific use case and figure out the best combination
    of RAM, CPU, network throughput, and GPU capacity that meets your performance
    requirements at the lowest cost. If your inferences are entirely GPU-bound, the
    Inferentia instance will probably give you the best price-performance balance.
    If you need more traditional compute resources with some GPU, the P2/P3 family
    will work well. If you need very little overall capacity, Elastic Inference provides
    the cheapest GPU option.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要查看你的具体用例，并找出最佳的RAM、CPU、网络吞吐量和GPU容量组合，以最低的成本满足你的性能需求。如果你的推理完全受GPU限制，Inferentia实例可能会给你提供最佳的性能价格比。如果你需要更多一些的GPU计算资源，P2/P3系列将工作得很好。如果你需要的整体容量非常小，Elastic
    Inference提供了最便宜的GPU选项。
- en: In the next section, we'll cover one more optimization technique for models
    deployed to specific hardware.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍针对特定硬件部署的模型的另一种优化技术。
- en: Optimizing models with SageMaker Neo
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Neo优化模型
- en: In the previous section, we saw how Elastic Inference can reduce inference costs
    for deep learning models. Similarly, SageMaker Neo lets you improve inference
    performance and reduce costs by compiling trained ML models for better performance
    on specific platforms. While that will help in general, it's particularly effective
    when you are trying to run inference on low-powered edge devices.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了Elastic Inference如何降低深度学习模型的推理成本。同样，SageMaker Neo允许你通过为特定平台编译训练好的ML模型以获得更好的性能来提高推理性能并降低成本。虽然这有助于一般情况，但在你尝试在低功耗边缘设备上运行推理时尤其有效。
- en: In order to use SageMaker Neo, you simply start a compilation job with a trained
    model in a supported framework. When the compilation job completes, you can deploy
    the artifact to a SageMaker endpoint or to an edge device using the *Greengrass*
    IoT platform.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SageMaker Neo，您只需使用支持框架中的训练模型启动一个编译作业。当编译作业完成后，您可以将工件部署到SageMaker端点或使用*Greengrass*
    IoT平台部署到边缘设备。
- en: 'The *Model optimization with SageMaker Neo* section in the notebook demonstrates
    how to compile our XGBoost model for use on a hosted endpoint:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的*使用SageMaker Neo进行模型优化*部分演示了如何编译我们的XGBoost模型以在托管端点使用：
- en: 'First, we need to get the length (number of features) of an input record:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要获取输入记录的长度（特征数量）：
- en: '[PRE16]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we''ll compile one of our trained models. We need to specify the target
    platform, which in this case is just a standard `ml_m5` family:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将编译我们的一个训练模型。我们需要指定目标平台，在这个例子中，它只是一个标准的`ml_m5`系列：
- en: '[PRE17]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once the compilation job finishes, we can deploy the compiled model as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦编译作业完成，我们可以按照以下方式部署编译后的模型：
- en: '[PRE18]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s test the endpoint to see whether we see a speed-up:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们测试端点以查看是否可以看到速度提升：
- en: '[PRE19]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After sending in a few invocation requests, let''s check the CloudWatch metrics.
    Back in the console page for the compiled endpoint, click on **View invocation
    metrics** in the **Monitor** section, as shown in the following screenshot:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 发送了几次调用请求后，让我们检查CloudWatch度量。回到编译端点的控制台页面，在**监控**部分点击**查看调用度量**，如下面的截图所示：
- en: '![Figure 10.6 – The Monitor section of the endpoint console'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6 – 端点控制台中的监控部分'
- en: '](img/B17249_10_06.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_10_06.jpg]'
- en: Figure 10.6 – The Monitor section of the endpoint console
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 端点控制台中的监控部分
- en: 'You''ll now see the CloudWatch metrics console, as seen in the following screenshot.
    Here, choose the **ModelLatency** and **OverheadLatency** metrics:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将看到CloudWatch度量控制台，如下面的截图所示。在此，选择**ModelLatency**和**OverheadLatency**度量：
- en: '![Figure 10.7 – CloudWatch metrics console'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 – CloudWatch度量控制台'
- en: '](img/B17249_10_07.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17249_10_07.jpg]'
- en: Figure 10.7 – CloudWatch metrics console
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – CloudWatch度量控制台
- en: The model latency in my simple tests showed 10 milliseconds for a regular XGBoost
    endpoint and went down to 9 milliseconds after compiling with Neo. The impact
    of a compiled model will be much more significant if you are using a deep learning
    model on a lower-powered device.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的简单测试中，模型的延迟在常规XGBoost端点上是10毫秒，在用Neo编译后下降到9毫秒。如果您在低功耗设备上使用深度学习模型，编译模型的冲击将更为显著。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at several ways to improve inference performance
    and reduce inference cost. These methods include using batch inference where possible,
    deploying several models behind a single inference endpoint to reduce costs and
    help with advanced canary or blue/green deployments, scaling inference endpoints
    to meet demand, and using Elastic Inference and SageMaker Neo to provide better
    inference performance at a lower cost.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了多种提高推理性能和降低推理成本的方法。这些方法包括尽可能使用批量推理，在单个推理端点后面部署多个模型以降低成本并帮助进行高级金丝雀或蓝/绿部署，扩展推理端点以满足需求，以及使用Elastic
    Inference和SageMaker Neo以更低的成本提供更好的推理性能。
- en: In the next chapter, we'll discuss monitoring and other important operational
    aspects of ML.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论监控以及ML的其他重要操作方面。
