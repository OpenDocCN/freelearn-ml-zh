- en: 3D Scene Reconstruction Using Structure from Motion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D场景重建使用运动结构
- en: In the previous chapter, you learned how to detect and track an object of interest
    in the video stream of a webcam, even if the object is viewed from different angles
    or distances, or under partial occlusion. Here, we will take the tracking of interesting
    features a step further and see what we can learn about the entire visual scene
    by studying similarities between image frames.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何在网络摄像头的视频流中检测和跟踪感兴趣的对象，即使对象从不同的角度或距离观看，或者部分遮挡。在这里，我们将进一步跟踪有趣的特征，并通过研究图像帧之间的相似性来了解整个视觉场景。
- en: The goal of this chapter is to study how to reconstruct a scene in 3D by inferring
    the geometrical features of the scene from camera motion. This technique is sometimes
    referred to as **structure from motion**. By looking at the same scene from different
    angles, we will be able to infer the real-world 3D coordinates of different features
    in the scene. This process is known as **triangulation**, which allows us to **reconstruct**
    the scene as a **3D point cloud**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是研究如何通过从相机运动中推断场景的几何特征来重建3D场景。这种技术有时被称为**运动结构**。通过从不同角度观察相同的场景，我们将能够推断场景中不同特征的实世界3D坐标。这个过程被称为**三角测量**，它允许我们将场景**重建**为一个**3D点云**。
- en: If we take two pictures of the same scene from different angles, we can use
    **feature matching** or **optic flow** to estimate any translational and rotational
    movement that the camera underwent between taking the two pictures. However, in
    order for this to work, we will first have to calibrate our camera.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从不同角度拍摄同一场景的两张照片，我们可以使用**特征匹配**或**光流**来估计相机在拍摄两张照片之间所经历的任何平移和旋转运动。然而，为了使这可行，我们首先必须校准我们的相机。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about camera calibration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习相机标定
- en: Setting up the app
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置应用程序
- en: Estimating the camera motion from a pair of images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一对图像中估计相机运动
- en: Reconstructing the scene
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重建场景
- en: Understanding 3D point cloud visualization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解3D点云可视化
- en: Learning about structure from motion
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习运动结构
- en: Once you complete the app, you will understand the classical approaches that
    are used to make a 3D reconstruction of a scene or object given several images
    taken from different view points. You will be able to apply these approaches in
    your own apps related to constructing 3D models from camera images or videos.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成应用程序，你将了解用于从不同视角拍摄的多张图像中重建场景或对象的经典方法。你将能够将这些方法应用于你自己的应用程序，这些应用程序与从相机图像或视频中构建3D模型相关。
- en: Getting started
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: This chapter has been tested with **OpenCV 4.1.0** and **wxPython 4.0.4** ([http://www.wxpython.org/download.php](http://www.wxpython.org/download.php)).
    It also requires NumPy ([http://www.numpy.org](http://www.numpy.org)) and Matplotlib
    ([http://www.matplotlib.org/downloads.html](http://www.matplotlib.org/downloads.html)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已使用**OpenCV 4.1.0**和**wxPython 4.0.4**（[http://www.wxpython.org/download.php](http://www.wxpython.org/download.php)）进行测试。它还需要NumPy（[http://www.numpy.org](http://www.numpy.org)）和Matplotlib（[http://www.matplotlib.org/downloads.html](http://www.matplotlib.org/downloads.html)）。
- en: Note that you may have to obtain the so-called *extra* modules from [https://github.com/Itseez/opencv_contrib](https://github.com/Itseez/opencv_contrib) and
    install OpenCV with the `OPENCV_EXTRA_MODULES_PATH` variable set in order to install
    **scale-invariant feature transform** (**SIFT**). Also, note that you may have
    to obtain a license to use SIFT in commercial applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可能需要从[https://github.com/Itseez/opencv_contrib](https://github.com/Itseez/opencv_contrib)获取所谓的*额外*模块，并设置`OPENCV_EXTRA_MODULES_PATH`变量来安装**尺度不变特征变换**（**SIFT**）。此外，请注意，你可能需要获得许可证才能在商业应用中使用SIFT。
- en: You can find the code that we present in this chapter in our GitHub repository, [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter4](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter4).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我们的GitHub仓库中找到本章中展示的代码，[https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter4](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter4)。
- en: Planning the app
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划应用程序
- en: The final app will extract and visualize structure from motion on a pair of
    images. We will assume that these two images have been taken with the same camera,
    whose internal camera parameters we know. If these parameters are not known, they
    need to be estimated first in a camera calibration process.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的应用程序将从一对图像中提取并可视化运动结构。我们将假设这两张图像是用同一台相机拍摄的，我们已知其内部相机参数。如果这些参数未知，它们需要在相机标定过程中首先进行估计。
- en: 'The final app will then consist of the following modules and scripts:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的应用程序将包括以下模块和脚本：
- en: '`chapter4.main`: This is the main function routine for starting the application.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter4.main`: 这是启动应用程序的主要函数例程。'
- en: '`scene3D.SceneReconstruction3D`: This is a class that contains a range of functionalities
    for calculating and visualizing structure from motion. It includes the following
    public methods:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scene3D.SceneReconstruction3D`: 这是一个包含一系列用于计算和可视化运动结构的功能的类。它包括以下公共方法：'
- en: '`__init__`: This constructor will accept the intrinsic camera matrix and the
    distortion coefficients.'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__`: 此构造函数将接受内禀相机矩阵和畸变系数。'
- en: '`load_image_pair`: This is a method used to load two images that have been
    taken with the camera described earlier from the file.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_image_pair`: 这是一个用于从文件中加载之前描述的相机拍摄的两张图像的方法。'
- en: '`plot_optic_flow`: This is a method used to visualize the optic flow between
    the two image frames.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_optic_flow`: 这是一个用于可视化两张图像帧之间光流的方法。'
- en: '`draw_epipolar_lines`: This method is used to draw the epipolar lines of the
    two images.'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`draw_epipolar_lines`: 此方法用于绘制两张图像的极线。'
- en: '`plot_rectified_images`: This method is used to plot a rectified version of
    the two images.'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_rectified_images`: 这是一个用于绘制两张图像校正版本的方法。'
- en: '`plot_point_cloud`: This is a method used to visualize the recovered real-world
    coordinates of the scene as a 3D point cloud. In order to arrive at a 3D point
    cloud, we will need to exploit epipolar geometry. However, epipolar geometry assumes
    the pinhole camera model, which no real camera follows.'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_point_cloud`: 这是一个用于将场景的真实世界坐标作为3D点云可视化的方法。为了得到一个3D点云，我们需要利用极几何。然而，极几何假设了针孔相机模型，而现实中的相机并不遵循此模型。'
- en: 'The complete procedure of the app involves the following steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的完整流程包括以下步骤：
- en: '**Camera calibration**: We will use a chessboard pattern to extract the intrinsic
    camera matrix as well as the distortion coefficients, which are important for
    performing the scene reconstruction.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**相机标定**: 我们将使用棋盘图案来提取内禀相机矩阵以及畸变系数，这些对于执行场景重建非常重要。'
- en: '**Feature matching**: We will match points in two 2D images of the same visual
    scene, either via SIFT or via optic flow, as seen in the following screenshot:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征匹配**: 我们将在同一视觉场景的两个2D图像中匹配点，无论是通过SIFT还是通过光流，如以下截图所示：'
- en: '![](img/ba0e6aff-282a-4ab8-a0bd-ee62bebbb9b3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba0e6aff-282a-4ab8-a0bd-ee62bebbb9b3.png)'
- en: '**Image rectification**: By estimating the camera motion from a pair of images,
    we will extract the **essential matrix** and rectify the images, as shown in the
    following screenshot:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像校正**: 通过从一对图像中估计相机运动，我们将提取**基础矩阵**并校正图像，如以下截图所示：'
- en: '![](img/7f66eaa1-9a22-4830-8963-46351a36fb56.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f66eaa1-9a22-4830-8963-46351a36fb56.png)'
- en: '**Triangulation**: We will reconstruct the 3D real-world coordinates of the
    image points by making use of constraints from **epipolar geometry**.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**三角测量法**: 我们将通过利用**极几何**的约束来重建图像点的3D真实世界坐标。'
- en: '**3D point cloud visualization**: Finally, we will visualize the recovered
    3D structure of the scene using scatterplots in Matplotlib, which is most compelling
    when studied using the Pan axes button from pyplot. This button lets you rotate
    and scale the point cloud in all three dimensions. In the following screenshot,
    the color corresponds to the depth of a point in the scene:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3D点云可视化**: 最后，我们将使用Matplotlib中的散点图来可视化恢复的场景的3D结构，这在使用pyplot的Pan轴按钮进行研究时最为引人入胜。此按钮允许你在三个维度中旋转和缩放点云。在以下截图中，颜色对应于场景中点的深度：'
- en: '![](img/e3be9c94-8fb3-42f8-9285-683cdd36ea99.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3be9c94-8fb3-42f8-9285-683cdd36ea99.png)'
- en: First, we need to rectify our images to make them look as if they have come
    from a pinhole camera. For that, we need to estimate the parameters of the camera,
    which leads us to the field of camera calibration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要校正我们的图像，使它们看起来就像是从针孔相机拍摄出来的。为此，我们需要估计相机的参数，这把我们引向了相机标定的领域。
- en: Learning about camera calibration
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习相机标定
- en: So far, we have worked with whatever image came straight out of our webcam,
    without questioning the way in which it was taken. However, every camera lens
    has unique parameters, such as focal length, principal point, and lens distortion.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直使用从我们的网络摄像头直接输出的图像，而没有质疑其拍摄方式。然而，每个相机镜头都有独特的参数，例如焦距、主点和镜头畸变。
- en: 'What happens behind the covers when a camera takes a picture is this: light
    falls through a lens, followed by an aperture, before falling on the surface of
    a light sensor. This process can be approximated with the pinhole camera model.
    The process of estimating the parameters of a real-world lens such that it would
    fit the pinhole camera model is called camera calibration (or **camera resectioning**,
    and it should not be confused with *photometric* camera calibration). So, let''s
    start by learning about the pinhole camera model in the next section.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当相机拍照时，其背后的过程是这样的：光线穿过镜头，然后通过光圈，最后落在光敏器的表面上。这个过程可以用针孔相机模型来近似。估计现实世界镜头参数的过程，使其适合针孔相机模型，被称为相机标定（或**相机重投影**，不应与*光度学*相机标定混淆）。因此，让我们从下一节开始学习针孔相机模型。
- en: Understanding the pinhole camera model
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解针孔相机模型
- en: The **pinhole camera model **is a simplification of a real camera in which there
    is no lens and the camera aperture is approximated by a single point (the pinhole).
    The formulas described here also hold exactly for a camera with a thin lens as
    well as describing the main parameters of any usual camera.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**针孔相机模型**是对真实相机的简化，其中没有镜头，相机光圈被近似为一个单点（针孔）。这里描述的公式也完全适用于带薄镜头的相机，以及描述任何普通相机的主要参数。'
- en: 'When viewing a real-world 3D scene (such as a tree), light rays pass through
    the point-sized aperture, and fall on a 2D image plane inside the camera, as seen
    in the following diagram:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当观察现实世界的3D场景（如一棵树）时，光线穿过点大小的孔径，并落在相机内部的2D图像平面上，如下所示图所示：
- en: '![](img/3ea08414-847b-4aac-aef3-b8ed24b114d7.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3ea08414-847b-4aac-aef3-b8ed24b114d7.png)'
- en: In this model, a 3D point with coordinates (*X*, *Y*, *Z*) is mapped to a 2D
    point with coordinates (*x*, *y*) that lies on the image plane. Note that this
    leads to the tree appearing upside down on the image plane.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，具有坐标(*X*, *Y*, *Z*)的3D点被映射到图像平面上的具有坐标(*x*, *y*)的2D点上。请注意，这导致树在图像平面上是倒置的。
- en: The line that is perpendicular to the image plane and passes through the pinhole
    is called the **principal ray**, and its length is called the **focal length**.
    The focal length is a part of the internal camera parameters, as it may vary depending
    on the camera being used. In a simple camera with a lens, the **pinhole** is replaced
    with a lens and the focal plane is placed at the focal length of the lens in order
    to avoid blurring as much as possible.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直于图像平面并通过针孔的线称为**主光线**，其长度称为**焦距**。焦距是内部相机参数的一部分，因为它可能取决于所使用的相机。在简单的带镜头的相机中，**针孔**被镜头取代，焦平面放置在镜头的焦距处，以尽可能减少模糊。
- en: 'Hartley and Zisserman found a mathematical formula to describe how a 2D point
    with coordinates (*x*, *y*) can be inferred from a 3D point with coordinates(*X, Y, Z*) and
    the camera''s intrinsic parameters, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 哈特利和齐isserman发现了一个数学公式，用以描述如何从具有坐标(*x*, *y*)的2D点推断出具有坐标(*X, Y, Z*)的3D点，以及相机的内在参数，如下所示：
- en: '![](img/ad7b9889-55c5-4862-a51f-cfb1ec9bcf7b.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad7b9889-55c5-4862-a51f-cfb1ec9bcf7b.png)'
- en: The 3 x 3 matrix in the preceding formula is the **intrinsic camera matrix**—a
    matrix that compactly describes all internal camera parameters. The matrix comprises
    focal lengths (*f[x ]*and *f[y]*) and optical centers *c[x]* and *c[y]*, which
    in the case of digital imaging are simply expressed in pixel coordinates. As mentioned
    earlier, the focal length is the distance between the pinhole and the image plane.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个公式中的3x3矩阵是**内在相机矩阵**——一个紧凑地描述所有内部相机参数的矩阵。该矩阵包括焦距(*f[x]*和*f[y]*)和光学中心*c[x]*和*c[y]*，在数字成像中，它们简单地用像素坐标表示。如前所述，焦距是针孔和图像平面之间的距离。
- en: A pinhole camera has only one focal length, in which case, *f[x]* = *f[x ]*= *f[x ]*.
    However, in real cameras, these two values might differ, for example, due to imperfections
    of lenses, imperfections of the focal plane (which is represented by a digital
    camera sensor), or imperfections of assembly. The difference can be also intentional
    for some purpose, which can be achieved by simply involving a lens that has different
    curvature in different directions. The point at which the principal ray intersects
    the image plane is called the **principal point**, and its relative position on
    the image plane is captured by the optical center (or principal point offset).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 小孔相机只有一个焦距，在这种情况下，*f[x]* = *f[x ]*= *f[x ]*。然而，在实际相机中，这两个值可能不同，例如，由于镜头、焦平面（由数字相机传感器表示）或组装的不完美。这种差异也可能是出于某种目的而故意造成的，这可以通过简单地使用在不同方向上具有不同曲率的镜头来实现。主光线与图像平面相交的点称为**主点**，其在图像平面上的相对位置由光学中心（或主点偏移）捕捉。
- en: In addition, a camera might be subject to radial or tangential distortion, leading
    to a **fish-eye effect**. This is because of hardware imperfections and lens misalignments.
    These distortions can be described with a list of the **distortion coefficients**.
    Sometimes, radial distortions are actually a desired artistic effect. At other
    times, they need to be corrected.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相机可能受到径向或切向畸变的影响，导致**鱼眼效应**。这是由于硬件不完美和镜头错位造成的。这些畸变可以用**畸变系数**的列表来描述。有时，径向畸变实际上是一种期望的艺术效果。在其他时候，它们需要被校正。
- en: For more information on the pinhole camera model, there are many good tutorials
    out there on the web, such as [http://ksimek.github.io/2013/08/13/intrinsic](http://ksimek.github.io/2013/08/13/intrinsic).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关于小孔相机模型，网上有许多很好的教程，例如[http://ksimek.github.io/2013/08/13/intrinsic](http://ksimek.github.io/2013/08/13/intrinsic)。
- en: Because these parameters are specific to the camera hardware (hence the name
    *intrinsic*), we need to calculate them only once in the lifetime of a camera.
    This is called **camera calibration**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些参数是针对相机硬件的特定参数（因此得名 *intrinsic*），我们只需要在相机的整个生命周期中计算一次。这被称为**相机校准**。
- en: Next, we will cover the parameters of the intrinsic camera.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍相机的内在参数。
- en: Estimating the intrinsic camera parameters
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计相机的内在参数
- en: In OpenCV, camera calibration is fairly straightforward. The official documentation
    provides a good overview of the topic and some sample C++ scripts at [http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html](http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 OpenCV 中，相机校准相当直接。官方文档提供了该主题的良好概述和一些示例 C++ 脚本，请参阅[http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html](http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html)。
- en: For educational purposes, we will develop our own calibration script in Python.
    We will need to present a special pattern image, with known geometry (chessboard
    plate or black circles on a white background), to the camera we wish to calibrate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了教育目的，我们将使用 Python 开发自己的校准脚本。我们需要向要校准的相机展示一个具有已知几何形状（棋盘格板或白色背景上的黑色圆圈）的特殊图案图像。
- en: Because we know the geometry of the pattern image, we can use feature detection
    to study the properties of the internal camera matrix. For example, if the camera
    suffers from undesired radial distortion, the different corners of the chessboard
    pattern will appear distorted in the image and not lie on a rectangular grid.
    By taking about 10-20 snapshots of the chessboard pattern from different points
    of view, we can collect enough information to correctly infer the camera matrix
    and the distortion coefficients.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道图案图像的几何形状，我们可以使用特征检测来研究内部相机矩阵的性质。例如，如果相机受到不希望的径向畸变，棋盘格图案的不同角落将在图像中变形，并且不会位于矩形网格上。通过从不同的视角拍摄大约
    10-20 张棋盘格图案的快照，我们可以收集足够的信息来正确推断相机矩阵和畸变系数。
- en: 'For this, we will use the `calibrate.py` script, which first imports the following
    modules:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用 `calibrate.py` 脚本，该脚本首先导入以下模块：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Analogous to previous chapters, we will use a simple layout based on `BaseLayout` that
    embeds processing of the webcam video stream.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于前面的章节，我们将使用基于 `BaseLayout` 的简单布局，该布局嵌入处理网络摄像头视频流。
- en: 'The `main` function of the script will generate the GUI and execute the `main`
    loop of the app:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的 `main` 函数将生成 GUI 并执行应用程序的 `main` 循环：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The latter is accomplished with the following steps in the body of the function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 后者是通过以下步骤在函数体中完成的：
- en: 'First, connect to the camera and set standard VGA resolution:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，连接到相机并设置标准VGA分辨率：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly to the previous chapters, create a `wx` application and the `layout` class,
    which we will compose later in this section:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于前面的章节，创建一个`wx`应用程序和`layout`类，我们将在本节后面组合它们：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Show the GUI and execute the `MainLoop` of the `app`:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示GUI并执行`app`的`MainLoop`：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the next section, we'll prepare the camera calibration GUI, which we used
    in the `main` function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将准备相机校准GUI，这是我们将在本节后面使用的。
- en: Defining the camera calibration GUI
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义相机校准GUI
- en: 'The GUI is a customized version of the generic `BaseLayout`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GUI是通用`BaseLayout`的定制版本：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The layout consists of only the current camera frame and a single button below
    it. This button allows us to start the calibration process:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 布局仅由当前相机帧和其下方的单个按钮组成。此按钮允许我们开始校准过程：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For these changes to take effect, `pnl` needs to be added to the list of existing
    panels:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些更改生效，`pnl`需要添加到现有面板列表中：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The rest of the visualization pipeline is handled by the `BaseLayout` class.
    We only need to make sure that we initialize the required variables and provide `process_frame`
    methods.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的可视化管道由`BaseLayout`类处理。我们只需要确保初始化所需的变量并提供`process_frame`方法。
- en: Now that we have defined a GUI for camera calibration, let's initialize a camera
    calibration algorithm in the next section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了一个用于相机校准的GUI，接下来我们将初始化一个相机校准算法。
- en: Initializing the algorithm
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化算法
- en: 'In order to perform the calibration process, we need to do some bookkeeping.
    We will do that by following the next steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行校准过程，我们需要做一些记账工作。我们将通过以下步骤来完成：
- en: 'For now, let''s focus on a single 10 x 7 chessboard. The algorithm will detect
    all the `9` x `6` inner corners of the chessboard (referred to as *object points*)
    and store the detected image points of these corners in a list. So, let''s first
    initialize the `chessboard_size` to the number of inner corners:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们专注于一个10 x 7的棋盘。算法将检测棋盘的所有`9` x `6`个内部角落（称为*对象点*）并将这些角落检测到的图像点存储在列表中。因此，我们首先将`chessboard_size`初始化为内部角落的数量：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we need to enumerate all the object points and assign them object point
    coordinates so that the first point has coordinates (0,0), the second one (top
    row) has coordinates (1,0), and the last one has coordinates (8,5):'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要枚举所有对象点并将它们分配对象点坐标，以便第一个点的坐标为（0，0），第二个点（顶部行）的坐标为（1，0），最后一个点的坐标为（8，5）：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We also need to keep track of whether we are currently recording the object
    and image points or not. We will initiate this process once the user clicks on
    the `self.button_calibrate` button. After that, the algorithm will try to detect
    a chessboard in all subsequent frames until `self.record_min_num_frames` chessboards
    have been detected:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要跟踪我们是否正在记录对象和图像点。一旦用户点击`self.button_calibrate`按钮，我们将启动此过程。之后，算法将尝试在所有后续帧中检测棋盘，直到检测到`self.record_min_num_frames`个棋盘：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Whenever the `self.button_calibrate` button is clicked on, we reset all the
    bookkeeping variables, disable the button, and start recording:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当点击`self.button_calibrate`按钮时，我们将重置所有记账变量，禁用按钮，并开始记录：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Resetting the bookkeeping variables involves clearing the lists of recorded
    object and image points (`self.obj_points` and `self.img_points`) as well as resetting
    the number of detected chessboards (`self.recordCnt`) to `0`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重置记账变量包括清除记录的对象和图像点列表（`self.obj_points`和`self.img_points`）以及将检测到的棋盘数量（`self.recordCnt`）重置为`0`：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the next section, we'll collect the image and object points.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将收集图像和对象点。
- en: Collecting image and object points
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集图像和对象点
- en: 'The `process_frame` method is responsible for doing the hard work of the calibration
    technique, and we will collect images and object points by using the following
    steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_frame`方法负责执行校准技术的艰苦工作，我们将通过以下步骤收集图像和对象点：'
- en: 'After the `self.button_calibrate` button has been clicked on, this method starts
    collecting data until a total of `self.record_min_num_frames` chessboards are
    detected:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在点击`self.button_calibrate`按钮后，此方法开始收集数据，直到检测到总共`self.record_min_num_frames`个棋盘：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `cv2.findChessboardCorners` function will parse a grayscale image (`img_gray`)
    to find a chessboard of size `self.chessboard_size`. If the image indeed contains
    a chessboard, the function will return `true` (`ret`) as well as a list of chessboard
    corners (`corners`).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`cv2.findChessboardCorners`函数将解析灰度图像（`img_gray`）以找到大小为`self.chessboard_size`的棋盘。如果图像确实包含棋盘，该函数将返回`true`（`ret`）以及一个棋盘角列表（`corners`）。'
- en: 'Then, drawing the chessboard is straightforward:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，绘制棋盘是直接的：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result looks like this (drawing the chessboard corners in color for the
    effect):'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果看起来像这样（用彩色绘制棋盘角以增强效果）：
- en: '![](img/4af66364-a9e5-48f4-a3f2-1b2d5d9db3c3.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4af66364-a9e5-48f4-a3f2-1b2d5d9db3c3.png)'
- en: 'We could now simply store the list of detected corners and move on to the next
    frame. However, in order to make the calibration as accurate as possible, OpenCV
    provides a function to refine the corner point measurement:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以简单地存储检测到的角列表并继续下一帧。然而，为了使校准尽可能准确，OpenCV提供了一个用于细化角点测量的函数：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will refine the coordinates of the detected corners to subpixel precision.
    Now we are ready to append the object and image points to the list and advance
    the frame counter:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这将细化检测到的角的坐标到亚像素精度。现在我们准备好将对象点和图像点添加到列表中并前进帧计数器：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the next section, let's learn how to find the camera matrix, which will be
    required to accomplish an appropriate 3D reconstruction.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何找到相机矩阵，这对于完成适当的3D重建是必需的。
- en: Finding the camera matrix
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找相机矩阵
- en: 'Once we have collected enough data (that is, once `self.record_cnt` reaches
    the value of `self.record_min_num_frames`), the algorithm is ready to perform
    the calibration. This process can be performed with a single call to `cv2.calibrateCamera`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦收集到足够的数据（即，一旦`self.record_cnt`达到`self.record_min_num_frames`的值），算法就准备好执行校准。这个过程可以通过对`cv2.calibrateCamera`的单次调用来完成：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The function returns `true` on success (`ret`), the intrinsic camera matrix
    (`K`), the distortion coefficients (`dist`), as well as two rotation and translation
    matrices (`rvecs` and `tvecs`). For now, we are mainly interested in the camera
    matrix and the distortion coefficients, because these will allow us to compensate
    for any imperfections of the internal camera hardware.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 函数在成功时返回`true`（`ret`），内禀相机矩阵（`K`），畸变系数（`dist`），以及两个旋转和平移矩阵（`rvecs`和`tvecs`）。目前，我们主要对相机矩阵和畸变系数感兴趣，因为这些将允许我们补偿内部相机硬件的任何不完美。
- en: 'We will simply `print` them on the console for easy inspection:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简单地`打印`它们到控制台以便于检查：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For example, the calibration of my laptop''s webcam recovered the following
    values:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我的笔记本电脑网络摄像头的校准恢复了以下值：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This tells us that the focal lengths of my webcam are `fx=3366.9644` pixels
    and `fy=3296.8392` pixels, with the optical center at `cx=299.1099` pixels and
    `cy=269.4368` pixels.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，我的网络摄像头的焦距为`fx=3366.9644`像素和`fy=3296.8392`像素，光学中心在`cx=299.1099`像素和`cy=269.4368`像素。
- en: 'A good idea might be to double-check the accuracy of the calibration process.
    This can be done by projecting the object points onto the image using the recovered
    camera parameters so that we can compare them with the list of image points we
    collected with the `cv2.findChessboardCorners` function. If the two points are
    roughly the same, we know that the calibration was successful. Even better, we
    can calculate the `mean error` of the reconstruction by projecting every object
    point in the list:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好主意可能是双重检查校准过程的准确性。这可以通过使用恢复的相机参数将对象点投影到图像上来完成，以便我们可以将它们与我们使用`cv2.findChessboardCorners`函数收集到的图像点列表进行比较。如果这两个点大致相同，我们知道校准是成功的。甚至更好，我们可以通过将列表中的每个对象点投影来计算重建的`平均误差`：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Performing this check on my laptop's webcam resulted in a mean error of 0.95
    pixels, which is fairly close to 0.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的笔记本电脑的网络摄像头上进行此检查的结果是平均误差为0.95像素，这相当接近0。
- en: With the internal camera parameters recovered, we can now set out to take beautiful,
    undistorted pictures of the world, possibly from different viewpoints so that
    we can extract some structure from motion. First, let's see how to set up our
    app.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在恢复内部相机参数后，我们现在可以开始拍摄美丽、无畸变的照片，可能从不同的视角，以便我们可以从运动中提取一些结构。首先，让我们看看如何设置我们的应用程序。
- en: Setting up the app
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置应用程序
- en: 'Going forward, we will be using a famous open source dataset called `fountain-P11`.
    It depicts a Swiss fountain viewed from various angles:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用一个著名的开源数据集，称为`fountain-P11`。它展示了从不同角度观看的瑞士喷泉：
- en: '![](img/ac6bebd0-35f9-4265-97b3-6621dd489e1c.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac6bebd0-35f9-4265-97b3-6621dd489e1c.png)'
- en: The dataset consists of 11 high-resolution images and can be downloaded from [https://icwww.epfl.ch/multiview/denseMVS.html](https://icwww.epfl.ch/multiview/denseMVS.html).
    Had we taken the pictures ourselves, we would have had to go through the entire
    camera calibration procedure to recover the intrinsic camera matrix and the distortion
    coefficients. Luckily, these parameters are known for the camera that took the
    fountain dataset, so we can go ahead and hardcode these values in our code.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含11张高分辨率图像，可以从[https://icwww.epfl.ch/multiview/denseMVS.html](https://icwww.epfl.ch/multiview/denseMVS.html)下载。如果我们自己拍照，我们就必须通过整个相机标定过程来恢复内禀相机矩阵和畸变系数。幸运的是，这些参数对于拍摄喷泉数据集的相机是已知的，因此我们可以继续在我们的代码中硬编码这些值。
- en: Let's prepare the `main` routine function in the next section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中准备`main`主函数。
- en: Understanding the main routine function
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解主函数
- en: 'Our `main` routine function will consist of creating and interacting with an
    instance of the `SceneReconstruction3D` class. This code can be found in the `chapter4.py`
    file. The dependencies of the module are `numpy` and the class itself, which are
    imported as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`main`主函数将包括创建和与`SceneReconstruction3D`类的实例进行交互。这段代码可以在`chapter4.py`文件中找到。该模块的依赖项是`numpy`以及类本身，它们被导入如下：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we define the `main` function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义`main`函数：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The function consists of the following steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 函数包括以下步骤：
- en: 'We define the intrinsic camera matrix for the camera that took photos of the
    fountain dataset (`K`) and set distortion coefficients (`d`):'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为拍摄喷泉数据集照片的相机定义了内禀相机矩阵（`K`）和畸变系数（`d`）：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: According to the photographer, these images are already distortion-free, so
    we have set all the distortion coefficients to 0.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据摄影师的说法，这些图像已经是无畸变的，因此我们将所有畸变系数设置为0。
- en: Note that if you want to run the code presented in this chapter on a dataset
    other than `fountain-P11`, you will have to adjust the intrinsic camera matrix
    and the distortion coefficients.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您想在除`fountain-P11`之外的数据集上运行本章中展示的代码，您将必须调整内禀相机矩阵和畸变系数。
- en: 'Next, we create an instance of the  `SceneReconstruction3D` class and load
    a pair of images, which we would like to apply to our structure-from-motion techniques.
    The dataset is downloaded into a subdirectory called `fountain_dense`:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`SceneReconstruction3D`类的实例，并加载一对图像，我们希望将这些图像应用于我们的运动结构技术。数据集被下载到名为`fountain_dense`的子目录中：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we are ready to call methods from the class that perform various computations:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好调用类中执行各种计算的方法：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will implement these methods throughout the rest of the chapter, and they
    will be explained in detail in the upcoming sections.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的其余部分实现这些方法，它们将在接下来的章节中详细解释。
- en: So now that we have prepared the main script of the app, let's start implementing
    the `SceneReconstruction3D` class, which does all the heavy lifting and incorporates
    the computations for 3D reconstruction.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经准备好了应用程序的主脚本，让我们开始实现`SceneReconstruction3D`类，它执行所有繁重的工作，并包含3D重建的计算。
- en: Implementing the SceneReconstruction3D class
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现`SceneReconstruction3D`类
- en: 'All of the relevant 3D scene reconstruction code for this chapter can be found
    as part of the `SceneReconstruction3D` class in the `scene3D` module. Upon instantiation,
    the class stores the intrinsic camera parameters to be used in all subsequent
    calculations:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所有相关的3D场景重建代码都可以在`scene3D`模块中作为`SceneReconstruction3D`类的一部分找到。在实例化时，该类存储用于所有后续计算的内禀相机参数：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Then, we need to load a pair of images on which to operate.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要加载一对图像来进行操作。
- en: 'In order to do it, first, we create a static method that will load an image
    and convert it to an RGB format if it is grayscale, as other methods expect a
    three-channel image. In the case of the fountain sequence, all images are of a
    relatively high resolution. If an optional `downscale` flag is set, the method
    will downscale the image to a width of roughly `600` pixels:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，首先，我们创建一个静态方法，该方法将加载一个图像，如果它是灰度图像，则将其转换为RGB格式，因为其他方法期望一个三通道图像。在喷泉序列的情况下，所有图像都具有相对较高的分辨率。如果设置了可选的`downscale`标志，则该方法将图像下采样到大约`600`像素的宽度：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we create a method that loads a pair of images and compensates them for
    the radial and tangential lens distortions using the distortion coefficients specified
    earlier (if there are any):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个方法，加载一对图像，并使用之前指定的畸变系数（如果有）对它们进行径向和切向镜头畸变的补偿：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, we are ready to move on to the heart of the project—estimating the
    camera motion and reconstructing the scene!
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备进入项目的核心——估计相机运动和重建场景！
- en: Estimating the camera motion from a pair of images
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一对图像中估计相机运动
- en: Now that we have loaded two images (`self.img1` and `self.img2`) of the same
    scene, such as two examples from the fountain dataset, we find ourselves in a
    similar situation as in the previous chapter. We are given two images that supposedly
    show the same rigid object or static scene but from different viewpoints.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了同一场景的两个图像（`self.img1`和`self.img2`），例如来自喷泉数据集的两个示例，我们发现自己处于与上一章类似的情况。我们得到了两张据说显示相同刚性物体或静态场景但来自不同视点的图像。
- en: However, this time we want to go a step further—if the only thing that changes
    between taking the two pictures is the location of the camera, can we infer the
    relative camera motion by looking at the matching features?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次我们想要更进一步——如果两张照片之间唯一改变的是相机的位置，我们能否通过观察匹配特征来推断相对相机运动？
- en: Well, of course we can. Otherwise, this chapter would not make much sense, would
    it? We will take the location and orientation of the camera in the first image
    as a given and then find out how much we have to reorient and relocate the camera
    so that its viewpoint matches that from the second image.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，当然可以。否则，这一章就没有太多意义了，对吧？我们将以第一张图像中相机的位置和方向为已知条件，然后找出我们需要重新定位和重新定向相机多少，以便其视点与第二张图像匹配。
- en: In other words, we need to recover the **essential matrix** of the camera in
    the second image. An essential matrix is a 4 x 3 matrix that is the concatenation
    of a 3 x 3 rotation matrix and a 3 x 1 translation matrix. It is often denoted
    by *[R | t]*. You can think of it as capturing the position and orientation of
    the camera in the second image relative to the camera in the first image.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们需要恢复第二张图像中相机的**基础矩阵**。基础矩阵是一个4 x 3的矩阵，它是3 x 3旋转矩阵和3 x 1平移矩阵的连接。它通常表示为*[R
    | t]*。你可以将其视为捕捉第二张图像中相对于第一张图像中相机的位置和方向。
- en: 'The crucial step in recovering the essential matrix (as well as all other transformations
    in this chapter) is feature matching. We can either apply the SIFT detector to
    the two images or calculate the optic flow between the two images. The user may
    choose their favorite method by specifying a feature extraction mode, which will
    be implemented by the following private method:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复基础矩阵（以及本章中所有其他变换）的关键步骤是特征匹配。我们可以对两张图像应用SIFT检测器，或者计算两张图像之间的光流。用户可以通过指定特征提取模式来选择他们喜欢的方
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's learn how to perform point matching using rich feature descriptors in
    the next section.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用丰富的特征描述符进行点匹配。
- en: Applying point matching with rich feature descriptors
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用具有丰富特征描述符的点匹配
- en: 'A robust way of extracting important features from an image is by using the
    SIFT detector. In this chapter, we want to use it for two images, `self.img1`
    and `self.img2`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中提取重要特征的一种鲁棒方法是使用SIFT检测器。在本章中，我们想要使用它来处理两张图像，`self.img1`和`self.img2`：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For feature matching, we will use a `BruteForce` matcher, so that other matchers
    (such as **FLANN**) can work as well:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征匹配，我们将使用`BruteForce`匹配器，这样其他匹配器（如**FLANN**）也可以工作：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For each of the `matches`, we need to recover the corresponding image coordinates.
    These are maintained in the `self.match_pts1` and `self.match_pts2` lists:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个`匹配`，我们需要恢复相应的图像坐标。这些坐标保存在`self.match_pts1`和`self.match_pts2`列表中：
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following screenshot shows an example of the feature matcher applied to
    two arbitrary frames of the fountain sequence:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了将特征匹配器应用于喷泉序列的两个任意帧的示例：
- en: '![](img/a6e6d350-9f24-4319-b2e6-3572e1a177c7.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a6e6d350-9f24-4319-b2e6-3572e1a177c7.png)'
- en: In the next section, we'll learn about point matching using optic flow.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习使用光流进行点匹配。
- en: Using point matching with optic flow
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用光流进行点匹配
- en: An alternative to using rich features is using optic flow. Optic flow is the
    process of estimating motion between two consecutive image frames by calculating
    a displacement vector. A displacement vector can be calculated for every pixel
    in the image (dense) or only for selected points (sparse).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 使用丰富特征的替代方案是使用光流。光流是通过计算位移向量来估计两个连续图像帧之间的运动的过程。位移向量可以计算图像中的每个像素（密集）或仅计算选定的点（稀疏）。
- en: One of the most commonly used techniques for calculating dense optic flow is
    the Lukas-Kanade method. It can be implemented in OpenCV with a single line of
    code, by using the `cv2.calcOpticalFlowPyrLK` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 计算密集光流最常用的技术之一是Lukas-Kanade方法。它可以通过使用`cv2.calcOpticalFlowPyrLK`函数在OpenCV中以单行代码实现。
- en: 'But before that, we need to select some points in the image that are worth
    tracking. Again, this is a question of feature selection. If we are interested
    in getting an exact result for only a few highly salient image points, we can
    use Shi-Tomasi''s `cv2.goodFeaturesToTrack` function. This function might recover
    features like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这之前，我们需要在图像中选取一些值得追踪的点。同样，这也是一个特征选择的问题。如果我们只想对几个非常突出的图像点获得精确的结果，我们可以使用Shi-Tomasi的`cv2.goodFeaturesToTrack`函数。这个函数可能会恢复出如下特征：
- en: '![](img/c7cca296-58d7-40ca-a677-03e7e3168037.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c7cca296-58d7-40ca-a677-03e7e3168037.png)'
- en: 'However, in order to infer structure from motion, we might need many more features
    and not just the most salient Harris corners. An alternative would be to detect
    the **Features from Accelerated Segment Test** (**FAST**) features:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了从运动中推断结构，我们可能需要更多的特征，而不仅仅是最突出的Harris角。一个替代方案是检测**加速分割测试（FAST**）特征：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can then calculate the optic flow for these features. In other words, we
    want to find the points in the second image that most likely correspond to the
    `first_key_points` from the first image. For this, we need to convert the keypoint
    list into a NumPy array of (*x*, *y*) coordinates:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算这些特征的光流。换句话说，我们想要找到第二张图像中最可能对应于第一张图像中的`first_key_points`的点。为此，我们需要将关键点列表转换为(*x*,
    *y*)坐标的NumPy数组：
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then the optic flow will return a list of corresponding features in the second
    image (`second_key_arr`):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后光流将返回第二张图像中对应特征的一个列表（`second_key_arr`）：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The function also returns a vector of status bits (`status`), which indicate
    whether the flow for a keypoint has been found or not, and a vector of estimated
    error values (`err`). If we were to ignore these two additional vectors, the recovered
    flow field could look something like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数还返回一个状态位向量（`status`），它指示关键点的光流是否已找到，以及一个估计误差值向量（`err`）。如果我们忽略这两个附加向量，恢复的光流场可能看起来像这样：
- en: '![](img/5765bf9b-d466-4d75-92ef-ff05cfc81bf3.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5765bf9b-d466-4d75-92ef-ff05cfc81bf3.png)'
- en: In this image, an arrow is drawn for each keypoint, starting at the location
    of the keypoint in the first image and pointing to the location of the same keypoint
    in the second image. By inspecting the flow image, we can see that the camera
    moved mostly to the right, but there also seems to be a rotational component.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图像中，每个关键点都画了一个箭头，从第一张图像中关键点的位置开始，指向第二张图像中相同关键点的位置。通过检查光流图像，我们可以看到相机主要向右移动，但似乎还有一个旋转分量。
- en: 'However, some of these arrows are really large, and some of them make no sense.
    For example, it is very unlikely that a pixel in the bottom-right image corner
    actually moved all the way to the top of the image. It is much more likely that
    the flow calculation for this particular keypoint is wrong. Thus, we want to exclude
    all the keypoints for which the status bit is 0 or the estimated error is larger
    than a certain value:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其中一些箭头非常大，而且一些箭头没有意义。例如，图像右下角的像素实际上移动到图像顶部的可能性非常小。更有可能的是，这个特定关键点的光流计算是错误的。因此，我们希望排除所有状态位为0或估计误差大于某个值的特征点：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we draw the flow field again with a limited set of keypoints, the image
    will look like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次使用有限的关键点集绘制流场，图像将看起来像这样：
- en: '![](img/2814c59d-0556-4d6c-a49c-addee3d5b932.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2814c59d-0556-4d6c-a49c-addee3d5b932.png)'
- en: 'The flow field can be drawn with the following public method, which first extracts
    the keypoints using the preceding code and then draws the actual arrows on the
    image:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 流场可以使用以下公共方法绘制，该方法首先使用前面的代码提取关键点，然后在图像上绘制实际的箭头：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The advantage of using optic flow instead of rich features is that the process
    is usually faster and can accommodate the matching of many more points, making
    the reconstruction denser.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用光流而不是丰富特征的优势在于，该过程通常更快，并且可以容纳更多点的匹配，使重建更密集。
- en: The caveat in working with the optic flow is that it works best for consecutive
    images taken by the same hardware, whereas rich features are mostly agnostic to
    this.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理光流时需要注意的问题是，它最适合由相同硬件连续拍摄的照片，而丰富的特征对此则大多不敏感。
- en: Let's learn how to find the camera matrices in the next section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中学习如何找到相机矩阵。
- en: Finding the camera matrices
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找相机矩阵
- en: 'Now that we have obtained the matches between keypoints, we can calculate two
    important camera matrices—the fundamental matrix and the essential matrix. These
    matrices will specify the camera motion in terms of rotational and translational
    components. Obtaining the fundamental matrix (`self.F`) is another OpenCV one-liner:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了关键点之间的匹配，我们可以计算两个重要的相机矩阵——基础矩阵和本质矩阵。这些矩阵将指定相机的运动，以旋转和平移分量表示。获取基础矩阵
    (`self.F`) 是另一个 OpenCV 一行代码：
- en: '[PRE38]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The only difference between `fundamental_matrix` and `essential_matrix` is
    that the latter operates on rectified images:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`fundamental_matrix` 和 `essential_matrix` 之间的唯一区别是后者作用于校正后的图像：'
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The essential matrix (`self.E`) can then be decomposed into rotational and
    translational components, denoted by *[R | t]*, using **singular value decomposition**
    (**SVD**):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本质矩阵 (`self.E`) 可以通过奇异值分解（**SVD**）分解为旋转和平移分量，表示为 *[R | t]*：
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Using the unitary matrices *U* and *V* in combination with an additional matrix,
    *W*, we can now reconstruct *[R | t]*. However, it can be shown that this decomposition
    has four possible solutions and only one of them is the valid second camera matrix.
    The only thing we can do is check all four possible solutions and find the one
    that predicts that all the imaged keypoints lie in front of both cameras.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单位矩阵 *U* 和 *V* 与一个额外的矩阵 *W* 结合，我们现在可以重建 *[R | t]*。然而，可以证明这种分解有四个可能的解，其中只有一个才是有效的第二个相机矩阵。我们唯一能做的就是检查所有四个可能的解，找到预测所有图像关键点都位于两个相机前面的那个解。
- en: 'But prior to that, we need to convert the keypoints from 2D image coordinates
    to homogeneous coordinates. We achieve this by adding a *z* coordinate, which
    we set to `1`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但在那之前，我们需要将关键点从 2D 图像坐标转换为齐次坐标。我们通过添加一个 *z* 坐标来实现这一点，并将其设置为 `1`：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We then iterate over the four possible solutions and choose the one that has
    `_in_front_of_both_cameras` returning `True`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后遍历四个可能的解，并选择返回 `_in_front_of_both_cameras` 为 `True` 的那个解：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can finally construct the *[R | t]* matrices of the two cameras. The
    first camera is simply a canonical camera (no translation and no rotation):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们最终可以构建两个相机的 *[R | t]* 矩阵。第一个相机只是一个标准相机（没有平移和旋转）：
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The second camera matrix consists of *[R | t]*, recovered earlier:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个相机矩阵由之前恢复的 *[R | t]* 组成：
- en: '[PRE44]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `__InFrontOfBothCameras` private method is a helper function that makes
    sure that every pair of keypoints is mapped to 3D coordinates that make them lie
    in front of both cameras:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`__InFrontOfBothCameras` 私有方法是一个辅助函数，确保每一对关键点都映射到使它们位于两个相机前面的 3D 坐标：'
- en: '[PRE45]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If the function finds any keypoint that is not in front of both cameras, it
    will return `False`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数发现任何不在两个相机前面的关键点，它将返回 `False`：
- en: '[PRE46]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: So now that we have found the camera matrices, let's rectify an image in the
    next section, which is a good means to validating whenever the recovered matrices
    are correct.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然我们已经找到了相机矩阵，那么在下一节中校正图像，这是一个验证恢复的矩阵是否正确的好方法。
- en: Applying image rectification
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用图像校正
- en: Maybe the easiest way to make sure that we have recovered the correct camera
    matrices is to rectify the images. If they are rectified correctly, then a point
    in the first image and a point in the second image that corresponds to the same
    3D world point will lie on the same vertical coordinate.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 确保我们已经恢复了正确的相机矩阵的最简单方法可能是校正图像。如果它们被正确校正，那么第一张图像中的一个点和第二张图像中的一个点将对应于相同的 3D 世界点，并将位于相同的垂直坐标上。
- en: 'In a more concrete example, such as in our case, since we know that the cameras
    are upright, we can verify that horizontal lines in the rectified image correspond
    to horizontal lines in the 3D scene. Thus, we follow these steps to rectify our
    image:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更具体的例子中，比如在我们的案例中，因为我们知道相机是垂直的，我们可以验证校正图像中的水平线与3D场景中的水平线相对应。因此，我们遵循以下步骤来校正我们的图像：
- en: 'First, we perform all the steps described in the previous subsections to obtain
    the *[R | t]* matrix of the second camera:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们执行前一小节中描述的所有步骤，以获得第二个相机的*[R | t]*矩阵：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, rectification can be performed with two OpenCV one-liners that remap
    the image coordinates to the rectified coordinates based on the camera matrix
    (`self.K`), the distortion coefficients (`self.d`), the rotational component of
    the essential matrix (`R`), and the translational component of the essential matrix
    (`T`):'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，可以使用两个OpenCV单行代码执行校正，这些代码根据相机矩阵(`self.K`)、畸变系数(`self.d`)、基础矩阵的旋转分量(`R`)和基础矩阵的平移分量(`T`)将图像坐标重新映射到校正坐标：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To make sure that the rectification is accurate, we plot the two rectified
    images (`img_rect1` and `img_rect2`) next to each other:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保校正准确，我们将两个校正后的图像(`img_rect1`和`img_rect2`)并排放置：
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We also draw horizontal blue lines after every `25` pixels, across the side-by-side
    images, to further help us visually investigate the rectification process:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还在每`25`像素后绘制水平蓝色线条，穿过并排的图像，以进一步帮助我们视觉上研究校正过程：
- en: '[PRE50]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now we can easily convince ourselves that the rectification was successful,
    as shown here:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们很容易就能说服自己，校正已经成功，如下所示：
- en: '![](img/d2f5e874-9cc4-4721-9797-407769b822ee.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2f5e874-9cc4-4721-9797-407769b822ee.png)'
- en: Now that we have rectified our image, let's learn how to reconstruct the 3D
    scene in the next section.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经校正了我们的图像，让我们在下一节学习如何重建3D场景。
- en: Reconstructing the scene
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建场景
- en: Finally, we can reconstruct the 3D scene by making use of a process called **triangulation**.
    We are able to infer the 3D coordinates of a point because of the way **epipolar
    geometry** works. By calculating the essential matrix, we get to know more about
    the geometry of the visual scene than we might think. Because the two cameras
    depict the same real-world scene, we know that most of the 3D real-world points
    will be found in both images.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过使用称为**三角测量**的过程来重建3D场景。由于**极线几何**的工作方式，我们能够推断出点的3D坐标。通过计算基础矩阵，我们比我们想象的更多地了解了视觉场景的几何形状。因为两个相机描绘了同一个真实世界的场景，我们知道大多数3D真实世界点将出现在两个图像中。
- en: Moreover, we know that the mapping from the 2D image points to the corresponding
    3D real-world points will follow the rules of geometry. If we study a sufficiently
    large number of image points, we can construct, and solve, a (large) system of
    linear equations to get the ground truth of the real-world coordinates.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们知道从2D图像点到相应3D真实世界点的映射将遵循几何规则。如果我们研究足够多的图像点，我们就可以构建并解决一个（大）线性方程组，以获得真实世界坐标的地面真实值。
- en: Let's return to the Swiss fountain dataset. If we ask two photographers to take
    a picture of the fountain from different viewpoints at the same time, it is not
    hard to realize that the first photographer might show up in the picture of the
    second photographer, and vice versa. The point on the image plane where the other
    photographer is visible is called the **epipole** or **epipolar point**.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到瑞士喷泉数据集。如果我们要求两位摄影师同时从不同的视角拍摄喷泉的照片，不难意识到第一位摄影师可能会出现在第二位摄影师的照片中，反之亦然。图像平面上可以看到另一位摄影师的点被称为**共轭点**或**极线点**。
- en: In more technical terms, the epipole is the point on one camera's image plane
    onto which the center of projection of the other camera projects. It is interesting
    to note that both the epipoles in their respective image planes, and both the
    centers of projection, lie on a single 3D line.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 用更技术性的术语来说，共轭点是另一个相机的投影中心在第一个相机图像平面上的点。值得注意的是，它们各自图像平面上的共轭点和各自的投影中心都位于一条单一的3D直线上。
- en: By looking at the lines between the epipoles and image points, we can limit
    the number of possible 3D coordinates of the image points. In fact, if the projection
    point is known, then the epipolar line (which is the line between the image point
    and the epipole) is known, and, in turn, the same point projected onto the second
    image must lie on that particular epipolar line. *Confusing?* I thought so.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察极点和图像点之间的线条，我们可以限制图像点的可能3D坐标数量。实际上，如果已知投影点，那么极线（即图像点和极点之间的线）是已知的，并且反过来，该点在第二张图像上的投影必须位于特定的极线上。*困惑吗？*我想是的。
- en: 'Let''s just look at these images:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些图像：
- en: '![](img/c524cbb7-fb6e-4f76-b44a-39b9a0eb14fc.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c524cbb7-fb6e-4f76-b44a-39b9a0eb14fc.png)'
- en: Each line here is the epipolar line of a particular point in the image. Ideally,
    all the epipolar lines drawn in the left-hand image should intersect at a point,
    and that point typically lies outside the image. If the calculation is accurate,
    then that point should coincide with the location of the second camera as seen
    from the first camera.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里每一行都是图像中特定点的极线。理想情况下，左手图像中绘制的所有极线都应该相交于一个点，并且该点通常位于图像之外。如果计算准确，那么该点应该与从第一台相机看到的第二台相机的位置重合。
- en: In other words, the epipolar lines in the left-hand image tell us that the camera
    that took the right-hand image is located to our (that is, the first camera's)
    right-hand side. Analogously, the epipolar lines in the right-hand image tell
    us that the camera that took the image on the left is located to our (that is,
    the second camera's) left-hand side.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，左手图像中的极线告诉我们，拍摄右手图像的相机位于我们的右侧（即第一台相机）。类似地，右手图像中的极线告诉我们，拍摄左侧图像的相机位于我们的左侧（即第二台相机）。
- en: Moreover, for each point observed in one image, the same point must be observed
    in the other image on a known epipolar line. This is known as the **epipolar constraint**.
    We can use this fact to show that if two image points correspond to the same 3D
    point, then the projection lines of those two image points must intersect precisely
    at the 3D point. This means that the 3D point can be calculated from two image
    points, which is what we are going to do next.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于在一张图像中观察到的每个点，该点必须在另一张图像上以已知的极线观察到。这被称为**极线约束**。我们可以利用这个事实来证明，如果两个图像点对应于同一个3D点，那么这两个图像点的投影线必须精确相交于该3D点。这意味着可以从两个图像点计算出3D点，这正是我们接下来要做的。
- en: 'Luckily, OpenCV again provides a wrapper to solve an extensive set of linear
    equations, which is done by following these steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，OpenCV再次提供了一个用于解决大量线性方程的包装器，这是通过以下步骤完成的：
- en: 'First, we have to convert our list of matching feature points into a NumPy
    array:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须将我们的匹配特征点列表转换为NumPy数组：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '**Triangulation** is performed next using the preceding two *[R | t]* matrices
    (`self.Rt1` for the first camera and `self.Rt2` for the second camera):'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**三角剖分**接下来使用前面提到的两个*[R | t]*矩阵（`self.Rt1`用于第一台相机，`self.Rt2`用于第二台相机）进行：'
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will return the triangulated real-world points using 4D homogeneous coordinates.
    To convert them to 3D coordinates, we need to divide the (*X*, *Y*, *Z*) coordinates
    by the fourth coordinate, usually referred to as *W*:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使用4D齐次坐标返回三角剖分的真实世界点。要将它们转换为3D坐标，我们需要将(*X*, *Y*, *Z*)坐标除以第四个坐标，通常称为*W*：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: So now that we have obtained the points in the 3D space, let's visualize them
    in the next section to see how they look.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经获得了3D空间中的点，让我们在下一节中可视化它们，看看它们看起来如何。
- en: Understanding 3D point cloud visualization
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解3D点云可视化
- en: The last step is visualizing the triangulated 3D real-world points. An easy
    way of creating 3D scatterplots is by using Matplotlib. However, if you are looking
    for more professional visualization tools, you may be interested in **Mayavi**
    ([http://docs.enthought.com/mayavi/mayavi](http://docs.enthought.com/mayavi/mayavi)),
    **VisPy** ([http://vispy.org](http://vispy.org)), or the **Point Cloud Library**
    ([http://pointclouds.org](http://pointclouds.org)).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是可视化三角剖分的3D真实世界点。创建3D散点图的一个简单方法是通过使用Matplotlib。然而，如果您正在寻找更专业的可视化工具，您可能会对**Mayavi**([http://docs.enthought.com/mayavi/mayavi](http://docs.enthought.com/mayavi/mayavi))、**VisPy**([http://vispy.org](http://vispy.org))或**点云库**([http://pointclouds.org](http://pointclouds.org))感兴趣。
- en: Although the last one does not have Python support for point cloud visualization
    yet, it is an excellent tool for point cloud segmentation, filtering, and sample
    consensus model fitting. For more information, head over to **Strawlab**'s GitHub
    repository at [https://github.com/strawlab/python-pcl](https://github.com/strawlab/python-pcl).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最后一个还没有为点云可视化提供Python支持，但它是一个用于点云分割、过滤和样本一致性模型拟合的出色工具。更多信息，请访问**Strawlab**的GitHub仓库[https://github.com/strawlab/python-pcl](https://github.com/strawlab/python-pcl)。
- en: 'Before we can plot our 3D point cloud, we obviously have to extract the *[R
    | t]* matrix and perform the triangulation as explained earlier:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够绘制我们的3D点云之前，我们显然必须提取*[R | t]*矩阵并执行如前所述的三角剖分：
- en: '[PRE54]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, all we need to do is open a Matplotlib figure and draw each entry of
    `pts3D` in a 3D scatterplot:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需要打开一个Matplotlib图形并绘制`pts3D`的每个条目在3D散点图中：
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The result is most compelling when studied using pyplot's `pan axes` button,
    which lets you rotate and scale the point cloud in all three dimensions. In the
    following screenshot, two projections are illustrated.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pyplot的`pan axes`按钮研究结果最为引人入胜，它允许你在三个维度中旋转和缩放点云。在下图中，展示了两个投影。
- en: 'The first one is from the top and the second one is from some vertical angle
    from the left of the fountain. The color of a point corresponds to the depth of
    that point (*y* coordinate). Most of the points lie near a plane that makes an
    angle with the *XZ* plane (points from red to green). These points represent the
    wall behind the fountain. The other points (from yellow to blue) represent the
    rest of the structure of the fountain:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是从顶部拍摄的，第二个是从喷泉左侧某个垂直角度拍摄的。点的颜色对应于该点的深度（*y*坐标）。大多数点都位于与*XZ*平面成角度的平面上（从红色到绿色）。这些点代表喷泉后面的墙壁。其他点（从黄色到蓝色）代表喷泉的其余结构：
- en: '![](img/2b3c5918-30af-4d8f-ab31-644a5aa2d12e.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b3c5918-30af-4d8f-ab31-644a5aa2d12e.png)'
- en: 'The projection from some vertical angle from the left of the fountain is shown
    in the following screenshot:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 从喷泉左侧某个垂直角度的投影显示在下图中：
- en: '![](img/368d1588-2a17-4455-836e-4cf24c4f58d4.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/368d1588-2a17-4455-836e-4cf24c4f58d4.png)'
- en: So now that you have completed your first app for 3D reconstruction, you have
    started to dive into a computer vision field called structure from motion. This
    is an intensively developing field. Let's understand what this field of research
    is trying to deal with within the next section.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，现在你已经完成了你的第一个3D重建应用程序，你已经开始深入到计算机视觉领域中的结构从运动领域。这是一个快速发展领域。让我们在下一节中了解这个研究领域试图解决的问题。 '
- en: Learning about structure from motion
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习从运动中获取结构
- en: So far in this chapter, we have gone through some math and we can reconstruct
    the depth of a scene based on a couple of images taken from different angles,
    which is a problem of reconstruction of a 3D structure from camera motion.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经介绍了一些数学知识，并且可以根据从不同角度拍摄的一组图像重建场景的深度，这是一个从相机运动重建3D结构的问题。
- en: In computer vision, the process of reconstruction of 3D structures of the scene
    based on the sequence of images is usually referred to as **structure from motion**.
    A similar set of problems is the structure from stereo vision—in reconstruction
    from stereo vision, there are two cameras, located at a certain distance from
    each other and in structure from motion, there are different images taken from
    different angles and positions. *There's not much difference conceptually, right?*
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，根据图像序列重建场景3D结构的过程通常被称为**从运动中获取结构**。类似的问题集是**从立体视觉中获取结构**——在立体视觉重建中，有两个相机，它们彼此之间相隔一定距离，而在从运动中获取结构中，有从不同角度和位置拍摄的不同图像。*在概念上没有太大的区别，对吧？*
- en: Let's think about human vision. People are good at estimating distance and relative
    locations of objects. A person doesn't even need two eyes for it—we can look with
    one eye and estimate distances and relative locations pretty well. Moreover, stereoscopic
    vision only takes place when the distance between eyes is of a similar order of
    magnitude as the distance to an object when the projections of the scene on the
    eye have noticeable differences.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下人类视觉。人们擅长估计物体的距离和相对位置。一个人甚至不需要两只眼睛来做这件事——我们可以用一只眼睛看，并且相当准确地估计距离和相对位置。此外，立体视觉只有在眼睛之间的距离与物体到眼睛的投影有显著差异时才会发生。
- en: For example, if one object is a football field away, the relative location of
    the eyes doesn't matter, whereas if you look at your nose, the view changes a
    lot. To illustrate further that stereoscopy is not the essence of our vision,
    we could look at a photograph where we can describe the relative location of the
    objects pretty well, but what we are actually looking at is a flat surface.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个物体在足球场外，眼睛的相对位置并不重要，而如果你看你的鼻子，视角会改变很多。为了进一步说明立体视觉并不是我们视觉的本质，我们可以看看一张我们可以很好地描述物体相对位置的相片，但我们实际上看到的是一个平面。
- en: People do not have such skills at infancy; observations show that infants are
    bad at locating the placements of the objects. So, probably, a person learns this
    skill during their conscious life by looking at the world and playing with objects.
    Next, a question arises—*if a person learns the 3D structure of the world, can't
    we make a computer to do so?*
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在大脑发育的早期并不具备这样的技能；观察表明，婴儿在定位物体位置方面很糟糕。因此，可能一个人在意识生活中通过观察世界和玩物体来学习这种技能。接下来，一个问题出现了——*如果一个人学会了世界的3D结构，我们能否让计算机也做到这一点？*
- en: There are already interesting models that try to do so. For example, **Vid2Depth**
    ([https://arxiv.org/pdf/1802.05522.pdf](https://arxiv.org/pdf/1802.05522.pdf)) is
    a deep learning model where the authors train a model that predicts depth in a
    single image; meanwhile, the model is trained on a sequence of video frames without
    any depth annotation. Similar problems are active topics for research nowadays.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有一些有趣的模型试图做到这一点。例如，**Vid2Depth** ([https://arxiv.org/pdf/1802.05522.pdf](https://arxiv.org/pdf/1802.05522.pdf))
    是一个深度学习模型，其中作者训练了一个模型，该模型可以预测单张图像中的深度；同时，该模型在没有任何深度标注的视频帧序列上进行了训练。类似的问题现在是研究的热点。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored a way of reconstructing a scene in 3D by inferring
    the geometrical features of 2D images taken by the same camera. We wrote a script
    to calibrate a camera, and you learned about fundamental and essential matrices.
    We used this knowledge to perform triangulation. We then went on to visualize
    the real-world geometry of a scene in a 3D point cloud using simple 3D scatterplots
    in Matplotlib.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了一种通过推断由同一相机拍摄的2D图像的几何特征来重建场景的方法。我们编写了一个脚本来校准相机，并学习了基本和必要的矩阵。我们利用这些知识来进行三角测量。然后，我们继续使用Matplotlib中的简单3D散点图来可视化场景的真实世界几何形状。
- en: Going forward from here, it will be possible to store the triangulated 3D points
    in a file that can be parsed by the Point Cloud Library or to repeat the procedure
    for different image pairs so that we can generate a denser and more accurate reconstruction.
    Although we have covered a lot in this chapter, there is a lot more left to do.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们将能够将三角化的3D点存储在可以被点云库解析的文件中，或者对不同的图像对重复该过程，以便我们可以生成更密集和更精确的重建。尽管我们在这章中已经涵盖了大量的内容，但还有很多工作要做。
- en: Typically, when talking about a structure-from-motion pipeline, we include two
    additional steps that we have not talked about so far—**bundle adjustment** and
    **geometry fitting**. One of the most important steps in such a pipeline is to
    refine the 3D estimate in order to minimize reconstruction errors. Typically,
    we would also want to get all points that do not belong to our object of interest
    out of the cloud. But with the basic code in hand, you can now go ahead and write
    your own advanced structure-from-motion pipeline!
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们谈论运动结构化流程时，我们会包括两个之前未曾提及的额外步骤——**捆绑调整**和**几何拟合**。在这样的流程中，最重要的步骤之一是细化3D估计，以最小化重建误差。通常，我们还想将不属于我们感兴趣对象的所有点从点云中移除。但是，有了基本的代码在手，你现在可以继续编写你自己的高级运动结构化流程！
- en: In the next chapter, we will use the concepts we learned in 3D Scene reconstruction.
    We will use the key points and features that we learned to extract in this chapter,
    and we'll apply other alignment algorithms to create panoramas. We will also dive
    deep into other topics in computational photography, understand the core concepts,
    and create **High Dynamic Range** (**HDR**) images.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用我们在3D场景重建中学到的概念。我们将使用本章中我们学到的关键点和特征来提取，并将应用其他对齐算法来创建全景图。我们还将深入研究计算摄影学的其他主题，理解核心概念，并创建**高动态范围**（**HDR**）图像。
