- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Dimensionality Reduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度缩减
- en: In this chapter, we’ll go through a number of dimension-reduction tasks. We’ll
    look at the conditions in which dimension-reduction is required and learn how
    to use dimension-reduction algorithms efficiently in C++ with various libraries.
    Dimensionality reduction involves transforming high-dimensional data into a new
    representation with fewer dimensions while preserving the most crucial information
    from the original data. Such a transformation can help us visualize multidimensional
    space, which can be useful in the data exploration stage or when identifying the
    most relevant features in dataset samples. Some **machine learning** (**ML**)
    techniques can perform better or faster if our data has a smaller number of features
    since it can consume fewer computational resources. The main purpose of this kind
    of transformation is to save the essential features—those features that hold the
    most critical information present in the original data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论多个维度缩减任务。我们将探讨需要维度缩减的条件，并学习如何使用各种库在 C++ 中高效地使用维度缩减算法。维度缩减涉及将高维数据转换成具有较少维度的新的表示形式，同时保留原始数据中最关键的信息。这种转换可以帮助我们可视化多维空间，这在数据探索阶段或识别数据集样本中最相关的特征时可能很有用。如果我们的数据具有较少的特征，某些机器学习（ML）技术可能会表现得更好或更快，因为它们可以消耗更少的计算资源。这种转换的主要目的是保存关键特征——那些在原始数据中包含最关键信息的特征。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of dimension-reduction methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度缩减方法的概述
- en: Exploring linear methods for dimension-reduction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索用于维度缩减的线性方法
- en: Exploring non-linear methods for dimension-reduction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索用于维度缩减的非线性方法
- en: Understanding dimension-reduction algorithms with various С++ libraries
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种 C++ 库理解维度缩减算法
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The technologies you’ll need for this chapter are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的技术如下：
- en: The Tapkee library
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tapkee 库
- en: The `Dlib` library
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dlib` 库'
- en: The `plotcpp` library
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plotcpp` 库'
- en: A modern C++ compiler with C++20 support
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 C++20 的现代 C++ 编译器
- en: The CMake build system, version >= 3.24
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMake 构建系统，版本 >= 3.24
- en: 'The code files for this chapter can be found at the following GitHub repository:
    [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在以下 GitHub 仓库中找到：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06)。
- en: An overview of dimension-reduction methods
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度缩减方法的概述
- en: The main goal of dimension-reduction methods is to make the dimension of the
    transformed representation correspond with the internal dimension of the data.
    In other words, it should be similar to the minimum number of variables necessary
    to express all the possible properties of the data. Reducing the dimension helps
    mitigate the impact of the curse of dimensionality and other undesirable properties
    that occur in high-dimensional spaces. As a result, reducing dimensionality can
    effectively solve problems regarding classification, visualization, and compressing
    high-dimensional data. It makes sense to apply dimensionality reduction only when
    particular data is redundant; otherwise, we can lose important information. In
    other words, if we are able to solve the problem using data of smaller dimensions
    with the same level of efficiency and accuracy, then some of our data is redundant.
    Dimensionality reduction allows us to reduce the time and computational costs
    of solving a problem. It also makes data and the results of data analysis easier
    to interpret.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 维度缩减方法的主要目标是使变换表示的维度与数据的内部维度相对应。换句话说，它应该类似于表达数据所有可能属性所需的最小变量数。降低维度有助于减轻维度诅咒以及其他在高维空间中出现的不可取属性的影响。因此，降低维度可以有效地解决与分类、可视化和压缩高维数据相关的问题。只有在特定数据冗余的情况下才应用维度缩减是有意义的；否则，我们可能会丢失重要信息。换句话说，如果我们能够使用具有相同效率和精度的较小维度数据来解决问题，那么我们的一些数据就是冗余的。维度缩减使我们能够降低解决问题的耗时和计算成本。它还使数据和数据分析的结果更容易解释。
- en: It makes sense to reduce the number of features when the information that can
    be used to solve the problem at hand qualitatively is contained in a specific
    subset of features. Non-informative features are a source of additional noise
    and affect the accuracy of the model parameter’s estimation. In addition, datasets
    with a large number of features can contain groups of correlated variables. The
    presence of such feature groups leads to the duplication of information, which
    may distort the model’s results and affect how well it estimates the values of
    its parameters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当解决问题所需的信息定性上包含在特定特征子集中时，减少特征的数量是有意义的。非信息性特征是额外噪声的来源，并影响模型参数估计的准确性。此外，具有大量特征的集合可能包含相关变量的组。这种特征组的出现会导致信息重复，这可能会扭曲模型的结果并影响其对参数值的估计能力。
- en: The methods surrounding dimensionality reduction are mainly unsupervised because
    we don’t know which features or variables can be excluded from the original dataset
    without losing the most crucial information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 维度约简的方法主要是无监督的，因为我们不知道哪些特征或变量可以从原始数据集中排除而不丢失最重要的信息。
- en: 'Some real-life examples of dimensionality reduction include the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 维度约简的一些实际例子包括以下内容：
- en: In recommender systems, dimensionality reduction can be used to represent users
    and items as vectors in a lower-dimensional space, making it easier to find similar
    users or items
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推荐系统中，维度约简可以用来将用户和项目表示为低维空间中的向量，这使得找到相似的用户或项目变得更容易。
- en: In image recognition, dimensionality reduction techniques, such as **principal
    component analysis** (**PCA**), can be applied to reduce the size of images while
    preserving their important features
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像识别中，可以应用如**主成分分析**（PCA）之类的维度约简技术，以在保留重要特征的同时减小图像的大小。
- en: In text analysis, dimensionality reduction can be employed to transform large
    collections of documents into lower-dimensional representations that capture the
    main topics discussed in the documents
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本分析中，维度约简可以用来将大量文档转换为低维表示，这些表示捕获了文档中讨论的主要主题。
- en: 'Dimensionality reduction methods can be classified into two groups: feature
    selection and the creation of new low-dimensional features. These methods can
    then be subdivided into **linear** and **non-linear** approaches, depending on
    the nature of the data and the mathematical apparatus being used.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 维度约简方法可以分为两组：特征选择和创建新的低维特征。这些方法可以根据数据的性质和所使用的数学工具分为**线性**和**非线性**方法。
- en: Feature selection methods
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择方法
- en: 'Feature selection methods don’t change the initial values of the variables
    or features; instead, they remove the irrelevant features from the source dataset.
    Some of the feature selection methods we can use are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择方法不会改变变量或特征的初始值；相反，它们从源数据集中移除无关的特征。我们可以使用的某些特征选择方法如下：
- en: '**Missing value ratio**: This method is based on the idea that a feature that
    misses many values should be eliminated from a dataset because it doesn’t contain
    valuable information and can distort the model’s performance results. So, if we
    have some criteria for identifying missing values, we can calculate their ratio
    to typical values and set a threshold that we can use to eliminate features with
    a high missing value ratio.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失值比率**：这种方法基于这样的想法，即缺失许多值的特征应该从数据集中删除，因为它不包含有价值的信息，并且可能扭曲模型性能的结果。因此，如果我们有一些识别缺失值的准则，我们可以计算它们与典型值的比率，并设置一个阈值，我们可以用它来消除具有高缺失值比率的特征。'
- en: '**Low variance filter**: This method is used to remove features with low variance
    because such features don’t contain enough information to improve model performance.
    To apply this method, we need to calculate the variance for each feature, sort
    them in ascending order by this value, and leave only those with the highest variance
    values.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低方差过滤器**：这种方法用于移除具有低方差的特征，因为这些特征不包含足够的信息来提高模型性能。要应用这种方法，我们需要计算每个特征的方差，按此值升序排序，然后只保留方差值最高的那些特征。'
- en: '**High correlation filter**: This method is based on the idea that if two features
    have a high correlation, then they carry similar information. Also, highly correlated
    features can significantly reduce the performance of some ML models, such as linear
    and logistic regression. Therefore, the primary goal of this method is to leave
    only the features that have a high correlation with target values and don’t have
    much correlation with each other.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高相关性滤波器**：这种方法基于这样的想法，如果两个特征具有高相关性，那么它们携带相似的信息。此外，高度相关的特征可以显著降低某些机器学习模型的性能，例如线性回归和逻辑回归。因此，这种方法的主要目标是仅保留与目标值高度相关且彼此之间相关性不大的特征。'
- en: '**Random forest**: This method can be used for feature selection effectively
    (although it wasn’t initially designed for this kind of task). After we’ve built
    the forest, we can estimate what features are most important by estimating the
    impurity factor in the tree’s nodes. This factor shows the measure of split distinctness
    in the tree’s nodes, and it demonstrates how well the current feature (a random
    tree only uses one feature in a node to split input data) splits data into two
    distinct buckets. Then, this estimation can be averaged across all the trees in
    the forest. Features that split data better than others can be selected as the
    most important ones.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**：这种方法可以有效地用于特征选择（尽管它最初并不是为此类任务设计的）。在我们构建森林之后，我们可以通过估计树节点中的不纯度因子来估计哪些特征最重要。这个因子显示了树节点中分裂的独特性度量，并展示了当前特征（随机树仅在节点中使用一个特征来分割输入数据）如何将数据分割成两个不同的桶。然后，这个估计可以跨森林中的所有树进行平均。分割数据比其他特征更好的特征可以被选为最重要的特征。'
- en: '**Backward feature elimination and forward feature selection**: These are iterative
    methods that are used for feature selection. In backward feature elimination,
    after we’ve trained the model with a full feature set and estimated its performance,
    we remove its features one by one and train the model with a reduced feature set.
    Then, we compare the model’s performances and decide how much performance is improved
    by removing feature changes—in other words, we’re deciding how important each
    feature is. In forward feature selection, the training process goes in the opposite
    direction. We start with one feature and then add more of them. These methods
    are very computationally expensive and can only be used on small datasets.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆向特征消除和正向特征选择**：这些是用于特征选择的迭代方法。在逆向特征消除中，我们在使用完整特征集训练模型并估计其性能后，逐个移除其特征，并使用减少的特征集重新训练模型。然后，我们比较模型的性能，并决定通过移除特征变化而提高的性能有多少——换句话说，我们正在决定每个特征的重要性。在正向特征选择中，训练过程的方向相反。我们从单个特征开始，然后添加更多。这些方法计算成本非常高，只能用于小型数据集。'
- en: Dimensionality reduction methods
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低方法
- en: Dimensionality reduction methods transform an original feature set into a new
    feature set that usually contains new features that weren’t present in the initial
    dataset. These methods can also be divided into two subclasses—linear and non-linear.
    The non-linear methods are usually more computationally expensive, so if we have
    a prior assumption about our feature’s data linearity, we can choose the more
    suitable class of methods at the initial stage.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低方法将原始特征集转换成新的特征集，通常包含初始数据集中不存在的新特征。这些方法也可以分为两个子类——线性和非线性。非线性方法通常计算成本更高，因此如果我们对特征数据的线性有先验假设，我们可以在初始阶段选择更合适的方法类。
- en: The following sections will describe the various linear and non-linear methods
    we can use for dimension-reduction.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将描述我们可以用于降维的各种线性和非线性方法。
- en: Exploring linear methods for dimension-reduction
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索降维的线性方法
- en: 'In this section, we will describe the most popular linear methods that are
    used for dimension-reduction, such as the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述用于降维的最流行的线性方法，例如以下内容：
- en: PCA
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA
- en: '**Singular value** **decomposition** (**SVD**)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）'
- en: '**Independent component** **analysis** (**ICA**)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立成分分析**（**ICA**）'
- en: '**Linear discriminant** **analysis** (**LDA**)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）'
- en: Factor analysis
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分析
- en: '**Multidimensional** **scaling** (**MDS**)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多维尺度分析**（**MDS**）'
- en: Principal component analysis
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析
- en: '**PCA** is one of the most intuitively simple and frequently used methods for
    applying dimension-reduction to data and projecting it onto an orthogonal subspace
    of features. In a very general form, it can be represented as the assumption that
    all our observations look like some ellipsoid in the subspace of our original
    space. Our new basis in this space coincides with the axes of this ellipsoid.
    This assumption allows us to get rid of strongly correlated features simultaneously
    since the basis vectors of the space we project them onto are orthogonal.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA**）是将数据降维并投影到特征正交子空间的最直观简单且常用方法之一。在非常一般的形式中，它可以表示为我们所有的观测值看起来都像是原始空间子空间中的某个椭球体。在这个空间中，我们的新基与这个椭球体的轴相一致。这个假设允许我们同时去除强相关特征，因为我们投影它们的空间基向量是正交的。'
- en: The dimension of this ellipsoid is equal to the dimension of the original space,
    but our assumption that the data lies in a subspace of a smaller dimension allows
    us to discard the other subspaces in the new projection; namely, the subspace
    with the least extension of the ellipsoid. We can do this greedily, choosing a
    new element one by one on the basis of our new subspace, and then taking the axis
    of the ellipsoid with maximum dispersion successively from the remaining dimensions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个椭球体的维度等于原始空间的维度，但我们的假设是数据位于一个较小维度的子空间中，这使得我们可以丢弃新投影中的其他子空间；即椭球体扩展最少的子空间。我们可以贪婪地这样做，根据我们的新子空间逐个选择新的元素，然后从剩余的维度中依次选择椭球体的最大分散度的轴。
- en: 'To reduce the dimension of our data from ![](img/B19849_Formula_001.png) to
    ![](img/B19849_Formula_002.png), we need to choose the top ![](img/B19849_Formula_003.png)
    axes of such an ellipsoid, sorted in descending order by dispersion along the
    axes. To begin with, we calculate the variances and covariances of the original
    features. This is done by using a **covariance matrix**. By the definition of
    covariance, for two signs, ![](img/B19849_Formula_004.png) and ![](img/B19849_Formula_005.png),
    their covariance should be as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的数据维度从![图片](img/B19849_Formula_001.png)降低到![图片](img/B19849_Formula_002.png)，我们需要选择这样一个椭球体的前![](img/B19849_Formula_003.png)个轴，并按轴上的分散度降序排列。首先，我们计算原始特征的方差和协方差。这是通过使用**协方差矩阵**来完成的。根据协方差的定义，对于两个符号，![](img/B19849_Formula_004.png)和![](img/B19849_Formula_005.png)，它们的协方差应该是以下这样：
- en: '![](img/B19849_Formula_006.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_006.jpg)'
- en: Here, ![](img/B19849_Formula_007.png) is the mean of the ![](img/B19849_Formula_008.png)
    feature.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_007.png)是![](img/B19849_Formula_008.png)特征的均值。
- en: 'In this case, we note that the covariance is symmetric and that the covariance
    of the vector itself is equal to its dispersion. Thus, the covariance matrix is
    a symmetric matrix where the dispersions of the corresponding features lie on
    the diagonal and the covariances of the corresponding pairs of features lie outside
    the diagonal. In the matrix view, where ![](img/B19849_Formula_009.png) is the
    observation matrix, our covariance matrix looks like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们注意到协方差是对称的，并且向量的协方差等于其分散度。因此，协方差矩阵是一个对称矩阵，其中对应特征的分散度位于对角线上，而对应特征对的协方差位于对角线之外。在矩阵视图中，其中![](img/B19849_Formula_009.png)是观测矩阵，我们的协方差矩阵看起来是这样的：
- en: '![](img/B19849_Formula_010.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_010.jpg)'
- en: The covariance matrix is a generalization of variance in the case of multidimensional
    random variables—it also describes the shape (spread) of a random variable, as
    does the variance. Matrices, such as linear operators, have eigenvalues and eigenvectors.
    They are interesting because when we act on the corresponding linear space or
    transform it with our matrix, the eigenvectors remain in place, and they are only
    multiplied by the corresponding eigenvalues. This means they define a subspace
    that remains in place or *goes into itself* when we apply a linear operator matrix
    to it. Formally, an eigenvector, ![](img/B19849_Formula_011.png), with an eigenvalue
    for a matrix is defined simply as ![](img/B19849_Formula_012.png).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是多维随机变量中方差的推广——它也描述了随机变量的形状（分布），就像方差一样。矩阵，如线性算子，有特征值和特征向量。它们很有趣，因为当我们对相应的线性空间或用我们的矩阵对其进行变换时，特征向量保持不变，它们只乘以相应的特征值。这意味着它们定义了一个子空间，当我们对它应用线性算子矩阵时，这个子空间保持不变或*进入自身*。形式上，一个特征向量![](img/B19849_Formula_011.png)，对于矩阵有一个特征值，被简单地定义为![](img/B19849_Formula_012.png)。
- en: The covariance matrix for our sample, ![](img/B19849_Formula_013.png), can be
    represented as a product, ![](img/B19849_Formula_014.png). From the Rayleigh relation,
    it follows that the maximum variation of our dataset can be achieved along the
    eigenvector of this matrix, which corresponds to the maximum eigenvalue. This
    is also true for projections on a higher number of dimensions—the variance (covariance
    matrix) of the projection onto the *m*-dimensional space is maximum in the direction
    of ![](img/B19849_Formula_015.png) eigenvectors with maximum eigenvalues. Thus,
    the principal components that we would like to project our data for are simply
    the eigenvectors of the corresponding top *k* pieces of the eigenvalues of this
    matrix.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们样本的协方差矩阵！[](img/B19849_Formula_013.png)可以表示为乘积！[](img/B19849_Formula_014.png)。从瑞利关系可以得出，我们的数据集的最大变化可以沿着这个矩阵的特征向量实现，这对应于最大的特征值。对于更高维度的投影也是一样——投影到*m*-维空间上的方差（协方差矩阵）在具有最大特征值的特征向量方向上最大。因此，我们想要将数据投影到的主要成分只是这个矩阵对应于前*k*个特征值的特征向量。
- en: The largest vector has a direction similar to the regression line, and by projecting
    our sample onto it, we lose information, similar to the sum of the residual members
    of the regression. It is necessary to make the operation, ![](img/B19849_Formula_016.png)
    (the vector length (magnitude) should be equal to one), perform the projection.
    If we don’t have a single vector and have a hyperplane instead, then instead of
    the vector, ![](img/B19849_Formula_017.png), we take the matrix of basis vectors,
    ![](img/B19849_Formula_018.png). The resulting vector (or matrix) is an array
    of projections of our observations; that is, we need to multiply our data matrix
    on the basis vectors matrix, and we get the projection of our data orthogonally.
    Now, if we multiply the transpose of our data matrix and the matrix of the principal
    component vectors, we restore the original sample in the space where we projected
    it onto the basis of the principal components. If the number of components is
    less than the dimension of the original space, we lose some information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的向量方向与回归线相似，通过将我们的样本投影到它上面，我们丢失了信息，类似于回归的残差成员的总和。进行操作！[](img/B19849_Formula_016.png)（向量长度（大小）应等于一），进行投影。如果我们没有单个向量，而是有一个超平面，那么我们不是取向量！[](img/B19849_Formula_017.png)，而是取基向量矩阵！[](img/B19849_Formula_018.png)。得到的向量（或矩阵）是我们观察值的投影数组；也就是说，我们需要将我们的数据矩阵乘以基向量矩阵，我们得到数据的正交投影。现在，如果我们乘以数据矩阵的转置和主成分向量矩阵，我们就可以在将原始样本投影到主成分基上的空间中恢复原始样本。如果成分的数量小于原始空间的维度，我们会丢失一些信息。
- en: Singular value decomposition
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单值分解
- en: SVD is an important method that’s used to analyze data. The resulting matrix
    decomposition has a meaningful interpretation from an ML point of view. It can
    also be used to calculate PCA. SVD is rather slow. Therefore, when the matrices
    are too large, randomized algorithms are used. However, the SVD calculation is
    computationally more efficient than the calculation for the covariance matrix
    and its eigenvalues in the original PCA approach. Therefore, PCA is often implemented
    in terms of SVD. Let’s take a look.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: SVD是一种重要的数据分析方法。从机器学习的角度来看，其结果矩阵分解具有有意义的解释。它还可以用于计算PCA。SVD相对较慢。因此，当矩阵太大时，会使用随机算法。然而，SVD的计算效率比原始PCA方法中协方差矩阵及其特征值的计算要高。因此，PCA通常通过SVD来实现。让我们看看。
- en: 'The essence of SVD is straightforward—any matrix (real or complex) is represented
    as a product of three matrices:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SVD的本质很简单——任何矩阵（实数或复数）都可以表示为三个矩阵的乘积：
- en: '![](img/B19849_Formula_019.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_019.jpg)'
- en: 'Here, ![](img/B19849_Formula_020.png) is a unitary matrix of order ![](img/B19849_Formula_0211.png)
    and ![](img/B19849_Formula_0221.png) is a matrix of size ![](img/B19849_Formula_023.png)
    on the main diagonal, which is where there are non-negative numbers called singular
    values (elements outside the main diagonal are zero—such matrices are sometimes
    called rectangular diagonal matrices). ![](img/B19849_Formula_024.png) is a Hermitian-conjugate
    ![](img/B19849_Formula_025.png) matrix of order ![](img/B19849_Formula_026.png).
    The ![](img/B19849_Formula_027.png) columns of the matrices ![](img/B19849_Formula_028.png)
    and ![](img/B19849_Formula_029.png) columns of the matrix ![](img/B19849_Formula_030.png)
    are called the left and right singular vectors of matrix ![](img/B19849_Formula_0311.png),
    respectively. To reduce the number of dimensions, matrix ![](img/B19849_Formula_0221.png)
    is important, the elements of which, when raised to the second power, can be interpreted
    as a variance that each component puts into a joint distribution, and they are
    in descending order: ![](img/B19849_Formula_033.png). Therefore, when we choose
    the number of components in SVD (as in PCA), we should take the sum of their variances
    into account.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_020.png)是一个阶数为![](img/B19849_Formula_0211.png)的单位矩阵，而![](img/B19849_Formula_0221.png)是一个主对角线上的大小为![](img/B19849_Formula_023.png)的矩阵，其中包含非负数，称为奇异值（主对角线以外的元素为零——这样的矩阵有时被称为矩形对角矩阵）。![](img/B19849_Formula_024.png)是一个阶数为![](img/B19849_Formula_026.png)的厄米共轭![](img/B19849_Formula_025.png)矩阵。矩阵![](img/B19849_Formula_028.png)的![](img/B19849_Formula_027.png)列和矩阵![](img/B19849_Formula_029.png)的![](img/B19849_Formula_030.png)列分别被称为矩阵![](img/B19849_Formula_0311.png)的左奇异向量和右奇异向量。为了减少维度，矩阵![](img/B19849_Formula_0221.png)很重要，其元素平方后可以解释为每个成分对联合分布的贡献的方差，并且它们按降序排列：![](img/B19849_Formula_033.png)。因此，当我们选择SVD（如PCA）中的成分数量时，应该考虑它们的方差之和。
- en: 'The relation between SVD and PCA can be described in the following way: ![](img/B19849_Formula_034.png)
    is the covariance matrix given by ![](img/B19849_Formula_035.png). It is a symmetric
    matrix, so it can be diagonalized as ![](img/B19849_Formula_036.png), where ![](img/B19849_Formula_025.png)
    is a matrix of eigenvectors (each column is an eigenvector) and ![](img/B19849_Formula_038.png)
    is a diagonal matrix of eigenvalues, ![](img/B19849_Formula_039.png), in decreasing
    order on the diagonal. The eigenvectors are called principal axes or principal
    directions of the data. Projections of the data on the principal axes are called
    **principal components**, also known as **principal component scores**. They are
    newly transformed variables. The ![](img/B19849_Formula_040.png) principal component
    is given by the ![](img/B19849_Formula_0411.png) column of ![](img/B19849_Formula_042.png).
    The coordinates of the ![](img/B19849_Formula_043.png) data point in the new principal
    component’s space are given by the ![](img/B19849_Formula_044.png) row of ![](img/B19849_Formula_045.png).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: SVD与PCA之间的关系可以用以下方式描述：![](img/B19849_Formula_034.png)是![](img/B19849_Formula_035.png)给出的协方差矩阵。它是一个对称矩阵，因此它可以被对角化成![](img/B19849_Formula_036.png)，其中![](img/B19849_Formula_025.png)是一个特征向量矩阵（每一列都是一个特征向量）和![](img/B19849_Formula_038.png)是一个对角矩阵，包含特征值![](img/B19849_Formula_039.png)，对角线上的值按降序排列。特征向量被称为数据的主轴或主方向。数据在主轴上的投影被称为**主成分**，也称为**主成分得分**。它们是新的变换变量。![](img/B19849_Formula_040.png)主成分由![](img/B19849_Formula_0411.png)的![](img/B19849_Formula_042.png)列给出。![](img/B19849_Formula_043.png)数据点在新主成分空间中的坐标由![](img/B19849_Formula_044.png)的![](img/B19849_Formula_045.png)行给出。
- en: By performing SVD on ![](img/B19849_Formula_046.png), we get ![](img/B19849_Formula_047.png),
    where ![](img/B19849_Formula_048.png) is a unitary matrix and ![](img/B19849_Formula_049.png)
    is the diagonal matrix of singular values, ![](img/B19849_Formula_050.png). We
    can observe that ![](img/B19849_Formula_0511.png), which means that the right
    singular vectors, ![](img/B19849_Formula_052.png), are principal directions and
    that singular values are related to the eigenvalues of the covariance matrix via
    ![](img/B19849_Formula_053.png). Principal components are given by ![](img/B19849_Formula_054.png).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对![](img/B19849_Formula_046.png)执行SVD，我们得到![](img/B19849_Formula_047.png)，其中![](img/B19849_Formula_048.png)是一个单位矩阵，而![](img/B19849_Formula_049.png)是奇异值对角矩阵，![](img/B19849_Formula_050.png)。我们可以观察到![](img/B19849_Formula_0511.png)，这意味着右奇异向量![](img/B19849_Formula_052.png)是主方向，而奇异值与协方差矩阵的特征值通过![](img/B19849_Formula_053.png)相关。主成分由![](img/B19849_Formula_054.png)给出。
- en: Independent component analysis
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独立成分分析
- en: 'The **ICA** method was proposed as a way to solve the problem of **blind signal
    separation** (**BSS**); that is, selecting independent signals from mixed data.
    Let’s look at an example of the task of BSS. Suppose we have two people in the
    same room who are talking and generating acoustic waves. We have two microphones
    in different parts of the room, recording sound. The analysis system receives
    two signals from the two microphones, each of which is a digitized mixture of
    two acoustic waves—one from people speaking and one from some other noise (for
    example, playing music). Our goal is to select our initial signals from the incoming
    mixtures. Mathematically, the problem can be described as follows. We represent
    the incoming mixture in the form of a linear combination, where ![](img/B19849_Formula_055.png)
    represents the displacement coefficients and ![](img/B19849_Formula_056.png) represents
    the values of the vector of independent components:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**ICA** 方法被提出作为一种解决**盲信号分离**（**BSS**）问题的方法；也就是说，从混合数据中选择独立信号。让我们看看BSS任务的一个例子。假设同一房间里有两人在交谈并产生声波。我们在房间的不同部分放置了两个麦克风，记录声音。分析系统从两个麦克风接收两个信号，每个信号都是两个声波的数字化混合——一个来自说话的人，另一个来自一些其他噪声（例如，播放音乐）。我们的目标是选择来自混合信号的初始信号。从数学上讲，这个问题可以描述如下。我们用线性组合的形式表示进入的混合，其中![img/B19849_Formula_055.png](img/B19849_Formula_055.png)
    表示位移系数，![img/B19849_Formula_056.png](img/B19849_Formula_056.png) 表示独立成分向量的值：'
- en: '![](img/B19849_Formula_057.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![img/B19849_Formula_057.jpg](img/B19849_Formula_057.jpg)'
- en: 'In matrix form, this can be expressed as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以矩阵形式，这可以表示如下：
- en: '![](img/B19849_Formula_058.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![img/B19849_Formula_058.jpg](img/B19849_Formula_058.jpg)'
- en: 'Here, we have to find the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们必须找到以下内容：
- en: '![](img/B19849_Formula_059.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![img/B19849_Formula_059.jpg](img/B19849_Formula_059.jpg)'
- en: 'In this equation, ![](img/B19849_Formula_060.png) is a matrix of input signal
    values, ![](img/B19849_Formula_0611.png) is a matrix of displacement coefficients
    or mixing matrix, and ![](img/B19849_Formula_062.png) is a matrix of independent
    components. Thus, the problem is divided into two. The first part is to get an
    estimate, ![](img/B19849_Formula_063.png), of the variables,![](img/B19849_Formula_064.png),
    of the original independent components. The second part is to find the matrix,
    ![](img/B19849_Formula_065.png). How this method works is based on two principles:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![img/B19849_Formula_060.png](img/B19849_Formula_060.png) 是输入信号值的矩阵，![img/B19849_Formula_0611.png](img/B19849_Formula_0611.png)
    是位移系数或混合矩阵的矩阵，![img/B19849_Formula_062.png](img/B19849_Formula_062.png) 是独立成分的矩阵。因此，问题被分为两部分。第一部分是得到原始独立成分变量，![img/B19849_Formula_063.png](img/B19849_Formula_063.png)，的估计值，![img/B19849_Formula_064.png](img/B19849_Formula_064.png)。第二部分是找到矩阵，![img/B19849_Formula_065.png](img/B19849_Formula_065.png)。这种方法的工作原理基于两个原则：
- en: Independent components must be statistically independent (![](img/B19849_Formula_066.png)
    matrix values). Roughly speaking, the values of one vector of an independent component
    do not affect the values of another component.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立成分必须是统计独立的（![img/B19849_Formula_066.png](img/B19849_Formula_066.png) 矩阵值）。粗略地说，独立成分的一个向量的值不会影响另一个成分的值。
- en: Independent components must have a non-Gaussian distribution.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立成分必须具有非高斯分布。
- en: The theoretical basis of ICA is the central limit theorem, which states that
    the distribution of the sum (average or linear combination) of ![](img/B19849_Formula_067.png)
    independent random variables approaches Gaussian for ![](img/B19849_Formula_068.png).
    In particular, if ![](img/B19849_Formula_069.png) are random variables independent
    of each other, taken from an arbitrary distribution with an average, ![](img/B19849_Formula_070.png),
    and a variance of ![](img/B19849_Formula_0711.png), then if we denote the mean
    of these variables as ![](img/B19849_Formula_072.png), we can say that ![](img/B19849_Formula_073.png)
    approaches the Gaussian with a mean of `0` and a variance of `1`. To solve the
    BSS problem, we need to find the matrix, ![](img/B19849_Formula_074.png), so that
    ![](img/B19849_Formula_075.png). Here, the ![](img/B19849_Formula_076.png) should
    be as close as possible to the original independent sources. We can consider this
    approach as the inverse process of the central limit theorem. All ICA methods
    are based on the same fundamental approach —finding a matrix, *W*, that maximizes
    non-Gaussianity, thereby minimizing the independence of ![](img/B19849_Formula_076.png).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 的理论基础是大数定律，该定律指出，![](img/B19849_Formula_067.png) 个独立随机变量的和（平均或线性组合）的分布对于
    ![](img/B19849_Formula_068.png) 趋近于高斯分布。特别是，如果 ![](img/B19849_Formula_069.png)
    是彼此独立的随机变量，来自具有平均 ![](img/B19849_Formula_070.png) 和方差 ![](img/B19849_Formula_0711.png)
    的任意分布，那么如果我们把这些变量的均值表示为 ![](img/B19849_Formula_072.png)，我们可以说 ![](img/B19849_Formula_073.png)
    趋近于均值为 `0` 和方差为 `1` 的高斯分布。为了解决 BSS 问题，我们需要找到矩阵 ![](img/B19849_Formula_074.png)，使得
    ![](img/B19849_Formula_075.png)。在这里，![](img/B19849_Formula_076.png) 应尽可能接近原始独立源。我们可以将这种方法视为大数定律的逆过程。所有
    ICA 方法都基于相同的基本方法——找到一个矩阵 *W*，该矩阵最大化非高斯性，从而最小化 ![](img/B19849_Formula_076.png)
    的独立性。
- en: 'The Fast ICA algorithm aims to maximize the function, ![](img/B19849_Formula_078.png),
    where ![](img/B19849_Formula_079.png) are components of ![](img/B19849_Formula_080.png).
    Therefore, we can rewrite the function’s equation in the following form:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 快速 ICA 算法旨在最大化函数 ![](img/B19849_Formula_078.png)，其中 ![](img/B19849_Formula_079.png)
    是 ![](img/B19849_Formula_080.png) 的组成部分。因此，我们可以将函数的方程重写为以下形式：
- en: '![](img/B19849_Formula_0811.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_0811.jpg)'
- en: Here, the ![](img/B19849_Formula_0821.png) vector is the *i*th row of the matrix,
    *W*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B19849_Formula_0821.png) 向量是矩阵 *W* 的第 *i* 行。
- en: 'The ICA algorithm performs the following steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ICA 算法执行以下步骤：
- en: It chooses the initial value of *w*.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它选择 *w* 的初始值。
- en: It calculates ![](img/B19849_Formula_083.png), where ![](img/B19849_Formula_084.png)
    is the derivative of the function, *G(z)*.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它计算 ![](img/B19849_Formula_083.png)，其中 ![](img/B19849_Formula_084.png) 是函数 *G(z)*
    的导数。
- en: It normalizes ![](img/B19849_Formula_085.png).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它标准化 ![](img/B19849_Formula_085.png)。
- en: It repeats the previous two steps until *w* stops changing.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它重复前两个步骤，直到 *w* 停止变化。
- en: 'To measure non-Gaussianity, Fast ICA relies on a nonquadratic non-linear function,
    *G (z)*, that can take the following forms:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量非高斯性，快速 ICA 依赖于一个非二次非线性函数 *G(z)*，它可以采取以下形式：
- en: '![](img/B19849_Formula_086.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_086.jpg)'
- en: Linear discriminant analysis
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: 'LDA is a type of multivariate analysis that allows us to estimate differences
    between two or more groups of objects at the same time. The basis of discriminant
    analysis is the assumption that the descriptions of the objects of each *k*th
    class are instances of a multidimensional random variable that’s distributed according
    to the normal (Gaussian) law, ![](img/B19849_Formula_087.png), with an average,
    ![](img/B19849_Formula_088.png), and the following covariance matrix:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 是一种多元分析方法，它允许我们同时估计两个或更多组对象之间的差异。判别分析的基础是假设每个 *k* 类对象的描述是服从正态（高斯）分布的多维随机变量的实例，其平均值为
    ![](img/B19849_Formula_088.png)，协方差矩阵如下：
- en: '![](img/B19849_Formula_089.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_089.jpg)'
- en: 'The index, ![](img/B19849_Formula_015.png), indicates the dimension of the
    feature space. Consider a simplified geometric interpretation of the LDA algorithm
    for the case of two classes. Let the discriminant variables, ![](img/B19849_Formula_0911.png),
    be the axes of the ![](img/B19849_Formula_092.png)-dimensional Euclidean space.
    Each object (sample) is a point of this space with coordinates representing the
    fixed values of each variable. If both classes differ from each other in observable
    variables (features), they can be represented as clusters of points in different
    regions of the considered space that may partially overlap. To determine the position
    of each class, we can calculate its **centroid**, which is an imaginary point
    whose coordinates are the average values of the variables (features) in the class.
    The task of discriminant analysis is to create an additional ![](img/B19849_Formula_093.png)
    axis that passes through a cloud of points in such a way that the projections
    on it provide the best separability into two classes (in other words, it maximizes
    the distance between classes). Its position is given by a **linear discriminant**
    (**LD**) function with weights, ![](img/B19849_Formula_094.png), that determine
    the contribution of each initial variable, ![](img/B19849_Formula_095.png):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 指数![图片](img/B19849_Formula_015.png)表示特征空间的维度。考虑LDA算法在两类情况下的简化几何解释。让判别变量![图片](img/B19849_Formula_0911.png)是![图片](img/B19849_Formula_092.png)维欧几里得空间的轴。每个对象（样本）是这个空间中的一个点，其坐标代表每个变量的固定值。如果两个类别在可观察变量（特征）上有所不同，它们可以表示为考虑空间中不同区域的不同点簇，这些区域可能部分重叠。为了确定每个类别的位置，我们可以计算其**质心**，这是一个想象中的点，其坐标是类别中变量（特征）的平均值。判别分析的任务是创建一个额外的![图片](img/B19849_Formula_093.png)轴，它通过点云，使得其投影提供最佳的类别分离（换句话说，它最大化了类别之间的距离）。其位置由一个**线性判别**（**LD**）函数给出，该函数具有权重![图片](img/B19849_Formula_094.png)，这些权重决定了每个初始变量![图片](img/B19849_Formula_095.png)的贡献：
- en: '![](img/B19849_Formula_096.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_096.jpg)'
- en: If we assume that the covariance matrices of the objects of classes 1 and 2
    are equal, that is, ![](img/B19849_Formula_097.png), then the vector of coefficients,
    ![](img/B19849_Formula_098.png), of the LD, ![](img/B19849_Formula_099.png), can
    be calculated using the formula ![](img/B19849_Formula_100.png), where ![](img/B19849_Formula_1011.png)
    is the inverse of the covariance matrix and ![](img/B19849_Formula_088.png) is
    the mean of the ![](img/B19849_Formula_103.png) class. The resulting axis coincides
    with the equation of a line passing through the centroids of two groups of class
    objects. The generalized Mahalanobis distance, which is equal to the distance
    between them in the multidimensional feature space, is estimated as ![](img/B19849_Formula_104.png).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设类别1和2的对象协方差矩阵相等，即![图片](img/B19849_Formula_097.png)，那么LD（线性判别）的系数向量![图片](img/B19849_Formula_098.png)可以使用公式![图片](img/B19849_Formula_100.png)计算，其中![图片](img/B19849_Formula_1011.png)是协方差矩阵的逆，![图片](img/B19849_Formula_088.png)是![图片](img/B19849_Formula_103.png)类别的均值。得到的轴与通过两组类别对象质心的直线方程相一致。广义马氏距离，等于它们在多维特征空间中的距离，估计为![图片](img/B19849_Formula_104.png)。
- en: 'Thus, in addition to the assumption regarding the normal (Gaussian) distribution
    of class data, which in practice occurs quite rarely, the LDA has a stronger assumption
    about the statistical equality of intragroup dispersions and correlation matrices.
    If there are no significant differences between them, they are combined into a
    calculated covariance matrix, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了关于类别数据正态（高斯）分布的假设外，这在实践中相当罕见，LDA对组内散布和协方差矩阵的统计相等性有更强的假设。如果没有显著差异，它们将被组合成一个计算出的协方差矩阵，如下所示：
- en: '![](img/B19849_Formula_105.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_105.jpg)'
- en: 'This principle can be generalized to a larger number of classes. The final
    algorithm may look like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个原理可以推广到更多的类别。最终的算法可能看起来像这样：
- en: '![](img/B19849_Formula_106.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_106.jpg)'
- en: 'The interclass scattering matrix is calculated like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 类间散布矩阵的计算方式如下：
- en: '![](img/B19849_Formula_107.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19849_Formula_107.jpg)'
- en: Here, ![](img/B19849_Formula_108.png) is the mean of all objects (samples),
    ![](img/B19849_Formula_029.png) is the number of classes, ![](img/B19849_Formula_110.png)
    is the number of objects in the *i*th class, ![](img/B19849_Formula_1111.png)
    is the intraclass’ mean, ![](img/B19849_Formula_1121.png) is the scattering matrix
    for the *i*th class, and ![](img/B19849_Formula_1131.png) is a centering matrix,
    where ![](img/B19849_Formula_114.png) is the *n* x *n* matrix of all 1s.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![公式图片](img/B19849_Formula_108.png)是所有对象（样本）的平均值，![公式图片](img/B19849_Formula_029.png)是类别的数量，![公式图片](img/B19849_Formula_110.png)是第*i*类中的对象数量，![公式图片](img/B19849_Formula_1111.png)是类内均值，![公式图片](img/B19849_Formula_1121.png)是第*i*类的散射矩阵，![公式图片](img/B19849_Formula_1131.png)是一个中心化矩阵，其中![公式图片](img/B19849_Formula_114.png)是所有1的n
    x n矩阵。
- en: Based on these matrices, the ![](img/B19849_Formula_115.png) matrix is calculated,
    for which the eigenvalues and the corresponding eigenvectors are determined. In
    the diagonal elements of the matrix, we must select the *s* of the largest eigenvalues
    and transform the matrix, leaving only the corresponding *s* rows in it. The resulting
    matrix can be used to convert all objects into lower-dimensional space.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些矩阵，计算得到![公式图片](img/B19849_Formula_115.png)矩阵，其中确定了特征值和相应的特征向量。在矩阵的对角元素中，我们必须选择最大特征值的*s*，并转换矩阵，只留下对应的*s*行。得到的矩阵可以用来将所有对象转换为低维空间。
- en: This method requires labeled data, meaning it is a supervised method.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法需要标记数据，这意味着它是一种监督方法。
- en: Factor analysis
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因子分析
- en: '**Factor analysis** is used to reduce the number of variables that are used
    to describe data and determine the relationships between them. During the analysis,
    variables that correlate with each other are combined into one factor. As a result,
    the dispersion between components is redistributed, and the structure of factors
    becomes more understandable. After combining the variables, the correlation of
    components within each factor becomes higher than their correlation with components
    from other factors. It is assumed that known variables depend on a smaller number
    of unknown variables and that we have a random error that can be expressed as
    follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**因子分析**用于减少描述数据所使用的变量数量，并确定它们之间的关系。在分析过程中，相互关联的变量被组合成一个因子。结果，组件之间的分散度被重新分配，因子的结构变得更加可理解。在组合变量之后，每个因子内组件之间的相关性高于它们与其他因子组件的相关性。假设已知变量依赖于更少的未知变量，并且我们有一个可以表示为以下形式的随机误差：'
- en: '![](img/B19849_Formula_116.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![公式图片](img/B19849_Formula_116.jpg)'
- en: Here, ![](img/B19849_Formula_117.png) is the load and ![](img/B19849_Formula_118.png)
    is the factor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![公式图片](img/B19849_Formula_117.png)是负荷，![公式图片](img/B19849_Formula_118.png)是因子。
- en: The concept of **factor load** is essential. It is used to describe the role
    of the factor (variable) when we wish to form a specific vector from a new basis.
    The essence of factor analysis is the procedure of rotating factors, that is,
    redistributing the dispersion according to a specific method. The purpose of rotations
    is to define a simple structure of factor loadings. Rotation can be orthogonal
    and oblique. In the first form of rotation, each successive factor is determined
    to maximize the variability that remains from the previous factors. Therefore,
    the factors are independent and uncorrelated with each other. The second type
    is a transformation in which factors correlate with each other. There are about
    13 methods of rotation that are used in both forms. The factors that have a similar
    effect on the elements of the new basis are combined into one group. Then, from
    each group, it is recommended to leave one representative. Some algorithms, instead
    of choosing a representative, calculate a new factor with some heuristics that
    become central to the group.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**因子负荷**的概念至关重要。它用于描述当我们希望从一个新的基中形成特定向量时，因子（变量）所起的作用。因子分析的本质是旋转因子的过程，即根据特定方法重新分配分散度。旋转的目的是定义因子负荷的简单结构。旋转可以是正交的，也可以是斜交的。在旋转的第一种形式中，每个后续因子被确定以最大化从前一个因子中剩余的变异性。因此，因子是相互独立且不相关的。第二种类型是一种因子之间相互关联的转换。大约有13种旋转方法在两种形式中都被使用。对新的基的元素有相似影响作用的因子被组合成一个组。然后，从每个组中，建议留下一个代表。一些算法，而不是选择一个代表，通过一些成为该组核心的启发式方法计算一个新的因子。'
- en: 'Dimensionality reduction occurs while transitioning to a system of factors
    that are representatives of groups, and the other factors are discarded. There
    are several commonly used criteria for determining the number of factors. Some
    of these criteria can be used together to complement each other. An example of
    a criterion that’s used to determine the number of factors is the Kaiser criterion
    or the eigenvalue criterion: only factors with eigenvalues equal to or greater
    than *one* are selected. This means that if a factor does not select a variance
    equivalent to at least one variance of one variable, then it is omitted. The general
    factor analysis algorithm follows these steps:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在过渡到代表群体的因素系统时发生降维，其他因素被丢弃。有几个常用的标准用于确定因素的数量。其中一些标准可以一起使用以相互补充。用于确定因素数量的标准之一是凯撒准则或特征值准则：只选择特征值等于或大于
    *一* 的因素。这意味着如果一个因素没有选择至少与一个变量的方差等效的方差，则它被省略。一般的因子分析算法遵循以下步骤：
- en: It calculates the correlation matrix.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它计算相关矩阵。
- en: It selects the number of factors for inclusion, for example, with the Kaiser
    criterion.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它选择要包含的因素数量，例如，使用凯撒准则。
- en: It extracts the initial set of factors. There are several different extraction
    methods, including maximum likelihood, PCA, and principal axis extraction.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提取初始因素集。有几种不同的提取方法，包括最大似然法、主成分分析（PCA）和主轴提取。
- en: It rotates the factors to a final solution that is equal to the one that was
    obtained in the initial extraction but that has the most straightforward interpretation.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将因素旋转到最终解决方案，该解决方案等于在初始提取中获得的解决方案，但具有最直接的解释。
- en: Multidimensional scaling
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多维尺度分析
- en: MDS can be considered as an alternative to factor analysis when, in addition
    to the correlation matrices, an arbitrary type of object similarity matrix can
    be used as input data. MDS is not so much a formal mathematical procedure but
    rather a method of efficiently placing objects, thus keeping an appropriate distance
    between them in a new feature space. The dimension of the new space in MDS is
    always substantially less than the original space. The data that’s used for analysis
    by MDS is often obtained from the matrix of pairwise comparisons of objects.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当除了相关矩阵外，还可以使用任意类型的对象相似性矩阵作为输入数据时，MDS可以被视为因子分析的一种替代方法。MDS不仅仅是一个正式的数学过程，而是一种高效放置对象的方法，从而在新的特征空间中保持它们之间适当的距离。MDS中新的空间维度总是显著小于原始空间。用于MDS分析的数据通常来自对象成对比较矩阵。
- en: The main MDS algorithm’s goal is to restore the unknown dimension, ![](img/B19849_Formula_119.png),
    of the analyzed feature space and assign coordinates to each object in such a
    way that the calculated pairwise Euclidean distances between the objects coincide
    as much as possible with the specified pairwise comparison matrix. We are talking
    about restoring the coordinates of the new reduced feature space with the accuracy
    of orthogonal transformation, ensuring the pairwise distances between the objects
    do not change.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的MDS算法目标是恢复分析特征空间的未知维度，![](img/B19849_Formula_119.png)，并按这种方式为每个对象分配坐标，即尽可能使对象之间的计算成对欧几里得距离与指定的成对比较矩阵相吻合。我们谈论的是以正交变换的精度恢复新的降低特征空间的坐标，确保对象之间的成对距离不发生变化。
- en: Thus, the aim of MDS methods can also be formulated in order to display the
    configuration information of the original multidimensional data that’s given by
    the pairwise comparison matrix. This is provided as a configuration of points
    in the corresponding space of lower dimension.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MDS方法的目标也可以表述为显示由成对比较矩阵给出的原始多维数据的配置信息。这以低维空间中点的配置形式提供。
- en: 'Classical MDS assumes that the unknown coordinate matrix, ![](img/B19849_Formula_009.png),
    can be expressed by eigenvalue decomposition, ![](img/B19849_Formula_1211.png).
    ![](img/B19849_Formula_1221.png) can be computed from the proximity matrix ![](img/B19849_Formula_1231.png)
    (a matrix with distances between samples) by using double centering. The general
    MDS algorithm follows these steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 经典MDS假设未知的坐标矩阵，![](img/B19849_Formula_009.png)，可以通过特征值分解，![](img/B19849_Formula_1211.png)，来表示。![](img/B19849_Formula_1221.png)可以通过使用双中心化从邻近矩阵
    ![](img/B19849_Formula_1231.png)（一个包含样本之间距离的矩阵）计算得出。一般的MDS算法遵循以下步骤：
- en: It computes the squared proximity matrix, ![](img/B19849_Formula_1241.png).
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它计算平方邻近矩阵，![](img/B19849_Formula_1241.png)。
- en: It applies double centering, ![](img/B19849_Formula_125.png), using the centering
    matrix, ![](img/B19849_Formula_126.png), where ![](img/B19849_Formula_026.png)
    is the number of objects.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它应用双重中心化，![](img/B19849_Formula_125.png)，使用中心化矩阵，![](img/B19849_Formula_126.png)，其中![](img/B19849_Formula_026.png)是对象的数量。
- en: It determines the ![](img/B19849_Formula_128.png) largest eigenvalues, ![](img/B19849_Formula_129.png),
    and the corresponding eigenvectors, ![](img/B19849_Formula_130.png), of ![](img/B19849_Formula_1311.png)
    (where ![](img/B19849_Formula_1321.png) is the number of dimensions desired for
    the output).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它确定![](img/B19849_Formula_128.png)最大的特征值![](img/B19849_Formula_129.png)和相应的特征向量![](img/B19849_Formula_130.png)，这些特征向量属于![](img/B19849_Formula_1311.png)（其中![](img/B19849_Formula_1321.png)是所需的输出维度数量）。
- en: It computes ![](img/B19849_Formula_133.png), where ![](img/B19849_Formula_134.png)
    is the matrix of ![](img/B19849_Formula_135.png) eigenvectors and ![](img/B19849_Formula_136.png)
    is the diagonal matrix of ![](img/B19849_Formula_137.png) eigenvalues of ![](img/B19849_Formula_138.png).
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它计算![](img/B19849_Formula_133.png)，其中![](img/B19849_Formula_134.png)是![](img/B19849_Formula_135.png)的特征向量矩阵，![](img/B19849_Formula_136.png)是![](img/B19849_Formula_138.png)的特征值对角矩阵。
- en: The disadvantage of the MDS method is that it does not take into account the
    distribution of nearby points since it uses Euclidean distances in calculations.
    If you ever find multidimensional data lying on a curved manifold, the distance
    between data points can be much more than Euclidean.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: MDS方法的缺点是它在计算中使用了欧几里得距离，因此没有考虑到邻近点的分布。如果你发现多维数据位于一个弯曲的流形上，数据点之间的距离可能比欧几里得距离大得多。
- en: Now that we’ve discussed the linear methods we can use for dimension-reduction,
    let’s look at what non-linear methods exist.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了可用于降维的线性方法，让我们来看看存在哪些非线性方法。
- en: Exploring non-linear methods for dimension-reduction
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索降维的非线性方法
- en: 'In this section, we’ll discuss the widespread non-linear methods and algorithms
    that are used for dimension-reduction, such as the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论广泛使用的非线性方法和算法，这些方法和算法用于降维，例如以下内容：
- en: Kernel PCA
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核PCA
- en: Isomap
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isomap
- en: Sammon mapping
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sammon映射
- en: Distributed **stochastic neighbor** **embedding** (**SNE**)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式**随机邻域嵌入**（**SNE**）
- en: Autoencoders
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器
- en: Kernel PCA
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核PCA
- en: Classic PCA is a linear projection method that works well if the data is linearly
    separable. However, in the case of linearly non-separable data, a non-linear approach
    is required. The basic idea of working with linearly inseparable data is to project
    it into a space with a larger number of dimensions, where it becomes linearly
    separable. We can choose a non-linear mapping function, ![](img/B19849_Formula_139.png),
    so that the sample mapping, *x*, can be written as ![](img/B19849_Formula_140.png).
    This is called the **kernel function**. The term *kernel* describes a function
    that calculates the scalar product of mapping (in a higher-order space) samples
    *x* with ![](img/B19849_Formula_1411.png). This scalar product can be interpreted
    as the distance measured in the new space. In other words, the ![](img/B19849_Formula_1421.png)
    function maps the original *d*-dimensional elements into the *k*-dimensional feature
    space of a higher dimension by creating non-linear combinations of the original
    objects. For example, a function that displays 2D samples, ![](img/B19849_Formula_1431.png),
    in 3D space can look like ![](img/B19849_Formula_144.png).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的主成分分析（PCA）是一种线性投影方法，当数据线性可分时效果良好。然而，在数据线性不可分的情况下，需要采用非线性方法。处理线性不可分数据的基本思想是将数据投影到一个具有更多维度的空间中，使其变得线性可分。我们可以选择一个非线性映射函数，![](img/B19849_Formula_139.png)，使得样本映射，*x*，可以表示为![](img/B19849_Formula_140.png)。这被称为**核函数**。术语*核*描述了一个函数，它计算映射（在更高阶空间）样本*x*与![](img/B19849_Formula_1411.png)的标量积。这个标量积可以解释为新空间中测量的距离。换句话说，![](img/B19849_Formula_1421.png)函数通过创建原始对象的非线性组合，将原始的*d*-维元素映射到更高维度的*k*-维特征空间。例如，一个在3D空间中显示2D样本，![](img/B19849_Formula_1431.png)的函数可能看起来像![](img/B19849_Formula_144.png)。
- en: 'In a linear PCA approach, we are interested in the principal components that
    maximize the variance in the dataset. We can maximize variance by calculating
    the eigenvectors (principal components) that correspond to the largest eigenvalues
    based on the covariance matrix of our data and project our data onto these eigenvectors.
    This approach can be generalized to data that is mapped into a higher dimension
    space using the kernel function. However, in practice, the covariance matrix in
    a multidimensional space is not explicitly calculated since we can use a method
    called the **kernel trick**. The kernel trick allows us to project data onto the
    principal components without explicitly calculating the projections, which is
    much more efficient. The general approach is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性PCA方法中，我们关注的是最大化数据集方差的特征成分。我们可以通过根据数据协方差矩阵计算与最大特征值对应的特征向量（主成分）来最大化方差，并将数据投影到这些特征向量上。这种方法可以推广到使用核函数映射到更高维空间的数据。然而，在实践中，多维空间中的协方差矩阵并不是显式计算的，因为我们可以使用一种称为**核技巧**的方法。核技巧允许我们将数据投影到主成分上，而无需显式计算投影，这要高效得多。一般方法如下：
- en: Compute the **kernel matrix** equal to ![](img/B19849_Formula_145.png).
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算等于 ![img/B19849_Formula_145.png](img/B19849_Formula_145.png) 的核矩阵。
- en: Make it so that it has a zero mean value, ![](img/B19849_Formula_146.png), where
    ![](img/B19849_Formula_147.png) is a matrix of *N x N* size with *1/N* elements.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使其具有零均值值 ![img/B19849_Formula_146.png](img/B19849_Formula_146.png)，其中 ![img/B19849_Formula_147.png](img/B19849_Formula_147.png)
    是一个大小为 *N x N* 的矩阵，其元素为 *1/N*。
- en: Calculate the eigenvalues and eigenvectors of ![](img/B19849_Formula_148.png).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算特征值和特征向量 ![img/B19849_Formula_148.png](img/B19849_Formula_148.png)。
- en: Sort the eigenvectors in descending order, according to their eigenvalues.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照特征值降序排列特征向量。
- en: Take ![](img/B19849_Formula_149.png) eigenvectors that correspond to the largest
    eigenvalues, where ![](img/B19849_Formula_150.png) is the number of dimensions
    of a new feature space.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与最大特征值对应的 ![img/B19849_Formula_149.png](img/B19849_Formula_149.png) 个特征向量，其中
    ![img/B19849_Formula_150.png](img/B19849_Formula_150.png) 是新特征空间的维度数。
- en: These eigenvectors are projections of our data onto the corresponding main components.
    The main difficulty of this process is selecting the correct kernel and configuring
    its hyperparameters. Two frequently used kernels are the polynomial kernel ![](img/B19849_Formula_1511.png)and
    the Gaussian (Radial Basis Function (RBF)) ![](img/B19849_Formula_1521.png) ones.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征向量是我们数据在对应主成分上的投影。这个过程的主要困难在于选择正确的核函数以及配置其超参数。两种常用的核函数是多项式核 ![img/B19849_Formula_1511.png](img/B19849_Formula_1511.png)
    和高斯（径向基函数（RBF））核 ![img/B19849_Formula_1521.png](img/B19849_Formula_1521.png)。
- en: Isomap
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Isomap
- en: The **Isomap** algorithm is based on the manifold projection technique. In mathematics,
    the **manifold** is a topological space (which is, in general, a set of points
    with their neighbors) that locally resembles the Euclidian space near each point.
    For example, one-dimensional manifolds include lines and circles but not figures
    with self-intersections. 2D manifolds are called **surfaces**; for example, they
    can be a sphere, a plane, or a torus, but these surfaces can’t have self-intersection.
    For example, a circle is a one-dimensional manifold embedded into a 2D space.
    Here, each arc of the circle locally resembles a straight-line segment. A 3D curve
    can also be a manifold if it can be divided into straight-line segments that can
    be embedded in 3D space without self-intersections. A 3D shape can be a manifold
    if its surface can be divided into flat plane patches without self-intersections.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**Isomap** 算法基于流形投影技术。在数学中，**流形**是一个拓扑空间（通常是一组点及其邻居），在每一点附近局部类似于欧几里得空间。例如，一维流形包括直线和圆，但不包括有自交的图形。二维流形被称为**曲面**；例如，它们可以是球体、平面或环面，但这些曲面不能有自交。例如，圆是一个嵌入到二维空间中的一维流形。在这里，圆的每一段弧在局部上类似于直线段。如果一条三维曲线可以被分成可以嵌入到三维空间中且没有自交的直线段，那么它也可以是一个流形。三维形状如果其表面可以被分成没有自交的平面片，那么它也可以是一个流形。'
- en: The basics of applying manifold projection techniques are to search for a manifold
    that is close to the data, project the data onto the manifold, and then unfold
    it. The most popular technique that’s used to find the manifold is to build a
    graph based on information about data points. Usually, these data points are placed
    into the graph nodes, and the edges simulate the relationships between the data
    points.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 应用流形投影技术的基本方法是寻找一个接近数据的流形，将数据投影到流形上，然后展开它。用于寻找流形的最流行技术是基于数据点信息构建一个图。通常，这些数据点被放置在图节点中，边模拟数据点之间的关系。
- en: 'The Isomap algorithm depends on two parameters:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Isomap算法依赖于两个参数：
- en: The number of neighbors, ![](img/B19849_Formula_153.png), used to search for
    geodetic distances
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于搜索测地距离的邻居数量，![](img/B19849_Formula_153.png)
- en: The dimension of the final space, ![](img/B19849_Formula_1321.png)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终空间的维度，![](img/B19849_Formula_1321.png)
- en: 'In brief, the Isomap algorithm follows these steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Isomap算法遵循以下步骤：
- en: First, it constructs a graph representing geodesic distances. For each point,
    we search the ![](img/B19849_Formula_155.png) nearest neighbors and construct
    a weighted, undirected graph from the distances to these nearest neighbors. The
    edge weight is the Euclidean distance to the neighbor.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它构建一个表示测地距离的图。对于每个点，我们搜索![](img/B19849_Formula_155.png)个最近邻，并从这些最近邻的距离构建一个加权无向图。边权重是到邻居的欧几里得距离。
- en: Using an algorithm to find the shortest distance in the graph, for example,
    Dijkstra’s algorithm, we need to find the shortest distance between each pair
    of vertices. We can consider this distance as a geodesic distance on a manifold.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用寻找图中最短距离的算法，例如Dijkstra算法，我们需要找到每对顶点之间的最短距离。我们可以将这个距离视为流形上的测地距离。
- en: Based on the matrix of pairwise geodesic distances we obtained in the previous
    step, train the MDS algorithm.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于我们在上一步中获得的成对测地距离矩阵，训练MDS算法。
- en: The MDS algorithm associates a set of points in the ![](img/B19849_Formula_027.png)-dimensional
    space with the initial set of distances.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MDS算法将![](img/B19849_Formula_027.png)维空间中的一组点与初始距离集相关联。
- en: Sammon mapping
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sammon映射
- en: '**Sammon mapping** is one of the first non-linear dimensionality reduction
    algorithms. In contrast to traditional dimensionality reduction methods, such
    as PCA, Sammon mapping does not define a data conversion function directly. On
    the contrary, it only determines the measure of how well the conversion results
    (a specific dataset of a smaller dimension) correspond to the structure of the
    original dataset. In other words, it does not try to find the optimal transformation
    of the original data; instead, it searches for another dataset of lower dimensions
    with a structure that’s as close to the original one as possible. The algorithm
    can be described as follows. Let’s say we have ![](img/B19849_Formula_157.png)-dimensional
    vectors, ![](img/B19849_Formula_158.png). Here, ![](img/B19849_Formula_026.png)
    vectors are defined in the ![](img/B19849_Formula_160.png)-dimensional space,
    ![](img/B19849_Formula_1611.png), which is denoted by ![](img/B19849_Formula_1621.png).
    The distances between the vectors in the ![](img/B19849_Formula_163.png)-dimensional
    space will be denoted by ![](img/B19849_Formula_164.png) and in the ![](img/B19849_Formula_160.png)-dimensional
    space, ![](img/B19849_Formula_166.png). To determine the distance between the
    vectors, we can use any metric; in particular, the Euclidean distance. The goal
    of non-linear Sammon mapping is to search a selection of vectors, ![](img/B19849_Formula_19.png),
    in order to minimize the error function, ![](img/B19849_Formula_168.png), which
    is defined by the following formula:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sammon映射**是第一个非线性降维算法之一。与传统降维方法，如PCA不同，Sammon映射并不直接定义数据转换函数。相反，它只确定转换结果（一个较小维度的特定数据集）与原始数据集结构对应的好坏程度。换句话说，它并不试图找到原始数据的最佳转换；相反，它寻找另一个低维数据集，其结构尽可能接近原始数据集。该算法可以描述如下。假设我们有一个![](img/B19849_Formula_157.png)维向量，![](img/B19849_Formula_158.png)。在这里，![](img/B19849_Formula_026.png)向量定义在![](img/B19849_Formula_160.png)维空间，![](img/B19849_Formula_1611.png)，用![](img/B19849_Formula_1621.png)表示。在![](img/B19849_Formula_163.png)维空间中向量的距离将用![](img/B19849_Formula_164.png)表示，在![](img/B19849_Formula_160.png)维空间中，用![](img/B19849_Formula_166.png)表示。为了确定向量之间的距离，我们可以使用任何度量；特别是欧几里得距离。非线性Sammon映射的目标是在选择一组向量![](img/B19849_Formula_19.png)中搜索，以最小化误差函数![](img/B19849_Formula_168.png)，该函数由以下公式定义：'
- en: '![](img/B19849_Formula_169.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_169.jpg)'
- en: 'To minimize the error function, ![](img/B19849_Formula_168.png), Sammon used
    Newton’s minimization method, which can be simplified as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化误差函数![](img/B19849_Formula_168.png)，Sammon使用了牛顿最小化方法，可以简化如下：
- en: '![](img/B19849_Formula_171.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_171.jpg)'
- en: Here, *η* is the learning rate.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*η*是学习率。
- en: Distributed stochastic neighbor embedding
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式随机近邻嵌入
- en: 'The SNE problem is formulated as follows: we have a dataset with points described
    by a multidimensional variable with a dimension of space substantially higher
    than three. It is necessary to obtain a new variable that exists in a 2D or 3D
    space that would maximally preserve the structure and patterns in the original
    data. The difference between t-SNE and the classic SNE lies in the modifications
    that simplify the process of finding the global minima. The main modification
    is replacing the normal distribution with the Student’s t-distribution for low-dimensional
    data. SNE begins by converting the multidimensional Euclidean distance between
    points into conditional probabilities that reflect the similarity of points. Mathematically,
    it looks like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: SNE问题被表述如下：我们有一个数据集，其中的点由一个多维变量描述，其空间维度远高于三个。有必要获得一个存在于2D或3D空间的新变量，该变量将最大限度地保留原始数据中的结构和模式。t-SNE与经典SNE之间的区别在于简化寻找全局最小值过程的修改。主要的修改是用Student的t分布替换低维数据的正态分布。SNE开始时将点之间的多维欧几里得距离转换为反映点相似性的条件概率。从数学上看，它看起来是这样的：
- en: '![](img/B19849_Formula_172.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_172.jpg)'
- en: This formula shows how close the point ![](img/B19849_Formula_173.png) lies
    to the point ![](img/B19849_Formula_312.png) with a Gaussian distribution around
    ![](img/B19849_Formula_312.png), with a given deviation of ![](img/B19849_Formula_176.png).
    ![](img/B19849_Formula_176.png) is different for each point. It is chosen so that
    the points in areas with higher density have less variance than others.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式显示了点 ![](img/B19849_Formula_173.png) 在以 ![](img/B19849_Formula_312.png)
    为中心的正态分布中相对于点 ![](img/B19849_Formula_312.png) 的距离，给定偏差为 ![](img/B19849_Formula_176.png)。![](img/B19849_Formula_176.png)
    对于每个点都是不同的。它是这样选择的，使得密度较高的区域中的点比其他区域的点具有更小的方差。
- en: 'Let’s denote 2D or 3D mappings of the (![](img/B19849_Formula_312.png), ![](img/B19849_Formula_173.png))
    pair as the (![](img/B19849_Formula_111.png), ![](img/B19849_Formula_1811.png))
    pair. It is necessary to estimate the conditional probability using the same formula.
    The standard deviation is ![](img/B19849_Formula_1821.png):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们将 (![](img/B19849_Formula_312.png), ![](img/B19849_Formula_173.png)) 对的二维或三维映射表示为
    (![](img/B19849_Formula_111.png), ![](img/B19849_Formula_1811.png)) 对。使用相同的公式估计条件概率是必要的。标准差是
    ![](img/B19849_Formula_1821.png):'
- en: '![](img/B19849_Formula_183.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_183.jpg)'
- en: 'If the mapping points, ![](img/B19849_Formula_111.png) and ![](img/B19849_Formula_1811.png),
    correctly simulate the similarity between the original points of the higher dimension,
    ![](img/B19849_Formula_312.png) and ![](img/B19849_Formula_187.png), then the
    corresponding conditional probabilities, ![](img/B19849_Formula_188.png) and ![](img/B19849_Formula_189.png),
    will be equivalent. As an obvious assessment of the quality of how ![](img/B19849_Formula_189.png)
    reflects ![](img/B19849_Formula_188.png), divergence, or the Kullback-Leibler
    distance is used. SNE minimizes the sum of such distances for all mapping points
    using gradient descent. The following formula determines the loss function for
    this method:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果映射点 ![](img/B19849_Formula_111.png) 和 ![](img/B19849_Formula_1811.png) 正确模拟了高维原始点
    ![](img/B19849_Formula_312.png) 和 ![](img/B19849_Formula_187.png) 之间的相似性，那么相应的条件概率
    ![](img/B19849_Formula_188.png) 和 ![](img/B19849_Formula_189.png) 将是等价的。作为一个对
    ![](img/B19849_Formula_189.png) 如何反映 ![](img/B19849_Formula_188.png)、发散或 Kullback-Leibler
    距离的明显评估，使用梯度下降法最小化所有映射点之间的这种距离之和。以下公式确定了该方法的损失函数：
- en: '![](img/B19849_Formula_192.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_192.jpg)'
- en: 'It has the following gradient:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有以下梯度：
- en: '![](img/B19849_Formula_193.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_193.jpg)'
- en: 'The authors of this problem proposed the following physical analogy for the
    optimization process. Let’s imagine that springs connect all the mapping points.
    The stiffness of the spring connecting points ![](img/B19849_Formula_194.png)
    and ![](img/B19849_Formula_195.png) depends on the difference between the similarity
    of two points in a multidimensional space and two points in a mapping space. In
    this analogy, the gradient is the resultant force that acts on a point in the
    mapping space. If we let the system go, after some time, it results in balance,
    and this is the desired distribution. Algorithmically, it searches for balance
    while taking the following moments into account:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题的作者提出了以下物理类比来描述优化过程。让我们想象所有映射点之间都连接着弹簧。连接点 ![](img/B19849_Formula_194.png)
    和 ![](img/B19849_Formula_195.png) 的弹簧的刚度取决于多维空间中两点相似性与映射空间中两点相似性之间的差异。在这个类比中，梯度是作用在映射空间中某点的合力。如果我们让系统自由发展，经过一段时间后，它会达到平衡，这就是我们想要的分布。算法上，它会在考虑以下因素的同时寻找平衡：
- en: '![](img/B19849_Formula_196.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_196.jpg)'
- en: Here, ![](img/B19849_Formula_197.png) is the learning rate and ![](img/B19849_Formula_198.png)
    is the coefficient of inertia. Classic SNE also allows us to get good results
    but can be associated with difficulties when optimizing the loss function and
    the crowding problem. t-SNE doesn’t solve these problems in general, but it makes
    them much more manageable.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B19849_Formula_197.png) 是学习率，![](img/B19849_Formula_198.png) 是惯性系数。经典
    SNE 也允许我们获得良好的结果，但可能会在优化损失函数和拥挤问题时遇到困难。t-SNE 并没有解决这些问题，但使它们更容易管理。
- en: The loss function in t-SNE has two principal differences from the loss function
    of classic SNE. The first one is that it has a symmetric form of similarity in
    a multidimensional space and a simpler gradient version. Secondly, instead of
    using a Gaussian distribution for points from the mapping space, the t-distribution
    (Student) is used.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 中的损失函数与经典 SNE 的损失函数有两个主要区别。第一个区别是它在多维空间中具有对称的相似性形式和更简单的梯度版本。其次，对于映射空间中的点，不是使用高斯分布，而是使用
    t 分布（学生分布）。
- en: Autoencoders
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器
- en: '**Autoencoders** represent a particular class of neural networks that are configured
    so that the output of the autoencoder is as close as possible to the input signal.
    In its most straightforward representation, the autoencoder can be modeled as
    a multilayer perceptron in which the number of neurons in the output layer is
    equal to the number of inputs. The following diagram shows that by choosing an
    intermediate hidden layer of a smaller dimension, we compress the source data
    into the lower dimension. Usually, values from this intermediate layer are a result
    of an autoencoder:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**代表一类特定的神经网络，其配置使得自编码器的输出尽可能接近输入信号。在其最直接的表现形式中，自编码器可以被建模为一个多层感知器，其中输出层的神经元数量等于输入的数量。以下图表显示，通过选择一个较小维度的中间隐藏层，我们可以将源数据压缩到较低维度。通常，这个中间层的值是自编码器的一个结果：'
- en: '![Figure 6.1 – Autoencoder architecture](img/B19849_06_01.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 自编码器架构](img/B19849_06_01.jpg)'
- en: Figure 6.1 – Autoencoder architecture
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 自编码器架构
- en: Now that we have learned about the linear and non-linear methods that can be
    used for dimension-reduction and explored the components of each of the methods
    in detail, we can enhance our implementation of dimension-reduction with the help
    of some practical examples.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了可用于降维的线性和非线性方法，并详细探讨了每种方法的组成部分，我们可以借助一些实际示例来增强我们的降维实现。
- en: Understanding dimension-reduction algorithms with various С++ libraries
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用各种 C++ 库理解降维算法
- en: 'Let’s look at how to use dimensionality reduction algorithms in practice. All
    of these examples use the same dataset, which contains four normally distributed
    2D point sets that have been transformed with Swiss roll mapping, ![](img/B19849_Formula_199.png),
    into a 3D space. You can find the dataset and related details in the book''s GitHub
    repository here: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition).
    The following graph shows the result of this mapping.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在实践中使用降维算法。所有这些示例都使用相同的数据集，该数据集包含四个经过瑞士卷映射（![](img/B19849_Formula_199.png)）转换的通常分布的二维点集，进入三维空间。您可以在本书的
    GitHub 仓库中找到数据集和相关细节：[https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition)。以下图表显示了这种映射的结果。
- en: '![Figure 6.2 – Swiss roll dataset](img/B19849_06_02.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 瑞士卷数据集](img/B19849_06_02.jpg)'
- en: Figure 6.2 – Swiss roll dataset
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 瑞士卷数据集
- en: This dataset is labeled. Each of the normally distributed parts has its own
    labels, and we can see these labels as a certain color on the result. We use these
    colors to show transformation results for each of the algorithms we’ll be using
    in the following samples. This gives us an idea of how the algorithm works. The
    following sections provide concrete examples of how to use the `Dlib`, `Tapkee`,
    and libraries.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集已标记。每个通常分布的部分都有自己的标签，我们可以在结果中看到这些标签以某种颜色表示。我们使用这些颜色来显示以下示例中我们将使用的每个算法的转换结果。这让我们对算法的工作原理有了概念。以下部分提供了如何使用
    `Dlib`、`Tapkee` 和其他库的具体示例。
- en: Using the Dlib library
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Dlib 库
- en: There are three dimensionality reduction methods in the `Dlib` library—two linear
    ones, known as PCA and LDA, and one non-linear one, known as Sammon mapping.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib` 库中有三种降维方法——两种线性方法，称为 PCA 和 LDA，以及一种非线性方法，称为 Sammon 映射。'
- en: PCA
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA
- en: 'PCA is one of the most popular dimensionality reduction algorithms and it has
    a couple of implementations in the `Dlib` library. There is the `Dlib::vector_normalizer_pca`
    type, for which objects can be used to perform PCA on user data. This implementation
    also normalizes the data. In some cases, this automatic normalization is useful
    because we always have to perform PCA on normalized data. An object of this type
    should be parameterized with the input data sample type. After we’ve instantiated
    an object of this type, we use the `train()` method to fit the model to our data.
    The `train()` method takes `std::vector` as samples and the `eps` value as parameters.
    The `eps` value controls how many dimensions should be preserved after the PCA
    has been transformed. This can be seen in the following code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是最受欢迎的降维算法之一，在`Dlib`库中有几个实现。有`Dlib::vector_normalizer_pca`类型，可以用于对用户数据进行PCA操作。这种实现也进行了数据归一化。在某些情况下，这种自动归一化是有用的，因为我们总是必须对归一化的数据进行PCA。这种类型的对象应该用输入数据样本类型进行参数化。在我们实例化这种类型的对象之后，我们使用`train()`方法将模型拟合到我们的数据。`train()`方法接受`std::vector`作为样本，以及`eps`值作为参数。`eps`值控制PCA变换后应该保留多少维度。这可以在以下代码中看到：
- en: '[PRE0]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After the algorithm has been trained, we use the object to transform individual
    samples. Take a look at the first loop in the code and notice how the `pca([data[i]])`
    call performs this transformation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法训练完成后，我们使用该对象来转换单个样本。看看代码中的第一个循环，注意`pca([data[i]])`调用是如何执行这种转换的。
- en: 'The following graph shows the result of the PCA transformation:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了PCA变换的结果：
- en: '![Figure 6.3 – Dlib PCA transformation visualization](img/B19849_06_03.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – Dlib PCA变换可视化](img/B19849_06_03.jpg)'
- en: Figure 6.3 – Dlib PCA transformation visualization
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – Dlib PCA变换可视化
- en: Data compression with PCA
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PCA数据压缩
- en: We can use dimensionality reduction algorithms for a slightly different task—data
    compression with information loss. This can be easily demonstrated when applying
    the PCA algorithm to images. Let’s implement PCA from scratch with the `Dlib`
    library using SVD decomposition. We can’t use an existing implementation because
    it performs normalization in a way we can’t fully control.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用降维算法来完成一个稍微不同的任务——具有信息损失的数据压缩。当将PCA算法应用于图像时，这可以很容易地演示。让我们使用SVD分解从头实现PCA，使用`Dlib`库。我们不能使用现有的实现，因为它以我们无法完全控制的方式进行归一化。
- en: 'First, we need to load an image and transform it into matrix form:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要加载一个图像并将其转换为矩阵形式：
- en: '[PRE1]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After we’ve loaded the RGB image, we convert it into grayscale and transform
    its values into floating points. The next step is to transform the image matrix
    into samples that we can use for PCA training. This can be done by splitting the
    image into rectangular patches that are 8 x 8 in size with the `Dlib::subm()`
    function and then flattening them with the `Dlib::reshape_to_column_vector()`
    function:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们加载RGB图像后，我们将其转换为灰度图，并将其值转换为浮点数。下一步是将图像矩阵转换为可用于PCA训练的样本。这可以通过使用`Dlib::subm()`函数将图像分割成8
    x 8大小的矩形块，然后使用`Dlib::reshape_to_column_vector()`函数将它们展平来完成：
- en: '[PRE2]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When we have our samples, we can normalize them by subtracting the mean and
    dividing them by their standard deviation. We can make these operations vectorized
    by converting our vector of samples into the matrix type. We do this with the
    `Dlib::mat()` function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有了我们的样本后，我们可以通过减去平均值并除以它们的标准差来对它们进行归一化。我们可以通过将我们的样本向量转换为矩阵类型来使这些操作向量化。我们使用`Dlib::mat()`函数来完成这个操作：
- en: '[PRE3]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After we’ve prepared the data samples, we calculate the covariance matrix with
    the `Dlib::` **covariance()** function and perform SVD with the `Dlib::svd()`
    function. The SVD results are the eigenvalues matrix and the eigenvectors matrix.
    We sorted the eigenvectors according to the eigenvalues and left only a small
    number (in our case, 10 of them) of eigenvectors corresponding to the biggest
    eigenvalues. The number of eigenvectors we left is the number of dimensions in
    the new feature space:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们准备好的数据样本之后，我们使用`Dlib::covariance()`函数计算协方差矩阵，并使用`Dlib::svd()`函数执行奇异值分解。SVD的结果是特征值矩阵和特征向量矩阵。我们根据特征值对特征向量进行排序，并只留下对应于最大特征值的小部分特征向量（在我们的例子中，是10个）。我们留下的特征向量数量是新特征空间的维度数：
- en: '[PRE4]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Our PCA transformation matrix is called `pca`. We used it to reduce the dimensions
    of each of our samples with simple matrix multiplication. Look at the following
    cycle and notice the `pca *` `data[i]` operation:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主成分分析转换矩阵被称为`pca`。我们使用它通过简单的矩阵乘法来降低每个样本的维度。看看以下循环并注意`pca * data[i]`操作：
- en: '[PRE5]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our data has been compressed and we can see its new size in the console output.
    Now, we can restore the original dimension of the data to be able to see the image.
    To do this, we need to use the transposed PCA matrix to multiply the reduced samples.
    Also, we need to denormalize the restored sample to get actual pixel values. This
    can be done by multiplying the standard deviation and adding the mean we got from
    the previous steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据已经被压缩，我们可以在控制台输出中看到其新的大小。现在，我们可以恢复数据的原始维度以便查看图像。为此，我们需要使用转置的PCA矩阵来乘以减少的样本。此外，我们需要对恢复的样本进行反归一化以获取实际的像素值。这可以通过乘以标准差并加上我们从上一步骤中得到的均值来完成：
- en: '[PRE6]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After we’ve restored the pixel values, we reshape them and place them in their
    original location in the image:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们恢复像素值后，我们将它们重塑并放置在图像的原始位置：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s look at the result of compressing a standard test image that is widely
    used in image processing. The following is the Lena 512 x 512 px image:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看压缩在图像处理中广泛使用的标准测试图像的结果。以下是一个512 x 512像素的Lena图像：
- en: '![Figure 6.4 – Original image before compression](img/B19849_06_04.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 压缩前的原始图像](img/B19849_06_04.jpg)'
- en: Figure 6.4 – Original image before compression
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 压缩前的原始图像
- en: 'Its original grayscale size is 262,144 bytes. After we perform PCA compression
    with only 10 principal components, its size becomes 45,760 bytes. We can see the
    result in the following figure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其原始灰度大小为262,144字节。在我们仅使用10个主成分进行PCA压缩后，其大小变为45,760字节。我们可以在以下图表中看到结果：
- en: '![Figure 6.5 – Image after compression](img/B19849_06_05.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 压缩后的图像](img/B19849_06_05.jpg)'
- en: Figure 6.5 – Image after compression
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 压缩后的图像
- en: Here, we can see that most of the essential visual information was preserved,
    despite the high compression rate.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，尽管压缩率很高，但大部分重要的视觉信息都被保留了。
- en: LDA
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LDA
- en: 'The `Dlib` library also has an implementation of the LDA algorithm, which can
    be used for dimensionality reduction. It’s a supervised algorithm, so it needs
    labeled data. This algorithm is implemented with the `Dlib::compute_lda_transform()`
    function, which takes four parameters. The first one is the input/output parameter—as
    input, it is used to pass input training data (in matrix form) and as output,
    it receives the LDA transformation matrix. The second parameter is the output
    for the mean values. The third parameter is the labels for the input data, while
    the fourth one is the desired number of target dimensions. The following code
    shows an example of how to use LDA for dimension-reduction with the `Dlib` library:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dlib`库也实现了LDA算法，可用于降维。它是一个监督算法，因此需要标记的数据。此算法通过`Dlib::compute_lda_transform()`函数实现，该函数接受四个参数。第一个参数是输入/输出参数——作为输入，它用于传递输入训练数据（以矩阵形式），作为输出，它接收LDA转换矩阵。第二个参数是均值值的输出。第三个参数是输入数据的标签，第四个参数是期望的目标维度数。以下代码展示了如何使用`Dlib`库中的LDA进行降维的示例：'
- en: '[PRE8]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To perform an actual LDA transform after the algorithm has been trained, we
    multiply our samples with the LDA matrix. In our case, we also transposed them.
    The following code shows the essential part of this example:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法训练后执行实际的LDA转换，我们将样本与LDA矩阵相乘。在我们的情况下，我们还对它们进行了转置。以下代码展示了此示例的关键部分：
- en: '[PRE9]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following graph shows the result of using LDA reduction on two components:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了在两个组件上使用LDA降维的结果：
- en: '![Figure 6.6 – The Dlib LDA transformation visualization](img/B19849_06_06.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – Dlib LDA转换可视化](img/B19849_06_06.jpg)'
- en: Figure 6.6 – The Dlib LDA transformation visualization
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – Dlib LDA转换可视化
- en: In the following block, we will see how to use Sammon mapping dimensionality
    reduction algorithm implementation from the `Dlib` library.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下块中，我们将看到如何使用来自`Dlib`库的Sammon映射降维算法实现。
- en: Sammon mapping
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sammon映射
- en: 'In the `Dlib` library, Sammon mapping is implemented with the `Dlib::sammon_projection`
    type. We need to create an instance of this type and then use it as a functional
    object. Functional object call arguments are the data that we need to transform
    and the number of dimensions of the new feature space. The input data should be
    in the form of the `std::vector` of the single samples of the `Dlib::matrix` type.
    All samples should have the same number of dimensions. The result of using this
    functional object is a new vector of samples with a reduced number of dimensions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Dlib` 库中，Sammon 映射是通过 `Dlib::sammon_projection` 类型实现的。我们需要创建此类型的实例，然后将其用作功能对象。功能对象调用参数是我们需要转换的数据和新的特征空间维度数。输入数据应以
    `std::vector` 的形式提供，其中包含单个 `Dlib::matrix` 类型的样本。所有样本应具有相同数量的维度。使用此功能对象的结果是一个具有减少维度数的新样本向量：
- en: '[PRE10]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following graph shows the result of using this dimensionality reduction
    algorithm:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了使用此降维算法的结果：
- en: '![Figure 6.7 – The Dlib Sammon mapping transformation visualization](img/B19849_06_07.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – Dlib Sammon 映射变换可视化](img/B19849_06_07.jpg)'
- en: Figure 6.7 – The Dlib Sammon mapping transformation visualization
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – Dlib Sammon 映射变换可视化
- en: In the next section, we will learn how to use the Tapkee library for solving
    dimensionality reduction tasks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用 Tapkee 库来解决降维任务。
- en: Using the Tapkee library
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Tapkee 库
- en: 'The Tapkee library contains numerous dimensionality reduction algorithms, both
    linear and non-linear ones. This is the headers-only C++ template library so it
    doesn’t require any compilation, and can be easily integrated into your application.
    It has a couple of dependencies: the `fmt` library for the formatted output and
    the Eigen3 as the math backend.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Tapkee 库包含许多降维算法，包括线性和非线性算法。这是一个仅包含头文件的 C++ 模板库，因此不需要编译，可以轻松集成到您的应用程序中。它有几个依赖项：用于格式化输出的
    `fmt` 库和作为数学后端的 Eigen3。
- en: 'There are no special classes for algorithms in this library because it provides
    a uniform API based on parameters to build a dimensionality reduction object.
    So, using this approach, we can define a single function that will take a set
    of parameters and high-dimensional input data and perform dimensionality reduction.
    The result will be a 2D plot. The following code sample shows its implementation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个库中没有为算法设置特殊类，因为它提供了一个基于参数的统一 API 来构建降维对象。因此，使用这种方法，我们可以定义一个函数，该函数将接受一组参数和高度数据输入，并执行降维。结果将是一个二维图。以下代码示例展示了其实现：
- en: '[PRE11]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is the single function that we will use to see several algorithms from
    the Tapkee library. The main parameter it takes is `tapkee::ParametersSet`; an
    object of this type can be initialized with the dimensionality reduction method
    type, the number of target dimensions, and some special parameters that configure
    the selected method. The `with_kernel` parameter specifies if this function will
    attach the kernel transform to the algorithm pipeline or not. The library API
    allows you to attach the kernel transform for any algorithm but it will be used
    only if the algorithm implementation uses it. In our case, we will use it only
    for the `KernelPCA` method. The last parameters for the Reduction function are
    the input data, labels for plotting, and the output file name.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将使用的单个函数，通过它我们可以看到 Tapkee 库中的几个算法。它接受的主要参数是 `tapkee::ParametersSet`；这种类型的对象可以用降维方法类型、目标维度数和一些配置所选方法的特殊参数进行初始化。`with_kernel`
    参数指定此函数是否将核变换附加到算法管道中。库 API 允许您为任何算法附加核变换，但它只有在算法实现使用它时才会被使用。在我们的情况下，我们只将其用于 `KernelPCA`
    方法。降维函数的最后一个参数是输入数据、绘图标签和输出文件名。
- en: 'Let’s look into the implementation. At first, we define the callback functors
    for a method pipeline. There are the Gaussian kernel callback, the linear distance
    callback, and the feature callback. Notice that all of these callback objects
    were initialized with the input data. It was done to reduce the data coping in
    the pipeline. The library API requires callback objects to be able to produce
    some result for two data indices. These callback objects should implement functionality
    to get access to the original data. So, all our callbacks were constructed from
    library-defined classes and store references to the original data containers.
    `tapkee::DenseMatrix` is just a typedef for the Eigen3 dense matrix. Another important
    thing is the index map usage for the data access, for example, the `indices` variable;
    it allows you to use your data more flexibly. The following snippet shows the
    dimensionality reduction object creation and application:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看实现。首先，我们定义了一个方法管道的回调函数。有高斯核回调、线性距离回调和特征回调。请注意，所有这些回调对象都是用输入数据初始化的。这样做是为了减少管道中的数据复制。库API要求回调对象能够为两个数据索引产生一些结果。这些回调对象应该实现获取访问原始数据的功能。因此，我们所有的回调都是基于库定义的类构建的，并存储对原始数据容器的引用。`tapkee::DenseMatrix`只是Eigen3密集矩阵的一个typedef。另一个重要的事情是索引映射的使用，用于数据访问，例如`indices`变量；它允许您更灵活地使用数据。以下片段显示了降维对象创建和应用：
- en: '[PRE12]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see that the API is uniform and the builder functions that perform configurations
    start with the word `with`. At first, we passed the parameters to the build, then
    three callback functors, and finally called the `embedRange` method that does
    the actual dimensionality reduction. `eigen_features_callback` is needed to access
    particular data values with indices, and `eigen_distance_callback` is used in
    some algorithms to measure the distance between item vectors; in our case, it’s
    just the Euclidean distance, but you can define any you need.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到API是一致的，执行配置的构建函数以“with”字开头。最初，我们将参数传递给构建函数，然后是三个回调函数，最后调用执行实际降维的`embedRange`方法。`eigen_features_callback`用于通过索引访问特定的数据值，而`eigen_distance_callback`在有些算法中用于测量项目向量的距离；在我们的例子中，它只是欧几里得距离，但您可以定义任何需要的。
- en: In the last part of the function, the `Clusters` object is populated with new
    2D coordinates and labels. Then, this object is used to plot the dimensionality
    reduction result. The plotting approach from the previous chapters was applied.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的最后部分，`Clusters`对象被填充了新的2D坐标和标签。然后，使用此对象来绘制降维结果。应用了之前章节中的绘图方法。
- en: 'In the following subsections, we will learn how to use different dimensionality
    reduction methods with our general function. These methods will be as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将学习如何使用我们的通用函数来应用不同的降维方法。这些方法如下：
- en: PCA
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA
- en: Kernel PCA
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心PCA
- en: MDS
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MDS
- en: Isomap
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isomap
- en: Factor analysis
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子分析
- en: t-distributed SNEs
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-分布SNEs
- en: PCA
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA
- en: 'Having the general function for the dimensionality reduction, we can use it
    for applying different methods. The following code shows how to create a parameter
    set to configure the PCA method:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有降维的一般函数后，我们可以用它来应用不同的方法。以下代码展示了如何创建一个参数集来配置PCA方法：
- en: '[PRE13]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first argument is the initialization of the `tapkee::ParametersSet` type
    object. We used the `tapkee::PCA` enumeration value and specified the number of
    target dimensions. Also, we didn’t use a kernel. `input_data` and `labels_data`
    are input data loaded from the file, and these variables have `tapkee::DenseMatrix`
    type, which actually is the Eigen3 dense matrix. The last parameter is the name
    of the output file for the plot image.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是`tapkee::ParametersSet`类型对象的初始化。我们使用了`tapkee::PCA`枚举值并指定了目标维度的数量。此外，我们没有使用核函数。`input_data`和`labels_data`是从文件中加载的输入数据，这些变量具有`tapkee::DenseMatrix`类型，实际上就是Eigen3密集矩阵。最后一个参数是用于绘图图像的输出文件名。
- en: 'The following graph shows the result of applying the Tapkee PCA implementation
    to our data:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了将Tapkee PCA实现应用于我们的数据的结果：
- en: '![Figure 6.8 – The Tapkee PCA transformation visualization](img/B19849_06_08.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – Tapkee PCA变换可视化](img/B19849_06_08.jpg)'
- en: Figure 6.8 – The Tapkee PCA transformation visualization
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – Tapkee PCA变换可视化
- en: You can see that this method was not able to spatially separate our 3D data
    in the 2D space.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这种方法无法在2D空间中从空间上分离我们的3D数据。
- en: Kernel PCA
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心PCA
- en: 'The non-linear version of PCA is also implemented in the Tapkee library. To
    use this method, we define the kernel method and pass it as a callback to the
    `withKernel` builder method of library API. We already did it in our general function,
    so the only thing we have to do is pass the `with_kernel` parameter as `true`.
    We used the Gaussian kernel, which is defined as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Tapkee 库中，PCA 的非线性版本也得到了实现。要使用此方法，我们定义核方法并将其作为回调传递给库 API 的 `withKernel` 构建方法。我们已经在我们的通用函数中做到了这一点，所以我们唯一要做的就是将
    `with_kernel` 参数设置为 `true`。我们使用了高斯核，其定义如下：
- en: '![](img/B19849_Formula_200.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19849_Formula_200.jpg)'
- en: 'Here, *ᵧ* is the sale coefficient that can be estimated as a median of the
    item difference, or just configured manually. The kernel callback function is
    defined in the following way:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*ᵧ* 是销售系数，可以估计为项目差异的中位数，或者手动配置。核回调函数定义如下：
- en: '[PRE14]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Tapkee` requires that you define the method named `kernel` and the function
    operator. Our implementation is very simple; the main feature here is that the
    reference to the particular data is stored as a member. It’s done in a such way
    because the library will use only indices to call the kernel functor. The actual
    call to our general function looks like this:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tapkee` 要求您定义名为 `kernel` 的方法和函数操作符。我们的实现非常简单；这里的主要特性是将特定数据的引用存储为成员。这样做是因为库将仅使用索引来调用核运算符。对我们通用函数的实际调用如下：'
- en: '[PRE15]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following graph shows the result of applying the Tapkee kernel PCA implementation
    to our data:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了将 Tapkee 核 PCA 实现应用于我们的数据的结果：
- en: '![Figure 6.9 – The Tapkee kernel PCA transformation visualization](img/B19849_06_09.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – Tapkee 核 PCA 变换可视化](img/B19849_06_09.jpg)'
- en: Figure 6.9 – The Tapkee kernel PCA transformation visualization
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – Tapkee 核 PCA 变换可视化
- en: We can see that this type of kernel makes some parts of the data separated,
    but that other ones were reduced too much.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这种核函数使得数据的一些部分被分离出来，但其他部分也被过度简化了。
- en: MDS
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MDS
- en: 'To use the MDS algorithm, we should just pass the method name to our general
    dimensionality reduction function. There are no other configurable parameters,
    especially for this algorithm. The following example shows how to use this method:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 MDS 算法，我们只需将方法名称传递给我们的通用降维函数。对于此算法，没有其他可配置的参数。以下示例显示了如何使用此方法：
- en: '[PRE16]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following graph shows the result of applying the Tapkee MDS algorithm to
    our data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了将 Tapkee MDS 算法应用于我们的数据的结果：
- en: '![Figure 6.10 – The Tapkee MDS transformation visualization](img/B19849_06_10.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – Tapkee MDS 变换可视化](img/B19849_06_10.jpg)'
- en: Figure 6.10 – The Tapkee MDS transformation visualization
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – Tapkee MDS 变换可视化
- en: You can see that the result is very similar to the PCA algorithm, and there
    is the same problem that the method was not able to spatially separate our data
    in the 2D space.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，结果与 PCA 算法非常相似，并且存在相同的问题，即该方法无法在 2D 空间中空间分离我们的数据。
- en: Isomap
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Isomap
- en: 'The Isomap method can be applied to our data as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Isomap 方法可以按以下方式应用于我们的数据：
- en: '[PRE17]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Apart from the method name and target dimensions, the `num_neighbors` parameter
    was passed. This is the number of nearest neighbor values that will be used by
    the `Isomap` algorithm.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 除了方法名称和目标维度外，还传递了 `num_neighbors` 参数。这是 `Isomap` 算法将使用的最近邻值的数量。
- en: 'The following graph shows the result of applying the Tapkee Isomap implementation
    to our data:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了将 Tapkee Isomap 实现应用于我们的数据的结果：
- en: '![Figure 6.11 – The Tapkee Isomap transformation visualization](img/B19849_06_11.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – Tapkee Isomap 变换可视化](img/B19849_06_11.jpg)'
- en: Figure 6.11 – The Tapkee Isomap transformation visualization
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – Tapkee Isomap 变换可视化
- en: You can see that this method can spatially separate our data in the 2D space,
    but the clusters are too close to each other. Also, you can play with the number
    of neighbor parameters to get another separation.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，这种方法可以在 2D 空间中空间分离我们的数据，但簇彼此之间太近。此外，您还可以调整邻居参数的数量以获得另一种分离。
- en: Factor analysis
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因子分析
- en: 'The factor analysis method can be applied to our data as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分析方法可以按以下方式应用于我们的数据：
- en: '[PRE18]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Apart from the method name and target dimensions, the `fa_epsilon` and `max_iteration`
    parameters can be passed. `fa_epsilon` is used to check the algorithm’s convergence.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 除了方法名称和目标维度外，还可以传递 `fa_epsilon` 和 `max_iteration` 参数。`fa_epsilon` 用于检查算法的收敛性。
- en: 'The following graph shows the result of applying the Tapkee factor analysis
    implementation to our data:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了将Tapkee因子分析实现应用于我们的数据的结果：
- en: '![Figure 6.12 – The Tapkee factor analysis transformation visualization](img/B19849_06_12.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – Tapkee因子分析变换可视化](img/B19849_06_12.jpg)'
- en: Figure 6.12 – The Tapkee factor analysis transformation visualization
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – Tapkee因子分析变换可视化
- en: This method also fails to clearly separate our data in the 2D space.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也未能清楚地在我们数据的2D空间中分离我们的数据。
- en: t-SNE
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: t-SNE
- en: 'The t-SNE method can be applied to our data as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE方法可以按以下方式应用于我们的数据：
- en: '[PRE19]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Apart from the method name and target dimensions, the `sne_perplexity` parameter
    was specified. This parameter regulates the algorithm convergence. Also, you can
    change the `sne_theta` value, which is the learning rate.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 除了方法名称和目标维度外，还指定了`sne_perplexity`参数。此参数调节算法收敛。此外，您还可以更改`sne_theta`值，这是学习率。
- en: 'The following graph shows the result of applying the Tapkee t-SNE implementation
    to our data:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了将Tapkee t-SNE实现应用于我们的数据的结果：
- en: '![Figure 6.13 – The Tapkee t-SNE transformation visualization](img/B19849_06_13.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – Tapkee t-SNE变换可视化](img/B19849_06_13.jpg)'
- en: Figure 6.13 – The Tapkee t-SNE transformation visualization
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – Tapkee t-SNE变换可视化
- en: You can see that this method gave the most reasonable separation of our data
    in the 2D space; there are distinct bounds between all clusters.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，这种方法在我们的2D空间中给出了最合理的数据分离；所有簇之间都有明显的界限。
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned that dimensionality reduction is the process of
    transferring data that has a higher dimension into a new representation of data
    with a lower dimension. It is used to reduce the number of correlated features
    in a dataset and extract the most informative features. Such a transformation
    can help increase the performance of other algorithms, reduce computational complexity,
    and make human-readable visualizations.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到降维是将具有更高维度的数据转移到具有较低维度的数据新表示的过程。它用于减少数据集中相关特征的数量并提取最有信息量的特征。这种转换可以帮助提高其他算法的性能，减少计算复杂度，并生成人类可读的可视化。
- en: We learned that there are two different approaches to solving this task. One
    is feature selection, which doesn’t create new features, while the second one
    is dimensionality reduction algorithms, which make new feature sets. We also learned
    that dimensionality reduction algorithms are linear and non-linear and that we
    should select either type, depending on our data. We saw that there are a lot
    of different algorithms with different properties and computational complexity
    and that it makes sense to try different ones to see which are the best solutions
    for particular tasks. Note that different libraries have different implementations
    for identical algorithms, so their results can differ, even for the same data.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到解决这个任务有两种不同的方法。一种方法是特征选择，它不会创建新的特征，而第二种方法是降维算法，它会创建新的特征集。我们还了解到降维算法有线性和非线性之分，我们应该根据我们的数据选择其中一种类型。我们看到了许多具有不同特性和计算复杂度的不同算法，尝试不同的算法来查看哪些是特定任务的最好解决方案是有意义的。请注意，不同的库对相同的算法有不同的实现，因此即使对于相同的数据，它们的结果也可能不同。
- en: The area of dimensionality reduction algorithms is a field that’s in continual
    development. There is, for example, a new algorithm called **Uniform Manifold
    Approximation and Projection** (**UMAP**) that’s based on Riemannian geometry
    and algebraic topology. It competes with the t-SNE algorithm in terms of visualization
    quality but also preserves more of the original data’s global structure after
    the transformation is complete. It is also much more computationally effective,
    which makes it suitable for large-scale datasets. However, at the moment, there
    is no C++ implementation of it.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 降维算法的领域是一个持续发展的领域。例如，有一个名为**统一流形逼近与投影**（**UMAP**）的新算法，它基于黎曼几何和代数拓扑。它在可视化质量方面与t-SNE算法竞争，但在变换完成后也保留了更多原始数据的全局结构。它也具有更高的计算效率，这使得它适合大规模数据集。然而，目前还没有C++的实现。
- en: In the next chapter, we will discuss classification tasks and how to solve them.
    Usually, when we have to solve a classification task, we have to divide a group
    of objects into several subgroups. Objects in such subgroups share some common
    properties that are distinct from the properties in other subgroups.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论分类任务及其解决方法。通常，当我们需要解决一个分类任务时，我们必须将一组对象划分为几个子组。这些子组中的对象具有一些共同属性，这些属性与其他子组的属性不同。
- en: Further reading
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'A survey of dimensionality reduction techniques: [https://arxiv.org/pdf/1403.2877.pdf](https://arxiv.org/pdf/1403.2877.pdf)'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低技术综述：[https://arxiv.org/pdf/1403.2877.pdf](https://arxiv.org/pdf/1403.2877.pdf)
- en: 'A short tutorial for dimensionality reduction: [https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf](https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低的简短教程：[https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf](https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf)
- en: 'Guide to 12 dimensionality reduction techniques (with Python code): [https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 12种维度降低技术指南（附带Python代码）：[https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/)
- en: 'A geometric and intuitive explanation of the covariance matrix and its relationship
    with linear transformation, an essential building block for understanding and
    using PCA and SVD: [https://datascienceplus.com/understanding-the-covariance-matrix](https://datascienceplus.com/understanding-the-covariance-matrix)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协方差矩阵的几何和直观解释及其与线性变换的关系，这是理解和使用PCA和SVD的必要构建块：[https://datascienceplus.com/understanding-the-covariance-matrix](https://datascienceplus.com/understanding-the-covariance-matrix)
- en: 'The kernel trick: [https://dscm.quora.com/The-Kernel-Trick](https://dscm.quora.com/The-Kernel-Trick)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核技巧：[https://dscm.quora.com/The-Kernel-Trick](https://dscm.quora.com/The-Kernel-Trick)
