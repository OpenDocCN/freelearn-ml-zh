- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Introduction to Machine Learning for Time-Series
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列机器学习简介
- en: In previous chapters, we've talked about time-series, time-series analysis,
    and preprocessing. In this chapter, we'll talk about machine learning for time-series.
    **Machine learning** is the study of algorithms that improve through experience.
    These algorithms or models can make systematic, repeatable, validated decisions
    based on data. This chapter is meant to give an introduction given both the context
    and the technical background to much of what we'll use in the remainder of this
    book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了时间序列、时间序列分析和预处理。在本章中，我们将讨论时间序列的机器学习。**机器学习**是通过经验改进的算法研究。这些算法或模型可以根据数据做出系统的、可重复的、经过验证的决策。本章旨在介绍我们在本书剩余部分将使用的许多内容的背景和技术基础。
- en: We'll go through different kinds of problems and applications of machine learning
    in time-series, and types of analyses relevant to machine learning and time-series
    analysis. We'll explain the main machine learning problems with time-series, such
    as forecasting, classification, regression, segmentation, and anomaly detection.
    We'll then review the basics of machine learning as relevant to time-series. Then,
    we'll look at the history and current uses of machine learning for time-series.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍机器学习在时间序列中的不同问题和应用，以及与机器学习和时间序列分析相关的各种分析类型。我们将解释与时间序列相关的主要机器学习问题，如预测、分类、回归、分割和异常检测。然后，我们将回顾与时间序列相关的机器学习基础知识。接着，我们将探讨机器学习在时间序列中的历史与当前应用。
- en: 'We''re going to cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Machine learning with time-series
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列中的机器学习
- en: Supervised, unsupervised, and reinforcement learning
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习、无监督学习和强化学习
- en: History of Machine Learning
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的历史
- en: Machine learning workflow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习工作流
- en: Cross-validation
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Error metrics for time-series
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列的误差度量
- en: Comparing time-series
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较时间序列
- en: Machine learning algorithms for time-series
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间序列的机器学习算法
- en: We'll start with a general introduction to machine learning with time-series.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从时间序列的机器学习概述开始。
- en: Machine learning with time-series
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列中的机器学习
- en: In this section, I'll give an introduction to applications and the main categories
    of machine learning with time-series.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍时间序列中机器学习的应用及其主要类别。
- en: Machine learning approaches for time-series are crucial in domains such as economics,
    medicine, meteorology, demography, and many others. Time-Series datasets are ubiquitous
    and occur in domains as diverse as healthcare, economics, social sciences, Internet-of-Things
    applications, operations management, digital marketing, cloud infrastructure,
    the simulation of robotic systems, and others. These datasets are of immense practical
    importance, as they can be leveraged to forecast and predict the detection of
    anomalies more effectively, thereby supporting decision making.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的机器学习方法在经济学、医学、气象学、人口学等领域至关重要。时间序列数据集无处不在，广泛应用于医疗保健、经济学、社会科学、物联网应用、运营管理、数字营销、云基础设施、机器人系统仿真等多个领域。这些数据集具有巨大的实际意义，因为它们可以被用来更有效地预测和检测异常，从而支持决策制定。
- en: 'The technical applications within machine learning for time-series abound in
    techniques. A few applications are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列中的机器学习应用技术丰富多样。以下是一些应用实例：
- en: Curve fitting
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合
- en: Regression
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Classification
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Forecasting
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Segmentation/clustering
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割/聚类
- en: Anomaly detection
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: Reinforcement learning
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: We will examine these technical applications in this book. These different applications
    have different statistical methods and models behind them that can overlap.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书中探讨这些技术应用。这些不同的应用背后有不同的统计方法和模型，它们可能会有所重叠。
- en: Let's go briefly through some of these applications for an overview of what
    to expect in the chapters to come.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要回顾这些应用，以便对接下来的章节有所预期。
- en: '**Curve fitting** is the task of fitting a mathematical function (a curve)
    to a series of points. The mathematical function is defined by parameters, and
    the parameters are adapted to fit the time-series through optimization. Curve
    fitting can be employed as a visual aid on graphs or for inference (extrapolation).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线拟合**是将数学函数（曲线）拟合到一系列数据点的任务。数学函数由参数定义，这些参数通过优化调整以适应时间序列。曲线拟合可以作为图表的视觉辅助工具，也可以用于推断（外推）。'
- en: '**Regression** is an umbrella term for statistical approaches for finding relationships
    between independent variables (features) and independent variables (targets).
    For instance, we could be predicting the exact temperature based on the release
    of carbon dioxide and methane. If there''s more than one outcome variable, this
    is called multi-target (or multi-output). An example of this could be predicting
    the temperature for different locations at the same time.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**是一个统称，指的是寻找独立变量（特征）与因变量（目标）之间关系的统计方法。例如，我们可能在基于二氧化碳和甲烷的释放来预测准确的温度。如果有多个结果变量，这称为多目标（或多输出）。一个例子可能是同时预测不同地点的温度。'
- en: When the problem is assigning labels to a time-series (or a part of it), this
    is called **classification**. The main difference to regression is that the prediction
    is categorical rather than continuous. The model that's used for classification
    is often referred to as a classifier. Classification can be binary, when there
    are precisely two classes, or multi-class, when there are more categories. An
    example would be detecting eye movements or epilepsy in EEG signals.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题是为时间序列（或其一部分）分配标签时，这称为**分类**。与回归的主要区别在于，预测结果是分类的而不是连续的。用于分类的模型通常称为分类器。分类可以是二分类，即只有两个类别，或者是多分类，即有多个类别。一个例子可能是检测脑电图信号中的眼动或癫痫。
- en: Making predictions about the future is called **forecasting**. Forecasting can
    be based only on the time-series values itself or on other variables. The techniques
    can range from curve fitting to extrapolating, from analysis of the current trends
    and variability to complex machine learning techniques. For example, we could
    be forecasting global temperatures based on the data of the last 100 years, or
    we could be forecasting the economic wellbeing of a nation. The antonym of forecasting
    is **backcasting**, where we make predictions about the past. We could be backcasting
    temperatures backward in time from before we have data available.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关于未来的预测称为**预测**。预测可以仅基于时间序列本身，也可以基于其他变量。技术可以从曲线拟合到外推，从分析当前趋势和变异性到复杂的机器学习技术。例如，我们可能基于过去100年的数据来预测全球温度，或者我们可能在预测一个国家的经济福祉。预测的反义词是**回溯**，即对过去进行预测。我们可能在没有数据可用之前，从过去回溯预测温度。
- en: '**Segmentation**, or **clustering**, is the process of grouping parts of the
    time-series into clusters (or segments) of different regimes, behaviors, or baselines.
    An example would be different activity levels in brain waves.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**分段**，或称**聚类**，是将时间序列的部分数据按照不同的状态、行为或基线分组的过程。例如，大脑波动中的不同活动水平就是一个例子。'
- en: Within the context of time-series, **anomaly detection**, also known as **outlier
    detection**, is the task of identifying events that are rare or outside the norm.
    These could be novel, changes of regime, noise, or just exceptions. A rather crude
    example would be an outage of the electricity grid detectable in a sudden drop
    of the voltage. More subtle perhaps, by way of an example, would be an increase
    in the number of calls to a call center within a certain period. In both cases,
    anomaly detection could provide actionable business insights. Techniques in anomaly
    detection can range from simple thresholding or statistics to a set of rules,
    to pattern-based approaches based on the time-series distribution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列的背景下，**异常检测**，也称为**离群点检测**，是识别稀有事件或不符合常规的事件的任务。这些事件可能是新颖的、状态变化、噪声或只是例外。例如，一个相对粗略的例子是通过电压的突然下降检测到电网停运。更微妙的例子可能是，在特定时间段内，呼叫中心接到的电话数量增加。在这两种情况下，异常检测可以提供可行的商业见解。异常检测技术可以从简单的阈值设置或统计方法到一组规则，甚至是基于时间序列分布的模式识别方法。
- en: Finally, **reinforcement learning** is the practice of learning based on maximizing
    expected rewards from a series of decisions. Reinforcement learning algorithms
    are employed in environments where there's a high level of uncertainty. This could
    mean that the conditions are unstable (high variation) or there's a general lack
    of information. Applications are bidding and pricing algorithms in stock trading
    or general auctions, and control tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**强化学习**是基于最大化一系列决策的期望奖励来进行学习的实践。强化学习算法被应用于不确定性较高的环境中。这可能意味着条件不稳定（高变异性），或者普遍缺乏信息。应用包括股票交易或一般拍卖中的竞价和定价算法，以及控制任务。
- en: Let's dive into a bit more detail about what these terms mean.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这些术语的含义。
- en: Supervised, unsupervised, and reinforcement learning
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督、无监督和强化学习
- en: 'Machine learning can be broadly categorized into supervised, unsupervised,
    and reinforcement learning, as this diagram shows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以大致分为监督学习、无监督学习和强化学习，正如该图所示：
- en: '![taxonomy.png](img/B17577_04_01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![taxonomy.png](img/B17577_04_01.png)'
- en: 'Figure 4.1: Dividing machine learning into categories'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：将机器学习划分为不同类别
- en: In **supervised** learning, the features are mapped to outcomes ![](img/B17577_04_001.png)in
    a process called **prediction** (sometimes **inference**).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在**监督**学习中，特征与结果相匹配！[](img/B17577_04_001.png)在一个叫做**预测**（有时叫做**推理**）的过程中。
- en: In the supervised case, parameters are estimated from labeled observations.
    We need to have the outcome available for each observation as the **target** column
    (or columns, in the plural).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督的情况下，参数是从带标签的观测值中估算出来的。我们需要为每个观测值提供结果，作为**目标**列（或者多列）。
- en: Therefore, the machine learning algorithm finds a mapping from X to Y.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，机器学习算法会找到从 X 到 Y 的映射。
- en: '![](img/B17577_04_002.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_002.png)'
- en: The function ![](img/B17577_04_003.png) is just one possible mapping or model
    of the input distribution X to the output distribution Y.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 函数！[](img/B17577_04_003.png)只是输入分布 X 到输出分布 Y 的一种可能映射或模型。
- en: Supervised machine learning can be categorized into classification and regression.
    In regression, our targets are continuous, and the goal is to predict the value.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习可以分为分类和回归。在回归中，我们的目标是连续的，目标是预测值。
- en: '![](img/B17577_04_004.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_004.png)'
- en: The target Y can be real-valued, either a single value or a higher dimensionality
    (multioutput).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 目标 Y 可以是实数值，可以是单一值或更高维度（多输出）。
- en: The labels match the dataset in length, but there can be several labels for
    each observation as well (multi-output).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 标签与数据集的长度相匹配，但每个观测值也可以有多个标签（多输出）。
- en: An example would be the number of products sold in a shop on a specific day
    or the amount of oil coming through a pipeline over the next month. The features
    could include current sales, demand, or day of the week.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是商店在特定日期销售的产品数量，或者下个月管道中流过的石油量。特征可以包括当前销售量、需求或星期几。
- en: In **classification**, the goal is to predict the class of the observation.
    In this case, ![](img/B17577_04_005.png) could be drawn from a categorical distribution,
    a distribution consisting of ordinal values, for example, integers as in ![](img/B17577_04_006.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在**分类**中，目标是预测观测值的类别。在这种情况下，![](img/B17577_04_005.png)可以从一个类别分布中抽取，例如整数的顺序值，如![](img/B17577_04_006.png)。
- en: 'Sometimes, we want to find a function that would give us probabilities or scores
    for given observations:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们希望找到一个函数，能够为给定的观测值提供概率或得分：
- en: '![](img/B17577_04_007.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_007.png)'
- en: In practical terms, regression and classification are very similar problems,
    and often regression and classification algorithms can be applied interchangeably.
    However, understanding the distinction is crucial to dealing with any specific
    problem in an appropriate manner.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，回归和分类是非常相似的问题，通常回归和分类算法可以互换应用。然而，理解两者的区别对以适当的方式处理具体问题至关重要。
- en: In time-series **forecasting**, the historical values are extrapolated into
    the future. The only features are the past values. For example, we could be estimating
    the calls into a call center over the next month based on calls during the last
    2 years. The forecasting task could be univariate, relying on and extrapolating
    a single feature, or multivariate, where multiple features are projected into
    the future.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列**预测**中，历史值会被外推到未来。唯一的特征是过去的值。例如，我们可以基于过去两年的电话量，估算下个月呼叫中心的电话量。预测任务可以是单变量的，依赖于并外推单一特征，或者是多变量的，其中多个特征被预测到未来。
- en: In **unsupervised** learning, the algorithm's task is to categorize observations
    based on their features. Examples of unsupervised learning are clustering or recommender
    algorithms.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在**无监督**学习中，算法的任务是根据特征对观测值进行分类。无监督学习的例子包括聚类或推荐算法。
- en: In most of the book, we'll be talking about supervised algorithms, although
    we'll also talk about unsupervised tasks such as change detection and clustering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大部分内容中，我们将讨论监督算法，尽管我们也会讨论一些无监督任务，如变化检测和聚类。
- en: The mapping function, f, predicts an outcome ![](img/B17577_04_008.png)Each
    function is specified by a set of parameters, and the optimization results in
    a set of parameters that minimizes the mismatch between Y and ![](img/B17577_04_009.png).
    Usually, this is done heuristically.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 映射函数f预测一个结果 ![](img/B17577_04_008.png) 每个函数由一组参数指定，优化结果是一组参数，最小化Y与 ![](img/B17577_04_009.png)
    之间的匹配度。通常，这一过程是通过启发式方法完成的。
- en: 'The match (mismatch) between Y and ![](img/B17577_04_010.png) is measured by
    an error function, ![](img/B17577_04_011.png), so the optimization consists of
    estimating parameters ![](img/B17577_04_012.png) for the function f that minimizes the
    error:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Y与 ![](img/B17577_04_010.png) 之间的匹配（不匹配）通过误差函数 ![](img/B17577_04_011.png) 来衡量，因此优化的目标是估计参数
    ![](img/B17577_04_012.png)，以最小化误差：
- en: '![](img/B17577_04_013.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_013.png)'
- en: In this formula, since the error function is part of the optimization, ![](img/B17577_04_014.png)
    is called the **objective function**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在此公式中，由于误差函数是优化的一部分，![](img/B17577_04_014.png) 被称为**目标函数**。
- en: In **reinforcement learning**, an agent is interacting with the environment
    through actions and gets feedback in the shape of rewards. You can find out more
    about reinforcement learning for time-series in *Chapter 11*, *Reinforcement Learning
    for Time-Series*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在**强化学习**中，代理通过动作与环境互动，并通过奖励的形式获得反馈。你可以在*第11章*，*时间序列的强化学习*中了解更多关于时间序列的强化学习内容。
- en: Contrary to the situation in supervised learning, no labeled data is available,
    but rather the environment is explored and exploited based on the expectation
    of cumulative rewards.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习中的情况相反，强化学习中没有标签数据可用，而是通过探索和利用环境来期望获得累计奖励。
- en: '**Machine learning**, the study of algorithms that improve with experience,
    can be traced back to the 1960s when statistical methods (discussed in *Chapter
    1*, *Time-Series with Python*) were discovered. Let''s start with a brief history
    of machine learning to give some context. This will provide some more terminology
    and a basic idea of the principal directions in machine learning. We''ll give
    some more detailed context in the appropriate chapters.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**，即通过经验不断改进的算法研究，可以追溯到 1960 年代，当时统计方法（在*第一章*，*使用 Python 进行时间序列分析*中讨论）被发现。让我们从机器学习的简短历史开始，以便提供一些背景。这将提供一些术语以及机器学习中主要方向的基本概念。我们将在相关章节中提供更详细的背景。'
- en: History of machine learning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的历史
- en: Biological neural networks were conceptualized as a mathematical model by Warren
    McCulloch and Walter Pitts in 1943 in what was the foundation of artificial neural
    networks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经网络最早由沃伦·麦卡洛克和沃尔特·皮茨于1943年提出，作为一种数学模型，奠定了人工神经网络的基础。
- en: 'Frank Rosenblatt developed the so-called **perceptron** in 1958, which is a
    **fully connected feed-forward neural network** in today''s terms. This schematic
    shows a perceptron with two input neurons and a single output neuron (based on
    an image on Wikimedia Commons):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 弗兰克·罗森布拉特在1958年开发了所谓的**感知器**，在今天的术语中，这是一种**全连接的前馈神经网络**。该示意图展示了一个具有两个输入神经元和一个输出神经元的感知器（基于维基共享资源上的图像）：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_SIwGeX/Screenshot
    2021-08-15 at 18.23.06.png](img/B17577_04_03.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_SIwGeX/Screenshot
    2021-08-15 at 18.23.06.png](img/B17577_04_03.png)'
- en: 'Figure 4.3: Perceptron'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：感知器
- en: The connections to the output neuron *y* have weights *w*[1] and *w*[2]. This
    is a simple linear model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出神经元 *y* 的连接具有权重 *w*[1] 和 *w*[2]。这是一个简单的线性模型。
- en: Another important step was how errors can be propagated backward through the
    network. The basics of **backpropagation** were published by Henry J. Kelley shortly
    afterward (1960) as a mechanism for training these networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要步骤是如何通过网络反向传播误差。**反向传播**的基础由亨利·J·凯利在不久后（1960年）发表，作为训练这些网络的一种机制。
- en: This research was dealt a severe blow, however, when, in 1969, Marvin Minsky
    and Seymour Papert published the book "*Perceptrons*", which included a simple
    proof that linear functions (as in the 2-layer perceptron) could not model non-linear
    functions. According to the authors, this meant that perceptrons, wouldn't be
    useful or interesting in practice. The fact that perceptrons could have more than
    two layers, parameters of which could be learned via backpropagation, was glossed
    over in the book. Research in artificial neural networks only picked up again
    in the 1980s.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这项研究在1969年遭遇了严重打击，当时马文·明斯基（Marvin Minsky）和西摩·帕珀特（Seymour Papert）出版了《*感知机*》一书，书中简单地证明了线性函数（如二层感知机）无法建模非线性函数。根据作者的观点，这意味着感知机在实践中既无用也不值得研究。书中忽略了感知机可以有超过两层，并且这些层的参数可以通过反向传播学习的事实。人工神经网络的研究直到1980年代才重新开始。
- en: The **nearest neighbor algorithm** was described by Evelyn Fix and Joseph Hodges
    in 1951, and then expanded in 1967 by Thomas Cover and Peter E. Hart. The nearest
    neighbor algorithm can be applied to both classification and regression. It works
    by retrieving the k most similar instances between a new data point, and all known
    instances in the dataset (k is a parameter). In the case of classification, the
    algorithm votes for the most frequent label; in the case of regression, it averages
    the labels.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**最近邻算法**由伊夫琳·费克斯（Evelyn Fix）和约瑟夫·霍奇斯（Joseph Hodges）于1951年描述，随后由托马斯·科弗（Thomas
    Cover）和彼得·E·哈特（Peter E. Hart）在1967年进行了扩展。最近邻算法可以应用于分类和回归。它通过检索一个新数据点与数据集中所有已知实例之间最相似的k个实例（k是一个参数）来工作。在分类的情况下，算法投票选出最频繁的标签；在回归的情况下，它对标签取平均值。'
- en: Another important milestone was the development of the decision tree algorithm.
    The ID3 decision tree algorithm (Iterative Dichotomiser 3) was published by Ross
    Quinlan in a 1979 paper and is the precursor to decision trees used today.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的里程碑是决策树算法的发展。ID3决策树算法（迭代二分法3）由罗斯·昆兰（Ross Quinlan）在1979年的论文中发布，它是今天使用的决策树算法的前身。
- en: The **CART** algorithm (**Classification And Regression Tree**) was published
    by Leo Breiman in 1984\. The **C4.5** algorithm, a descendant of ID3, came out
    in 1992 (Ross Quinlan) and is regarded today as a landmark in machine learning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**CART**算法（**分类与回归树**）由利奥·布雷曼（Leo Breiman）于1984年发布。**C4.5**算法，ID3的后代，于1992年由罗斯·昆兰发布，被认为是机器学习领域的一个里程碑。'
- en: 'What was revolutionary about the decision tree is that it consists of step
    functions that partition the feature space of the data points into pockets that
    have a similar outcome. While many machine learning algorithms struggle when there
    are many interactions to consider, decision trees thrive in these situations.
    The following diagram illustrates what a decision tree looks like:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的革命性在于它由步骤函数组成，这些函数将数据点的特征空间划分成若干口袋，每个口袋的输出结果相似。尽管许多机器学习算法在考虑多重交互时表现不佳，但决策树在这些情况下表现得很好。以下图示展示了决策树的形态：
- en: '![../../../../Desktop/Screenshot%202021-04-26%20at%2000.09](img/B17577_04_04.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![../../../../Desktop/Screenshot%202021-04-26%20at%2000.09](img/B17577_04_04.png)'
- en: 'Figure 4.4: Decision tree'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：决策树
- en: Each node or split in the tree is a single question based on the value of a
    feature. At each iteration during the tree construction, a statistical function
    called a split criterion is applied to decide on the best feature to query. Typical
    choices for a split criterion are the Gini impurity or information entropy, which
    both minimize the variability of the targets within branches.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的每个节点或分支都是一个基于特征值的单一问题。在树构建的每次迭代中，都会应用一种称为分裂标准的统计函数，用来决定查询最佳特征。分裂标准的典型选择包括基尼不纯度和信息熵，它们都可以最小化分支内目标变量的变异性。
- en: 'Decision trees, in turn, form the basis of ensemble techniques such as the
    random forest or gradient boosted trees. There are two main ensemble techniques:
    boosting and bagging. **Boosting** was invented by Robert Schapire in 1990, and
    consists of incrementally adding base learners in a cascade. A **base learner**
    (also **weak learner**) is a very simple model that in itself is only weakly correlated
    to the targets. Each time when adding a new base learner to the existing ones,
    the importance (weights) of data points in the training set are rebalanced.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树反过来成为集成技术的基础，例如随机森林或梯度提升树。集成技术主要有两种：提升（boosting）和自助法（bagging）。**提升**由罗伯特·沙皮尔（Robert
    Schapire）于1990年发明，它通过级联的方式逐步增加基学习器。**基学习器**（也称为**弱学习器**）是一个非常简单的模型，本身与目标变量的相关性很弱。每次向现有的基学习器中添加新学习器时，都会重新平衡训练集中数据点的重要性（权重）。
- en: This means that in each iteration, the algorithm comes to grips with more and
    more samples that it struggles with, leading to higher precision with each new
    addition of a base learner.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在每次迭代中，算法会处理越来越多它难以应对的样本，随着每次新增一个基学习器，精度也会提高。
- en: This formed the basis for **AdaBoost**, an adaptive boosting algorithm, which
    won its inventors, Yoav Freund and Robert Schapire, the Gödel Prize, a prestigious
    recognition for outstanding papers in the area of theoretical computer science.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这为**AdaBoost**（自适应提升算法）奠定了基础，该算法的发明者Yoav Freund和Robert Schapire因此获得了Gödel奖，这是理论计算机科学领域最具声望的论文奖项。
- en: 'This illustration (from Wikipedia) shows how each base classifier is trained
    subsequently on different subsets of the dataset, where weights are changed for
    each new training:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这张插图（来自Wikipedia）展示了每个基分类器是如何在数据集的不同子集上依次训练的，并且每次新的训练都改变了权重：
- en: '![https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1024px-Ensemble_Boosting.svg.png](img/B17577_04_05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1024px-Ensemble_Boosting.svg.png](img/B17577_04_05.png)'
- en: 'Figure 4.5: Boosting'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：提升法
- en: '**Bagging** is the basis for the random forest, and was invented in 1994 by
    Leo Breiman. Bagging consists of two parts, the bootstrap and aggregation. **Bootstrapping**
    is sampling with replacements from the training set. A separate model can be trained
    on each sample in isolation. These models together form an ensemble. The predictions
    from the individual models can then be aggregated into a combined decision, for
    example, by taking the mean.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**袋装法**是随机森林的基础，由Leo Breiman于1994年发明。袋装法包括两个部分：自助法（bootstrap）和聚合（aggregation）。**自助法**是从训练集中进行有放回的抽样。每个样本可以独立地训练一个模型，这些模型共同形成一个集成。各个模型的预测结果可以被聚合成一个综合决策，例如通过取平均值。'
- en: 'The following diagram (source: Wikipedia) shows how a bagged ensemble is trained
    and used for prediction. This is how a **random forest** (Leo Breiman, 2001) learns
    with the decision tree as a base learner.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图（来源：Wikipedia）展示了一个袋装集成如何被训练并用于预测。这就是**随机森林**（Leo Breiman, 2001）如何使用决策树作为基学习器进行学习的方式。
- en: '![https://upload.wikimedia.org/wikipedia/commons/b/bd/Bagging_for_Classification_with_descripitons.png](img/B17577_04_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![https://upload.wikimedia.org/wikipedia/commons/b/bd/Bagging_for_Classification_with_descripitons.png](img/B17577_04_06.png)'
- en: 'Figure 4.6: Bagging'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：袋装法
- en: 'The following table shows the main differences between bagging and boosting:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了袋装法和提升法之间的主要区别：
- en: '|  | Bagging | Boosting |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | Bagging | Boosting |'
- en: '| Base learners are trained: | Independently (can be learned in parallel) |
    Sequentially |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 基学习器的训练方式： | 独立训练（可以并行训练） | 顺序训练 |'
- en: '| Weights are: | Left unchanged | Changed after every iteration |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 权重： | 保持不变 | 每次迭代后改变 |'
- en: '| Base learners are weighted: | Equally | According to training performance
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 基学习器的权重： | 相等 | 根据训练表现 |'
- en: 'Figure 4.7: Differences between bagging and boosting'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：袋装法与提升法的区别
- en: '**Gradient boosting** (developed by Friedman and others) is a further extension
    of boosting with Unhyphenate. In gradient boosting, new weak learners are added
    in a fashion that they are maximally correlated with the negative gradient of
    the loss function. These are some popular implementations for gradient boosted
    trees:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升**（由Friedman等人开发）是提升方法的进一步扩展，采用了Unhyphenate。在梯度提升中，新的弱学习器以与损失函数负梯度最大相关的方式被添加。这些是梯度提升树的一些流行实现：'
- en: CatBoost (by Andrey Gulin and others at Yandex)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CatBoost（由Yandex的Andrey Gulin等人开发）
- en: Light Gradient Boosting Machine (LightGBM, at Microsoft)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Light Gradient Boosting Machine（LightGBM，微软公司）
- en: XGBoost
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: Backpropagation was rediscovered in 1986 by David Rumelhart, Geoffrey Hinton,
    and Ronald J. Williams. Shortly after, deeper networks were developed that could
    be applied to more interesting problems that attracted attention.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法是由David Rumelhart、Geoffrey Hinton和Ronald J. Williams在1986年重新发现的。随后，发展出了更深层的网络，能够应用于更有趣的问题，并引起了关注。
- en: Between 1995 and 1997, Sepp Hochreiter and Jürgen Schmidhuber proposed a recurrent
    neural network architecture, the **long short-term memory** (**LSTM**). For many
    years, LSTMs constituted the state-of-the-art for many applications in voice recognition,
    translation, and more. Today, recurrent neural networks, have been largely replaced
    by transformers or ConvNets, even for sequence modeling tasks. With LSTM's high
    demands on computing resources, some people go as far as regarding the LSTM as
    obsolete given the alternatives.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1995到1997年间，Sepp Hochreiter和Jürgen Schmidhuber提出了一种循环神经网络架构——**长短期记忆网络** (**LSTM**)。多年来，LSTM在语音识别、翻译等许多应用中占据了领先地位。如今，循环神经网络已经在许多任务中被transformer或ConvNets取代，即使是序列建模任务。鉴于LSTM对计算资源的高需求，一些人甚至认为LSTM已过时，尤其是在有了替代方案之后。
- en: '**Support Vector Machines** (**SVMs**) were developed in the early 1990s at
    AT&T Bell Laboratories by Vladimir Vapnik and colleagues based on statistical
    learning frameworks described by Vapnik and Chervonenkis. In classification, SVMs
    maximize the distance between the two categories in a projected space. As part
    of the training, a hyperplane, called a support vector, is constructed that separates
    positive and negative examples.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机** (**SVMs**)是在1990年代初期，由弗拉基米尔· Vapnik 和同事们在AT&T贝尔实验室开发的，基于Vapnik和Chervonenkis描述的统计学习框架。在分类中，SVM通过最大化投影空间中两类之间的距离来进行操作。在训练过程中，构建一个超平面（称为支持向量）来分离正负例子。'
- en: In the next section, we'll go through the basics of machine learning modeling
    and scientific practices in model validation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍机器学习建模的基础知识以及模型验证中的科学实践。
- en: Machine learning workflow
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流
- en: In the next section, we'll go through the basics of time-series and machine
    learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍时间序列和机器学习的基础知识。
- en: Machine learning mostly deals with numerical data that is in tabular form as
    a matrix of size ![](img/B17577_04_015.png). The layout is generally in a way
    that each row ![](img/B17577_04_016.png) represents an observation, and each column
    ![](img/B17577_04_017.png) represents a feature.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习主要处理数值数据，这些数据以大小为![](img/B17577_04_015.png)的矩阵形式呈现。布局通常是每一行![](img/B17577_04_016.png)表示一个观察，每一列![](img/B17577_04_017.png)表示一个特征。
- en: In time-series problems, the column related to time doesn't necessarily serve
    as a feature, but rather as an index to slice and order the dataset. Time columns
    can, however, be transformed into features, as we'll see in *Chapter 3*, *Preprocessing
    time-series*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列问题中，与时间相关的列不一定作为特征使用，而是作为索引来切割和排序数据集。然而，时间列可以被转换为特征，正如我们在*第3章*《时间序列预处理》中看到的那样。
- en: 'Each observation is described by a vector of M features. Although a few machine
    learning algorithms can deal with non-numerical data internally, typically, each
    feature is either numerical or gets converted to numbers before feeding it into
    a machine learning algorithm. An example of a conversion is representing Male
    as 0 and Female as 1\. Put simply, each feature can be defined as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个观察由M个特征的向量描述。虽然一些机器学习算法可以内部处理非数值数据，但通常每个特征要么是数值型的，要么在输入机器学习算法之前被转换为数字。例如，Male可以表示为0，Female表示为1。简单来说，每个特征可以定义如下：
- en: '![](img/B17577_04_018.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_018.png)'
- en: The machine learning workflow can be separated into three processes, as shown
    in the following diagram. I've added data loading and time-series analysis, which
    informs machine learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流可以分为三个过程，如下图所示。我已经添加了数据加载和时间序列分析，它们为机器学习提供了信息。
- en: '![machine_learning_workflow_cropped.png](img/B17577_04_07.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![machine_learning_workflow_cropped.png](img/B17577_04_07.png)'
- en: 'Figure 4.8: Machine learning workflow'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：机器学习工作流
- en: We first must transform (or preprocess) our data, train or fit a model, and
    then we can apply the trained model to new data. This diagram, very simplistic
    perhaps, puts the focus on the three different stages of the machine learning
    process. Each stage comes with its own challenges and particularities for time-series
    data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先必须对数据进行转换（或预处理），训练或拟合一个模型，然后可以将训练好的模型应用于新数据。这个图示虽然可能过于简化，但将焦点放在了机器学习过程的三个不同阶段。每个阶段都有其挑战和针对时间序列数据的特殊性。
- en: This can also help to think about the data flow from input to transform to training
    to prediction. We should keep in mind the available historical data and its limitations,
    as well as the future data points that are to be used for predictions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这也有助于思考数据流的过程，从输入到转换，再到训练和预测。我们应该牢记可用的历史数据及其局限性，以及将用于预测的未来数据点。
- en: In the next section, we'll discuss the general principles of cross-validation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论交叉验证的一般原则。
- en: Cross-validation
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'Here''s a well-known saying in machine learning attributed to George Box, whom
    we''ve encountered several times already in this book: "All models are wrong,
    but some are useful."'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习中一条广为人知的格言，归功于我们在本书中已经遇到过几次的乔治·博克斯：“所有的模型都是错的，但有些是有用的。”
- en: Machine learning algorithms make repeatable decisions and, given the correct
    controls, these decisions can be free from the cognitive biases that underlie
    much of human decision making. The point is to make sure that our model is useful
    by validating performance. In machine learning, the process of testing a model
    on data it hasn't seen in training is called cross-validation (sometimes, **out-of-sample
    testing**).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法做出可重复的决策，并且在正确的控制条件下，这些决策可以摆脱许多人类决策中的认知偏差。关键是通过验证性能来确保我们的模型是有用的。在机器学习中，在未见过的数据上测试模型的过程称为交叉验证（有时也叫**样本外测试**）。
- en: 'To ensure that parameters estimated on a dataset of limited size are still
    valid for more data, we must go through a validation that makes sure that the
    quality holds up. For validation, we usually split the dataset into at least two
    parts, the training set and the test set. We estimate parameters on a training
    set, and then run the model on the test set to get an idea of the quality of the
    model on unseen data points. This is illustrated in the following diagram:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保在有限数据集上估计的参数仍然适用于更多的数据，我们必须进行验证，以确保质量保持一致。验证时，我们通常将数据集拆分为至少两个部分，即训练集和测试集。我们在训练集上估计参数，然后在测试集上运行模型，以了解模型在未见过的数据点上的表现。这个过程在下图中有所示意：
- en: '![](img/B17577_04_08.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_08.png)'
- en: 'Figure 4.9: Cross-validation'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：交叉验证
- en: Usually, in machine learning we would shuffle points randomly before splitting
    between training and test. However, in time-series, we would take older data points
    for training and newer points for testing. For instance, having 1 year of data
    available on the email opening propensities of customers, we would train a model
    on 9 months' worth of data, validate our model on 2 months' worth of data, and
    test the final performance on the dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在机器学习中，我们会在划分训练集和测试集之前随机打乱数据点。然而，在时间序列中，我们会将较早的数据点用于训练，较新的数据点用于测试。例如，如果我们有一年的数据可用，记录了客户打开电子邮件的倾向，我们会用9个月的数据训练模型，利用2个月的数据验证模型，并在数据集上测试最终的性能。
- en: The use of validation and test can be seen as a nested process in the sense
    that the test set checks the main testing process that involves the validation
    dataset. Often, the separation of validation and test sets is omitted, so the
    dataset is split only into training and test sets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集和测试集的使用可以被看作是一个嵌套过程，测试集检查涉及验证数据集的主要测试过程。通常，验证集和测试集的划分会被省略，因此数据集仅被拆分为训练集和测试集。
- en: 'A note about terminology: while a **loss function** is part of the optimization
    for training your model, a **metric** is used to evaluate your model. The evaluation
    can be post-hoc, after training, or during training as additional information.
    In this section, we''ll discuss both metrics and loss functions.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 术语说明：虽然**损失函数**是训练模型优化的一部分，但**度量**用于评估模型。评估可以是事后进行的，也可以是在训练过程中作为附加信息进行的。在本节中，我们将讨论度量和损失函数。
- en: It is good practice to start a project with an assessment of how to measure
    performance. We need to choose how to measure performance to translate the business
    problem into a metric or a loss. Some algorithms allow flexibility in the choice
    of objective functions, others don't, but we can measure performance with a different
    metric.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的做法是，在项目开始时，先评估如何衡量性能。我们需要选择如何衡量性能，将商业问题转化为度量标准或损失。一些算法允许在选择目标函数时具有灵活性，另一些则不允许，但我们可以通过不同的度量来衡量性能。
- en: Next, we'll be discussing error and loss metrics for regression and classification.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论回归和分类的误差与损失度量。
- en: Error metrics for time-series
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列的误差度量
- en: Time-series data is defined as a set of data points containing details about
    different points in time. Generally, time-series data contains data points sampled
    or observed at an equal interval of time.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据被定义为包含有关不同时间点的详细信息的数据点集合。通常，时间序列数据包含在相等时间间隔内采样或观察到的数据点。
- en: For the different applications that we discussed earlier, we need to be able
    to quantify the performance of the model, be it a regression, classification,
    or another type of model, and choose a metric that captures the performance we
    want to achieve. Once we have chosen a metric for our model, we can then build
    and train models to improve them. Often, we'd start with a simpler model and then
    try to improve on the performance of this simpler model as a baseline. In the
    end, we want to find the model that is best according to our metric.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们之前讨论的不同应用，我们需要能够量化模型的性能，无论是回归、分类还是其他类型的模型，并选择一个能够捕捉我们希望达到的性能的指标。一旦选择了适合我们模型的指标，我们就可以构建并训练模型以改善它们。通常，我们会从一个较简单的模型开始，然后尝试在这个简单模型的基础上提高性能。最终，我们希望找到一个根据我们指标表现最好的模型。
- en: In this section, we'll discuss commonly used performance measures and their
    properties. Generally, for an error measure, the smaller the values, the better
    the prediction (or the forecast). In changing the parameters of our model, we
    want to reduce the error.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论常用的性能度量及其特性。通常，对于误差度量，值越小，预测（或预测结果）越好。在改变模型参数时，我们希望减少误差。
- en: There's not just a single metric that's apt for the purpose of any arbitrary
    application or dataset. Depending on the dataset, you might have to search and
    try different error metrics and see which one best captures your objective. In
    some circumstances, you might even want to define your own metric.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 并没有单一的指标适用于任何任意应用或数据集。根据数据集的不同，你可能需要尝试不同的误差指标，看看哪一个最能捕捉你的目标。在某些情况下，你甚至可能想要定义你自己的指标。
- en: Regression
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: Time-Series regression is the task of identifying patterns and signals in the
    features in relation to the behavior of time-series, for example, how skill improves
    with the time invested in practice.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列回归是识别与时间序列行为相关的特征中的模式和信号的任务，例如，技能如何随着练习时间的投入而提高。
- en: During training, when your regression model gives a result on the training set,
    we can utilize a metric that compares the model output to the training set values,
    and during validation, we can calculate the same measure to know how good our
    regression predictions line up to the validation set targets. The error metric
    summarizes the difference between the values predicted by your machine learning
    model and the actual values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，当你的回归模型在训练集上给出结果时，我们可以使用一个指标来将模型输出与训练集值进行比较；在验证过程中，我们可以计算相同的度量，以了解我们的回归预测与验证集目标的匹配程度。误差指标总结了机器学习模型预测值与实际值之间的差异。
- en: 'If ![](img/B17577_04_019.png) is a prediction of the model for time step *t*,
    and the actual target value is *y*[t], intuitively, for a particular point, *t*,
    of our dataset, the **forecast error** (also **prediction error** or **residual**)
    is the difference between the actual values of the target and the values our model
    predicts:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果！[](img/B17577_04_019.png)是模型在时间步*t*的预测值，而实际目标值是*y*[t]，直观地说，对于数据集中的某一点*t*，**预测误差**（也叫**预测误差**或**残差**）是目标实际值与我们模型预测值之间的差异：
- en: '![](img/B17577_04_020.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_020.png)'
- en: 'This compares the actual target Y to the predicted targets ![](img/B17577_04_021.png).
    According to this formula, the error is negative if the prediction is higher than
    the actual target value. The **sum of squares of the residuals** (SS, also **residual
    sum of squares**) ignores the direction of the error:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该图比较了实际目标Y与预测目标！[](img/B17577_04_021.png)。根据这个公式，当预测值高于实际目标值时，误差为负。**残差的平方和**（SS，也叫**残差平方和**）忽略了误差的方向：
- en: '![](img/B17577_04_022.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_022.png)'
- en: While both the residual and the squared residual could already be used to measure
    the performance of predictions over a time-series, they are not commonly used
    as a regression metric or loss.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管残差和平方残差已经可以用来衡量时间序列预测的性能，但它们并不常用作回归指标或损失函数。
- en: 'Let''s start with the most commonly used metric for regression: the **coefficient
    of determination**. This is a relatively simple formula based on a ratio of the
    sum of the squares of the residuals, SS, and the total sum of squares, TSS, a
    measure of the variability:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从回归中最常用的度量开始：**决定系数**。这是一个相对简单的公式，基于残差平方和 SS 与总平方和 TSS 的比率，TSS 是一种变异性度量：
- en: '![](img/B17577_04_023.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_023.png)'
- en: In this fraction, the nominator is the sum of the squares of the residuals,
    SS, the unexplained variance.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个分数中，分子是残差的平方和，SS，表示未解释的方差。
- en: The denominator is TSS, the total sum of squares. This is defined as ![](img/B17577_04_024.png),
    where ![](img/B17577_04_025.png) is the mean of the series, ![](img/B17577_04_026.png).
    The total sum of squares represents the explained variance of the time-series.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 分母是 TSS，总平方和。它被定义为 ![](img/B17577_04_024.png)，其中 ![](img/B17577_04_025.png)
    是序列的均值，![](img/B17577_04_026.png)。总平方和表示时间序列的解释方差。
- en: Basically, we are measuring the summed squares of residuals in relation to the
    total variance of the time-series. This fraction, between 0 and 1, where 0 is
    best – no error at all, is inverted by subtracting from 1, so that finally 0 is
    worst and 1 is best.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们是在衡量残差的平方和与时间序列总方差的关系。这个分数介于 0 和 1 之间，0 表示最好——没有误差，1 表示最差。通过从 1 中减去该值来反转它，因此最终
    0 是最差的，1 是最好的。
- en: 'When expanded, this looks as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展后，结果如下所示：
- en: '![](img/B17577_04_027.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_027.png)'
- en: '![](img/B17577_04_028.png) expresses the proportion of the variance in the
    dependent variable that is predictable from the independent variable. As mentioned,
    it is bounded between 0 and 1, where 1 means there''s a perfect relationship,
    and 0 means there''s no relationship at all.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B17577_04_028.png) 表示可以从自变量预测的因变量方差的比例。如前所述，它的值在 0 和 1 之间，其中 1 表示存在完美的关系，0
    表示没有任何关系。'
- en: 'The coefficient of determination, ![](img/B17577_04_029.png), is not an error
    measure since an error measure expresses the distribution of residuals so that
    high is bad and low is good. We could, however, express an error measure, let''s
    call it the **r-error (RE)**, very similar to the above, as:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 决定系数，![](img/B17577_04_029.png)，不是一个误差度量，因为误差度量表示残差的分布，高的不好，低的好。然而，我们可以表达一个误差度量，称之为**r-误差（RE）**，它与上述非常相似，定义如下：
- en: '![](img/B17577_04_030.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_030.png)'
- en: This is rarely used in practice. An error measure very similar to RE is the
    **mean relative absolute error** (**MRAE**), which we'll discuss further ahead.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在实际中很少使用。一个与 RE 非常相似的误差度量是**均值相对绝对误差**（**MRAE**），我们将在后面进一步讨论。
- en: 'Naively, we could take the average error, where we just take the mean over
    the forecast error – the mean error:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 天真地说，我们可以取平均误差，即仅对预测误差求均值——即均值误差：
- en: '![](img/B17577_04_031.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_031.png)'
- en: Here, *N* is the number of points (or the number of discrete time steps). We
    calculate the error for each point and then take the mean over all these errors.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N* 是点的数量（或离散时间步数）。我们计算每个点的误差，然后对这些误差取平均。
- en: If the ME is positive, the model systematically underestimates the targets,
    if it's positive, it overestimates the targets on the whole. While this can be
    useful, it's a serious problem for an error metric, however, because the effects
    of positive and negative errors cancel each other out. Therefore, a low ME does
    not mean that predictions are good, rather that the average is close to zero.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ME 为正，模型会系统性地低估目标值；如果为负，则会高估目标值。虽然这可能有用，但作为误差度量，这仍然是一个严重的问题，因为正误差和负误差的效应会相互抵消。因此，低
    ME 并不意味着预测很好，而是表示平均值接近零。
- en: Furthermore, most regression models include a constant term that is equal to
    the mean of the target, so this value would be exactly 0\. In conclusion, our
    naïve measure is useless in practical settings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数回归模型包括一个常数项，该常数等于目标的均值，因此这个值将正好为 0。总之，我们的天真度量在实际设置中是无用的。
- en: 'I''ve included the ME for discussion of why most measures that are commonly
    used discard the direction of the error and for highlighting the importance of
    the main components of the basic error metrics:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我包括了 ME 来讨论为什么大多数常用的度量会忽略误差的方向，并强调基本误差指标的主要组成部分的重要性：
- en: The residual operation
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差操作
- en: The integration
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 积分
- en: In the case of the ME, the residual operation is the identify function, which
    means the residual doesn't change. More often, the square or absolute functions
    are used. The integration of the errors is often the (arithmetic) mean, but sometimes
    the median; however, it can be a more complex operation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在均值误差（ME）的情况下，残差操作是恒等函数，这意味着残差不会改变。更常见的是使用平方或绝对值函数。误差的整合通常是（算术）平均值，但有时也可以是中位数；不过，它也可以是更复杂的操作。
- en: 'In practice, the most popular error metrics are the mean squared error (MSE),
    mean absolute error (MAE), and the root mean squared error (RMSE). These most
    important error metrics are defined in the following table:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，最常用的误差度量是均方误差（MSE）、平均绝对误差（MAE）和均方根误差（RMSE）。以下表格定义了这些最重要的误差度量：
- en: '| Metric Name | Definition |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 定义 |'
- en: '| Mean squared error | ![](img/B17577_04_032.png) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 均方误差 | ![](img/B17577_04_032.png) |'
- en: '| Mean absolute error | ![](img/B17577_04_033.png) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 平均绝对误差 | ![](img/B17577_04_033.png) |'
- en: '| Root mean squared error | ![](img/B17577_04_034.png) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 均方根误差 | ![](img/B17577_04_034.png) |'
- en: 'Figure 4.10: Popular regression metrics'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10：常见的回归指标
- en: With the **mean squared error (MSE)**, we calculate the residual for each point,
    then square them, so positive and negative errors don't cancel each other out.
    Then we take the mean over these squared errors. An MSE of 0 indicates perfect
    performance. This can happen with toy datasets that you can play around with for
    fun; however, in practice, this will only happen if you made a mistake in building
    your dataset or in validation, because real life is always more complex than you
    can capture with a model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**均方误差（MSE）**，我们计算每个点的残差，然后对它们进行平方，这样正误差和负误差就不会互相抵消。然后我们取这些平方误差的平均值。MSE为0表示完美表现。这种情况可能出现在你玩弄的小数据集上；然而，实际上，只有在你在构建数据集或验证过程中出错时，才会出现这种情况，因为现实生活总是比你能通过模型捕捉到的更复杂。
- en: The **mean absolute error (MAE)** is very similar to the MSE, only instead of
    squaring the residuals, we take their absolute values. As opposed to the MSE,
    all errors contribute in linear proportion (rather than being squared).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差（MAE）**与MSE非常相似，不同之处在于，我们没有对残差进行平方，而是取它们的绝对值。与MSE不同，所有误差都是线性贡献的（而不是被平方）。'
- en: A major difference between taking the absolute versus taking the square is in
    how outliers or extreme values are treated. The square function forces a higher
    weight on values that are very different. With the MSE, the error grows quadratically
    instead of linearly as is the case with the MAE. This means that the MSE punishes
    extreme values much more strongly and, as a result, it is less robust to outliers
    in the dataset than the MAE. The distribution of the errors is a major concern
    in choosing an error metric that's right for the job.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 选择绝对值与平方之间的一个主要区别在于如何处理异常值或极端值。平方函数对与众不同的值赋予更高的权重。在MSE中，误差是按平方而不是线性增长的，这意味着MSE对极端值的惩罚远远比MAE更强，因此它对数据集中的异常值不如MAE稳健。误差分布是选择合适的误差度量时的重要考虑因素。
- en: Another common metric is the **root mean squared error (RMSE)**, or **root mean
    square deviation (RMSD),** which, as the name suggests, is the square root of
    the MSE. In that sense, RMSE is a scaled version of the MSE. Which one to take
    between the two is a presentation choice – both of them would lead to the same
    models.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的指标是**均方根误差（RMSE）**，或**均方根偏差（RMSD）**，顾名思义，它是均方误差（MSE）的平方根。从这个意义上说，RMSE是MSE的缩放版本。选择这两个指标中的哪一个，取决于展示的需求——它们都会导致相同的模型。
- en: What makes the RMSE interesting as a choice is that it comes in the same units
    and scale as the predicted variable, which makes it more intuitive. Finally, the
    RMSE is equivalent to the standard deviation or the error. This connection between
    standard deviation and the distribution of errors is quite meaningful, and you
    can summarize the error distribution with other measures such as the standard
    error or confidence interval (both of which we've discussed in *Chapter 2*, *Time-Series
    Analysis with Python*).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE作为选择的有趣之处在于，它与预测变量具有相同的单位和尺度，使得它更加直观。最后，RMSE等同于标准差或误差。标准差与误差分布之间的联系非常有意义，你可以用其他度量来总结误差分布，比如标准误差或置信区间（我们在*第二章*《使用Python进行时间序列分析》中讨论过这两者）。
- en: 'There are many more metrics and they all have their purpose. The following
    table sums up a few more popular error metrics in time-series modeling:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的指标，它们各自有其作用。下表总结了时间序列建模中一些更常见的误差指标：
- en: '| Metric Name | Definition |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 定义 |'
- en: '| Median absolute error | ![](img/B17577_04_035.png) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 中位数绝对误差 | ![](img/B17577_04_035.png) |'
- en: '| Mean absolute percentage error | ![](img/B17577_04_036.png) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 均值绝对百分比误差 | ![](img/B17577_04_036.png) |'
- en: '| Symmetric mean absolute percentage error | ![](img/B17577_04_037.png) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 对称均值绝对百分比误差 | ![](img/B17577_04_037.png) |'
- en: '| Normalized mean squared error | ![](img/B17577_04_038.png) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 归一化均方误差 | ![](img/B17577_04_038.png) |'
- en: 'Figure 4.11: More metrics for regression'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11：更多回归指标
- en: The **median absolute error (MdAE)** is similar to the MAE. However, instead
    of the mean operation for integration, a different average, the median, is employed.
    Since the median is unaffected by values at the tails, this measure is even more
    robust than the MAE.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**中位数绝对误差 (MdAE)** 类似于MAE。但是，与均值操作不同，中位数被用来进行集成。由于中位数不受尾部数据的影响，因此这个度量比MAE更加稳健。'
- en: The **mean percentage error (MAPE)** is the mean average error normalized by
    the target. 0 represents a perfect model, and higher than 1 means the model's
    predictions are systematically higher than the targets. The MAPE doesn't have
    an upper bound. Additionally, since it deals with percentages in terms of the
    target (scaling or dividing by the targets), positive and negative residuals are
    treated differently. As a result, if the prediction is bigger than the target,
    the MAPE is higher than for the same error in the other direction. Therefore,
    depending on the sign of the residual, the MAPE is higher or lower!
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**均值百分比误差 (MAPE)** 是通过目标值标准化的平均误差。0代表完美的模型，值大于1意味着模型的预测系统性地高于目标值。MAPE没有上限。此外，由于它是基于目标的百分比（通过目标进行缩放或除法运算），正残差和负残差会被区别对待。因此，如果预测值大于目标值，MAPE会比同样方向的误差要高。根据残差的符号，MAPE会更高或更低！'
- en: The common choice for the denominator is the target; however, you can also scale
    by the mean of the prediction and target. This is called the **symmetric mean
    absolute percentage error (SMAPE)**. The SMAPE has not only a lower bound but
    also an upper bound, which makes the percentage much easier to interpret.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的分母选择是目标值；但是，你也可以通过预测值和目标值的均值进行缩放。这被称为**对称均值绝对百分比误差 (SMAPE)**。SMAPE不仅有下限，还有上限，这使得百分比更加易于理解。
- en: Scaling can have different benefits as well. If you want to compare models validated
    on different datasets, the measures presented before wouldn't be helpful. The
    split between training, validation, and test sets is randomized, so when you compare
    model performances, any of these measures would confound the effects of dataset
    variance in the validation set and the effect of the model performance itself.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放也可以带来不同的好处。如果你想比较在不同数据集上验证的模型，那么之前提到的度量方法将无法提供帮助。训练集、验证集和测试集的划分是随机的，因此，当你比较模型的表现时，这些度量方法会混淆数据集差异在验证集中的影响以及模型性能本身的影响。
- en: Therefore, the **normalized mean squared error (NMSE)** is particularly intuitive
    as a presentation choice over the MSE because it scales the model's performance
    with the deviation. The NMSE normalizes the MSE obtained after dividing it by
    the target variance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**归一化均方误差 (NMSE)** 作为一种展示选择，比MSE更直观，因为它根据偏差对模型的表现进行缩放。NMSE在将MSE除以目标方差后进行归一化。
- en: There are lots of other error measures. Some error measures compare predictions
    against predictions of a naïve model that returns the average target value.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的误差度量。某些误差度量将预测值与返回平均目标值的简单模型的预测进行比较。
- en: 'The prediction performance of this naïve model is:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该简单模型的预测性能为：
- en: '![](img/B17577_04_039.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_039.png)'
- en: We can normalize prediction errors by dividing the prediction error by this
    naïve prediction error.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将预测误差除以该简单模型的预测误差来对预测误差进行归一化。
- en: 'This way, we can define several other measures:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以定义几个其他的度量：
- en: '| **Metric Name** | **Definition** |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| **指标名称** | **定义** |'
- en: '| Mean relative absolute error | ![](img/B17577_04_040.png) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 均值相对绝对误差 | ![](img/B17577_04_040.png) |'
- en: '| Median relative absolute error | ![](img/B17577_04_041.png) |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 中位数相对绝对误差 | ![](img/B17577_04_041.png) |'
- en: '| Relative root mean squared error | ![](img/B17577_04_042.png) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 相对均方根误差 | ![](img/B17577_04_042.png) |'
- en: 'Figure 4.12: Normalized regression metrics'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12：归一化回归指标
- en: All these measures should be intuitive if you have the idea of the naïve model
    in mind and you want to compare the performance of the naïve model with the same
    error metric by dividing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你心中有朴素模型的概念，并且想通过相同的误差度量来比较朴素模型的表现，那么这些度量应该是直观的。
- en: The **mean relative absolute error** (**MRAE**) is very similar to the coefficient
    of determination, with the only difference being that the MRAE takes the averages
    rather than the sums.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**均值相对绝对误差**（**MRAE**）与决定系数非常相似，唯一的区别是 MRAE 使用平均值而不是总和。'
- en: Another error is the **root mean squared logarithmic error** (**RMSLE**).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个误差是**均方根对数误差**（**RMSLE**）。
- en: '| **Metric Name** | **Definition** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **度量名称** | **定义** |'
- en: '| Root mean squared logarithmic error | ![](img/B17577_04_043.png) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 均方根对数误差 | ![](img/B17577_04_043.png) |'
- en: 'Figure 4.13: Root mean squared logarithmic error'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13：均方根对数误差
- en: In the case of the RMSLE, you take the log of the residuals as the basic operation.
    This is to avoid penalizing large differences in the error when both predicted
    and true values are very high values. Because of the inflection point of the logarithm
    at 1, the RMLSE has the unique property that it penalizes the underestimation
    of the actual value more severely than it does for overestimation. This can be
    useful when the distributions of errors don't follow a normal distribution similar
    to the scaling operations that we've discussed in *Chapter 3*, *Preprocessing
    Time-Series*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RMSLE，基本操作是对残差取对数。这是为了避免在预测值和真实值都很大的情况下，误差的巨大差异被惩罚得过重。由于对数在 1 处有拐点，RMSLE
    具有一个独特的特性，它对低估实际值的惩罚比对高估实际值的惩罚更严重。这在误差分布不遵循正态分布时非常有用，类似于我们在*第 3 章*，*时间序列预处理*中讨论的缩放操作。
- en: We could extend the number of metrics if we take into account metrics based
    on entropy, such as Theil's Uncertainty. **Theil's U** is a normalized measure
    of the total prediction error. U is between 0 and 1, where 0 means a perfect fit.
    It is based on the concept of conditional entropy, and can also be used as a measure
    of uncertainty or even as a correlation measure in categorical-categorical cases.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑基于熵的度量，比如泰尔不确定性，我们可以扩展度量的数量。**泰尔不确定性（Theil's U）**是一个标准化的预测误差度量。U 的值介于
    0 和 1 之间，其中 0 表示完美拟合。它基于条件熵的概念，还可以作为不确定性度量，甚至可以作为类别-类别情况下的相关性度量。
- en: As these headers say, the first two concentrate on quantifying the performance
    of models. The last section is useful for distance-based models, which are often
    used as a solid baseline for performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如这些标题所示，前两项专注于量化模型的表现。最后一部分对于基于距离的模型非常有用，这些模型通常作为性能的坚实基准。
- en: Let's switch over to error metrics for classification tasks.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们切换到分类任务的误差度量。
- en: Classification
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: Many metrics are specific to more binary classification (where there are exactly
    two classes), although some of them can be extended to the case of multi-class
    classification, where the number of classes is bigger than two.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 许多度量特定于二分类任务（即只有两个类别），尽管其中一些可以扩展到多类别分类任务，其中类别数大于两个。
- en: 'In binary classification, we can contrast the prediction against the actual
    outcome in a **confusion matrix**, where predictions and actual outcomes are cross-tabulated
    like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类中，我们可以通过**混淆矩阵**将预测结果与实际结果进行对比，其中预测结果与实际结果会交叉列出，如下所示：
- en: '|  |  | **Actual outcome** |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **实际结果** |'
- en: '|  |  | false | true |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 假阴性（TN） | 真阳性（TP） |'
- en: '| **Predicted outcome** | false | true negative (TN) | false negative (FN)
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **预测结果** | 假阴性（TN） | 假阳性（FP） | 真阳性（TP） |'
- en: '| true | false positive (FP) | true positive (TP) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 真正 | 假阳性（FP） | 真阳性（TP） |'
- en: 'Figure 4.14: Confusion matrix'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14：混淆矩阵
- en: This is a crucial visualization for classification tasks, and many measures
    are based on summarizing this.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分类任务中的一个关键可视化图表，许多度量都是基于对这一图表的总结来进行的。
- en: 'Two of the most important metrics for classification are precision and recall.
    **Recall** is the ratio of the number of correctly predicted positive instances
    across all positive instances. We can also state this in terms of the confusion
    matrix as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 分类任务中最重要的两个度量是精确度和召回率。**召回率**是正确预测的正实例数与所有正实例数之比。我们也可以在混淆矩阵中按如下方式表达：
- en: '![](img/B17577_04_044.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_044.png)'
- en: 'Recall is also called the **true positive rate** or **sensitivity**. It focuses
    on the true predictions, ignoring the negative instances; however, we might also
    want to know how accurate the positive predictions are. This is **precision**
    defined as:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率也叫做**真正正例率**或**灵敏度**。它关注的是正确预测的正例，忽略了负例；然而，我们也可能想知道正例预测的准确性。这就是**精度**，定义如下：
- en: '![](img/B17577_04_045.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_045.png)'
- en: 'We can visualize these two metrics as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式可视化这两个指标：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 12)/Screenshot 2021-04-26 at 00.05.38.png](img/B17577_04_09.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 12)/Screenshot 2021-04-26 at 00.05.38.png](img/B17577_04_09.png)'
- en: 'Figure 4.15: False positives (FP), true positives (TP), and false negatives
    (FN)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15：假正例（FP）、真正正例（TP）和假负例（FN）
- en: In this graph, **false positives** (**FP**), **true positives** (**TP**), and
    **false negatives** (**FN**) are shown. You can see instances that are actually
    true, instances that are classified as true by the model, and the intersection
    of the two – instances that are true and that the model classifies as true.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，**假正例**（**FP**）、**真正正例**（**TP**）和**假负例**（**FN**）被显示出来。你可以看到实际上为真的实例，模型将其分类为真的实例，以及两者的交集——既为真的实例，且被模型分类为真的实例。
- en: We can quickly count and calculate precision and recall. We have four true positives
    and six false positives. The precision for this example is therefore ![](img/B17577_04_046.png).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速计算精度和召回率。我们有四个真正的正例和六个假正例。因此，此示例的精度为！[](img/B17577_04_046.png)。
- en: We have four false negatives. The recall is ![](img/B17577_04_047.png).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有四个假负例。召回率为！[](img/B17577_04_047.png)。
- en: 'Both recall and precision are obviously important, so why not integrate them?
    The ![](img/B17577_04_048.png) score is the harmonic mean of precision and sensitivity:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率和精度显然都很重要，那么为什么不将它们结合起来呢？![](img/B17577_04_048.png)得分是精度和灵敏度的调和平均值：
- en: '![](img/B17577_04_049.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_049.png)'
- en: 'We can also parametrize the relative importance of recall and precision. This
    is a generalized version of the ![](img/B17577_04_050.png) score, the ![](img/B17577_04_051.png)
    score:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以参数化召回率和精度的相对重要性。这是![](img/B17577_04_050.png)得分的广义版本，即![](img/B17577_04_051.png)得分：
- en: '![](img/B17577_04_052.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_052.png)'
- en: 'Another very useful metric comes from the **receiver operator curve (ROC)**,
    which plots the **true positive rate** (**TPR**) against the **false positive
    rate** (**FPR**) at various threshold settings. The **false positive rate**, also
    called the **false alarm ratio**, is defined analogously to the true positive
    rate (recall) as:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常有用的指标来自**接收者操作特征曲线（ROC）**，它绘制了在不同阈值设置下，**真正正例率**（**TPR**）与**假正例率**（**FPR**）之间的关系。**假正例率**，也称为**假警报率**，与真正正例率（召回率）的定义类似，如下所示：
- en: '![](img/B17577_04_053.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_053.png)'
- en: An ROC graph shows the relationship between sensitivity and specificity, with
    the general idea being that it is very hard to do both and find all positive instances
    (sensitivity) and to do them correctly. More often than not, you have to compromise
    between sensitivity and specificity. This plot illustrates how well your model
    maneuvers this issue. The **area under the curve** summarizes the plot and is
    a metric that's often used in practice.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ROC图展示了灵敏度和特异性之间的关系，其基本思路是，很难做到两者兼得，同时找到所有正实例（灵敏度）并做到正确分类。往往你需要在灵敏度和特异性之间做出折衷。这个图形展示了你的模型如何处理这个问题。**曲线下面积**总结了这个图形，是实践中常用的一个指标。
- en: 'Another less common metric is the **Correlation Ratio**, which was introduced
    by Karl Pearson as a measure of categorical-continuous association:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不太常见的指标是**相关比率**，它由卡尔·皮尔逊（Karl Pearson）提出，作为分类-连续型关联的度量：
- en: '![](img/B17577_04_054.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_054.png)'
- en: 'where ![](img/B17577_04_055.png) is the number of observations in category
    *x*, and we define:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/B17577_04_055.png)是类别*x*中观测值的数量，我们定义为：
- en: '![](img/B17577_04_056.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_056.png)'
- en: The correlation ratio is based on the variance within individual categories
    and the variance across the whole population. ![](img/B17577_04_057.png) is in
    the range [0,1] where 0 means a category is not associated, and 1 means a category
    is associated with absolute certainty.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 相关比率是基于各个类别内的方差与整个数据集的方差之间的比例。！[](img/B17577_04_057.png)的取值范围为[0,1]，其中0表示该类别无关联，1表示该类别与绝对确定性相关。
- en: In the next section, we'll examine similarity measures between time-series.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将考察时间序列之间的相似度度量。
- en: Comparing time-series
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较时间序列
- en: Similarity measures have applications in time-series indexing for retrieval
    in search, clustering, forecasting, regression, and classification, but if we
    want to decide whether two temporal sequences are similar, how do we measure the
    similarity?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度度量在时间序列索引、搜索检索、聚类、预测、回归和分类中有广泛应用，但是如果我们想决定两个时间序列是否相似，应该如何衡量相似度呢？
- en: The simplest would be to use the Pearson correlation coefficient; however, other
    measures can be more informative.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方式是使用皮尔逊相关系数；然而，其他度量方法可能更具信息性。
- en: 'We''ll go through a series of measures to compare a pair of time-series:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一系列度量来比较一对时间序列：
- en: Euclidean distance
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Dynamic time warping
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态时间规整
- en: Granger causality
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格兰杰因果关系
- en: 'The **Euclidean distance**, a generic distance, is applicable to any pair of
    vectors, including time-series:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**欧几里得距离**，一种通用距离，适用于任何一对向量，包括时间序列：'
- en: '![](img/B17577_04_058.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_058.png)'
- en: The Euclidean distance can be useful; however, in practice, for time-series
    you can do better. You can take the Euclidean distance over the time-series that
    has been transformed by the fast Fourier transformed to a frequency domain.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离在某些情况下是有用的；然而，在实践中，对于时间序列，你可以做得更好。你可以对经过快速傅里叶变换到频域的时间序列使用欧几里得距离。
- en: Intuitively, the exact time position and its duration of events in time-series
    can vary. **Dynamic time warping** (**DTW**) is one of the algorithms for measuring
    similarity between two temporal sequences, which may vary in speed. Intuitively,
    the exact time position and its duration of events in time-series can vary. A
    similarity measure between time-series should be able to deal with these kinds
    of shifts and elongations.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，时间序列中事件的精确时间位置及其持续时间可能会有所不同。**动态时间规整**（**DTW**）是衡量两个可能在速度上有所不同的时间序列相似性的算法之一。直观地，时间序列中事件的精确时间位置及其持续时间可能会有所不同。时间序列之间的相似度度量应该能够处理这些时间的位移和延长。
- en: In general, DTW is a method that calculates an optimal match between two given
    time sequences with certain restrictions and rules according to a heuristic. Basically,
    it attempts to match indexes from the first sequence to indexes from the other
    sequence. DTW is an edit distance – it expresses the cost of transforming a sequence
    t1 into t2.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，DTW是一种根据启发式方法计算两个给定时间序列之间最佳匹配的算法，具有某些限制和规则。基本上，它尝试将第一个序列的索引与另一个序列的索引匹配。DTW是一种编辑距离——它表示将序列t1转换为t2的代价。
- en: DTW has been applied to automatic speech recognition because of its ability
    to cope with different speeds. DTW, however, fails at quantifying dissimilarity
    between non-matching sequences.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DTW能够处理不同的速度，它已经被应用于自动语音识别。然而，DTW在量化不匹配序列之间的不相似性方面存在问题。
- en: DTW is applied to each feature dimension independently and then the distances
    can be summed up. Alternatively, the warping can cover all features simultaneously
    by calculating the distance between two points as the Euclidean distance across
    all dimensions. Thus, this Dependent Warping (![](img/B17577_04_059.png)) is a
    multivariate approach.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: DTW应用于每个特征维度，然后可以将距离相加。或者，规整可以通过计算两个点之间的欧几里得距离来同时覆盖所有特征。因此，这种依赖规整（![](img/B17577_04_059.png)）是一种多变量方法。
- en: '**Granger causality** determines if a time-series can help to forecast another
    time-series. Although the question of true causality in the measure is debatable,
    the measure considers values of one series prior in time to values of the other,
    and it can be argued that the measure shows a temporal relationship or a relationship
    in the predictive sense.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**格兰杰因果关系**用于判断一个时间序列是否能够帮助预测另一个时间序列。尽管该度量中的真正因果关系问题存在争议，但该度量考虑了一个序列在时间上先于另一个序列的值，可以认为该度量显示了时间关系或预测意义上的关系。'
- en: 'Granger causality is quite intuitive in both its idea and its formulation.
    Its two principles are (simplified):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 格兰杰因果关系在其思想和公式中都非常直观。它的两个原则（简化版）是：
- en: The cause must precede the effect
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原因必须先于结果
- en: The cause has a unique effect on the result
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原因对结果有独特的影响
- en: Therefore, if we can fit a model that shows that X and Y have a relationship
    in which Y systematically follows X, this is taken to mean that X Granger causes
    Y.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们可以拟合一个模型，表明X和Y之间存在一种关系，其中Y系统地跟随X，这意味着X对Y具有格兰杰因果关系。
- en: Machine learning algorithms for time-series
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于时间序列的机器学习算法
- en: An important distinction in machine learning for time-series is the one between
    univariate and multivariate, in which algorithms are univariate, which means that
    they can only work with a single feature, or multi-variate, which means that they
    work with many features.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列机器学习中的一个重要区别是单变量与多变量的区别，单变量算法只能处理单一特征，而多变量算法则可以处理多个特征。
- en: In univariate datasets, each case has a single series and a class label. Earlier
    models (classical modeling) focused on univariate datasets and applications. This
    is also reflected in the availability of datasets.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在单变量数据集中，每个案例只有一个序列和一个类别标签。早期的模型（经典建模）侧重于单变量数据集和应用。这也体现在数据集的可用性上。
- en: One of the most important repositories for time-series datasets, the **UCR**
    (**University of California, Riverside**) archive, which was released first in
    2002, has provided a valuable resource for univariate time-series. It now contains
    about 120 datasets, but is lacking multivariate datasets. Furthermore, the M competitions
    (especially M3, 4, and 5) have a lot of available time-series datasets.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据集最重要的存储库之一是**UCR**（**加利福尼亚大学河滨分校**）档案，首次发布于2002年，已为单变量时间序列提供了宝贵的资源。现在它包含大约120个数据集，但缺少多变量数据集。此外，M竞赛（特别是M3、M4和M5）提供了大量的时间序列数据集。
- en: Multivariate time-series are datasets that have multiple feature dimensions.
    Many real-life datasets are inherently multivariate – multivariate cases are much
    more frequent in practice than univariate. Examples include human activity recognition,
    diagnoses based on an **electrocardiogram** (**ECG**), **electroencephalogram**
    (**EEG**), **magnetoencephalography** (**MEG**), and systems monitoring.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量时间序列是具有多个特征维度的数据集。许多现实生活中的数据集本质上是多变量的——在实践中，多变量情况比单变量情况要常见得多。示例包括人类活动识别、基于**心电图**（**ECG**）、**脑电图**（**EEG**）、**脑磁图**（**MEG**）的诊断以及系统监控。
- en: Only recently (Anthony Bagnall and others, 2018) created the **UAE** (**University
    of East Anglia**) archive with 30 multivariate datasets. Another archive for multivariate
    datasets is the MTS archive.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最近（Anthony Bagnall等人，2018年）创建了**UAE**（**东英吉利大学**）档案，包含30个多变量数据集。另一个多变量数据集档案是MTS档案。
- en: In the next section, we'll briefly discuss distance-based approaches.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要讨论基于距离的方法。
- en: Distance-based approaches
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于距离的方法
- en: In the k-nearest-neighbor approaches (kNN for short), which we mentioned earlier,
    training examples are stored, and then, at inference time when a prediction for
    a new data point is required, the prediction is based on the closest k neighbors.
    This requires a distance measure between examples.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前提到的k近邻方法（简称kNN）中，训练样本被存储，然后在推理时，当需要对一个新的数据点进行预测时，预测是基于最接近的k个邻居。这需要一种样本之间的距离度量。
- en: I've introduced two measures for time-series, **Dynamic Time Warping** (**DTW**)
    and Euclidean distances, earlier in this chapter. Many distance-based approaches
    take either of these as a distance measure.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章早些时候介绍了用于时间序列的两种度量方法，**动态时间规整**（**DTW**）和欧几里得距离。许多基于距离的方法采用这些作为距离度量。
- en: Another approach that has been tried is extracting features from time-series,
    and then storing these extracted features for retrieval with kNN. These features
    include shapelets or **scale-invariant features** (**SIFT**). SIFT features are
    extracted from time-series as shapes surrounding the extremum (Adeline Bailly
    and others, 2015).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种尝试过的方法是从时间序列中提取特征，然后将这些提取的特征存储起来，通过kNN进行检索。这些特征包括形状特征或**尺度不变特征**（**SIFT**）。SIFT特征是从时间序列中提取的形状，围绕着极值点（Adeline
    Bailly等人，2015年）。
- en: We've discussed shapelets and ROCKET in separate sections in *Chapter 3*, *Preprocessing
    Time-Series*, so we'll keep their descriptions brief, but focus on their applications
    in machine learning.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*《时间序列预处理》中已经分别讨论了形状特征和ROCKET，因此我们将简要描述它们，但重点放在它们在机器学习中的应用。
- en: Shapelets
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 形状特征
- en: 'We''ve discussed shapelets in *Chapter 3*, *Preprocessing Time-Series*, so
    we''ll keep it brief here. Shapelets for time-series were presented in the research
    paper "*Time-Series Shapelets*: *a novel technique that allows accurate, interpretable
    and fast classification*" (Lexiang Ye and Eamonn Keogh, 2011). The basic idea
    of shapelets is decomposing the time-series into discriminative subsections (called
    **Shapelets**). A few methods have been presented that are based on shapelet features.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*中已讨论过形状元（shapelets），*时间序列预处理*，因此在这里简要介绍。时间序列的形状元在研究论文“*时间序列形状元*：*一种新技术，可以实现准确、可解释且快速的分类*”（Lexiang
    Ye 和 Eamonn Keogh，2011）中提出。形状元的基本思想是将时间序列分解为具有判别力的子部分（称为**形状元**）。基于形状元特征，已经提出了一些方法。
- en: The **Shapelet Transform Classifier** (**STC**; Hills and others, 2014) consists
    of taking the shapelets as feature transformation and then feeding the shapelets
    into a machine learning algorithm. They tested the C4.5 decision tree, Naïve Bayes,
    1NN, SVM, and a rotation forest, but didn't find any significant differences between
    these methods in a classification setting.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '**形状元变换分类器**（**STC**；Hills等人，2014）由将形状元作为特征转换，然后将形状元输入到机器学习算法中组成。他们测试了C4.5决策树、朴素贝叶斯、1NN、SVM和旋转森林，但在分类设置中没有发现这些方法之间的显著差异。'
- en: The **Generalized random shapelet forest** (**gRFS**; Karlsson and others, 2016)
    follows the idea of the random forest. Each tree is built on a distinct set of
    shapelets of random length, which are extracted from one random dimension for
    each tree. A decision tree is trained on top of these shapelets. These random
    shapelet trees are then integrated as the ensemble model, which is the gRFS.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**广义随机形状元森林**（**gRFS**；Karlsson等人，2016）遵循随机森林的思路。每棵树是基于一组随机长度的形状元构建的，这些形状元从每棵树的一个随机维度中提取。然后在这些形状元上训练决策树。这些随机形状元树被整合为集成模型，构成gRFS。'
- en: ROCKET
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROCKET
- en: We've explained ROCKET in *Chapter 3*, *Preprocessing Time-Series*. Each input
    feature gets transformed separately by 10,000 random kernels (this number can
    be changed). In practice, this is a very fast process. These transformed features
    can be fed into a machine learning algorithm. Its inventors, Angus Dempster, François
    Petitjean, and Geoff Webb, recommended a linear model in the original publication
    (2019).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*中已解释过ROCKET，*时间序列预处理*。每个输入特征通过10,000个随机内核单独转换（这个数字可以更改）。在实践中，这是一个非常快速的过程。这些转换后的特征可以输入到机器学习算法中。其发明者Angus
    Dempster、François Petitjean和Geoff Webb在原始出版物（2019年）中推荐了线性模型。
- en: 'Recently, a new variant, MINIROCKET, was published that is about 75 times faster
    than ROCKET while maintaining roughly the same accuracy – *MINIROCKET: A Very
    Fast (Almost) Deterministic Transform for Time-Series Classification* (*Angus
    Dempster, Daniel F. Schmidt, and Geoff Webb, 2020*).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，发布了一个新的变体MINIROCKET，其速度是ROCKET的约75倍，同时保持大致相同的准确性——*MINIROCKET：一个非常快速（几乎）确定性的时间序列分类变换*（*Angus
    Dempster, Daniel F. Schmidt, 和 Geoff Webb, 2020*）。
- en: In machine learning research, **critical difference (CD) diagrams** are a powerful
    visualization tool for comparing outcomes of multiple algorithms. The average
    ranks indicate how algorithms stack up in relation to each other (a lower rank
    is better). Algorithmic results are compared statistically – a horizontal line
    links algorithms, where the differences between them can't be statistically separated.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习研究中，**关键差异（CD）图**是比较多个算法结果的强大可视化工具。平均排名表示算法之间的相对表现（排名越低越好）。算法结果通过统计方法进行比较——一条水平线连接算法，表示它们之间的差异无法在统计学上分开。
- en: 'Here''s a critical difference diagram that illustrates the comparative performances
    of MiniRocket with other algorithms (from the MiniRocket repo by Dempster and
    others):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个关键差异图，展示了MiniRocket与其他算法的比较性能（来自Dempster等人发布的MiniRocket代码库）：
- en: '![](img/B17577_04_10.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_10.png)'
- en: 'Figure 4.16: Mean rank of MiniRocket in terms of accuracy versus other state-of-the-art
    approaches on 109 datasets of the UCR archive'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16：MiniRocket在109个UCR数据集上的准确率与其他最先进方法的平均排名
- en: The numbers show the rank of the algorithms across 109 datasets in the test.
    We can see that MiniRocket is better than Rocket, but worse than TS-CHIEF and
    HIVE-COTE, although the difference between them is not statistically significant.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字显示了算法在109个数据集上的排名。我们可以看到，MiniRocket优于Rocket，但不如TS-CHIEF和HIVE-COTE，尽管它们之间的差异在统计学上并不显著。
- en: We'll discuss InceptionTime in *Chapter 10*, *Deep Learning for Time-Series*.
    Some of the other methods mentioned will be introduced in the following sections.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第10章*，*时间序列深度学习*中讨论InceptionTime。其他提到的方法将在接下来的章节中介绍。
- en: Time-Series Forest and Canonical Interval Forest
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列森林和典型区间森林
- en: The main innovation of the **Time-Series Forest** (**TSF**; by Houtao Deng and
    others, 2013) was the introduction of the entrance gain as a split criterion for
    the tree nodes. They showed that an ensemble classifier based on simple features
    such as mean, deviation, and slope outperforms 1NN classifiers with DTW while
    being computationally efficient (due to parallelism).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间序列森林**（**TSF**；由Houtao Deng等人于2013年提出）的主要创新是引入了作为树节点分割标准的入口增益。他们展示了基于简单特征（如均值、偏差和斜率）的集成分类器，优于使用动态时间规整（DTW）的1NN分类器，同时在计算上也更为高效（由于并行处理）。'
- en: The **Proximity Forest (PF)**, introduced by a group of researchers lead by
    Geoff Webb, is a tree ensemble based on the similarity of each time-series to
    a set of reference time-series (distance-based features). They found that PF attains
    a classification performance comparable to BOSS and Shapelet Transforms.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**接近森林（PF）**，由Geoff Webb领导的一组研究人员提出，是一个基于每个时间序列与一组参考时间序列相似度（基于距离的特征）的树集成。他们发现PF的分类性能与BOSS和形状转换（Shapelet
    Transforms）相当。'
- en: '**TS-CHIEF**, short for **Time-Series Combination of Heterogeneous and Integrated
    Embedding Forest**, comes from the same group (Ahmed Shifaz, Charlotte Pelletier,
    François Petitjean, and Geoff Webb, 2020), and it extends PF with dictionary-based
    (BOSS) and interval-based (RISE) splitters while keeping the original features
    introduced with PF. The authors claim that depending on the dataset size, it can
    run between 900 times and 46,000 times faster than HIVE-COTE.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**TS-CHIEF**，即**时间序列异质与集成嵌入森林**（Time-Series Combination of Heterogeneous and
    Integrated Embedding Forest）的缩写，来自同一组研究人员（Ahmed Shifaz、Charlotte Pelletier、François
    Petitjean和Geoff Webb，2020年），它通过引入基于字典的（BOSS）和基于区间的（RISE）分割器，扩展了PF，同时保持了PF中引入的原始特征。作者声称，根据数据集的大小，它的运行速度比HIVE-COTE快900倍到46,000倍不等。'
- en: The idea of the **Canonical Interval Forest** (**CIF**; by Matthew Middlehurst,
    James Large, and Anthony Bagnall, 2020) was to extend the TSF with the catch22
    features. It is an ensemble of time-series trees based on the 22 Catch22 features
    and summary statistics extracted from phase-dependent intervals. They also used
    the entrance gain criterion for the trees.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**典型区间森林**（**CIF**；由Matthew Middlehurst、James Large和Anthony Bagnall于2020年提出）的理念是通过引入Catch22特征来扩展TSF。它是一个基于22个Catch22特征和从相位依赖区间提取的总结统计量的时间序列树集成。他们还为树使用了入口增益标准。'
- en: In the next section, we describe the evolution of symbolic approaches, from
    BOSS to the **Temporal Dictionary Ensemble** (**TDE**).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述符号方法的演变，从BOSS到**时间字典集成**（**TDE**）。
- en: Symbolic approaches
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号方法
- en: Symbolic approaches are methods that transform a numeric time-series to symbols
    from an alphabet.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 符号方法是将数值时间序列转换为字母表符号的方法。
- en: '**Symbolic Aggregate ApproXimation** (**SAX**) was first published by Eamonn
    Keogh and Jessica Lin in 2002\. It extends **Piecewise Aggregate Approximation**
    (**PAA**), which calculates averages within equal segments of the time-series.
    In SAX, these averages are then quantized (binned), so the alphabet corresponds
    to intervals of the original numerical values. The two important parameters are
    the number of segments in PAA and the number of bins.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**符号聚合近似法**（**SAX**）首次由Eamonn Keogh和Jessica Lin于2002年提出。它扩展了**分段聚合近似法**（**PAA**），后者计算时间序列中各相等片段内的均值。在SAX中，这些均值被量化（分箱），因此字母表对应于原始数值区间。两个重要的参数是PAA中的片段数和箱的数量。'
- en: 'The plot below (from Thach Le Nguyen''s MrSEQL repository on GitHub) illustrates
    how SAX works:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 下图（来自Thach Le Nguyen的MrSEQL GitHub库）展示了SAX的工作原理：
- en: '![https://github.com/lnthach/Mr-SEQL/raw/master/figs/sax_demo.png](img/B17577_04_11.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![https://github.com/lnthach/Mr-SEQL/raw/master/figs/sax_demo.png](img/B17577_04_11.png)'
- en: 'Figure 4.17: SAX'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17：SAX
- en: You can see the segments as a grid along the *x* axis and the bins as a grid
    along the *y* axis. Each segment is then replaced with its mean value. The time-series
    is discretized by replacing it in each segment with the bin ID (letter in the
    plot).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这些片段视为沿*X*轴排列的网格，将这些箱视为沿*Y*轴排列的网格。每个片段都会被其均值所替代。时间序列通过在每个片段中用箱ID（图中的字母）替代，从而被离散化。
- en: '**Symbolic Fourier Approximation** (**SFA**; Patrick Schäfer and Mikael Högqvist,
    2012) also transforms a time-series to a symbolic representation, but using the
    frequency domain. The dimensionality of the dataset is first reduced by performing
    Discrete Fourier Transformation, low-pass filtered, and then quantized.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**符号傅里叶近似**（**SFA**；Patrick Schäfer 和 Mikael Högqvist，2012）也将时间序列转换为符号表示，但使用频域。首先，通过执行离散傅里叶变换、低通滤波，然后量化，从而减少数据集的维度。'
- en: The **Bag of SFA Symbols (BOSS;** Patrick Schäfer, 2015 and 2016) is based on
    histograms of n-grams to form a **bag-of-patterns** (**BoP**) from SFA representations.
    BOSS has been extended as **BOSS in Vector Space** (**BOSS VS**). The BOSS VS
    classifier is one to four orders of magnitude faster than the state of the art
    and significantly more accurate than the 1-NN DTW.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**SFA符号包（BOSS；**Patrick Schäfer，2015 和 2016）基于n-gram的直方图，通过SFA表示来形成**模式包**（**BoP**）。BOSS已经扩展为**向量空间中的BOSS**（**BOSS
    VS**）。BOSS VS分类器比现有的最先进技术快一到四个数量级，并且比1-NN DTW更为准确。'
- en: Contract BOSS (cBOSS; Matthew Middlehurst, William Vickers, and Anthony Bagnall,
    2019) speeds up BOSS by introducing a new parameter limiting the number of base
    models.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 合同BOSS（cBOSS；Matthew Middlehurst，William Vickers，和 Anthony Bagnall，2019）通过引入限制基模型数量的新参数来加速BOSS。
- en: '**SEQL** (Thach Le Nguyen, Severin Gsponer, and Georgiana Ifrim, 2017) is a
    symbolic sequence learning algorithm that selects the most discriminative subsequences
    for a linear model using greedy gradient descent. This is illustrated here (from
    the MrSEQL GitHub repo):'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**SEQL**（Thach Le Nguyen，Severin Gsponer，和 Georgiana Ifrim，2017）是一种符号序列学习算法，利用贪婪的梯度下降方法选择最具辨别性的子序列以供线性模型使用。如下所示（来自MrSEQL
    GitHub仓库）：'
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 11)/Screenshot 2021-04-25 at 23.48.05.png](img/B17577_04_12.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/(A Document
    Being Saved By screencaptureui 11)/Screenshot 2021-04-25 at 23.48.05.png](img/B17577_04_12.png)'
- en: 'Figure 4.18: SEQL'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18：SEQL
- en: The multiple representation sequence learner (**MrSEQL**; Thach Le Nguyen, Severin
    Gsponer, Iulia Ilie, and Georgiana Ifrim, 2019) is extending SEQL by selecting
    transformed features across multiple resolutions and multiple domains.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 多重表示序列学习器（**MrSEQL**；Thach Le Nguyen，Severin Gsponer，Iulia Ilie，和 Georgiana
    Ifrim，2019）通过在多个分辨率和多个领域中选择转换特征，扩展了SEQL。
- en: '**WEASEL+MUSE** (Patrick Schäfer and Ulf Leser, 2017 and 2018) consists of
    two stages. WEASEL stands for **Word Extraction for Time-Series Classification**,
    while MUSE stands for **Multivariate Unsupervised Symbols and Derivatives**. This
    deserves emphasizing – while WEASEL is a univariate method, MUSE extends the method
    for multivariate problems.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**WEASEL+MUSE**（Patrick Schäfer 和 Ulf Leser，2017 和 2018）由两个阶段组成。WEASEL代表**时间序列分类的词提取**，而MUSE代表**多变量无监督符号与导数**。这一点值得强调——虽然WEASEL是单变量方法，MUSE将此方法扩展到了多变量问题。'
- en: In a first step, WEASEL derives features from windows at multiple lengths from
    the truncated Fourier transform and discretization. This acts in a way similar
    to a low-pass filter, keeping only the first *l* coefficients. These coefficients
    are then discretized into an alphabet of fixed size and counted as **Bag-of-Patterns**
    (**BOP**) in histograms. This is done in isolation for each feature.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，WEASEL从多个长度的窗口中提取特征，这些窗口来自截断的傅里叶变换和离散化。这一过程类似于低通滤波器，仅保留前*l*个系数。然后，这些系数被离散化为固定大小的字母表，并在直方图中计数为**模式包**（**BOP**）。这是为每个特征单独完成的。
- en: In a second step (MUSE), the histogram features are concatenated across dimensions,
    and a statistical test, the χ2 test, is used for filter-based feature selection,
    resulting in a much smaller but more discriminative feature set.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步（MUSE）中，直方图特征在不同维度上进行连接，并使用统计检验（χ2检验）进行基于滤波器的特征选择，结果是一个更小但更具辨别力的特征集。
- en: These BOPs are then fed into a logistic regression algorithm for classification.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些BOP被输入到逻辑回归算法中进行分类。
- en: HIVE-COTE
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HIVE-COTE
- en: The **Hierarchical Vote Collective of Transformation-Based Ensembles** (**HIVE-COTE**)
    is the current state of the art in terms of classification accuracy.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于转换集成的分层投票集体**（**HIVE-COTE**）是当前分类准确度方面的最先进技术。'
- en: 'Proposed in 2016 and adapted in 2020 (Anthony Bagnall and others, 2020), it''s
    an ensemble method that combines a heterogeneous collection of different methods:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在2016年提出，并在2020年进行了调整（Anthony Bagnall 等人，2020），它是一种集成方法，结合了不同方法的异质集合：
- en: '**Shapelet Transform Classifier** (**STC**)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状转换分类器**（**STC**）'
- en: '**Time-Series Forest** (**TSF**)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列森林** (**TSF**)'
- en: '**Contractable Bag of Symbolic-Fourier Approximation Symbols** (**CBOSS**)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**符号傅里叶近似符号的可收缩袋** (**CBOSS**)'
- en: '**Random Interval Spectral Ensemble** (**RISE**)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机区间光谱集成** (**RISE**)'
- en: '**Random Interval Spectral Ensemble (RISE**) is a tree-based time-series classification
    algorithm, originally introduced as **Random Interval Features** (**RIF**) at
    the same time as HIVE-COTE (Jason Lines, Sarah Taylor, and Anthony Bagnall, 2016).
    At each iteration of RISE, a set of Fourier, autocorrelation, and partial autocorrelation
    features are extracted, and a decision tree is trained. RISE''s runtime complexity
    is quadratic to the series length, which can be a problem, and a new version has
    been released, **c-RISE** (*c* for *contract*), where the algorithm can be stopped earlier.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机区间光谱集成（RISE**）是一个基于树的时间序列分类算法，最初在HIVE-COTE（Jason Lines，Sarah Taylor，和Anthony
    Bagnall，2016）发布时作为**随机区间特征**（**RIF**）引入。在RISE的每次迭代中，提取一组傅里叶、自相关和部分自相关特征，并训练决策树。RISE的运行时复杂度是与序列长度的平方成正比，这可能是一个问题，且已经发布了新版本**c-RISE**（*c*代表*收缩*），该算法可以提前停止。'
- en: The runtime complexity of HIVE-COTE, the quadratic runtime to the length of
    the series, is one of the biggest obstacles to its adoption. STC and another model,
    the **elastic ensemble** (**EE**) were the two slowest base models in the original
    algorithm from 2016\. One of the main differences of the new version (1.0) includes
    dropping EE. They re-implemented STC and BOSS to make them more efficient, and
    they replaced RISE with c-RISE.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: HIVE-COTE的运行时复杂度是与序列长度的平方成正比，这是其应用的最大障碍之一。STC和另一个模型，**弹性集成**（**EE**）是2016年原始算法中两个最慢的基础模型。新版本（1.0）的主要区别之一就是去掉了EE。他们重新实现了STC和BOSS，使它们更高效，并用c-RISE替代了RISE。
- en: Each of these base learners is trained separately. The base learners are weighted
    probabilistically based on a **Cross-Validation** **Accuracy Weighted Probabilistic
    Ensemble** (**CAWPE**) structure (James Large, Jason Lines, and Anthony Bagnall,
    2019).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础学习器分别训练。基础学习器基于**交叉验证** **准确度加权概率集成**（**CAWPE**）结构（James Large，Jason Lines，和Anthony
    Bagnall，2019）按概率加权。
- en: In publications postdating HIVE-COTE 1.0, the group showed that the ensemble
    is even stronger when replacing the CIF with the TSF (2020) and when replacing
    BOSS with the **Temporal Dictionary Ensemble** (**TDE**, 2021).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在HIVE-COTE 1.0之后的出版物中，该团队展示了通过将CIF替换为TSF（2020）并将BOSS替换为**时间字典集成**（**TDE**，2021），集成效果更强。
- en: In the next section, we will discuss the performance and trade-offs of different
    approaches.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论不同方法的性能和权衡。
- en: Discussion
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Generally, there''s a trade-off between accuracy and prediction times, and
    in these methods, there''s a huge difference in time complexity and model accuracy.
    This chart illustrates this compromise (from Patrick Schäfer''s GitHub repository
    of SFA):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，准确性与预测时间之间存在权衡，在这些方法中，时间复杂度和模型准确性差异巨大。此图表展示了这种妥协（来自Patrick Schäfer的GitHub
    SFA仓库）：
- en: '![](img/B17577_04_13.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_04_13.png)'
- en: 'Figure 4.19: Machine learning algorithms: query time versus accuracy'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19：机器学习算法：查询时间与准确度的关系
- en: Features could be the result of simple operations or themselves be the outcome
    of machine learning models. We could imagine second-order features as the combination
    of the original features, and third-order features as the combination of second-order
    features, and so on, a potentially large preprocessing pipeline, where features
    are combined and created.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可能是简单操作的结果，或本身就是机器学习模型的输出。我们可以将二阶特征看作是原始特征的组合，三阶特征则是二阶特征的组合，依此类推，形成一个可能庞大的预处理管道，其中特征被组合和创建。
- en: 'We can sum up the different algorithms in this table:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下表中总结不同的算法：
- en: '| Type | Univariate | Multivariate |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 单变量 | 多变量 |'
- en: '| Distance-based | DTW, Proximity Forest (PF) | DTW-D |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 基于距离 | DTW，邻近森林（PF） | DTW-D |'
- en: '| Dictionary-based/Symbolic | BOSS, CBOSS, S-BOSS, WEASEL, Temporal Dictionary
    Ensemble (TDE), SAX-VSM, BOSS | WEASEL+MUSE |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 基于字典/符号 | BOSS，CBOSS，S-BOSS，WEASEL，时间字典集成（TDE），SAX-VSM，BOSS | WEASEL+MUSE
    |'
- en: '| Shapelets | The Shapelet Transform Classifier (STC), MrSEQL |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 形状特征 | 形状变换分类器（STC），MrSEQL |  |'
- en: '| Interval and Spectral-based | Time-Series Forest (TSF), Random Interval Spectral
    Ensemble (RISE) |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 基于区间和光谱 | 时间序列森林（TSF），随机区间光谱集成（RISE） |  |'
- en: '| Deep learning | ResNet, FCN, InceptionTime | TapNet |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | ResNet，FCN，InceptionTime | TapNet |'
- en: '| Ensemble | The Hierarchical Vote Collective of Transformation-based Ensembles
    (HIVE-COTE), Time-Series Combination of Heterogeneous and Integrated Embeddings
    Forest (TS-CHIEF) |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 集成 | 基于变换集成的分层投票集合（HIVE-COTE）、时间序列异构与集成嵌入森林（TS-CHIEF） |  |'
- en: 'Figure 4.20: Detailed taxonomy of time-series machine learning algorithms'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.20：时间序列机器学习算法的详细分类
- en: This classification is far from perfect, but hopefully useful. TDE is both an
    ensemble and a dictionary-based model. HIVE-COTE is based on BOSS features. Furthermore,
    the two featurization methods – Random Convolutional Kernel Transform (ROCKET)
    and Canonical Time-Series Characteristics (Catch22), operate on features individually;
    however, machine learning algorithms that train on and predict based on these
    features as inputs can therefore work in a multivariate setting. The ROCKET features
    together with a linear classifier were indeed found to be highly competitive with
    multivariate approaches. Because of the high dimensionality, the machine learning
    model can potentially take interactions between the original features into account.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类远非完美，但希望对你有所帮助。TDE既是一个集成模型，也是一个基于字典的模型。HIVE-COTE基于BOSS特征。此外，两个特征化方法——随机卷积核变换（ROCKET）和规范时间序列特征（Catch22），分别对特征进行操作；然而，基于这些特征作为输入进行训练和预测的机器学习算法，能够在多变量环境下工作。实际上，ROCKET特征与线性分类器结合的表现，确实与多变量方法竞争力强。由于高维度，机器学习模型有可能考虑到原始特征之间的交互作用。
- en: 'A review paper that I would highly recommend to readers is "*The great multivariate
    time-series classification bake-off*", by Alejandro Pasos Ruiz, Michael Flynn,
    and Anthony Bagnall (2020). It compares state-of-the-art algorithms (16 of which
    were included in the analysis) on 26 multivariate datasets from the UAE archive.
    The approaches included the following:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐给读者的一篇综述文章是 "*The great multivariate time-series classification bake-off*"，由Alejandro
    Pasos Ruiz、Michael Flynn和Anthony Bagnall（2020）撰写。文章对26个来自UAE档案的多变量数据集上的最先进算法（其中16个在分析中被纳入）进行了比较。所涉及的算法包括以下几种：
- en: Dynamic time warping
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态时间规整
- en: MUSE+WEASEL
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MUSE+WEASEL
- en: RISE
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RISE
- en: CBOSS
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CBOSS
- en: TSF
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TSF
- en: gRSF
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gRSF
- en: ROCKET
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROCKET
- en: HIVE-COTE 1.0
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HIVE-COTE 1.0
- en: CIF
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIF
- en: ResNet
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet
- en: STC
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: STC
- en: 'The critical difference diagram (as found on [timeseriesclassification.com](http://timeseriesclassification.com))
    shows the rank of the algorithms across 26 datasets in the test. Links between
    algorithms show that the differences between them can''t be statistically separated
    (based on the Wilcoxon rank-sum test):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 临界差异图（见于[timeseriesclassification.com](http://timeseriesclassification.com)）显示了26个数据集上算法的排名。算法之间的链接表明，它们之间的差异不能通过统计方法分离（基于Wilcoxon秩和检验）：
- en: '![../../../../Desktop/Screenshot%202021-04-25%20at%2021.24](img/B17577_04_14.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![../../../../Desktop/Screenshot%202021-04-25%20at%2021.24](img/B17577_04_14.png)'
- en: Figure 4.21 Critical difference diagram of time-series classification algorithms
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.21 时间序列分类算法的临界差异图
- en: They found a clique of top-performing classifiers, with ROCKET at the top achieving
    a considerable improvement in at least an order of magnitude less time. ROCKET
    was followed by HIVE-COTE and CIF.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 他们发现了一组表现最佳的分类器，ROCKET位居榜首，在更短的时间内取得了至少数量级的显著提升。紧随其后的是HIVE-COTE和CIF。
- en: In a study from 2019, Hassan Fawaz and others compared deep learning algorithms
    for time-series across 12 multivariate datasets from the MTSC archive. The fully
    connected convolutional network (FCN) was best, followed by ResNet – on 85 univariate
    datasets from the UCR repository, ResNet beat FCN to the top spot (winning on
    50 out of 85 datasets). In a separate comparison involving just ResNet with some
    of the state-of-the-art non-deep learning methods on both univariate and multivariate
    datasets, they found that ResNet's performance was behind HIVE-COTE, although
    not significantly worse across datasets, while beating other approaches such as
    BOSS and 1NN with DTW (the latter to a statistically significant degree). We'll
    talk more about this paper in *Chapter 10*, *Deep Learning for Time-Series*.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年的一项研究中，Hassan Fawaz等人比较了12个来自MTSC档案的多变量数据集上的深度学习算法。全连接卷积网络（FCN）表现最佳，其次是ResNet——在来自UCR仓库的85个单变量数据集中，ResNet超越了FCN，取得了第一的位置（在85个数据集中的50个中获胜）。在一项仅涉及ResNet与一些最先进的非深度学习方法的对比中，他们发现ResNet的表现不如HIVE-COTE，尽管在数据集之间并没有显著的差距，同时超越了其他方法，如BOSS和使用DTW的1NN（后者在统计上具有显著差异）。我们将在*第10章*，*时间序列的深度学习*中进一步讨论这篇文章。
- en: In another comparison study on multivariate time-series classification on 20
    datasets from the MTSC archive (Bhaskar Dhariyal, Thach Le Nguyen, Severin Gsponer,
    and Georgiana Ifrim, 2020), it was established that ROCKET won on 14 datasets
    and was much better than most deep learning algorithms, while at the same time
    being the fastest method – ROCKET's runtime on the 20 datasets was 34 minutes,
    while the DTW ran for days.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个关于20个数据集的多变量时间序列分类的比较研究中（Bhaskar Dhariyal, Thach Le Nguyen, Severin Gsponer,
    和 Georgiana Ifrim，2020），结果表明，ROCKET在14个数据集上获胜，并且比大多数深度学习算法表现得要好，同时也是最快的方法——ROCKET在20个数据集上的运行时间为34分钟，而DTW则需要几天。
- en: 'Here''s the critical diagram created with Hassan Fawaz''s Python script from
    their results:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过哈桑·法瓦兹（Hassan Fawaz）的Python脚本从他们的结果中创建的关键图表：
- en: '![../../../../Downloads/Machine-Learning%20for%20Time-Series%20with%20Python/mtsc_cd-di](img/B17577_04_15.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![../../../../Downloads/Machine-Learning%20for%20Time-Series%20with%20Python/mtsc_cd-di](img/B17577_04_15.png)'
- en: 'Figure 4.22: Critical difference diagram of multivariate time-series classification'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.22：多变量时间序列分类的关键差异图
- en: Many different feature sets were tried, the best of which (`9_stat_MulPAA`)
    didn't end up far off.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试了许多不同的特征集，其中最佳的（`9_stat_MulPAA`）结果并不远离理想值。
- en: Implementations
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Great algorithms would be worth much less in practice without software that
    provides them in a way that makes them easy to use and reliable to use in a production
    setting of a company. Alternatively, implementing algorithms from scratch can
    take time, and is not without complications. Therefore, it's a boon that there
    are many reliable, available implementations in Python.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 伟大的算法如果没有提供它们的可靠软件，实际上会变得价值不大。这样的软件能够让它们易于使用，并且在公司生产环境中可靠地使用。另一方面，从零开始实现算法可能需要时间，并且也不是没有复杂性。因此，Python中有许多可靠且可用的实现是一个巨大的福音。
- en: 'The following table summarizes implementations of supervised algorithms for
    regression and classification:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表总结了回归和分类的监督算法实现：
- en: '| Algorithm | sktime | Pyts |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | sktime | Pyts |'
- en: '| Autoregressive Integrated Moving Average (ARIMA) | X |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 自回归积分滑动平均（ARIMA） | X |  |'
- en: '| DTW | X | X |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| DTW | X | X |'
- en: '| BATS | X |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| BATS | X |  |'
- en: '| MUSE+WEASEL | X | X |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| MUSE+WEASEL | X | X |'
- en: '| MrSEQL | X |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| MrSEQL | X |  |'
- en: '| ROCKET | X | X |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| ROCKET | X | X |'
- en: '| BOSS | X | X |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| BOSS | X | X |'
- en: '| Bag-of-SFA Symbols in Vector Space (BOSSVS) |  | X |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 向量空间中的SFA符号袋（BOSSVS） |  | X |'
- en: '| CBOSS | X |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| CBOSS | X |  |'
- en: '| SAX-VSM |  | X |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| SAX-VSM |  | X |'
- en: '| RISE | X |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| RISE | X |  |'
- en: '| HIVE-COTE | X |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| HIVE-COTE | X |  |'
- en: '| Time-Series Forest | X |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 时间序列森林 | X |  |'
- en: 'Figure 4.23: Pyts versus SkTime implementations of machine learning algorithms'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.23：Pyts与SkTime机器学习算法实现对比
- en: It's not an accident that sktime has so many implementations. It is actively
    used in research activity by the group around Anthony Bagnall at the University
    of East Anglia. Pyts is being maintained by Johann Faouzi and Hicham Janati, postdoctoral
    fellows at the Paris Brain Institute and the **Centre de Mathématiques Appliquées**
    (**CMAP**) in Rémy. Johann Faouzi is also behind the tslearn library that implements
    time-series analysis and feature extraction algorithms.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: sktime有这么多实现并非偶然。它在东英吉利大学安东尼·巴格纳尔（Anthony Bagnall）团队的研究活动中得到了广泛应用。Pyts由约翰·法乌兹（Johann
    Faouzi）和希沙姆·贾纳提（Hicham Janati）维护，他们是巴黎脑科学研究所和**应用数学中心**（**CMAP**）的博士后研究员。约翰·法乌兹还是实现时间序列分析和特征提取算法的tslearn库的幕后推手。
- en: I've omitted deep learning algorithms from the table, which are often implemented
    as part of different libraries. Please note that sktime allows use of the prophet
    forecaster through the same interface. For example, the sktime-DL library implements
    ResNet, InceptionTime, and TapNet algorithms, and dl-4-tsc implements more than
    a dozen deep learning models. We'll come to deep learning model implementations
    in *Chapter 10*, *Deep Learning for Time-Series*.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 我省略了深度学习算法，因为它们通常作为不同库的一部分实现。请注意，sktime通过相同的接口支持使用prophet预测器。例如，sktime-DL库实现了ResNet、InceptionTime和TapNet算法，而dl-4-tsc实现了十多个深度学习模型。我们将在*第10章*，*时间序列的深度学习*中讨论深度学习模型的实现。
- en: Facebook's Prophet contains a single model, a special case of the **Generalized
    Additive Model** (**GAM**). The Statsmodels library contains a GAM as well as
    linear regression models and a **Generalized Linear Model** (**GLM**), **moving
    average** (**MA**), **Autoregressive Integrated Moving Average** (**ARIMA**),
    and **Vector Autoregressions** (**VAR**).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的Prophet包含一个单一模型，是**广义加性模型**（**GAM**）的特殊情况。Statsmodels库包含一个GAM模型，以及线性回归模型和**广义线性模型**（**GLM**）、**移动平均**（**MA**）、**自回归积分滑动平均模型**（**ARIMA**）和**向量自回归模型**（**VAR**）。
- en: 'The Darts library provides a consistent interface to several models for time-series
    processing and forecasting. It includes both classical and deep learning algorithms:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Darts库提供了一个一致的接口，可以访问多种时间序列处理和预测模型。它包括经典算法和深度学习算法：
- en: Exponential smoothing
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数平滑
- en: ARIMA
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARIMA
- en: Temporal convolutional network
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间卷积网络
- en: Transformer
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer
- en: N-BEATS
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-BEATS
- en: This concludes our overview of time-series machine learning libraries in Python.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对Python中时间序列机器学习库的概述。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've talked about both the context and the technical background
    of machine learning with time-series. Machine learning algorithms or models can
    make systematic, repeatable, validated decisions based on data. We explained the
    main machine learning problems with time-series such as forecasting, classification,
    regression, segmentation, and anomaly detection. We then reviewed the basics of
    machine learning as relevant to time-series, and we looked at the history and
    current uses of machine learning for time-series.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了时间序列机器学习的背景和技术背景。机器学习算法或模型可以基于数据做出系统的、可重复的、经过验证的决策。我们解释了与时间序列相关的主要机器学习问题，如预测、分类、回归、分割和异常检测。然后，我们回顾了与时间序列相关的机器学习基础知识，并考察了机器学习在时间序列中的历史和当前应用。
- en: We discussed different types of methods based on the approach and features used.
    Furthermore, we discussed many algorithms, concentrating on state-of-the-art machine
    learning approaches.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了基于方法和使用的特征的不同类型方法。此外，我们讨论了许多算法，集中在最先进的机器学习方法上。
- en: I will discuss approaches including deep learning or classical models, such
    as autoregressive and moving averages, in chapters dedicated to them (for example,
    in *chapter 5*, *Time-Series Forecasting with Moving Averages and Autoregressive
    Models*, and *chapter 10*, *Deep Learning for Time-Series*).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在专门的章节中讨论包括深度学习或经典模型（如自回归模型和移动平均模型）的不同方法（例如，在*第5章*，*使用移动平均和自回归模型进行时间序列预测*，以及*第10章*，*时间序列的深度学习*中）。
