- en: Threads, Synchronization, and Memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程、同步和内存
- en: In the last chapter, we saw how to write CUDA programs that leverage the processing
    capabilities of a GPU by executing multiple threads and blocks in parallel. In
    all programs, until the last chapter, all threads were independent of each other
    and there was no communication between multiple threads. Most of the real-life
    applications need communication between intermediate threads. So, in this chapter,
    we will look in detail at how communication between different threads can be done,
    and explain the synchronization between multiple threads working on the same data.
    We will examine the hierarchical memory architecture of a CUDA and how different
    memories can be used to accelerate CUDA programs. The last part of this chapter
    explains a very useful application of a CUDA in the dot product of vectors and
    matrix multiplication, using all the concepts we have covered earlier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何编写CUDA程序，通过并行执行多个线程和块来利用GPU的处理能力。在所有程序中，直到上一章，所有线程都是相互独立的，并且多个线程之间没有通信。大多数现实生活中的应用程序需要中间线程之间的通信。因此，在本章中，我们将详细探讨如何在不同线程之间进行通信，并解释在处理相同数据的多线程之间的同步。我们将检查CUDA的分层内存架构以及如何使用不同的内存来加速CUDA程序。本章的最后部分解释了CUDA在向量点积和矩阵乘法中的一个非常有用应用，使用我们之前覆盖的所有概念。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Thread calls
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程调用
- en: CUDA memory architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA内存架构
- en: Global, local, and cache memory
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局、局部和缓存内存
- en: Shared memory and thread synchronization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存和线程同步
- en: Atomic operations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子操作
- en: Constant and texture memory
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常量和纹理内存
- en: Dot product and a matrix multiplication example
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点积和矩阵乘法示例
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires familiarity with the basic C or C++ programming language
    and the codes that were explained in the previous chapters. All the code used
    in this chapter can be downloaded from the following GitHub link: [https://GitHub.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA).
    The code can be executed on any operating system, though it has only been tested
    on Windows 10\.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求熟悉基本的C或C++编程语言以及前几章中解释的代码。本章中使用的所有代码都可以从以下GitHub链接下载：[https://GitHub.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA](https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA)。代码可以在任何操作系统上执行，尽管它只在Windows
    10上进行了测试。
- en: 'Check out the following video to see the code in action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际运行情况：
- en: '[http://bit.ly/2prnGAD](http://bit.ly/2prnGAD)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2prnGAD](http://bit.ly/2prnGAD)'
- en: Threads
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程
- en: 'The CUDA has a hierarchical architecture in terms of parallel execution. The
    kernel execution can be done in parallel with multiple blocks. Each block is further
    divided into multiple threads. In the last chapter, we saw that CUDA runtime can
    carry out parallel operations by launching the same copies of the kernel multiple
    times. We saw that it can be done in two ways: either by launching multiple blocks
    in parallel, with one thread per block, or by launching a single block, with many
    threads in parallel. So, two questions you might ask are, which method should
    I use in my code? And, is there any limitation on the number of blocks and threads
    that can be launched in parallel?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA在并行执行方面具有分层架构。内核执行可以在多个块上并行进行。每个块进一步分为多个线程。在上一章中，我们看到了CUDA运行时可以通过多次启动内核的相同副本来执行并行操作。我们看到了两种方法：要么并行启动多个块，每个块一个线程，要么启动单个块，并行启动多个线程。所以，你可能会有两个问题，我应该在我的代码中使用哪种方法？以及，并行启动的块和线程数量有什么限制？
- en: The answers to these questions are pivotal. As we will see later on in this
    chapter, threads in the same blocks can communicate with each other via shared
    memory. So, there is an advantage to launching one block with many threads in
    parallel so that they can communicate with each other. In the last chapter, we
    also saw the `maxThreadPerBlock` property that limits the number of threads that
    can be launched per block. Its value is 512 or 1,024 for the latest GPUs. Similarly,
    in the second method, the maximum number of blocks that can be launched in parallel
    is limited to 65,535.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的答案至关重要。正如我们将在本章后面看到的那样，同一块中的线程可以通过共享内存相互通信。因此，并行启动一个包含许多线程的块是有优势的，这样它们就可以相互通信。在上一章中，我们也看到了`maxThreadPerBlock`属性，它限制了每个块可以启动的线程数。对于最新的GPU，其值为512或1,024。同样，在第二种方法中，并行启动的最大块数限制为65,535。
- en: 'Ideally, instead of launching multiple threads per single block or multiple
    blocks with a single thread, we launch multiple blocks with each having multiple
    threads (which can be equal to `maxThreadPerBlock`) in parallel. So, suppose you
    want to launch N = 50,000 threads in parallel in the vector add example, which
    we saw in the last chapter. The kernel call would be as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们不是在每个单独的块中启动多个线程，或者不是在单个线程中启动多个块，而是在并行中启动多个块，每个块都有多个线程（可以等于`maxThreadPerBlock`）。所以，假设你想要在向量加法示例中并行启动N
    = 50,000个线程，这是我们上一章看到的。内核调用如下：
- en: '`gpuAdd<< <((N +511)/512),512 > >>(d_a,d_b,d_c)`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpuAdd<< <((N +511)/512),512 > >>(d_a,d_b,d_c)`'
- en: The maximum threads per block are 512, and hence the total number of blocks
    is calculated by dividing the total number of threads (N) by 512\. But if N is
    not an exact multiple of 512, then N divided by 512 may give a wrong number of
    blocks, which is one less than the actual count. So, to get the next highest integer
    value for the number of blocks, 511 is added to N and then it is divided by 512\.
    It is basically the **ceil** operation on division.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块的线程数最大为512，因此总块数是通过将总线程数（N）除以512来计算的。但如果N不是512的准确倍数，那么N除以512可能会给出错误的块数，这个块数比实际数量少一个。因此，为了得到块数的下一个最高整数值，将511加到N上，然后再除以512。这基本上是对除法进行**向上取整**操作。
- en: 'Now, the question is, will this work for all values of N? The answer, sadly,
    is no. From the preceding discussion, the total number of blocks can''t go beyond
    65,535\. So, in the afore as-mentioned kernel call, if `(N+511)/512` is above
    65,535, then again the code will fail. To overcome this, a small constant number
    of blocks and threads are launched with some modification in the kernel code,
    which we will see further by rewriting the kernel for our vector addition program,
    as seen in [Chapter 2](bf5e2281-2978-4e37-89d8-8c4b781a34cd.xhtml), *Parallel
    Programming using Cuda C*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是，这对所有N的值都适用吗？答案是，很遗憾，不适用。从前面的讨论中可以看出，总块数不能超过65,535。因此，在前面提到的内核调用中，如果`(N+511)/512`超过65,535，那么代码将再次失败。为了克服这个问题，通过在内核代码中做一些修改，启动了少量块和线程，我们将在重写向量加法程序内核时进一步看到，如[第2章](bf5e2281-2978-4e37-89d8-8c4b781a34cd.xhtml)中所述，*使用Cuda
    C进行并行编程*：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This kernel code is similar to what we wrote in the last chapter. It has two
    modifications. One modification is in the calculation of thread ID and the second
    is the inclusion of the `while` loop in the kernel function. The change in thread
    ID calculation is due to the launching of multiple threads and blocks in parallel.
    This calculation can be understood by considering blocks and threads as a two-dimensional
    matrix with the number of blocks equal to the number of rows, and the number of
    columns equal to the number of threads per block. We will take an example of three
    blocks and three threads/blocks, as shown in the following table:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核代码与我们上一章中写的类似。它有两个修改。一个修改是在线程ID的计算中，第二个修改是在内核函数中包含`while`循环。线程ID计算的变化是由于并行启动多个线程和块。可以通过将块和线程视为一个二维矩阵来理解这个计算，其中块的数量等于行数，列数等于每个块的线程数。以下是一个例子，有三个块和三个线程/块，如下表所示：
- en: '![](img/af773f86-ee8e-4395-af5f-b6139b230bac.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af773f86-ee8e-4395-af5f-b6139b230bac.png)'
- en: 'We can get the ID of each block by using `blockIdx.x` and the ID of each thread
    in the current block by the `threadIdx.x` command. So, for the thread shown in
    green, the block ID will be 2 and the thread ID will be 1\. But what if we want
    a unique index for this thread among all the threads? This can be calculated by
    multiplying its block ID with the total number of threads per block, which is
    given by `blockDim.x`, and then summing it with its thread ID. This can be represented
    mathematically as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用`blockIdx.x`获取每个块的ID，通过`threadIdx.x`命令获取当前块中每个线程的ID。因此，对于显示为绿色的线程，块ID将是2，线程ID将是1。但如果我们想要一个在所有线程中唯一的索引呢？这可以通过将其块ID乘以每个块的总线程数（由`blockDim.x`给出）来计算，然后加上其线程ID。这可以用以下数学公式表示：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For example, in green, `threadIdx.x = 1`, `blockIdx.x = 2` , and `blockDim.x
    = 3` equals `tid = 7`. This calculation is very important to learn as it will
    be used widely in your code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在绿色部分，`threadIdx.x = 1`，`blockIdx.x = 2`，`blockDim.x = 3` 等于 `tid = 7`。这个计算非常重要，因为它将在你的代码中被广泛使用。
- en: 'The `while` loop is included in the code because when N is very large, the
    total number of threads can''t be equal to N because of the limitation described
    earlier. So, one thread has to do multiple operations separated by the total number
    of threads launched. This value can be calculated by multiplying `blockDim.x`
    by `gridDim.x`, which gives block and grid dimensions, respectively. Inside the
    `while` loop, the thread ID is incremented by this offset value. Now, this code
    will work for any value of N. To complete the program, we will write the main
    function for this code as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环被包含在代码中，因为当N非常大时，由于前面描述的限制，总线程数不能等于N。因此，一个线程必须执行多个操作，这些操作由启动的总线程数分隔。这个值可以通过将`blockDim.x`乘以`gridDim.x`来计算，这分别给出了块和网格的维度。在`while`循环内部，线程ID通过这个偏移值增加。现在，这段代码将对任何N值都有效。为了完成程序，我们将为这段代码编写以下主函数：'
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Again, the main function is very similar to what we wrote last time. The only
    changes are in terms of how we launch the kernel function. The kernel is launched
    with 512 blocks, each having 512 threads in parallel. This will solve the problem
    for large values of N. Instead of printing the addition of a very long vector,
    only one print statement, which indicates whether the calculated answer is right
    or wrong, is printed. The output of the code will be seen as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，主要功能与我们上次写的内容非常相似。唯一的变化在于我们如何启动核函数。核函数以512个块的方式启动，每个块包含512个并行线程。这将解决N值较大的问题。我们不再打印一个非常长的向量的加法，而只打印一条指示计算结果是否正确的打印语句。代码的输出将如下所示：
- en: '![](img/e869db0f-e17a-4b05-84b4-9d28f024085d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e869db0f-e17a-4b05-84b4-9d28f024085d.png)'
- en: This section explained the hierarchical execution concept in a CUDA. The next
    section will take this concept further by explaining a hierarchical memory architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了CUDA中的分层执行概念。下一节将进一步解释这个概念，通过解释分层内存架构。
- en: Memory architecture
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存架构
- en: 'The execution of code on a GPU is divided among streaming multiprocessors,
    blocks, and threads. The GPU has several different memory spaces, with each having
    particular features and uses and different speeds and scopes. This memory space
    is hierarchically divided into different chunks, like global memory, shared memory,
    local memory, constant memory, and texture memory, and each of them can be accessed
    from different points in the program. This memory architecture is shown in preceding
    diagram:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上执行代码被分配到流多处理器、块和线程中。GPU有几个不同的内存空间，每个空间都有特定的特性和用途，以及不同的速度和范围。这个内存空间被分层划分为不同的部分，如全局内存、共享内存、局部内存、常量内存和纹理内存，并且它们可以从程序的不同点访问。这个内存架构在先前的图中显示：
- en: '![](img/ec8e0d7d-05a0-4eca-9c5a-648df0f49a97.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec8e0d7d-05a0-4eca-9c5a-648df0f49a97.png)'
- en: As shown in the diagram, each thread has its own local memory and a register
    file. Unlike processors, GPU cores have lots of registers to store local data.
    When the data of a thread does not fit in the register file, the local memory
    is used. Both of them are unique to each thread. The register file is the fastest
    memory. Threads in the same blocks have shared memory that can be accessed by
    all threads in that block. It is used for communication between threads. There
    is a global memory that can be accessed by all blocks and all threads. Global
    memory has a large memory access latency. There is a concept of caching to speed
    up this operation. L1 and L2 caches are available, as shown in the following table.
    There is a read-only constant memory that is used to store constants and kernel
    parameters. Finally, there is a texture memory that can take advantage of different
    two-dimensional or three-dimensional access patterns.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，每个线程都有自己的本地内存和寄存器文件。与处理器不同，GPU核心拥有大量的寄存器来存储本地数据。当线程的数据不适合寄存器文件时，会使用本地内存。这两者都是每个线程独有的。寄存器文件是最快的内存。同一块中的线程共享内存，可以被该块中的所有线程访问。它用于线程间的通信。存在一个全局内存，可以被所有块和所有线程访问。全局内存具有较大的内存访问延迟。存在一种缓存的概念来加速这一操作。L1和L2缓存如以下表格所示。存在一个只读的常量内存，用于存储常量和内核参数。最后，存在一个纹理内存，可以利用不同的二维或三维访问模式。
- en: 'The features of all memories are summarized in the following table:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所有内存的特性总结在以下表格中：
- en: '| **Memory** | **Access Pattern** | **Speed** | **Cached?** | **Scope** | **Lifetime**
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **内存** | **访问模式** | **速度** | **缓存？** | **作用域** | **生命周期** |'
- en: '| Global | Read and Write | Slow | Yes | Host and All Threads | Entire Program
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 全局 | 读写 | 慢 | 是 | 主机和所有线程 | 整个程序 |'
- en: '| Local | Read and Write | Slow | Yes | Each Thread | Thread |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 本地 | 读写 | 慢 | 是 | 每个线程 | 线程 |'
- en: '| Registers | Read and Write | Fast | - | Each Thread | Thread |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 寄存器 | 读写 | 快 | - | 每个线程 | 线程 |'
- en: '| Shared | Read and Write | Fast | No | Each Block | Block |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 共享 | 读写 | 快 | 否 | 每个块 | 块 |'
- en: '| Constant | Read only | Slow | Yes | Host and All Threads | Entire Program
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 常量 | 只读 | 慢 | 是 | 主机和所有线程 | 整个程序 |'
- en: '| Texture | Read only | Slow | Yes | Host and All Threads | Entire Program
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 纹理 | 只读 | 慢 | 是 | 主机和所有线程 | 整个程序 |'
- en: The preceding table describes important features of all memories. The scope
    defines the part of the program that can use this memory, and lifetime defines
    the time for which data in that memory will be visible to the program. Apart from
    this, L1 and L2 caches are also available for GPU programs for faster memory access.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表格描述了所有内存的重要特性。作用域定义了程序可以使用此内存的部分，生命周期定义了该内存中的数据对程序可见的时间。除此之外，L1和L2缓存也适用于GPU程序以实现更快的内存访问。
- en: To summarize, all threads have a register file, which is the fastest. Multiple
    threads in the same blocks have shared memory that is faster than global memory.
    All blocks can access global memory, which will be the slowest. Constant and texture
    memory are used for a special purpose, which will be discussed in the next section.
    Memory access is the biggest bottleneck in the fast execution of the program.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，所有线程都有一个寄存器文件，这是最快的。同一块中的多个线程有共享内存，比全局内存快。所有块都可以访问全局内存，这将是最慢的。常量和纹理内存用于特殊目的，将在下一节讨论。内存访问是程序快速执行中的最大瓶颈。
- en: Global memory
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局内存
- en: 'All blocks have read and write access to global memory. This memory is slow
    but can be accessed from anywhere in your device code. The concept of caching
    is used to speed up access to a global memory. All memories allocated using `cudaMalloc`
    will be a global memory. The following simple example demonstrates how you can
    use a global memory from your program:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所有块都可以读写全局内存。这种内存较慢，但可以从设备代码的任何地方访问。使用缓存的概念来加速对全局内存的访问。使用`cudaMalloc`分配的所有内存都将是一个全局内存。以下简单示例演示了您如何从程序中使用全局内存：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code demonstrates how you can write in global memory from your device
    code. The memory is allocated using `cudaMalloc` from the host code and a pointer
    to this array is passed as a parameter to the kernel function. The kernel function
    populates this memory chunk with values of the thread ID. This is copied back
    to host memory for printing. The result is shown as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码演示了您如何从设备代码写入全局内存。内存是通过主机代码中的`cudaMalloc`分配的，并将指向此数组的指针作为参数传递给内核函数。内核函数用线程ID的值填充这个内存块。然后将其复制回主机内存以打印。结果如下所示：
- en: '![](img/a67ad366-a6c8-4cc6-bd8d-3b8e7327152a.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a67ad366-a6c8-4cc6-bd8d-3b8e7327152a.png)'
- en: As we are using global memory, this operation will be slow. There are advanced
    concepts to speed up this operation which will be explained later on. In the next
    section, we will explain local memory and registers that are unique to all threads.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是全局内存，这个操作将会较慢。有一些高级概念可以加快这个操作，稍后将会解释。在下一节中，我们将解释所有线程独有的局部内存和寄存器。
- en: Local memory and registers
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部内存和寄存器
- en: Local memory and register files are unique to each thread. Register files are
    the fastest memory available for each thread. When variables of the kernel do
    not fit in register files, they use local memory. This is called **register spilling**.
    Basically, local memory is a part of global memory that is unique for each thread.
    Access to local memory will be slow compared to register files. Though local memory
    is cached in L1 and L2 caches, register spilling might not affect your program
    adversely.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存和寄存器文件是每个线程独有的。寄存器文件是每个线程可用的最快内存。当内核的变量不适合寄存器文件时，它们会使用局部内存。这被称为**寄存器溢出**。基本上，局部内存是全局内存的一部分，对每个线程来说是唯一的。与寄存器文件相比，访问局部内存会较慢。尽管局部内存被缓存在L1和L2缓存中，但寄存器溢出可能不会对程序产生负面影响。
- en: 'A simple program to understand how to use local memory is shown as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的程序，用于说明如何使用局部内存：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `t_local` variable will be local to each thread and stored in a register
    file. When this variable is used for computation in the kernel function, the computation
    will be the fastest. The output of the preceding code is shown as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`t_local`变量将属于每个线程，并存储在寄存器文件中。当这个变量在内核函数中进行计算时，计算将是最快的。前述代码的输出如下所示：'
- en: '![](img/01037099-35aa-46bd-bf0b-3ffc7eb92540.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01037099-35aa-46bd-bf0b-3ffc7eb92540.png)'
- en: Cache memory
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存内存
- en: On the latest GPUs, there is an L1 cache per multiprocessor and an L2 cache,
    which is shared between all multiprocessors. Both global and local memories use
    these caches. As L1 is near to thread execution, it is very fast. As shown in
    the diagram for memory architecture earlier, the L1 cache and shared memory use
    the same 64 KB. Both can be configured for how many bytes they will use out of
    the 64 KB. All global memory access goes through an L2 cache. Texture memory and
    constant memory have their separate caches.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新的GPU上，每个多处理器都有一个L1缓存和一个L2缓存，这些缓存是所有多处理器共享的。全局和局部内存都使用这些缓存。由于L1缓存靠近线程执行，因此它非常快。如前所述的内存架构图所示，L1缓存和共享内存使用相同的64
    KB。它们都可以配置为使用64 KB中的多少字节。所有全局内存访问都通过L2缓存进行。纹理内存和常量内存有自己的单独缓存。
- en: Thread synchronization
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程同步
- en: Up until now, whatever examples we have seen in this book had all threads independent
    of each other. But rarely, in real life, do you find examples where threads operate
    on data and terminate without passing results to any other threads. So there has
    to be some mechanism for threads to communicate with each other, and that is why
    the concept of shared memory is explained in this section. When many threads work
    in parallel and operate on the same data or read and write from the same memory
    location, there has to be synchronization between all threads. Thus, thread synchronization
    is also explained in this section. The last part of this section explains atomic
    operations, which are very useful in read-modified write conditions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在这本书中看到的例子中，所有线程都是相互独立的。但在现实生活中，很少能找到线程在操作数据并终止时没有将结果传递给其他线程的例子。因此，线程之间必须有一些通信机制，这就是为什么在本节中解释了共享内存的概念。当许多线程并行工作并操作相同的数据或从相同的内存位置读取和写入时，所有线程之间必须进行同步。因此，本节还解释了线程同步。本节的最后部分解释了原子操作，这在读取-修改-写入条件下非常有用。
- en: Shared memory
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享内存
- en: 'Shared memory is available on-chip, and hence it is much faster than global
    memory. Shared memory latency is roughly 100 times lower than uncached global
    memory latency. All the threads from the same block can access shared memory.
    This is very useful in many applications where threads need to share their results
    with other threads. However, it can also create chaos or false results if it is
    not synchronized. If one thread reads data from memory before the other thread
    has written to it, it can lead to false results. So, the memory access should
    be controlled or managed properly. This is done by the `__syncthreads()` directive,
    which ensures that all the `write` operations to memory are completed before moving
    ahead in the programs. This is also called a **barrier**. The meaning of barrier
    is that all threads will reach this line and wait for other threads to finish.
    After all threads have reached this barrier, they can move further. To demonstrate
    the use of shared memory and thread synchronization, an example of a moving average
    is taken. The kernel function for that is shown as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存位于芯片上，因此它比全局内存快得多。共享内存的延迟大约是全球未缓存内存延迟的100倍低。来自同一块的线程都可以访问共享内存。这在许多需要线程之间共享结果的程序中非常有用。然而，如果不进行同步，它也可能导致混乱或错误的结果。如果一个线程在另一个线程写入之前从内存中读取数据，可能会导致错误的结果。因此，内存访问应该得到适当的控制或管理。这是通过`__syncthreads()`指令完成的，它确保在程序前进之前所有对内存的`write`操作都已完成。这也被称为**屏障**。屏障的含义是所有线程都将到达这一行并等待其他线程完成。在所有线程都到达这个屏障之后，它们可以继续前进。为了演示共享内存和线程同步的使用，我们取了一个移动平均的例子。该内核函数如下所示：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The moving average operation is nothing but finding an average of all elements
    in an array up to the current element. Many threads will need the same data of
    an array for their calculation. This is an ideal case of using shared memory,
    and it will provide faster data than global memory. This will reduce the number
    of global memory accesses per thread, which in turn will reduce the latency of
    the program. The shared memory location is defined using the `__shared__` directive.
    In this example, the shared memory of ten float elements is defined. Normally,
    the size of shared memory should be equal to the number of threads per block.
    Here, we are working on an array of 10, and hence we have taken the shared memory
    of this size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 移动平均操作不过是找到数组中所有元素的平均值，直到当前元素。许多线程将需要数组中的相同数据来进行计算。这是使用共享内存的理想情况，它将提供比全局内存更快的速度。这将减少每个线程的全局内存访问次数，从而降低程序的延迟。共享内存位置是通过`__shared__`指令定义的。在这个例子中，定义了十个浮点元素的共享内存。通常，共享内存的大小应该等于每个块中的线程数。在这里，我们正在处理一个包含10个元素的数组，因此我们采用了这个大小的共享内存。
- en: The next step is to copy data from global memory to this shared memory. All
    the threads copy the element indexed by its thread ID to the shared array. Now,
    this is a shared memory write operation and, in the next line, we will read from
    this shared array. So, before proceeding, we should ensure that all shared memory
    write operations are completed. Therefore, let's introduce the `__synchronizethreads()`
    barrier.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据从全局内存复制到共享内存。所有线程将根据其线程ID索引的元素复制到共享数组。现在，这是一个共享内存写操作，在下一行中，我们将从这个共享数组中读取。因此，在继续之前，我们应该确保所有共享内存写操作都已完成。因此，让我们引入`__synchronizethreads()`屏障。
- en: 'Next, the `for` loop calculates the average of all elements up to the current
    elements using the values in shared memory and stores the answer in global memory
    which is indexed by the current thread ID. The last line copies the calculated
    value in shared memory also. This line will have no effect on the overall execution
    of the code because shared memory has a lifetime up until the end of the current
    block execution, and this is the last line after which block execution is complete.
    It is just used to demonstrate this concept about shared memory. Now, we will
    try to write the main function for this code as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`for`循环使用共享内存中的值计算所有元素的平均值，并将结果存储在全局内存中，全局内存是通过当前线程ID索引的。最后一行也将计算出的值复制到共享内存中。这一行对代码的整体执行没有影响，因为共享内存的寿命直到当前块执行结束，这是块的最后一行。它只是用来演示这个关于共享内存的概念。现在，我们将尝试编写这个代码的主函数如下：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the `main` function, after allocating memory for host and device arrays,
    host array is populated with values from zero to nine. This is copied to device
    memory where the moving average is calculated and the result is stored. The result
    from device memory is copied back to host memory and then printed on the console.
    The output on the console is shown as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数中，在为宿主和设备数组分配内存之后，宿主数组被填充了从零到九的值。这些值被复制到设备内存中，在那里计算移动平均值，并将结果存储起来。设备内存中的结果被复制回宿主内存，然后打印到控制台。控制台上的输出如下所示：
- en: '![](img/d25f4dfd-77ed-4a21-8a9b-3ec1b3a7ca9c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d25f4dfd-77ed-4a21-8a9b-3ec1b3a7ca9c.png)'
- en: This section demonstrated the use of shared memory when multiple threads use
    data from the same memory location. The next section demonstrates the use of the
    `atomic` operations, which are very important in read-modified write operations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本节演示了当多个线程使用同一内存位置的数据时共享内存的使用。下一节将演示使用`atomic`操作，这在读取-修改-写入操作中非常重要。
- en: Atomic operations
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子操作
- en: Consider a situation in which a large number of threads try to modify a small
    portion of memory. This is a frequently occurring phenomenon. It creates more
    problems when we try to perform a read-modify-write operation. The example of
    this operation is `d_out[i] ++,` where the first `d_out[i]` is read from memory,
    then incremented and then written back to the memory. However, when multiple threads
    are doing this operation on the same memory location, it can give a wrong output.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一种情况：大量线程试图修改内存的一小部分。这是一个经常发生的情况。当我们尝试执行读取-修改-写入操作时，这会引发更多的问题。这种操作的例子是`d_out[i]
    ++,`其中首先从内存中读取`d_out[i]`，然后增加，并写回内存。然而，当多个线程在相同的内存位置执行此操作时，可能会得到错误的结果。
- en: 'Suppose one memory location has an initial value of six, and threads p and
    q are trying to increment this memory location, then the final answer should be
    eight. But at the time of execution, it may happen that both the p and q threads
    read this value simultaneously, then both will get the value six. They increment
    it to seven and both will store this seven in the memory. So instead of eight,
    our final answer is seven, which is wrong. How this can be dangerous is understood
    by taking an example of ATM cash withdrawal. Suppose you have a balance of Rs
    5,000 in your account. You have two ATM cards of the same account. You and your
    friend go to two different ATMs simultaneously to withdraw Rs 4,000\. Both of
    you swipe your card simultaneously; so, when the ATM checks for the balance, both
    will show a balance of Rs 5,000\. When both of you withdraw Rs 4,000, then both
    machines will look at the initial balance, which was Rs 5,000\. The amount to
    withdraw is less than the balance, and hence both machines will give Rs 4,000\.
    Even though your balance was Rs 5,000, you got Rs 8,000, which is dangerous. To
    demonstrate this phenomenon, one example of large threads trying to access a small
    array is taken. The kernel function for this example is shown as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个内存位置的初始值为六，线程p和q都试图增加这个内存位置，那么最终的答案应该是八。但在执行时，可能会发生p和q线程同时读取这个值的情况，那么它们都会得到六这个值。它们将这个值增加到七，并将这个七存储在内存中。所以，最终答案不是八，而是七，这是错误的。这种错误可能带来的危险可以通过ATM取款的一个例子来理解。假设你在账户中有5,000卢比。你有两张相同的账户ATM卡。你和你的朋友同时去两个不同的ATM机取款4,000卢比。你们同时刷卡；所以，当ATM检查余额时，两个ATM都会显示5,000卢比的余额。当你们两人都取款4,000卢比时，那么两个机器都会查看初始余额，即5,000卢比。要取的金额小于余额，因此两个机器都会给出4,000卢比。尽管你的余额是5,000卢比，但你得到了8,000卢比，这是危险的。为了演示这种现象，我们取了一个大量线程尝试访问小数组的例子。这个例子的内核函数如下所示：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The kernel function is just incrementing memory location in the `d_a[tid] +=1`
    line. The issue is how many times this memory location is incremented. The total
    number of threads is 10,000 and the array is only of size 10\. We are indexing
    an array by taking the `modulo` operation between the thread ID and the size of
    the array. So, 1,000 threads will try to increment the same memory location. Ideally,
    every location in the array should be incremented 1,000 times. But as we will
    see in the output, this is not the case. Before seeing the output, we will try
    to write the `main` function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 内核函数只是在`d_a[tid] +=1`行中增加内存位置。问题是这个内存位置增加了多少次。线程总数是10,000，而数组的大小只有10。我们通过将线程ID与数组大小进行`取模`操作来索引数组。因此，1,000个线程将尝试增加相同的内存位置。理想情况下，数组的每个位置都应该增加1,000次。但正如我们将在输出中看到的，情况并非如此。在查看输出之前，我们将尝试编写`main`函数：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the `main` function, the device array is declared and initialized to zero.
    Here, a special `cudaMemSet` function is used to initialize memory on the device.
    This is passed as a parameter to the kernel, which increments these 10 memory
    locations. Here, a total of 10,000 threads are launched as 1,000 blocks and 100
    threads per block. The answer stored on the device after the kernel's execution
    is copied back to the host, and the value of each memory location is displayed
    on the console.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数中，设备数组被声明并初始化为零。在这里，使用特殊的`cudaMemSet`函数在设备上初始化内存。这作为参数传递给内核，它增加这10个内存位置。在这里，总共启动了10,000个线程，分为1,000个块，每个块100个线程。内核执行后在设备上存储的答案被复制回主机，每个内存位置的价值在控制台上显示。
- en: 'The output is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/220c69ea-0945-4d74-9c6c-58b48f265382.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/220c69ea-0945-4d74-9c6c-58b48f265382.png)'
- en: 'As discussed previously, ideally, each memory location should have been incremented
    1,000 times, but most of the memory locations have values of 16 and 17\. This
    is because many threads read the same locations simultaneously and hence increment
    the same value and store it in memory. As the timing of the thread''s execution
    is beyond the control of the programmer, how many times simultaneous memory access
    will happen is not known. If you run your program a second time, then will your
    output be same as the first time? Your output might look like the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，理想情况下，每个内存位置应该增加1,000次，但大多数内存位置的价值为16和17。这是因为许多线程同时读取相同的地址，因此增加相同的值并将其存储在内存中。由于线程执行的时机超出了程序员的控制，因此无法知道同时发生内存访问的次数。如果你再次运行你的程序，你的输出会与第一次相同吗？你的输出可能看起来像以下这样：
- en: '![](img/4492405e-802e-4611-8571-0b5e79f7c4e4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4492405e-802e-4611-8571-0b5e79f7c4e4.png)'
- en: As you might have guessed, every time you run your program, the memory locations
    may have different values. This happens because of the random execution of all
    threads on the device.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，每次运行你的程序时，内存位置可能具有不同的值。这是因为设备上所有线程的随机执行导致的。
- en: 'To solve this problem, CUDA provides an API called `atomicAdd` operations.
    It is a `blocking` operation, which means that when multiple threads are trying
    to access the same memory location, only one thread can access the memory location
    at a time. Other threads have to wait for this thread to finish and `write` its
    answer on memory. The kernel function to use the `atomicAdd` operation is shown
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，CUDA提供了一个名为`atomicAdd`操作的API。这是一个`阻塞`操作，这意味着当多个线程试图访问相同的内存位置时，一次只能有一个线程可以访问该内存位置。其他线程必须等待这个线程完成并在内存上`写入`其答案。使用`atomicAdd`操作的内核函数如下所示：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `kernel` function is quite similar to what we saw earlier. Instead of incrementing
    memory location using the `+=` operator, the `atomicAdd` function is used. It
    takes two arguments. The first is the memory location we want to increment, and
    the second is the value by which this location has to be incremented. In this
    code, 1,000 threads will again try to access the same location; so when one thread
    is using this location, the other 999 threads have to wait. This will increase
    the cost in terms of execution time. The `main` function of increment using `atomic`
    operations is shown as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel`函数与之前看到的非常相似。不是使用`+=`运算符增加内存位置，而是使用`atomicAdd`函数。它接受两个参数。第一个是我们想要增加的内存位置，第二个是这个位置需要增加的值。在这段代码中，1,000个线程将再次尝试访问相同的位置；因此，当一个线程使用这个位置时，其他999个线程必须等待。这将增加执行时间方面的成本。使用`atomic`操作增加的`main`函数如下所示：'
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the `main` function, the array with 10 elements is initialized with a zero
    value and passed to the kernel. But now, the kernel will do the `atomic add` operation.
    So, the output of this program should be accurate. Each element in the array should
    be incremented 1,000 times. The following will be the output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数中，包含10个元素的数组被初始化为零值并传递给内核。但现在，内核将执行`atomic add`操作。因此，这个程序的输出应该是准确的。数组中的每个元素应该增加1,000次。以下将是输出结果：
- en: '![](img/29bea1ee-443f-46cb-9742-0841c6ff3b47.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/29bea1ee-443f-46cb-9742-0841c6ff3b47.png)'
- en: If you measure the execution time of the program with atomic operations, it
    may take a longer time than that taken by simple programs using global memory.
    This is because many threads are waiting for memory access in the atomic operation.
    Use of shared memory can help to speed up operations. Also, if the same number
    of threads are accessing more memory locations, then the atomic operation will
    incur less time overhead as a smaller number of threads having to wait for memory
    access.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用原子操作来测量程序的执行时间，它可能比使用全局内存的简单程序花费更长的时间。这是因为许多线程在原子操作中等待内存访问。使用共享内存可以帮助加快操作。此外，如果相同数量的线程访问更多的内存位置，那么原子操作将产生较少的时间开销，因为需要等待内存访问的线程数量更少。
- en: In this section, we have seen that atomic operations help in avoiding race conditions
    in memory operations and make the code simpler to write and understand. In the
    next section, we will explain two special types of memories, constant and texture,
    which help in accelerating certain types of code.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解到原子操作有助于避免内存操作中的竞态条件，并使代码编写和理解更加简单。在下一节中，我们将解释两种特殊的内存类型，即常量和纹理内存，它们有助于加速某些类型的代码。
- en: Constant memory
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常量内存
- en: 'The CUDA language makes another type of memory available to the programmer,
    which is known as **constant** memory. NVIDIA hardware provides 64 KB of this
    constant memory, which is used to store data that remains constant throughout
    the execution of the kernel. This constant memory is cached on-chip so that the
    use of constant memory instead of global memory can speed up execution. The use
    of constant memory will also reduce memory bandwidth to the device''s global memory.
    In this section, we will see how to use constant memory in CUDA programs. A simple
    program that performs a simple math operation, `a*x + b`, where `a` and `b` are
    constants, is taken as an example. The `kernel` function code for this program
    is shown as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA语言为程序员提供了一种另一种类型的内存，称为**常量**内存。NVIDIA硬件提供了64 KB的这种常量内存，用于存储在整个内核执行过程中保持不变的数据。这种常量内存被缓存到芯片上，因此使用常量内存而不是全局内存可以加快执行速度。使用常量内存还将减少设备全局内存的带宽。在本节中，我们将了解如何在CUDA程序中使用常量内存。以执行简单数学运算`a*x
    + b`的简单程序为例，其中`a`和`b`是常量。该程序的`kernel`函数代码如下所示：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Constant memory variables are defined using the `__constant__` keyword. In
    the preceding code, two float variables, `constant_f` and `constant_g`, are defined
    as constants that will not change throughout the kernel''s execution. The second
    thing to note is that once variables are defined as constants, they should not
    be defined again in the kernel function. The kernel function computes a simple
    mathematical operation using these two constants. There is a special way in which
    constant variables are copied to memory from the `main` function. This is shown
    in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存变量使用`__constant__`关键字定义。在前面的代码中，两个浮点变量`constant_f`和`constant_g`被定义为在整个内核执行过程中不会改变常量。第二点要注意的是，一旦变量被定义为常量，就不应该在内核函数中再次定义。内核函数使用这两个常量计算一个简单的数学运算。常量变量从`main`函数复制到内存中有一个特殊的方法。以下代码展示了这一点：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the `main` function, the `h_f` and `h_g` constants are defined and initialized
    on the host, which will be copied to constant memory. The `cudaMemcpyToSymbol`
    instruction is used to copy these constants onto constant memory for kernel execution.
    It has five arguments. First is the destination, which is defined using the `__constant__`
    keyword. Second is the host address, third is the size of the transfer, fourth
    is memory offset, which is taken as zero, and fifth is the direction of data transfer,
    which is taken as the host to the device. The last two arguments are optional,
    and hence they are omitted in the second call to the `cudaMemcpyToSymbol` instruction.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数中，`h_f`和`h_g`常量在主机上定义并初始化，这些常量将被复制到常量内存中。使用`cudaMemcpyToSymbol`指令将这些常量复制到常量内存中以便内核执行。它有五个参数。第一个是目标，使用`__constant__`关键字定义。第二个是主机地址，第三个是传输的大小，第四个是内存偏移量，这里取为零，第五个是数据传输的方向，这里取为主机到设备。最后两个参数是可选的，因此在`cudaMemcpyToSymbol`指令的第二次调用中省略了它们。
- en: 'The output of the code is shown as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出如下所示：
- en: '![](img/e604f1e3-6ad4-4b42-a22e-55ad71959484.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e604f1e3-6ad4-4b42-a22e-55ad71959484.png)'
- en: One thing to note is that constant memory is a `Read-only` memory. This example
    is used just to explain the use of the constant memory from the CUDA program.
    It is not the optimal use of constant memory. As discussed earlier, constant memory
    helps in conserving memory bandwidth to global memory. To understand this, you
    have to understand the concept of warp. One warp is a collection of 32 threads
    woven together and executed in lockstep. A single read from constant memory can
    be broadcast to half warp, which can reduce up to 15 memory transactions. Also,
    constant memory is cached so that memory access to a nearby location will not
    incur an additional memory transaction. When each half warp, which contains 16
    threads, operates on the same memory locations, the use of constant memory saves
    a lot of execution time. It should also be noted that if half-warp threads use
    completely different memory locations, then the use of constant memory may increase
    the execution time. So, the constant memory should be used with proper care.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个需要注意的事项是常量内存是`只读`内存。这个例子只是用来解释从CUDA程序中使用常量内存。这不是常量内存的最佳使用。如前所述，常量内存有助于节省全局内存的内存带宽。要理解这一点，你必须理解warp的概念。一个warp是一组32个线程交织在一起并同步执行的集合。从常量内存的单次读取可以广播到半warp，这可以减少多达15次内存事务。此外，常量内存被缓存，因此对附近位置的内存访问不会产生额外的内存事务。当每个包含16个线程的半warp在相同的内存位置上操作时，使用常量内存可以节省大量的执行时间。还应该注意的是，如果半warp线程使用完全不同的内存位置，那么使用常量内存可能会增加执行时间。因此，应该谨慎使用常量内存。
- en: Texture memory
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 纹理内存
- en: '**Texture** memory is another read-only memory that can accelerate the program
    and reduce memory bandwidth when data is read in a certain pattern. Like constant
    memory, it is also cached on a chip. This memory was originally designed for rendering
    graphics, but it can also be used for general purpose computing applications.
    It is very effective when applications have memory access that exhibits a great
    deal of spatial locality. The meaning of spatial locality is that each thread
    is likely to read from the nearby location what other nearby threads read. This
    is great in image processing applications where we work on 4-point connectivity
    and 8-point connectivity. A two-dimensional spatial locality for accessing memory
    location by threads may look something like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**纹理**内存是另一种只读内存，可以在以特定模式读取数据时加速程序并减少内存带宽。像常量内存一样，它也缓存于芯片上。这种内存最初是为渲染图形而设计的，但它也可以用于通用计算应用。当应用具有大量空间局部性的内存访问时，它非常有效。空间局部性的意义是每个线程很可能从其他附近线程读取的附近位置读取。这在图像处理应用中非常好，我们在其中处理4点连通性和8点连通性。线程通过访问内存位置进行二维空间局部性访问可能看起来像这样：'
- en: '| Thread 0 | Thread 2 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 线程 0 | 线程 2 |'
- en: '| Thread 1 | Thread 3 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 线程 1 | 线程 3 |'
- en: 'General global memory cache will not be able to capture this spatial locality
    and will result in lots of memory traffic to global memory. Texture memory is
    designed for this kind of access pattern so that it will only read from memory
    once, and then it will be cached so that execution will be much faster. Texture
    memory supports one and two-dimensional `fetch` operations. Using texture memory
    in your CUDA program is not trivial, especially for those who are not programming
    experts. In this section, a simple example of how to copy array values using texture
    memory is explained. The `kernel` function for using texture memory is explained
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通用全局内存缓存将无法捕捉这种空间局部性，从而导致大量的内存流量到全局内存。纹理内存是为这种访问模式设计的，因此它只会从内存中读取一次，然后将其缓存起来，从而使执行速度大大加快。纹理内存支持一维和二维的`fetch`操作。在您的CUDA程序中使用纹理内存并不简单，尤其是对于那些不是编程专家的人来说。在本节中，解释了如何使用纹理内存复制数组值的简单示例。使用纹理内存的`kernel`函数解释如下：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The part of texture memory that should be fetched is defined by texture reference.
    In code, it is defined using the texture API. It has three arguments. The first
    argument indicates the data type of texture elements. In this example, it is a
    `float`. The second argument indicates the type of texture reference, which can
    be one-dimensional, two-dimensional, and so on. Here, it is a one-dimensional
    reference. The third argument specifies the read mode and it is an optional argument.
    Please make sure that this texture reference is declared as a static global variable,
    and it should not be passed as parameters to any function. In the kernel function,
    data stored at the thread ID is read from this texture reference and copied to
    the `d_out` global memory pointer. Here, we are not using any spatial locality
    as this example is only taken to show you how to use texture memory from CUDA
    programs. The spatial locality will be explained in the next chapter when we see
    some image processing applications with CUDA. The `main` function for this example
    is shown as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 应该被读取的纹理内存部分由纹理引用定义。在代码中，它使用纹理API定义。它有三个参数。第一个参数指示纹理元素的数据类型。在这个例子中，它是一个`float`。第二个参数指示纹理引用的类型，可以是单维、二维等。在这里，它是一个单维引用。第三个参数指定读取模式，它是一个可选参数。请确保将此纹理引用声明为静态全局变量，并且它不应作为任何函数的参数传递。在内核函数中，存储在线程ID中的数据从这个纹理引用中读取，并复制到`d_out`全局内存指针。在这里，我们没有使用任何空间局部性，因为这个例子只是为了向您展示如何从CUDA程序中使用纹理内存。空间局部性将在下一章中解释，当我们看到一些使用CUDA的图像处理应用时。此例的`main`函数如下所示：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the `main` function, after declaring and allocating memory for host and device
    arrays, the host array is initialized with values from zero to nine. In this example,
    you will see the first use of CUDA arrays. They are similar to normal arrays,
    but they are dedicated to textures. They are read-only to kernel functions and
    can be written to device memory from the host by using the `cudaMemcpyToArray`
    function, as shown in the preceding code. The second and third arguments in that
    function are width and height offset that are taken as 0, 0, meaning that we are
    starting from the top left corner. They are opaque memory layouts optimized for
    texture memory fetches.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在`main`函数中，在声明和为主机和设备数组分配内存之后，主机数组使用从零到九的值进行初始化。在这个例子中，您将看到CUDA数组的首次使用。它们类似于普通数组，但它们是专门用于纹理的。它们对内核函数是只读的，可以通过使用`cudaMemcpyToArray`函数从主机写入设备内存，如前述代码所示。该函数中的第二个和第三个参数是宽度和高度偏移量，取值为0，0，这意味着我们从左上角开始。它们是针对纹理内存读取优化的不透明内存布局。
- en: 'The `cudaBindTextureToArray` functions bind texture reference to this CUDA
    array. This means, it copies this array to a texture reference starting from the
    top left corner. After binding the texture reference, the kernel is called, which
    uses this texture reference and computes the array to be stored on device memory.
    After the kernel finishes, the output array is copied back to the host for displaying
    on the console. When using texture memory, we have to unbind the texture from
    our code. This is done by using the `cudaUnbindTexture` function. The `cudaFreeArray`
    function is used to free up memory used by the CUDA array. The output of the program
    displayed on the console is shown as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaBindTextureToArray`函数将纹理引用绑定到这个CUDA数组。这意味着，它从左上角开始将这个数组复制到纹理引用。绑定纹理引用后，调用内核，该内核使用这个纹理引用并计算要存储在设备内存上的数组。内核完成后，输出数组被复制回主机以在控制台上显示。当使用纹理内存时，我们必须使用`cudaUnbindTexture`函数从我们的代码中解除纹理的绑定。`cudaFreeArray`函数用于释放CUDA数组使用的内存。程序在控制台上显示的输出如下：'
- en: '![](img/047e2343-482f-480a-aa49-834805450ca6.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/047e2343-482f-480a-aa49-834805450ca6.png)'
- en: This section finishes our discussion on memory architecture in CUDA. When the
    memories available in CUDA are used judiciously according to your application,
    it improves the performance of the program drastically. You need to look carefully
    at the memory access pattern of all threads in your application and then select
    which memory you should use for your application. The last section of this chapter
    briefly describes the complex CUDA program, which uses all the concepts we have
    used up until this point.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结束了我们对CUDA内存架构的讨论。当您根据您的应用程序合理地使用CUDA中可用的内存时，它可以极大地提高程序的性能。您需要仔细查看您应用程序中所有线程的内存访问模式，然后选择您应该为您的应用程序使用的内存。本章的最后一节简要描述了使用我们到目前为止所使用的所有概念的复杂CUDA程序。
- en: Dot product and matrix multiplication example
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 点积和矩阵乘法示例
- en: Up to this point, we have learned almost all the important concepts related
    to basic parallel programming using CUDA. In this section, we will show you how
    to write CUDA programs for important mathematical operations like dot product
    and matrix multiplication, which are used in almost all applications. This will
    make use of all the concepts we saw earlier and help you in writing code for your
    applications.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们几乎已经学习了所有与CUDA基本并行编程相关的重要概念。在本节中，我们将向您展示如何编写CUDA程序来执行像点积和矩阵乘法这样的重要数学运算，这些运算几乎在所有应用中都会用到。这将利用我们之前看到的所有概念，并帮助您为您的应用程序编写代码。
- en: Dot product
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 点积
- en: 'The dot product between two vectors is an important mathematical operation.
    It will also explain one important concept in CUDA programming that is called
    **reduction** operation. The dot product between two vectors can be defined as
    follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量的点积是一个重要的数学运算。它还将解释CUDA编程中的一个重要概念，称为**归约**操作。两个向量的点积可以定义为如下：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, if you see this operation, it is very similar to an element-wise addition
    operation on vectors. Instead of addition, you have to perform element-wise multiplication.
    All threads also have to keep running the sum of multiplication they have performed
    because all individual multiplications need to be summed up to get a final answer
    of the dot product. The answer of the dot product will be a single number. This
    operation where the final answer is the reduced version of the original two arrays
    is called a **reduce** operation in CUDA. It is useful in many applications. To
    perform this operation in CUDA, we will start by writing a kernel function for
    it, as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您看到这个操作，它与向量上的逐元素加法操作非常相似。除了加法之外，您必须执行逐元素乘法。所有线程还必须继续运行它们所执行的乘法总和，因为所有单个乘法都需要相加以得到点积的最终答案。点积的答案将是一个单一的数字。在CUDA中，最终答案是原始两个数组的归约版本的操作称为**归约**操作。它在许多应用中非常有用。要在CUDA中执行此操作，我们将首先编写一个内核函数，如下所示：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `kernel` function takes two input arrays as input and stores the final partial
    sum in the third array. Shared memory is defined to store intermediate answers
    of the partial answer. The size of the shared memory is equal to the number of
    threads per block, as all separate blocks will have the separate copy of this
    shared memory. After that, two indexes are calculated; the first one, which calculates
    the unique thread ID, is similar to what we have done in the vector addition example.
    The second index is used to store the partial product answer on shared memory.
    Again, every block has a separate copy of shared memory, so only the thread ID
    used to index the shared memory is of a given block.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel` 函数接受两个输入数组作为输入，并将最终的局部和在第三个数组中存储。共享内存被定义为存储部分答案的中间答案。共享内存的大小等于每个块中的线程数，因为所有单独的块都将有这个共享内存的单独副本。之后，计算两个索引；第一个索引，用于计算唯一的线程ID，类似于我们在向量加法示例中所做的。第二个索引用于在共享内存中存储部分乘积答案。同样，每个块都有一个单独的共享内存副本，所以只有用于索引共享内存的线程ID是给定块的。'
- en: The `while` loop will perform element-wise multiplication of elements indexed
    by the thread ID. It will also do multiplication of elements that is offset by
    the total threads to the current thread ID. The partial sum of this element is
    stored in the shared memory. We are going to use these results from the shared
    memory to calculate the partial sum for a single block. So, before reading this
    shared memory block, we must ensure that all threads have finished writing to
    this shared memory. This is ensured by using the `__syncthreads()` directive.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`while` 循环将对由线程ID索引的元素执行逐元素乘法。它还将对偏移总线程数到当前线程ID的元素进行乘法。该元素的局部和存储在共享内存中。我们将使用这些来自共享内存的结果来计算单个块的局部和。因此，在读取这个共享内存块之前，我们必须确保所有线程都已经完成了对这个共享内存的写入。这通过使用
    `__syncthreads()` 指令来确保。'
- en: Now, one method to get an answer for the dot product is that one thread iterates
    over all these partial sums to get a final answer. One thread can perform the
    reduce operation. This will take N operations to complete, where N is the number
    of partial sums to be added (equal to the number of threads per block) to get
    a final answer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，获取点积答案的一种方法是一个线程遍历所有这些部分和以获得最终答案。一个线程可以执行归约操作。这将需要 N 次操作来完成，其中 N 是要添加以获得最终答案的部分和的数量（等于每个块中的线程数）。
- en: The question is, can we do this reduce operation in parallel? The answer is
    yes. The idea is that every thread will add two elements of the partial sum and
    store the answer in the location of the first element. Since each thread combines
    two entries in one, the operation can be completed in half entries. Now, we will
    repeat this operation for the remaining half until we get the final answer that
    calculates the partial sum for this entire block. The complexity of this operation
    is `log[2](N)` , which is far better than the complexity of N when one thread
    performs the reduce operation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是，我们能否并行执行这个归约操作？答案是肯定的。想法是每个线程将添加两个部分和的元素并将答案存储在第一个元素的位置。由于每个线程结合了一个条目，所以操作可以在一半的条目中完成。现在，我们将重复这个操作，直到我们得到最终的答案，这个答案计算了整个块的局部和。这个操作的复杂度是
    `log[2](N)` ，这比一个线程执行归约操作的复杂度 N 要好得多。
- en: The operation explained is calculated by the block starting with `while (i !=
    0)` . The block sums the partial answer of the current thread and the thread offset
    by `blockdim/2`. It continues this addition until we get a final single answer,
    which is a sum of all partial products in a given block. The final answer is stored
    in the global memory. Each block will have a separate answer to be stored in the
    global memory so that it is indexed by the block ID, which is unique for each
    block. Still, we have not got the final answer. This can be performed in the `device`
    function or the `main` function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 解释的操作是通过以 `while (i != 0)` 开始的块来计算的。该块将当前线程的局部答案和偏移 `blockdim/2` 的线程的局部答案相加。它继续进行这种加法，直到我们得到一个最终的单一答案，这是给定块中所有部分乘积的总和。最终的答案存储在全局内存中。每个块都将有一个单独的答案存储在全局内存中，以便通过块ID索引，每个块都有一个唯一的块ID。尽管如此，我们还没有得到最终的答案。这可以在
    `device` 函数或 `main` 函数中执行。
- en: 'Normally, the last few additions in the reduce operation need very little resources.
    Much of the GPU resource remains idle, and that is not the optimal use of the
    GPU. So, the final addition operation of all partial sums for an individual block
    is done in the `main` function. The `main` function is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在归约操作的最后几个加法中需要的资源非常少。大部分GPU资源都处于空闲状态，这不是GPU的最佳使用。因此，单个块的各个部分的总和的最终加法操作是在`main`函数中完成的。`main`函数如下：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Three arrays are defined and memory is allocated for both host and device to
    store inputs and output. The two host arrays are initialized inside a `for` loop.
    One array is initialized with `0` to `N` and the second is initialized with a
    constant value `2`. The calculation of the number of blocks in a grid and number
    of threads in a block is also done. It is similar to what we did at the start
    of this chapter. Bear in mind, you can also keep these value as constants, like
    we did in the first program of this chapter, to avoid complexity.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了三个数组，并为主机和设备分配了内存以存储输入和输出。两个主机数组在`for`循环内部初始化。一个数组初始化为从`0`到`N`，另一个数组初始化为常数`2`。计算网格中的块数和块中的线程数也已完成。这与我们在本章开头所做的是类似的。请注意，您也可以将这些值作为常数保留，就像我们在本章的第一个程序中所做的那样，以避免复杂性。
- en: 'These arrays are copied to device memory and passed as parameters to the `kernel`
    function. The `kernel` function will return an array, which has answers of the
    partial products of individual blocks indexed by their block ID. This array is
    copied back to the host in the `partial_sum` array. The final answer of the dot
    product is calculated by iterating over this `partial_sum` array, using the `for`
    loop starting from zero to the number of blocks per grid. The final dot product
    is stored in `h_c`. To check whether the calculated dot product is correct or
    not, the following code can be added to the `main` function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组被复制到设备内存，并作为参数传递给`kernel`函数。`kernel`函数将返回一个数组，该数组包含由其块ID索引的各个块的乘积答案。这个数组被复制回主机到`partial_sum`数组中。点积的最终答案通过遍历这个`partial_sum`数组，使用从零开始的`for`循环到每个网格的块数来计算。最终的点积存储在`h_c`中。为了检查计算出的点积是否正确，可以在`main`函数中添加以下代码：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The answer is verified with the answer calculated mathematically. In two input
    arrays, if one array has values from `0` to `N-1` and the second array has a constant
    value of 2, then the dot product will be `N*(N+1)`. We print the answer of the
    dot product calculated mathematically, along with whether it has been calculated
    correctly or not. The host and device memory is freed up in the end. The output
    of the program is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 答案通过数学计算的结果进行验证。在两个输入数组中，如果一个数组有从`0`到`N-1`的值，而第二个数组有一个常数值`2`，那么点积将是`N*(N+1)`。我们打印出数学计算出的点积答案，以及是否计算正确。最后释放主机和设备内存。程序的输出如下：
- en: '![](img/ec2ca87b-9356-443f-83ee-d4d81cdcc920.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec2ca87b-9356-443f-83ee-d4d81cdcc920.png)'
- en: Matrix multiplication
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'The second most important mathematical operation performed on a GPU using CUDA
    is matrix multiplication. It is a very complicated mathematical operation when
    the sizes of the matrices are very large. It should be kept in mind that for matrix
    multiplication, the number of columns in the first matrix should be equal to the
    number of rows in the second matrix. Matrix multiplication is not a cumulative
    operation. To avoid complexity, in this example, we are taking a square matrix
    of the same size. If you are familiar with the mathematics of matrix multiplication,
    then you may recall that a row in the first matrix will be multiplied with all
    the columns in the second matrix. This is repeated for all rows in the first matrix.
    It is shown as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CUDA在GPU上执行的第二重要的数学操作是矩阵乘法。当矩阵的大小非常大时，这是一个非常复杂的数学操作。应记住，对于矩阵乘法，第一个矩阵的列数应等于第二个矩阵的行数。矩阵乘法不是一个累积操作。为了避免复杂性，在这个例子中，我们取了一个相同大小的方阵。如果您熟悉矩阵乘法的数学，那么您可能会回忆起第一个矩阵的每一行将与第二个矩阵的所有列相乘。这将对第一个矩阵的所有行重复进行。如下所示：
- en: '![](img/64671ce5-f20d-4b13-a9d2-b0ac3767f5c6.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/64671ce5-f20d-4b13-a9d2-b0ac3767f5c6.png)'
- en: 'Same data is reused many times, so this is an ideal case of using shared memory.
    In this section, we will make two separate kernel functions, with and without
    using shared memory. You can compare the execution of two kernels to get an idea
    of how shared memory improves the performance of the program. We will first start
    by writing a `kernel` function without using shared memory:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的数据被多次重用，因此这是使用共享内存的理想情况。在本节中，我们将制作两个分别使用和不使用共享内存的单独的`kernel`函数。你可以比较两个内核的执行来了解共享内存如何提高程序的性能。我们首先从编写一个不使用共享内存的`kernel`函数开始：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Matrix multiplication is performed using two-dimensional threads. If we launch
    two-dimensional threads with each thread performing a single element of the output
    matrix, then up to 16 x 16 matrices can be multiplied. If the size is greater
    than this, then it will need more than 512 threads for computation, which is not
    possible on most GPUs. So, we need to launch multiple blocks with each containing
    less than 512 threads. To accomplish this, the output matrix is divided into small
    square blocks having dimensions of `TILE_SIZE` in both directions. Each thread
    in a block will calculate elements of this square block. The total number of blocks
    for matrix multiplication will be calculated by dividing the size of the matrix
    by the size of this small square defined by `TILE_SIZE`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法使用二维线程执行。如果我们使用二维线程启动，每个线程执行输出矩阵的单个元素，那么最多可以乘以16 x 16的矩阵。如果大小大于这个值，那么计算将需要超过512个线程，这在大多数GPU上是不可能的。因此，我们需要启动多个块，每个块包含少于512个线程。为了实现这一点，输出矩阵被分成小正方形块，这两个方向上的维度都是`TILE_SIZE`。块中的每个线程将计算这个正方形块的元素。矩阵乘法的总块数将通过将矩阵的大小除以由`TILE_SIZE`定义的小正方形的大小来计算。
- en: If you understand this, then calculating the row and column index for the output
    will be very easy. It is similar to what we have done up till now, with `blockdim.x`
    being equal to `TILE_SIZE`. Now, every element in the output will be the dot product
    of one row in the first matrix and one column in the second matrix. Both the matrices
    have the same size so the dot product has to be performed for a number of elements
    equal to the size variable. So the `for` loop in the `kernel` function is running
    from `0` to `size`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你理解了这一点，那么计算输出矩阵的行和列索引将会非常容易。这与我们到目前为止所做的是类似的，其中`blockdim.x`等于`TILE_SIZE`。现在，输出矩阵中的每个元素都将是一个矩阵第一行和一个矩阵第二列的点积。两个矩阵具有相同的大小，因此必须对等于大小变量的元素数量执行点积。因此，`kernel`函数中的`for`循环从`0`运行到`size`。
- en: 'To calculate the individual index of both matrices, consider that this matrix
    is stored as a linear array in system memory in row-major fashion. Its meaning
    is that all elements in the first row are placed in a consecutive memory location
    and then rows are placed one after the other, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算两个矩阵的单独索引，考虑这个矩阵以行主序方式存储在系统内存中作为一个线性数组。这意味着第一行中的所有元素都放置在连续的内存位置，然后依次放置行，如下所示：
- en: '![](img/5f13d0c1-02da-4efa-8f76-3da8da6f4735.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f13d0c1-02da-4efa-8f76-3da8da6f4735.png)'
- en: The index of a linear array can be calculated by its row ID multiplied by the
    size of the matrix plus its column ID. So, the index for *M[1,0]* will be 2 as
    its row ID is 1, the matrix size is 2 and the column ID is zero. This method is
    used to calculate the element index in both the matrices.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 线性数组的索引可以通过其行ID乘以矩阵的大小加上其列ID来计算。因此，*M[1,0]*的索引将是2，因为其行ID是1，矩阵大小是2，列ID是0。这种方法用于计算两个矩阵中的元素索引。
- en: 'To calculate the element at `[row, col]` in the resultant matrix, the index
    in the first matrix will be equal to `row*size + k` , and for the second matrix,
    it will be `k*size + col`. This is a very simple `kernel` function. There is a
    large amount of data reuse in matrix multiplication. This function is not utilizing
    the advantage of shared memory. So, we will try to modify the kernel function
    that makes use of shared memory. The modified `kernel` function is shown as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算结果矩阵中`[row, col]`位置的元素，第一个矩阵中的索引将等于`row*size + k`，而对于第二个矩阵，它将是`k*size + col`。这是一个非常简单的`kernel`函数。在矩阵乘法中有大量的数据重用。这个函数没有利用共享内存的优势。因此，我们将尝试修改利用共享内存的`kernel`函数。修改后的`kernel`函数如下所示：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A two-shared memory with size equal to the size of a small square block, which
    is `TILE_SIZE`, is defined for storing data for reuse. Row and column indexes
    are calculated in the same way as seen earlier. First, this shared memory is filled
    up in the first `for` loop. After that, `__syncthreads()` is included so that
    memory read from shared memory only happens when all threads have finished writing
    to it. The last `for` loop again calculates the dot product. As this is done by
    only using shared memory, this considerably reduces memory traffic to a global
    memory, which in turn improves the performance of the program for larger matrix
    dimensions. The `main` function of this program is shown as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了一个大小等于小方块块大小的两个共享内存，即`TILE_SIZE`，用于存储可重复使用的数据。行和列索引的计算方式与之前相同。首先，在第一个`for`循环中填充这个共享内存。之后，包含`__syncthreads()`，以确保只有当所有线程都完成写入后，才从共享内存中读取内存。最后一个`for`循环再次计算点积。由于这仅通过使用共享内存来完成，这大大减少了全局内存的内存流量，从而提高了程序在大矩阵维度上的性能。该程序的`main`函数如下所示：
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After defining and allocating memory for host and device arrays, the host array
    is filled with some random values. These arrays are copied to device memory so
    that it can be passed to the `kernel` functions. The number of grid blocks and
    the number of block threads is defined using the `dim3` structure, with the dimensions
    equal to that calculated earlier. You can call any of the kernels. The calculated
    answer is copied back to the host memory. To display the output on the console,
    the following code is added to the `main` function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义和分配主机和设备数组的内存之后，主机数组被填充了一些随机值。这些数组被复制到设备内存中，以便可以将其传递给`kernel`函数。使用`dim3`结构定义了网格块的数量和块线程的数量，其维度等于之前计算的值。您可以调用任何内核。将计算出的答案复制回主机内存。为了在控制台上显示输出，以下代码被添加到`main`函数中：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The memory used to store matrices on device memory is also freed up. The output
    on the console is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在设备内存上存储矩阵的内存也被释放。控制台输出如下：
- en: '![](img/b86bd951-bfd0-4b27-96b2-a20663044300.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b86bd951-bfd0-4b27-96b2-a20663044300.png)'
- en: This section demonstrated CUDA programs for two important mathematical operations
    used in a wide range of applications. It also explained the use of shared memory
    and multidimensional threads.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本节演示了在广泛应用的数学运算中使用的两个重要CUDA程序。它还解释了共享内存和多维线程的使用。
- en: Summary
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter explained the launch of multiple blocks, with each having multiple
    threads from the kernel function. It showed the method for choosing the two parameters
    for a large value of threads. It also explained the hierarchical memory architecture
    that can be used by CUDA programs. The memory nearest to the thread being executed
    is fast, and as we move away from it, memories get slower. When multiple threads
    want to communicate with each other, then CUDA provides the flexibility of using
    shared memory, by which threads from the same blocks can communicate with each
    other. When multiple threads use the same memory location, then there should be
    synchronization between the memory access; otherwise, the final result will not
    be as expected. We also saw the use of an atomic operation to accomplish this
    synchronization. If some parameters remain constant throughout the kernel's execution,
    then it can be stored in constant memory for speed up. When CUDA programs exhibit
    a certain communication pattern like spatial locality, then texture memory should
    be used to improve the performance of the program. To summarize, to improve the
    performance of CUDA programs, we should reduce memory traffic to slow memories.
    If this is done efficiently, drastic improvement in the performance of the program
    can be achieved.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了多个块的启动，每个块都有来自内核函数的多个线程。它展示了选择大量线程的两个参数的方法。它还解释了CUDA程序可以使用的分层内存架构。最接近正在执行的线程的内存速度快，随着我们远离它，内存速度变慢。当多个线程想要相互通信时，CUDA提供了使用共享内存的灵活性，使得同一块的线程可以相互通信。当多个线程使用相同的内存位置时，应该在内存访问之间进行同步；否则，最终结果将不会如预期。我们还看到了使用原子操作来完成这种同步的方法。如果某些参数在整个内核执行过程中保持不变，则可以将其存储在常量内存中以提高速度。当CUDA程序表现出某种通信模式，如空间局部性时，应使用纹理内存来提高程序的性能。总之，为了提高CUDA程序的性能，我们应该减少慢速内存的内存流量。如果这样做效率高，程序的性能可以得到显著提高。
- en: In the next chapter, the concept of CUDA streams will be discussed, which is
    similar to multitasking in CPU programs. How we can measure the performance of
    CUDA programs will also be discussed. It will also show the use of CUDA in simple
    image processing applications.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论 CUDA 流的概念，它与 CPU 程序中的多任务类似。我们还将讨论如何衡量 CUDA 程序的性能。它还将展示 CUDA 在简单的图像处理应用中的使用。
- en: Questions
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose you want to launch 100,000 threads in parallel. What is the best choice
    of the number of blocks in a grid and number of threads in a block and why?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想要并行启动 100,000 个线程。在网格中块的数量和每个块中的线程数量最佳选择是什么，为什么？
- en: Write a CUDA program to find out the cube of every element in an array when
    the number of elements in the array is 100,000.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个 CUDA 程序，找出数组中每个元素的立方值，当数组中的元素数量为 100,000 时。
- en: 'State whether the following statement is true or false and give a reason: An
    assignment operator between local variables will be faster than an assignment
    operator between global variables.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下陈述是对还是错，并给出理由：局部变量之间的赋值运算符将比全局变量之间的赋值运算符更快。
- en: What is register spilling? How it can harm the performance of your CUDA program?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册溢出是什么？它如何损害你的 CUDA 程序的性能？
- en: 'State whether the following line of code will give the required output or not:
    `d_out[i]` = `d_out[i-1]`.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下代码行是否会给出所需输出：`d_out[i]` = `d_out[i-1]`。
- en: 'State whether the following statement is true or false and give a reason: Atomic
    operations increase the execution time for CUDA programs.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下陈述是对还是错，并给出理由：原子操作会增加 CUDA 程序的执行时间。
- en: Which kinds of communication patterns are ideal for using texture memory in
    your CUDA programs?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 CUDA 程序中使用纹理内存的理想通信模式有哪些？
- en: What will be the effect of using the `__syncthreads` directive inside an if
    statement?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 if 语句中使用 `__syncthreads` 指令会有什么影响？
