- en: Chapter 3. Clustering – Finding Related Posts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 聚类——寻找相关帖子
- en: In the previous chapter, you learned how to find the classes or categories of
    individual datapoints. With a handful of training data items that were paired
    with their respective classes, you learned a model, which we can now use to classify
    future data items. We called this supervised learning because the learning was
    guided by a teacher; in our case, the teacher had the form of correct classifications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学会了如何找到单个数据点的类别或类别。通过一小部分带有相应类别的训练数据，你学到了一个模型，我们现在可以用它来分类未来的数据项。我们称这种方法为监督学习，因为学习过程是由老师引导的；在我们这里，老师表现为正确的分类。
- en: Let's now imagine that we do not possess those labels by which we can learn
    the classification model. This could be, for example, because they were too expensive
    to collect. Just imagine the cost if the only way to obtain millions of labels
    will be to ask humans to classify those manually. What could we have done in that
    case?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们没有那些标签来学习分类模型。例如，可能是因为收集这些标签的成本过高。试想一下，如果获得数百万个标签的唯一方式是让人类手动分类，那会有多么昂贵。那我们该如何应对这种情况呢？
- en: Well, of course, we will not be able to learn a classification model. Still,
    we could find some pattern within the data itself. That is, let the data describe
    itself. This is what we will do in this chapter, where we consider the challenge
    of a question and answer website. When a user is browsing our site, perhaps because
    they were searching for particular information, the search engine will most likely
    point them to a specific answer. If the presented answers are not what they were
    looking for, the website should present (at least) the related answers so that
    they can quickly see what other answers are available and hopefully stay on our
    site.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们无法学习一个分类模型。然而，我们可以在数据本身中找到某些模式。也就是说，让数据自我描述。这就是我们在本章要做的事情，我们将面临一个问答网站的挑战。当用户浏览我们的网站时，可能是因为他们在寻找特定信息，搜索引擎最有可能将他们指向一个特定的答案。如果所呈现的答案不是他们想要的，网站应该至少提供相关答案，让用户能够快速看到其他可用的答案，并希望能够停留在我们的网站上。
- en: The naïve approach will be to simply take the post, calculate its similarity
    to all other posts and display the top *n* most similar posts as links on the
    page. Quickly, this will become very costly. Instead, we need a method that quickly
    finds all the related posts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真的方法是直接拿帖子，计算它与所有其他帖子的相似度，并将最相似的*前n个*帖子作为链接显示在页面上。很快，这将变得非常昂贵。相反，我们需要一种方法，能够快速找到所有相关的帖子。
- en: We will achieve this goal in this chapter using clustering. This is a method
    of arranging items so that similar items are in one cluster and dissimilar items
    are in distinct ones. The tricky thing that we have to tackle first is how to
    turn text into something on which we can calculate similarity. With such a similarity
    measurement, we will then proceed to investigate how we can leverage that to quickly
    arrive at a cluster that contains similar posts. Once there, we will only have
    to check out those documents that also belong to that cluster. To achieve this,
    we will introduce you to the marvelous SciKit library, which comes with diverse
    machine learning methods that we will also use in the following chapters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过聚类来实现这一目标。这是一种将项目排列在一起的方法，使得相似的项目在同一个簇中，而不同的项目则在不同的簇中。我们首先要解决的棘手问题是如何将文本转换为能够计算相似度的形式。有了这样的相似度测量后，我们将继续探讨如何利用它快速找到包含相似帖子的小组。一旦找到了，我们只需要检查那些也属于该小组的文档。为了实现这一目标，我们将向你介绍神奇的SciKit库，它提供了多种机器学习方法，我们将在接下来的章节中使用这些方法。
- en: Measuring the relatedness of posts
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量帖子之间的相关性
- en: From the machine learning point of view, raw text is useless. Only if we manage
    to transform it into meaningful numbers, can we then feed it into our machine
    learning algorithms, such as clustering. This is true for more mundane operations
    on text such as similarity measurement.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，原始文本是无用的。只有当我们能够将其转换为有意义的数字时，才能将其输入到机器学习算法中，比如聚类。这对于文本的更常见操作，如相似度测量，亦是如此。
- en: How not to do it
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何避免这么做
- en: One text similarity measure is the Levenshtein distance, which also goes by
    the name Edit Distance. Let's say we have two words, "machine" and "mchiene".
    The similarity between them can be expressed as the minimum set of edits that
    are necessary to turn one word into the other. In this case, the edit distance
    will be 2, as we have to add an "a" after the "m" and delete the first "e". This
    algorithm is, however, quite costly as it is bound by the length of the first
    word times the length of the second word.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一种文本相似度度量方法是 Levenshtein 距离，也叫编辑距离。假设我们有两个单词，“machine”和“mchiene”。它们之间的相似度可以通过将一个单词转换成另一个单词所需的最少编辑次数来表示。在这种情况下，编辑距离是
    2，因为我们需要在“m”后面添加一个“a”，并删除第一个“e”。然而，这个算法的代价比较高，因为它的时间复杂度是第一个单词的长度乘以第二个单词的长度。
- en: Looking at our posts, we could cheat by treating whole words as characters and
    performing the edit distance calculation on the word level. Let's say we have
    two posts (let's concentrate on the following title, for simplicity's sake) called
    "How to format my hard disk" and "Hard disk format problems", we will need an
    edit distance of 5 because of removing "how", "to", "format", "my" and then adding
    "format" and "problems" in the end. Thus, one could express the difference between
    two posts as the number of words that have to be added or deleted so that one
    text morphs into the other. Although we could speed up the overall approach quite
    a bit, the time complexity remains the same.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们的帖子，我们可以通过将整个单词视为字符，并在单词层面上进行编辑距离计算来“作弊”。假设我们有两个帖子（为了简单起见，我们集中关注以下标题）：“How
    to format my hard disk”和“Hard disk format problems”，由于删除“how”，“to”，“format”，“my”，然后在最后添加“format”和“problems”，我们需要编辑距离为
    5。因此，可以将两个帖子之间的差异表示为需要添加或删除的单词数量，以便一个文本转变为另一个文本。尽管我们可以大大加速整体方法，但时间复杂度保持不变。
- en: But even if it would have been fast enough, there is another problem. In the
    earlier post, the word "format" accounts for an edit distance of 2, due to deleting
    it first, then adding it. So, our distance seems to be not robust enough to take
    word reordering into account.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使速度足够快，还是存在另一个问题。在前面的帖子中，“format”一词的编辑距离为 2，因为它首先被删除，然后又被添加。因此，我们的距离度量似乎还不够稳健，无法考虑单词顺序的变化。
- en: How to do it
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'More robust than edit distance is the so-called **bag of word** approach. It
    totally ignores the order of words and simply uses word counts as their basis.
    For each word in the post, its occurrence is counted and noted in a vector. Not
    surprisingly, this step is also called vectorization. The vector is typically
    huge as it contains as many elements as words occur in the whole dataset. Take,
    for instance, two example posts with the following word counts:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 比编辑距离更为稳健的方法是所谓的**词袋模型**。它完全忽略了单词的顺序，仅仅通过单词的计数来作为基础。对于每个帖子中的单词，它的出现次数会被计数并记录在一个向量中。不出所料，这一步也叫做向量化。这个向量通常非常庞大，因为它包含了整个数据集中出现的单词数量。例如，考虑两个帖子及其单词计数如下：
- en: '| Word | Occurrences in post 1 | Occurrences in post 2 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| Word | 在帖子 1 中的出现次数 | 在帖子 2 中的出现次数 |'
- en: '| --- | --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| disk | 1 | 1 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| disk | 1 | 1 |'
- en: '| format | 1 | 1 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| format | 1 | 1 |'
- en: '| how | 1 | 0 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| how | 1 | 0 |'
- en: '| hard | 1 | 1 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| hard | 1 | 1 |'
- en: '| my | 1 | 0 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| my | 1 | 0 |'
- en: '| problems | 0 | 1 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| problems | 0 | 1 |'
- en: '| to | 1 | 0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| to | 1 | 0 |'
- en: 'The columns Occurrences in post 1 and Occurrences in post 2 can now be treated
    as simple vectors. We can simply calculate the Euclidean distance between the
    vectors of all posts and take the nearest one (too slow, as we have found out
    earlier). And as such, we can use them later as our feature vectors in the clustering
    steps according to the following procedure:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “在帖子 1 中的出现次数”和“在帖子 2 中的出现次数”这两列现在可以视为简单的向量。我们可以直接计算所有帖子向量之间的欧几里得距离，并取最近的一个（但这太慢了，正如我们之前发现的那样）。因此，我们可以根据以下步骤将它们作为聚类步骤中的特征向量使用：
- en: Extract salient features from each post and store it as a vector per post.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个帖子中提取显著特征，并将其存储为每个帖子的向量。
- en: Then compute clustering on the vectors.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后对这些向量进行聚类计算。
- en: Determine the cluster for the post in question.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定相关帖子的聚类。
- en: From this cluster, fetch a handful of posts having a different similarity to
    the post in question. This will increase diversity.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个聚类中提取一些与目标帖子相似度不同的帖子。这将增加多样性。
- en: But there is some more work to be done before we get there. Before we can do
    that work, we need some data to work on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们进入下一步之前，还需要做一些准备工作。在我们开始这项工作之前，我们需要一些数据来处理。
- en: Preprocessing – similarity measured as a similar number of common words
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理 – 相似度通过相同单词的数量来衡量
- en: As we have seen earlier, the bag of word approach is both fast and robust. It
    is, though, not without challenges. Let's dive directly into them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，词袋方法既快速又稳健。但它也并非没有挑战。让我们直接深入探讨这些挑战。
- en: Converting raw text into a bag of words
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将原始文本转换为词袋
- en: 'We do not have to write custom code for counting words and representing those
    counts as a vector. SciKit''s `CountVectorizer` method does the job not only efficiently
    but also has a very convenient interface. SciKit''s functions and classes are
    imported via the `sklearn` package:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要编写自定义代码来计数单词并将这些计数表示为向量。SciKit 的 `CountVectorizer` 方法不仅高效完成这项工作，而且界面也非常便捷。SciKit
    的函数和类是通过 `sklearn` 包导入的：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `min_df` parameter determines how `CountVectorizer` treats seldom words
    (minimum document frequency). If it is set to an integer, all words occurring
    less than that value will be dropped. If it is a fraction, all words that occur
    in less than that fraction of the overall dataset will be dropped. The `max_df`
    parameter works in a similar manner. If we print the instance, we see what other
    parameters SciKit provides together with their default values:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_df` 参数决定了 `CountVectorizer` 如何处理少见词（最小文档频率）。如果设置为整数，所有出现频率低于该值的词汇将被丢弃。如果设置为小数，则所有在整体数据集中出现频率低于该小数的词汇将被丢弃。`max_df`
    参数以类似的方式工作。如果我们打印实例，我们可以看到 SciKit 提供的其他参数及其默认值：'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We see that, as expected, the counting is done at word level (`analyzer=word`)
    and words are determined by the regular expression pattern `token_pattern`. It
    will, for example, tokenize "cross-validated" into "cross" and "validated". Let''s
    ignore the other parameters for now and consider the following two example subject
    lines:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，正如预期的那样，计数是按单词级别进行的（`analyzer=word`），并且单词是通过正则表达式模式 `token_pattern` 来确定的。例如，它会将“cross-validated”拆分为“cross”和“validated”。暂时忽略其他参数，我们考虑以下两个示例主题行：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can now put this list of subject lines into the `fit_transform()` function
    of our vectorizer, which does all the hard vectorization work.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这个主题行列表传入我们向量化器的`fit_transform()`函数，它会完成所有复杂的向量化工作。
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The vectorizer has detected seven words for which we can fetch the counts individually:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化器已经检测到七个词汇，我们可以单独获取它们的计数：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This means that the first sentence contains all the words except "problems",
    while the second contains all but "how", "my", and "to". In fact, these are exactly
    the same columns as we have seen in the preceding table. From `X`, we can extract
    a feature vector that we will use to compare two documents with each other.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着第一句包含了除了“problems”之外的所有单词，而第二句包含了除了“how”、“my”和“to”之外的所有单词。事实上，这些正是我们在前面表格中看到的相同列。从`X`中，我们可以提取出一个特征向量，用来比较两个文档之间的差异。
- en: We will start with a naïve approach first, to point out some preprocessing peculiarities
    we have to account for. So let's pick a random post, for which we then create
    the count vector. We will then compare its distance to all the count vectors and
    fetch the post with the smallest one.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用一种天真的方法，指出一些我们必须考虑的预处理特性。然后我们选择一个随机帖子，为它创建计数向量。接着我们将比较它与所有计数向量的距离，并提取出距离最小的那个帖子。
- en: Counting words
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计数单词
- en: 'Let''s play with the toy dataset consisting of the following posts:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们玩玩这个由以下帖子组成的玩具数据集：
- en: '| Post filename | Post content |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 帖子文件名 | 帖子内容 |'
- en: '| --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `01.txt` | This is a toy post about machine learning. Actually, it contains
    not much interesting stuff. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| `01.txt` | 这是一个关于机器学习的玩具帖子。实际上，它并没有太多有趣的内容。 |'
- en: '| `02.txt` | Imaging databases can get huge. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| `02.txt` | 成像数据库可能非常庞大。 |'
- en: '| `03.txt` | Most imaging databases save images permanently. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `03.txt` | 大多数成像数据库会永久保存图像。 |'
- en: '| `04.txt` | Imaging databases store images. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `04.txt` | 成像数据库存储图像。 |'
- en: '| `05.txt` | Imaging databases store images. Imaging databases store images.
    Imaging databases store images. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `05.txt` | 成像数据库存储图像。成像数据库存储图像。成像数据库存储图像。 |'
- en: In this post dataset, we want to find the most similar post for the short post
    "imaging databases".
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个帖子数据集中，我们希望找到与短帖子“成像数据库”最相似的帖子。
- en: 'Assuming that the posts are located in the directory `DIR`, we can feed `CountVectorizer`
    with it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设帖子位于目录 `DIR` 中，我们可以将它传入 `CountVectorizer`：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We have to notify the vectorizer about the full dataset so that it knows upfront
    what words are to be expected:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通知向量化器有关完整数据集的信息，以便它提前知道哪些词汇是预期的：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Unsurprisingly, we have five posts with a total of 25 different words. The
    following words that have been tokenized will be counted:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，我们有五篇帖子，总共有25个不同的单词。以下是已被标记的单词，将被计数：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we can vectorize our new post.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将新帖子向量化了。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that the count vectors returned by the `transform` method are sparse.
    That is, each vector does not store one count value for each word, as most of
    those counts will be zero (the post does not contain the word). Instead, it uses
    the more memory-efficient implementation `coo_matrix` (for "COOrdinate"). Our
    new post, for instance, actually contains only two elements:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`transform`方法返回的计数向量是稀疏的。也就是说，每个向量不会为每个单词存储一个计数值，因为大多数计数值都是零（该帖子不包含该单词）。相反，它使用了更节省内存的实现`coo_matrix`（"COOrdinate"）。例如，我们的新帖子实际上只包含两个元素：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Via its `toarray()` member, we can once again access the full `ndarray`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过其`toarray()`成员，我们可以再次访问完整的`ndarray`：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We need to use the full array, if we want to use it as a vector for similarity
    calculations. For the similarity measurement (the naïve one), we calculate the
    Euclidean distance between the count vectors of the new post and all the old posts:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将其用作相似性计算的向量，我们需要使用整个数组。对于相似性测量（最简单的计算方法），我们计算新帖子和所有旧帖子之间计数向量的欧几里得距离：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `norm()` function calculates the Euclidean norm (shortest distance). This
    is just one obvious first pick and there are many more interesting ways to calculate
    the distance. Just take a look at the paper *Distance Coefficients between Two
    Lists or Sets* in The Python Papers Source Codes, in which Maurice Ling nicely
    presents 35 different ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`norm()`函数计算欧几里得范数（最短距离）。这只是一个明显的首选，实际上有许多更有趣的方式来计算距离。你可以看看论文《*Distance Coefficients
    between Two Lists or Sets*》在《The Python Papers Source Codes》中，Maurice Ling 精妙地展示了35种不同的计算方法。'
- en: 'With `dist_raw`, we just need to iterate over all the posts and remember the
    nearest one:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dist_raw`，我们只需遍历所有帖子并记住最接近的一个：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Congratulations, we have our first similarity measurement. Post 0 is most dissimilar
    from our new post. Quite understandably, it does not have a single word in common
    with the new post. We can also understand that Post 1 is very similar to the new
    post, but not the winner, as it contains one word more than Post 3, which is not
    contained in the new post.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们已经得到了第一次相似性测量结果。帖子0与我们新帖子的相似性最小。可以理解的是，它与新帖子没有一个共同的单词。我们也可以理解，帖子1与新帖子非常相似，但并不是最相似的，因为它包含了一个比帖子3多的单词，而该单词在新帖子中并不存在。
- en: Looking at Post 3 and Post 4, however, the picture is not so clear any more.
    Post 4 is the same as Post 3 duplicated three times. So, it should also be of
    the same similarity to the new post as Post 3.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，看看帖子3和帖子4，情况就不那么清晰了。帖子4是帖子3的三倍复制。因此，它与新帖子的相似性应该与帖子3相同。
- en: 'Printing the corresponding feature vectors explains why:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 打印相应的特征向量可以解释为什么：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Obviously, using only the counts of the raw words is too simple. We will have
    to normalize them to get vectors of unit length.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，仅使用原始单词的计数太简单了。我们必须将它们归一化，以获得单位长度的向量。
- en: Normalizing word count vectors
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 归一化单词计数向量
- en: 'We will have to extend `dist_raw` to calculate the vector distance not on the
    raw vectors but on the normalized instead:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要扩展`dist_raw`，以便计算向量距离时不使用原始向量，而是使用归一化后的向量：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This leads to the following similarity measurement:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下的相似性测量结果：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This looks a bit better now. Post 3 and Post 4 are calculated as being equally
    similar. One could argue whether that much repetition would be a delight to the
    reader, but from the point of counting the words in the posts this seems to be
    right.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来好多了。帖子3和帖子4被计算为相等的相似度。有人可能会争辩说如此重复的内容是否能让读者感到愉悦，但从计算帖子中单词数量的角度来看，这似乎是正确的。
- en: Removing less important words
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除不太重要的单词
- en: Let's have another look at Post 2\. Of its words that are not in the new post,
    we have "most", "save", "images", and "permanently". They are actually quite different
    in the overall importance to the post. Words such as "most" appear very often
    in all sorts of different contexts and are called stop words. They do not carry
    as much information and thus should not be weighed as much as words such as "images",
    which doesn't occur often in different contexts. The best option would be to remove
    all the words that are so frequent that they do not help to distinguish between
    different texts. These words are called stop words.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看看帖子2。它与新帖子中的不同单词有“most”、“save”、“images”和“permanently”。这些词在整体重要性上其实是相当不同的。“most”这样的词在各种不同的语境中出现得非常频繁，被称为停用词。它们并不包含太多信息，因此不应像“images”这样的词那样被赋予同等重要性，因为“images”并不经常出现在不同的语境中。最佳的做法是移除那些频繁出现、不帮助区分不同文本的词。这些词被称为停用词。
- en: 'As this is such a common step in text processing, there is a simple parameter
    in `CountVectorizer` to achieve that:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是文本处理中的常见步骤，`CountVectorizer` 中有一个简单的参数可以实现这一点：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you have a clear picture of what kind of stop words you would want to remove,
    you can also pass a list of them. Setting `stop_words` to `english` will use a
    set of 318 English stop words. To find out which ones, you can use `get_stop_words()`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您清楚想要移除哪些停用词，您也可以传递一个词表。将 `stop_words` 设置为 `english` 将使用 318 个英语停用词的集合。要查看这些停用词，您可以使用
    `get_stop_words()`：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The new word list is seven words lighter:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 新的单词列表减少了七个单词：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Without stop words, we arrive at the following similarity measurement:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 没有停用词后，我们得到了以下的相似度度量：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Post 2 is now on par with Post 1\. It has, however, changed not much overall
    since our posts are kept short for demonstration purposes. It will become vital
    when we look at real-world data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在帖子 2 与帖子 1 相当。然而，由于我们的帖子为了演示目的保持简短，它们的变化并不大。它将在我们查看实际数据时变得至关重要。
- en: Stemming
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取
- en: One thing is still missing. We count similar words in different variants as
    different words. Post 2, for instance, contains "imaging" and "images". It will
    make sense to count them together. After all, it is the same concept they are
    referring to.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事没有完成。我们将不同形式的相似单词计为不同的单词。例如，帖子 2 包含了 "imaging" 和 "images"。将它们计为相同的词是有意义的。毕竟，它们指的是相同的概念。
- en: We need a function that reduces words to their specific word stem. SciKit does
    not contain a stemmer by default. With the **Natural Language Toolkit** (**NLTK**),
    we can download a free software toolkit, which provides a stemmer that we can
    easily plug into `CountVectorizer`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个将单词还原为其特定词干的函数。SciKit 默认不包含词干提取器。通过 **自然语言工具包**（**NLTK**），我们可以下载一个免费的软件工具包，提供一个可以轻松集成到
    `CountVectorizer` 中的词干提取器。
- en: Installing and using NLTK
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装和使用 NLTK
- en: How to install NLTK on your operating system is described in detail at [http://nltk.org/install.html](http://nltk.org/install.html).
    Unfortunately, it is not yet officially supported for Python 3, which means that
    also pip install will not work. We can, however, download the package from [http://www.nltk.org/nltk3-alpha/](http://www.nltk.org/nltk3-alpha/)
    and install it manually after uncompressing using Python's `setup.py` install.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在您的操作系统上安装 NLTK 的详细说明可以在 [http://nltk.org/install.html](http://nltk.org/install.html)
    上找到。不幸的是，它目前还没有正式支持 Python 3，这意味着 pip 安装也无法使用。然而，我们可以从 [http://www.nltk.org/nltk3-alpha/](http://www.nltk.org/nltk3-alpha/)
    下载该软件包，在解压后使用 Python 的 `setup.py` 安装进行手动安装。
- en: 'To check whether your installation was successful, open a Python interpreter
    and type:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查您的安装是否成功，请打开 Python 解释器并输入：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: You will find a very nice tutorial to NLTK in the book *Python 3 Text Processing
    with NLTK 3 Cookbook*, *Jacob Perkins*, *Packt Publishing*. To play a little bit
    with a stemmer, you can visit the web page [http://text-processing.com/demo/stem/](http://text-processing.com/demo/stem/).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在《*Python 3 Text Processing with NLTK 3 Cookbook*》一书中找到关于 NLTK 的一个非常好的教程，作者是
    *Jacob Perkins*，由 *Packt Publishing* 出版。为了稍微体验一下词干提取器，您可以访问网页 [http://text-processing.com/demo/stem/](http://text-processing.com/demo/stem/)。
- en: NLTK comes with different stemmers. This is necessary, because every language
    has a different set of rules for stemming. For English, we can take `SnowballStemmer`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 提供了不同的词干提取器。这是必要的，因为每种语言都有不同的词干提取规则。对于英语，我们可以使用 `SnowballStemmer`。
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: Note that stemming does not necessarily have to result in valid English words.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，词干提取不一定会产生有效的英语单词。
- en: 'It also works with verbs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它也适用于动词：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This means, it works most of the time:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它大多数时候都能正常工作：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Extending the vectorizer with NLTK's stemmer
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用NLTK的词干提取器扩展向量器
- en: 'We need to stem the posts before we feed them into `CountVectorizer`. The class
    provides several hooks with which we can customize the stage''s preprocessing
    and tokenization. The preprocessor and tokenizer can be set as parameters in the
    constructor. We do not want to place the stemmer into any of them, because we
    will then have to do the tokenization and normalization by ourselves. Instead,
    we overwrite the `build_analyzer` method:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在将帖子输入到`CountVectorizer`之前，我们需要进行词干提取。该类提供了几个钩子，允许我们自定义该阶段的预处理和分词。预处理器和分词器可以作为构造函数中的参数进行设置。我们不希望将词干提取器放入其中，因为那样我们就必须自己进行分词和归一化处理。相反，我们重写`build_analyzer`方法：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will do the following process for each post:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这将对每篇帖子执行以下过程：
- en: The first step is lower casing the raw post in the preprocessing step (done
    in the parent class).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是在预处理步骤中将原始帖子转换为小写（由父类完成）。
- en: Extracting all individual words in the tokenization step (done in the parent
    class).
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分词步骤中提取所有单独的词汇（由父类完成）。
- en: This concludes with converting each word into its stemmed version.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的结果是将每个单词转换为其词干形式。
- en: 'As a result, we now have one feature less, because "images" and "imaging" collapsed
    to one. Now, the set of feature names is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是我们现在少了一个特征，因为“images”和“imaging”合并为一个特征。现在，特征名称集如下所示：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Running our new stemmed vectorizer over our posts, we see that collapsing "imaging"
    and "images", revealed that actually Post 2 is the most similar post to our new
    post, as it contains the concept "imag" twice:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的新词干提取向量器对帖子进行处理时，我们看到将“imaging”和“images”合并后，实际上帖子2与我们的新帖子最为相似，因为它包含了“imag”这个概念两次：
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Stop words on steroids
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停用词强化版
- en: Now that we have a reasonable way to extract a compact vector from a noisy textual
    post, let's step back for a while to think about what the feature values actually
    mean.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种合理的方式来从嘈杂的文本帖子中提取紧凑的向量，让我们退一步思考一下这些特征值实际上意味着什么。
- en: The feature values simply count occurrences of terms in a post. We silently
    assumed that higher values for a term also mean that the term is of greater importance
    to the given post. But what about, for instance, the word "subject", which naturally
    occurs in each and every single post? Alright, we can tell `CountVectorizer` to
    remove it as well by means of its `max_df` parameter. We can, for instance, set
    it to `0.9` so that all words that occur in more than 90 percent of all posts
    will always be ignored. But, what about words that appear in 89 percent of all
    posts? How low will we be willing to set `max_df`? The problem is that however
    we set it, there will always be the problem that some terms are just more discriminative
    than others.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值仅仅是计数帖子中术语的出现次数。我们默认假设某个术语的值越高，意味着该术语对给定帖子越重要。但是，像“subject”这样的词怎么处理呢？它在每篇帖子中都自然出现。好吧，我们可以告诉`CountVectorizer`通过它的`max_df`参数来删除它。例如，我们可以将其设置为`0.9`，这样所有出现在90%以上帖子中的词将始终被忽略。但是，像出现在89%帖子中的词呢？我们应该将`max_df`设置得多低呢？问题在于，无论我们怎么设置，总会有一些术语比其他术语更具区分性。
- en: This can only be solved by counting term frequencies for every post and in addition
    discount those that appear in many posts. In other words, we want a high value
    for a given term in a given value, if that term occurs often in that particular
    post and very seldom anywhere else.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这只能通过对每篇帖子的术语频率进行计数来解决，并且要对出现在许多帖子中的术语进行折扣处理。换句话说，如果某个术语在特定帖子中出现得很频繁，而在其他地方很少出现，我们希望它的值较高。
- en: 'This is exactly what **term frequency – inverse document frequency** (**TF-IDF**)
    does. TF stands for the counting part, while IDF factors in the discounting. A
    naïve implementation will look like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是**词频–逆文档频率**（**TF-IDF**）所做的。TF代表计数部分，而IDF考虑了折扣。一个简单的实现可能如下所示：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You see that we did not simply count the terms, but also normalize the counts
    by the document length. This way, longer documents do not have an unfair advantage
    over shorter ones.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到我们不仅仅是计数术语，还通过文档长度对计数进行了归一化。这样，较长的文档就不会相对于较短的文档占有不公平的优势。
- en: 'For the following documents, `D`, consisting of three already tokenized documents,
    we can see how the terms are treated differently, although all appear equally
    often per document:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下文档`D`，它由三篇已经分词的文档组成，我们可以看到术语是如何被不同对待的，尽管它们在每篇文档中出现的频率相同：
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We see that `a` carries no meaning for any document since it is contained everywhere.
    The `b` term is more important for the document `abb` than for `abc` as it occurs
    there twice.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，`a` 对任何文档没有意义，因为它在所有地方都出现。`b` 这个术语对文档 `abb` 更重要，因为它在那里出现了两次，而在 `abc` 中只有一次。
- en: 'In reality, there are more corner cases to handle than the preceding example
    does. Thanks to SciKit, we don''t have to think of them as they are already nicely
    packaged in `TfidfVectorizer`, which is inherited from `CountVectorizer`. Sure
    enough, we don''t want to miss our stemmer:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，需要处理的特殊情况比前面的例子更多。感谢SciKit，我们不必再考虑它们，因为它们已经被很好地封装在 `TfidfVectorizer` 中，该类继承自
    `CountVectorizer`。当然，我们不想忘记使用词干提取器：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The resulting document vectors will not contain counts any more. Instead they
    will contain the individual TF-IDF values per term.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 结果得到的文档向量将不再包含计数信息，而是包含每个术语的单独TF-IDF值。
- en: Our achievements and goals
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的成就和目标
- en: 'Our current text pre-processing phase includes the following steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的文本预处理阶段包括以下步骤：
- en: Firstly, tokenizing the text.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，对文本进行标记化处理。
- en: This is followed by throwing away words that occur way too often to be of any
    help in detecting relevant posts.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是丢弃那些出现过于频繁的单词，因为它们对检测相关帖子没有任何帮助。
- en: Throwing away words that occur way so seldom so that there is only little chance
    that they occur in future posts.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃那些出现频率极低的单词，这些单词几乎不会出现在未来的帖子中。
- en: Counting the remaining words.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算剩余单词的频率。
- en: Finally, calculating TF-IDF values from the counts, considering the whole text
    corpus.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，从计数中计算TF-IDF值，考虑整个文本语料库。
- en: Again, we can congratulate ourselves. With this process, we are able to convert
    a bunch of noisy text into a concise representation of feature values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 再次祝贺我们自己。通过这个过程，我们能够将一堆杂乱无章的文本转换为简洁的特征值表示。
- en: 'But, as simple and powerful the bag of words approach with its extensions is,
    it has some drawbacks, which we should be aware of:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，尽管词袋方法及其扩展既简单又强大，但它也有一些缺点，我们应该意识到：
- en: '**It does not cover word relations**: With the aforementioned vectorization
    approach, the text "Car hits wall" and "Wall hits car" will both have the same
    feature vector.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它没有覆盖单词之间的关系**：使用上述的向量化方法，文本 "Car hits wall" 和 "Wall hits car" 将具有相同的特征向量。'
- en: '**It does not capture negations correctly**: For instance, the text "I will
    eat ice cream" and "I will not eat ice cream" will look very similar by means
    of their feature vectors although they contain quite the opposite meaning. This
    problem, however, can be easily changed by not only counting individual words,
    also called "unigrams", but instead also considering bigrams (pairs of words)
    or trigrams (three words in a row).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它无法正确捕捉否定**：例如，文本 "I will eat ice cream" 和 "I will not eat ice cream" 在它们的特征向量上看起来非常相似，尽管它们传达了完全相反的意思。然而，这个问题可以通过不仅统计单个词汇（也称为“单元词”），还考虑二元词组（词对）或三元词组（三个连续的词）来轻松解决。'
- en: '**It totally fails with misspelled words**: Although it is clear to the human
    beings among us readers that "database" and "databas" convey the same meaning,
    our approach will treat them as totally different words.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它在处理拼写错误的单词时完全失败**：虽然对我们这些人类读者来说，“database”和“databas”传达的是相同的意义，但我们的处理方法会将它们视为完全不同的单词。'
- en: For brevity's sake, let's nevertheless stick with the current approach, which
    we can now use to efficiently build clusters from.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们仍然坚持使用当前的方法，利用它可以高效地构建簇。
- en: Clustering
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Finally, we have our vectors, which we believe capture the posts to a sufficient
    degree. Not surprisingly, there are many ways to group them together. Most clustering
    algorithms fall into one of the two methods: flat and hierarchical clustering.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了向量，我们认为它们足以捕捉帖子内容。不出所料，有许多方法可以将它们进行分组。大多数聚类算法可以归纳为两种方法：平面聚类和层次聚类。
- en: Flat clustering divides the posts into a set of clusters without relating the
    clusters to each other. The goal is simply to come up with a partitioning such
    that all posts in one cluster are most similar to each other while being dissimilar
    from the posts in all other clusters. Many flat clustering algorithms require
    the number of clusters to be specified up front.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 平面聚类将帖子分为一组簇，而不考虑簇之间的关系。目标仅仅是找到一种划分方式，使得同一簇中的所有帖子彼此最为相似，同时与其他簇中的帖子差异较大。许多平面聚类算法要求在开始时就指定簇的数量。
- en: In hierarchical clustering, the number of clusters does not have to be specified.
    Instead, hierarchical clustering creates a hierarchy of clusters. While similar
    posts are grouped into one cluster, similar clusters are again grouped into one
    *uber-cluster*. This is done recursively, until only one cluster is left that
    contains everything. In this hierarchy, one can then choose the desired number
    of clusters after the fact. However, this comes at the cost of lower efficiency.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类中，聚类的数量不需要事先指定。相反，层次聚类会创建一个聚类的层级结构。相似的文档会被分到一个聚类中，而相似的聚类又会被分到一个*超聚类*中。这是递归进行的，直到只剩下一个包含所有内容的聚类。在这个层级结构中，我们可以在事后选择所需的聚类数量。然而，这样做的代价是效率较低。
- en: SciKit provides a wide range of clustering approaches in the `sklearn.cluster`
    package. You can get a quick overview of advantages and drawbacks of each of them
    at [http://scikit-learn.org/dev/modules/clustering.html](http://scikit-learn.org/dev/modules/clustering.html).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: SciKit 提供了 `sklearn.cluster` 包中多种聚类方法。你可以在[http://scikit-learn.org/dev/modules/clustering.html](http://scikit-learn.org/dev/modules/clustering.html)中快速了解它们的优缺点。
- en: In the following sections, we will use the flat clustering method K-means and
    play a bit with the desired number of clusters.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用平面聚类方法 K-means，并尝试调整聚类数量。
- en: K-means
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means
- en: k-means is the most widely used flat clustering algorithm. After initializing
    it with the desired number of clusters, `num_clusters`, it maintains that number
    of so-called cluster centroids. Initially, it will pick any `num_clusters` posts
    and set the centroids to their feature vector. Then it will go through all other
    posts and assign them the nearest centroid as their current cluster. Following
    this, it will move each centroid into the middle of all the vectors of that particular
    class. This changes, of course, the cluster assignment. Some posts are now nearer
    to another cluster. So it will update the assignments for those changed posts.
    This is done as long as the centroids move considerably. After some iterations,
    the movements will fall below a threshold and we consider clustering to be converged.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是最广泛使用的平面聚类算法。在初始化时设定所需的聚类数量 `num_clusters`，它会维持该数量的所谓聚类中心。最初，它会随机选择
    `num_clusters` 个文档，并将聚类中心设置为这些文档的特征向量。然后，它会遍历所有其他文档，将它们分配到离它们最近的聚类中心。接下来，它会将每个聚类中心移到该类别所有向量的中间位置。这会改变聚类分配。某些文档现在可能更接近另一个聚类。因此，它会更新这些文档的聚类分配。这个过程会一直进行，直到聚类中心的移动幅度小于设定的阈值，认为聚类已经收敛。
- en: 'Let''s play this through with a toy example of posts containing only two words.
    Each point in the following chart represents one document:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个简单的示例来演示，每个示例只包含两个词。下图中的每个点代表一个文档：
- en: '![K-means](img/2772OS_03_01.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![K-means](img/2772OS_03_01.jpg)'
- en: 'After running one iteration of K-means, that is, taking any two vectors as
    starting points, assigning the labels to the rest and updating the cluster centers
    to now be the center point of all points in that cluster, we get the following
    clustering:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行一次 K-means 迭代后，即选择任意两个向量作为起始点，给其他点分配标签，并更新聚类中心使其成为该类中所有点的中心点，我们得到了以下的聚类结果：
- en: '![K-means](img/2772OS_03_02.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![K-means](img/2772OS_03_02.jpg)'
- en: 'Because the cluster centers moved, we have to reassign the cluster labels and
    recalculate the cluster centers. After iteration 2, we get the following clustering:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因为聚类中心发生了移动，我们需要重新分配聚类标签，并重新计算聚类中心。在第二次迭代后，我们得到了以下的聚类结果：
- en: '![K-means](img/2772OS_03_03.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![K-means](img/2772OS_03_03.jpg)'
- en: The arrows show the movements of the cluster centers. After five iterations
    in this example, the cluster centers don't move noticeably any more (SciKit's
    tolerance threshold is 0.0001 by default).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头表示聚类中心的移动。在这个示例中，经过五次迭代后，聚类中心的移动几乎不再明显（SciKit 默认的容忍阈值是 0.0001）。
- en: After the clustering has settled, we just need to note down the cluster centers
    and their identity. Each new document that comes in, we then have to vectorize
    and compare against all cluster centers. The cluster center with the smallest
    distance to our new post vector belongs to the cluster we will assign to the new
    post.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类完成后，我们只需要记录聚类中心及其身份。每当有新的文档进入时，我们需要将其向量化并与所有聚类中心进行比较。与新文档向量距离最小的聚类中心所属的聚类即为我们为新文档分配的聚类。
- en: Getting test data to evaluate our ideas on
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取测试数据以评估我们的想法
- en: In order to test clustering, let's move away from the toy text examples and
    find a dataset that resembles the data we are expecting in the future so that
    we can test our approach. For our purpose, we need documents about technical topics
    that are already grouped together so that we can check whether our algorithm works
    as expected when we apply it later to the posts we hope to receive.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试聚类，我们不再使用简单的文本示例，而是寻找一个与我们未来预期的数据类似的数据集，以便测试我们的方法。为了这个目的，我们需要一些已经按技术主题分组的文档，这样我们就可以检查我们算法在后续应用到我们期望接收到的帖子时是否能够如预期那样工作。
- en: One standard dataset in machine learning is the `20newsgroup` dataset, which
    contains 18,826 posts from 20 different newsgroups. Among the groups' topics are
    technical ones such as `comp.sys.mac.hardware` or `sci.crypt`, as well as more
    politics- and religion-related ones such as `talk.politics.guns` or `soc.religion.christian`.
    We will restrict ourselves to the technical groups. If we assume each newsgroup
    as one cluster, we can nicely test whether our approach of finding related posts
    works.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个标准数据集是`20newsgroup`数据集，包含来自20个不同新闻组的18,826篇帖子。组内的主题包括技术类的`comp.sys.mac.hardware`或`sci.crypt`，以及与政治或宗教相关的主题，如`talk.politics.guns`或`soc.religion.christian`。我们将只关注技术类新闻组。如果我们将每个新闻组视为一个聚类，那么可以很好地测试我们寻找相关帖子的方式是否有效。
- en: The dataset can be downloaded from [http://people.csail.mit.edu/jrennie/20Newsgroups](http://people.csail.mit.edu/jrennie/20Newsgroups).
    Much more comfortable, however, is to download it from MLComp at [http://mlcomp.org/datasets/379](http://mlcomp.org/datasets/379)
    (free registration required). SciKit already contains custom loaders for that
    dataset and rewards you with very convenient data loading options.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从[http://people.csail.mit.edu/jrennie/20Newsgroups](http://people.csail.mit.edu/jrennie/20Newsgroups)下载。更方便的方式是从MLComp下载，地址是[http://mlcomp.org/datasets/379](http://mlcomp.org/datasets/379)（需要免费注册）。SciKit已经为该数据集提供了自定义加载器，并提供了非常便捷的数据加载选项。
- en: The dataset comes in the form of a ZIP file `dataset-379-20news-18828_WJQIG.zip`,
    which we have to unzip to get the directory `379`, which contains the datasets.
    We also have to notify SciKit about the path containing that data directory. It
    contains a metadata file and three directories `test`, `train`, and `raw`. The
    `test` and `train` directories split the whole dataset into 60 percent of training
    and 40 percent of testing posts. If you go this route, then you either need to
    set the environment variable `MLCOMP_DATASETS_HOME` or you specify the path directly
    with the `mlcomp_root` parameter when loading the dataset.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以ZIP文件`dataset-379-20news-18828_WJQIG.zip`的形式提供，我们需要解压这个文件，得到包含数据集的`379`目录。我们还需要告诉SciKit该数据目录所在的路径。该目录包含一个元数据文件和三个子目录`test`、`train`和`raw`。`test`和`train`目录将整个数据集分成60%的训练集和40%的测试集。如果你选择这种方式，你要么需要设置环境变量`MLCOMP_DATASETS_HOME`，要么在加载数据集时使用`mlcomp_root`参数直接指定路径。
- en: Note
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '[http://mlcomp.org](http://mlcomp.org) is a website for comparing machine learning
    programs on diverse datasets. It serves two purposes: finding the right dataset
    to tune your machine learning program, and exploring how other people use a particular
    dataset. For instance, you can see how well other people''s algorithms performed
    on particular datasets and compare against them.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://mlcomp.org](http://mlcomp.org)是一个用于比较各种数据集上的机器学习程序的网站。它有两个用途：帮助你找到合适的数据集来调整你的机器学习程序，以及探索其他人如何使用特定的数据集。例如，你可以查看其他人的算法在特定数据集上的表现，并与他们进行比较。'
- en: 'For convenience, the `sklearn.datasets` module also contains the `fetch_20newsgroups`
    function, which automatically downloads the data behind the scenes:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，`sklearn.datasets`模块还包含`fetch_20newsgroups`函数，它会自动在后台下载数据：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can choose between training and test sets:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练集和测试集之间进行选择：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For simplicity''s sake, we will restrict ourselves to only some newsgroups
    so that the overall experimentation cycle is shorter. We can achieve this with
    the `categories` parameter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化实验周期，我们将只选择一些新闻组。我们可以通过`categories`参数来实现这一点：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Clustering posts
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类帖子
- en: You would have already noticed one thing—real data is noisy. The newsgroup dataset
    is no exception. It even contains invalid characters that will result in `UnicodeDecodeError`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到一件事——真实数据是噪声重重的。新组数据集也不例外，它甚至包含无效字符，可能导致`UnicodeDecodeError`错误。
- en: 'We have to tell the vectorizer to ignore them:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要告诉向量化器忽略它们：
- en: '[PRE34]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We now have a pool of 3,529 posts and extracted for each of them a feature vector
    of 4,712 dimensions. That is what K-means takes as input. We will fix the cluster
    size to 50 for this chapter and hope you are curious enough to try out different
    values as an exercise.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个包含 3,529 篇帖子的数据池，并为每篇帖子提取了一个 4,712 维的特征向量。这就是 K-means 所需要的输入。我们将本章的聚类大小固定为
    50，希望你足够好奇，能尝试不同的值作为练习。
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'That''s it. We provided a random state just so that you can get the same results.
    In real-world applications, you will not do this. After fitting, we can get the
    clustering information out of members of `km`. For every vectorized post that
    has been fit, there is a corresponding integer label in `km.labels_`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们提供了一个随机状态，目的是让你能够得到相同的结果。在实际应用中，你不会这样做。拟合之后，我们可以从 `km` 的成员中获取聚类信息。对于每一个已经拟合的向量化帖子，在
    `km.labels_` 中都有一个对应的整数标签：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The cluster centers can be accessed via `km.cluster_centers_`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 `km.cluster_centers_` 访问聚类中心。
- en: In the next section, we will see how we can assign a cluster to a newly arriving
    post using `km.predict`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用 `km.predict` 为新到的帖子分配聚类。
- en: Solving our initial challenge
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决我们初始挑战
- en: 'We will now put everything together and demonstrate our system for the following
    new post that we assign to the `new_post` variable:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把所有内容整合在一起，并为以下我们分配给 `new_post` 变量的新帖子演示我们的系统：
- en: '*"Disk drive problems. Hi, I have a problem with my hard disk.*'
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"磁盘驱动器问题。你好，我的硬盘有问题。*'
- en: ''
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*After 1 year it is working only sporadically now.*'
  id: totrans-191
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*用了一年后，现在它只能间歇性工作。*'
- en: ''
  id: totrans-192
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*I tried to format it, but now it doesn''t boot any more.*'
  id: totrans-193
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我试图格式化它，但现在它无法再启动了。*'
- en: ''
  id: totrans-194
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Any ideas? Thanks."*'
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*有什么想法吗？谢谢。"*'
- en: 'As you learned earlier, you will first have to vectorize this post before you
    predict its label:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你之前学到的，你首先需要将这个帖子向量化，然后才能预测它的标签：
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that we have the clustering, we do not need to compare `new_post_vec` to
    all post vectors. Instead, we can focus only on the posts of the same cluster.
    Let''s fetch their indices in the original data set:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了聚类结果，我们不需要将`new_post_vec`与所有帖子向量进行比较。相反，我们可以只关注同一聚类中的帖子。让我们获取它们在原始数据集中的索引：
- en: '[PRE38]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The comparison in the bracket results in a Boolean array, and `nonzero` converts
    that array into a smaller array containing the indices of the `True` elements.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的比较结果是一个布尔数组，`nonzero` 将该数组转换为一个较小的数组，包含 `True` 元素的索引。
- en: 'Using `similar_indices`, we then simply have to build a list of posts together
    with their similarity scores:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `similar_indices`，我们只需构建一个包含帖子及其相似度得分的列表：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We found 131 posts in the cluster of our post. To give the user a quick idea
    of what kind of similar posts are available, we can now present the most similar
    post (`show_at_1`), and two less similar but still related ones – all from the
    same cluster.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在我们帖子所在的聚类中找到了 131 篇帖子。为了让用户快速了解可用的相似帖子，我们现在可以展示最相似的帖子（`show_at_1`），以及两个相对较少相似但仍然相关的帖子——它们都来自同一聚类。
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following table shows the posts together with their similarity values:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了帖子及其相似度值：
- en: '| Position | Similarity | Excerpt from post |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | 相似度 | 帖子摘录 |'
- en: '| --- | --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 1.038 | BOOT PROBLEM with IDE controllerHi,I''ve got a Multi I/O card
    (IDE controller + serial/parallel interface) and two floppy drives (5 1/4, 3 1/2)
    and a Quantum ProDrive 80AT connected to it. I was able to format the hard disk,
    but I could not boot from it. I can boot from drive A: (which disk drive does
    not matter) but if I remove the disk from drive A and press the reset switch,
    the LED of drive A: continues to glow, and the hard disk is not accessed at all.
    I guess this must be a problem of either the Multi I/o card or floppy disk drive
    settings (jumper configuration?) Does someone have any hint what could be the
    reason for it. […] |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.038 | 使用 IDE 控制器的引导问题，您好，我有一个多 I/O 卡（IDE 控制器 + 串行/并行接口）和两个软盘驱动器（5 1/4，3
    1/2）以及一个连接到它的 Quantum ProDrive 80AT。我能够格式化硬盘，但无法从中启动。我可以从 A: 驱动器启动（哪个磁盘驱动器无关紧要），但如果我从
    A: 驱动器中移除磁盘并按下重置开关，A: 驱动器的 LED 灯会继续亮着，而硬盘根本没有被访问。我猜这可能是多 I/O 卡或软盘驱动器设置（跳线配置？）的问题。有谁知道可能是什么原因吗？[…]
    |'
- en: '| 2 | 1.150 | Booting from B driveI have a 5 1/4" drive as drive A. How can
    I make the system boot from my 3 1/2" B drive? (Optimally, the computer would
    be able to boot: from either A or B, checking them in order for a bootable disk.
    But: if I have to switch cables around and simply switch the drives so that: it
    can''t boot 5 1/4" disks, that''s OK. Also, boot_b won''t do the trick for me.
    […][…] |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.150 | 从B驱动器启动我有一个5 1/4"的驱动器作为A驱动器。我如何让系统从我的3 1/2" B驱动器启动？（理想情况下，计算机应该能够从A或B驱动器启动，按照顺序检查它们是否有可启动的磁盘。但是：如果我必须交换电缆并简单地交换驱动器，以便它无法启动5
    1/4"磁盘，那也可以。另外，boot_b也不能帮我实现这个目的。[……][……] |'
- en: '| 3 | 1.280 | IBM PS/1 vs TEAC FDHello, I already tried our national news group
    without success. I tried to replace a friend s original IBM floppy disk in his
    PS/1-PC with a normal TEAC drive. I already identified the power supply on pins
    3 (5V) and 6 (12V), shorted pin 6 (5.25"/3.5" switch) and inserted pullup resistors
    (2K2) on pins 8, 26, 28, 30, and 34\. The computer doesn''t complain about a missing
    FD, but the FD s light stays on all the time. The drive spins up o.k. when I insert
    a disk, but I can''t access it. The TEAC works fine in a normal PC. Are there
    any points I missed? […][…] |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1.280 | IBM PS/1与TEAC FD大家好，我已经尝试过我们的国家新闻组但没有成功。我试图用普通的TEAC驱动器替换我朋友在PS/1-PC中使用的IBM软盘驱动器。我已经确定了电源供应在3号引脚（5V）和6号引脚（12V）的位置，将6号引脚（5.25"/3.5"切换）短接，并在8、26、28、30和34号引脚上插入了上拉电阻（2K2）。计算机没有抱怨缺少软盘，但软盘的指示灯一直亮着。当我插入磁盘时，驱动器正常启动，但我无法访问它。TEAC在普通PC中工作正常。我是否漏掉了什么？[……][……]
    |'
- en: It is interesting how the posts reflect the similarity measurement score. The
    first post contains all the salient words from our new post. The second also revolves
    around booting problems, but is about floppy disks and not hard disks. Finally,
    the third is neither about hard disks, nor about booting problems. Still, of all
    the posts, we would say that they belong to the same domain as the new post.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，帖子如何反映相似度测量分数。第一篇帖子包含了我们新帖子的所有突出词汇。第二篇帖子也围绕启动问题展开，但涉及的是软盘而不是硬盘。最后，第三篇既不是关于硬盘的，也不是关于启动问题的。不过，在所有帖子中，我们会说它们属于与新帖子相同的领域。
- en: Another look at noise
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 再看噪声
- en: 'We should not expect a perfect clustering in the sense that posts from the
    same newsgroup (for example, `comp.graphics`) are also clustered together. An
    example will give us a quick impression of the noise that we have to expect. For
    the sake of simplicity, we will focus on one of the shorter posts:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应期待完美的聚类，也就是说，来自同一新闻组（例如，`comp.graphics`）的帖子也会被聚在一起。一个例子能让我们快速了解我们可能会遇到的噪声。为了简化起见，我们将关注其中一个较短的帖子：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'For this post, there is no real indication that it belongs to `comp.graphics`
    considering only the wording that is left after the preprocessing step:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 就这个帖子而言，考虑到预处理步骤后剩下的文字，根本没有明显的迹象表明它属于`comp.graphics`：
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This is only after tokenization, lowercasing, and stop word removal. If we
    also subtract those words that will be later filtered out via `min_df` and `max_df`,
    which will be done later in `fit_transform`, it gets even worse:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是在分词、转换为小写和去除停用词之后。如果我们还减去那些将通过`min_df`和`max_df`在稍后的`fit_transform`中过滤掉的词汇，情况就会变得更糟：
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Even more, most of the words occur frequently in other posts as well, as we
    can check with the IDF scores. Remember that the higher TF-IDF, the more discriminative
    a term is for a given post. As IDF is a multiplicative factor here, a low value
    of it signals that it is not of great value in general.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，大多数词汇在其他帖子中也频繁出现，我们可以通过IDF得分来检查这一点。记住，TF-IDF值越高，术语对于给定帖子的区分度就越高。由于IDF在这里是一个乘法因子，它的低值意味着它在一般情况下并不具有很大的价值。
- en: '[PRE44]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: So, the terms with the highest discriminative power, `birmingham` and `kingdom`,
    clearly are not that computer graphics related, the same is the case with the
    terms with lower IDF scores. Understandably, posts from different newsgroups will
    be clustered together.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有最高区分度的术语`birmingham`和`kingdom`显然与计算机图形学并不相关，IDF得分较低的术语也是如此。可以理解，不同新闻组的帖子将被聚类在一起。
- en: For our goal, however, this is no big deal, as we are only interested in cutting
    down the number of posts that we have to compare a new post to. After all, the
    particular newsgroup from where our training data came from is of no special interest.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于我们的目标来说，这并不是什么大问题，因为我们只对减少我们必须与之比较的新帖子的数量感兴趣。毕竟，我们训练数据来源的特定新闻组并不特别重要。
- en: Tweaking the parameters
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整参数
- en: So what about all the other parameters? Can we tweak them to get better results?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 那么其他的参数呢？我们能调整它们以获得更好的结果吗？
- en: Sure. We can, of course, tweak the number of clusters, or play with the vectorizer's
    `max_features` parameter (you should try that!). Also, we can play with different
    cluster center initializations. Then there are more exciting alternatives to K-means
    itself. There are, for example, clustering approaches that let you even use different
    similarity measurements, such as Cosine similarity, Pearson, or Jaccard. An exciting
    field for you to play.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当然。我们当然可以调整聚类的数量，或者调试向量化器的`max_features`参数（你应该试试这个！）。另外，我们还可以尝试不同的聚类中心初始化方式。除此之外，还有比K-means更令人兴奋的替代方法。例如，有些聚类方法甚至允许你使用不同的相似度度量，比如余弦相似度、皮尔逊相关系数或杰卡德相似系数。这是一个值得你探索的有趣领域。
- en: But before you go there, you will have to define what you actually mean by "better".
    SciKit has a complete package dedicated only to this definition. The package is
    called `sklearn.metrics` and also contains a full range of different metrics to
    measure clustering quality. Maybe that should be the first place to go now. Right
    into the sources of the metrics package.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 但在你深入之前，你需要定义一下“更好”到底意味着什么。SciKit为这个定义提供了一个完整的包。这个包叫做`sklearn.metrics`，它也包含了各种用于衡量聚类质量的度量指标，也许这是你现在应该去的第一个地方。直接去查看这些度量包的源代码吧。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: That was a tough ride from pre-processing over clustering to a solution that
    can convert noisy text into a meaningful concise vector representation, which
    we can cluster. If we look at the efforts we had to do to finally being able to
    cluster, it was more than half of the overall task. But on the way, we learned
    quite a bit on text processing and how simple counting can get you very far in
    the noisy real-world data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这一路从预处理到聚类，再到能够将嘈杂的文本转换为有意义且简洁的向量表示以便进行聚类，确实很不容易。如果我们看看为了最终能够聚类所做的努力，这几乎占据了整个任务的一半。但在这个过程中，我们学到了很多关于文本处理的知识，以及如何通过简单的计数在嘈杂的现实世界数据中取得很大进展。
- en: The ride has been made much smoother, though, because of SciKit and its powerful
    packages. And there is more to explore. In this chapter, we were scratching the
    surface of its capabilities. In the next chapters, we will see more of its power.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，由于SciKit及其强大的包，这个过程变得更加顺利了。并且，还有更多值得探索的内容。在本章中，我们只是粗略地触及了它的能力。接下来的章节中，我们将进一步了解它的强大功能。
