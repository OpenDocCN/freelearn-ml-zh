- en: 6\. Ensemble Modeling
  id: totrans-0
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6\. 集成建模
- en: Overview
  id: totrans-1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter examines different ways of performing ensemble modeling, along
    with its benefits and limitations. By the end of the chapter, you will be able
    to recognize the underfitting and overfitting of data on machine learning models.
    You will also be able to devise a bagging classifier using decision trees and
    implement adaptive boosting and gradient boosting models. Finally, you will be
    able to build a stacked ensemble using a number of classifiers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了执行集成建模的不同方法及其优缺点。通过本章内容的学习，你将能够识别机器学习模型中的欠拟合和过拟合问题。你还将能够使用决策树设计一个袋装分类器，并实现自适应提升和梯度提升模型。最后，你将能够利用多个分类器构建一个堆叠集成模型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapters, we discussed the two types of supervised learning
    problems: regression and classification. We looked at a number of algorithms for
    each type and delved into how those algorithms worked.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了两种类型的监督学习问题：回归和分类。我们研究了每种类型的若干算法，并深入了解了这些算法是如何工作的。
- en: But there are times when these algorithms, no matter how complex they are, just
    don't seem to perform well on the data that we have. There could be a variety
    of causes and reasons for this – perhaps the data is not good enough, perhaps
    there really is no trend where we are trying to find one, or perhaps the model
    itself is too complex.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时这些算法，无论多么复杂，似乎都无法在我们拥有的数据上表现良好。这可能有多种原因——也许数据不够好，也许我们试图找到的趋势根本不存在，或者模型本身过于复杂。
- en: Wait. What?! How can a model being too complex be a problem? If a model is too
    complex and there isn't enough data, the model could fit so well to the data that
    it learns even the noise and outliers, which is not what we want.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么？！模型过于复杂怎么会是问题？如果一个模型过于复杂，并且数据不足，那么模型可能会非常拟合数据，甚至学习到噪声和异常值，这正是我们不希望发生的。
- en: Often, where a single complex algorithm can give us a result that is way off
    from actual results, aggregating the results from a group of models can give us
    a result that's closer to the actual truth. This is because there is a high likelihood
    that the errors from all the individual models would cancel out when we take them
    all into account when making a prediction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当一个复杂的单一算法给出的结果与实际结果相差甚远时，将多个模型的结果进行聚合可以得到一个更接近实际情况的结果。这是因为考虑所有模型的结果时，单个模型的误差很可能会相互抵消。
- en: This approach of grouping multiple algorithms to give an aggregated prediction
    is what ensemble modeling is based on. The ultimate goal of an ensemble method
    is to combine several underperforming base estimators (that is, individual algorithms)
    in such a way that the overall performance of the system improves and the ensemble
    of algorithms results in a model that is more robust and can generalize well compared
    to an individual algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个算法组合在一起以给出综合预测的这种方法是集成建模的基础。集成方法的最终目标是以某种方式将多个表现不佳的基础估计器（即单个算法）组合起来，从而提高系统的整体性能，并使得算法的集成模型比单个算法更为健壮，能够更好地进行泛化。
- en: In the first half of this chapter, we will discuss how building an ensemble
    model can help us build a robust system that makes accurate predictions without
    increasing variance. We will start by talking about some reasons as to why a model
    may not perform well, and then move on to discussing the concepts of bias and
    variance, as well as overfitting and underfitting. We will introduce ensemble
    modeling as a solution for these performance issues and discuss different ensemble
    methods that could be used to overcome different types of problems when it comes
    to underperforming models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前半部分，我们将讨论如何通过构建集成模型来帮助我们建立一个健壮的系统，该系统能够做出准确的预测，而不会增加方差。我们将首先讨论模型表现不佳的一些原因，然后介绍偏差和方差的概念，以及过拟合和欠拟合问题。我们将把集成建模作为解决这些性能问题的一种方案，并讨论可用于解决不同类型问题的集成方法，尤其是针对表现不佳的模型。
- en: We will discuss three types of ensemble methods; namely, bagging, boosting,
    and stacking. Each of these will be discussed right from the basic theory to discussions
    on which use cases each type deals with well and which use cases each type might
    not be a good fit for. We will also go through a number of exercises to implement
    the models using the scikit-learn library in Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论三种集成方法；即：bagging、boosting 和 stacking。每种方法将从基本理论讲起，并讨论每种方法适合解决的用例，以及哪些用例可能不适合该方法。我们还将通过一些练习，使用
    Python 中的 scikit-learn 库来实现这些模型。
- en: Before diving deep into the topics, we shall first get familiar with a dataset
    that we will be using to demonstrate and understand the different concepts that
    are to be covered in this chapter. The next exercise enables us to do that. Before
    we delve into the exercise, it is necessary to become familiar with the concept
    of one-hot encoding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨这些主题之前，我们首先要熟悉一个数据集，使用这个数据集来演示和理解本章将要涉及的不同概念。接下来的练习将帮助我们做到这一点。在进入练习之前，有必要先了解一下独热编码的概念。
- en: One-Hot Encoding
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 独热编码
- en: So, what is one-hot encoding? Well, in machine learning, we sometimes have categorical
    input features such as name, gender, and color. Such features contain label values
    rather than numeric values, such as John and Tom for name, male and female for
    gender, and red, blue, and green for color. Here, blue is one such label for the
    categorical feature – color. All machine learning models can work with numeric
    data, but many machine learning models cannot work with categorical data because
    of the way their underlying algorithms are designed. For example, decision trees
    can work with categorical data, but logistic regression cannot.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是独热编码呢？在机器学习中，我们有时会遇到分类输入特征，比如姓名、性别和颜色。这些特征包含的是标签值，而非数值，例如姓名中的 John 和 Tom，性别中的
    male 和 female，颜色中的 red、blue 和 green。在这里，blue 就是分类特征——颜色的一个标签。所有机器学习模型都可以处理数值数据，但许多机器学习模型无法处理分类数据，因为它们的基础算法设计方式不支持这种数据。例如，决策树可以处理分类数据，但逻辑回归不能。
- en: 'In order to still make use of categorical features with models such as logistic
    regression, we transform such features into a usable numeric format. Figure 6.1
    shows an example of what this transformation looks like:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在使用像逻辑回归这样的模型时仍能利用分类特征，我们将这些特征转化为可用的数值格式。图 6.1 显示了这种转换的样子：
- en: '![Figure 6.1: One-hot encoding'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1：独热编码'
- en: '](img/image-A78FXQ2Y.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-A78FXQ2Y.jpg)'
- en: 'Figure 6.1: One-hot encoding'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：独热编码
- en: 'Figure 6.2 shows how one-hot encoding changes the dataset, once applied:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 显示了应用独热编码后数据集的变化：
- en: '![Figure 6.2: One-hot encoding applied'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2：应用独热编码'
- en: '](img/image-46R63UX3.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-46R63UX3.jpg)'
- en: 'Figure 6.2: One-hot encoding applied'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：应用独热编码
- en: Basically, in this example, there are 3 categories of colors – red, blue, and
    green, and therefore 3 binary variables are needed – color_red, color_blue, and
    color_green. A 1 value is used to represent the binary variable for the color
    and 0 values for the other colors. These binary variables – color_red, color_blue,
    and color_green – are also known as dummy variables. Armed with this information,
    we can proceed to our exercise.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，在这个例子中，有三种颜色类别——红色、蓝色和绿色，因此需要三个二元变量——color_red、color_blue 和 color_green。使用
    1 来表示该颜色的二元变量，其他颜色使用 0 表示。这些二元变量——color_red、color_blue 和 color_green——也被称为虚拟变量。有了这些信息，我们可以继续进行我们的练习。
- en: 'Exercise 6.01: Importing Modules and Preparing the Dataset'
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 6.01：导入模块并准备数据集
- en: 'In this exercise, we will import all the modules we will need for this chapter
    and get our dataset in shape for the exercises to come:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将导入本章所需的所有模块，并将数据集整理好，以便进行接下来的练习：
- en: 'Import all the modules required to manipulate the data and evaluate the model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所有需要的模块来操作数据和评估模型：
- en: import pandas as pd
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import numpy as np
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: import numpy as np
- en: '%matplotlib inline'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '%matplotlib inline'
- en: import matplotlib.pyplot as plt
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: from sklearn.model_selection import train_test_split
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.model_selection import train_test_split
- en: from sklearn.metrics import accuracy_score
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.metrics import accuracy_score
- en: from sklearn.model_selection import KFold
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.model_selection import KFold
- en: The dataset that we will use in this exercise is the Titanic dataset, which
    was introduced in the previous chapters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本次练习中使用的数据集是 Titanic 数据集，它在前几章中已经介绍过。
- en: 'Read the dataset and print the first five rows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据集并打印前五行：
- en: data = pd.read_csv('titanic.csv')
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: data = pd.read_csv('titanic.csv')
- en: data.head()
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: data.head()
- en: Note
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The code snippet presented above assumes that the dataset is stored in the
    same folder as that of the Jupyter Notebook for this exercise. However, if this
    dataset is saved in the Datasets folder, you then need to use the following code:
    data = pd.read_csv(''../Datasets/titanic. csv'')'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码假设数据集存储在与本次练习的 Jupyter Notebook 相同的文件夹中。如果数据集保存在 Datasets 文件夹中，则需要使用以下代码：`data
    = pd.read_csv('../Datasets/titanic.csv')`
- en: 'The output is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 6.3: The first five rows'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.3: 前五行'
- en: '](img/image-YPAWIKGL.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-YPAWIKGL.jpg)'
- en: 'Figure 6.3: The first five rows'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.3: 前五行'
- en: In order to make the dataset ready for use, we will add a preprocess function,
    which will preprocess the dataset to get it into a format that is ingestible by
    the scikit-learn library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据集准备好使用，我们将添加一个 `preprocess` 函数，该函数将预处理数据集，并将其转换为 scikit-learn 库可以接受的格式。
- en: First, we create a fix_age function to preprocess the age column and get an
    integer value. If the age is null, the function returns a value of -1 to differentiate
    it from the available values, otherwise it returns the value. We then apply this
    function to the age column.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个 `fix_age` 函数来预处理年龄列并获取整数值。如果年龄为 null，该函数返回 -1，以区分其他有效值，否则返回实际值。然后，我们将此函数应用于年龄列。
- en: 'Then, we convert the Gender column into a binary variable with 1 for female
    and 0 for male values, and subsequently create dummy binary columns for the Embarked
    column using pandas'' get_dummies function. Following this, we combine the DataFrame
    containing the dummy columns with the remaining numerical columns to create the
    final DataFrame, which is returned by the function:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将性别（Gender）列转换为二元变量，女性为 1，男性为 0，然后使用 pandas 的 `get_dummies` 函数为登船港口（Embarked）列创建虚拟二元列。之后，我们将包含虚拟列的
    DataFrame 与其余的数值列结合，生成最终的 DataFrame，该函数将返回该 DataFrame：
- en: 'def preprocess(data):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'def preprocess(data):'
- en: 'def fix_age(age):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 'def fix_age(age):'
- en: 'if np.isnan(age):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'if np.isnan(age):'
- en: return -1
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: return -1
- en: 'else:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'else:'
- en: return age
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: return age
- en: data.loc[:, 'Age'] = data.Age.apply(fix_age)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: data.loc[:, 'Age'] = data.Age.apply(fix_age)
- en: 'data.loc[:, ''Gender''] = data.Gender.apply(lambda s: \'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'data.loc[:, ''Gender''] = data.Gender.apply(lambda s: \'
- en: int(s == 'female'))
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: int(s == 'female'))
- en: embarked = pd.get_dummies(data.Embarked, \
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: embarked = pd.get_dummies(data.Embarked, \
- en: prefix='Emb')[['Emb_C',\
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: prefix='Emb')[['Emb_C',\
- en: '''Emb_Q'',''Emb_S'']]'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '''Emb_Q'',''Emb_S'']]'
- en: cols = ['Pclass','Gender','Age','SibSp','Parch','Fare']
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: cols = ['Pclass','Gender','Age','SibSp','Parch','Fare']
- en: return pd.concat([data[cols], embarked], axis=1).values
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: return pd.concat([data[cols], embarked], axis=1).values
- en: Split the dataset into training and validation sets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集拆分为训练集和验证集。
- en: We split the dataset into two parts – one on which we will train the models
    during the exercises (train), and another on which we will make predictions to
    evaluate the performance of each of those models (val). We will use the function
    we wrote in the previous step to preprocess the training and validation datasets
    separately.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集拆分为两部分——一个用于在练习过程中训练模型（train），另一个用于进行预测并评估每个模型的性能（val）。我们将使用之前编写的函数分别预处理训练集和验证集数据。
- en: 'Here, the Survived binary variable is the target variable that determines whether
    or not the individual in each row survived the sinking of the Titanic, so we create
    y_train and y_val as the dependent variable columns from both the splits:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Survived` 二元变量是目标变量，用于确定每行数据中的个体是否在泰坦尼克号沉船中幸存。因此，我们将创建 `y_train` 和 `y_val`
    作为从两个数据子集拆分出的因变量列：
- en: train, val = train_test_split(data, test_size=0.2, random_state=11)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: train, val = train_test_split(data, test_size=0.2, random_state=11)
- en: x_train = preprocess(train)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: x_train = preprocess(train)
- en: y_train = train['Survived'].values
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: y_train = train['Survived'].values
- en: x_val = preprocess(val)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: x_val = preprocess(val)
- en: y_val = val['Survived'].values
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: y_val = val['Survived'].values
- en: print(x_train.shape)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: print(x_train.shape)
- en: print(y_train.shape)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: print(y_train.shape)
- en: print(x_val.shape)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: print(x_val.shape)
- en: print(y_val.shape)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: print(y_val.shape)
- en: 'You should get the following output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到如下输出：
- en: (712, 9)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (712, 9)
- en: (712,)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (712,)
- en: (179, 9)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (179, 9)
- en: (179,)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (179,)
- en: As we can see, the dataset is now split into 2 subsets, with the training set
    having 712 data points and the validation set having 179 data points.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据集现在已分为两部分，训练集包含 712 个数据点，验证集包含 179 个数据点。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2Nm6KHM.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参阅 [https://packt.live/2Nm6KHM](https://packt.live/2Nm6KHM)。
- en: You can also run this example online at https://packt.live/2YWh9zg. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，访问 [https://packt.live/2YWh9zg](https://packt.live/2YWh9zg)。您必须执行整个
    Notebook 才能获得预期结果。
- en: In this exercise, we began by loading the data and importing the necessary Python
    modules. We then preprocessed different columns of our dataset to make it usable
    for training machine learning models. Finally, we split the dataset into two subsets.
    And now, before doing anything further with the dataset, we will try to understand
    two important concepts of machine learning – overfitting and underfitting.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们首先加载了数据并导入了必要的 Python 模块。然后，我们对数据集的不同列进行了预处理，使其可以用于训练机器学习模型。最后，我们将数据集分为两个子集。现在，在对数据集进行进一步操作之前，我们将尝试理解机器学习中的两个重要概念——过拟合和欠拟合。
- en: Overfitting and Underfitting
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过拟合与欠拟合
- en: Let's say we fit a supervised learning algorithm to our data and subsequently
    use the model to perform a prediction on a hold-out validation set. The performance
    of this model will be considered to be good based on how well it generalizes,
    that is, how well it makes predictions for data points in an independent validation
    dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将一个有监督学习算法拟合到我们的数据上，然后使用该模型对一个独立的验证集进行预测。根据模型的泛化能力——即它对独立验证集中的数据点进行预测的能力——来评估其性能。
- en: Sometimes, we find that the model is not able to make accurate predictions and
    gives poor performance on the validation data. This poor performance can be the
    result of a model that is too simple to model the data appropriately, or a model
    that is too complex to generalize to the validation dataset. In the former case,
    the model has a high bias and results in underfitting, while, in the latter case,
    the model has a high variance and results in overfitting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们会发现模型无法做出准确的预测，并且在验证数据上表现较差。这种差劲的表现可能是因为模型过于简单，无法适当地建模数据，或者模型过于复杂，无法对验证数据集进行泛化。在前一种情况下，模型具有高偏差，导致欠拟合；在后一种情况下，模型具有高方差，导致过拟合。
- en: Bias
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差
- en: The bias in the prediction of a machine learning model represents the difference
    between the predicted target value and the true target value of a data point.
    A model is said to have a high bias if the average predicted values are far off
    from the true values and is conversely said to have a low bias if the average
    predicted values are close to the true values.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型预测中的偏差表示预测的目标值与数据点的真实目标值之间的差异。如果平均预测值与真实值相差较远，则模型被认为具有高偏差；反之，如果平均预测值接近真实值，则模型被认为具有低偏差。
- en: A high bias indicates that the model cannot capture the complexity in the data
    and is unable to identify the relevant relationships between the inputs and outputs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 高偏差表示模型无法捕捉数据中的复杂性，无法识别输入和输出之间的相关关系。
- en: Variance
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 方差
- en: The variance in prediction of a machine learning model represents how scattered
    the predicted values are compared to the true values. A model is said to have
    high variance if the predictions are scattered and unstable and is conversely
    said to have low variance if the predictions are consistent and not very scattered.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型预测中的方差表示预测值与真实值之间的分散程度。如果预测值分散且不稳定，则模型被认为具有高方差；反之，如果预测值一致且不太分散，则模型被认为具有低方差。
- en: 'A high variance indicates the model''s inability to generalize and make accurate
    predictions on data points previously unseen by the model. As you can see in the
    following figure, the center of these circles represents the true target value
    of the data points. And the dots represent the predicted target value of the data
    points:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 高方差表明模型无法泛化，并且在模型之前未见过的数据点上无法做出准确的预测。如图所示，这些圆的中心表示数据点的真实目标值，圆点表示数据点的预测目标值：
- en: '![Figure 6.4: Visual representation of data points having high and low bias
    and variance'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4：具有高偏差和低偏差的数据显示点的视觉表示](img/image-9XB2XBIW.jpg)'
- en: '](img/image-9XB2XBIW.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-9XB2XBIW.jpg)'
- en: 'Figure 6.4: Visual representation of data points having high and low bias and
    variance'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：具有高偏差和低偏差的数据显示点的视觉表示
- en: Underfitting
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 欠拟合
- en: 'Let''s say that we fit a simple model on the training dataset, one with low
    model complexity, such as a simple linear model. We have fit a function that''s
    able to represent the relationship between the X (input data) and Y (target output)
    data points in the training data to some extent, but we see that the training
    error is still high:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在训练数据集上拟合一个简单的模型，比如一个简单的线性模型。我们拟合了一个能够在一定程度上表示训练数据中 X（输入数据）和 Y（目标输出）数据点之间关系的函数，但我们发现训练误差依然很高：
- en: '![Figure 6.5: Underfitting versus an ideal fit in regression'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：回归中的欠拟合与理想拟合'
- en: '](img/image-MSSATPXC.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-MSSATPXC.jpg)'
- en: 'Figure 6.5: Underfitting versus an ideal fit in regression'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：回归中的欠拟合与理想拟合
- en: For example, look at the two regression plots shown in Figure 6.5\. While the
    first plot shows a model that fits a straight line to the data, the second plot
    shows a model that attempts to fit a relatively more complex polynomial to the
    data, one that seems to represent the mapping between X and Y quite well.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看图 6.5 中显示的两张回归图。第一张图展示了一个将直线拟合到数据的模型，而第二张图展示了一个尝试将相对复杂的多项式拟合到数据的模型，它似乎很好地表示了
    X 和 Y 之间的映射关系。
- en: If we look closely at the first model (on the left in the figure), the straight
    line is usually far away from the individual data points, as opposed to the second
    model where the data points are quite close to the curve. According to the definition
    of bias that we made in the previous section, we can say that the first model
    has a high bias. And, if we refer to the definition of the variance of a model,
    the first model is quite consistent in its predictions in that it predicts a fixed
    straight line-based output for a given input. Hence, the first model has a low
    variance and we can say that the first model demonstrates underfitting, since
    it shows the characteristics of a high bias and low variance; that is, while it
    is unable to capture the complexity in the mapping between the inputs and outputs,
    it is consistent in its predictions. This model will have a high prediction error
    on both the training data and validation data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察第一个模型（图中左侧），直线通常远离个别数据点，而第二个模型中的数据点则紧密贴合曲线。根据我们在上一节中定义的偏差，我们可以说第一个模型具有较高的偏差。而如果参考模型的方差定义，第一个模型的预测相对一致，因为它对给定输入总是预测一个固定的直线输出。因此，第一个模型具有较低的方差，我们可以说它展示了欠拟合的特征，因为它具有高偏差和低方差的特征；也就是说，尽管它无法捕捉到输入与输出之间的复杂映射，但它在预测上保持一致。该模型在训练数据和验证数据上的预测误差较大。
- en: Overfitting
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'Let''s say that we trained a highly complex model that is able to make predictions
    on the training dataset almost perfectly. We have managed to fit a function to
    represent the relationship between the X and Y data points in the training data
    such that the predicted error on the training data is extremely low:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们训练了一个非常复杂的模型，它几乎能够完美地预测训练数据集上的数据。我们已经成功地拟合了一个函数来表示训练数据中 X 和 Y 数据点之间的关系，使得训练数据上的预测误差极低：
- en: '![Figure 6.6: An ideal fit versus overfitting in regression'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.6：回归中的理想拟合与过拟合'
- en: '](img/image-VBA7P45K.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-VBA7P45K.jpg)'
- en: 'Figure 6.6: An ideal fit versus overfitting in regression'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：回归中的理想拟合与过拟合
- en: Looking at the two plots in Figure 6.6, we can see that the second plot shows
    a model that attempts to fit a highly complex function to the data points, compared
    to the plot on the left, which represents the ideal fit for the given data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看图 6.6 中的两张图，我们可以看到第二张图展示了一个试图将一个高度复杂的函数拟合到数据点上的模型，而左侧的图表示了给定数据的理想拟合。
- en: 'It is evident that, when we try to use the second-plot model to predict the
    Y values for X data points that did not appear in the training set, we will see
    that the predictions are way off from the corresponding true values. This is a
    case of overfitting, the phenomenon where the model fits the data too well so
    that it is unable to generalize to new data points, since the model learns even
    the random noise and outliers in the training data. This model shows the characteristics
    of high variance and low bias: while the average predicted values would be close
    to the true values, they would be quite scattered compared to the true values.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，当我们尝试使用第二张图中的模型预测在训练集未出现的 X 数据点对应的 Y 值时，我们会发现预测值与真实值相差甚远。这就是过拟合的表现，模型对数据的拟合过于完美，以至于无法对新数据点进行泛化，因为模型学到了训练数据中的随机噪声和离群点。该模型表现出高方差和低偏差的特征：尽管平均预测值与真实值接近，但与真实值相比，它们的分散性较大。
- en: 'Another way in which overfitting can happen is when the number of data points
    is less than or equal to the degree of the polynomial that we are trying to fit
    to the model. We should, therefore, avoid a model where:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合可能发生的另一种情况是当数据点的数量小于或等于我们尝试拟合到模型的多项式的阶数时。因此，我们应该避免使用以下类型的模型：
- en: degree of polynomial > number of data points
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式的阶数 > 数据点的数量
- en: With an extremely small dataset, trying to fit even a simple model can therefore
    also lead to overfitting.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集极其小的情况下，甚至尝试拟合一个简单的模型也可能导致过拟合。
- en: Overcoming the Problem of Underfitting and Overfitting
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 克服欠拟合与过拟合的问题
- en: From the previous sections, we can see that, as we move from an overly simplistic
    to an overly complex model, we go from having an underfitting model with a high
    bias and low variance to an overfitting model with a low bias and high variance.
    The purpose of a supervised machine learning algorithm is to achieve a low bias
    with low variance and arrive at a place between underfitting and overfitting.
    This will also help the algorithm generalize well from the training data to validation
    data points, resulting in good prediction performance on data the model has never
    seen.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的章节中我们可以看到，当我们从过于简单的模型走向过于复杂的模型时，我们会从一个具有高偏差、低方差的欠拟合模型过渡到一个具有低偏差、高方差的过拟合模型。监督学习算法的目标是实现低偏差和低方差，达到欠拟合与过拟合之间的平衡。这也有助于算法从训练数据到验证数据的良好泛化，从而在模型从未见过的数据上表现出色。
- en: The best way to improve performance when the model underfits the data is to
    increase the model complexity so as to identify the relevant relationships in
    the data. This can be done by adding new features, or by creating an ensemble
    of high-bias models. However, in this case, adding more data to train on would
    not help, as the constraining factor is model complexity and more data will not
    help to reduce the model's bias.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型欠拟合数据时，提高模型复杂度是改善性能的最佳方式，从而识别数据中的相关关系。这可以通过添加新特征或创建高偏差模型的集成来实现。然而，在这种情况下，增加更多的训练数据并不会有帮助，因为限制因素是模型复杂度，更多的数据并不能减少模型的偏差。
- en: 'Overfitting is, however, more difficult to tackle. Here are some common techniques
    used to overcome the problem posed by overfitting:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，过拟合问题更难解决。以下是一些常用的技术，用于克服过拟合带来的问题：
- en: 'Getting more data: A highly complex model can easily overfit to a small dataset,
    but will not be able to as easily on a larger dataset.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 获取更多数据：一个高度复杂的模型很容易在小数据集上发生过拟合，但在更大的数据集上则不容易发生。
- en: 'Dimensionality reduction: Reducing the number of features can help make the
    model less complex.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 降维：减少特征数量有助于使模型更简单。
- en: 'Regularization: A new term is added to the cost function to adjust the coefficients
    (especially the high-degree coefficients in linear regression) toward a low value.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化：在成本函数中添加一个新的项，以调整系数（特别是在线性回归中，高次项系数）向较低的值逼近。
- en: 'Ensemble modeling: Aggregating the predictions of several overfitting models
    can effectively eliminate high variance in prediction and perform better than
    individual models that overfit to the training data.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 集成建模：聚合多个过拟合模型的预测可以有效消除预测中的高方差，表现得比单个过拟合训练数据的模型更好。
- en: 'We will talk in more detail about ensemble modeling techniques in the following
    sections of this chapter. Some of the common types of ensembles are:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的后续部分详细讨论集成建模技术。常见的集成类型包括：
- en: 'Bagging: A shorter term for Bootstrap Aggregation, this technique is also used
    to decrease the model''s variance and avoid overfitting. It involves taking a
    subset of features and data points at a time, training a model on each subset,
    and subsequently aggregating the results from all the models into a final prediction.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法：Bootstrap聚合法的简称，该技术也用于减少模型的方差并避免过拟合。它涉及一次取出特征和数据点的一个子集，在每个子集上训练一个模型，并随后将所有模型的结果聚合成最终的预测。
- en: 'Boosting: This technique is used to reduce bias rather than to reduce variance
    and involves incrementally training new models that focus on the misclassified
    data points in the previous model.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法：该技术用于减少偏差，而不是减少方差，涉及逐步训练新的模型，重点关注前一个模型中分类错误的数据点。
- en: 'Stacking: The aim of this technique is to increase the predictive power of
    the classifier, as it involves training multiple models and then using a combiner
    algorithm to make the final prediction by using the predictions from all these
    models'' additional inputs.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠法：该技术的目标是提高分类器的预测能力，因为它涉及训练多个模型，然后使用组合算法通过利用所有这些模型的预测输入来进行最终预测。
- en: Let's start with bagging, and then move on to boosting and stacking.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从袋装法开始，然后再介绍提升法和堆叠法。
- en: Bagging
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 袋装法
- en: The term bagging is derived from a technique called bootstrap aggregation. In
    order to implement a successful predictive model, it's important to know in what
    situation we could benefit from using bootstrapping methods to build ensemble
    models. Such models are used extensively both in industry as well as academia.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 术语“bagging”来源于一种叫做自助法聚合（bootstrap aggregation）的方法。为了实现一个成功的预测模型，了解在什么情况下我们可以通过使用自助法方法来构建集成模型是很重要的。这类模型在工业界和学术界都得到了广泛应用。
- en: One such application would be that these models can be used for the quality
    assessment of Wikipedia articles. Features such as article_length, number_of_references,
    number_of_headings, and number_of_images are used to build a classifier that classifies
    Wikipedia articles into low- or high-quality articles. Out of the several models
    that were tried for this task, the random forest model – a well-known bagging-based
    ensemble classifier that we will discuss in our next section – outperforms all
    other models such as SVM, logistic regression, and even neural networks, with
    the best precision and recall scores of 87.3% and 87.2%, respectively. This demonstrates
    the power of such models as well as their potential to be used in real-life applications.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个应用是，这些模型可以用于维基百科文章的质量评估。诸如文章长度、参考文献数量、标题数量和图片数量等特征被用来构建分类器，将维基百科文章分为低质量或高质量文章。在为此任务尝试的多个模型中，随机森林模型——一种我们将在下一节讨论的基于bagging的集成分类器——优于其他所有模型，如SVM、逻辑回归甚至神经网络，其最佳精度和召回率分别为87.3%和87.2%。这展示了这类模型的强大能力以及它们在实际应用中的潜力。
- en: In this section, we'll talk about a way to use bootstrap methods to create an
    ensemble model that minimizes variance and look at how we can build an ensemble
    of decision trees, that is, the random forest algorithm. But what is bootstrapping
    and how does it help us build robust ensemble models?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将讨论如何使用自助法方法创建一个最小化方差的集成模型，并看看我们如何构建一个决策树集成模型，也就是随机森林算法。但什么是自助法，它如何帮助我们构建稳健的集成模型呢？
- en: Bootstrapping
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自助法
- en: 'The bootstrap method essentially refers to drawing multiple samples (each known
    as a resample) from the dataset consisting of randomly chosen data points, where
    there can be an overlap in the data points contained in each resample and each
    data point has an equal probability of being selected from the overall dataset:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法方法本质上是指从包含随机选择数据点的数据集中绘取多个样本（每个样本称为重采样），其中每个重采样中的数据点可能会有重叠，并且每个数据点被从整个数据集中选择的概率是相等的：
- en: '![Figure 6.7: Randomly choosing data points'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7：随机选择数据点'
- en: '](img/image-HQEFDI7L.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-HQEFDI7L.jpg)'
- en: 'Figure 6.7: Randomly choosing data points'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：随机选择数据点
- en: From the previous diagram, we can see that each of the five bootstrapped samples
    taken from the primary dataset is different and has different characteristics.
    As such, training models on each of these resamples would result in different
    predictions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图示中，我们可以看到，从主数据集中取出的五个重采样样本各不相同，并具有不同的特征。因此，在这些重采样上训练模型将会得出不同的预测结果。
- en: 'The following are the advantages of bootstrapping:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是自助法的优点：
- en: Each resample can contain different characteristics from that of the entire
    dataset, allowing us a different perspective of how the data behaves.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个重采样可能包含与整个数据集不同的特征，从而为我们提供了不同的视角来观察数据的行为。
- en: Algorithms that make use of bootstrapping are powerfully built and handle unseen
    data better, especially on smaller datasets that have a tendency to cause overfitting.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自助法的算法具有强大的构建能力，能够更好地处理未见过的数据，尤其是在较小的数据集上，这类数据集往往会导致过拟合。
- en: The bootstrap method can test the stability of a prediction by testing models
    using datasets with different variations and characteristics, resulting in a model
    that is more robust.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法方法可以通过使用具有不同变异性和特征的数据集测试模型的稳定性，从而得出一个更加稳健的模型。
- en: 'Now that we are aware of what bootstrapping is, what exactly does a bagging
    ensemble do? In simple words, bagging means aggregating the outputs of parallel
    models, each of which is built by bootstrapping data. It is essentially an ensemble
    model that generates multiple versions of a predictor on each resample and uses
    these to get an aggregated predictor. The aggregation step gives us a meta prediction,
    which involves taking an average over the models when predicting a continuous
    numerical value for regression problems, while taking a vote when predicting a
    class for classification problems. Voting can be of two types:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了什么是自助抽样，那么袋装集成究竟是做什么的呢？简单来说，袋装意味着聚合并行模型的输出，每个模型都是通过自助抽样数据构建的。它本质上是一个集成模型，在每次重采样时生成多个预测器版本，并利用这些版本得到一个聚合的预测器。聚合步骤为我们提供了一个元预测，这个过程涉及对回归问题时的连续数值预测进行求平均，而在分类问题中，则是进行投票。投票可以分为两种类型：
- en: Hard voting (class-based)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 硬投票（基于类别的投票）
- en: Soft voting (probabilistic)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 软投票（概率投票）
- en: In hard voting, we consider the majority among the classes predicted by the
    base estimators, whereas in soft voting, we average the probabilities of belonging
    to a class and then predict the class.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬投票中，我们考虑由基础估计器预测的各个类别的多数意见，而在软投票中，我们首先平均各类别的概率值，然后再做出预测。
- en: 'The following diagram gives us a visual representation of how a bagging estimator
    is built from the bootstrap sampling shown in Figure 6.7:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下图为我们提供了一个可视化的示意，展示了如何通过图 6.7 中所示的自助抽样构建袋装估计器：
- en: '![Figure 6.8: Bagging estimator built from bootstrap sampling'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8：通过自助抽样构建的袋装估计器'
- en: '](img/image-NAPC770H.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8：通过自助抽样构建的袋装估计器](img/image-NAPC770H.jpg)'
- en: 'Figure 6.8: Bagging estimator built from bootstrap sampling'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：通过自助抽样构建的袋装估计器
- en: Since each model is essentially independent of the others, all the base models
    can be trained in parallel, considerably speeding up the training process as a
    resampled dataset is smaller in size than the original dataset, and therefore
    allowing us to take advantage of the computational power we have on our hands
    today.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型本质上是独立的，因此所有基础模型可以并行训练，这大大加快了训练过程，因为重采样的数据集比原始数据集小，从而使我们能够充分利用当前计算能力。
- en: 'Bagging essentially helps to reduce the variance of the entire ensemble. It
    does so by introducing randomization into its formulation procedure and is usually
    used with a base predictor that has a tendency to overfit the training data. The
    primary point of consideration here would be the stability (or lack thereof) of
    the training dataset: bagging proves effective in cases where a slight perturbation
    in data leads to a significant change in model results, that is, the model with
    high variance. This is how bagging helps in countering variance.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装本质上有助于减少整个集成的方差。它通过引入随机化到其构建过程中来实现这一点，通常用于那些容易对训练数据过拟合的基础预测器。在这里需要考虑的主要因素是训练数据集的稳定性（或缺乏稳定性）：袋装在数据轻微扰动会导致模型结果显著变化的情况下表现有效，也就是高方差的模型。这就是袋装如何帮助减少方差的方式。
- en: scikit-learn uses BaggingClassifier and BaggingRegressor to implement generic
    bagging ensembles for classification and regression tasks, respectively. The primary
    inputs to these are the base estimators to use on each resample, along with the
    number of estimators to use (that is, the number of resamples).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 使用 BaggingClassifier 和 BaggingRegressor 来分别实现用于分类和回归任务的通用袋装集成方法。这些方法的主要输入是用于每次重采样的基础估计器，以及要使用的估计器数量（即重采样次数）。
- en: 'Exercise 6.02: Using the Bagging Classifier'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 6.02：使用袋装分类器
- en: In this exercise, we will use scikit-learn's bagging classifier as our ensemble,
    with DecisionTreeClassifier as the base estimator. We know that decision trees
    are prone to overfitting, and so will have a high variance and low bias, both
    being important characteristics for the base estimators to be used in bagging
    ensembles.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 scikit-learn 的袋装分类器作为我们的集成方法，并使用决策树分类器作为基础估计器。我们知道决策树容易发生过拟合，因此它们具有高方差和低偏差，这两者都是用于袋装集成中的基础估计器的重要特征。
- en: 'The dataset that we will use in this exercise is the Titanic dataset. Please
    complete Exercise 6.01, Importing Modules and Preparing the Dataset, before you
    embark on this exercise:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个练习中使用 Titanic 数据集。请在开始本练习之前，先完成练习 6.01：导入模块并准备数据集：
- en: 'Import the base and ensemble classifiers:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 导入基础分类器和集成分类器：
- en: from sklearn.tree import DecisionTreeClassifier
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从 sklearn.tree 导入决策树分类器（DecisionTreeClassifier）
- en: from sklearn.ensemble import BaggingClassifier
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import BaggingClassifier
- en: Specify the hyperparameters and initialize the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: Here, we will first specify the hyperparameters of the base estimator, for which
    we are using the decision tree classifier with the entropy or information gain
    as the splitting criterion. We will not specify any limits on the depth of the
    tree or size/number of leaves on each tree to grow fully. Following this, we will
    define the hyperparameters for the bagging classifier and pass the base estimator
    object to the classifier as a hyperparameter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将首先指定基础估计器的超参数，我们使用决策树分类器，并将熵或信息增益作为分裂准则。我们不会对树的深度或每棵树的叶子数量/大小设置任何限制，以便它完全生长。接下来，我们将为
    Bagging 分类器定义超参数，并将基础估计器对象作为超参数传递给分类器。
- en: 'We will take 50 base estimators for our example, which will run in parallel
    and utilize all the processes available in the machine (which is done by specifying
    n_jobs=-1). Additionally, we will specify max_samples as 0.5, indicating that
    the number of data points in the bootstrap should be half that in the total dataset.
    We will also set a random state (to any arbitrary value, which will stay constant
    throughout) to maintain the reproducibility of the results:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以 50 个基础估计器作为示例，这些估计器将并行运行，并利用机器上可用的所有进程（通过指定 n_jobs=-1 实现）。此外，我们将指定 max_samples
    为 0.5，表示引导法中的数据点数量应为总数据集的一半。我们还将设置一个随机状态（为任意值，且在整个过程中保持不变）以确保结果的可重复性：
- en: 'dt_params = {''criterion'': ''entropy'', ''random_state'': 11}'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'dt_params = {''criterion'': ''entropy'', ''random_state'': 11}'
- en: dt = DecisionTreeClassifier(**dt_params)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: dt = DecisionTreeClassifier(**dt_params)
- en: 'bc_params = {''base_estimator'': dt, ''n_estimators'': 50, \'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 'bc_params = {''base_estimator'': dt, ''n_estimators'': 50, \'
- en: '''max_samples'': 0.5, ''random_state'': 11, ''n_jobs'': -1}'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '''max_samples'': 0.5, ''random_state'': 11, ''n_jobs'': -1}'
- en: bc = BaggingClassifier(**bc_params)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: bc = BaggingClassifier(**bc_params)
- en: Fit the bagging classifier model to the training data and calculate the prediction accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Bagging 分类器模型拟合到训练数据上并计算预测准确率。
- en: 'Let''s now fit the bagging classifier and find the meta predictions for both
    the training and validation sets. Following this, let''s find the prediction accuracy
    on the training and validation datasets:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们拟合 Bagging 分类器，并找出训练集和验证集的元预测值。接下来，我们计算训练集和验证数据集的预测准确率：
- en: bc.fit(x_train, y_train)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: bc.fit(x_train, y_train)
- en: bc_preds_train = bc.predict(x_train)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: bc_preds_train = bc.predict(x_train)
- en: bc_preds_val = bc.predict(x_val)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: bc_preds_val = bc.predict(x_val)
- en: print('Bagging Classifier:\n> Accuracy on training data = {:.4f}'\
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: print('Bagging 分类器:\n> 训练数据上的准确率 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据上的准确率 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=bc_preds_train), \
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=bc_preds_train)，\
- en: accuracy_score(y_true=y_val, y_pred=bc_preds_val)))
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=bc_preds_val)))
- en: 'The output is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: 'Bagging Classifier:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 分类器：
- en: Accuracy on training data = 0.9270
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据上的准确率 = 0.9270
- en: Accuracy on validation data = 0.8659
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据上的准确率 = 0.8659
- en: Fit the decision tree model to the training data to compare prediction accuracy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 将决策树模型拟合到训练数据上，以比较预测准确率。
- en: 'Let''s also fit the decision tree (from the object we initialized in Step 2)
    so that we will be able to compare the prediction accuracies of the ensemble with
    that of the base predictor:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们也拟合决策树（来自步骤 2 中初始化的对象），这样我们就可以比较集成模型和基础预测器的预测准确率：
- en: dt.fit(x_train, y_train)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: dt.fit(x_train, y_train)
- en: dt_preds_train = dt.predict(x_train)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: dt_preds_train = dt.predict(x_train)
- en: dt_preds_val = dt.predict(x_val)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: dt_preds_val = dt.predict(x_val)
- en: print('Decision Tree:\n> Accuracy on training data = {:.4f}'\
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: print('决策树:\n> 训练数据上的准确率 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据上的准确率 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=dt_preds_train), \
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=dt_preds_train)，\
- en: accuracy_score(y_true=y_val, y_pred=dt_preds_val)))
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=dt_preds_val)))
- en: 'The output is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: 'Decision Tree:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树：
- en: Accuracy on training data = 0.9831
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据上的准确率 = 0.9831
- en: Accuracy on validation data = 0.7709
  id: totrans-190
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据上的准确率 = 0.7709
- en: Here, we can see that, although the decision tree has a much higher training
    accuracy than the bagging classifier, its accuracy on the validation dataset is
    lower, a clear signal that the decision tree is overfitting to the training data.
    The bagging ensemble, on the other hand, reduces the overall variance and results
    in a much more accurate prediction.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，尽管决策树的训练准确率远高于袋装分类器，但其在验证数据集上的准确率较低，这清楚地表明决策树对训练数据进行了过拟合。而袋装集成方法则通过减少整体方差，产生了更为准确的预测。
- en: Note
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/37O6735.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这一特定部分的源代码，请参阅 https://packt.live/37O6735。
- en: You can also run this example online at https://packt.live/2Nh3ayB. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/2Nh3ayB 上在线运行这个示例。你必须执行整个Notebook才能得到预期的结果。
- en: Next, we will look at perhaps the most widely known bagging-based machine learning
    model there is, the random forest model. Random forest is a bagging ensemble model
    that uses a decision tree as the base estimator.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论可能是最广为人知的基于袋装的机器学习模型——随机森林模型。随机森林是一个袋装集成模型，使用决策树作为基础估计器。
- en: Random Forest
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: An issue that is commonly faced with decision trees is that the split on each
    node is performed using a greedy algorithm that minimizes the entropy of the leaf
    nodes. Keeping this in mind, the base estimator decision trees in a bagging classifier
    can still be similar in terms of the features they split on, and so can have predictions
    that are quite similar. However, bagging is only useful in reducing the variance
    in predictions if the predictions from the base models are not correlated.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树常面临的一个问题是，在每个节点上的分割是通过贪婪算法完成的，旨在最小化叶节点的熵。考虑到这一点，袋装分类器中的基础估计器决策树在分割特征时仍然可能非常相似，因此它们的预测也可能非常相似。然而，袋装方法只有在基础模型的预测不相关时，才有助于减少预测的方差。
- en: The random forest algorithm attempts to overcome this problem by not only bootstrapping
    the data points in the overall training dataset, but also bootstrapping the features
    available for each tree to split on. This ensures that when the greedy algorithm
    is searching for the best feature to split on, the overall best feature may not
    always be available in the bootstrapped features for the base estimator, and so
    would not be chosen – resulting in base trees that have different structures.
    This simple tweak lets the best estimators be trained in such a way that the predictions
    from each tree in the forest have a lower probability of being correlated to the
    predictions from other trees.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法通过不仅对整体训练数据集中的数据点进行自助抽样，还对每棵树可分割的特征进行自助抽样，从而试图克服这个问题。这确保了当贪婪算法在寻找最佳分割特征时，整体最佳特征可能并不总是出现在基础估计器的自助抽样特征中，因此不会被选择——这导致基础树具有不同的结构。这个简单的调整使得最好的估计器能够以一种方式进行训练，从而使得森林中每棵树的预测与其他树的预测之间的相关性较低。
- en: Each base estimator in the random forest has a random sample of data points
    as well as a random sample of features. And since the ensemble is made up of decision
    trees, the algorithm is called a random forest.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林中的每个基础估计器都有一个数据点的随机样本和一个特征的随机样本。由于该集成由决策树组成，因此该算法被称为随机森林。
- en: 'Exercise 6.03: Building the Ensemble Model Using Random Forest'
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 6.03：使用随机森林构建集成模型
- en: The two primary parameters that random forest takes are the fraction of features
    and the fraction of data points to bootstrap on, to train each base decision tree.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的两个主要参数是用于训练每个基础决策树的特征的分数和数据点的分数。
- en: In this exercise, we will use scikit-learn's random forest classifier to build
    the ensemble model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用 scikit-learn 的随机森林分类器来构建集成模型。
- en: 'The dataset that we will use in this exercise is the Titanic dataset. This
    exercise is a continuation of Exercise 6.02, Using a Bagging Classifier:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习中，我们将使用泰坦尼克号数据集。这个练习是练习6.02《使用袋装分类器》的延续：
- en: 'Import the ensemble classifier:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 导入集成分类器：
- en: from sklearn.ensemble import RandomForestClassifier
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `sklearn.ensemble` 的 `RandomForestClassifier`
- en: Specify the hyperparameters and initialize the model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: 'Here, we will use entropy as the splitting criterion for the decision trees
    in a forest comprising 100 trees. As before, we will not specify any limits regarding
    the depth of the trees or the size/number of leaves. Unlike the bagging classifier,
    which took max_samples as an input during initialization, the random forest algorithm
    takes in only max_features, indicating the number (or fraction) of features in
    the bootstrap sample. We will specify the value for this as 0.5, so that only
    three out of six features are considered for each tree:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用熵（entropy）作为决策树的分裂标准，森林中包含100棵树。与之前一样，我们不会指定树的深度或叶子节点的大小/数量的限制。与袋装分类器不同，袋装分类器在初始化时需要输入max_samples，而随机森林算法只需要max_features，表示自助法样本中的特征数量（或比例）。我们将此值设置为0.5，以便每棵树只考虑六个特征中的三个：
- en: 'rf_params = {''n_estimators'': 100, ''criterion'': ''entropy'', \'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 'rf_params = {''n_estimators'': 100, ''criterion'': ''entropy'', \'
- en: '''max_features'': 0.5, ''min_samples_leaf'': 10, \'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '''max_features'': 0.5, ''min_samples_leaf'': 10, \'
- en: '''random_state'': 11, ''n_jobs'': -1}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '''random_state'': 11, ''n_jobs'': -1}'
- en: rf = RandomForestClassifier(**rf_params)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: rf = RandomForestClassifier(**rf_params)
- en: Fit the random forest classifier model to the training data and calculate the
    prediction accuracy.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将随机森林分类器模型拟合到训练数据，并计算预测准确度。
- en: 'Let''s now fit the random forest model and find the meta predictions for both
    the training and validation sets. Following this, let''s find the prediction accuracy
    on the training and validation datasets:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将拟合随机森林模型，并找到训练集和验证集的元预测。接下来，找出训练和验证数据集的预测准确度：
- en: rf.fit(x_train, y_train)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: rf.fit(x_train, y_train)
- en: rf_preds_train = rf.predict(x_train)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: rf_preds_train = rf.predict(x_train)
- en: rf_preds_val = rf.predict(x_val)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: rf_preds_val = rf.predict(x_val)
- en: print('Random Forest:\n> Accuracy on training data = {:.4f}'\
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: print('随机森林：\n> 训练数据集准确度 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据集准确度 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=rf_preds_train), \
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=rf_preds_train), \
- en: accuracy_score(y_true=y_val, y_pred=rf_preds_val)))
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=rf_preds_val)))
- en: 'The output is as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: 'Random Forest:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林：
- en: Accuracy on training data = 0.8385
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据集准确度 = 0.8385
- en: Accuracy on validation data = 0.8771
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据集准确度 = 0.8771
- en: If we compare the prediction accuracies of random forest on our dataset to that
    of the bagging classifier, we can see that the accuracy on the validation set
    is pretty much the same, although the latter has higher accuracy with regard to
    the training dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将随机森林在我们的数据集上的预测准确度与袋装分类器进行比较，我们会发现验证集上的准确度几乎相同，尽管后者在训练数据集上的准确度较高。
- en: Note
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/3dlvGtd.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问该特定部分的源代码，请参阅 https://packt.live/3dlvGtd。
- en: You can also run this example online at https://packt.live/2NkSPS5\. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 https://packt.live/2NkSPS5 上在线运行此示例。你必须执行整个Notebook才能获得预期结果。
- en: Boosting
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提升
- en: The second ensemble technique we'll be looking at is boosting, which involves
    incrementally training new models that focus on the misclassified data points
    in the previous model and utilizes weighted averages to turn weak models (underfitting
    models having a high bias) into stronger models. Unlike bagging, where each base
    estimator could be trained independently of the others, the training of each base
    estimator in a boosted algorithm depends on the previous one.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的第二种集成技术是提升（boosting），它通过逐步训练新模型来集中关注上一模型中被误分类的数据点，并利用加权平均将弱模型（具有较高偏差的欠拟合模型）转化为更强的模型。与袋装方法不同，袋装方法中的每个基础估计器可以独立训练，而提升算法中的每个基础估计器的训练依赖于前一个模型。
- en: Although boosting also uses the concept of bootstrapping, it's done differently
    from bagging, since each sample of data is weighted, implying that some bootstrapped
    samples can be used for training more often than other samples. When training
    each model, the algorithm keeps track of which features are most useful and which
    data samples have the most prediction error; these are given higher weightage
    and are considered to require more iterations to properly train the model.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提升（boosting）也使用了自助法（bootstrapping）概念，但与袋装（bagging）不同，它的实现方式不同，因为每个数据样本都有权重，这意味着某些自助法样本在训练中可能会比其他样本更频繁地被使用。在训练每个模型时，算法会跟踪哪些特征最有用，以及哪些数据样本的预测误差最大；这些样本会被赋予更高的权重，并被认为需要更多的迭代才能正确训练模型。
- en: When predicting the output, the boosting ensemble takes a weighted average of
    the predictions from each base estimator, giving a higher weight to the ones that
    had lower errors during the training stage. This means that, for the data points
    that are misclassified by the model in an iteration, the weights for those data
    points are increased so that the next model is more likely to classify it correctly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测输出时，提升集成会对每个基础估计器的预测结果进行加权平均，给训练阶段错误较低的预测结果赋予更高的权重。这意味着，对于在某次迭代中被错误分类的数据点，这些数据点的权重会被增加，从而使下一个模型更可能正确分类这些数据点。
- en: 'As was the case with bagging, the results from all the boosting base estimators
    are aggregated to produce a meta prediction. However, unlike bagging, the accuracy
    of a boosted ensemble increases significantly with the number of base estimators
    in the boosted ensemble:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装法（bagging）类似，所有提升（boosting）基础估计器的结果被汇总以产生一个元预测。然而，与袋装法不同，提升集成的准确性会随着基础估计器数量的增加而显著提高：
- en: '![Figure 6.9: A boosted ensemble'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9：一个提升集成模型'
- en: '](img/image-88NHH80A.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-88NHH80A.jpg)'
- en: 'Figure 6.9: A boosted ensemble'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：一个提升集成模型
- en: In the diagram, we can see that, after each iteration, the misclassified points
    have increased weights (represented by larger icons) so that the next base estimator
    that is trained is able to focus on those points. The final predictor has aggregated
    the decision boundaries from each of its base estimators.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在图示中，我们可以看到，在每次迭代之后，被错误分类的点的权重（以较大的图标表示）增加，以便下一个训练的基础估计器能够集中关注这些点。最终的预测器将每个基础估计器的决策边界进行汇总。
- en: Boosting is used extensively in real-world applications. For example, the commercial
    web search engines Yahoo and Yandex use variants of boosting in their machine-learned
    ranking engines. Ranking is the task of finding the most relevant documents given
    a search query. Particularly, in the case of Yandex, they use a gradient boosting-based
    approach to build an ensemble tree model that outperforms other models, including
    Yandex's previously used models, by achieving the lowest discounted cumulative
    gain of 4.14123\. This shows how useful boosting-based modeling can prove in real-life
    scenarios.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 提升在实际应用中被广泛使用。例如，商业化的网页搜索引擎 Yahoo 和 Yandex 在其机器学习排名引擎中使用了提升的变体。排名是根据搜索查询找到最相关文档的任务。特别是对于
    Yandex 来说，他们采用了一种基于梯度提升（gradient boosting）的方法来构建集成树模型，该模型通过实现最低的折扣累积增益 4.14123，比其他模型，包括
    Yandex 之前使用的模型表现得更好。这显示了基于提升的建模在现实场景中的巨大实用价值。
- en: Note
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Read more on Yandex at the following link: http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读更多关于 Yandex 的信息，请访问以下链接： http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118.
- en: Adaptive Boosting
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应提升
- en: 'Let''s now talk about a boosting technique called adaptive boosting, which
    is best used to boost the performance of decision stumps for binary classification
    problems. Decision stumps are essentially decision trees with a maximum depth
    of one (only one split is made on a single feature), and, as such, are weak learners.
    The primary principle that adaptive boosting works on is the same: to improve
    the areas where the base estimator fails to turn an ensemble of weak learners
    into a strong learner.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们谈谈一种叫做自适应提升（adaptive boosting）的技术，它最适用于提升决策树桩在二分类问题中的表现。决策树桩本质上是深度为一的决策树（仅对单一特征进行一次分裂），因此是弱学习器。自适应提升工作的主要原理与此相同：通过提高基础估计器无法正确分类的区域，将一组弱学习器转化为强学习器。
- en: To start with, the first base estimator takes a bootstrap of data points from
    the main training set and fits a decision stump to classify the sampled points,
    after which the trained decision tree stump is fit to the complete training data.
    For the samples that are misclassified, the weights are increased so that there
    is a higher probability of these data points being selected in the bootstrap for
    the next base estimator. A decision stump is again trained on the new bootstrap
    to classify the data points in the sample. Subsequently, the mini ensemble comprising
    the two base estimators is used to classify the data points in the entire training
    set. The misclassified data points from the second round are given a higher weight
    to improve their probability of selection and so on until the ensemble reaches
    the limit regarding the number of base estimators it should contain.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，首个基学习器从主训练集抽取一个自助法的数据点，并拟合一个决策桩来分类这些样本点，接着，训练过的决策桩会拟合整个训练数据。对于那些被错误分类的样本，增加权重，使这些数据点在下一个基学习器的自助法抽样中被选中的概率更高。然后，再次对新抽样的数据点进行训练，进行分类。接着，由这两个基学习器组成的小型集成模型被用来分类整个训练集中的数据点。第二轮中被错误分类的数据点会被赋予更高的权重，以提高它们被选中的概率，直到集成模型达到它应该包含的基学习器数量的限制。
- en: One drawback of adaptive boosting is that the algorithm is easily influenced
    by noisy data points and outliers since it tries to fit every point perfectly.
    As such, it is prone to overfitting if the number of estimators is very high.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应提升的一个缺点是，算法很容易受到噪声数据点和异常值的影响，因为它尝试完美拟合每一个数据点。因此，如果基学习器数量过多，它容易出现过拟合。
- en: 'Exercise 6.04: Implementing Adaptive Boosting'
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 6.04：实现自适应提升
- en: 'In this exercise, we''ll use scikit-learn''s implementation of adaptive boosting
    for classification, AdaBoostClassifier:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 scikit-learn 的自适应提升算法进行分类，即 AdaBoostClassifier：
- en: 'We will again be using the Titanic dataset. This exercise is a continuation
    of Exercise 6.03, Building the Ensemble Model Using Random Forest:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用 Titanic 数据集。这个练习是练习 6.03《使用随机森林构建集成模型》的延续：
- en: 'Import the classifier:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 导入分类器：
- en: from sklearn.ensemble import AdaBoostClassifier
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: from sklearn.ensemble import AdaBoostClassifier
- en: Specify the hyperparameters and initialize the model.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: 'Here, we will first specify the hyperparameters of the base estimator, for
    which we are using the decision tree classifier with a maximum depth of one, that
    is, a decision stump. Following this, we will define the hyperparameters for the
    AdaBoost classifier and pass the base estimator object to the classifier as a hyperparameter:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先指定基学习器的超参数，使用的是一个最大深度为 1 的决策树分类器，即一个决策桩。接着，我们会定义 AdaBoost 分类器的超参数，并将基学习器对象作为超参数传递给分类器：
- en: 'dt_params = {''max_depth'': 1, ''random_state'': 11}'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 'dt_params = {''max_depth'': 1, ''random_state'': 11}'
- en: dt = DecisionTreeClassifier(**dt_params)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: dt = DecisionTreeClassifier(**dt_params)
- en: 'ab_params = {''n_estimators'': 100, ''base_estimator'': dt, \'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'ab_params = {''n_estimators'': 100, ''base_estimator'': dt, \'
- en: '''random_state'': 11}'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '''random_state'': 11}'
- en: ab = AdaBoostClassifier(**ab_params)
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ab = AdaBoostClassifier(**ab_params)
- en: Fit the model to the training data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型拟合到训练数据上。
- en: 'Let''s now fit the AdaBoost model and find the meta predictions for both the
    training and validation sets. Following this, let''s find the prediction accuracy
    on the training and validation datasets:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们拟合 AdaBoost 模型，并计算训练集和验证集的元预测。接着，我们来计算训练集和验证集的数据集上的预测准确度：
- en: ab.fit(x_train, y_train)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ab.fit(x_train, y_train)
- en: ab_preds_train = ab.predict(x_train)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ab_preds_train = ab.predict(x_train)
- en: ab_preds_val = ab.predict(x_val)
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ab_preds_val = ab.predict(x_val)
- en: print('Adaptive Boosting:\n> Accuracy on training data = {:.4f}'\
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: print('自适应提升：\n> 训练数据的准确率 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据的准确率 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=ab_preds_train), \
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=ab_preds_train), \
- en: accuracy_score(y_true=y_val, y_pred=ab_preds_val)
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=ab_preds_val)
- en: ))
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: 'The output is as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: 'Adaptive Boosting:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应提升：
- en: Accuracy on training data = 0.8272
  id: totrans-271
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据的准确率 = 0.8272
- en: Accuracy on validation data = 0.8547
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据的准确率 = 0.8547
- en: Calculate the prediction accuracy of the model on the training and validation
    data for a varying number of base estimators.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 计算模型在不同数量的基学习器下，对训练数据和验证数据的预测准确度。
- en: 'Earlier, we claimed that the accuracy tends to increase with an increasing
    number of base estimators, but also that the model has a tendency to overfit if
    too many base estimators are used. Let''s calculate the prediction accuracies
    so that we can find the point where the model begins to overfit the training data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们提到过，准确率随着基本估计器数量的增加而增加，但如果使用的基本估计器过多，模型也有可能出现过拟合。现在让我们计算预测准确率，以便找出模型开始过拟合训练数据的点：
- en: 'ab_params = {''base_estimator'': dt, ''random_state'': 11}'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 'ab_params = {''base_estimator'': dt, ''random_state'': 11}'
- en: n_estimator_values = list(range(10, 210, 10))
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: n_estimator_values = list(range(10, 210, 10))
- en: train_accuracies, val_accuracies = [], []
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: train_accuracies, val_accuracies = [], []
- en: 'for n_estimators in n_estimator_values:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对于n_estimators中的每个值：
- en: ab = AdaBoostClassifier(n_estimators=n_estimators, **ab_params)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ab = AdaBoostClassifier(n_estimators=n_estimators, **ab_params)
- en: ab.fit(x_train, y_train)
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ab.fit(x_train, y_train)
- en: ab_preds_train = ab.predict(x_train)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ab_preds_train = ab.predict(x_train)
- en: ab_preds_val = ab.predict(x_val)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ab_preds_val = ab.predict(x_val)
- en: train_accuracies.append(accuracy_score(y_true=y_train, \
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: train_accuracies.append(accuracy_score(y_true=y_train, \
- en: y_pred=ab_preds_train))
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=ab_preds_train))
- en: val_accuracies.append(accuracy_score(y_true=y_val, \
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: val_accuracies.append(accuracy_score(y_true=y_val, \
- en: y_pred=ab_preds_val))
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=ab_preds_val))
- en: 'Plot a line graph to visualize the trend of the prediction accuracies on both
    the training and validation datasets:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制折线图，以可视化训练和验证数据集上预测准确率的趋势：
- en: plt.figure(figsize=(10,7))
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: plt.figure(figsize=(10,7))
- en: plt.plot(n_estimator_values, train_accuracies, label='Train')
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(n_estimator_values, train_accuracies, label='训练')
- en: plt.plot(n_estimator_values, val_accuracies, label='Validation')
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: plt.plot(n_estimator_values, val_accuracies, label='验证')
- en: plt.ylabel('Accuracy score')
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: plt.ylabel('准确率')
- en: plt.xlabel('n_estimators')
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: plt.xlabel('n_estimators')
- en: plt.legend()
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: plt.legend()
- en: plt.show()
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: plt.show()
- en: 'The output is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.10: The trend of the prediction accuracies'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10：预测准确率的趋势'
- en: '](img/image-OQWVD6TB.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-OQWVD6TB.jpg)'
- en: 'Figure 6.10: The trend of the prediction accuracies'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：预测准确率的趋势
- en: As was mentioned earlier, we can see that the training accuracy almost consistently
    increases as the number of decision tree stumps increases from 10 to 200\. However,
    the validation accuracy fluctuates between 0.84 and 0.86 and begins to drop as
    the number of decision stumps goes higher. This happens because the AdaBoost algorithm
    is trying to fit the noisy data points and outliers as well.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，我们可以看到训练准确率几乎始终随着决策树桩数量的增加（从10到200）而不断提高。然而，验证准确率在0.84到0.86之间波动，并且随着决策树桩数量的增加而开始下降。这是因为AdaBoost算法也在尝试拟合噪声数据点和异常值。
- en: Note
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/2V4zB7K.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 https://packt.live/2V4zB7K。
- en: You can also run this example online at https://packt.live/3dhSBpu. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址为 https://packt.live/3dhSBpu。您必须执行整个Notebook才能获得预期的结果。
- en: Gradient Boosting
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度提升
- en: Gradient boosting is an extension of the boosting method that visualizes boosting
    as an optimization problem. A loss function is defined that is representative
    of the error residuals (the difference between the predicted and true values),
    and the gradient descent algorithm is used to optimize the loss function.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是提升方法的扩展，它将提升视为一个优化问题。定义了一个损失函数，代表了误差残差（预测值与真实值之间的差异），并使用梯度下降算法来优化损失函数。
- en: In the first step, a base estimator (which would be a weak learner) is added
    and trained on the entire training dataset. The loss associated with the prediction
    is calculated and, in order to reduce the error residuals, the loss function is
    updated to add more base estimators for the data points where the existing estimators
    are performing poorly. Subsequently, the algorithm iteratively adds new base estimators
    and computes the loss to allow the optimization algorithm to update the model
    and minimize the residuals themselves.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，添加一个基本估计器（即弱学习器），并在整个训练数据集上进行训练。计算预测的损失，并为减少误差残差，更新损失函数，添加更多的基本估计器，来处理现有估计器表现不佳的数据点。接下来，算法迭代地添加新的基本估计器并计算损失，使得优化算法能够更新模型并最小化残差。
- en: In the case of adaptive boosting, decision stumps were used as the weak learners
    for the base estimators. However, for gradient boosting methods, larger trees
    can be used, but the weak learners should still be constrained by providing a
    limit to the maximum number of layers, nodes, splits, or leaf nodes. This ensures
    that the base estimators are still weak learners, but they can be constructed
    in a greedy manner.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在自适应提升的情况下，决策树桩被用作基础估计器的弱学习器。然而，对于梯度提升方法，可以使用更大的树，但弱学习器仍然需要受到限制，通过设置最大层数、节点数、分裂数或叶节点数的限制。这确保了基础估计器仍然是弱学习器，但它们可以以贪婪的方式构建。
- en: From the previous chapters, we know that the gradient descent algorithm can
    be used to minimize a set of parameters, such as the coefficients in a regression
    equation. When building an ensemble, however, we have decision trees instead of
    parameters that need to be optimized. After calculating the loss at each step,
    the gradient descent algorithm then has to modify the parameters of the new tree
    that's to be added to the ensemble in such a way that reduces the loss. This approach
    is more commonly known as functional gradient descent.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的章节我们知道，梯度下降算法可以用来最小化一组参数，比如回归方程中的系数。然而，在构建集成时，我们使用的是决策树，而不是需要优化的参数。在每一步计算损失后，梯度下降算法必须调整将要加入集成的新树的参数，以减少损失。这种方法更常被称为功能梯度下降。
- en: 'Exercise 6.05: Implementing GradientBoostingClassifier to Build an Ensemble Model'
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习 6.05：实现 GradientBoostingClassifier 来构建集成模型
- en: The two primary parameters that the gradient boosting classifier takes are the
    fraction of features and the fraction of data points to bootstrap on, to train
    each base decision tree.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升分类器的两个主要参数是用于训练每棵基础决策树的特征的比例和需要进行自助抽样的数据点比例。
- en: In this exercise, we will use scikit-learn's gradient boosting classifier to
    build the boosting ensemble model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 scikit-learn 的梯度提升分类器来构建提升集成模型。
- en: 'This exercise is a continuation of Exercise 6.04, Implementing Adaptive Boosting:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习是练习 6.04：实现自适应提升的延续。
- en: 'Import the ensemble classifier:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 导入集成分类器：
- en: from sklearn.ensemble import GradientBoostingClassifier
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 sklearn.ensemble 的导入 GradientBoostingClassifier
- en: Specify the hyperparameters and initialize the model.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 指定超参数并初始化模型。
- en: 'Here, we will use 100 decision trees as the base estimator, with each tree
    having a maximum depth of three and a minimum of five samples in each of its leaves.
    Although we are not using decision stumps, as in the previous example, the tree
    is still small and would be considered a weak learner:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 100 棵决策树作为基础估计器，每棵树的最大深度为 3，并且每个叶节点至少有 5 个样本。尽管我们没有像之前的示例那样使用决策树桩，但树仍然很小，可以认为是一个弱学习器：
- en: 'gbc_params = {''n_estimators'': 100, ''max_depth'': 3, \'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 'gbc_params = {''n_estimators'': 100, ''max_depth'': 3, \'
- en: '''min_samples_leaf'': 5, ''random_state'': 11}'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '''min_samples_leaf'': 5, ''random_state'': 11}'
- en: gbc = GradientBoostingClassifier(**gbc_params)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: gbc = GradientBoostingClassifier(**gbc_params)
- en: Fit the gradient boosting model to the training data and calculate the prediction accuracy.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 将梯度提升模型拟合到训练数据，并计算预测准确率。
- en: 'Let''s now fit the ensemble model and find the meta predictions for both the
    training and validation set. Following this, we will find the prediction accuracy
    on the training and validation datasets:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们拟合集成模型并找到训练集和验证集的元预测。接下来，我们将计算训练集和验证集上的预测准确率：
- en: gbc.fit(x_train, y_train)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: gbc.fit(x_train, y_train)
- en: gbc_preds_train = gbc.predict(x_train)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: gbc_preds_train = gbc.predict(x_train)
- en: gbc_preds_val = gbc.predict(x_val)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: gbc_preds_val = gbc.predict(x_val)
- en: print('Gradient Boosting Classifier:'\
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: print('梯度提升分类器：'\
- en: '''\n> Accuracy on training data = {:.4f}''\'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 训练数据的准确率 = {:.4f}''\'
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据的准确率 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=gbc_preds_train), \
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=gbc_preds_train), \
- en: accuracy_score(y_true=y_val, y_pred=gbc_preds_val)))
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=gbc_preds_val)))
- en: 'The output is as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: 'Gradient Boosting Classifier:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升分类器：
- en: Accuracy on training data = 0.8961
  id: totrans-332
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据上的准确率 = 0.8961
- en: Accuracy on validation data = 0.8771
  id: totrans-333
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据上的准确率 = 0.8771
- en: Note
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/37QANjZ.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 https://packt.live/37QANjZ。
- en: You can also run this example online at https://packt.live/2YljJ2D. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是https://packt.live/2YljJ2D。你必须执行整个Notebook才能获得期望的结果。
- en: We can see that the gradient boosting ensemble has greater accuracy on both
    the training and validation datasets compared to those for the adaptive boosting ensemble.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，梯度提升集成模型在训练和验证数据集上的准确度都高于自适应提升集成模型。
- en: Stacking
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 堆叠
- en: Stacking, or stacked generalization, is also called meta ensembling. It is a
    model ensembling technique that consists of combining data from multiple models'
    predictions and using them as features to generate a new model. The stacked model
    will most likely outperform each of the individual models due to the smoothing
    effect it adds, as well as due to its ability to "choose" the base model that
    performs best in certain scenarios. Keeping this in mind, stacking is usually
    most effective when each of the base models is significantly different from each
    other.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠（Stacking）或堆叠泛化，也叫元集成（meta ensembling），是一种模型集成技术，旨在将多个模型的预测结果合并并作为特征，生成一个新的模型。堆叠模型通常会超越每个单独模型的表现，因为它加入了平滑效应，并且能“选择”在某些情境下表现最好的基础模型。考虑到这一点，堆叠通常在每个基础模型彼此之间有显著差异时最为有效。
- en: Stacking is widely used in real-world applications. One popular example comes
    from the well-known Netflix competition whose two top performers built solutions
    that were based on stacking models. Netflix is a well-known streaming platform
    and the competition was about building the best recommendation engine. The winning
    algorithm was based on feature-weighted-linear-stacking, which basically had meta
    features derived from individual models/algorithms such as Singular Value Decomposition
    (SVD), Restricted Boltzmann Machines (RBMs), and K-Nearest Neighbors (KNN). One
    such meta feature was the standard deviation of the prediction of a 60-factor
    ordinal SVD. These meta features were found necessary to be able to achieve the
    winning model, which proves the power of stacking in real-world applications.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠在实际应用中被广泛使用。一个著名的例子来自著名的Netflix竞赛，两位顶尖参赛者基于堆叠模型构建了解决方案。Netflix是一个知名的流媒体平台，这场竞赛的内容是构建最好的推荐引擎。获胜算法基于特征加权线性堆叠（feature-weighted-linear-stacking），其基本上是通过从单个模型/算法（如奇异值分解（SVD）、限制玻尔兹曼机（RBM）和K最近邻（KNN））中提取元特征。例如，一个元特征是60因子序数SVD的标准差。这些元特征被认为是实现获胜模型所必需的，证明了堆叠在实际应用中的强大能力。
- en: Stacking uses the predictions of the base models as additional features when
    training the final model – these are known as meta features. The stacked model
    essentially acts as a classifier that determines where each model is performing
    well and where it is performing poorly.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠使用基础模型的预测作为训练最终模型时的附加特征——这些被称为元特征。堆叠模型本质上充当一个分类器，确定每个模型在何处表现良好，何处表现较差。
- en: However, you cannot simply train the base models on the full training data,
    generate predictions on the full validation dataset, and then output these for
    second-level training. This runs the risk of your base model predictions already
    having "seen" the test set and therefore overfitting when feeding these predictions.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不能简单地将基础模型在完整的训练数据上训练，在完整的验证数据集上生成预测，然后将这些预测用于二级训练。这么做有可能导致你的基础模型预测已经“看到”测试集，因此在输入这些预测时会发生过拟合。
- en: 'It is important to note that the value of the meta features for each row cannot
    be predicted using a model that contained that row in the training data, as we
    then run the risk of overfitting since the base predictions would have already
    "seen" the target variable for that row. The common practice is to divide the
    training data into k subsets so that, when finding the meta features for each
    of those subsets, we only train the model on the remaining data. Doing this also
    avoids the problem of overfitting the data the model has already "seen":'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，对于每一行的元特征，其值不能使用包含该行的训练数据中的模型进行预测，因为这样我们就有可能发生过拟合，因为基础预测模型已经“看到”了该行的目标变量。常见的做法是将训练数据分成k个子集，这样，在为每个子集寻找元特征时，我们只在剩余数据上训练模型。这样做还避免了模型已经“看到”数据而发生过拟合的问题：
- en: '![Figure 6.11: A stacking ensemble'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11：堆叠集成'
- en: '](img/image-P8YI1C1F.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-P8YI1C1F.jpg)'
- en: 'Figure 6.11: A stacking ensemble'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：堆叠集成
- en: 'The preceding diagram shows how this is done: we divide the training data into
    k folds and find the predictions from the base models on each fold by training
    the model on the remaining k-1 folds. So, once we have the meta predictions for
    each of the folds, we can use those meta predictions along with the original features
    to train the stacked model.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示展示了如何执行此操作：我们将训练数据划分为k个子集，并通过在其余k-1个子集上训练模型来找到每个子集上的基础模型预测值。因此，一旦我们获得了每个子集的元预测值，我们就可以将这些元预测值与原始特征一起用于训练堆叠模型。
- en: 'Exercise 6.06: Building a Stacked Model'
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 练习6.06：构建堆叠模型
- en: In this exercise, we will use a support vector machine (scikit-learn's LinearSVC)
    and k-nearest neighbors (scikit-learn's KNeighborsClassifier) as the base predictors,
    and the stacked model will be a logistic regression classifier.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用支持向量机（scikit-learn的LinearSVC）和k近邻（scikit-learn的KNeighborsClassifier）作为基础预测器，而堆叠模型将是一个逻辑回归分类器。
- en: 'This exercise is a continuation of Exercise 6.05, Implementing GradientBoostingClassifier
    to Build an Ensemble Model:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习是练习6.05的延续，练习6.05中实现了GradientBoostingClassifier来构建集成模型：
- en: 'Import the base models and the model used for stacking:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 导入基础模型和用于堆叠的模型：
- en: Base models
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型
- en: from sklearn.neighbors import KNeighborsClassifier
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 来自sklearn.neighbors的KNeighborsClassifier
- en: from sklearn.svm import LinearSVC
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 来自sklearn.svm的LinearSVC
- en: Stacking model
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠模型
- en: from sklearn.linear_model import LogisticRegression
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 来自sklearn.linear_model的LogisticRegression
- en: Create a new training set with additional columns for predictions from base predictors.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的训练集，新增基础预测器的预测结果列。
- en: 'We need to create two new columns for predicted values from each model to be
    used as features for the ensemble model in both the test and training set. Since
    NumPy arrays are immutable, we will create a new array that will have the same
    number of rows as the training dataset, and two columns more than those in the
    training dataset. Once the dataset is created, let''s print it to see what it
    looks like:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每个模型的预测值创建两个新列，作为集成模型的特征，分别用于测试集和训练集。由于NumPy数组是不可变的，我们将创建一个新的数组，该数组的行数与训练数据集相同，列数比训练数据集多两个。创建完数据集后，我们可以打印它来查看效果：
- en: x_train_with_metapreds = np.zeros((x_train.shape[0], \
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_with_metapreds = np.zeros((x_train.shape[0], \
- en: x_train.shape[1]+2))
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: x_train.shape[1]+2))
- en: x_train_with_metapreds[:, :-2] = x_train
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_with_metapreds[:, :-2] = x_train
- en: x_train_with_metapreds[:, -2:] = -1
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_with_metapreds[:, -2:] = -1
- en: print(x_train_with_metapreds)
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: print(x_train_with_metapreds)
- en: 'The output is as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.12: The new columns for the predicted values'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12：预测值的新列'
- en: '](img/image-BQNA0P7L.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-BQNA0P7L.jpg)'
- en: 'Figure 6.12: The new columns for the predicted values'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：预测值的新列
- en: As we can see, there are two extra columns filled with -1 values at the end
    of each row.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，每行的末尾有两个额外的列，填充了-1值。
- en: Train base models using the k-fold strategy.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k折交叉验证策略训练基础模型。
- en: Let's take k=5\. For each of the five folds, train on the other four folds and
    predict on the fifth fold. These predictions should then be added to the placeholder
    columns for base predictions in the new NumPy array.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 设定k=5\. 对于这五个子集，在其他四个子集上进行训练，并在第五个子集上进行预测。这些预测结果应该被添加到新的NumPy数组中用于存放基本预测结果的占位符列中。
- en: First, we initialize the KFold object with the value of k and a random state
    to maintain reproducibility. The kf.split() function takes the dataset to split
    as an input and returns an iterator, with each element in the iterator corresponding
    to the list of indices in the training and validation folds respectively. These
    index values in each loop over the iterator can be used to subdivide the training
    data for training and prediction for each row.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化KFold对象，设置k值和随机状态以保持可复现性。kf.split()函数接受需要拆分的数据集作为输入，并返回一个迭代器，每个迭代器元素对应于训练和验证子集的索引列表。在每次循环中，这些索引值可以用于划分训练数据，并为每行执行训练和预测。
- en: 'Once the data is adequately divided, we train the two base predictors on four-fifths
    of the data and predict the values on the remaining one-fifth of the rows. These
    predictions are then inserted into the two placeholder columns we initialized
    with -1 in Step 2:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被适当地划分，我们在四分之五的数据上训练两个基础预测器，并在剩余的五分之一行上进行预测。然后，这些预测结果被插入我们在第2步初始化时填充为-1的两个占位符列中：
- en: kf = KFold(n_splits=5, random_state=11)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: kf = KFold(n_splits=5, random_state=11)
- en: 'for train_indices, val_indices in kf.split(x_train):'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 'for train_indices, val_indices in kf.split(x_train):'
- en: kfold_x_train, kfold_x_val = x_train[train_indices], \
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: kfold_x_train, kfold_x_val = x_train[train_indices], \
- en: x_train[val_indices]
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: x_train[val_indices]
- en: kfold_y_train, kfold_y_val = y_train[train_indices], \
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: kfold_y_train, kfold_y_val = y_train[train_indices], \
- en: y_train[val_indices]
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: y_train[val_indices]
- en: svm = LinearSVC(random_state=11, max_iter=1000)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: svm = LinearSVC(random_state=11, max_iter=1000)
- en: svm.fit(kfold_x_train, kfold_y_train)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: svm.fit(kfold_x_train, kfold_y_train)
- en: svm_pred = svm.predict(kfold_x_val)
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: svm_pred = svm.predict(kfold_x_val)
- en: knn = KNeighborsClassifier(n_neighbors=4)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: knn = KNeighborsClassifier(n_neighbors=4)
- en: knn.fit(kfold_x_train, kfold_y_train)
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: knn.fit(kfold_x_train, kfold_y_train)
- en: knn_pred = knn.predict(kfold_x_val)
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: knn_pred = knn.predict(kfold_x_val)
- en: x_train_with_metapreds[val_indices, -2] = svm_pred
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_with_metapreds[val_indices, -2] = svm_pred
- en: x_train_with_metapreds[val_indices, -1] = knn_pred
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: x_train_with_metapreds[val_indices, -1] = knn_pred
- en: Create a new validation set with additional columns for predictions from base predictors.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的验证集，添加来自基础预测器的预测的附加列。
- en: 'As we did in Step 2, we will add two placeholder columns for the base model
    predictions in the validation dataset as well:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们在步骤 2 中所做的，我们也将在验证数据集中为基础模型预测添加两列占位符：
- en: x_val_with_metapreds = np.zeros((x_val.shape[0], \
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: x_val_with_metapreds = np.zeros((x_val.shape[0], \
- en: x_val.shape[1]+2))
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: x_val.shape[1]+2))
- en: x_val_with_metapreds[:, :-2] = x_val
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: x_val_with_metapreds[:, :-2] = x_val
- en: x_val_with_metapreds[:, -2:] = -1
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: x_val_with_metapreds[:, -2:] = -1
- en: print(x_val_with_metapreds)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: print(x_val_with_metapreds)
- en: 'The output is as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 6.13: Additional columns for predictions from base predictors'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.13：来自基础预测器的预测的附加列'
- en: '](img/image-VC7JID4I.jpg)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-VC7JID4I.jpg)'
- en: 'Figure 6.13: Additional columns for predictions from base predictors'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13：来自基础预测器的预测的附加列
- en: Fit base models on the complete training set to get meta features for the validation
    set.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整的训练集上拟合基础模型，以获取验证集的元特征。
- en: 'Next, we will train the two base predictors on the complete training dataset
    to get the meta prediction values for the validation dataset. This is similar
    to what we did for each fold in Step 3:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在完整的训练数据集上训练两个基础预测器，以获取验证数据集的元预测值。这类似于我们在步骤 3 中为每个折叠所做的工作：
- en: svm = LinearSVC(random_state=11, max_iter=1000)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: svm = LinearSVC(random_state=11, max_iter=1000)
- en: svm.fit(x_train, y_train)
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: svm.fit(x_train, y_train)
- en: knn = KNeighborsClassifier(n_neighbors=4)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: knn = KNeighborsClassifier(n_neighbors=4)
- en: knn.fit(x_train, y_train)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: knn.fit(x_train, y_train)
- en: svm_pred = svm.predict(x_val)
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: svm_pred = svm.predict(x_val)
- en: knn_pred = knn.predict(x_val)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: knn_pred = knn.predict(x_val)
- en: x_val_with_metapreds[:, -2] = svm_pred
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: x_val_with_metapreds[:, -2] = svm_pred
- en: x_val_with_metapreds[:, -1] = knn_pred
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: x_val_with_metapreds[:, -1] = knn_pred
- en: Train the stacked model and use the final predictions to calculate accuracy.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 训练堆叠模型并使用最终预测计算准确度。
- en: 'The final step is to train the logistic regression model on all the columns
    of the training dataset plus the meta predictions from the base estimators. We
    use the model to find the prediction accuracies for both the training and validation datasets:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是使用训练数据集的所有列加上基础估算器的元预测来训练逻辑回归模型。我们使用该模型来计算训练集和验证集的预测准确度：
- en: lr = LogisticRegression(random_state=11)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: lr = LogisticRegression(random_state=11)
- en: lr.fit(x_train_with_metapreds, y_train)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: lr.fit(x_train_with_metapreds, y_train)
- en: lr_preds_train = lr.predict(x_train_with_metapreds)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: lr_preds_train = lr.predict(x_train_with_metapreds)
- en: lr_preds_val = lr.predict(x_val_with_metapreds)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: lr_preds_val = lr.predict(x_val_with_metapreds)
- en: print('Stacked Classifier:\n> Accuracy on training data = {:.4f}'\
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: print('堆叠分类器：\n> 训练数据集上的准确度 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据集上的准确度 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=lr_preds_train), \
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=lr_preds_train), \
- en: accuracy_score(y_true=y_val, y_pred=lr_preds_val)))
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=lr_preds_val)))
- en: 'The output is as follows:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: 'Stacked Classifier:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠分类器：
- en: Accuracy on training data = 0.7837
  id: totrans-421
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据集上的准确度 = 0.7837
- en: Accuracy on validation data = 0.8827
  id: totrans-422
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据集上的准确度 = 0.8827
- en: Note
  id: totrans-423
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Owing to randomization, you might get an output that varies slightly in comparison
    to the output presented in the preceding step.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机化，你可能会得到一个与前一步骤中呈现的输出略有不同的结果。
- en: Compare the accuracy with that of base models.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 比较与基础模型的准确度。
- en: 'To get a sense of the performance boost from stacking, we calculate the accuracies
    of the base predictors on the training and validation datasets and compare this
    with that of the stacked model:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解堆叠模型带来的性能提升，我们计算基础预测器在训练集和验证集上的准确度，并将其与堆叠模型的准确度进行比较：
- en: print('SVM:\n> Accuracy on training data = {:.4f}'\
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: print('SVM:\n> 训练数据集上的准确度 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据集上的准确度 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=svm.predict(x_train)), \
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=svm.predict(x_train)), \
- en: accuracy_score(y_true=y_val, y_pred=svm_pred)))
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=svm_pred)))
- en: print('kNN:\n> Accuracy on training data = {:.4f}'\
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: print('kNN:\n> 训练数据的准确度 = {:.4f}'\
- en: '''\n> Accuracy on validation data = {:.4f}''\'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '''\n> 验证数据的准确度 = {:.4f}''\'
- en: .format(accuracy_score(y_true=y_train, \
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: .format(accuracy_score(y_true=y_train, \
- en: y_pred=knn.predict(x_train)), \
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: y_pred=knn.predict(x_train)), \
- en: accuracy_score(y_true=y_val, y_pred=knn_pred)))
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: accuracy_score(y_true=y_val, y_pred=knn_pred)))
- en: 'The output is as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: SVM
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: SVM
- en: Accuracy on training data = 0.7205
  id: totrans-439
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据的准确度 = 0.7205
- en: Accuracy on validation data = 0.7430
  id: totrans-440
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据的准确度 = 0.7430
- en: 'kNN:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 'kNN:'
- en: Accuracy on training data = 0.7921
  id: totrans-442
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据的准确度 = 0.7921
- en: Accuracy on validation data = 0.6816
  id: totrans-443
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 验证数据的准确度 = 0.6816
- en: Note
  id: totrans-444
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Owing to randomization, you might get an output that varies slightly in comparison
    to the output presented in the preceding step.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机化，您可能会得到与前一步骤中呈现的输出略有不同的结果。
- en: As we can see, not only does the stacked model give us a validation accuracy
    that is significantly higher than either of the base predictors, but it also has
    the highest accuracy, nearly 89%, of all the ensemble models discussed in this
    chapter.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，堆叠模型不仅给出了比任何基模型显著更高的验证准确度，而且它的准确度几乎达到89%，是本章讨论的所有集成模型中最高的。
- en: Note
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to https://packt.live/37QANjZ.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/37QANjZ](https://packt.live/37QANjZ)。
- en: You can also run this example online at https://packt.live/2YljJ2D. You must
    execute the entire Notebook in order to get the desired result.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在 [https://packt.live/2YljJ2D](https://packt.live/2YljJ2D) 上在线运行此示例。您必须执行整个Notebook才能获得预期结果。
- en: 'Activity 6.01: Stacking with Standalone and Ensemble Algorithms'
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 活动 6.01：使用独立和集成算法的堆叠
- en: 'In this activity, we''ll use the Boston House Prices: Advanced Regression Techniques
    Database (available at https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
    or on GitHub at https://packt.live/2Vk002e).'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用波士顿房价：高级回归技术数据库（可以在 [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/)
    或 GitHub上的 [https://packt.live/2Vk002e](https://packt.live/2Vk002e) 获得）。
- en: This dataset is aimed toward solving a regression problem (that is, the target
    variable takes on a range of continuous values). In this activity, we will use
    decision trees, k-nearest neighbors, random forest, and gradient boosting algorithms
    to train individual regressors on the data. Then, we will build a stacked linear
    regression model that uses all these algorithms and compare the performance of
    each. We will use the mean absolute error (MAE) as the evaluation metric for this
    activity.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集旨在解决回归问题（即，目标变量具有一系列连续值）。在本活动中，我们将使用决策树、k最近邻、随机森林和梯度提升算法训练各个回归模型。然后，我们将构建一个堆叠线性回归模型，使用所有这些算法，并比较每个模型的表现。我们将使用平均绝对误差（MAE）作为本活动的评估标准。
- en: Note
  id: totrans-453
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The MAE function, mean_absolute_error(), can be used in a similar way to the
    accuracy_score() measure used previously.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: MAE函数（mean_absolute_error()）可以像以前使用的accuracy_score()度量一样使用。
- en: 'The steps to be performed are as follows:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 需要执行的步骤如下：
- en: Import the relevant libraries.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 导入相关库。
- en: Read the data.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据。
- en: Preprocess the dataset to remove null values and one-hot encoded categorical
    variables to prepare the data for modeling.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理数据集以去除空值，并进行独热编码以处理类别变量，为建模准备数据。
- en: Divide the dataset into train and validation DataFrames.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集划分为训练和验证DataFrame。
- en: Initialize dictionaries in which to store the train and validation MAE values.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化字典以存储训练和验证的MAE值。
- en: 'Train a DecisionTreeRegressor model (dt) with the following hyperparameters
    and save the scores:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下超参数训练一个DecisionTreeRegressor模型（dt）并保存分数：
- en: dt_params = {
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: dt_params = {
- en: '''criterion'': ''mae'','
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '''criterion'': ''mae'','
- en: '''min_samples_leaf'': 15,'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '''min_samples_leaf'': 15,'
- en: '''random_state'': 11'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '''random_state'': 11'
- en: '}'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Train a KNeighborsRegressor model (knn) with the following hyperparameters
    and save the scores:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下超参数训练一个KNeighborsRegressor模型（knn）并保存分数：
- en: knn_params = {
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: knn_params = {
- en: '''n_neighbors'': 5'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '''n_neighbors'': 5'
- en: '}'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Train a RandomForestRegressor model (rf) with the following hyperparameters
    and save the scores:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下超参数训练一个RandomForestRegressor模型（rf）并保存分数：
- en: rf_params = {
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: rf_params = {
- en: '''n_estimators'': 20,'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '''n_estimators'': 20,'
- en: '''criterion'': ''mae'','
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '''criterion'': ''mae'','
- en: '''max_features'': ''sqrt'','
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '''max_features'': ''sqrt'','
- en: '''min_samples_leaf'': 10,'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '''min_samples_leaf'': 10,'
- en: '''random_state'': 11,'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '''random_state'': 11,'
- en: '''n_jobs'': -1'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '''n_jobs'': -1'
- en: '}'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Train a GradientBoostingRegressor model (gbr) with the following hyperparameters
    and save the scores:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下超参数训练一个GradientBoostingRegressor模型（gbr）并保存分数：
- en: gbr_params = {
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: gbr_params = {
- en: '''n_estimators'': 20,'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '''n_estimators'': 20,'
- en: '''criterion'': ''mae'','
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '''criterion'': ''mae'','
- en: '''max_features'': ''sqrt'','
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '''max_features'': ''sqrt'','
- en: '''min_samples_leaf'': 10,'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '''min_samples_leaf'': 10,'
- en: '''random_state'': 11'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '''random_state'': 11'
- en: '}'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Prepare the training and validation datasets, with the four meta estimators
    having the same hyperparameters that were used in the previous steps.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 准备训练和验证数据集，四个元估计器的超参数与前面的步骤中使用的超参数相同。
- en: Train a LinearRegression model (lr) as the stacked model.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个线性回归模型（lr）作为堆叠模型。
- en: Visualize the train and validation errors for each individual model and the
    stacked model.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化每个单独模型和堆叠模型的训练和验证误差。
- en: 'The output will be as follows:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 6.14: Visualization of training and validation errors'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.14：训练和验证误差的可视化'
- en: '](img/image-ZZ2AC8ZH.jpg)'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image-ZZ2AC8ZH.jpg)'
- en: 'Figure 6.14: Visualization of training and validation errors'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：训练和验证误差的可视化
- en: Note
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found via this link.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以通过以下链接找到。
- en: Thus, we have successfully demonstrated how stacking as an ensembling technique
    proves to be superior to any individual machine learning model in terms of the
    validation set accuracy across different datasets.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功演示了堆叠作为一种集成技术，在不同数据集上，相较于任何单一机器学习模型，在验证集准确性方面表现更优。
- en: Summary
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started off with a discussion on overfitting and underfitting
    and how they can affect the performance of a model on unseen data. The chapter
    looked at ensemble modeling as a solution for these models and went on to discuss
    different ensemble methods that could be used, and how they could decrease the
    overall bias or variance encountered when making predictions. We first discussed
    bagging algorithms and introduced the concept of bootstrapping.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先讨论了过拟合和欠拟合，以及它们如何影响模型在未见数据上的表现。本章探讨了集成建模作为这些模型的解决方案，并进一步讨论了可以使用的不同集成方法，以及它们如何减少在进行预测时遇到的整体偏差或方差。我们首先讨论了袋装算法，并介绍了自助采样的概念。
- en: Then, we looked at random forest as a classic example of a bagged ensemble and
    solved exercises that involved building a bagging classifier and random forest
    classifier on the previously seen Titanic dataset. We then moved on to discussing
    boosting algorithms, how they successfully reduce bias in the system, and gained
    an understanding of how to implement adaptive boosting and gradient boosting.
    The last ensemble method we discussed was stacking, which, as we saw from the
    exercise, gave us the best accuracy score of all the ensemble methods we implemented.
    Although building an ensemble model is a great way to decrease bias and variance,
    and such models generally outperform any single model by itself, they themselves
    come with their own problems and use cases. While bagging is great when trying
    to avoid overfitting, boosting can reduce both bias and variance, though it may
    still have a tendency to overfit. Stacking, on the other hand, is a good choice
    for when one model performs well on a portion of the data while another model
    performs better on another portion of the data.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们以随机森林作为经典的袋装集成方法的例子，解决了涉及在之前看到的泰坦尼克号数据集上构建袋装分类器和随机森林分类器的练习。然后，我们讨论了提升算法，它们如何成功地减少系统中的偏差，并理解了如何实现自适应提升和梯度提升。我们讨论的最后一种集成方法是堆叠，从练习中我们看到，堆叠在所有实现的集成方法中给出了最佳的准确性得分。虽然构建集成模型是减少偏差和方差的好方法，并且这些模型通常比单一模型表现更好，但它们本身也有各自的问题和使用场景。袋装在避免过拟合时非常有效，而提升方法则能减少偏差和方差，尽管它仍然可能会有过拟合的趋势。堆叠则适用于当一个模型在部分数据上表现良好，而另一个模型在另一部分数据上表现更好时。
- en: In the next chapter, we will explore more ways to overcome the problems of overfitting
    and underfitting in detail by looking at validation techniques, that is, ways
    to judge our model's performance, and how to use different metrics as indicators
    to build the best possible model for our use case.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将详细探讨如何通过查看验证技术来克服过拟合和欠拟合的问题，验证技术即是判断我们模型表现的方式，以及如何使用不同的指标作为标志，为我们的使用案例构建最佳模型。
