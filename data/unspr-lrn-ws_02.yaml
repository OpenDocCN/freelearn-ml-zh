- en: 1\. Introduction to Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 聚类介绍
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: Finding insights and value in data is the ambitious promise that has been seen
    in the rise of machine learning. Within machine learning, there are predictive
    approaches to understanding dense information in deeper ways, as well as approaches
    to predicting outcomes based on changing inputs. In this chapter, we will learn
    what supervised learning and unsupervised learning are, and how they are applied
    to different use cases. Once you have a deeper understanding of where unsupervised
    learning is useful, we will walk through some foundational techniques that provide
    value quickly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中寻找洞察和价值是机器学习崛起时所承诺的雄心壮志。在机器学习中，有些方法是通过预测性方式深入理解密集信息，还有一些方法则是根据输入的变化来预测结果。在本章中，我们将了解监督学习和无监督学习是什么，以及它们如何应用于不同的使用场景。一旦你对无监督学习的应用领域有了更深入的理解，我们将逐步介绍一些能快速提供价值的基础技术。
- en: By the end of this chapter, you will be able to implement k-means clustering
    algorithms using built-in Python packages and calculate the silhouette score.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够使用内置的Python包实现k-means聚类算法，并计算轮廓系数。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Have you ever been asked to take a look at some data and came up empty handed?
    Maybe you weren't familiar with the dataset, or maybe you didn't even know where
    to start. This may have been extremely frustrating, and even embarrassing, depending
    on who asked you to take care of the task.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有被要求查看一些数据，但最终一无所获？也许你对数据集不熟悉，或者甚至不知道从哪里开始。这可能让你感到非常沮丧，甚至尴尬，尤其是如果是别人要求你负责这项任务的话。
- en: You are not alone, and, interestingly enough, there are many times the data
    itself is simply too confusing to be made sense of. As you try and figure out
    what all those numbers in your spreadsheet mean, you're most likely mimicking
    what many unsupervised algorithms do when they try to find meaning in data. The
    reality is that many unprocessed real-world datasets may not have any useful insights.
    One example to consider is the fact that these days, individuals generate massive
    amounts of granular data on a daily basis – whether it's their actions on a website,
    their purchase history, or what apps they use on their phone. If you were to look
    at this information on the surface, it would be a big, unorganized mess with no
    hope of clarity. Don't fret, however; this book will prepare you for such tall
    tasks so that you'll never be frustrated again when dealing with data exploration
    tasks, no matter how large.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你不是孤单的，有趣的是，数据本身有时也太混乱，无法理清。当你试图弄清楚电子表格中那些数字的意义时，你很可能是在模仿许多无监督算法的做法，它们试图从数据中找到意义。现实情况是，许多未经处理的真实世界数据集可能没有任何有用的见解。一个值得考虑的例子是，现如今，个人每天都会生成大量的细粒度数据——无论是他们在网站上的行为、购买历史，还是他们手机上使用的应用程序。如果你仅仅从表面上查看这些信息，它会是一团大杂烩，完全没有任何明确的意义。但别担心；本书将帮助你准备好应对这些繁重的任务，让你在处理数据探索任务时不再感到沮丧，无论数据有多庞大。
- en: For this book, we have developed some best-in-class content to help you understand
    how unsupervised algorithms work and where to use them. We'll cover some of the
    foundations of finding clusters in your data, how to reduce the size of your data
    so it's easier to understand, and how each of these sides of unsupervised learning
    can be applied in the real world. We hope you will come away from this book with
    a strong real-world understanding of unsupervised learning, the problems that
    it can solve, and those it cannot.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们开发了一些最佳的内容，帮助你理解无监督算法如何工作以及如何使用它们。我们将涵盖如何在数据中找到聚类的基础知识，如何减少数据的大小以便更容易理解，以及无监督学习的这些方面如何应用于现实世界。我们希望你能从本书中收获关于无监督学习的扎实实际理解，了解它能解决的问题以及无法解决的问题。
- en: Unsupervised Learning versus Supervised Learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习与监督学习
- en: '**Unsupervised learning** is the field of practice that helps find patterns
    in cluttered data and is one of the most exciting areas of development in machine
    learning today. If you have explored machine learning bookwork before, you are
    probably familiar with the common breakout of problems in either supervised or
    unsupervised learning. **Supervised learning** encompasses the problem set of
    having a labeled dataset that can be used to either classify data (for example,
    predicting smokers and non-smokers, if you''re looking at a lung health dataset)
    or finding a pattern in clearly defined data (for example, predicting the sale
    price of a home based on how many bedrooms it has). This model most closely mirrors
    an intuitive human approach to learning.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**是帮助在杂乱数据中寻找模式的领域，是当前机器学习中最令人兴奋的一个发展方向。如果你曾经研究过机器学习的书籍，你大概熟悉问题通常会分为监督学习或无监督学习这两种类型。**监督学习**涵盖了有标记数据集的问题集，数据集可以用于对数据进行分类（例如，在分析肺部健康数据集时预测吸烟者和非吸烟者）或在明确定义的数据中寻找模式（例如，根据房屋的卧室数量预测房屋的售价）。该模型最接近人类直观的学习方式。'
- en: 'For example, if you wanted to learn how to not burn your food with a basic
    understanding of cooking, you could build a dataset by putting your food on the
    burner and seeing how long it takes (input) for your food to burn (output). Eventually,
    as you continue to burn your food, you will build a mental model of when burning
    will occur and how to avoid it in the future. Development in supervised learning
    was once fast paced and valuable, but it has simmered down in recent years. Many
    of the obstacles around getting to know your data have already been tackled and
    are listed in the following image:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你想学习如何在基本的烹饪知识下避免烧焦食物，你可以通过将食物放在炉灶上并观察食物烧焦所需的时间（输入），来构建一个数据集（输出）。随着你不断地烧焦食物，你将建立一个心理模型，了解何时会发生烧焦并学会如何避免将来再发生。监督学习的发展曾经是快速且有价值的，但近年来已经逐渐平稳。许多关于了解数据的障碍已经得到解决，并在以下图片中列出：
- en: '![Figure 1.1: Differences between unsupervised and supervised learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：无监督学习和监督学习的区别](img/B15923_01_01.jpg)'
- en: '](img/B15923_01_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_01.jpg)'
- en: 'Figure 1.1: Differences between unsupervised and supervised learning'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：无监督学习和监督学习的区别
- en: Conversely, unsupervised learning encompasses the problem set of having a tremendous
    amount of data that is unlabeled. Labeled data, in this case, would be data that
    has a supplied "target" outcome that you are trying to find the correlation to
    with supplied data. For instance, in the preceding example, you know that your
    "target outcome" is whether your food was burned; this is an example of labeled
    data. Unlabeled data is when you do not know what the "target" outcome is, and
    you have only supplied input data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，**无监督学习**涵盖了拥有大量未标记数据的问题集。在这种情况下，标记数据是指具有给定“目标”结果的数据，你需要通过提供的数据找出与之相关的关系。例如，在前面的例子中，你知道你的“目标结果”是食物是否烧焦；这就是标记数据的一个例子。未标记数据则是你不知道“目标”结果是什么，只提供了输入数据。
- en: Building upon the previous example, imagine you were just dropped on planet
    Earth with zero knowledge of how cooking works. You are given 100 days, a stove,
    and a fridge full of food without any instructions on what to do. Your initial
    exploration of a kitchen could go in infinite directions. On day 10, you may finally
    learn how to open the fridge; on day 30, you may learn that food can go on the
    stove; and after many more days, you may unwittingly make an edible meal. As you
    can see, trying to find meaning in a kitchen devoid of adequate informational
    structure leads to very noisy data that is completely irrelevant to actually preparing
    a meal.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的例子，假设你被放到地球上，完全不知道烹饪是怎么回事。你被给了100天时间，一个炉灶，以及一冰箱满满的食物，却没有任何指导说明该做什么。你初次探索厨房的过程可能会走向无数个方向。第10天，你可能终于学会如何打开冰箱；第30天，你可能学会了食物可以放到炉灶上；而经过更多的时间，你可能会不小心做出一顿可以食用的饭菜。正如你所看到的，试图在没有足够信息结构的厨房里找到意义，往往会导致产生非常杂乱无序的数据，这些数据对于实际做饭完全没有用处。
- en: Unsupervised learning can be an answer to this problem. Looking back at your
    100 days of data, you can use **clustering** to find patterns of similar attributes
    across days and deduce which foods are similar and may lead to a "good" meal.
    However, unsupervised learning isn't a magical answer. Simply finding clusters
    can be just as likely to help you find pockets of similar, yet ultimately useless,
    data. Expanding on the cooking example, we can illustrate this shortcoming with
    the concept of the "third variable". Just because you have a cluster of really
    great recipes doesn't mean they are infallible. During your research, you may
    have found a unifying factor that all good meals were cooked on a stove. This
    does not mean that every meal cooked on a stove will be good, and you cannot easily
    jump to that conclusion for all future scenarios.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可能是解决此问题的答案。回顾你的100天数据，你可以使用**聚类**来找出各天之间相似属性的模式，并推断哪些食物相似，可能会导致一顿“好”饭。然而，无监督学习并不是一种神奇的解决方案。仅仅找到聚类，同样可能帮助你发现一些相似却最终没有用的数据。以烹饪为例，我们可以通过“第三变量”的概念来说明这一缺点。仅仅因为你有一个很好的菜谱聚类，并不意味着它们是无懈可击的。在你的研究过程中，你可能发现一个统一的因素，所有的好饭菜都是在炉子上做的。这并不意味着每一顿在炉子上做的饭菜都会是好饭菜，而且你不能轻易地把这个结论套用到所有未来的场景中。
- en: This challenge is what makes unsupervised learning so exciting. How can we find
    smarter techniques to speed up the process of finding clusters of information
    that are beneficial to our end goals? The following sections would help us answer
    this question.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是使无监督学习如此令人兴奋的挑战。我们如何找到更智能的技术，加速找到对最终目标有益的信息聚类的过程？以下章节将帮助我们回答这个问题。
- en: Clustering
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Clustering is the overarching process that involves finding groups of similar
    data that exist in your dataset, which can be extremely valuable if you are trying
    to find its underlying meaning. If you were a store owner and you wanted to understand
    which customers are more valuable without a set idea of what valuable is, clustering
    would be a great place to start to find patterns in your data. You may have a
    few high-level ideas of what denotes a valuable customer, but you aren't entirely
    sure in the face of a large mountain of available data. Through clustering, you
    can find commonalities among similar groups in your data. For example, if you
    look more deeply at a cluster of similar people, you may learn that everyone in
    that group visits your website for longer periods of time than others. This can
    show you what the value is and also provide a clean sample size for future supervised
    learning experiments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一个涉及在数据集中找到相似数据组的总体过程，如果你试图发现数据的潜在含义，它可以非常有价值。如果你是一个商店老板，想要了解哪些顾客更有价值，但又没有确切的价值定义，聚类是一个很好的起点，可以帮助你在数据中找出模式。你可能有一些关于什么是有价值顾客的高层次想法，但在面对大量可用数据时，你并不完全确定。通过聚类，你可以发现数据中相似群体之间的共性。例如，如果你深入观察一个相似群体的聚类，你可能会发现这个群体的每个人在你的网站上停留的时间都比其他人长。这可以帮助你了解什么是价值，并为未来的监督学习实验提供干净的样本数据。
- en: Identifying Clusters
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定聚类
- en: 'The following image shows two scatterplots:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了两个散点图：
- en: '![Figure 1.2: Two distinct scatterplots'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2：两个不同的散点图'
- en: '](img/B15923_01_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_02.jpg)'
- en: 'Figure 1.2: Two distinct scatterplots'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：两个不同的散点图
- en: 'The following image separates the two scatterplots into two distinct clusters:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像将这两个散点图分成了两个不同的聚类：
- en: '![Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3：清晰显示提供的数据集中存在聚类的散点图'
- en: '](img/B15923_01_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_03.jpg)'
- en: 'Figure 1.3: Scatterplots clearly showing clusters that exist in a provided
    dataset'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：清晰显示提供的数据集中存在聚类的散点图
- en: '*Figure 1.2* and *Figure 1.3* display randomly generated number pairs (x and
    y coordinates) pulled from two distinct Gaussian distributions centered at different
    locations. Simply by glancing at the first image, it should be obvious where the
    clusters exist in your data; in real life, it will never be this easy. Now that
    you know that the data can be clearly separated into two clusters, you can start
    to understand what differences exist between the two groups.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.2* 和 *图1.3* 显示了从两个不同位置的高斯分布中随机生成的数对（x 和 y 坐标）。仅通过看第一张图，应该很容易看出数据中存在的聚类；但在现实中，情况绝不会如此简单。现在你已经知道数据可以清晰地分成两个聚类，你可以开始理解这两个群体之间存在的差异。'
- en: Rewinding a bit from where unsupervised learning fits into the larger machine
    learning environment, let's begin by understanding the building blocks of clustering.
    The most basic definition finds clusters simply as groupings of similar data as
    subsets of a larger dataset. As an example, imagine that you had a room with 10
    people in it and each person had a job either in finance or as a scientist. If
    you told all the financial workers to stand together and all the scientists to
    do the same, you would have effectively formed two clusters based on job types.
    Finding clusters can be immensely valuable in identifying items that are more
    similar and, on the other end of the scale, quite different from one another.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从无监督学习在更大机器学习环境中的定位开始，让我们先了解聚类的基本构建块。最基本的定义将聚类视为一个较大数据集的子集，即相似数据的分组。举个例子，假设你有一个房间，里面有10个人，每个人的工作要么是在金融领域，要么是科学家。如果你让所有金融工作者站到一起，所有科学家也站在一起，你就已经根据职业类型有效地形成了两个聚类。寻找聚类可以在识别相似的项时非常有价值，而在另一端，它也能帮助识别出彼此间有较大差异的项。
- en: Two-Dimensional Data
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二维数据
- en: 'To understand this, imagine that you were given a simple 1,000-row dataset
    by your employer that had two columns of numerical data, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，假设你从雇主那里获得了一个简单的1,000行数据集，包含两列数值数据，如下所示：
- en: '![Figure 1.4: Two-dimensional raw data in an array'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.4：二维原始数据在数组中的表示'
- en: '](img/B15923_01_04.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_04.jpg)'
- en: 'Figure 1.4: Two-dimensional raw data in an array'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：二维原始数据在数组中的表示
- en: At first glance, this dataset provides no real structure or understanding.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，这个数据集没有提供任何实际的结构或理解。
- en: A **dimension** in a dataset is another way of simply counting the number of
    features available. In most organized data tables, you can view the number of
    features as the number of columns. So, using the 1,000-row dataset example of
    size (1,000 x 2), you will have 1,000 observations across two dimensions. Please
    note that dimensions of dataset should not be confused with the dimensions of
    an array.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的**维度**是另一种简单的方式，用来计数可用特征的数量。在大多数有组织的数据表中，你可以通过列数来查看特征的数量。因此，使用一个1,000行的数据集示例，大小为(1,000
    x 2)，你将有1,000个观测值，跨越两个维度。请注意，数据集的维度不应与数组的维度混淆。
- en: You begin by plotting the first column against the second column to get a better
    idea of what the data structure looks like. There will be plenty of times where
    the cause of differences between groups will prove to be underwhelming; however,
    the cases that have differences that you can take action on are extremely rewarding.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将第一列与第二列进行绘图，以便更好地了解数据结构。会有很多时候，组之间差异的原因可能显得微不足道；然而，能够采取行动的那些差异会极具回报。
- en: 'Exercise 1.01: Identifying Clusters in Data'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.01：识别数据中的聚类
- en: You are given two-dimensional plots of data that you suspect have clusters of
    similar data. Please look at the two-dimensional graphs provided in the exercise
    and identify the groups of data points to drive the point home that machine learning
    is important. Without using any algorithmic approaches, identify where these clusters
    exist in the data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到两个维度的数据图表，怀疑这些数据可能存在相似的聚类。请查看练习中提供的二维图表，并识别数据点的分组，目的是强调机器学习的重要性。在不使用任何算法方法的情况下，识别这些聚类在数据中的位置。
- en: 'This exercise will help you start building your intuition of how we can identify
    clusters using our own eyes and thought processes. As you complete this exercise,
    think of the rationale of why a group of data points should be considered a cluster
    versus a group that should not be considered a cluster. Follow these steps to
    complete this exercise:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习将帮助你建立直觉，了解我们如何使用自己的眼睛和思维过程来识别聚类。在完成这个练习时，思考为什么一组数据点应该被视为一个聚类，而另一组数据点则不应被视为聚类。按照以下步骤完成这个练习：
- en: 'Identify the clusters in the following scatterplot:![Figure 1.5: Two-dimensional
    scatterplot'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下散点图中识别聚类：![图1.5：二维散点图
- en: '](img/B15923_01_05.jpg)'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_05.jpg)'
- en: 'Figure 1.5: Two-dimensional scatterplot'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.5：二维散点图
- en: 'The clusters are as follows:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类如下：
- en: '![Figure 1.6: Clusters in the scatterplot'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图1.6：散点图中的聚类'
- en: '](img/B15923_01_06.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_06.jpg)'
- en: 'Figure 1.6: Clusters in the scatterplot'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.6：散点图中的聚类
- en: 'Identify the clusters in the following scatterplot:![Figure 1.7: Two-dimensional
    scatterplot'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下散点图中识别聚类：![图1.7：二维散点图
- en: '](img/B15923_01_07.jpg)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_07.jpg)'
- en: 'Figure 1.7: Two-dimensional scatterplot'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.7：二维散点图
- en: 'The clusters are as follows:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类如下：
- en: '![Figure 1.8: Clusters in the scatterplot'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.8：散点图中的聚类'
- en: '](img/B15923_01_08.jpg)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_08.jpg)'
- en: 'Figure 1.8: Clusters in the scatterplot'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.8：散点图中的聚类
- en: 'Identify the clusters in the following scatterplot:![Figure 1.9: Two-dimensional
    scatterplot'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下图中识别聚类：![图 1.9：二维散点图
- en: '](img/B15923_01_09.jpg)'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_09.jpg)'
- en: 'Figure 1.9: Two-dimensional scatterplot'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：二维散点图
- en: 'The clusters are as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类如下：
- en: '![Figure 1.10: Clusters in the scatterplot'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10：散点图中的聚类'
- en: '](img/B15923_01_10.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_10.jpg)'
- en: 'Figure 1.10: Clusters in the scatterplot'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：散点图中的聚类
- en: Most of these examples were likely quite easy for you to understand, and that's
    the point. The human brain and eyes are incredible at finding patterns in the
    real world. Within milliseconds of viewing each plot, you could tell what fitted
    together and what didn't. While it is easy for you, a computer does not have the
    ability to see and process plots in the same manner that we do.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子大多数对你来说应该非常容易理解，这正是重点。人类的大脑和眼睛在发现现实世界中的模式方面非常出色。在看到每个图表的毫秒内，你就能分辨出哪些是匹配的，哪些不是。虽然对你来说很容易，但计算机并不具备像我们一样查看和处理图表的能力。
- en: However, this is not always a bad thing. Look back at the preceding scatterplot.
    Were you able to find the six discrete clusters in the data just by looking at
    the plot? You probably found only three to four clusters in this scatterplot,
    while a computer would be able to see all six. The human brain is magnificent,
    but it also lacks the nuances that come with a strictly logic-based approach.
    Through algorithmic clustering, you will learn how to build a model that works
    even better than a human at these tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不总是坏事。回顾前面的散点图。你能仅通过查看该图就找到数据中的六个离散聚类吗？你可能只找到了三到四个聚类，而计算机能够看到所有六个。人类的大脑非常强大，但它也缺乏基于严格逻辑的方法所带来的细节。通过算法聚类，你将学会如何建立一个比人类更擅长完成这些任务的模型。
- en: We'll look at the clustering algorithm in the next section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中介绍聚类算法。
- en: Introduction to k-means Clustering
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 聚类简介
- en: Hopefully, by now, you can see that finding clusters is extremely valuable in
    a machine learning workflow. But, how can you actually find these clusters? One
    of the most basic yet popular approaches is to use a cluster analysis technique
    called **k-means clustering**. The k-means clustering works by searching for k
    clusters in your data and the workflow is actually quite intuitive. We will start
    with the no-math introduction to k-means, followed by an implementation in Python.
    **Cluster membership** refers to where the points go as the algorithm processes
    the data. Consider it like choosing players for a sports team, where all the players
    are in a pool but, for each successive run, the player is assigned to a team (in
    this case, a cluster).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到现在为止，你能看到在机器学习工作流中，寻找聚类是非常有价值的。但是，你如何实际找到这些聚类呢？一种最基本但又非常流行的方法是使用一种叫做 **k-means
    聚类** 的聚类分析技术。k-means 聚类通过在数据中寻找 k 个聚类来工作，这个工作流实际上非常直观。我们将从无数学推导的 k-means 介绍开始，然后进行
    Python 实现。**聚类成员**是指在算法处理数据时，点被分配到哪里。你可以将其看作是为一支运动队挑选队员，其中所有队员都在一个池中，但每一次运行后，队员会被分配到某个队伍（在这里是某个聚类）。
- en: No-Math k-means Walkthrough
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无数学推导的 k-means 算法演示
- en: 'The no-math algorithm for k-means clustering is pretty simple:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 无数学推导的 k-means 聚类算法非常简单：
- en: First, we'll pick "k" centroids, where "k" would be the expected distinct number
    of clusters. The value of k will be chosen by us and determines the type of clustering
    we obtain.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们会选择“k”个质心，其中“k”是我们预期的聚类数量。k 的值由我们选择，决定了我们得到的聚类类型。
- en: Then, we will place the "k" centroids at random places among the existing training
    data.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们会将“k”个质心随机放置在现有的训练数据中。
- en: Next, the distance from each centroid to all the points in the training data
    will be calculated. We will go into detail about distance functions shortly, but
    for now, let's just consider it as how far points are from each other.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将计算每个质心到训练数据中所有点的距离。我们稍后会详细讲解距离函数，但现在，我们先把它当作是点与点之间的距离。
- en: Now, all the training points will be grouped with their nearest centroid.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，所有的训练点将与其最近的质心分组。
- en: Isolating the grouped training points along with their respective centroid,
    calculate the mean data point in the group and move the previous centroid to the
    mean location.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分组的训练点与各自的质心分开，计算该组中的均值数据点，并将之前的质心移动到均值位置。
- en: This process is to be repeated until convergence or until maximum iteration
    limit has been achieved.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程需要重复，直到收敛或达到最大迭代次数。
- en: 'And that''s it. The following image represents original raw data:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。下面的图像表示原始原始数据：
- en: '![Figure 1.11: Original raw data charted on x and y coordinates'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11：原始原始数据在 x 和 y 坐标上的图示'
- en: '](img/B15923_01_11.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_11.jpg)'
- en: 'Figure 1.11: Original raw data charted on x and y coordinates'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：原始原始数据在 x 和 y 坐标上的图示
- en: 'Provided with the original data in the preceding image, we can visualize the
    iterative process of k-means by showing the predicted clusters in each step:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面图像中的原始数据，我们可以通过展示每一步的预测聚类来可视化 k-means 的迭代过程：
- en: '![Figure 1.12: Reading from left to right, red points are randomly initialized
    centroids,'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12：从左到右读取，红色点是随机初始化的质心，'
- en: and the closest data points are assigned to groupings of each centroid
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 并将最近的数据点分配给每个质心的分组
- en: '](img/B15923_01_12.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_12.jpg)'
- en: 'Figure 1.12: Reading from left to right, red points are randomly initialized
    centroids, and the closest data points are assigned to groupings of each centroid'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：从左到右读取，红色点是随机初始化的质心，并将最近的数据点分配给每个质心的分组
- en: K-means Clustering In-Depth Walkthrough
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 聚类深入讲解
- en: 'To understand k-means at a deeper level, let''s walk through the example that
    was provided in the introduction again with some of the math that supports k-means.
    The most important math that underpins this algorithm is the distance function.
    A distance function is basically any formula that allows you to quantitatively
    understand how far one object is from another, with the most popular one being
    the Euclidean distance formula. This formula works by subtracting the respective
    components of each point and squaring to remove negatives, followed by adding
    the resulting distances and square rooting them:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解 k-means，我们再通过引言中提供的示例，并结合一些支持 k-means 的数学原理进行讲解。支撑这个算法的最重要的数学公式是距离函数。距离函数基本上是任何能够定量地表示一个物体距离另一个物体有多远的公式，其中最常用的是欧几里得距离公式。这个公式通过相减每个点的相应组件并平方以去除负值，然后将结果的距离加起来并开平方：
- en: '![Figure 1.13: Euclidean distance formula'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：欧几里得距离公式'
- en: '](img/B15923_01_13.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_13.jpg)'
- en: 'Figure 1.13: Euclidean distance formula'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：欧几里得距离公式
- en: 'If you notice, the preceding formula holds true for data points having only
    two dimensions (the number of co-ordinates). A generic way of representing the
    preceding equation for higher-dimensional points is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，前面的公式适用于只有二维数据点（坐标数）的情况。表示高维数据点的一般方式如下：
- en: '![Figure 1.14: Euclidean distance formula for higher dimensional points'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：高维点的欧几里得距离公式'
- en: '](img/B15923_01_14.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_14.jpg)'
- en: 'Figure 1.14: Euclidean distance formula for higher dimensional points'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：高维点的欧几里得距离公式
- en: Let's see the terms involved in calculation of Euclidean distance between two
    points *p* and *q* in a higher dimensional space. Here, *n* is the number of dimensions
    of the two points. We compute the difference between the respective components
    of points *p* and *q* (*p*i and *q*i are known as the *i*th component of point
    *p* and *q* respectively) and square each of them. This squared value of the difference
    is summed up for all *n* components, and then square root of this sum is obtained.
    This value represents the Euclidean distance between point *p* and *q*. If you
    substitute n = 2 in the preceding equation, it will decompose to the equation
    represented in *Figure 1.13*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下在高维空间中计算两点 *p* 和 *q* 之间欧几里得距离时涉及的术语。这里，*n* 是这两个点的维度数。我们计算点 *p* 和 *q*
    的相应组件之间的差异（*p*i 和 *q*i 分别是点 *p* 和 *q* 的第 *i* 组件），并将每个差值平方。然后，将所有 *n* 维度的平方差值相加，再对这个和开平方。这个值表示点
    *p* 和 *q* 之间的欧几里得距离。如果你将 n = 2 代入前面的公式，它将分解为图 1.13 中表示的公式。
- en: Now coming back again to our discussion on k-means. Centroids are randomly set
    at the beginning as points in your n-dimensional space. Each of these centers
    is fed into the preceding formula as (*a*, *b*), and a point in your space is
    fed in as (*x*, *y*). Distances are calculated between each point and the coordinates
    of every centroid, with the centroid the shortest distance away chosen as the
    point's group.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在再次回到我们关于 k-means 的讨论。质心在开始时会随机设置为你 n 维空间中的点。这些质心中的每一个都作为 (*a*, *b*) 输入到前面的公式中，而空间中的一个点则作为
    (*x*, *y*) 输入。然后计算每个点与每个质心坐标之间的距离，选择距离最短的质心作为该点的分组。
- en: 'As an example, let''s pick three random centroids, an arbitrary point, and,
    using the Euclidean distance formula, calculate the distance from each point to
    the centroid:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们选择三个随机质心，一个任意点，并使用欧几里得距离公式，计算从每个点到质心的距离：
- en: 'Random centroids: [ (2,5), (8,3), (4,5) ].'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机质心：[ (2,5), (8,3), (4,5) ]。
- en: 'Arbitrary point x: (0, 8).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意点 x：（0, 8）。
- en: 'Distance from point to each centroid: [ 3.61, 9.43, 5.00 ].'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从点到每个质心的距离：[ 3.61, 9.43, 5.00 ]。
- en: Since the arbitrary point x is closest to the first centroid, it will be assigned
    to the first centroid.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任意点 x 最接近第一个质心，它将被分配给第一个质心。
- en: Alternative Distance Metric – Manhattan Distance
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 曼哈顿距离：替代距离度量
- en: Euclidean distance is the most common distance metric for many machine learning
    applications and is often known colloquially as the distance metric; however,
    it is not the only, or even the best, distance metric for every situation. Another
    popular distance metric that can be used for clustering is **Manhattan distance**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是许多机器学习应用中最常用的距离度量，并且通常被俗称为距离度量；然而，它并不是唯一的，甚至在某些情况下也不是最佳的距离度量。另一个流行的可以用于聚类的距离度量是**曼哈顿距离**。
- en: 'Manhattan distance is called as such because it mirrors the concept of traveling
    through a metropolis (such as New York City) that has many square blocks. Euclidean
    distance relies on diagonals due to its basis in Pythagorean theorem, while Manhattan
    distance constrains distance to only right angles. The formula for Manhattan distance
    is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离之所以叫这个名字，是因为它类似于在拥有许多方形街区的大城市（例如纽约市）中旅行的概念。欧几里得距离由于基于勾股定理而依赖于对角线，而曼哈顿距离则将距离限制为仅在直角之间。曼哈顿距离的公式如下：
- en: '![Figure 1.15: Manhattan distance formula'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15：曼哈顿距离公式'
- en: '](img/B15923_01_15.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_15.jpg)'
- en: 'Figure 1.15: Manhattan distance formula'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：曼哈顿距离公式
- en: Here *p*i and *q*i are the *i*th component of points *p* and *q*, respectively.
    Building upon our examples of Euclidean distance, where we want to find the distance
    between two points, if our two points were (1,2) and (2,3), then the Manhattan
    distance would equal `|1-2| + |2-3| = 1 + 1 = 2`. This functionality scales to
    any number of dimensions. In practice, Manhattan distance may outperform Euclidean
    distance when it comes to high dimensional data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *p*i 和 *q*i 分别是点 *p* 和 *q* 的第 *i* 个分量。基于我们之前讨论的欧几里得距离的例子，如果我们想找到两个点之间的距离，假设这两个点分别是
    (1,2) 和 (2,3)，那么曼哈顿距离将等于 `|1-2| + |2-3| = 1 + 1 = 2`。这种功能可以扩展到任何维度。在实践中，当处理高维数据时，曼哈顿距离可能会优于欧几里得距离。
- en: Deeper Dimensions
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高维度
- en: 'The preceding examples can be clearly visualized when your data is only two-dimensional.
    This is for convenience, to help drive the point home of how k-means works and
    could lead you into a false understanding of how easy clustering is. In many of
    your own applications, your data will likely be orders of magnitude larger to
    the point that it cannot be perceived by visualization (anything beyond three
    dimensions will be unperceivable to humans). In the previous examples, you could
    mentally work out a few two-dimensional lines to separate the data into its own
    groups. At higher dimensions, you will need to be aided by a computer to find
    an n-dimensional hyperplane that adequately separates the dataset. In practice,
    this is where clustering methods such as k-means provide significant value. The
    following image shows the two-dimensional, three-dimensional, and n-dimensional
    plots:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据只有二维时，前面的示例可以清晰地可视化。这是为了方便起见，帮助阐明 k-means 的工作原理，同时可能会导致你误解聚类的容易程度。在你自己的许多应用中，数据可能会大得多，以至于无法通过可视化来感知（超过三维的数据对于人类来说是不可感知的）。在前面的示例中，你可以通过脑中计算几个二维的线条来将数据分成不同的组。在更高维度的情况下，你需要借助计算机来找到一个
    n 维超平面，合理地分隔数据集。实际上，这就是 k-means 等聚类方法提供显著价值的地方。下图展示了二维、三维和 n 维的图形：
- en: '![Figure 1.16: Two-dimensional, three-dimensional, and n-dimensional plots'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.16：二维、三维和 n 维图形'
- en: '](img/B15923_01_16.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_16.jpg)'
- en: 'Figure 1.16: Two-dimensional, three-dimensional, and n-dimensional plots'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：二维、三维和 n 维图形
- en: In the next exercise, we will calculate Euclidean distance. We'll build our
    set of tools by using the `NumPy` and `Math` Python packages. `NumPy` is a scientific
    computing package for Python that pre-packages common mathematical functions in
    highly optimized formats.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将计算欧几里得距离。我们将通过使用 `NumPy` 和 `Math` Python 包来构建工具集。`NumPy` 是一个用于 Python
    的科学计算包，它将常见的数学函数预先打包为高度优化的格式。
- en: As the name implies, the `Math` package is a basic library that makes implementing
    foundational math building blocks, such as exponentials and square roots, much
    easier. By using a package such as `NumPy` or `Math`, we help cut down the time
    spent creating custom math functions from scratch and instead focus on developing
    our solutions. You will see how each of these packages is used in practice in
    the following exercise.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，`Math` 包是一个基础库，它使实现基础数学构件（如指数和平方根）变得更加容易。通过使用像 `NumPy` 或 `Math` 这样的包，我们可以减少从头开始创建自定义数学函数的时间，而是专注于开发解决方案。你将在接下来的练习中看到如何实际使用这些包。
- en: 'Exercise 1.02: Calculating Euclidean Distance in Python'
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.02：在 Python 中计算欧几里得距离
- en: In this exercise, we will create an example point along with three sample centroids
    to help illustrate how Euclidean distance works. Understanding this distance formula
    is the basis for the rest of our work in clustering.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将创建一个示例点，以及三个示例质心，帮助说明欧几里得距离的工作原理。理解这个距离公式是我们在聚类中工作的基础。
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成此练习：
- en: 'Open a Jupyter notebook and create a naïve formula that captures the direct
    math of Euclidean distance, as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Jupyter notebook，并创建一个简单的公式，直接计算欧几里得距离，如下所示：
- en: '[PRE0]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This approach is considered naïve because it performs element-wise calculations
    on your data points (slow) compared to a more real-world implementation using
    vectors and matrix math to achieve significant performance increases.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种方法被认为是幼稚的，因为它对数据点进行逐元素计算（较慢），与使用向量和矩阵数学的更现实的实现相比，后者可以显著提高性能。
- en: 'Create the data points in Python as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Python 中创建数据点，如下所示：
- en: '[PRE1]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the formula you created to calculate the Euclidean distance in *Step 1*:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你创建的公式来计算 *步骤 1* 中的欧几里得距离：
- en: '[PRE2]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The shortest distance between our point, `x`, and the centroids is `3.61`, which
    is equivalent to the distance between `(0, 8)` and `(2, 5)`. Since this is the
    minimum distance, our example point, `x`, will be assigned as a member of the
    first centroid's group.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的点 `x` 与质心之间的最短距离是 `3.61`，相当于 `(0, 8)` 和 `(2, 5)` 之间的距离。由于这是最小距离，我们的示例点 `x`
    将被分配到第一个质心所在的组。
- en: In this example, our formula was used on a single point, x (0, 8). Beyond this
    single point, the same process will be repeated for every remaining point in your
    dataset until each point is assigned to a cluster. After each point is assigned,
    the mean point is calculated among all of the points within each cluster. The
    calculation of the mean among these points is the same as calculating the mean
    between single integers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们的公式应用于一个单独的点x (0, 8)。在这个单点之后，相同的过程将对数据集中的每个剩余点重复，直到每个点都被分配到一个簇。每分配一个点后，都会计算该簇内所有点的均值。计算这些点的均值与计算单个整数的均值是一样的。
- en: While there was only one point in this example, by completing this process,
    you have effectively assigned a point to its first cluster using Euclidean distance.
    We'll build upon this approach with more than one point in the following exercise.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个示例中只有一个点，但通过完成这个过程，你实际上已经使用欧几里得距离将一个点分配到了它的第一个簇。我们将在下一个练习中使用多个点来扩展这个方法。
- en: Note
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VUvCuz](https://packt.live/2VUvCuz).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/2VUvCuz](https://packt.live/2VUvCuz)。
- en: You can also run this example online at [https://packt.live/3ebDwpZ](https://packt.live/3ebDwpZ).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是[https://packt.live/3ebDwpZ](https://packt.live/3ebDwpZ)。
- en: 'Exercise 1.03: Forming Clusters with the Notion of Distance'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.03：使用距离的概念形成簇
- en: 'It is very intuitive for our human minds to see groups of dots on a plot and
    determine which dots belong to discrete clusters. However, how do we ask a computer
    to repeat this same task? In this exercise, you''ll help teach a computer an approach
    to forming clusters of its own with the notion of distance. We will build upon
    how we use these distance metrics in the next exercise:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们人类的大脑来说，看到图中的点组并判断哪些点属于不同的簇是非常直观的。然而，如何让计算机重复执行这一任务呢？在这个练习中，你将帮助计算机学习如何使用“距离”的概念来形成自己的簇。我们将在下一个练习中进一步探讨如何使用这些距离度量：
- en: 'Create a list of points, [ (0,8), (3,8), (3,4) ], that are assigned to cluster
    one:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个点的列表，[ (0,8), (3,8), (3,4) ]，它们被分配到第一个簇：
- en: '[PRE3]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To find the new centroid among your list of points, calculate the mean point
    between all of the points. Calculation of the mean scales to infinite points,
    as you simply add the integers at each position and divide by the total number
    of points. For example, if your two points are (0,1,2) and (3,4,5), the mean calculation
    would be [ (0+3)/2, (1+4)/2, (2+5)/2 ]:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在点集列表中找到新的质心，计算所有点的平均点。均值的计算适用于无限个点，你只需将每个位置的整数相加并除以点的总数。例如，如果你的两个点是(0,1,2)和(3,4,5)，那么均值计算为[
    (0+3)/2, (1+4)/2, (2+5)/2 ]：
- en: '[PRE4]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE5]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After a new centroid is calculated, repeat the cluster membership calculation
    we looked at in *Exercise 1.02*, *Calculating Euclidean Distance in Python*, and
    then repeat the previous two steps to find the new cluster centroid. Eventually,
    the new cluster centroid will be the same as the centroid before the cluster membership
    calculation and the exercise will be complete. How many times this repeats depends
    on the data you are clustering.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算出新的质心后，重复我们在*练习 1.02*中看到的簇成员计算，*在 Python 中计算欧几里得距离*，然后重复前面两个步骤以找到新的簇质心。最终，新的簇质心将与簇成员计算之前的质心相同，练习将完成。这个过程重复多少次取决于你正在聚类的数据。
- en: Once you have moved the centroid location to the new mean point of (2, 6.67),
    you can compare it to the initial list of centroids you entered the problem with.
    If the new mean point is different than the centroid that is currently in your
    list, you will have to go through another iteration of the preceding two exercises.
    Once the new mean point you calculate is the same as the centroid you started
    the problem with, you have completed a run of k-means and reached a point called
    **convergence**. However, in practice, sometimes the number of iterations required
    to reach convergence is very large and such large computations may not be practically
    feasible. In such cases, we need to set a maximum limit to the number of iterations.
    Once this iteration limit is reached, we stop further processing.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦你将质心位置移动到新的均值点(2, 6.67)，你可以将其与最初输入问题时的质心列表进行比较。如果新的均值点与当前列表中的质心不同，你将需要再次执行前面两个练习的迭代过程。一旦你计算出的新均值点与开始时的质心相同，就意味着你完成了一次k-means运行，并达到了一个叫做**收敛**的点。然而，在实际操作中，达到收敛所需的迭代次数可能非常大，这种大规模计算在实践中可能不可行。在这种情况下，我们需要设定一个最大迭代次数限制。一旦达到这个迭代限制，我们就停止进一步处理。
- en: Note
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3iJ3JiT](https://packt.live/3iJ3JiT).
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要获取本节的源代码，请参考[https://packt.live/3iJ3JiT](https://packt.live/3iJ3JiT)。
- en: You can also run this example online at [https://packt.live/38CCpOG](https://packt.live/38CCpOG).
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以在线运行此示例，访问链接：[https://packt.live/38CCpOG](https://packt.live/38CCpOG)。
- en: In the next exercise, we will implement k-means from scratch. To do this, we
    will start employing common packages from the Python ecosystem that will serve
    as building blocks for the rest of your career. One of the most popular machine
    learning libraries is called scikit-learn ([https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)),
    which has many built-in algorithms and functions to support your understanding
    of how the algorithms work. We will also be using functions from SciPy ([https://docs.scipy.org/doc/scipy/reference/](https://docs.scipy.org/doc/scipy/reference/)),
    which is a package much like NumPy and abstracts away basic scientific math functions
    that allow for more efficient deployment. Finally, the next exercise will introduce
    `matplotlib` ([https://matplotlib.org/3.1.1/contents.html](https://matplotlib.org/3.1.1/contents.html)),
    which is a plotting library that creates graphical representations of the data
    you are working with.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将从头实现k-means。为此，我们将开始使用Python生态系统中的常见包，这些包将作为你职业生涯中其他部分的构建模块。最受欢迎的机器学习库之一是scikit-learn（[https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)），它包含许多内置的算法和函数，支持你理解算法的工作原理。我们还将使用SciPy（[https://docs.scipy.org/doc/scipy/reference/](https://docs.scipy.org/doc/scipy/reference/)）中的函数，它是一个类似NumPy的包，抽象出基本的科学数学函数，从而实现更高效的部署。最后，下一个练习将介绍`matplotlib`（[https://matplotlib.org/3.1.1/contents.html](https://matplotlib.org/3.1.1/contents.html)），这是一个绘图库，可以创建你所处理数据的图形表示。
- en: 'Exercise 1.04: K-means from Scratch – Part 1: Data Generation'
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.04：从头实现K-means – 第1部分：数据生成
- en: 'The next two exercises focus on the creation of exercise data and the implementation
    of k-means from scratch on your training data. This exercise relies on scikit-learn,
    an open source Python package that enables the fast prototyping of popular machine
    learning models. Within scikit-learn, we will be using the `datasets` functionality
    to create a synthetic blob dataset. In addition to harnessing the power of scikit-learn,
    we will also rely on Matplotlib, a popular plotting library for Python that makes
    it easy for us to visualize our data. To do this, perform the following steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个练习将专注于生成练习数据和从头实现k-means算法。这个练习依赖于scikit-learn，一个开源的Python包，能够快速原型化流行的机器学习模型。在scikit-learn中，我们将使用`datasets`功能来创建一个合成的blob数据集。除了利用scikit-learn的强大功能，我们还将依赖于Matplotlib，这是一个流行的Python绘图库，使我们能够轻松地可视化数据。为此，按照以下步骤操作：
- en: 'Import the necessary libraries:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE6]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more details on the `KMeans` library at [https://scikit-learn.org/stable/modules/clustering.html#k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means).
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在`KMeans`库的官方文档中找到更多细节：[https://scikit-learn.org/stable/modules/clustering.html#k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)。
- en: Generate a random cluster dataset to experiment on X = coordinate points, y
    = cluster labels, and define random centroids. We will achieve this with the `make_blobs`
    function that we imported from `sklearn.datasets`, which, as the name implies,
    generates blobs of data points.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机的聚类数据集来进行实验，X = 坐标点，y = 聚类标签，并定义随机的质心。我们将使用从`sklearn.datasets`导入的`make_blobs`函数来实现，正如其名字所示，它生成数据点的簇。
- en: '[PRE7]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here the `n_samples` parameter determines the total number of data points generated
    by the blobs. The `centers` parameter determines the number of centroids for the
    blob. The `n_feature` attribute defines the number of dimensions generated by
    the dataset. Here, the data will be two dimensional.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`n_samples`参数决定了由数据点簇生成的总数据点数量。`centers`参数决定了数据簇的质心数量。`n_feature`属性定义了数据集生成的维度数量。这里，数据将是二维的。
- en: In order to generate the same data points in all the iterations (which in turn
    are generated randomly) for reproducibility of results, we set the `random_state`
    parameter to `800`. Different values of the `random_state` parameter would yield
    different results. If we do not set the `random_state` parameter, each time on
    execution we will obtain different results.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了在所有迭代中生成相同的数据点（这些数据点是随机生成的），以保证结果的可复现性，我们将`random_state`参数设置为`800`。`random_state`参数的不同值会产生不同的结果。如果不设置`random_state`参数，每次执行时都会获得不同的结果。
- en: 'Print the data:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据：
- en: '[PRE8]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Plot the coordinate points using the scatterplot functionality we imported
    from `matplotlib.pyplot`. This function takes input lists of points and presents
    them graphically for ease of understanding. Please review the `matplotlib` documentation
    if you want to explore the parameters provided at a deeper level:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从`matplotlib.pyplot`导入的散点图功能绘制坐标点。此函数接受坐标点的输入列表，并将其图形化展示，以便更容易理解。如果您想更深入地了解该函数提供的参数，请查阅`matplotlib`文档：
- en: '[PRE10]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The plot appears as follows:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图如下所示：
- en: '![Figure 1.17: Plot of the coordinates'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.17：坐标图'
- en: '](img/B15923_01_17.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_17.jpg)'
- en: 'Figure 1.17: Plot of the coordinates'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.17：坐标图
- en: Print the array of `y`, which is the labels provided by scikit-learn and serves
    as the ground truth for comparison.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`y`数组，该数组是由scikit-learn提供的标签，并作为比较的基准真值。
- en: '[PRE11]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE12]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Plot the coordinate points with the correct cluster labels:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正确的聚类标签绘制坐标点：
- en: '[PRE13]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The plot appears as follows:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图如下所示：
- en: '![Figure 1.18: Plot of the coordinates with correct cluster labels'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.18：带有正确聚类标签的坐标图'
- en: '](img/B15923_01_18.jpg)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_18.jpg)'
- en: 'Figure 1.18: Plot of the coordinates with correct cluster labels'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18：带有正确聚类标签的坐标图
- en: By completing the preceding steps, you have generated the data and visually
    explored how it is put together. By visualizing the ground truth, you have established
    a baseline that provides a relative metric for algorithm accuracy.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成前面的步骤，您已经生成了数据，并通过可视化了解了数据的构成。通过可视化基准真值，您为算法准确性提供了一个相对度量的基准。
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2JM8Q1S](https://packt.live/2JM8Q1S).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/2JM8Q1S](https://packt.live/2JM8Q1S)。
- en: You can also run this example online at [https://packt.live/3ecjKdT](https://packt.live/3ecjKdT).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3ecjKdT](https://packt.live/3ecjKdT)在线运行此示例。
- en: With data in hand, in the next exercise, we'll continue by building your unsupervised
    learning toolset with an optimized version of the Euclidean distance function
    from the `SciPy` package, `cdist`. You will compare a non-vectorized, clearly
    understandable version of the approach with `cdist`, which has been specially
    tweaked for maximum performance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数据后，在接下来的练习中，我们将继续构建无监督学习工具集，使用从`SciPy`包中优化过的欧几里得距离函数`cdist`。您将比较一个非向量化的、易于理解的版本和`cdist`，后者经过特别优化，以实现最佳性能。
- en: 'Exercise 1.05: K-means from Scratch – Part 2: Implementing k-means'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.05：从头开始实现 K-means 算法——第二部分：实现 K-means
- en: Let's recreate these results on our own. We will go over an example implementing
    this with some optimizations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们自己重现这些结果。我们将通过一个示例进行讲解，并进行一些优化。
- en: Note
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This exercise is a continuation of the previous exercise and should be performed
    in the same Jupyter notebook.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习是上一练习的延续，应在同一个 Jupyter notebook 中进行。
- en: 'For this exercise, we will rely on SciPy, a Python package that allows easy
    access to highly optimized versions of scientific calculations. In particular,
    we will be implementing Euclidean distance with `cdist`, the functionally of which
    replicates the barebones implementation of our distance metric in a much more
    efficient manner. Follow these steps to complete this exercise:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习，我们将依赖于 SciPy，这是一个 Python 包，允许轻松访问高效优化的科学计算版本。特别地，我们将使用 `cdist` 实现欧几里得距离，其功能在更高效的方式下复制了我们度量距离的基本实现。请按照以下步骤完成此练习：
- en: 'The basis of this exercise will be comparing a basic implementation of Euclidean
    distance with an optimized version provided in SciPy. First, import the optimized
    Euclidean distance reference:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本练习的基础将是将一个基本的欧几里得距离实现与 SciPy 提供的优化版本进行比较。首先，导入优化的欧几里得距离参考：
- en: '[PRE14]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Identify a subset of `X` you want to explore. For this example, we are only
    selecting five points to make the lesson clearer; however, this approach scales
    to any number of points. We chose points 105-109, inclusive:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定您想要探索的 `X` 子集。对于本例，我们仅选择了五个点以使讲解更清晰；然而，这种方法适用于任意数量的点。我们选择了点 105-109（包括 105
    和 109）：
- en: '[PRE15]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE16]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Calculate the distances and choose the index of the shortest distance as a
    cluster:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算距离并选择最短距离的索引作为一个簇：
- en: '[PRE17]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the `k_means` function as follows and initialize the k-centroids randomly.
    Repeat this process until the difference between the new/old `centroids` equals
    `0`, using the `while` loop:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示定义 `k_means` 函数并随机初始化 k-中心。重复此过程，直到新旧 `centroids` 之间的差异为 `0`，使用 `while`
    循环：
- en: '[PRE18]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Do not break this code, as it might lead to an error.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请不要中断此代码，因为这可能会导致错误。
- en: 'Zip together the historical steps of centers and their labels:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将历史步骤的中心和它们的标签捆绑在一起：
- en: '[PRE19]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following plots may differ from what you can see if we haven''t set the
    random seed. The first plot looks as follows:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有设置随机种子，以下图可能会与您看到的不同。第一个图如下所示：
- en: '![Figure 1.19: First scatterplot'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.19：第一个散点图'
- en: '](img/B15923_01_19.jpg)'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_01_19.jpg)'
- en: 'Figure 1.19: First scatterplot'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19：第一个散点图
- en: 'The second plot appears as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图如下所示：
- en: '![Figure 1.20: Second scatterplot'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.20：第二个散点图'
- en: '](img/B15923_01_20.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_20.jpg)'
- en: 'Figure 1.20: Second scatterplot'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20：第二个散点图
- en: 'The third plot appears as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个图如下所示：
- en: '![Figure 1.21: Third scatterplot'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.21：第三个散点图'
- en: '](img/B15923_01_21.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_21.jpg)'
- en: 'Figure 1.21: Third scatterplot'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21：第三个散点图
- en: 'The fourth plot appears as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个图如下所示：
- en: '![Figure 1.22: Fourth scatterplot'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.22：第四个散点图'
- en: '](img/B15923_01_22.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_22.jpg)'
- en: 'Figure 1.22: Fourth scatterplot'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22：第四个散点图
- en: 'The fifth plot looks as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 第五个图如下所示：
- en: '![Figure 1.23: Fifth scatterplot'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.23：第五个散点图'
- en: '](img/B15923_01_23.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_23.jpg)'
- en: 'Figure 1.23: Fifth scatterplot'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.23：第五个散点图
- en: As shown by the preceding images, k-means takes an iterative approach to refine
    optimal clusters based on distance. The algorithm starts with random initialization
    of centroids and, depending on the complexity of the data, quickly finds the separations
    that make the most sense.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面图片所示，k-means 使用迭代方法根据距离优化集群。该算法从随机初始化中心开始，并根据数据的复杂度，迅速找到最合理的分割。
- en: Note
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2JM8Q1S](https://packt.live/2JM8Q1S).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2JM8Q1S](https://packt.live/2JM8Q1S)。
- en: You can also run this example online at [https://packt.live/3ecjKdT](https://packt.live/3ecjKdT).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址是 [https://packt.live/3ecjKdT](https://packt.live/3ecjKdT)。
- en: Clustering Performance – Silhouette Score
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类性能 – 轮廓分数
- en: Understanding the performance of unsupervised learning methods is inherently
    much more difficult than supervised learning methods because there is no ground
    truth available. For supervised learning, there are many robust performance metrics—the
    most straightforward of these being accuracy in the form of comparing model-predicted
    labels to actual labels and seeing how many the model got correct. Unfortunately,
    for clustering, we do not have labels to rely on and need to build an understanding
    of how "different" our clusters are. We achieve this with the silhouette score
    metric. We can also use silhouette scores to find the optimal "K" numbers of clusters
    for our unsupervised learning methods.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 理解无监督学习方法的表现本质上比监督学习方法要困难得多，因为没有可用的真实标签。对于监督学习，有许多强健的性能指标，其中最直接的就是通过比较模型预测的标签与实际标签，看看模型预测对了多少个。不幸的是，对于聚类，我们没有标签可依赖，需要建立对我们的簇有多“不同”的理解。我们通过轮廓得分指标来实现这一点。我们还可以使用轮廓得分来找到无监督学习方法的最佳“K”值。
- en: The silhouette metric works by analyzing how well a point fits within its cluster.
    The metric ranges from -1 to 1\. If the average silhouette score across your clustering
    is one, then you will have achieved perfect clusters and there will be minimal
    confusion about which point belongs where. For the plots in the previous exercise,
    the silhouette score will be much closer to one since the blobs are tightly condensed
    and there is a fair amount of distance between each blob. This is very rare, though;
    the silhouette score should be treated as an attempt at doing the best you can,
    since hitting one is highly unlikely. If the silhouette score is positive, it
    means that a point is closer to the assigned cluster than it is to the neighboring
    clusters. If the silhouette score is 0, then a point lies on the boundary between
    the assigned cluster and the next closest cluster. If the silhouette score is
    negative, then it indicates that a given point is assigned to an incorrect cluster,
    and the given point in fact likely belongs to a neighboring cluster.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓指标通过分析一个点在其簇内的拟合程度来工作。该指标的范围从-1到1。如果你的聚类的平均轮廓得分为1，那么你将获得完美的簇，并且对于每个点属于哪个簇几乎没有任何混淆。在前面的练习中的图形中，轮廓得分将接近1，因为这些簇紧密聚集，并且每个簇之间有相当大的距离。然而，这种情况非常罕见；轮廓得分应该被视为尽力而为的一个尝试，因为得到1是非常不可能的。如果轮廓得分为正，意味着该点离其分配的簇比离邻近的簇更近。如果轮廓得分为0，则表示该点位于分配的簇和下一个最接近簇之间的边界上。如果轮廓得分为负，则表示该点被分配到了错误的簇，实际上该点可能属于邻近的簇。
- en: 'Mathematically, the silhouette score calculation is quite straightforward and
    is obtained using the **Simplified Silhouette Index** (**SSI**):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，轮廓得分的计算相当简单，可以通过**简化轮廓指数**（**SSI**）来获得：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here *a*i is the distance from point *i* to its own cluster centroid, and bi
    is the distance from point *i* to the nearest cluster centroid.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*a*i是点*i*到其自身簇质心的距离，bi是点*i*到最近簇质心的距离。
- en: The intuition captured here is that ai represents how cohesive the cluster of
    point *i*' is as a clear cluster, and bi represents how far apart the clusters
    lie. We will use the optimized implementation of `silhouette_score` in scikit-learn
    in *Activity 1.01*, *Implementing k-means Clustering*. Using it is simple and
    only requires that you pass in the feature array and the predicted cluster labels
    from your k-means clustering method.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里捕捉到的直觉是，ai表示点*i*所在簇的凝聚力，即它作为一个清晰簇的程度，而bi表示各簇之间的距离。我们将在*活动1.01*中使用scikit-learn中`silhouette_score`的优化实现，*实现k-means聚类*。使用这个方法很简单，只需要传入特征数组和来自k-means聚类方法的预测簇标签。
- en: In the next exercise, we will use the `pandas` library ([https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/))
    to read a CSV file. Pandas is a Python library that makes data wrangling easier
    through the use of DataFrames. If you look back at the arrays you built with NumPy,
    you probably noticed that the resulting data structures are quite unwieldly. To
    extract subsets from the data, you had to index using brackets and specific numbers
    of rows. Instead of this approach, pandas allows an easier-to-understand approach
    to moving data around and getting it into the format necessary for unsupervised
    learning and other machine learning techniques.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将使用`pandas`库（[https://pandas.pydata.org/pandas-docs/stable/](https://pandas.pydata.org/pandas-docs/stable/)）来读取CSV文件。Pandas是一个Python库，通过DataFrame简化数据处理。如果回顾您使用NumPy构建的数组，您可能会注意到，最终的数据结构相当笨重。为了从数据中提取子集，您需要使用方括号和特定的行号进行索引。与这种方法不同，pandas提供了一种更易于理解的数据操作方法，使得将数据移到适合无监督学习及其他机器学习技术所需的格式变得更加简单。
- en: Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To read data in Python, you will use `variable_name = pd.read_csv('file_name.csv',
    header=None)`
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中读取数据，您将使用`variable_name = pd.read_csv('file_name.csv', header=None)`
- en: Here, the parameter `header = None` explicitly mentions that there is no presence
    of column names. If your file contains column names, then retain those default
    values. Also, if you specify `header = None` for a file which contains column
    names, Pandas will treat the row containing names of column as the row containing
    data only.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，参数`header = None`明确表示没有列名。如果您的文件包含列名，则保留这些默认值。此外，如果您的文件包含列名，但您指定了`header
    = None`，Pandas将把包含列名的行当作数据行处理。
- en: 'Exercise 1.06: Calculating the Silhouette Score'
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.06：计算轮廓系数
- en: 'In this exercise, we will calculate the silhouette score of a dataset with
    a fixed number of clusters. For this, we will use the seeds dataset, which is
    available at [https://packt.live/2UQA79z](https://packt.live/2UQA79z). The following
    note outlines more information regarding this dataset, in addition to further
    exploration in the next activity. For the purpose of this exercise, please disregard
    the specific details of what this dataset is comprised of as it is of greater
    importance to learn about the silhouette score. As we go into the next activity,
    you will gain more context as needed to create a smart machine learning system.
    Follow these steps to complete this exercise:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将计算一个固定聚类数的数据集的轮廓系数。为此，我们将使用种子数据集，数据集可在[https://packt.live/2UQA79z](https://packt.live/2UQA79z)获得。以下说明提供了关于此数据集的更多信息，并将在下一个活动中进行进一步探索。为了完成此练习，请忽略该数据集具体包含哪些内容，因为学习轮廓系数更为重要。在下一个活动中，您将根据需要获得更多的背景知识，以创建智能的机器学习系统。按照以下步骤完成此练习：
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    It can be accessed at [https://packt.live/2UQA79z](https://packt.live/2UQA79z)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集来自[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。它可以通过[https://packt.live/2UQA79z](https://packt.live/2UQA79z)访问。
- en: 'Citation: Contributors gratefully acknowledge support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：贡献者衷心感谢波兰科学院农业物理研究所（位于卢布林）对他们工作的支持。
- en: 'Load the seeds data file using pandas, a package that makes data wrangling
    much easier through the use of DataFrames:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas加载种子数据文件，pandas是一个通过DataFrame简化数据处理的包：
- en: '[PRE21]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Separate the `X` features, since we want to treat this as an unsupervised learning
    problem:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离`X`特征，因为我们希望将其视为无监督学习问题：
- en: '[PRE22]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Bring back the `k_means` function we made earlier for reference:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带回我们之前制作的`k_means`函数供参考：
- en: '[PRE23]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Convert our seeds `X` feature DataFrame into a `NumPy` matrix:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的种子`X`特征DataFrame转换为`NumPy`矩阵：
- en: '[PRE24]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Run our `k_means` function on the seeds matrix:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在种子矩阵上运行我们的`k_means`函数：
- en: '[PRE25]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Calculate the silhouette score for the `Area (''A'')` and `Length of Kernel
    (''LK'')` columns:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`Area ('A')`和`Length of Kernel ('LK')`列的轮廓系数：
- en: '[PRE26]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output should be similar to the following:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容：
- en: '[PRE27]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this exercise, we calculated the silhouette score for the `Area ('A')` and
    `Length of Kernel ('LK')` columns of the seeds dataset. We will use this technique
    in the next activity to determine the performance of our k-means clustering algorithm.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们计算了`Area ('A')`和`Length of Kernel ('LK')`列的轮廓系数。我们将在下一个活动中使用这一技术来确定我们k-means聚类算法的性能。
- en: Note
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2UOqW9H](https://packt.live/2UOqW9H).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2UOqW9H](https://packt.live/2UOqW9H)。
- en: You can also run this example online at [https://packt.live/3fbtJ4y](https://packt.live/3fbtJ4y).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3fbtJ4y](https://packt.live/3fbtJ4y)在线运行此示例。
- en: 'Activity 1.01: Implementing k-means Clustering'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.01：实现k-means聚类
- en: You are implementing a k-means clustering algorithm from scratch to prove that
    you understand how it works. You will be using the seeds dataset provided by the
    UCI ML repository. The seeds dataset is a classic in the data science world and
    contains features of wheat kernels that are used to predict three different types
    of wheat species. The download location can be found later in this activity.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在从头实现一个k-means聚类算法，以证明您理解其工作原理。您将使用UCI ML库提供的种子数据集。种子数据集在数据科学界是经典之作，包含了小麦种子特征，用于预测三种不同类型的小麦品种。下载位置将在本活动中后续提供。
- en: For this activity, you should use Matplotlib, NumPy, scikit-learn metrics, and
    pandas.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，您应该使用Matplotlib、NumPy、scikit-learn指标和pandas。
- en: By loading and reshaping data easily, you can focus more on learning k-means
    instead of writing data loader functionality.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通过轻松加载和重塑数据，您可以将更多精力集中在学习k-means上，而不是编写数据加载器功能。
- en: 'The following seeds data features are provided for reference:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 提供以下种子数据特征供参考：
- en: '[PRE28]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The aim here is to truly understand how k-means works. To do so, you need to
    take what you have learned in the previous sections and implement k-means from
    scratch in Python.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是深入理解k-means是如何工作的。为此，您需要将前面章节中学到的知识付诸实践，在Python中从头实现k-means。
- en: 'Please open your favorite editing platform and try the following steps:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 请打开您喜欢的编辑平台，尝试以下步骤：
- en: Using `NumPy` or the `math` package and the Euclidean distance formula, write
    a function that calculates the distance between two coordinates.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`NumPy`或`math`包和欧几里得距离公式，编写一个函数，计算两个坐标之间的距离。
- en: Write a function that calculates the distance from the centroids to each of
    the points in your dataset and returns the cluster membership.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数，计算数据集中每个点到质心的距离，并返回聚类成员关系。
- en: 'Write a k-means function that takes in a dataset and the number of clusters
    (K) and returns the final cluster centroids, as well as the data points that make
    up that cluster''s membership. After implementing k-means from scratch, apply
    your custom algorithm to the seeds dataset, which is located here: [https://packt.live/2Xh2FdS](https://packt.live/2Xh2FdS).'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个k-means函数，该函数接受数据集和聚类数（K）作为输入，返回最终的聚类质心以及组成该聚类的成员数据点。在从头实现k-means之后，将您的自定义算法应用于种子数据集，数据集位于：[https://packt.live/2Xh2FdS](https://packt.live/2Xh2FdS)。
- en: Note
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    It can be accessed at [https://packt.live/2Xh2FdS](https://packt.live/2Xh2FdS).
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本数据集来源于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)，可以通过[https://packt.live/2Xh2FdS](https://packt.live/2Xh2FdS)访问。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCI机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: 'Citation: Contributors gratefully acknowledge support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引用：贡献者衷心感谢波兰科学院农业物理研究所对其工作的支持。
- en: Remove the classes supplied in this dataset and see whether your k-means algorithm
    can group the different wheat species into their proper groups just based on plant
    characteristics!
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除此数据集中提供的类别，并查看您的k-means算法是否能仅凭植物特征将不同的小麦品种分组到正确的类别中！
- en: Calculate the silhouette score using the scikit-learn implementation.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现计算轮廓系数。
- en: 'In completing this exercise, you have gained hands-on experience of tuning
    a k-means clustering algorithm for a real-world dataset. The seeds dataset is
    seen as a classic "hello world"-type problem in the data science space and is
    helpful for testing foundational techniques. Your final clustering algorithm should
    do a decent job of finding the three clusters of wheat species types that exist
    in the data, as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本练习后，你已获得了在真实世界数据集上调优k-means聚类算法的实践经验。种子数据集被视为数据科学领域中的经典“hello world”问题，有助于测试基础技术。你的最终聚类算法应能有效地识别数据中存在的三种小麦物种类型，如下所示：
- en: '![Figure 1.24: Expected plot of three clusters of wheat species'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.24：预期的三种小麦物种聚类图'
- en: '](img/B15923_01_24.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_01_24.jpg)'
- en: 'Figure 1.24: Expected plot of three clusters of wheat species'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.24：预期的三种小麦物种聚类图
- en: Note
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 418.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第418页找到。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explored what clustering is and why it is important
    in a variety of data challenges. Building upon this foundation of clustering knowledge,
    you implemented k-means, which is one of the simplest, yet most popular, methods
    of unsupervised learning. If you have reached this summary and can repeat what
    k-means does step by step to a friend, then you're ready to move on to more complex
    forms of clustering.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了什么是聚类及其在各种数据挑战中的重要性。在掌握聚类知识的基础上，你实现了k-means，这是最简单但也是最受欢迎的无监督学习方法之一。如果你已经阅读了本总结，并能逐步向朋友解释k-means的工作原理，那么你已经准备好进入更复杂的聚类形式。
- en: From here, we will be moving on to hierarchical clustering, which, in one configuration,
    reuses the centroid learning approach that we used in k-means. We will build upon
    this approach by outlining additional clustering methodologies and approaches
    in the next chapter.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍层次聚类，在某种配置下，它重用了我们在k-means中使用的质心学习方法。我们将在下一章通过概述更多的聚类方法和技术，进一步发展这一方法。
